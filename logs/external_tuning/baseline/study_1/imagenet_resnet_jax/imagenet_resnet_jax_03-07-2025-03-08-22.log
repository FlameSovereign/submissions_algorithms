python submission_runner.py --framework=jax --workload=imagenet_resnet --submission_path=prize_qualification_baselines/external_tuning/jax_nadamw_full_budget.py --data_dir=/data/imagenet/jax --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=submissions/rolling_leaderboard/external_tuning/baseline/study_1 --overwrite=True --save_checkpoints=False --rng_seed=2076705849 --imagenet_v2_data_dir=/data/imagenet/jax --tuning_ruleset=external --tuning_search_space=prize_qualification_baselines/external_tuning/tuning_search_space.json --num_tuning_trials=5 --hparam_start_index=4 --hparam_end_index=5 2>&1 | tee -a /logs/imagenet_resnet_jax_03-07-2025-03-08-22.log
2025-03-07 03:08:39.486713: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1741316919.888082       9 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1741316919.976036       9 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
I0307 03:09:30.213095 140114851837120 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_1/imagenet_resnet_jax.
I0307 03:09:33.032100 140114851837120 xla_bridge.py:884] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA
I0307 03:09:33.036125 140114851837120 xla_bridge.py:884] Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory
I0307 03:09:33.049833 140114851837120 submission_runner.py:606] Using RNG seed 2076705849
I0307 03:09:38.745136 140114851837120 submission_runner.py:615] --- Tuning run 5/5 ---
I0307 03:09:38.745319 140114851837120 submission_runner.py:620] Creating tuning directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_1/imagenet_resnet_jax/trial_5.
I0307 03:09:38.745529 140114851837120 logger_utils.py:97] Saving hparams to /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_1/imagenet_resnet_jax/trial_5/hparams.json.
I0307 03:09:38.988321 140114851837120 submission_runner.py:218] Initializing dataset.
I0307 03:09:40.400418 140114851837120 dataset_info.py:690] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0307 03:09:40.785325 140114851837120 reader.py:261] Creating a tf.data.Dataset reading 1024 files located in folders: /data/imagenet/jax/imagenet2012/5.1.0.
I0307 03:09:41.061070 140114851837120 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split train, from /data/imagenet/jax/imagenet2012/5.1.0
I0307 03:09:42.688720 140114851837120 submission_runner.py:229] Initializing model.
I0307 03:10:06.637539 140114851837120 submission_runner.py:272] Initializing optimizer.
I0307 03:10:07.770235 140114851837120 submission_runner.py:279] Initializing metrics bundle.
I0307 03:10:07.770487 140114851837120 submission_runner.py:301] Initializing checkpoint and logger.
I0307 03:10:07.771648 140114851837120 checkpoints.py:1101] Found no checkpoint files in /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_1/imagenet_resnet_jax/trial_5 with prefix checkpoint_
I0307 03:10:07.771754 140114851837120 submission_runner.py:321] Saving meta data to /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_1/imagenet_resnet_jax/trial_5/meta_data_0.json.
I0307 03:10:08.348506 140114851837120 submission_runner.py:325] Saving flags to /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_1/imagenet_resnet_jax/trial_5/flags_0.json.
I0307 03:10:08.662737 140114851837120 submission_runner.py:337] Starting training loop.
I0307 03:11:06.400853 139978310919936 logging_writer.py:48] [0] global_step=0, grad_norm=0.6642182469367981, loss=6.905142784118652
I0307 03:11:06.770888 140114851837120 spec.py:321] Evaluating on the training split.
I0307 03:11:07.241950 140114851837120 dataset_info.py:690] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0307 03:11:07.265557 140114851837120 reader.py:261] Creating a tf.data.Dataset reading 1024 files located in folders: /data/imagenet/jax/imagenet2012/5.1.0.
I0307 03:11:07.307982 140114851837120 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split train, from /data/imagenet/jax/imagenet2012/5.1.0
I0307 03:11:26.885003 140114851837120 spec.py:333] Evaluating on the validation split.
I0307 03:11:27.457725 140114851837120 dataset_info.py:690] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0307 03:11:27.504317 140114851837120 reader.py:261] Creating a tf.data.Dataset reading 64 files located in folders: /data/imagenet/jax/imagenet2012/5.1.0.
I0307 03:11:27.717481 140114851837120 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split validation, from /data/imagenet/jax/imagenet2012/5.1.0
I0307 03:12:06.596318 140114851837120 spec.py:349] Evaluating on the test split.
I0307 03:12:07.065954 140114851837120 dataset_info.py:690] Load dataset info from /data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0
I0307 03:12:07.110389 140114851837120 reader.py:261] Creating a tf.data.Dataset reading 16 files located in folders: /data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0.
I0307 03:12:07.148624 140114851837120 logging_logger.py:49] Constructing tf.data.Dataset imagenet_v2 for split test, from /data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0
I0307 03:12:40.796940 140114851837120 submission_runner.py:469] Time since start: 152.13s, 	Step: 1, 	{'train/accuracy': 0.0009367027669213712, 'train/loss': 6.911309719085693, 'validation/accuracy': 0.0009999999310821295, 'validation/loss': 6.9110894203186035, 'validation/num_examples': 50000, 'test/accuracy': 0.0009000000427477062, 'test/loss': 6.911230564117432, 'test/num_examples': 10000, 'score': 58.10783839225769, 'total_duration': 152.13403797149658, 'accumulated_submission_time': 58.10783839225769, 'accumulated_eval_time': 94.02588820457458, 'accumulated_logging_time': 0}
I0307 03:12:40.861787 139958438303488 logging_writer.py:48] [1] accumulated_eval_time=94.0259, accumulated_logging_time=0, accumulated_submission_time=58.1078, global_step=1, preemption_count=0, score=58.1078, test/accuracy=0.0009, test/loss=6.91123, test/num_examples=10000, total_duration=152.134, train/accuracy=0.000936703, train/loss=6.91131, validation/accuracy=0.001, validation/loss=6.91109, validation/num_examples=50000
I0307 03:13:17.315962 139958220224256 logging_writer.py:48] [100] global_step=100, grad_norm=0.6852942705154419, loss=6.814773082733154
I0307 03:13:54.293330 139958438303488 logging_writer.py:48] [200] global_step=200, grad_norm=0.8125346302986145, loss=6.560222625732422
I0307 03:14:31.039427 139958220224256 logging_writer.py:48] [300] global_step=300, grad_norm=1.00302255153656, loss=6.315918922424316
I0307 03:15:08.854429 139958438303488 logging_writer.py:48] [400] global_step=400, grad_norm=1.9059172868728638, loss=6.0299482345581055
I0307 03:15:46.300152 139958220224256 logging_writer.py:48] [500] global_step=500, grad_norm=2.1473851203918457, loss=5.879052639007568
I0307 03:16:24.018403 139958438303488 logging_writer.py:48] [600] global_step=600, grad_norm=5.436864376068115, loss=5.653287410736084
I0307 03:17:01.420168 139958220224256 logging_writer.py:48] [700] global_step=700, grad_norm=4.532629489898682, loss=5.527723789215088
I0307 03:17:38.990586 139958438303488 logging_writer.py:48] [800] global_step=800, grad_norm=3.1195220947265625, loss=5.3544158935546875
I0307 03:18:16.889464 139958220224256 logging_writer.py:48] [900] global_step=900, grad_norm=6.931826114654541, loss=5.178353786468506
I0307 03:18:54.916092 139958438303488 logging_writer.py:48] [1000] global_step=1000, grad_norm=6.28564977645874, loss=5.05932092666626
I0307 03:19:32.509000 139958220224256 logging_writer.py:48] [1100] global_step=1100, grad_norm=5.9979753494262695, loss=5.037680625915527
I0307 03:20:11.340538 139958438303488 logging_writer.py:48] [1200] global_step=1200, grad_norm=6.322093963623047, loss=4.915658950805664
I0307 03:20:49.118203 139958220224256 logging_writer.py:48] [1300] global_step=1300, grad_norm=4.331947326660156, loss=4.743523120880127
I0307 03:21:10.984873 140114851837120 spec.py:321] Evaluating on the training split.
I0307 03:21:23.061204 140114851837120 spec.py:333] Evaluating on the validation split.
I0307 03:22:08.493863 140114851837120 spec.py:349] Evaluating on the test split.
I0307 03:22:10.411840 140114851837120 submission_runner.py:469] Time since start: 721.75s, 	Step: 1358, 	{'train/accuracy': 0.1669921875, 'train/loss': 4.284590244293213, 'validation/accuracy': 0.13679999113082886, 'validation/loss': 4.54769229888916, 'validation/num_examples': 50000, 'test/accuracy': 0.09920000284910202, 'test/loss': 5.027939319610596, 'test/num_examples': 10000, 'score': 568.0293328762054, 'total_duration': 721.7490515708923, 'accumulated_submission_time': 568.0293328762054, 'accumulated_eval_time': 153.45283269882202, 'accumulated_logging_time': 0.0838019847869873}
I0307 03:22:10.434523 139958446696192 logging_writer.py:48] [1358] accumulated_eval_time=153.453, accumulated_logging_time=0.083802, accumulated_submission_time=568.029, global_step=1358, preemption_count=0, score=568.029, test/accuracy=0.0992, test/loss=5.02794, test/num_examples=10000, total_duration=721.749, train/accuracy=0.166992, train/loss=4.28459, validation/accuracy=0.1368, validation/loss=4.54769, validation/num_examples=50000
I0307 03:22:26.238780 139958455088896 logging_writer.py:48] [1400] global_step=1400, grad_norm=6.341775417327881, loss=4.557075500488281
I0307 03:23:04.016545 139958446696192 logging_writer.py:48] [1500] global_step=1500, grad_norm=4.4254560470581055, loss=4.446526050567627
I0307 03:23:41.575120 139958455088896 logging_writer.py:48] [1600] global_step=1600, grad_norm=10.255730628967285, loss=4.535643100738525
I0307 03:24:19.265453 139958446696192 logging_writer.py:48] [1700] global_step=1700, grad_norm=8.633370399475098, loss=4.423858165740967
I0307 03:24:56.635207 139958455088896 logging_writer.py:48] [1800] global_step=1800, grad_norm=5.345402240753174, loss=4.24707555770874
I0307 03:25:34.376671 139958446696192 logging_writer.py:48] [1900] global_step=1900, grad_norm=4.710641860961914, loss=4.057476043701172
I0307 03:26:11.689933 139958455088896 logging_writer.py:48] [2000] global_step=2000, grad_norm=9.643660545349121, loss=4.079148292541504
I0307 03:26:49.030702 139958446696192 logging_writer.py:48] [2100] global_step=2100, grad_norm=5.558612823486328, loss=3.9604835510253906
I0307 03:27:26.919723 139958455088896 logging_writer.py:48] [2200] global_step=2200, grad_norm=5.332648277282715, loss=3.8353357315063477
I0307 03:28:04.510266 139958446696192 logging_writer.py:48] [2300] global_step=2300, grad_norm=5.89664363861084, loss=3.8085029125213623
I0307 03:28:41.956105 139958455088896 logging_writer.py:48] [2400] global_step=2400, grad_norm=4.60947322845459, loss=3.83154559135437
I0307 03:29:19.081965 139958446696192 logging_writer.py:48] [2500] global_step=2500, grad_norm=3.6764776706695557, loss=3.694878578186035
I0307 03:29:57.171282 139958455088896 logging_writer.py:48] [2600] global_step=2600, grad_norm=3.3424174785614014, loss=3.470569610595703
I0307 03:30:35.511815 139958446696192 logging_writer.py:48] [2700] global_step=2700, grad_norm=3.830954074859619, loss=3.5661211013793945
I0307 03:30:40.542982 140114851837120 spec.py:321] Evaluating on the training split.
I0307 03:30:52.285884 140114851837120 spec.py:333] Evaluating on the validation split.
I0307 03:31:13.443032 140114851837120 spec.py:349] Evaluating on the test split.
I0307 03:31:15.298161 140114851837120 submission_runner.py:469] Time since start: 1266.64s, 	Step: 2714, 	{'train/accuracy': 0.32669004797935486, 'train/loss': 3.140263795852661, 'validation/accuracy': 0.2846199870109558, 'validation/loss': 3.3939952850341797, 'validation/num_examples': 50000, 'test/accuracy': 0.2150000035762787, 'test/loss': 3.982311964035034, 'test/num_examples': 10000, 'score': 1077.9494602680206, 'total_duration': 1266.635375738144, 'accumulated_submission_time': 1077.9494602680206, 'accumulated_eval_time': 188.20797300338745, 'accumulated_logging_time': 0.11478090286254883}
I0307 03:31:15.334783 139958455088896 logging_writer.py:48] [2714] accumulated_eval_time=188.208, accumulated_logging_time=0.114781, accumulated_submission_time=1077.95, global_step=2714, preemption_count=0, score=1077.95, test/accuracy=0.215, test/loss=3.98231, test/num_examples=10000, total_duration=1266.64, train/accuracy=0.32669, train/loss=3.14026, validation/accuracy=0.28462, validation/loss=3.394, validation/num_examples=50000
I0307 03:31:48.396461 139958446696192 logging_writer.py:48] [2800] global_step=2800, grad_norm=3.9148330688476562, loss=3.5444860458374023
I0307 03:32:26.137560 139958455088896 logging_writer.py:48] [2900] global_step=2900, grad_norm=3.9591968059539795, loss=3.5886449813842773
I0307 03:33:04.530573 139958446696192 logging_writer.py:48] [3000] global_step=3000, grad_norm=3.048278331756592, loss=3.351832389831543
I0307 03:33:42.741426 139958455088896 logging_writer.py:48] [3100] global_step=3100, grad_norm=3.6868746280670166, loss=3.4992380142211914
I0307 03:34:20.703448 139958446696192 logging_writer.py:48] [3200] global_step=3200, grad_norm=4.54324197769165, loss=3.4026222229003906
I0307 03:34:58.838646 139958455088896 logging_writer.py:48] [3300] global_step=3300, grad_norm=3.952907085418701, loss=3.2721965312957764
I0307 03:35:37.217434 139958446696192 logging_writer.py:48] [3400] global_step=3400, grad_norm=3.156049966812134, loss=3.290841817855835
I0307 03:36:15.702818 139958455088896 logging_writer.py:48] [3500] global_step=3500, grad_norm=5.440057277679443, loss=3.199150562286377
I0307 03:36:53.871968 139958446696192 logging_writer.py:48] [3600] global_step=3600, grad_norm=2.535860538482666, loss=3.1018269062042236
I0307 03:37:32.350319 139958455088896 logging_writer.py:48] [3700] global_step=3700, grad_norm=2.540585517883301, loss=2.9571895599365234
I0307 03:38:10.528505 139958446696192 logging_writer.py:48] [3800] global_step=3800, grad_norm=3.0557291507720947, loss=3.16086745262146
I0307 03:38:48.630656 139958455088896 logging_writer.py:48] [3900] global_step=3900, grad_norm=2.8747377395629883, loss=2.964712619781494
I0307 03:39:26.842556 139958446696192 logging_writer.py:48] [4000] global_step=4000, grad_norm=2.7875308990478516, loss=3.064246416091919
I0307 03:39:45.371740 140114851837120 spec.py:321] Evaluating on the training split.
I0307 03:39:56.729047 140114851837120 spec.py:333] Evaluating on the validation split.
I0307 03:40:15.070736 140114851837120 spec.py:349] Evaluating on the test split.
I0307 03:40:16.969601 140114851837120 submission_runner.py:469] Time since start: 1808.31s, 	Step: 4049, 	{'train/accuracy': 0.4413265287876129, 'train/loss': 2.4695708751678467, 'validation/accuracy': 0.3942199945449829, 'validation/loss': 2.749650478363037, 'validation/num_examples': 50000, 'test/accuracy': 0.2955000102519989, 'test/loss': 3.4448471069335938, 'test/num_examples': 10000, 'score': 1587.8253746032715, 'total_duration': 1808.3067936897278, 'accumulated_submission_time': 1587.8253746032715, 'accumulated_eval_time': 219.8057723045349, 'accumulated_logging_time': 0.15940308570861816}
I0307 03:40:16.993776 139958455088896 logging_writer.py:48] [4049] accumulated_eval_time=219.806, accumulated_logging_time=0.159403, accumulated_submission_time=1587.83, global_step=4049, preemption_count=0, score=1587.83, test/accuracy=0.2955, test/loss=3.44485, test/num_examples=10000, total_duration=1808.31, train/accuracy=0.441327, train/loss=2.46957, validation/accuracy=0.39422, validation/loss=2.74965, validation/num_examples=50000
I0307 03:40:36.883864 139958446696192 logging_writer.py:48] [4100] global_step=4100, grad_norm=2.4714457988739014, loss=2.9218037128448486
I0307 03:41:14.860364 139958455088896 logging_writer.py:48] [4200] global_step=4200, grad_norm=2.1857001781463623, loss=2.898159980773926
I0307 03:41:53.249112 139958446696192 logging_writer.py:48] [4300] global_step=4300, grad_norm=2.539231300354004, loss=2.8392086029052734
I0307 03:42:31.396338 139958455088896 logging_writer.py:48] [4400] global_step=4400, grad_norm=2.6007840633392334, loss=2.8866801261901855
I0307 03:43:09.712044 139958446696192 logging_writer.py:48] [4500] global_step=4500, grad_norm=3.0393881797790527, loss=2.7465617656707764
I0307 03:43:47.582282 139958455088896 logging_writer.py:48] [4600] global_step=4600, grad_norm=2.8137712478637695, loss=2.6731302738189697
I0307 03:44:25.792278 139958446696192 logging_writer.py:48] [4700] global_step=4700, grad_norm=2.3340067863464355, loss=2.7384958267211914
I0307 03:45:03.726356 139958455088896 logging_writer.py:48] [4800] global_step=4800, grad_norm=2.2971458435058594, loss=2.8735711574554443
I0307 03:45:42.026852 139958446696192 logging_writer.py:48] [4900] global_step=4900, grad_norm=2.1183063983917236, loss=2.7056262493133545
I0307 03:46:20.306046 139958455088896 logging_writer.py:48] [5000] global_step=5000, grad_norm=3.0050222873687744, loss=2.7270328998565674
I0307 03:46:58.535765 139958446696192 logging_writer.py:48] [5100] global_step=5100, grad_norm=1.8725526332855225, loss=2.5957491397857666
I0307 03:47:36.658350 139958455088896 logging_writer.py:48] [5200] global_step=5200, grad_norm=1.7982556819915771, loss=2.741605520248413
I0307 03:48:14.987116 139958446696192 logging_writer.py:48] [5300] global_step=5300, grad_norm=2.187596559524536, loss=2.736098527908325
I0307 03:48:47.006943 140114851837120 spec.py:321] Evaluating on the training split.
I0307 03:48:58.394129 140114851837120 spec.py:333] Evaluating on the validation split.
I0307 03:49:19.326984 140114851837120 spec.py:349] Evaluating on the test split.
I0307 03:49:21.141345 140114851837120 submission_runner.py:469] Time since start: 2352.48s, 	Step: 5385, 	{'train/accuracy': 0.5342793464660645, 'train/loss': 1.9912950992584229, 'validation/accuracy': 0.4821600019931793, 'validation/loss': 2.259072780609131, 'validation/num_examples': 50000, 'test/accuracy': 0.36830002069473267, 'test/loss': 2.961452007293701, 'test/num_examples': 10000, 'score': 2097.672739982605, 'total_duration': 2352.478568315506, 'accumulated_submission_time': 2097.672739982605, 'accumulated_eval_time': 253.9401466846466, 'accumulated_logging_time': 0.1956174373626709}
I0307 03:49:21.216342 139958455088896 logging_writer.py:48] [5385] accumulated_eval_time=253.94, accumulated_logging_time=0.195617, accumulated_submission_time=2097.67, global_step=5385, preemption_count=0, score=2097.67, test/accuracy=0.3683, test/loss=2.96145, test/num_examples=10000, total_duration=2352.48, train/accuracy=0.534279, train/loss=1.9913, validation/accuracy=0.48216, validation/loss=2.25907, validation/num_examples=50000
I0307 03:49:27.294184 139958446696192 logging_writer.py:48] [5400] global_step=5400, grad_norm=2.353290557861328, loss=2.5980730056762695
I0307 03:50:05.454434 139958455088896 logging_writer.py:48] [5500] global_step=5500, grad_norm=2.101269245147705, loss=2.5561540126800537
I0307 03:50:43.332391 139958446696192 logging_writer.py:48] [5600] global_step=5600, grad_norm=2.210263729095459, loss=2.535104513168335
I0307 03:51:21.659837 139958455088896 logging_writer.py:48] [5700] global_step=5700, grad_norm=2.206078052520752, loss=2.450174331665039
I0307 03:51:59.693017 139958446696192 logging_writer.py:48] [5800] global_step=5800, grad_norm=2.2288107872009277, loss=2.4876608848571777
I0307 03:52:37.857205 139958455088896 logging_writer.py:48] [5900] global_step=5900, grad_norm=2.3511369228363037, loss=2.705270290374756
I0307 03:53:16.015128 139958446696192 logging_writer.py:48] [6000] global_step=6000, grad_norm=2.031294107437134, loss=2.41278338432312
I0307 03:53:54.393023 139958455088896 logging_writer.py:48] [6100] global_step=6100, grad_norm=2.43917179107666, loss=2.4978439807891846
I0307 03:54:32.833108 139958446696192 logging_writer.py:48] [6200] global_step=6200, grad_norm=3.095970630645752, loss=2.4513254165649414
I0307 03:55:10.715764 139958455088896 logging_writer.py:48] [6300] global_step=6300, grad_norm=2.1371142864227295, loss=2.5028016567230225
I0307 03:55:49.182565 139958446696192 logging_writer.py:48] [6400] global_step=6400, grad_norm=2.8738362789154053, loss=2.4988293647766113
I0307 03:56:27.131083 139958455088896 logging_writer.py:48] [6500] global_step=6500, grad_norm=2.3101141452789307, loss=2.482828140258789
I0307 03:57:05.183356 139958446696192 logging_writer.py:48] [6600] global_step=6600, grad_norm=2.0492897033691406, loss=2.400455951690674
I0307 03:57:43.663520 139958455088896 logging_writer.py:48] [6700] global_step=6700, grad_norm=1.8186883926391602, loss=2.2740159034729004
I0307 03:57:51.404408 140114851837120 spec.py:321] Evaluating on the training split.
I0307 03:58:02.763318 140114851837120 spec.py:333] Evaluating on the validation split.
I0307 03:58:25.464941 140114851837120 spec.py:349] Evaluating on the test split.
I0307 03:58:27.314346 140114851837120 submission_runner.py:469] Time since start: 2898.65s, 	Step: 6721, 	{'train/accuracy': 0.5532923936843872, 'train/loss': 1.8886045217514038, 'validation/accuracy': 0.49771997332572937, 'validation/loss': 2.190770387649536, 'validation/num_examples': 50000, 'test/accuracy': 0.38050001859664917, 'test/loss': 2.974987506866455, 'test/num_examples': 10000, 'score': 2607.7103476524353, 'total_duration': 2898.6515686511993, 'accumulated_submission_time': 2607.7103476524353, 'accumulated_eval_time': 289.8500511646271, 'accumulated_logging_time': 0.2792384624481201}
I0307 03:58:27.358782 139958446696192 logging_writer.py:48] [6721] accumulated_eval_time=289.85, accumulated_logging_time=0.279238, accumulated_submission_time=2607.71, global_step=6721, preemption_count=0, score=2607.71, test/accuracy=0.3805, test/loss=2.97499, test/num_examples=10000, total_duration=2898.65, train/accuracy=0.553292, train/loss=1.8886, validation/accuracy=0.49772, validation/loss=2.19077, validation/num_examples=50000
I0307 03:58:57.631370 139958455088896 logging_writer.py:48] [6800] global_step=6800, grad_norm=2.399522304534912, loss=2.3187897205352783
I0307 03:59:35.446302 139958446696192 logging_writer.py:48] [6900] global_step=6900, grad_norm=1.966009259223938, loss=2.2761642932891846
I0307 04:00:13.988907 139958455088896 logging_writer.py:48] [7000] global_step=7000, grad_norm=2.819890022277832, loss=2.1969985961914062
I0307 04:00:52.278278 139958446696192 logging_writer.py:48] [7100] global_step=7100, grad_norm=2.3180925846099854, loss=2.2817373275756836
I0307 04:01:30.711946 139958455088896 logging_writer.py:48] [7200] global_step=7200, grad_norm=2.1173083782196045, loss=2.1855812072753906
I0307 04:02:08.823400 139958446696192 logging_writer.py:48] [7300] global_step=7300, grad_norm=2.3198626041412354, loss=2.2624268531799316
I0307 04:02:46.906961 139958455088896 logging_writer.py:48] [7400] global_step=7400, grad_norm=2.0313541889190674, loss=2.2721548080444336
I0307 04:03:25.249509 139958446696192 logging_writer.py:48] [7500] global_step=7500, grad_norm=2.2391281127929688, loss=2.284548759460449
I0307 04:04:04.127384 139958455088896 logging_writer.py:48] [7600] global_step=7600, grad_norm=2.463541269302368, loss=2.1958112716674805
I0307 04:04:42.319648 139958446696192 logging_writer.py:48] [7700] global_step=7700, grad_norm=1.7751238346099854, loss=2.242327928543091
I0307 04:05:21.141032 139958455088896 logging_writer.py:48] [7800] global_step=7800, grad_norm=1.8583849668502808, loss=2.2035717964172363
I0307 04:05:59.409615 139958446696192 logging_writer.py:48] [7900] global_step=7900, grad_norm=1.508779764175415, loss=2.297419548034668
I0307 04:06:37.628611 139958455088896 logging_writer.py:48] [8000] global_step=8000, grad_norm=1.9981215000152588, loss=2.3156867027282715
I0307 04:06:57.318661 140114851837120 spec.py:321] Evaluating on the training split.
I0307 04:07:08.067094 140114851837120 spec.py:333] Evaluating on the validation split.
I0307 04:07:30.608865 140114851837120 spec.py:349] Evaluating on the test split.
I0307 04:07:32.416054 140114851837120 submission_runner.py:469] Time since start: 3443.75s, 	Step: 8052, 	{'train/accuracy': 0.5985132455825806, 'train/loss': 1.6628903150558472, 'validation/accuracy': 0.5411199927330017, 'validation/loss': 1.9675277471542358, 'validation/num_examples': 50000, 'test/accuracy': 0.4173000156879425, 'test/loss': 2.738194227218628, 'test/num_examples': 10000, 'score': 3117.529729127884, 'total_duration': 3443.7532715797424, 'accumulated_submission_time': 3117.529729127884, 'accumulated_eval_time': 324.94742250442505, 'accumulated_logging_time': 0.33165979385375977}
I0307 04:07:32.444697 139958446696192 logging_writer.py:48] [8052] accumulated_eval_time=324.947, accumulated_logging_time=0.33166, accumulated_submission_time=3117.53, global_step=8052, preemption_count=0, score=3117.53, test/accuracy=0.4173, test/loss=2.73819, test/num_examples=10000, total_duration=3443.75, train/accuracy=0.598513, train/loss=1.66289, validation/accuracy=0.54112, validation/loss=1.96753, validation/num_examples=50000
I0307 04:07:51.448205 139958455088896 logging_writer.py:48] [8100] global_step=8100, grad_norm=1.7102675437927246, loss=2.2774906158447266
I0307 04:08:29.888243 139958446696192 logging_writer.py:48] [8200] global_step=8200, grad_norm=2.2149534225463867, loss=2.23954439163208
I0307 04:09:07.721213 139958455088896 logging_writer.py:48] [8300] global_step=8300, grad_norm=1.556829810142517, loss=2.097536087036133
I0307 04:09:45.816332 139958446696192 logging_writer.py:48] [8400] global_step=8400, grad_norm=2.7497076988220215, loss=2.214071035385132
I0307 04:10:24.063601 139958455088896 logging_writer.py:48] [8500] global_step=8500, grad_norm=1.978114128112793, loss=2.1811397075653076
I0307 04:11:02.461760 139958446696192 logging_writer.py:48] [8600] global_step=8600, grad_norm=2.2583203315734863, loss=2.113957166671753
I0307 04:11:40.885169 139958455088896 logging_writer.py:48] [8700] global_step=8700, grad_norm=1.4150352478027344, loss=2.11657452583313
I0307 04:12:19.097389 139958446696192 logging_writer.py:48] [8800] global_step=8800, grad_norm=1.6915607452392578, loss=2.207265853881836
I0307 04:12:57.861474 139958455088896 logging_writer.py:48] [8900] global_step=8900, grad_norm=1.7256736755371094, loss=2.199481248855591
I0307 04:13:36.145980 139958446696192 logging_writer.py:48] [9000] global_step=9000, grad_norm=2.4202089309692383, loss=2.1975011825561523
I0307 04:14:14.699228 139958455088896 logging_writer.py:48] [9100] global_step=9100, grad_norm=1.5280410051345825, loss=2.0879523754119873
I0307 04:14:53.212576 139958446696192 logging_writer.py:48] [9200] global_step=9200, grad_norm=1.9089839458465576, loss=2.2046151161193848
I0307 04:15:31.689791 139958455088896 logging_writer.py:48] [9300] global_step=9300, grad_norm=1.8605443239212036, loss=2.0152342319488525
I0307 04:16:02.417279 140114851837120 spec.py:321] Evaluating on the training split.
I0307 04:16:13.366490 140114851837120 spec.py:333] Evaluating on the validation split.
I0307 04:16:36.714109 140114851837120 spec.py:349] Evaluating on the test split.
I0307 04:16:38.554243 140114851837120 submission_runner.py:469] Time since start: 3989.89s, 	Step: 9381, 	{'train/accuracy': 0.616609513759613, 'train/loss': 1.5495808124542236, 'validation/accuracy': 0.5597599744796753, 'validation/loss': 1.8752597570419312, 'validation/num_examples': 50000, 'test/accuracy': 0.4319000244140625, 'test/loss': 2.651991605758667, 'test/num_examples': 10000, 'score': 3627.354485273361, 'total_duration': 3989.89146232605, 'accumulated_submission_time': 3627.354485273361, 'accumulated_eval_time': 361.0843553543091, 'accumulated_logging_time': 0.36838865280151367}
I0307 04:16:38.600652 139958446696192 logging_writer.py:48] [9381] accumulated_eval_time=361.084, accumulated_logging_time=0.368389, accumulated_submission_time=3627.35, global_step=9381, preemption_count=0, score=3627.35, test/accuracy=0.4319, test/loss=2.65199, test/num_examples=10000, total_duration=3989.89, train/accuracy=0.61661, train/loss=1.54958, validation/accuracy=0.55976, validation/loss=1.87526, validation/num_examples=50000
I0307 04:16:46.382578 139958455088896 logging_writer.py:48] [9400] global_step=9400, grad_norm=2.9726152420043945, loss=2.1010446548461914
I0307 04:17:24.883241 139958446696192 logging_writer.py:48] [9500] global_step=9500, grad_norm=1.7234114408493042, loss=1.9099180698394775
I0307 04:18:03.376250 139958455088896 logging_writer.py:48] [9600] global_step=9600, grad_norm=1.9011094570159912, loss=2.1433441638946533
I0307 04:18:41.524786 139958446696192 logging_writer.py:48] [9700] global_step=9700, grad_norm=1.70677649974823, loss=2.204739809036255
I0307 04:19:20.002073 139958455088896 logging_writer.py:48] [9800] global_step=9800, grad_norm=1.717026948928833, loss=1.9939265251159668
I0307 04:19:58.655240 139958446696192 logging_writer.py:48] [9900] global_step=9900, grad_norm=1.576187252998352, loss=2.0144925117492676
I0307 04:20:36.843721 139958455088896 logging_writer.py:48] [10000] global_step=10000, grad_norm=1.5836519002914429, loss=2.039613723754883
I0307 04:21:15.376172 139958446696192 logging_writer.py:48] [10100] global_step=10100, grad_norm=1.9675586223602295, loss=2.07127046585083
I0307 04:21:53.912369 139958455088896 logging_writer.py:48] [10200] global_step=10200, grad_norm=1.5401462316513062, loss=2.103214740753174
I0307 04:22:32.306352 139958446696192 logging_writer.py:48] [10300] global_step=10300, grad_norm=2.065264940261841, loss=2.1504576206207275
I0307 04:23:10.639429 139958455088896 logging_writer.py:48] [10400] global_step=10400, grad_norm=1.8908201456069946, loss=2.104532241821289
I0307 04:23:49.255989 139958446696192 logging_writer.py:48] [10500] global_step=10500, grad_norm=1.8116074800491333, loss=2.0616939067840576
I0307 04:24:27.739303 139958455088896 logging_writer.py:48] [10600] global_step=10600, grad_norm=1.9447826147079468, loss=2.058908700942993
I0307 04:25:06.070162 139958446696192 logging_writer.py:48] [10700] global_step=10700, grad_norm=1.748533844947815, loss=2.1241683959960938
I0307 04:25:08.741443 140114851837120 spec.py:321] Evaluating on the training split.
I0307 04:25:25.440820 140114851837120 spec.py:333] Evaluating on the validation split.
I0307 04:25:49.098107 140114851837120 spec.py:349] Evaluating on the test split.
I0307 04:25:50.913452 140114851837120 submission_runner.py:469] Time since start: 4542.25s, 	Step: 10708, 	{'train/accuracy': 0.6346460580825806, 'train/loss': 1.4815704822540283, 'validation/accuracy': 0.572439968585968, 'validation/loss': 1.8022783994674683, 'validation/num_examples': 50000, 'test/accuracy': 0.45180001854896545, 'test/loss': 2.528625965118408, 'test/num_examples': 10000, 'score': 4137.336168289185, 'total_duration': 4542.2506721019745, 'accumulated_submission_time': 4137.336168289185, 'accumulated_eval_time': 403.2563259601593, 'accumulated_logging_time': 0.42371392250061035}
I0307 04:25:51.041811 139958455088896 logging_writer.py:48] [10708] accumulated_eval_time=403.256, accumulated_logging_time=0.423714, accumulated_submission_time=4137.34, global_step=10708, preemption_count=0, score=4137.34, test/accuracy=0.4518, test/loss=2.52863, test/num_examples=10000, total_duration=4542.25, train/accuracy=0.634646, train/loss=1.48157, validation/accuracy=0.57244, validation/loss=1.80228, validation/num_examples=50000
I0307 04:26:26.683473 139958446696192 logging_writer.py:48] [10800] global_step=10800, grad_norm=1.7269277572631836, loss=2.0703887939453125
I0307 04:27:04.956449 139958455088896 logging_writer.py:48] [10900] global_step=10900, grad_norm=1.5174005031585693, loss=2.108327865600586
I0307 04:27:43.355731 139958446696192 logging_writer.py:48] [11000] global_step=11000, grad_norm=1.6584504842758179, loss=2.061211585998535
I0307 04:28:21.880708 139958455088896 logging_writer.py:48] [11100] global_step=11100, grad_norm=1.7688642740249634, loss=2.101553201675415
I0307 04:29:00.133497 139958446696192 logging_writer.py:48] [11200] global_step=11200, grad_norm=1.7355906963348389, loss=2.1348445415496826
I0307 04:29:38.784090 139958455088896 logging_writer.py:48] [11300] global_step=11300, grad_norm=1.6303881406784058, loss=2.017026424407959
I0307 04:30:17.301053 139958446696192 logging_writer.py:48] [11400] global_step=11400, grad_norm=1.704437017440796, loss=2.029625177383423
I0307 04:30:56.048508 139958455088896 logging_writer.py:48] [11500] global_step=11500, grad_norm=1.5728280544281006, loss=2.0492827892303467
I0307 04:31:34.383203 139958446696192 logging_writer.py:48] [11600] global_step=11600, grad_norm=1.8397865295410156, loss=2.026702404022217
I0307 04:32:12.693288 139958455088896 logging_writer.py:48] [11700] global_step=11700, grad_norm=1.6674460172653198, loss=1.8380491733551025
I0307 04:32:51.497242 139958446696192 logging_writer.py:48] [11800] global_step=11800, grad_norm=1.5616382360458374, loss=2.0502841472625732
I0307 04:33:29.649964 139958455088896 logging_writer.py:48] [11900] global_step=11900, grad_norm=1.8223363161087036, loss=1.9428757429122925
I0307 04:34:08.535244 139958446696192 logging_writer.py:48] [12000] global_step=12000, grad_norm=1.6725274324417114, loss=1.9492348432540894
I0307 04:34:20.920013 140114851837120 spec.py:321] Evaluating on the training split.
I0307 04:34:33.147053 140114851837120 spec.py:333] Evaluating on the validation split.
I0307 04:35:00.065328 140114851837120 spec.py:349] Evaluating on the test split.
I0307 04:35:01.869845 140114851837120 submission_runner.py:469] Time since start: 5093.21s, 	Step: 12033, 	{'train/accuracy': 0.631257951259613, 'train/loss': 1.4887652397155762, 'validation/accuracy': 0.5737599730491638, 'validation/loss': 1.8011565208435059, 'validation/num_examples': 50000, 'test/accuracy': 0.4471000134944916, 'test/loss': 2.5648012161254883, 'test/num_examples': 10000, 'score': 4647.0611782073975, 'total_duration': 5093.207044124603, 'accumulated_submission_time': 4647.0611782073975, 'accumulated_eval_time': 444.2060983181, 'accumulated_logging_time': 0.5600624084472656}
I0307 04:35:01.936743 139958455088896 logging_writer.py:48] [12033] accumulated_eval_time=444.206, accumulated_logging_time=0.560062, accumulated_submission_time=4647.06, global_step=12033, preemption_count=0, score=4647.06, test/accuracy=0.4471, test/loss=2.5648, test/num_examples=10000, total_duration=5093.21, train/accuracy=0.631258, train/loss=1.48877, validation/accuracy=0.57376, validation/loss=1.80116, validation/num_examples=50000
I0307 04:35:28.156945 139958446696192 logging_writer.py:48] [12100] global_step=12100, grad_norm=1.794585943222046, loss=2.0410330295562744
I0307 04:36:06.430623 139958455088896 logging_writer.py:48] [12200] global_step=12200, grad_norm=1.7873549461364746, loss=1.9097709655761719
I0307 04:36:44.824192 139958446696192 logging_writer.py:48] [12300] global_step=12300, grad_norm=1.804127812385559, loss=2.065913677215576
I0307 04:37:23.204723 139958455088896 logging_writer.py:48] [12400] global_step=12400, grad_norm=1.5838749408721924, loss=2.0143609046936035
I0307 04:38:01.689098 139958446696192 logging_writer.py:48] [12500] global_step=12500, grad_norm=1.5632145404815674, loss=1.8672648668289185
I0307 04:38:40.182309 139958455088896 logging_writer.py:48] [12600] global_step=12600, grad_norm=2.1277003288269043, loss=1.953787088394165
I0307 04:39:18.393070 139958446696192 logging_writer.py:48] [12700] global_step=12700, grad_norm=1.8304888010025024, loss=1.9987528324127197
I0307 04:39:56.882426 139958455088896 logging_writer.py:48] [12800] global_step=12800, grad_norm=1.3685874938964844, loss=1.912841796875
I0307 04:40:35.661543 139958446696192 logging_writer.py:48] [12900] global_step=12900, grad_norm=1.4481042623519897, loss=2.0435266494750977
I0307 04:41:14.575125 139958455088896 logging_writer.py:48] [13000] global_step=13000, grad_norm=1.4923312664031982, loss=2.0192437171936035
I0307 04:41:53.504537 139958446696192 logging_writer.py:48] [13100] global_step=13100, grad_norm=1.4966726303100586, loss=1.780875563621521
I0307 04:42:32.010662 139958455088896 logging_writer.py:48] [13200] global_step=13200, grad_norm=1.430992603302002, loss=1.9541661739349365
I0307 04:43:10.583742 139958446696192 logging_writer.py:48] [13300] global_step=13300, grad_norm=1.4787890911102295, loss=1.8486725091934204
I0307 04:43:31.940032 140114851837120 spec.py:321] Evaluating on the training split.
I0307 04:43:48.965934 140114851837120 spec.py:333] Evaluating on the validation split.
I0307 04:44:11.907194 140114851837120 spec.py:349] Evaluating on the test split.
I0307 04:44:13.727847 140114851837120 submission_runner.py:469] Time since start: 5645.06s, 	Step: 13356, 	{'train/accuracy': 0.6477399468421936, 'train/loss': 1.4090443849563599, 'validation/accuracy': 0.586899995803833, 'validation/loss': 1.7182719707489014, 'validation/num_examples': 50000, 'test/accuracy': 0.45590001344680786, 'test/loss': 2.4721691608428955, 'test/num_examples': 10000, 'score': 5156.9120326042175, 'total_duration': 5645.0649626255035, 'accumulated_submission_time': 5156.9120326042175, 'accumulated_eval_time': 485.9937734603882, 'accumulated_logging_time': 0.6360607147216797}
I0307 04:44:13.810664 139958455088896 logging_writer.py:48] [13356] accumulated_eval_time=485.994, accumulated_logging_time=0.636061, accumulated_submission_time=5156.91, global_step=13356, preemption_count=0, score=5156.91, test/accuracy=0.4559, test/loss=2.47217, test/num_examples=10000, total_duration=5645.06, train/accuracy=0.64774, train/loss=1.40904, validation/accuracy=0.5869, validation/loss=1.71827, validation/num_examples=50000
I0307 04:44:33.007205 139958446696192 logging_writer.py:48] [13400] global_step=13400, grad_norm=1.6791447401046753, loss=1.922492504119873
I0307 04:45:11.706969 139958455088896 logging_writer.py:48] [13500] global_step=13500, grad_norm=2.0988595485687256, loss=1.9783053398132324
I0307 04:45:50.240014 139958446696192 logging_writer.py:48] [13600] global_step=13600, grad_norm=1.9712024927139282, loss=1.9720640182495117
I0307 04:46:28.778352 139958455088896 logging_writer.py:48] [13700] global_step=13700, grad_norm=1.4053866863250732, loss=1.986659288406372
I0307 04:47:07.310376 139958446696192 logging_writer.py:48] [13800] global_step=13800, grad_norm=1.2783262729644775, loss=1.911372184753418
I0307 04:47:46.071480 139958455088896 logging_writer.py:48] [13900] global_step=13900, grad_norm=1.9559860229492188, loss=1.9830374717712402
I0307 04:48:24.737117 139958446696192 logging_writer.py:48] [14000] global_step=14000, grad_norm=1.469761848449707, loss=2.0163979530334473
I0307 04:49:03.545938 139958455088896 logging_writer.py:48] [14100] global_step=14100, grad_norm=1.6245315074920654, loss=1.9280749559402466
I0307 04:49:42.313335 139958446696192 logging_writer.py:48] [14200] global_step=14200, grad_norm=1.48980712890625, loss=1.9919604063034058
I0307 04:50:20.633236 139958455088896 logging_writer.py:48] [14300] global_step=14300, grad_norm=1.4643633365631104, loss=1.9133325815200806
I0307 04:50:58.994564 139958446696192 logging_writer.py:48] [14400] global_step=14400, grad_norm=1.4209955930709839, loss=1.8275622129440308
I0307 04:51:37.506418 139958455088896 logging_writer.py:48] [14500] global_step=14500, grad_norm=1.3821746110916138, loss=1.9267736673355103
I0307 04:52:15.913257 139958446696192 logging_writer.py:48] [14600] global_step=14600, grad_norm=1.5531805753707886, loss=1.956472396850586
I0307 04:52:44.037315 140114851837120 spec.py:321] Evaluating on the training split.
I0307 04:53:02.229511 140114851837120 spec.py:333] Evaluating on the validation split.
I0307 04:53:28.408004 140114851837120 spec.py:349] Evaluating on the test split.
I0307 04:53:30.189678 140114851837120 submission_runner.py:469] Time since start: 6201.53s, 	Step: 14674, 	{'train/accuracy': 0.6534597873687744, 'train/loss': 1.3843954801559448, 'validation/accuracy': 0.5932999849319458, 'validation/loss': 1.7101521492004395, 'validation/num_examples': 50000, 'test/accuracy': 0.4629000127315521, 'test/loss': 2.4998021125793457, 'test/num_examples': 10000, 'score': 5666.989051818848, 'total_duration': 6201.526761770248, 'accumulated_submission_time': 5666.989051818848, 'accumulated_eval_time': 532.1459667682648, 'accumulated_logging_time': 0.7269675731658936}
I0307 04:53:30.272058 139958455088896 logging_writer.py:48] [14674] accumulated_eval_time=532.146, accumulated_logging_time=0.726968, accumulated_submission_time=5666.99, global_step=14674, preemption_count=0, score=5666.99, test/accuracy=0.4629, test/loss=2.4998, test/num_examples=10000, total_duration=6201.53, train/accuracy=0.65346, train/loss=1.3844, validation/accuracy=0.5933, validation/loss=1.71015, validation/num_examples=50000
I0307 04:53:40.556769 139958446696192 logging_writer.py:48] [14700] global_step=14700, grad_norm=1.3954498767852783, loss=1.8893098831176758
I0307 04:54:18.379208 139958455088896 logging_writer.py:48] [14800] global_step=14800, grad_norm=1.6998038291931152, loss=1.9287716150283813
I0307 04:54:56.886532 139958446696192 logging_writer.py:48] [14900] global_step=14900, grad_norm=1.638964056968689, loss=1.974867820739746
I0307 04:55:35.589957 139958455088896 logging_writer.py:48] [15000] global_step=15000, grad_norm=1.6703964471817017, loss=2.0590403079986572
I0307 04:56:14.650406 139958446696192 logging_writer.py:48] [15100] global_step=15100, grad_norm=1.6871421337127686, loss=1.9095124006271362
I0307 04:56:53.019706 139958455088896 logging_writer.py:48] [15200] global_step=15200, grad_norm=1.7494248151779175, loss=1.9666962623596191
I0307 04:57:31.136470 139958446696192 logging_writer.py:48] [15300] global_step=15300, grad_norm=1.498869776725769, loss=1.8654191493988037
I0307 04:58:09.452943 139958455088896 logging_writer.py:48] [15400] global_step=15400, grad_norm=1.3509966135025024, loss=1.977690577507019
I0307 04:58:48.049191 139958446696192 logging_writer.py:48] [15500] global_step=15500, grad_norm=1.9638581275939941, loss=1.7914215326309204
I0307 04:59:27.212175 139958455088896 logging_writer.py:48] [15600] global_step=15600, grad_norm=1.4829996824264526, loss=1.7814409732818604
I0307 05:00:06.697868 139958446696192 logging_writer.py:48] [15700] global_step=15700, grad_norm=1.7080777883529663, loss=1.9149203300476074
I0307 05:00:45.543663 139958455088896 logging_writer.py:48] [15800] global_step=15800, grad_norm=1.5990082025527954, loss=1.7480427026748657
I0307 05:01:24.127948 139958446696192 logging_writer.py:48] [15900] global_step=15900, grad_norm=1.5129038095474243, loss=1.9239656925201416
I0307 05:02:00.545610 140114851837120 spec.py:321] Evaluating on the training split.
I0307 05:02:17.458524 140114851837120 spec.py:333] Evaluating on the validation split.
I0307 05:02:43.165687 140114851837120 spec.py:349] Evaluating on the test split.
I0307 05:02:44.963889 140114851837120 submission_runner.py:469] Time since start: 6756.30s, 	Step: 15995, 	{'train/accuracy': 0.6622090339660645, 'train/loss': 1.342408299446106, 'validation/accuracy': 0.5983999967575073, 'validation/loss': 1.6766204833984375, 'validation/num_examples': 50000, 'test/accuracy': 0.4706000089645386, 'test/loss': 2.452061414718628, 'test/num_examples': 10000, 'score': 6177.107513427734, 'total_duration': 6756.300977230072, 'accumulated_submission_time': 6177.107513427734, 'accumulated_eval_time': 576.5640850067139, 'accumulated_logging_time': 0.8178987503051758}
I0307 05:02:45.049776 139958455088896 logging_writer.py:48] [15995] accumulated_eval_time=576.564, accumulated_logging_time=0.817899, accumulated_submission_time=6177.11, global_step=15995, preemption_count=0, score=6177.11, test/accuracy=0.4706, test/loss=2.45206, test/num_examples=10000, total_duration=6756.3, train/accuracy=0.662209, train/loss=1.34241, validation/accuracy=0.5984, validation/loss=1.67662, validation/num_examples=50000
I0307 05:02:47.448934 139958446696192 logging_writer.py:48] [16000] global_step=16000, grad_norm=1.5187795162200928, loss=1.7747160196304321
I0307 05:03:25.932045 139958455088896 logging_writer.py:48] [16100] global_step=16100, grad_norm=1.9068541526794434, loss=1.9461884498596191
I0307 05:04:04.718600 139958446696192 logging_writer.py:48] [16200] global_step=16200, grad_norm=1.734354019165039, loss=1.8644297122955322
I0307 05:04:43.798083 139958455088896 logging_writer.py:48] [16300] global_step=16300, grad_norm=1.7167974710464478, loss=1.9037561416625977
I0307 05:05:23.203360 139958446696192 logging_writer.py:48] [16400] global_step=16400, grad_norm=1.5496872663497925, loss=1.8273999691009521
I0307 05:06:02.139273 139958455088896 logging_writer.py:48] [16500] global_step=16500, grad_norm=1.493830919265747, loss=1.7245877981185913
I0307 05:06:40.683326 139958446696192 logging_writer.py:48] [16600] global_step=16600, grad_norm=1.7595971822738647, loss=1.865810751914978
I0307 05:07:19.212584 139958455088896 logging_writer.py:48] [16700] global_step=16700, grad_norm=1.6147061586380005, loss=1.9352267980575562
I0307 05:07:57.767578 139958446696192 logging_writer.py:48] [16800] global_step=16800, grad_norm=1.7038158178329468, loss=1.9424622058868408
I0307 05:08:36.322334 139958455088896 logging_writer.py:48] [16900] global_step=16900, grad_norm=1.6952623128890991, loss=1.9452235698699951
I0307 05:09:15.236708 139958446696192 logging_writer.py:48] [17000] global_step=17000, grad_norm=1.3973078727722168, loss=1.927616000175476
I0307 05:09:53.832123 139958455088896 logging_writer.py:48] [17100] global_step=17100, grad_norm=1.5660852193832397, loss=1.84954833984375
I0307 05:10:32.238404 139958446696192 logging_writer.py:48] [17200] global_step=17200, grad_norm=1.4738842248916626, loss=1.918262243270874
I0307 05:11:10.632688 139958455088896 logging_writer.py:48] [17300] global_step=17300, grad_norm=1.6535606384277344, loss=1.8522639274597168
I0307 05:11:14.989356 140114851837120 spec.py:321] Evaluating on the training split.
I0307 05:11:32.128577 140114851837120 spec.py:333] Evaluating on the validation split.
I0307 05:11:55.814402 140114851837120 spec.py:349] Evaluating on the test split.
I0307 05:11:57.655724 140114851837120 submission_runner.py:469] Time since start: 7308.99s, 	Step: 17312, 	{'train/accuracy': 0.6724928021430969, 'train/loss': 1.2972861528396606, 'validation/accuracy': 0.6060999631881714, 'validation/loss': 1.6379177570343018, 'validation/num_examples': 50000, 'test/accuracy': 0.48330003023147583, 'test/loss': 2.385103225708008, 'test/num_examples': 10000, 'score': 6686.900541543961, 'total_duration': 7308.9928143024445, 'accumulated_submission_time': 6686.900541543961, 'accumulated_eval_time': 619.2302832603455, 'accumulated_logging_time': 0.9116013050079346}
I0307 05:11:57.682555 139958446696192 logging_writer.py:48] [17312] accumulated_eval_time=619.23, accumulated_logging_time=0.911601, accumulated_submission_time=6686.9, global_step=17312, preemption_count=0, score=6686.9, test/accuracy=0.4833, test/loss=2.3851, test/num_examples=10000, total_duration=7308.99, train/accuracy=0.672493, train/loss=1.29729, validation/accuracy=0.6061, validation/loss=1.63792, validation/num_examples=50000
I0307 05:12:31.887846 139958455088896 logging_writer.py:48] [17400] global_step=17400, grad_norm=1.4332993030548096, loss=2.021731376647949
I0307 05:13:10.103075 139958446696192 logging_writer.py:48] [17500] global_step=17500, grad_norm=1.7654290199279785, loss=1.8858603239059448
I0307 05:13:48.865660 139958455088896 logging_writer.py:48] [17600] global_step=17600, grad_norm=1.6187602281570435, loss=1.8164417743682861
I0307 05:14:27.443350 139958446696192 logging_writer.py:48] [17700] global_step=17700, grad_norm=1.4625898599624634, loss=1.8758622407913208
I0307 05:15:05.968054 139958455088896 logging_writer.py:48] [17800] global_step=17800, grad_norm=1.651663064956665, loss=1.9529913663864136
I0307 05:15:44.624465 139958446696192 logging_writer.py:48] [17900] global_step=17900, grad_norm=1.6904926300048828, loss=1.9254419803619385
I0307 05:16:22.910146 139958455088896 logging_writer.py:48] [18000] global_step=18000, grad_norm=1.5702283382415771, loss=1.749155879020691
I0307 05:17:01.522402 139958446696192 logging_writer.py:48] [18100] global_step=18100, grad_norm=1.535385251045227, loss=1.919593334197998
I0307 05:17:40.159768 139958455088896 logging_writer.py:48] [18200] global_step=18200, grad_norm=1.5541762113571167, loss=1.7371379137039185
I0307 05:18:18.613962 139958446696192 logging_writer.py:48] [18300] global_step=18300, grad_norm=1.5379239320755005, loss=1.7272175550460815
I0307 05:18:57.240538 139958455088896 logging_writer.py:48] [18400] global_step=18400, grad_norm=1.509568691253662, loss=1.7174667119979858
I0307 05:19:35.832475 139958446696192 logging_writer.py:48] [18500] global_step=18500, grad_norm=1.6954309940338135, loss=1.6919100284576416
I0307 05:20:14.602575 139958455088896 logging_writer.py:48] [18600] global_step=18600, grad_norm=1.5201860666275024, loss=1.7625175714492798
I0307 05:20:27.734463 140114851837120 spec.py:321] Evaluating on the training split.
I0307 05:20:43.025357 140114851837120 spec.py:333] Evaluating on the validation split.
I0307 05:21:07.435222 140114851837120 spec.py:349] Evaluating on the test split.
I0307 05:21:09.272799 140114851837120 submission_runner.py:469] Time since start: 7860.61s, 	Step: 18635, 	{'train/accuracy': 0.6758410334587097, 'train/loss': 1.2892130613327026, 'validation/accuracy': 0.6082800030708313, 'validation/loss': 1.614640235900879, 'validation/num_examples': 50000, 'test/accuracy': 0.4741000235080719, 'test/loss': 2.3687267303466797, 'test/num_examples': 10000, 'score': 7196.809233188629, 'total_duration': 7860.610020399094, 'accumulated_submission_time': 7196.809233188629, 'accumulated_eval_time': 660.7685797214508, 'accumulated_logging_time': 0.946190357208252}
I0307 05:21:09.298325 139958446696192 logging_writer.py:48] [18635] accumulated_eval_time=660.769, accumulated_logging_time=0.94619, accumulated_submission_time=7196.81, global_step=18635, preemption_count=0, score=7196.81, test/accuracy=0.4741, test/loss=2.36873, test/num_examples=10000, total_duration=7860.61, train/accuracy=0.675841, train/loss=1.28921, validation/accuracy=0.60828, validation/loss=1.61464, validation/num_examples=50000
I0307 05:21:34.796627 139958455088896 logging_writer.py:48] [18700] global_step=18700, grad_norm=1.693549633026123, loss=1.9132894277572632
I0307 05:22:13.358518 139958446696192 logging_writer.py:48] [18800] global_step=18800, grad_norm=1.597736120223999, loss=1.7262643575668335
I0307 05:22:51.801858 139958455088896 logging_writer.py:48] [18900] global_step=18900, grad_norm=1.4427757263183594, loss=1.7591404914855957
I0307 05:23:30.448874 139958446696192 logging_writer.py:48] [19000] global_step=19000, grad_norm=1.5162556171417236, loss=1.681302785873413
I0307 05:24:08.798249 139958455088896 logging_writer.py:48] [19100] global_step=19100, grad_norm=1.5346734523773193, loss=1.7969558238983154
I0307 05:24:47.669227 139958446696192 logging_writer.py:48] [19200] global_step=19200, grad_norm=1.5655713081359863, loss=1.7394506931304932
I0307 05:25:26.209159 139958455088896 logging_writer.py:48] [19300] global_step=19300, grad_norm=1.6853158473968506, loss=1.7998325824737549
I0307 05:26:05.302004 139958446696192 logging_writer.py:48] [19400] global_step=19400, grad_norm=2.251044511795044, loss=1.7461429834365845
I0307 05:26:43.944145 139958455088896 logging_writer.py:48] [19500] global_step=19500, grad_norm=1.6274700164794922, loss=1.7819364070892334
I0307 05:27:22.435737 139958446696192 logging_writer.py:48] [19600] global_step=19600, grad_norm=1.4498326778411865, loss=1.6985650062561035
I0307 05:28:01.282620 139958455088896 logging_writer.py:48] [19700] global_step=19700, grad_norm=1.741579294204712, loss=1.8742179870605469
I0307 05:28:40.306485 139958446696192 logging_writer.py:48] [19800] global_step=19800, grad_norm=1.78114914894104, loss=1.8596489429473877
I0307 05:29:18.879786 139958455088896 logging_writer.py:48] [19900] global_step=19900, grad_norm=1.5641764402389526, loss=1.8863754272460938
I0307 05:29:39.481815 140114851837120 spec.py:321] Evaluating on the training split.
I0307 05:29:55.053146 140114851837120 spec.py:333] Evaluating on the validation split.
I0307 05:30:19.506792 140114851837120 spec.py:349] Evaluating on the test split.
I0307 05:30:21.329324 140114851837120 submission_runner.py:469] Time since start: 8412.67s, 	Step: 19954, 	{'train/accuracy': 0.6624082922935486, 'train/loss': 1.3506443500518799, 'validation/accuracy': 0.6001999974250793, 'validation/loss': 1.6645383834838867, 'validation/num_examples': 50000, 'test/accuracy': 0.47300001978874207, 'test/loss': 2.4019932746887207, 'test/num_examples': 10000, 'score': 7706.858216047287, 'total_duration': 8412.666400671005, 'accumulated_submission_time': 7706.858216047287, 'accumulated_eval_time': 702.6159074306488, 'accumulated_logging_time': 0.9797472953796387}
I0307 05:30:21.393316 139958446696192 logging_writer.py:48] [19954] accumulated_eval_time=702.616, accumulated_logging_time=0.979747, accumulated_submission_time=7706.86, global_step=19954, preemption_count=0, score=7706.86, test/accuracy=0.473, test/loss=2.40199, test/num_examples=10000, total_duration=8412.67, train/accuracy=0.662408, train/loss=1.35064, validation/accuracy=0.6002, validation/loss=1.66454, validation/num_examples=50000
I0307 05:30:39.722958 139958455088896 logging_writer.py:48] [20000] global_step=20000, grad_norm=1.8118911981582642, loss=1.8501002788543701
I0307 05:31:18.174540 139958446696192 logging_writer.py:48] [20100] global_step=20100, grad_norm=1.5984891653060913, loss=1.8620612621307373
I0307 05:31:56.729290 139958455088896 logging_writer.py:48] [20200] global_step=20200, grad_norm=1.5889685153961182, loss=1.818732500076294
I0307 05:32:35.347279 139958446696192 logging_writer.py:48] [20300] global_step=20300, grad_norm=1.7309166193008423, loss=1.8677822351455688
I0307 05:33:13.173996 139958455088896 logging_writer.py:48] [20400] global_step=20400, grad_norm=1.8190224170684814, loss=1.8790122270584106
I0307 05:33:51.471778 139958446696192 logging_writer.py:48] [20500] global_step=20500, grad_norm=1.4914591312408447, loss=1.7435890436172485
I0307 05:34:29.763387 139958455088896 logging_writer.py:48] [20600] global_step=20600, grad_norm=1.6979504823684692, loss=1.864235758781433
I0307 05:35:08.274844 139958446696192 logging_writer.py:48] [20700] global_step=20700, grad_norm=1.6361525058746338, loss=1.7739859819412231
I0307 05:35:46.627544 139958455088896 logging_writer.py:48] [20800] global_step=20800, grad_norm=1.7041082382202148, loss=1.7294411659240723
I0307 05:36:24.700330 139958446696192 logging_writer.py:48] [20900] global_step=20900, grad_norm=1.7136224508285522, loss=1.9323334693908691
I0307 05:37:03.242588 139958455088896 logging_writer.py:48] [21000] global_step=21000, grad_norm=1.607810616493225, loss=1.7188454866409302
I0307 05:37:41.873172 139958446696192 logging_writer.py:48] [21100] global_step=21100, grad_norm=1.77225923538208, loss=1.7491869926452637
I0307 05:38:20.446995 139958455088896 logging_writer.py:48] [21200] global_step=21200, grad_norm=1.580157995223999, loss=1.8384199142456055
I0307 05:38:51.512854 140114851837120 spec.py:321] Evaluating on the training split.
I0307 05:39:05.527766 140114851837120 spec.py:333] Evaluating on the validation split.
I0307 05:39:28.900275 140114851837120 spec.py:349] Evaluating on the test split.
I0307 05:39:30.733924 140114851837120 submission_runner.py:469] Time since start: 8962.07s, 	Step: 21282, 	{'train/accuracy': 0.6584023833274841, 'train/loss': 1.3560289144515991, 'validation/accuracy': 0.596340000629425, 'validation/loss': 1.696319818496704, 'validation/num_examples': 50000, 'test/accuracy': 0.47360002994537354, 'test/loss': 2.440589666366577, 'test/num_examples': 10000, 'score': 8216.837393283844, 'total_duration': 8962.071113348007, 'accumulated_submission_time': 8216.837393283844, 'accumulated_eval_time': 741.8369216918945, 'accumulated_logging_time': 1.0525918006896973}
I0307 05:39:30.775398 139958446696192 logging_writer.py:48] [21282] accumulated_eval_time=741.837, accumulated_logging_time=1.05259, accumulated_submission_time=8216.84, global_step=21282, preemption_count=0, score=8216.84, test/accuracy=0.4736, test/loss=2.44059, test/num_examples=10000, total_duration=8962.07, train/accuracy=0.658402, train/loss=1.35603, validation/accuracy=0.59634, validation/loss=1.69632, validation/num_examples=50000
I0307 05:39:38.194679 139958455088896 logging_writer.py:48] [21300] global_step=21300, grad_norm=1.7552027702331543, loss=1.7804336547851562
I0307 05:40:17.045881 139958446696192 logging_writer.py:48] [21400] global_step=21400, grad_norm=1.6084651947021484, loss=1.836005687713623
I0307 05:40:55.632483 139958455088896 logging_writer.py:48] [21500] global_step=21500, grad_norm=1.6451201438903809, loss=1.9677848815917969
I0307 05:41:34.378749 139958446696192 logging_writer.py:48] [21600] global_step=21600, grad_norm=1.6113700866699219, loss=1.7321484088897705
I0307 05:42:13.088197 139958455088896 logging_writer.py:48] [21700] global_step=21700, grad_norm=1.7618663311004639, loss=1.7543118000030518
I0307 05:42:51.348537 139958446696192 logging_writer.py:48] [21800] global_step=21800, grad_norm=1.6688179969787598, loss=1.8301516771316528
I0307 05:43:29.714225 139958455088896 logging_writer.py:48] [21900] global_step=21900, grad_norm=1.7727992534637451, loss=1.9042301177978516
I0307 05:44:08.501349 139958446696192 logging_writer.py:48] [22000] global_step=22000, grad_norm=1.868965744972229, loss=1.8839397430419922
I0307 05:44:46.872979 139958455088896 logging_writer.py:48] [22100] global_step=22100, grad_norm=1.7796097993850708, loss=1.7477883100509644
I0307 05:45:25.525798 139958446696192 logging_writer.py:48] [22200] global_step=22200, grad_norm=1.9053620100021362, loss=1.7895078659057617
I0307 05:46:03.923190 139958455088896 logging_writer.py:48] [22300] global_step=22300, grad_norm=1.7749866247177124, loss=1.815134882926941
I0307 05:46:42.479133 139958446696192 logging_writer.py:48] [22400] global_step=22400, grad_norm=2.2404706478118896, loss=1.7674230337142944
I0307 05:47:20.762452 139958455088896 logging_writer.py:48] [22500] global_step=22500, grad_norm=1.9554520845413208, loss=1.9531755447387695
I0307 05:47:58.826842 139958446696192 logging_writer.py:48] [22600] global_step=22600, grad_norm=1.959328532218933, loss=1.8074995279312134
I0307 05:48:00.802126 140114851837120 spec.py:321] Evaluating on the training split.
I0307 05:48:16.854773 140114851837120 spec.py:333] Evaluating on the validation split.
I0307 05:48:39.349132 140114851837120 spec.py:349] Evaluating on the test split.
I0307 05:48:41.133019 140114851837120 submission_runner.py:469] Time since start: 9512.47s, 	Step: 22606, 	{'train/accuracy': 0.6674904227256775, 'train/loss': 1.3119453191757202, 'validation/accuracy': 0.604420006275177, 'validation/loss': 1.655828833580017, 'validation/num_examples': 50000, 'test/accuracy': 0.48410001397132874, 'test/loss': 2.401235342025757, 'test/num_examples': 10000, 'score': 8726.7188642025, 'total_duration': 9512.470099449158, 'accumulated_submission_time': 8726.7188642025, 'accumulated_eval_time': 782.167638540268, 'accumulated_logging_time': 1.1036078929901123}
I0307 05:48:41.162901 139958455088896 logging_writer.py:48] [22606] accumulated_eval_time=782.168, accumulated_logging_time=1.10361, accumulated_submission_time=8726.72, global_step=22606, preemption_count=0, score=8726.72, test/accuracy=0.4841, test/loss=2.40124, test/num_examples=10000, total_duration=9512.47, train/accuracy=0.66749, train/loss=1.31195, validation/accuracy=0.60442, validation/loss=1.65583, validation/num_examples=50000
I0307 05:49:17.706431 139958446696192 logging_writer.py:48] [22700] global_step=22700, grad_norm=1.6731675863265991, loss=1.7819287776947021
I0307 05:49:56.707417 139958455088896 logging_writer.py:48] [22800] global_step=22800, grad_norm=1.8807278871536255, loss=1.7385308742523193
I0307 05:50:35.768363 139958446696192 logging_writer.py:48] [22900] global_step=22900, grad_norm=1.9028042554855347, loss=1.8629508018493652
I0307 05:51:14.669787 139958455088896 logging_writer.py:48] [23000] global_step=23000, grad_norm=1.7720093727111816, loss=1.816784381866455
I0307 05:51:53.162494 139958446696192 logging_writer.py:48] [23100] global_step=23100, grad_norm=1.76393461227417, loss=1.7632935047149658
I0307 05:52:31.747775 139958455088896 logging_writer.py:48] [23200] global_step=23200, grad_norm=1.6368906497955322, loss=1.7848347425460815
I0307 05:53:09.872826 139958446696192 logging_writer.py:48] [23300] global_step=23300, grad_norm=1.6650099754333496, loss=1.7227344512939453
I0307 05:53:48.572958 139958455088896 logging_writer.py:48] [23400] global_step=23400, grad_norm=1.8008947372436523, loss=1.7572742700576782
I0307 05:54:26.817737 139958446696192 logging_writer.py:48] [23500] global_step=23500, grad_norm=1.6222931146621704, loss=1.701082706451416
I0307 05:55:05.673592 139958455088896 logging_writer.py:48] [23600] global_step=23600, grad_norm=1.7100962400436401, loss=1.8261698484420776
I0307 05:55:44.415148 139958446696192 logging_writer.py:48] [23700] global_step=23700, grad_norm=1.7470453977584839, loss=1.7040873765945435
I0307 05:56:23.318291 139958455088896 logging_writer.py:48] [23800] global_step=23800, grad_norm=1.6201059818267822, loss=1.662765383720398
I0307 05:57:02.186888 139958446696192 logging_writer.py:48] [23900] global_step=23900, grad_norm=1.6959128379821777, loss=1.7182035446166992
I0307 05:57:11.358258 140114851837120 spec.py:321] Evaluating on the training split.
I0307 05:57:29.101736 140114851837120 spec.py:333] Evaluating on the validation split.
I0307 05:57:54.797288 140114851837120 spec.py:349] Evaluating on the test split.
I0307 05:57:56.658144 140114851837120 submission_runner.py:469] Time since start: 10068.00s, 	Step: 23925, 	{'train/accuracy': 0.6863042116165161, 'train/loss': 1.2308433055877686, 'validation/accuracy': 0.6212999820709229, 'validation/loss': 1.570596694946289, 'validation/num_examples': 50000, 'test/accuracy': 0.4953000247478485, 'test/loss': 2.3210699558258057, 'test/num_examples': 10000, 'score': 9236.770344734192, 'total_duration': 10067.995245695114, 'accumulated_submission_time': 9236.770344734192, 'accumulated_eval_time': 827.4673681259155, 'accumulated_logging_time': 1.142059087753296}
I0307 05:57:56.750174 139958455088896 logging_writer.py:48] [23925] accumulated_eval_time=827.467, accumulated_logging_time=1.14206, accumulated_submission_time=9236.77, global_step=23925, preemption_count=0, score=9236.77, test/accuracy=0.4953, test/loss=2.32107, test/num_examples=10000, total_duration=10068, train/accuracy=0.686304, train/loss=1.23084, validation/accuracy=0.6213, validation/loss=1.5706, validation/num_examples=50000
I0307 05:58:26.098120 139958446696192 logging_writer.py:48] [24000] global_step=24000, grad_norm=1.6015115976333618, loss=1.8069525957107544
I0307 05:59:04.469776 139958455088896 logging_writer.py:48] [24100] global_step=24100, grad_norm=1.6369694471359253, loss=1.7531352043151855
I0307 05:59:43.234951 139958446696192 logging_writer.py:48] [24200] global_step=24200, grad_norm=2.0671324729919434, loss=1.8506819009780884
I0307 06:00:21.789240 139958455088896 logging_writer.py:48] [24300] global_step=24300, grad_norm=1.677071452140808, loss=1.7822182178497314
I0307 06:01:00.677718 139958446696192 logging_writer.py:48] [24400] global_step=24400, grad_norm=2.0874173641204834, loss=1.6990560293197632
I0307 06:01:39.269837 139958455088896 logging_writer.py:48] [24500] global_step=24500, grad_norm=1.5531420707702637, loss=1.73014497756958
I0307 06:02:17.842951 139958446696192 logging_writer.py:48] [24600] global_step=24600, grad_norm=1.7409636974334717, loss=1.7066428661346436
I0307 06:02:56.604674 139958455088896 logging_writer.py:48] [24700] global_step=24700, grad_norm=1.7059427499771118, loss=1.7915335893630981
I0307 06:03:35.520309 139958446696192 logging_writer.py:48] [24800] global_step=24800, grad_norm=1.76839017868042, loss=1.7465413808822632
I0307 06:04:14.362726 139958455088896 logging_writer.py:48] [24900] global_step=24900, grad_norm=1.5980894565582275, loss=1.8115476369857788
I0307 06:04:53.105555 139958446696192 logging_writer.py:48] [25000] global_step=25000, grad_norm=1.6191719770431519, loss=1.8644647598266602
I0307 06:05:32.230924 139958455088896 logging_writer.py:48] [25100] global_step=25100, grad_norm=1.8337501287460327, loss=1.7417722940444946
I0307 06:06:11.132564 139958446696192 logging_writer.py:48] [25200] global_step=25200, grad_norm=1.8527859449386597, loss=1.7413101196289062
I0307 06:06:26.874387 140114851837120 spec.py:321] Evaluating on the training split.
I0307 06:06:42.487478 140114851837120 spec.py:333] Evaluating on the validation split.
I0307 06:07:09.348854 140114851837120 spec.py:349] Evaluating on the test split.
I0307 06:07:11.269668 140114851837120 submission_runner.py:469] Time since start: 10622.61s, 	Step: 25242, 	{'train/accuracy': 0.6750438213348389, 'train/loss': 1.2924892902374268, 'validation/accuracy': 0.6092999577522278, 'validation/loss': 1.6240028142929077, 'validation/num_examples': 50000, 'test/accuracy': 0.4871000349521637, 'test/loss': 2.398207426071167, 'test/num_examples': 10000, 'score': 9746.749855041504, 'total_duration': 10622.606882333755, 'accumulated_submission_time': 9746.749855041504, 'accumulated_eval_time': 871.8626046180725, 'accumulated_logging_time': 1.243048906326294}
I0307 06:07:11.314281 139958455088896 logging_writer.py:48] [25242] accumulated_eval_time=871.863, accumulated_logging_time=1.24305, accumulated_submission_time=9746.75, global_step=25242, preemption_count=0, score=9746.75, test/accuracy=0.4871, test/loss=2.39821, test/num_examples=10000, total_duration=10622.6, train/accuracy=0.675044, train/loss=1.29249, validation/accuracy=0.6093, validation/loss=1.624, validation/num_examples=50000
I0307 06:07:34.284380 139958446696192 logging_writer.py:48] [25300] global_step=25300, grad_norm=1.6500312089920044, loss=1.7830824851989746
I0307 06:08:12.974220 139958455088896 logging_writer.py:48] [25400] global_step=25400, grad_norm=1.52143132686615, loss=1.6500904560089111
I0307 06:08:51.348174 139958446696192 logging_writer.py:48] [25500] global_step=25500, grad_norm=1.617978572845459, loss=1.7607630491256714
I0307 06:09:30.062298 139958455088896 logging_writer.py:48] [25600] global_step=25600, grad_norm=1.7207472324371338, loss=1.7327970266342163
I0307 06:10:09.073701 139958446696192 logging_writer.py:48] [25700] global_step=25700, grad_norm=1.60136079788208, loss=1.691597819328308
I0307 06:10:47.500513 139958455088896 logging_writer.py:48] [25800] global_step=25800, grad_norm=1.8232954740524292, loss=1.7569130659103394
I0307 06:11:26.062820 139958446696192 logging_writer.py:48] [25900] global_step=25900, grad_norm=1.7278918027877808, loss=1.6773488521575928
I0307 06:12:05.310727 139958455088896 logging_writer.py:48] [26000] global_step=26000, grad_norm=1.724223017692566, loss=1.809876799583435
I0307 06:12:43.904793 139958446696192 logging_writer.py:48] [26100] global_step=26100, grad_norm=1.6896100044250488, loss=1.7577288150787354
I0307 06:13:22.559317 139958455088896 logging_writer.py:48] [26200] global_step=26200, grad_norm=1.4897860288619995, loss=1.6853139400482178
I0307 06:14:01.598101 139958446696192 logging_writer.py:48] [26300] global_step=26300, grad_norm=1.6207165718078613, loss=1.8000130653381348
I0307 06:14:40.495411 139958455088896 logging_writer.py:48] [26400] global_step=26400, grad_norm=1.9654123783111572, loss=1.8318274021148682
I0307 06:15:19.185780 139958446696192 logging_writer.py:48] [26500] global_step=26500, grad_norm=1.7080531120300293, loss=1.6591111421585083
I0307 06:15:41.497266 140114851837120 spec.py:321] Evaluating on the training split.
I0307 06:15:59.111448 140114851837120 spec.py:333] Evaluating on the validation split.
I0307 06:16:24.101640 140114851837120 spec.py:349] Evaluating on the test split.
I0307 06:16:25.894393 140114851837120 submission_runner.py:469] Time since start: 11177.23s, 	Step: 26558, 	{'train/accuracy': 0.6721341013908386, 'train/loss': 1.2982786893844604, 'validation/accuracy': 0.6142599582672119, 'validation/loss': 1.6181321144104004, 'validation/num_examples': 50000, 'test/accuracy': 0.48830002546310425, 'test/loss': 2.3275299072265625, 'test/num_examples': 10000, 'score': 10256.790889978409, 'total_duration': 11177.231453895569, 'accumulated_submission_time': 10256.790889978409, 'accumulated_eval_time': 916.259539604187, 'accumulated_logging_time': 1.2962830066680908}
I0307 06:16:25.925798 139958455088896 logging_writer.py:48] [26558] accumulated_eval_time=916.26, accumulated_logging_time=1.29628, accumulated_submission_time=10256.8, global_step=26558, preemption_count=0, score=10256.8, test/accuracy=0.4883, test/loss=2.32753, test/num_examples=10000, total_duration=11177.2, train/accuracy=0.672134, train/loss=1.29828, validation/accuracy=0.61426, validation/loss=1.61813, validation/num_examples=50000
I0307 06:16:42.603992 139958446696192 logging_writer.py:48] [26600] global_step=26600, grad_norm=1.7408833503723145, loss=1.7021446228027344
I0307 06:17:21.505916 139958455088896 logging_writer.py:48] [26700] global_step=26700, grad_norm=1.7384424209594727, loss=1.7590217590332031
I0307 06:18:00.084518 139958446696192 logging_writer.py:48] [26800] global_step=26800, grad_norm=1.671230435371399, loss=1.6612778902053833
I0307 06:18:38.812177 139958455088896 logging_writer.py:48] [26900] global_step=26900, grad_norm=1.7380192279815674, loss=1.7497875690460205
I0307 06:19:17.434395 139958446696192 logging_writer.py:48] [27000] global_step=27000, grad_norm=1.7038555145263672, loss=1.8574477434158325
I0307 06:19:55.822360 139958455088896 logging_writer.py:48] [27100] global_step=27100, grad_norm=1.5897712707519531, loss=1.6608006954193115
I0307 06:20:34.833376 139958446696192 logging_writer.py:48] [27200] global_step=27200, grad_norm=1.7919950485229492, loss=1.767108678817749
I0307 06:21:13.537589 139958455088896 logging_writer.py:48] [27300] global_step=27300, grad_norm=1.7818288803100586, loss=1.623809814453125
I0307 06:21:52.250820 139958446696192 logging_writer.py:48] [27400] global_step=27400, grad_norm=1.737765908241272, loss=1.6356441974639893
I0307 06:22:31.187816 139958455088896 logging_writer.py:48] [27500] global_step=27500, grad_norm=1.8701465129852295, loss=1.8500481843948364
I0307 06:23:09.978766 139958446696192 logging_writer.py:48] [27600] global_step=27600, grad_norm=1.726172924041748, loss=1.7840405702590942
I0307 06:23:48.427185 139958455088896 logging_writer.py:48] [27700] global_step=27700, grad_norm=1.8014216423034668, loss=1.6777187585830688
I0307 06:24:26.287658 139958446696192 logging_writer.py:48] [27800] global_step=27800, grad_norm=1.968658447265625, loss=1.6386003494262695
I0307 06:24:56.064354 140114851837120 spec.py:321] Evaluating on the training split.
I0307 06:25:11.526099 140114851837120 spec.py:333] Evaluating on the validation split.
I0307 06:25:34.412227 140114851837120 spec.py:349] Evaluating on the test split.
I0307 06:25:36.242668 140114851837120 submission_runner.py:469] Time since start: 11727.58s, 	Step: 27879, 	{'train/accuracy': 0.6741868257522583, 'train/loss': 1.301221251487732, 'validation/accuracy': 0.6135199666023254, 'validation/loss': 1.6117511987686157, 'validation/num_examples': 50000, 'test/accuracy': 0.4880000352859497, 'test/loss': 2.358656644821167, 'test/num_examples': 10000, 'score': 10766.786980390549, 'total_duration': 11727.57971572876, 'accumulated_submission_time': 10766.786980390549, 'accumulated_eval_time': 956.4376459121704, 'accumulated_logging_time': 1.3355915546417236}
I0307 06:25:36.350422 139958455088896 logging_writer.py:48] [27879] accumulated_eval_time=956.438, accumulated_logging_time=1.33559, accumulated_submission_time=10766.8, global_step=27879, preemption_count=0, score=10766.8, test/accuracy=0.488, test/loss=2.35866, test/num_examples=10000, total_duration=11727.6, train/accuracy=0.674187, train/loss=1.30122, validation/accuracy=0.61352, validation/loss=1.61175, validation/num_examples=50000
I0307 06:25:44.800871 139958446696192 logging_writer.py:48] [27900] global_step=27900, grad_norm=1.5794677734375, loss=1.7389748096466064
I0307 06:26:23.426771 139958455088896 logging_writer.py:48] [28000] global_step=28000, grad_norm=2.0191032886505127, loss=1.7422816753387451
I0307 06:27:01.776144 139958446696192 logging_writer.py:48] [28100] global_step=28100, grad_norm=2.111607789993286, loss=1.8117907047271729
I0307 06:27:40.659216 139958455088896 logging_writer.py:48] [28200] global_step=28200, grad_norm=1.5419598817825317, loss=1.75431489944458
I0307 06:28:19.649739 139958446696192 logging_writer.py:48] [28300] global_step=28300, grad_norm=1.6786384582519531, loss=1.7538561820983887
I0307 06:28:58.091098 139958455088896 logging_writer.py:48] [28400] global_step=28400, grad_norm=1.7189098596572876, loss=1.7764045000076294
I0307 06:29:36.666304 139958446696192 logging_writer.py:48] [28500] global_step=28500, grad_norm=1.9252604246139526, loss=1.6842666864395142
I0307 06:30:15.329556 139958455088896 logging_writer.py:48] [28600] global_step=28600, grad_norm=1.9039417505264282, loss=1.826651692390442
I0307 06:30:54.005341 139958446696192 logging_writer.py:48] [28700] global_step=28700, grad_norm=2.000436782836914, loss=1.75802481174469
I0307 06:31:32.610943 139958455088896 logging_writer.py:48] [28800] global_step=28800, grad_norm=1.7597081661224365, loss=1.6746525764465332
I0307 06:32:10.995554 139958446696192 logging_writer.py:48] [28900] global_step=28900, grad_norm=1.7070313692092896, loss=1.8675458431243896
I0307 06:32:49.720737 139958455088896 logging_writer.py:48] [29000] global_step=29000, grad_norm=1.7526568174362183, loss=1.7301714420318604
I0307 06:33:28.109761 139958446696192 logging_writer.py:48] [29100] global_step=29100, grad_norm=1.7499057054519653, loss=1.7691657543182373
I0307 06:34:06.391545 140114851837120 spec.py:321] Evaluating on the training split.
I0307 06:34:24.291645 140114851837120 spec.py:333] Evaluating on the validation split.
I0307 06:34:48.912526 140114851837120 spec.py:349] Evaluating on the test split.
I0307 06:34:50.730432 140114851837120 submission_runner.py:469] Time since start: 12282.07s, 	Step: 29200, 	{'train/accuracy': 0.6728914380073547, 'train/loss': 1.287576675415039, 'validation/accuracy': 0.6141999959945679, 'validation/loss': 1.6045410633087158, 'validation/num_examples': 50000, 'test/accuracy': 0.48820000886917114, 'test/loss': 2.3589296340942383, 'test/num_examples': 10000, 'score': 11276.647282123566, 'total_duration': 12282.067492246628, 'accumulated_submission_time': 11276.647282123566, 'accumulated_eval_time': 1000.7763388156891, 'accumulated_logging_time': 1.4897840023040771}
I0307 06:34:50.794231 139958455088896 logging_writer.py:48] [29200] accumulated_eval_time=1000.78, accumulated_logging_time=1.48978, accumulated_submission_time=11276.6, global_step=29200, preemption_count=0, score=11276.6, test/accuracy=0.4882, test/loss=2.35893, test/num_examples=10000, total_duration=12282.1, train/accuracy=0.672891, train/loss=1.28758, validation/accuracy=0.6142, validation/loss=1.60454, validation/num_examples=50000
I0307 06:34:51.235920 139958446696192 logging_writer.py:48] [29200] global_step=29200, grad_norm=1.706483244895935, loss=1.7040375471115112
I0307 06:35:30.012772 139958455088896 logging_writer.py:48] [29300] global_step=29300, grad_norm=1.7919156551361084, loss=1.7917885780334473
I0307 06:36:08.874104 139958446696192 logging_writer.py:48] [29400] global_step=29400, grad_norm=1.806328535079956, loss=1.6676671504974365
I0307 06:36:47.136109 139958455088896 logging_writer.py:48] [29500] global_step=29500, grad_norm=1.6555731296539307, loss=1.7277201414108276
I0307 06:37:26.021093 139958446696192 logging_writer.py:48] [29600] global_step=29600, grad_norm=1.839026927947998, loss=1.6430613994598389
I0307 06:38:04.762097 139958455088896 logging_writer.py:48] [29700] global_step=29700, grad_norm=1.8123549222946167, loss=1.74531888961792
I0307 06:38:43.731163 139958446696192 logging_writer.py:48] [29800] global_step=29800, grad_norm=1.8648414611816406, loss=1.7308387756347656
I0307 06:39:22.543602 139958455088896 logging_writer.py:48] [29900] global_step=29900, grad_norm=1.6243793964385986, loss=1.7241263389587402
I0307 06:40:01.532274 139958446696192 logging_writer.py:48] [30000] global_step=30000, grad_norm=1.704240083694458, loss=1.6513687372207642
I0307 06:40:40.056182 139958455088896 logging_writer.py:48] [30100] global_step=30100, grad_norm=1.909647822380066, loss=1.7765915393829346
I0307 06:41:18.726147 139958446696192 logging_writer.py:48] [30200] global_step=30200, grad_norm=1.58527672290802, loss=1.6723825931549072
I0307 06:41:57.388148 139958455088896 logging_writer.py:48] [30300] global_step=30300, grad_norm=1.7280367612838745, loss=1.7265942096710205
I0307 06:42:35.999362 139958446696192 logging_writer.py:48] [30400] global_step=30400, grad_norm=2.0673203468322754, loss=1.7535282373428345
I0307 06:43:14.791451 139958455088896 logging_writer.py:48] [30500] global_step=30500, grad_norm=2.0544216632843018, loss=1.6912693977355957
I0307 06:43:20.989237 140114851837120 spec.py:321] Evaluating on the training split.
I0307 06:43:38.747849 140114851837120 spec.py:333] Evaluating on the validation split.
I0307 06:44:01.618854 140114851837120 spec.py:349] Evaluating on the test split.
I0307 06:44:03.477681 140114851837120 submission_runner.py:469] Time since start: 12834.81s, 	Step: 30517, 	{'train/accuracy': 0.6762595772743225, 'train/loss': 1.2833436727523804, 'validation/accuracy': 0.6150599718093872, 'validation/loss': 1.6105945110321045, 'validation/num_examples': 50000, 'test/accuracy': 0.4961000382900238, 'test/loss': 2.325723648071289, 'test/num_examples': 10000, 'score': 11786.69568514824, 'total_duration': 12834.81475353241, 'accumulated_submission_time': 11786.69568514824, 'accumulated_eval_time': 1043.2646017074585, 'accumulated_logging_time': 1.561824083328247}
I0307 06:44:03.543182 139958446696192 logging_writer.py:48] [30517] accumulated_eval_time=1043.26, accumulated_logging_time=1.56182, accumulated_submission_time=11786.7, global_step=30517, preemption_count=0, score=11786.7, test/accuracy=0.4961, test/loss=2.32572, test/num_examples=10000, total_duration=12834.8, train/accuracy=0.67626, train/loss=1.28334, validation/accuracy=0.61506, validation/loss=1.61059, validation/num_examples=50000
I0307 06:44:35.753299 139958455088896 logging_writer.py:48] [30600] global_step=30600, grad_norm=1.7346382141113281, loss=1.7754929065704346
I0307 06:45:14.159120 139958446696192 logging_writer.py:48] [30700] global_step=30700, grad_norm=1.88593327999115, loss=1.7688498497009277
I0307 06:45:52.847359 139958455088896 logging_writer.py:48] [30800] global_step=30800, grad_norm=1.7151063680648804, loss=1.7730106115341187
I0307 06:46:31.513345 139958446696192 logging_writer.py:48] [30900] global_step=30900, grad_norm=1.6617920398712158, loss=1.69374418258667
I0307 06:47:10.536110 139958455088896 logging_writer.py:48] [31000] global_step=31000, grad_norm=2.0189077854156494, loss=1.716475009918213
I0307 06:47:49.110199 139958446696192 logging_writer.py:48] [31100] global_step=31100, grad_norm=1.5252270698547363, loss=1.6781784296035767
I0307 06:48:27.862472 139958455088896 logging_writer.py:48] [31200] global_step=31200, grad_norm=1.6185104846954346, loss=1.7256019115447998
I0307 06:49:06.760812 139958446696192 logging_writer.py:48] [31300] global_step=31300, grad_norm=1.8118308782577515, loss=1.6900508403778076
I0307 06:49:46.082574 139958455088896 logging_writer.py:48] [31400] global_step=31400, grad_norm=1.6112709045410156, loss=1.735947847366333
I0307 06:50:24.850326 139958446696192 logging_writer.py:48] [31500] global_step=31500, grad_norm=1.907942295074463, loss=1.6480093002319336
I0307 06:51:03.396898 139958455088896 logging_writer.py:48] [31600] global_step=31600, grad_norm=1.76736319065094, loss=1.710923671722412
I0307 06:51:42.371760 139958446696192 logging_writer.py:48] [31700] global_step=31700, grad_norm=1.9836431741714478, loss=1.7838270664215088
I0307 06:52:20.971630 139958455088896 logging_writer.py:48] [31800] global_step=31800, grad_norm=1.7275974750518799, loss=1.810982584953308
I0307 06:52:33.540744 140114851837120 spec.py:321] Evaluating on the training split.
I0307 06:52:49.196939 140114851837120 spec.py:333] Evaluating on the validation split.
I0307 06:53:10.755984 140114851837120 spec.py:349] Evaluating on the test split.
I0307 06:53:12.559364 140114851837120 submission_runner.py:469] Time since start: 13383.90s, 	Step: 31834, 	{'train/accuracy': 0.6870814561843872, 'train/loss': 1.224181890487671, 'validation/accuracy': 0.6255599856376648, 'validation/loss': 1.556512475013733, 'validation/num_examples': 50000, 'test/accuracy': 0.5060000419616699, 'test/loss': 2.2561988830566406, 'test/num_examples': 10000, 'score': 12296.54272723198, 'total_duration': 13383.896453380585, 'accumulated_submission_time': 12296.54272723198, 'accumulated_eval_time': 1082.2830548286438, 'accumulated_logging_time': 1.6350719928741455}
I0307 06:53:12.602485 139958446696192 logging_writer.py:48] [31834] accumulated_eval_time=1082.28, accumulated_logging_time=1.63507, accumulated_submission_time=12296.5, global_step=31834, preemption_count=0, score=12296.5, test/accuracy=0.506, test/loss=2.2562, test/num_examples=10000, total_duration=13383.9, train/accuracy=0.687081, train/loss=1.22418, validation/accuracy=0.62556, validation/loss=1.55651, validation/num_examples=50000
I0307 06:53:38.354187 139958455088896 logging_writer.py:48] [31900] global_step=31900, grad_norm=1.6737611293792725, loss=1.627119541168213
I0307 06:54:17.263545 139958446696192 logging_writer.py:48] [32000] global_step=32000, grad_norm=2.1054203510284424, loss=1.7254323959350586
I0307 06:54:55.708554 139958455088896 logging_writer.py:48] [32100] global_step=32100, grad_norm=1.7595981359481812, loss=1.6661019325256348
I0307 06:55:34.673208 139958446696192 logging_writer.py:48] [32200] global_step=32200, grad_norm=1.8652007579803467, loss=1.639552116394043
I0307 06:56:13.180420 139958455088896 logging_writer.py:48] [32300] global_step=32300, grad_norm=1.8939472436904907, loss=1.730391263961792
I0307 06:56:51.482145 139958446696192 logging_writer.py:48] [32400] global_step=32400, grad_norm=1.7858805656433105, loss=1.660120964050293
I0307 06:57:30.042356 139958455088896 logging_writer.py:48] [32500] global_step=32500, grad_norm=1.9011496305465698, loss=1.7018380165100098
I0307 06:58:09.090938 139958446696192 logging_writer.py:48] [32600] global_step=32600, grad_norm=1.5508314371109009, loss=1.5769588947296143
I0307 06:58:47.426794 139958455088896 logging_writer.py:48] [32700] global_step=32700, grad_norm=1.7679790258407593, loss=1.7790857553482056
I0307 06:59:25.876576 139958446696192 logging_writer.py:48] [32800] global_step=32800, grad_norm=1.8622301816940308, loss=1.7908843755722046
I0307 07:00:04.378336 139958455088896 logging_writer.py:48] [32900] global_step=32900, grad_norm=1.692840576171875, loss=1.6958427429199219
I0307 07:00:42.171114 139958446696192 logging_writer.py:48] [33000] global_step=33000, grad_norm=1.637844443321228, loss=1.6931352615356445
I0307 07:01:20.205740 139958455088896 logging_writer.py:48] [33100] global_step=33100, grad_norm=1.9219011068344116, loss=1.6528968811035156
I0307 07:01:42.607114 140114851837120 spec.py:321] Evaluating on the training split.
I0307 07:01:54.919211 140114851837120 spec.py:333] Evaluating on the validation split.
I0307 07:02:22.524085 140114851837120 spec.py:349] Evaluating on the test split.
I0307 07:02:24.301770 140114851837120 submission_runner.py:469] Time since start: 13935.64s, 	Step: 33159, 	{'train/accuracy': 0.6834940910339355, 'train/loss': 1.244361400604248, 'validation/accuracy': 0.6220200061798096, 'validation/loss': 1.579487919807434, 'validation/num_examples': 50000, 'test/accuracy': 0.49400001764297485, 'test/loss': 2.3179776668548584, 'test/num_examples': 10000, 'score': 12806.397213220596, 'total_duration': 13935.638862133026, 'accumulated_submission_time': 12806.397213220596, 'accumulated_eval_time': 1123.9775485992432, 'accumulated_logging_time': 1.6861281394958496}
I0307 07:02:24.390051 139958446696192 logging_writer.py:48] [33159] accumulated_eval_time=1123.98, accumulated_logging_time=1.68613, accumulated_submission_time=12806.4, global_step=33159, preemption_count=0, score=12806.4, test/accuracy=0.494, test/loss=2.31798, test/num_examples=10000, total_duration=13935.6, train/accuracy=0.683494, train/loss=1.24436, validation/accuracy=0.62202, validation/loss=1.57949, validation/num_examples=50000
I0307 07:02:40.672026 139958455088896 logging_writer.py:48] [33200] global_step=33200, grad_norm=1.7812886238098145, loss=1.8122079372406006
I0307 07:03:19.484076 139958446696192 logging_writer.py:48] [33300] global_step=33300, grad_norm=1.8825420141220093, loss=1.75177001953125
I0307 07:03:58.110160 139958455088896 logging_writer.py:48] [33400] global_step=33400, grad_norm=2.1126675605773926, loss=1.7345906496047974
I0307 07:04:36.511288 139958446696192 logging_writer.py:48] [33500] global_step=33500, grad_norm=1.7120823860168457, loss=1.712850570678711
I0307 07:05:15.265676 139958455088896 logging_writer.py:48] [33600] global_step=33600, grad_norm=2.173023223876953, loss=1.7688294649124146
I0307 07:05:54.066261 139958446696192 logging_writer.py:48] [33700] global_step=33700, grad_norm=1.6187398433685303, loss=1.887233853340149
I0307 07:06:33.420908 139958455088896 logging_writer.py:48] [33800] global_step=33800, grad_norm=1.9998619556427002, loss=1.6972417831420898
I0307 07:07:12.425907 139958446696192 logging_writer.py:48] [33900] global_step=33900, grad_norm=1.8819576501846313, loss=1.6875970363616943
I0307 07:07:51.253935 139958455088896 logging_writer.py:48] [34000] global_step=34000, grad_norm=1.789308786392212, loss=1.7503654956817627
I0307 07:08:29.973826 139958446696192 logging_writer.py:48] [34100] global_step=34100, grad_norm=1.8519245386123657, loss=1.7033770084381104
I0307 07:09:08.503120 139958455088896 logging_writer.py:48] [34200] global_step=34200, grad_norm=1.738524079322815, loss=1.6514978408813477
I0307 07:09:46.811625 139958446696192 logging_writer.py:48] [34300] global_step=34300, grad_norm=1.8052515983581543, loss=1.7701281309127808
I0307 07:10:25.445918 139958455088896 logging_writer.py:48] [34400] global_step=34400, grad_norm=1.6563026905059814, loss=1.6386877298355103
I0307 07:10:54.459084 140114851837120 spec.py:321] Evaluating on the training split.
I0307 07:11:06.907355 140114851837120 spec.py:333] Evaluating on the validation split.
I0307 07:11:32.219544 140114851837120 spec.py:349] Evaluating on the test split.
I0307 07:11:33.999306 140114851837120 submission_runner.py:469] Time since start: 14485.34s, 	Step: 34476, 	{'train/accuracy': 0.682059109210968, 'train/loss': 1.257495641708374, 'validation/accuracy': 0.6212599873542786, 'validation/loss': 1.574296236038208, 'validation/num_examples': 50000, 'test/accuracy': 0.4919000267982483, 'test/loss': 2.345731496810913, 'test/num_examples': 10000, 'score': 13316.319287776947, 'total_duration': 14485.336347579956, 'accumulated_submission_time': 13316.319287776947, 'accumulated_eval_time': 1163.5175640583038, 'accumulated_logging_time': 1.782294511795044}
I0307 07:11:34.087192 139958446696192 logging_writer.py:48] [34476] accumulated_eval_time=1163.52, accumulated_logging_time=1.78229, accumulated_submission_time=13316.3, global_step=34476, preemption_count=0, score=13316.3, test/accuracy=0.4919, test/loss=2.34573, test/num_examples=10000, total_duration=14485.3, train/accuracy=0.682059, train/loss=1.2575, validation/accuracy=0.62126, validation/loss=1.5743, validation/num_examples=50000
I0307 07:11:43.988950 139958455088896 logging_writer.py:48] [34500] global_step=34500, grad_norm=1.7862125635147095, loss=1.7607413530349731
I0307 07:12:23.013884 139958446696192 logging_writer.py:48] [34600] global_step=34600, grad_norm=2.1814420223236084, loss=1.7419649362564087
I0307 07:13:01.575117 139958455088896 logging_writer.py:48] [34700] global_step=34700, grad_norm=1.7365810871124268, loss=1.6718196868896484
I0307 07:13:40.415956 139958446696192 logging_writer.py:48] [34800] global_step=34800, grad_norm=1.9178658723831177, loss=1.6780463457107544
I0307 07:14:18.968233 139958455088896 logging_writer.py:48] [34900] global_step=34900, grad_norm=1.6110416650772095, loss=1.55189049243927
I0307 07:14:57.411198 139958446696192 logging_writer.py:48] [35000] global_step=35000, grad_norm=1.689528226852417, loss=1.6115939617156982
I0307 07:15:37.150484 139958455088896 logging_writer.py:48] [35100] global_step=35100, grad_norm=1.8736929893493652, loss=1.6628810167312622
I0307 07:16:15.845594 139958446696192 logging_writer.py:48] [35200] global_step=35200, grad_norm=1.705364465713501, loss=1.713787317276001
I0307 07:16:54.632219 139958455088896 logging_writer.py:48] [35300] global_step=35300, grad_norm=1.763889193534851, loss=1.7569869756698608
I0307 07:17:33.637477 139958446696192 logging_writer.py:48] [35400] global_step=35400, grad_norm=1.8739967346191406, loss=1.663432240486145
I0307 07:18:12.427674 139958455088896 logging_writer.py:48] [35500] global_step=35500, grad_norm=1.7100982666015625, loss=1.7111871242523193
I0307 07:18:51.003426 139958446696192 logging_writer.py:48] [35600] global_step=35600, grad_norm=1.9920094013214111, loss=1.8373304605484009
I0307 07:19:29.144711 139958455088896 logging_writer.py:48] [35700] global_step=35700, grad_norm=2.2600364685058594, loss=1.8196592330932617
I0307 07:20:04.283171 140114851837120 spec.py:321] Evaluating on the training split.
I0307 07:20:17.250112 140114851837120 spec.py:333] Evaluating on the validation split.
I0307 07:20:38.044872 140114851837120 spec.py:349] Evaluating on the test split.
I0307 07:20:39.855826 140114851837120 submission_runner.py:469] Time since start: 15031.19s, 	Step: 35792, 	{'train/accuracy': 0.6935387253761292, 'train/loss': 1.2039399147033691, 'validation/accuracy': 0.6328399777412415, 'validation/loss': 1.5139334201812744, 'validation/num_examples': 50000, 'test/accuracy': 0.4994000196456909, 'test/loss': 2.279236078262329, 'test/num_examples': 10000, 'score': 13826.368644952774, 'total_duration': 15031.192873477936, 'accumulated_submission_time': 13826.368644952774, 'accumulated_eval_time': 1199.0900118350983, 'accumulated_logging_time': 1.8780553340911865}
I0307 07:20:39.942681 139958446696192 logging_writer.py:48] [35792] accumulated_eval_time=1199.09, accumulated_logging_time=1.87806, accumulated_submission_time=13826.4, global_step=35792, preemption_count=0, score=13826.4, test/accuracy=0.4994, test/loss=2.27924, test/num_examples=10000, total_duration=15031.2, train/accuracy=0.693539, train/loss=1.20394, validation/accuracy=0.63284, validation/loss=1.51393, validation/num_examples=50000
I0307 07:20:43.590442 139958455088896 logging_writer.py:48] [35800] global_step=35800, grad_norm=1.91851806640625, loss=1.6439365148544312
I0307 07:21:22.306331 139958446696192 logging_writer.py:48] [35900] global_step=35900, grad_norm=2.01377010345459, loss=1.7195203304290771
I0307 07:22:00.755299 139958455088896 logging_writer.py:48] [36000] global_step=36000, grad_norm=1.7088009119033813, loss=1.594712257385254
I0307 07:22:39.470556 139958446696192 logging_writer.py:48] [36100] global_step=36100, grad_norm=2.1414918899536133, loss=1.755629062652588
I0307 07:23:18.154638 139958455088896 logging_writer.py:48] [36200] global_step=36200, grad_norm=1.8688204288482666, loss=1.667767882347107
I0307 07:23:56.746851 139958446696192 logging_writer.py:48] [36300] global_step=36300, grad_norm=1.782238483428955, loss=1.665809154510498
I0307 07:24:36.265652 139958455088896 logging_writer.py:48] [36400] global_step=36400, grad_norm=1.9312753677368164, loss=1.6497392654418945
I0307 07:25:14.690176 139958446696192 logging_writer.py:48] [36500] global_step=36500, grad_norm=1.7314857244491577, loss=1.7082178592681885
I0307 07:25:53.450382 139958455088896 logging_writer.py:48] [36600] global_step=36600, grad_norm=1.582811951637268, loss=1.564285397529602
I0307 07:26:32.035760 139958446696192 logging_writer.py:48] [36700] global_step=36700, grad_norm=1.5739301443099976, loss=1.7500215768814087
I0307 07:27:10.972946 139958455088896 logging_writer.py:48] [36800] global_step=36800, grad_norm=1.8432856798171997, loss=1.6911516189575195
I0307 07:27:49.559014 139958446696192 logging_writer.py:48] [36900] global_step=36900, grad_norm=1.9841653108596802, loss=1.5991142988204956
I0307 07:28:28.158987 139958455088896 logging_writer.py:48] [37000] global_step=37000, grad_norm=1.9664791822433472, loss=1.6602224111557007
I0307 07:29:06.864439 139958446696192 logging_writer.py:48] [37100] global_step=37100, grad_norm=1.892457365989685, loss=1.6999419927597046
I0307 07:29:09.910580 140114851837120 spec.py:321] Evaluating on the training split.
I0307 07:29:22.719949 140114851837120 spec.py:333] Evaluating on the validation split.
I0307 07:29:43.598837 140114851837120 spec.py:349] Evaluating on the test split.
I0307 07:29:45.420242 140114851837120 submission_runner.py:469] Time since start: 15576.76s, 	Step: 37109, 	{'train/accuracy': 0.6819794178009033, 'train/loss': 1.2457482814788818, 'validation/accuracy': 0.620419979095459, 'validation/loss': 1.5681740045547485, 'validation/num_examples': 50000, 'test/accuracy': 0.49900001287460327, 'test/loss': 2.3057148456573486, 'test/num_examples': 10000, 'score': 14336.162461996078, 'total_duration': 15576.757170677185, 'accumulated_submission_time': 14336.162461996078, 'accumulated_eval_time': 1234.5993437767029, 'accumulated_logging_time': 1.999044418334961}
I0307 07:29:45.558031 139958455088896 logging_writer.py:48] [37109] accumulated_eval_time=1234.6, accumulated_logging_time=1.99904, accumulated_submission_time=14336.2, global_step=37109, preemption_count=0, score=14336.2, test/accuracy=0.499, test/loss=2.30571, test/num_examples=10000, total_duration=15576.8, train/accuracy=0.681979, train/loss=1.24575, validation/accuracy=0.62042, validation/loss=1.56817, validation/num_examples=50000
I0307 07:30:20.790825 139958446696192 logging_writer.py:48] [37200] global_step=37200, grad_norm=1.720515489578247, loss=1.6249445676803589
I0307 07:30:59.251790 139958455088896 logging_writer.py:48] [37300] global_step=37300, grad_norm=1.7720911502838135, loss=1.5694977045059204
I0307 07:31:37.764158 139958446696192 logging_writer.py:48] [37400] global_step=37400, grad_norm=1.8763123750686646, loss=1.6870217323303223
I0307 07:32:16.048406 139958455088896 logging_writer.py:48] [37500] global_step=37500, grad_norm=1.7230571508407593, loss=1.6895781755447388
I0307 07:32:54.884468 139958446696192 logging_writer.py:48] [37600] global_step=37600, grad_norm=1.8571741580963135, loss=1.715185523033142
I0307 07:33:33.875494 139958455088896 logging_writer.py:48] [37700] global_step=37700, grad_norm=1.945777416229248, loss=1.7207460403442383
I0307 07:34:12.962093 139958446696192 logging_writer.py:48] [37800] global_step=37800, grad_norm=1.8614487648010254, loss=1.7804112434387207
I0307 07:34:51.506747 139958455088896 logging_writer.py:48] [37900] global_step=37900, grad_norm=2.1841225624084473, loss=1.6744401454925537
I0307 07:35:30.129466 139958446696192 logging_writer.py:48] [38000] global_step=38000, grad_norm=1.8506660461425781, loss=1.7090446949005127
I0307 07:36:08.610601 139958455088896 logging_writer.py:48] [38100] global_step=38100, grad_norm=1.9052647352218628, loss=1.618605613708496
I0307 07:36:47.427571 139958446696192 logging_writer.py:48] [38200] global_step=38200, grad_norm=1.668683648109436, loss=1.6400723457336426
I0307 07:37:26.296606 139958455088896 logging_writer.py:48] [38300] global_step=38300, grad_norm=2.060072183609009, loss=1.647825002670288
I0307 07:38:05.229456 139958446696192 logging_writer.py:48] [38400] global_step=38400, grad_norm=1.8141402006149292, loss=1.6202664375305176
I0307 07:38:15.442479 140114851837120 spec.py:321] Evaluating on the training split.
I0307 07:38:28.119236 140114851837120 spec.py:333] Evaluating on the validation split.
I0307 07:38:52.705993 140114851837120 spec.py:349] Evaluating on the test split.
I0307 07:38:54.530183 140114851837120 submission_runner.py:469] Time since start: 16125.87s, 	Step: 38427, 	{'train/accuracy': 0.6908681392669678, 'train/loss': 1.2203547954559326, 'validation/accuracy': 0.6298999786376953, 'validation/loss': 1.5351871252059937, 'validation/num_examples': 50000, 'test/accuracy': 0.5031999945640564, 'test/loss': 2.2728452682495117, 'test/num_examples': 10000, 'score': 14845.872792005539, 'total_duration': 16125.867277383804, 'accumulated_submission_time': 14845.872792005539, 'accumulated_eval_time': 1273.6868858337402, 'accumulated_logging_time': 2.1693007946014404}
I0307 07:38:54.603043 139958455088896 logging_writer.py:48] [38427] accumulated_eval_time=1273.69, accumulated_logging_time=2.1693, accumulated_submission_time=14845.9, global_step=38427, preemption_count=0, score=14845.9, test/accuracy=0.5032, test/loss=2.27285, test/num_examples=10000, total_duration=16125.9, train/accuracy=0.690868, train/loss=1.22035, validation/accuracy=0.6299, validation/loss=1.53519, validation/num_examples=50000
I0307 07:39:23.379113 139958446696192 logging_writer.py:48] [38500] global_step=38500, grad_norm=1.8005969524383545, loss=1.6227178573608398
I0307 07:40:02.005879 139958455088896 logging_writer.py:48] [38600] global_step=38600, grad_norm=1.7881797552108765, loss=1.5675418376922607
I0307 07:40:40.520345 139958446696192 logging_writer.py:48] [38700] global_step=38700, grad_norm=2.049055814743042, loss=1.7497663497924805
I0307 07:41:19.489404 139958455088896 logging_writer.py:48] [38800] global_step=38800, grad_norm=1.630042552947998, loss=1.6837530136108398
I0307 07:41:58.261810 139958446696192 logging_writer.py:48] [38900] global_step=38900, grad_norm=2.048137664794922, loss=1.5941659212112427
I0307 07:42:36.953265 139958455088896 logging_writer.py:48] [39000] global_step=39000, grad_norm=1.658625602722168, loss=1.7515233755111694
I0307 07:43:15.477165 139958446696192 logging_writer.py:48] [39100] global_step=39100, grad_norm=2.0012474060058594, loss=1.6124999523162842
I0307 07:43:54.069045 139958455088896 logging_writer.py:48] [39200] global_step=39200, grad_norm=1.6840600967407227, loss=1.6849411725997925
I0307 07:44:32.573086 139958446696192 logging_writer.py:48] [39300] global_step=39300, grad_norm=1.8088968992233276, loss=1.6064040660858154
I0307 07:45:11.622979 139958455088896 logging_writer.py:48] [39400] global_step=39400, grad_norm=1.8775742053985596, loss=1.7334197759628296
I0307 07:45:50.482240 139958446696192 logging_writer.py:48] [39500] global_step=39500, grad_norm=1.936121940612793, loss=1.6417486667633057
I0307 07:46:29.284329 139958455088896 logging_writer.py:48] [39600] global_step=39600, grad_norm=1.9726344347000122, loss=1.690024733543396
I0307 07:47:08.301416 139958446696192 logging_writer.py:48] [39700] global_step=39700, grad_norm=1.8833197355270386, loss=1.7019141912460327
I0307 07:47:24.870893 140114851837120 spec.py:321] Evaluating on the training split.
I0307 07:47:37.599577 140114851837120 spec.py:333] Evaluating on the validation split.
I0307 07:47:57.362362 140114851837120 spec.py:349] Evaluating on the test split.
I0307 07:47:59.202815 140114851837120 submission_runner.py:469] Time since start: 16670.54s, 	Step: 39744, 	{'train/accuracy': 0.6814612150192261, 'train/loss': 1.255134105682373, 'validation/accuracy': 0.6175000071525574, 'validation/loss': 1.583661675453186, 'validation/num_examples': 50000, 'test/accuracy': 0.492900013923645, 'test/loss': 2.313690662384033, 'test/num_examples': 10000, 'score': 15355.978217601776, 'total_duration': 16670.53989672661, 'accumulated_submission_time': 15355.978217601776, 'accumulated_eval_time': 1308.0186321735382, 'accumulated_logging_time': 2.2670600414276123}
I0307 07:47:59.393233 139958455088896 logging_writer.py:48] [39744] accumulated_eval_time=1308.02, accumulated_logging_time=2.26706, accumulated_submission_time=15356, global_step=39744, preemption_count=0, score=15356, test/accuracy=0.4929, test/loss=2.31369, test/num_examples=10000, total_duration=16670.5, train/accuracy=0.681461, train/loss=1.25513, validation/accuracy=0.6175, validation/loss=1.58366, validation/num_examples=50000
I0307 07:48:21.441097 139958446696192 logging_writer.py:48] [39800] global_step=39800, grad_norm=1.6756032705307007, loss=1.6078910827636719
I0307 07:48:59.848274 139958455088896 logging_writer.py:48] [39900] global_step=39900, grad_norm=1.8572349548339844, loss=1.685471773147583
I0307 07:49:38.508459 139958446696192 logging_writer.py:48] [40000] global_step=40000, grad_norm=1.6815104484558105, loss=1.6993415355682373
I0307 07:50:18.394755 139958455088896 logging_writer.py:48] [40100] global_step=40100, grad_norm=1.7076531648635864, loss=1.6738996505737305
I0307 07:50:56.951720 139958446696192 logging_writer.py:48] [40200] global_step=40200, grad_norm=1.84308922290802, loss=1.7663836479187012
I0307 07:51:35.746830 139958455088896 logging_writer.py:48] [40300] global_step=40300, grad_norm=1.6655869483947754, loss=1.682119607925415
I0307 07:52:14.519033 139958446696192 logging_writer.py:48] [40400] global_step=40400, grad_norm=1.8376039266586304, loss=1.618780255317688
I0307 07:52:53.314450 139958455088896 logging_writer.py:48] [40500] global_step=40500, grad_norm=1.659190058708191, loss=1.5763744115829468
I0307 07:53:31.978626 139958446696192 logging_writer.py:48] [40600] global_step=40600, grad_norm=1.916930079460144, loss=1.6949362754821777
I0307 07:54:10.595105 139958455088896 logging_writer.py:48] [40700] global_step=40700, grad_norm=1.783810019493103, loss=1.5869039297103882
I0307 07:54:49.040209 139958446696192 logging_writer.py:48] [40800] global_step=40800, grad_norm=1.7109700441360474, loss=1.7429043054580688
I0307 07:55:27.700375 139958455088896 logging_writer.py:48] [40900] global_step=40900, grad_norm=1.6744277477264404, loss=1.7388598918914795
I0307 07:56:06.245787 139958446696192 logging_writer.py:48] [41000] global_step=41000, grad_norm=1.746891736984253, loss=1.609724760055542
I0307 07:56:29.211926 140114851837120 spec.py:321] Evaluating on the training split.
I0307 07:56:42.328567 140114851837120 spec.py:333] Evaluating on the validation split.
I0307 07:57:07.132478 140114851837120 spec.py:349] Evaluating on the test split.
I0307 07:57:08.905089 140114851837120 submission_runner.py:469] Time since start: 17220.24s, 	Step: 41061, 	{'train/accuracy': 0.6843909025192261, 'train/loss': 1.2503833770751953, 'validation/accuracy': 0.6226999759674072, 'validation/loss': 1.5629533529281616, 'validation/num_examples': 50000, 'test/accuracy': 0.4952000379562378, 'test/loss': 2.304218053817749, 'test/num_examples': 10000, 'score': 15865.581531047821, 'total_duration': 17220.24215555191, 'accumulated_submission_time': 15865.581531047821, 'accumulated_eval_time': 1347.7116103172302, 'accumulated_logging_time': 2.5246074199676514}
I0307 07:57:08.993713 139958455088896 logging_writer.py:48] [41061] accumulated_eval_time=1347.71, accumulated_logging_time=2.52461, accumulated_submission_time=15865.6, global_step=41061, preemption_count=0, score=15865.6, test/accuracy=0.4952, test/loss=2.30422, test/num_examples=10000, total_duration=17220.2, train/accuracy=0.684391, train/loss=1.25038, validation/accuracy=0.6227, validation/loss=1.56295, validation/num_examples=50000
I0307 07:57:24.650771 139958446696192 logging_writer.py:48] [41100] global_step=41100, grad_norm=2.7570226192474365, loss=1.6399643421173096
I0307 07:58:03.133756 139958455088896 logging_writer.py:48] [41200] global_step=41200, grad_norm=1.7554346323013306, loss=1.7016271352767944
I0307 07:58:42.143037 139958446696192 logging_writer.py:48] [41300] global_step=41300, grad_norm=1.9554718732833862, loss=1.7953054904937744
I0307 07:59:21.406682 139958455088896 logging_writer.py:48] [41400] global_step=41400, grad_norm=2.017660617828369, loss=1.6964448690414429
I0307 08:00:00.372660 139958446696192 logging_writer.py:48] [41500] global_step=41500, grad_norm=1.6783808469772339, loss=1.6526546478271484
I0307 08:00:38.859527 139958455088896 logging_writer.py:48] [41600] global_step=41600, grad_norm=1.806100606918335, loss=1.7537786960601807
I0307 08:01:17.902139 139958446696192 logging_writer.py:48] [41700] global_step=41700, grad_norm=1.8211417198181152, loss=1.6303493976593018
I0307 08:01:56.706333 139958455088896 logging_writer.py:48] [41800] global_step=41800, grad_norm=1.5436040163040161, loss=1.5275903940200806
I0307 08:02:35.142291 139958446696192 logging_writer.py:48] [41900] global_step=41900, grad_norm=1.9169801473617554, loss=1.6142724752426147
I0307 08:03:13.979084 139958455088896 logging_writer.py:48] [42000] global_step=42000, grad_norm=1.7283722162246704, loss=1.6158818006515503
I0307 08:03:53.036025 139958446696192 logging_writer.py:48] [42100] global_step=42100, grad_norm=1.683430552482605, loss=1.6143558025360107
I0307 08:04:31.637835 139958455088896 logging_writer.py:48] [42200] global_step=42200, grad_norm=1.8026987314224243, loss=1.6421549320220947
I0307 08:05:10.023591 139958446696192 logging_writer.py:48] [42300] global_step=42300, grad_norm=2.4445996284484863, loss=1.6081454753875732
I0307 08:05:38.960220 140114851837120 spec.py:321] Evaluating on the training split.
I0307 08:05:51.579535 140114851837120 spec.py:333] Evaluating on the validation split.
I0307 08:06:13.625900 140114851837120 spec.py:349] Evaluating on the test split.
I0307 08:06:15.546071 140114851837120 submission_runner.py:469] Time since start: 17766.88s, 	Step: 42375, 	{'train/accuracy': 0.7084462642669678, 'train/loss': 1.1190699338912964, 'validation/accuracy': 0.6439399719238281, 'validation/loss': 1.4614160060882568, 'validation/num_examples': 50000, 'test/accuracy': 0.5172000527381897, 'test/loss': 2.173079490661621, 'test/num_examples': 10000, 'score': 16375.375456809998, 'total_duration': 17766.88316130638, 'accumulated_submission_time': 16375.375456809998, 'accumulated_eval_time': 1384.2973074913025, 'accumulated_logging_time': 2.643423080444336}
I0307 08:06:15.666883 139958455088896 logging_writer.py:48] [42375] accumulated_eval_time=1384.3, accumulated_logging_time=2.64342, accumulated_submission_time=16375.4, global_step=42375, preemption_count=0, score=16375.4, test/accuracy=0.5172, test/loss=2.17308, test/num_examples=10000, total_duration=17766.9, train/accuracy=0.708446, train/loss=1.11907, validation/accuracy=0.64394, validation/loss=1.46142, validation/num_examples=50000
I0307 08:06:25.575042 139958446696192 logging_writer.py:48] [42400] global_step=42400, grad_norm=1.6958309412002563, loss=1.637982964515686
I0307 08:07:04.057437 139958455088896 logging_writer.py:48] [42500] global_step=42500, grad_norm=1.8913538455963135, loss=1.6936194896697998
I0307 08:07:43.572051 139958446696192 logging_writer.py:48] [42600] global_step=42600, grad_norm=2.0426509380340576, loss=1.6391412019729614
I0307 08:08:22.504422 139958455088896 logging_writer.py:48] [42700] global_step=42700, grad_norm=1.642592430114746, loss=1.526587724685669
I0307 08:09:01.094471 139958446696192 logging_writer.py:48] [42800] global_step=42800, grad_norm=1.8360713720321655, loss=1.681046962738037
I0307 08:09:40.072017 139958455088896 logging_writer.py:48] [42900] global_step=42900, grad_norm=1.7266981601715088, loss=1.720337152481079
I0307 08:10:18.646516 139958446696192 logging_writer.py:48] [43000] global_step=43000, grad_norm=1.732933521270752, loss=1.7052955627441406
I0307 08:10:57.293417 139958455088896 logging_writer.py:48] [43100] global_step=43100, grad_norm=1.970731258392334, loss=1.5871590375900269
I0307 08:11:36.301512 139958446696192 logging_writer.py:48] [43200] global_step=43200, grad_norm=1.8126306533813477, loss=1.6582419872283936
I0307 08:12:15.009607 139958455088896 logging_writer.py:48] [43300] global_step=43300, grad_norm=1.7967278957366943, loss=1.6761837005615234
I0307 08:12:53.520679 139958446696192 logging_writer.py:48] [43400] global_step=43400, grad_norm=1.7458163499832153, loss=1.6534565687179565
I0307 08:13:32.008779 139958455088896 logging_writer.py:48] [43500] global_step=43500, grad_norm=2.002281427383423, loss=1.619093894958496
I0307 08:14:10.850107 139958446696192 logging_writer.py:48] [43600] global_step=43600, grad_norm=1.9311965703964233, loss=1.7046401500701904
I0307 08:14:45.606418 140114851837120 spec.py:321] Evaluating on the training split.
I0307 08:14:58.308312 140114851837120 spec.py:333] Evaluating on the validation split.
I0307 08:15:20.785968 140114851837120 spec.py:349] Evaluating on the test split.
I0307 08:15:22.585743 140114851837120 submission_runner.py:469] Time since start: 18313.92s, 	Step: 43690, 	{'train/accuracy': 0.6755420565605164, 'train/loss': 1.2704815864562988, 'validation/accuracy': 0.6170399785041809, 'validation/loss': 1.586108684539795, 'validation/num_examples': 50000, 'test/accuracy': 0.492000013589859, 'test/loss': 2.3367161750793457, 'test/num_examples': 10000, 'score': 16885.124462127686, 'total_duration': 18313.922828435898, 'accumulated_submission_time': 16885.124462127686, 'accumulated_eval_time': 1421.2764596939087, 'accumulated_logging_time': 2.814091205596924}
I0307 08:15:22.694640 139958455088896 logging_writer.py:48] [43690] accumulated_eval_time=1421.28, accumulated_logging_time=2.81409, accumulated_submission_time=16885.1, global_step=43690, preemption_count=0, score=16885.1, test/accuracy=0.492, test/loss=2.33672, test/num_examples=10000, total_duration=18313.9, train/accuracy=0.675542, train/loss=1.27048, validation/accuracy=0.61704, validation/loss=1.58611, validation/num_examples=50000
I0307 08:15:27.105702 139958446696192 logging_writer.py:48] [43700] global_step=43700, grad_norm=1.8195829391479492, loss=1.6859288215637207
I0307 08:16:05.819963 139958455088896 logging_writer.py:48] [43800] global_step=43800, grad_norm=1.8373303413391113, loss=1.623253345489502
I0307 08:16:44.948673 139958446696192 logging_writer.py:48] [43900] global_step=43900, grad_norm=1.7296030521392822, loss=1.7153584957122803
I0307 08:17:23.549137 139958455088896 logging_writer.py:48] [44000] global_step=44000, grad_norm=1.7270146608352661, loss=1.6681840419769287
I0307 08:18:01.966423 139958446696192 logging_writer.py:48] [44100] global_step=44100, grad_norm=1.761550784111023, loss=1.567706823348999
I0307 08:18:40.563721 139958455088896 logging_writer.py:48] [44200] global_step=44200, grad_norm=1.862528920173645, loss=1.6085108518600464
I0307 08:19:19.268414 139958446696192 logging_writer.py:48] [44300] global_step=44300, grad_norm=1.8725734949111938, loss=1.6204603910446167
I0307 08:19:57.978204 139958455088896 logging_writer.py:48] [44400] global_step=44400, grad_norm=1.8136399984359741, loss=1.6281113624572754
I0307 08:20:36.493026 139958446696192 logging_writer.py:48] [44500] global_step=44500, grad_norm=1.7752150297164917, loss=1.7286443710327148
I0307 08:21:15.322409 139958455088896 logging_writer.py:48] [44600] global_step=44600, grad_norm=1.6623337268829346, loss=1.6336942911148071
I0307 08:21:54.099106 139958446696192 logging_writer.py:48] [44700] global_step=44700, grad_norm=1.9047678709030151, loss=1.668772578239441
I0307 08:22:33.117984 139958455088896 logging_writer.py:48] [44800] global_step=44800, grad_norm=2.116763114929199, loss=1.6786489486694336
I0307 08:23:12.117014 139958446696192 logging_writer.py:48] [44900] global_step=44900, grad_norm=1.9218591451644897, loss=1.5591109991073608
I0307 08:23:50.762139 139958455088896 logging_writer.py:48] [45000] global_step=45000, grad_norm=2.236111640930176, loss=1.695631742477417
I0307 08:23:52.723871 140114851837120 spec.py:321] Evaluating on the training split.
I0307 08:24:05.804138 140114851837120 spec.py:333] Evaluating on the validation split.
I0307 08:24:25.110511 140114851837120 spec.py:349] Evaluating on the test split.
I0307 08:24:26.940293 140114851837120 submission_runner.py:469] Time since start: 18858.28s, 	Step: 45006, 	{'train/accuracy': 0.7000757455825806, 'train/loss': 1.1672563552856445, 'validation/accuracy': 0.636959969997406, 'validation/loss': 1.4977331161499023, 'validation/num_examples': 50000, 'test/accuracy': 0.508400022983551, 'test/loss': 2.257812976837158, 'test/num_examples': 10000, 'score': 17394.98658514023, 'total_duration': 18858.27737212181, 'accumulated_submission_time': 17394.98658514023, 'accumulated_eval_time': 1455.4926941394806, 'accumulated_logging_time': 2.9451756477355957}
I0307 08:24:27.041407 139958446696192 logging_writer.py:48] [45006] accumulated_eval_time=1455.49, accumulated_logging_time=2.94518, accumulated_submission_time=17395, global_step=45006, preemption_count=0, score=17395, test/accuracy=0.5084, test/loss=2.25781, test/num_examples=10000, total_duration=18858.3, train/accuracy=0.700076, train/loss=1.16726, validation/accuracy=0.63696, validation/loss=1.49773, validation/num_examples=50000
I0307 08:25:03.957002 139958455088896 logging_writer.py:48] [45100] global_step=45100, grad_norm=1.7377667427062988, loss=1.6746878623962402
I0307 08:25:42.854335 139958446696192 logging_writer.py:48] [45200] global_step=45200, grad_norm=1.9395768642425537, loss=1.6821234226226807
I0307 08:26:21.417287 139958455088896 logging_writer.py:48] [45300] global_step=45300, grad_norm=1.9004778861999512, loss=1.7453936338424683
I0307 08:27:00.220995 139958446696192 logging_writer.py:48] [45400] global_step=45400, grad_norm=1.788196086883545, loss=1.6243577003479004
I0307 08:27:38.499498 139958455088896 logging_writer.py:48] [45500] global_step=45500, grad_norm=1.8631073236465454, loss=1.7180304527282715
I0307 08:28:17.039043 139958446696192 logging_writer.py:48] [45600] global_step=45600, grad_norm=1.6729161739349365, loss=1.7096819877624512
I0307 08:28:56.138344 139958455088896 logging_writer.py:48] [45700] global_step=45700, grad_norm=1.94948410987854, loss=1.6751798391342163
I0307 08:29:34.688027 139958446696192 logging_writer.py:48] [45800] global_step=45800, grad_norm=2.0405235290527344, loss=1.5683891773223877
I0307 08:30:13.533611 139958455088896 logging_writer.py:48] [45900] global_step=45900, grad_norm=1.792144775390625, loss=1.6754043102264404
I0307 08:30:51.894759 139958446696192 logging_writer.py:48] [46000] global_step=46000, grad_norm=1.9581608772277832, loss=1.5518356561660767
I0307 08:31:30.409506 139958455088896 logging_writer.py:48] [46100] global_step=46100, grad_norm=1.8594021797180176, loss=1.5252511501312256
I0307 08:32:09.545411 139958446696192 logging_writer.py:48] [46200] global_step=46200, grad_norm=1.7933427095413208, loss=1.6929994821548462
I0307 08:32:48.579009 139958455088896 logging_writer.py:48] [46300] global_step=46300, grad_norm=1.830422043800354, loss=1.7235279083251953
I0307 08:32:57.181201 140114851837120 spec.py:321] Evaluating on the training split.
I0307 08:33:10.022707 140114851837120 spec.py:333] Evaluating on the validation split.
I0307 08:33:32.311538 140114851837120 spec.py:349] Evaluating on the test split.
I0307 08:33:34.138208 140114851837120 submission_runner.py:469] Time since start: 19405.48s, 	Step: 46322, 	{'train/accuracy': 0.6997169852256775, 'train/loss': 1.1739345788955688, 'validation/accuracy': 0.6357199549674988, 'validation/loss': 1.5046179294586182, 'validation/num_examples': 50000, 'test/accuracy': 0.498600035905838, 'test/loss': 2.2680470943450928, 'test/num_examples': 10000, 'score': 17904.942356348038, 'total_duration': 19405.47530937195, 'accumulated_submission_time': 17904.942356348038, 'accumulated_eval_time': 1492.449557542801, 'accumulated_logging_time': 3.084902048110962}
I0307 08:33:34.194733 139958446696192 logging_writer.py:48] [46322] accumulated_eval_time=1492.45, accumulated_logging_time=3.0849, accumulated_submission_time=17904.9, global_step=46322, preemption_count=0, score=17904.9, test/accuracy=0.4986, test/loss=2.26805, test/num_examples=10000, total_duration=19405.5, train/accuracy=0.699717, train/loss=1.17393, validation/accuracy=0.63572, validation/loss=1.50462, validation/num_examples=50000
I0307 08:34:05.058892 139958455088896 logging_writer.py:48] [46400] global_step=46400, grad_norm=1.8332667350769043, loss=1.5129624605178833
I0307 08:34:43.401313 139958446696192 logging_writer.py:48] [46500] global_step=46500, grad_norm=1.9057027101516724, loss=1.6932908296585083
I0307 08:35:21.855769 139958455088896 logging_writer.py:48] [46600] global_step=46600, grad_norm=1.9706319570541382, loss=1.616876244544983
I0307 08:36:00.566053 139958446696192 logging_writer.py:48] [46700] global_step=46700, grad_norm=1.8816152811050415, loss=1.6597449779510498
I0307 08:36:39.261426 139958455088896 logging_writer.py:48] [46800] global_step=46800, grad_norm=1.914137363433838, loss=1.63462233543396
I0307 08:37:17.877963 139958446696192 logging_writer.py:48] [46900] global_step=46900, grad_norm=1.8934357166290283, loss=1.6657410860061646
I0307 08:37:56.610856 139958455088896 logging_writer.py:48] [47000] global_step=47000, grad_norm=1.7757524251937866, loss=1.608441710472107
I0307 08:38:35.128002 139958446696192 logging_writer.py:48] [47100] global_step=47100, grad_norm=1.9452511072158813, loss=1.5112606287002563
I0307 08:39:14.001868 139958455088896 logging_writer.py:48] [47200] global_step=47200, grad_norm=1.828646183013916, loss=1.6392736434936523
I0307 08:39:52.926204 139958446696192 logging_writer.py:48] [47300] global_step=47300, grad_norm=1.9190396070480347, loss=1.6268254518508911
I0307 08:40:32.494681 139958455088896 logging_writer.py:48] [47400] global_step=47400, grad_norm=2.0642292499542236, loss=1.6478190422058105
I0307 08:41:10.999060 139958446696192 logging_writer.py:48] [47500] global_step=47500, grad_norm=2.0089056491851807, loss=1.597534418106079
I0307 08:41:50.038249 139958455088896 logging_writer.py:48] [47600] global_step=47600, grad_norm=2.088834524154663, loss=1.728771448135376
I0307 08:42:04.432556 140114851837120 spec.py:321] Evaluating on the training split.
I0307 08:42:17.059295 140114851837120 spec.py:333] Evaluating on the validation split.
I0307 08:42:42.359719 140114851837120 spec.py:349] Evaluating on the test split.
I0307 08:42:44.135777 140114851837120 submission_runner.py:469] Time since start: 19955.47s, 	Step: 47637, 	{'train/accuracy': 0.700593888759613, 'train/loss': 1.1661262512207031, 'validation/accuracy': 0.6414399743080139, 'validation/loss': 1.4891859292984009, 'validation/num_examples': 50000, 'test/accuracy': 0.5142000317573547, 'test/loss': 2.2373485565185547, 'test/num_examples': 10000, 'score': 18415.017733097076, 'total_duration': 19955.47287249565, 'accumulated_submission_time': 18415.017733097076, 'accumulated_eval_time': 1532.1526184082031, 'accumulated_logging_time': 3.162531852722168}
I0307 08:42:44.227689 139958446696192 logging_writer.py:48] [47637] accumulated_eval_time=1532.15, accumulated_logging_time=3.16253, accumulated_submission_time=18415, global_step=47637, preemption_count=0, score=18415, test/accuracy=0.5142, test/loss=2.23735, test/num_examples=10000, total_duration=19955.5, train/accuracy=0.700594, train/loss=1.16613, validation/accuracy=0.64144, validation/loss=1.48919, validation/num_examples=50000
I0307 08:43:09.261087 139958455088896 logging_writer.py:48] [47700] global_step=47700, grad_norm=1.7604469060897827, loss=1.5706627368927002
I0307 08:43:47.943496 139958446696192 logging_writer.py:48] [47800] global_step=47800, grad_norm=1.7946280241012573, loss=1.6828839778900146
I0307 08:44:26.557366 139958455088896 logging_writer.py:48] [47900] global_step=47900, grad_norm=1.9854735136032104, loss=1.6102176904678345
I0307 08:45:05.197846 139958446696192 logging_writer.py:48] [48000] global_step=48000, grad_norm=1.9936460256576538, loss=1.688554286956787
I0307 08:45:43.712578 139958455088896 logging_writer.py:48] [48100] global_step=48100, grad_norm=1.904584527015686, loss=1.6898372173309326
I0307 08:46:22.364330 139958446696192 logging_writer.py:48] [48200] global_step=48200, grad_norm=1.6690611839294434, loss=1.5690429210662842
I0307 08:47:00.887014 139958455088896 logging_writer.py:48] [48300] global_step=48300, grad_norm=1.853587031364441, loss=1.653914451599121
I0307 08:47:39.317570 139958446696192 logging_writer.py:48] [48400] global_step=48400, grad_norm=1.749636173248291, loss=1.6381471157073975
I0307 08:48:18.227214 139958455088896 logging_writer.py:48] [48500] global_step=48500, grad_norm=1.8792756795883179, loss=1.7117176055908203
I0307 08:48:57.448245 139958446696192 logging_writer.py:48] [48600] global_step=48600, grad_norm=1.752768874168396, loss=1.5539227724075317
I0307 08:49:36.333441 139958455088896 logging_writer.py:48] [48700] global_step=48700, grad_norm=1.93438720703125, loss=1.6945215463638306
I0307 08:50:15.235024 139958446696192 logging_writer.py:48] [48800] global_step=48800, grad_norm=1.9038302898406982, loss=1.5740338563919067
I0307 08:50:54.497466 139958455088896 logging_writer.py:48] [48900] global_step=48900, grad_norm=1.9510873556137085, loss=1.5449658632278442
I0307 08:51:14.285388 140114851837120 spec.py:321] Evaluating on the training split.
I0307 08:51:27.259856 140114851837120 spec.py:333] Evaluating on the validation split.
I0307 08:51:54.093962 140114851837120 spec.py:349] Evaluating on the test split.
I0307 08:51:55.939269 140114851837120 submission_runner.py:469] Time since start: 20507.28s, 	Step: 48952, 	{'train/accuracy': 0.6927614808082581, 'train/loss': 1.2026716470718384, 'validation/accuracy': 0.6314799785614014, 'validation/loss': 1.526813268661499, 'validation/num_examples': 50000, 'test/accuracy': 0.5072000026702881, 'test/loss': 2.239039897918701, 'test/num_examples': 10000, 'score': 18924.90866971016, 'total_duration': 20507.276310682297, 'accumulated_submission_time': 18924.90866971016, 'accumulated_eval_time': 1573.8062844276428, 'accumulated_logging_time': 3.278242826461792}
I0307 08:51:56.065672 139958446696192 logging_writer.py:48] [48952] accumulated_eval_time=1573.81, accumulated_logging_time=3.27824, accumulated_submission_time=18924.9, global_step=48952, preemption_count=0, score=18924.9, test/accuracy=0.5072, test/loss=2.23904, test/num_examples=10000, total_duration=20507.3, train/accuracy=0.692761, train/loss=1.20267, validation/accuracy=0.63148, validation/loss=1.52681, validation/num_examples=50000
I0307 08:52:14.995515 139958455088896 logging_writer.py:48] [49000] global_step=49000, grad_norm=1.9772422313690186, loss=1.6578731536865234
I0307 08:52:53.781948 139958446696192 logging_writer.py:48] [49100] global_step=49100, grad_norm=1.81453537940979, loss=1.5999724864959717
I0307 08:53:32.753982 139958455088896 logging_writer.py:48] [49200] global_step=49200, grad_norm=1.7960821390151978, loss=1.746987223625183
I0307 08:54:11.393184 139958446696192 logging_writer.py:48] [49300] global_step=49300, grad_norm=1.9301178455352783, loss=1.662926435470581
I0307 08:54:49.890140 139958455088896 logging_writer.py:48] [49400] global_step=49400, grad_norm=1.7640597820281982, loss=1.7042995691299438
I0307 08:55:28.337278 139958446696192 logging_writer.py:48] [49500] global_step=49500, grad_norm=1.7954373359680176, loss=1.5855134725570679
I0307 08:56:07.071586 139958455088896 logging_writer.py:48] [49600] global_step=49600, grad_norm=1.8015111684799194, loss=1.6069897413253784
I0307 08:56:45.862042 139958446696192 logging_writer.py:48] [49700] global_step=49700, grad_norm=1.7110697031021118, loss=1.6149126291275024
I0307 08:57:24.830454 139958455088896 logging_writer.py:48] [49800] global_step=49800, grad_norm=1.7868081331253052, loss=1.6006649732589722
I0307 08:58:03.764586 139958446696192 logging_writer.py:48] [49900] global_step=49900, grad_norm=1.9047194719314575, loss=1.4993352890014648
I0307 08:58:42.380374 139958455088896 logging_writer.py:48] [50000] global_step=50000, grad_norm=1.9097208976745605, loss=1.742789626121521
I0307 08:59:21.674959 139958446696192 logging_writer.py:48] [50100] global_step=50100, grad_norm=1.8497272729873657, loss=1.634188175201416
I0307 09:00:00.596228 139958455088896 logging_writer.py:48] [50200] global_step=50200, grad_norm=1.9245237112045288, loss=1.6837475299835205
I0307 09:00:26.065153 140114851837120 spec.py:321] Evaluating on the training split.
I0307 09:00:38.710440 140114851837120 spec.py:333] Evaluating on the validation split.
I0307 09:01:04.537779 140114851837120 spec.py:349] Evaluating on the test split.
I0307 09:01:06.327946 140114851837120 submission_runner.py:469] Time since start: 21057.67s, 	Step: 50267, 	{'train/accuracy': 0.7000358700752258, 'train/loss': 1.1629594564437866, 'validation/accuracy': 0.6376599669456482, 'validation/loss': 1.5028278827667236, 'validation/num_examples': 50000, 'test/accuracy': 0.5098000168800354, 'test/loss': 2.257024049758911, 'test/num_examples': 10000, 'score': 19434.730928182602, 'total_duration': 21057.665006875992, 'accumulated_submission_time': 19434.730928182602, 'accumulated_eval_time': 1614.0688846111298, 'accumulated_logging_time': 3.4351966381073}
I0307 09:01:06.394984 139958446696192 logging_writer.py:48] [50267] accumulated_eval_time=1614.07, accumulated_logging_time=3.4352, accumulated_submission_time=19434.7, global_step=50267, preemption_count=0, score=19434.7, test/accuracy=0.5098, test/loss=2.25702, test/num_examples=10000, total_duration=21057.7, train/accuracy=0.700036, train/loss=1.16296, validation/accuracy=0.63766, validation/loss=1.50283, validation/num_examples=50000
I0307 09:01:19.678353 139958455088896 logging_writer.py:48] [50300] global_step=50300, grad_norm=1.9070771932601929, loss=1.6798652410507202
I0307 09:01:58.476435 139958446696192 logging_writer.py:48] [50400] global_step=50400, grad_norm=1.970993161201477, loss=1.6724799871444702
I0307 09:02:37.017758 139958455088896 logging_writer.py:48] [50500] global_step=50500, grad_norm=2.292482376098633, loss=1.6369887590408325
I0307 09:03:16.230460 139958446696192 logging_writer.py:48] [50600] global_step=50600, grad_norm=1.9661369323730469, loss=1.6341884136199951
I0307 09:03:55.128662 139958455088896 logging_writer.py:48] [50700] global_step=50700, grad_norm=1.9196699857711792, loss=1.5900615453720093
I0307 09:04:34.077005 139958446696192 logging_writer.py:48] [50800] global_step=50800, grad_norm=1.7906228303909302, loss=1.5882102251052856
I0307 09:05:13.304174 139958455088896 logging_writer.py:48] [50900] global_step=50900, grad_norm=1.8473294973373413, loss=1.4968445301055908
I0307 09:05:51.997812 139958446696192 logging_writer.py:48] [51000] global_step=51000, grad_norm=1.8170413970947266, loss=1.589300513267517
I0307 09:06:30.424628 139958455088896 logging_writer.py:48] [51100] global_step=51100, grad_norm=1.8580150604248047, loss=1.536133050918579
I0307 09:07:09.609990 139958446696192 logging_writer.py:48] [51200] global_step=51200, grad_norm=2.0090444087982178, loss=1.6992321014404297
I0307 09:07:48.629978 139958455088896 logging_writer.py:48] [51300] global_step=51300, grad_norm=1.8351812362670898, loss=1.6495795249938965
I0307 09:08:28.499033 139958446696192 logging_writer.py:48] [51400] global_step=51400, grad_norm=1.785790205001831, loss=1.5554673671722412
I0307 09:09:07.349303 139958455088896 logging_writer.py:48] [51500] global_step=51500, grad_norm=1.8235121965408325, loss=1.5990139245986938
I0307 09:09:36.686592 140114851837120 spec.py:321] Evaluating on the training split.
I0307 09:09:49.434040 140114851837120 spec.py:333] Evaluating on the validation split.
I0307 09:10:12.624209 140114851837120 spec.py:349] Evaluating on the test split.
I0307 09:10:14.416223 140114851837120 submission_runner.py:469] Time since start: 21605.75s, 	Step: 51576, 	{'train/accuracy': 0.7090840339660645, 'train/loss': 1.1309502124786377, 'validation/accuracy': 0.6428399682044983, 'validation/loss': 1.4667868614196777, 'validation/num_examples': 50000, 'test/accuracy': 0.5195000171661377, 'test/loss': 2.168747901916504, 'test/num_examples': 10000, 'score': 19944.843731164932, 'total_duration': 21605.753293275833, 'accumulated_submission_time': 19944.843731164932, 'accumulated_eval_time': 1651.798335313797, 'accumulated_logging_time': 3.536867380142212}
I0307 09:10:14.479800 139958446696192 logging_writer.py:48] [51576] accumulated_eval_time=1651.8, accumulated_logging_time=3.53687, accumulated_submission_time=19944.8, global_step=51576, preemption_count=0, score=19944.8, test/accuracy=0.5195, test/loss=2.16875, test/num_examples=10000, total_duration=21605.8, train/accuracy=0.709084, train/loss=1.13095, validation/accuracy=0.64284, validation/loss=1.46679, validation/num_examples=50000
I0307 09:10:24.517619 139958455088896 logging_writer.py:48] [51600] global_step=51600, grad_norm=1.7392048835754395, loss=1.433021903038025
I0307 09:11:03.483701 139958446696192 logging_writer.py:48] [51700] global_step=51700, grad_norm=2.018881320953369, loss=1.6330679655075073
I0307 09:11:42.262484 139958455088896 logging_writer.py:48] [51800] global_step=51800, grad_norm=1.860789179801941, loss=1.7168478965759277
I0307 09:12:20.970001 139958446696192 logging_writer.py:48] [51900] global_step=51900, grad_norm=1.9164057970046997, loss=1.7086122035980225
I0307 09:13:00.093165 139958455088896 logging_writer.py:48] [52000] global_step=52000, grad_norm=2.282294988632202, loss=1.7877752780914307
I0307 09:13:39.555814 139958446696192 logging_writer.py:48] [52100] global_step=52100, grad_norm=1.780320644378662, loss=1.6112781763076782
I0307 09:14:18.352686 139958455088896 logging_writer.py:48] [52200] global_step=52200, grad_norm=2.0243570804595947, loss=1.7274682521820068
I0307 09:14:57.085728 139958446696192 logging_writer.py:48] [52300] global_step=52300, grad_norm=1.823783040046692, loss=1.6354784965515137
I0307 09:15:36.211997 139958455088896 logging_writer.py:48] [52400] global_step=52400, grad_norm=1.7742291688919067, loss=1.605916142463684
I0307 09:16:15.074291 139958446696192 logging_writer.py:48] [52500] global_step=52500, grad_norm=1.8552247285842896, loss=1.6625163555145264
I0307 09:16:53.900868 139958455088896 logging_writer.py:48] [52600] global_step=52600, grad_norm=1.8267472982406616, loss=1.5814034938812256
I0307 09:17:32.726737 139958446696192 logging_writer.py:48] [52700] global_step=52700, grad_norm=1.992903232574463, loss=1.4808087348937988
I0307 09:18:11.681577 139958455088896 logging_writer.py:48] [52800] global_step=52800, grad_norm=1.9518797397613525, loss=1.6045410633087158
I0307 09:18:44.702817 140114851837120 spec.py:321] Evaluating on the training split.
I0307 09:18:57.479861 140114851837120 spec.py:333] Evaluating on the validation split.
I0307 09:19:16.819871 140114851837120 spec.py:349] Evaluating on the test split.
I0307 09:19:18.650388 140114851837120 submission_runner.py:469] Time since start: 22149.99s, 	Step: 52886, 	{'train/accuracy': 0.6998963356018066, 'train/loss': 1.1588245630264282, 'validation/accuracy': 0.6385399699211121, 'validation/loss': 1.4866459369659424, 'validation/num_examples': 50000, 'test/accuracy': 0.5121999979019165, 'test/loss': 2.2063119411468506, 'test/num_examples': 10000, 'score': 20454.87086224556, 'total_duration': 22149.987450361252, 'accumulated_submission_time': 20454.87086224556, 'accumulated_eval_time': 1685.7457132339478, 'accumulated_logging_time': 3.6505279541015625}
I0307 09:19:18.751824 139958446696192 logging_writer.py:48] [52886] accumulated_eval_time=1685.75, accumulated_logging_time=3.65053, accumulated_submission_time=20454.9, global_step=52886, preemption_count=0, score=20454.9, test/accuracy=0.5122, test/loss=2.20631, test/num_examples=10000, total_duration=22150, train/accuracy=0.699896, train/loss=1.15882, validation/accuracy=0.63854, validation/loss=1.48665, validation/num_examples=50000
I0307 09:19:24.841617 139958455088896 logging_writer.py:48] [52900] global_step=52900, grad_norm=2.028409242630005, loss=1.5766096115112305
I0307 09:20:03.290011 139958446696192 logging_writer.py:48] [53000] global_step=53000, grad_norm=1.998970627784729, loss=1.61250638961792
I0307 09:20:42.155580 139958455088896 logging_writer.py:48] [53100] global_step=53100, grad_norm=1.792568325996399, loss=1.5367331504821777
I0307 09:21:21.269405 139958446696192 logging_writer.py:48] [53200] global_step=53200, grad_norm=2.0018906593322754, loss=1.7160011529922485
I0307 09:22:00.425766 139958455088896 logging_writer.py:48] [53300] global_step=53300, grad_norm=2.0610201358795166, loss=1.574752688407898
I0307 09:22:39.410856 139958446696192 logging_writer.py:48] [53400] global_step=53400, grad_norm=1.8299962282180786, loss=1.5787975788116455
I0307 09:23:18.069918 139958455088896 logging_writer.py:48] [53500] global_step=53500, grad_norm=1.950010895729065, loss=1.5947097539901733
I0307 09:23:57.099730 139958446696192 logging_writer.py:48] [53600] global_step=53600, grad_norm=1.8117338418960571, loss=1.6569722890853882
I0307 09:24:36.004580 139958455088896 logging_writer.py:48] [53700] global_step=53700, grad_norm=1.9939172267913818, loss=1.4876434803009033
I0307 09:25:14.521476 139958446696192 logging_writer.py:48] [53800] global_step=53800, grad_norm=1.871069073677063, loss=1.572996973991394
I0307 09:25:54.341993 139958455088896 logging_writer.py:48] [53900] global_step=53900, grad_norm=2.0487217903137207, loss=1.561413288116455
I0307 09:26:32.925576 139958446696192 logging_writer.py:48] [54000] global_step=54000, grad_norm=1.9638350009918213, loss=1.5910671949386597
I0307 09:27:11.565006 139958455088896 logging_writer.py:48] [54100] global_step=54100, grad_norm=1.8048802614212036, loss=1.569148302078247
I0307 09:27:48.785757 140114851837120 spec.py:321] Evaluating on the training split.
I0307 09:28:01.280492 140114851837120 spec.py:333] Evaluating on the validation split.
I0307 09:28:24.331596 140114851837120 spec.py:349] Evaluating on the test split.
I0307 09:28:26.150340 140114851837120 submission_runner.py:469] Time since start: 22697.49s, 	Step: 54197, 	{'train/accuracy': 0.7091836333274841, 'train/loss': 1.128458857536316, 'validation/accuracy': 0.6462799906730652, 'validation/loss': 1.4478954076766968, 'validation/num_examples': 50000, 'test/accuracy': 0.5200999975204468, 'test/loss': 2.172381639480591, 'test/num_examples': 10000, 'score': 20964.741470336914, 'total_duration': 22697.48745083809, 'accumulated_submission_time': 20964.741470336914, 'accumulated_eval_time': 1723.1101608276367, 'accumulated_logging_time': 3.778111219406128}
I0307 09:28:26.237501 139958446696192 logging_writer.py:48] [54197] accumulated_eval_time=1723.11, accumulated_logging_time=3.77811, accumulated_submission_time=20964.7, global_step=54197, preemption_count=0, score=20964.7, test/accuracy=0.5201, test/loss=2.17238, test/num_examples=10000, total_duration=22697.5, train/accuracy=0.709184, train/loss=1.12846, validation/accuracy=0.64628, validation/loss=1.4479, validation/num_examples=50000
I0307 09:28:27.870107 139958455088896 logging_writer.py:48] [54200] global_step=54200, grad_norm=2.192096471786499, loss=1.549852967262268
I0307 09:29:06.381997 139958446696192 logging_writer.py:48] [54300] global_step=54300, grad_norm=1.794508934020996, loss=1.5042524337768555
I0307 09:29:45.158046 139958455088896 logging_writer.py:48] [54400] global_step=54400, grad_norm=1.8535611629486084, loss=1.552072286605835
I0307 09:30:24.807603 139958446696192 logging_writer.py:48] [54500] global_step=54500, grad_norm=2.0033986568450928, loss=1.6980085372924805
I0307 09:31:03.611820 139958455088896 logging_writer.py:48] [54600] global_step=54600, grad_norm=1.7398959398269653, loss=1.501792550086975
I0307 09:31:42.693155 139958446696192 logging_writer.py:48] [54700] global_step=54700, grad_norm=1.9619324207305908, loss=1.5371358394622803
I0307 09:32:21.588823 139958455088896 logging_writer.py:48] [54800] global_step=54800, grad_norm=2.159471273422241, loss=1.696493148803711
I0307 09:33:00.532541 139958446696192 logging_writer.py:48] [54900] global_step=54900, grad_norm=1.9230362176895142, loss=1.6694071292877197
I0307 09:33:39.375481 139958455088896 logging_writer.py:48] [55000] global_step=55000, grad_norm=1.8854988813400269, loss=1.6379268169403076
I0307 09:34:18.885519 139958446696192 logging_writer.py:48] [55100] global_step=55100, grad_norm=1.8225812911987305, loss=1.5706602334976196
I0307 09:34:57.821877 139958455088896 logging_writer.py:48] [55200] global_step=55200, grad_norm=1.9224902391433716, loss=1.672346591949463
I0307 09:35:36.763975 139958446696192 logging_writer.py:48] [55300] global_step=55300, grad_norm=1.9900449514389038, loss=1.6134238243103027
I0307 09:36:16.177668 139958455088896 logging_writer.py:48] [55400] global_step=55400, grad_norm=1.874524474143982, loss=1.6431019306182861
I0307 09:36:54.864247 139958446696192 logging_writer.py:48] [55500] global_step=55500, grad_norm=1.8364324569702148, loss=1.5304778814315796
I0307 09:36:56.408481 140114851837120 spec.py:321] Evaluating on the training split.
I0307 09:37:08.794258 140114851837120 spec.py:333] Evaluating on the validation split.
I0307 09:37:30.784325 140114851837120 spec.py:349] Evaluating on the test split.
I0307 09:37:32.605471 140114851837120 submission_runner.py:469] Time since start: 23243.94s, 	Step: 55505, 	{'train/accuracy': 0.7135483026504517, 'train/loss': 1.104453444480896, 'validation/accuracy': 0.6495999693870544, 'validation/loss': 1.4517191648483276, 'validation/num_examples': 50000, 'test/accuracy': 0.5208000540733337, 'test/loss': 2.1793429851531982, 'test/num_examples': 10000, 'score': 21474.750054359436, 'total_duration': 23243.942525863647, 'accumulated_submission_time': 21474.750054359436, 'accumulated_eval_time': 1759.3069467544556, 'accumulated_logging_time': 3.8836164474487305}
I0307 09:37:32.693791 139958455088896 logging_writer.py:48] [55505] accumulated_eval_time=1759.31, accumulated_logging_time=3.88362, accumulated_submission_time=21474.8, global_step=55505, preemption_count=0, score=21474.8, test/accuracy=0.5208, test/loss=2.17934, test/num_examples=10000, total_duration=23243.9, train/accuracy=0.713548, train/loss=1.10445, validation/accuracy=0.6496, validation/loss=1.45172, validation/num_examples=50000
I0307 09:38:10.405770 139958446696192 logging_writer.py:48] [55600] global_step=55600, grad_norm=2.0158462524414062, loss=1.6478564739227295
I0307 09:38:48.915645 139958455088896 logging_writer.py:48] [55700] global_step=55700, grad_norm=2.0001373291015625, loss=1.6410508155822754
I0307 09:39:28.309548 139958446696192 logging_writer.py:48] [55800] global_step=55800, grad_norm=1.8018519878387451, loss=1.630077600479126
I0307 09:40:07.197241 139958455088896 logging_writer.py:48] [55900] global_step=55900, grad_norm=1.9061565399169922, loss=1.6178112030029297
I0307 09:40:46.222851 139958446696192 logging_writer.py:48] [56000] global_step=56000, grad_norm=1.88567316532135, loss=1.6208953857421875
I0307 09:41:24.961552 139958455088896 logging_writer.py:48] [56100] global_step=56100, grad_norm=2.530904769897461, loss=1.579054594039917
I0307 09:42:04.540380 139958446696192 logging_writer.py:48] [56200] global_step=56200, grad_norm=2.1631228923797607, loss=1.64632248878479
I0307 09:42:43.576762 139958455088896 logging_writer.py:48] [56300] global_step=56300, grad_norm=1.8201671838760376, loss=1.5838507413864136
I0307 09:43:22.604007 139958446696192 logging_writer.py:48] [56400] global_step=56400, grad_norm=2.0695641040802, loss=1.6147719621658325
I0307 09:44:01.866944 139958455088896 logging_writer.py:48] [56500] global_step=56500, grad_norm=2.0051662921905518, loss=1.6003996133804321
I0307 09:44:40.819709 139958446696192 logging_writer.py:48] [56600] global_step=56600, grad_norm=2.266583204269409, loss=1.5323119163513184
I0307 09:45:20.583481 139958455088896 logging_writer.py:48] [56700] global_step=56700, grad_norm=2.170884609222412, loss=1.6753456592559814
I0307 09:46:00.226843 139958446696192 logging_writer.py:48] [56800] global_step=56800, grad_norm=2.0549707412719727, loss=1.6185739040374756
I0307 09:46:02.931517 140114851837120 spec.py:321] Evaluating on the training split.
I0307 09:46:15.471883 140114851837120 spec.py:333] Evaluating on the validation split.
I0307 09:46:34.393451 140114851837120 spec.py:349] Evaluating on the test split.
I0307 09:46:36.234265 140114851837120 submission_runner.py:469] Time since start: 23787.57s, 	Step: 56808, 	{'train/accuracy': 0.7122129797935486, 'train/loss': 1.115829348564148, 'validation/accuracy': 0.6426799893379211, 'validation/loss': 1.4683713912963867, 'validation/num_examples': 50000, 'test/accuracy': 0.5197000503540039, 'test/loss': 2.186711549758911, 'test/num_examples': 10000, 'score': 21984.769142866135, 'total_duration': 23787.571313381195, 'accumulated_submission_time': 21984.769142866135, 'accumulated_eval_time': 1792.609483242035, 'accumulated_logging_time': 4.043093919754028}
I0307 09:46:36.336285 139958455088896 logging_writer.py:48] [56808] accumulated_eval_time=1792.61, accumulated_logging_time=4.04309, accumulated_submission_time=21984.8, global_step=56808, preemption_count=0, score=21984.8, test/accuracy=0.5197, test/loss=2.18671, test/num_examples=10000, total_duration=23787.6, train/accuracy=0.712213, train/loss=1.11583, validation/accuracy=0.64268, validation/loss=1.46837, validation/num_examples=50000
I0307 09:47:12.886879 139958446696192 logging_writer.py:48] [56900] global_step=56900, grad_norm=2.0147788524627686, loss=1.6310412883758545
I0307 09:47:52.159847 139958455088896 logging_writer.py:48] [57000] global_step=57000, grad_norm=1.9867700338363647, loss=1.6185301542282104
I0307 09:48:31.375234 139958446696192 logging_writer.py:48] [57100] global_step=57100, grad_norm=1.7754074335098267, loss=1.609810471534729
I0307 09:49:10.608585 139958455088896 logging_writer.py:48] [57200] global_step=57200, grad_norm=1.725303053855896, loss=1.5559314489364624
I0307 09:49:49.872123 139958446696192 logging_writer.py:48] [57300] global_step=57300, grad_norm=1.9709320068359375, loss=1.4904706478118896
I0307 09:50:29.793526 139958455088896 logging_writer.py:48] [57400] global_step=57400, grad_norm=1.8591355085372925, loss=1.6628553867340088
I0307 09:51:08.991308 139958446696192 logging_writer.py:48] [57500] global_step=57500, grad_norm=1.8166389465332031, loss=1.5937657356262207
2025-03-07 09:51:19.167573: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 09:51:48.399004 139958455088896 logging_writer.py:48] [57600] global_step=57600, grad_norm=1.9289188385009766, loss=1.5393872261047363
I0307 09:52:27.960240 139958446696192 logging_writer.py:48] [57700] global_step=57700, grad_norm=1.909670352935791, loss=1.5938031673431396
I0307 09:53:45.984432 139958455088896 logging_writer.py:48] [57800] global_step=57800, grad_norm=2.0650062561035156, loss=1.6870769262313843
I0307 09:54:24.188125 139958446696192 logging_writer.py:48] [57900] global_step=57900, grad_norm=1.9210100173950195, loss=1.5336312055587769
I0307 09:55:03.038929 139958455088896 logging_writer.py:48] [58000] global_step=58000, grad_norm=1.8365639448165894, loss=1.697326421737671
I0307 09:55:06.245819 140114851837120 spec.py:321] Evaluating on the training split.
I0307 09:55:19.063687 140114851837120 spec.py:333] Evaluating on the validation split.
I0307 09:55:44.503933 140114851837120 spec.py:349] Evaluating on the test split.
I0307 09:55:46.329652 140114851837120 submission_runner.py:469] Time since start: 24337.67s, 	Step: 58009, 	{'train/accuracy': 0.7186702489852905, 'train/loss': 1.0814591646194458, 'validation/accuracy': 0.6558200120925903, 'validation/loss': 1.4147255420684814, 'validation/num_examples': 50000, 'test/accuracy': 0.5295000076293945, 'test/loss': 2.159853458404541, 'test/num_examples': 10000, 'score': 22494.511714935303, 'total_duration': 24337.666731119156, 'accumulated_submission_time': 22494.511714935303, 'accumulated_eval_time': 1832.69313955307, 'accumulated_logging_time': 4.169570684432983}
I0307 09:55:46.457479 139958446696192 logging_writer.py:48] [58009] accumulated_eval_time=1832.69, accumulated_logging_time=4.16957, accumulated_submission_time=22494.5, global_step=58009, preemption_count=0, score=22494.5, test/accuracy=0.5295, test/loss=2.15985, test/num_examples=10000, total_duration=24337.7, train/accuracy=0.71867, train/loss=1.08146, validation/accuracy=0.65582, validation/loss=1.41473, validation/num_examples=50000
I0307 09:56:23.064887 139958455088896 logging_writer.py:48] [58100] global_step=58100, grad_norm=1.9253747463226318, loss=1.5905401706695557
I0307 09:57:03.059856 139958446696192 logging_writer.py:48] [58200] global_step=58200, grad_norm=1.754517674446106, loss=1.5599048137664795
I0307 09:57:42.896686 139958455088896 logging_writer.py:48] [58300] global_step=58300, grad_norm=1.7536903619766235, loss=1.5488561391830444
I0307 09:58:22.214810 139958446696192 logging_writer.py:48] [58400] global_step=58400, grad_norm=1.71400785446167, loss=1.613617181777954
I0307 09:59:01.506694 139958455088896 logging_writer.py:48] [58500] global_step=58500, grad_norm=2.1661040782928467, loss=1.636135220527649
I0307 09:59:41.690723 139958446696192 logging_writer.py:48] [58600] global_step=58600, grad_norm=2.071713447570801, loss=1.7045929431915283
I0307 10:00:21.264109 139958455088896 logging_writer.py:48] [58700] global_step=58700, grad_norm=1.835430383682251, loss=1.4632201194763184
I0307 10:01:00.600746 139958446696192 logging_writer.py:48] [58800] global_step=58800, grad_norm=1.8869996070861816, loss=1.7006807327270508
I0307 10:01:39.924548 139958455088896 logging_writer.py:48] [58900] global_step=58900, grad_norm=1.911614179611206, loss=1.7176371812820435
I0307 10:02:19.736301 139958446696192 logging_writer.py:48] [59000] global_step=59000, grad_norm=1.8164265155792236, loss=1.604114294052124
I0307 10:02:59.337059 139958455088896 logging_writer.py:48] [59100] global_step=59100, grad_norm=2.0381462574005127, loss=1.5086556673049927
I0307 10:03:38.726196 139958446696192 logging_writer.py:48] [59200] global_step=59200, grad_norm=1.7972114086151123, loss=1.5989899635314941
I0307 10:04:16.592221 140114851837120 spec.py:321] Evaluating on the training split.
I0307 10:04:29.093368 140114851837120 spec.py:333] Evaluating on the validation split.
I0307 10:04:52.061308 140114851837120 spec.py:349] Evaluating on the test split.
I0307 10:04:53.892796 140114851837120 submission_runner.py:469] Time since start: 24885.23s, 	Step: 59296, 	{'train/accuracy': 0.7154216766357422, 'train/loss': 1.0934923887252808, 'validation/accuracy': 0.6480000019073486, 'validation/loss': 1.4511841535568237, 'validation/num_examples': 50000, 'test/accuracy': 0.5241000056266785, 'test/loss': 2.161376953125, 'test/num_examples': 10000, 'score': 23004.466549158096, 'total_duration': 24885.22984647751, 'accumulated_submission_time': 23004.466549158096, 'accumulated_eval_time': 1869.99351644516, 'accumulated_logging_time': 4.32004189491272}
I0307 10:04:54.035237 139958455088896 logging_writer.py:48] [59296] accumulated_eval_time=1869.99, accumulated_logging_time=4.32004, accumulated_submission_time=23004.5, global_step=59296, preemption_count=0, score=23004.5, test/accuracy=0.5241, test/loss=2.16138, test/num_examples=10000, total_duration=24885.2, train/accuracy=0.715422, train/loss=1.09349, validation/accuracy=0.648, validation/loss=1.45118, validation/num_examples=50000
I0307 10:04:55.988415 139958446696192 logging_writer.py:48] [59300] global_step=59300, grad_norm=1.8473159074783325, loss=1.5642783641815186
I0307 10:05:35.451648 139958455088896 logging_writer.py:48] [59400] global_step=59400, grad_norm=1.8961609601974487, loss=1.5150489807128906
I0307 10:06:15.457307 139958446696192 logging_writer.py:48] [59500] global_step=59500, grad_norm=2.0450947284698486, loss=1.7131381034851074
I0307 10:06:55.443735 139958455088896 logging_writer.py:48] [59600] global_step=59600, grad_norm=1.9693256616592407, loss=1.485755205154419
I0307 10:07:35.396008 139958446696192 logging_writer.py:48] [59700] global_step=59700, grad_norm=1.945259690284729, loss=1.6544239521026611
I0307 10:08:15.412709 139958455088896 logging_writer.py:48] [59800] global_step=59800, grad_norm=1.7801103591918945, loss=1.5613863468170166
I0307 10:08:55.208848 139958446696192 logging_writer.py:48] [59900] global_step=59900, grad_norm=2.0126028060913086, loss=1.602898359298706
I0307 10:09:35.029094 139958455088896 logging_writer.py:48] [60000] global_step=60000, grad_norm=2.1513144969940186, loss=1.5093547105789185
2025-03-07 10:09:46.042633: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 10:10:14.852792 139958446696192 logging_writer.py:48] [60100] global_step=60100, grad_norm=2.0284712314605713, loss=1.574700951576233
I0307 10:10:54.289583 139958455088896 logging_writer.py:48] [60200] global_step=60200, grad_norm=2.016679048538208, loss=1.644403338432312
I0307 10:11:34.034498 139958446696192 logging_writer.py:48] [60300] global_step=60300, grad_norm=2.264986515045166, loss=1.5487035512924194
I0307 10:12:13.841627 139958455088896 logging_writer.py:48] [60400] global_step=60400, grad_norm=2.0088281631469727, loss=1.4374891519546509
I0307 10:12:53.876166 139958446696192 logging_writer.py:48] [60500] global_step=60500, grad_norm=2.019061803817749, loss=1.6108331680297852
I0307 10:13:23.995732 140114851837120 spec.py:321] Evaluating on the training split.
I0307 10:13:36.513370 140114851837120 spec.py:333] Evaluating on the validation split.
I0307 10:13:58.158127 140114851837120 spec.py:349] Evaluating on the test split.
I0307 10:13:59.992081 140114851837120 submission_runner.py:469] Time since start: 25431.33s, 	Step: 60576, 	{'train/accuracy': 0.7141661047935486, 'train/loss': 1.122248649597168, 'validation/accuracy': 0.6419999599456787, 'validation/loss': 1.4695637226104736, 'validation/num_examples': 50000, 'test/accuracy': 0.5178000330924988, 'test/loss': 2.1612889766693115, 'test/num_examples': 10000, 'score': 23514.23480796814, 'total_duration': 25431.32913160324, 'accumulated_submission_time': 23514.23480796814, 'accumulated_eval_time': 1905.9896619319916, 'accumulated_logging_time': 4.4986419677734375}
I0307 10:14:00.070726 139958455088896 logging_writer.py:48] [60576] accumulated_eval_time=1905.99, accumulated_logging_time=4.49864, accumulated_submission_time=23514.2, global_step=60576, preemption_count=0, score=23514.2, test/accuracy=0.5178, test/loss=2.16129, test/num_examples=10000, total_duration=25431.3, train/accuracy=0.714166, train/loss=1.12225, validation/accuracy=0.642, validation/loss=1.46956, validation/num_examples=50000
I0307 10:14:10.010318 139958446696192 logging_writer.py:48] [60600] global_step=60600, grad_norm=2.0051016807556152, loss=1.6356117725372314
I0307 10:14:49.158695 139958455088896 logging_writer.py:48] [60700] global_step=60700, grad_norm=1.8443427085876465, loss=1.5371209383010864
I0307 10:15:28.664849 139958446696192 logging_writer.py:48] [60800] global_step=60800, grad_norm=2.018841505050659, loss=1.6752514839172363
I0307 10:16:08.485712 139958455088896 logging_writer.py:48] [60900] global_step=60900, grad_norm=1.9602762460708618, loss=1.5534415245056152
I0307 10:16:47.920063 139958446696192 logging_writer.py:48] [61000] global_step=61000, grad_norm=1.887816309928894, loss=1.4448974132537842
I0307 10:17:27.854738 139958455088896 logging_writer.py:48] [61100] global_step=61100, grad_norm=2.0245113372802734, loss=1.5912065505981445
I0307 10:18:07.886147 139958446696192 logging_writer.py:48] [61200] global_step=61200, grad_norm=2.0036213397979736, loss=1.5969594717025757
I0307 10:18:49.532460 139958455088896 logging_writer.py:48] [61300] global_step=61300, grad_norm=2.2273173332214355, loss=1.6434433460235596
I0307 10:20:07.874296 139958446696192 logging_writer.py:48] [61400] global_step=61400, grad_norm=1.9107614755630493, loss=1.5396360158920288
I0307 10:20:46.665285 139958455088896 logging_writer.py:48] [61500] global_step=61500, grad_norm=2.126871347427368, loss=1.6062548160552979
I0307 10:21:26.734284 139958446696192 logging_writer.py:48] [61600] global_step=61600, grad_norm=1.8899083137512207, loss=1.5978806018829346
I0307 10:22:06.392852 139958455088896 logging_writer.py:48] [61700] global_step=61700, grad_norm=1.9242092370986938, loss=1.4600318670272827
I0307 10:22:30.149777 140114851837120 spec.py:321] Evaluating on the training split.
I0307 10:22:42.693135 140114851837120 spec.py:333] Evaluating on the validation split.
I0307 10:23:04.608749 140114851837120 spec.py:349] Evaluating on the test split.
I0307 10:23:06.427412 140114851837120 submission_runner.py:469] Time since start: 25977.76s, 	Step: 61761, 	{'train/accuracy': 0.727957546710968, 'train/loss': 1.0601836442947388, 'validation/accuracy': 0.6521399617195129, 'validation/loss': 1.4187124967575073, 'validation/num_examples': 50000, 'test/accuracy': 0.5294000506401062, 'test/loss': 2.111990213394165, 'test/num_examples': 10000, 'score': 24024.12597298622, 'total_duration': 25977.76452112198, 'accumulated_submission_time': 24024.12597298622, 'accumulated_eval_time': 1942.2671494483948, 'accumulated_logging_time': 4.62351655960083}
I0307 10:23:06.492857 139958446696192 logging_writer.py:48] [61761] accumulated_eval_time=1942.27, accumulated_logging_time=4.62352, accumulated_submission_time=24024.1, global_step=61761, preemption_count=0, score=24024.1, test/accuracy=0.5294, test/loss=2.11199, test/num_examples=10000, total_duration=25977.8, train/accuracy=0.727958, train/loss=1.06018, validation/accuracy=0.65214, validation/loss=1.41871, validation/num_examples=50000
I0307 10:23:22.489182 139958455088896 logging_writer.py:48] [61800] global_step=61800, grad_norm=1.8351467847824097, loss=1.5192195177078247
I0307 10:24:02.578709 139958446696192 logging_writer.py:48] [61900] global_step=61900, grad_norm=1.7725781202316284, loss=1.4431228637695312
I0307 10:24:43.101081 139958455088896 logging_writer.py:48] [62000] global_step=62000, grad_norm=1.9477362632751465, loss=1.4738717079162598
I0307 10:25:23.225769 139958446696192 logging_writer.py:48] [62100] global_step=62100, grad_norm=1.7637560367584229, loss=1.4977620840072632
I0307 10:26:03.022417 139958455088896 logging_writer.py:48] [62200] global_step=62200, grad_norm=2.1234288215637207, loss=1.5820491313934326
I0307 10:26:42.486153 139958446696192 logging_writer.py:48] [62300] global_step=62300, grad_norm=2.139864206314087, loss=1.5346715450286865
I0307 10:27:22.704906 139958455088896 logging_writer.py:48] [62400] global_step=62400, grad_norm=2.1178512573242188, loss=1.56899893283844
I0307 10:28:02.918024 139958446696192 logging_writer.py:48] [62500] global_step=62500, grad_norm=1.9307883977890015, loss=1.6245709657669067
I0307 10:28:43.238306 139958455088896 logging_writer.py:48] [62600] global_step=62600, grad_norm=2.0037834644317627, loss=1.677211880683899
I0307 10:29:23.737407 139958446696192 logging_writer.py:48] [62700] global_step=62700, grad_norm=2.2033612728118896, loss=1.5892572402954102
I0307 10:30:03.962412 139958455088896 logging_writer.py:48] [62800] global_step=62800, grad_norm=1.8881094455718994, loss=1.6221587657928467
I0307 10:30:43.965416 139958446696192 logging_writer.py:48] [62900] global_step=62900, grad_norm=2.0487165451049805, loss=1.483066439628601
I0307 10:31:23.736501 139958455088896 logging_writer.py:48] [63000] global_step=63000, grad_norm=1.8942071199417114, loss=1.5252104997634888
I0307 10:31:36.471380 140114851837120 spec.py:321] Evaluating on the training split.
I0307 10:31:49.167909 140114851837120 spec.py:333] Evaluating on the validation split.
I0307 10:32:06.141898 140114851837120 spec.py:349] Evaluating on the test split.
I0307 10:32:07.950801 140114851837120 submission_runner.py:469] Time since start: 26519.29s, 	Step: 63033, 	{'train/accuracy': 0.709980845451355, 'train/loss': 1.105270504951477, 'validation/accuracy': 0.6415199637413025, 'validation/loss': 1.4797725677490234, 'validation/num_examples': 50000, 'test/accuracy': 0.5175999999046326, 'test/loss': 2.203636407852173, 'test/num_examples': 10000, 'score': 24533.935073375702, 'total_duration': 26519.287886857986, 'accumulated_submission_time': 24533.935073375702, 'accumulated_eval_time': 1973.7464096546173, 'accumulated_logging_time': 4.706680774688721}
I0307 10:32:08.056538 139958446696192 logging_writer.py:48] [63033] accumulated_eval_time=1973.75, accumulated_logging_time=4.70668, accumulated_submission_time=24533.9, global_step=63033, preemption_count=0, score=24533.9, test/accuracy=0.5176, test/loss=2.20364, test/num_examples=10000, total_duration=26519.3, train/accuracy=0.709981, train/loss=1.10527, validation/accuracy=0.64152, validation/loss=1.47977, validation/num_examples=50000
I0307 10:32:35.165253 139958455088896 logging_writer.py:48] [63100] global_step=63100, grad_norm=1.8473329544067383, loss=1.5325125455856323
I0307 10:33:15.816319 139958446696192 logging_writer.py:48] [63200] global_step=63200, grad_norm=1.935977578163147, loss=1.4397974014282227
I0307 10:33:56.566413 139958455088896 logging_writer.py:48] [63300] global_step=63300, grad_norm=2.143394947052002, loss=1.5954002141952515
I0307 10:34:49.605934 139958446696192 logging_writer.py:48] [63400] global_step=63400, grad_norm=2.1642355918884277, loss=1.6321022510528564
I0307 10:35:28.466986 139958455088896 logging_writer.py:48] [63500] global_step=63500, grad_norm=1.975650668144226, loss=1.6385984420776367
I0307 10:36:08.550684 139958446696192 logging_writer.py:48] [63600] global_step=63600, grad_norm=1.9484678506851196, loss=1.6697092056274414
I0307 10:36:49.011684 139958455088896 logging_writer.py:48] [63700] global_step=63700, grad_norm=1.9002363681793213, loss=1.528860330581665
I0307 10:37:29.545433 139958446696192 logging_writer.py:48] [63800] global_step=63800, grad_norm=1.9055449962615967, loss=1.5313432216644287
I0307 10:38:09.728478 139958455088896 logging_writer.py:48] [63900] global_step=63900, grad_norm=2.0379793643951416, loss=1.5685937404632568
I0307 10:38:49.854050 139958446696192 logging_writer.py:48] [64000] global_step=64000, grad_norm=1.8583499193191528, loss=1.5300952196121216
I0307 10:39:30.494744 139958455088896 logging_writer.py:48] [64100] global_step=64100, grad_norm=1.98782217502594, loss=1.6682586669921875
I0307 10:40:10.856743 139958446696192 logging_writer.py:48] [64200] global_step=64200, grad_norm=1.876457691192627, loss=1.522918939590454
I0307 10:40:38.030952 140114851837120 spec.py:321] Evaluating on the training split.
I0307 10:40:50.367987 140114851837120 spec.py:333] Evaluating on the validation split.
I0307 10:41:11.032156 140114851837120 spec.py:349] Evaluating on the test split.
I0307 10:41:12.851019 140114851837120 submission_runner.py:469] Time since start: 27064.19s, 	Step: 64269, 	{'train/accuracy': 0.721121609210968, 'train/loss': 1.0733492374420166, 'validation/accuracy': 0.647379994392395, 'validation/loss': 1.4394407272338867, 'validation/num_examples': 50000, 'test/accuracy': 0.5190000534057617, 'test/loss': 2.194363594055176, 'test/num_examples': 10000, 'score': 25043.733003139496, 'total_duration': 27064.188108682632, 'accumulated_submission_time': 25043.733003139496, 'accumulated_eval_time': 2008.56631398201, 'accumulated_logging_time': 4.842400312423706}
I0307 10:41:12.942508 139958455088896 logging_writer.py:48] [64269] accumulated_eval_time=2008.57, accumulated_logging_time=4.8424, accumulated_submission_time=25043.7, global_step=64269, preemption_count=0, score=25043.7, test/accuracy=0.519, test/loss=2.19436, test/num_examples=10000, total_duration=27064.2, train/accuracy=0.721122, train/loss=1.07335, validation/accuracy=0.64738, validation/loss=1.43944, validation/num_examples=50000
I0307 10:41:26.127463 139958446696192 logging_writer.py:48] [64300] global_step=64300, grad_norm=1.9245132207870483, loss=1.4907737970352173
I0307 10:42:06.997898 139958455088896 logging_writer.py:48] [64400] global_step=64400, grad_norm=1.8304123878479004, loss=1.5808292627334595
I0307 10:42:47.630864 139958446696192 logging_writer.py:48] [64500] global_step=64500, grad_norm=1.9054524898529053, loss=1.5254908800125122
I0307 10:43:28.175340 139958455088896 logging_writer.py:48] [64600] global_step=64600, grad_norm=1.8571866750717163, loss=1.4617947340011597
I0307 10:44:09.230999 139958446696192 logging_writer.py:48] [64700] global_step=64700, grad_norm=1.9100396633148193, loss=1.5704810619354248
I0307 10:44:49.987877 139958455088896 logging_writer.py:48] [64800] global_step=64800, grad_norm=1.9787514209747314, loss=1.602146029472351
I0307 10:45:35.819711 139958446696192 logging_writer.py:48] [64900] global_step=64900, grad_norm=1.9430558681488037, loss=1.5328001976013184
I0307 10:46:16.477130 139958455088896 logging_writer.py:48] [65000] global_step=65000, grad_norm=2.1736385822296143, loss=1.6360554695129395
I0307 10:46:59.914598 139958446696192 logging_writer.py:48] [65100] global_step=65100, grad_norm=2.0214943885803223, loss=1.4715847969055176
I0307 10:47:40.800505 139958455088896 logging_writer.py:48] [65200] global_step=65200, grad_norm=2.1806938648223877, loss=1.6421082019805908
I0307 10:48:21.728895 139958446696192 logging_writer.py:48] [65300] global_step=65300, grad_norm=1.9336436986923218, loss=1.4922856092453003
I0307 10:49:02.829388 139958455088896 logging_writer.py:48] [65400] global_step=65400, grad_norm=2.0075433254241943, loss=1.5721306800842285
I0307 10:49:42.942997 140114851837120 spec.py:321] Evaluating on the training split.
I0307 10:49:55.970637 140114851837120 spec.py:333] Evaluating on the validation split.
I0307 10:50:16.479918 140114851837120 spec.py:349] Evaluating on the test split.
I0307 10:50:18.303950 140114851837120 submission_runner.py:469] Time since start: 27609.64s, 	Step: 65497, 	{'train/accuracy': 0.7298309803009033, 'train/loss': 1.0494109392166138, 'validation/accuracy': 0.6518999934196472, 'validation/loss': 1.4330863952636719, 'validation/num_examples': 50000, 'test/accuracy': 0.5172000527381897, 'test/loss': 2.2050514221191406, 'test/num_examples': 10000, 'score': 25553.560755729675, 'total_duration': 27609.641005516052, 'accumulated_submission_time': 25553.560755729675, 'accumulated_eval_time': 2043.9270718097687, 'accumulated_logging_time': 4.9603271484375}
I0307 10:50:18.388903 139958446696192 logging_writer.py:48] [65497] accumulated_eval_time=2043.93, accumulated_logging_time=4.96033, accumulated_submission_time=25553.6, global_step=65497, preemption_count=0, score=25553.6, test/accuracy=0.5172, test/loss=2.20505, test/num_examples=10000, total_duration=27609.6, train/accuracy=0.729831, train/loss=1.04941, validation/accuracy=0.6519, validation/loss=1.43309, validation/num_examples=50000
I0307 10:50:19.973246 139958455088896 logging_writer.py:48] [65500] global_step=65500, grad_norm=1.8517951965332031, loss=1.6187936067581177
I0307 10:50:59.946599 139958446696192 logging_writer.py:48] [65600] global_step=65600, grad_norm=1.9722917079925537, loss=1.5919103622436523
I0307 10:51:40.328564 139958455088896 logging_writer.py:48] [65700] global_step=65700, grad_norm=2.1100518703460693, loss=1.449852466583252
I0307 10:52:20.304533 139958446696192 logging_writer.py:48] [65800] global_step=65800, grad_norm=1.8536258935928345, loss=1.438267469406128
I0307 10:52:59.787526 139958455088896 logging_writer.py:48] [65900] global_step=65900, grad_norm=1.773643970489502, loss=1.449657917022705
I0307 10:53:39.997920 139958446696192 logging_writer.py:48] [66000] global_step=66000, grad_norm=2.135331869125366, loss=1.5929142236709595
I0307 10:54:20.442583 139958455088896 logging_writer.py:48] [66100] global_step=66100, grad_norm=2.018620491027832, loss=1.6603904962539673
I0307 10:55:00.783784 139958446696192 logging_writer.py:48] [66200] global_step=66200, grad_norm=2.1137423515319824, loss=1.6007637977600098
2025-03-07 10:55:35.403625: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 10:55:41.364635 139958455088896 logging_writer.py:48] [66300] global_step=66300, grad_norm=1.879311203956604, loss=1.5815417766571045
I0307 10:57:02.705958 139958446696192 logging_writer.py:48] [66400] global_step=66400, grad_norm=1.9373383522033691, loss=1.6045689582824707
I0307 10:57:42.954480 139958455088896 logging_writer.py:48] [66500] global_step=66500, grad_norm=1.9498172998428345, loss=1.5179314613342285
I0307 10:58:22.950397 139958446696192 logging_writer.py:48] [66600] global_step=66600, grad_norm=2.085012674331665, loss=1.5464589595794678
I0307 10:58:48.637384 140114851837120 spec.py:321] Evaluating on the training split.
I0307 10:59:02.074015 140114851837120 spec.py:333] Evaluating on the validation split.
I0307 10:59:22.410695 140114851837120 spec.py:349] Evaluating on the test split.
I0307 10:59:24.252910 140114851837120 submission_runner.py:469] Time since start: 28155.59s, 	Step: 66665, 	{'train/accuracy': 0.7349728941917419, 'train/loss': 1.0126609802246094, 'validation/accuracy': 0.6545799970626831, 'validation/loss': 1.403482437133789, 'validation/num_examples': 50000, 'test/accuracy': 0.5242000222206116, 'test/loss': 2.132732629776001, 'test/num_examples': 10000, 'score': 26063.64106655121, 'total_duration': 28155.589996814728, 'accumulated_submission_time': 26063.64106655121, 'accumulated_eval_time': 2079.5424258708954, 'accumulated_logging_time': 5.076892375946045}
I0307 10:59:24.411733 139958455088896 logging_writer.py:48] [66665] accumulated_eval_time=2079.54, accumulated_logging_time=5.07689, accumulated_submission_time=26063.6, global_step=66665, preemption_count=0, score=26063.6, test/accuracy=0.5242, test/loss=2.13273, test/num_examples=10000, total_duration=28155.6, train/accuracy=0.734973, train/loss=1.01266, validation/accuracy=0.65458, validation/loss=1.40348, validation/num_examples=50000
I0307 10:59:48.696227 139958446696192 logging_writer.py:48] [66700] global_step=66700, grad_norm=1.9701323509216309, loss=1.5694268941879272
I0307 11:00:34.334388 139958455088896 logging_writer.py:48] [66800] global_step=66800, grad_norm=2.047152280807495, loss=1.6429919004440308
I0307 11:01:14.634473 139958446696192 logging_writer.py:48] [66900] global_step=66900, grad_norm=1.924630045890808, loss=1.5354137420654297
I0307 11:01:55.326932 139958455088896 logging_writer.py:48] [67000] global_step=67000, grad_norm=1.995207667350769, loss=1.6534416675567627
I0307 11:02:36.814105 139958446696192 logging_writer.py:48] [67100] global_step=67100, grad_norm=2.1362059116363525, loss=1.446251392364502
I0307 11:03:16.799669 139958455088896 logging_writer.py:48] [67200] global_step=67200, grad_norm=2.3282079696655273, loss=1.4663236141204834
I0307 11:03:59.739662 139958446696192 logging_writer.py:48] [67300] global_step=67300, grad_norm=2.046419143676758, loss=1.4433223009109497
I0307 11:04:50.086437 139958455088896 logging_writer.py:48] [67400] global_step=67400, grad_norm=2.233339309692383, loss=1.5981247425079346
I0307 11:05:28.858190 139958446696192 logging_writer.py:48] [67500] global_step=67500, grad_norm=2.0158023834228516, loss=1.513922929763794
2025-03-07 11:05:47.043367: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 11:06:11.760459 139958455088896 logging_writer.py:48] [67600] global_step=67600, grad_norm=2.207531213760376, loss=1.5823439359664917
I0307 11:06:49.879476 139958446696192 logging_writer.py:48] [67700] global_step=67700, grad_norm=2.0051400661468506, loss=1.4393147230148315
I0307 11:07:28.199208 139958455088896 logging_writer.py:48] [67800] global_step=67800, grad_norm=1.9388188123703003, loss=1.5141496658325195
I0307 11:07:54.588370 140114851837120 spec.py:321] Evaluating on the training split.
I0307 11:08:08.149695 140114851837120 spec.py:333] Evaluating on the validation split.
I0307 11:08:27.596487 140114851837120 spec.py:349] Evaluating on the test split.
I0307 11:08:29.419279 140114851837120 submission_runner.py:469] Time since start: 28700.76s, 	Step: 67868, 	{'train/accuracy': 0.7333784699440002, 'train/loss': 1.0174095630645752, 'validation/accuracy': 0.6527599692344666, 'validation/loss': 1.4294004440307617, 'validation/num_examples': 50000, 'test/accuracy': 0.52510005235672, 'test/loss': 2.167860507965088, 'test/num_examples': 10000, 'score': 26573.65345621109, 'total_duration': 28700.756385087967, 'accumulated_submission_time': 26573.65345621109, 'accumulated_eval_time': 2114.3731939792633, 'accumulated_logging_time': 5.257038116455078}
I0307 11:08:29.518437 139958446696192 logging_writer.py:48] [67868] accumulated_eval_time=2114.37, accumulated_logging_time=5.25704, accumulated_submission_time=26573.7, global_step=67868, preemption_count=0, score=26573.7, test/accuracy=0.5251, test/loss=2.16786, test/num_examples=10000, total_duration=28700.8, train/accuracy=0.733378, train/loss=1.01741, validation/accuracy=0.65276, validation/loss=1.4294, validation/num_examples=50000
I0307 11:08:42.542644 139958455088896 logging_writer.py:48] [67900] global_step=67900, grad_norm=1.8937954902648926, loss=1.3724579811096191
I0307 11:09:22.291569 139958446696192 logging_writer.py:48] [68000] global_step=68000, grad_norm=1.8542238473892212, loss=1.4819996356964111
I0307 11:10:01.911855 139958455088896 logging_writer.py:48] [68100] global_step=68100, grad_norm=2.0890486240386963, loss=1.6028096675872803
I0307 11:10:43.182487 139958446696192 logging_writer.py:48] [68200] global_step=68200, grad_norm=2.6315371990203857, loss=1.7055050134658813
I0307 11:11:24.115274 139958455088896 logging_writer.py:48] [68300] global_step=68300, grad_norm=2.3905911445617676, loss=1.6039462089538574
I0307 11:12:04.253581 139958446696192 logging_writer.py:48] [68400] global_step=68400, grad_norm=1.9712883234024048, loss=1.5588723421096802
I0307 11:12:42.868122 139958455088896 logging_writer.py:48] [68500] global_step=68500, grad_norm=2.16257381439209, loss=1.6890225410461426
I0307 11:13:21.900092 139958446696192 logging_writer.py:48] [68600] global_step=68600, grad_norm=1.968531608581543, loss=1.522534966468811
I0307 11:14:00.705207 139958455088896 logging_writer.py:48] [68700] global_step=68700, grad_norm=1.9581278562545776, loss=1.504148244857788
I0307 11:14:39.917391 139958446696192 logging_writer.py:48] [68800] global_step=68800, grad_norm=2.0269229412078857, loss=1.5712010860443115
I0307 11:16:01.780200 139958455088896 logging_writer.py:48] [68900] global_step=68900, grad_norm=2.0478196144104004, loss=1.5618973970413208
I0307 11:16:41.558454 139958446696192 logging_writer.py:48] [69000] global_step=69000, grad_norm=2.0136382579803467, loss=1.5818772315979004
I0307 11:16:59.463415 140114851837120 spec.py:321] Evaluating on the training split.
I0307 11:17:12.086588 140114851837120 spec.py:333] Evaluating on the validation split.
I0307 11:17:31.257671 140114851837120 spec.py:349] Evaluating on the test split.
I0307 11:17:33.076453 140114851837120 submission_runner.py:469] Time since start: 29244.41s, 	Step: 69046, 	{'train/accuracy': 0.7481465339660645, 'train/loss': 0.9577359557151794, 'validation/accuracy': 0.65065997838974, 'validation/loss': 1.4381996393203735, 'validation/num_examples': 50000, 'test/accuracy': 0.5209000110626221, 'test/loss': 2.1692967414855957, 'test/num_examples': 10000, 'score': 27083.43567442894, 'total_duration': 29244.413563728333, 'accumulated_submission_time': 27083.43567442894, 'accumulated_eval_time': 2147.986088991165, 'accumulated_logging_time': 5.380809783935547}
I0307 11:17:33.234907 139958455088896 logging_writer.py:48] [69046] accumulated_eval_time=2147.99, accumulated_logging_time=5.38081, accumulated_submission_time=27083.4, global_step=69046, preemption_count=0, score=27083.4, test/accuracy=0.5209, test/loss=2.1693, test/num_examples=10000, total_duration=29244.4, train/accuracy=0.748147, train/loss=0.957736, validation/accuracy=0.65066, validation/loss=1.4382, validation/num_examples=50000
I0307 11:17:54.996542 139958446696192 logging_writer.py:48] [69100] global_step=69100, grad_norm=2.162893533706665, loss=1.6154251098632812
I0307 11:18:34.809530 139958455088896 logging_writer.py:48] [69200] global_step=69200, grad_norm=2.1030242443084717, loss=1.5564236640930176
I0307 11:19:14.536058 139958446696192 logging_writer.py:48] [69300] global_step=69300, grad_norm=2.1029560565948486, loss=1.4447197914123535
I0307 11:19:54.992232 139958455088896 logging_writer.py:48] [69400] global_step=69400, grad_norm=2.220632553100586, loss=1.5044200420379639
I0307 11:20:39.085695 139958446696192 logging_writer.py:48] [69500] global_step=69500, grad_norm=2.3860955238342285, loss=1.5364837646484375
I0307 11:21:24.059192 139958455088896 logging_writer.py:48] [69600] global_step=69600, grad_norm=1.8654013872146606, loss=1.4450889825820923
I0307 11:22:06.394464 139958446696192 logging_writer.py:48] [69700] global_step=69700, grad_norm=1.9523265361785889, loss=1.4762396812438965
I0307 11:22:47.061496 139958455088896 logging_writer.py:48] [69800] global_step=69800, grad_norm=1.9701467752456665, loss=1.5636705160140991
I0307 11:23:28.781330 139958446696192 logging_writer.py:48] [69900] global_step=69900, grad_norm=2.3529739379882812, loss=1.5016305446624756
I0307 11:24:08.908884 139958455088896 logging_writer.py:48] [70000] global_step=70000, grad_norm=2.0444247722625732, loss=1.543992519378662
I0307 11:24:50.227249 139958446696192 logging_writer.py:48] [70100] global_step=70100, grad_norm=2.0003883838653564, loss=1.5055444240570068
I0307 11:25:29.003896 139958455088896 logging_writer.py:48] [70200] global_step=70200, grad_norm=2.0651307106018066, loss=1.4183005094528198
I0307 11:26:03.486165 140114851837120 spec.py:321] Evaluating on the training split.
I0307 11:26:16.726713 140114851837120 spec.py:333] Evaluating on the validation split.
I0307 11:26:39.106107 140114851837120 spec.py:349] Evaluating on the test split.
I0307 11:26:40.925502 140114851837120 submission_runner.py:469] Time since start: 29792.26s, 	Step: 70286, 	{'train/accuracy': 0.7789381146430969, 'train/loss': 0.8355110883712769, 'validation/accuracy': 0.663159966468811, 'validation/loss': 1.3805304765701294, 'validation/num_examples': 50000, 'test/accuracy': 0.526900053024292, 'test/loss': 2.1390299797058105, 'test/num_examples': 10000, 'score': 27593.508466959, 'total_duration': 29792.2626080513, 'accumulated_submission_time': 27593.508466959, 'accumulated_eval_time': 2185.425276994705, 'accumulated_logging_time': 5.573187589645386}
I0307 11:26:41.052448 139958446696192 logging_writer.py:48] [70286] accumulated_eval_time=2185.43, accumulated_logging_time=5.57319, accumulated_submission_time=27593.5, global_step=70286, preemption_count=0, score=27593.5, test/accuracy=0.5269, test/loss=2.13903, test/num_examples=10000, total_duration=29792.3, train/accuracy=0.778938, train/loss=0.835511, validation/accuracy=0.66316, validation/loss=1.38053, validation/num_examples=50000
I0307 11:26:47.006302 139958455088896 logging_writer.py:48] [70300] global_step=70300, grad_norm=1.870987892150879, loss=1.5076347589492798
I0307 11:27:26.143365 139958446696192 logging_writer.py:48] [70400] global_step=70400, grad_norm=2.0591413974761963, loss=1.544425129890442
I0307 11:28:07.350632 139958455088896 logging_writer.py:48] [70500] global_step=70500, grad_norm=2.131694793701172, loss=1.5442553758621216
I0307 11:28:48.679835 139958446696192 logging_writer.py:48] [70600] global_step=70600, grad_norm=2.121910572052002, loss=1.5666450262069702
I0307 11:29:34.352425 139958455088896 logging_writer.py:48] [70700] global_step=70700, grad_norm=2.2469112873077393, loss=1.5357952117919922
I0307 11:30:12.346379 139958446696192 logging_writer.py:48] [70800] global_step=70800, grad_norm=1.996899127960205, loss=1.593322992324829
I0307 11:30:50.553760 139958455088896 logging_writer.py:48] [70900] global_step=70900, grad_norm=2.0966789722442627, loss=1.5538907051086426
I0307 11:31:28.327919 139958446696192 logging_writer.py:48] [71000] global_step=71000, grad_norm=1.8589727878570557, loss=1.538002371788025
I0307 11:32:09.331027 139958455088896 logging_writer.py:48] [71100] global_step=71100, grad_norm=2.106781482696533, loss=1.5596940517425537
I0307 11:32:50.806683 139958446696192 logging_writer.py:48] [71200] global_step=71200, grad_norm=1.9974504709243774, loss=1.4193623065948486
2025-03-07 11:33:29.624929: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 11:33:31.277287 139958455088896 logging_writer.py:48] [71300] global_step=71300, grad_norm=2.1428802013397217, loss=1.5960851907730103
I0307 11:34:10.223524 139958446696192 logging_writer.py:48] [71400] global_step=71400, grad_norm=2.0037009716033936, loss=1.609775424003601
I0307 11:34:53.646557 139958455088896 logging_writer.py:48] [71500] global_step=71500, grad_norm=2.1273088455200195, loss=1.4320343732833862
I0307 11:35:11.106173 140114851837120 spec.py:321] Evaluating on the training split.
I0307 11:35:24.462585 140114851837120 spec.py:333] Evaluating on the validation split.
I0307 11:35:46.772426 140114851837120 spec.py:349] Evaluating on the test split.
I0307 11:35:48.587372 140114851837120 submission_runner.py:469] Time since start: 30339.92s, 	Step: 71538, 	{'train/accuracy': 0.7571149468421936, 'train/loss': 0.935540497303009, 'validation/accuracy': 0.6527799963951111, 'validation/loss': 1.4146453142166138, 'validation/num_examples': 50000, 'test/accuracy': 0.5223000049591064, 'test/loss': 2.1298270225524902, 'test/num_examples': 10000, 'score': 28103.37496995926, 'total_duration': 30339.924479722977, 'accumulated_submission_time': 28103.37496995926, 'accumulated_eval_time': 2222.906327724457, 'accumulated_logging_time': 5.740803003311157}
I0307 11:35:48.685383 139958446696192 logging_writer.py:48] [71538] accumulated_eval_time=2222.91, accumulated_logging_time=5.7408, accumulated_submission_time=28103.4, global_step=71538, preemption_count=0, score=28103.4, test/accuracy=0.5223, test/loss=2.12983, test/num_examples=10000, total_duration=30339.9, train/accuracy=0.757115, train/loss=0.93554, validation/accuracy=0.65278, validation/loss=1.41465, validation/num_examples=50000
I0307 11:36:18.661580 139958455088896 logging_writer.py:48] [71600] global_step=71600, grad_norm=2.0263075828552246, loss=1.4950041770935059
I0307 11:37:10.147817 139958446696192 logging_writer.py:48] [71700] global_step=71700, grad_norm=2.222475528717041, loss=1.5866761207580566
I0307 11:37:56.319981 139958455088896 logging_writer.py:48] [71800] global_step=71800, grad_norm=2.122534990310669, loss=1.5077611207962036
I0307 11:38:41.356266 139958446696192 logging_writer.py:48] [71900] global_step=71900, grad_norm=2.24130916595459, loss=1.5459767580032349
I0307 11:39:26.976732 139958455088896 logging_writer.py:48] [72000] global_step=72000, grad_norm=2.0084612369537354, loss=1.5445520877838135
I0307 11:40:08.629458 139958446696192 logging_writer.py:48] [72100] global_step=72100, grad_norm=1.9169795513153076, loss=1.5318779945373535
I0307 11:40:48.596534 139958455088896 logging_writer.py:48] [72200] global_step=72200, grad_norm=2.1744298934936523, loss=1.5752068758010864
I0307 11:41:32.483669 139958446696192 logging_writer.py:48] [72300] global_step=72300, grad_norm=1.9316117763519287, loss=1.4782896041870117
I0307 11:42:22.464662 139958455088896 logging_writer.py:48] [72400] global_step=72400, grad_norm=1.9651119709014893, loss=1.4927761554718018
I0307 11:43:11.392477 139958446696192 logging_writer.py:48] [72500] global_step=72500, grad_norm=1.8900450468063354, loss=1.4422425031661987
I0307 11:44:18.638880 140114851837120 spec.py:321] Evaluating on the training split.
I0307 11:44:32.310302 140114851837120 spec.py:333] Evaluating on the validation split.
I0307 11:44:54.518215 140114851837120 spec.py:349] Evaluating on the test split.
I0307 11:44:56.307663 140114851837120 submission_runner.py:469] Time since start: 30887.64s, 	Step: 72588, 	{'train/accuracy': 0.7161591053009033, 'train/loss': 1.095617413520813, 'validation/accuracy': 0.6501399874687195, 'validation/loss': 1.4448403120040894, 'validation/num_examples': 50000, 'test/accuracy': 0.515500009059906, 'test/loss': 2.2124476432800293, 'test/num_examples': 10000, 'score': 28613.17933011055, 'total_duration': 30887.644747257233, 'accumulated_submission_time': 28613.17933011055, 'accumulated_eval_time': 2260.5749423503876, 'accumulated_logging_time': 5.866234302520752}
I0307 11:44:56.400295 139958455088896 logging_writer.py:48] [72588] accumulated_eval_time=2260.57, accumulated_logging_time=5.86623, accumulated_submission_time=28613.2, global_step=72588, preemption_count=0, score=28613.2, test/accuracy=0.5155, test/loss=2.21245, test/num_examples=10000, total_duration=30887.6, train/accuracy=0.716159, train/loss=1.09562, validation/accuracy=0.65014, validation/loss=1.44484, validation/num_examples=50000
I0307 11:45:01.539402 139958446696192 logging_writer.py:48] [72600] global_step=72600, grad_norm=1.9916437864303589, loss=1.5616569519042969
I0307 11:45:41.076233 139958455088896 logging_writer.py:48] [72700] global_step=72700, grad_norm=1.9181272983551025, loss=1.5386486053466797
I0307 11:46:33.974567 139958446696192 logging_writer.py:48] [72800] global_step=72800, grad_norm=2.2183196544647217, loss=1.5350860357284546
I0307 11:47:29.800320 139958455088896 logging_writer.py:48] [72900] global_step=72900, grad_norm=2.17167067527771, loss=1.524547815322876
I0307 11:48:14.339040 139958446696192 logging_writer.py:48] [73000] global_step=73000, grad_norm=1.9942936897277832, loss=1.440595269203186
I0307 11:48:52.539710 139958455088896 logging_writer.py:48] [73100] global_step=73100, grad_norm=2.158677816390991, loss=1.4294853210449219
I0307 11:49:30.950845 139958446696192 logging_writer.py:48] [73200] global_step=73200, grad_norm=2.032968282699585, loss=1.4295294284820557
I0307 11:50:10.828000 139958455088896 logging_writer.py:48] [73300] global_step=73300, grad_norm=1.9491503238677979, loss=1.525414228439331
I0307 11:50:47.326709 139958446696192 logging_writer.py:48] [73400] global_step=73400, grad_norm=2.5386850833892822, loss=1.6660888195037842
I0307 11:51:25.641182 139958455088896 logging_writer.py:48] [73500] global_step=73500, grad_norm=1.942760705947876, loss=1.421937346458435
I0307 11:52:05.628131 139958446696192 logging_writer.py:48] [73600] global_step=73600, grad_norm=1.8903063535690308, loss=1.456594705581665
I0307 11:52:48.251497 139958455088896 logging_writer.py:48] [73700] global_step=73700, grad_norm=2.203686237335205, loss=1.4867883920669556
I0307 11:53:26.676221 140114851837120 spec.py:321] Evaluating on the training split.
I0307 11:53:40.703757 140114851837120 spec.py:333] Evaluating on the validation split.
I0307 11:54:04.986270 140114851837120 spec.py:349] Evaluating on the test split.
I0307 11:54:06.731512 140114851837120 submission_runner.py:469] Time since start: 31438.07s, 	Step: 73787, 	{'train/accuracy': 0.7195471525192261, 'train/loss': 1.0715832710266113, 'validation/accuracy': 0.6534599661827087, 'validation/loss': 1.4173479080200195, 'validation/num_examples': 50000, 'test/accuracy': 0.5235000252723694, 'test/loss': 2.1732168197631836, 'test/num_examples': 10000, 'score': 29123.30188345909, 'total_duration': 31438.068590402603, 'accumulated_submission_time': 29123.30188345909, 'accumulated_eval_time': 2300.630066871643, 'accumulated_logging_time': 5.974643707275391}
I0307 11:54:06.835431 139958446696192 logging_writer.py:48] [73787] accumulated_eval_time=2300.63, accumulated_logging_time=5.97464, accumulated_submission_time=29123.3, global_step=73787, preemption_count=0, score=29123.3, test/accuracy=0.5235, test/loss=2.17322, test/num_examples=10000, total_duration=31438.1, train/accuracy=0.719547, train/loss=1.07158, validation/accuracy=0.65346, validation/loss=1.41735, validation/num_examples=50000
I0307 11:54:12.411424 139958455088896 logging_writer.py:48] [73800] global_step=73800, grad_norm=2.2776453495025635, loss=1.5615098476409912
I0307 11:54:52.177991 139958446696192 logging_writer.py:48] [73900] global_step=73900, grad_norm=1.9496650695800781, loss=1.504127860069275
I0307 11:55:31.316690 139958455088896 logging_writer.py:48] [74000] global_step=74000, grad_norm=2.377758264541626, loss=1.6028101444244385
I0307 11:56:21.790194 139958446696192 logging_writer.py:48] [74100] global_step=74100, grad_norm=2.055647373199463, loss=1.543494462966919
I0307 11:57:12.467710 139958455088896 logging_writer.py:48] [74200] global_step=74200, grad_norm=1.858789086341858, loss=1.4937623739242554
I0307 11:57:59.050770 139958446696192 logging_writer.py:48] [74300] global_step=74300, grad_norm=2.148552179336548, loss=1.5659949779510498
I0307 11:58:43.939733 139958455088896 logging_writer.py:48] [74400] global_step=74400, grad_norm=2.0459249019622803, loss=1.480751633644104
I0307 11:59:30.754234 139958446696192 logging_writer.py:48] [74500] global_step=74500, grad_norm=2.3319342136383057, loss=1.5573863983154297
I0307 12:00:31.637408 139958455088896 logging_writer.py:48] [74600] global_step=74600, grad_norm=2.039633274078369, loss=1.41963791847229
I0307 12:01:29.598222 139958446696192 logging_writer.py:48] [74700] global_step=74700, grad_norm=2.276592493057251, loss=1.4366753101348877
I0307 12:02:22.323859 139958455088896 logging_writer.py:48] [74800] global_step=74800, grad_norm=2.1261327266693115, loss=1.595861554145813
I0307 12:02:37.062111 140114851837120 spec.py:321] Evaluating on the training split.
I0307 12:02:50.275315 140114851837120 spec.py:333] Evaluating on the validation split.
I0307 12:03:09.220432 140114851837120 spec.py:349] Evaluating on the test split.
I0307 12:03:10.991234 140114851837120 submission_runner.py:469] Time since start: 31982.33s, 	Step: 74828, 	{'train/accuracy': 0.7311463356018066, 'train/loss': 1.0393157005310059, 'validation/accuracy': 0.6581000089645386, 'validation/loss': 1.3984721899032593, 'validation/num_examples': 50000, 'test/accuracy': 0.5253000259399414, 'test/loss': 2.1217665672302246, 'test/num_examples': 10000, 'score': 29633.388894557953, 'total_duration': 31982.32825422287, 'accumulated_submission_time': 29633.388894557953, 'accumulated_eval_time': 2334.558949947357, 'accumulated_logging_time': 6.0988829135894775}
I0307 12:03:11.109583 139958446696192 logging_writer.py:48] [74828] accumulated_eval_time=2334.56, accumulated_logging_time=6.09888, accumulated_submission_time=29633.4, global_step=74828, preemption_count=0, score=29633.4, test/accuracy=0.5253, test/loss=2.12177, test/num_examples=10000, total_duration=31982.3, train/accuracy=0.731146, train/loss=1.03932, validation/accuracy=0.6581, validation/loss=1.39847, validation/num_examples=50000
I0307 12:04:04.052270 139958455088896 logging_writer.py:48] [74900] global_step=74900, grad_norm=2.1179895401000977, loss=1.536064863204956
I0307 12:04:51.119894 139958446696192 logging_writer.py:48] [75000] global_step=75000, grad_norm=2.1404104232788086, loss=1.5298010110855103
I0307 12:05:36.118986 139958455088896 logging_writer.py:48] [75100] global_step=75100, grad_norm=1.885380506515503, loss=1.4317846298217773
I0307 12:06:22.904335 139958446696192 logging_writer.py:48] [75200] global_step=75200, grad_norm=2.006807565689087, loss=1.4244493246078491
I0307 12:07:05.446231 139958455088896 logging_writer.py:48] [75300] global_step=75300, grad_norm=1.8894493579864502, loss=1.4554469585418701
I0307 12:07:44.852040 139958446696192 logging_writer.py:48] [75400] global_step=75400, grad_norm=2.015057325363159, loss=1.4948055744171143
I0307 12:08:32.106993 139958455088896 logging_writer.py:48] [75500] global_step=75500, grad_norm=2.0077829360961914, loss=1.6853276491165161
I0307 12:09:19.325940 139958446696192 logging_writer.py:48] [75600] global_step=75600, grad_norm=2.237431526184082, loss=1.5623631477355957
I0307 12:10:08.905840 139958455088896 logging_writer.py:48] [75700] global_step=75700, grad_norm=2.032895803451538, loss=1.4816768169403076
I0307 12:11:01.315977 139958446696192 logging_writer.py:48] [75800] global_step=75800, grad_norm=2.395914316177368, loss=1.6046844720840454
I0307 12:11:41.477657 140114851837120 spec.py:321] Evaluating on the training split.
I0307 12:11:55.402802 140114851837120 spec.py:333] Evaluating on the validation split.
I0307 12:12:15.285480 140114851837120 spec.py:349] Evaluating on the test split.
I0307 12:12:17.122685 140114851837120 submission_runner.py:469] Time since start: 32528.46s, 	Step: 75872, 	{'train/accuracy': 0.7360092401504517, 'train/loss': 1.005659580230713, 'validation/accuracy': 0.6552000045776367, 'validation/loss': 1.4164284467697144, 'validation/num_examples': 50000, 'test/accuracy': 0.5297000408172607, 'test/loss': 2.1351070404052734, 'test/num_examples': 10000, 'score': 30143.615664958954, 'total_duration': 32528.45977473259, 'accumulated_submission_time': 30143.615664958954, 'accumulated_eval_time': 2370.203828573227, 'accumulated_logging_time': 6.238525629043579}
I0307 12:12:17.250679 139958455088896 logging_writer.py:48] [75872] accumulated_eval_time=2370.2, accumulated_logging_time=6.23853, accumulated_submission_time=30143.6, global_step=75872, preemption_count=0, score=30143.6, test/accuracy=0.5297, test/loss=2.13511, test/num_examples=10000, total_duration=32528.5, train/accuracy=0.736009, train/loss=1.00566, validation/accuracy=0.6552, validation/loss=1.41643, validation/num_examples=50000
I0307 12:12:28.800249 139958446696192 logging_writer.py:48] [75900] global_step=75900, grad_norm=1.9739971160888672, loss=1.51665198802948
I0307 12:13:08.178412 139958455088896 logging_writer.py:48] [76000] global_step=76000, grad_norm=1.9676176309585571, loss=1.5136501789093018
I0307 12:13:58.478277 139958446696192 logging_writer.py:48] [76100] global_step=76100, grad_norm=2.0355024337768555, loss=1.470780849456787
I0307 12:14:39.897977 139958455088896 logging_writer.py:48] [76200] global_step=76200, grad_norm=2.1152093410491943, loss=1.636535406112671
I0307 12:15:19.580084 139958446696192 logging_writer.py:48] [76300] global_step=76300, grad_norm=2.052947759628296, loss=1.5637662410736084
I0307 12:15:58.117821 139958455088896 logging_writer.py:48] [76400] global_step=76400, grad_norm=2.0329205989837646, loss=1.5466408729553223
I0307 12:16:34.740663 139958446696192 logging_writer.py:48] [76500] global_step=76500, grad_norm=1.9707388877868652, loss=1.417573094367981
I0307 12:17:13.117767 139958455088896 logging_writer.py:48] [76600] global_step=76600, grad_norm=2.315026044845581, loss=1.43258535861969
I0307 12:17:51.444865 139958446696192 logging_writer.py:48] [76700] global_step=76700, grad_norm=1.883867621421814, loss=1.4454002380371094
I0307 12:18:30.347158 139958455088896 logging_writer.py:48] [76800] global_step=76800, grad_norm=2.1106209754943848, loss=1.5563372373580933
I0307 12:19:14.204595 139958446696192 logging_writer.py:48] [76900] global_step=76900, grad_norm=2.5439696311950684, loss=1.4518249034881592
I0307 12:20:03.754659 139958455088896 logging_writer.py:48] [77000] global_step=77000, grad_norm=2.1079492568969727, loss=1.464224934577942
I0307 12:20:45.604602 139958446696192 logging_writer.py:48] [77100] global_step=77100, grad_norm=2.0078494548797607, loss=1.416473627090454
I0307 12:20:47.208449 140114851837120 spec.py:321] Evaluating on the training split.
I0307 12:21:00.650173 140114851837120 spec.py:333] Evaluating on the validation split.
I0307 12:21:22.327370 140114851837120 spec.py:349] Evaluating on the test split.
I0307 12:21:24.161899 140114851837120 submission_runner.py:469] Time since start: 33075.50s, 	Step: 77104, 	{'train/accuracy': 0.7465720772743225, 'train/loss': 0.9621585011482239, 'validation/accuracy': 0.6601200103759766, 'validation/loss': 1.3823796510696411, 'validation/num_examples': 50000, 'test/accuracy': 0.5356000065803528, 'test/loss': 2.0757267475128174, 'test/num_examples': 10000, 'score': 30653.403213977814, 'total_duration': 33075.49897146225, 'accumulated_submission_time': 30653.403213977814, 'accumulated_eval_time': 2407.1570892333984, 'accumulated_logging_time': 6.396036624908447}
I0307 12:21:24.298681 139958455088896 logging_writer.py:48] [77104] accumulated_eval_time=2407.16, accumulated_logging_time=6.39604, accumulated_submission_time=30653.4, global_step=77104, preemption_count=0, score=30653.4, test/accuracy=0.5356, test/loss=2.07573, test/num_examples=10000, total_duration=33075.5, train/accuracy=0.746572, train/loss=0.962159, validation/accuracy=0.66012, validation/loss=1.38238, validation/num_examples=50000
I0307 12:22:01.946469 139958446696192 logging_writer.py:48] [77200] global_step=77200, grad_norm=2.2752268314361572, loss=1.5742474794387817
I0307 12:22:41.440687 139958455088896 logging_writer.py:48] [77300] global_step=77300, grad_norm=1.937117099761963, loss=1.5262415409088135
I0307 12:23:27.448420 139958446696192 logging_writer.py:48] [77400] global_step=77400, grad_norm=2.0588815212249756, loss=1.5135128498077393
I0307 12:24:17.375982 139958455088896 logging_writer.py:48] [77500] global_step=77500, grad_norm=2.11246657371521, loss=1.534053921699524
I0307 12:25:51.013035 139958446696192 logging_writer.py:48] [77600] global_step=77600, grad_norm=2.1911118030548096, loss=1.4705551862716675
I0307 12:26:32.176292 139958455088896 logging_writer.py:48] [77700] global_step=77700, grad_norm=2.3889598846435547, loss=1.5119845867156982
I0307 12:27:15.838007 139958446696192 logging_writer.py:48] [77800] global_step=77800, grad_norm=1.9486041069030762, loss=1.37355637550354
I0307 12:28:02.976846 139958455088896 logging_writer.py:48] [77900] global_step=77900, grad_norm=2.031062364578247, loss=1.4830728769302368
I0307 12:29:03.617178 139958446696192 logging_writer.py:48] [78000] global_step=78000, grad_norm=1.9562387466430664, loss=1.455867886543274
I0307 12:29:54.779643 140114851837120 spec.py:321] Evaluating on the training split.
I0307 12:30:08.266942 140114851837120 spec.py:333] Evaluating on the validation split.
I0307 12:30:28.905484 140114851837120 spec.py:349] Evaluating on the test split.
I0307 12:30:30.732839 140114851837120 submission_runner.py:469] Time since start: 33622.07s, 	Step: 78066, 	{'train/accuracy': 0.7436224222183228, 'train/loss': 0.981124758720398, 'validation/accuracy': 0.66211998462677, 'validation/loss': 1.3890818357467651, 'validation/num_examples': 50000, 'test/accuracy': 0.536300003528595, 'test/loss': 2.1141469478607178, 'test/num_examples': 10000, 'score': 31163.74857854843, 'total_duration': 33622.069925546646, 'accumulated_submission_time': 31163.74857854843, 'accumulated_eval_time': 2443.110126018524, 'accumulated_logging_time': 6.558728218078613}
I0307 12:30:30.808141 139958455088896 logging_writer.py:48] [78066] accumulated_eval_time=2443.11, accumulated_logging_time=6.55873, accumulated_submission_time=31163.7, global_step=78066, preemption_count=0, score=31163.7, test/accuracy=0.5363, test/loss=2.11415, test/num_examples=10000, total_duration=33622.1, train/accuracy=0.743622, train/loss=0.981125, validation/accuracy=0.66212, validation/loss=1.38908, validation/num_examples=50000
I0307 12:30:44.930636 139958446696192 logging_writer.py:48] [78100] global_step=78100, grad_norm=2.037684679031372, loss=1.4294447898864746
I0307 12:31:34.453320 139958455088896 logging_writer.py:48] [78200] global_step=78200, grad_norm=2.062720537185669, loss=1.5603007078170776
I0307 12:32:22.076153 139958446696192 logging_writer.py:48] [78300] global_step=78300, grad_norm=2.1338210105895996, loss=1.5159552097320557
I0307 12:33:07.334591 139958455088896 logging_writer.py:48] [78400] global_step=78400, grad_norm=2.018585205078125, loss=1.4042061567306519
I0307 12:33:57.090205 139958446696192 logging_writer.py:48] [78500] global_step=78500, grad_norm=2.094852924346924, loss=1.4245944023132324
I0307 12:34:56.915123 139958455088896 logging_writer.py:48] [78600] global_step=78600, grad_norm=1.9272561073303223, loss=1.5202999114990234
I0307 12:35:46.229574 139958446696192 logging_writer.py:48] [78700] global_step=78700, grad_norm=2.2567243576049805, loss=1.4283185005187988
I0307 12:36:45.412989 139958455088896 logging_writer.py:48] [78800] global_step=78800, grad_norm=2.147162914276123, loss=1.4945474863052368
I0307 12:37:26.794629 139958446696192 logging_writer.py:48] [78900] global_step=78900, grad_norm=2.523620843887329, loss=1.380476951599121
I0307 12:38:07.185147 139958455088896 logging_writer.py:48] [79000] global_step=79000, grad_norm=2.0825188159942627, loss=1.4088151454925537
I0307 12:38:48.327755 139958446696192 logging_writer.py:48] [79100] global_step=79100, grad_norm=2.2009875774383545, loss=1.4776190519332886
I0307 12:39:01.095827 140114851837120 spec.py:321] Evaluating on the training split.
I0307 12:39:15.343714 140114851837120 spec.py:333] Evaluating on the validation split.
I0307 12:39:38.629489 140114851837120 spec.py:349] Evaluating on the test split.
I0307 12:39:40.462197 140114851837120 submission_runner.py:469] Time since start: 34171.80s, 	Step: 79129, 	{'train/accuracy': 0.7447185516357422, 'train/loss': 0.9756001830101013, 'validation/accuracy': 0.6759399771690369, 'validation/loss': 1.3317054510116577, 'validation/num_examples': 50000, 'test/accuracy': 0.5445000529289246, 'test/loss': 2.052213191986084, 'test/num_examples': 10000, 'score': 31673.886323451996, 'total_duration': 34171.79928994179, 'accumulated_submission_time': 31673.886323451996, 'accumulated_eval_time': 2482.4763338565826, 'accumulated_logging_time': 6.662399768829346}
I0307 12:39:40.544786 139958455088896 logging_writer.py:48] [79129] accumulated_eval_time=2482.48, accumulated_logging_time=6.6624, accumulated_submission_time=31673.9, global_step=79129, preemption_count=0, score=31673.9, test/accuracy=0.5445, test/loss=2.05221, test/num_examples=10000, total_duration=34171.8, train/accuracy=0.744719, train/loss=0.9756, validation/accuracy=0.67594, validation/loss=1.33171, validation/num_examples=50000
I0307 12:40:09.915343 139958446696192 logging_writer.py:48] [79200] global_step=79200, grad_norm=2.064300775527954, loss=1.4673614501953125
I0307 12:41:02.375040 139958455088896 logging_writer.py:48] [79300] global_step=79300, grad_norm=2.32261323928833, loss=1.5680508613586426
I0307 12:41:55.696604 139958446696192 logging_writer.py:48] [79400] global_step=79400, grad_norm=2.15578031539917, loss=1.447601556777954
I0307 12:42:41.121293 139958455088896 logging_writer.py:48] [79500] global_step=79500, grad_norm=2.4014694690704346, loss=1.3922199010849
I0307 12:43:22.385776 139958446696192 logging_writer.py:48] [79600] global_step=79600, grad_norm=2.0456509590148926, loss=1.3966318368911743
I0307 12:44:06.070303 139958455088896 logging_writer.py:48] [79700] global_step=79700, grad_norm=2.1673293113708496, loss=1.424833059310913
I0307 12:44:53.339632 139958446696192 logging_writer.py:48] [79800] global_step=79800, grad_norm=2.153372049331665, loss=1.5355545282363892
I0307 12:45:44.312464 139958455088896 logging_writer.py:48] [79900] global_step=79900, grad_norm=2.1840596199035645, loss=1.580856204032898
I0307 12:46:24.000236 139958446696192 logging_writer.py:48] [80000] global_step=80000, grad_norm=2.302114963531494, loss=1.5776784420013428
I0307 12:47:04.203274 139958455088896 logging_writer.py:48] [80100] global_step=80100, grad_norm=2.0763142108917236, loss=1.5285264253616333
I0307 12:47:44.238326 139958446696192 logging_writer.py:48] [80200] global_step=80200, grad_norm=2.097015619277954, loss=1.4174139499664307
I0307 12:48:10.756367 140114851837120 spec.py:321] Evaluating on the training split.
I0307 12:48:23.471724 140114851837120 spec.py:333] Evaluating on the validation split.
I0307 12:48:41.812461 140114851837120 spec.py:349] Evaluating on the test split.
I0307 12:48:43.648576 140114851837120 submission_runner.py:469] Time since start: 34714.99s, 	Step: 80269, 	{'train/accuracy': 0.7380221486091614, 'train/loss': 1.000010371208191, 'validation/accuracy': 0.6665799617767334, 'validation/loss': 1.362206220626831, 'validation/num_examples': 50000, 'test/accuracy': 0.5385000109672546, 'test/loss': 2.1026439666748047, 'test/num_examples': 10000, 'score': 32183.937160491943, 'total_duration': 34714.98567056656, 'accumulated_submission_time': 32183.937160491943, 'accumulated_eval_time': 2515.368390083313, 'accumulated_logging_time': 6.772404193878174}
I0307 12:48:43.743149 139958455088896 logging_writer.py:48] [80269] accumulated_eval_time=2515.37, accumulated_logging_time=6.7724, accumulated_submission_time=32183.9, global_step=80269, preemption_count=0, score=32183.9, test/accuracy=0.5385, test/loss=2.10264, test/num_examples=10000, total_duration=34715, train/accuracy=0.738022, train/loss=1.00001, validation/accuracy=0.66658, validation/loss=1.36221, validation/num_examples=50000
I0307 12:48:56.240663 139958446696192 logging_writer.py:48] [80300] global_step=80300, grad_norm=2.106672525405884, loss=1.4665871858596802
I0307 12:49:35.195049 139958455088896 logging_writer.py:48] [80400] global_step=80400, grad_norm=2.0475223064422607, loss=1.42156183719635
I0307 12:50:13.956549 139958446696192 logging_writer.py:48] [80500] global_step=80500, grad_norm=1.9808636903762817, loss=1.4880419969558716
I0307 12:50:53.385997 139958455088896 logging_writer.py:48] [80600] global_step=80600, grad_norm=2.2292122840881348, loss=1.4786102771759033
I0307 12:51:31.908787 139958446696192 logging_writer.py:48] [80700] global_step=80700, grad_norm=2.333547830581665, loss=1.453328013420105
I0307 12:52:10.783883 139958455088896 logging_writer.py:48] [80800] global_step=80800, grad_norm=2.056208372116089, loss=1.360684871673584
I0307 12:52:49.557485 139958446696192 logging_writer.py:48] [80900] global_step=80900, grad_norm=1.997512698173523, loss=1.4292926788330078
I0307 12:53:28.486534 139958455088896 logging_writer.py:48] [81000] global_step=81000, grad_norm=1.9304637908935547, loss=1.4002676010131836
I0307 12:54:07.531904 139958446696192 logging_writer.py:48] [81100] global_step=81100, grad_norm=2.2267417907714844, loss=1.4159506559371948
I0307 12:54:47.334476 139958455088896 logging_writer.py:48] [81200] global_step=81200, grad_norm=2.280965566635132, loss=1.5300137996673584
I0307 12:55:26.400412 139958446696192 logging_writer.py:48] [81300] global_step=81300, grad_norm=2.0557472705841064, loss=1.4682153463363647
I0307 12:56:06.260131 139958455088896 logging_writer.py:48] [81400] global_step=81400, grad_norm=2.125502347946167, loss=1.5009819269180298
I0307 12:56:45.957663 139958446696192 logging_writer.py:48] [81500] global_step=81500, grad_norm=2.124082088470459, loss=1.4442254304885864
I0307 12:57:13.965629 140114851837120 spec.py:321] Evaluating on the training split.
I0307 12:57:27.153550 140114851837120 spec.py:333] Evaluating on the validation split.
I0307 12:57:43.649100 140114851837120 spec.py:349] Evaluating on the test split.
I0307 12:57:45.490363 140114851837120 submission_runner.py:469] Time since start: 35256.83s, 	Step: 81573, 	{'train/accuracy': 0.7387993931770325, 'train/loss': 0.9945427775382996, 'validation/accuracy': 0.6657199859619141, 'validation/loss': 1.3603932857513428, 'validation/num_examples': 50000, 'test/accuracy': 0.541100025177002, 'test/loss': 2.0774476528167725, 'test/num_examples': 10000, 'score': 32693.982009410858, 'total_duration': 35256.82742023468, 'accumulated_submission_time': 32693.982009410858, 'accumulated_eval_time': 2546.8929257392883, 'accumulated_logging_time': 6.901867389678955}
I0307 12:57:45.618340 139958455088896 logging_writer.py:48] [81573] accumulated_eval_time=2546.89, accumulated_logging_time=6.90187, accumulated_submission_time=32694, global_step=81573, preemption_count=0, score=32694, test/accuracy=0.5411, test/loss=2.07745, test/num_examples=10000, total_duration=35256.8, train/accuracy=0.738799, train/loss=0.994543, validation/accuracy=0.66572, validation/loss=1.36039, validation/num_examples=50000
I0307 12:57:56.413907 139958446696192 logging_writer.py:48] [81600] global_step=81600, grad_norm=2.4305176734924316, loss=1.5632625818252563
I0307 12:58:35.322956 139958455088896 logging_writer.py:48] [81700] global_step=81700, grad_norm=2.156552791595459, loss=1.4740660190582275
I0307 12:59:14.768809 139958446696192 logging_writer.py:48] [81800] global_step=81800, grad_norm=2.1892805099487305, loss=1.4681371450424194
I0307 12:59:54.273282 139958455088896 logging_writer.py:48] [81900] global_step=81900, grad_norm=2.1764707565307617, loss=1.4738022089004517
I0307 13:00:33.689234 139958446696192 logging_writer.py:48] [82000] global_step=82000, grad_norm=2.125192403793335, loss=1.5079374313354492
I0307 13:01:13.194459 139958455088896 logging_writer.py:48] [82100] global_step=82100, grad_norm=2.1982295513153076, loss=1.40470552444458
I0307 13:01:52.661095 139958446696192 logging_writer.py:48] [82200] global_step=82200, grad_norm=2.1424062252044678, loss=1.5070102214813232
I0307 13:02:31.236982 139958455088896 logging_writer.py:48] [82300] global_step=82300, grad_norm=2.2963204383850098, loss=1.4795355796813965
I0307 13:03:11.082570 139958446696192 logging_writer.py:48] [82400] global_step=82400, grad_norm=2.3592467308044434, loss=1.5962531566619873
I0307 13:03:50.848001 139958455088896 logging_writer.py:48] [82500] global_step=82500, grad_norm=2.3077666759490967, loss=1.484678864479065
I0307 13:04:30.422638 139958446696192 logging_writer.py:48] [82600] global_step=82600, grad_norm=2.220752477645874, loss=1.5252383947372437
I0307 13:05:10.518074 139958455088896 logging_writer.py:48] [82700] global_step=82700, grad_norm=2.2530357837677, loss=1.5537011623382568
I0307 13:05:49.734906 139958446696192 logging_writer.py:48] [82800] global_step=82800, grad_norm=2.002741575241089, loss=1.466184377670288
I0307 13:06:15.521482 140114851837120 spec.py:321] Evaluating on the training split.
I0307 13:06:28.140448 140114851837120 spec.py:333] Evaluating on the validation split.
I0307 13:06:41.769214 140114851837120 spec.py:349] Evaluating on the test split.
I0307 13:06:43.602984 140114851837120 submission_runner.py:469] Time since start: 35794.94s, 	Step: 82866, 	{'train/accuracy': 0.7383410334587097, 'train/loss': 0.9955034852027893, 'validation/accuracy': 0.6617799997329712, 'validation/loss': 1.3765740394592285, 'validation/num_examples': 50000, 'test/accuracy': 0.5332000255584717, 'test/loss': 2.1239564418792725, 'test/num_examples': 10000, 'score': 33203.71964597702, 'total_duration': 35794.940029382706, 'accumulated_submission_time': 33203.71964597702, 'accumulated_eval_time': 2574.9742288589478, 'accumulated_logging_time': 7.051493167877197}
I0307 13:06:43.678038 139958455088896 logging_writer.py:48] [82866] accumulated_eval_time=2574.97, accumulated_logging_time=7.05149, accumulated_submission_time=33203.7, global_step=82866, preemption_count=0, score=33203.7, test/accuracy=0.5332, test/loss=2.12396, test/num_examples=10000, total_duration=35794.9, train/accuracy=0.738341, train/loss=0.995503, validation/accuracy=0.66178, validation/loss=1.37657, validation/num_examples=50000
I0307 13:06:57.519540 139958446696192 logging_writer.py:48] [82900] global_step=82900, grad_norm=2.0141847133636475, loss=1.4384721517562866
I0307 13:07:37.552072 139958455088896 logging_writer.py:48] [83000] global_step=83000, grad_norm=2.2748069763183594, loss=1.5169240236282349
I0307 13:08:17.597425 139958446696192 logging_writer.py:48] [83100] global_step=83100, grad_norm=2.33217716217041, loss=1.4897143840789795
I0307 13:08:57.315530 139958455088896 logging_writer.py:48] [83200] global_step=83200, grad_norm=2.1157965660095215, loss=1.4270001649856567
I0307 13:09:38.158298 139958446696192 logging_writer.py:48] [83300] global_step=83300, grad_norm=2.2066738605499268, loss=1.5459576845169067
I0307 13:10:18.043584 139958455088896 logging_writer.py:48] [83400] global_step=83400, grad_norm=2.216287851333618, loss=1.403063416481018
I0307 13:10:57.455681 139958446696192 logging_writer.py:48] [83500] global_step=83500, grad_norm=2.58429217338562, loss=1.485031008720398
I0307 13:11:37.292097 139958455088896 logging_writer.py:48] [83600] global_step=83600, grad_norm=2.0106289386749268, loss=1.3782461881637573
I0307 13:12:16.397513 139958446696192 logging_writer.py:48] [83700] global_step=83700, grad_norm=2.6079227924346924, loss=1.6409491300582886
I0307 13:12:55.612991 139958455088896 logging_writer.py:48] [83800] global_step=83800, grad_norm=2.11468243598938, loss=1.5012677907943726
I0307 13:13:35.624541 139958446696192 logging_writer.py:48] [83900] global_step=83900, grad_norm=2.112569808959961, loss=1.5384912490844727
I0307 13:14:14.629993 139958455088896 logging_writer.py:48] [84000] global_step=84000, grad_norm=2.2030322551727295, loss=1.4476978778839111
I0307 13:14:53.721666 139958446696192 logging_writer.py:48] [84100] global_step=84100, grad_norm=2.1585512161254883, loss=1.4603559970855713
I0307 13:15:13.663486 140114851837120 spec.py:321] Evaluating on the training split.
I0307 13:15:26.606939 140114851837120 spec.py:333] Evaluating on the validation split.
I0307 13:15:42.257745 140114851837120 spec.py:349] Evaluating on the test split.
I0307 13:15:44.096207 140114851837120 submission_runner.py:469] Time since start: 36335.43s, 	Step: 84152, 	{'train/accuracy': 0.7338767647743225, 'train/loss': 1.0242502689361572, 'validation/accuracy': 0.6631399989128113, 'validation/loss': 1.3823490142822266, 'validation/num_examples': 50000, 'test/accuracy': 0.5340999960899353, 'test/loss': 2.1232519149780273, 'test/num_examples': 10000, 'score': 33713.52746009827, 'total_duration': 36335.43326711655, 'accumulated_submission_time': 33713.52746009827, 'accumulated_eval_time': 2605.406754732132, 'accumulated_logging_time': 7.158366441726685}
I0307 13:15:44.188603 139958455088896 logging_writer.py:48] [84152] accumulated_eval_time=2605.41, accumulated_logging_time=7.15837, accumulated_submission_time=33713.5, global_step=84152, preemption_count=0, score=33713.5, test/accuracy=0.5341, test/loss=2.12325, test/num_examples=10000, total_duration=36335.4, train/accuracy=0.733877, train/loss=1.02425, validation/accuracy=0.66314, validation/loss=1.38235, validation/num_examples=50000
I0307 13:16:03.706964 139958446696192 logging_writer.py:48] [84200] global_step=84200, grad_norm=2.213961362838745, loss=1.407505750656128
I0307 13:16:43.872943 139958455088896 logging_writer.py:48] [84300] global_step=84300, grad_norm=2.026226043701172, loss=1.4009491205215454
I0307 13:17:23.703604 139958446696192 logging_writer.py:48] [84400] global_step=84400, grad_norm=2.3172059059143066, loss=1.4474927186965942
I0307 13:18:04.066039 139958455088896 logging_writer.py:48] [84500] global_step=84500, grad_norm=2.359602451324463, loss=1.4130589962005615
I0307 13:18:43.539292 139958446696192 logging_writer.py:48] [84600] global_step=84600, grad_norm=2.290390729904175, loss=1.476417064666748
I0307 13:19:22.620539 139958455088896 logging_writer.py:48] [84700] global_step=84700, grad_norm=2.0405938625335693, loss=1.479096531867981
I0307 13:20:02.404330 139958446696192 logging_writer.py:48] [84800] global_step=84800, grad_norm=2.26029634475708, loss=1.5290985107421875
I0307 13:20:41.842684 139958455088896 logging_writer.py:48] [84900] global_step=84900, grad_norm=1.989357590675354, loss=1.4300510883331299
I0307 13:21:21.958968 139958446696192 logging_writer.py:48] [85000] global_step=85000, grad_norm=2.2899489402770996, loss=1.395653486251831
I0307 13:22:01.977025 139958455088896 logging_writer.py:48] [85100] global_step=85100, grad_norm=2.2669310569763184, loss=1.5240559577941895
I0307 13:22:41.630693 139958446696192 logging_writer.py:48] [85200] global_step=85200, grad_norm=2.568066120147705, loss=1.5388433933258057
I0307 13:23:21.897296 139958455088896 logging_writer.py:48] [85300] global_step=85300, grad_norm=2.0879502296447754, loss=1.4400653839111328
I0307 13:24:01.980068 139958446696192 logging_writer.py:48] [85400] global_step=85400, grad_norm=2.095695972442627, loss=1.4510071277618408
I0307 13:24:14.099159 140114851837120 spec.py:321] Evaluating on the training split.
I0307 13:24:26.484262 140114851837120 spec.py:333] Evaluating on the validation split.
I0307 13:24:45.049287 140114851837120 spec.py:349] Evaluating on the test split.
I0307 13:24:46.890056 140114851837120 submission_runner.py:469] Time since start: 36878.23s, 	Step: 85431, 	{'train/accuracy': 0.7504982352256775, 'train/loss': 0.9383268356323242, 'validation/accuracy': 0.674560010433197, 'validation/loss': 1.3241186141967773, 'validation/num_examples': 50000, 'test/accuracy': 0.5450000166893005, 'test/loss': 2.051173210144043, 'test/num_examples': 10000, 'score': 34223.25968837738, 'total_duration': 36878.22714519501, 'accumulated_submission_time': 34223.25968837738, 'accumulated_eval_time': 2638.1974868774414, 'accumulated_logging_time': 7.280217170715332}
I0307 13:24:47.016278 139958455088896 logging_writer.py:48] [85431] accumulated_eval_time=2638.2, accumulated_logging_time=7.28022, accumulated_submission_time=34223.3, global_step=85431, preemption_count=0, score=34223.3, test/accuracy=0.545, test/loss=2.05117, test/num_examples=10000, total_duration=36878.2, train/accuracy=0.750498, train/loss=0.938327, validation/accuracy=0.67456, validation/loss=1.32412, validation/num_examples=50000
I0307 13:25:15.299486 139958446696192 logging_writer.py:48] [85500] global_step=85500, grad_norm=2.428260087966919, loss=1.6011730432510376
I0307 13:25:55.954774 139958455088896 logging_writer.py:48] [85600] global_step=85600, grad_norm=2.1807379722595215, loss=1.3402385711669922
I0307 13:26:36.260100 139958446696192 logging_writer.py:48] [85700] global_step=85700, grad_norm=2.2530150413513184, loss=1.418821096420288
I0307 13:27:16.488780 139958455088896 logging_writer.py:48] [85800] global_step=85800, grad_norm=2.302011489868164, loss=1.5453189611434937
I0307 13:27:56.484721 139958446696192 logging_writer.py:48] [85900] global_step=85900, grad_norm=2.436920642852783, loss=1.4240268468856812
I0307 13:28:36.174324 139958455088896 logging_writer.py:48] [86000] global_step=86000, grad_norm=2.321467399597168, loss=1.5118964910507202
I0307 13:29:16.368153 139958446696192 logging_writer.py:48] [86100] global_step=86100, grad_norm=2.2716739177703857, loss=1.4452625513076782
I0307 13:29:56.619741 139958455088896 logging_writer.py:48] [86200] global_step=86200, grad_norm=2.262317657470703, loss=1.3201332092285156
I0307 13:30:36.654030 139958446696192 logging_writer.py:48] [86300] global_step=86300, grad_norm=2.1932594776153564, loss=1.4641013145446777
I0307 13:31:16.484779 139958455088896 logging_writer.py:48] [86400] global_step=86400, grad_norm=2.138530731201172, loss=1.4572105407714844
I0307 13:31:56.326509 139958446696192 logging_writer.py:48] [86500] global_step=86500, grad_norm=2.22817063331604, loss=1.443005919456482
I0307 13:32:35.618122 139958455088896 logging_writer.py:48] [86600] global_step=86600, grad_norm=2.3432374000549316, loss=1.5435820817947388
I0307 13:33:15.536161 139958446696192 logging_writer.py:48] [86700] global_step=86700, grad_norm=2.109125852584839, loss=1.4380000829696655
I0307 13:33:17.059874 140114851837120 spec.py:321] Evaluating on the training split.
I0307 13:33:29.953877 140114851837120 spec.py:333] Evaluating on the validation split.
I0307 13:33:45.249881 140114851837120 spec.py:349] Evaluating on the test split.
I0307 13:33:47.081019 140114851837120 submission_runner.py:469] Time since start: 37418.42s, 	Step: 86705, 	{'train/accuracy': 0.7451769709587097, 'train/loss': 0.9667693972587585, 'validation/accuracy': 0.6660400032997131, 'validation/loss': 1.360815405845642, 'validation/num_examples': 50000, 'test/accuracy': 0.5379000306129456, 'test/loss': 2.1146790981292725, 'test/num_examples': 10000, 'score': 34733.12170672417, 'total_duration': 37418.418092012405, 'accumulated_submission_time': 34733.12170672417, 'accumulated_eval_time': 2668.2184591293335, 'accumulated_logging_time': 7.439556837081909}
I0307 13:33:47.193737 139958455088896 logging_writer.py:48] [86705] accumulated_eval_time=2668.22, accumulated_logging_time=7.43956, accumulated_submission_time=34733.1, global_step=86705, preemption_count=0, score=34733.1, test/accuracy=0.5379, test/loss=2.11468, test/num_examples=10000, total_duration=37418.4, train/accuracy=0.745177, train/loss=0.966769, validation/accuracy=0.66604, validation/loss=1.36082, validation/num_examples=50000
I0307 13:34:25.765328 139958446696192 logging_writer.py:48] [86800] global_step=86800, grad_norm=2.2328341007232666, loss=1.4942611455917358
I0307 13:35:05.449694 139958455088896 logging_writer.py:48] [86900] global_step=86900, grad_norm=2.247365713119507, loss=1.3849244117736816
I0307 13:35:45.611525 139958446696192 logging_writer.py:48] [87000] global_step=87000, grad_norm=2.496910572052002, loss=1.4460978507995605
I0307 13:36:25.514355 139958455088896 logging_writer.py:48] [87100] global_step=87100, grad_norm=2.1180644035339355, loss=1.372434377670288
I0307 13:37:05.535493 139958446696192 logging_writer.py:48] [87200] global_step=87200, grad_norm=2.1993188858032227, loss=1.3737396001815796
I0307 13:37:45.797846 139958455088896 logging_writer.py:48] [87300] global_step=87300, grad_norm=2.2960610389709473, loss=1.5553721189498901
I0307 13:38:26.302808 139958446696192 logging_writer.py:48] [87400] global_step=87400, grad_norm=2.0624215602874756, loss=1.4870312213897705
I0307 13:39:06.459975 139958455088896 logging_writer.py:48] [87500] global_step=87500, grad_norm=2.4266488552093506, loss=1.4311511516571045
I0307 13:39:46.995716 139958446696192 logging_writer.py:48] [87600] global_step=87600, grad_norm=2.411829710006714, loss=1.4837191104888916
I0307 13:40:27.856713 139958455088896 logging_writer.py:48] [87700] global_step=87700, grad_norm=2.1852900981903076, loss=1.3796491622924805
I0307 13:41:07.936815 139958446696192 logging_writer.py:48] [87800] global_step=87800, grad_norm=2.1944808959960938, loss=1.5379406213760376
I0307 13:41:48.197507 139958455088896 logging_writer.py:48] [87900] global_step=87900, grad_norm=2.1473584175109863, loss=1.5524241924285889
I0307 13:42:17.218766 140114851837120 spec.py:321] Evaluating on the training split.
I0307 13:42:30.081855 140114851837120 spec.py:333] Evaluating on the validation split.
I0307 13:42:43.448091 140114851837120 spec.py:349] Evaluating on the test split.
I0307 13:42:45.280064 140114851837120 submission_runner.py:469] Time since start: 37956.62s, 	Step: 87973, 	{'train/accuracy': 0.7443398833274841, 'train/loss': 0.9653928279876709, 'validation/accuracy': 0.669979989528656, 'validation/loss': 1.360177755355835, 'validation/num_examples': 50000, 'test/accuracy': 0.5406000018119812, 'test/loss': 2.0746076107025146, 'test/num_examples': 10000, 'score': 35242.965539455414, 'total_duration': 37956.617122888565, 'accumulated_submission_time': 35242.965539455414, 'accumulated_eval_time': 2696.279561519623, 'accumulated_logging_time': 7.583616495132446}
I0307 13:42:45.380776 139958446696192 logging_writer.py:48] [87973] accumulated_eval_time=2696.28, accumulated_logging_time=7.58362, accumulated_submission_time=35243, global_step=87973, preemption_count=0, score=35243, test/accuracy=0.5406, test/loss=2.07461, test/num_examples=10000, total_duration=37956.6, train/accuracy=0.74434, train/loss=0.965393, validation/accuracy=0.66998, validation/loss=1.36018, validation/num_examples=50000
I0307 13:42:56.448998 139958455088896 logging_writer.py:48] [88000] global_step=88000, grad_norm=2.395772933959961, loss=1.395527720451355
I0307 13:43:35.975681 139958446696192 logging_writer.py:48] [88100] global_step=88100, grad_norm=2.064743995666504, loss=1.4404367208480835
I0307 13:44:16.019625 139958455088896 logging_writer.py:48] [88200] global_step=88200, grad_norm=2.583362340927124, loss=1.416712999343872
I0307 13:44:56.573690 139958446696192 logging_writer.py:48] [88300] global_step=88300, grad_norm=2.25716495513916, loss=1.4505088329315186
I0307 13:45:36.440390 139958455088896 logging_writer.py:48] [88400] global_step=88400, grad_norm=2.1045830249786377, loss=1.2928918600082397
I0307 13:46:16.442219 139958446696192 logging_writer.py:48] [88500] global_step=88500, grad_norm=2.2264344692230225, loss=1.3531296253204346
I0307 13:46:56.615123 139958455088896 logging_writer.py:48] [88600] global_step=88600, grad_norm=2.2251992225646973, loss=1.4310497045516968
I0307 13:47:37.429585 139958446696192 logging_writer.py:48] [88700] global_step=88700, grad_norm=2.4207794666290283, loss=1.4180322885513306
I0307 13:48:17.894690 139958455088896 logging_writer.py:48] [88800] global_step=88800, grad_norm=2.1984171867370605, loss=1.3988593816757202
2025-03-07 13:48:20.298105: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 13:48:58.375200 139958446696192 logging_writer.py:48] [88900] global_step=88900, grad_norm=2.173976182937622, loss=1.4083021879196167
I0307 13:49:38.664614 139958455088896 logging_writer.py:48] [89000] global_step=89000, grad_norm=2.4421982765197754, loss=1.5282776355743408
I0307 13:50:18.927277 139958446696192 logging_writer.py:48] [89100] global_step=89100, grad_norm=2.0661473274230957, loss=1.4998042583465576
I0307 13:50:59.589563 139958455088896 logging_writer.py:48] [89200] global_step=89200, grad_norm=2.0346240997314453, loss=1.3659321069717407
I0307 13:51:15.589837 140114851837120 spec.py:321] Evaluating on the training split.
I0307 13:51:28.430521 140114851837120 spec.py:333] Evaluating on the validation split.
I0307 13:51:41.087730 140114851837120 spec.py:349] Evaluating on the test split.
I0307 13:51:43.017650 140114851837120 submission_runner.py:469] Time since start: 38494.35s, 	Step: 89241, 	{'train/accuracy': 0.741230845451355, 'train/loss': 0.983771562576294, 'validation/accuracy': 0.6613799929618835, 'validation/loss': 1.3798116445541382, 'validation/num_examples': 50000, 'test/accuracy': 0.5296000242233276, 'test/loss': 2.1156599521636963, 'test/num_examples': 10000, 'score': 35752.98774909973, 'total_duration': 38494.3547039032, 'accumulated_submission_time': 35752.98774909973, 'accumulated_eval_time': 2723.707176923752, 'accumulated_logging_time': 7.722415208816528}
I0307 13:51:43.125111 139958446696192 logging_writer.py:48] [89241] accumulated_eval_time=2723.71, accumulated_logging_time=7.72242, accumulated_submission_time=35753, global_step=89241, preemption_count=0, score=35753, test/accuracy=0.5296, test/loss=2.11566, test/num_examples=10000, total_duration=38494.4, train/accuracy=0.741231, train/loss=0.983772, validation/accuracy=0.66138, validation/loss=1.37981, validation/num_examples=50000
I0307 13:52:07.351354 139958455088896 logging_writer.py:48] [89300] global_step=89300, grad_norm=2.2300100326538086, loss=1.409661889076233
I0307 13:52:47.391150 139958446696192 logging_writer.py:48] [89400] global_step=89400, grad_norm=2.5681488513946533, loss=1.4555480480194092
I0307 13:53:28.105335 139958455088896 logging_writer.py:48] [89500] global_step=89500, grad_norm=2.498399496078491, loss=1.4540932178497314
I0307 13:54:08.830155 139958446696192 logging_writer.py:48] [89600] global_step=89600, grad_norm=2.366560935974121, loss=1.4598898887634277
I0307 13:54:48.851575 139958455088896 logging_writer.py:48] [89700] global_step=89700, grad_norm=2.0960114002227783, loss=1.3930782079696655
I0307 13:55:29.146235 139958446696192 logging_writer.py:48] [89800] global_step=89800, grad_norm=2.4010493755340576, loss=1.4162378311157227
I0307 13:56:10.072414 139958455088896 logging_writer.py:48] [89900] global_step=89900, grad_norm=2.149329900741577, loss=1.3672363758087158
I0307 13:56:50.260527 139958446696192 logging_writer.py:48] [90000] global_step=90000, grad_norm=2.2439467906951904, loss=1.3645681142807007
2025-03-07 13:57:12.617079: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 13:57:30.690445 139958455088896 logging_writer.py:48] [90100] global_step=90100, grad_norm=2.2788562774658203, loss=1.3938720226287842
I0307 13:58:10.985658 139958446696192 logging_writer.py:48] [90200] global_step=90200, grad_norm=2.080054759979248, loss=1.399790644645691
I0307 13:58:51.188524 139958455088896 logging_writer.py:48] [90300] global_step=90300, grad_norm=2.20534086227417, loss=1.355867624282837
I0307 13:59:31.530293 139958446696192 logging_writer.py:48] [90400] global_step=90400, grad_norm=2.36789870262146, loss=1.3537527322769165
I0307 14:00:11.340692 139958455088896 logging_writer.py:48] [90500] global_step=90500, grad_norm=2.2954866886138916, loss=1.5084013938903809
I0307 14:00:13.306263 140114851837120 spec.py:321] Evaluating on the training split.
I0307 14:00:26.187967 140114851837120 spec.py:333] Evaluating on the validation split.
I0307 14:00:38.691122 140114851837120 spec.py:349] Evaluating on the test split.
I0307 14:00:40.500181 140114851837120 submission_runner.py:469] Time since start: 39031.84s, 	Step: 90506, 	{'train/accuracy': 0.7584900856018066, 'train/loss': 0.91380774974823, 'validation/accuracy': 0.6789199709892273, 'validation/loss': 1.2958568334579468, 'validation/num_examples': 50000, 'test/accuracy': 0.5558000206947327, 'test/loss': 1.9668797254562378, 'test/num_examples': 10000, 'score': 36262.987285375595, 'total_duration': 39031.837233781815, 'accumulated_submission_time': 36262.987285375595, 'accumulated_eval_time': 2750.9008872509003, 'accumulated_logging_time': 7.857999801635742}
I0307 14:00:40.590499 139958446696192 logging_writer.py:48] [90506] accumulated_eval_time=2750.9, accumulated_logging_time=7.858, accumulated_submission_time=36263, global_step=90506, preemption_count=0, score=36263, test/accuracy=0.5558, test/loss=1.96688, test/num_examples=10000, total_duration=39031.8, train/accuracy=0.75849, train/loss=0.913808, validation/accuracy=0.67892, validation/loss=1.29586, validation/num_examples=50000
I0307 14:01:18.632485 139958455088896 logging_writer.py:48] [90600] global_step=90600, grad_norm=2.221052408218384, loss=1.3881611824035645
I0307 14:01:59.088628 139958446696192 logging_writer.py:48] [90700] global_step=90700, grad_norm=2.0731313228607178, loss=1.4280409812927246
I0307 14:02:39.429048 139958455088896 logging_writer.py:48] [90800] global_step=90800, grad_norm=2.1946208477020264, loss=1.4051916599273682
I0307 14:03:19.388074 139958446696192 logging_writer.py:48] [90900] global_step=90900, grad_norm=2.2114834785461426, loss=1.3632731437683105
I0307 14:03:59.618782 139958455088896 logging_writer.py:48] [91000] global_step=91000, grad_norm=2.236334800720215, loss=1.373241662979126
I0307 14:04:40.380969 139958446696192 logging_writer.py:48] [91100] global_step=91100, grad_norm=2.470705270767212, loss=1.3214848041534424
I0307 14:05:21.140713 139958455088896 logging_writer.py:48] [91200] global_step=91200, grad_norm=2.1583175659179688, loss=1.524841547012329
I0307 14:06:01.477125 139958446696192 logging_writer.py:48] [91300] global_step=91300, grad_norm=2.2979447841644287, loss=1.4606351852416992
2025-03-07 14:06:04.141566: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 14:06:42.132656 139958455088896 logging_writer.py:48] [91400] global_step=91400, grad_norm=2.2115731239318848, loss=1.4604811668395996
I0307 14:07:22.102134 139958446696192 logging_writer.py:48] [91500] global_step=91500, grad_norm=2.3590102195739746, loss=1.4160860776901245
I0307 14:08:02.317787 139958455088896 logging_writer.py:48] [91600] global_step=91600, grad_norm=2.365922689437866, loss=1.4957027435302734
I0307 14:08:42.159356 139958446696192 logging_writer.py:48] [91700] global_step=91700, grad_norm=2.2287800312042236, loss=1.4241430759429932
I0307 14:09:10.679172 140114851837120 spec.py:321] Evaluating on the training split.
I0307 14:09:23.852542 140114851837120 spec.py:333] Evaluating on the validation split.
I0307 14:09:36.427772 140114851837120 spec.py:349] Evaluating on the test split.
I0307 14:09:38.254622 140114851837120 submission_runner.py:469] Time since start: 39569.59s, 	Step: 91772, 	{'train/accuracy': 0.7578722834587097, 'train/loss': 0.9105977416038513, 'validation/accuracy': 0.6764999628067017, 'validation/loss': 1.3148294687271118, 'validation/num_examples': 50000, 'test/accuracy': 0.5533000230789185, 'test/loss': 2.0216121673583984, 'test/num_examples': 10000, 'score': 36772.89505529404, 'total_duration': 39569.59167146683, 'accumulated_submission_time': 36772.89505529404, 'accumulated_eval_time': 2778.476126432419, 'accumulated_logging_time': 7.977968454360962}
I0307 14:09:38.354388 139958455088896 logging_writer.py:48] [91772] accumulated_eval_time=2778.48, accumulated_logging_time=7.97797, accumulated_submission_time=36772.9, global_step=91772, preemption_count=0, score=36772.9, test/accuracy=0.5533, test/loss=2.02161, test/num_examples=10000, total_duration=39569.6, train/accuracy=0.757872, train/loss=0.910598, validation/accuracy=0.6765, validation/loss=1.31483, validation/num_examples=50000
I0307 14:09:49.955342 139958446696192 logging_writer.py:48] [91800] global_step=91800, grad_norm=2.358738899230957, loss=1.4447301626205444
I0307 14:10:29.778166 139958455088896 logging_writer.py:48] [91900] global_step=91900, grad_norm=2.2671186923980713, loss=1.3143410682678223
I0307 14:11:10.053551 139958446696192 logging_writer.py:48] [92000] global_step=92000, grad_norm=2.120126724243164, loss=1.4082682132720947
I0307 14:11:50.224342 139958455088896 logging_writer.py:48] [92100] global_step=92100, grad_norm=2.291205883026123, loss=1.394365668296814
I0307 14:12:30.271603 139958446696192 logging_writer.py:48] [92200] global_step=92200, grad_norm=2.1254701614379883, loss=1.4834853410720825
I0307 14:13:10.488373 139958455088896 logging_writer.py:48] [92300] global_step=92300, grad_norm=2.5182008743286133, loss=1.4667519330978394
I0307 14:13:51.232060 139958446696192 logging_writer.py:48] [92400] global_step=92400, grad_norm=2.242516279220581, loss=1.3571263551712036
I0307 14:14:31.201571 139958455088896 logging_writer.py:48] [92500] global_step=92500, grad_norm=2.260751962661743, loss=1.4295647144317627
2025-03-07 14:14:54.856839: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 14:15:11.761849 139958446696192 logging_writer.py:48] [92600] global_step=92600, grad_norm=2.4864697456359863, loss=1.3973768949508667
I0307 14:15:52.548509 139958455088896 logging_writer.py:48] [92700] global_step=92700, grad_norm=2.4686009883880615, loss=1.364459753036499
I0307 14:16:33.144528 139958446696192 logging_writer.py:48] [92800] global_step=92800, grad_norm=2.1977126598358154, loss=1.3297415971755981
I0307 14:17:13.307285 139958455088896 logging_writer.py:48] [92900] global_step=92900, grad_norm=2.1380417346954346, loss=1.4403119087219238
I0307 14:17:53.716530 139958446696192 logging_writer.py:48] [93000] global_step=93000, grad_norm=2.541865110397339, loss=1.4911586046218872
I0307 14:18:08.325871 140114851837120 spec.py:321] Evaluating on the training split.
I0307 14:18:21.160388 140114851837120 spec.py:333] Evaluating on the validation split.
I0307 14:18:33.804485 140114851837120 spec.py:349] Evaluating on the test split.
I0307 14:18:35.634944 140114851837120 submission_runner.py:469] Time since start: 40106.97s, 	Step: 93037, 	{'train/accuracy': 0.7685148119926453, 'train/loss': 0.8690674304962158, 'validation/accuracy': 0.6793599724769592, 'validation/loss': 1.2999283075332642, 'validation/num_examples': 50000, 'test/accuracy': 0.5532000064849854, 'test/loss': 2.01275372505188, 'test/num_examples': 10000, 'score': 37282.67379975319, 'total_duration': 40106.97201538086, 'accumulated_submission_time': 37282.67379975319, 'accumulated_eval_time': 2805.785019159317, 'accumulated_logging_time': 8.116628885269165}
I0307 14:18:35.729360 139958455088896 logging_writer.py:48] [93037] accumulated_eval_time=2805.79, accumulated_logging_time=8.11663, accumulated_submission_time=37282.7, global_step=93037, preemption_count=0, score=37282.7, test/accuracy=0.5532, test/loss=2.01275, test/num_examples=10000, total_duration=40107, train/accuracy=0.768515, train/loss=0.869067, validation/accuracy=0.67936, validation/loss=1.29993, validation/num_examples=50000
I0307 14:19:01.704426 139958446696192 logging_writer.py:48] [93100] global_step=93100, grad_norm=2.3008546829223633, loss=1.50586998462677
I0307 14:19:41.851087 139958455088896 logging_writer.py:48] [93200] global_step=93200, grad_norm=2.0918099880218506, loss=1.3407034873962402
I0307 14:20:22.275609 139958446696192 logging_writer.py:48] [93300] global_step=93300, grad_norm=2.2333641052246094, loss=1.4657951593399048
I0307 14:21:02.541551 139958455088896 logging_writer.py:48] [93400] global_step=93400, grad_norm=2.589019298553467, loss=1.4860483407974243
I0307 14:21:42.912376 139958446696192 logging_writer.py:48] [93500] global_step=93500, grad_norm=2.1977226734161377, loss=1.34879732131958
I0307 14:22:23.210072 139958455088896 logging_writer.py:48] [93600] global_step=93600, grad_norm=2.2626612186431885, loss=1.3427979946136475
I0307 14:23:03.747203 139958446696192 logging_writer.py:48] [93700] global_step=93700, grad_norm=2.290383815765381, loss=1.3429521322250366
I0307 14:23:44.132731 139958455088896 logging_writer.py:48] [93800] global_step=93800, grad_norm=2.1760642528533936, loss=1.3229225873947144
2025-03-07 14:23:47.489259: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 14:24:24.657248 139958446696192 logging_writer.py:48] [93900] global_step=93900, grad_norm=2.232707977294922, loss=1.4144498109817505
I0307 14:25:04.789685 139958455088896 logging_writer.py:48] [94000] global_step=94000, grad_norm=2.154700756072998, loss=1.3122704029083252
I0307 14:25:45.426162 139958446696192 logging_writer.py:48] [94100] global_step=94100, grad_norm=2.3127095699310303, loss=1.3391835689544678
I0307 14:26:25.728452 139958455088896 logging_writer.py:48] [94200] global_step=94200, grad_norm=2.355613946914673, loss=1.3565078973770142
I0307 14:27:05.792883 140114851837120 spec.py:321] Evaluating on the training split.
I0307 14:27:18.763812 140114851837120 spec.py:333] Evaluating on the validation split.
I0307 14:27:31.639862 140114851837120 spec.py:349] Evaluating on the test split.
I0307 14:27:33.466437 140114851837120 submission_runner.py:469] Time since start: 40644.80s, 	Step: 94300, 	{'train/accuracy': 0.76664137840271, 'train/loss': 0.8711822628974915, 'validation/accuracy': 0.6797199845314026, 'validation/loss': 1.2980207204818726, 'validation/num_examples': 50000, 'test/accuracy': 0.5515000224113464, 'test/loss': 2.027400255203247, 'test/num_examples': 10000, 'score': 37792.54945540428, 'total_duration': 40644.80345582962, 'accumulated_submission_time': 37792.54945540428, 'accumulated_eval_time': 2833.4583435058594, 'accumulated_logging_time': 8.245412111282349}
I0307 14:27:33.569784 139958446696192 logging_writer.py:48] [94300] accumulated_eval_time=2833.46, accumulated_logging_time=8.24541, accumulated_submission_time=37792.5, global_step=94300, preemption_count=0, score=37792.5, test/accuracy=0.5515, test/loss=2.0274, test/num_examples=10000, total_duration=40644.8, train/accuracy=0.766641, train/loss=0.871182, validation/accuracy=0.67972, validation/loss=1.29802, validation/num_examples=50000
I0307 14:27:33.993007 139958455088896 logging_writer.py:48] [94300] global_step=94300, grad_norm=2.4001078605651855, loss=1.503745675086975
I0307 14:28:14.721647 139958446696192 logging_writer.py:48] [94400] global_step=94400, grad_norm=2.53143048286438, loss=1.3833969831466675
I0307 14:28:55.224869 139958455088896 logging_writer.py:48] [94500] global_step=94500, grad_norm=2.277252197265625, loss=1.4167779684066772
I0307 14:29:35.846676 139958446696192 logging_writer.py:48] [94600] global_step=94600, grad_norm=2.2425012588500977, loss=1.428858995437622
I0307 14:30:16.449015 139958455088896 logging_writer.py:48] [94700] global_step=94700, grad_norm=2.24041748046875, loss=1.446597695350647
I0307 14:30:56.695245 139958446696192 logging_writer.py:48] [94800] global_step=94800, grad_norm=2.2218258380889893, loss=1.4069578647613525
I0307 14:31:36.609157 139958455088896 logging_writer.py:48] [94900] global_step=94900, grad_norm=2.380021095275879, loss=1.4078929424285889
I0307 14:32:16.710278 139958446696192 logging_writer.py:48] [95000] global_step=95000, grad_norm=2.150805711746216, loss=1.388917326927185
2025-03-07 14:32:40.979837: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 14:32:57.235406 139958455088896 logging_writer.py:48] [95100] global_step=95100, grad_norm=2.511024236679077, loss=1.4232563972473145
I0307 14:33:37.291444 139958446696192 logging_writer.py:48] [95200] global_step=95200, grad_norm=2.3951714038848877, loss=1.3916258811950684
I0307 14:34:17.423630 139958455088896 logging_writer.py:48] [95300] global_step=95300, grad_norm=2.4264445304870605, loss=1.4541683197021484
I0307 14:34:56.367340 139958446696192 logging_writer.py:48] [95400] global_step=95400, grad_norm=2.4013068675994873, loss=1.4053049087524414
I0307 14:35:36.309480 139958455088896 logging_writer.py:48] [95500] global_step=95500, grad_norm=2.3471791744232178, loss=1.5310299396514893
I0307 14:36:03.752325 140114851837120 spec.py:321] Evaluating on the training split.
I0307 14:36:16.297901 140114851837120 spec.py:333] Evaluating on the validation split.
I0307 14:36:29.011342 140114851837120 spec.py:349] Evaluating on the test split.
I0307 14:36:30.830922 140114851837120 submission_runner.py:469] Time since start: 41182.17s, 	Step: 95570, 	{'train/accuracy': 0.7712053656578064, 'train/loss': 0.8572474122047424, 'validation/accuracy': 0.6849600076675415, 'validation/loss': 1.2875492572784424, 'validation/num_examples': 50000, 'test/accuracy': 0.5561000108718872, 'test/loss': 2.026799201965332, 'test/num_examples': 10000, 'score': 38302.53266644478, 'total_duration': 41182.167934179306, 'accumulated_submission_time': 38302.53266644478, 'accumulated_eval_time': 2860.5366995334625, 'accumulated_logging_time': 8.395722150802612}
I0307 14:36:30.976820 139958446696192 logging_writer.py:48] [95570] accumulated_eval_time=2860.54, accumulated_logging_time=8.39572, accumulated_submission_time=38302.5, global_step=95570, preemption_count=0, score=38302.5, test/accuracy=0.5561, test/loss=2.0268, test/num_examples=10000, total_duration=41182.2, train/accuracy=0.771205, train/loss=0.857247, validation/accuracy=0.68496, validation/loss=1.28755, validation/num_examples=50000
I0307 14:36:43.364417 139958455088896 logging_writer.py:48] [95600] global_step=95600, grad_norm=2.477288007736206, loss=1.4170079231262207
I0307 14:37:23.932541 139958446696192 logging_writer.py:48] [95700] global_step=95700, grad_norm=2.494586944580078, loss=1.4469797611236572
I0307 14:38:04.021061 139958455088896 logging_writer.py:48] [95800] global_step=95800, grad_norm=2.268724203109741, loss=1.3492401838302612
I0307 14:38:43.891771 139958446696192 logging_writer.py:48] [95900] global_step=95900, grad_norm=2.1705267429351807, loss=1.2348219156265259
I0307 14:39:23.375307 139958455088896 logging_writer.py:48] [96000] global_step=96000, grad_norm=2.3153951168060303, loss=1.496189832687378
I0307 14:40:03.158766 139958446696192 logging_writer.py:48] [96100] global_step=96100, grad_norm=2.373054265975952, loss=1.3631999492645264
I0307 14:40:42.104314 139958455088896 logging_writer.py:48] [96200] global_step=96200, grad_norm=2.4035091400146484, loss=1.5260354280471802
I0307 14:41:22.188544 139958446696192 logging_writer.py:48] [96300] global_step=96300, grad_norm=2.4764883518218994, loss=1.3109852075576782
2025-03-07 14:41:26.400372: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 14:42:02.228061 139958455088896 logging_writer.py:48] [96400] global_step=96400, grad_norm=2.4264214038848877, loss=1.4544068574905396
I0307 14:42:42.122689 139958446696192 logging_writer.py:48] [96500] global_step=96500, grad_norm=2.3802661895751953, loss=1.3654625415802002
I0307 14:43:21.765539 139958455088896 logging_writer.py:48] [96600] global_step=96600, grad_norm=2.5270237922668457, loss=1.4320796728134155
I0307 14:44:01.656974 139958446696192 logging_writer.py:48] [96700] global_step=96700, grad_norm=2.402432918548584, loss=1.3741568326950073
I0307 14:44:41.363028 139958455088896 logging_writer.py:48] [96800] global_step=96800, grad_norm=2.4498536586761475, loss=1.4199014902114868
I0307 14:45:01.031370 140114851837120 spec.py:321] Evaluating on the training split.
I0307 14:45:13.849754 140114851837120 spec.py:333] Evaluating on the validation split.
I0307 14:45:26.400053 140114851837120 spec.py:349] Evaluating on the test split.
I0307 14:45:28.205297 140114851837120 submission_runner.py:469] Time since start: 41719.54s, 	Step: 96851, 	{'train/accuracy': 0.76566481590271, 'train/loss': 0.8755190968513489, 'validation/accuracy': 0.6824199557304382, 'validation/loss': 1.2965515851974487, 'validation/num_examples': 50000, 'test/accuracy': 0.5570000410079956, 'test/loss': 1.9973702430725098, 'test/num_examples': 10000, 'score': 38812.40683746338, 'total_duration': 41719.5423374176, 'accumulated_submission_time': 38812.40683746338, 'accumulated_eval_time': 2887.710411787033, 'accumulated_logging_time': 8.569047451019287}
I0307 14:45:28.353940 139958446696192 logging_writer.py:48] [96851] accumulated_eval_time=2887.71, accumulated_logging_time=8.56905, accumulated_submission_time=38812.4, global_step=96851, preemption_count=0, score=38812.4, test/accuracy=0.557, test/loss=1.99737, test/num_examples=10000, total_duration=41719.5, train/accuracy=0.765665, train/loss=0.875519, validation/accuracy=0.68242, validation/loss=1.29655, validation/num_examples=50000
I0307 14:45:48.717169 139958455088896 logging_writer.py:48] [96900] global_step=96900, grad_norm=2.366213798522949, loss=1.4414808750152588
I0307 14:46:28.914675 139958446696192 logging_writer.py:48] [97000] global_step=97000, grad_norm=2.2084732055664062, loss=1.378995656967163
I0307 14:47:09.238168 139958455088896 logging_writer.py:48] [97100] global_step=97100, grad_norm=2.244314670562744, loss=1.3485444784164429
I0307 14:47:49.267429 139958446696192 logging_writer.py:48] [97200] global_step=97200, grad_norm=2.4920413494110107, loss=1.4407445192337036
I0307 14:48:29.706851 139958455088896 logging_writer.py:48] [97300] global_step=97300, grad_norm=2.5802054405212402, loss=1.3332643508911133
I0307 14:49:10.364241 139958446696192 logging_writer.py:48] [97400] global_step=97400, grad_norm=2.774198293685913, loss=1.424168348312378
I0307 14:49:51.058118 139958455088896 logging_writer.py:48] [97500] global_step=97500, grad_norm=2.450275182723999, loss=1.3907783031463623
2025-03-07 14:50:16.034084: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 14:50:32.102641 139958446696192 logging_writer.py:48] [97600] global_step=97600, grad_norm=2.433856725692749, loss=1.3592963218688965
I0307 14:51:12.210357 139958455088896 logging_writer.py:48] [97700] global_step=97700, grad_norm=2.188981533050537, loss=1.3795689344406128
I0307 14:51:52.822843 139958446696192 logging_writer.py:48] [97800] global_step=97800, grad_norm=2.332608222961426, loss=1.3924849033355713
I0307 14:52:33.141752 139958455088896 logging_writer.py:48] [97900] global_step=97900, grad_norm=2.415764093399048, loss=1.4268828630447388
I0307 14:53:13.221328 139958446696192 logging_writer.py:48] [98000] global_step=98000, grad_norm=2.427637815475464, loss=1.4007455110549927
I0307 14:53:53.864116 139958455088896 logging_writer.py:48] [98100] global_step=98100, grad_norm=2.3271288871765137, loss=1.3471121788024902
I0307 14:53:58.250318 140114851837120 spec.py:321] Evaluating on the training split.
I0307 14:54:11.235555 140114851837120 spec.py:333] Evaluating on the validation split.
I0307 14:54:23.925718 140114851837120 spec.py:349] Evaluating on the test split.
I0307 14:54:25.756617 140114851837120 submission_runner.py:469] Time since start: 42257.09s, 	Step: 98112, 	{'train/accuracy': 0.767020046710968, 'train/loss': 0.8678561449050903, 'validation/accuracy': 0.6813399791717529, 'validation/loss': 1.30287766456604, 'validation/num_examples': 50000, 'test/accuracy': 0.5586000084877014, 'test/loss': 2.0183568000793457, 'test/num_examples': 10000, 'score': 39322.10249710083, 'total_duration': 42257.09367632866, 'accumulated_submission_time': 39322.10249710083, 'accumulated_eval_time': 2915.216523170471, 'accumulated_logging_time': 8.764290809631348}
I0307 14:54:25.902962 139958446696192 logging_writer.py:48] [98112] accumulated_eval_time=2915.22, accumulated_logging_time=8.76429, accumulated_submission_time=39322.1, global_step=98112, preemption_count=0, score=39322.1, test/accuracy=0.5586, test/loss=2.01836, test/num_examples=10000, total_duration=42257.1, train/accuracy=0.76702, train/loss=0.867856, validation/accuracy=0.68134, validation/loss=1.30288, validation/num_examples=50000
I0307 14:55:01.476007 139958455088896 logging_writer.py:48] [98200] global_step=98200, grad_norm=2.658379554748535, loss=1.36057448387146
I0307 14:55:41.762398 139958446696192 logging_writer.py:48] [98300] global_step=98300, grad_norm=2.6502492427825928, loss=1.4787514209747314
I0307 14:56:22.272091 139958455088896 logging_writer.py:48] [98400] global_step=98400, grad_norm=2.3603179454803467, loss=1.424193024635315
I0307 14:57:02.595068 139958446696192 logging_writer.py:48] [98500] global_step=98500, grad_norm=2.2624638080596924, loss=1.3951002359390259
I0307 14:57:42.494120 139958455088896 logging_writer.py:48] [98600] global_step=98600, grad_norm=2.5157296657562256, loss=1.3906148672103882
I0307 14:58:23.098724 139958446696192 logging_writer.py:48] [98700] global_step=98700, grad_norm=2.4320621490478516, loss=1.3574185371398926
I0307 14:59:03.671294 139958455088896 logging_writer.py:48] [98800] global_step=98800, grad_norm=2.599550485610962, loss=1.4679425954818726
2025-03-07 14:59:08.848023: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 14:59:43.572439 139958446696192 logging_writer.py:48] [98900] global_step=98900, grad_norm=2.308121919631958, loss=1.4005179405212402
I0307 15:00:23.909165 139958455088896 logging_writer.py:48] [99000] global_step=99000, grad_norm=2.533780574798584, loss=1.3777244091033936
I0307 15:01:03.384797 139958446696192 logging_writer.py:48] [99100] global_step=99100, grad_norm=2.415283203125, loss=1.3447980880737305
I0307 15:01:43.470386 139958455088896 logging_writer.py:48] [99200] global_step=99200, grad_norm=2.4957168102264404, loss=1.3181877136230469
I0307 15:02:23.710189 139958446696192 logging_writer.py:48] [99300] global_step=99300, grad_norm=2.630704879760742, loss=1.4165902137756348
I0307 15:02:56.095656 140114851837120 spec.py:321] Evaluating on the training split.
I0307 15:03:08.854472 140114851837120 spec.py:333] Evaluating on the validation split.
I0307 15:03:21.626790 140114851837120 spec.py:349] Evaluating on the test split.
I0307 15:03:23.452014 140114851837120 submission_runner.py:469] Time since start: 42794.79s, 	Step: 99382, 	{'train/accuracy': 0.7692322731018066, 'train/loss': 0.8620010018348694, 'validation/accuracy': 0.6821399927139282, 'validation/loss': 1.2936266660690308, 'validation/num_examples': 50000, 'test/accuracy': 0.5515000224113464, 'test/loss': 2.029322624206543, 'test/num_examples': 10000, 'score': 39832.10922241211, 'total_duration': 42794.78906965256, 'accumulated_submission_time': 39832.10922241211, 'accumulated_eval_time': 2942.572678565979, 'accumulated_logging_time': 8.941336870193481}
I0307 15:03:23.584299 139958455088896 logging_writer.py:48] [99382] accumulated_eval_time=2942.57, accumulated_logging_time=8.94134, accumulated_submission_time=39832.1, global_step=99382, preemption_count=0, score=39832.1, test/accuracy=0.5515, test/loss=2.02932, test/num_examples=10000, total_duration=42794.8, train/accuracy=0.769232, train/loss=0.862001, validation/accuracy=0.68214, validation/loss=1.29363, validation/num_examples=50000
I0307 15:03:31.226115 139958446696192 logging_writer.py:48] [99400] global_step=99400, grad_norm=2.6058766841888428, loss=1.388484239578247
I0307 15:04:11.521138 139958455088896 logging_writer.py:48] [99500] global_step=99500, grad_norm=2.75980281829834, loss=1.4012764692306519
I0307 15:04:51.430553 139958446696192 logging_writer.py:48] [99600] global_step=99600, grad_norm=2.3823659420013428, loss=1.489052414894104
I0307 15:05:31.509592 139958455088896 logging_writer.py:48] [99700] global_step=99700, grad_norm=2.465351104736328, loss=1.3819119930267334
I0307 15:06:11.910828 139958446696192 logging_writer.py:48] [99800] global_step=99800, grad_norm=2.1831793785095215, loss=1.34478759765625
I0307 15:06:52.226971 139958455088896 logging_writer.py:48] [99900] global_step=99900, grad_norm=2.3287312984466553, loss=1.371849536895752
I0307 15:07:32.629364 139958446696192 logging_writer.py:48] [100000] global_step=100000, grad_norm=2.2903034687042236, loss=1.3869950771331787
2025-03-07 15:07:58.433019: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 15:08:12.911069 139958455088896 logging_writer.py:48] [100100] global_step=100100, grad_norm=2.404193162918091, loss=1.345913290977478
I0307 15:08:52.559043 139958446696192 logging_writer.py:48] [100200] global_step=100200, grad_norm=2.3034660816192627, loss=1.309943437576294
I0307 15:09:32.523372 139958455088896 logging_writer.py:48] [100300] global_step=100300, grad_norm=2.3550751209259033, loss=1.3835762739181519
I0307 15:10:12.702133 139958446696192 logging_writer.py:48] [100400] global_step=100400, grad_norm=2.471569061279297, loss=1.3540029525756836
I0307 15:10:52.751136 139958455088896 logging_writer.py:48] [100500] global_step=100500, grad_norm=2.408928632736206, loss=1.4363791942596436
I0307 15:11:33.677714 139958446696192 logging_writer.py:48] [100600] global_step=100600, grad_norm=2.4586830139160156, loss=1.3473707437515259
I0307 15:11:53.708530 140114851837120 spec.py:321] Evaluating on the training split.
I0307 15:12:06.873766 140114851837120 spec.py:333] Evaluating on the validation split.
I0307 15:12:19.311367 140114851837120 spec.py:349] Evaluating on the test split.
I0307 15:12:21.133649 140114851837120 submission_runner.py:469] Time since start: 43332.47s, 	Step: 100651, 	{'train/accuracy': 0.7721819281578064, 'train/loss': 0.8489797115325928, 'validation/accuracy': 0.6789999604225159, 'validation/loss': 1.306251883506775, 'validation/num_examples': 50000, 'test/accuracy': 0.5533000230789185, 'test/loss': 2.054805040359497, 'test/num_examples': 10000, 'score': 40342.058062553406, 'total_duration': 43332.470703840256, 'accumulated_submission_time': 40342.058062553406, 'accumulated_eval_time': 2969.9975974559784, 'accumulated_logging_time': 9.095003843307495}
I0307 15:12:21.305294 139958455088896 logging_writer.py:48] [100651] accumulated_eval_time=2970, accumulated_logging_time=9.095, accumulated_submission_time=40342.1, global_step=100651, preemption_count=0, score=40342.1, test/accuracy=0.5533, test/loss=2.05481, test/num_examples=10000, total_duration=43332.5, train/accuracy=0.772182, train/loss=0.84898, validation/accuracy=0.679, validation/loss=1.30625, validation/num_examples=50000
I0307 15:12:41.414046 139958446696192 logging_writer.py:48] [100700] global_step=100700, grad_norm=2.3906924724578857, loss=1.4151582717895508
I0307 15:13:21.500535 139958455088896 logging_writer.py:48] [100800] global_step=100800, grad_norm=2.2305126190185547, loss=1.2373617887496948
I0307 15:14:01.759593 139958446696192 logging_writer.py:48] [100900] global_step=100900, grad_norm=2.880075454711914, loss=1.4726356267929077
I0307 15:14:41.851049 139958455088896 logging_writer.py:48] [101000] global_step=101000, grad_norm=2.8345131874084473, loss=1.370048999786377
I0307 15:15:22.520217 139958446696192 logging_writer.py:48] [101100] global_step=101100, grad_norm=2.4748752117156982, loss=1.397851824760437
I0307 15:16:02.587985 139958455088896 logging_writer.py:48] [101200] global_step=101200, grad_norm=2.6721675395965576, loss=1.327021837234497
I0307 15:16:42.325679 139958446696192 logging_writer.py:48] [101300] global_step=101300, grad_norm=2.0874757766723633, loss=1.178532600402832
2025-03-07 15:16:48.601333: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 15:17:23.040451 139958455088896 logging_writer.py:48] [101400] global_step=101400, grad_norm=2.3325934410095215, loss=1.4003527164459229
I0307 15:18:02.952380 139958446696192 logging_writer.py:48] [101500] global_step=101500, grad_norm=2.470245361328125, loss=1.4182612895965576
I0307 15:18:42.879891 139958455088896 logging_writer.py:48] [101600] global_step=101600, grad_norm=2.481067657470703, loss=1.338890790939331
I0307 15:19:22.958302 139958446696192 logging_writer.py:48] [101700] global_step=101700, grad_norm=2.6700632572174072, loss=1.45204496383667
I0307 15:20:03.093838 139958455088896 logging_writer.py:48] [101800] global_step=101800, grad_norm=2.343414306640625, loss=1.358525276184082
I0307 15:20:43.204486 139958446696192 logging_writer.py:48] [101900] global_step=101900, grad_norm=2.763883590698242, loss=1.3484036922454834
I0307 15:20:51.266733 140114851837120 spec.py:321] Evaluating on the training split.
I0307 15:21:04.220458 140114851837120 spec.py:333] Evaluating on the validation split.
I0307 15:21:16.863515 140114851837120 spec.py:349] Evaluating on the test split.
I0307 15:21:18.682487 140114851837120 submission_runner.py:469] Time since start: 43870.02s, 	Step: 101921, 	{'train/accuracy': 0.7595264315605164, 'train/loss': 0.8818293809890747, 'validation/accuracy': 0.6751199960708618, 'validation/loss': 1.337367296218872, 'validation/num_examples': 50000, 'test/accuracy': 0.5442000031471252, 'test/loss': 2.0929226875305176, 'test/num_examples': 10000, 'score': 40851.84210205078, 'total_duration': 43870.01954984665, 'accumulated_submission_time': 40851.84210205078, 'accumulated_eval_time': 2997.4131529331207, 'accumulated_logging_time': 9.294449090957642}
I0307 15:21:18.784071 139958455088896 logging_writer.py:48] [101921] accumulated_eval_time=2997.41, accumulated_logging_time=9.29445, accumulated_submission_time=40851.8, global_step=101921, preemption_count=0, score=40851.8, test/accuracy=0.5442, test/loss=2.09292, test/num_examples=10000, total_duration=43870, train/accuracy=0.759526, train/loss=0.881829, validation/accuracy=0.67512, validation/loss=1.33737, validation/num_examples=50000
I0307 15:21:51.268316 139958446696192 logging_writer.py:48] [102000] global_step=102000, grad_norm=2.73478627204895, loss=1.4464514255523682
I0307 15:22:31.219174 139958455088896 logging_writer.py:48] [102100] global_step=102100, grad_norm=2.296943426132202, loss=1.2746514081954956
I0307 15:23:11.284618 139958446696192 logging_writer.py:48] [102200] global_step=102200, grad_norm=2.463892936706543, loss=1.3781533241271973
I0307 15:23:51.141358 139958455088896 logging_writer.py:48] [102300] global_step=102300, grad_norm=2.3364779949188232, loss=1.3660224676132202
I0307 15:24:31.275706 139958446696192 logging_writer.py:48] [102400] global_step=102400, grad_norm=2.3599581718444824, loss=1.478617787361145
I0307 15:25:11.191588 139958455088896 logging_writer.py:48] [102500] global_step=102500, grad_norm=2.2363483905792236, loss=1.4219194650650024
2025-03-07 15:25:37.813076: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 15:25:51.605208 139958446696192 logging_writer.py:48] [102600] global_step=102600, grad_norm=2.3858354091644287, loss=1.3098920583724976
I0307 15:26:32.201232 139958455088896 logging_writer.py:48] [102700] global_step=102700, grad_norm=2.34039306640625, loss=1.2228829860687256
I0307 15:27:12.425416 139958446696192 logging_writer.py:48] [102800] global_step=102800, grad_norm=2.300966262817383, loss=1.3354603052139282
I0307 15:27:52.616810 139958455088896 logging_writer.py:48] [102900] global_step=102900, grad_norm=2.486494541168213, loss=1.303365707397461
I0307 15:28:33.209077 139958446696192 logging_writer.py:48] [103000] global_step=103000, grad_norm=2.294564723968506, loss=1.3551057577133179
I0307 15:29:13.544743 139958455088896 logging_writer.py:48] [103100] global_step=103100, grad_norm=2.667414665222168, loss=1.3800603151321411
I0307 15:29:49.081016 140114851837120 spec.py:321] Evaluating on the training split.
I0307 15:30:01.917904 140114851837120 spec.py:333] Evaluating on the validation split.
I0307 15:30:14.357410 140114851837120 spec.py:349] Evaluating on the test split.
I0307 15:30:16.180151 140114851837120 submission_runner.py:469] Time since start: 44407.52s, 	Step: 103189, 	{'train/accuracy': 0.7863719463348389, 'train/loss': 0.8021854162216187, 'validation/accuracy': 0.692039966583252, 'validation/loss': 1.2510658502578735, 'validation/num_examples': 50000, 'test/accuracy': 0.5560000538825989, 'test/loss': 1.9731688499450684, 'test/num_examples': 10000, 'score': 41361.91395354271, 'total_duration': 44407.51722383499, 'accumulated_submission_time': 41361.91395354271, 'accumulated_eval_time': 3024.5121088027954, 'accumulated_logging_time': 9.471859216690063}
I0307 15:30:16.243071 139958446696192 logging_writer.py:48] [103189] accumulated_eval_time=3024.51, accumulated_logging_time=9.47186, accumulated_submission_time=41361.9, global_step=103189, preemption_count=0, score=41361.9, test/accuracy=0.556, test/loss=1.97317, test/num_examples=10000, total_duration=44407.5, train/accuracy=0.786372, train/loss=0.802185, validation/accuracy=0.69204, validation/loss=1.25107, validation/num_examples=50000
I0307 15:30:21.214676 139958455088896 logging_writer.py:48] [103200] global_step=103200, grad_norm=2.3900983333587646, loss=1.4093888998031616
I0307 15:31:01.205303 139958446696192 logging_writer.py:48] [103300] global_step=103300, grad_norm=2.573943614959717, loss=1.34367036819458
I0307 15:31:41.602314 139958455088896 logging_writer.py:48] [103400] global_step=103400, grad_norm=2.2295186519622803, loss=1.4072885513305664
I0307 15:32:21.713543 139958446696192 logging_writer.py:48] [103500] global_step=103500, grad_norm=2.249746322631836, loss=1.3234506845474243
I0307 15:33:01.362373 139958455088896 logging_writer.py:48] [103600] global_step=103600, grad_norm=2.3891725540161133, loss=1.3283084630966187
I0307 15:33:41.708945 139958446696192 logging_writer.py:48] [103700] global_step=103700, grad_norm=2.384828567504883, loss=1.3234487771987915
I0307 15:34:21.621386 139958455088896 logging_writer.py:48] [103800] global_step=103800, grad_norm=2.3130695819854736, loss=1.237579345703125
2025-03-07 15:34:28.567428: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 15:35:02.032851 139958446696192 logging_writer.py:48] [103900] global_step=103900, grad_norm=2.3781449794769287, loss=1.3620630502700806
I0307 15:35:41.927848 139958455088896 logging_writer.py:48] [104000] global_step=104000, grad_norm=2.4947509765625, loss=1.3437949419021606
I0307 15:36:22.198361 139958446696192 logging_writer.py:48] [104100] global_step=104100, grad_norm=2.5253539085388184, loss=1.54952073097229
I0307 15:37:02.410176 139958455088896 logging_writer.py:48] [104200] global_step=104200, grad_norm=2.586890459060669, loss=1.406899094581604
I0307 15:37:42.791184 139958446696192 logging_writer.py:48] [104300] global_step=104300, grad_norm=2.335111618041992, loss=1.3365182876586914
I0307 15:38:23.264773 139958455088896 logging_writer.py:48] [104400] global_step=104400, grad_norm=2.4242427349090576, loss=1.3160430192947388
I0307 15:38:46.195876 140114851837120 spec.py:321] Evaluating on the training split.
I0307 15:38:59.281956 140114851837120 spec.py:333] Evaluating on the validation split.
I0307 15:39:11.946147 140114851837120 spec.py:349] Evaluating on the test split.
I0307 15:39:13.779773 140114851837120 submission_runner.py:469] Time since start: 44945.12s, 	Step: 104458, 	{'train/accuracy': 0.7827845811843872, 'train/loss': 0.7984907627105713, 'validation/accuracy': 0.6882399916648865, 'validation/loss': 1.2776323556900024, 'validation/num_examples': 50000, 'test/accuracy': 0.5537000298500061, 'test/loss': 2.0164968967437744, 'test/num_examples': 10000, 'score': 41871.68844270706, 'total_duration': 44945.11683702469, 'accumulated_submission_time': 41871.68844270706, 'accumulated_eval_time': 3052.095820903778, 'accumulated_logging_time': 9.563466548919678}
I0307 15:39:13.880813 139958446696192 logging_writer.py:48] [104458] accumulated_eval_time=3052.1, accumulated_logging_time=9.56347, accumulated_submission_time=41871.7, global_step=104458, preemption_count=0, score=41871.7, test/accuracy=0.5537, test/loss=2.0165, test/num_examples=10000, total_duration=44945.1, train/accuracy=0.782785, train/loss=0.798491, validation/accuracy=0.68824, validation/loss=1.27763, validation/num_examples=50000
I0307 15:39:31.011562 139958455088896 logging_writer.py:48] [104500] global_step=104500, grad_norm=2.2569265365600586, loss=1.321169376373291
I0307 15:40:11.115069 139958446696192 logging_writer.py:48] [104600] global_step=104600, grad_norm=2.368969678878784, loss=1.3282434940338135
I0307 15:40:51.452481 139958455088896 logging_writer.py:48] [104700] global_step=104700, grad_norm=2.6554067134857178, loss=1.2723588943481445
I0307 15:41:31.187539 139958446696192 logging_writer.py:48] [104800] global_step=104800, grad_norm=2.6518166065216064, loss=1.3482697010040283
I0307 15:42:11.268313 139958455088896 logging_writer.py:48] [104900] global_step=104900, grad_norm=2.1547327041625977, loss=1.2988297939300537
I0307 15:42:50.441612 139958446696192 logging_writer.py:48] [105000] global_step=105000, grad_norm=2.6027591228485107, loss=1.3468034267425537
2025-03-07 15:43:17.918755: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 15:43:30.720626 139958455088896 logging_writer.py:48] [105100] global_step=105100, grad_norm=2.426098108291626, loss=1.3097832202911377
I0307 15:44:10.468968 139958446696192 logging_writer.py:48] [105200] global_step=105200, grad_norm=2.676396131515503, loss=1.4515714645385742
I0307 15:44:50.527618 139958455088896 logging_writer.py:48] [105300] global_step=105300, grad_norm=2.514300584793091, loss=1.3339940309524536
I0307 15:45:30.627189 139958446696192 logging_writer.py:48] [105400] global_step=105400, grad_norm=2.6041324138641357, loss=1.4276405572891235
I0307 15:46:11.423423 139958455088896 logging_writer.py:48] [105500] global_step=105500, grad_norm=2.41202974319458, loss=1.3946471214294434
I0307 15:46:51.961550 139958446696192 logging_writer.py:48] [105600] global_step=105600, grad_norm=2.57146954536438, loss=1.2489949464797974
I0307 15:47:32.279119 139958455088896 logging_writer.py:48] [105700] global_step=105700, grad_norm=2.8360512256622314, loss=1.4469871520996094
I0307 15:47:44.137173 140114851837120 spec.py:321] Evaluating on the training split.
I0307 15:47:56.943275 140114851837120 spec.py:333] Evaluating on the validation split.
I0307 15:48:09.599537 140114851837120 spec.py:349] Evaluating on the test split.
I0307 15:48:11.428711 140114851837120 submission_runner.py:469] Time since start: 45482.77s, 	Step: 105730, 	{'train/accuracy': 0.7887037396430969, 'train/loss': 0.776313304901123, 'validation/accuracy': 0.6886599659919739, 'validation/loss': 1.2676867246627808, 'validation/num_examples': 50000, 'test/accuracy': 0.5593000054359436, 'test/loss': 1.9992930889129639, 'test/num_examples': 10000, 'score': 42381.77481198311, 'total_duration': 45482.76578807831, 'accumulated_submission_time': 42381.77481198311, 'accumulated_eval_time': 3079.3871760368347, 'accumulated_logging_time': 9.688700437545776}
I0307 15:48:11.525179 139958446696192 logging_writer.py:48] [105730] accumulated_eval_time=3079.39, accumulated_logging_time=9.6887, accumulated_submission_time=42381.8, global_step=105730, preemption_count=0, score=42381.8, test/accuracy=0.5593, test/loss=1.99929, test/num_examples=10000, total_duration=45482.8, train/accuracy=0.788704, train/loss=0.776313, validation/accuracy=0.68866, validation/loss=1.26769, validation/num_examples=50000
I0307 15:48:40.398682 139958455088896 logging_writer.py:48] [105800] global_step=105800, grad_norm=2.2239840030670166, loss=1.3178917169570923
I0307 15:49:20.332102 139958446696192 logging_writer.py:48] [105900] global_step=105900, grad_norm=2.4950902462005615, loss=1.3501085042953491
I0307 15:50:00.553072 139958455088896 logging_writer.py:48] [106000] global_step=106000, grad_norm=2.5884647369384766, loss=1.440497636795044
I0307 15:50:40.664450 139958446696192 logging_writer.py:48] [106100] global_step=106100, grad_norm=2.48227858543396, loss=1.316748023033142
I0307 15:51:20.863193 139958455088896 logging_writer.py:48] [106200] global_step=106200, grad_norm=2.6466143131256104, loss=1.2988548278808594
I0307 15:52:01.154715 139958446696192 logging_writer.py:48] [106300] global_step=106300, grad_norm=2.462911367416382, loss=1.3901618719100952
2025-03-07 15:52:09.127325: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 15:52:41.667987 139958455088896 logging_writer.py:48] [106400] global_step=106400, grad_norm=2.6953158378601074, loss=1.282676100730896
I0307 15:53:21.842051 139958446696192 logging_writer.py:48] [106500] global_step=106500, grad_norm=2.630693197250366, loss=1.313261866569519
I0307 15:54:01.992279 139958455088896 logging_writer.py:48] [106600] global_step=106600, grad_norm=2.6894469261169434, loss=1.4377100467681885
I0307 15:54:42.031903 139958446696192 logging_writer.py:48] [106700] global_step=106700, grad_norm=2.5042479038238525, loss=1.3384284973144531
I0307 15:55:22.072089 139958455088896 logging_writer.py:48] [106800] global_step=106800, grad_norm=2.315229892730713, loss=1.259027123451233
I0307 15:56:01.970991 139958446696192 logging_writer.py:48] [106900] global_step=106900, grad_norm=2.643872022628784, loss=1.3394190073013306
I0307 15:56:41.634612 140114851837120 spec.py:321] Evaluating on the training split.
I0307 15:56:54.522815 140114851837120 spec.py:333] Evaluating on the validation split.
I0307 15:57:07.054162 140114851837120 spec.py:349] Evaluating on the test split.
I0307 15:57:08.889881 140114851837120 submission_runner.py:469] Time since start: 46020.23s, 	Step: 106999, 	{'train/accuracy': 0.7856544852256775, 'train/loss': 0.7834033370018005, 'validation/accuracy': 0.686739981174469, 'validation/loss': 1.2815196514129639, 'validation/num_examples': 50000, 'test/accuracy': 0.5581000447273254, 'test/loss': 1.9943041801452637, 'test/num_examples': 10000, 'score': 42891.70066475868, 'total_duration': 46020.22691607475, 'accumulated_submission_time': 42891.70066475868, 'accumulated_eval_time': 3106.6422216892242, 'accumulated_logging_time': 9.82244324684143}
I0307 15:57:09.013717 139958455088896 logging_writer.py:48] [106999] accumulated_eval_time=3106.64, accumulated_logging_time=9.82244, accumulated_submission_time=42891.7, global_step=106999, preemption_count=0, score=42891.7, test/accuracy=0.5581, test/loss=1.9943, test/num_examples=10000, total_duration=46020.2, train/accuracy=0.785654, train/loss=0.783403, validation/accuracy=0.68674, validation/loss=1.28152, validation/num_examples=50000
I0307 15:57:09.781171 139958446696192 logging_writer.py:48] [107000] global_step=107000, grad_norm=2.7717888355255127, loss=1.3452332019805908
I0307 15:57:49.760161 139958455088896 logging_writer.py:48] [107100] global_step=107100, grad_norm=2.6229801177978516, loss=1.2675470113754272
I0307 15:58:29.597045 139958446696192 logging_writer.py:48] [107200] global_step=107200, grad_norm=2.593921422958374, loss=1.4053605794906616
I0307 15:59:09.141928 139958455088896 logging_writer.py:48] [107300] global_step=107300, grad_norm=2.686197280883789, loss=1.3258662223815918
I0307 15:59:49.931924 139958446696192 logging_writer.py:48] [107400] global_step=107400, grad_norm=2.4866714477539062, loss=1.4497754573822021
I0307 16:00:30.169988 139958455088896 logging_writer.py:48] [107500] global_step=107500, grad_norm=2.6534976959228516, loss=1.387769103050232
2025-03-07 16:00:59.293736: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 16:01:11.026119 139958446696192 logging_writer.py:48] [107600] global_step=107600, grad_norm=2.5508251190185547, loss=1.3423593044281006
I0307 16:01:51.552746 139958455088896 logging_writer.py:48] [107700] global_step=107700, grad_norm=2.6282832622528076, loss=1.237905502319336
I0307 16:02:32.081968 139958446696192 logging_writer.py:48] [107800] global_step=107800, grad_norm=2.983555555343628, loss=1.4056675434112549
I0307 16:03:12.402529 139958455088896 logging_writer.py:48] [107900] global_step=107900, grad_norm=2.723268508911133, loss=1.2230271100997925
I0307 16:03:52.960669 139958446696192 logging_writer.py:48] [108000] global_step=108000, grad_norm=2.630789279937744, loss=1.2383044958114624
I0307 16:04:33.081309 139958455088896 logging_writer.py:48] [108100] global_step=108100, grad_norm=2.3097171783447266, loss=1.3777945041656494
I0307 16:05:13.160941 139958446696192 logging_writer.py:48] [108200] global_step=108200, grad_norm=2.613917112350464, loss=1.467329502105713
I0307 16:05:38.927580 140114851837120 spec.py:321] Evaluating on the training split.
I0307 16:05:51.905636 140114851837120 spec.py:333] Evaluating on the validation split.
I0307 16:06:04.369364 140114851837120 spec.py:349] Evaluating on the test split.
I0307 16:06:06.220044 140114851837120 submission_runner.py:469] Time since start: 46557.56s, 	Step: 108265, 	{'train/accuracy': 0.7967952489852905, 'train/loss': 0.7369797825813293, 'validation/accuracy': 0.693399965763092, 'validation/loss': 1.250244379043579, 'validation/num_examples': 50000, 'test/accuracy': 0.5676000118255615, 'test/loss': 1.9796468019485474, 'test/num_examples': 10000, 'score': 43401.43486213684, 'total_duration': 46557.55706334114, 'accumulated_submission_time': 43401.43486213684, 'accumulated_eval_time': 3133.9344544410706, 'accumulated_logging_time': 9.976380348205566}
I0307 16:06:06.387510 139958455088896 logging_writer.py:48] [108265] accumulated_eval_time=3133.93, accumulated_logging_time=9.97638, accumulated_submission_time=43401.4, global_step=108265, preemption_count=0, score=43401.4, test/accuracy=0.5676, test/loss=1.97965, test/num_examples=10000, total_duration=46557.6, train/accuracy=0.796795, train/loss=0.73698, validation/accuracy=0.6934, validation/loss=1.25024, validation/num_examples=50000
I0307 16:06:20.817665 139958446696192 logging_writer.py:48] [108300] global_step=108300, grad_norm=2.5192453861236572, loss=1.3481347560882568
I0307 16:07:00.922289 139958455088896 logging_writer.py:48] [108400] global_step=108400, grad_norm=2.316370964050293, loss=1.3508855104446411
I0307 16:07:41.159734 139958446696192 logging_writer.py:48] [108500] global_step=108500, grad_norm=2.4735825061798096, loss=1.2587908506393433
I0307 16:08:20.895570 139958455088896 logging_writer.py:48] [108600] global_step=108600, grad_norm=2.702366590499878, loss=1.4133236408233643
I0307 16:09:01.482828 139958446696192 logging_writer.py:48] [108700] global_step=108700, grad_norm=2.559739112854004, loss=1.3455134630203247
I0307 16:09:41.291057 139958455088896 logging_writer.py:48] [108800] global_step=108800, grad_norm=2.574968099594116, loss=1.3501609563827515
2025-03-07 16:09:50.549489: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 16:10:22.319708 139958446696192 logging_writer.py:48] [108900] global_step=108900, grad_norm=2.7216897010803223, loss=1.2697511911392212
I0307 16:11:02.787620 139958455088896 logging_writer.py:48] [109000] global_step=109000, grad_norm=2.7145299911499023, loss=1.420426368713379
I0307 16:11:43.076851 139958446696192 logging_writer.py:48] [109100] global_step=109100, grad_norm=2.5081794261932373, loss=1.1749194860458374
I0307 16:12:23.772008 139958455088896 logging_writer.py:48] [109200] global_step=109200, grad_norm=2.6057395935058594, loss=1.3136787414550781
I0307 16:13:04.430042 139958446696192 logging_writer.py:48] [109300] global_step=109300, grad_norm=2.546023368835449, loss=1.2206742763519287
I0307 16:13:45.075118 139958455088896 logging_writer.py:48] [109400] global_step=109400, grad_norm=2.5799081325531006, loss=1.229583740234375
I0307 16:14:25.820153 139958446696192 logging_writer.py:48] [109500] global_step=109500, grad_norm=2.6267170906066895, loss=1.2343039512634277
I0307 16:14:36.415761 140114851837120 spec.py:321] Evaluating on the training split.
I0307 16:14:49.286467 140114851837120 spec.py:333] Evaluating on the validation split.
I0307 16:15:01.704246 140114851837120 spec.py:349] Evaluating on the test split.
I0307 16:15:03.518489 140114851837120 submission_runner.py:469] Time since start: 47094.86s, 	Step: 109527, 	{'train/accuracy': 0.7989476919174194, 'train/loss': 0.7366896271705627, 'validation/accuracy': 0.6920199990272522, 'validation/loss': 1.241847038269043, 'validation/num_examples': 50000, 'test/accuracy': 0.5640000104904175, 'test/loss': 1.9837467670440674, 'test/num_examples': 10000, 'score': 43911.2816901207, 'total_duration': 47094.85551953316, 'accumulated_submission_time': 43911.2816901207, 'accumulated_eval_time': 3161.036957025528, 'accumulated_logging_time': 10.175127983093262}
I0307 16:15:03.614859 139958455088896 logging_writer.py:48] [109527] accumulated_eval_time=3161.04, accumulated_logging_time=10.1751, accumulated_submission_time=43911.3, global_step=109527, preemption_count=0, score=43911.3, test/accuracy=0.564, test/loss=1.98375, test/num_examples=10000, total_duration=47094.9, train/accuracy=0.798948, train/loss=0.73669, validation/accuracy=0.69202, validation/loss=1.24185, validation/num_examples=50000
I0307 16:15:33.630975 139958446696192 logging_writer.py:48] [109600] global_step=109600, grad_norm=2.514618158340454, loss=1.2724714279174805
I0307 16:16:13.829023 139958455088896 logging_writer.py:48] [109700] global_step=109700, grad_norm=2.6581342220306396, loss=1.3262763023376465
I0307 16:16:53.803833 139958446696192 logging_writer.py:48] [109800] global_step=109800, grad_norm=2.842350721359253, loss=1.2948615550994873
I0307 16:17:33.963646 139958455088896 logging_writer.py:48] [109900] global_step=109900, grad_norm=2.589646816253662, loss=1.2882823944091797
I0307 16:18:13.711594 139958446696192 logging_writer.py:48] [110000] global_step=110000, grad_norm=2.994176149368286, loss=1.3158236742019653
2025-03-07 16:18:43.652734: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 16:18:54.211189 139958455088896 logging_writer.py:48] [110100] global_step=110100, grad_norm=2.501598358154297, loss=1.2260485887527466
I0307 16:19:34.062333 139958446696192 logging_writer.py:48] [110200] global_step=110200, grad_norm=2.419387102127075, loss=1.2171131372451782
I0307 16:20:13.962342 139958455088896 logging_writer.py:48] [110300] global_step=110300, grad_norm=2.7252402305603027, loss=1.3709361553192139
I0307 16:20:54.288036 139958446696192 logging_writer.py:48] [110400] global_step=110400, grad_norm=2.660778045654297, loss=1.3669472932815552
I0307 16:21:34.885940 139958455088896 logging_writer.py:48] [110500] global_step=110500, grad_norm=2.5183279514312744, loss=1.2306184768676758
I0307 16:22:15.369519 139958446696192 logging_writer.py:48] [110600] global_step=110600, grad_norm=2.358917236328125, loss=1.1985454559326172
I0307 16:22:55.544706 139958455088896 logging_writer.py:48] [110700] global_step=110700, grad_norm=2.7480504512786865, loss=1.3998920917510986
I0307 16:23:33.720879 140114851837120 spec.py:321] Evaluating on the training split.
I0307 16:23:46.589967 140114851837120 spec.py:333] Evaluating on the validation split.
I0307 16:23:59.191180 140114851837120 spec.py:349] Evaluating on the test split.
I0307 16:24:01.024732 140114851837120 submission_runner.py:469] Time since start: 47632.36s, 	Step: 110794, 	{'train/accuracy': 0.8114436864852905, 'train/loss': 0.6931107044219971, 'validation/accuracy': 0.6977799534797668, 'validation/loss': 1.2324331998825073, 'validation/num_examples': 50000, 'test/accuracy': 0.5680000185966492, 'test/loss': 1.9631637334823608, 'test/num_examples': 10000, 'score': 44421.214066028595, 'total_duration': 47632.36178588867, 'accumulated_submission_time': 44421.214066028595, 'accumulated_eval_time': 3188.340614080429, 'accumulated_logging_time': 10.29618525505066}
I0307 16:24:01.108938 139958446696192 logging_writer.py:48] [110794] accumulated_eval_time=3188.34, accumulated_logging_time=10.2962, accumulated_submission_time=44421.2, global_step=110794, preemption_count=0, score=44421.2, test/accuracy=0.568, test/loss=1.96316, test/num_examples=10000, total_duration=47632.4, train/accuracy=0.811444, train/loss=0.693111, validation/accuracy=0.69778, validation/loss=1.23243, validation/num_examples=50000
I0307 16:24:03.931387 139958455088896 logging_writer.py:48] [110800] global_step=110800, grad_norm=2.4671154022216797, loss=1.2982327938079834
I0307 16:24:43.673990 139958446696192 logging_writer.py:48] [110900] global_step=110900, grad_norm=2.6739144325256348, loss=1.4440017938613892
I0307 16:25:23.738177 139958455088896 logging_writer.py:48] [111000] global_step=111000, grad_norm=2.7051844596862793, loss=1.3868815898895264
I0307 16:26:03.604964 139958446696192 logging_writer.py:48] [111100] global_step=111100, grad_norm=2.6000213623046875, loss=1.380508303642273
I0307 16:26:43.192487 139958455088896 logging_writer.py:48] [111200] global_step=111200, grad_norm=2.3777647018432617, loss=1.2074573040008545
I0307 16:27:23.027173 139958446696192 logging_writer.py:48] [111300] global_step=111300, grad_norm=2.636298656463623, loss=1.375022292137146
2025-03-07 16:27:32.923874: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 16:28:03.346629 139958455088896 logging_writer.py:48] [111400] global_step=111400, grad_norm=2.6196625232696533, loss=1.2658305168151855
I0307 16:28:42.983307 139958446696192 logging_writer.py:48] [111500] global_step=111500, grad_norm=2.605823040008545, loss=1.2920732498168945
I0307 16:29:22.858673 139958455088896 logging_writer.py:48] [111600] global_step=111600, grad_norm=2.677503824234009, loss=1.387621283531189
I0307 16:30:03.754140 139958446696192 logging_writer.py:48] [111700] global_step=111700, grad_norm=2.5956172943115234, loss=1.2292859554290771
I0307 16:30:44.207379 139958455088896 logging_writer.py:48] [111800] global_step=111800, grad_norm=2.6113181114196777, loss=1.2352193593978882
I0307 16:31:24.491395 139958446696192 logging_writer.py:48] [111900] global_step=111900, grad_norm=2.5505807399749756, loss=1.3186115026474
I0307 16:32:05.076804 139958455088896 logging_writer.py:48] [112000] global_step=112000, grad_norm=3.168403387069702, loss=1.3501768112182617
I0307 16:32:31.309140 140114851837120 spec.py:321] Evaluating on the training split.
I0307 16:32:44.253433 140114851837120 spec.py:333] Evaluating on the validation split.
I0307 16:32:56.806654 140114851837120 spec.py:349] Evaluating on the test split.
I0307 16:32:58.628158 140114851837120 submission_runner.py:469] Time since start: 48169.97s, 	Step: 112066, 	{'train/accuracy': 0.8172432780265808, 'train/loss': 0.6669880747795105, 'validation/accuracy': 0.6976000070571899, 'validation/loss': 1.2342274188995361, 'validation/num_examples': 50000, 'test/accuracy': 0.5746000409126282, 'test/loss': 1.9357752799987793, 'test/num_examples': 10000, 'score': 44931.23804187775, 'total_duration': 48169.96520256996, 'accumulated_submission_time': 44931.23804187775, 'accumulated_eval_time': 3215.659420967102, 'accumulated_logging_time': 10.406671524047852}
I0307 16:32:58.734478 139958446696192 logging_writer.py:48] [112066] accumulated_eval_time=3215.66, accumulated_logging_time=10.4067, accumulated_submission_time=44931.2, global_step=112066, preemption_count=0, score=44931.2, test/accuracy=0.5746, test/loss=1.93578, test/num_examples=10000, total_duration=48170, train/accuracy=0.817243, train/loss=0.666988, validation/accuracy=0.6976, validation/loss=1.23423, validation/num_examples=50000
I0307 16:33:12.676491 139958455088896 logging_writer.py:48] [112100] global_step=112100, grad_norm=2.7584590911865234, loss=1.3617182970046997
I0307 16:33:52.712318 139958446696192 logging_writer.py:48] [112200] global_step=112200, grad_norm=2.588561534881592, loss=1.3410428762435913
I0307 16:34:33.040758 139958455088896 logging_writer.py:48] [112300] global_step=112300, grad_norm=2.788327693939209, loss=1.2333850860595703
I0307 16:35:13.341739 139958446696192 logging_writer.py:48] [112400] global_step=112400, grad_norm=3.047409772872925, loss=1.264121651649475
I0307 16:35:53.890328 139958455088896 logging_writer.py:48] [112500] global_step=112500, grad_norm=2.3560569286346436, loss=1.3082250356674194
2025-03-07 16:36:24.150447: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 16:36:34.239870 139958446696192 logging_writer.py:48] [112600] global_step=112600, grad_norm=2.478090286254883, loss=1.3649702072143555
I0307 16:37:14.222146 139958455088896 logging_writer.py:48] [112700] global_step=112700, grad_norm=2.388617992401123, loss=1.3204361200332642
I0307 16:37:54.198786 139958446696192 logging_writer.py:48] [112800] global_step=112800, grad_norm=2.6040751934051514, loss=1.2438160181045532
I0307 16:38:34.668847 139958455088896 logging_writer.py:48] [112900] global_step=112900, grad_norm=2.665808916091919, loss=1.2931638956069946
I0307 16:39:14.853539 139958446696192 logging_writer.py:48] [113000] global_step=113000, grad_norm=3.0298385620117188, loss=1.3753582239151
I0307 16:39:55.293041 139958455088896 logging_writer.py:48] [113100] global_step=113100, grad_norm=2.5428361892700195, loss=1.1780723333358765
I0307 16:40:35.327306 139958446696192 logging_writer.py:48] [113200] global_step=113200, grad_norm=2.5967040061950684, loss=1.2398967742919922
I0307 16:41:15.958113 139958455088896 logging_writer.py:48] [113300] global_step=113300, grad_norm=2.627087116241455, loss=1.2293179035186768
I0307 16:41:28.739434 140114851837120 spec.py:321] Evaluating on the training split.
I0307 16:41:41.330629 140114851837120 spec.py:333] Evaluating on the validation split.
I0307 16:41:53.850624 140114851837120 spec.py:349] Evaluating on the test split.
I0307 16:41:55.678269 140114851837120 submission_runner.py:469] Time since start: 48707.02s, 	Step: 113332, 	{'train/accuracy': 0.8114436864852905, 'train/loss': 0.6976011395454407, 'validation/accuracy': 0.6915000081062317, 'validation/loss': 1.2647820711135864, 'validation/num_examples': 50000, 'test/accuracy': 0.5711000561714172, 'test/loss': 1.9702421426773071, 'test/num_examples': 10000, 'score': 45441.060131549835, 'total_duration': 48707.01532649994, 'accumulated_submission_time': 45441.060131549835, 'accumulated_eval_time': 3242.598057746887, 'accumulated_logging_time': 10.541358470916748}
I0307 16:41:55.788803 139958446696192 logging_writer.py:48] [113332] accumulated_eval_time=3242.6, accumulated_logging_time=10.5414, accumulated_submission_time=45441.1, global_step=113332, preemption_count=0, score=45441.1, test/accuracy=0.5711, test/loss=1.97024, test/num_examples=10000, total_duration=48707, train/accuracy=0.811444, train/loss=0.697601, validation/accuracy=0.6915, validation/loss=1.26478, validation/num_examples=50000
I0307 16:42:23.628541 139958455088896 logging_writer.py:48] [113400] global_step=113400, grad_norm=2.4818131923675537, loss=1.3288558721542358
I0307 16:43:03.401382 139958446696192 logging_writer.py:48] [113500] global_step=113500, grad_norm=2.4267349243164062, loss=1.196141242980957
I0307 16:43:43.612393 139958455088896 logging_writer.py:48] [113600] global_step=113600, grad_norm=2.576812505722046, loss=1.224280595779419
I0307 16:44:23.556342 139958446696192 logging_writer.py:48] [113700] global_step=113700, grad_norm=2.7682809829711914, loss=1.3013657331466675
I0307 16:45:03.568434 139958455088896 logging_writer.py:48] [113800] global_step=113800, grad_norm=2.66107177734375, loss=1.3883856534957886
2025-03-07 16:45:14.069844: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 16:45:43.911345 139958446696192 logging_writer.py:48] [113900] global_step=113900, grad_norm=2.7183737754821777, loss=1.2669345140457153
I0307 16:46:24.026092 139958455088896 logging_writer.py:48] [114000] global_step=114000, grad_norm=2.704697370529175, loss=1.3113102912902832
I0307 16:47:04.500037 139958446696192 logging_writer.py:48] [114100] global_step=114100, grad_norm=2.4746224880218506, loss=1.2227389812469482
I0307 16:47:44.365662 139958455088896 logging_writer.py:48] [114200] global_step=114200, grad_norm=2.7377917766571045, loss=1.2037699222564697
I0307 16:48:24.335762 139958446696192 logging_writer.py:48] [114300] global_step=114300, grad_norm=2.8864870071411133, loss=1.3372788429260254
I0307 16:49:04.873070 139958455088896 logging_writer.py:48] [114400] global_step=114400, grad_norm=2.746241807937622, loss=1.3019708395004272
I0307 16:49:45.490915 139958446696192 logging_writer.py:48] [114500] global_step=114500, grad_norm=2.7251391410827637, loss=1.3775849342346191
I0307 16:50:25.966783 139958455088896 logging_writer.py:48] [114600] global_step=114600, grad_norm=2.6809275150299072, loss=1.261305332183838
I0307 16:50:25.978497 140114851837120 spec.py:321] Evaluating on the training split.
I0307 16:50:38.719674 140114851837120 spec.py:333] Evaluating on the validation split.
I0307 16:50:51.289897 140114851837120 spec.py:349] Evaluating on the test split.
I0307 16:50:53.110255 140114851837120 submission_runner.py:469] Time since start: 49244.45s, 	Step: 114601, 	{'train/accuracy': 0.8248764276504517, 'train/loss': 0.6415936350822449, 'validation/accuracy': 0.6953799724578857, 'validation/loss': 1.2429535388946533, 'validation/num_examples': 50000, 'test/accuracy': 0.5675000548362732, 'test/loss': 1.982709527015686, 'test/num_examples': 10000, 'score': 45951.060353040695, 'total_duration': 49244.447271585464, 'accumulated_submission_time': 45951.060353040695, 'accumulated_eval_time': 3269.7295668125153, 'accumulated_logging_time': 10.687488317489624}
I0307 16:50:53.200127 139958446696192 logging_writer.py:48] [114601] accumulated_eval_time=3269.73, accumulated_logging_time=10.6875, accumulated_submission_time=45951.1, global_step=114601, preemption_count=0, score=45951.1, test/accuracy=0.5675, test/loss=1.98271, test/num_examples=10000, total_duration=49244.4, train/accuracy=0.824876, train/loss=0.641594, validation/accuracy=0.69538, validation/loss=1.24295, validation/num_examples=50000
I0307 16:51:33.726876 139958455088896 logging_writer.py:48] [114700] global_step=114700, grad_norm=2.7476465702056885, loss=1.25164794921875
I0307 16:52:13.731062 139958446696192 logging_writer.py:48] [114800] global_step=114800, grad_norm=2.7671918869018555, loss=1.2527189254760742
I0307 16:52:53.553097 139958455088896 logging_writer.py:48] [114900] global_step=114900, grad_norm=2.545152425765991, loss=1.3598713874816895
I0307 16:53:33.680491 139958446696192 logging_writer.py:48] [115000] global_step=115000, grad_norm=2.601973056793213, loss=1.2218308448791504
2025-03-07 16:54:05.054238: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 16:54:14.230712 139958455088896 logging_writer.py:48] [115100] global_step=115100, grad_norm=2.973522424697876, loss=1.3033984899520874
I0307 16:54:54.200487 139958446696192 logging_writer.py:48] [115200] global_step=115200, grad_norm=2.8207266330718994, loss=1.3490033149719238
I0307 16:55:34.467061 139958455088896 logging_writer.py:48] [115300] global_step=115300, grad_norm=2.7754108905792236, loss=1.3428514003753662
I0307 16:56:14.642925 139958446696192 logging_writer.py:48] [115400] global_step=115400, grad_norm=2.6701738834381104, loss=1.3987442255020142
I0307 16:56:54.215365 139958455088896 logging_writer.py:48] [115500] global_step=115500, grad_norm=2.88759446144104, loss=1.386682152748108
I0307 16:57:34.726053 139958446696192 logging_writer.py:48] [115600] global_step=115600, grad_norm=2.5971992015838623, loss=1.24066162109375
I0307 16:58:15.373209 139958455088896 logging_writer.py:48] [115700] global_step=115700, grad_norm=2.6832501888275146, loss=1.1957130432128906
I0307 16:58:55.731622 139958446696192 logging_writer.py:48] [115800] global_step=115800, grad_norm=2.7493929862976074, loss=1.2800061702728271
I0307 16:59:23.321317 140114851837120 spec.py:321] Evaluating on the training split.
I0307 16:59:36.077997 140114851837120 spec.py:333] Evaluating on the validation split.
I0307 16:59:48.545869 140114851837120 spec.py:349] Evaluating on the test split.
I0307 16:59:50.363761 140114851837120 submission_runner.py:469] Time since start: 49781.70s, 	Step: 115870, 	{'train/accuracy': 0.8361766338348389, 'train/loss': 0.597150444984436, 'validation/accuracy': 0.70278000831604, 'validation/loss': 1.209159255027771, 'validation/num_examples': 50000, 'test/accuracy': 0.5736000537872314, 'test/loss': 1.946031093597412, 'test/num_examples': 10000, 'score': 46460.99710583687, 'total_duration': 49781.700803518295, 'accumulated_submission_time': 46460.99710583687, 'accumulated_eval_time': 3296.7717957496643, 'accumulated_logging_time': 10.807193756103516}
I0307 16:59:50.523761 139958455088896 logging_writer.py:48] [115870] accumulated_eval_time=3296.77, accumulated_logging_time=10.8072, accumulated_submission_time=46461, global_step=115870, preemption_count=0, score=46461, test/accuracy=0.5736, test/loss=1.94603, test/num_examples=10000, total_duration=49781.7, train/accuracy=0.836177, train/loss=0.59715, validation/accuracy=0.70278, validation/loss=1.20916, validation/num_examples=50000
I0307 17:00:03.054986 139958446696192 logging_writer.py:48] [115900] global_step=115900, grad_norm=2.7105371952056885, loss=1.2632670402526855
I0307 17:00:43.334204 139958455088896 logging_writer.py:48] [116000] global_step=116000, grad_norm=2.6568362712860107, loss=1.2164174318313599
I0307 17:01:23.341155 139958446696192 logging_writer.py:48] [116100] global_step=116100, grad_norm=2.663684129714966, loss=1.2471152544021606
I0307 17:02:03.992151 139958455088896 logging_writer.py:48] [116200] global_step=116200, grad_norm=2.665618658065796, loss=1.1751797199249268
I0307 17:02:44.192835 139958446696192 logging_writer.py:48] [116300] global_step=116300, grad_norm=2.7923924922943115, loss=1.1571282148361206
2025-03-07 17:02:55.984029: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 17:03:25.138759 139958455088896 logging_writer.py:48] [116400] global_step=116400, grad_norm=2.804243564605713, loss=1.3498642444610596
I0307 17:04:05.336488 139958446696192 logging_writer.py:48] [116500] global_step=116500, grad_norm=3.0423882007598877, loss=1.3103711605072021
I0307 17:04:45.540392 139958455088896 logging_writer.py:48] [116600] global_step=116600, grad_norm=2.944411039352417, loss=1.185988187789917
I0307 17:05:25.665784 139958446696192 logging_writer.py:48] [116700] global_step=116700, grad_norm=2.6688618659973145, loss=1.2020044326782227
I0307 17:06:05.606208 139958455088896 logging_writer.py:48] [116800] global_step=116800, grad_norm=2.671544313430786, loss=1.2679072618484497
I0307 17:06:45.711242 139958446696192 logging_writer.py:48] [116900] global_step=116900, grad_norm=2.9223830699920654, loss=1.205629825592041
I0307 17:07:26.052662 139958455088896 logging_writer.py:48] [117000] global_step=117000, grad_norm=2.9769787788391113, loss=1.2641117572784424
I0307 17:08:06.348985 139958446696192 logging_writer.py:48] [117100] global_step=117100, grad_norm=2.5072407722473145, loss=1.2116928100585938
I0307 17:08:20.653248 140114851837120 spec.py:321] Evaluating on the training split.
I0307 17:08:33.564700 140114851837120 spec.py:333] Evaluating on the validation split.
I0307 17:08:46.077341 140114851837120 spec.py:349] Evaluating on the test split.
I0307 17:08:47.903588 140114851837120 submission_runner.py:469] Time since start: 50319.24s, 	Step: 117137, 	{'train/accuracy': 0.8185586333274841, 'train/loss': 0.6665962934494019, 'validation/accuracy': 0.6997599601745605, 'validation/loss': 1.2240430116653442, 'validation/num_examples': 50000, 'test/accuracy': 0.5723000168800354, 'test/loss': 1.9356892108917236, 'test/num_examples': 10000, 'score': 46970.93736863136, 'total_duration': 50319.24065065384, 'accumulated_submission_time': 46970.93736863136, 'accumulated_eval_time': 3324.021938562393, 'accumulated_logging_time': 11.00585150718689}
I0307 17:08:47.980538 139958455088896 logging_writer.py:48] [117137] accumulated_eval_time=3324.02, accumulated_logging_time=11.0059, accumulated_submission_time=46970.9, global_step=117137, preemption_count=0, score=46970.9, test/accuracy=0.5723, test/loss=1.93569, test/num_examples=10000, total_duration=50319.2, train/accuracy=0.818559, train/loss=0.666596, validation/accuracy=0.69976, validation/loss=1.22404, validation/num_examples=50000
I0307 17:09:14.116733 139958446696192 logging_writer.py:48] [117200] global_step=117200, grad_norm=3.045832395553589, loss=1.2553904056549072
I0307 17:09:54.633938 139958455088896 logging_writer.py:48] [117300] global_step=117300, grad_norm=3.4012441635131836, loss=1.3170522451400757
I0307 17:10:34.898123 139958446696192 logging_writer.py:48] [117400] global_step=117400, grad_norm=3.397097587585449, loss=1.3151686191558838
I0307 17:11:15.283941 139958455088896 logging_writer.py:48] [117500] global_step=117500, grad_norm=2.733363628387451, loss=1.1800551414489746
2025-03-07 17:11:47.802391: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 17:11:55.954057 139958446696192 logging_writer.py:48] [117600] global_step=117600, grad_norm=2.5965425968170166, loss=1.1573338508605957
I0307 17:12:36.101881 139958455088896 logging_writer.py:48] [117700] global_step=117700, grad_norm=3.071012496948242, loss=1.2738797664642334
I0307 17:13:16.232145 139958446696192 logging_writer.py:48] [117800] global_step=117800, grad_norm=2.878843069076538, loss=1.2525728940963745
I0307 17:13:56.455348 139958455088896 logging_writer.py:48] [117900] global_step=117900, grad_norm=2.9474098682403564, loss=1.24373197555542
I0307 17:14:36.588778 139958446696192 logging_writer.py:48] [118000] global_step=118000, grad_norm=2.868797779083252, loss=1.293466329574585
I0307 17:15:16.781277 139958455088896 logging_writer.py:48] [118100] global_step=118100, grad_norm=2.658888339996338, loss=1.1711888313293457
I0307 17:15:57.004414 139958446696192 logging_writer.py:48] [118200] global_step=118200, grad_norm=2.8764820098876953, loss=1.2122210264205933
I0307 17:16:37.316926 139958455088896 logging_writer.py:48] [118300] global_step=118300, grad_norm=2.612410068511963, loss=1.1797635555267334
I0307 17:17:18.097463 139958446696192 logging_writer.py:48] [118400] global_step=118400, grad_norm=2.814584493637085, loss=1.2262766361236572
I0307 17:17:18.155832 140114851837120 spec.py:321] Evaluating on the training split.
I0307 17:17:30.893230 140114851837120 spec.py:333] Evaluating on the validation split.
I0307 17:17:43.485788 140114851837120 spec.py:349] Evaluating on the test split.
I0307 17:17:45.305372 140114851837120 submission_runner.py:469] Time since start: 50856.64s, 	Step: 118401, 	{'train/accuracy': 0.8016581535339355, 'train/loss': 0.7241121530532837, 'validation/accuracy': 0.7034199833869934, 'validation/loss': 1.2237718105316162, 'validation/num_examples': 50000, 'test/accuracy': 0.5711000561714172, 'test/loss': 1.9605035781860352, 'test/num_examples': 10000, 'score': 47480.93937563896, 'total_duration': 50856.642407655716, 'accumulated_submission_time': 47480.93937563896, 'accumulated_eval_time': 3351.1712486743927, 'accumulated_logging_time': 11.105561017990112}
I0307 17:17:45.384060 139958455088896 logging_writer.py:48] [118401] accumulated_eval_time=3351.17, accumulated_logging_time=11.1056, accumulated_submission_time=47480.9, global_step=118401, preemption_count=0, score=47480.9, test/accuracy=0.5711, test/loss=1.9605, test/num_examples=10000, total_duration=50856.6, train/accuracy=0.801658, train/loss=0.724112, validation/accuracy=0.70342, validation/loss=1.22377, validation/num_examples=50000
I0307 17:18:25.431228 139958446696192 logging_writer.py:48] [118500] global_step=118500, grad_norm=2.980583906173706, loss=1.3029820919036865
I0307 17:19:05.125699 139958455088896 logging_writer.py:48] [118600] global_step=118600, grad_norm=2.7336325645446777, loss=1.2935739755630493
I0307 17:19:45.388176 139958446696192 logging_writer.py:48] [118700] global_step=118700, grad_norm=2.9355814456939697, loss=1.3226172924041748
I0307 17:20:25.273122 139958455088896 logging_writer.py:48] [118800] global_step=118800, grad_norm=3.721351385116577, loss=1.190791368484497
2025-03-07 17:20:37.891896: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 17:21:05.483244 139958446696192 logging_writer.py:48] [118900] global_step=118900, grad_norm=2.6819007396698, loss=1.2219359874725342
I0307 17:21:45.321823 139958455088896 logging_writer.py:48] [119000] global_step=119000, grad_norm=2.644831418991089, loss=1.1118191480636597
I0307 17:22:25.321554 139958446696192 logging_writer.py:48] [119100] global_step=119100, grad_norm=3.1322975158691406, loss=1.3567602634429932
I0307 17:23:05.640807 139958455088896 logging_writer.py:48] [119200] global_step=119200, grad_norm=2.759747266769409, loss=1.1938581466674805
I0307 17:23:45.388225 139958446696192 logging_writer.py:48] [119300] global_step=119300, grad_norm=2.703993082046509, loss=1.2734227180480957
I0307 17:24:25.635060 139958455088896 logging_writer.py:48] [119400] global_step=119400, grad_norm=2.969726800918579, loss=1.1738815307617188
I0307 17:25:05.554925 139958446696192 logging_writer.py:48] [119500] global_step=119500, grad_norm=2.8642802238464355, loss=1.2430033683776855
I0307 17:25:45.717617 139958455088896 logging_writer.py:48] [119600] global_step=119600, grad_norm=2.733550786972046, loss=1.2247933149337769
I0307 17:26:15.386881 140114851837120 spec.py:321] Evaluating on the training split.
I0307 17:26:27.895136 140114851837120 spec.py:333] Evaluating on the validation split.
I0307 17:26:40.368146 140114851837120 spec.py:349] Evaluating on the test split.
I0307 17:26:42.152308 140114851837120 submission_runner.py:469] Time since start: 51393.49s, 	Step: 119674, 	{'train/accuracy': 0.800203263759613, 'train/loss': 0.7269607186317444, 'validation/accuracy': 0.7064399719238281, 'validation/loss': 1.198792815208435, 'validation/num_examples': 50000, 'test/accuracy': 0.5806000232696533, 'test/loss': 1.9103903770446777, 'test/num_examples': 10000, 'score': 47990.759323596954, 'total_duration': 51393.48934221268, 'accumulated_submission_time': 47990.759323596954, 'accumulated_eval_time': 3377.936456680298, 'accumulated_logging_time': 11.215101718902588}
I0307 17:26:42.268822 139958446696192 logging_writer.py:48] [119674] accumulated_eval_time=3377.94, accumulated_logging_time=11.2151, accumulated_submission_time=47990.8, global_step=119674, preemption_count=0, score=47990.8, test/accuracy=0.5806, test/loss=1.91039, test/num_examples=10000, total_duration=51393.5, train/accuracy=0.800203, train/loss=0.726961, validation/accuracy=0.70644, validation/loss=1.19879, validation/num_examples=50000
I0307 17:26:53.103432 139958455088896 logging_writer.py:48] [119700] global_step=119700, grad_norm=2.782876968383789, loss=1.1158263683319092
I0307 17:27:33.402562 139958446696192 logging_writer.py:48] [119800] global_step=119800, grad_norm=2.665264129638672, loss=1.2528395652770996
I0307 17:28:13.658542 139958455088896 logging_writer.py:48] [119900] global_step=119900, grad_norm=2.774954080581665, loss=1.2726764678955078
I0307 17:28:54.009577 139958446696192 logging_writer.py:48] [120000] global_step=120000, grad_norm=2.9384641647338867, loss=1.2230350971221924
2025-03-07 17:29:27.581274: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 17:29:34.661037 139958455088896 logging_writer.py:48] [120100] global_step=120100, grad_norm=3.108231544494629, loss=1.2913206815719604
I0307 17:30:15.757303 139958446696192 logging_writer.py:48] [120200] global_step=120200, grad_norm=3.078293561935425, loss=1.212583303451538
I0307 17:30:56.014389 139958455088896 logging_writer.py:48] [120300] global_step=120300, grad_norm=3.0556182861328125, loss=1.220681071281433
I0307 17:31:36.361816 139958446696192 logging_writer.py:48] [120400] global_step=120400, grad_norm=2.969719886779785, loss=1.2212834358215332
I0307 17:32:16.816712 139958455088896 logging_writer.py:48] [120500] global_step=120500, grad_norm=3.080341100692749, loss=1.2249600887298584
I0307 17:32:57.132079 139958446696192 logging_writer.py:48] [120600] global_step=120600, grad_norm=2.8815805912017822, loss=1.1965763568878174
I0307 17:33:37.772740 139958455088896 logging_writer.py:48] [120700] global_step=120700, grad_norm=3.02519154548645, loss=1.2626222372055054
I0307 17:34:18.164081 139958446696192 logging_writer.py:48] [120800] global_step=120800, grad_norm=2.6680593490600586, loss=1.201368808746338
I0307 17:34:58.436605 139958455088896 logging_writer.py:48] [120900] global_step=120900, grad_norm=2.9488894939422607, loss=1.2679120302200317
I0307 17:35:12.504365 140114851837120 spec.py:321] Evaluating on the training split.
I0307 17:35:25.037274 140114851837120 spec.py:333] Evaluating on the validation split.
I0307 17:35:37.476321 140114851837120 spec.py:349] Evaluating on the test split.
I0307 17:35:39.282459 140114851837120 submission_runner.py:469] Time since start: 51930.62s, 	Step: 120936, 	{'train/accuracy': 0.8026745915412903, 'train/loss': 0.7251353859901428, 'validation/accuracy': 0.704759955406189, 'validation/loss': 1.1900979280471802, 'validation/num_examples': 50000, 'test/accuracy': 0.5764000415802002, 'test/loss': 1.916110634803772, 'test/num_examples': 10000, 'score': 48500.78496360779, 'total_duration': 51930.61951708794, 'accumulated_submission_time': 48500.78496360779, 'accumulated_eval_time': 3404.714349269867, 'accumulated_logging_time': 11.390337944030762}
I0307 17:35:39.369933 139958446696192 logging_writer.py:48] [120936] accumulated_eval_time=3404.71, accumulated_logging_time=11.3903, accumulated_submission_time=48500.8, global_step=120936, preemption_count=0, score=48500.8, test/accuracy=0.5764, test/loss=1.91611, test/num_examples=10000, total_duration=51930.6, train/accuracy=0.802675, train/loss=0.725135, validation/accuracy=0.70476, validation/loss=1.1901, validation/num_examples=50000
I0307 17:36:06.022525 139958455088896 logging_writer.py:48] [121000] global_step=121000, grad_norm=2.865844488143921, loss=1.1394729614257812
I0307 17:36:46.241764 139958446696192 logging_writer.py:48] [121100] global_step=121100, grad_norm=2.8355488777160645, loss=1.243817925453186
I0307 17:37:27.355665 139958455088896 logging_writer.py:48] [121200] global_step=121200, grad_norm=3.0122263431549072, loss=1.2271796464920044
I0307 17:38:07.840651 139958446696192 logging_writer.py:48] [121300] global_step=121300, grad_norm=2.707287311553955, loss=1.191938877105713
2025-03-07 17:38:21.415549: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 17:38:47.459795 139958455088896 logging_writer.py:48] [121400] global_step=121400, grad_norm=2.8699140548706055, loss=1.3083045482635498
I0307 17:39:27.812870 139958446696192 logging_writer.py:48] [121500] global_step=121500, grad_norm=2.864671230316162, loss=1.3010329008102417
I0307 17:40:08.129052 139958455088896 logging_writer.py:48] [121600] global_step=121600, grad_norm=3.0806126594543457, loss=1.2296582460403442
I0307 17:40:48.195480 139958446696192 logging_writer.py:48] [121700] global_step=121700, grad_norm=2.722132921218872, loss=1.2094721794128418
I0307 17:41:28.807725 139958455088896 logging_writer.py:48] [121800] global_step=121800, grad_norm=3.164090871810913, loss=1.3253880739212036
I0307 17:42:09.330066 139958446696192 logging_writer.py:48] [121900] global_step=121900, grad_norm=3.13588809967041, loss=1.3286277055740356
I0307 17:42:49.113395 139958455088896 logging_writer.py:48] [122000] global_step=122000, grad_norm=2.6402688026428223, loss=1.1334068775177002
I0307 17:43:29.312828 139958446696192 logging_writer.py:48] [122100] global_step=122100, grad_norm=2.994255781173706, loss=1.1854782104492188
I0307 17:44:09.109152 139958455088896 logging_writer.py:48] [122200] global_step=122200, grad_norm=2.991696357727051, loss=1.2106316089630127
I0307 17:44:09.471166 140114851837120 spec.py:321] Evaluating on the training split.
I0307 17:44:22.275691 140114851837120 spec.py:333] Evaluating on the validation split.
I0307 17:44:34.794357 140114851837120 spec.py:349] Evaluating on the test split.
I0307 17:44:36.631574 140114851837120 submission_runner.py:469] Time since start: 52467.97s, 	Step: 122202, 	{'train/accuracy': 0.7899991869926453, 'train/loss': 0.7714759707450867, 'validation/accuracy': 0.6962400078773499, 'validation/loss': 1.2297719717025757, 'validation/num_examples': 50000, 'test/accuracy': 0.5662000179290771, 'test/loss': 1.9854764938354492, 'test/num_examples': 10000, 'score': 49010.68418121338, 'total_duration': 52467.968636751175, 'accumulated_submission_time': 49010.68418121338, 'accumulated_eval_time': 3431.8745787143707, 'accumulated_logging_time': 11.527794361114502}
I0307 17:44:36.747079 139958446696192 logging_writer.py:48] [122202] accumulated_eval_time=3431.87, accumulated_logging_time=11.5278, accumulated_submission_time=49010.7, global_step=122202, preemption_count=0, score=49010.7, test/accuracy=0.5662, test/loss=1.98548, test/num_examples=10000, total_duration=52468, train/accuracy=0.789999, train/loss=0.771476, validation/accuracy=0.69624, validation/loss=1.22977, validation/num_examples=50000
I0307 17:45:16.339736 139958455088896 logging_writer.py:48] [122300] global_step=122300, grad_norm=2.7737927436828613, loss=1.1048694849014282
I0307 17:45:56.487276 139958446696192 logging_writer.py:48] [122400] global_step=122400, grad_norm=2.8762171268463135, loss=1.1922123432159424
I0307 17:46:36.370202 139958455088896 logging_writer.py:48] [122500] global_step=122500, grad_norm=2.938030958175659, loss=1.2298246622085571
2025-03-07 17:47:10.304805: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 17:47:16.567856 139958446696192 logging_writer.py:48] [122600] global_step=122600, grad_norm=3.253610849380493, loss=1.297623872756958
I0307 17:47:56.812425 139958455088896 logging_writer.py:48] [122700] global_step=122700, grad_norm=2.719505786895752, loss=1.0996952056884766
I0307 17:48:36.952948 139958446696192 logging_writer.py:48] [122800] global_step=122800, grad_norm=2.8536460399627686, loss=1.20894455909729
I0307 17:49:16.492650 139958455088896 logging_writer.py:48] [122900] global_step=122900, grad_norm=2.68803334236145, loss=1.1218146085739136
I0307 17:49:56.404252 139958446696192 logging_writer.py:48] [123000] global_step=123000, grad_norm=2.6867828369140625, loss=1.2098350524902344
I0307 17:50:36.340112 139958455088896 logging_writer.py:48] [123100] global_step=123100, grad_norm=3.08964467048645, loss=1.3103582859039307
I0307 17:51:16.594701 139958446696192 logging_writer.py:48] [123200] global_step=123200, grad_norm=2.8725063800811768, loss=1.2048438787460327
I0307 17:51:56.812777 139958455088896 logging_writer.py:48] [123300] global_step=123300, grad_norm=3.1102569103240967, loss=1.3208101987838745
I0307 17:52:36.835556 139958446696192 logging_writer.py:48] [123400] global_step=123400, grad_norm=3.0631940364837646, loss=1.2243632078170776
I0307 17:53:07.061002 140114851837120 spec.py:321] Evaluating on the training split.
I0307 17:53:19.931676 140114851837120 spec.py:333] Evaluating on the validation split.
I0307 17:53:32.481958 140114851837120 spec.py:349] Evaluating on the test split.
I0307 17:53:34.317535 140114851837120 submission_runner.py:469] Time since start: 53005.65s, 	Step: 123477, 	{'train/accuracy': 0.8029336333274841, 'train/loss': 0.7145070433616638, 'validation/accuracy': 0.7053599953651428, 'validation/loss': 1.1961065530776978, 'validation/num_examples': 50000, 'test/accuracy': 0.5788000226020813, 'test/loss': 1.9366860389709473, 'test/num_examples': 10000, 'score': 49520.81037211418, 'total_duration': 53005.65456581116, 'accumulated_submission_time': 49520.81037211418, 'accumulated_eval_time': 3459.1308851242065, 'accumulated_logging_time': 11.67727518081665}
I0307 17:53:34.383061 139958455088896 logging_writer.py:48] [123477] accumulated_eval_time=3459.13, accumulated_logging_time=11.6773, accumulated_submission_time=49520.8, global_step=123477, preemption_count=0, score=49520.8, test/accuracy=0.5788, test/loss=1.93669, test/num_examples=10000, total_duration=53005.7, train/accuracy=0.802934, train/loss=0.714507, validation/accuracy=0.70536, validation/loss=1.19611, validation/num_examples=50000
I0307 17:53:44.137756 139958446696192 logging_writer.py:48] [123500] global_step=123500, grad_norm=2.891087532043457, loss=1.1749848127365112
I0307 17:54:24.663496 139958455088896 logging_writer.py:48] [123600] global_step=123600, grad_norm=3.265821695327759, loss=1.2251869440078735
I0307 17:55:05.005627 139958446696192 logging_writer.py:48] [123700] global_step=123700, grad_norm=2.872702121734619, loss=1.2272329330444336
I0307 17:55:45.188588 139958455088896 logging_writer.py:48] [123800] global_step=123800, grad_norm=3.033715009689331, loss=1.24894118309021
2025-03-07 17:55:59.794014: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 17:56:25.663374 139958446696192 logging_writer.py:48] [123900] global_step=123900, grad_norm=2.7421629428863525, loss=1.2094610929489136
I0307 17:57:05.760468 139958455088896 logging_writer.py:48] [124000] global_step=124000, grad_norm=3.0023863315582275, loss=1.3314181566238403
I0307 17:57:45.711783 139958446696192 logging_writer.py:48] [124100] global_step=124100, grad_norm=3.0050840377807617, loss=1.1853322982788086
I0307 17:58:25.164028 139958455088896 logging_writer.py:48] [124200] global_step=124200, grad_norm=3.569791555404663, loss=1.3527705669403076
I0307 17:59:05.170046 139958446696192 logging_writer.py:48] [124300] global_step=124300, grad_norm=3.0683505535125732, loss=1.245130181312561
I0307 17:59:45.159433 139958455088896 logging_writer.py:48] [124400] global_step=124400, grad_norm=3.0842342376708984, loss=1.1883864402770996
I0307 18:00:24.942730 139958446696192 logging_writer.py:48] [124500] global_step=124500, grad_norm=2.9426119327545166, loss=1.163285493850708
I0307 18:01:05.158066 139958455088896 logging_writer.py:48] [124600] global_step=124600, grad_norm=2.9348268508911133, loss=1.2031910419464111
I0307 18:01:45.541380 139958446696192 logging_writer.py:48] [124700] global_step=124700, grad_norm=3.0109195709228516, loss=1.1483445167541504
I0307 18:02:04.362010 140114851837120 spec.py:321] Evaluating on the training split.
I0307 18:02:16.918981 140114851837120 spec.py:333] Evaluating on the validation split.
I0307 18:02:29.410911 140114851837120 spec.py:349] Evaluating on the test split.
I0307 18:02:31.253790 140114851837120 submission_runner.py:469] Time since start: 53542.59s, 	Step: 124748, 	{'train/accuracy': 0.79691481590271, 'train/loss': 0.724306046962738, 'validation/accuracy': 0.7020800113677979, 'validation/loss': 1.2199281454086304, 'validation/num_examples': 50000, 'test/accuracy': 0.5723000168800354, 'test/loss': 1.958957314491272, 'test/num_examples': 10000, 'score': 50030.59896612167, 'total_duration': 53542.59084534645, 'accumulated_submission_time': 50030.59896612167, 'accumulated_eval_time': 3486.0224640369415, 'accumulated_logging_time': 11.781381845474243}
I0307 18:02:31.352684 139958455088896 logging_writer.py:48] [124748] accumulated_eval_time=3486.02, accumulated_logging_time=11.7814, accumulated_submission_time=50030.6, global_step=124748, preemption_count=0, score=50030.6, test/accuracy=0.5723, test/loss=1.95896, test/num_examples=10000, total_duration=53542.6, train/accuracy=0.796915, train/loss=0.724306, validation/accuracy=0.70208, validation/loss=1.21993, validation/num_examples=50000
I0307 18:02:52.388816 139958446696192 logging_writer.py:48] [124800] global_step=124800, grad_norm=2.8072562217712402, loss=1.1084866523742676
I0307 18:03:33.017360 139958455088896 logging_writer.py:48] [124900] global_step=124900, grad_norm=2.9470815658569336, loss=1.2943328619003296
I0307 18:04:13.429233 139958446696192 logging_writer.py:48] [125000] global_step=125000, grad_norm=3.0836119651794434, loss=1.2000991106033325
2025-03-07 18:04:47.978384: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 18:04:53.457102 139958455088896 logging_writer.py:48] [125100] global_step=125100, grad_norm=2.854360818862915, loss=1.1820858716964722
I0307 18:05:33.748567 139958446696192 logging_writer.py:48] [125200] global_step=125200, grad_norm=3.137657642364502, loss=1.2923855781555176
I0307 18:06:14.048793 139958455088896 logging_writer.py:48] [125300] global_step=125300, grad_norm=2.910827398300171, loss=1.2235162258148193
I0307 18:06:54.062386 139958446696192 logging_writer.py:48] [125400] global_step=125400, grad_norm=3.168875217437744, loss=1.2303322553634644
I0307 18:07:34.275450 139958455088896 logging_writer.py:48] [125500] global_step=125500, grad_norm=3.0041818618774414, loss=1.2100008726119995
I0307 18:08:14.837404 139958446696192 logging_writer.py:48] [125600] global_step=125600, grad_norm=3.0143232345581055, loss=1.1401934623718262
I0307 18:08:54.748265 139958455088896 logging_writer.py:48] [125700] global_step=125700, grad_norm=3.1081464290618896, loss=1.2516945600509644
I0307 18:09:34.760627 139958446696192 logging_writer.py:48] [125800] global_step=125800, grad_norm=2.931037664413452, loss=1.1180284023284912
I0307 18:10:14.623991 139958455088896 logging_writer.py:48] [125900] global_step=125900, grad_norm=3.201249122619629, loss=1.1398751735687256
I0307 18:10:54.550082 139958446696192 logging_writer.py:48] [126000] global_step=126000, grad_norm=2.991466522216797, loss=1.1911413669586182
I0307 18:11:01.387452 140114851837120 spec.py:321] Evaluating on the training split.
I0307 18:11:14.490563 140114851837120 spec.py:333] Evaluating on the validation split.
I0307 18:11:27.064594 140114851837120 spec.py:349] Evaluating on the test split.
I0307 18:11:28.916534 140114851837120 submission_runner.py:469] Time since start: 54080.25s, 	Step: 126018, 	{'train/accuracy': 0.7934669852256775, 'train/loss': 0.7466307282447815, 'validation/accuracy': 0.6970199942588806, 'validation/loss': 1.2577674388885498, 'validation/num_examples': 50000, 'test/accuracy': 0.5732000470161438, 'test/loss': 1.9762790203094482, 'test/num_examples': 10000, 'score': 50540.44911456108, 'total_duration': 54080.25355267525, 'accumulated_submission_time': 50540.44911456108, 'accumulated_eval_time': 3513.5513110160828, 'accumulated_logging_time': 11.91389274597168}
I0307 18:11:29.072705 139958455088896 logging_writer.py:48] [126018] accumulated_eval_time=3513.55, accumulated_logging_time=11.9139, accumulated_submission_time=50540.4, global_step=126018, preemption_count=0, score=50540.4, test/accuracy=0.5732, test/loss=1.97628, test/num_examples=10000, total_duration=54080.3, train/accuracy=0.793467, train/loss=0.746631, validation/accuracy=0.69702, validation/loss=1.25777, validation/num_examples=50000
I0307 18:12:02.280207 139958446696192 logging_writer.py:48] [126100] global_step=126100, grad_norm=2.9951438903808594, loss=1.2344141006469727
I0307 18:12:42.573705 139958455088896 logging_writer.py:48] [126200] global_step=126200, grad_norm=3.420369863510132, loss=1.1535911560058594
I0307 18:13:22.375917 139958446696192 logging_writer.py:48] [126300] global_step=126300, grad_norm=3.012430429458618, loss=1.1893479824066162
2025-03-07 18:13:37.758783: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 18:14:02.956967 139958455088896 logging_writer.py:48] [126400] global_step=126400, grad_norm=2.868997812271118, loss=1.1554845571517944
I0307 18:14:43.126141 139958446696192 logging_writer.py:48] [126500] global_step=126500, grad_norm=2.9438536167144775, loss=1.153306007385254
I0307 18:15:23.661696 139958455088896 logging_writer.py:48] [126600] global_step=126600, grad_norm=2.984187602996826, loss=1.219393014907837
I0307 18:16:03.902686 139958446696192 logging_writer.py:48] [126700] global_step=126700, grad_norm=2.770888566970825, loss=1.1249494552612305
I0307 18:16:43.929278 139958455088896 logging_writer.py:48] [126800] global_step=126800, grad_norm=2.8086392879486084, loss=1.1398719549179077
I0307 18:17:24.349710 139958446696192 logging_writer.py:48] [126900] global_step=126900, grad_norm=3.133518695831299, loss=1.1640818119049072
I0307 18:18:04.688517 139958455088896 logging_writer.py:48] [127000] global_step=127000, grad_norm=2.850269079208374, loss=1.1838325262069702
I0307 18:18:44.703482 139958446696192 logging_writer.py:48] [127100] global_step=127100, grad_norm=3.063711166381836, loss=1.2156320810317993
I0307 18:19:25.048050 139958455088896 logging_writer.py:48] [127200] global_step=127200, grad_norm=2.910449504852295, loss=1.116062879562378
I0307 18:19:59.165282 140114851837120 spec.py:321] Evaluating on the training split.
I0307 18:20:11.871051 140114851837120 spec.py:333] Evaluating on the validation split.
I0307 18:20:24.447389 140114851837120 spec.py:349] Evaluating on the test split.
I0307 18:20:26.309261 140114851837120 submission_runner.py:469] Time since start: 54617.65s, 	Step: 127286, 	{'train/accuracy': 0.808613657951355, 'train/loss': 0.69827800989151, 'validation/accuracy': 0.7039399743080139, 'validation/loss': 1.1903411149978638, 'validation/num_examples': 50000, 'test/accuracy': 0.58160001039505, 'test/loss': 1.9108591079711914, 'test/num_examples': 10000, 'score': 51050.360624074936, 'total_duration': 54617.64628458023, 'accumulated_submission_time': 51050.360624074936, 'accumulated_eval_time': 3540.695063352585, 'accumulated_logging_time': 12.097843170166016}
I0307 18:20:26.416472 139958446696192 logging_writer.py:48] [127286] accumulated_eval_time=3540.7, accumulated_logging_time=12.0978, accumulated_submission_time=51050.4, global_step=127286, preemption_count=0, score=51050.4, test/accuracy=0.5816, test/loss=1.91086, test/num_examples=10000, total_duration=54617.6, train/accuracy=0.808614, train/loss=0.698278, validation/accuracy=0.70394, validation/loss=1.19034, validation/num_examples=50000
I0307 18:20:32.606851 139958455088896 logging_writer.py:48] [127300] global_step=127300, grad_norm=3.266637086868286, loss=1.2356653213500977
I0307 18:21:12.696173 139958446696192 logging_writer.py:48] [127400] global_step=127400, grad_norm=3.165576457977295, loss=1.016408085823059
I0307 18:21:53.119828 139958455088896 logging_writer.py:48] [127500] global_step=127500, grad_norm=3.581057071685791, loss=1.233743667602539
2025-03-07 18:22:29.211666: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 18:22:33.532816 139958446696192 logging_writer.py:48] [127600] global_step=127600, grad_norm=2.8626346588134766, loss=1.129878282546997
I0307 18:23:13.854860 139958455088896 logging_writer.py:48] [127700] global_step=127700, grad_norm=3.0221147537231445, loss=1.1674039363861084
I0307 18:23:53.978077 139958446696192 logging_writer.py:48] [127800] global_step=127800, grad_norm=2.9485738277435303, loss=1.1621779203414917
I0307 18:24:33.782431 139958455088896 logging_writer.py:48] [127900] global_step=127900, grad_norm=3.208129405975342, loss=1.1782094240188599
I0307 18:25:13.793568 139958446696192 logging_writer.py:48] [128000] global_step=128000, grad_norm=3.3073627948760986, loss=1.1651989221572876
I0307 18:25:54.145521 139958455088896 logging_writer.py:48] [128100] global_step=128100, grad_norm=3.025757074356079, loss=1.1273123025894165
I0307 18:26:34.411849 139958446696192 logging_writer.py:48] [128200] global_step=128200, grad_norm=2.904442071914673, loss=1.1432100534439087
I0307 18:27:14.582320 139958455088896 logging_writer.py:48] [128300] global_step=128300, grad_norm=3.017329216003418, loss=1.1724889278411865
I0307 18:27:54.874043 139958446696192 logging_writer.py:48] [128400] global_step=128400, grad_norm=3.8183839321136475, loss=1.2559707164764404
I0307 18:28:34.693163 139958455088896 logging_writer.py:48] [128500] global_step=128500, grad_norm=3.102588176727295, loss=1.142594575881958
I0307 18:28:56.338890 140114851837120 spec.py:321] Evaluating on the training split.
I0307 18:29:08.933375 140114851837120 spec.py:333] Evaluating on the validation split.
I0307 18:29:21.615564 140114851837120 spec.py:349] Evaluating on the test split.
I0307 18:29:23.474535 140114851837120 submission_runner.py:469] Time since start: 55154.81s, 	Step: 128555, 	{'train/accuracy': 0.8170041441917419, 'train/loss': 0.6612815856933594, 'validation/accuracy': 0.7135599851608276, 'validation/loss': 1.1651833057403564, 'validation/num_examples': 50000, 'test/accuracy': 0.5886000394821167, 'test/loss': 1.8988839387893677, 'test/num_examples': 10000, 'score': 51560.09570503235, 'total_duration': 55154.81159615517, 'accumulated_submission_time': 51560.09570503235, 'accumulated_eval_time': 3567.8305106163025, 'accumulated_logging_time': 12.241714000701904}
I0307 18:29:23.605980 139958446696192 logging_writer.py:48] [128555] accumulated_eval_time=3567.83, accumulated_logging_time=12.2417, accumulated_submission_time=51560.1, global_step=128555, preemption_count=0, score=51560.1, test/accuracy=0.5886, test/loss=1.89888, test/num_examples=10000, total_duration=55154.8, train/accuracy=0.817004, train/loss=0.661282, validation/accuracy=0.71356, validation/loss=1.16518, validation/num_examples=50000
I0307 18:29:42.258551 139958455088896 logging_writer.py:48] [128600] global_step=128600, grad_norm=3.050856828689575, loss=1.1537420749664307
I0307 18:30:22.834881 139958446696192 logging_writer.py:48] [128700] global_step=128700, grad_norm=3.0866429805755615, loss=1.1492528915405273
I0307 18:31:03.112567 139958455088896 logging_writer.py:48] [128800] global_step=128800, grad_norm=3.189682960510254, loss=1.1727666854858398
2025-03-07 18:31:19.383920: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 18:31:43.388618 139958446696192 logging_writer.py:48] [128900] global_step=128900, grad_norm=2.8461546897888184, loss=1.117661952972412
I0307 18:32:23.798994 139958455088896 logging_writer.py:48] [129000] global_step=129000, grad_norm=3.2342145442962646, loss=1.2108638286590576
I0307 18:33:03.313110 139958446696192 logging_writer.py:48] [129100] global_step=129100, grad_norm=2.979412078857422, loss=1.209221363067627
I0307 18:33:43.563136 139958455088896 logging_writer.py:48] [129200] global_step=129200, grad_norm=3.028395414352417, loss=1.201352834701538
I0307 18:34:24.388905 139958446696192 logging_writer.py:48] [129300] global_step=129300, grad_norm=3.00492262840271, loss=1.227197289466858
I0307 18:35:04.475601 139958455088896 logging_writer.py:48] [129400] global_step=129400, grad_norm=3.0880932807922363, loss=1.2446062564849854
I0307 18:35:44.931246 139958446696192 logging_writer.py:48] [129500] global_step=129500, grad_norm=3.286208391189575, loss=1.2001488208770752
I0307 18:36:25.117719 139958455088896 logging_writer.py:48] [129600] global_step=129600, grad_norm=3.151939630508423, loss=1.0976933240890503
I0307 18:37:05.167413 139958446696192 logging_writer.py:48] [129700] global_step=129700, grad_norm=3.3469715118408203, loss=1.114134430885315
I0307 18:37:45.477986 139958455088896 logging_writer.py:48] [129800] global_step=129800, grad_norm=3.239304780960083, loss=1.1299638748168945
I0307 18:37:53.609169 140114851837120 spec.py:321] Evaluating on the training split.
I0307 18:38:06.545469 140114851837120 spec.py:333] Evaluating on the validation split.
I0307 18:38:19.044298 140114851837120 spec.py:349] Evaluating on the test split.
I0307 18:38:20.893575 140114851837120 submission_runner.py:469] Time since start: 55692.23s, 	Step: 129821, 	{'train/accuracy': 0.8074178695678711, 'train/loss': 0.6945946216583252, 'validation/accuracy': 0.7057200074195862, 'validation/loss': 1.1962287425994873, 'validation/num_examples': 50000, 'test/accuracy': 0.5779000520706177, 'test/loss': 1.9390509128570557, 'test/num_examples': 10000, 'score': 52069.90982294083, 'total_duration': 55692.23065185547, 'accumulated_submission_time': 52069.90982294083, 'accumulated_eval_time': 3595.114734888077, 'accumulated_logging_time': 12.415164232254028}
I0307 18:38:21.068298 139958446696192 logging_writer.py:48] [129821] accumulated_eval_time=3595.11, accumulated_logging_time=12.4152, accumulated_submission_time=52069.9, global_step=129821, preemption_count=0, score=52069.9, test/accuracy=0.5779, test/loss=1.93905, test/num_examples=10000, total_duration=55692.2, train/accuracy=0.807418, train/loss=0.694595, validation/accuracy=0.70572, validation/loss=1.19623, validation/num_examples=50000
I0307 18:38:53.661535 139958455088896 logging_writer.py:48] [129900] global_step=129900, grad_norm=2.8414342403411865, loss=1.0962613821029663
I0307 18:39:34.108798 139958446696192 logging_writer.py:48] [130000] global_step=130000, grad_norm=3.1323959827423096, loss=1.140785813331604
2025-03-07 18:40:10.761801: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 18:40:14.069122 139958455088896 logging_writer.py:48] [130100] global_step=130100, grad_norm=3.175647735595703, loss=1.120509147644043
I0307 18:40:53.972926 139958446696192 logging_writer.py:48] [130200] global_step=130200, grad_norm=2.9829251766204834, loss=1.0048911571502686
I0307 18:41:34.061373 139958455088896 logging_writer.py:48] [130300] global_step=130300, grad_norm=3.222602605819702, loss=1.1379387378692627
I0307 18:42:14.581735 139958446696192 logging_writer.py:48] [130400] global_step=130400, grad_norm=3.1828973293304443, loss=1.186320185661316
I0307 18:42:54.391197 139958455088896 logging_writer.py:48] [130500] global_step=130500, grad_norm=3.1566219329833984, loss=1.1528483629226685
I0307 18:43:34.369745 139958446696192 logging_writer.py:48] [130600] global_step=130600, grad_norm=3.003323793411255, loss=1.1090807914733887
I0307 18:44:14.056337 139958455088896 logging_writer.py:48] [130700] global_step=130700, grad_norm=3.0677874088287354, loss=1.1954786777496338
I0307 18:44:53.983475 139958446696192 logging_writer.py:48] [130800] global_step=130800, grad_norm=3.1548283100128174, loss=1.0893750190734863
I0307 18:45:34.329130 139958455088896 logging_writer.py:48] [130900] global_step=130900, grad_norm=3.158816337585449, loss=1.1083242893218994
I0307 18:46:14.134796 139958446696192 logging_writer.py:48] [131000] global_step=131000, grad_norm=3.302762031555176, loss=1.2117013931274414
I0307 18:46:50.967590 140114851837120 spec.py:321] Evaluating on the training split.
I0307 18:47:03.885820 140114851837120 spec.py:333] Evaluating on the validation split.
I0307 18:47:16.404992 140114851837120 spec.py:349] Evaluating on the test split.
I0307 18:47:18.242485 140114851837120 submission_runner.py:469] Time since start: 56229.58s, 	Step: 131093, 	{'train/accuracy': 0.8235211968421936, 'train/loss': 0.6234328746795654, 'validation/accuracy': 0.7139399647712708, 'validation/loss': 1.1620432138442993, 'validation/num_examples': 50000, 'test/accuracy': 0.5890000462532043, 'test/loss': 1.864998698234558, 'test/num_examples': 10000, 'score': 52579.62643289566, 'total_duration': 56229.579538583755, 'accumulated_submission_time': 52579.62643289566, 'accumulated_eval_time': 3622.389446258545, 'accumulated_logging_time': 12.6214280128479}
I0307 18:47:18.344559 139958455088896 logging_writer.py:48] [131093] accumulated_eval_time=3622.39, accumulated_logging_time=12.6214, accumulated_submission_time=52579.6, global_step=131093, preemption_count=0, score=52579.6, test/accuracy=0.589, test/loss=1.865, test/num_examples=10000, total_duration=56229.6, train/accuracy=0.823521, train/loss=0.623433, validation/accuracy=0.71394, validation/loss=1.16204, validation/num_examples=50000
I0307 18:47:21.697900 139958446696192 logging_writer.py:48] [131100] global_step=131100, grad_norm=3.3207056522369385, loss=1.226027250289917
I0307 18:48:02.132310 139958455088896 logging_writer.py:48] [131200] global_step=131200, grad_norm=3.083951473236084, loss=1.1584742069244385
I0307 18:48:41.703270 139958446696192 logging_writer.py:48] [131300] global_step=131300, grad_norm=3.2490763664245605, loss=1.0891813039779663
2025-03-07 18:48:59.066966: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 18:49:22.464531 139958455088896 logging_writer.py:48] [131400] global_step=131400, grad_norm=3.2212517261505127, loss=1.1369991302490234
I0307 18:50:02.760451 139958446696192 logging_writer.py:48] [131500] global_step=131500, grad_norm=3.0810492038726807, loss=1.1580723524093628
I0307 18:50:43.047203 139958455088896 logging_writer.py:48] [131600] global_step=131600, grad_norm=3.305366277694702, loss=1.1807990074157715
I0307 18:51:23.399905 139958446696192 logging_writer.py:48] [131700] global_step=131700, grad_norm=3.5473523139953613, loss=1.0900136232376099
I0307 18:52:03.441765 139958455088896 logging_writer.py:48] [131800] global_step=131800, grad_norm=3.1130783557891846, loss=1.1896151304244995
I0307 18:52:43.391486 139958446696192 logging_writer.py:48] [131900] global_step=131900, grad_norm=3.2525999546051025, loss=1.1190063953399658
I0307 18:53:23.422146 139958455088896 logging_writer.py:48] [132000] global_step=132000, grad_norm=3.034473180770874, loss=1.0438123941421509
I0307 18:54:03.715887 139958446696192 logging_writer.py:48] [132100] global_step=132100, grad_norm=3.123868942260742, loss=1.1578311920166016
I0307 18:54:43.897757 139958455088896 logging_writer.py:48] [132200] global_step=132200, grad_norm=3.500349283218384, loss=1.1489229202270508
I0307 18:55:23.684708 139958446696192 logging_writer.py:48] [132300] global_step=132300, grad_norm=3.133209466934204, loss=1.2160438299179077
I0307 18:55:48.613253 140114851837120 spec.py:321] Evaluating on the training split.
I0307 18:56:01.456807 140114851837120 spec.py:333] Evaluating on the validation split.
I0307 18:56:13.988115 140114851837120 spec.py:349] Evaluating on the test split.
I0307 18:56:15.843916 140114851837120 submission_runner.py:469] Time since start: 56767.18s, 	Step: 132363, 	{'train/accuracy': 0.823660671710968, 'train/loss': 0.6290507316589355, 'validation/accuracy': 0.7130599617958069, 'validation/loss': 1.1777054071426392, 'validation/num_examples': 50000, 'test/accuracy': 0.5835000276565552, 'test/loss': 1.928019642829895, 'test/num_examples': 10000, 'score': 53089.719497442245, 'total_duration': 56767.1809694767, 'accumulated_submission_time': 53089.719497442245, 'accumulated_eval_time': 3649.6199090480804, 'accumulated_logging_time': 12.750117778778076}
I0307 18:56:15.992856 139958455088896 logging_writer.py:48] [132363] accumulated_eval_time=3649.62, accumulated_logging_time=12.7501, accumulated_submission_time=53089.7, global_step=132363, preemption_count=0, score=53089.7, test/accuracy=0.5835, test/loss=1.92802, test/num_examples=10000, total_duration=56767.2, train/accuracy=0.823661, train/loss=0.629051, validation/accuracy=0.71306, validation/loss=1.17771, validation/num_examples=50000
I0307 18:56:31.437901 139958446696192 logging_writer.py:48] [132400] global_step=132400, grad_norm=3.1761820316314697, loss=1.068373441696167
I0307 18:57:11.459163 139958455088896 logging_writer.py:48] [132500] global_step=132500, grad_norm=3.1775856018066406, loss=1.1232963800430298
2025-03-07 18:57:49.117407: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 18:57:51.522193 139958446696192 logging_writer.py:48] [132600] global_step=132600, grad_norm=3.2284414768218994, loss=1.205110788345337
I0307 18:58:31.849291 139958455088896 logging_writer.py:48] [132700] global_step=132700, grad_norm=3.4110848903656006, loss=1.092348575592041
I0307 18:59:11.378924 139958446696192 logging_writer.py:48] [132800] global_step=132800, grad_norm=3.2854394912719727, loss=1.188441514968872
I0307 18:59:51.397218 139958455088896 logging_writer.py:48] [132900] global_step=132900, grad_norm=3.3269855976104736, loss=1.1543105840682983
I0307 19:00:31.572901 139958446696192 logging_writer.py:48] [133000] global_step=133000, grad_norm=3.1945345401763916, loss=1.1722540855407715
I0307 19:01:11.570559 139958455088896 logging_writer.py:48] [133100] global_step=133100, grad_norm=3.383169412612915, loss=1.18850839138031
I0307 19:01:51.841284 139958446696192 logging_writer.py:48] [133200] global_step=133200, grad_norm=3.3520009517669678, loss=1.0878968238830566
I0307 19:02:31.647243 139958455088896 logging_writer.py:48] [133300] global_step=133300, grad_norm=2.957223892211914, loss=1.0679740905761719
I0307 19:03:11.850876 139958446696192 logging_writer.py:48] [133400] global_step=133400, grad_norm=3.219480037689209, loss=1.1503276824951172
I0307 19:03:52.005304 139958455088896 logging_writer.py:48] [133500] global_step=133500, grad_norm=3.3548645973205566, loss=1.17988920211792
I0307 19:04:32.247395 139958446696192 logging_writer.py:48] [133600] global_step=133600, grad_norm=3.3986477851867676, loss=1.2345565557479858
I0307 19:04:45.993106 140114851837120 spec.py:321] Evaluating on the training split.
I0307 19:04:58.885005 140114851837120 spec.py:333] Evaluating on the validation split.
I0307 19:05:11.368900 140114851837120 spec.py:349] Evaluating on the test split.
I0307 19:05:13.217071 140114851837120 submission_runner.py:469] Time since start: 57304.55s, 	Step: 133635, 	{'train/accuracy': 0.8267697691917419, 'train/loss': 0.6224292516708374, 'validation/accuracy': 0.7147200107574463, 'validation/loss': 1.1556929349899292, 'validation/num_examples': 50000, 'test/accuracy': 0.5840000510215759, 'test/loss': 1.8953264951705933, 'test/num_examples': 10000, 'score': 53599.53695297241, 'total_duration': 57304.55410575867, 'accumulated_submission_time': 53599.53695297241, 'accumulated_eval_time': 3676.84366106987, 'accumulated_logging_time': 12.932210683822632}
I0307 19:05:13.300780 139958455088896 logging_writer.py:48] [133635] accumulated_eval_time=3676.84, accumulated_logging_time=12.9322, accumulated_submission_time=53599.5, global_step=133635, preemption_count=0, score=53599.5, test/accuracy=0.584, test/loss=1.89533, test/num_examples=10000, total_duration=57304.6, train/accuracy=0.82677, train/loss=0.622429, validation/accuracy=0.71472, validation/loss=1.15569, validation/num_examples=50000
I0307 19:05:39.556265 139958446696192 logging_writer.py:48] [133700] global_step=133700, grad_norm=3.0162949562072754, loss=1.072645664215088
I0307 19:06:19.632441 139958455088896 logging_writer.py:48] [133800] global_step=133800, grad_norm=3.1263186931610107, loss=1.0441508293151855
2025-03-07 19:06:38.306916: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 19:07:00.516073 139958446696192 logging_writer.py:48] [133900] global_step=133900, grad_norm=3.158052921295166, loss=1.152838945388794
I0307 19:07:41.151765 139958455088896 logging_writer.py:48] [134000] global_step=134000, grad_norm=3.361823081970215, loss=1.1101878881454468
I0307 19:08:21.628382 139958446696192 logging_writer.py:48] [134100] global_step=134100, grad_norm=3.365906000137329, loss=1.051782488822937
I0307 19:09:02.029160 139958455088896 logging_writer.py:48] [134200] global_step=134200, grad_norm=3.108710289001465, loss=1.0825128555297852
I0307 19:09:42.426079 139958446696192 logging_writer.py:48] [134300] global_step=134300, grad_norm=3.350403070449829, loss=1.1478962898254395
I0307 19:10:22.569430 139958455088896 logging_writer.py:48] [134400] global_step=134400, grad_norm=3.0984199047088623, loss=1.1563682556152344
I0307 19:11:02.574930 139958446696192 logging_writer.py:48] [134500] global_step=134500, grad_norm=3.2180886268615723, loss=1.1421838998794556
I0307 19:11:43.268304 139958455088896 logging_writer.py:48] [134600] global_step=134600, grad_norm=3.2068898677825928, loss=1.1705883741378784
I0307 19:12:23.589495 139958446696192 logging_writer.py:48] [134700] global_step=134700, grad_norm=3.3514089584350586, loss=1.1524673700332642
I0307 19:13:04.356839 139958455088896 logging_writer.py:48] [134800] global_step=134800, grad_norm=3.2446367740631104, loss=1.1606817245483398
I0307 19:13:43.446006 140114851837120 spec.py:321] Evaluating on the training split.
I0307 19:13:56.519764 140114851837120 spec.py:333] Evaluating on the validation split.
I0307 19:14:09.083974 140114851837120 spec.py:349] Evaluating on the test split.
I0307 19:14:10.949180 140114851837120 submission_runner.py:469] Time since start: 57842.29s, 	Step: 134898, 	{'train/accuracy': 0.8265505433082581, 'train/loss': 0.6189639568328857, 'validation/accuracy': 0.7134999632835388, 'validation/loss': 1.1628515720367432, 'validation/num_examples': 50000, 'test/accuracy': 0.5850000381469727, 'test/loss': 1.8954309225082397, 'test/num_examples': 10000, 'score': 54109.48800945282, 'total_duration': 57842.28623962402, 'accumulated_submission_time': 54109.48800945282, 'accumulated_eval_time': 3704.346643447876, 'accumulated_logging_time': 13.056741714477539}
I0307 19:14:11.096613 139958446696192 logging_writer.py:48] [134898] accumulated_eval_time=3704.35, accumulated_logging_time=13.0567, accumulated_submission_time=54109.5, global_step=134898, preemption_count=0, score=54109.5, test/accuracy=0.585, test/loss=1.89543, test/num_examples=10000, total_duration=57842.3, train/accuracy=0.826551, train/loss=0.618964, validation/accuracy=0.7135, validation/loss=1.16285, validation/num_examples=50000
I0307 19:14:12.192036 139958455088896 logging_writer.py:48] [134900] global_step=134900, grad_norm=3.2195661067962646, loss=1.1382036209106445
I0307 19:14:52.654145 139958446696192 logging_writer.py:48] [135000] global_step=135000, grad_norm=3.166156768798828, loss=1.0500777959823608
2025-03-07 19:15:31.552427: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 19:15:33.012532 139958455088896 logging_writer.py:48] [135100] global_step=135100, grad_norm=3.2247300148010254, loss=1.0938671827316284
I0307 19:16:13.804088 139958446696192 logging_writer.py:48] [135200] global_step=135200, grad_norm=3.4949147701263428, loss=1.2265543937683105
I0307 19:16:54.276753 139958455088896 logging_writer.py:48] [135300] global_step=135300, grad_norm=3.0951554775238037, loss=1.045167088508606
I0307 19:17:34.698506 139958446696192 logging_writer.py:48] [135400] global_step=135400, grad_norm=2.90065598487854, loss=1.0718133449554443
I0307 19:18:15.528033 139958455088896 logging_writer.py:48] [135500] global_step=135500, grad_norm=3.446308135986328, loss=1.1307728290557861
I0307 19:18:56.700074 139958446696192 logging_writer.py:48] [135600] global_step=135600, grad_norm=3.0803239345550537, loss=0.9835848808288574
I0307 19:19:37.547487 139958455088896 logging_writer.py:48] [135700] global_step=135700, grad_norm=3.2534122467041016, loss=1.1060490608215332
I0307 19:20:18.170422 139958446696192 logging_writer.py:48] [135800] global_step=135800, grad_norm=3.2934391498565674, loss=1.0360746383666992
I0307 19:20:58.363379 139958455088896 logging_writer.py:48] [135900] global_step=135900, grad_norm=3.5135278701782227, loss=1.0778695344924927
I0307 19:21:38.716243 139958446696192 logging_writer.py:48] [136000] global_step=136000, grad_norm=3.298462390899658, loss=1.1377606391906738
I0307 19:22:19.461044 139958455088896 logging_writer.py:48] [136100] global_step=136100, grad_norm=3.4379262924194336, loss=1.085571527481079
I0307 19:22:41.085040 140114851837120 spec.py:321] Evaluating on the training split.
I0307 19:22:53.538683 140114851837120 spec.py:333] Evaluating on the validation split.
I0307 19:23:05.972072 140114851837120 spec.py:349] Evaluating on the test split.
I0307 19:23:07.792409 140114851837120 submission_runner.py:469] Time since start: 58379.13s, 	Step: 136154, 	{'train/accuracy': 0.8370336294174194, 'train/loss': 0.5735855102539062, 'validation/accuracy': 0.7195000052452087, 'validation/loss': 1.154090404510498, 'validation/num_examples': 50000, 'test/accuracy': 0.6008000373840332, 'test/loss': 1.8812024593353271, 'test/num_examples': 10000, 'score': 54619.31276035309, 'total_duration': 58379.12945151329, 'accumulated_submission_time': 54619.31276035309, 'accumulated_eval_time': 3731.0538036823273, 'accumulated_logging_time': 13.213061094284058}
I0307 19:23:07.890152 139958446696192 logging_writer.py:48] [136154] accumulated_eval_time=3731.05, accumulated_logging_time=13.2131, accumulated_submission_time=54619.3, global_step=136154, preemption_count=0, score=54619.3, test/accuracy=0.6008, test/loss=1.8812, test/num_examples=10000, total_duration=58379.1, train/accuracy=0.837034, train/loss=0.573586, validation/accuracy=0.7195, validation/loss=1.15409, validation/num_examples=50000
I0307 19:23:26.939903 139958455088896 logging_writer.py:48] [136200] global_step=136200, grad_norm=3.3768277168273926, loss=1.1161171197891235
I0307 19:24:07.404529 139958446696192 logging_writer.py:48] [136300] global_step=136300, grad_norm=3.392627716064453, loss=1.1077625751495361
2025-03-07 19:24:26.511401: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 19:24:47.584533 139958455088896 logging_writer.py:48] [136400] global_step=136400, grad_norm=3.2404441833496094, loss=1.0503380298614502
I0307 19:25:28.486566 139958446696192 logging_writer.py:48] [136500] global_step=136500, grad_norm=3.3411314487457275, loss=1.1535685062408447
I0307 19:26:08.698132 139958455088896 logging_writer.py:48] [136600] global_step=136600, grad_norm=3.261404514312744, loss=1.2039586305618286
I0307 19:26:48.345964 139958446696192 logging_writer.py:48] [136700] global_step=136700, grad_norm=3.4316139221191406, loss=1.1043199300765991
I0307 19:27:28.629565 139958455088896 logging_writer.py:48] [136800] global_step=136800, grad_norm=3.341780424118042, loss=1.100520133972168
I0307 19:28:09.062121 139958446696192 logging_writer.py:48] [136900] global_step=136900, grad_norm=3.258995532989502, loss=1.0418347120285034
I0307 19:28:49.315225 139958455088896 logging_writer.py:48] [137000] global_step=137000, grad_norm=3.5287134647369385, loss=1.1154303550720215
I0307 19:29:29.907978 139958446696192 logging_writer.py:48] [137100] global_step=137100, grad_norm=3.4708902835845947, loss=1.0632727146148682
I0307 19:30:09.900559 139958455088896 logging_writer.py:48] [137200] global_step=137200, grad_norm=3.083096504211426, loss=1.0101120471954346
I0307 19:30:50.488776 139958446696192 logging_writer.py:48] [137300] global_step=137300, grad_norm=3.5969078540802, loss=1.0684258937835693
I0307 19:31:31.404404 139958455088896 logging_writer.py:48] [137400] global_step=137400, grad_norm=3.32667875289917, loss=1.023057222366333
I0307 19:31:38.188046 140114851837120 spec.py:321] Evaluating on the training split.
I0307 19:31:50.843862 140114851837120 spec.py:333] Evaluating on the validation split.
I0307 19:32:03.276255 140114851837120 spec.py:349] Evaluating on the test split.
I0307 19:32:05.083658 140114851837120 submission_runner.py:469] Time since start: 58916.42s, 	Step: 137417, 	{'train/accuracy': 0.8367546200752258, 'train/loss': 0.5768634080886841, 'validation/accuracy': 0.7145400047302246, 'validation/loss': 1.1788935661315918, 'validation/num_examples': 50000, 'test/accuracy': 0.5929000377655029, 'test/loss': 1.91801917552948, 'test/num_examples': 10000, 'score': 55129.43868684769, 'total_duration': 58916.420729637146, 'accumulated_submission_time': 55129.43868684769, 'accumulated_eval_time': 3757.94922375679, 'accumulated_logging_time': 13.331732273101807}
I0307 19:32:05.223361 139958446696192 logging_writer.py:48] [137417] accumulated_eval_time=3757.95, accumulated_logging_time=13.3317, accumulated_submission_time=55129.4, global_step=137417, preemption_count=0, score=55129.4, test/accuracy=0.5929, test/loss=1.91802, test/num_examples=10000, total_duration=58916.4, train/accuracy=0.836755, train/loss=0.576863, validation/accuracy=0.71454, validation/loss=1.17889, validation/num_examples=50000
I0307 19:32:39.206229 139958455088896 logging_writer.py:48] [137500] global_step=137500, grad_norm=3.2338693141937256, loss=1.0356487035751343
2025-03-07 19:33:19.442755: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 19:33:19.930094 139958446696192 logging_writer.py:48] [137600] global_step=137600, grad_norm=3.2765164375305176, loss=1.0697336196899414
I0307 19:34:00.833602 139958455088896 logging_writer.py:48] [137700] global_step=137700, grad_norm=3.666175365447998, loss=1.126571536064148
I0307 19:34:41.399354 139958446696192 logging_writer.py:48] [137800] global_step=137800, grad_norm=3.47029185295105, loss=1.1029313802719116
I0307 19:35:21.795172 139958455088896 logging_writer.py:48] [137900] global_step=137900, grad_norm=3.3369247913360596, loss=1.1106629371643066
I0307 19:36:02.408134 139958446696192 logging_writer.py:48] [138000] global_step=138000, grad_norm=3.4244275093078613, loss=1.0907870531082153
I0307 19:36:42.786568 139958455088896 logging_writer.py:48] [138100] global_step=138100, grad_norm=3.627556085586548, loss=1.099100947380066
I0307 19:37:22.896998 139958446696192 logging_writer.py:48] [138200] global_step=138200, grad_norm=3.545813798904419, loss=1.0687772035598755
I0307 19:38:03.284454 139958455088896 logging_writer.py:48] [138300] global_step=138300, grad_norm=3.394192695617676, loss=1.0438724756240845
I0307 19:38:43.481373 139958446696192 logging_writer.py:48] [138400] global_step=138400, grad_norm=3.465104818344116, loss=1.1152493953704834
I0307 19:39:24.017873 139958455088896 logging_writer.py:48] [138500] global_step=138500, grad_norm=3.6282832622528076, loss=1.0590367317199707
I0307 19:40:04.345283 139958446696192 logging_writer.py:48] [138600] global_step=138600, grad_norm=3.4948689937591553, loss=1.0369842052459717
I0307 19:40:35.427298 140114851837120 spec.py:321] Evaluating on the training split.
I0307 19:40:48.249351 140114851837120 spec.py:333] Evaluating on the validation split.
I0307 19:41:00.705800 140114851837120 spec.py:349] Evaluating on the test split.
I0307 19:41:02.547064 140114851837120 submission_runner.py:469] Time since start: 59453.88s, 	Step: 138678, 	{'train/accuracy': 0.8482341766357422, 'train/loss': 0.5301579833030701, 'validation/accuracy': 0.7227199673652649, 'validation/loss': 1.13791823387146, 'validation/num_examples': 50000, 'test/accuracy': 0.5957000255584717, 'test/loss': 1.887203574180603, 'test/num_examples': 10000, 'score': 55639.46609830856, 'total_duration': 59453.88412427902, 'accumulated_submission_time': 55639.46609830856, 'accumulated_eval_time': 3785.068791627884, 'accumulated_logging_time': 13.49641466140747}
I0307 19:41:02.656346 139958455088896 logging_writer.py:48] [138678] accumulated_eval_time=3785.07, accumulated_logging_time=13.4964, accumulated_submission_time=55639.5, global_step=138678, preemption_count=0, score=55639.5, test/accuracy=0.5957, test/loss=1.8872, test/num_examples=10000, total_duration=59453.9, train/accuracy=0.848234, train/loss=0.530158, validation/accuracy=0.72272, validation/loss=1.13792, validation/num_examples=50000
I0307 19:41:11.830186 139958446696192 logging_writer.py:48] [138700] global_step=138700, grad_norm=3.168583869934082, loss=0.9399234652519226
I0307 19:41:52.373493 139958455088896 logging_writer.py:48] [138800] global_step=138800, grad_norm=3.3121376037597656, loss=1.060532569885254
2025-03-07 19:42:12.575767: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 19:42:32.817437 139958446696192 logging_writer.py:48] [138900] global_step=138900, grad_norm=3.227531671524048, loss=1.053135871887207
I0307 19:43:13.054420 139958455088896 logging_writer.py:48] [139000] global_step=139000, grad_norm=3.6043834686279297, loss=1.131098747253418
I0307 19:43:53.546005 139958446696192 logging_writer.py:48] [139100] global_step=139100, grad_norm=3.143564462661743, loss=1.0242832899093628
I0307 19:44:33.383944 139958455088896 logging_writer.py:48] [139200] global_step=139200, grad_norm=3.275888442993164, loss=1.0303791761398315
I0307 19:45:12.882592 139958446696192 logging_writer.py:48] [139300] global_step=139300, grad_norm=3.318364143371582, loss=1.0337722301483154
I0307 19:45:53.249323 139958455088896 logging_writer.py:48] [139400] global_step=139400, grad_norm=3.311030626296997, loss=1.0013142824172974
I0307 19:46:32.829836 139958446696192 logging_writer.py:48] [139500] global_step=139500, grad_norm=3.555528163909912, loss=1.148437261581421
I0307 19:47:11.977405 139958455088896 logging_writer.py:48] [139600] global_step=139600, grad_norm=3.588487148284912, loss=1.1204575300216675
I0307 19:47:51.989891 139958446696192 logging_writer.py:48] [139700] global_step=139700, grad_norm=3.6645402908325195, loss=1.012508749961853
I0307 19:48:31.801208 139958455088896 logging_writer.py:48] [139800] global_step=139800, grad_norm=3.2632973194122314, loss=0.9929779767990112
I0307 19:49:11.929693 139958446696192 logging_writer.py:48] [139900] global_step=139900, grad_norm=3.5478150844573975, loss=1.0544437170028687
I0307 19:49:32.674648 140114851837120 spec.py:321] Evaluating on the training split.
I0307 19:49:45.569333 140114851837120 spec.py:333] Evaluating on the validation split.
I0307 19:49:58.134081 140114851837120 spec.py:349] Evaluating on the test split.
I0307 19:49:59.974457 140114851837120 submission_runner.py:469] Time since start: 59991.31s, 	Step: 139952, 	{'train/accuracy': 0.8473173975944519, 'train/loss': 0.5286949872970581, 'validation/accuracy': 0.7236999869346619, 'validation/loss': 1.1220107078552246, 'validation/num_examples': 50000, 'test/accuracy': 0.5966000556945801, 'test/loss': 1.8753621578216553, 'test/num_examples': 10000, 'score': 56149.29556059837, 'total_duration': 59991.31153130531, 'accumulated_submission_time': 56149.29556059837, 'accumulated_eval_time': 3812.368437767029, 'accumulated_logging_time': 13.639651775360107}
I0307 19:50:00.114104 139958455088896 logging_writer.py:48] [139952] accumulated_eval_time=3812.37, accumulated_logging_time=13.6397, accumulated_submission_time=56149.3, global_step=139952, preemption_count=0, score=56149.3, test/accuracy=0.5966, test/loss=1.87536, test/num_examples=10000, total_duration=59991.3, train/accuracy=0.847317, train/loss=0.528695, validation/accuracy=0.7237, validation/loss=1.12201, validation/num_examples=50000
I0307 19:50:19.865096 139958446696192 logging_writer.py:48] [140000] global_step=140000, grad_norm=3.3943047523498535, loss=1.1201512813568115
I0307 19:50:59.932817 139958455088896 logging_writer.py:48] [140100] global_step=140100, grad_norm=3.5596048831939697, loss=1.062705397605896
2025-03-07 19:51:00.192462: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 19:51:41.141796 139958446696192 logging_writer.py:48] [140200] global_step=140200, grad_norm=3.40122127532959, loss=1.000923752784729
I0307 19:52:21.923952 139958455088896 logging_writer.py:48] [140300] global_step=140300, grad_norm=3.3805651664733887, loss=0.9561313390731812
I0307 19:53:02.383253 139958446696192 logging_writer.py:48] [140400] global_step=140400, grad_norm=3.684723377227783, loss=1.116982102394104
I0307 19:53:42.813946 139958455088896 logging_writer.py:48] [140500] global_step=140500, grad_norm=3.3367056846618652, loss=1.0973312854766846
I0307 19:54:23.154219 139958446696192 logging_writer.py:48] [140600] global_step=140600, grad_norm=3.2918946743011475, loss=1.1172270774841309
I0307 19:55:03.949019 139958455088896 logging_writer.py:48] [140700] global_step=140700, grad_norm=3.4519147872924805, loss=1.1059739589691162
I0307 19:55:44.680370 139958446696192 logging_writer.py:48] [140800] global_step=140800, grad_norm=3.916203022003174, loss=1.0690315961837769
I0307 19:56:25.396716 139958455088896 logging_writer.py:48] [140900] global_step=140900, grad_norm=3.5575175285339355, loss=1.065727949142456
I0307 19:57:06.216111 139958446696192 logging_writer.py:48] [141000] global_step=141000, grad_norm=3.700713872909546, loss=0.947057843208313
I0307 19:57:46.803159 139958455088896 logging_writer.py:48] [141100] global_step=141100, grad_norm=3.6810598373413086, loss=0.9922800064086914
I0307 19:58:27.813608 139958446696192 logging_writer.py:48] [141200] global_step=141200, grad_norm=3.375286102294922, loss=1.0582681894302368
I0307 19:58:30.336045 140114851837120 spec.py:321] Evaluating on the training split.
I0307 19:58:43.187229 140114851837120 spec.py:333] Evaluating on the validation split.
I0307 19:58:55.680489 140114851837120 spec.py:349] Evaluating on the test split.
I0307 19:58:57.482476 140114851837120 submission_runner.py:469] Time since start: 60528.82s, 	Step: 141207, 	{'train/accuracy': 0.8557477593421936, 'train/loss': 0.5097410082817078, 'validation/accuracy': 0.7257399559020996, 'validation/loss': 1.1164480447769165, 'validation/num_examples': 50000, 'test/accuracy': 0.5961000323295593, 'test/loss': 1.8718180656433105, 'test/num_examples': 10000, 'score': 56659.34239411354, 'total_duration': 60528.81952023506, 'accumulated_submission_time': 56659.34239411354, 'accumulated_eval_time': 3839.5146555900574, 'accumulated_logging_time': 13.803614139556885}
I0307 19:58:57.622547 139958455088896 logging_writer.py:48] [141207] accumulated_eval_time=3839.51, accumulated_logging_time=13.8036, accumulated_submission_time=56659.3, global_step=141207, preemption_count=0, score=56659.3, test/accuracy=0.5961, test/loss=1.87182, test/num_examples=10000, total_duration=60528.8, train/accuracy=0.855748, train/loss=0.509741, validation/accuracy=0.72574, validation/loss=1.11645, validation/num_examples=50000
I0307 19:59:35.570350 139958446696192 logging_writer.py:48] [141300] global_step=141300, grad_norm=3.9099819660186768, loss=1.134312629699707
2025-03-07 19:59:56.543119: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 20:00:16.435002 139958455088896 logging_writer.py:48] [141400] global_step=141400, grad_norm=3.6660051345825195, loss=1.080832839012146
I0307 20:00:56.630066 139958446696192 logging_writer.py:48] [141500] global_step=141500, grad_norm=4.037746429443359, loss=1.1597850322723389
I0307 20:01:36.869220 139958455088896 logging_writer.py:48] [141600] global_step=141600, grad_norm=3.825946807861328, loss=1.1317429542541504
I0307 20:02:17.195449 139958446696192 logging_writer.py:48] [141700] global_step=141700, grad_norm=3.5585219860076904, loss=1.0811755657196045
I0307 20:02:57.539513 139958455088896 logging_writer.py:48] [141800] global_step=141800, grad_norm=3.4417338371276855, loss=1.0315234661102295
I0307 20:03:38.479910 139958446696192 logging_writer.py:48] [141900] global_step=141900, grad_norm=3.5992605686187744, loss=1.0589866638183594
I0307 20:04:18.822454 139958455088896 logging_writer.py:48] [142000] global_step=142000, grad_norm=3.388836145401001, loss=0.9865435361862183
I0307 20:04:59.263997 139958446696192 logging_writer.py:48] [142100] global_step=142100, grad_norm=3.5116851329803467, loss=1.0231175422668457
I0307 20:05:39.852284 139958455088896 logging_writer.py:48] [142200] global_step=142200, grad_norm=3.4440383911132812, loss=0.9776287078857422
I0307 20:06:20.040714 139958446696192 logging_writer.py:48] [142300] global_step=142300, grad_norm=3.563622236251831, loss=1.035979151725769
I0307 20:07:00.865174 139958455088896 logging_writer.py:48] [142400] global_step=142400, grad_norm=3.1902401447296143, loss=0.995769202709198
I0307 20:07:27.775587 140114851837120 spec.py:321] Evaluating on the training split.
I0307 20:07:40.853312 140114851837120 spec.py:333] Evaluating on the validation split.
I0307 20:07:53.287716 140114851837120 spec.py:349] Evaluating on the test split.
I0307 20:07:55.138803 140114851837120 submission_runner.py:469] Time since start: 61066.48s, 	Step: 142467, 	{'train/accuracy': 0.8450254797935486, 'train/loss': 0.5447291135787964, 'validation/accuracy': 0.7154399752616882, 'validation/loss': 1.1721899509429932, 'validation/num_examples': 50000, 'test/accuracy': 0.5888000130653381, 'test/loss': 1.9206199645996094, 'test/num_examples': 10000, 'score': 57169.31097364426, 'total_duration': 61066.4758272171, 'accumulated_submission_time': 57169.31097364426, 'accumulated_eval_time': 3866.8776421546936, 'accumulated_logging_time': 13.976761102676392}
I0307 20:07:55.304039 139958446696192 logging_writer.py:48] [142467] accumulated_eval_time=3866.88, accumulated_logging_time=13.9768, accumulated_submission_time=57169.3, global_step=142467, preemption_count=0, score=57169.3, test/accuracy=0.5888, test/loss=1.92062, test/num_examples=10000, total_duration=61066.5, train/accuracy=0.845025, train/loss=0.544729, validation/accuracy=0.71544, validation/loss=1.17219, validation/num_examples=50000
I0307 20:08:09.099949 139958455088896 logging_writer.py:48] [142500] global_step=142500, grad_norm=3.586808919906616, loss=1.008890151977539
I0307 20:08:49.660907 139958446696192 logging_writer.py:48] [142600] global_step=142600, grad_norm=3.777543544769287, loss=1.0185391902923584
2025-03-07 20:08:50.883862: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 20:09:30.226928 139958455088896 logging_writer.py:48] [142700] global_step=142700, grad_norm=3.615060567855835, loss=1.0262775421142578
I0307 20:10:10.799879 139958446696192 logging_writer.py:48] [142800] global_step=142800, grad_norm=3.727320671081543, loss=1.084883213043213
I0307 20:10:50.546627 139958455088896 logging_writer.py:48] [142900] global_step=142900, grad_norm=3.2574024200439453, loss=0.951209306716919
I0307 20:11:30.442555 139958446696192 logging_writer.py:48] [143000] global_step=143000, grad_norm=3.6147079467773438, loss=1.037714958190918
I0307 20:12:11.130837 139958455088896 logging_writer.py:48] [143100] global_step=143100, grad_norm=3.787721872329712, loss=1.1112655401229858
I0307 20:12:51.823321 139958446696192 logging_writer.py:48] [143200] global_step=143200, grad_norm=3.398153066635132, loss=1.095307469367981
I0307 20:13:32.446758 139958455088896 logging_writer.py:48] [143300] global_step=143300, grad_norm=3.6639747619628906, loss=1.0283927917480469
I0307 20:14:13.291929 139958446696192 logging_writer.py:48] [143400] global_step=143400, grad_norm=3.673058271408081, loss=1.0397796630859375
I0307 20:14:53.520605 139958455088896 logging_writer.py:48] [143500] global_step=143500, grad_norm=3.4913747310638428, loss=0.9678249359130859
I0307 20:15:34.411950 139958446696192 logging_writer.py:48] [143600] global_step=143600, grad_norm=3.570483684539795, loss=0.9618151187896729
I0307 20:16:15.678591 139958455088896 logging_writer.py:48] [143700] global_step=143700, grad_norm=3.57348895072937, loss=0.9491640329360962
I0307 20:16:25.500835 140114851837120 spec.py:321] Evaluating on the training split.
I0307 20:16:38.486860 140114851837120 spec.py:333] Evaluating on the validation split.
I0307 20:16:50.955524 140114851837120 spec.py:349] Evaluating on the test split.
I0307 20:16:52.795276 140114851837120 submission_runner.py:469] Time since start: 61604.13s, 	Step: 143725, 	{'train/accuracy': 0.8621252775192261, 'train/loss': 0.48647230863571167, 'validation/accuracy': 0.7249599695205688, 'validation/loss': 1.1248472929000854, 'validation/num_examples': 50000, 'test/accuracy': 0.5969000458717346, 'test/loss': 1.8661229610443115, 'test/num_examples': 10000, 'score': 57679.31414628029, 'total_duration': 61604.13234233856, 'accumulated_submission_time': 57679.31414628029, 'accumulated_eval_time': 3894.171902656555, 'accumulated_logging_time': 14.188604831695557}
I0307 20:16:52.910927 139958446696192 logging_writer.py:48] [143725] accumulated_eval_time=3894.17, accumulated_logging_time=14.1886, accumulated_submission_time=57679.3, global_step=143725, preemption_count=0, score=57679.3, test/accuracy=0.5969, test/loss=1.86612, test/num_examples=10000, total_duration=61604.1, train/accuracy=0.862125, train/loss=0.486472, validation/accuracy=0.72496, validation/loss=1.12485, validation/num_examples=50000
I0307 20:17:24.079397 139958455088896 logging_writer.py:48] [143800] global_step=143800, grad_norm=3.7050657272338867, loss=0.934415876865387
2025-03-07 20:17:46.058814: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 20:18:04.466826 139958446696192 logging_writer.py:48] [143900] global_step=143900, grad_norm=3.688462495803833, loss=0.998263955116272
I0307 20:18:44.739021 139958455088896 logging_writer.py:48] [144000] global_step=144000, grad_norm=3.5053751468658447, loss=1.0431432723999023
I0307 20:19:24.920762 139958446696192 logging_writer.py:48] [144100] global_step=144100, grad_norm=3.8013315200805664, loss=1.0216355323791504
I0307 20:20:05.364756 139958455088896 logging_writer.py:48] [144200] global_step=144200, grad_norm=3.6724886894226074, loss=1.0132291316986084
I0307 20:20:45.270276 139958446696192 logging_writer.py:48] [144300] global_step=144300, grad_norm=3.4394726753234863, loss=0.9420425891876221
I0307 20:21:25.216673 139958455088896 logging_writer.py:48] [144400] global_step=144400, grad_norm=3.723919153213501, loss=1.0415962934494019
I0307 20:22:06.218655 139958446696192 logging_writer.py:48] [144500] global_step=144500, grad_norm=3.564847469329834, loss=1.0286691188812256
I0307 20:22:46.886586 139958455088896 logging_writer.py:48] [144600] global_step=144600, grad_norm=3.420417070388794, loss=0.9848488569259644
I0307 20:23:27.215501 139958446696192 logging_writer.py:48] [144700] global_step=144700, grad_norm=3.3288724422454834, loss=0.9385930299758911
I0307 20:24:07.762682 139958455088896 logging_writer.py:48] [144800] global_step=144800, grad_norm=3.574083089828491, loss=0.9822949767112732
I0307 20:24:48.519397 139958446696192 logging_writer.py:48] [144900] global_step=144900, grad_norm=3.52221417427063, loss=0.8786890506744385
I0307 20:25:22.809296 140114851837120 spec.py:321] Evaluating on the training split.
I0307 20:25:35.843162 140114851837120 spec.py:333] Evaluating on the validation split.
I0307 20:25:48.416630 140114851837120 spec.py:349] Evaluating on the test split.
I0307 20:25:50.270984 140114851837120 submission_runner.py:469] Time since start: 62141.61s, 	Step: 144986, 	{'train/accuracy': 0.86328125, 'train/loss': 0.48145413398742676, 'validation/accuracy': 0.7263799905776978, 'validation/loss': 1.1341686248779297, 'validation/num_examples': 50000, 'test/accuracy': 0.5992000102996826, 'test/loss': 1.8745211362838745, 'test/num_examples': 10000, 'score': 58188.998012542725, 'total_duration': 62141.60804104805, 'accumulated_submission_time': 58188.998012542725, 'accumulated_eval_time': 3921.633396625519, 'accumulated_logging_time': 14.366646528244019}
I0307 20:25:50.362945 139958455088896 logging_writer.py:48] [144986] accumulated_eval_time=3921.63, accumulated_logging_time=14.3666, accumulated_submission_time=58189, global_step=144986, preemption_count=0, score=58189, test/accuracy=0.5992, test/loss=1.87452, test/num_examples=10000, total_duration=62141.6, train/accuracy=0.863281, train/loss=0.481454, validation/accuracy=0.72638, validation/loss=1.13417, validation/num_examples=50000
I0307 20:25:56.527853 139958446696192 logging_writer.py:48] [145000] global_step=145000, grad_norm=4.1895060539245605, loss=1.0656381845474243
I0307 20:26:36.717773 139958455088896 logging_writer.py:48] [145100] global_step=145100, grad_norm=3.3860270977020264, loss=1.0328627824783325
2025-03-07 20:26:38.888192: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 20:27:17.216685 139958446696192 logging_writer.py:48] [145200] global_step=145200, grad_norm=3.3137564659118652, loss=0.8884693384170532
I0307 20:27:57.858321 139958455088896 logging_writer.py:48] [145300] global_step=145300, grad_norm=3.3980705738067627, loss=0.8353527784347534
I0307 20:28:38.580808 139958446696192 logging_writer.py:48] [145400] global_step=145400, grad_norm=3.518932342529297, loss=0.9754734039306641
I0307 20:29:18.558541 139958455088896 logging_writer.py:48] [145500] global_step=145500, grad_norm=3.8230557441711426, loss=0.9266124367713928
I0307 20:29:59.245749 139958446696192 logging_writer.py:48] [145600] global_step=145600, grad_norm=3.391514539718628, loss=0.9435385465621948
I0307 20:30:39.398961 139958455088896 logging_writer.py:48] [145700] global_step=145700, grad_norm=3.4676990509033203, loss=0.943982720375061
I0307 20:31:20.055123 139958446696192 logging_writer.py:48] [145800] global_step=145800, grad_norm=3.9672892093658447, loss=0.9872801303863525
I0307 20:32:00.502005 139958455088896 logging_writer.py:48] [145900] global_step=145900, grad_norm=3.6845011711120605, loss=1.0157561302185059
I0307 20:32:40.871563 139958446696192 logging_writer.py:48] [146000] global_step=146000, grad_norm=4.139065742492676, loss=0.9251096248626709
I0307 20:33:21.527319 139958455088896 logging_writer.py:48] [146100] global_step=146100, grad_norm=3.71897029876709, loss=1.033461093902588
I0307 20:34:02.555721 139958446696192 logging_writer.py:48] [146200] global_step=146200, grad_norm=3.590770959854126, loss=0.9830236434936523
I0307 20:34:20.470282 140114851837120 spec.py:321] Evaluating on the training split.
I0307 20:34:33.108173 140114851837120 spec.py:333] Evaluating on the validation split.
I0307 20:34:45.599131 140114851837120 spec.py:349] Evaluating on the test split.
I0307 20:34:47.441496 140114851837120 submission_runner.py:469] Time since start: 62678.78s, 	Step: 146245, 	{'train/accuracy': 0.8625438213348389, 'train/loss': 0.47874048352241516, 'validation/accuracy': 0.7259199619293213, 'validation/loss': 1.1368765830993652, 'validation/num_examples': 50000, 'test/accuracy': 0.5873000025749207, 'test/loss': 1.9123883247375488, 'test/num_examples': 10000, 'score': 58698.92954111099, 'total_duration': 62678.77855491638, 'accumulated_submission_time': 58698.92954111099, 'accumulated_eval_time': 3948.604412794113, 'accumulated_logging_time': 14.484930038452148}
I0307 20:34:47.638401 139958455088896 logging_writer.py:48] [146245] accumulated_eval_time=3948.6, accumulated_logging_time=14.4849, accumulated_submission_time=58698.9, global_step=146245, preemption_count=0, score=58698.9, test/accuracy=0.5873, test/loss=1.91239, test/num_examples=10000, total_duration=62678.8, train/accuracy=0.862544, train/loss=0.47874, validation/accuracy=0.72592, validation/loss=1.13688, validation/num_examples=50000
I0307 20:35:10.414488 139958446696192 logging_writer.py:48] [146300] global_step=146300, grad_norm=3.8467397689819336, loss=0.9235705733299255
2025-03-07 20:35:33.549465: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 20:35:51.783726 139958455088896 logging_writer.py:48] [146400] global_step=146400, grad_norm=3.6573147773742676, loss=1.0178749561309814
I0307 20:36:32.134351 139958446696192 logging_writer.py:48] [146500] global_step=146500, grad_norm=3.681304931640625, loss=1.0534250736236572
I0307 20:37:12.676376 139958455088896 logging_writer.py:48] [146600] global_step=146600, grad_norm=3.796879768371582, loss=0.9748427867889404
I0307 20:37:52.867225 139958446696192 logging_writer.py:48] [146700] global_step=146700, grad_norm=3.819899797439575, loss=0.8727154731750488
I0307 20:38:33.003525 139958455088896 logging_writer.py:48] [146800] global_step=146800, grad_norm=3.840993881225586, loss=0.9457219839096069
I0307 20:39:13.461603 139958446696192 logging_writer.py:48] [146900] global_step=146900, grad_norm=3.385775566101074, loss=0.8732107281684875
I0307 20:39:53.797928 139958455088896 logging_writer.py:48] [147000] global_step=147000, grad_norm=3.822831153869629, loss=1.0393165349960327
I0307 20:40:34.454959 139958446696192 logging_writer.py:48] [147100] global_step=147100, grad_norm=3.7011704444885254, loss=1.0173720121383667
I0307 20:41:15.138563 139958455088896 logging_writer.py:48] [147200] global_step=147200, grad_norm=3.451850652694702, loss=0.9029321074485779
I0307 20:41:55.501013 139958446696192 logging_writer.py:48] [147300] global_step=147300, grad_norm=3.7525060176849365, loss=0.974972128868103
I0307 20:42:35.766478 139958455088896 logging_writer.py:48] [147400] global_step=147400, grad_norm=3.8211536407470703, loss=0.9850648641586304
I0307 20:43:16.365289 139958446696192 logging_writer.py:48] [147500] global_step=147500, grad_norm=3.7451369762420654, loss=1.0242254734039307
I0307 20:43:17.544939 140114851837120 spec.py:321] Evaluating on the training split.
I0307 20:43:30.336924 140114851837120 spec.py:333] Evaluating on the validation split.
I0307 20:43:42.906253 140114851837120 spec.py:349] Evaluating on the test split.
I0307 20:43:44.730308 140114851837120 submission_runner.py:469] Time since start: 63216.07s, 	Step: 147504, 	{'train/accuracy': 0.8783880472183228, 'train/loss': 0.423285573720932, 'validation/accuracy': 0.7319599986076355, 'validation/loss': 1.1093332767486572, 'validation/num_examples': 50000, 'test/accuracy': 0.6055999994277954, 'test/loss': 1.8725615739822388, 'test/num_examples': 10000, 'score': 59208.65251708031, 'total_duration': 63216.06735801697, 'accumulated_submission_time': 59208.65251708031, 'accumulated_eval_time': 3975.7895686626434, 'accumulated_logging_time': 14.718544244766235}
I0307 20:43:44.830641 139958455088896 logging_writer.py:48] [147504] accumulated_eval_time=3975.79, accumulated_logging_time=14.7185, accumulated_submission_time=59208.7, global_step=147504, preemption_count=0, score=59208.7, test/accuracy=0.6056, test/loss=1.87256, test/num_examples=10000, total_duration=63216.1, train/accuracy=0.878388, train/loss=0.423286, validation/accuracy=0.73196, validation/loss=1.10933, validation/num_examples=50000
I0307 20:44:24.169809 139958446696192 logging_writer.py:48] [147600] global_step=147600, grad_norm=3.8718478679656982, loss=0.9565704464912415
2025-03-07 20:44:27.298618: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 20:45:05.173590 139958455088896 logging_writer.py:48] [147700] global_step=147700, grad_norm=3.903738260269165, loss=1.079420804977417
I0307 20:45:45.724343 139958446696192 logging_writer.py:48] [147800] global_step=147800, grad_norm=4.087470054626465, loss=1.0196439027786255
I0307 20:46:26.122981 139958455088896 logging_writer.py:48] [147900] global_step=147900, grad_norm=3.8656904697418213, loss=1.0053972005844116
I0307 20:47:07.079561 139958446696192 logging_writer.py:48] [148000] global_step=148000, grad_norm=4.185024261474609, loss=1.0890462398529053
I0307 20:47:47.582186 139958455088896 logging_writer.py:48] [148100] global_step=148100, grad_norm=3.8163092136383057, loss=1.0888153314590454
I0307 20:48:28.106589 139958446696192 logging_writer.py:48] [148200] global_step=148200, grad_norm=3.656075954437256, loss=0.921840250492096
I0307 20:49:08.892895 139958455088896 logging_writer.py:48] [148300] global_step=148300, grad_norm=3.701899528503418, loss=1.011133074760437
I0307 20:49:49.377215 139958446696192 logging_writer.py:48] [148400] global_step=148400, grad_norm=3.891824960708618, loss=1.0305085182189941
I0307 20:50:29.439980 139958455088896 logging_writer.py:48] [148500] global_step=148500, grad_norm=3.7079384326934814, loss=0.9858111143112183
I0307 20:51:09.605625 139958446696192 logging_writer.py:48] [148600] global_step=148600, grad_norm=3.5129921436309814, loss=0.956099808216095
I0307 20:51:50.487049 139958455088896 logging_writer.py:48] [148700] global_step=148700, grad_norm=3.8190667629241943, loss=1.0329841375350952
I0307 20:52:15.105254 140114851837120 spec.py:321] Evaluating on the training split.
I0307 20:52:27.791240 140114851837120 spec.py:333] Evaluating on the validation split.
I0307 20:52:40.144818 140114851837120 spec.py:349] Evaluating on the test split.
I0307 20:52:41.950446 140114851837120 submission_runner.py:469] Time since start: 63753.29s, 	Step: 148761, 	{'train/accuracy': 0.8730069994926453, 'train/loss': 0.44121915102005005, 'validation/accuracy': 0.7265799641609192, 'validation/loss': 1.1275993585586548, 'validation/num_examples': 50000, 'test/accuracy': 0.6033000349998474, 'test/loss': 1.8813526630401611, 'test/num_examples': 10000, 'score': 59718.75491428375, 'total_duration': 63753.28751850128, 'accumulated_submission_time': 59718.75491428375, 'accumulated_eval_time': 4002.634577035904, 'accumulated_logging_time': 14.84362506866455}
I0307 20:52:42.051087 139958446696192 logging_writer.py:48] [148761] accumulated_eval_time=4002.63, accumulated_logging_time=14.8436, accumulated_submission_time=59718.8, global_step=148761, preemption_count=0, score=59718.8, test/accuracy=0.6033, test/loss=1.88135, test/num_examples=10000, total_duration=63753.3, train/accuracy=0.873007, train/loss=0.441219, validation/accuracy=0.72658, validation/loss=1.1276, validation/num_examples=50000
I0307 20:52:58.396220 139958455088896 logging_writer.py:48] [148800] global_step=148800, grad_norm=3.8423585891723633, loss=1.0005011558532715
2025-03-07 20:53:22.614681: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 20:53:39.634732 139958446696192 logging_writer.py:48] [148900] global_step=148900, grad_norm=3.663986921310425, loss=0.9323484301567078
I0307 20:54:20.055463 139958455088896 logging_writer.py:48] [149000] global_step=149000, grad_norm=3.727646827697754, loss=0.8707119822502136
I0307 20:55:00.289215 139958446696192 logging_writer.py:48] [149100] global_step=149100, grad_norm=3.8626692295074463, loss=0.9854865074157715
I0307 20:55:40.854759 139958455088896 logging_writer.py:48] [149200] global_step=149200, grad_norm=3.9112749099731445, loss=0.9974905252456665
I0307 20:56:21.527160 139958446696192 logging_writer.py:48] [149300] global_step=149300, grad_norm=3.8096835613250732, loss=0.992074191570282
I0307 20:57:02.054436 139958455088896 logging_writer.py:48] [149400] global_step=149400, grad_norm=4.14524507522583, loss=1.0149273872375488
I0307 20:57:43.183209 139958446696192 logging_writer.py:48] [149500] global_step=149500, grad_norm=3.893021583557129, loss=0.9953163862228394
I0307 20:58:24.387206 139958455088896 logging_writer.py:48] [149600] global_step=149600, grad_norm=3.432661533355713, loss=0.9077385663986206
I0307 20:59:05.560202 139958446696192 logging_writer.py:48] [149700] global_step=149700, grad_norm=3.6347599029541016, loss=0.9267492890357971
I0307 20:59:46.055257 139958455088896 logging_writer.py:48] [149800] global_step=149800, grad_norm=3.6238491535186768, loss=0.992331862449646
I0307 21:00:27.288280 139958446696192 logging_writer.py:48] [149900] global_step=149900, grad_norm=3.73630690574646, loss=0.8766195774078369
I0307 21:01:08.809459 139958455088896 logging_writer.py:48] [150000] global_step=150000, grad_norm=3.7239537239074707, loss=0.8879472017288208
I0307 21:01:12.084454 140114851837120 spec.py:321] Evaluating on the training split.
I0307 21:01:25.137010 140114851837120 spec.py:333] Evaluating on the validation split.
I0307 21:01:37.629289 140114851837120 spec.py:349] Evaluating on the test split.
I0307 21:01:39.465335 140114851837120 submission_runner.py:469] Time since start: 64290.80s, 	Step: 150009, 	{'train/accuracy': 0.8852439522743225, 'train/loss': 0.40227600932121277, 'validation/accuracy': 0.7309399843215942, 'validation/loss': 1.121312141418457, 'validation/num_examples': 50000, 'test/accuracy': 0.6033000349998474, 'test/loss': 1.8759082555770874, 'test/num_examples': 10000, 'score': 60228.58671402931, 'total_duration': 64290.80240774155, 'accumulated_submission_time': 60228.58671402931, 'accumulated_eval_time': 4030.0152719020844, 'accumulated_logging_time': 14.997449159622192}
I0307 21:01:39.591219 139958446696192 logging_writer.py:48] [150009] accumulated_eval_time=4030.02, accumulated_logging_time=14.9974, accumulated_submission_time=60228.6, global_step=150009, preemption_count=0, score=60228.6, test/accuracy=0.6033, test/loss=1.87591, test/num_examples=10000, total_duration=64290.8, train/accuracy=0.885244, train/loss=0.402276, validation/accuracy=0.73094, validation/loss=1.12131, validation/num_examples=50000
I0307 21:02:16.802991 139958455088896 logging_writer.py:48] [150100] global_step=150100, grad_norm=4.021123886108398, loss=0.9496684670448303
2025-03-07 21:02:20.737438: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 21:02:57.625946 139958446696192 logging_writer.py:48] [150200] global_step=150200, grad_norm=4.057938098907471, loss=0.8854503631591797
I0307 21:03:38.679086 139958455088896 logging_writer.py:48] [150300] global_step=150300, grad_norm=3.754232168197632, loss=0.966631293296814
I0307 21:04:19.225780 139958446696192 logging_writer.py:48] [150400] global_step=150400, grad_norm=3.797771692276001, loss=0.9622377753257751
I0307 21:04:59.804975 139958455088896 logging_writer.py:48] [150500] global_step=150500, grad_norm=3.7996649742126465, loss=0.9342484474182129
I0307 21:05:40.276943 139958446696192 logging_writer.py:48] [150600] global_step=150600, grad_norm=3.6778674125671387, loss=0.9026556015014648
I0307 21:06:20.776313 139958455088896 logging_writer.py:48] [150700] global_step=150700, grad_norm=4.191445350646973, loss=0.8732277154922485
I0307 21:07:01.616837 139958446696192 logging_writer.py:48] [150800] global_step=150800, grad_norm=3.775095224380493, loss=0.9521976709365845
I0307 21:07:42.238931 139958455088896 logging_writer.py:48] [150900] global_step=150900, grad_norm=3.868410110473633, loss=0.9675413966178894
I0307 21:08:23.688583 139958446696192 logging_writer.py:48] [151000] global_step=151000, grad_norm=3.8958115577697754, loss=0.9147018194198608
I0307 21:09:04.025831 139958455088896 logging_writer.py:48] [151100] global_step=151100, grad_norm=3.9890592098236084, loss=0.8600596189498901
I0307 21:09:45.050529 139958446696192 logging_writer.py:48] [151200] global_step=151200, grad_norm=3.5043375492095947, loss=0.8719143867492676
I0307 21:10:09.519589 140114851837120 spec.py:321] Evaluating on the training split.
I0307 21:10:22.317907 140114851837120 spec.py:333] Evaluating on the validation split.
I0307 21:10:34.791256 140114851837120 spec.py:349] Evaluating on the test split.
I0307 21:10:36.587446 140114851837120 submission_runner.py:469] Time since start: 64827.92s, 	Step: 151261, 	{'train/accuracy': 0.8906050324440002, 'train/loss': 0.3854796290397644, 'validation/accuracy': 0.7303400039672852, 'validation/loss': 1.1059155464172363, 'validation/num_examples': 50000, 'test/accuracy': 0.6074000000953674, 'test/loss': 1.8362197875976562, 'test/num_examples': 10000, 'score': 60738.342767477036, 'total_duration': 64827.92450714111, 'accumulated_submission_time': 60738.342767477036, 'accumulated_eval_time': 4057.082931280136, 'accumulated_logging_time': 15.149745464324951}
I0307 21:10:36.702983 139958455088896 logging_writer.py:48] [151261] accumulated_eval_time=4057.08, accumulated_logging_time=15.1497, accumulated_submission_time=60738.3, global_step=151261, preemption_count=0, score=60738.3, test/accuracy=0.6074, test/loss=1.83622, test/num_examples=10000, total_duration=64827.9, train/accuracy=0.890605, train/loss=0.38548, validation/accuracy=0.73034, validation/loss=1.10592, validation/num_examples=50000
I0307 21:10:53.123015 139958446696192 logging_writer.py:48] [151300] global_step=151300, grad_norm=3.5478501319885254, loss=0.8776825070381165
2025-03-07 21:11:17.795023: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 21:11:33.695253 139958455088896 logging_writer.py:48] [151400] global_step=151400, grad_norm=3.8110713958740234, loss=0.9568029046058655
I0307 21:12:14.420778 139958446696192 logging_writer.py:48] [151500] global_step=151500, grad_norm=4.016208648681641, loss=0.9401662349700928
I0307 21:12:54.675546 139958455088896 logging_writer.py:48] [151600] global_step=151600, grad_norm=3.8685691356658936, loss=0.9531210064888
I0307 21:13:35.188945 139958446696192 logging_writer.py:48] [151700] global_step=151700, grad_norm=3.8424291610717773, loss=0.9605056047439575
I0307 21:14:15.960304 139958455088896 logging_writer.py:48] [151800] global_step=151800, grad_norm=3.694822072982788, loss=0.8943450450897217
I0307 21:14:56.498406 139958446696192 logging_writer.py:48] [151900] global_step=151900, grad_norm=3.7728028297424316, loss=1.0125118494033813
I0307 21:15:37.323668 139958455088896 logging_writer.py:48] [152000] global_step=152000, grad_norm=4.171741962432861, loss=0.9339078664779663
I0307 21:16:17.852651 139958446696192 logging_writer.py:48] [152100] global_step=152100, grad_norm=3.721007823944092, loss=0.9675135612487793
I0307 21:16:58.186926 139958455088896 logging_writer.py:48] [152200] global_step=152200, grad_norm=3.6495771408081055, loss=0.8241931200027466
I0307 21:17:38.864492 139958446696192 logging_writer.py:48] [152300] global_step=152300, grad_norm=4.02016544342041, loss=0.9557390213012695
I0307 21:18:19.813847 139958455088896 logging_writer.py:48] [152400] global_step=152400, grad_norm=3.954019784927368, loss=0.8428287506103516
I0307 21:19:00.283375 139958446696192 logging_writer.py:48] [152500] global_step=152500, grad_norm=3.9769740104675293, loss=0.8942967057228088
I0307 21:19:06.709477 140114851837120 spec.py:321] Evaluating on the training split.
I0307 21:19:19.659042 140114851837120 spec.py:333] Evaluating on the validation split.
I0307 21:19:32.156585 140114851837120 spec.py:349] Evaluating on the test split.
I0307 21:19:33.967551 140114851837120 submission_runner.py:469] Time since start: 65365.30s, 	Step: 152517, 	{'train/accuracy': 0.8903658986091614, 'train/loss': 0.3809801936149597, 'validation/accuracy': 0.7302599549293518, 'validation/loss': 1.1175459623336792, 'validation/num_examples': 50000, 'test/accuracy': 0.6018000245094299, 'test/loss': 1.8724627494812012, 'test/num_examples': 10000, 'score': 61248.171390771866, 'total_duration': 65365.30461025238, 'accumulated_submission_time': 61248.171390771866, 'accumulated_eval_time': 4084.3408143520355, 'accumulated_logging_time': 15.293487787246704}
I0307 21:19:34.052483 139958455088896 logging_writer.py:48] [152517] accumulated_eval_time=4084.34, accumulated_logging_time=15.2935, accumulated_submission_time=61248.2, global_step=152517, preemption_count=0, score=61248.2, test/accuracy=0.6018, test/loss=1.87246, test/num_examples=10000, total_duration=65365.3, train/accuracy=0.890366, train/loss=0.38098, validation/accuracy=0.73026, validation/loss=1.11755, validation/num_examples=50000
I0307 21:20:08.184375 139958446696192 logging_writer.py:48] [152600] global_step=152600, grad_norm=3.857412099838257, loss=0.9451596736907959
2025-03-07 21:20:13.407845: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 21:20:49.862227 139958455088896 logging_writer.py:48] [152700] global_step=152700, grad_norm=3.721363067626953, loss=0.8905656337738037
I0307 21:21:30.754814 139958446696192 logging_writer.py:48] [152800] global_step=152800, grad_norm=3.8893401622772217, loss=0.9422975182533264
I0307 21:22:11.589299 139958455088896 logging_writer.py:48] [152900] global_step=152900, grad_norm=4.063387393951416, loss=0.9554774761199951
I0307 21:22:52.215516 139958446696192 logging_writer.py:48] [153000] global_step=153000, grad_norm=3.998467206954956, loss=0.9469995498657227
I0307 21:23:32.559724 139958455088896 logging_writer.py:48] [153100] global_step=153100, grad_norm=4.441196441650391, loss=0.8277722001075745
I0307 21:24:13.196944 139958446696192 logging_writer.py:48] [153200] global_step=153200, grad_norm=3.764557123184204, loss=0.9184601902961731
I0307 21:24:53.888403 139958455088896 logging_writer.py:48] [153300] global_step=153300, grad_norm=4.038524150848389, loss=0.9302271604537964
I0307 21:25:34.629490 139958446696192 logging_writer.py:48] [153400] global_step=153400, grad_norm=3.876760959625244, loss=0.930461049079895
I0307 21:26:15.422876 139958455088896 logging_writer.py:48] [153500] global_step=153500, grad_norm=4.076067924499512, loss=1.0427528619766235
I0307 21:26:55.835658 139958446696192 logging_writer.py:48] [153600] global_step=153600, grad_norm=3.557725667953491, loss=0.8533851504325867
I0307 21:27:36.948517 139958455088896 logging_writer.py:48] [153700] global_step=153700, grad_norm=3.9098379611968994, loss=0.9133765697479248
I0307 21:28:04.175443 140114851837120 spec.py:321] Evaluating on the training split.
I0307 21:28:16.839342 140114851837120 spec.py:333] Evaluating on the validation split.
I0307 21:28:29.350799 140114851837120 spec.py:349] Evaluating on the test split.
I0307 21:28:31.201267 140114851837120 submission_runner.py:469] Time since start: 65902.54s, 	Step: 153768, 	{'train/accuracy': 0.9008290767669678, 'train/loss': 0.34036892652511597, 'validation/accuracy': 0.7346199750900269, 'validation/loss': 1.101989984512329, 'validation/num_examples': 50000, 'test/accuracy': 0.6124000549316406, 'test/loss': 1.846618890762329, 'test/num_examples': 10000, 'score': 61758.120441913605, 'total_duration': 65902.5383348465, 'accumulated_submission_time': 61758.120441913605, 'accumulated_eval_time': 4111.366451740265, 'accumulated_logging_time': 15.404479742050171}
I0307 21:28:31.323217 139958446696192 logging_writer.py:48] [153768] accumulated_eval_time=4111.37, accumulated_logging_time=15.4045, accumulated_submission_time=61758.1, global_step=153768, preemption_count=0, score=61758.1, test/accuracy=0.6124, test/loss=1.84662, test/num_examples=10000, total_duration=65902.5, train/accuracy=0.900829, train/loss=0.340369, validation/accuracy=0.73462, validation/loss=1.10199, validation/num_examples=50000
I0307 21:28:44.880474 139958455088896 logging_writer.py:48] [153800] global_step=153800, grad_norm=3.861849308013916, loss=0.9380381107330322
2025-03-07 21:29:10.773589: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 21:29:25.649613 139958446696192 logging_writer.py:48] [153900] global_step=153900, grad_norm=4.199507236480713, loss=0.9647101759910583
I0307 21:30:06.082628 139958455088896 logging_writer.py:48] [154000] global_step=154000, grad_norm=4.043545246124268, loss=0.9477872848510742
I0307 21:30:46.767034 139958446696192 logging_writer.py:48] [154100] global_step=154100, grad_norm=3.829542875289917, loss=0.8869666457176208
I0307 21:31:27.410167 139958455088896 logging_writer.py:48] [154200] global_step=154200, grad_norm=3.781280755996704, loss=0.9060336351394653
I0307 21:32:07.742472 139958446696192 logging_writer.py:48] [154300] global_step=154300, grad_norm=3.8720474243164062, loss=0.9232461452484131
I0307 21:32:48.084575 139958455088896 logging_writer.py:48] [154400] global_step=154400, grad_norm=3.880246162414551, loss=0.9157320857048035
I0307 21:33:28.206880 139958446696192 logging_writer.py:48] [154500] global_step=154500, grad_norm=3.8223073482513428, loss=0.822635293006897
I0307 21:34:09.042055 139958455088896 logging_writer.py:48] [154600] global_step=154600, grad_norm=4.037779331207275, loss=0.9389688968658447
I0307 21:34:49.327561 139958446696192 logging_writer.py:48] [154700] global_step=154700, grad_norm=4.078386306762695, loss=0.8953501582145691
I0307 21:35:29.511399 139958455088896 logging_writer.py:48] [154800] global_step=154800, grad_norm=4.3247833251953125, loss=0.9080260396003723
I0307 21:36:09.915266 139958446696192 logging_writer.py:48] [154900] global_step=154900, grad_norm=3.69754958152771, loss=0.8392011523246765
I0307 21:36:51.051354 139958455088896 logging_writer.py:48] [155000] global_step=155000, grad_norm=4.132946968078613, loss=0.9004117846488953
I0307 21:37:01.480797 140114851837120 spec.py:321] Evaluating on the training split.
I0307 21:37:14.274424 140114851837120 spec.py:333] Evaluating on the validation split.
I0307 21:37:26.816781 140114851837120 spec.py:349] Evaluating on the test split.
I0307 21:37:28.652529 140114851837120 submission_runner.py:469] Time since start: 66439.99s, 	Step: 155026, 	{'train/accuracy': 0.9110331535339355, 'train/loss': 0.30523690581321716, 'validation/accuracy': 0.7399399876594543, 'validation/loss': 1.091164231300354, 'validation/num_examples': 50000, 'test/accuracy': 0.6074000000953674, 'test/loss': 1.8635671138763428, 'test/num_examples': 10000, 'score': 62268.102655887604, 'total_duration': 66439.98958063126, 'accumulated_submission_time': 62268.102655887604, 'accumulated_eval_time': 4138.5379774570465, 'accumulated_logging_time': 15.553490161895752}
I0307 21:37:28.738522 139958446696192 logging_writer.py:48] [155026] accumulated_eval_time=4138.54, accumulated_logging_time=15.5535, accumulated_submission_time=62268.1, global_step=155026, preemption_count=0, score=62268.1, test/accuracy=0.6074, test/loss=1.86357, test/num_examples=10000, total_duration=66440, train/accuracy=0.911033, train/loss=0.305237, validation/accuracy=0.73994, validation/loss=1.09116, validation/num_examples=50000
I0307 21:37:59.742226 139958455088896 logging_writer.py:48] [155100] global_step=155100, grad_norm=4.2187418937683105, loss=0.9148809909820557
2025-03-07 21:38:05.541968: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 21:38:39.735213 139958446696192 logging_writer.py:48] [155200] global_step=155200, grad_norm=3.8802874088287354, loss=0.8692117929458618
I0307 21:39:20.087081 139958455088896 logging_writer.py:48] [155300] global_step=155300, grad_norm=3.720539093017578, loss=0.8857587575912476
I0307 21:40:00.610874 139958446696192 logging_writer.py:48] [155400] global_step=155400, grad_norm=4.22770881652832, loss=0.9799824357032776
I0307 21:40:40.874550 139958455088896 logging_writer.py:48] [155500] global_step=155500, grad_norm=4.162535190582275, loss=0.8451104760169983
I0307 21:41:21.642064 139958446696192 logging_writer.py:48] [155600] global_step=155600, grad_norm=4.240951061248779, loss=0.9768506288528442
I0307 21:42:02.092012 139958455088896 logging_writer.py:48] [155700] global_step=155700, grad_norm=3.9528589248657227, loss=0.8216407895088196
I0307 21:42:42.746985 139958446696192 logging_writer.py:48] [155800] global_step=155800, grad_norm=4.251620292663574, loss=0.9134035110473633
I0307 21:43:23.045433 139958455088896 logging_writer.py:48] [155900] global_step=155900, grad_norm=3.705038070678711, loss=0.8834381103515625
I0307 21:44:03.478796 139958446696192 logging_writer.py:48] [156000] global_step=156000, grad_norm=3.998227834701538, loss=0.9055978655815125
I0307 21:44:43.909760 139958455088896 logging_writer.py:48] [156100] global_step=156100, grad_norm=3.9442684650421143, loss=0.9300867319107056
I0307 21:45:24.885953 139958446696192 logging_writer.py:48] [156200] global_step=156200, grad_norm=3.771681547164917, loss=0.8391450047492981
I0307 21:45:58.772879 140114851837120 spec.py:321] Evaluating on the training split.
I0307 21:46:11.587698 140114851837120 spec.py:333] Evaluating on the validation split.
I0307 21:46:24.011985 140114851837120 spec.py:349] Evaluating on the test split.
I0307 21:46:25.843640 140114851837120 submission_runner.py:469] Time since start: 66977.18s, 	Step: 156284, 	{'train/accuracy': 0.9184669852256775, 'train/loss': 0.29308050870895386, 'validation/accuracy': 0.7384399771690369, 'validation/loss': 1.0832815170288086, 'validation/num_examples': 50000, 'test/accuracy': 0.6162000298500061, 'test/loss': 1.8467997312545776, 'test/num_examples': 10000, 'score': 62777.95733046532, 'total_duration': 66977.18071866035, 'accumulated_submission_time': 62777.95733046532, 'accumulated_eval_time': 4165.608562707901, 'accumulated_logging_time': 15.66888952255249}
I0307 21:46:25.979984 139958455088896 logging_writer.py:48] [156284] accumulated_eval_time=4165.61, accumulated_logging_time=15.6689, accumulated_submission_time=62778, global_step=156284, preemption_count=0, score=62778, test/accuracy=0.6162, test/loss=1.8468, test/num_examples=10000, total_duration=66977.2, train/accuracy=0.918467, train/loss=0.293081, validation/accuracy=0.73844, validation/loss=1.08328, validation/num_examples=50000
I0307 21:46:32.984001 139958446696192 logging_writer.py:48] [156300] global_step=156300, grad_norm=3.892443895339966, loss=0.8289775848388672
2025-03-07 21:46:59.288248: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 21:47:13.606091 139958455088896 logging_writer.py:48] [156400] global_step=156400, grad_norm=3.9426352977752686, loss=0.8873320817947388
I0307 21:47:54.266343 139958446696192 logging_writer.py:48] [156500] global_step=156500, grad_norm=3.9984986782073975, loss=0.8661661744117737
I0307 21:48:34.552705 139958455088896 logging_writer.py:48] [156600] global_step=156600, grad_norm=3.8044376373291016, loss=0.7721013426780701
I0307 21:49:14.853017 139958446696192 logging_writer.py:48] [156700] global_step=156700, grad_norm=3.9327056407928467, loss=0.8471195697784424
I0307 21:49:55.815187 139958455088896 logging_writer.py:48] [156800] global_step=156800, grad_norm=4.122162818908691, loss=0.889427661895752
I0307 21:50:35.704643 139958446696192 logging_writer.py:48] [156900] global_step=156900, grad_norm=4.257153511047363, loss=0.9126847386360168
I0307 21:51:16.072293 139958455088896 logging_writer.py:48] [157000] global_step=157000, grad_norm=3.989025831222534, loss=0.8874413967132568
I0307 21:51:56.526286 139958446696192 logging_writer.py:48] [157100] global_step=157100, grad_norm=4.331434726715088, loss=0.9455122351646423
I0307 21:52:36.812437 139958455088896 logging_writer.py:48] [157200] global_step=157200, grad_norm=3.792410373687744, loss=0.801250696182251
I0307 21:53:17.016737 139958446696192 logging_writer.py:48] [157300] global_step=157300, grad_norm=4.571711540222168, loss=0.8841179609298706
I0307 21:53:57.644388 139958455088896 logging_writer.py:48] [157400] global_step=157400, grad_norm=3.930128812789917, loss=0.8733222484588623
I0307 21:54:38.259639 139958446696192 logging_writer.py:48] [157500] global_step=157500, grad_norm=3.9752249717712402, loss=0.804253876209259
I0307 21:54:56.003293 140114851837120 spec.py:321] Evaluating on the training split.
I0307 21:55:08.961457 140114851837120 spec.py:333] Evaluating on the validation split.
I0307 21:55:21.450883 140114851837120 spec.py:349] Evaluating on the test split.
I0307 21:55:23.292977 140114851837120 submission_runner.py:469] Time since start: 67514.63s, 	Step: 157545, 	{'train/accuracy': 0.9288305044174194, 'train/loss': 0.256733238697052, 'validation/accuracy': 0.7404199838638306, 'validation/loss': 1.0825177431106567, 'validation/num_examples': 50000, 'test/accuracy': 0.614300012588501, 'test/loss': 1.845710039138794, 'test/num_examples': 10000, 'score': 63287.78874731064, 'total_duration': 67514.63003993034, 'accumulated_submission_time': 63287.78874731064, 'accumulated_eval_time': 4192.898053407669, 'accumulated_logging_time': 15.846901178359985}
I0307 21:55:23.390967 139958455088896 logging_writer.py:48] [157545] accumulated_eval_time=4192.9, accumulated_logging_time=15.8469, accumulated_submission_time=63287.8, global_step=157545, preemption_count=0, score=63287.8, test/accuracy=0.6143, test/loss=1.84571, test/num_examples=10000, total_duration=67514.6, train/accuracy=0.928831, train/loss=0.256733, validation/accuracy=0.74042, validation/loss=1.08252, validation/num_examples=50000
I0307 21:55:45.866556 139958446696192 logging_writer.py:48] [157600] global_step=157600, grad_norm=4.037565231323242, loss=0.8645870089530945
2025-03-07 21:55:52.627977: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 21:56:25.700425 139958455088896 logging_writer.py:48] [157700] global_step=157700, grad_norm=3.891099452972412, loss=0.8063623905181885
I0307 21:57:05.701497 139958446696192 logging_writer.py:48] [157800] global_step=157800, grad_norm=3.665344715118408, loss=0.8093268871307373
I0307 21:57:45.713726 139958455088896 logging_writer.py:48] [157900] global_step=157900, grad_norm=4.2027482986450195, loss=0.8628377914428711
I0307 21:58:25.591306 139958446696192 logging_writer.py:48] [158000] global_step=158000, grad_norm=4.144299507141113, loss=0.8853520154953003
I0307 21:59:05.668419 139958455088896 logging_writer.py:48] [158100] global_step=158100, grad_norm=3.9328904151916504, loss=0.8328929543495178
I0307 21:59:45.749290 139958446696192 logging_writer.py:48] [158200] global_step=158200, grad_norm=3.8963770866394043, loss=0.7776582837104797
I0307 22:00:25.964073 139958455088896 logging_writer.py:48] [158300] global_step=158300, grad_norm=4.567346096038818, loss=0.8528055548667908
I0307 22:01:06.395363 139958446696192 logging_writer.py:48] [158400] global_step=158400, grad_norm=4.042398929595947, loss=0.827862024307251
I0307 22:01:46.752021 139958455088896 logging_writer.py:48] [158500] global_step=158500, grad_norm=4.071812152862549, loss=0.8168255090713501
I0307 22:02:26.620003 139958446696192 logging_writer.py:48] [158600] global_step=158600, grad_norm=4.03975772857666, loss=0.8614027500152588
I0307 22:03:06.940703 139958455088896 logging_writer.py:48] [158700] global_step=158700, grad_norm=3.989156484603882, loss=0.851571798324585
I0307 22:03:47.611485 139958446696192 logging_writer.py:48] [158800] global_step=158800, grad_norm=4.7487406730651855, loss=0.8693786263465881
I0307 22:03:53.409781 140114851837120 spec.py:321] Evaluating on the training split.
I0307 22:04:06.012269 140114851837120 spec.py:333] Evaluating on the validation split.
I0307 22:04:18.549080 140114851837120 spec.py:349] Evaluating on the test split.
I0307 22:04:20.396399 140114851837120 submission_runner.py:469] Time since start: 68051.73s, 	Step: 158815, 	{'train/accuracy': 0.9248046875, 'train/loss': 0.2694341242313385, 'validation/accuracy': 0.7346199750900269, 'validation/loss': 1.1146613359451294, 'validation/num_examples': 50000, 'test/accuracy': 0.6093000173568726, 'test/loss': 1.8701332807540894, 'test/num_examples': 10000, 'score': 63797.63259458542, 'total_duration': 68051.73343467712, 'accumulated_submission_time': 63797.63259458542, 'accumulated_eval_time': 4219.884444236755, 'accumulated_logging_time': 15.969777584075928}
I0307 22:04:20.516647 139958455088896 logging_writer.py:48] [158815] accumulated_eval_time=4219.88, accumulated_logging_time=15.9698, accumulated_submission_time=63797.6, global_step=158815, preemption_count=0, score=63797.6, test/accuracy=0.6093, test/loss=1.87013, test/num_examples=10000, total_duration=68051.7, train/accuracy=0.924805, train/loss=0.269434, validation/accuracy=0.73462, validation/loss=1.11466, validation/num_examples=50000
2025-03-07 22:04:41.853003: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 22:04:54.951990 139958446696192 logging_writer.py:48] [158900] global_step=158900, grad_norm=4.043679714202881, loss=0.813572883605957
I0307 22:05:35.540185 139958455088896 logging_writer.py:48] [159000] global_step=159000, grad_norm=4.087508201599121, loss=0.8722594976425171
I0307 22:06:15.561372 139958446696192 logging_writer.py:48] [159100] global_step=159100, grad_norm=3.711780071258545, loss=0.82014000415802
I0307 22:06:55.635831 139958455088896 logging_writer.py:48] [159200] global_step=159200, grad_norm=3.9669368267059326, loss=0.8413206338882446
I0307 22:07:35.777490 139958446696192 logging_writer.py:48] [159300] global_step=159300, grad_norm=4.3841633796691895, loss=0.866202175617218
I0307 22:08:16.316675 139958455088896 logging_writer.py:48] [159400] global_step=159400, grad_norm=4.1063456535339355, loss=0.8455696702003479
I0307 22:08:57.162873 139958446696192 logging_writer.py:48] [159500] global_step=159500, grad_norm=4.022164344787598, loss=0.8741631507873535
I0307 22:09:38.101211 139958455088896 logging_writer.py:48] [159600] global_step=159600, grad_norm=4.076586723327637, loss=0.8309606909751892
I0307 22:10:18.773051 139958446696192 logging_writer.py:48] [159700] global_step=159700, grad_norm=3.9431116580963135, loss=0.7589678764343262
I0307 22:10:59.062161 139958455088896 logging_writer.py:48] [159800] global_step=159800, grad_norm=4.427978515625, loss=0.8490553498268127
I0307 22:11:39.833815 139958446696192 logging_writer.py:48] [159900] global_step=159900, grad_norm=4.347787380218506, loss=0.844910204410553
I0307 22:12:20.189042 139958455088896 logging_writer.py:48] [160000] global_step=160000, grad_norm=4.006387710571289, loss=0.8546168208122253
I0307 22:12:50.784210 140114851837120 spec.py:321] Evaluating on the training split.
I0307 22:13:03.237434 140114851837120 spec.py:333] Evaluating on the validation split.
I0307 22:13:15.811231 140114851837120 spec.py:349] Evaluating on the test split.
I0307 22:13:17.645453 140114851837120 submission_runner.py:469] Time since start: 68588.98s, 	Step: 160076, 	{'train/accuracy': 0.9137635231018066, 'train/loss': 0.2998613119125366, 'validation/accuracy': 0.7390199899673462, 'validation/loss': 1.091567039489746, 'validation/num_examples': 50000, 'test/accuracy': 0.6096000075340271, 'test/loss': 1.8660664558410645, 'test/num_examples': 10000, 'score': 64307.715534210205, 'total_duration': 68588.98250818253, 'accumulated_submission_time': 64307.715534210205, 'accumulated_eval_time': 4246.745504617691, 'accumulated_logging_time': 16.1235671043396}
I0307 22:13:17.733016 139958446696192 logging_writer.py:48] [160076] accumulated_eval_time=4246.75, accumulated_logging_time=16.1236, accumulated_submission_time=64307.7, global_step=160076, preemption_count=0, score=64307.7, test/accuracy=0.6096, test/loss=1.86607, test/num_examples=10000, total_duration=68589, train/accuracy=0.913764, train/loss=0.299861, validation/accuracy=0.73902, validation/loss=1.09157, validation/num_examples=50000
I0307 22:13:27.847752 139958455088896 logging_writer.py:48] [160100] global_step=160100, grad_norm=4.002440452575684, loss=0.7662978768348694
2025-03-07 22:13:35.177374: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 22:14:07.745701 139958446696192 logging_writer.py:48] [160200] global_step=160200, grad_norm=4.211515426635742, loss=0.8814191818237305
I0307 22:14:47.590473 139958455088896 logging_writer.py:48] [160300] global_step=160300, grad_norm=3.836958169937134, loss=0.804987907409668
I0307 22:15:28.092817 139958446696192 logging_writer.py:48] [160400] global_step=160400, grad_norm=4.054490566253662, loss=0.8182017207145691
I0307 22:16:08.455694 139958455088896 logging_writer.py:48] [160500] global_step=160500, grad_norm=4.197205066680908, loss=0.798922061920166
I0307 22:16:48.405067 139958446696192 logging_writer.py:48] [160600] global_step=160600, grad_norm=4.1743268966674805, loss=0.8698573112487793
I0307 22:17:28.718429 139958455088896 logging_writer.py:48] [160700] global_step=160700, grad_norm=4.179225444793701, loss=0.9026311635971069
I0307 22:18:09.222066 139958446696192 logging_writer.py:48] [160800] global_step=160800, grad_norm=4.3559250831604, loss=0.8985326290130615
I0307 22:18:49.444025 139958455088896 logging_writer.py:48] [160900] global_step=160900, grad_norm=4.3132758140563965, loss=0.8647766709327698
I0307 22:19:29.513118 139958446696192 logging_writer.py:48] [161000] global_step=161000, grad_norm=4.330453395843506, loss=0.8236238360404968
I0307 22:20:09.967941 139958455088896 logging_writer.py:48] [161100] global_step=161100, grad_norm=4.0628886222839355, loss=0.8589652180671692
I0307 22:20:50.415360 139958446696192 logging_writer.py:48] [161200] global_step=161200, grad_norm=4.03345251083374, loss=0.8370744585990906
I0307 22:21:31.056957 139958455088896 logging_writer.py:48] [161300] global_step=161300, grad_norm=3.925238609313965, loss=0.7622025012969971
I0307 22:21:48.006438 140114851837120 spec.py:321] Evaluating on the training split.
I0307 22:22:01.205830 140114851837120 spec.py:333] Evaluating on the validation split.
I0307 22:22:13.689065 140114851837120 spec.py:349] Evaluating on the test split.
I0307 22:22:15.495640 140114851837120 submission_runner.py:469] Time since start: 69126.83s, 	Step: 161343, 	{'train/accuracy': 0.9080635905265808, 'train/loss': 0.3138331472873688, 'validation/accuracy': 0.739139974117279, 'validation/loss': 1.093545913696289, 'validation/num_examples': 50000, 'test/accuracy': 0.6201000213623047, 'test/loss': 1.853371024131775, 'test/num_examples': 10000, 'score': 64817.81100797653, 'total_duration': 69126.83268904686, 'accumulated_submission_time': 64817.81100797653, 'accumulated_eval_time': 4274.234495162964, 'accumulated_logging_time': 16.239080667495728}
I0307 22:22:15.617533 139958446696192 logging_writer.py:48] [161343] accumulated_eval_time=4274.23, accumulated_logging_time=16.2391, accumulated_submission_time=64817.8, global_step=161343, preemption_count=0, score=64817.8, test/accuracy=0.6201, test/loss=1.85337, test/num_examples=10000, total_duration=69126.8, train/accuracy=0.908064, train/loss=0.313833, validation/accuracy=0.73914, validation/loss=1.09355, validation/num_examples=50000
2025-03-07 22:22:26.783389: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 22:22:38.879671 139958455088896 logging_writer.py:48] [161400] global_step=161400, grad_norm=3.9354841709136963, loss=0.7899969816207886
I0307 22:23:18.856336 139958446696192 logging_writer.py:48] [161500] global_step=161500, grad_norm=3.993720531463623, loss=0.8186063170433044
I0307 22:23:58.847225 139958455088896 logging_writer.py:48] [161600] global_step=161600, grad_norm=4.189305305480957, loss=0.9269444942474365
I0307 22:24:38.823310 139958446696192 logging_writer.py:48] [161700] global_step=161700, grad_norm=4.10365629196167, loss=0.8929964303970337
I0307 22:25:18.797947 139958455088896 logging_writer.py:48] [161800] global_step=161800, grad_norm=4.626221179962158, loss=0.8400805592536926
I0307 22:25:58.867541 139958446696192 logging_writer.py:48] [161900] global_step=161900, grad_norm=4.369399547576904, loss=0.8550925254821777
I0307 22:26:39.044369 139958455088896 logging_writer.py:48] [162000] global_step=162000, grad_norm=5.123626708984375, loss=0.8836132884025574
I0307 22:27:19.243843 139958446696192 logging_writer.py:48] [162100] global_step=162100, grad_norm=4.870841979980469, loss=0.8952022194862366
I0307 22:27:59.586240 139958455088896 logging_writer.py:48] [162200] global_step=162200, grad_norm=4.442325115203857, loss=0.8046557903289795
I0307 22:28:39.443900 139958446696192 logging_writer.py:48] [162300] global_step=162300, grad_norm=3.94474458694458, loss=0.8046886324882507
I0307 22:29:19.810299 139958455088896 logging_writer.py:48] [162400] global_step=162400, grad_norm=4.538724899291992, loss=0.9103143811225891
I0307 22:30:00.024688 139958446696192 logging_writer.py:48] [162500] global_step=162500, grad_norm=4.448851108551025, loss=0.8124123811721802
I0307 22:30:40.173560 139958455088896 logging_writer.py:48] [162600] global_step=162600, grad_norm=4.239114761352539, loss=0.8097953796386719
I0307 22:30:45.913683 140114851837120 spec.py:321] Evaluating on the training split.
I0307 22:30:58.502307 140114851837120 spec.py:333] Evaluating on the validation split.
I0307 22:31:11.035819 140114851837120 spec.py:349] Evaluating on the test split.
I0307 22:31:12.885951 140114851837120 submission_runner.py:469] Time since start: 69664.22s, 	Step: 162615, 	{'train/accuracy': 0.9080436825752258, 'train/loss': 0.317469984292984, 'validation/accuracy': 0.7396799921989441, 'validation/loss': 1.0973786115646362, 'validation/num_examples': 50000, 'test/accuracy': 0.6109000444412231, 'test/loss': 1.8552061319351196, 'test/num_examples': 10000, 'score': 65327.9159655571, 'total_duration': 69664.22300601006, 'accumulated_submission_time': 65327.9159655571, 'accumulated_eval_time': 4301.2065596580505, 'accumulated_logging_time': 16.400709629058838}
I0307 22:31:12.993816 139958446696192 logging_writer.py:48] [162615] accumulated_eval_time=4301.21, accumulated_logging_time=16.4007, accumulated_submission_time=65327.9, global_step=162615, preemption_count=0, score=65327.9, test/accuracy=0.6109, test/loss=1.85521, test/num_examples=10000, total_duration=69664.2, train/accuracy=0.908044, train/loss=0.31747, validation/accuracy=0.73968, validation/loss=1.09738, validation/num_examples=50000
2025-03-07 22:31:16.108390: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 22:31:48.533507 139958455088896 logging_writer.py:48] [162700] global_step=162700, grad_norm=4.05349588394165, loss=0.7451330423355103
I0307 22:32:28.840581 139958446696192 logging_writer.py:48] [162800] global_step=162800, grad_norm=3.8687379360198975, loss=0.6797392964363098
I0307 22:33:09.717299 139958455088896 logging_writer.py:48] [162900] global_step=162900, grad_norm=4.176376819610596, loss=0.8460304737091064
I0307 22:33:50.267208 139958446696192 logging_writer.py:48] [163000] global_step=163000, grad_norm=4.364407539367676, loss=0.7770759463310242
I0307 22:34:31.250444 139958455088896 logging_writer.py:48] [163100] global_step=163100, grad_norm=4.249474048614502, loss=0.8136887550354004
I0307 22:35:11.792486 139958446696192 logging_writer.py:48] [163200] global_step=163200, grad_norm=4.297787189483643, loss=0.8093266487121582
I0307 22:35:52.621139 139958455088896 logging_writer.py:48] [163300] global_step=163300, grad_norm=4.223614692687988, loss=0.8230586647987366
I0307 22:36:32.857175 139958446696192 logging_writer.py:48] [163400] global_step=163400, grad_norm=4.363816738128662, loss=0.8599804639816284
I0307 22:37:13.181189 139958455088896 logging_writer.py:48] [163500] global_step=163500, grad_norm=4.282144069671631, loss=0.8529096841812134
I0307 22:37:53.464845 139958446696192 logging_writer.py:48] [163600] global_step=163600, grad_norm=4.125451564788818, loss=0.7735145092010498
I0307 22:38:34.233777 139958455088896 logging_writer.py:48] [163700] global_step=163700, grad_norm=4.235424041748047, loss=0.7876517176628113
I0307 22:39:15.048352 139958446696192 logging_writer.py:48] [163800] global_step=163800, grad_norm=5.028263568878174, loss=0.8642459511756897
I0307 22:39:42.975223 140114851837120 spec.py:321] Evaluating on the training split.
I0307 22:39:55.918409 140114851837120 spec.py:333] Evaluating on the validation split.
I0307 22:40:08.409048 140114851837120 spec.py:349] Evaluating on the test split.
I0307 22:40:10.241148 140114851837120 submission_runner.py:469] Time since start: 70201.58s, 	Step: 163870, 	{'train/accuracy': 0.9150190949440002, 'train/loss': 0.2908560633659363, 'validation/accuracy': 0.7402600049972534, 'validation/loss': 1.0952625274658203, 'validation/num_examples': 50000, 'test/accuracy': 0.6164000034332275, 'test/loss': 1.8572149276733398, 'test/num_examples': 10000, 'score': 65837.7168314457, 'total_duration': 70201.57818841934, 'accumulated_submission_time': 65837.7168314457, 'accumulated_eval_time': 4328.472270488739, 'accumulated_logging_time': 16.54002356529236}
I0307 22:40:10.378886 139958455088896 logging_writer.py:48] [163870] accumulated_eval_time=4328.47, accumulated_logging_time=16.54, accumulated_submission_time=65837.7, global_step=163870, preemption_count=0, score=65837.7, test/accuracy=0.6164, test/loss=1.85721, test/num_examples=10000, total_duration=70201.6, train/accuracy=0.915019, train/loss=0.290856, validation/accuracy=0.74026, validation/loss=1.09526, validation/num_examples=50000
2025-03-07 22:40:11.803458: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 22:40:23.025431 139958446696192 logging_writer.py:48] [163900] global_step=163900, grad_norm=4.503458499908447, loss=0.7048282623291016
I0307 22:41:04.005213 139958455088896 logging_writer.py:48] [164000] global_step=164000, grad_norm=4.170454502105713, loss=0.778223991394043
I0307 22:41:44.416114 139958446696192 logging_writer.py:48] [164100] global_step=164100, grad_norm=4.5689496994018555, loss=0.7382544279098511
I0307 22:42:25.089751 139958455088896 logging_writer.py:48] [164200] global_step=164200, grad_norm=4.601482391357422, loss=0.8393750786781311
I0307 22:43:05.639250 139958446696192 logging_writer.py:48] [164300] global_step=164300, grad_norm=4.316922664642334, loss=0.8698602914810181
I0307 22:43:46.557726 139958455088896 logging_writer.py:48] [164400] global_step=164400, grad_norm=4.8941497802734375, loss=0.8229015469551086
I0307 22:44:27.511682 139958446696192 logging_writer.py:48] [164500] global_step=164500, grad_norm=4.132422924041748, loss=0.7267683744430542
I0307 22:45:08.257997 139958455088896 logging_writer.py:48] [164600] global_step=164600, grad_norm=4.120513439178467, loss=0.7914488315582275
I0307 22:45:48.932263 139958446696192 logging_writer.py:48] [164700] global_step=164700, grad_norm=4.047755718231201, loss=0.7838582396507263
I0307 22:46:29.655474 139958455088896 logging_writer.py:48] [164800] global_step=164800, grad_norm=4.4643049240112305, loss=0.8106744289398193
I0307 22:47:10.578867 139958446696192 logging_writer.py:48] [164900] global_step=164900, grad_norm=4.438126564025879, loss=0.7759295105934143
I0307 22:47:51.610394 139958455088896 logging_writer.py:48] [165000] global_step=165000, grad_norm=4.637678623199463, loss=0.8340616226196289
I0307 22:48:32.173139 139958446696192 logging_writer.py:48] [165100] global_step=165100, grad_norm=4.707699775695801, loss=0.7611472606658936
I0307 22:48:40.473707 139958455088896 logging_writer.py:48] [165121] global_step=165121, preemption_count=0, score=66347.6
I0307 22:48:42.496511 140114851837120 submission_runner.py:646] Tuning trial 5/5
I0307 22:48:42.496694 140114851837120 submission_runner.py:647] Hyperparameters: Hyperparameters(dropout_rate=0.1, label_smoothing=0.0, learning_rate=0.0017486387539278373, one_minus_beta1=0.06733926164, beta2=0.9955159689799007, weight_decay=0.08121616522670176, warmup_factor=0.02)
I0307 22:48:42.501521 140114851837120 submission_runner.py:648] Metrics: {'eval_results': [(1, {'train/accuracy': 0.0009367027669213712, 'train/loss': 6.911309719085693, 'validation/accuracy': 0.0009999999310821295, 'validation/loss': 6.9110894203186035, 'validation/num_examples': 50000, 'test/accuracy': 0.0009000000427477062, 'test/loss': 6.911230564117432, 'test/num_examples': 10000, 'score': 58.10783839225769, 'total_duration': 152.13403797149658, 'accumulated_submission_time': 58.10783839225769, 'accumulated_eval_time': 94.02588820457458, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1358, {'train/accuracy': 0.1669921875, 'train/loss': 4.284590244293213, 'validation/accuracy': 0.13679999113082886, 'validation/loss': 4.54769229888916, 'validation/num_examples': 50000, 'test/accuracy': 0.09920000284910202, 'test/loss': 5.027939319610596, 'test/num_examples': 10000, 'score': 568.0293328762054, 'total_duration': 721.7490515708923, 'accumulated_submission_time': 568.0293328762054, 'accumulated_eval_time': 153.45283269882202, 'accumulated_logging_time': 0.0838019847869873, 'global_step': 1358, 'preemption_count': 0}), (2714, {'train/accuracy': 0.32669004797935486, 'train/loss': 3.140263795852661, 'validation/accuracy': 0.2846199870109558, 'validation/loss': 3.3939952850341797, 'validation/num_examples': 50000, 'test/accuracy': 0.2150000035762787, 'test/loss': 3.982311964035034, 'test/num_examples': 10000, 'score': 1077.9494602680206, 'total_duration': 1266.635375738144, 'accumulated_submission_time': 1077.9494602680206, 'accumulated_eval_time': 188.20797300338745, 'accumulated_logging_time': 0.11478090286254883, 'global_step': 2714, 'preemption_count': 0}), (4049, {'train/accuracy': 0.4413265287876129, 'train/loss': 2.4695708751678467, 'validation/accuracy': 0.3942199945449829, 'validation/loss': 2.749650478363037, 'validation/num_examples': 50000, 'test/accuracy': 0.2955000102519989, 'test/loss': 3.4448471069335938, 'test/num_examples': 10000, 'score': 1587.8253746032715, 'total_duration': 1808.3067936897278, 'accumulated_submission_time': 1587.8253746032715, 'accumulated_eval_time': 219.8057723045349, 'accumulated_logging_time': 0.15940308570861816, 'global_step': 4049, 'preemption_count': 0}), (5385, {'train/accuracy': 0.5342793464660645, 'train/loss': 1.9912950992584229, 'validation/accuracy': 0.4821600019931793, 'validation/loss': 2.259072780609131, 'validation/num_examples': 50000, 'test/accuracy': 0.36830002069473267, 'test/loss': 2.961452007293701, 'test/num_examples': 10000, 'score': 2097.672739982605, 'total_duration': 2352.478568315506, 'accumulated_submission_time': 2097.672739982605, 'accumulated_eval_time': 253.9401466846466, 'accumulated_logging_time': 0.1956174373626709, 'global_step': 5385, 'preemption_count': 0}), (6721, {'train/accuracy': 0.5532923936843872, 'train/loss': 1.8886045217514038, 'validation/accuracy': 0.49771997332572937, 'validation/loss': 2.190770387649536, 'validation/num_examples': 50000, 'test/accuracy': 0.38050001859664917, 'test/loss': 2.974987506866455, 'test/num_examples': 10000, 'score': 2607.7103476524353, 'total_duration': 2898.6515686511993, 'accumulated_submission_time': 2607.7103476524353, 'accumulated_eval_time': 289.8500511646271, 'accumulated_logging_time': 0.2792384624481201, 'global_step': 6721, 'preemption_count': 0}), (8052, {'train/accuracy': 0.5985132455825806, 'train/loss': 1.6628903150558472, 'validation/accuracy': 0.5411199927330017, 'validation/loss': 1.9675277471542358, 'validation/num_examples': 50000, 'test/accuracy': 0.4173000156879425, 'test/loss': 2.738194227218628, 'test/num_examples': 10000, 'score': 3117.529729127884, 'total_duration': 3443.7532715797424, 'accumulated_submission_time': 3117.529729127884, 'accumulated_eval_time': 324.94742250442505, 'accumulated_logging_time': 0.33165979385375977, 'global_step': 8052, 'preemption_count': 0}), (9381, {'train/accuracy': 0.616609513759613, 'train/loss': 1.5495808124542236, 'validation/accuracy': 0.5597599744796753, 'validation/loss': 1.8752597570419312, 'validation/num_examples': 50000, 'test/accuracy': 0.4319000244140625, 'test/loss': 2.651991605758667, 'test/num_examples': 10000, 'score': 3627.354485273361, 'total_duration': 3989.89146232605, 'accumulated_submission_time': 3627.354485273361, 'accumulated_eval_time': 361.0843553543091, 'accumulated_logging_time': 0.36838865280151367, 'global_step': 9381, 'preemption_count': 0}), (10708, {'train/accuracy': 0.6346460580825806, 'train/loss': 1.4815704822540283, 'validation/accuracy': 0.572439968585968, 'validation/loss': 1.8022783994674683, 'validation/num_examples': 50000, 'test/accuracy': 0.45180001854896545, 'test/loss': 2.528625965118408, 'test/num_examples': 10000, 'score': 4137.336168289185, 'total_duration': 4542.2506721019745, 'accumulated_submission_time': 4137.336168289185, 'accumulated_eval_time': 403.2563259601593, 'accumulated_logging_time': 0.42371392250061035, 'global_step': 10708, 'preemption_count': 0}), (12033, {'train/accuracy': 0.631257951259613, 'train/loss': 1.4887652397155762, 'validation/accuracy': 0.5737599730491638, 'validation/loss': 1.8011565208435059, 'validation/num_examples': 50000, 'test/accuracy': 0.4471000134944916, 'test/loss': 2.5648012161254883, 'test/num_examples': 10000, 'score': 4647.0611782073975, 'total_duration': 5093.207044124603, 'accumulated_submission_time': 4647.0611782073975, 'accumulated_eval_time': 444.2060983181, 'accumulated_logging_time': 0.5600624084472656, 'global_step': 12033, 'preemption_count': 0}), (13356, {'train/accuracy': 0.6477399468421936, 'train/loss': 1.4090443849563599, 'validation/accuracy': 0.586899995803833, 'validation/loss': 1.7182719707489014, 'validation/num_examples': 50000, 'test/accuracy': 0.45590001344680786, 'test/loss': 2.4721691608428955, 'test/num_examples': 10000, 'score': 5156.9120326042175, 'total_duration': 5645.0649626255035, 'accumulated_submission_time': 5156.9120326042175, 'accumulated_eval_time': 485.9937734603882, 'accumulated_logging_time': 0.6360607147216797, 'global_step': 13356, 'preemption_count': 0}), (14674, {'train/accuracy': 0.6534597873687744, 'train/loss': 1.3843954801559448, 'validation/accuracy': 0.5932999849319458, 'validation/loss': 1.7101521492004395, 'validation/num_examples': 50000, 'test/accuracy': 0.4629000127315521, 'test/loss': 2.4998021125793457, 'test/num_examples': 10000, 'score': 5666.989051818848, 'total_duration': 6201.526761770248, 'accumulated_submission_time': 5666.989051818848, 'accumulated_eval_time': 532.1459667682648, 'accumulated_logging_time': 0.7269675731658936, 'global_step': 14674, 'preemption_count': 0}), (15995, {'train/accuracy': 0.6622090339660645, 'train/loss': 1.342408299446106, 'validation/accuracy': 0.5983999967575073, 'validation/loss': 1.6766204833984375, 'validation/num_examples': 50000, 'test/accuracy': 0.4706000089645386, 'test/loss': 2.452061414718628, 'test/num_examples': 10000, 'score': 6177.107513427734, 'total_duration': 6756.300977230072, 'accumulated_submission_time': 6177.107513427734, 'accumulated_eval_time': 576.5640850067139, 'accumulated_logging_time': 0.8178987503051758, 'global_step': 15995, 'preemption_count': 0}), (17312, {'train/accuracy': 0.6724928021430969, 'train/loss': 1.2972861528396606, 'validation/accuracy': 0.6060999631881714, 'validation/loss': 1.6379177570343018, 'validation/num_examples': 50000, 'test/accuracy': 0.48330003023147583, 'test/loss': 2.385103225708008, 'test/num_examples': 10000, 'score': 6686.900541543961, 'total_duration': 7308.9928143024445, 'accumulated_submission_time': 6686.900541543961, 'accumulated_eval_time': 619.2302832603455, 'accumulated_logging_time': 0.9116013050079346, 'global_step': 17312, 'preemption_count': 0}), (18635, {'train/accuracy': 0.6758410334587097, 'train/loss': 1.2892130613327026, 'validation/accuracy': 0.6082800030708313, 'validation/loss': 1.614640235900879, 'validation/num_examples': 50000, 'test/accuracy': 0.4741000235080719, 'test/loss': 2.3687267303466797, 'test/num_examples': 10000, 'score': 7196.809233188629, 'total_duration': 7860.610020399094, 'accumulated_submission_time': 7196.809233188629, 'accumulated_eval_time': 660.7685797214508, 'accumulated_logging_time': 0.946190357208252, 'global_step': 18635, 'preemption_count': 0}), (19954, {'train/accuracy': 0.6624082922935486, 'train/loss': 1.3506443500518799, 'validation/accuracy': 0.6001999974250793, 'validation/loss': 1.6645383834838867, 'validation/num_examples': 50000, 'test/accuracy': 0.47300001978874207, 'test/loss': 2.4019932746887207, 'test/num_examples': 10000, 'score': 7706.858216047287, 'total_duration': 8412.666400671005, 'accumulated_submission_time': 7706.858216047287, 'accumulated_eval_time': 702.6159074306488, 'accumulated_logging_time': 0.9797472953796387, 'global_step': 19954, 'preemption_count': 0}), (21282, {'train/accuracy': 0.6584023833274841, 'train/loss': 1.3560289144515991, 'validation/accuracy': 0.596340000629425, 'validation/loss': 1.696319818496704, 'validation/num_examples': 50000, 'test/accuracy': 0.47360002994537354, 'test/loss': 2.440589666366577, 'test/num_examples': 10000, 'score': 8216.837393283844, 'total_duration': 8962.071113348007, 'accumulated_submission_time': 8216.837393283844, 'accumulated_eval_time': 741.8369216918945, 'accumulated_logging_time': 1.0525918006896973, 'global_step': 21282, 'preemption_count': 0}), (22606, {'train/accuracy': 0.6674904227256775, 'train/loss': 1.3119453191757202, 'validation/accuracy': 0.604420006275177, 'validation/loss': 1.655828833580017, 'validation/num_examples': 50000, 'test/accuracy': 0.48410001397132874, 'test/loss': 2.401235342025757, 'test/num_examples': 10000, 'score': 8726.7188642025, 'total_duration': 9512.470099449158, 'accumulated_submission_time': 8726.7188642025, 'accumulated_eval_time': 782.167638540268, 'accumulated_logging_time': 1.1036078929901123, 'global_step': 22606, 'preemption_count': 0}), (23925, {'train/accuracy': 0.6863042116165161, 'train/loss': 1.2308433055877686, 'validation/accuracy': 0.6212999820709229, 'validation/loss': 1.570596694946289, 'validation/num_examples': 50000, 'test/accuracy': 0.4953000247478485, 'test/loss': 2.3210699558258057, 'test/num_examples': 10000, 'score': 9236.770344734192, 'total_duration': 10067.995245695114, 'accumulated_submission_time': 9236.770344734192, 'accumulated_eval_time': 827.4673681259155, 'accumulated_logging_time': 1.142059087753296, 'global_step': 23925, 'preemption_count': 0}), (25242, {'train/accuracy': 0.6750438213348389, 'train/loss': 1.2924892902374268, 'validation/accuracy': 0.6092999577522278, 'validation/loss': 1.6240028142929077, 'validation/num_examples': 50000, 'test/accuracy': 0.4871000349521637, 'test/loss': 2.398207426071167, 'test/num_examples': 10000, 'score': 9746.749855041504, 'total_duration': 10622.606882333755, 'accumulated_submission_time': 9746.749855041504, 'accumulated_eval_time': 871.8626046180725, 'accumulated_logging_time': 1.243048906326294, 'global_step': 25242, 'preemption_count': 0}), (26558, {'train/accuracy': 0.6721341013908386, 'train/loss': 1.2982786893844604, 'validation/accuracy': 0.6142599582672119, 'validation/loss': 1.6181321144104004, 'validation/num_examples': 50000, 'test/accuracy': 0.48830002546310425, 'test/loss': 2.3275299072265625, 'test/num_examples': 10000, 'score': 10256.790889978409, 'total_duration': 11177.231453895569, 'accumulated_submission_time': 10256.790889978409, 'accumulated_eval_time': 916.259539604187, 'accumulated_logging_time': 1.2962830066680908, 'global_step': 26558, 'preemption_count': 0}), (27879, {'train/accuracy': 0.6741868257522583, 'train/loss': 1.301221251487732, 'validation/accuracy': 0.6135199666023254, 'validation/loss': 1.6117511987686157, 'validation/num_examples': 50000, 'test/accuracy': 0.4880000352859497, 'test/loss': 2.358656644821167, 'test/num_examples': 10000, 'score': 10766.786980390549, 'total_duration': 11727.57971572876, 'accumulated_submission_time': 10766.786980390549, 'accumulated_eval_time': 956.4376459121704, 'accumulated_logging_time': 1.3355915546417236, 'global_step': 27879, 'preemption_count': 0}), (29200, {'train/accuracy': 0.6728914380073547, 'train/loss': 1.287576675415039, 'validation/accuracy': 0.6141999959945679, 'validation/loss': 1.6045410633087158, 'validation/num_examples': 50000, 'test/accuracy': 0.48820000886917114, 'test/loss': 2.3589296340942383, 'test/num_examples': 10000, 'score': 11276.647282123566, 'total_duration': 12282.067492246628, 'accumulated_submission_time': 11276.647282123566, 'accumulated_eval_time': 1000.7763388156891, 'accumulated_logging_time': 1.4897840023040771, 'global_step': 29200, 'preemption_count': 0}), (30517, {'train/accuracy': 0.6762595772743225, 'train/loss': 1.2833436727523804, 'validation/accuracy': 0.6150599718093872, 'validation/loss': 1.6105945110321045, 'validation/num_examples': 50000, 'test/accuracy': 0.4961000382900238, 'test/loss': 2.325723648071289, 'test/num_examples': 10000, 'score': 11786.69568514824, 'total_duration': 12834.81475353241, 'accumulated_submission_time': 11786.69568514824, 'accumulated_eval_time': 1043.2646017074585, 'accumulated_logging_time': 1.561824083328247, 'global_step': 30517, 'preemption_count': 0}), (31834, {'train/accuracy': 0.6870814561843872, 'train/loss': 1.224181890487671, 'validation/accuracy': 0.6255599856376648, 'validation/loss': 1.556512475013733, 'validation/num_examples': 50000, 'test/accuracy': 0.5060000419616699, 'test/loss': 2.2561988830566406, 'test/num_examples': 10000, 'score': 12296.54272723198, 'total_duration': 13383.896453380585, 'accumulated_submission_time': 12296.54272723198, 'accumulated_eval_time': 1082.2830548286438, 'accumulated_logging_time': 1.6350719928741455, 'global_step': 31834, 'preemption_count': 0}), (33159, {'train/accuracy': 0.6834940910339355, 'train/loss': 1.244361400604248, 'validation/accuracy': 0.6220200061798096, 'validation/loss': 1.579487919807434, 'validation/num_examples': 50000, 'test/accuracy': 0.49400001764297485, 'test/loss': 2.3179776668548584, 'test/num_examples': 10000, 'score': 12806.397213220596, 'total_duration': 13935.638862133026, 'accumulated_submission_time': 12806.397213220596, 'accumulated_eval_time': 1123.9775485992432, 'accumulated_logging_time': 1.6861281394958496, 'global_step': 33159, 'preemption_count': 0}), (34476, {'train/accuracy': 0.682059109210968, 'train/loss': 1.257495641708374, 'validation/accuracy': 0.6212599873542786, 'validation/loss': 1.574296236038208, 'validation/num_examples': 50000, 'test/accuracy': 0.4919000267982483, 'test/loss': 2.345731496810913, 'test/num_examples': 10000, 'score': 13316.319287776947, 'total_duration': 14485.336347579956, 'accumulated_submission_time': 13316.319287776947, 'accumulated_eval_time': 1163.5175640583038, 'accumulated_logging_time': 1.782294511795044, 'global_step': 34476, 'preemption_count': 0}), (35792, {'train/accuracy': 0.6935387253761292, 'train/loss': 1.2039399147033691, 'validation/accuracy': 0.6328399777412415, 'validation/loss': 1.5139334201812744, 'validation/num_examples': 50000, 'test/accuracy': 0.4994000196456909, 'test/loss': 2.279236078262329, 'test/num_examples': 10000, 'score': 13826.368644952774, 'total_duration': 15031.192873477936, 'accumulated_submission_time': 13826.368644952774, 'accumulated_eval_time': 1199.0900118350983, 'accumulated_logging_time': 1.8780553340911865, 'global_step': 35792, 'preemption_count': 0}), (37109, {'train/accuracy': 0.6819794178009033, 'train/loss': 1.2457482814788818, 'validation/accuracy': 0.620419979095459, 'validation/loss': 1.5681740045547485, 'validation/num_examples': 50000, 'test/accuracy': 0.49900001287460327, 'test/loss': 2.3057148456573486, 'test/num_examples': 10000, 'score': 14336.162461996078, 'total_duration': 15576.757170677185, 'accumulated_submission_time': 14336.162461996078, 'accumulated_eval_time': 1234.5993437767029, 'accumulated_logging_time': 1.999044418334961, 'global_step': 37109, 'preemption_count': 0}), (38427, {'train/accuracy': 0.6908681392669678, 'train/loss': 1.2203547954559326, 'validation/accuracy': 0.6298999786376953, 'validation/loss': 1.5351871252059937, 'validation/num_examples': 50000, 'test/accuracy': 0.5031999945640564, 'test/loss': 2.2728452682495117, 'test/num_examples': 10000, 'score': 14845.872792005539, 'total_duration': 16125.867277383804, 'accumulated_submission_time': 14845.872792005539, 'accumulated_eval_time': 1273.6868858337402, 'accumulated_logging_time': 2.1693007946014404, 'global_step': 38427, 'preemption_count': 0}), (39744, {'train/accuracy': 0.6814612150192261, 'train/loss': 1.255134105682373, 'validation/accuracy': 0.6175000071525574, 'validation/loss': 1.583661675453186, 'validation/num_examples': 50000, 'test/accuracy': 0.492900013923645, 'test/loss': 2.313690662384033, 'test/num_examples': 10000, 'score': 15355.978217601776, 'total_duration': 16670.53989672661, 'accumulated_submission_time': 15355.978217601776, 'accumulated_eval_time': 1308.0186321735382, 'accumulated_logging_time': 2.2670600414276123, 'global_step': 39744, 'preemption_count': 0}), (41061, {'train/accuracy': 0.6843909025192261, 'train/loss': 1.2503833770751953, 'validation/accuracy': 0.6226999759674072, 'validation/loss': 1.5629533529281616, 'validation/num_examples': 50000, 'test/accuracy': 0.4952000379562378, 'test/loss': 2.304218053817749, 'test/num_examples': 10000, 'score': 15865.581531047821, 'total_duration': 17220.24215555191, 'accumulated_submission_time': 15865.581531047821, 'accumulated_eval_time': 1347.7116103172302, 'accumulated_logging_time': 2.5246074199676514, 'global_step': 41061, 'preemption_count': 0}), (42375, {'train/accuracy': 0.7084462642669678, 'train/loss': 1.1190699338912964, 'validation/accuracy': 0.6439399719238281, 'validation/loss': 1.4614160060882568, 'validation/num_examples': 50000, 'test/accuracy': 0.5172000527381897, 'test/loss': 2.173079490661621, 'test/num_examples': 10000, 'score': 16375.375456809998, 'total_duration': 17766.88316130638, 'accumulated_submission_time': 16375.375456809998, 'accumulated_eval_time': 1384.2973074913025, 'accumulated_logging_time': 2.643423080444336, 'global_step': 42375, 'preemption_count': 0}), (43690, {'train/accuracy': 0.6755420565605164, 'train/loss': 1.2704815864562988, 'validation/accuracy': 0.6170399785041809, 'validation/loss': 1.586108684539795, 'validation/num_examples': 50000, 'test/accuracy': 0.492000013589859, 'test/loss': 2.3367161750793457, 'test/num_examples': 10000, 'score': 16885.124462127686, 'total_duration': 18313.922828435898, 'accumulated_submission_time': 16885.124462127686, 'accumulated_eval_time': 1421.2764596939087, 'accumulated_logging_time': 2.814091205596924, 'global_step': 43690, 'preemption_count': 0}), (45006, {'train/accuracy': 0.7000757455825806, 'train/loss': 1.1672563552856445, 'validation/accuracy': 0.636959969997406, 'validation/loss': 1.4977331161499023, 'validation/num_examples': 50000, 'test/accuracy': 0.508400022983551, 'test/loss': 2.257812976837158, 'test/num_examples': 10000, 'score': 17394.98658514023, 'total_duration': 18858.27737212181, 'accumulated_submission_time': 17394.98658514023, 'accumulated_eval_time': 1455.4926941394806, 'accumulated_logging_time': 2.9451756477355957, 'global_step': 45006, 'preemption_count': 0}), (46322, {'train/accuracy': 0.6997169852256775, 'train/loss': 1.1739345788955688, 'validation/accuracy': 0.6357199549674988, 'validation/loss': 1.5046179294586182, 'validation/num_examples': 50000, 'test/accuracy': 0.498600035905838, 'test/loss': 2.2680470943450928, 'test/num_examples': 10000, 'score': 17904.942356348038, 'total_duration': 19405.47530937195, 'accumulated_submission_time': 17904.942356348038, 'accumulated_eval_time': 1492.449557542801, 'accumulated_logging_time': 3.084902048110962, 'global_step': 46322, 'preemption_count': 0}), (47637, {'train/accuracy': 0.700593888759613, 'train/loss': 1.1661262512207031, 'validation/accuracy': 0.6414399743080139, 'validation/loss': 1.4891859292984009, 'validation/num_examples': 50000, 'test/accuracy': 0.5142000317573547, 'test/loss': 2.2373485565185547, 'test/num_examples': 10000, 'score': 18415.017733097076, 'total_duration': 19955.47287249565, 'accumulated_submission_time': 18415.017733097076, 'accumulated_eval_time': 1532.1526184082031, 'accumulated_logging_time': 3.162531852722168, 'global_step': 47637, 'preemption_count': 0}), (48952, {'train/accuracy': 0.6927614808082581, 'train/loss': 1.2026716470718384, 'validation/accuracy': 0.6314799785614014, 'validation/loss': 1.526813268661499, 'validation/num_examples': 50000, 'test/accuracy': 0.5072000026702881, 'test/loss': 2.239039897918701, 'test/num_examples': 10000, 'score': 18924.90866971016, 'total_duration': 20507.276310682297, 'accumulated_submission_time': 18924.90866971016, 'accumulated_eval_time': 1573.8062844276428, 'accumulated_logging_time': 3.278242826461792, 'global_step': 48952, 'preemption_count': 0}), (50267, {'train/accuracy': 0.7000358700752258, 'train/loss': 1.1629594564437866, 'validation/accuracy': 0.6376599669456482, 'validation/loss': 1.5028278827667236, 'validation/num_examples': 50000, 'test/accuracy': 0.5098000168800354, 'test/loss': 2.257024049758911, 'test/num_examples': 10000, 'score': 19434.730928182602, 'total_duration': 21057.665006875992, 'accumulated_submission_time': 19434.730928182602, 'accumulated_eval_time': 1614.0688846111298, 'accumulated_logging_time': 3.4351966381073, 'global_step': 50267, 'preemption_count': 0}), (51576, {'train/accuracy': 0.7090840339660645, 'train/loss': 1.1309502124786377, 'validation/accuracy': 0.6428399682044983, 'validation/loss': 1.4667868614196777, 'validation/num_examples': 50000, 'test/accuracy': 0.5195000171661377, 'test/loss': 2.168747901916504, 'test/num_examples': 10000, 'score': 19944.843731164932, 'total_duration': 21605.753293275833, 'accumulated_submission_time': 19944.843731164932, 'accumulated_eval_time': 1651.798335313797, 'accumulated_logging_time': 3.536867380142212, 'global_step': 51576, 'preemption_count': 0}), (52886, {'train/accuracy': 0.6998963356018066, 'train/loss': 1.1588245630264282, 'validation/accuracy': 0.6385399699211121, 'validation/loss': 1.4866459369659424, 'validation/num_examples': 50000, 'test/accuracy': 0.5121999979019165, 'test/loss': 2.2063119411468506, 'test/num_examples': 10000, 'score': 20454.87086224556, 'total_duration': 22149.987450361252, 'accumulated_submission_time': 20454.87086224556, 'accumulated_eval_time': 1685.7457132339478, 'accumulated_logging_time': 3.6505279541015625, 'global_step': 52886, 'preemption_count': 0}), (54197, {'train/accuracy': 0.7091836333274841, 'train/loss': 1.128458857536316, 'validation/accuracy': 0.6462799906730652, 'validation/loss': 1.4478954076766968, 'validation/num_examples': 50000, 'test/accuracy': 0.5200999975204468, 'test/loss': 2.172381639480591, 'test/num_examples': 10000, 'score': 20964.741470336914, 'total_duration': 22697.48745083809, 'accumulated_submission_time': 20964.741470336914, 'accumulated_eval_time': 1723.1101608276367, 'accumulated_logging_time': 3.778111219406128, 'global_step': 54197, 'preemption_count': 0}), (55505, {'train/accuracy': 0.7135483026504517, 'train/loss': 1.104453444480896, 'validation/accuracy': 0.6495999693870544, 'validation/loss': 1.4517191648483276, 'validation/num_examples': 50000, 'test/accuracy': 0.5208000540733337, 'test/loss': 2.1793429851531982, 'test/num_examples': 10000, 'score': 21474.750054359436, 'total_duration': 23243.942525863647, 'accumulated_submission_time': 21474.750054359436, 'accumulated_eval_time': 1759.3069467544556, 'accumulated_logging_time': 3.8836164474487305, 'global_step': 55505, 'preemption_count': 0}), (56808, {'train/accuracy': 0.7122129797935486, 'train/loss': 1.115829348564148, 'validation/accuracy': 0.6426799893379211, 'validation/loss': 1.4683713912963867, 'validation/num_examples': 50000, 'test/accuracy': 0.5197000503540039, 'test/loss': 2.186711549758911, 'test/num_examples': 10000, 'score': 21984.769142866135, 'total_duration': 23787.571313381195, 'accumulated_submission_time': 21984.769142866135, 'accumulated_eval_time': 1792.609483242035, 'accumulated_logging_time': 4.043093919754028, 'global_step': 56808, 'preemption_count': 0}), (58009, {'train/accuracy': 0.7186702489852905, 'train/loss': 1.0814591646194458, 'validation/accuracy': 0.6558200120925903, 'validation/loss': 1.4147255420684814, 'validation/num_examples': 50000, 'test/accuracy': 0.5295000076293945, 'test/loss': 2.159853458404541, 'test/num_examples': 10000, 'score': 22494.511714935303, 'total_duration': 24337.666731119156, 'accumulated_submission_time': 22494.511714935303, 'accumulated_eval_time': 1832.69313955307, 'accumulated_logging_time': 4.169570684432983, 'global_step': 58009, 'preemption_count': 0}), (59296, {'train/accuracy': 0.7154216766357422, 'train/loss': 1.0934923887252808, 'validation/accuracy': 0.6480000019073486, 'validation/loss': 1.4511841535568237, 'validation/num_examples': 50000, 'test/accuracy': 0.5241000056266785, 'test/loss': 2.161376953125, 'test/num_examples': 10000, 'score': 23004.466549158096, 'total_duration': 24885.22984647751, 'accumulated_submission_time': 23004.466549158096, 'accumulated_eval_time': 1869.99351644516, 'accumulated_logging_time': 4.32004189491272, 'global_step': 59296, 'preemption_count': 0}), (60576, {'train/accuracy': 0.7141661047935486, 'train/loss': 1.122248649597168, 'validation/accuracy': 0.6419999599456787, 'validation/loss': 1.4695637226104736, 'validation/num_examples': 50000, 'test/accuracy': 0.5178000330924988, 'test/loss': 2.1612889766693115, 'test/num_examples': 10000, 'score': 23514.23480796814, 'total_duration': 25431.32913160324, 'accumulated_submission_time': 23514.23480796814, 'accumulated_eval_time': 1905.9896619319916, 'accumulated_logging_time': 4.4986419677734375, 'global_step': 60576, 'preemption_count': 0}), (61761, {'train/accuracy': 0.727957546710968, 'train/loss': 1.0601836442947388, 'validation/accuracy': 0.6521399617195129, 'validation/loss': 1.4187124967575073, 'validation/num_examples': 50000, 'test/accuracy': 0.5294000506401062, 'test/loss': 2.111990213394165, 'test/num_examples': 10000, 'score': 24024.12597298622, 'total_duration': 25977.76452112198, 'accumulated_submission_time': 24024.12597298622, 'accumulated_eval_time': 1942.2671494483948, 'accumulated_logging_time': 4.62351655960083, 'global_step': 61761, 'preemption_count': 0}), (63033, {'train/accuracy': 0.709980845451355, 'train/loss': 1.105270504951477, 'validation/accuracy': 0.6415199637413025, 'validation/loss': 1.4797725677490234, 'validation/num_examples': 50000, 'test/accuracy': 0.5175999999046326, 'test/loss': 2.203636407852173, 'test/num_examples': 10000, 'score': 24533.935073375702, 'total_duration': 26519.287886857986, 'accumulated_submission_time': 24533.935073375702, 'accumulated_eval_time': 1973.7464096546173, 'accumulated_logging_time': 4.706680774688721, 'global_step': 63033, 'preemption_count': 0}), (64269, {'train/accuracy': 0.721121609210968, 'train/loss': 1.0733492374420166, 'validation/accuracy': 0.647379994392395, 'validation/loss': 1.4394407272338867, 'validation/num_examples': 50000, 'test/accuracy': 0.5190000534057617, 'test/loss': 2.194363594055176, 'test/num_examples': 10000, 'score': 25043.733003139496, 'total_duration': 27064.188108682632, 'accumulated_submission_time': 25043.733003139496, 'accumulated_eval_time': 2008.56631398201, 'accumulated_logging_time': 4.842400312423706, 'global_step': 64269, 'preemption_count': 0}), (65497, {'train/accuracy': 0.7298309803009033, 'train/loss': 1.0494109392166138, 'validation/accuracy': 0.6518999934196472, 'validation/loss': 1.4330863952636719, 'validation/num_examples': 50000, 'test/accuracy': 0.5172000527381897, 'test/loss': 2.2050514221191406, 'test/num_examples': 10000, 'score': 25553.560755729675, 'total_duration': 27609.641005516052, 'accumulated_submission_time': 25553.560755729675, 'accumulated_eval_time': 2043.9270718097687, 'accumulated_logging_time': 4.9603271484375, 'global_step': 65497, 'preemption_count': 0}), (66665, {'train/accuracy': 0.7349728941917419, 'train/loss': 1.0126609802246094, 'validation/accuracy': 0.6545799970626831, 'validation/loss': 1.403482437133789, 'validation/num_examples': 50000, 'test/accuracy': 0.5242000222206116, 'test/loss': 2.132732629776001, 'test/num_examples': 10000, 'score': 26063.64106655121, 'total_duration': 28155.589996814728, 'accumulated_submission_time': 26063.64106655121, 'accumulated_eval_time': 2079.5424258708954, 'accumulated_logging_time': 5.076892375946045, 'global_step': 66665, 'preemption_count': 0}), (67868, {'train/accuracy': 0.7333784699440002, 'train/loss': 1.0174095630645752, 'validation/accuracy': 0.6527599692344666, 'validation/loss': 1.4294004440307617, 'validation/num_examples': 50000, 'test/accuracy': 0.52510005235672, 'test/loss': 2.167860507965088, 'test/num_examples': 10000, 'score': 26573.65345621109, 'total_duration': 28700.756385087967, 'accumulated_submission_time': 26573.65345621109, 'accumulated_eval_time': 2114.3731939792633, 'accumulated_logging_time': 5.257038116455078, 'global_step': 67868, 'preemption_count': 0}), (69046, {'train/accuracy': 0.7481465339660645, 'train/loss': 0.9577359557151794, 'validation/accuracy': 0.65065997838974, 'validation/loss': 1.4381996393203735, 'validation/num_examples': 50000, 'test/accuracy': 0.5209000110626221, 'test/loss': 2.1692967414855957, 'test/num_examples': 10000, 'score': 27083.43567442894, 'total_duration': 29244.413563728333, 'accumulated_submission_time': 27083.43567442894, 'accumulated_eval_time': 2147.986088991165, 'accumulated_logging_time': 5.380809783935547, 'global_step': 69046, 'preemption_count': 0}), (70286, {'train/accuracy': 0.7789381146430969, 'train/loss': 0.8355110883712769, 'validation/accuracy': 0.663159966468811, 'validation/loss': 1.3805304765701294, 'validation/num_examples': 50000, 'test/accuracy': 0.526900053024292, 'test/loss': 2.1390299797058105, 'test/num_examples': 10000, 'score': 27593.508466959, 'total_duration': 29792.2626080513, 'accumulated_submission_time': 27593.508466959, 'accumulated_eval_time': 2185.425276994705, 'accumulated_logging_time': 5.573187589645386, 'global_step': 70286, 'preemption_count': 0}), (71538, {'train/accuracy': 0.7571149468421936, 'train/loss': 0.935540497303009, 'validation/accuracy': 0.6527799963951111, 'validation/loss': 1.4146453142166138, 'validation/num_examples': 50000, 'test/accuracy': 0.5223000049591064, 'test/loss': 2.1298270225524902, 'test/num_examples': 10000, 'score': 28103.37496995926, 'total_duration': 30339.924479722977, 'accumulated_submission_time': 28103.37496995926, 'accumulated_eval_time': 2222.906327724457, 'accumulated_logging_time': 5.740803003311157, 'global_step': 71538, 'preemption_count': 0}), (72588, {'train/accuracy': 0.7161591053009033, 'train/loss': 1.095617413520813, 'validation/accuracy': 0.6501399874687195, 'validation/loss': 1.4448403120040894, 'validation/num_examples': 50000, 'test/accuracy': 0.515500009059906, 'test/loss': 2.2124476432800293, 'test/num_examples': 10000, 'score': 28613.17933011055, 'total_duration': 30887.644747257233, 'accumulated_submission_time': 28613.17933011055, 'accumulated_eval_time': 2260.5749423503876, 'accumulated_logging_time': 5.866234302520752, 'global_step': 72588, 'preemption_count': 0}), (73787, {'train/accuracy': 0.7195471525192261, 'train/loss': 1.0715832710266113, 'validation/accuracy': 0.6534599661827087, 'validation/loss': 1.4173479080200195, 'validation/num_examples': 50000, 'test/accuracy': 0.5235000252723694, 'test/loss': 2.1732168197631836, 'test/num_examples': 10000, 'score': 29123.30188345909, 'total_duration': 31438.068590402603, 'accumulated_submission_time': 29123.30188345909, 'accumulated_eval_time': 2300.630066871643, 'accumulated_logging_time': 5.974643707275391, 'global_step': 73787, 'preemption_count': 0}), (74828, {'train/accuracy': 0.7311463356018066, 'train/loss': 1.0393157005310059, 'validation/accuracy': 0.6581000089645386, 'validation/loss': 1.3984721899032593, 'validation/num_examples': 50000, 'test/accuracy': 0.5253000259399414, 'test/loss': 2.1217665672302246, 'test/num_examples': 10000, 'score': 29633.388894557953, 'total_duration': 31982.32825422287, 'accumulated_submission_time': 29633.388894557953, 'accumulated_eval_time': 2334.558949947357, 'accumulated_logging_time': 6.0988829135894775, 'global_step': 74828, 'preemption_count': 0}), (75872, {'train/accuracy': 0.7360092401504517, 'train/loss': 1.005659580230713, 'validation/accuracy': 0.6552000045776367, 'validation/loss': 1.4164284467697144, 'validation/num_examples': 50000, 'test/accuracy': 0.5297000408172607, 'test/loss': 2.1351070404052734, 'test/num_examples': 10000, 'score': 30143.615664958954, 'total_duration': 32528.45977473259, 'accumulated_submission_time': 30143.615664958954, 'accumulated_eval_time': 2370.203828573227, 'accumulated_logging_time': 6.238525629043579, 'global_step': 75872, 'preemption_count': 0}), (77104, {'train/accuracy': 0.7465720772743225, 'train/loss': 0.9621585011482239, 'validation/accuracy': 0.6601200103759766, 'validation/loss': 1.3823796510696411, 'validation/num_examples': 50000, 'test/accuracy': 0.5356000065803528, 'test/loss': 2.0757267475128174, 'test/num_examples': 10000, 'score': 30653.403213977814, 'total_duration': 33075.49897146225, 'accumulated_submission_time': 30653.403213977814, 'accumulated_eval_time': 2407.1570892333984, 'accumulated_logging_time': 6.396036624908447, 'global_step': 77104, 'preemption_count': 0}), (78066, {'train/accuracy': 0.7436224222183228, 'train/loss': 0.981124758720398, 'validation/accuracy': 0.66211998462677, 'validation/loss': 1.3890818357467651, 'validation/num_examples': 50000, 'test/accuracy': 0.536300003528595, 'test/loss': 2.1141469478607178, 'test/num_examples': 10000, 'score': 31163.74857854843, 'total_duration': 33622.069925546646, 'accumulated_submission_time': 31163.74857854843, 'accumulated_eval_time': 2443.110126018524, 'accumulated_logging_time': 6.558728218078613, 'global_step': 78066, 'preemption_count': 0}), (79129, {'train/accuracy': 0.7447185516357422, 'train/loss': 0.9756001830101013, 'validation/accuracy': 0.6759399771690369, 'validation/loss': 1.3317054510116577, 'validation/num_examples': 50000, 'test/accuracy': 0.5445000529289246, 'test/loss': 2.052213191986084, 'test/num_examples': 10000, 'score': 31673.886323451996, 'total_duration': 34171.79928994179, 'accumulated_submission_time': 31673.886323451996, 'accumulated_eval_time': 2482.4763338565826, 'accumulated_logging_time': 6.662399768829346, 'global_step': 79129, 'preemption_count': 0}), (80269, {'train/accuracy': 0.7380221486091614, 'train/loss': 1.000010371208191, 'validation/accuracy': 0.6665799617767334, 'validation/loss': 1.362206220626831, 'validation/num_examples': 50000, 'test/accuracy': 0.5385000109672546, 'test/loss': 2.1026439666748047, 'test/num_examples': 10000, 'score': 32183.937160491943, 'total_duration': 34714.98567056656, 'accumulated_submission_time': 32183.937160491943, 'accumulated_eval_time': 2515.368390083313, 'accumulated_logging_time': 6.772404193878174, 'global_step': 80269, 'preemption_count': 0}), (81573, {'train/accuracy': 0.7387993931770325, 'train/loss': 0.9945427775382996, 'validation/accuracy': 0.6657199859619141, 'validation/loss': 1.3603932857513428, 'validation/num_examples': 50000, 'test/accuracy': 0.541100025177002, 'test/loss': 2.0774476528167725, 'test/num_examples': 10000, 'score': 32693.982009410858, 'total_duration': 35256.82742023468, 'accumulated_submission_time': 32693.982009410858, 'accumulated_eval_time': 2546.8929257392883, 'accumulated_logging_time': 6.901867389678955, 'global_step': 81573, 'preemption_count': 0}), (82866, {'train/accuracy': 0.7383410334587097, 'train/loss': 0.9955034852027893, 'validation/accuracy': 0.6617799997329712, 'validation/loss': 1.3765740394592285, 'validation/num_examples': 50000, 'test/accuracy': 0.5332000255584717, 'test/loss': 2.1239564418792725, 'test/num_examples': 10000, 'score': 33203.71964597702, 'total_duration': 35794.940029382706, 'accumulated_submission_time': 33203.71964597702, 'accumulated_eval_time': 2574.9742288589478, 'accumulated_logging_time': 7.051493167877197, 'global_step': 82866, 'preemption_count': 0}), (84152, {'train/accuracy': 0.7338767647743225, 'train/loss': 1.0242502689361572, 'validation/accuracy': 0.6631399989128113, 'validation/loss': 1.3823490142822266, 'validation/num_examples': 50000, 'test/accuracy': 0.5340999960899353, 'test/loss': 2.1232519149780273, 'test/num_examples': 10000, 'score': 33713.52746009827, 'total_duration': 36335.43326711655, 'accumulated_submission_time': 33713.52746009827, 'accumulated_eval_time': 2605.406754732132, 'accumulated_logging_time': 7.158366441726685, 'global_step': 84152, 'preemption_count': 0}), (85431, {'train/accuracy': 0.7504982352256775, 'train/loss': 0.9383268356323242, 'validation/accuracy': 0.674560010433197, 'validation/loss': 1.3241186141967773, 'validation/num_examples': 50000, 'test/accuracy': 0.5450000166893005, 'test/loss': 2.051173210144043, 'test/num_examples': 10000, 'score': 34223.25968837738, 'total_duration': 36878.22714519501, 'accumulated_submission_time': 34223.25968837738, 'accumulated_eval_time': 2638.1974868774414, 'accumulated_logging_time': 7.280217170715332, 'global_step': 85431, 'preemption_count': 0}), (86705, {'train/accuracy': 0.7451769709587097, 'train/loss': 0.9667693972587585, 'validation/accuracy': 0.6660400032997131, 'validation/loss': 1.360815405845642, 'validation/num_examples': 50000, 'test/accuracy': 0.5379000306129456, 'test/loss': 2.1146790981292725, 'test/num_examples': 10000, 'score': 34733.12170672417, 'total_duration': 37418.418092012405, 'accumulated_submission_time': 34733.12170672417, 'accumulated_eval_time': 2668.2184591293335, 'accumulated_logging_time': 7.439556837081909, 'global_step': 86705, 'preemption_count': 0}), (87973, {'train/accuracy': 0.7443398833274841, 'train/loss': 0.9653928279876709, 'validation/accuracy': 0.669979989528656, 'validation/loss': 1.360177755355835, 'validation/num_examples': 50000, 'test/accuracy': 0.5406000018119812, 'test/loss': 2.0746076107025146, 'test/num_examples': 10000, 'score': 35242.965539455414, 'total_duration': 37956.617122888565, 'accumulated_submission_time': 35242.965539455414, 'accumulated_eval_time': 2696.279561519623, 'accumulated_logging_time': 7.583616495132446, 'global_step': 87973, 'preemption_count': 0}), (89241, {'train/accuracy': 0.741230845451355, 'train/loss': 0.983771562576294, 'validation/accuracy': 0.6613799929618835, 'validation/loss': 1.3798116445541382, 'validation/num_examples': 50000, 'test/accuracy': 0.5296000242233276, 'test/loss': 2.1156599521636963, 'test/num_examples': 10000, 'score': 35752.98774909973, 'total_duration': 38494.3547039032, 'accumulated_submission_time': 35752.98774909973, 'accumulated_eval_time': 2723.707176923752, 'accumulated_logging_time': 7.722415208816528, 'global_step': 89241, 'preemption_count': 0}), (90506, {'train/accuracy': 0.7584900856018066, 'train/loss': 0.91380774974823, 'validation/accuracy': 0.6789199709892273, 'validation/loss': 1.2958568334579468, 'validation/num_examples': 50000, 'test/accuracy': 0.5558000206947327, 'test/loss': 1.9668797254562378, 'test/num_examples': 10000, 'score': 36262.987285375595, 'total_duration': 39031.837233781815, 'accumulated_submission_time': 36262.987285375595, 'accumulated_eval_time': 2750.9008872509003, 'accumulated_logging_time': 7.857999801635742, 'global_step': 90506, 'preemption_count': 0}), (91772, {'train/accuracy': 0.7578722834587097, 'train/loss': 0.9105977416038513, 'validation/accuracy': 0.6764999628067017, 'validation/loss': 1.3148294687271118, 'validation/num_examples': 50000, 'test/accuracy': 0.5533000230789185, 'test/loss': 2.0216121673583984, 'test/num_examples': 10000, 'score': 36772.89505529404, 'total_duration': 39569.59167146683, 'accumulated_submission_time': 36772.89505529404, 'accumulated_eval_time': 2778.476126432419, 'accumulated_logging_time': 7.977968454360962, 'global_step': 91772, 'preemption_count': 0}), (93037, {'train/accuracy': 0.7685148119926453, 'train/loss': 0.8690674304962158, 'validation/accuracy': 0.6793599724769592, 'validation/loss': 1.2999283075332642, 'validation/num_examples': 50000, 'test/accuracy': 0.5532000064849854, 'test/loss': 2.01275372505188, 'test/num_examples': 10000, 'score': 37282.67379975319, 'total_duration': 40106.97201538086, 'accumulated_submission_time': 37282.67379975319, 'accumulated_eval_time': 2805.785019159317, 'accumulated_logging_time': 8.116628885269165, 'global_step': 93037, 'preemption_count': 0}), (94300, {'train/accuracy': 0.76664137840271, 'train/loss': 0.8711822628974915, 'validation/accuracy': 0.6797199845314026, 'validation/loss': 1.2980207204818726, 'validation/num_examples': 50000, 'test/accuracy': 0.5515000224113464, 'test/loss': 2.027400255203247, 'test/num_examples': 10000, 'score': 37792.54945540428, 'total_duration': 40644.80345582962, 'accumulated_submission_time': 37792.54945540428, 'accumulated_eval_time': 2833.4583435058594, 'accumulated_logging_time': 8.245412111282349, 'global_step': 94300, 'preemption_count': 0}), (95570, {'train/accuracy': 0.7712053656578064, 'train/loss': 0.8572474122047424, 'validation/accuracy': 0.6849600076675415, 'validation/loss': 1.2875492572784424, 'validation/num_examples': 50000, 'test/accuracy': 0.5561000108718872, 'test/loss': 2.026799201965332, 'test/num_examples': 10000, 'score': 38302.53266644478, 'total_duration': 41182.167934179306, 'accumulated_submission_time': 38302.53266644478, 'accumulated_eval_time': 2860.5366995334625, 'accumulated_logging_time': 8.395722150802612, 'global_step': 95570, 'preemption_count': 0}), (96851, {'train/accuracy': 0.76566481590271, 'train/loss': 0.8755190968513489, 'validation/accuracy': 0.6824199557304382, 'validation/loss': 1.2965515851974487, 'validation/num_examples': 50000, 'test/accuracy': 0.5570000410079956, 'test/loss': 1.9973702430725098, 'test/num_examples': 10000, 'score': 38812.40683746338, 'total_duration': 41719.5423374176, 'accumulated_submission_time': 38812.40683746338, 'accumulated_eval_time': 2887.710411787033, 'accumulated_logging_time': 8.569047451019287, 'global_step': 96851, 'preemption_count': 0}), (98112, {'train/accuracy': 0.767020046710968, 'train/loss': 0.8678561449050903, 'validation/accuracy': 0.6813399791717529, 'validation/loss': 1.30287766456604, 'validation/num_examples': 50000, 'test/accuracy': 0.5586000084877014, 'test/loss': 2.0183568000793457, 'test/num_examples': 10000, 'score': 39322.10249710083, 'total_duration': 42257.09367632866, 'accumulated_submission_time': 39322.10249710083, 'accumulated_eval_time': 2915.216523170471, 'accumulated_logging_time': 8.764290809631348, 'global_step': 98112, 'preemption_count': 0}), (99382, {'train/accuracy': 0.7692322731018066, 'train/loss': 0.8620010018348694, 'validation/accuracy': 0.6821399927139282, 'validation/loss': 1.2936266660690308, 'validation/num_examples': 50000, 'test/accuracy': 0.5515000224113464, 'test/loss': 2.029322624206543, 'test/num_examples': 10000, 'score': 39832.10922241211, 'total_duration': 42794.78906965256, 'accumulated_submission_time': 39832.10922241211, 'accumulated_eval_time': 2942.572678565979, 'accumulated_logging_time': 8.941336870193481, 'global_step': 99382, 'preemption_count': 0}), (100651, {'train/accuracy': 0.7721819281578064, 'train/loss': 0.8489797115325928, 'validation/accuracy': 0.6789999604225159, 'validation/loss': 1.306251883506775, 'validation/num_examples': 50000, 'test/accuracy': 0.5533000230789185, 'test/loss': 2.054805040359497, 'test/num_examples': 10000, 'score': 40342.058062553406, 'total_duration': 43332.470703840256, 'accumulated_submission_time': 40342.058062553406, 'accumulated_eval_time': 2969.9975974559784, 'accumulated_logging_time': 9.095003843307495, 'global_step': 100651, 'preemption_count': 0}), (101921, {'train/accuracy': 0.7595264315605164, 'train/loss': 0.8818293809890747, 'validation/accuracy': 0.6751199960708618, 'validation/loss': 1.337367296218872, 'validation/num_examples': 50000, 'test/accuracy': 0.5442000031471252, 'test/loss': 2.0929226875305176, 'test/num_examples': 10000, 'score': 40851.84210205078, 'total_duration': 43870.01954984665, 'accumulated_submission_time': 40851.84210205078, 'accumulated_eval_time': 2997.4131529331207, 'accumulated_logging_time': 9.294449090957642, 'global_step': 101921, 'preemption_count': 0}), (103189, {'train/accuracy': 0.7863719463348389, 'train/loss': 0.8021854162216187, 'validation/accuracy': 0.692039966583252, 'validation/loss': 1.2510658502578735, 'validation/num_examples': 50000, 'test/accuracy': 0.5560000538825989, 'test/loss': 1.9731688499450684, 'test/num_examples': 10000, 'score': 41361.91395354271, 'total_duration': 44407.51722383499, 'accumulated_submission_time': 41361.91395354271, 'accumulated_eval_time': 3024.5121088027954, 'accumulated_logging_time': 9.471859216690063, 'global_step': 103189, 'preemption_count': 0}), (104458, {'train/accuracy': 0.7827845811843872, 'train/loss': 0.7984907627105713, 'validation/accuracy': 0.6882399916648865, 'validation/loss': 1.2776323556900024, 'validation/num_examples': 50000, 'test/accuracy': 0.5537000298500061, 'test/loss': 2.0164968967437744, 'test/num_examples': 10000, 'score': 41871.68844270706, 'total_duration': 44945.11683702469, 'accumulated_submission_time': 41871.68844270706, 'accumulated_eval_time': 3052.095820903778, 'accumulated_logging_time': 9.563466548919678, 'global_step': 104458, 'preemption_count': 0}), (105730, {'train/accuracy': 0.7887037396430969, 'train/loss': 0.776313304901123, 'validation/accuracy': 0.6886599659919739, 'validation/loss': 1.2676867246627808, 'validation/num_examples': 50000, 'test/accuracy': 0.5593000054359436, 'test/loss': 1.9992930889129639, 'test/num_examples': 10000, 'score': 42381.77481198311, 'total_duration': 45482.76578807831, 'accumulated_submission_time': 42381.77481198311, 'accumulated_eval_time': 3079.3871760368347, 'accumulated_logging_time': 9.688700437545776, 'global_step': 105730, 'preemption_count': 0}), (106999, {'train/accuracy': 0.7856544852256775, 'train/loss': 0.7834033370018005, 'validation/accuracy': 0.686739981174469, 'validation/loss': 1.2815196514129639, 'validation/num_examples': 50000, 'test/accuracy': 0.5581000447273254, 'test/loss': 1.9943041801452637, 'test/num_examples': 10000, 'score': 42891.70066475868, 'total_duration': 46020.22691607475, 'accumulated_submission_time': 42891.70066475868, 'accumulated_eval_time': 3106.6422216892242, 'accumulated_logging_time': 9.82244324684143, 'global_step': 106999, 'preemption_count': 0}), (108265, {'train/accuracy': 0.7967952489852905, 'train/loss': 0.7369797825813293, 'validation/accuracy': 0.693399965763092, 'validation/loss': 1.250244379043579, 'validation/num_examples': 50000, 'test/accuracy': 0.5676000118255615, 'test/loss': 1.9796468019485474, 'test/num_examples': 10000, 'score': 43401.43486213684, 'total_duration': 46557.55706334114, 'accumulated_submission_time': 43401.43486213684, 'accumulated_eval_time': 3133.9344544410706, 'accumulated_logging_time': 9.976380348205566, 'global_step': 108265, 'preemption_count': 0}), (109527, {'train/accuracy': 0.7989476919174194, 'train/loss': 0.7366896271705627, 'validation/accuracy': 0.6920199990272522, 'validation/loss': 1.241847038269043, 'validation/num_examples': 50000, 'test/accuracy': 0.5640000104904175, 'test/loss': 1.9837467670440674, 'test/num_examples': 10000, 'score': 43911.2816901207, 'total_duration': 47094.85551953316, 'accumulated_submission_time': 43911.2816901207, 'accumulated_eval_time': 3161.036957025528, 'accumulated_logging_time': 10.175127983093262, 'global_step': 109527, 'preemption_count': 0}), (110794, {'train/accuracy': 0.8114436864852905, 'train/loss': 0.6931107044219971, 'validation/accuracy': 0.6977799534797668, 'validation/loss': 1.2324331998825073, 'validation/num_examples': 50000, 'test/accuracy': 0.5680000185966492, 'test/loss': 1.9631637334823608, 'test/num_examples': 10000, 'score': 44421.214066028595, 'total_duration': 47632.36178588867, 'accumulated_submission_time': 44421.214066028595, 'accumulated_eval_time': 3188.340614080429, 'accumulated_logging_time': 10.29618525505066, 'global_step': 110794, 'preemption_count': 0}), (112066, {'train/accuracy': 0.8172432780265808, 'train/loss': 0.6669880747795105, 'validation/accuracy': 0.6976000070571899, 'validation/loss': 1.2342274188995361, 'validation/num_examples': 50000, 'test/accuracy': 0.5746000409126282, 'test/loss': 1.9357752799987793, 'test/num_examples': 10000, 'score': 44931.23804187775, 'total_duration': 48169.96520256996, 'accumulated_submission_time': 44931.23804187775, 'accumulated_eval_time': 3215.659420967102, 'accumulated_logging_time': 10.406671524047852, 'global_step': 112066, 'preemption_count': 0}), (113332, {'train/accuracy': 0.8114436864852905, 'train/loss': 0.6976011395454407, 'validation/accuracy': 0.6915000081062317, 'validation/loss': 1.2647820711135864, 'validation/num_examples': 50000, 'test/accuracy': 0.5711000561714172, 'test/loss': 1.9702421426773071, 'test/num_examples': 10000, 'score': 45441.060131549835, 'total_duration': 48707.01532649994, 'accumulated_submission_time': 45441.060131549835, 'accumulated_eval_time': 3242.598057746887, 'accumulated_logging_time': 10.541358470916748, 'global_step': 113332, 'preemption_count': 0}), (114601, {'train/accuracy': 0.8248764276504517, 'train/loss': 0.6415936350822449, 'validation/accuracy': 0.6953799724578857, 'validation/loss': 1.2429535388946533, 'validation/num_examples': 50000, 'test/accuracy': 0.5675000548362732, 'test/loss': 1.982709527015686, 'test/num_examples': 10000, 'score': 45951.060353040695, 'total_duration': 49244.447271585464, 'accumulated_submission_time': 45951.060353040695, 'accumulated_eval_time': 3269.7295668125153, 'accumulated_logging_time': 10.687488317489624, 'global_step': 114601, 'preemption_count': 0}), (115870, {'train/accuracy': 0.8361766338348389, 'train/loss': 0.597150444984436, 'validation/accuracy': 0.70278000831604, 'validation/loss': 1.209159255027771, 'validation/num_examples': 50000, 'test/accuracy': 0.5736000537872314, 'test/loss': 1.946031093597412, 'test/num_examples': 10000, 'score': 46460.99710583687, 'total_duration': 49781.700803518295, 'accumulated_submission_time': 46460.99710583687, 'accumulated_eval_time': 3296.7717957496643, 'accumulated_logging_time': 10.807193756103516, 'global_step': 115870, 'preemption_count': 0}), (117137, {'train/accuracy': 0.8185586333274841, 'train/loss': 0.6665962934494019, 'validation/accuracy': 0.6997599601745605, 'validation/loss': 1.2240430116653442, 'validation/num_examples': 50000, 'test/accuracy': 0.5723000168800354, 'test/loss': 1.9356892108917236, 'test/num_examples': 10000, 'score': 46970.93736863136, 'total_duration': 50319.24065065384, 'accumulated_submission_time': 46970.93736863136, 'accumulated_eval_time': 3324.021938562393, 'accumulated_logging_time': 11.00585150718689, 'global_step': 117137, 'preemption_count': 0}), (118401, {'train/accuracy': 0.8016581535339355, 'train/loss': 0.7241121530532837, 'validation/accuracy': 0.7034199833869934, 'validation/loss': 1.2237718105316162, 'validation/num_examples': 50000, 'test/accuracy': 0.5711000561714172, 'test/loss': 1.9605035781860352, 'test/num_examples': 10000, 'score': 47480.93937563896, 'total_duration': 50856.642407655716, 'accumulated_submission_time': 47480.93937563896, 'accumulated_eval_time': 3351.1712486743927, 'accumulated_logging_time': 11.105561017990112, 'global_step': 118401, 'preemption_count': 0}), (119674, {'train/accuracy': 0.800203263759613, 'train/loss': 0.7269607186317444, 'validation/accuracy': 0.7064399719238281, 'validation/loss': 1.198792815208435, 'validation/num_examples': 50000, 'test/accuracy': 0.5806000232696533, 'test/loss': 1.9103903770446777, 'test/num_examples': 10000, 'score': 47990.759323596954, 'total_duration': 51393.48934221268, 'accumulated_submission_time': 47990.759323596954, 'accumulated_eval_time': 3377.936456680298, 'accumulated_logging_time': 11.215101718902588, 'global_step': 119674, 'preemption_count': 0}), (120936, {'train/accuracy': 0.8026745915412903, 'train/loss': 0.7251353859901428, 'validation/accuracy': 0.704759955406189, 'validation/loss': 1.1900979280471802, 'validation/num_examples': 50000, 'test/accuracy': 0.5764000415802002, 'test/loss': 1.916110634803772, 'test/num_examples': 10000, 'score': 48500.78496360779, 'total_duration': 51930.61951708794, 'accumulated_submission_time': 48500.78496360779, 'accumulated_eval_time': 3404.714349269867, 'accumulated_logging_time': 11.390337944030762, 'global_step': 120936, 'preemption_count': 0}), (122202, {'train/accuracy': 0.7899991869926453, 'train/loss': 0.7714759707450867, 'validation/accuracy': 0.6962400078773499, 'validation/loss': 1.2297719717025757, 'validation/num_examples': 50000, 'test/accuracy': 0.5662000179290771, 'test/loss': 1.9854764938354492, 'test/num_examples': 10000, 'score': 49010.68418121338, 'total_duration': 52467.968636751175, 'accumulated_submission_time': 49010.68418121338, 'accumulated_eval_time': 3431.8745787143707, 'accumulated_logging_time': 11.527794361114502, 'global_step': 122202, 'preemption_count': 0}), (123477, {'train/accuracy': 0.8029336333274841, 'train/loss': 0.7145070433616638, 'validation/accuracy': 0.7053599953651428, 'validation/loss': 1.1961065530776978, 'validation/num_examples': 50000, 'test/accuracy': 0.5788000226020813, 'test/loss': 1.9366860389709473, 'test/num_examples': 10000, 'score': 49520.81037211418, 'total_duration': 53005.65456581116, 'accumulated_submission_time': 49520.81037211418, 'accumulated_eval_time': 3459.1308851242065, 'accumulated_logging_time': 11.67727518081665, 'global_step': 123477, 'preemption_count': 0}), (124748, {'train/accuracy': 0.79691481590271, 'train/loss': 0.724306046962738, 'validation/accuracy': 0.7020800113677979, 'validation/loss': 1.2199281454086304, 'validation/num_examples': 50000, 'test/accuracy': 0.5723000168800354, 'test/loss': 1.958957314491272, 'test/num_examples': 10000, 'score': 50030.59896612167, 'total_duration': 53542.59084534645, 'accumulated_submission_time': 50030.59896612167, 'accumulated_eval_time': 3486.0224640369415, 'accumulated_logging_time': 11.781381845474243, 'global_step': 124748, 'preemption_count': 0}), (126018, {'train/accuracy': 0.7934669852256775, 'train/loss': 0.7466307282447815, 'validation/accuracy': 0.6970199942588806, 'validation/loss': 1.2577674388885498, 'validation/num_examples': 50000, 'test/accuracy': 0.5732000470161438, 'test/loss': 1.9762790203094482, 'test/num_examples': 10000, 'score': 50540.44911456108, 'total_duration': 54080.25355267525, 'accumulated_submission_time': 50540.44911456108, 'accumulated_eval_time': 3513.5513110160828, 'accumulated_logging_time': 11.91389274597168, 'global_step': 126018, 'preemption_count': 0}), (127286, {'train/accuracy': 0.808613657951355, 'train/loss': 0.69827800989151, 'validation/accuracy': 0.7039399743080139, 'validation/loss': 1.1903411149978638, 'validation/num_examples': 50000, 'test/accuracy': 0.58160001039505, 'test/loss': 1.9108591079711914, 'test/num_examples': 10000, 'score': 51050.360624074936, 'total_duration': 54617.64628458023, 'accumulated_submission_time': 51050.360624074936, 'accumulated_eval_time': 3540.695063352585, 'accumulated_logging_time': 12.097843170166016, 'global_step': 127286, 'preemption_count': 0}), (128555, {'train/accuracy': 0.8170041441917419, 'train/loss': 0.6612815856933594, 'validation/accuracy': 0.7135599851608276, 'validation/loss': 1.1651833057403564, 'validation/num_examples': 50000, 'test/accuracy': 0.5886000394821167, 'test/loss': 1.8988839387893677, 'test/num_examples': 10000, 'score': 51560.09570503235, 'total_duration': 55154.81159615517, 'accumulated_submission_time': 51560.09570503235, 'accumulated_eval_time': 3567.8305106163025, 'accumulated_logging_time': 12.241714000701904, 'global_step': 128555, 'preemption_count': 0}), (129821, {'train/accuracy': 0.8074178695678711, 'train/loss': 0.6945946216583252, 'validation/accuracy': 0.7057200074195862, 'validation/loss': 1.1962287425994873, 'validation/num_examples': 50000, 'test/accuracy': 0.5779000520706177, 'test/loss': 1.9390509128570557, 'test/num_examples': 10000, 'score': 52069.90982294083, 'total_duration': 55692.23065185547, 'accumulated_submission_time': 52069.90982294083, 'accumulated_eval_time': 3595.114734888077, 'accumulated_logging_time': 12.415164232254028, 'global_step': 129821, 'preemption_count': 0}), (131093, {'train/accuracy': 0.8235211968421936, 'train/loss': 0.6234328746795654, 'validation/accuracy': 0.7139399647712708, 'validation/loss': 1.1620432138442993, 'validation/num_examples': 50000, 'test/accuracy': 0.5890000462532043, 'test/loss': 1.864998698234558, 'test/num_examples': 10000, 'score': 52579.62643289566, 'total_duration': 56229.579538583755, 'accumulated_submission_time': 52579.62643289566, 'accumulated_eval_time': 3622.389446258545, 'accumulated_logging_time': 12.6214280128479, 'global_step': 131093, 'preemption_count': 0}), (132363, {'train/accuracy': 0.823660671710968, 'train/loss': 0.6290507316589355, 'validation/accuracy': 0.7130599617958069, 'validation/loss': 1.1777054071426392, 'validation/num_examples': 50000, 'test/accuracy': 0.5835000276565552, 'test/loss': 1.928019642829895, 'test/num_examples': 10000, 'score': 53089.719497442245, 'total_duration': 56767.1809694767, 'accumulated_submission_time': 53089.719497442245, 'accumulated_eval_time': 3649.6199090480804, 'accumulated_logging_time': 12.750117778778076, 'global_step': 132363, 'preemption_count': 0}), (133635, {'train/accuracy': 0.8267697691917419, 'train/loss': 0.6224292516708374, 'validation/accuracy': 0.7147200107574463, 'validation/loss': 1.1556929349899292, 'validation/num_examples': 50000, 'test/accuracy': 0.5840000510215759, 'test/loss': 1.8953264951705933, 'test/num_examples': 10000, 'score': 53599.53695297241, 'total_duration': 57304.55410575867, 'accumulated_submission_time': 53599.53695297241, 'accumulated_eval_time': 3676.84366106987, 'accumulated_logging_time': 12.932210683822632, 'global_step': 133635, 'preemption_count': 0}), (134898, {'train/accuracy': 0.8265505433082581, 'train/loss': 0.6189639568328857, 'validation/accuracy': 0.7134999632835388, 'validation/loss': 1.1628515720367432, 'validation/num_examples': 50000, 'test/accuracy': 0.5850000381469727, 'test/loss': 1.8954309225082397, 'test/num_examples': 10000, 'score': 54109.48800945282, 'total_duration': 57842.28623962402, 'accumulated_submission_time': 54109.48800945282, 'accumulated_eval_time': 3704.346643447876, 'accumulated_logging_time': 13.056741714477539, 'global_step': 134898, 'preemption_count': 0}), (136154, {'train/accuracy': 0.8370336294174194, 'train/loss': 0.5735855102539062, 'validation/accuracy': 0.7195000052452087, 'validation/loss': 1.154090404510498, 'validation/num_examples': 50000, 'test/accuracy': 0.6008000373840332, 'test/loss': 1.8812024593353271, 'test/num_examples': 10000, 'score': 54619.31276035309, 'total_duration': 58379.12945151329, 'accumulated_submission_time': 54619.31276035309, 'accumulated_eval_time': 3731.0538036823273, 'accumulated_logging_time': 13.213061094284058, 'global_step': 136154, 'preemption_count': 0}), (137417, {'train/accuracy': 0.8367546200752258, 'train/loss': 0.5768634080886841, 'validation/accuracy': 0.7145400047302246, 'validation/loss': 1.1788935661315918, 'validation/num_examples': 50000, 'test/accuracy': 0.5929000377655029, 'test/loss': 1.91801917552948, 'test/num_examples': 10000, 'score': 55129.43868684769, 'total_duration': 58916.420729637146, 'accumulated_submission_time': 55129.43868684769, 'accumulated_eval_time': 3757.94922375679, 'accumulated_logging_time': 13.331732273101807, 'global_step': 137417, 'preemption_count': 0}), (138678, {'train/accuracy': 0.8482341766357422, 'train/loss': 0.5301579833030701, 'validation/accuracy': 0.7227199673652649, 'validation/loss': 1.13791823387146, 'validation/num_examples': 50000, 'test/accuracy': 0.5957000255584717, 'test/loss': 1.887203574180603, 'test/num_examples': 10000, 'score': 55639.46609830856, 'total_duration': 59453.88412427902, 'accumulated_submission_time': 55639.46609830856, 'accumulated_eval_time': 3785.068791627884, 'accumulated_logging_time': 13.49641466140747, 'global_step': 138678, 'preemption_count': 0}), (139952, {'train/accuracy': 0.8473173975944519, 'train/loss': 0.5286949872970581, 'validation/accuracy': 0.7236999869346619, 'validation/loss': 1.1220107078552246, 'validation/num_examples': 50000, 'test/accuracy': 0.5966000556945801, 'test/loss': 1.8753621578216553, 'test/num_examples': 10000, 'score': 56149.29556059837, 'total_duration': 59991.31153130531, 'accumulated_submission_time': 56149.29556059837, 'accumulated_eval_time': 3812.368437767029, 'accumulated_logging_time': 13.639651775360107, 'global_step': 139952, 'preemption_count': 0}), (141207, {'train/accuracy': 0.8557477593421936, 'train/loss': 0.5097410082817078, 'validation/accuracy': 0.7257399559020996, 'validation/loss': 1.1164480447769165, 'validation/num_examples': 50000, 'test/accuracy': 0.5961000323295593, 'test/loss': 1.8718180656433105, 'test/num_examples': 10000, 'score': 56659.34239411354, 'total_duration': 60528.81952023506, 'accumulated_submission_time': 56659.34239411354, 'accumulated_eval_time': 3839.5146555900574, 'accumulated_logging_time': 13.803614139556885, 'global_step': 141207, 'preemption_count': 0}), (142467, {'train/accuracy': 0.8450254797935486, 'train/loss': 0.5447291135787964, 'validation/accuracy': 0.7154399752616882, 'validation/loss': 1.1721899509429932, 'validation/num_examples': 50000, 'test/accuracy': 0.5888000130653381, 'test/loss': 1.9206199645996094, 'test/num_examples': 10000, 'score': 57169.31097364426, 'total_duration': 61066.4758272171, 'accumulated_submission_time': 57169.31097364426, 'accumulated_eval_time': 3866.8776421546936, 'accumulated_logging_time': 13.976761102676392, 'global_step': 142467, 'preemption_count': 0}), (143725, {'train/accuracy': 0.8621252775192261, 'train/loss': 0.48647230863571167, 'validation/accuracy': 0.7249599695205688, 'validation/loss': 1.1248472929000854, 'validation/num_examples': 50000, 'test/accuracy': 0.5969000458717346, 'test/loss': 1.8661229610443115, 'test/num_examples': 10000, 'score': 57679.31414628029, 'total_duration': 61604.13234233856, 'accumulated_submission_time': 57679.31414628029, 'accumulated_eval_time': 3894.171902656555, 'accumulated_logging_time': 14.188604831695557, 'global_step': 143725, 'preemption_count': 0}), (144986, {'train/accuracy': 0.86328125, 'train/loss': 0.48145413398742676, 'validation/accuracy': 0.7263799905776978, 'validation/loss': 1.1341686248779297, 'validation/num_examples': 50000, 'test/accuracy': 0.5992000102996826, 'test/loss': 1.8745211362838745, 'test/num_examples': 10000, 'score': 58188.998012542725, 'total_duration': 62141.60804104805, 'accumulated_submission_time': 58188.998012542725, 'accumulated_eval_time': 3921.633396625519, 'accumulated_logging_time': 14.366646528244019, 'global_step': 144986, 'preemption_count': 0}), (146245, {'train/accuracy': 0.8625438213348389, 'train/loss': 0.47874048352241516, 'validation/accuracy': 0.7259199619293213, 'validation/loss': 1.1368765830993652, 'validation/num_examples': 50000, 'test/accuracy': 0.5873000025749207, 'test/loss': 1.9123883247375488, 'test/num_examples': 10000, 'score': 58698.92954111099, 'total_duration': 62678.77855491638, 'accumulated_submission_time': 58698.92954111099, 'accumulated_eval_time': 3948.604412794113, 'accumulated_logging_time': 14.484930038452148, 'global_step': 146245, 'preemption_count': 0}), (147504, {'train/accuracy': 0.8783880472183228, 'train/loss': 0.423285573720932, 'validation/accuracy': 0.7319599986076355, 'validation/loss': 1.1093332767486572, 'validation/num_examples': 50000, 'test/accuracy': 0.6055999994277954, 'test/loss': 1.8725615739822388, 'test/num_examples': 10000, 'score': 59208.65251708031, 'total_duration': 63216.06735801697, 'accumulated_submission_time': 59208.65251708031, 'accumulated_eval_time': 3975.7895686626434, 'accumulated_logging_time': 14.718544244766235, 'global_step': 147504, 'preemption_count': 0}), (148761, {'train/accuracy': 0.8730069994926453, 'train/loss': 0.44121915102005005, 'validation/accuracy': 0.7265799641609192, 'validation/loss': 1.1275993585586548, 'validation/num_examples': 50000, 'test/accuracy': 0.6033000349998474, 'test/loss': 1.8813526630401611, 'test/num_examples': 10000, 'score': 59718.75491428375, 'total_duration': 63753.28751850128, 'accumulated_submission_time': 59718.75491428375, 'accumulated_eval_time': 4002.634577035904, 'accumulated_logging_time': 14.84362506866455, 'global_step': 148761, 'preemption_count': 0}), (150009, {'train/accuracy': 0.8852439522743225, 'train/loss': 0.40227600932121277, 'validation/accuracy': 0.7309399843215942, 'validation/loss': 1.121312141418457, 'validation/num_examples': 50000, 'test/accuracy': 0.6033000349998474, 'test/loss': 1.8759082555770874, 'test/num_examples': 10000, 'score': 60228.58671402931, 'total_duration': 64290.80240774155, 'accumulated_submission_time': 60228.58671402931, 'accumulated_eval_time': 4030.0152719020844, 'accumulated_logging_time': 14.997449159622192, 'global_step': 150009, 'preemption_count': 0}), (151261, {'train/accuracy': 0.8906050324440002, 'train/loss': 0.3854796290397644, 'validation/accuracy': 0.7303400039672852, 'validation/loss': 1.1059155464172363, 'validation/num_examples': 50000, 'test/accuracy': 0.6074000000953674, 'test/loss': 1.8362197875976562, 'test/num_examples': 10000, 'score': 60738.342767477036, 'total_duration': 64827.92450714111, 'accumulated_submission_time': 60738.342767477036, 'accumulated_eval_time': 4057.082931280136, 'accumulated_logging_time': 15.149745464324951, 'global_step': 151261, 'preemption_count': 0}), (152517, {'train/accuracy': 0.8903658986091614, 'train/loss': 0.3809801936149597, 'validation/accuracy': 0.7302599549293518, 'validation/loss': 1.1175459623336792, 'validation/num_examples': 50000, 'test/accuracy': 0.6018000245094299, 'test/loss': 1.8724627494812012, 'test/num_examples': 10000, 'score': 61248.171390771866, 'total_duration': 65365.30461025238, 'accumulated_submission_time': 61248.171390771866, 'accumulated_eval_time': 4084.3408143520355, 'accumulated_logging_time': 15.293487787246704, 'global_step': 152517, 'preemption_count': 0}), (153768, {'train/accuracy': 0.9008290767669678, 'train/loss': 0.34036892652511597, 'validation/accuracy': 0.7346199750900269, 'validation/loss': 1.101989984512329, 'validation/num_examples': 50000, 'test/accuracy': 0.6124000549316406, 'test/loss': 1.846618890762329, 'test/num_examples': 10000, 'score': 61758.120441913605, 'total_duration': 65902.5383348465, 'accumulated_submission_time': 61758.120441913605, 'accumulated_eval_time': 4111.366451740265, 'accumulated_logging_time': 15.404479742050171, 'global_step': 153768, 'preemption_count': 0}), (155026, {'train/accuracy': 0.9110331535339355, 'train/loss': 0.30523690581321716, 'validation/accuracy': 0.7399399876594543, 'validation/loss': 1.091164231300354, 'validation/num_examples': 50000, 'test/accuracy': 0.6074000000953674, 'test/loss': 1.8635671138763428, 'test/num_examples': 10000, 'score': 62268.102655887604, 'total_duration': 66439.98958063126, 'accumulated_submission_time': 62268.102655887604, 'accumulated_eval_time': 4138.5379774570465, 'accumulated_logging_time': 15.553490161895752, 'global_step': 155026, 'preemption_count': 0}), (156284, {'train/accuracy': 0.9184669852256775, 'train/loss': 0.29308050870895386, 'validation/accuracy': 0.7384399771690369, 'validation/loss': 1.0832815170288086, 'validation/num_examples': 50000, 'test/accuracy': 0.6162000298500061, 'test/loss': 1.8467997312545776, 'test/num_examples': 10000, 'score': 62777.95733046532, 'total_duration': 66977.18071866035, 'accumulated_submission_time': 62777.95733046532, 'accumulated_eval_time': 4165.608562707901, 'accumulated_logging_time': 15.66888952255249, 'global_step': 156284, 'preemption_count': 0}), (157545, {'train/accuracy': 0.9288305044174194, 'train/loss': 0.256733238697052, 'validation/accuracy': 0.7404199838638306, 'validation/loss': 1.0825177431106567, 'validation/num_examples': 50000, 'test/accuracy': 0.614300012588501, 'test/loss': 1.845710039138794, 'test/num_examples': 10000, 'score': 63287.78874731064, 'total_duration': 67514.63003993034, 'accumulated_submission_time': 63287.78874731064, 'accumulated_eval_time': 4192.898053407669, 'accumulated_logging_time': 15.846901178359985, 'global_step': 157545, 'preemption_count': 0}), (158815, {'train/accuracy': 0.9248046875, 'train/loss': 0.2694341242313385, 'validation/accuracy': 0.7346199750900269, 'validation/loss': 1.1146613359451294, 'validation/num_examples': 50000, 'test/accuracy': 0.6093000173568726, 'test/loss': 1.8701332807540894, 'test/num_examples': 10000, 'score': 63797.63259458542, 'total_duration': 68051.73343467712, 'accumulated_submission_time': 63797.63259458542, 'accumulated_eval_time': 4219.884444236755, 'accumulated_logging_time': 15.969777584075928, 'global_step': 158815, 'preemption_count': 0}), (160076, {'train/accuracy': 0.9137635231018066, 'train/loss': 0.2998613119125366, 'validation/accuracy': 0.7390199899673462, 'validation/loss': 1.091567039489746, 'validation/num_examples': 50000, 'test/accuracy': 0.6096000075340271, 'test/loss': 1.8660664558410645, 'test/num_examples': 10000, 'score': 64307.715534210205, 'total_duration': 68588.98250818253, 'accumulated_submission_time': 64307.715534210205, 'accumulated_eval_time': 4246.745504617691, 'accumulated_logging_time': 16.1235671043396, 'global_step': 160076, 'preemption_count': 0}), (161343, {'train/accuracy': 0.9080635905265808, 'train/loss': 0.3138331472873688, 'validation/accuracy': 0.739139974117279, 'validation/loss': 1.093545913696289, 'validation/num_examples': 50000, 'test/accuracy': 0.6201000213623047, 'test/loss': 1.853371024131775, 'test/num_examples': 10000, 'score': 64817.81100797653, 'total_duration': 69126.83268904686, 'accumulated_submission_time': 64817.81100797653, 'accumulated_eval_time': 4274.234495162964, 'accumulated_logging_time': 16.239080667495728, 'global_step': 161343, 'preemption_count': 0}), (162615, {'train/accuracy': 0.9080436825752258, 'train/loss': 0.317469984292984, 'validation/accuracy': 0.7396799921989441, 'validation/loss': 1.0973786115646362, 'validation/num_examples': 50000, 'test/accuracy': 0.6109000444412231, 'test/loss': 1.8552061319351196, 'test/num_examples': 10000, 'score': 65327.9159655571, 'total_duration': 69664.22300601006, 'accumulated_submission_time': 65327.9159655571, 'accumulated_eval_time': 4301.2065596580505, 'accumulated_logging_time': 16.400709629058838, 'global_step': 162615, 'preemption_count': 0}), (163870, {'train/accuracy': 0.9150190949440002, 'train/loss': 0.2908560633659363, 'validation/accuracy': 0.7402600049972534, 'validation/loss': 1.0952625274658203, 'validation/num_examples': 50000, 'test/accuracy': 0.6164000034332275, 'test/loss': 1.8572149276733398, 'test/num_examples': 10000, 'score': 65837.7168314457, 'total_duration': 70201.57818841934, 'accumulated_submission_time': 65837.7168314457, 'accumulated_eval_time': 4328.472270488739, 'accumulated_logging_time': 16.54002356529236, 'global_step': 163870, 'preemption_count': 0})], 'global_step': 165121}
I0307 22:48:42.501863 140114851837120 submission_runner.py:649] Timing: 66347.56746983528
I0307 22:48:42.501911 140114851837120 submission_runner.py:651] Total number of evals: 130
I0307 22:48:42.501947 140114851837120 submission_runner.py:652] ====================
I0307 22:48:42.502185 140114851837120 submission_runner.py:750] Final imagenet_resnet score: 4
