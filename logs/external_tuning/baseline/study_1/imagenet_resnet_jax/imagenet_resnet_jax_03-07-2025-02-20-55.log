python submission_runner.py --framework=jax --workload=imagenet_resnet --submission_path=prize_qualification_baselines/external_tuning/jax_nadamw_full_budget.py --data_dir=/data/imagenet/jax --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=submissions/rolling_leaderboard/external_tuning/baseline/study_1 --overwrite=True --save_checkpoints=False --rng_seed=-1721477756 --imagenet_v2_data_dir=/data/imagenet/jax --tuning_ruleset=external --tuning_search_space=prize_qualification_baselines/external_tuning/tuning_search_space.json --num_tuning_trials=5 --hparam_start_index=0 --hparam_end_index=1 2>&1 | tee -a /logs/imagenet_resnet_jax_03-07-2025-02-20-55.log
2025-03-07 02:21:12.801826: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1741314073.226617       9 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1741314073.381043       9 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
I0307 02:22:01.333380 140178276385984 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_1/imagenet_resnet_jax.
I0307 02:22:04.243169 140178276385984 xla_bridge.py:884] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA
I0307 02:22:04.247279 140178276385984 xla_bridge.py:884] Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory
I0307 02:22:04.286425 140178276385984 submission_runner.py:606] Using RNG seed -1721477756
I0307 02:22:09.689512 140178276385984 submission_runner.py:615] --- Tuning run 1/5 ---
I0307 02:22:09.689718 140178276385984 submission_runner.py:620] Creating tuning directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_1/imagenet_resnet_jax/trial_1.
I0307 02:22:09.689930 140178276385984 logger_utils.py:97] Saving hparams to /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_1/imagenet_resnet_jax/trial_1/hparams.json.
I0307 02:22:09.935194 140178276385984 submission_runner.py:218] Initializing dataset.
I0307 02:22:11.030940 140178276385984 dataset_info.py:690] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0307 02:22:11.364851 140178276385984 reader.py:261] Creating a tf.data.Dataset reading 1024 files located in folders: /data/imagenet/jax/imagenet2012/5.1.0.
I0307 02:22:11.681022 140178276385984 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split train, from /data/imagenet/jax/imagenet2012/5.1.0
I0307 02:22:13.417707 140178276385984 submission_runner.py:229] Initializing model.
I0307 02:22:36.973168 140178276385984 submission_runner.py:272] Initializing optimizer.
I0307 02:22:38.060767 140178276385984 submission_runner.py:279] Initializing metrics bundle.
I0307 02:22:38.061017 140178276385984 submission_runner.py:301] Initializing checkpoint and logger.
I0307 02:22:38.062646 140178276385984 checkpoints.py:1101] Found no checkpoint files in /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_1/imagenet_resnet_jax/trial_1 with prefix checkpoint_
I0307 02:22:38.062750 140178276385984 submission_runner.py:321] Saving meta data to /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_1/imagenet_resnet_jax/trial_1/meta_data_0.json.
I0307 02:22:38.586709 140178276385984 submission_runner.py:325] Saving flags to /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_1/imagenet_resnet_jax/trial_1/flags_0.json.
I0307 02:22:38.956353 140178276385984 submission_runner.py:337] Starting training loop.
I0307 02:23:37.283388 140040332101376 logging_writer.py:48] [0] global_step=0, grad_norm=0.5972574949264526, loss=6.9275312423706055
I0307 02:23:37.686998 140178276385984 spec.py:321] Evaluating on the training split.
I0307 02:23:38.149356 140178276385984 dataset_info.py:690] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0307 02:23:38.174237 140178276385984 reader.py:261] Creating a tf.data.Dataset reading 1024 files located in folders: /data/imagenet/jax/imagenet2012/5.1.0.
I0307 02:23:38.215826 140178276385984 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split train, from /data/imagenet/jax/imagenet2012/5.1.0
I0307 02:23:58.248763 140178276385984 spec.py:333] Evaluating on the validation split.
I0307 02:23:58.982451 140178276385984 dataset_info.py:690] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0307 02:23:59.041465 140178276385984 reader.py:261] Creating a tf.data.Dataset reading 64 files located in folders: /data/imagenet/jax/imagenet2012/5.1.0.
I0307 02:23:59.326573 140178276385984 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split validation, from /data/imagenet/jax/imagenet2012/5.1.0
I0307 02:24:50.269060 140178276385984 spec.py:349] Evaluating on the test split.
I0307 02:24:50.831551 140178276385984 dataset_info.py:690] Load dataset info from /data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0
I0307 02:24:50.896334 140178276385984 reader.py:261] Creating a tf.data.Dataset reading 16 files located in folders: /data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0.
I0307 02:24:50.947182 140178276385984 logging_logger.py:49] Constructing tf.data.Dataset imagenet_v2 for split test, from /data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0
I0307 02:25:43.390129 140178276385984 submission_runner.py:469] Time since start: 184.43s, 	Step: 1, 	{'train/accuracy': 0.000996492337435484, 'train/loss': 6.913202285766602, 'validation/accuracy': 0.0012599999317899346, 'validation/loss': 6.912908554077148, 'validation/num_examples': 50000, 'test/accuracy': 0.0014000000664964318, 'test/loss': 6.912501335144043, 'test/num_examples': 10000, 'score': 58.73045778274536, 'total_duration': 184.43373084068298, 'accumulated_submission_time': 58.73045778274536, 'accumulated_eval_time': 125.70307064056396, 'accumulated_logging_time': 0}
I0307 02:25:43.426548 140021201876736 logging_writer.py:48] [1] accumulated_eval_time=125.703, accumulated_logging_time=0, accumulated_submission_time=58.7305, global_step=1, preemption_count=0, score=58.7305, test/accuracy=0.0014, test/loss=6.9125, test/num_examples=10000, total_duration=184.434, train/accuracy=0.000996492, train/loss=6.9132, validation/accuracy=0.00126, validation/loss=6.91291, validation/num_examples=50000
I0307 02:26:20.075662 140021193484032 logging_writer.py:48] [100] global_step=100, grad_norm=0.6023036241531372, loss=6.903253078460693
I0307 02:26:57.606960 140021201876736 logging_writer.py:48] [200] global_step=200, grad_norm=0.5890704989433289, loss=6.856897830963135
I0307 02:27:34.270762 140021193484032 logging_writer.py:48] [300] global_step=300, grad_norm=0.6127339005470276, loss=6.8100385665893555
I0307 02:28:12.415488 140021201876736 logging_writer.py:48] [400] global_step=400, grad_norm=0.6777313351631165, loss=6.700458526611328
I0307 02:28:49.939241 140021193484032 logging_writer.py:48] [500] global_step=500, grad_norm=0.7108950018882751, loss=6.659962177276611
I0307 02:29:28.245624 140021201876736 logging_writer.py:48] [600] global_step=600, grad_norm=0.737955629825592, loss=6.538133144378662
I0307 02:30:06.378425 140021193484032 logging_writer.py:48] [700] global_step=700, grad_norm=0.7598188519477844, loss=6.420166969299316
I0307 02:30:43.956595 140021201876736 logging_writer.py:48] [800] global_step=800, grad_norm=0.8075785636901855, loss=6.350034236907959
I0307 02:31:21.914664 140021193484032 logging_writer.py:48] [900] global_step=900, grad_norm=0.9479500651359558, loss=6.263558387756348
I0307 02:31:59.623788 140021201876736 logging_writer.py:48] [1000] global_step=1000, grad_norm=1.1497530937194824, loss=6.155426502227783
I0307 02:32:37.140204 140021193484032 logging_writer.py:48] [1100] global_step=1100, grad_norm=1.7556341886520386, loss=6.055049896240234
I0307 02:33:14.530888 140021201876736 logging_writer.py:48] [1200] global_step=1200, grad_norm=1.8637416362762451, loss=5.989307880401611
I0307 02:33:50.602723 140021193484032 logging_writer.py:48] [1300] global_step=1300, grad_norm=1.8020809888839722, loss=5.986990928649902
I0307 02:34:13.551647 140178276385984 spec.py:321] Evaluating on the training split.
I0307 02:34:26.305310 140178276385984 spec.py:333] Evaluating on the validation split.
I0307 02:34:43.602203 140178276385984 spec.py:349] Evaluating on the test split.
I0307 02:34:45.608510 140178276385984 submission_runner.py:469] Time since start: 726.65s, 	Step: 1364, 	{'train/accuracy': 0.0675223171710968, 'train/loss': 5.486059188842773, 'validation/accuracy': 0.06019999831914902, 'validation/loss': 5.564065456390381, 'validation/num_examples': 50000, 'test/accuracy': 0.04190000146627426, 'test/loss': 5.753705978393555, 'test/num_examples': 10000, 'score': 568.6404049396515, 'total_duration': 726.6521196365356, 'accumulated_submission_time': 568.6404049396515, 'accumulated_eval_time': 157.75989651679993, 'accumulated_logging_time': 0.04564213752746582}
I0307 02:34:45.620240 140021210269440 logging_writer.py:48] [1364] accumulated_eval_time=157.76, accumulated_logging_time=0.0456421, accumulated_submission_time=568.64, global_step=1364, preemption_count=0, score=568.64, test/accuracy=0.0419, test/loss=5.75371, test/num_examples=10000, total_duration=726.652, train/accuracy=0.0675223, train/loss=5.48606, validation/accuracy=0.0602, validation/loss=5.56407, validation/num_examples=50000
I0307 02:34:59.889343 140021218662144 logging_writer.py:48] [1400] global_step=1400, grad_norm=2.2569077014923096, loss=5.87114953994751
I0307 02:35:38.170260 140021210269440 logging_writer.py:48] [1500] global_step=1500, grad_norm=2.326798677444458, loss=5.790853023529053
I0307 02:36:15.623812 140021218662144 logging_writer.py:48] [1600] global_step=1600, grad_norm=2.689077377319336, loss=5.737402439117432
I0307 02:36:53.922273 140021210269440 logging_writer.py:48] [1700] global_step=1700, grad_norm=2.9589405059814453, loss=5.654199600219727
I0307 02:37:31.843892 140021218662144 logging_writer.py:48] [1800] global_step=1800, grad_norm=2.1987767219543457, loss=5.634825229644775
I0307 02:38:09.883155 140021210269440 logging_writer.py:48] [1900] global_step=1900, grad_norm=2.805422067642212, loss=5.614474773406982
I0307 02:38:47.660102 140021218662144 logging_writer.py:48] [2000] global_step=2000, grad_norm=2.8775787353515625, loss=5.3982133865356445
I0307 02:39:26.056442 140021210269440 logging_writer.py:48] [2100] global_step=2100, grad_norm=2.946021556854248, loss=5.466464519500732
I0307 02:40:03.892498 140021218662144 logging_writer.py:48] [2200] global_step=2200, grad_norm=4.687397480010986, loss=5.475462436676025
I0307 02:40:42.132753 140021210269440 logging_writer.py:48] [2300] global_step=2300, grad_norm=2.648090124130249, loss=5.324443817138672
I0307 02:41:19.815844 140021218662144 logging_writer.py:48] [2400] global_step=2400, grad_norm=2.8555121421813965, loss=5.383031845092773
I0307 02:41:56.878464 140021210269440 logging_writer.py:48] [2500] global_step=2500, grad_norm=4.0920562744140625, loss=5.298574924468994
I0307 02:42:33.007626 140021218662144 logging_writer.py:48] [2600] global_step=2600, grad_norm=4.089080333709717, loss=5.254491329193115
I0307 02:43:11.023749 140021210269440 logging_writer.py:48] [2700] global_step=2700, grad_norm=3.4435813426971436, loss=5.20633602142334
I0307 02:43:15.951972 140178276385984 spec.py:321] Evaluating on the training split.
I0307 02:43:27.457762 140178276385984 spec.py:333] Evaluating on the validation split.
I0307 02:43:42.906248 140178276385984 spec.py:349] Evaluating on the test split.
I0307 02:43:44.773858 140178276385984 submission_runner.py:469] Time since start: 1265.82s, 	Step: 2714, 	{'train/accuracy': 0.15852199494838715, 'train/loss': 4.433286190032959, 'validation/accuracy': 0.13793998956680298, 'validation/loss': 4.577001571655273, 'validation/num_examples': 50000, 'test/accuracy': 0.09740000218153, 'test/loss': 4.945664882659912, 'test/num_examples': 10000, 'score': 1078.768221616745, 'total_duration': 1265.8174669742584, 'accumulated_submission_time': 1078.768221616745, 'accumulated_eval_time': 186.58173441886902, 'accumulated_logging_time': 0.07293891906738281}
I0307 02:43:44.805512 140021218662144 logging_writer.py:48] [2714] accumulated_eval_time=186.582, accumulated_logging_time=0.0729389, accumulated_submission_time=1078.77, global_step=2714, preemption_count=0, score=1078.77, test/accuracy=0.0974, test/loss=4.94566, test/num_examples=10000, total_duration=1265.82, train/accuracy=0.158522, train/loss=4.43329, validation/accuracy=0.13794, validation/loss=4.577, validation/num_examples=50000
I0307 02:44:17.743394 140021210269440 logging_writer.py:48] [2800] global_step=2800, grad_norm=7.35196590423584, loss=5.258956432342529
I0307 02:44:56.689736 140021218662144 logging_writer.py:48] [2900] global_step=2900, grad_norm=4.71108865737915, loss=5.02265739440918
I0307 02:45:34.896342 140021210269440 logging_writer.py:48] [3000] global_step=3000, grad_norm=5.593664169311523, loss=4.980876922607422
I0307 02:46:13.122857 140021218662144 logging_writer.py:48] [3100] global_step=3100, grad_norm=4.9626288414001465, loss=5.057960510253906
I0307 02:46:51.493702 140021210269440 logging_writer.py:48] [3200] global_step=3200, grad_norm=6.511869430541992, loss=4.904543399810791
I0307 02:47:30.208460 140021218662144 logging_writer.py:48] [3300] global_step=3300, grad_norm=4.1183295249938965, loss=4.916942119598389
I0307 02:48:08.162527 140021210269440 logging_writer.py:48] [3400] global_step=3400, grad_norm=5.440701007843018, loss=4.900638103485107
I0307 02:48:46.459086 140021218662144 logging_writer.py:48] [3500] global_step=3500, grad_norm=5.5788187980651855, loss=4.8542070388793945
I0307 02:49:24.836356 140021210269440 logging_writer.py:48] [3600] global_step=3600, grad_norm=6.083763599395752, loss=4.760305404663086
I0307 02:50:03.002540 140021218662144 logging_writer.py:48] [3700] global_step=3700, grad_norm=3.8591763973236084, loss=4.705846309661865
I0307 02:50:41.111590 140021210269440 logging_writer.py:48] [3800] global_step=3800, grad_norm=4.152601718902588, loss=4.826165676116943
I0307 02:51:19.469606 140021218662144 logging_writer.py:48] [3900] global_step=3900, grad_norm=7.777606010437012, loss=4.740706920623779
I0307 02:51:57.645063 140021210269440 logging_writer.py:48] [4000] global_step=4000, grad_norm=7.149509429931641, loss=4.611461639404297
I0307 02:52:14.894427 140178276385984 spec.py:321] Evaluating on the training split.
I0307 02:52:26.548394 140178276385984 spec.py:333] Evaluating on the validation split.
I0307 02:52:49.190812 140178276385984 spec.py:349] Evaluating on the test split.
I0307 02:52:50.983607 140178276385984 submission_runner.py:469] Time since start: 1812.03s, 	Step: 4046, 	{'train/accuracy': 0.25958624482154846, 'train/loss': 3.62833571434021, 'validation/accuracy': 0.22817999124526978, 'validation/loss': 3.8408868312835693, 'validation/num_examples': 50000, 'test/accuracy': 0.16610001027584076, 'test/loss': 4.363452434539795, 'test/num_examples': 10000, 'score': 1588.6849162578583, 'total_duration': 1812.0272243022919, 'accumulated_submission_time': 1588.6849162578583, 'accumulated_eval_time': 222.6708743572235, 'accumulated_logging_time': 0.11372518539428711}
I0307 02:52:51.012246 140021218662144 logging_writer.py:48] [4046] accumulated_eval_time=222.671, accumulated_logging_time=0.113725, accumulated_submission_time=1588.68, global_step=4046, preemption_count=0, score=1588.68, test/accuracy=0.1661, test/loss=4.36345, test/num_examples=10000, total_duration=1812.03, train/accuracy=0.259586, train/loss=3.62834, validation/accuracy=0.22818, validation/loss=3.84089, validation/num_examples=50000
I0307 02:53:11.631621 140021210269440 logging_writer.py:48] [4100] global_step=4100, grad_norm=4.6782145500183105, loss=4.531415939331055
I0307 02:53:48.394665 140021218662144 logging_writer.py:48] [4200] global_step=4200, grad_norm=3.8500754833221436, loss=4.53690242767334
I0307 02:54:26.156387 140021210269440 logging_writer.py:48] [4300] global_step=4300, grad_norm=4.877558708190918, loss=4.6024017333984375
I0307 02:55:04.346602 140021218662144 logging_writer.py:48] [4400] global_step=4400, grad_norm=5.643440246582031, loss=4.615644454956055
I0307 02:55:42.122545 140021210269440 logging_writer.py:48] [4500] global_step=4500, grad_norm=5.094004154205322, loss=4.488424301147461
I0307 02:56:22.994346 140021218662144 logging_writer.py:48] [4600] global_step=4600, grad_norm=6.646955966949463, loss=4.411019325256348
I0307 02:57:07.634569 140021210269440 logging_writer.py:48] [4700] global_step=4700, grad_norm=5.585386276245117, loss=4.431753158569336
I0307 02:57:46.529436 140021218662144 logging_writer.py:48] [4800] global_step=4800, grad_norm=5.751361846923828, loss=4.311801910400391
I0307 02:58:25.224724 140021210269440 logging_writer.py:48] [4900] global_step=4900, grad_norm=4.463071823120117, loss=4.337073802947998
I0307 02:59:04.926986 140021218662144 logging_writer.py:48] [5000] global_step=5000, grad_norm=4.68021297454834, loss=4.343108177185059
I0307 02:59:43.678289 140021210269440 logging_writer.py:48] [5100] global_step=5100, grad_norm=6.246310710906982, loss=4.316645622253418
I0307 03:00:22.067000 140021218662144 logging_writer.py:48] [5200] global_step=5200, grad_norm=6.551656246185303, loss=4.181707859039307
I0307 03:01:00.859963 140021210269440 logging_writer.py:48] [5300] global_step=5300, grad_norm=5.660995006561279, loss=4.107912063598633
I0307 03:01:21.250612 140178276385984 spec.py:321] Evaluating on the training split.
I0307 03:01:32.081791 140178276385984 spec.py:333] Evaluating on the validation split.
I0307 03:01:50.980476 140178276385984 spec.py:349] Evaluating on the test split.
I0307 03:01:52.780544 140178276385984 submission_runner.py:469] Time since start: 2353.82s, 	Step: 5354, 	{'train/accuracy': 0.3443279564380646, 'train/loss': 3.1209475994110107, 'validation/accuracy': 0.3086400032043457, 'validation/loss': 3.3249173164367676, 'validation/num_examples': 50000, 'test/accuracy': 0.2297000139951706, 'test/loss': 3.9242238998413086, 'test/num_examples': 10000, 'score': 2098.7594044208527, 'total_duration': 2353.8241600990295, 'accumulated_submission_time': 2098.7594044208527, 'accumulated_eval_time': 254.20077347755432, 'accumulated_logging_time': 0.1502981185913086}
I0307 03:01:52.817951 140021218662144 logging_writer.py:48] [5354] accumulated_eval_time=254.201, accumulated_logging_time=0.150298, accumulated_submission_time=2098.76, global_step=5354, preemption_count=0, score=2098.76, test/accuracy=0.2297, test/loss=3.92422, test/num_examples=10000, total_duration=2353.82, train/accuracy=0.344328, train/loss=3.12095, validation/accuracy=0.30864, validation/loss=3.32492, validation/num_examples=50000
I0307 03:02:10.921736 140021210269440 logging_writer.py:48] [5400] global_step=5400, grad_norm=4.719639301300049, loss=4.2573466300964355
I0307 03:02:49.607078 140021218662144 logging_writer.py:48] [5500] global_step=5500, grad_norm=7.012298583984375, loss=4.037341594696045
I0307 03:03:28.459859 140021210269440 logging_writer.py:48] [5600] global_step=5600, grad_norm=4.2443108558654785, loss=4.124582767486572
I0307 03:04:07.282417 140021218662144 logging_writer.py:48] [5700] global_step=5700, grad_norm=4.388775825500488, loss=4.10183048248291
I0307 03:04:46.091873 140021210269440 logging_writer.py:48] [5800] global_step=5800, grad_norm=5.263556003570557, loss=4.071770668029785
I0307 03:05:24.982398 140021218662144 logging_writer.py:48] [5900] global_step=5900, grad_norm=5.331692695617676, loss=4.014183521270752
I0307 03:06:03.114249 140021210269440 logging_writer.py:48] [6000] global_step=6000, grad_norm=5.520096778869629, loss=4.042496681213379
I0307 03:06:41.329019 140021218662144 logging_writer.py:48] [6100] global_step=6100, grad_norm=6.930004119873047, loss=4.020079135894775
I0307 03:07:19.828711 140021210269440 logging_writer.py:48] [6200] global_step=6200, grad_norm=7.354726791381836, loss=3.8752050399780273
I0307 03:07:58.506221 140021218662144 logging_writer.py:48] [6300] global_step=6300, grad_norm=5.6439995765686035, loss=4.002445697784424
I0307 03:08:37.471418 140021210269440 logging_writer.py:48] [6400] global_step=6400, grad_norm=6.623445510864258, loss=3.9399585723876953
I0307 03:09:16.021956 140021218662144 logging_writer.py:48] [6500] global_step=6500, grad_norm=6.357580184936523, loss=3.866063117980957
I0307 03:09:54.446785 140021210269440 logging_writer.py:48] [6600] global_step=6600, grad_norm=5.155400276184082, loss=3.8275067806243896
I0307 03:10:23.032556 140178276385984 spec.py:321] Evaluating on the training split.
I0307 03:10:34.506443 140178276385984 spec.py:333] Evaluating on the validation split.
I0307 03:10:53.039379 140178276385984 spec.py:349] Evaluating on the test split.
I0307 03:10:54.889435 140178276385984 submission_runner.py:469] Time since start: 2895.93s, 	Step: 6676, 	{'train/accuracy': 0.4188257157802582, 'train/loss': 2.6895928382873535, 'validation/accuracy': 0.37417998909950256, 'validation/loss': 2.9200215339660645, 'validation/num_examples': 50000, 'test/accuracy': 0.2785000205039978, 'test/loss': 3.572272777557373, 'test/num_examples': 10000, 'score': 2608.8417177200317, 'total_duration': 2895.933042526245, 'accumulated_submission_time': 2608.8417177200317, 'accumulated_eval_time': 286.05760288238525, 'accumulated_logging_time': 0.19629478454589844}
I0307 03:10:54.963291 140021218662144 logging_writer.py:48] [6676] accumulated_eval_time=286.058, accumulated_logging_time=0.196295, accumulated_submission_time=2608.84, global_step=6676, preemption_count=0, score=2608.84, test/accuracy=0.2785, test/loss=3.57227, test/num_examples=10000, total_duration=2895.93, train/accuracy=0.418826, train/loss=2.68959, validation/accuracy=0.37418, validation/loss=2.92002, validation/num_examples=50000
I0307 03:11:04.667486 140021210269440 logging_writer.py:48] [6700] global_step=6700, grad_norm=5.1989312171936035, loss=3.8167495727539062
I0307 03:11:42.740116 140021218662144 logging_writer.py:48] [6800] global_step=6800, grad_norm=4.880583763122559, loss=3.8140478134155273
I0307 03:12:20.878660 140021210269440 logging_writer.py:48] [6900] global_step=6900, grad_norm=5.632176399230957, loss=3.827101469039917
I0307 03:13:00.128325 140021218662144 logging_writer.py:48] [7000] global_step=7000, grad_norm=5.275969505310059, loss=3.743422746658325
I0307 03:13:51.211755 140021210269440 logging_writer.py:48] [7100] global_step=7100, grad_norm=5.647085666656494, loss=3.7882156372070312
I0307 03:14:41.338168 140021218662144 logging_writer.py:48] [7200] global_step=7200, grad_norm=8.856196403503418, loss=3.6910035610198975
I0307 03:15:19.693132 140021210269440 logging_writer.py:48] [7300] global_step=7300, grad_norm=7.256224632263184, loss=3.7130913734436035
I0307 03:15:58.040035 140021218662144 logging_writer.py:48] [7400] global_step=7400, grad_norm=4.411647796630859, loss=3.689216136932373
I0307 03:16:36.276244 140021210269440 logging_writer.py:48] [7500] global_step=7500, grad_norm=6.963475227355957, loss=3.610027313232422
I0307 03:17:14.970336 140021218662144 logging_writer.py:48] [7600] global_step=7600, grad_norm=5.136322021484375, loss=3.5576331615448
I0307 03:17:53.607987 140021210269440 logging_writer.py:48] [7700] global_step=7700, grad_norm=6.843466281890869, loss=3.6500587463378906
I0307 03:18:31.977307 140021218662144 logging_writer.py:48] [7800] global_step=7800, grad_norm=5.449273109436035, loss=3.631596803665161
I0307 03:19:10.578447 140021210269440 logging_writer.py:48] [7900] global_step=7900, grad_norm=8.709358215332031, loss=3.6646714210510254
I0307 03:19:25.253376 140178276385984 spec.py:321] Evaluating on the training split.
I0307 03:19:36.922710 140178276385984 spec.py:333] Evaluating on the validation split.
I0307 03:19:56.236526 140178276385984 spec.py:349] Evaluating on the test split.
I0307 03:19:58.089219 140178276385984 submission_runner.py:469] Time since start: 3439.13s, 	Step: 7939, 	{'train/accuracy': 0.4820830523967743, 'train/loss': 2.3453423976898193, 'validation/accuracy': 0.4251599907875061, 'validation/loss': 2.626068592071533, 'validation/num_examples': 50000, 'test/accuracy': 0.32830002903938293, 'test/loss': 3.2708113193511963, 'test/num_examples': 10000, 'score': 3118.9543426036835, 'total_duration': 3439.132836341858, 'accumulated_submission_time': 3118.9543426036835, 'accumulated_eval_time': 318.89340806007385, 'accumulated_logging_time': 0.31638145446777344}
I0307 03:19:58.172982 140021218662144 logging_writer.py:48] [7939] accumulated_eval_time=318.893, accumulated_logging_time=0.316381, accumulated_submission_time=3118.95, global_step=7939, preemption_count=0, score=3118.95, test/accuracy=0.3283, test/loss=3.27081, test/num_examples=10000, total_duration=3439.13, train/accuracy=0.482083, train/loss=2.34534, validation/accuracy=0.42516, validation/loss=2.62607, validation/num_examples=50000
I0307 03:20:22.289097 140021210269440 logging_writer.py:48] [8000] global_step=8000, grad_norm=6.194087028503418, loss=3.599823474884033
I0307 03:21:00.831261 140021218662144 logging_writer.py:48] [8100] global_step=8100, grad_norm=6.029751300811768, loss=3.5148110389709473
I0307 03:21:39.025644 140021210269440 logging_writer.py:48] [8200] global_step=8200, grad_norm=5.516979694366455, loss=3.579348087310791
I0307 03:22:17.228192 140021218662144 logging_writer.py:48] [8300] global_step=8300, grad_norm=5.023279666900635, loss=3.547346353530884
I0307 03:22:54.498992 140021210269440 logging_writer.py:48] [8400] global_step=8400, grad_norm=4.505528450012207, loss=3.499429225921631
I0307 03:23:31.787800 140021218662144 logging_writer.py:48] [8500] global_step=8500, grad_norm=5.965455055236816, loss=3.620079278945923
I0307 03:24:09.966960 140021210269440 logging_writer.py:48] [8600] global_step=8600, grad_norm=4.709263801574707, loss=3.466458320617676
I0307 03:24:48.201375 140021218662144 logging_writer.py:48] [8700] global_step=8700, grad_norm=3.9169678688049316, loss=3.5059289932250977
I0307 03:25:26.911119 140021210269440 logging_writer.py:48] [8800] global_step=8800, grad_norm=6.260866641998291, loss=3.4086499214172363
I0307 03:26:05.358429 140021218662144 logging_writer.py:48] [8900] global_step=8900, grad_norm=6.205537796020508, loss=3.4279255867004395
I0307 03:26:43.871020 140021210269440 logging_writer.py:48] [9000] global_step=9000, grad_norm=5.137519359588623, loss=3.49202036857605
I0307 03:27:22.461615 140021218662144 logging_writer.py:48] [9100] global_step=9100, grad_norm=6.205265522003174, loss=3.4201302528381348
I0307 03:28:00.509212 140021210269440 logging_writer.py:48] [9200] global_step=9200, grad_norm=5.531891822814941, loss=3.451197385787964
I0307 03:28:28.198469 140178276385984 spec.py:321] Evaluating on the training split.
I0307 03:28:39.695293 140178276385984 spec.py:333] Evaluating on the validation split.
I0307 03:28:59.512890 140178276385984 spec.py:349] Evaluating on the test split.
I0307 03:29:01.345192 140178276385984 submission_runner.py:469] Time since start: 3982.39s, 	Step: 9273, 	{'train/accuracy': 0.5176777839660645, 'train/loss': 2.1896069049835205, 'validation/accuracy': 0.46333998441696167, 'validation/loss': 2.463589668273926, 'validation/num_examples': 50000, 'test/accuracy': 0.3579000234603882, 'test/loss': 3.092560291290283, 'test/num_examples': 10000, 'score': 3628.821580171585, 'total_duration': 3982.388798236847, 'accumulated_submission_time': 3628.821580171585, 'accumulated_eval_time': 352.04007840156555, 'accumulated_logging_time': 0.40958476066589355}
I0307 03:29:01.380282 140021218662144 logging_writer.py:48] [9273] accumulated_eval_time=352.04, accumulated_logging_time=0.409585, accumulated_submission_time=3628.82, global_step=9273, preemption_count=0, score=3628.82, test/accuracy=0.3579, test/loss=3.09256, test/num_examples=10000, total_duration=3982.39, train/accuracy=0.517678, train/loss=2.18961, validation/accuracy=0.46334, validation/loss=2.46359, validation/num_examples=50000
I0307 03:29:12.102579 140021210269440 logging_writer.py:48] [9300] global_step=9300, grad_norm=6.634688377380371, loss=3.3990888595581055
I0307 03:29:50.309807 140021218662144 logging_writer.py:48] [9400] global_step=9400, grad_norm=6.4215216636657715, loss=3.4232192039489746
I0307 03:30:28.960367 140021210269440 logging_writer.py:48] [9500] global_step=9500, grad_norm=6.468711853027344, loss=3.376408815383911
I0307 03:31:07.472253 140021218662144 logging_writer.py:48] [9600] global_step=9600, grad_norm=5.683620452880859, loss=3.4246654510498047
I0307 03:31:45.779708 140021210269440 logging_writer.py:48] [9700] global_step=9700, grad_norm=8.243931770324707, loss=3.42488431930542
I0307 03:32:24.154246 140021218662144 logging_writer.py:48] [9800] global_step=9800, grad_norm=5.823230266571045, loss=3.376675605773926
I0307 03:33:02.870835 140021210269440 logging_writer.py:48] [9900] global_step=9900, grad_norm=4.287245273590088, loss=3.3839266300201416
I0307 03:33:41.573775 140021218662144 logging_writer.py:48] [10000] global_step=10000, grad_norm=4.350796222686768, loss=3.3386387825012207
I0307 03:34:20.116992 140021210269440 logging_writer.py:48] [10100] global_step=10100, grad_norm=4.409387588500977, loss=3.3056981563568115
I0307 03:34:58.577068 140021218662144 logging_writer.py:48] [10200] global_step=10200, grad_norm=4.431865692138672, loss=3.2932310104370117
I0307 03:35:37.336462 140021210269440 logging_writer.py:48] [10300] global_step=10300, grad_norm=6.114281177520752, loss=3.3268959522247314
I0307 03:36:15.925362 140021218662144 logging_writer.py:48] [10400] global_step=10400, grad_norm=7.31864595413208, loss=3.3470406532287598
I0307 03:36:54.539343 140021210269440 logging_writer.py:48] [10500] global_step=10500, grad_norm=7.984176158905029, loss=3.302563190460205
I0307 03:37:31.387358 140178276385984 spec.py:321] Evaluating on the training split.
I0307 03:37:48.186372 140178276385984 spec.py:333] Evaluating on the validation split.
I0307 03:38:08.269936 140178276385984 spec.py:349] Evaluating on the test split.
I0307 03:38:10.112161 140178276385984 submission_runner.py:469] Time since start: 4531.16s, 	Step: 10596, 	{'train/accuracy': 0.5426697731018066, 'train/loss': 2.040057897567749, 'validation/accuracy': 0.4890799820423126, 'validation/loss': 2.310072660446167, 'validation/num_examples': 50000, 'test/accuracy': 0.37550002336502075, 'test/loss': 2.9704039096832275, 'test/num_examples': 10000, 'score': 4138.6773002147675, 'total_duration': 4531.155784845352, 'accumulated_submission_time': 4138.6773002147675, 'accumulated_eval_time': 390.76485323905945, 'accumulated_logging_time': 0.453249454498291}
I0307 03:38:10.147453 140021218662144 logging_writer.py:48] [10596] accumulated_eval_time=390.765, accumulated_logging_time=0.453249, accumulated_submission_time=4138.68, global_step=10596, preemption_count=0, score=4138.68, test/accuracy=0.3755, test/loss=2.9704, test/num_examples=10000, total_duration=4531.16, train/accuracy=0.54267, train/loss=2.04006, validation/accuracy=0.48908, validation/loss=2.31007, validation/num_examples=50000
I0307 03:38:12.004305 140021210269440 logging_writer.py:48] [10600] global_step=10600, grad_norm=6.414921283721924, loss=3.3781261444091797
I0307 03:38:50.488544 140021218662144 logging_writer.py:48] [10700] global_step=10700, grad_norm=4.969325542449951, loss=3.335785388946533
I0307 03:39:29.122479 140021210269440 logging_writer.py:48] [10800] global_step=10800, grad_norm=8.710420608520508, loss=3.21468448638916
I0307 03:40:07.711274 140021218662144 logging_writer.py:48] [10900] global_step=10900, grad_norm=4.736459255218506, loss=3.251319646835327
I0307 03:40:46.200274 140021210269440 logging_writer.py:48] [11000] global_step=11000, grad_norm=6.675834655761719, loss=3.2419440746307373
I0307 03:41:24.896782 140021218662144 logging_writer.py:48] [11100] global_step=11100, grad_norm=6.5662312507629395, loss=3.2644968032836914
I0307 03:42:03.418684 140021210269440 logging_writer.py:48] [11200] global_step=11200, grad_norm=5.523723602294922, loss=3.301085948944092
I0307 03:42:42.141952 140021218662144 logging_writer.py:48] [11300] global_step=11300, grad_norm=5.435121536254883, loss=3.3072350025177
I0307 03:43:20.804360 140021210269440 logging_writer.py:48] [11400] global_step=11400, grad_norm=4.596254825592041, loss=3.2091174125671387
I0307 03:43:59.464223 140021218662144 logging_writer.py:48] [11500] global_step=11500, grad_norm=5.3232269287109375, loss=3.169729471206665
I0307 03:44:37.770413 140021210269440 logging_writer.py:48] [11600] global_step=11600, grad_norm=6.9249467849731445, loss=3.067960023880005
I0307 03:45:16.507700 140021218662144 logging_writer.py:48] [11700] global_step=11700, grad_norm=6.441717147827148, loss=3.24336576461792
I0307 03:45:54.923528 140021210269440 logging_writer.py:48] [11800] global_step=11800, grad_norm=3.4150760173797607, loss=3.1722543239593506
I0307 03:46:33.272629 140021218662144 logging_writer.py:48] [11900] global_step=11900, grad_norm=4.8510870933532715, loss=3.2394866943359375
I0307 03:46:40.279366 140178276385984 spec.py:321] Evaluating on the training split.
I0307 03:46:52.830615 140178276385984 spec.py:333] Evaluating on the validation split.
I0307 03:47:12.290865 140178276385984 spec.py:349] Evaluating on the test split.
I0307 03:47:14.156154 140178276385984 submission_runner.py:469] Time since start: 5075.20s, 	Step: 11919, 	{'train/accuracy': 0.5757134556770325, 'train/loss': 1.8782466650009155, 'validation/accuracy': 0.5187399983406067, 'validation/loss': 2.147243022918701, 'validation/num_examples': 50000, 'test/accuracy': 0.39900001883506775, 'test/loss': 2.8007309436798096, 'test/num_examples': 10000, 'score': 4648.66397023201, 'total_duration': 5075.199770689011, 'accumulated_submission_time': 4648.66397023201, 'accumulated_eval_time': 424.641606092453, 'accumulated_logging_time': 0.49742960929870605}
I0307 03:47:14.201636 140021210269440 logging_writer.py:48] [11919] accumulated_eval_time=424.642, accumulated_logging_time=0.49743, accumulated_submission_time=4648.66, global_step=11919, preemption_count=0, score=4648.66, test/accuracy=0.399, test/loss=2.80073, test/num_examples=10000, total_duration=5075.2, train/accuracy=0.575713, train/loss=1.87825, validation/accuracy=0.51874, validation/loss=2.14724, validation/num_examples=50000
I0307 03:47:45.600784 140021218662144 logging_writer.py:48] [12000] global_step=12000, grad_norm=5.7329511642456055, loss=3.1728579998016357
I0307 03:48:23.948227 140021210269440 logging_writer.py:48] [12100] global_step=12100, grad_norm=5.587213516235352, loss=3.2871267795562744
I0307 03:49:02.496054 140021218662144 logging_writer.py:48] [12200] global_step=12200, grad_norm=5.339432716369629, loss=3.246551036834717
I0307 03:49:40.944180 140021210269440 logging_writer.py:48] [12300] global_step=12300, grad_norm=4.272019386291504, loss=3.2147018909454346
I0307 03:50:19.931371 140021218662144 logging_writer.py:48] [12400] global_step=12400, grad_norm=6.250687599182129, loss=3.206232786178589
I0307 03:50:58.571796 140021210269440 logging_writer.py:48] [12500] global_step=12500, grad_norm=6.619192123413086, loss=3.166311740875244
I0307 03:51:37.394807 140021218662144 logging_writer.py:48] [12600] global_step=12600, grad_norm=5.718137264251709, loss=3.23836088180542
I0307 03:52:16.122916 140021210269440 logging_writer.py:48] [12700] global_step=12700, grad_norm=4.674267292022705, loss=3.2395544052124023
I0307 03:52:54.703186 140021218662144 logging_writer.py:48] [12800] global_step=12800, grad_norm=2.75105881690979, loss=3.0965960025787354
I0307 03:53:32.876203 140021210269440 logging_writer.py:48] [12900] global_step=12900, grad_norm=4.100986003875732, loss=3.1911730766296387
I0307 03:54:11.729713 140021218662144 logging_writer.py:48] [13000] global_step=13000, grad_norm=7.236172676086426, loss=3.106971025466919
I0307 03:54:50.090755 140021210269440 logging_writer.py:48] [13100] global_step=13100, grad_norm=6.233433246612549, loss=3.1575276851654053
I0307 03:55:28.397240 140021218662144 logging_writer.py:48] [13200] global_step=13200, grad_norm=3.6675264835357666, loss=3.0963008403778076
I0307 03:55:44.525687 140178276385984 spec.py:321] Evaluating on the training split.
I0307 03:56:02.999341 140178276385984 spec.py:333] Evaluating on the validation split.
I0307 03:56:21.394924 140178276385984 spec.py:349] Evaluating on the test split.
I0307 03:56:23.227138 140178276385984 submission_runner.py:469] Time since start: 5624.27s, 	Step: 13243, 	{'train/accuracy': 0.5942283272743225, 'train/loss': 1.7778406143188477, 'validation/accuracy': 0.536899983882904, 'validation/loss': 2.0516693592071533, 'validation/num_examples': 50000, 'test/accuracy': 0.4289000332355499, 'test/loss': 2.710283041000366, 'test/num_examples': 10000, 'score': 5158.840147972107, 'total_duration': 5624.270761728287, 'accumulated_submission_time': 5158.840147972107, 'accumulated_eval_time': 463.343022108078, 'accumulated_logging_time': 0.5522055625915527}
I0307 03:56:23.266623 140021210269440 logging_writer.py:48] [13243] accumulated_eval_time=463.343, accumulated_logging_time=0.552206, accumulated_submission_time=5158.84, global_step=13243, preemption_count=0, score=5158.84, test/accuracy=0.4289, test/loss=2.71028, test/num_examples=10000, total_duration=5624.27, train/accuracy=0.594228, train/loss=1.77784, validation/accuracy=0.5369, validation/loss=2.05167, validation/num_examples=50000
I0307 03:56:45.576938 140021218662144 logging_writer.py:48] [13300] global_step=13300, grad_norm=6.554685115814209, loss=3.1118969917297363
I0307 03:57:26.148165 140021210269440 logging_writer.py:48] [13400] global_step=13400, grad_norm=5.253948211669922, loss=3.020291328430176
I0307 03:58:04.538233 140021218662144 logging_writer.py:48] [13500] global_step=13500, grad_norm=5.339576244354248, loss=3.1892857551574707
I0307 03:58:43.329159 140021210269440 logging_writer.py:48] [13600] global_step=13600, grad_norm=5.021080493927002, loss=3.107581615447998
I0307 03:59:21.775485 140021218662144 logging_writer.py:48] [13700] global_step=13700, grad_norm=5.8379950523376465, loss=3.1066343784332275
I0307 04:00:00.736730 140021210269440 logging_writer.py:48] [13800] global_step=13800, grad_norm=5.3025054931640625, loss=3.1395766735076904
I0307 04:00:39.297392 140021218662144 logging_writer.py:48] [13900] global_step=13900, grad_norm=4.006241321563721, loss=3.099487066268921
I0307 04:01:18.251062 140021210269440 logging_writer.py:48] [14000] global_step=14000, grad_norm=5.253549098968506, loss=3.161842107772827
I0307 04:01:56.901782 140021218662144 logging_writer.py:48] [14100] global_step=14100, grad_norm=4.0395073890686035, loss=3.129998207092285
I0307 04:02:35.312159 140021210269440 logging_writer.py:48] [14200] global_step=14200, grad_norm=4.0008134841918945, loss=3.0001425743103027
I0307 04:03:13.816202 140021218662144 logging_writer.py:48] [14300] global_step=14300, grad_norm=5.464789390563965, loss=3.099943161010742
I0307 04:03:52.325067 140021210269440 logging_writer.py:48] [14400] global_step=14400, grad_norm=5.322556018829346, loss=3.078263759613037
I0307 04:04:30.544817 140021218662144 logging_writer.py:48] [14500] global_step=14500, grad_norm=8.167302131652832, loss=3.06432843208313
I0307 04:04:53.491462 140178276385984 spec.py:321] Evaluating on the training split.
I0307 04:05:13.668273 140178276385984 spec.py:333] Evaluating on the validation split.
I0307 04:05:34.563025 140178276385984 spec.py:349] Evaluating on the test split.
I0307 04:05:36.845212 140178276385984 submission_runner.py:469] Time since start: 6177.89s, 	Step: 14560, 	{'train/accuracy': 0.6015425324440002, 'train/loss': 1.7793742418289185, 'validation/accuracy': 0.5474199652671814, 'validation/loss': 2.0388450622558594, 'validation/num_examples': 50000, 'test/accuracy': 0.4229000210762024, 'test/loss': 2.727372646331787, 'test/num_examples': 10000, 'score': 5668.890580654144, 'total_duration': 6177.888833761215, 'accumulated_submission_time': 5668.890580654144, 'accumulated_eval_time': 506.6967315673828, 'accumulated_logging_time': 0.6243801116943359}
I0307 04:05:36.937049 140021210269440 logging_writer.py:48] [14560] accumulated_eval_time=506.697, accumulated_logging_time=0.62438, accumulated_submission_time=5668.89, global_step=14560, preemption_count=0, score=5668.89, test/accuracy=0.4229, test/loss=2.72737, test/num_examples=10000, total_duration=6177.89, train/accuracy=0.601543, train/loss=1.77937, validation/accuracy=0.54742, validation/loss=2.03885, validation/num_examples=50000
I0307 04:05:52.871815 140021218662144 logging_writer.py:48] [14600] global_step=14600, grad_norm=6.0093536376953125, loss=2.996411085128784
I0307 04:06:31.309971 140021210269440 logging_writer.py:48] [14700] global_step=14700, grad_norm=4.098805904388428, loss=3.001549005508423
I0307 04:07:09.733120 140021218662144 logging_writer.py:48] [14800] global_step=14800, grad_norm=4.905823707580566, loss=3.001613140106201
I0307 04:07:48.447061 140021210269440 logging_writer.py:48] [14900] global_step=14900, grad_norm=5.952868461608887, loss=3.0413975715637207
I0307 04:08:27.024156 140021218662144 logging_writer.py:48] [15000] global_step=15000, grad_norm=4.8788628578186035, loss=3.0191006660461426
I0307 04:09:06.059883 140021210269440 logging_writer.py:48] [15100] global_step=15100, grad_norm=4.737414360046387, loss=3.095660924911499
I0307 04:09:44.854442 140021218662144 logging_writer.py:48] [15200] global_step=15200, grad_norm=5.445488452911377, loss=3.0232536792755127
I0307 04:10:23.659453 140021210269440 logging_writer.py:48] [15300] global_step=15300, grad_norm=4.8191728591918945, loss=3.0074002742767334
I0307 04:11:02.168931 140021218662144 logging_writer.py:48] [15400] global_step=15400, grad_norm=8.895259857177734, loss=3.0425424575805664
I0307 04:11:40.605783 140021210269440 logging_writer.py:48] [15500] global_step=15500, grad_norm=3.7165400981903076, loss=3.016386032104492
I0307 04:12:19.377258 140021218662144 logging_writer.py:48] [15600] global_step=15600, grad_norm=5.770209789276123, loss=3.0886545181274414
I0307 04:12:58.161164 140021210269440 logging_writer.py:48] [15700] global_step=15700, grad_norm=5.766438007354736, loss=2.9740917682647705
I0307 04:13:37.204901 140021218662144 logging_writer.py:48] [15800] global_step=15800, grad_norm=4.900140762329102, loss=2.969874382019043
I0307 04:14:07.134986 140178276385984 spec.py:321] Evaluating on the training split.
I0307 04:14:25.939296 140178276385984 spec.py:333] Evaluating on the validation split.
I0307 04:14:45.629411 140178276385984 spec.py:349] Evaluating on the test split.
I0307 04:14:47.409339 140178276385984 submission_runner.py:469] Time since start: 6728.45s, 	Step: 15878, 	{'train/accuracy': 0.6091358065605164, 'train/loss': 1.7038657665252686, 'validation/accuracy': 0.5549799799919128, 'validation/loss': 1.9675432443618774, 'validation/num_examples': 50000, 'test/accuracy': 0.43970000743865967, 'test/loss': 2.628751754760742, 'test/num_examples': 10000, 'score': 6178.918691635132, 'total_duration': 6728.452962398529, 'accumulated_submission_time': 6178.918691635132, 'accumulated_eval_time': 546.9710538387299, 'accumulated_logging_time': 0.7399330139160156}
I0307 04:14:47.478589 140021210269440 logging_writer.py:48] [15878] accumulated_eval_time=546.971, accumulated_logging_time=0.739933, accumulated_submission_time=6178.92, global_step=15878, preemption_count=0, score=6178.92, test/accuracy=0.4397, test/loss=2.62875, test/num_examples=10000, total_duration=6728.45, train/accuracy=0.609136, train/loss=1.70387, validation/accuracy=0.55498, validation/loss=1.96754, validation/num_examples=50000
I0307 04:14:56.397465 140021218662144 logging_writer.py:48] [15900] global_step=15900, grad_norm=6.331028461456299, loss=3.108611822128296
I0307 04:15:34.862107 140021210269440 logging_writer.py:48] [16000] global_step=16000, grad_norm=6.224076271057129, loss=3.071636438369751
I0307 04:16:13.325425 140021218662144 logging_writer.py:48] [16100] global_step=16100, grad_norm=5.655739784240723, loss=3.0347824096679688
I0307 04:16:52.223027 140021210269440 logging_writer.py:48] [16200] global_step=16200, grad_norm=6.408766269683838, loss=3.0581958293914795
I0307 04:17:30.966429 140021218662144 logging_writer.py:48] [16300] global_step=16300, grad_norm=4.4458112716674805, loss=2.991474151611328
I0307 04:18:09.579579 140021210269440 logging_writer.py:48] [16400] global_step=16400, grad_norm=7.236523628234863, loss=3.0553274154663086
I0307 04:18:48.161804 140021218662144 logging_writer.py:48] [16500] global_step=16500, grad_norm=5.389794826507568, loss=2.964674234390259
I0307 04:19:26.792262 140021210269440 logging_writer.py:48] [16600] global_step=16600, grad_norm=5.150828838348389, loss=3.039247512817383
I0307 04:20:05.453759 140021218662144 logging_writer.py:48] [16700] global_step=16700, grad_norm=5.312948703765869, loss=3.071476936340332
I0307 04:20:44.049031 140021210269440 logging_writer.py:48] [16800] global_step=16800, grad_norm=4.514318943023682, loss=2.920041799545288
I0307 04:21:22.742425 140021218662144 logging_writer.py:48] [16900] global_step=16900, grad_norm=5.127954483032227, loss=3.094334840774536
I0307 04:22:01.587805 140021210269440 logging_writer.py:48] [17000] global_step=17000, grad_norm=5.440582275390625, loss=2.9835386276245117
I0307 04:22:39.921010 140021218662144 logging_writer.py:48] [17100] global_step=17100, grad_norm=5.28525447845459, loss=3.0248727798461914
I0307 04:23:17.622131 140178276385984 spec.py:321] Evaluating on the training split.
I0307 04:23:33.234293 140178276385984 spec.py:333] Evaluating on the validation split.
I0307 04:23:54.145343 140178276385984 spec.py:349] Evaluating on the test split.
I0307 04:23:56.115504 140178276385984 submission_runner.py:469] Time since start: 7277.16s, 	Step: 17199, 	{'train/accuracy': 0.6215322017669678, 'train/loss': 1.6433767080307007, 'validation/accuracy': 0.5615800023078918, 'validation/loss': 1.9205811023712158, 'validation/num_examples': 50000, 'test/accuracy': 0.4382000267505646, 'test/loss': 2.6020491123199463, 'test/num_examples': 10000, 'score': 6688.884175777435, 'total_duration': 7277.159107208252, 'accumulated_submission_time': 6688.884175777435, 'accumulated_eval_time': 585.464373588562, 'accumulated_logging_time': 0.8444693088531494}
I0307 04:23:56.195299 140021210269440 logging_writer.py:48] [17199] accumulated_eval_time=585.464, accumulated_logging_time=0.844469, accumulated_submission_time=6688.88, global_step=17199, preemption_count=0, score=6688.88, test/accuracy=0.4382, test/loss=2.60205, test/num_examples=10000, total_duration=7277.16, train/accuracy=0.621532, train/loss=1.64338, validation/accuracy=0.56158, validation/loss=1.92058, validation/num_examples=50000
I0307 04:23:57.069357 140021218662144 logging_writer.py:48] [17200] global_step=17200, grad_norm=6.051843166351318, loss=2.9803953170776367
I0307 04:24:35.369197 140021210269440 logging_writer.py:48] [17300] global_step=17300, grad_norm=4.633184909820557, loss=3.022127628326416
I0307 04:25:13.757349 140021218662144 logging_writer.py:48] [17400] global_step=17400, grad_norm=3.544023275375366, loss=2.9783835411071777
I0307 04:25:52.283108 140021210269440 logging_writer.py:48] [17500] global_step=17500, grad_norm=3.972139596939087, loss=3.047941207885742
I0307 04:26:30.820527 140021218662144 logging_writer.py:48] [17600] global_step=17600, grad_norm=6.2441229820251465, loss=3.103881359100342
I0307 04:27:09.442053 140021210269440 logging_writer.py:48] [17700] global_step=17700, grad_norm=3.548171043395996, loss=3.0197737216949463
I0307 04:27:48.324444 140021218662144 logging_writer.py:48] [17800] global_step=17800, grad_norm=3.817997932434082, loss=2.9754886627197266
I0307 04:28:27.188692 140021210269440 logging_writer.py:48] [17900] global_step=17900, grad_norm=5.792604923248291, loss=3.036989212036133
I0307 04:29:05.779175 140021218662144 logging_writer.py:48] [18000] global_step=18000, grad_norm=4.858471393585205, loss=2.9355154037475586
I0307 04:29:44.857406 140021210269440 logging_writer.py:48] [18100] global_step=18100, grad_norm=5.21724796295166, loss=2.9047603607177734
I0307 04:30:23.456329 140021218662144 logging_writer.py:48] [18200] global_step=18200, grad_norm=4.031919956207275, loss=3.005791425704956
I0307 04:31:02.197093 140021210269440 logging_writer.py:48] [18300] global_step=18300, grad_norm=3.648054838180542, loss=3.0151314735412598
I0307 04:31:41.004463 140021218662144 logging_writer.py:48] [18400] global_step=18400, grad_norm=6.307304382324219, loss=2.937765121459961
I0307 04:32:19.658987 140021210269440 logging_writer.py:48] [18500] global_step=18500, grad_norm=4.502012252807617, loss=3.038717269897461
I0307 04:32:26.218811 140178276385984 spec.py:321] Evaluating on the training split.
I0307 04:32:39.508173 140178276385984 spec.py:333] Evaluating on the validation split.
I0307 04:32:59.765326 140178276385984 spec.py:349] Evaluating on the test split.
I0307 04:33:01.588718 140178276385984 submission_runner.py:469] Time since start: 7822.63s, 	Step: 18518, 	{'train/accuracy': 0.6163504123687744, 'train/loss': 1.6868990659713745, 'validation/accuracy': 0.5631600022315979, 'validation/loss': 1.9450334310531616, 'validation/num_examples': 50000, 'test/accuracy': 0.44350001215934753, 'test/loss': 2.576486825942993, 'test/num_examples': 10000, 'score': 7198.761182785034, 'total_duration': 7822.632341384888, 'accumulated_submission_time': 7198.761182785034, 'accumulated_eval_time': 620.834246635437, 'accumulated_logging_time': 0.942030668258667}
I0307 04:33:01.616527 140021218662144 logging_writer.py:48] [18518] accumulated_eval_time=620.834, accumulated_logging_time=0.942031, accumulated_submission_time=7198.76, global_step=18518, preemption_count=0, score=7198.76, test/accuracy=0.4435, test/loss=2.57649, test/num_examples=10000, total_duration=7822.63, train/accuracy=0.61635, train/loss=1.6869, validation/accuracy=0.56316, validation/loss=1.94503, validation/num_examples=50000
I0307 04:33:33.302930 140021210269440 logging_writer.py:48] [18600] global_step=18600, grad_norm=3.4559648036956787, loss=2.943641424179077
I0307 04:34:11.753628 140021218662144 logging_writer.py:48] [18700] global_step=18700, grad_norm=4.268629550933838, loss=2.9838857650756836
I0307 04:34:50.781423 140021210269440 logging_writer.py:48] [18800] global_step=18800, grad_norm=4.9644646644592285, loss=3.0212326049804688
I0307 04:35:29.394070 140021218662144 logging_writer.py:48] [18900] global_step=18900, grad_norm=4.3841872215271, loss=2.972292900085449
I0307 04:36:08.358269 140021210269440 logging_writer.py:48] [19000] global_step=19000, grad_norm=3.273296356201172, loss=2.948578119277954
I0307 04:36:46.821244 140021218662144 logging_writer.py:48] [19100] global_step=19100, grad_norm=3.757323980331421, loss=2.9061169624328613
I0307 04:37:25.269036 140021210269440 logging_writer.py:48] [19200] global_step=19200, grad_norm=4.892921447753906, loss=3.0918402671813965
I0307 04:38:03.971440 140021218662144 logging_writer.py:48] [19300] global_step=19300, grad_norm=3.0407514572143555, loss=2.9808237552642822
I0307 04:38:42.031521 140021210269440 logging_writer.py:48] [19400] global_step=19400, grad_norm=4.528264045715332, loss=3.0472936630249023
I0307 04:39:20.216202 140021218662144 logging_writer.py:48] [19500] global_step=19500, grad_norm=3.6457974910736084, loss=3.1035187244415283
I0307 04:39:58.372720 140021210269440 logging_writer.py:48] [19600] global_step=19600, grad_norm=3.6200060844421387, loss=2.99786114692688
I0307 04:40:36.332569 140021218662144 logging_writer.py:48] [19700] global_step=19700, grad_norm=4.445927143096924, loss=2.9701480865478516
I0307 04:41:14.997482 140021210269440 logging_writer.py:48] [19800] global_step=19800, grad_norm=4.224107265472412, loss=2.845085859298706
I0307 04:41:31.849529 140178276385984 spec.py:321] Evaluating on the training split.
I0307 04:41:47.353732 140178276385984 spec.py:333] Evaluating on the validation split.
I0307 04:42:09.463454 140178276385984 spec.py:349] Evaluating on the test split.
I0307 04:42:11.279891 140178276385984 submission_runner.py:469] Time since start: 8372.32s, 	Step: 19845, 	{'train/accuracy': 0.6201769709587097, 'train/loss': 1.6656979322433472, 'validation/accuracy': 0.5651000142097473, 'validation/loss': 1.926265001296997, 'validation/num_examples': 50000, 'test/accuracy': 0.43790000677108765, 'test/loss': 2.6407697200775146, 'test/num_examples': 10000, 'score': 7708.849365949631, 'total_duration': 8372.323499679565, 'accumulated_submission_time': 7708.849365949631, 'accumulated_eval_time': 660.2645676136017, 'accumulated_logging_time': 0.9778788089752197}
I0307 04:42:11.319363 140021218662144 logging_writer.py:48] [19845] accumulated_eval_time=660.265, accumulated_logging_time=0.977879, accumulated_submission_time=7708.85, global_step=19845, preemption_count=0, score=7708.85, test/accuracy=0.4379, test/loss=2.64077, test/num_examples=10000, total_duration=8372.32, train/accuracy=0.620177, train/loss=1.6657, validation/accuracy=0.5651, validation/loss=1.92627, validation/num_examples=50000
I0307 04:42:33.130340 140021210269440 logging_writer.py:48] [19900] global_step=19900, grad_norm=4.445054531097412, loss=2.8959908485412598
I0307 04:43:11.533271 140021218662144 logging_writer.py:48] [20000] global_step=20000, grad_norm=3.9470717906951904, loss=3.013192653656006
I0307 04:43:50.529908 140021210269440 logging_writer.py:48] [20100] global_step=20100, grad_norm=3.1808886528015137, loss=3.02228045463562
I0307 04:44:28.969083 140021218662144 logging_writer.py:48] [20200] global_step=20200, grad_norm=4.337940692901611, loss=3.0907845497131348
I0307 04:45:07.293365 140021210269440 logging_writer.py:48] [20300] global_step=20300, grad_norm=3.6772093772888184, loss=3.013852119445801
I0307 04:45:46.232808 140021218662144 logging_writer.py:48] [20400] global_step=20400, grad_norm=3.5469207763671875, loss=2.8727662563323975
I0307 04:46:25.044905 140021210269440 logging_writer.py:48] [20500] global_step=20500, grad_norm=5.4221343994140625, loss=2.9203906059265137
I0307 04:47:04.078374 140021218662144 logging_writer.py:48] [20600] global_step=20600, grad_norm=3.8413875102996826, loss=2.979818105697632
I0307 04:47:42.736656 140021210269440 logging_writer.py:48] [20700] global_step=20700, grad_norm=3.854741334915161, loss=2.950937271118164
I0307 04:48:21.120244 140021218662144 logging_writer.py:48] [20800] global_step=20800, grad_norm=5.174901962280273, loss=2.832163095474243
I0307 04:48:59.773317 140021210269440 logging_writer.py:48] [20900] global_step=20900, grad_norm=3.1799864768981934, loss=3.0670228004455566
I0307 04:49:38.270374 140021218662144 logging_writer.py:48] [21000] global_step=21000, grad_norm=3.3350205421447754, loss=3.0082478523254395
I0307 04:50:17.096712 140021210269440 logging_writer.py:48] [21100] global_step=21100, grad_norm=4.321451663970947, loss=2.9179747104644775
I0307 04:50:41.499869 140178276385984 spec.py:321] Evaluating on the training split.
I0307 04:50:55.477157 140178276385984 spec.py:333] Evaluating on the validation split.
I0307 04:51:17.100330 140178276385984 spec.py:349] Evaluating on the test split.
I0307 04:51:18.923470 140178276385984 submission_runner.py:469] Time since start: 8919.97s, 	Step: 21164, 	{'train/accuracy': 0.6351442933082581, 'train/loss': 1.6175873279571533, 'validation/accuracy': 0.5804600119590759, 'validation/loss': 1.8682241439819336, 'validation/num_examples': 50000, 'test/accuracy': 0.4603000283241272, 'test/loss': 2.5139081478118896, 'test/num_examples': 10000, 'score': 8218.854027748108, 'total_duration': 8919.967081785202, 'accumulated_submission_time': 8218.854027748108, 'accumulated_eval_time': 697.6881215572357, 'accumulated_logging_time': 1.0596222877502441}
I0307 04:51:18.952246 140021218662144 logging_writer.py:48] [21164] accumulated_eval_time=697.688, accumulated_logging_time=1.05962, accumulated_submission_time=8218.85, global_step=21164, preemption_count=0, score=8218.85, test/accuracy=0.4603, test/loss=2.51391, test/num_examples=10000, total_duration=8919.97, train/accuracy=0.635144, train/loss=1.61759, validation/accuracy=0.58046, validation/loss=1.86822, validation/num_examples=50000
I0307 04:51:33.350204 140021210269440 logging_writer.py:48] [21200] global_step=21200, grad_norm=3.7415127754211426, loss=2.889479875564575
I0307 04:52:12.196893 140021218662144 logging_writer.py:48] [21300] global_step=21300, grad_norm=3.6430776119232178, loss=2.9063501358032227
I0307 04:52:50.925285 140021210269440 logging_writer.py:48] [21400] global_step=21400, grad_norm=2.9354665279388428, loss=2.922255277633667
I0307 04:53:29.812935 140021218662144 logging_writer.py:48] [21500] global_step=21500, grad_norm=2.759343147277832, loss=2.9352149963378906
I0307 04:54:08.517295 140021210269440 logging_writer.py:48] [21600] global_step=21600, grad_norm=2.7947890758514404, loss=2.968705892562866
I0307 04:54:46.414048 140021218662144 logging_writer.py:48] [21700] global_step=21700, grad_norm=3.1854937076568604, loss=2.953838348388672
I0307 04:55:24.883723 140021210269440 logging_writer.py:48] [21800] global_step=21800, grad_norm=3.674410820007324, loss=2.9464187622070312
I0307 04:56:03.463751 140021218662144 logging_writer.py:48] [21900] global_step=21900, grad_norm=3.1959686279296875, loss=3.0432729721069336
I0307 04:56:41.800029 140021210269440 logging_writer.py:48] [22000] global_step=22000, grad_norm=4.998528480529785, loss=2.9187285900115967
I0307 04:57:20.499665 140021218662144 logging_writer.py:48] [22100] global_step=22100, grad_norm=4.412266254425049, loss=2.9589216709136963
I0307 04:57:59.101353 140021210269440 logging_writer.py:48] [22200] global_step=22200, grad_norm=3.6669232845306396, loss=3.0380473136901855
I0307 04:58:37.702139 140021218662144 logging_writer.py:48] [22300] global_step=22300, grad_norm=4.436744213104248, loss=2.8901963233947754
I0307 04:59:16.305447 140021210269440 logging_writer.py:48] [22400] global_step=22400, grad_norm=3.887770414352417, loss=2.9253950119018555
I0307 04:59:49.310158 140178276385984 spec.py:321] Evaluating on the training split.
I0307 05:00:03.624245 140178276385984 spec.py:333] Evaluating on the validation split.
I0307 05:00:28.305207 140178276385984 spec.py:349] Evaluating on the test split.
I0307 05:00:30.125776 140178276385984 submission_runner.py:469] Time since start: 9471.17s, 	Step: 22486, 	{'train/accuracy': 0.6356425285339355, 'train/loss': 1.5772041082382202, 'validation/accuracy': 0.5849599838256836, 'validation/loss': 1.812549352645874, 'validation/num_examples': 50000, 'test/accuracy': 0.4580000340938568, 'test/loss': 2.490973711013794, 'test/num_examples': 10000, 'score': 8729.067692518234, 'total_duration': 9471.169401407242, 'accumulated_submission_time': 8729.067692518234, 'accumulated_eval_time': 738.5037069320679, 'accumulated_logging_time': 1.0967564582824707}
I0307 05:00:30.147434 140021218662144 logging_writer.py:48] [22486] accumulated_eval_time=738.504, accumulated_logging_time=1.09676, accumulated_submission_time=8729.07, global_step=22486, preemption_count=0, score=8729.07, test/accuracy=0.458, test/loss=2.49097, test/num_examples=10000, total_duration=9471.17, train/accuracy=0.635643, train/loss=1.5772, validation/accuracy=0.58496, validation/loss=1.81255, validation/num_examples=50000
I0307 05:00:36.067451 140021210269440 logging_writer.py:48] [22500] global_step=22500, grad_norm=4.659914970397949, loss=3.0469062328338623
I0307 05:01:14.899647 140021218662144 logging_writer.py:48] [22600] global_step=22600, grad_norm=2.7121505737304688, loss=2.8821001052856445
I0307 05:01:53.477278 140021210269440 logging_writer.py:48] [22700] global_step=22700, grad_norm=3.685938596725464, loss=2.980693817138672
I0307 05:02:32.216717 140021218662144 logging_writer.py:48] [22800] global_step=22800, grad_norm=3.8122873306274414, loss=2.990269184112549
I0307 05:03:11.046787 140021210269440 logging_writer.py:48] [22900] global_step=22900, grad_norm=3.4415571689605713, loss=2.861819267272949
I0307 05:03:49.571647 140021218662144 logging_writer.py:48] [23000] global_step=23000, grad_norm=3.20678448677063, loss=2.9839706420898438
I0307 05:04:28.521922 140021210269440 logging_writer.py:48] [23100] global_step=23100, grad_norm=3.252471685409546, loss=2.8914380073547363
I0307 05:05:07.020970 140021218662144 logging_writer.py:48] [23200] global_step=23200, grad_norm=2.9702296257019043, loss=2.9096286296844482
I0307 05:05:45.952630 140021210269440 logging_writer.py:48] [23300] global_step=23300, grad_norm=2.8541810512542725, loss=2.9292328357696533
I0307 05:06:24.772224 140021218662144 logging_writer.py:48] [23400] global_step=23400, grad_norm=3.4033727645874023, loss=2.9592185020446777
I0307 05:07:03.249534 140021210269440 logging_writer.py:48] [23500] global_step=23500, grad_norm=2.8822367191314697, loss=2.9049601554870605
I0307 05:07:41.937021 140021218662144 logging_writer.py:48] [23600] global_step=23600, grad_norm=2.8113059997558594, loss=2.9426937103271484
I0307 05:08:20.594871 140021210269440 logging_writer.py:48] [23700] global_step=23700, grad_norm=2.496689558029175, loss=2.8251864910125732
I0307 05:08:59.476685 140021218662144 logging_writer.py:48] [23800] global_step=23800, grad_norm=4.229024410247803, loss=2.9757001399993896
I0307 05:09:00.231662 140178276385984 spec.py:321] Evaluating on the training split.
I0307 05:09:16.459613 140178276385984 spec.py:333] Evaluating on the validation split.
I0307 05:09:36.560245 140178276385984 spec.py:349] Evaluating on the test split.
I0307 05:09:38.345715 140178276385984 submission_runner.py:469] Time since start: 10019.39s, 	Step: 23803, 	{'train/accuracy': 0.6248405575752258, 'train/loss': 1.6467496156692505, 'validation/accuracy': 0.5753799676895142, 'validation/loss': 1.8854948282241821, 'validation/num_examples': 50000, 'test/accuracy': 0.45680001378059387, 'test/loss': 2.5530502796173096, 'test/num_examples': 10000, 'score': 9238.982190132141, 'total_duration': 10019.38933634758, 'accumulated_submission_time': 9238.982190132141, 'accumulated_eval_time': 776.6177186965942, 'accumulated_logging_time': 1.1485655307769775}
I0307 05:09:38.386388 140021210269440 logging_writer.py:48] [23803] accumulated_eval_time=776.618, accumulated_logging_time=1.14857, accumulated_submission_time=9238.98, global_step=23803, preemption_count=0, score=9238.98, test/accuracy=0.4568, test/loss=2.55305, test/num_examples=10000, total_duration=10019.4, train/accuracy=0.624841, train/loss=1.64675, validation/accuracy=0.57538, validation/loss=1.88549, validation/num_examples=50000
I0307 05:10:16.687261 140021218662144 logging_writer.py:48] [23900] global_step=23900, grad_norm=3.523252487182617, loss=2.835383176803589
I0307 05:10:55.284648 140021210269440 logging_writer.py:48] [24000] global_step=24000, grad_norm=3.0726120471954346, loss=2.9167134761810303
I0307 05:11:33.773321 140021218662144 logging_writer.py:48] [24100] global_step=24100, grad_norm=3.2645323276519775, loss=2.943772077560425
I0307 05:12:12.454082 140021210269440 logging_writer.py:48] [24200] global_step=24200, grad_norm=2.957216262817383, loss=2.861990451812744
I0307 05:12:51.258336 140021218662144 logging_writer.py:48] [24300] global_step=24300, grad_norm=3.383913040161133, loss=2.879011869430542
I0307 05:13:29.879393 140021210269440 logging_writer.py:48] [24400] global_step=24400, grad_norm=3.0119946002960205, loss=2.891735553741455
I0307 05:14:08.533736 140021218662144 logging_writer.py:48] [24500] global_step=24500, grad_norm=3.1606833934783936, loss=2.999058485031128
I0307 05:14:47.269583 140021210269440 logging_writer.py:48] [24600] global_step=24600, grad_norm=3.5287063121795654, loss=3.01002836227417
I0307 05:15:25.759615 140021218662144 logging_writer.py:48] [24700] global_step=24700, grad_norm=3.4962570667266846, loss=2.8416996002197266
I0307 05:16:04.712409 140021210269440 logging_writer.py:48] [24800] global_step=24800, grad_norm=2.911694049835205, loss=2.8029510974884033
I0307 05:16:43.267728 140021218662144 logging_writer.py:48] [24900] global_step=24900, grad_norm=3.392327308654785, loss=2.8857898712158203
I0307 05:17:21.953643 140021210269440 logging_writer.py:48] [25000] global_step=25000, grad_norm=3.2346200942993164, loss=2.962629795074463
I0307 05:18:00.779285 140021218662144 logging_writer.py:48] [25100] global_step=25100, grad_norm=3.025845766067505, loss=2.9421353340148926
I0307 05:18:08.450477 140178276385984 spec.py:321] Evaluating on the training split.
I0307 05:18:23.569906 140178276385984 spec.py:333] Evaluating on the validation split.
I0307 05:18:43.786313 140178276385984 spec.py:349] Evaluating on the test split.
I0307 05:18:45.572911 140178276385984 submission_runner.py:469] Time since start: 10566.62s, 	Step: 25121, 	{'train/accuracy': 0.626953125, 'train/loss': 1.6276590824127197, 'validation/accuracy': 0.5781199932098389, 'validation/loss': 1.8583282232284546, 'validation/num_examples': 50000, 'test/accuracy': 0.45020002126693726, 'test/loss': 2.554035186767578, 'test/num_examples': 10000, 'score': 9748.885342121124, 'total_duration': 10566.616530179977, 'accumulated_submission_time': 9748.885342121124, 'accumulated_eval_time': 813.7401127815247, 'accumulated_logging_time': 1.2130684852600098}
I0307 05:18:45.666441 140021210269440 logging_writer.py:48] [25121] accumulated_eval_time=813.74, accumulated_logging_time=1.21307, accumulated_submission_time=9748.89, global_step=25121, preemption_count=0, score=9748.89, test/accuracy=0.4502, test/loss=2.55404, test/num_examples=10000, total_duration=10566.6, train/accuracy=0.626953, train/loss=1.62766, validation/accuracy=0.57812, validation/loss=1.85833, validation/num_examples=50000
I0307 05:19:17.156112 140021218662144 logging_writer.py:48] [25200] global_step=25200, grad_norm=3.2488019466400146, loss=2.9796853065490723
I0307 05:19:55.804489 140021210269440 logging_writer.py:48] [25300] global_step=25300, grad_norm=2.8154027462005615, loss=2.7782416343688965
I0307 05:20:34.320834 140021218662144 logging_writer.py:48] [25400] global_step=25400, grad_norm=2.8421244621276855, loss=2.886218309402466
I0307 05:21:13.531476 140021210269440 logging_writer.py:48] [25500] global_step=25500, grad_norm=2.6474008560180664, loss=2.9609289169311523
I0307 05:21:52.558453 140021218662144 logging_writer.py:48] [25600] global_step=25600, grad_norm=2.854011297225952, loss=2.8735804557800293
I0307 05:22:31.198383 140021210269440 logging_writer.py:48] [25700] global_step=25700, grad_norm=3.1691572666168213, loss=2.9252312183380127
I0307 05:23:09.837063 140021218662144 logging_writer.py:48] [25800] global_step=25800, grad_norm=3.151818037033081, loss=2.842451333999634
I0307 05:23:48.432126 140021210269440 logging_writer.py:48] [25900] global_step=25900, grad_norm=3.139493465423584, loss=2.9334752559661865
I0307 05:24:26.905623 140021218662144 logging_writer.py:48] [26000] global_step=26000, grad_norm=2.705742359161377, loss=2.7485709190368652
I0307 05:25:05.814776 140021210269440 logging_writer.py:48] [26100] global_step=26100, grad_norm=2.9141581058502197, loss=2.8312113285064697
I0307 05:25:44.582257 140021218662144 logging_writer.py:48] [26200] global_step=26200, grad_norm=3.0097110271453857, loss=2.8397088050842285
I0307 05:26:23.649128 140021210269440 logging_writer.py:48] [26300] global_step=26300, grad_norm=3.264864921569824, loss=2.8895063400268555
I0307 05:27:02.179381 140021218662144 logging_writer.py:48] [26400] global_step=26400, grad_norm=3.0038976669311523, loss=2.799008369445801
I0307 05:27:15.928287 140178276385984 spec.py:321] Evaluating on the training split.
I0307 05:27:33.900270 140178276385984 spec.py:333] Evaluating on the validation split.
I0307 05:27:54.472659 140178276385984 spec.py:349] Evaluating on the test split.
I0307 05:27:56.285219 140178276385984 submission_runner.py:469] Time since start: 11117.33s, 	Step: 26436, 	{'train/accuracy': 0.6262555718421936, 'train/loss': 1.6446563005447388, 'validation/accuracy': 0.5775600075721741, 'validation/loss': 1.8790220022201538, 'validation/num_examples': 50000, 'test/accuracy': 0.45600003004074097, 'test/loss': 2.5358023643493652, 'test/num_examples': 10000, 'score': 10259.000326633453, 'total_duration': 11117.328831672668, 'accumulated_submission_time': 10259.000326633453, 'accumulated_eval_time': 854.0970220565796, 'accumulated_logging_time': 1.3244616985321045}
I0307 05:27:56.404771 140021210269440 logging_writer.py:48] [26436] accumulated_eval_time=854.097, accumulated_logging_time=1.32446, accumulated_submission_time=10259, global_step=26436, preemption_count=0, score=10259, test/accuracy=0.456, test/loss=2.5358, test/num_examples=10000, total_duration=11117.3, train/accuracy=0.626256, train/loss=1.64466, validation/accuracy=0.57756, validation/loss=1.87902, validation/num_examples=50000
I0307 05:28:21.832059 140021218662144 logging_writer.py:48] [26500] global_step=26500, grad_norm=2.9068756103515625, loss=2.8904128074645996
I0307 05:29:00.703066 140021210269440 logging_writer.py:48] [26600] global_step=26600, grad_norm=3.0347201824188232, loss=2.872915744781494
I0307 05:29:38.643908 140021218662144 logging_writer.py:48] [26700] global_step=26700, grad_norm=2.9806020259857178, loss=2.924356698989868
I0307 05:30:16.656492 140021210269440 logging_writer.py:48] [26800] global_step=26800, grad_norm=3.3512134552001953, loss=2.8154356479644775
I0307 05:30:55.596581 140021218662144 logging_writer.py:48] [26900] global_step=26900, grad_norm=3.4537274837493896, loss=2.8459675312042236
I0307 05:31:34.374486 140021210269440 logging_writer.py:48] [27000] global_step=27000, grad_norm=3.1118927001953125, loss=2.899658441543579
I0307 05:32:13.294554 140021218662144 logging_writer.py:48] [27100] global_step=27100, grad_norm=2.4922800064086914, loss=2.8440208435058594
I0307 05:32:52.114656 140021210269440 logging_writer.py:48] [27200] global_step=27200, grad_norm=3.0351243019104004, loss=2.8528804779052734
I0307 05:33:30.602661 140021218662144 logging_writer.py:48] [27300] global_step=27300, grad_norm=2.909926652908325, loss=2.7057371139526367
I0307 05:34:09.133925 140021210269440 logging_writer.py:48] [27400] global_step=27400, grad_norm=3.3172824382781982, loss=2.7510664463043213
I0307 05:34:47.471098 140021218662144 logging_writer.py:48] [27500] global_step=27500, grad_norm=2.882230520248413, loss=2.8620121479034424
I0307 05:35:26.734854 140021210269440 logging_writer.py:48] [27600] global_step=27600, grad_norm=2.588843822479248, loss=2.787853479385376
I0307 05:36:05.587620 140021218662144 logging_writer.py:48] [27700] global_step=27700, grad_norm=3.1385769844055176, loss=2.882089376449585
I0307 05:36:26.585087 140178276385984 spec.py:321] Evaluating on the training split.
I0307 05:36:43.792894 140178276385984 spec.py:333] Evaluating on the validation split.
I0307 05:37:05.713000 140178276385984 spec.py:349] Evaluating on the test split.
I0307 05:37:07.517947 140178276385984 submission_runner.py:469] Time since start: 11668.56s, 	Step: 27754, 	{'train/accuracy': 0.6438336968421936, 'train/loss': 1.5523779392242432, 'validation/accuracy': 0.5927199721336365, 'validation/loss': 1.7915163040161133, 'validation/num_examples': 50000, 'test/accuracy': 0.46890002489089966, 'test/loss': 2.48091197013855, 'test/num_examples': 10000, 'score': 10768.678529977798, 'total_duration': 11668.561493635178, 'accumulated_submission_time': 10768.678529977798, 'accumulated_eval_time': 895.0297713279724, 'accumulated_logging_time': 1.811082363128662}
I0307 05:37:07.576249 140021210269440 logging_writer.py:48] [27754] accumulated_eval_time=895.03, accumulated_logging_time=1.81108, accumulated_submission_time=10768.7, global_step=27754, preemption_count=0, score=10768.7, test/accuracy=0.4689, test/loss=2.48091, test/num_examples=10000, total_duration=11668.6, train/accuracy=0.643834, train/loss=1.55238, validation/accuracy=0.59272, validation/loss=1.79152, validation/num_examples=50000
I0307 05:37:25.726384 140021218662144 logging_writer.py:48] [27800] global_step=27800, grad_norm=3.060234308242798, loss=2.8974761962890625
I0307 05:38:04.700958 140021210269440 logging_writer.py:48] [27900] global_step=27900, grad_norm=2.751715898513794, loss=2.854611873626709
I0307 05:38:43.169127 140021218662144 logging_writer.py:48] [28000] global_step=28000, grad_norm=2.8483049869537354, loss=2.9803049564361572
I0307 05:39:21.817857 140021210269440 logging_writer.py:48] [28100] global_step=28100, grad_norm=2.407350540161133, loss=2.7827086448669434
I0307 05:40:00.401659 140021218662144 logging_writer.py:48] [28200] global_step=28200, grad_norm=3.4133636951446533, loss=2.8332815170288086
I0307 05:40:39.302020 140021210269440 logging_writer.py:48] [28300] global_step=28300, grad_norm=3.078937530517578, loss=2.9250102043151855
I0307 05:41:17.948050 140021218662144 logging_writer.py:48] [28400] global_step=28400, grad_norm=3.0607333183288574, loss=2.838916778564453
I0307 05:41:56.509888 140021210269440 logging_writer.py:48] [28500] global_step=28500, grad_norm=2.799940347671509, loss=2.855079174041748
I0307 05:42:35.167177 140021218662144 logging_writer.py:48] [28600] global_step=28600, grad_norm=3.322499990463257, loss=2.956029176712036
I0307 05:43:13.805341 140021210269440 logging_writer.py:48] [28700] global_step=28700, grad_norm=3.145285129547119, loss=2.8039820194244385
I0307 05:43:52.800701 140021218662144 logging_writer.py:48] [28800] global_step=28800, grad_norm=2.8908944129943848, loss=2.8416202068328857
I0307 05:44:31.325698 140021210269440 logging_writer.py:48] [28900] global_step=28900, grad_norm=3.1842141151428223, loss=2.844066858291626
I0307 05:45:10.128877 140021218662144 logging_writer.py:48] [29000] global_step=29000, grad_norm=4.397607803344727, loss=2.935682535171509
I0307 05:45:37.601352 140178276385984 spec.py:321] Evaluating on the training split.
I0307 05:45:52.004135 140178276385984 spec.py:333] Evaluating on the validation split.
I0307 05:46:14.081563 140178276385984 spec.py:349] Evaluating on the test split.
I0307 05:46:15.892875 140178276385984 submission_runner.py:469] Time since start: 12216.94s, 	Step: 29072, 	{'train/accuracy': 0.643953263759613, 'train/loss': 1.5473839044570923, 'validation/accuracy': 0.5926399827003479, 'validation/loss': 1.8019572496414185, 'validation/num_examples': 50000, 'test/accuracy': 0.47220003604888916, 'test/loss': 2.455782890319824, 'test/num_examples': 10000, 'score': 11278.552919864655, 'total_duration': 12216.936494588852, 'accumulated_submission_time': 11278.552919864655, 'accumulated_eval_time': 933.3212687969208, 'accumulated_logging_time': 1.8892240524291992}
I0307 05:46:15.994336 140021210269440 logging_writer.py:48] [29072] accumulated_eval_time=933.321, accumulated_logging_time=1.88922, accumulated_submission_time=11278.6, global_step=29072, preemption_count=0, score=11278.6, test/accuracy=0.4722, test/loss=2.45578, test/num_examples=10000, total_duration=12216.9, train/accuracy=0.643953, train/loss=1.54738, validation/accuracy=0.59264, validation/loss=1.80196, validation/num_examples=50000
I0307 05:46:27.319232 140021218662144 logging_writer.py:48] [29100] global_step=29100, grad_norm=3.018995523452759, loss=2.8156092166900635
I0307 05:47:05.950030 140021210269440 logging_writer.py:48] [29200] global_step=29200, grad_norm=3.2821640968322754, loss=2.864870071411133
I0307 05:47:44.519830 140021218662144 logging_writer.py:48] [29300] global_step=29300, grad_norm=2.8861756324768066, loss=2.921147108078003
I0307 05:48:23.460462 140021210269440 logging_writer.py:48] [29400] global_step=29400, grad_norm=3.0027589797973633, loss=2.8138458728790283
I0307 05:49:01.975781 140021218662144 logging_writer.py:48] [29500] global_step=29500, grad_norm=2.700312852859497, loss=2.807239294052124
I0307 05:49:40.626767 140021210269440 logging_writer.py:48] [29600] global_step=29600, grad_norm=3.622330665588379, loss=2.8841631412506104
I0307 05:50:19.809298 140021218662144 logging_writer.py:48] [29700] global_step=29700, grad_norm=3.023275375366211, loss=2.8565425872802734
I0307 05:50:58.877396 140021210269440 logging_writer.py:48] [29800] global_step=29800, grad_norm=3.6546578407287598, loss=2.8655200004577637
I0307 05:51:37.654569 140021218662144 logging_writer.py:48] [29900] global_step=29900, grad_norm=2.7906835079193115, loss=2.758716583251953
I0307 05:52:16.273432 140021210269440 logging_writer.py:48] [30000] global_step=30000, grad_norm=2.9895081520080566, loss=2.8808608055114746
I0307 05:52:55.575712 140021218662144 logging_writer.py:48] [30100] global_step=30100, grad_norm=2.9712791442871094, loss=2.8373780250549316
I0307 05:53:34.515913 140021210269440 logging_writer.py:48] [30200] global_step=30200, grad_norm=3.0674896240234375, loss=2.88443922996521
I0307 05:54:12.968075 140021218662144 logging_writer.py:48] [30300] global_step=30300, grad_norm=2.7367053031921387, loss=2.8079469203948975
I0307 05:54:46.275230 140178276385984 spec.py:321] Evaluating on the training split.
I0307 05:55:02.785651 140178276385984 spec.py:333] Evaluating on the validation split.
I0307 05:55:28.982441 140178276385984 spec.py:349] Evaluating on the test split.
I0307 05:55:30.729566 140178276385984 submission_runner.py:469] Time since start: 12771.77s, 	Step: 30387, 	{'train/accuracy': 0.6468630433082581, 'train/loss': 1.554618000984192, 'validation/accuracy': 0.5940999984741211, 'validation/loss': 1.7913073301315308, 'validation/num_examples': 50000, 'test/accuracy': 0.46870002150535583, 'test/loss': 2.4587674140930176, 'test/num_examples': 10000, 'score': 11788.623572349548, 'total_duration': 12771.77317738533, 'accumulated_submission_time': 11788.623572349548, 'accumulated_eval_time': 977.775559425354, 'accumulated_logging_time': 2.0716392993927}
I0307 05:55:30.769418 140021210269440 logging_writer.py:48] [30387] accumulated_eval_time=977.776, accumulated_logging_time=2.07164, accumulated_submission_time=11788.6, global_step=30387, preemption_count=0, score=11788.6, test/accuracy=0.4687, test/loss=2.45877, test/num_examples=10000, total_duration=12771.8, train/accuracy=0.646863, train/loss=1.55462, validation/accuracy=0.5941, validation/loss=1.79131, validation/num_examples=50000
I0307 05:55:36.360072 140021218662144 logging_writer.py:48] [30400] global_step=30400, grad_norm=2.821183681488037, loss=2.8543081283569336
I0307 05:56:15.228311 140021210269440 logging_writer.py:48] [30500] global_step=30500, grad_norm=3.389504909515381, loss=2.9026455879211426
I0307 05:56:53.671610 140021218662144 logging_writer.py:48] [30600] global_step=30600, grad_norm=2.4855432510375977, loss=2.8171911239624023
I0307 05:57:32.504925 140021210269440 logging_writer.py:48] [30700] global_step=30700, grad_norm=3.3795228004455566, loss=2.885997772216797
I0307 05:58:11.744186 140021218662144 logging_writer.py:48] [30800] global_step=30800, grad_norm=2.959286689758301, loss=2.9186160564422607
I0307 05:58:50.672960 140021210269440 logging_writer.py:48] [30900] global_step=30900, grad_norm=2.912036657333374, loss=2.789825677871704
I0307 05:59:29.451714 140021218662144 logging_writer.py:48] [31000] global_step=31000, grad_norm=3.708073139190674, loss=2.8412444591522217
I0307 06:00:08.555565 140021210269440 logging_writer.py:48] [31100] global_step=31100, grad_norm=3.0094151496887207, loss=2.8752410411834717
I0307 06:00:47.283276 140021218662144 logging_writer.py:48] [31200] global_step=31200, grad_norm=2.787419080734253, loss=2.8562281131744385
I0307 06:01:26.417330 140021210269440 logging_writer.py:48] [31300] global_step=31300, grad_norm=3.0051958560943604, loss=2.8654847145080566
I0307 06:02:04.910758 140021218662144 logging_writer.py:48] [31400] global_step=31400, grad_norm=3.0390212535858154, loss=2.85014009475708
I0307 06:02:43.699782 140021210269440 logging_writer.py:48] [31500] global_step=31500, grad_norm=2.895338296890259, loss=2.7636334896087646
I0307 06:03:21.892947 140021218662144 logging_writer.py:48] [31600] global_step=31600, grad_norm=3.106031894683838, loss=2.926759958267212
I0307 06:04:00.116317 140021210269440 logging_writer.py:48] [31700] global_step=31700, grad_norm=2.8605828285217285, loss=2.8253462314605713
I0307 06:04:00.922688 140178276385984 spec.py:321] Evaluating on the training split.
I0307 06:04:15.832267 140178276385984 spec.py:333] Evaluating on the validation split.
I0307 06:04:35.619158 140178276385984 spec.py:349] Evaluating on the test split.
I0307 06:04:37.442351 140178276385984 submission_runner.py:469] Time since start: 13318.49s, 	Step: 31703, 	{'train/accuracy': 0.6502909660339355, 'train/loss': 1.5120701789855957, 'validation/accuracy': 0.5998199582099915, 'validation/loss': 1.7403184175491333, 'validation/num_examples': 50000, 'test/accuracy': 0.47450003027915955, 'test/loss': 2.4055850505828857, 'test/num_examples': 10000, 'score': 12298.59297657013, 'total_duration': 13318.485968828201, 'accumulated_submission_time': 12298.59297657013, 'accumulated_eval_time': 1014.29518699646, 'accumulated_logging_time': 2.158447265625}
I0307 06:04:37.484034 140021218662144 logging_writer.py:48] [31703] accumulated_eval_time=1014.3, accumulated_logging_time=2.15845, accumulated_submission_time=12298.6, global_step=31703, preemption_count=0, score=12298.6, test/accuracy=0.4745, test/loss=2.40559, test/num_examples=10000, total_duration=13318.5, train/accuracy=0.650291, train/loss=1.51207, validation/accuracy=0.59982, validation/loss=1.74032, validation/num_examples=50000
I0307 06:05:15.484211 140021210269440 logging_writer.py:48] [31800] global_step=31800, grad_norm=3.027759313583374, loss=2.8471312522888184
I0307 06:05:54.352187 140021218662144 logging_writer.py:48] [31900] global_step=31900, grad_norm=2.7639827728271484, loss=2.7304413318634033
I0307 06:06:33.214556 140021210269440 logging_writer.py:48] [32000] global_step=32000, grad_norm=2.968658447265625, loss=2.896287679672241
I0307 06:07:12.243936 140021218662144 logging_writer.py:48] [32100] global_step=32100, grad_norm=3.353647232055664, loss=2.8851425647735596
I0307 06:07:50.728224 140021210269440 logging_writer.py:48] [32200] global_step=32200, grad_norm=2.984860897064209, loss=2.808173418045044
I0307 06:08:29.239416 140021218662144 logging_writer.py:48] [32300] global_step=32300, grad_norm=3.5999672412872314, loss=2.8782079219818115
I0307 06:09:08.046801 140021210269440 logging_writer.py:48] [32400] global_step=32400, grad_norm=4.3652238845825195, loss=2.8098630905151367
I0307 06:09:46.854476 140021218662144 logging_writer.py:48] [32500] global_step=32500, grad_norm=3.0254428386688232, loss=2.7658989429473877
I0307 06:10:26.066975 140021210269440 logging_writer.py:48] [32600] global_step=32600, grad_norm=3.060594320297241, loss=2.8641486167907715
I0307 06:11:04.840505 140021218662144 logging_writer.py:48] [32700] global_step=32700, grad_norm=3.627108335494995, loss=2.851407527923584
I0307 06:11:43.660382 140021210269440 logging_writer.py:48] [32800] global_step=32800, grad_norm=2.830367088317871, loss=2.8502085208892822
I0307 06:12:22.269398 140021218662144 logging_writer.py:48] [32900] global_step=32900, grad_norm=3.2041923999786377, loss=2.9697375297546387
I0307 06:13:01.211462 140021210269440 logging_writer.py:48] [33000] global_step=33000, grad_norm=3.701571464538574, loss=2.840613842010498
I0307 06:13:07.775402 140178276385984 spec.py:321] Evaluating on the training split.
I0307 06:13:19.692534 140178276385984 spec.py:333] Evaluating on the validation split.
I0307 06:13:46.048370 140178276385984 spec.py:349] Evaluating on the test split.
I0307 06:13:47.877289 140178276385984 submission_runner.py:469] Time since start: 13868.92s, 	Step: 33018, 	{'train/accuracy': 0.6458067297935486, 'train/loss': 1.522133469581604, 'validation/accuracy': 0.6018399596214294, 'validation/loss': 1.7434443235397339, 'validation/num_examples': 50000, 'test/accuracy': 0.4806000292301178, 'test/loss': 2.4198873043060303, 'test/num_examples': 10000, 'score': 12808.712173938751, 'total_duration': 13868.920781612396, 'accumulated_submission_time': 12808.712173938751, 'accumulated_eval_time': 1054.3969042301178, 'accumulated_logging_time': 2.239515542984009}
I0307 06:13:47.933065 140021218662144 logging_writer.py:48] [33018] accumulated_eval_time=1054.4, accumulated_logging_time=2.23952, accumulated_submission_time=12808.7, global_step=33018, preemption_count=0, score=12808.7, test/accuracy=0.4806, test/loss=2.41989, test/num_examples=10000, total_duration=13868.9, train/accuracy=0.645807, train/loss=1.52213, validation/accuracy=0.60184, validation/loss=1.74344, validation/num_examples=50000
I0307 06:14:20.283211 140021210269440 logging_writer.py:48] [33100] global_step=33100, grad_norm=2.926267147064209, loss=2.871103286743164
I0307 06:14:59.154368 140021218662144 logging_writer.py:48] [33200] global_step=33200, grad_norm=2.771693706512451, loss=2.811098098754883
I0307 06:15:38.018807 140021210269440 logging_writer.py:48] [33300] global_step=33300, grad_norm=2.756424903869629, loss=2.8380346298217773
I0307 06:16:16.193011 140021218662144 logging_writer.py:48] [33400] global_step=33400, grad_norm=2.9200754165649414, loss=2.6824471950531006
I0307 06:16:55.030533 140021210269440 logging_writer.py:48] [33500] global_step=33500, grad_norm=2.796163558959961, loss=2.7681474685668945
I0307 06:17:33.782437 140021218662144 logging_writer.py:48] [33600] global_step=33600, grad_norm=3.3192601203918457, loss=2.9331562519073486
I0307 06:18:12.699369 140021210269440 logging_writer.py:48] [33700] global_step=33700, grad_norm=3.4282889366149902, loss=2.86063551902771
I0307 06:18:51.905204 140021218662144 logging_writer.py:48] [33800] global_step=33800, grad_norm=3.0966451168060303, loss=2.7796051502227783
I0307 06:19:30.934619 140021210269440 logging_writer.py:48] [33900] global_step=33900, grad_norm=3.201658248901367, loss=2.776299238204956
I0307 06:20:09.698705 140021218662144 logging_writer.py:48] [34000] global_step=34000, grad_norm=3.1603875160217285, loss=2.816819667816162
I0307 06:20:48.170915 140021210269440 logging_writer.py:48] [34100] global_step=34100, grad_norm=3.3327763080596924, loss=2.7623534202575684
I0307 06:21:26.862205 140021218662144 logging_writer.py:48] [34200] global_step=34200, grad_norm=3.6046230792999268, loss=2.7862298488616943
I0307 06:22:05.127896 140021210269440 logging_writer.py:48] [34300] global_step=34300, grad_norm=3.1937921047210693, loss=2.8100199699401855
I0307 06:22:18.097569 140178276385984 spec.py:321] Evaluating on the training split.
I0307 06:22:29.896758 140178276385984 spec.py:333] Evaluating on the validation split.
I0307 06:22:52.629461 140178276385984 spec.py:349] Evaluating on the test split.
I0307 06:22:54.594635 140178276385984 submission_runner.py:469] Time since start: 14415.64s, 	Step: 34335, 	{'train/accuracy': 0.6493741869926453, 'train/loss': 1.547829270362854, 'validation/accuracy': 0.5983799695968628, 'validation/loss': 1.7821993827819824, 'validation/num_examples': 50000, 'test/accuracy': 0.47460001707077026, 'test/loss': 2.423468589782715, 'test/num_examples': 10000, 'score': 13318.718039989471, 'total_duration': 14415.638063907623, 'accumulated_submission_time': 13318.718039989471, 'accumulated_eval_time': 1090.8937418460846, 'accumulated_logging_time': 2.3205432891845703}
I0307 06:22:54.726198 140021218662144 logging_writer.py:48] [34335] accumulated_eval_time=1090.89, accumulated_logging_time=2.32054, accumulated_submission_time=13318.7, global_step=34335, preemption_count=0, score=13318.7, test/accuracy=0.4746, test/loss=2.42347, test/num_examples=10000, total_duration=14415.6, train/accuracy=0.649374, train/loss=1.54783, validation/accuracy=0.59838, validation/loss=1.7822, validation/num_examples=50000
I0307 06:23:20.512485 140021210269440 logging_writer.py:48] [34400] global_step=34400, grad_norm=2.858992576599121, loss=2.820125102996826
I0307 06:23:59.127971 140021218662144 logging_writer.py:48] [34500] global_step=34500, grad_norm=2.9981603622436523, loss=2.8428735733032227
I0307 06:24:38.067709 140021210269440 logging_writer.py:48] [34600] global_step=34600, grad_norm=3.5206589698791504, loss=2.8227295875549316
I0307 06:25:16.727962 140021218662144 logging_writer.py:48] [34700] global_step=34700, grad_norm=3.2358975410461426, loss=2.832918643951416
I0307 06:25:55.703145 140021210269440 logging_writer.py:48] [34800] global_step=34800, grad_norm=3.3724567890167236, loss=2.79531192779541
I0307 06:26:34.370188 140021218662144 logging_writer.py:48] [34900] global_step=34900, grad_norm=3.264432430267334, loss=2.8075413703918457
I0307 06:27:13.021559 140021210269440 logging_writer.py:48] [35000] global_step=35000, grad_norm=2.8822181224823, loss=2.7606873512268066
I0307 06:27:52.172638 140021218662144 logging_writer.py:48] [35100] global_step=35100, grad_norm=3.358638048171997, loss=2.8363802433013916
I0307 06:28:30.937784 140021210269440 logging_writer.py:48] [35200] global_step=35200, grad_norm=2.7915232181549072, loss=2.740222930908203
I0307 06:29:09.672881 140021218662144 logging_writer.py:48] [35300] global_step=35300, grad_norm=3.548891305923462, loss=2.8172576427459717
I0307 06:29:47.818811 140021210269440 logging_writer.py:48] [35400] global_step=35400, grad_norm=2.983360528945923, loss=2.8138585090637207
I0307 06:30:26.347212 140021218662144 logging_writer.py:48] [35500] global_step=35500, grad_norm=3.785734176635742, loss=2.796391725540161
I0307 06:31:05.712416 140021210269440 logging_writer.py:48] [35600] global_step=35600, grad_norm=2.856476306915283, loss=2.887301445007324
I0307 06:31:24.866329 140178276385984 spec.py:321] Evaluating on the training split.
I0307 06:31:36.311718 140178276385984 spec.py:333] Evaluating on the validation split.
I0307 06:32:02.391089 140178276385984 spec.py:349] Evaluating on the test split.
I0307 06:32:04.218281 140178276385984 submission_runner.py:469] Time since start: 14965.26s, 	Step: 35651, 	{'train/accuracy': 0.6356824040412903, 'train/loss': 1.5992987155914307, 'validation/accuracy': 0.5906000137329102, 'validation/loss': 1.8113071918487549, 'validation/num_examples': 50000, 'test/accuracy': 0.46390002965927124, 'test/loss': 2.5130488872528076, 'test/num_examples': 10000, 'score': 13828.68073129654, 'total_duration': 14965.26175737381, 'accumulated_submission_time': 13828.68073129654, 'accumulated_eval_time': 1130.2455112934113, 'accumulated_logging_time': 2.496436595916748}
I0307 06:32:04.329384 140021218662144 logging_writer.py:48] [35651] accumulated_eval_time=1130.25, accumulated_logging_time=2.49644, accumulated_submission_time=13828.7, global_step=35651, preemption_count=0, score=13828.7, test/accuracy=0.4639, test/loss=2.51305, test/num_examples=10000, total_duration=14965.3, train/accuracy=0.635682, train/loss=1.5993, validation/accuracy=0.5906, validation/loss=1.81131, validation/num_examples=50000
I0307 06:32:23.684295 140021210269440 logging_writer.py:48] [35700] global_step=35700, grad_norm=3.8100967407226562, loss=2.888244390487671
I0307 06:33:02.114152 140021218662144 logging_writer.py:48] [35800] global_step=35800, grad_norm=2.615618944168091, loss=2.746856689453125
I0307 06:33:41.118161 140021210269440 logging_writer.py:48] [35900] global_step=35900, grad_norm=3.141956329345703, loss=2.8802857398986816
I0307 06:34:19.721243 140021218662144 logging_writer.py:48] [36000] global_step=36000, grad_norm=2.861494779586792, loss=2.7918615341186523
I0307 06:34:58.585192 140021210269440 logging_writer.py:48] [36100] global_step=36100, grad_norm=2.6875412464141846, loss=2.6290371417999268
I0307 06:35:37.455425 140021218662144 logging_writer.py:48] [36200] global_step=36200, grad_norm=2.67742919921875, loss=2.7489473819732666
I0307 06:36:17.400922 140021210269440 logging_writer.py:48] [36300] global_step=36300, grad_norm=2.745457172393799, loss=2.7886202335357666
I0307 06:36:56.382338 140021218662144 logging_writer.py:48] [36400] global_step=36400, grad_norm=2.872620105743408, loss=2.723479986190796
I0307 06:37:35.308501 140021210269440 logging_writer.py:48] [36500] global_step=36500, grad_norm=3.0413269996643066, loss=2.7807059288024902
I0307 06:38:13.985854 140021218662144 logging_writer.py:48] [36600] global_step=36600, grad_norm=2.99360728263855, loss=2.8571670055389404
I0307 06:38:52.403316 140021210269440 logging_writer.py:48] [36700] global_step=36700, grad_norm=2.757580518722534, loss=2.8510324954986572
I0307 06:39:31.496909 140021218662144 logging_writer.py:48] [36800] global_step=36800, grad_norm=3.0560877323150635, loss=2.868258237838745
I0307 06:40:10.104917 140021210269440 logging_writer.py:48] [36900] global_step=36900, grad_norm=2.7687463760375977, loss=2.7851638793945312
I0307 06:40:34.275684 140178276385984 spec.py:321] Evaluating on the training split.
I0307 06:40:46.488158 140178276385984 spec.py:333] Evaluating on the validation split.
I0307 06:41:06.164056 140178276385984 spec.py:349] Evaluating on the test split.
I0307 06:41:07.990534 140178276385984 submission_runner.py:469] Time since start: 15509.03s, 	Step: 36964, 	{'train/accuracy': 0.6471619606018066, 'train/loss': 1.5200525522232056, 'validation/accuracy': 0.5983200073242188, 'validation/loss': 1.7462304830551147, 'validation/num_examples': 50000, 'test/accuracy': 0.47470003366470337, 'test/loss': 2.412858009338379, 'test/num_examples': 10000, 'score': 14338.458849191666, 'total_duration': 15509.033997297287, 'accumulated_submission_time': 14338.458849191666, 'accumulated_eval_time': 1163.9601674079895, 'accumulated_logging_time': 2.6400506496429443}
I0307 06:41:08.067405 140021218662144 logging_writer.py:48] [36964] accumulated_eval_time=1163.96, accumulated_logging_time=2.64005, accumulated_submission_time=14338.5, global_step=36964, preemption_count=0, score=14338.5, test/accuracy=0.4747, test/loss=2.41286, test/num_examples=10000, total_duration=15509, train/accuracy=0.647162, train/loss=1.52005, validation/accuracy=0.59832, validation/loss=1.74623, validation/num_examples=50000
I0307 06:41:22.469523 140021210269440 logging_writer.py:48] [37000] global_step=37000, grad_norm=2.9923319816589355, loss=2.7625482082366943
I0307 06:42:01.128226 140021218662144 logging_writer.py:48] [37100] global_step=37100, grad_norm=3.1045544147491455, loss=2.8225138187408447
I0307 06:42:39.695782 140021210269440 logging_writer.py:48] [37200] global_step=37200, grad_norm=2.7691712379455566, loss=2.777585506439209
I0307 06:43:18.471096 140021218662144 logging_writer.py:48] [37300] global_step=37300, grad_norm=2.759140729904175, loss=2.800771951675415
I0307 06:43:57.344632 140021210269440 logging_writer.py:48] [37400] global_step=37400, grad_norm=4.083551406860352, loss=2.741206407546997
I0307 06:44:36.105209 140021218662144 logging_writer.py:48] [37500] global_step=37500, grad_norm=3.049494743347168, loss=2.7313387393951416
I0307 06:45:15.542798 140021210269440 logging_writer.py:48] [37600] global_step=37600, grad_norm=2.955836057662964, loss=2.7741050720214844
I0307 06:45:54.398470 140021218662144 logging_writer.py:48] [37700] global_step=37700, grad_norm=3.2037198543548584, loss=2.837954521179199
I0307 06:46:33.231992 140021210269440 logging_writer.py:48] [37800] global_step=37800, grad_norm=3.3621633052825928, loss=2.836611270904541
I0307 06:47:11.718819 140021218662144 logging_writer.py:48] [37900] global_step=37900, grad_norm=2.9544458389282227, loss=2.817800998687744
I0307 06:47:50.968829 140021210269440 logging_writer.py:48] [38000] global_step=38000, grad_norm=2.9900264739990234, loss=2.851719379425049
I0307 06:48:30.048585 140021218662144 logging_writer.py:48] [38100] global_step=38100, grad_norm=2.535674810409546, loss=2.7281720638275146
I0307 06:49:08.877312 140021210269440 logging_writer.py:48] [38200] global_step=38200, grad_norm=3.0794718265533447, loss=2.7396485805511475
I0307 06:49:38.084398 140178276385984 spec.py:321] Evaluating on the training split.
I0307 06:49:49.955102 140178276385984 spec.py:333] Evaluating on the validation split.
I0307 06:50:08.873232 140178276385984 spec.py:349] Evaluating on the test split.
I0307 06:50:10.663202 140178276385984 submission_runner.py:469] Time since start: 16051.71s, 	Step: 38276, 	{'train/accuracy': 0.6376355290412903, 'train/loss': 1.5618999004364014, 'validation/accuracy': 0.5956400036811829, 'validation/loss': 1.7637141942977905, 'validation/num_examples': 50000, 'test/accuracy': 0.4739000201225281, 'test/loss': 2.428598642349243, 'test/num_examples': 10000, 'score': 14848.297893047333, 'total_duration': 16051.706707715988, 'accumulated_submission_time': 14848.297893047333, 'accumulated_eval_time': 1196.538817167282, 'accumulated_logging_time': 2.7606732845306396}
I0307 06:50:10.813158 140021218662144 logging_writer.py:48] [38276] accumulated_eval_time=1196.54, accumulated_logging_time=2.76067, accumulated_submission_time=14848.3, global_step=38276, preemption_count=0, score=14848.3, test/accuracy=0.4739, test/loss=2.4286, test/num_examples=10000, total_duration=16051.7, train/accuracy=0.637636, train/loss=1.5619, validation/accuracy=0.59564, validation/loss=1.76371, validation/num_examples=50000
I0307 06:50:20.580520 140021210269440 logging_writer.py:48] [38300] global_step=38300, grad_norm=2.863373279571533, loss=2.808413028717041
I0307 06:50:59.016191 140021218662144 logging_writer.py:48] [38400] global_step=38400, grad_norm=2.9761741161346436, loss=2.7939529418945312
I0307 06:51:37.969714 140021210269440 logging_writer.py:48] [38500] global_step=38500, grad_norm=3.36074161529541, loss=2.8138856887817383
I0307 06:52:16.837545 140021218662144 logging_writer.py:48] [38600] global_step=38600, grad_norm=3.5695314407348633, loss=2.76059889793396
I0307 06:52:55.470846 140021210269440 logging_writer.py:48] [38700] global_step=38700, grad_norm=2.950981855392456, loss=2.7675366401672363
I0307 06:53:34.544907 140021218662144 logging_writer.py:48] [38800] global_step=38800, grad_norm=2.9726176261901855, loss=2.7389774322509766
I0307 06:54:13.321069 140021210269440 logging_writer.py:48] [38900] global_step=38900, grad_norm=3.1106443405151367, loss=2.852402448654175
I0307 06:54:52.190423 140021218662144 logging_writer.py:48] [39000] global_step=39000, grad_norm=2.951322317123413, loss=2.8027563095092773
I0307 06:55:31.174957 140021210269440 logging_writer.py:48] [39100] global_step=39100, grad_norm=3.360496759414673, loss=2.748586654663086
I0307 06:56:10.279622 140021218662144 logging_writer.py:48] [39200] global_step=39200, grad_norm=3.2494332790374756, loss=2.8296358585357666
I0307 06:56:48.874116 140021210269440 logging_writer.py:48] [39300] global_step=39300, grad_norm=2.790227174758911, loss=2.73458194732666
I0307 06:57:27.678974 140021218662144 logging_writer.py:48] [39400] global_step=39400, grad_norm=3.408236026763916, loss=2.8689212799072266
I0307 06:58:06.420642 140021210269440 logging_writer.py:48] [39500] global_step=39500, grad_norm=3.2766635417938232, loss=2.7820162773132324
I0307 06:58:40.799492 140178276385984 spec.py:321] Evaluating on the training split.
I0307 06:58:52.858945 140178276385984 spec.py:333] Evaluating on the validation split.
I0307 06:59:19.480516 140178276385984 spec.py:349] Evaluating on the test split.
I0307 06:59:21.266967 140178276385984 submission_runner.py:469] Time since start: 16602.31s, 	Step: 39589, 	{'train/accuracy': 0.6508290767669678, 'train/loss': 1.5039514303207397, 'validation/accuracy': 0.6037600040435791, 'validation/loss': 1.7200927734375, 'validation/num_examples': 50000, 'test/accuracy': 0.4755000174045563, 'test/loss': 2.4143035411834717, 'test/num_examples': 10000, 'score': 15358.123179197311, 'total_duration': 16602.310455322266, 'accumulated_submission_time': 15358.123179197311, 'accumulated_eval_time': 1237.0061221122742, 'accumulated_logging_time': 2.9374351501464844}
I0307 06:59:21.371550 140021218662144 logging_writer.py:48] [39589] accumulated_eval_time=1237.01, accumulated_logging_time=2.93744, accumulated_submission_time=15358.1, global_step=39589, preemption_count=0, score=15358.1, test/accuracy=0.4755, test/loss=2.4143, test/num_examples=10000, total_duration=16602.3, train/accuracy=0.650829, train/loss=1.50395, validation/accuracy=0.60376, validation/loss=1.72009, validation/num_examples=50000
I0307 06:59:26.132187 140021210269440 logging_writer.py:48] [39600] global_step=39600, grad_norm=3.021230459213257, loss=2.8398139476776123
I0307 07:00:04.622909 140021218662144 logging_writer.py:48] [39700] global_step=39700, grad_norm=2.8977863788604736, loss=2.7456459999084473
I0307 07:00:43.325930 140021210269440 logging_writer.py:48] [39800] global_step=39800, grad_norm=2.93892765045166, loss=2.753847599029541
I0307 07:01:22.535902 140021218662144 logging_writer.py:48] [39900] global_step=39900, grad_norm=2.782771587371826, loss=2.8346896171569824
I0307 07:02:00.992570 140021210269440 logging_writer.py:48] [40000] global_step=40000, grad_norm=3.26312255859375, loss=2.844935417175293
I0307 07:02:40.091042 140021218662144 logging_writer.py:48] [40100] global_step=40100, grad_norm=2.8650002479553223, loss=2.7332425117492676
I0307 07:03:18.676385 140021210269440 logging_writer.py:48] [40200] global_step=40200, grad_norm=3.5405311584472656, loss=2.876805305480957
I0307 07:03:57.535039 140021218662144 logging_writer.py:48] [40300] global_step=40300, grad_norm=3.341456174850464, loss=2.807368278503418
I0307 07:04:36.487649 140021210269440 logging_writer.py:48] [40400] global_step=40400, grad_norm=2.779038906097412, loss=2.7949764728546143
I0307 07:05:15.134008 140021218662144 logging_writer.py:48] [40500] global_step=40500, grad_norm=4.028955936431885, loss=2.819446086883545
I0307 07:05:53.868756 140021210269440 logging_writer.py:48] [40600] global_step=40600, grad_norm=3.2297351360321045, loss=2.6638314723968506
I0307 07:06:32.977699 140021218662144 logging_writer.py:48] [40700] global_step=40700, grad_norm=2.680941343307495, loss=2.769803524017334
I0307 07:07:11.780261 140021210269440 logging_writer.py:48] [40800] global_step=40800, grad_norm=2.8543498516082764, loss=2.859830617904663
I0307 07:07:50.513434 140021218662144 logging_writer.py:48] [40900] global_step=40900, grad_norm=2.677018642425537, loss=2.738741159439087
I0307 07:07:51.277434 140178276385984 spec.py:321] Evaluating on the training split.
I0307 07:08:03.845893 140178276385984 spec.py:333] Evaluating on the validation split.
2025-03-07 07:08:07.341810: E external/xla/xla/stream_executor/cuda/cuda_driver.cc:1635] failed to alloc 1073741824 bytes on host: CUDA_ERROR_INVALID_VALUE: invalid argument
2025-03-07 07:08:07.676691: E external/xla/xla/stream_executor/cuda/cuda_driver.cc:1635] failed to alloc 966367744 bytes on host: CUDA_ERROR_INVALID_VALUE: invalid argument
2025-03-07 07:08:07.702785: E external/xla/xla/stream_executor/cuda/cuda_driver.cc:1635] failed to alloc 869731072 bytes on host: CUDA_ERROR_INVALID_VALUE: invalid argument
2025-03-07 07:08:07.726887: E external/xla/xla/stream_executor/cuda/cuda_driver.cc:1635] failed to alloc 782758144 bytes on host: CUDA_ERROR_INVALID_VALUE: invalid argument
2025-03-07 07:08:07.831672: E external/xla/xla/stream_executor/cuda/cuda_driver.cc:1635] failed to alloc 704482304 bytes on host: CUDA_ERROR_INVALID_VALUE: invalid argument
2025-03-07 07:08:07.879591: E external/xla/xla/stream_executor/cuda/cuda_driver.cc:1635] failed to alloc 634034176 bytes on host: CUDA_ERROR_INVALID_VALUE: invalid argument
2025-03-07 07:08:07.903200: E external/xla/xla/stream_executor/cuda/cuda_driver.cc:1635] failed to alloc 570630912 bytes on host: CUDA_ERROR_INVALID_VALUE: invalid argument
I0307 07:08:29.249424 140178276385984 spec.py:349] Evaluating on the test split.
I0307 07:08:30.978702 140178276385984 submission_runner.py:469] Time since start: 17152.02s, 	Step: 40903, 	{'train/accuracy': 0.6486367583274841, 'train/loss': 1.5019538402557373, 'validation/accuracy': 0.6007800102233887, 'validation/loss': 1.7279860973358154, 'validation/num_examples': 50000, 'test/accuracy': 0.46870002150535583, 'test/loss': 2.4473395347595215, 'test/num_examples': 10000, 'score': 15867.841248989105, 'total_duration': 17152.022185325623, 'accumulated_submission_time': 15867.841248989105, 'accumulated_eval_time': 1276.7072124481201, 'accumulated_logging_time': 3.0963544845581055}
I0307 07:08:31.063489 140021210269440 logging_writer.py:48] [40903] accumulated_eval_time=1276.71, accumulated_logging_time=3.09635, accumulated_submission_time=15867.8, global_step=40903, preemption_count=0, score=15867.8, test/accuracy=0.4687, test/loss=2.44734, test/num_examples=10000, total_duration=17152, train/accuracy=0.648637, train/loss=1.50195, validation/accuracy=0.60078, validation/loss=1.72799, validation/num_examples=50000
I0307 07:09:09.241995 140021218662144 logging_writer.py:48] [41000] global_step=41000, grad_norm=2.9323630332946777, loss=2.7777328491210938
I0307 07:09:47.879995 140021210269440 logging_writer.py:48] [41100] global_step=41100, grad_norm=2.969804525375366, loss=2.811894655227661
I0307 07:10:27.365735 140021218662144 logging_writer.py:48] [41200] global_step=41200, grad_norm=3.0430989265441895, loss=2.7110185623168945
I0307 07:11:07.609545 140021210269440 logging_writer.py:48] [41300] global_step=41300, grad_norm=2.6693248748779297, loss=2.752936840057373
I0307 07:11:46.653571 140021218662144 logging_writer.py:48] [41400] global_step=41400, grad_norm=3.672788381576538, loss=2.8360066413879395
I0307 07:12:25.174359 140021210269440 logging_writer.py:48] [41500] global_step=41500, grad_norm=3.0740411281585693, loss=2.830197334289551
I0307 07:13:04.247319 140021218662144 logging_writer.py:48] [41600] global_step=41600, grad_norm=3.6046643257141113, loss=2.846198558807373
I0307 07:13:42.844555 140021210269440 logging_writer.py:48] [41700] global_step=41700, grad_norm=3.5324175357818604, loss=2.696743965148926
I0307 07:14:22.064162 140021218662144 logging_writer.py:48] [41800] global_step=41800, grad_norm=3.0259130001068115, loss=2.7515194416046143
I0307 07:15:00.662442 140021210269440 logging_writer.py:48] [41900] global_step=41900, grad_norm=2.8569774627685547, loss=2.7926015853881836
I0307 07:15:39.273751 140021218662144 logging_writer.py:48] [42000] global_step=42000, grad_norm=3.455792188644409, loss=2.7839083671569824
I0307 07:16:17.771955 140021210269440 logging_writer.py:48] [42100] global_step=42100, grad_norm=2.9749441146850586, loss=2.817249298095703
I0307 07:16:56.706092 140021218662144 logging_writer.py:48] [42200] global_step=42200, grad_norm=3.004377841949463, loss=2.870241165161133
I0307 07:17:01.450907 140178276385984 spec.py:321] Evaluating on the training split.
I0307 07:17:12.574825 140178276385984 spec.py:333] Evaluating on the validation split.
I0307 07:17:37.631502 140178276385984 spec.py:349] Evaluating on the test split.
I0307 07:17:39.422902 140178276385984 submission_runner.py:469] Time since start: 17700.47s, 	Step: 42213, 	{'train/accuracy': 0.649832546710968, 'train/loss': 1.5172303915023804, 'validation/accuracy': 0.6005799770355225, 'validation/loss': 1.753570795059204, 'validation/num_examples': 50000, 'test/accuracy': 0.4838000237941742, 'test/loss': 2.393526792526245, 'test/num_examples': 10000, 'score': 16378.066253900528, 'total_duration': 17700.466415166855, 'accumulated_submission_time': 16378.066253900528, 'accumulated_eval_time': 1314.67906832695, 'accumulated_logging_time': 3.2084603309631348}
I0307 07:17:39.512437 140021210269440 logging_writer.py:48] [42213] accumulated_eval_time=1314.68, accumulated_logging_time=3.20846, accumulated_submission_time=16378.1, global_step=42213, preemption_count=0, score=16378.1, test/accuracy=0.4838, test/loss=2.39353, test/num_examples=10000, total_duration=17700.5, train/accuracy=0.649833, train/loss=1.51723, validation/accuracy=0.60058, validation/loss=1.75357, validation/num_examples=50000
I0307 07:18:13.411521 140021218662144 logging_writer.py:48] [42300] global_step=42300, grad_norm=2.882573127746582, loss=2.7159671783447266
I0307 07:18:52.645982 140021210269440 logging_writer.py:48] [42400] global_step=42400, grad_norm=3.637558937072754, loss=2.832594156265259
I0307 07:19:31.512486 140021218662144 logging_writer.py:48] [42500] global_step=42500, grad_norm=2.684987783432007, loss=2.7923057079315186
I0307 07:20:10.956250 140021210269440 logging_writer.py:48] [42600] global_step=42600, grad_norm=2.8956241607666016, loss=2.776130199432373
I0307 07:20:49.664575 140021218662144 logging_writer.py:48] [42700] global_step=42700, grad_norm=2.9650423526763916, loss=2.754155158996582
I0307 07:21:28.504369 140021210269440 logging_writer.py:48] [42800] global_step=42800, grad_norm=3.2570621967315674, loss=2.7611451148986816
I0307 07:22:07.445152 140021218662144 logging_writer.py:48] [42900] global_step=42900, grad_norm=3.275317907333374, loss=2.7698700428009033
I0307 07:22:46.461623 140021210269440 logging_writer.py:48] [43000] global_step=43000, grad_norm=3.24299693107605, loss=2.8405721187591553
I0307 07:23:25.108836 140021218662144 logging_writer.py:48] [43100] global_step=43100, grad_norm=3.2641797065734863, loss=2.6909587383270264
I0307 07:24:03.651655 140021210269440 logging_writer.py:48] [43200] global_step=43200, grad_norm=3.521303176879883, loss=2.896986722946167
I0307 07:24:41.985507 140021218662144 logging_writer.py:48] [43300] global_step=43300, grad_norm=3.322345018386841, loss=2.6789803504943848
I0307 07:25:20.883345 140021210269440 logging_writer.py:48] [43400] global_step=43400, grad_norm=2.9513936042785645, loss=2.701399087905884
I0307 07:25:59.877400 140021218662144 logging_writer.py:48] [43500] global_step=43500, grad_norm=2.8942763805389404, loss=2.7545790672302246
I0307 07:26:09.555046 140178276385984 spec.py:321] Evaluating on the training split.
I0307 07:26:20.917935 140178276385984 spec.py:333] Evaluating on the validation split.
I0307 07:26:42.531243 140178276385984 spec.py:349] Evaluating on the test split.
I0307 07:26:44.327218 140178276385984 submission_runner.py:469] Time since start: 18245.37s, 	Step: 43526, 	{'train/accuracy': 0.660574734210968, 'train/loss': 1.4861271381378174, 'validation/accuracy': 0.6094200015068054, 'validation/loss': 1.7295567989349365, 'validation/num_examples': 50000, 'test/accuracy': 0.4872000217437744, 'test/loss': 2.392664909362793, 'test/num_examples': 10000, 'score': 16887.953792095184, 'total_duration': 18245.37069272995, 'accumulated_submission_time': 16887.953792095184, 'accumulated_eval_time': 1349.4510612487793, 'accumulated_logging_time': 3.3152430057525635}
I0307 07:26:44.388562 140021210269440 logging_writer.py:48] [43526] accumulated_eval_time=1349.45, accumulated_logging_time=3.31524, accumulated_submission_time=16888, global_step=43526, preemption_count=0, score=16888, test/accuracy=0.4872, test/loss=2.39266, test/num_examples=10000, total_duration=18245.4, train/accuracy=0.660575, train/loss=1.48613, validation/accuracy=0.60942, validation/loss=1.72956, validation/num_examples=50000
I0307 07:27:13.738392 140021218662144 logging_writer.py:48] [43600] global_step=43600, grad_norm=3.298542022705078, loss=2.689107894897461
I0307 07:27:52.546497 140021210269440 logging_writer.py:48] [43700] global_step=43700, grad_norm=2.8797426223754883, loss=2.7214033603668213
I0307 07:28:32.223970 140021218662144 logging_writer.py:48] [43800] global_step=43800, grad_norm=3.134744167327881, loss=2.811596632003784
I0307 07:29:10.409476 140021210269440 logging_writer.py:48] [43900] global_step=43900, grad_norm=3.0462827682495117, loss=2.6864519119262695
I0307 07:29:48.842356 140021218662144 logging_writer.py:48] [44000] global_step=44000, grad_norm=2.873955726623535, loss=2.704113245010376
I0307 07:30:27.423661 140021210269440 logging_writer.py:48] [44100] global_step=44100, grad_norm=2.8614909648895264, loss=2.795583963394165
I0307 07:31:06.185145 140021218662144 logging_writer.py:48] [44200] global_step=44200, grad_norm=2.79958438873291, loss=2.745837450027466
I0307 07:31:44.978854 140021210269440 logging_writer.py:48] [44300] global_step=44300, grad_norm=3.300797462463379, loss=2.678922176361084
I0307 07:32:23.687402 140021218662144 logging_writer.py:48] [44400] global_step=44400, grad_norm=3.136582136154175, loss=2.7313671112060547
I0307 07:33:02.568206 140021210269440 logging_writer.py:48] [44500] global_step=44500, grad_norm=2.728149890899658, loss=2.7383363246917725
I0307 07:33:41.266776 140021218662144 logging_writer.py:48] [44600] global_step=44600, grad_norm=3.070244073867798, loss=2.7919723987579346
I0307 07:34:20.333820 140021210269440 logging_writer.py:48] [44700] global_step=44700, grad_norm=3.646599292755127, loss=2.7410027980804443
I0307 07:34:59.137633 140021218662144 logging_writer.py:48] [44800] global_step=44800, grad_norm=4.007065296173096, loss=2.9588823318481445
I0307 07:35:14.665725 140178276385984 spec.py:321] Evaluating on the training split.
I0307 07:35:25.581332 140178276385984 spec.py:333] Evaluating on the validation split.
I0307 07:35:48.148061 140178276385984 spec.py:349] Evaluating on the test split.
I0307 07:35:49.922641 140178276385984 submission_runner.py:469] Time since start: 18790.97s, 	Step: 44841, 	{'train/accuracy': 0.6543566584587097, 'train/loss': 1.4867870807647705, 'validation/accuracy': 0.608299970626831, 'validation/loss': 1.704209327697754, 'validation/num_examples': 50000, 'test/accuracy': 0.48590001463890076, 'test/loss': 2.38734769821167, 'test/num_examples': 10000, 'score': 17398.072768211365, 'total_duration': 18790.966079711914, 'accumulated_submission_time': 17398.072768211365, 'accumulated_eval_time': 1384.7077555656433, 'accumulated_logging_time': 3.398590326309204}
I0307 07:35:49.981966 140021210269440 logging_writer.py:48] [44841] accumulated_eval_time=1384.71, accumulated_logging_time=3.39859, accumulated_submission_time=17398.1, global_step=44841, preemption_count=0, score=17398.1, test/accuracy=0.4859, test/loss=2.38735, test/num_examples=10000, total_duration=18791, train/accuracy=0.654357, train/loss=1.48679, validation/accuracy=0.6083, validation/loss=1.70421, validation/num_examples=50000
I0307 07:36:13.820202 140021218662144 logging_writer.py:48] [44900] global_step=44900, grad_norm=3.1554923057556152, loss=2.804140090942383
I0307 07:36:52.317996 140021210269440 logging_writer.py:48] [45000] global_step=45000, grad_norm=3.0697226524353027, loss=2.7504537105560303
I0307 07:37:31.675173 140021218662144 logging_writer.py:48] [45100] global_step=45100, grad_norm=3.016033172607422, loss=2.77034854888916
I0307 07:38:10.577111 140021210269440 logging_writer.py:48] [45200] global_step=45200, grad_norm=2.9582009315490723, loss=2.6608822345733643
I0307 07:38:49.541155 140021218662144 logging_writer.py:48] [45300] global_step=45300, grad_norm=2.8596301078796387, loss=2.7424473762512207
I0307 07:39:28.061342 140021210269440 logging_writer.py:48] [45400] global_step=45400, grad_norm=3.0473244190216064, loss=2.7733912467956543
I0307 07:40:06.906542 140021218662144 logging_writer.py:48] [45500] global_step=45500, grad_norm=2.946936845779419, loss=2.818284273147583
I0307 07:40:45.498615 140021210269440 logging_writer.py:48] [45600] global_step=45600, grad_norm=2.8822827339172363, loss=2.7933034896850586
I0307 07:41:24.420499 140021218662144 logging_writer.py:48] [45700] global_step=45700, grad_norm=3.1061108112335205, loss=2.8089418411254883
I0307 07:42:02.694550 140021210269440 logging_writer.py:48] [45800] global_step=45800, grad_norm=2.7584993839263916, loss=2.725200653076172
I0307 07:42:41.441581 140021218662144 logging_writer.py:48] [45900] global_step=45900, grad_norm=2.820479154586792, loss=2.7549421787261963
I0307 07:43:20.258433 140021210269440 logging_writer.py:48] [46000] global_step=46000, grad_norm=3.3180911540985107, loss=2.8047776222229004
I0307 07:43:59.383308 140021218662144 logging_writer.py:48] [46100] global_step=46100, grad_norm=3.0712010860443115, loss=2.754082679748535
I0307 07:44:20.102102 140178276385984 spec.py:321] Evaluating on the training split.
I0307 07:44:31.005582 140178276385984 spec.py:333] Evaluating on the validation split.
I0307 07:44:57.898455 140178276385984 spec.py:349] Evaluating on the test split.
I0307 07:44:59.696151 140178276385984 submission_runner.py:469] Time since start: 19340.74s, 	Step: 46154, 	{'train/accuracy': 0.6581433415412903, 'train/loss': 1.5149755477905273, 'validation/accuracy': 0.607479989528656, 'validation/loss': 1.7512638568878174, 'validation/num_examples': 50000, 'test/accuracy': 0.49010002613067627, 'test/loss': 2.3885762691497803, 'test/num_examples': 10000, 'score': 17908.021098852158, 'total_duration': 19340.73963212967, 'accumulated_submission_time': 17908.021098852158, 'accumulated_eval_time': 1424.3016283512115, 'accumulated_logging_time': 3.4924120903015137}
I0307 07:44:59.773714 140021210269440 logging_writer.py:48] [46154] accumulated_eval_time=1424.3, accumulated_logging_time=3.49241, accumulated_submission_time=17908, global_step=46154, preemption_count=0, score=17908, test/accuracy=0.4901, test/loss=2.38858, test/num_examples=10000, total_duration=19340.7, train/accuracy=0.658143, train/loss=1.51498, validation/accuracy=0.60748, validation/loss=1.75126, validation/num_examples=50000
I0307 07:45:18.201532 140021218662144 logging_writer.py:48] [46200] global_step=46200, grad_norm=2.9618124961853027, loss=2.704253911972046
I0307 07:45:58.017221 140021210269440 logging_writer.py:48] [46300] global_step=46300, grad_norm=2.8151543140411377, loss=2.82016921043396
I0307 07:46:37.128648 140021218662144 logging_writer.py:48] [46400] global_step=46400, grad_norm=3.3454694747924805, loss=2.729919910430908
I0307 07:47:15.761411 140021210269440 logging_writer.py:48] [46500] global_step=46500, grad_norm=2.981642484664917, loss=2.848546028137207
I0307 07:47:54.137215 140021218662144 logging_writer.py:48] [46600] global_step=46600, grad_norm=2.9792251586914062, loss=2.736521005630493
I0307 07:48:32.905977 140021210269440 logging_writer.py:48] [46700] global_step=46700, grad_norm=2.96687650680542, loss=2.758119583129883
I0307 07:49:11.619703 140021218662144 logging_writer.py:48] [46800] global_step=46800, grad_norm=2.6406662464141846, loss=2.68000864982605
I0307 07:49:50.039623 140021210269440 logging_writer.py:48] [46900] global_step=46900, grad_norm=3.531815767288208, loss=2.8173460960388184
I0307 07:50:28.676503 140021218662144 logging_writer.py:48] [47000] global_step=47000, grad_norm=3.1632490158081055, loss=2.7256133556365967
I0307 07:51:07.500395 140021210269440 logging_writer.py:48] [47100] global_step=47100, grad_norm=2.91353702545166, loss=2.7652816772460938
I0307 07:51:46.132754 140021218662144 logging_writer.py:48] [47200] global_step=47200, grad_norm=3.0293915271759033, loss=2.8617818355560303
I0307 07:52:25.152920 140021210269440 logging_writer.py:48] [47300] global_step=47300, grad_norm=2.862774133682251, loss=2.7240116596221924
I0307 07:53:04.453036 140021218662144 logging_writer.py:48] [47400] global_step=47400, grad_norm=3.121750831604004, loss=2.695464849472046
I0307 07:53:30.005954 140178276385984 spec.py:321] Evaluating on the training split.
I0307 07:53:41.025344 140178276385984 spec.py:333] Evaluating on the validation split.
I0307 07:54:05.680977 140178276385984 spec.py:349] Evaluating on the test split.
I0307 07:54:07.463532 140178276385984 submission_runner.py:469] Time since start: 19888.51s, 	Step: 47467, 	{'train/accuracy': 0.6602957248687744, 'train/loss': 1.4626154899597168, 'validation/accuracy': 0.6132799983024597, 'validation/loss': 1.67678701877594, 'validation/num_examples': 50000, 'test/accuracy': 0.4881000220775604, 'test/loss': 2.350416421890259, 'test/num_examples': 10000, 'score': 18418.08557868004, 'total_duration': 19888.50702190399, 'accumulated_submission_time': 18418.08557868004, 'accumulated_eval_time': 1461.7590398788452, 'accumulated_logging_time': 3.5983238220214844}
I0307 07:54:07.544334 140021210269440 logging_writer.py:48] [47467] accumulated_eval_time=1461.76, accumulated_logging_time=3.59832, accumulated_submission_time=18418.1, global_step=47467, preemption_count=0, score=18418.1, test/accuracy=0.4881, test/loss=2.35042, test/num_examples=10000, total_duration=19888.5, train/accuracy=0.660296, train/loss=1.46262, validation/accuracy=0.61328, validation/loss=1.67679, validation/num_examples=50000
I0307 07:54:20.780643 140021218662144 logging_writer.py:48] [47500] global_step=47500, grad_norm=3.513054132461548, loss=2.7006430625915527
I0307 07:54:59.841388 140021210269440 logging_writer.py:48] [47600] global_step=47600, grad_norm=3.169386386871338, loss=2.8013103008270264
I0307 07:55:38.629788 140021218662144 logging_writer.py:48] [47700] global_step=47700, grad_norm=3.2302958965301514, loss=2.8562965393066406
I0307 07:56:17.653403 140021210269440 logging_writer.py:48] [47800] global_step=47800, grad_norm=3.1981308460235596, loss=2.8478286266326904
I0307 07:56:56.370430 140021218662144 logging_writer.py:48] [47900] global_step=47900, grad_norm=3.067117929458618, loss=2.7123074531555176
I0307 07:57:34.782302 140021210269440 logging_writer.py:48] [48000] global_step=48000, grad_norm=3.103882074356079, loss=2.8553833961486816
I0307 07:58:13.387086 140021218662144 logging_writer.py:48] [48100] global_step=48100, grad_norm=3.5334105491638184, loss=2.809959650039673
I0307 07:58:52.213886 140021210269440 logging_writer.py:48] [48200] global_step=48200, grad_norm=2.9300527572631836, loss=2.663796901702881
I0307 07:59:30.642497 140021218662144 logging_writer.py:48] [48300] global_step=48300, grad_norm=3.157658338546753, loss=2.754683256149292
I0307 08:00:09.693040 140021210269440 logging_writer.py:48] [48400] global_step=48400, grad_norm=3.6541457176208496, loss=2.688793420791626
I0307 08:00:48.497083 140021218662144 logging_writer.py:48] [48500] global_step=48500, grad_norm=2.8459413051605225, loss=2.753347873687744
I0307 08:01:27.223166 140021210269440 logging_writer.py:48] [48600] global_step=48600, grad_norm=3.2608275413513184, loss=2.785370349884033
I0307 08:02:06.342479 140021218662144 logging_writer.py:48] [48700] global_step=48700, grad_norm=2.866550922393799, loss=2.7796239852905273
I0307 08:02:37.505532 140178276385984 spec.py:321] Evaluating on the training split.
I0307 08:02:48.533714 140178276385984 spec.py:333] Evaluating on the validation split.
I0307 08:03:16.086043 140178276385984 spec.py:349] Evaluating on the test split.
I0307 08:03:17.858798 140178276385984 submission_runner.py:469] Time since start: 20438.90s, 	Step: 48781, 	{'train/accuracy': 0.6590800285339355, 'train/loss': 1.482071876525879, 'validation/accuracy': 0.6136199831962585, 'validation/loss': 1.6933525800704956, 'validation/num_examples': 50000, 'test/accuracy': 0.489300012588501, 'test/loss': 2.389267683029175, 'test/num_examples': 10000, 'score': 18927.877472162247, 'total_duration': 20438.90229868889, 'accumulated_submission_time': 18927.877472162247, 'accumulated_eval_time': 1502.112146615982, 'accumulated_logging_time': 3.713918685913086}
I0307 08:03:17.951579 140021210269440 logging_writer.py:48] [48781] accumulated_eval_time=1502.11, accumulated_logging_time=3.71392, accumulated_submission_time=18927.9, global_step=48781, preemption_count=0, score=18927.9, test/accuracy=0.4893, test/loss=2.38927, test/num_examples=10000, total_duration=20438.9, train/accuracy=0.65908, train/loss=1.48207, validation/accuracy=0.61362, validation/loss=1.69335, validation/num_examples=50000
I0307 08:03:25.848537 140021218662144 logging_writer.py:48] [48800] global_step=48800, grad_norm=3.1556715965270996, loss=2.7941553592681885
I0307 08:04:04.521497 140021210269440 logging_writer.py:48] [48900] global_step=48900, grad_norm=2.8559744358062744, loss=2.666031837463379
I0307 08:04:43.001307 140021218662144 logging_writer.py:48] [49000] global_step=49000, grad_norm=3.99051833152771, loss=2.731039524078369
I0307 08:05:21.723875 140021210269440 logging_writer.py:48] [49100] global_step=49100, grad_norm=3.1818408966064453, loss=2.8132174015045166
I0307 08:06:00.274113 140021218662144 logging_writer.py:48] [49200] global_step=49200, grad_norm=3.281801462173462, loss=2.9032790660858154
I0307 08:06:38.953911 140021210269440 logging_writer.py:48] [49300] global_step=49300, grad_norm=3.2063729763031006, loss=2.7408668994903564
I0307 08:07:17.293108 140021218662144 logging_writer.py:48] [49400] global_step=49400, grad_norm=3.353109359741211, loss=2.740787982940674
I0307 08:07:55.849806 140021210269440 logging_writer.py:48] [49500] global_step=49500, grad_norm=3.514979839324951, loss=2.7534823417663574
I0307 08:08:34.311720 140021218662144 logging_writer.py:48] [49600] global_step=49600, grad_norm=3.606783390045166, loss=2.7641654014587402
I0307 08:09:13.064104 140021210269440 logging_writer.py:48] [49700] global_step=49700, grad_norm=3.0542643070220947, loss=2.7290518283843994
I0307 08:09:51.622847 140021218662144 logging_writer.py:48] [49800] global_step=49800, grad_norm=2.9898908138275146, loss=2.73905611038208
I0307 08:10:30.652885 140021210269440 logging_writer.py:48] [49900] global_step=49900, grad_norm=3.4413671493530273, loss=2.6657769680023193
I0307 08:11:09.471395 140021218662144 logging_writer.py:48] [50000] global_step=50000, grad_norm=3.5733466148376465, loss=2.6831517219543457
I0307 08:11:47.920171 140178276385984 spec.py:321] Evaluating on the training split.
I0307 08:11:58.815628 140178276385984 spec.py:333] Evaluating on the validation split.
I0307 08:12:25.957340 140178276385984 spec.py:349] Evaluating on the test split.
I0307 08:12:27.704964 140178276385984 submission_runner.py:469] Time since start: 20988.75s, 	Step: 50099, 	{'train/accuracy': 0.6592195630073547, 'train/loss': 1.4656013250350952, 'validation/accuracy': 0.6134999990463257, 'validation/loss': 1.6840779781341553, 'validation/num_examples': 50000, 'test/accuracy': 0.48600003123283386, 'test/loss': 2.373300075531006, 'test/num_examples': 10000, 'score': 19437.684385299683, 'total_duration': 20988.74840116501, 'accumulated_submission_time': 19437.684385299683, 'accumulated_eval_time': 1541.8967187404633, 'accumulated_logging_time': 3.835242986679077}
I0307 08:12:27.826757 140021210269440 logging_writer.py:48] [50099] accumulated_eval_time=1541.9, accumulated_logging_time=3.83524, accumulated_submission_time=19437.7, global_step=50099, preemption_count=0, score=19437.7, test/accuracy=0.486, test/loss=2.3733, test/num_examples=10000, total_duration=20988.7, train/accuracy=0.65922, train/loss=1.4656, validation/accuracy=0.6135, validation/loss=1.68408, validation/num_examples=50000
I0307 08:12:28.624126 140021218662144 logging_writer.py:48] [50100] global_step=50100, grad_norm=3.1913676261901855, loss=2.784834623336792
I0307 08:13:07.782238 140021210269440 logging_writer.py:48] [50200] global_step=50200, grad_norm=2.853710412979126, loss=2.677910327911377
I0307 08:13:53.645987 140021218662144 logging_writer.py:48] [50300] global_step=50300, grad_norm=2.7990481853485107, loss=2.7888126373291016
I0307 08:14:37.917240 140021210269440 logging_writer.py:48] [50400] global_step=50400, grad_norm=3.0496745109558105, loss=2.7558939456939697
I0307 08:15:16.795234 140021218662144 logging_writer.py:48] [50500] global_step=50500, grad_norm=3.190821886062622, loss=2.8644847869873047
I0307 08:15:55.860190 140021210269440 logging_writer.py:48] [50600] global_step=50600, grad_norm=2.81416654586792, loss=2.6757900714874268
I0307 08:16:34.360891 140021218662144 logging_writer.py:48] [50700] global_step=50700, grad_norm=2.885830879211426, loss=2.6748006343841553
I0307 08:17:13.468113 140021210269440 logging_writer.py:48] [50800] global_step=50800, grad_norm=3.3151392936706543, loss=2.763813018798828
I0307 08:17:52.140992 140021218662144 logging_writer.py:48] [50900] global_step=50900, grad_norm=3.0384392738342285, loss=2.7143659591674805
I0307 08:18:30.479809 140021210269440 logging_writer.py:48] [51000] global_step=51000, grad_norm=2.9790213108062744, loss=2.746830463409424
I0307 08:19:09.532283 140021218662144 logging_writer.py:48] [51100] global_step=51100, grad_norm=2.951841115951538, loss=2.7032594680786133
I0307 08:19:48.751833 140021210269440 logging_writer.py:48] [51200] global_step=51200, grad_norm=3.0365099906921387, loss=2.714139461517334
I0307 08:20:28.109095 140021218662144 logging_writer.py:48] [51300] global_step=51300, grad_norm=2.9806976318359375, loss=2.738524913787842
I0307 08:20:57.806069 140178276385984 spec.py:321] Evaluating on the training split.
I0307 08:21:09.019837 140178276385984 spec.py:333] Evaluating on the validation split.
I0307 08:21:35.124633 140178276385984 spec.py:349] Evaluating on the test split.
I0307 08:21:36.872616 140178276385984 submission_runner.py:469] Time since start: 21537.92s, 	Step: 51377, 	{'train/accuracy': 0.6623485088348389, 'train/loss': 1.4459306001663208, 'validation/accuracy': 0.6115399599075317, 'validation/loss': 1.678749680519104, 'validation/num_examples': 50000, 'test/accuracy': 0.48840001225471497, 'test/loss': 2.377931594848633, 'test/num_examples': 10000, 'score': 19947.51396727562, 'total_duration': 21537.916051387787, 'accumulated_submission_time': 19947.51396727562, 'accumulated_eval_time': 1580.963044166565, 'accumulated_logging_time': 3.9767110347747803}
I0307 08:21:36.982596 140021210269440 logging_writer.py:48] [51377] accumulated_eval_time=1580.96, accumulated_logging_time=3.97671, accumulated_submission_time=19947.5, global_step=51377, preemption_count=0, score=19947.5, test/accuracy=0.4884, test/loss=2.37793, test/num_examples=10000, total_duration=21537.9, train/accuracy=0.662349, train/loss=1.44593, validation/accuracy=0.61154, validation/loss=1.67875, validation/num_examples=50000
I0307 08:21:46.234805 140021218662144 logging_writer.py:48] [51400] global_step=51400, grad_norm=3.2796030044555664, loss=2.8242831230163574
I0307 08:22:25.128056 140021210269440 logging_writer.py:48] [51500] global_step=51500, grad_norm=3.406276226043701, loss=2.825514316558838
I0307 08:23:03.965332 140021218662144 logging_writer.py:48] [51600] global_step=51600, grad_norm=3.148561954498291, loss=2.773134708404541
I0307 08:23:42.795421 140021210269440 logging_writer.py:48] [51700] global_step=51700, grad_norm=3.0158536434173584, loss=2.666511297225952
I0307 08:24:21.406352 140021218662144 logging_writer.py:48] [51800] global_step=51800, grad_norm=3.155055284500122, loss=2.807222604751587
I0307 08:25:00.143980 140021210269440 logging_writer.py:48] [51900] global_step=51900, grad_norm=3.3531792163848877, loss=2.7924447059631348
I0307 08:25:38.933713 140021218662144 logging_writer.py:48] [52000] global_step=52000, grad_norm=3.0895724296569824, loss=2.731337547302246
I0307 08:26:17.572593 140021210269440 logging_writer.py:48] [52100] global_step=52100, grad_norm=3.2051308155059814, loss=2.6825387477874756
I0307 08:26:56.483056 140021218662144 logging_writer.py:48] [52200] global_step=52200, grad_norm=2.8887441158294678, loss=2.756336212158203
I0307 08:27:35.522920 140021210269440 logging_writer.py:48] [52300] global_step=52300, grad_norm=3.349116086959839, loss=2.7737936973571777
I0307 08:28:15.055895 140021218662144 logging_writer.py:48] [52400] global_step=52400, grad_norm=3.546205759048462, loss=2.726999282836914
I0307 08:28:54.747174 140021210269440 logging_writer.py:48] [52500] global_step=52500, grad_norm=3.1003153324127197, loss=2.6813340187072754
I0307 08:29:33.712793 140021218662144 logging_writer.py:48] [52600] global_step=52600, grad_norm=3.2406680583953857, loss=2.688814878463745
I0307 08:30:07.128733 140178276385984 spec.py:321] Evaluating on the training split.
I0307 08:30:18.307193 140178276385984 spec.py:333] Evaluating on the validation split.
I0307 08:30:44.072179 140178276385984 spec.py:349] Evaluating on the test split.
I0307 08:30:45.841895 140178276385984 submission_runner.py:469] Time since start: 22086.89s, 	Step: 52687, 	{'train/accuracy': 0.6687459945678711, 'train/loss': 1.454114556312561, 'validation/accuracy': 0.617859959602356, 'validation/loss': 1.6900898218154907, 'validation/num_examples': 50000, 'test/accuracy': 0.5021000504493713, 'test/loss': 2.3211350440979004, 'test/num_examples': 10000, 'score': 20457.49803853035, 'total_duration': 22086.88534140587, 'accumulated_submission_time': 20457.49803853035, 'accumulated_eval_time': 1619.67600274086, 'accumulated_logging_time': 4.113572359085083}
I0307 08:30:45.921585 140021210269440 logging_writer.py:48] [52687] accumulated_eval_time=1619.68, accumulated_logging_time=4.11357, accumulated_submission_time=20457.5, global_step=52687, preemption_count=0, score=20457.5, test/accuracy=0.5021, test/loss=2.32114, test/num_examples=10000, total_duration=22086.9, train/accuracy=0.668746, train/loss=1.45411, validation/accuracy=0.61786, validation/loss=1.69009, validation/num_examples=50000
I0307 08:30:51.422285 140021218662144 logging_writer.py:48] [52700] global_step=52700, grad_norm=3.515087366104126, loss=2.788877487182617
I0307 08:31:29.938959 140021210269440 logging_writer.py:48] [52800] global_step=52800, grad_norm=2.8843472003936768, loss=2.7607388496398926
I0307 08:32:08.505991 140021218662144 logging_writer.py:48] [52900] global_step=52900, grad_norm=3.1518118381500244, loss=2.752365827560425
I0307 08:32:47.158223 140021210269440 logging_writer.py:48] [53000] global_step=53000, grad_norm=3.146328926086426, loss=2.7713279724121094
I0307 08:33:25.961407 140021218662144 logging_writer.py:48] [53100] global_step=53100, grad_norm=2.8072171211242676, loss=2.8079371452331543
I0307 08:34:04.056876 140021210269440 logging_writer.py:48] [53200] global_step=53200, grad_norm=3.444504737854004, loss=2.692553758621216
I0307 08:34:42.524478 140021218662144 logging_writer.py:48] [53300] global_step=53300, grad_norm=3.100010633468628, loss=2.678988456726074
I0307 08:35:21.485652 140021210269440 logging_writer.py:48] [53400] global_step=53400, grad_norm=3.2491509914398193, loss=2.6797711849212646
I0307 08:35:59.529311 140021218662144 logging_writer.py:48] [53500] global_step=53500, grad_norm=3.446894645690918, loss=2.6992931365966797
I0307 08:36:38.001613 140021210269440 logging_writer.py:48] [53600] global_step=53600, grad_norm=3.29746150970459, loss=2.777885675430298
I0307 08:37:16.952441 140021218662144 logging_writer.py:48] [53700] global_step=53700, grad_norm=3.1079678535461426, loss=2.7004916667938232
I0307 08:37:55.754644 140021210269440 logging_writer.py:48] [53800] global_step=53800, grad_norm=3.20405912399292, loss=2.7444305419921875
I0307 08:38:33.882069 140021218662144 logging_writer.py:48] [53900] global_step=53900, grad_norm=3.205554246902466, loss=2.7648329734802246
I0307 08:39:12.728864 140021210269440 logging_writer.py:48] [54000] global_step=54000, grad_norm=3.2547502517700195, loss=2.696869134902954
I0307 08:39:16.272377 140178276385984 spec.py:321] Evaluating on the training split.
I0307 08:39:27.126413 140178276385984 spec.py:333] Evaluating on the validation split.
I0307 08:39:51.707832 140178276385984 spec.py:349] Evaluating on the test split.
I0307 08:39:53.460160 140178276385984 submission_runner.py:469] Time since start: 22634.50s, 	Step: 54010, 	{'train/accuracy': 0.6610730290412903, 'train/loss': 1.4648584127426147, 'validation/accuracy': 0.6103399991989136, 'validation/loss': 1.694023609161377, 'validation/num_examples': 50000, 'test/accuracy': 0.48420003056526184, 'test/loss': 2.402348756790161, 'test/num_examples': 10000, 'score': 20967.674545049667, 'total_duration': 22634.503627300262, 'accumulated_submission_time': 20967.674545049667, 'accumulated_eval_time': 1656.8635902404785, 'accumulated_logging_time': 4.229868412017822}
I0307 08:39:53.557447 140021218662144 logging_writer.py:48] [54010] accumulated_eval_time=1656.86, accumulated_logging_time=4.22987, accumulated_submission_time=20967.7, global_step=54010, preemption_count=0, score=20967.7, test/accuracy=0.4842, test/loss=2.40235, test/num_examples=10000, total_duration=22634.5, train/accuracy=0.661073, train/loss=1.46486, validation/accuracy=0.61034, validation/loss=1.69402, validation/num_examples=50000
I0307 08:40:28.640646 140021210269440 logging_writer.py:48] [54100] global_step=54100, grad_norm=3.134951114654541, loss=2.704423427581787
I0307 08:41:07.209311 140021218662144 logging_writer.py:48] [54200] global_step=54200, grad_norm=3.363732099533081, loss=2.685101270675659
I0307 08:41:46.219940 140021210269440 logging_writer.py:48] [54300] global_step=54300, grad_norm=3.084437370300293, loss=2.781661033630371
I0307 08:42:24.746982 140021218662144 logging_writer.py:48] [54400] global_step=54400, grad_norm=2.9435951709747314, loss=2.671304702758789
I0307 08:43:03.334674 140021210269440 logging_writer.py:48] [54500] global_step=54500, grad_norm=3.17093563079834, loss=2.725231885910034
I0307 08:43:42.261132 140021218662144 logging_writer.py:48] [54600] global_step=54600, grad_norm=3.1617894172668457, loss=2.7426793575286865
I0307 08:44:21.083815 140021210269440 logging_writer.py:48] [54700] global_step=54700, grad_norm=3.4495904445648193, loss=2.736985206604004
I0307 08:45:00.002992 140021218662144 logging_writer.py:48] [54800] global_step=54800, grad_norm=2.8688924312591553, loss=2.741149425506592
I0307 08:45:39.757989 140021210269440 logging_writer.py:48] [54900] global_step=54900, grad_norm=2.932982921600342, loss=2.739377975463867
I0307 08:46:19.291872 140021218662144 logging_writer.py:48] [55000] global_step=55000, grad_norm=3.0199451446533203, loss=2.706801652908325
I0307 08:46:58.463725 140021210269440 logging_writer.py:48] [55100] global_step=55100, grad_norm=2.9199678897857666, loss=2.7401123046875
I0307 08:47:37.293026 140021218662144 logging_writer.py:48] [55200] global_step=55200, grad_norm=3.0497238636016846, loss=2.7841479778289795
I0307 08:48:15.816516 140021210269440 logging_writer.py:48] [55300] global_step=55300, grad_norm=3.0216262340545654, loss=2.741701602935791
I0307 08:48:23.662980 140178276385984 spec.py:321] Evaluating on the training split.
I0307 08:48:36.086048 140178276385984 spec.py:333] Evaluating on the validation split.
I0307 08:48:56.330281 140178276385984 spec.py:349] Evaluating on the test split.
I0307 08:48:58.093332 140178276385984 submission_runner.py:469] Time since start: 23179.14s, 	Step: 55321, 	{'train/accuracy': 0.6611726880073547, 'train/loss': 1.4714429378509521, 'validation/accuracy': 0.6130599975585938, 'validation/loss': 1.701870083808899, 'validation/num_examples': 50000, 'test/accuracy': 0.4928000271320343, 'test/loss': 2.3565683364868164, 'test/num_examples': 10000, 'score': 21477.61782360077, 'total_duration': 23179.13675260544, 'accumulated_submission_time': 21477.61782360077, 'accumulated_eval_time': 1691.293715953827, 'accumulated_logging_time': 4.355632781982422}
I0307 08:48:58.198809 140021218662144 logging_writer.py:48] [55321] accumulated_eval_time=1691.29, accumulated_logging_time=4.35563, accumulated_submission_time=21477.6, global_step=55321, preemption_count=0, score=21477.6, test/accuracy=0.4928, test/loss=2.35657, test/num_examples=10000, total_duration=23179.1, train/accuracy=0.661173, train/loss=1.47144, validation/accuracy=0.61306, validation/loss=1.70187, validation/num_examples=50000
I0307 08:49:29.462266 140021210269440 logging_writer.py:48] [55400] global_step=55400, grad_norm=2.846940755844116, loss=2.6066527366638184
I0307 08:50:08.225353 140021218662144 logging_writer.py:48] [55500] global_step=55500, grad_norm=2.959986448287964, loss=2.7418065071105957
I0307 08:50:46.518850 140021210269440 logging_writer.py:48] [55600] global_step=55600, grad_norm=2.9524948596954346, loss=2.7133898735046387
I0307 08:51:24.742542 140021218662144 logging_writer.py:48] [55700] global_step=55700, grad_norm=3.3009250164031982, loss=2.705070972442627
I0307 08:52:03.172948 140021210269440 logging_writer.py:48] [55800] global_step=55800, grad_norm=2.689926862716675, loss=2.736265182495117
I0307 08:52:41.838218 140021218662144 logging_writer.py:48] [55900] global_step=55900, grad_norm=2.789935350418091, loss=2.6714649200439453
I0307 08:53:20.733862 140021210269440 logging_writer.py:48] [56000] global_step=56000, grad_norm=2.9440274238586426, loss=2.665174722671509
I0307 08:53:59.499500 140021218662144 logging_writer.py:48] [56100] global_step=56100, grad_norm=3.167145252227783, loss=2.7123560905456543
I0307 08:54:39.597396 140021210269440 logging_writer.py:48] [56200] global_step=56200, grad_norm=3.127102851867676, loss=2.71437406539917
I0307 08:55:19.074871 140021218662144 logging_writer.py:48] [56300] global_step=56300, grad_norm=3.29843807220459, loss=2.779334306716919
I0307 08:55:58.159185 140021210269440 logging_writer.py:48] [56400] global_step=56400, grad_norm=2.863064765930176, loss=2.711988925933838
I0307 08:56:37.124735 140021218662144 logging_writer.py:48] [56500] global_step=56500, grad_norm=3.0963327884674072, loss=2.7920522689819336
I0307 08:57:15.629540 140021210269440 logging_writer.py:48] [56600] global_step=56600, grad_norm=2.70648193359375, loss=2.6537387371063232
I0307 08:57:28.238042 140178276385984 spec.py:321] Evaluating on the training split.
I0307 08:57:39.446102 140178276385984 spec.py:333] Evaluating on the validation split.
I0307 08:58:02.701950 140178276385984 spec.py:349] Evaluating on the test split.
I0307 08:58:04.451252 140178276385984 submission_runner.py:469] Time since start: 23725.49s, 	Step: 56633, 	{'train/accuracy': 0.6633051633834839, 'train/loss': 1.4805388450622559, 'validation/accuracy': 0.6139599680900574, 'validation/loss': 1.7096501588821411, 'validation/num_examples': 50000, 'test/accuracy': 0.4856000244617462, 'test/loss': 2.3927533626556396, 'test/num_examples': 10000, 'score': 21987.49871826172, 'total_duration': 23725.494714975357, 'accumulated_submission_time': 21987.49871826172, 'accumulated_eval_time': 1727.506727218628, 'accumulated_logging_time': 4.483761548995972}
I0307 08:58:04.582141 140021218662144 logging_writer.py:48] [56633] accumulated_eval_time=1727.51, accumulated_logging_time=4.48376, accumulated_submission_time=21987.5, global_step=56633, preemption_count=0, score=21987.5, test/accuracy=0.4856, test/loss=2.39275, test/num_examples=10000, total_duration=23725.5, train/accuracy=0.663305, train/loss=1.48054, validation/accuracy=0.61396, validation/loss=1.70965, validation/num_examples=50000
I0307 08:58:30.871218 140021210269440 logging_writer.py:48] [56700] global_step=56700, grad_norm=3.0882341861724854, loss=2.805172920227051
I0307 08:59:09.582104 140021218662144 logging_writer.py:48] [56800] global_step=56800, grad_norm=3.65386962890625, loss=2.694084405899048
I0307 08:59:48.370446 140021210269440 logging_writer.py:48] [56900] global_step=56900, grad_norm=3.174734115600586, loss=2.7971763610839844
I0307 09:00:27.008509 140021218662144 logging_writer.py:48] [57000] global_step=57000, grad_norm=3.675320863723755, loss=2.734516143798828
I0307 09:01:05.305564 140021210269440 logging_writer.py:48] [57100] global_step=57100, grad_norm=2.973618984222412, loss=2.7030715942382812
I0307 09:01:43.918316 140021218662144 logging_writer.py:48] [57200] global_step=57200, grad_norm=3.0833914279937744, loss=2.703914165496826
I0307 09:02:22.866129 140021210269440 logging_writer.py:48] [57300] global_step=57300, grad_norm=2.9610631465911865, loss=2.6168975830078125
I0307 09:03:02.602388 140021218662144 logging_writer.py:48] [57400] global_step=57400, grad_norm=3.819004535675049, loss=2.772296905517578
I0307 09:03:43.016914 140021210269440 logging_writer.py:48] [57500] global_step=57500, grad_norm=3.00142765045166, loss=2.6786210536956787
I0307 09:04:22.449029 140021218662144 logging_writer.py:48] [57600] global_step=57600, grad_norm=3.3056087493896484, loss=2.790884017944336
I0307 09:05:01.261803 140021210269440 logging_writer.py:48] [57700] global_step=57700, grad_norm=2.8955671787261963, loss=2.764322280883789
I0307 09:05:40.151863 140021218662144 logging_writer.py:48] [57800] global_step=57800, grad_norm=3.337578058242798, loss=2.727454662322998
I0307 09:06:18.964152 140021210269440 logging_writer.py:48] [57900] global_step=57900, grad_norm=3.737967014312744, loss=2.7184548377990723
I0307 09:06:34.792315 140178276385984 spec.py:321] Evaluating on the training split.
I0307 09:06:46.382122 140178276385984 spec.py:333] Evaluating on the validation split.
I0307 09:07:04.702576 140178276385984 spec.py:349] Evaluating on the test split.
I0307 09:07:06.484142 140178276385984 submission_runner.py:469] Time since start: 24267.53s, 	Step: 57942, 	{'train/accuracy': 0.6635243892669678, 'train/loss': 1.4601473808288574, 'validation/accuracy': 0.6194800138473511, 'validation/loss': 1.6809638738632202, 'validation/num_examples': 50000, 'test/accuracy': 0.49490001797676086, 'test/loss': 2.341952085494995, 'test/num_examples': 10000, 'score': 22497.533754587173, 'total_duration': 24267.5276324749, 'accumulated_submission_time': 22497.533754587173, 'accumulated_eval_time': 1759.198405265808, 'accumulated_logging_time': 4.650980472564697}
I0307 09:07:06.576231 140021218662144 logging_writer.py:48] [57942] accumulated_eval_time=1759.2, accumulated_logging_time=4.65098, accumulated_submission_time=22497.5, global_step=57942, preemption_count=0, score=22497.5, test/accuracy=0.4949, test/loss=2.34195, test/num_examples=10000, total_duration=24267.5, train/accuracy=0.663524, train/loss=1.46015, validation/accuracy=0.61948, validation/loss=1.68096, validation/num_examples=50000
I0307 09:07:29.427906 140021210269440 logging_writer.py:48] [58000] global_step=58000, grad_norm=3.1614632606506348, loss=2.793004035949707
I0307 09:08:08.128393 140021218662144 logging_writer.py:48] [58100] global_step=58100, grad_norm=2.8533613681793213, loss=2.644071102142334
I0307 09:08:47.273832 140021210269440 logging_writer.py:48] [58200] global_step=58200, grad_norm=3.010746955871582, loss=2.766343116760254
I0307 09:09:26.420374 140021218662144 logging_writer.py:48] [58300] global_step=58300, grad_norm=2.7664761543273926, loss=2.6729848384857178
I0307 09:10:04.788664 140021210269440 logging_writer.py:48] [58400] global_step=58400, grad_norm=3.561918020248413, loss=2.7047736644744873
I0307 09:10:43.204529 140021218662144 logging_writer.py:48] [58500] global_step=58500, grad_norm=2.8051767349243164, loss=2.6731221675872803
I0307 09:11:22.527479 140021210269440 logging_writer.py:48] [58600] global_step=58600, grad_norm=3.1993513107299805, loss=2.629831075668335
I0307 09:12:02.530052 140021218662144 logging_writer.py:48] [58700] global_step=58700, grad_norm=3.176349639892578, loss=2.696063995361328
I0307 09:12:42.815905 140021210269440 logging_writer.py:48] [58800] global_step=58800, grad_norm=2.789600372314453, loss=2.6269052028656006
I0307 09:13:21.334112 140021218662144 logging_writer.py:48] [58900] global_step=58900, grad_norm=3.47981595993042, loss=2.6884188652038574
I0307 09:14:00.392268 140021210269440 logging_writer.py:48] [59000] global_step=59000, grad_norm=3.346298933029175, loss=2.671065330505371
I0307 09:14:39.173547 140021218662144 logging_writer.py:48] [59100] global_step=59100, grad_norm=2.932861328125, loss=2.760551929473877
I0307 09:15:17.536390 140021210269440 logging_writer.py:48] [59200] global_step=59200, grad_norm=3.3313353061676025, loss=2.774054527282715
I0307 09:15:36.709403 140178276385984 spec.py:321] Evaluating on the training split.
I0307 09:15:48.893766 140178276385984 spec.py:333] Evaluating on the validation split.
I0307 09:16:10.813359 140178276385984 spec.py:349] Evaluating on the test split.
I0307 09:16:12.550585 140178276385984 submission_runner.py:469] Time since start: 24813.59s, 	Step: 59250, 	{'train/accuracy': 0.6815210580825806, 'train/loss': 1.407950520515442, 'validation/accuracy': 0.6273199915885925, 'validation/loss': 1.64316725730896, 'validation/num_examples': 50000, 'test/accuracy': 0.5069000124931335, 'test/loss': 2.278907299041748, 'test/num_examples': 10000, 'score': 23007.503632068634, 'total_duration': 24813.594052791595, 'accumulated_submission_time': 23007.503632068634, 'accumulated_eval_time': 1795.0393946170807, 'accumulated_logging_time': 4.769937992095947}
I0307 09:16:12.644715 140021218662144 logging_writer.py:48] [59250] accumulated_eval_time=1795.04, accumulated_logging_time=4.76994, accumulated_submission_time=23007.5, global_step=59250, preemption_count=0, score=23007.5, test/accuracy=0.5069, test/loss=2.27891, test/num_examples=10000, total_duration=24813.6, train/accuracy=0.681521, train/loss=1.40795, validation/accuracy=0.62732, validation/loss=1.64317, validation/num_examples=50000
I0307 09:16:32.572206 140021210269440 logging_writer.py:48] [59300] global_step=59300, grad_norm=3.746952772140503, loss=2.7660269737243652
I0307 09:17:11.555898 140021218662144 logging_writer.py:48] [59400] global_step=59400, grad_norm=3.3194830417633057, loss=2.6331214904785156
I0307 09:17:50.483584 140021210269440 logging_writer.py:48] [59500] global_step=59500, grad_norm=3.3851969242095947, loss=2.739877462387085
I0307 09:18:29.214809 140021218662144 logging_writer.py:48] [59600] global_step=59600, grad_norm=3.3341054916381836, loss=2.7408792972564697
I0307 09:19:07.491766 140021210269440 logging_writer.py:48] [59700] global_step=59700, grad_norm=3.0511703491210938, loss=2.6779122352600098
I0307 09:19:45.943933 140021218662144 logging_writer.py:48] [59800] global_step=59800, grad_norm=3.4773612022399902, loss=2.6555047035217285
I0307 09:20:24.713253 140021210269440 logging_writer.py:48] [59900] global_step=59900, grad_norm=3.262151002883911, loss=2.77388858795166
I0307 09:21:03.629917 140021218662144 logging_writer.py:48] [60000] global_step=60000, grad_norm=3.3654096126556396, loss=2.7249655723571777
I0307 09:21:43.567744 140021210269440 logging_writer.py:48] [60100] global_step=60100, grad_norm=3.0604426860809326, loss=2.653785228729248
I0307 09:22:22.000559 140021218662144 logging_writer.py:48] [60200] global_step=60200, grad_norm=3.186440944671631, loss=2.8492743968963623
I0307 09:23:00.743685 140021210269440 logging_writer.py:48] [60300] global_step=60300, grad_norm=3.0251901149749756, loss=2.6716089248657227
I0307 09:23:39.663237 140021218662144 logging_writer.py:48] [60400] global_step=60400, grad_norm=2.94826602935791, loss=2.5643086433410645
I0307 09:24:18.185326 140021210269440 logging_writer.py:48] [60500] global_step=60500, grad_norm=3.4757111072540283, loss=2.669894218444824
I0307 09:24:42.663628 140178276385984 spec.py:321] Evaluating on the training split.
I0307 09:24:55.230091 140178276385984 spec.py:333] Evaluating on the validation split.
I0307 09:25:16.341927 140178276385984 spec.py:349] Evaluating on the test split.
I0307 09:25:18.079669 140178276385984 submission_runner.py:469] Time since start: 25359.12s, 	Step: 60564, 	{'train/accuracy': 0.6716955900192261, 'train/loss': 1.43076491355896, 'validation/accuracy': 0.6256999969482422, 'validation/loss': 1.652513027191162, 'validation/num_examples': 50000, 'test/accuracy': 0.5027000308036804, 'test/loss': 2.324721097946167, 'test/num_examples': 10000, 'score': 23517.36927509308, 'total_duration': 25359.12310576439, 'accumulated_submission_time': 23517.36927509308, 'accumulated_eval_time': 1830.45521402359, 'accumulated_logging_time': 4.889114618301392}
I0307 09:25:18.159835 140021218662144 logging_writer.py:48] [60564] accumulated_eval_time=1830.46, accumulated_logging_time=4.88911, accumulated_submission_time=23517.4, global_step=60564, preemption_count=0, score=23517.4, test/accuracy=0.5027, test/loss=2.32472, test/num_examples=10000, total_duration=25359.1, train/accuracy=0.671696, train/loss=1.43076, validation/accuracy=0.6257, validation/loss=1.65251, validation/num_examples=50000
I0307 09:25:32.719503 140021210269440 logging_writer.py:48] [60600] global_step=60600, grad_norm=2.960792064666748, loss=2.703864574432373
I0307 09:26:11.375897 140021218662144 logging_writer.py:48] [60700] global_step=60700, grad_norm=3.680424690246582, loss=2.6953158378601074
I0307 09:26:49.979945 140021210269440 logging_writer.py:48] [60800] global_step=60800, grad_norm=3.34336256980896, loss=2.699702262878418
I0307 09:27:28.454910 140021218662144 logging_writer.py:48] [60900] global_step=60900, grad_norm=3.27528977394104, loss=2.771106481552124
I0307 09:28:07.206225 140021210269440 logging_writer.py:48] [61000] global_step=61000, grad_norm=3.493873357772827, loss=2.8098573684692383
I0307 09:28:45.880907 140021218662144 logging_writer.py:48] [61100] global_step=61100, grad_norm=3.2918713092803955, loss=2.7389583587646484
I0307 09:29:24.444986 140021210269440 logging_writer.py:48] [61200] global_step=61200, grad_norm=3.079240322113037, loss=2.745622158050537
I0307 09:30:03.519403 140021218662144 logging_writer.py:48] [61300] global_step=61300, grad_norm=3.207773447036743, loss=2.6251962184906006
I0307 09:30:42.625100 140021210269440 logging_writer.py:48] [61400] global_step=61400, grad_norm=3.477116346359253, loss=2.6368277072906494
I0307 09:31:21.516847 140021218662144 logging_writer.py:48] [61500] global_step=61500, grad_norm=3.1085000038146973, loss=2.8266804218292236
I0307 09:32:00.024977 140021210269440 logging_writer.py:48] [61600] global_step=61600, grad_norm=3.626220941543579, loss=2.781771659851074
I0307 09:32:38.789338 140021218662144 logging_writer.py:48] [61700] global_step=61700, grad_norm=3.009991407394409, loss=2.7568047046661377
I0307 09:33:17.396308 140021210269440 logging_writer.py:48] [61800] global_step=61800, grad_norm=2.8335483074188232, loss=2.6974709033966064
I0307 09:33:48.378916 140178276385984 spec.py:321] Evaluating on the training split.
I0307 09:34:00.402254 140178276385984 spec.py:333] Evaluating on the validation split.
I0307 09:34:22.790189 140178276385984 spec.py:349] Evaluating on the test split.
I0307 09:34:24.501790 140178276385984 submission_runner.py:469] Time since start: 25905.55s, 	Step: 61881, 	{'train/accuracy': 0.671875, 'train/loss': 1.4209444522857666, 'validation/accuracy': 0.6280800104141235, 'validation/loss': 1.634539008140564, 'validation/num_examples': 50000, 'test/accuracy': 0.5053000450134277, 'test/loss': 2.3042047023773193, 'test/num_examples': 10000, 'score': 24027.429548978806, 'total_duration': 25905.54525899887, 'accumulated_submission_time': 24027.429548978806, 'accumulated_eval_time': 1866.5778954029083, 'accumulated_logging_time': 4.998018741607666}
I0307 09:34:24.628334 140021218662144 logging_writer.py:48] [61881] accumulated_eval_time=1866.58, accumulated_logging_time=4.99802, accumulated_submission_time=24027.4, global_step=61881, preemption_count=0, score=24027.4, test/accuracy=0.5053, test/loss=2.3042, test/num_examples=10000, total_duration=25905.5, train/accuracy=0.671875, train/loss=1.42094, validation/accuracy=0.62808, validation/loss=1.63454, validation/num_examples=50000
I0307 09:34:32.507843 140021210269440 logging_writer.py:48] [61900] global_step=61900, grad_norm=3.315769672393799, loss=2.835029125213623
I0307 09:35:11.151399 140021218662144 logging_writer.py:48] [62000] global_step=62000, grad_norm=3.411038398742676, loss=2.6858339309692383
I0307 09:35:49.742396 140021210269440 logging_writer.py:48] [62100] global_step=62100, grad_norm=3.4607608318328857, loss=2.6523542404174805
I0307 09:36:28.301087 140021218662144 logging_writer.py:48] [62200] global_step=62200, grad_norm=3.1196835041046143, loss=2.6562469005584717
I0307 09:37:07.152621 140021210269440 logging_writer.py:48] [62300] global_step=62300, grad_norm=3.2439210414886475, loss=2.842759370803833
I0307 09:37:46.526107 140021218662144 logging_writer.py:48] [62400] global_step=62400, grad_norm=3.5435829162597656, loss=2.80953311920166
I0307 09:38:25.517318 140021210269440 logging_writer.py:48] [62500] global_step=62500, grad_norm=4.013607501983643, loss=2.7037928104400635
I0307 09:39:04.472696 140021218662144 logging_writer.py:48] [62600] global_step=62600, grad_norm=3.018059253692627, loss=2.7292513847351074
I0307 09:39:43.224414 140021210269440 logging_writer.py:48] [62700] global_step=62700, grad_norm=3.002168655395508, loss=2.683640241622925
I0307 09:40:21.868963 140021218662144 logging_writer.py:48] [62800] global_step=62800, grad_norm=2.9768872261047363, loss=2.769972801208496
I0307 09:41:00.983418 140021210269440 logging_writer.py:48] [62900] global_step=62900, grad_norm=3.670008420944214, loss=2.788907051086426
I0307 09:41:40.009176 140021218662144 logging_writer.py:48] [63000] global_step=63000, grad_norm=3.3652336597442627, loss=2.7942795753479004
I0307 09:42:18.741591 140021210269440 logging_writer.py:48] [63100] global_step=63100, grad_norm=2.9573757648468018, loss=2.6995770931243896
I0307 09:42:54.543482 140178276385984 spec.py:321] Evaluating on the training split.
I0307 09:43:06.402917 140178276385984 spec.py:333] Evaluating on the validation split.
I0307 09:43:32.958658 140178276385984 spec.py:349] Evaluating on the test split.
I0307 09:43:34.679689 140178276385984 submission_runner.py:469] Time since start: 26455.72s, 	Step: 63193, 	{'train/accuracy': 0.6750239133834839, 'train/loss': 1.3740328550338745, 'validation/accuracy': 0.6272000074386597, 'validation/loss': 1.6051738262176514, 'validation/num_examples': 50000, 'test/accuracy': 0.5029000043869019, 'test/loss': 2.2685818672180176, 'test/num_examples': 10000, 'score': 24537.16818213463, 'total_duration': 26455.723151683807, 'accumulated_submission_time': 24537.16818213463, 'accumulated_eval_time': 1906.7139070034027, 'accumulated_logging_time': 5.164235591888428}
I0307 09:43:34.790438 140021218662144 logging_writer.py:48] [63193] accumulated_eval_time=1906.71, accumulated_logging_time=5.16424, accumulated_submission_time=24537.2, global_step=63193, preemption_count=0, score=24537.2, test/accuracy=0.5029, test/loss=2.26858, test/num_examples=10000, total_duration=26455.7, train/accuracy=0.675024, train/loss=1.37403, validation/accuracy=0.6272, validation/loss=1.60517, validation/num_examples=50000
I0307 09:43:38.039014 140021210269440 logging_writer.py:48] [63200] global_step=63200, grad_norm=3.4420204162597656, loss=2.7305092811584473
I0307 09:44:16.525183 140021218662144 logging_writer.py:48] [63300] global_step=63300, grad_norm=3.121704339981079, loss=2.638185501098633
I0307 09:44:55.629525 140021210269440 logging_writer.py:48] [63400] global_step=63400, grad_norm=3.2577271461486816, loss=2.776226282119751
I0307 09:45:34.536899 140021218662144 logging_writer.py:48] [63500] global_step=63500, grad_norm=3.1267502307891846, loss=2.6622188091278076
I0307 09:46:13.662896 140021210269440 logging_writer.py:48] [63600] global_step=63600, grad_norm=3.6965856552124023, loss=2.6046767234802246
I0307 09:46:53.774787 140021218662144 logging_writer.py:48] [63700] global_step=63700, grad_norm=3.1068665981292725, loss=2.7092299461364746
I0307 09:47:33.164143 140021210269440 logging_writer.py:48] [63800] global_step=63800, grad_norm=2.9497790336608887, loss=2.6237525939941406
I0307 09:48:12.605930 140021218662144 logging_writer.py:48] [63900] global_step=63900, grad_norm=3.1717495918273926, loss=2.568462371826172
I0307 09:48:51.492346 140021210269440 logging_writer.py:48] [64000] global_step=64000, grad_norm=2.957498788833618, loss=2.6391360759735107
I0307 09:49:30.362692 140021218662144 logging_writer.py:48] [64100] global_step=64100, grad_norm=3.4422731399536133, loss=2.729450225830078
I0307 09:50:09.241964 140021210269440 logging_writer.py:48] [64200] global_step=64200, grad_norm=3.0872111320495605, loss=2.6531219482421875
I0307 09:50:47.916865 140021218662144 logging_writer.py:48] [64300] global_step=64300, grad_norm=3.7308309078216553, loss=2.705822229385376
I0307 09:51:26.883952 140021210269440 logging_writer.py:48] [64400] global_step=64400, grad_norm=3.406735897064209, loss=2.7408342361450195
I0307 09:52:04.683137 140178276385984 spec.py:321] Evaluating on the training split.
I0307 09:52:16.878665 140178276385984 spec.py:333] Evaluating on the validation split.
I0307 09:52:36.099384 140178276385984 spec.py:349] Evaluating on the test split.
I0307 09:52:37.837733 140178276385984 submission_runner.py:469] Time since start: 26998.88s, 	Step: 64499, 	{'train/accuracy': 0.6745654940605164, 'train/loss': 1.4092012643814087, 'validation/accuracy': 0.6245399713516235, 'validation/loss': 1.6473835706710815, 'validation/num_examples': 50000, 'test/accuracy': 0.5029000043869019, 'test/loss': 2.3262922763824463, 'test/num_examples': 10000, 'score': 25046.89953494072, 'total_duration': 26998.881210803986, 'accumulated_submission_time': 25046.89953494072, 'accumulated_eval_time': 1939.8683335781097, 'accumulated_logging_time': 5.304022312164307}
I0307 09:52:37.929570 140021218662144 logging_writer.py:48] [64499] accumulated_eval_time=1939.87, accumulated_logging_time=5.30402, accumulated_submission_time=25046.9, global_step=64499, preemption_count=0, score=25046.9, test/accuracy=0.5029, test/loss=2.32629, test/num_examples=10000, total_duration=26998.9, train/accuracy=0.674565, train/loss=1.4092, validation/accuracy=0.62454, validation/loss=1.64738, validation/num_examples=50000
I0307 09:52:38.764839 140021210269440 logging_writer.py:48] [64500] global_step=64500, grad_norm=3.3313074111938477, loss=2.6550779342651367
I0307 09:53:17.176295 140021218662144 logging_writer.py:48] [64600] global_step=64600, grad_norm=3.0680384635925293, loss=2.7021119594573975
I0307 09:53:55.959959 140021210269440 logging_writer.py:48] [64700] global_step=64700, grad_norm=3.191710948944092, loss=2.720255136489868
I0307 09:54:35.199752 140021218662144 logging_writer.py:48] [64800] global_step=64800, grad_norm=3.3435230255126953, loss=2.6982789039611816
I0307 09:55:14.201374 140021210269440 logging_writer.py:48] [64900] global_step=64900, grad_norm=3.006774425506592, loss=2.685908794403076
I0307 09:55:53.042914 140021218662144 logging_writer.py:48] [65000] global_step=65000, grad_norm=3.2403767108917236, loss=2.714154005050659
I0307 09:56:32.418835 140021210269440 logging_writer.py:48] [65100] global_step=65100, grad_norm=3.391430139541626, loss=2.665672540664673
I0307 09:57:11.386847 140021218662144 logging_writer.py:48] [65200] global_step=65200, grad_norm=3.0783157348632812, loss=2.578396797180176
I0307 09:57:50.020297 140021210269440 logging_writer.py:48] [65300] global_step=65300, grad_norm=3.7919068336486816, loss=2.6740567684173584
I0307 09:58:28.687324 140021218662144 logging_writer.py:48] [65400] global_step=65400, grad_norm=3.3088839054107666, loss=2.6526412963867188
I0307 09:59:07.821841 140021210269440 logging_writer.py:48] [65500] global_step=65500, grad_norm=3.401639223098755, loss=2.755898952484131
I0307 09:59:45.928471 140021218662144 logging_writer.py:48] [65600] global_step=65600, grad_norm=3.2627809047698975, loss=2.7005457878112793
I0307 10:00:24.405128 140021210269440 logging_writer.py:48] [65700] global_step=65700, grad_norm=3.1551249027252197, loss=2.6846864223480225
I0307 10:01:02.834926 140021218662144 logging_writer.py:48] [65800] global_step=65800, grad_norm=3.491610288619995, loss=2.731820821762085
I0307 10:01:08.139390 140178276385984 spec.py:321] Evaluating on the training split.
I0307 10:01:20.070493 140178276385984 spec.py:333] Evaluating on the validation split.
I0307 10:01:40.710071 140178276385984 spec.py:349] Evaluating on the test split.
I0307 10:01:42.454515 140178276385984 submission_runner.py:469] Time since start: 27543.50s, 	Step: 65815, 	{'train/accuracy': 0.6670320630073547, 'train/loss': 1.4286961555480957, 'validation/accuracy': 0.6154599785804749, 'validation/loss': 1.6613672971725464, 'validation/num_examples': 50000, 'test/accuracy': 0.49710002541542053, 'test/loss': 2.3397810459136963, 'test/num_examples': 10000, 'score': 25556.913395404816, 'total_duration': 27543.49799132347, 'accumulated_submission_time': 25556.913395404816, 'accumulated_eval_time': 1974.1832756996155, 'accumulated_logging_time': 5.456932783126831}
I0307 10:01:42.587631 140021210269440 logging_writer.py:48] [65815] accumulated_eval_time=1974.18, accumulated_logging_time=5.45693, accumulated_submission_time=25556.9, global_step=65815, preemption_count=0, score=25556.9, test/accuracy=0.4971, test/loss=2.33978, test/num_examples=10000, total_duration=27543.5, train/accuracy=0.667032, train/loss=1.4287, validation/accuracy=0.61546, validation/loss=1.66137, validation/num_examples=50000
I0307 10:02:16.031246 140021218662144 logging_writer.py:48] [65900] global_step=65900, grad_norm=3.1045827865600586, loss=2.6813831329345703
I0307 10:02:54.732054 140021210269440 logging_writer.py:48] [66000] global_step=66000, grad_norm=3.1994993686676025, loss=2.6578736305236816
I0307 10:03:33.384323 140021218662144 logging_writer.py:48] [66100] global_step=66100, grad_norm=3.1732070446014404, loss=2.73258638381958
I0307 10:04:13.558455 140021210269440 logging_writer.py:48] [66200] global_step=66200, grad_norm=3.2975752353668213, loss=2.6939592361450195
I0307 10:04:52.989025 140021218662144 logging_writer.py:48] [66300] global_step=66300, grad_norm=3.070277452468872, loss=2.6464827060699463
I0307 10:05:32.080685 140021210269440 logging_writer.py:48] [66400] global_step=66400, grad_norm=2.872957944869995, loss=2.6567025184631348
I0307 10:06:10.776747 140021218662144 logging_writer.py:48] [66500] global_step=66500, grad_norm=3.15445613861084, loss=2.839569330215454
I0307 10:06:49.845392 140021210269440 logging_writer.py:48] [66600] global_step=66600, grad_norm=3.706970453262329, loss=2.6895411014556885
I0307 10:07:28.417872 140021218662144 logging_writer.py:48] [66700] global_step=66700, grad_norm=3.3322975635528564, loss=2.6168062686920166
I0307 10:08:07.437456 140021210269440 logging_writer.py:48] [66800] global_step=66800, grad_norm=3.26613450050354, loss=2.69838285446167
I0307 10:08:46.371405 140021218662144 logging_writer.py:48] [66900] global_step=66900, grad_norm=3.1591789722442627, loss=2.7519783973693848
I0307 10:09:25.266442 140021210269440 logging_writer.py:48] [67000] global_step=67000, grad_norm=3.2439403533935547, loss=2.7186026573181152
I0307 10:10:03.826909 140021218662144 logging_writer.py:48] [67100] global_step=67100, grad_norm=3.5446770191192627, loss=2.6818087100982666
I0307 10:10:12.697152 140178276385984 spec.py:321] Evaluating on the training split.
I0307 10:10:24.385420 140178276385984 spec.py:333] Evaluating on the validation split.
I0307 10:10:42.669683 140178276385984 spec.py:349] Evaluating on the test split.
I0307 10:10:44.851562 140178276385984 submission_runner.py:469] Time since start: 28085.90s, 	Step: 67124, 	{'train/accuracy': 0.6716158986091614, 'train/loss': 1.4147002696990967, 'validation/accuracy': 0.6239399909973145, 'validation/loss': 1.6442785263061523, 'validation/num_examples': 50000, 'test/accuracy': 0.5065000057220459, 'test/loss': 2.2860329151153564, 'test/num_examples': 10000, 'score': 26066.837782144547, 'total_duration': 28085.895067214966, 'accumulated_submission_time': 26066.837782144547, 'accumulated_eval_time': 2006.33753824234, 'accumulated_logging_time': 5.6403584480285645}
I0307 10:10:44.948592 140021210269440 logging_writer.py:48] [67124] accumulated_eval_time=2006.34, accumulated_logging_time=5.64036, accumulated_submission_time=26066.8, global_step=67124, preemption_count=0, score=26066.8, test/accuracy=0.5065, test/loss=2.28603, test/num_examples=10000, total_duration=28085.9, train/accuracy=0.671616, train/loss=1.4147, validation/accuracy=0.62394, validation/loss=1.64428, validation/num_examples=50000
I0307 10:11:14.720620 140021218662144 logging_writer.py:48] [67200] global_step=67200, grad_norm=3.272304058074951, loss=2.7891762256622314
I0307 10:11:53.162724 140021210269440 logging_writer.py:48] [67300] global_step=67300, grad_norm=3.1265511512756348, loss=2.670032501220703
I0307 10:12:32.379397 140021218662144 logging_writer.py:48] [67400] global_step=67400, grad_norm=3.1070361137390137, loss=2.68833589553833
I0307 10:13:12.121698 140021210269440 logging_writer.py:48] [67500] global_step=67500, grad_norm=3.2239010334014893, loss=2.7730958461761475
I0307 10:13:51.398226 140021218662144 logging_writer.py:48] [67600] global_step=67600, grad_norm=3.4126393795013428, loss=2.734096050262451
I0307 10:14:30.271821 140021210269440 logging_writer.py:48] [67700] global_step=67700, grad_norm=3.3965816497802734, loss=2.6967620849609375
I0307 10:15:08.917861 140021218662144 logging_writer.py:48] [67800] global_step=67800, grad_norm=3.2926857471466064, loss=2.669408082962036
I0307 10:15:47.592216 140021210269440 logging_writer.py:48] [67900] global_step=67900, grad_norm=3.331066131591797, loss=2.697503089904785
I0307 10:16:26.187883 140021218662144 logging_writer.py:48] [68000] global_step=68000, grad_norm=3.263772487640381, loss=2.7343955039978027
I0307 10:17:05.406698 140021210269440 logging_writer.py:48] [68100] global_step=68100, grad_norm=3.619426965713501, loss=2.7322216033935547
I0307 10:17:44.039807 140021218662144 logging_writer.py:48] [68200] global_step=68200, grad_norm=3.24770450592041, loss=2.6047415733337402
I0307 10:18:22.777117 140021210269440 logging_writer.py:48] [68300] global_step=68300, grad_norm=2.908094644546509, loss=2.6360113620758057
I0307 10:19:02.181897 140021218662144 logging_writer.py:48] [68400] global_step=68400, grad_norm=3.262312412261963, loss=2.70300555229187
I0307 10:19:14.852994 140178276385984 spec.py:321] Evaluating on the training split.
I0307 10:19:26.264373 140178276385984 spec.py:333] Evaluating on the validation split.
I0307 10:19:50.325505 140178276385984 spec.py:349] Evaluating on the test split.
I0307 10:19:52.060798 140178276385984 submission_runner.py:469] Time since start: 28633.10s, 	Step: 68434, 	{'train/accuracy': 0.6814811825752258, 'train/loss': 1.375356674194336, 'validation/accuracy': 0.6348400115966797, 'validation/loss': 1.5984749794006348, 'validation/num_examples': 50000, 'test/accuracy': 0.5063000321388245, 'test/loss': 2.266976833343506, 'test/num_examples': 10000, 'score': 26576.572456598282, 'total_duration': 28633.104213237762, 'accumulated_submission_time': 26576.572456598282, 'accumulated_eval_time': 2043.5450999736786, 'accumulated_logging_time': 5.771882772445679}
I0307 10:19:52.137209 140021210269440 logging_writer.py:48] [68434] accumulated_eval_time=2043.55, accumulated_logging_time=5.77188, accumulated_submission_time=26576.6, global_step=68434, preemption_count=0, score=26576.6, test/accuracy=0.5063, test/loss=2.26698, test/num_examples=10000, total_duration=28633.1, train/accuracy=0.681481, train/loss=1.37536, validation/accuracy=0.63484, validation/loss=1.59847, validation/num_examples=50000
I0307 10:20:18.314247 140021218662144 logging_writer.py:48] [68500] global_step=68500, grad_norm=3.443086624145508, loss=2.6577346324920654
I0307 10:20:57.267455 140021210269440 logging_writer.py:48] [68600] global_step=68600, grad_norm=2.8966574668884277, loss=2.580456018447876
I0307 10:21:37.106429 140021218662144 logging_writer.py:48] [68700] global_step=68700, grad_norm=3.427494764328003, loss=2.695953130722046
I0307 10:22:16.399944 140021210269440 logging_writer.py:48] [68800] global_step=68800, grad_norm=3.153193235397339, loss=2.6486148834228516
I0307 10:22:55.679951 140021218662144 logging_writer.py:48] [68900] global_step=68900, grad_norm=3.173302173614502, loss=2.7185301780700684
I0307 10:23:34.250684 140021210269440 logging_writer.py:48] [69000] global_step=69000, grad_norm=3.50903058052063, loss=2.717404365539551
I0307 10:24:12.859745 140021218662144 logging_writer.py:48] [69100] global_step=69100, grad_norm=2.9644808769226074, loss=2.6037216186523438
I0307 10:24:51.632641 140021210269440 logging_writer.py:48] [69200] global_step=69200, grad_norm=3.544466495513916, loss=2.7589492797851562
I0307 10:25:30.529276 140021218662144 logging_writer.py:48] [69300] global_step=69300, grad_norm=3.582883834838867, loss=2.645399570465088
I0307 10:26:09.396189 140021210269440 logging_writer.py:48] [69400] global_step=69400, grad_norm=2.939920663833618, loss=2.706876516342163
I0307 10:26:48.441605 140021218662144 logging_writer.py:48] [69500] global_step=69500, grad_norm=3.323565721511841, loss=2.5468995571136475
I0307 10:27:27.395968 140021210269440 logging_writer.py:48] [69600] global_step=69600, grad_norm=3.155299425125122, loss=2.60162353515625
I0307 10:28:06.405413 140021218662144 logging_writer.py:48] [69700] global_step=69700, grad_norm=3.940291404724121, loss=2.671382427215576
I0307 10:28:22.240727 140178276385984 spec.py:321] Evaluating on the training split.
I0307 10:28:34.263756 140178276385984 spec.py:333] Evaluating on the validation split.
I0307 10:28:53.167666 140178276385984 spec.py:349] Evaluating on the test split.
I0307 10:28:54.892369 140178276385984 submission_runner.py:469] Time since start: 29175.94s, 	Step: 69742, 	{'train/accuracy': 0.6739277839660645, 'train/loss': 1.4021824598312378, 'validation/accuracy': 0.6243199706077576, 'validation/loss': 1.6376807689666748, 'validation/num_examples': 50000, 'test/accuracy': 0.4970000088214874, 'test/loss': 2.3001816272735596, 'test/num_examples': 10000, 'score': 27086.497886657715, 'total_duration': 29175.9358522892, 'accumulated_submission_time': 27086.497886657715, 'accumulated_eval_time': 2076.1965646743774, 'accumulated_logging_time': 5.893910884857178}
I0307 10:28:54.999781 140021210269440 logging_writer.py:48] [69742] accumulated_eval_time=2076.2, accumulated_logging_time=5.89391, accumulated_submission_time=27086.5, global_step=69742, preemption_count=0, score=27086.5, test/accuracy=0.497, test/loss=2.30018, test/num_examples=10000, total_duration=29175.9, train/accuracy=0.673928, train/loss=1.40218, validation/accuracy=0.62432, validation/loss=1.63768, validation/num_examples=50000
I0307 10:29:18.053791 140021218662144 logging_writer.py:48] [69800] global_step=69800, grad_norm=3.3573622703552246, loss=2.752201557159424
I0307 10:29:57.071881 140021210269440 logging_writer.py:48] [69900] global_step=69900, grad_norm=2.9780426025390625, loss=2.6748852729797363
I0307 10:30:36.246115 140021218662144 logging_writer.py:48] [70000] global_step=70000, grad_norm=3.0371477603912354, loss=2.6179304122924805
I0307 10:31:15.801882 140021210269440 logging_writer.py:48] [70100] global_step=70100, grad_norm=2.87045955657959, loss=2.7057440280914307
I0307 10:31:54.777764 140021218662144 logging_writer.py:48] [70200] global_step=70200, grad_norm=3.664562940597534, loss=2.7442221641540527
I0307 10:32:33.644576 140021210269440 logging_writer.py:48] [70300] global_step=70300, grad_norm=3.419532537460327, loss=2.757719039916992
I0307 10:33:12.591292 140021218662144 logging_writer.py:48] [70400] global_step=70400, grad_norm=4.56001615524292, loss=2.5918354988098145
I0307 10:33:51.245491 140021210269440 logging_writer.py:48] [70500] global_step=70500, grad_norm=3.5352134704589844, loss=2.606107711791992
I0307 10:34:29.929358 140021218662144 logging_writer.py:48] [70600] global_step=70600, grad_norm=3.558659553527832, loss=2.678074598312378
I0307 10:35:08.722833 140021210269440 logging_writer.py:48] [70700] global_step=70700, grad_norm=3.4455883502960205, loss=2.6320743560791016
I0307 10:35:47.372461 140021218662144 logging_writer.py:48] [70800] global_step=70800, grad_norm=3.365556478500366, loss=2.7276902198791504
I0307 10:36:26.307367 140021210269440 logging_writer.py:48] [70900] global_step=70900, grad_norm=3.409961462020874, loss=2.6211845874786377
I0307 10:37:05.123788 140021218662144 logging_writer.py:48] [71000] global_step=71000, grad_norm=3.304882526397705, loss=2.6604130268096924
I0307 10:37:25.123617 140178276385984 spec.py:321] Evaluating on the training split.
I0307 10:37:36.169798 140178276385984 spec.py:333] Evaluating on the validation split.
I0307 10:37:59.872437 140178276385984 spec.py:349] Evaluating on the test split.
I0307 10:38:01.618591 140178276385984 submission_runner.py:469] Time since start: 29722.66s, 	Step: 71053, 	{'train/accuracy': 0.6735690236091614, 'train/loss': 1.3958847522735596, 'validation/accuracy': 0.6253199577331543, 'validation/loss': 1.6287626028060913, 'validation/num_examples': 50000, 'test/accuracy': 0.49810001254081726, 'test/loss': 2.2989299297332764, 'test/num_examples': 10000, 'score': 27596.4241232872, 'total_duration': 29722.662075042725, 'accumulated_submission_time': 27596.4241232872, 'accumulated_eval_time': 2112.6913816928864, 'accumulated_logging_time': 6.063194513320923}
I0307 10:38:01.720325 140021210269440 logging_writer.py:48] [71053] accumulated_eval_time=2112.69, accumulated_logging_time=6.06319, accumulated_submission_time=27596.4, global_step=71053, preemption_count=0, score=27596.4, test/accuracy=0.4981, test/loss=2.29893, test/num_examples=10000, total_duration=29722.7, train/accuracy=0.673569, train/loss=1.39588, validation/accuracy=0.62532, validation/loss=1.62876, validation/num_examples=50000
I0307 10:38:20.485880 140021218662144 logging_writer.py:48] [71100] global_step=71100, grad_norm=3.700582265853882, loss=2.6014466285705566
I0307 10:39:00.133011 140021210269440 logging_writer.py:48] [71200] global_step=71200, grad_norm=3.3113603591918945, loss=2.723374843597412
I0307 10:39:39.467257 140021218662144 logging_writer.py:48] [71300] global_step=71300, grad_norm=3.1096243858337402, loss=2.5624313354492188
I0307 10:40:19.087559 140021210269440 logging_writer.py:48] [71400] global_step=71400, grad_norm=3.0135347843170166, loss=2.682126522064209
I0307 10:40:57.643769 140021218662144 logging_writer.py:48] [71500] global_step=71500, grad_norm=3.2438673973083496, loss=2.6608734130859375
I0307 10:41:36.306096 140021210269440 logging_writer.py:48] [71600] global_step=71600, grad_norm=3.009809970855713, loss=2.6610753536224365
I0307 10:42:14.990456 140021218662144 logging_writer.py:48] [71700] global_step=71700, grad_norm=3.2361693382263184, loss=2.6419668197631836
I0307 10:42:53.618327 140021210269440 logging_writer.py:48] [71800] global_step=71800, grad_norm=3.1683053970336914, loss=2.5528998374938965
I0307 10:43:32.425087 140021218662144 logging_writer.py:48] [71900] global_step=71900, grad_norm=3.1480915546417236, loss=2.6770713329315186
I0307 10:44:11.220623 140021210269440 logging_writer.py:48] [72000] global_step=72000, grad_norm=3.254741668701172, loss=2.7779576778411865
I0307 10:44:50.153599 140021218662144 logging_writer.py:48] [72100] global_step=72100, grad_norm=3.3629915714263916, loss=2.595019817352295
I0307 10:45:28.637903 140021210269440 logging_writer.py:48] [72200] global_step=72200, grad_norm=3.2010858058929443, loss=2.6666316986083984
I0307 10:46:07.256263 140021218662144 logging_writer.py:48] [72300] global_step=72300, grad_norm=3.889723062515259, loss=2.6495048999786377
I0307 10:46:31.916131 140178276385984 spec.py:321] Evaluating on the training split.
I0307 10:46:44.167524 140178276385984 spec.py:333] Evaluating on the validation split.
I0307 10:47:06.095447 140178276385984 spec.py:349] Evaluating on the test split.
I0307 10:47:07.843708 140178276385984 submission_runner.py:469] Time since start: 30268.89s, 	Step: 72365, 	{'train/accuracy': 0.6788703799247742, 'train/loss': 1.4000656604766846, 'validation/accuracy': 0.6317600011825562, 'validation/loss': 1.621607780456543, 'validation/num_examples': 50000, 'test/accuracy': 0.5060000419616699, 'test/loss': 2.3053808212280273, 'test/num_examples': 10000, 'score': 28106.429681777954, 'total_duration': 30268.88716864586, 'accumulated_submission_time': 28106.429681777954, 'accumulated_eval_time': 2148.618767261505, 'accumulated_logging_time': 6.223567008972168}
I0307 10:47:07.952730 140021210269440 logging_writer.py:48] [72365] accumulated_eval_time=2148.62, accumulated_logging_time=6.22357, accumulated_submission_time=28106.4, global_step=72365, preemption_count=0, score=28106.4, test/accuracy=0.506, test/loss=2.30538, test/num_examples=10000, total_duration=30268.9, train/accuracy=0.67887, train/loss=1.40007, validation/accuracy=0.63176, validation/loss=1.62161, validation/num_examples=50000
I0307 10:47:22.434441 140021218662144 logging_writer.py:48] [72400] global_step=72400, grad_norm=3.759146213531494, loss=2.6779870986938477
I0307 10:48:01.785943 140021210269440 logging_writer.py:48] [72500] global_step=72500, grad_norm=3.139263868331909, loss=2.625171422958374
I0307 10:48:41.194222 140021218662144 logging_writer.py:48] [72600] global_step=72600, grad_norm=3.564016103744507, loss=2.6697938442230225
I0307 10:49:20.256121 140021210269440 logging_writer.py:48] [72700] global_step=72700, grad_norm=2.9870011806488037, loss=2.6548075675964355
I0307 10:49:58.701735 140021218662144 logging_writer.py:48] [72800] global_step=72800, grad_norm=3.248133420944214, loss=2.7075843811035156
I0307 10:50:37.496136 140021210269440 logging_writer.py:48] [72900] global_step=72900, grad_norm=3.2588369846343994, loss=2.6464858055114746
I0307 10:51:16.392785 140021218662144 logging_writer.py:48] [73000] global_step=73000, grad_norm=3.383927583694458, loss=2.5899477005004883
I0307 10:51:55.408404 140021210269440 logging_writer.py:48] [73100] global_step=73100, grad_norm=3.1328983306884766, loss=2.662198066711426
I0307 10:52:33.767075 140021218662144 logging_writer.py:48] [73200] global_step=73200, grad_norm=3.204167127609253, loss=2.6138193607330322
I0307 10:53:13.037503 140021210269440 logging_writer.py:48] [73300] global_step=73300, grad_norm=3.036860466003418, loss=2.636279344558716
I0307 10:53:51.572766 140021218662144 logging_writer.py:48] [73400] global_step=73400, grad_norm=3.0277037620544434, loss=2.6570358276367188
I0307 10:54:29.864348 140021210269440 logging_writer.py:48] [73500] global_step=73500, grad_norm=3.4436209201812744, loss=2.5875494480133057
I0307 10:55:08.136507 140021218662144 logging_writer.py:48] [73600] global_step=73600, grad_norm=3.252950668334961, loss=2.6755640506744385
I0307 10:55:38.163453 140178276385984 spec.py:321] Evaluating on the training split.
I0307 10:55:50.474987 140178276385984 spec.py:333] Evaluating on the validation split.
I0307 10:56:12.290971 140178276385984 spec.py:349] Evaluating on the test split.
I0307 10:56:14.035867 140178276385984 submission_runner.py:469] Time since start: 30815.08s, 	Step: 73678, 	{'train/accuracy': 0.6807637214660645, 'train/loss': 1.412006139755249, 'validation/accuracy': 0.6336399912834167, 'validation/loss': 1.632641315460205, 'validation/num_examples': 50000, 'test/accuracy': 0.5109000205993652, 'test/loss': 2.3001463413238525, 'test/num_examples': 10000, 'score': 28616.465846061707, 'total_duration': 30815.07935500145, 'accumulated_submission_time': 28616.465846061707, 'accumulated_eval_time': 2184.4910113811493, 'accumulated_logging_time': 6.375152826309204}
I0307 10:56:14.165678 140021210269440 logging_writer.py:48] [73678] accumulated_eval_time=2184.49, accumulated_logging_time=6.37515, accumulated_submission_time=28616.5, global_step=73678, preemption_count=0, score=28616.5, test/accuracy=0.5109, test/loss=2.30015, test/num_examples=10000, total_duration=30815.1, train/accuracy=0.680764, train/loss=1.41201, validation/accuracy=0.63364, validation/loss=1.63264, validation/num_examples=50000
I0307 10:56:23.043206 140021218662144 logging_writer.py:48] [73700] global_step=73700, grad_norm=3.1576340198516846, loss=2.7320845127105713
I0307 10:57:02.105383 140021210269440 logging_writer.py:48] [73800] global_step=73800, grad_norm=3.030881881713867, loss=2.6111972332000732
I0307 10:57:41.313206 140021218662144 logging_writer.py:48] [73900] global_step=73900, grad_norm=3.2963035106658936, loss=2.6595566272735596
I0307 10:58:20.157641 140021210269440 logging_writer.py:48] [74000] global_step=74000, grad_norm=3.2702128887176514, loss=2.579272747039795
I0307 10:58:59.397362 140021218662144 logging_writer.py:48] [74100] global_step=74100, grad_norm=3.8201470375061035, loss=2.6237499713897705
I0307 10:59:38.078675 140021210269440 logging_writer.py:48] [74200] global_step=74200, grad_norm=3.4695591926574707, loss=2.592921257019043
I0307 11:00:17.315186 140021218662144 logging_writer.py:48] [74300] global_step=74300, grad_norm=3.5692005157470703, loss=2.550651788711548
I0307 11:00:56.024456 140021210269440 logging_writer.py:48] [74400] global_step=74400, grad_norm=3.0687055587768555, loss=2.6772937774658203
I0307 11:01:34.281612 140021218662144 logging_writer.py:48] [74500] global_step=74500, grad_norm=3.1863596439361572, loss=2.627950668334961
I0307 11:02:13.266695 140021210269440 logging_writer.py:48] [74600] global_step=74600, grad_norm=3.573415756225586, loss=2.656651020050049
I0307 11:02:51.539669 140021218662144 logging_writer.py:48] [74700] global_step=74700, grad_norm=3.4043543338775635, loss=2.714385509490967
I0307 11:03:30.763335 140021210269440 logging_writer.py:48] [74800] global_step=74800, grad_norm=3.3548741340637207, loss=2.6420254707336426
I0307 11:04:09.714803 140021218662144 logging_writer.py:48] [74900] global_step=74900, grad_norm=3.0931105613708496, loss=2.5997447967529297
I0307 11:04:44.598717 140178276385984 spec.py:321] Evaluating on the training split.
I0307 11:04:57.015040 140178276385984 spec.py:333] Evaluating on the validation split.
I0307 11:05:17.919959 140178276385984 spec.py:349] Evaluating on the test split.
I0307 11:05:19.688516 140178276385984 submission_runner.py:469] Time since start: 31360.73s, 	Step: 74960, 	{'train/accuracy': 0.6818598508834839, 'train/loss': 1.3522303104400635, 'validation/accuracy': 0.6339399814605713, 'validation/loss': 1.579234004020691, 'validation/num_examples': 50000, 'test/accuracy': 0.5148000121116638, 'test/loss': 2.217301607131958, 'test/num_examples': 10000, 'score': 29126.742819070816, 'total_duration': 31360.731994628906, 'accumulated_submission_time': 29126.742819070816, 'accumulated_eval_time': 2219.580649614334, 'accumulated_logging_time': 6.529333591461182}
I0307 11:05:19.783971 140021210269440 logging_writer.py:48] [74960] accumulated_eval_time=2219.58, accumulated_logging_time=6.52933, accumulated_submission_time=29126.7, global_step=74960, preemption_count=0, score=29126.7, test/accuracy=0.5148, test/loss=2.2173, test/num_examples=10000, total_duration=31360.7, train/accuracy=0.68186, train/loss=1.35223, validation/accuracy=0.63394, validation/loss=1.57923, validation/num_examples=50000
I0307 11:05:35.590278 140021218662144 logging_writer.py:48] [75000] global_step=75000, grad_norm=3.5197486877441406, loss=2.715115547180176
I0307 11:06:14.160490 140021210269440 logging_writer.py:48] [75100] global_step=75100, grad_norm=3.5902481079101562, loss=2.704763889312744
I0307 11:06:52.403504 140021218662144 logging_writer.py:48] [75200] global_step=75200, grad_norm=3.0995335578918457, loss=2.6810736656188965
I0307 11:07:31.334526 140021210269440 logging_writer.py:48] [75300] global_step=75300, grad_norm=3.3498215675354004, loss=2.5768706798553467
I0307 11:08:09.911425 140021218662144 logging_writer.py:48] [75400] global_step=75400, grad_norm=3.095742702484131, loss=2.632042407989502
I0307 11:08:48.675749 140021210269440 logging_writer.py:48] [75500] global_step=75500, grad_norm=2.9986371994018555, loss=2.6113526821136475
I0307 11:09:27.138844 140021218662144 logging_writer.py:48] [75600] global_step=75600, grad_norm=3.275862216949463, loss=2.6034672260284424
I0307 11:10:05.994714 140021210269440 logging_writer.py:48] [75700] global_step=75700, grad_norm=3.411268472671509, loss=2.6405603885650635
I0307 11:10:44.477018 140021218662144 logging_writer.py:48] [75800] global_step=75800, grad_norm=3.4515810012817383, loss=2.622069835662842
I0307 11:11:23.123942 140021210269440 logging_writer.py:48] [75900] global_step=75900, grad_norm=3.341289758682251, loss=2.58364200592041
I0307 11:12:02.013036 140021218662144 logging_writer.py:48] [76000] global_step=76000, grad_norm=3.630063056945801, loss=2.7107560634613037
I0307 11:12:40.696698 140021210269440 logging_writer.py:48] [76100] global_step=76100, grad_norm=3.38493275642395, loss=2.5693628787994385
I0307 11:13:19.655871 140021218662144 logging_writer.py:48] [76200] global_step=76200, grad_norm=3.7040607929229736, loss=2.664565324783325
I0307 11:13:50.041798 140178276385984 spec.py:321] Evaluating on the training split.
I0307 11:14:02.018722 140178276385984 spec.py:333] Evaluating on the validation split.
I0307 11:14:29.662049 140178276385984 spec.py:349] Evaluating on the test split.
I0307 11:14:31.398694 140178276385984 submission_runner.py:469] Time since start: 31912.44s, 	Step: 76280, 	{'train/accuracy': 0.692402720451355, 'train/loss': 1.3357133865356445, 'validation/accuracy': 0.6426999568939209, 'validation/loss': 1.573549509048462, 'validation/num_examples': 50000, 'test/accuracy': 0.5139000415802002, 'test/loss': 2.238517999649048, 'test/num_examples': 10000, 'score': 29636.832682847977, 'total_duration': 31912.44214630127, 'accumulated_submission_time': 29636.832682847977, 'accumulated_eval_time': 2260.9373409748077, 'accumulated_logging_time': 6.660996913909912}
I0307 11:14:31.454078 140021210269440 logging_writer.py:48] [76280] accumulated_eval_time=2260.94, accumulated_logging_time=6.661, accumulated_submission_time=29636.8, global_step=76280, preemption_count=0, score=29636.8, test/accuracy=0.5139, test/loss=2.23852, test/num_examples=10000, total_duration=31912.4, train/accuracy=0.692403, train/loss=1.33571, validation/accuracy=0.6427, validation/loss=1.57355, validation/num_examples=50000
I0307 11:14:39.781049 140021218662144 logging_writer.py:48] [76300] global_step=76300, grad_norm=3.182311773300171, loss=2.6301677227020264
I0307 11:15:18.332639 140021210269440 logging_writer.py:48] [76400] global_step=76400, grad_norm=4.2410736083984375, loss=2.70570969581604
I0307 11:15:57.139880 140021218662144 logging_writer.py:48] [76500] global_step=76500, grad_norm=3.182056188583374, loss=2.6249821186065674
I0307 11:16:36.153387 140021210269440 logging_writer.py:48] [76600] global_step=76600, grad_norm=3.4536426067352295, loss=2.604304790496826
I0307 11:17:14.884896 140021218662144 logging_writer.py:48] [76700] global_step=76700, grad_norm=3.1285831928253174, loss=2.6358554363250732
I0307 11:17:53.274257 140021210269440 logging_writer.py:48] [76800] global_step=76800, grad_norm=3.392307996749878, loss=2.5942652225494385
I0307 11:18:31.757233 140021218662144 logging_writer.py:48] [76900] global_step=76900, grad_norm=3.125917673110962, loss=2.6155340671539307
I0307 11:19:10.657597 140021210269440 logging_writer.py:48] [77000] global_step=77000, grad_norm=3.6993346214294434, loss=2.6168272495269775
I0307 11:19:49.181869 140021218662144 logging_writer.py:48] [77100] global_step=77100, grad_norm=3.856794595718384, loss=2.768867015838623
I0307 11:20:28.031849 140021210269440 logging_writer.py:48] [77200] global_step=77200, grad_norm=3.816087007522583, loss=2.674485206604004
I0307 11:21:06.587286 140021218662144 logging_writer.py:48] [77300] global_step=77300, grad_norm=3.7762160301208496, loss=2.585641384124756
I0307 11:21:44.920057 140021210269440 logging_writer.py:48] [77400] global_step=77400, grad_norm=2.9736082553863525, loss=2.632059097290039
I0307 11:22:23.671215 140021218662144 logging_writer.py:48] [77500] global_step=77500, grad_norm=3.210212469100952, loss=2.655165910720825
I0307 11:23:01.506751 140178276385984 spec.py:321] Evaluating on the training split.
I0307 11:23:13.522028 140178276385984 spec.py:333] Evaluating on the validation split.
I0307 11:23:31.068749 140178276385984 spec.py:349] Evaluating on the test split.
I0307 11:23:32.858638 140178276385984 submission_runner.py:469] Time since start: 32453.90s, 	Step: 77598, 	{'train/accuracy': 0.6854472160339355, 'train/loss': 1.3634953498840332, 'validation/accuracy': 0.6365999579429626, 'validation/loss': 1.5826818943023682, 'validation/num_examples': 50000, 'test/accuracy': 0.5142000317573547, 'test/loss': 2.2404658794403076, 'test/num_examples': 10000, 'score': 30146.72769021988, 'total_duration': 32453.902106523514, 'accumulated_submission_time': 30146.72769021988, 'accumulated_eval_time': 2292.289056301117, 'accumulated_logging_time': 6.738872289657593}
I0307 11:23:32.989033 140021210269440 logging_writer.py:48] [77598] accumulated_eval_time=2292.29, accumulated_logging_time=6.73887, accumulated_submission_time=30146.7, global_step=77598, preemption_count=0, score=30146.7, test/accuracy=0.5142, test/loss=2.24047, test/num_examples=10000, total_duration=32453.9, train/accuracy=0.685447, train/loss=1.3635, validation/accuracy=0.6366, validation/loss=1.58268, validation/num_examples=50000
I0307 11:23:34.184509 140021218662144 logging_writer.py:48] [77600] global_step=77600, grad_norm=3.3855252265930176, loss=2.6294071674346924
I0307 11:24:12.953855 140021210269440 logging_writer.py:48] [77700] global_step=77700, grad_norm=3.3390328884124756, loss=2.6074717044830322
I0307 11:24:51.704941 140021218662144 logging_writer.py:48] [77800] global_step=77800, grad_norm=3.412594795227051, loss=2.7219080924987793
I0307 11:25:30.603984 140021210269440 logging_writer.py:48] [77900] global_step=77900, grad_norm=3.41190767288208, loss=2.67396879196167
I0307 11:26:09.438149 140021218662144 logging_writer.py:48] [78000] global_step=78000, grad_norm=3.2909200191497803, loss=2.565606117248535
I0307 11:26:48.120144 140021210269440 logging_writer.py:48] [78100] global_step=78100, grad_norm=3.6418585777282715, loss=2.589566469192505
I0307 11:27:26.945191 140021218662144 logging_writer.py:48] [78200] global_step=78200, grad_norm=3.6894922256469727, loss=2.609210968017578
I0307 11:28:05.505486 140021210269440 logging_writer.py:48] [78300] global_step=78300, grad_norm=3.2541346549987793, loss=2.586656332015991
I0307 11:28:44.396677 140021218662144 logging_writer.py:48] [78400] global_step=78400, grad_norm=3.3065638542175293, loss=2.6212289333343506
I0307 11:29:23.247286 140021210269440 logging_writer.py:48] [78500] global_step=78500, grad_norm=3.003079891204834, loss=2.555725574493408
I0307 11:30:01.922367 140021218662144 logging_writer.py:48] [78600] global_step=78600, grad_norm=2.8656413555145264, loss=2.5717885494232178
I0307 11:30:40.825012 140021210269440 logging_writer.py:48] [78700] global_step=78700, grad_norm=3.478334426879883, loss=2.6505415439605713
I0307 11:31:19.562952 140021218662144 logging_writer.py:48] [78800] global_step=78800, grad_norm=3.793077230453491, loss=2.6501030921936035
I0307 11:31:58.639285 140021210269440 logging_writer.py:48] [78900] global_step=78900, grad_norm=3.7406861782073975, loss=2.718348741531372
I0307 11:32:02.859616 140178276385984 spec.py:321] Evaluating on the training split.
I0307 11:32:14.563643 140178276385984 spec.py:333] Evaluating on the validation split.
I0307 11:32:47.167019 140178276385984 spec.py:349] Evaluating on the test split.
I0307 11:32:48.923627 140178276385984 submission_runner.py:469] Time since start: 33009.97s, 	Step: 78912, 	{'train/accuracy': 0.6892936825752258, 'train/loss': 1.352744221687317, 'validation/accuracy': 0.6391800045967102, 'validation/loss': 1.577341079711914, 'validation/num_examples': 50000, 'test/accuracy': 0.5157000422477722, 'test/loss': 2.2470035552978516, 'test/num_examples': 10000, 'score': 30656.435901641846, 'total_duration': 33009.96707415581, 'accumulated_submission_time': 30656.435901641846, 'accumulated_eval_time': 2338.3528513908386, 'accumulated_logging_time': 6.89769172668457}
I0307 11:32:49.017190 140021218662144 logging_writer.py:48] [78912] accumulated_eval_time=2338.35, accumulated_logging_time=6.89769, accumulated_submission_time=30656.4, global_step=78912, preemption_count=0, score=30656.4, test/accuracy=0.5157, test/loss=2.247, test/num_examples=10000, total_duration=33010, train/accuracy=0.689294, train/loss=1.35274, validation/accuracy=0.63918, validation/loss=1.57734, validation/num_examples=50000
I0307 11:33:22.997768 140021210269440 logging_writer.py:48] [79000] global_step=79000, grad_norm=3.2795369625091553, loss=2.5997486114501953
I0307 11:34:01.385854 140021218662144 logging_writer.py:48] [79100] global_step=79100, grad_norm=3.830172061920166, loss=2.6694486141204834
I0307 11:34:39.539927 140021210269440 logging_writer.py:48] [79200] global_step=79200, grad_norm=2.953143358230591, loss=2.618880271911621
I0307 11:35:18.136425 140021218662144 logging_writer.py:48] [79300] global_step=79300, grad_norm=3.1785640716552734, loss=2.643899440765381
I0307 11:35:56.773840 140021210269440 logging_writer.py:48] [79400] global_step=79400, grad_norm=2.910982847213745, loss=2.612039089202881
I0307 11:36:35.269053 140021218662144 logging_writer.py:48] [79500] global_step=79500, grad_norm=3.365018606185913, loss=2.633514165878296
I0307 11:37:13.981981 140021210269440 logging_writer.py:48] [79600] global_step=79600, grad_norm=3.322937250137329, loss=2.6520273685455322
I0307 11:37:52.337898 140021218662144 logging_writer.py:48] [79700] global_step=79700, grad_norm=3.4534687995910645, loss=2.6896579265594482
I0307 11:38:31.433470 140021210269440 logging_writer.py:48] [79800] global_step=79800, grad_norm=3.5772950649261475, loss=2.769008159637451
I0307 11:39:10.390004 140021218662144 logging_writer.py:48] [79900] global_step=79900, grad_norm=3.715834617614746, loss=2.6148581504821777
I0307 11:39:49.430671 140021210269440 logging_writer.py:48] [80000] global_step=80000, grad_norm=3.7279627323150635, loss=2.630573034286499
I0307 11:40:28.180045 140021218662144 logging_writer.py:48] [80100] global_step=80100, grad_norm=3.8185923099517822, loss=2.6464548110961914
I0307 11:41:06.909059 140021210269440 logging_writer.py:48] [80200] global_step=80200, grad_norm=3.487380027770996, loss=2.656536340713501
I0307 11:41:18.966939 140178276385984 spec.py:321] Evaluating on the training split.
I0307 11:41:31.211950 140178276385984 spec.py:333] Evaluating on the validation split.
I0307 11:41:51.783205 140178276385984 spec.py:349] Evaluating on the test split.
I0307 11:41:53.547477 140178276385984 submission_runner.py:469] Time since start: 33554.59s, 	Step: 80232, 	{'train/accuracy': 0.6902303695678711, 'train/loss': 1.3328542709350586, 'validation/accuracy': 0.6419599652290344, 'validation/loss': 1.5575660467147827, 'validation/num_examples': 50000, 'test/accuracy': 0.5175000429153442, 'test/loss': 2.2019965648651123, 'test/num_examples': 10000, 'score': 31166.220273017883, 'total_duration': 33554.590924978256, 'accumulated_submission_time': 31166.220273017883, 'accumulated_eval_time': 2372.933177947998, 'accumulated_logging_time': 7.0192365646362305}
I0307 11:41:53.664050 140021218662144 logging_writer.py:48] [80232] accumulated_eval_time=2372.93, accumulated_logging_time=7.01924, accumulated_submission_time=31166.2, global_step=80232, preemption_count=0, score=31166.2, test/accuracy=0.5175, test/loss=2.202, test/num_examples=10000, total_duration=33554.6, train/accuracy=0.69023, train/loss=1.33285, validation/accuracy=0.64196, validation/loss=1.55757, validation/num_examples=50000
I0307 11:42:20.131700 140021210269440 logging_writer.py:48] [80300] global_step=80300, grad_norm=3.2054977416992188, loss=2.581535816192627
I0307 11:42:59.371340 140021218662144 logging_writer.py:48] [80400] global_step=80400, grad_norm=3.1104185581207275, loss=2.712204694747925
I0307 11:43:38.632392 140021210269440 logging_writer.py:48] [80500] global_step=80500, grad_norm=3.2681150436401367, loss=2.712286949157715
I0307 11:44:17.332197 140021218662144 logging_writer.py:48] [80600] global_step=80600, grad_norm=3.3884847164154053, loss=2.552135705947876
I0307 11:44:55.862231 140021210269440 logging_writer.py:48] [80700] global_step=80700, grad_norm=3.143756628036499, loss=2.6292059421539307
I0307 11:45:34.473532 140021218662144 logging_writer.py:48] [80800] global_step=80800, grad_norm=3.183882474899292, loss=2.6004531383514404
I0307 11:46:13.196922 140021210269440 logging_writer.py:48] [80900] global_step=80900, grad_norm=3.471304178237915, loss=2.642859935760498
I0307 11:46:51.951595 140021218662144 logging_writer.py:48] [81000] global_step=81000, grad_norm=3.2445218563079834, loss=2.6881322860717773
I0307 11:47:30.573261 140021210269440 logging_writer.py:48] [81100] global_step=81100, grad_norm=3.0547597408294678, loss=2.5619266033172607
I0307 11:48:09.311788 140021218662144 logging_writer.py:48] [81200] global_step=81200, grad_norm=3.372042179107666, loss=2.630122423171997
I0307 11:48:47.483335 140021210269440 logging_writer.py:48] [81300] global_step=81300, grad_norm=4.207265853881836, loss=2.600386142730713
I0307 11:49:26.147586 140021218662144 logging_writer.py:48] [81400] global_step=81400, grad_norm=3.349170207977295, loss=2.5418663024902344
I0307 11:50:04.479801 140021210269440 logging_writer.py:48] [81500] global_step=81500, grad_norm=3.399531602859497, loss=2.593571186065674
I0307 11:50:23.645032 140178276385984 spec.py:321] Evaluating on the training split.
I0307 11:50:35.786858 140178276385984 spec.py:333] Evaluating on the validation split.
I0307 11:50:58.656558 140178276385984 spec.py:349] Evaluating on the test split.
I0307 11:51:00.416160 140178276385984 submission_runner.py:469] Time since start: 34101.46s, 	Step: 81550, 	{'train/accuracy': 0.6890544891357422, 'train/loss': 1.3389873504638672, 'validation/accuracy': 0.6426799893379211, 'validation/loss': 1.5595519542694092, 'validation/num_examples': 50000, 'test/accuracy': 0.5101000070571899, 'test/loss': 2.2419626712799072, 'test/num_examples': 10000, 'score': 31676.05092048645, 'total_duration': 34101.459647893906, 'accumulated_submission_time': 31676.05092048645, 'accumulated_eval_time': 2409.7041358947754, 'accumulated_logging_time': 7.154574871063232}
I0307 11:51:00.501341 140021218662144 logging_writer.py:48] [81550] accumulated_eval_time=2409.7, accumulated_logging_time=7.15457, accumulated_submission_time=31676.1, global_step=81550, preemption_count=0, score=31676.1, test/accuracy=0.5101, test/loss=2.24196, test/num_examples=10000, total_duration=34101.5, train/accuracy=0.689054, train/loss=1.33899, validation/accuracy=0.64268, validation/loss=1.55955, validation/num_examples=50000
I0307 11:51:20.623474 140021210269440 logging_writer.py:48] [81600] global_step=81600, grad_norm=3.768760919570923, loss=2.7200076580047607
I0307 11:51:59.146486 140021218662144 logging_writer.py:48] [81700] global_step=81700, grad_norm=3.1793930530548096, loss=2.6437292098999023
I0307 11:52:37.993096 140021210269440 logging_writer.py:48] [81800] global_step=81800, grad_norm=3.282543659210205, loss=2.5628232955932617
I0307 11:53:16.456335 140021218662144 logging_writer.py:48] [81900] global_step=81900, grad_norm=3.579747200012207, loss=2.505758047103882
I0307 11:53:54.905841 140021210269440 logging_writer.py:48] [82000] global_step=82000, grad_norm=3.635972023010254, loss=2.6133551597595215
I0307 11:54:33.193124 140021218662144 logging_writer.py:48] [82100] global_step=82100, grad_norm=3.2556095123291016, loss=2.6673221588134766
I0307 11:55:11.986166 140021210269440 logging_writer.py:48] [82200] global_step=82200, grad_norm=3.7884256839752197, loss=2.632659912109375
I0307 11:55:50.785831 140021218662144 logging_writer.py:48] [82300] global_step=82300, grad_norm=3.4379615783691406, loss=2.5970687866210938
I0307 11:56:29.573064 140021210269440 logging_writer.py:48] [82400] global_step=82400, grad_norm=2.932488203048706, loss=2.5916025638580322
I0307 11:57:08.516499 140021218662144 logging_writer.py:48] [82500] global_step=82500, grad_norm=3.5861828327178955, loss=2.6749815940856934
I0307 11:57:47.577814 140021210269440 logging_writer.py:48] [82600] global_step=82600, grad_norm=3.6311287879943848, loss=2.5930042266845703
I0307 11:58:26.461145 140021218662144 logging_writer.py:48] [82700] global_step=82700, grad_norm=3.3639159202575684, loss=2.6233856678009033
I0307 11:59:05.295752 140021210269440 logging_writer.py:48] [82800] global_step=82800, grad_norm=3.3065743446350098, loss=2.6986303329467773
I0307 11:59:30.515251 140178276385984 spec.py:321] Evaluating on the training split.
I0307 11:59:42.733450 140178276385984 spec.py:333] Evaluating on the validation split.
I0307 12:00:02.366510 140178276385984 spec.py:349] Evaluating on the test split.
I0307 12:00:04.326698 140178276385984 submission_runner.py:469] Time since start: 34645.37s, 	Step: 82866, 	{'train/accuracy': 0.6954519748687744, 'train/loss': 1.317766785621643, 'validation/accuracy': 0.643559992313385, 'validation/loss': 1.5462788343429565, 'validation/num_examples': 50000, 'test/accuracy': 0.5209000110626221, 'test/loss': 2.200854778289795, 'test/num_examples': 10000, 'score': 32185.880859375, 'total_duration': 34645.37012696266, 'accumulated_submission_time': 32185.880859375, 'accumulated_eval_time': 2443.5153691768646, 'accumulated_logging_time': 7.285172462463379}
I0307 12:00:04.420724 140021218662144 logging_writer.py:48] [82866] accumulated_eval_time=2443.52, accumulated_logging_time=7.28517, accumulated_submission_time=32185.9, global_step=82866, preemption_count=0, score=32185.9, test/accuracy=0.5209, test/loss=2.20085, test/num_examples=10000, total_duration=34645.4, train/accuracy=0.695452, train/loss=1.31777, validation/accuracy=0.64356, validation/loss=1.54628, validation/num_examples=50000
I0307 12:00:18.056901 140021210269440 logging_writer.py:48] [82900] global_step=82900, grad_norm=3.5204312801361084, loss=2.550319194793701
I0307 12:00:56.416491 140021218662144 logging_writer.py:48] [83000] global_step=83000, grad_norm=3.6703903675079346, loss=2.6110899448394775
I0307 12:01:35.098566 140021210269440 logging_writer.py:48] [83100] global_step=83100, grad_norm=3.4931373596191406, loss=2.6146762371063232
I0307 12:02:13.548422 140021218662144 logging_writer.py:48] [83200] global_step=83200, grad_norm=3.3877720832824707, loss=2.6952898502349854
I0307 12:02:52.266710 140021210269440 logging_writer.py:48] [83300] global_step=83300, grad_norm=3.535874605178833, loss=2.6495673656463623
I0307 12:03:31.292255 140021218662144 logging_writer.py:48] [83400] global_step=83400, grad_norm=3.788625955581665, loss=2.6284027099609375
I0307 12:04:10.056746 140021210269440 logging_writer.py:48] [83500] global_step=83500, grad_norm=3.518930673599243, loss=2.5969886779785156
I0307 12:04:49.107781 140021218662144 logging_writer.py:48] [83600] global_step=83600, grad_norm=3.527501106262207, loss=2.5800983905792236
I0307 12:05:28.192024 140021210269440 logging_writer.py:48] [83700] global_step=83700, grad_norm=3.3223001956939697, loss=2.6346983909606934
I0307 12:06:07.475674 140021218662144 logging_writer.py:48] [83800] global_step=83800, grad_norm=3.6918554306030273, loss=2.5343310832977295
I0307 12:06:46.486538 140021210269440 logging_writer.py:48] [83900] global_step=83900, grad_norm=3.391633987426758, loss=2.578967571258545
I0307 12:07:25.795595 140021218662144 logging_writer.py:48] [84000] global_step=84000, grad_norm=3.6348211765289307, loss=2.5693936347961426
I0307 12:08:04.840770 140021210269440 logging_writer.py:48] [84100] global_step=84100, grad_norm=3.2155096530914307, loss=2.5504980087280273
I0307 12:08:34.349001 140178276385984 spec.py:321] Evaluating on the training split.
I0307 12:08:46.619202 140178276385984 spec.py:333] Evaluating on the validation split.
I0307 12:09:07.837348 140178276385984 spec.py:349] Evaluating on the test split.
I0307 12:09:09.584286 140178276385984 submission_runner.py:469] Time since start: 35190.63s, 	Step: 84177, 	{'train/accuracy': 0.6924425959587097, 'train/loss': 1.3435760736465454, 'validation/accuracy': 0.6429600119590759, 'validation/loss': 1.5658118724822998, 'validation/num_examples': 50000, 'test/accuracy': 0.5148000121116638, 'test/loss': 2.2513833045959473, 'test/num_examples': 10000, 'score': 32695.633306741714, 'total_duration': 35190.62775874138, 'accumulated_submission_time': 32695.633306741714, 'accumulated_eval_time': 2478.750470161438, 'accumulated_logging_time': 7.41370701789856}
I0307 12:09:09.670603 140021218662144 logging_writer.py:48] [84177] accumulated_eval_time=2478.75, accumulated_logging_time=7.41371, accumulated_submission_time=32695.6, global_step=84177, preemption_count=0, score=32695.6, test/accuracy=0.5148, test/loss=2.25138, test/num_examples=10000, total_duration=35190.6, train/accuracy=0.692443, train/loss=1.34358, validation/accuracy=0.64296, validation/loss=1.56581, validation/num_examples=50000
I0307 12:09:19.257390 140021210269440 logging_writer.py:48] [84200] global_step=84200, grad_norm=4.146152973175049, loss=2.6428122520446777
I0307 12:09:58.222767 140021218662144 logging_writer.py:48] [84300] global_step=84300, grad_norm=3.019611120223999, loss=2.5761895179748535
I0307 12:10:37.022517 140021210269440 logging_writer.py:48] [84400] global_step=84400, grad_norm=3.8590874671936035, loss=2.6905534267425537
I0307 12:11:16.124947 140021218662144 logging_writer.py:48] [84500] global_step=84500, grad_norm=3.746000289916992, loss=2.6363227367401123
I0307 12:11:54.645178 140021210269440 logging_writer.py:48] [84600] global_step=84600, grad_norm=3.2099618911743164, loss=2.4932162761688232
I0307 12:12:33.943045 140021218662144 logging_writer.py:48] [84700] global_step=84700, grad_norm=3.8080368041992188, loss=2.596400260925293
I0307 12:13:13.163653 140021210269440 logging_writer.py:48] [84800] global_step=84800, grad_norm=3.4609150886535645, loss=2.5638082027435303
I0307 12:13:52.023658 140021218662144 logging_writer.py:48] [84900] global_step=84900, grad_norm=3.54868221282959, loss=2.6659772396087646
I0307 12:14:31.692640 140021210269440 logging_writer.py:48] [85000] global_step=85000, grad_norm=3.4123027324676514, loss=2.643277883529663
I0307 12:15:11.099259 140021218662144 logging_writer.py:48] [85100] global_step=85100, grad_norm=3.403782844543457, loss=2.561791181564331
I0307 12:15:50.196742 140021210269440 logging_writer.py:48] [85200] global_step=85200, grad_norm=3.270142078399658, loss=2.6309733390808105
I0307 12:16:29.233260 140021218662144 logging_writer.py:48] [85300] global_step=85300, grad_norm=3.515883684158325, loss=2.649738073348999
I0307 12:17:08.515644 140021210269440 logging_writer.py:48] [85400] global_step=85400, grad_norm=3.645284414291382, loss=2.5349338054656982
I0307 12:17:39.745644 140178276385984 spec.py:321] Evaluating on the training split.
I0307 12:17:52.051637 140178276385984 spec.py:333] Evaluating on the validation split.
I0307 12:18:12.040012 140178276385984 spec.py:349] Evaluating on the test split.
I0307 12:18:13.779862 140178276385984 submission_runner.py:469] Time since start: 35734.82s, 	Step: 85481, 	{'train/accuracy': 0.6950533986091614, 'train/loss': 1.3141634464263916, 'validation/accuracy': 0.6448000073432922, 'validation/loss': 1.5536383390426636, 'validation/num_examples': 50000, 'test/accuracy': 0.5141000151634216, 'test/loss': 2.2255003452301025, 'test/num_examples': 10000, 'score': 33205.53166937828, 'total_duration': 35734.82333302498, 'accumulated_submission_time': 33205.53166937828, 'accumulated_eval_time': 2512.784499645233, 'accumulated_logging_time': 7.529149770736694}
I0307 12:18:13.878990 140021218662144 logging_writer.py:48] [85481] accumulated_eval_time=2512.78, accumulated_logging_time=7.52915, accumulated_submission_time=33205.5, global_step=85481, preemption_count=0, score=33205.5, test/accuracy=0.5141, test/loss=2.2255, test/num_examples=10000, total_duration=35734.8, train/accuracy=0.695053, train/loss=1.31416, validation/accuracy=0.6448, validation/loss=1.55364, validation/num_examples=50000
I0307 12:18:21.542202 140021210269440 logging_writer.py:48] [85500] global_step=85500, grad_norm=3.593613624572754, loss=2.5334768295288086
I0307 12:19:00.481000 140021218662144 logging_writer.py:48] [85600] global_step=85600, grad_norm=3.4990570545196533, loss=2.617168426513672
I0307 12:19:39.170589 140021210269440 logging_writer.py:48] [85700] global_step=85700, grad_norm=3.8031914234161377, loss=2.608743667602539
I0307 12:20:18.425710 140021218662144 logging_writer.py:48] [85800] global_step=85800, grad_norm=3.7730674743652344, loss=2.674370050430298
I0307 12:20:57.510738 140021210269440 logging_writer.py:48] [85900] global_step=85900, grad_norm=3.367896795272827, loss=2.622997522354126
I0307 12:21:36.680802 140021218662144 logging_writer.py:48] [86000] global_step=86000, grad_norm=3.3301703929901123, loss=2.6356289386749268
I0307 12:22:15.985874 140021210269440 logging_writer.py:48] [86100] global_step=86100, grad_norm=3.451237678527832, loss=2.5974202156066895
I0307 12:22:54.361470 140021218662144 logging_writer.py:48] [86200] global_step=86200, grad_norm=3.5694363117218018, loss=2.590718984603882
I0307 12:23:31.095305 140021210269440 logging_writer.py:48] [86300] global_step=86300, grad_norm=3.2750658988952637, loss=2.5529606342315674
I0307 12:24:09.313114 140021218662144 logging_writer.py:48] [86400] global_step=86400, grad_norm=3.6073930263519287, loss=2.6174118518829346
I0307 12:24:46.881584 140021210269440 logging_writer.py:48] [86500] global_step=86500, grad_norm=3.5947141647338867, loss=2.655778646469116
I0307 12:25:24.264080 140021218662144 logging_writer.py:48] [86600] global_step=86600, grad_norm=3.264683961868286, loss=2.6177947521209717
I0307 12:26:01.355615 140021210269440 logging_writer.py:48] [86700] global_step=86700, grad_norm=3.352242946624756, loss=2.635387897491455
I0307 12:26:38.500934 140021218662144 logging_writer.py:48] [86800] global_step=86800, grad_norm=4.079835891723633, loss=2.7104132175445557
I0307 12:26:44.039731 140178276385984 spec.py:321] Evaluating on the training split.
I0307 12:26:57.042080 140178276385984 spec.py:333] Evaluating on the validation split.
I0307 12:27:27.363854 140178276385984 spec.py:349] Evaluating on the test split.
I0307 12:27:29.126769 140178276385984 submission_runner.py:469] Time since start: 36290.17s, 	Step: 86815, 	{'train/accuracy': 0.6929807066917419, 'train/loss': 1.336341381072998, 'validation/accuracy': 0.644540011882782, 'validation/loss': 1.5634878873825073, 'validation/num_examples': 50000, 'test/accuracy': 0.5164999961853027, 'test/loss': 2.2240328788757324, 'test/num_examples': 10000, 'score': 33715.46340942383, 'total_duration': 36290.17020225525, 'accumulated_submission_time': 33715.46340942383, 'accumulated_eval_time': 2557.8713188171387, 'accumulated_logging_time': 7.696270704269409}
I0307 12:27:29.222618 140021210269440 logging_writer.py:48] [86815] accumulated_eval_time=2557.87, accumulated_logging_time=7.69627, accumulated_submission_time=33715.5, global_step=86815, preemption_count=0, score=33715.5, test/accuracy=0.5165, test/loss=2.22403, test/num_examples=10000, total_duration=36290.2, train/accuracy=0.692981, train/loss=1.33634, validation/accuracy=0.64454, validation/loss=1.56349, validation/num_examples=50000
I0307 12:28:02.910629 140021218662144 logging_writer.py:48] [86900] global_step=86900, grad_norm=3.5999531745910645, loss=2.6031546592712402
I0307 12:28:41.935586 140021210269440 logging_writer.py:48] [87000] global_step=87000, grad_norm=3.4012033939361572, loss=2.5568387508392334
I0307 12:29:21.536702 140021218662144 logging_writer.py:48] [87100] global_step=87100, grad_norm=3.7510266304016113, loss=2.616615056991577
I0307 12:30:00.819443 140021210269440 logging_writer.py:48] [87200] global_step=87200, grad_norm=3.0790340900421143, loss=2.604496955871582
I0307 12:30:40.137241 140021218662144 logging_writer.py:48] [87300] global_step=87300, grad_norm=3.0455074310302734, loss=2.538681745529175
I0307 12:31:19.363720 140021210269440 logging_writer.py:48] [87400] global_step=87400, grad_norm=3.921567440032959, loss=2.6435961723327637
I0307 12:31:58.158257 140021218662144 logging_writer.py:48] [87500] global_step=87500, grad_norm=3.0493698120117188, loss=2.584113836288452
I0307 12:32:36.971995 140021210269440 logging_writer.py:48] [87600] global_step=87600, grad_norm=3.432926893234253, loss=2.6058616638183594
I0307 12:33:16.309495 140021218662144 logging_writer.py:48] [87700] global_step=87700, grad_norm=3.824925422668457, loss=2.708392381668091
I0307 12:34:37.608492 140021210269440 logging_writer.py:48] [87800] global_step=87800, grad_norm=2.977123260498047, loss=2.5255744457244873
I0307 12:35:15.912668 140021218662144 logging_writer.py:48] [87900] global_step=87900, grad_norm=3.9929637908935547, loss=2.62703800201416
I0307 12:35:53.992433 140021210269440 logging_writer.py:48] [88000] global_step=88000, grad_norm=4.033884525299072, loss=2.606616258621216
I0307 12:35:59.408671 140178276385984 spec.py:321] Evaluating on the training split.
I0307 12:36:12.368220 140178276385984 spec.py:333] Evaluating on the validation split.
I0307 12:36:33.513458 140178276385984 spec.py:349] Evaluating on the test split.
I0307 12:36:35.288962 140178276385984 submission_runner.py:469] Time since start: 36836.33s, 	Step: 88015, 	{'train/accuracy': 0.6919841766357422, 'train/loss': 1.3221924304962158, 'validation/accuracy': 0.6390199661254883, 'validation/loss': 1.5645782947540283, 'validation/num_examples': 50000, 'test/accuracy': 0.5217000246047974, 'test/loss': 2.2037353515625, 'test/num_examples': 10000, 'score': 34225.476650476456, 'total_duration': 36836.33246874809, 'accumulated_submission_time': 34225.476650476456, 'accumulated_eval_time': 2593.7514803409576, 'accumulated_logging_time': 7.820662021636963}
I0307 12:36:35.385628 140021218662144 logging_writer.py:48] [88015] accumulated_eval_time=2593.75, accumulated_logging_time=7.82066, accumulated_submission_time=34225.5, global_step=88015, preemption_count=0, score=34225.5, test/accuracy=0.5217, test/loss=2.20374, test/num_examples=10000, total_duration=36836.3, train/accuracy=0.691984, train/loss=1.32219, validation/accuracy=0.63902, validation/loss=1.56458, validation/num_examples=50000
I0307 12:37:09.497957 140021210269440 logging_writer.py:48] [88100] global_step=88100, grad_norm=3.8256163597106934, loss=2.640275478363037
I0307 12:37:48.565330 140021218662144 logging_writer.py:48] [88200] global_step=88200, grad_norm=3.456014394760132, loss=2.5867722034454346
I0307 12:38:27.654585 140021210269440 logging_writer.py:48] [88300] global_step=88300, grad_norm=3.8538451194763184, loss=2.645010471343994
I0307 12:39:07.380396 140021218662144 logging_writer.py:48] [88400] global_step=88400, grad_norm=3.8815150260925293, loss=2.5983049869537354
I0307 12:39:46.824865 140021210269440 logging_writer.py:48] [88500] global_step=88500, grad_norm=3.699099540710449, loss=2.570110321044922
I0307 12:40:26.337766 140021218662144 logging_writer.py:48] [88600] global_step=88600, grad_norm=3.5397613048553467, loss=2.638557195663452
I0307 12:41:04.708555 140021210269440 logging_writer.py:48] [88700] global_step=88700, grad_norm=3.4519386291503906, loss=2.618569850921631
I0307 12:41:44.751479 140021218662144 logging_writer.py:48] [88800] global_step=88800, grad_norm=3.821218490600586, loss=2.5689027309417725
2025-03-07 12:41:50.553794: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 12:42:24.457659 140021210269440 logging_writer.py:48] [88900] global_step=88900, grad_norm=3.722287178039551, loss=2.5143837928771973
I0307 12:43:02.560449 140021218662144 logging_writer.py:48] [89000] global_step=89000, grad_norm=3.2558746337890625, loss=2.5928916931152344
I0307 12:43:40.459006 140021210269440 logging_writer.py:48] [89100] global_step=89100, grad_norm=3.395211696624756, loss=2.6093428134918213
I0307 12:44:17.992107 140021218662144 logging_writer.py:48] [89200] global_step=89200, grad_norm=3.6798789501190186, loss=2.610809087753296
I0307 12:45:06.134281 140178276385984 spec.py:321] Evaluating on the training split.
I0307 12:45:18.455046 140178276385984 spec.py:333] Evaluating on the validation split.
I0307 12:45:38.096737 140178276385984 spec.py:349] Evaluating on the test split.
I0307 12:45:39.830312 140178276385984 submission_runner.py:469] Time since start: 37380.87s, 	Step: 89241, 	{'train/accuracy': 0.7044602632522583, 'train/loss': 1.2716877460479736, 'validation/accuracy': 0.6524800062179565, 'validation/loss': 1.5168160200119019, 'validation/num_examples': 50000, 'test/accuracy': 0.5270000100135803, 'test/loss': 2.167029619216919, 'test/num_examples': 10000, 'score': 34736.05078983307, 'total_duration': 37380.87375640869, 'accumulated_submission_time': 34736.05078983307, 'accumulated_eval_time': 2627.4473009109497, 'accumulated_logging_time': 7.944999933242798}
I0307 12:45:39.921802 140021210269440 logging_writer.py:48] [89241] accumulated_eval_time=2627.45, accumulated_logging_time=7.945, accumulated_submission_time=34736.1, global_step=89241, preemption_count=0, score=34736.1, test/accuracy=0.527, test/loss=2.16703, test/num_examples=10000, total_duration=37380.9, train/accuracy=0.70446, train/loss=1.27169, validation/accuracy=0.65248, validation/loss=1.51682, validation/num_examples=50000
I0307 12:46:03.621703 140021218662144 logging_writer.py:48] [89300] global_step=89300, grad_norm=3.7307026386260986, loss=2.492544651031494
I0307 12:46:42.177333 140021210269440 logging_writer.py:48] [89400] global_step=89400, grad_norm=3.704228401184082, loss=2.610961437225342
I0307 12:47:21.722036 140021218662144 logging_writer.py:48] [89500] global_step=89500, grad_norm=3.7257516384124756, loss=2.5819342136383057
I0307 12:48:01.175093 140021210269440 logging_writer.py:48] [89600] global_step=89600, grad_norm=4.055235385894775, loss=2.5894579887390137
I0307 12:48:40.312811 140021218662144 logging_writer.py:48] [89700] global_step=89700, grad_norm=3.55281138420105, loss=2.5384297370910645
I0307 12:49:19.616960 140021210269440 logging_writer.py:48] [89800] global_step=89800, grad_norm=3.500960350036621, loss=2.5860042572021484
I0307 12:49:59.235492 140021218662144 logging_writer.py:48] [89900] global_step=89900, grad_norm=3.7324514389038086, loss=2.58493709564209
I0307 12:50:38.786568 140021210269440 logging_writer.py:48] [90000] global_step=90000, grad_norm=3.4367470741271973, loss=2.534489631652832
I0307 12:51:17.914782 140021218662144 logging_writer.py:48] [90100] global_step=90100, grad_norm=3.493279218673706, loss=2.4954583644866943
I0307 12:51:57.526801 140021218662144 logging_writer.py:48] [90200] global_step=90200, grad_norm=3.8165335655212402, loss=2.6804068088531494
I0307 12:52:36.711244 140021210269440 logging_writer.py:48] [90300] global_step=90300, grad_norm=3.38662052154541, loss=2.5181844234466553
I0307 12:53:16.355415 140021218662144 logging_writer.py:48] [90400] global_step=90400, grad_norm=3.676750659942627, loss=2.597679853439331
I0307 12:53:55.778545 140021210269440 logging_writer.py:48] [90500] global_step=90500, grad_norm=3.64591908454895, loss=2.5840251445770264
I0307 12:54:10.138759 140178276385984 spec.py:321] Evaluating on the training split.
I0307 12:54:21.916781 140178276385984 spec.py:333] Evaluating on the validation split.
I0307 12:54:44.830337 140178276385984 spec.py:349] Evaluating on the test split.
I0307 12:54:46.587661 140178276385984 submission_runner.py:469] Time since start: 37927.63s, 	Step: 90537, 	{'train/accuracy': 0.7014309763908386, 'train/loss': 1.2846760749816895, 'validation/accuracy': 0.6474999785423279, 'validation/loss': 1.5295343399047852, 'validation/num_examples': 50000, 'test/accuracy': 0.5282000303268433, 'test/loss': 2.18021821975708, 'test/num_examples': 10000, 'score': 35246.08141589165, 'total_duration': 37927.631106853485, 'accumulated_submission_time': 35246.08141589165, 'accumulated_eval_time': 2663.8959975242615, 'accumulated_logging_time': 8.067214012145996}
I0307 12:54:46.696261 140021218662144 logging_writer.py:48] [90537] accumulated_eval_time=2663.9, accumulated_logging_time=8.06721, accumulated_submission_time=35246.1, global_step=90537, preemption_count=0, score=35246.1, test/accuracy=0.5282, test/loss=2.18022, test/num_examples=10000, total_duration=37927.6, train/accuracy=0.701431, train/loss=1.28468, validation/accuracy=0.6475, validation/loss=1.52953, validation/num_examples=50000
I0307 12:55:11.835981 140021210269440 logging_writer.py:48] [90600] global_step=90600, grad_norm=3.729966163635254, loss=2.5809950828552246
I0307 12:55:51.222241 140021218662144 logging_writer.py:48] [90700] global_step=90700, grad_norm=3.3393027782440186, loss=2.587604522705078
I0307 12:56:30.087361 140021210269440 logging_writer.py:48] [90800] global_step=90800, grad_norm=3.6117770671844482, loss=2.576667308807373
I0307 12:57:08.202066 140021218662144 logging_writer.py:48] [90900] global_step=90900, grad_norm=3.3138725757598877, loss=2.4318888187408447
I0307 12:57:47.543695 140021210269440 logging_writer.py:48] [91000] global_step=91000, grad_norm=3.891291618347168, loss=2.5449862480163574
I0307 12:58:25.702009 140021218662144 logging_writer.py:48] [91100] global_step=91100, grad_norm=3.867039680480957, loss=2.542119026184082
I0307 12:59:06.918574 140021210269440 logging_writer.py:48] [91200] global_step=91200, grad_norm=3.5927417278289795, loss=2.5588414669036865
I0307 12:59:46.515475 140021218662144 logging_writer.py:48] [91300] global_step=91300, grad_norm=3.5093910694122314, loss=2.5803189277648926
I0307 13:00:25.540985 140021210269440 logging_writer.py:48] [91400] global_step=91400, grad_norm=3.283254861831665, loss=2.613856554031372
I0307 13:01:04.363796 140021218662144 logging_writer.py:48] [91500] global_step=91500, grad_norm=3.700812339782715, loss=2.6305296421051025
I0307 13:01:43.949927 140021210269440 logging_writer.py:48] [91600] global_step=91600, grad_norm=3.260427236557007, loss=2.6496732234954834
I0307 13:02:22.941860 140021218662144 logging_writer.py:48] [91700] global_step=91700, grad_norm=3.428246021270752, loss=2.5185234546661377
I0307 13:03:01.289061 140021210269440 logging_writer.py:48] [91800] global_step=91800, grad_norm=3.604800224304199, loss=2.5961880683898926
I0307 13:03:16.594225 140178276385984 spec.py:321] Evaluating on the training split.
I0307 13:03:29.799784 140178276385984 spec.py:333] Evaluating on the validation split.
I0307 13:03:48.688955 140178276385984 spec.py:349] Evaluating on the test split.
I0307 13:03:50.436123 140178276385984 submission_runner.py:469] Time since start: 38471.48s, 	Step: 91840, 	{'train/accuracy': 0.7085259556770325, 'train/loss': 1.2437835931777954, 'validation/accuracy': 0.6565999984741211, 'validation/loss': 1.4826949834823608, 'validation/num_examples': 50000, 'test/accuracy': 0.5263000130653381, 'test/loss': 2.1592321395874023, 'test/num_examples': 10000, 'score': 35755.80262589455, 'total_duration': 38471.47956061363, 'accumulated_submission_time': 35755.80262589455, 'accumulated_eval_time': 2697.737674474716, 'accumulated_logging_time': 8.196099519729614}
I0307 13:03:50.523233 140021218662144 logging_writer.py:48] [91840] accumulated_eval_time=2697.74, accumulated_logging_time=8.1961, accumulated_submission_time=35755.8, global_step=91840, preemption_count=0, score=35755.8, test/accuracy=0.5263, test/loss=2.15923, test/num_examples=10000, total_duration=38471.5, train/accuracy=0.708526, train/loss=1.24378, validation/accuracy=0.6566, validation/loss=1.48269, validation/num_examples=50000
I0307 13:04:14.654136 140021210269440 logging_writer.py:48] [91900] global_step=91900, grad_norm=3.863398313522339, loss=2.5791943073272705
I0307 13:04:54.159952 140021218662144 logging_writer.py:48] [92000] global_step=92000, grad_norm=3.338463306427002, loss=2.4875292778015137
I0307 13:05:34.050159 140021210269440 logging_writer.py:48] [92100] global_step=92100, grad_norm=3.3984553813934326, loss=2.5630130767822266
I0307 13:06:13.601007 140021218662144 logging_writer.py:48] [92200] global_step=92200, grad_norm=4.034179210662842, loss=2.582737922668457
I0307 13:06:53.065799 140021210269440 logging_writer.py:48] [92300] global_step=92300, grad_norm=3.717329978942871, loss=2.5805842876434326
I0307 13:07:32.832062 140021218662144 logging_writer.py:48] [92400] global_step=92400, grad_norm=3.444852828979492, loss=2.5319368839263916
I0307 13:08:17.434974 140021210269440 logging_writer.py:48] [92500] global_step=92500, grad_norm=3.7515578269958496, loss=2.528238296508789
I0307 13:08:59.586863 140021218662144 logging_writer.py:48] [92600] global_step=92600, grad_norm=4.136730670928955, loss=2.641153335571289
I0307 13:09:39.094318 140021210269440 logging_writer.py:48] [92700] global_step=92700, grad_norm=3.9628703594207764, loss=2.5349576473236084
I0307 13:10:17.702501 140021218662144 logging_writer.py:48] [92800] global_step=92800, grad_norm=3.237163543701172, loss=2.5615501403808594
I0307 13:10:57.445763 140021210269440 logging_writer.py:48] [92900] global_step=92900, grad_norm=3.6784255504608154, loss=2.48752760887146
I0307 13:11:38.208086 140021218662144 logging_writer.py:48] [93000] global_step=93000, grad_norm=3.718756914138794, loss=2.499147653579712
I0307 13:12:18.445261 140021210269440 logging_writer.py:48] [93100] global_step=93100, grad_norm=3.477501153945923, loss=2.559432029724121
I0307 13:12:20.605112 140178276385984 spec.py:321] Evaluating on the training split.
I0307 13:12:32.870244 140178276385984 spec.py:333] Evaluating on the validation split.
I0307 13:12:52.385544 140178276385984 spec.py:349] Evaluating on the test split.
I0307 13:12:54.105230 140178276385984 submission_runner.py:469] Time since start: 39015.15s, 	Step: 93106, 	{'train/accuracy': 0.7000358700752258, 'train/loss': 1.2826281785964966, 'validation/accuracy': 0.646340012550354, 'validation/loss': 1.5347332954406738, 'validation/num_examples': 50000, 'test/accuracy': 0.5135000348091125, 'test/loss': 2.214576244354248, 'test/num_examples': 10000, 'score': 36265.714277505875, 'total_duration': 39015.148689985275, 'accumulated_submission_time': 36265.714277505875, 'accumulated_eval_time': 2731.237590789795, 'accumulated_logging_time': 8.304457187652588}
I0307 13:12:54.194197 140021218662144 logging_writer.py:48] [93106] accumulated_eval_time=2731.24, accumulated_logging_time=8.30446, accumulated_submission_time=36265.7, global_step=93106, preemption_count=0, score=36265.7, test/accuracy=0.5135, test/loss=2.21458, test/num_examples=10000, total_duration=39015.1, train/accuracy=0.700036, train/loss=1.28263, validation/accuracy=0.64634, validation/loss=1.53473, validation/num_examples=50000
I0307 13:13:32.072269 140021210269440 logging_writer.py:48] [93200] global_step=93200, grad_norm=3.1412465572357178, loss=2.4468820095062256
I0307 13:14:12.254160 140021218662144 logging_writer.py:48] [93300] global_step=93300, grad_norm=3.447148084640503, loss=2.589407444000244
I0307 13:14:51.863128 140021210269440 logging_writer.py:48] [93400] global_step=93400, grad_norm=3.4835617542266846, loss=2.548299551010132
I0307 13:15:31.511225 140021218662144 logging_writer.py:48] [93500] global_step=93500, grad_norm=3.4863662719726562, loss=2.5573556423187256
I0307 13:16:11.525426 140021210269440 logging_writer.py:48] [93600] global_step=93600, grad_norm=3.633358955383301, loss=2.6421408653259277
I0307 13:16:50.696635 140021218662144 logging_writer.py:48] [93700] global_step=93700, grad_norm=3.780409097671509, loss=2.558659553527832
I0307 13:17:34.366494 140021210269440 logging_writer.py:48] [93800] global_step=93800, grad_norm=3.64357328414917, loss=2.471205234527588
I0307 13:18:16.407427 140021218662144 logging_writer.py:48] [93900] global_step=93900, grad_norm=3.5786960124969482, loss=2.525594711303711
I0307 13:18:55.833271 140021210269440 logging_writer.py:48] [94000] global_step=94000, grad_norm=4.4111328125, loss=2.628096103668213
I0307 13:19:37.316357 140021218662144 logging_writer.py:48] [94100] global_step=94100, grad_norm=3.2910850048065186, loss=2.5650715827941895
I0307 13:20:19.738560 140021210269440 logging_writer.py:48] [94200] global_step=94200, grad_norm=3.4078726768493652, loss=2.54106068611145
I0307 13:20:59.785292 140021218662144 logging_writer.py:48] [94300] global_step=94300, grad_norm=3.449455738067627, loss=2.6051533222198486
I0307 13:21:24.145415 140178276385984 spec.py:321] Evaluating on the training split.
I0307 13:21:37.181878 140178276385984 spec.py:333] Evaluating on the validation split.
I0307 13:21:57.838698 140178276385984 spec.py:349] Evaluating on the test split.
I0307 13:21:59.563032 140178276385984 submission_runner.py:469] Time since start: 39560.61s, 	Step: 94363, 	{'train/accuracy': 0.7093231678009033, 'train/loss': 1.2625337839126587, 'validation/accuracy': 0.656719982624054, 'validation/loss': 1.515025019645691, 'validation/num_examples': 50000, 'test/accuracy': 0.5198000073432922, 'test/loss': 2.1848230361938477, 'test/num_examples': 10000, 'score': 36775.48110985756, 'total_duration': 39560.606474876404, 'accumulated_submission_time': 36775.48110985756, 'accumulated_eval_time': 2766.6549985408783, 'accumulated_logging_time': 8.430238723754883}
I0307 13:21:59.719892 140021210269440 logging_writer.py:48] [94363] accumulated_eval_time=2766.65, accumulated_logging_time=8.43024, accumulated_submission_time=36775.5, global_step=94363, preemption_count=0, score=36775.5, test/accuracy=0.5198, test/loss=2.18482, test/num_examples=10000, total_duration=39560.6, train/accuracy=0.709323, train/loss=1.26253, validation/accuracy=0.65672, validation/loss=1.51503, validation/num_examples=50000
I0307 13:22:14.659430 140021218662144 logging_writer.py:48] [94400] global_step=94400, grad_norm=3.6839027404785156, loss=2.5912041664123535
I0307 13:22:54.209907 140021210269440 logging_writer.py:48] [94500] global_step=94500, grad_norm=3.3302597999572754, loss=2.52880597114563
I0307 13:23:34.208197 140021218662144 logging_writer.py:48] [94600] global_step=94600, grad_norm=3.5701022148132324, loss=2.493037700653076
I0307 13:24:14.014089 140021210269440 logging_writer.py:48] [94700] global_step=94700, grad_norm=3.4208414554595947, loss=2.5417287349700928
I0307 13:24:53.327493 140021218662144 logging_writer.py:48] [94800] global_step=94800, grad_norm=4.133678436279297, loss=2.662092685699463
I0307 13:25:32.394711 140021210269440 logging_writer.py:48] [94900] global_step=94900, grad_norm=3.3411755561828613, loss=2.486459255218506
I0307 13:26:16.421664 140021218662144 logging_writer.py:48] [95000] global_step=95000, grad_norm=3.4436497688293457, loss=2.574671506881714
I0307 13:26:57.368026 140021210269440 logging_writer.py:48] [95100] global_step=95100, grad_norm=3.8592312335968018, loss=2.571333169937134
I0307 13:27:36.818915 140021218662144 logging_writer.py:48] [95200] global_step=95200, grad_norm=3.8029444217681885, loss=2.4776530265808105
I0307 13:28:16.424374 140021210269440 logging_writer.py:48] [95300] global_step=95300, grad_norm=3.310279607772827, loss=2.550706624984741
I0307 13:28:56.122069 140021218662144 logging_writer.py:48] [95400] global_step=95400, grad_norm=3.6137607097625732, loss=2.640608549118042
I0307 13:29:35.913128 140021210269440 logging_writer.py:48] [95500] global_step=95500, grad_norm=4.727790832519531, loss=2.5423662662506104
I0307 13:30:14.874572 140021218662144 logging_writer.py:48] [95600] global_step=95600, grad_norm=3.5919580459594727, loss=2.598411798477173
I0307 13:30:29.691826 140178276385984 spec.py:321] Evaluating on the training split.
I0307 13:30:42.333201 140178276385984 spec.py:333] Evaluating on the validation split.
I0307 13:31:00.945487 140178276385984 spec.py:349] Evaluating on the test split.
I0307 13:31:02.668191 140178276385984 submission_runner.py:469] Time since start: 40103.71s, 	Step: 95639, 	{'train/accuracy': 0.7106584906578064, 'train/loss': 1.2461391687393188, 'validation/accuracy': 0.653499960899353, 'validation/loss': 1.4984668493270874, 'validation/num_examples': 50000, 'test/accuracy': 0.526199996471405, 'test/loss': 2.1798980236053467, 'test/num_examples': 10000, 'score': 37285.27674341202, 'total_duration': 40103.7116959095, 'accumulated_submission_time': 37285.27674341202, 'accumulated_eval_time': 2799.631216287613, 'accumulated_logging_time': 8.614967107772827}
I0307 13:31:02.800488 140021210269440 logging_writer.py:48] [95639] accumulated_eval_time=2799.63, accumulated_logging_time=8.61497, accumulated_submission_time=37285.3, global_step=95639, preemption_count=0, score=37285.3, test/accuracy=0.5262, test/loss=2.1799, test/num_examples=10000, total_duration=40103.7, train/accuracy=0.710658, train/loss=1.24614, validation/accuracy=0.6535, validation/loss=1.49847, validation/num_examples=50000
I0307 13:31:27.303179 140021218662144 logging_writer.py:48] [95700] global_step=95700, grad_norm=3.974236011505127, loss=2.669600486755371
I0307 13:32:07.173051 140021210269440 logging_writer.py:48] [95800] global_step=95800, grad_norm=3.39585280418396, loss=2.597046136856079
I0307 13:32:48.647870 140021218662144 logging_writer.py:48] [95900] global_step=95900, grad_norm=3.846853256225586, loss=2.528660535812378
I0307 13:33:27.499574 140021210269440 logging_writer.py:48] [96000] global_step=96000, grad_norm=3.826615571975708, loss=2.6138620376586914
I0307 13:34:05.398094 140021218662144 logging_writer.py:48] [96100] global_step=96100, grad_norm=3.6834335327148438, loss=2.58626651763916
I0307 13:34:47.197258 140021210269440 logging_writer.py:48] [96200] global_step=96200, grad_norm=3.806617498397827, loss=2.4788870811462402
I0307 13:35:33.025635 140021218662144 logging_writer.py:48] [96300] global_step=96300, grad_norm=3.7465243339538574, loss=2.5763118267059326
I0307 13:36:14.697602 140021210269440 logging_writer.py:48] [96400] global_step=96400, grad_norm=3.885533332824707, loss=2.589481830596924
I0307 13:36:52.917484 140021218662144 logging_writer.py:48] [96500] global_step=96500, grad_norm=3.7812576293945312, loss=2.667609453201294
I0307 13:37:32.728210 140021210269440 logging_writer.py:48] [96600] global_step=96600, grad_norm=4.028767108917236, loss=2.6177778244018555
I0307 13:38:13.915704 140021218662144 logging_writer.py:48] [96700] global_step=96700, grad_norm=3.937243938446045, loss=2.471137285232544
I0307 13:38:53.861757 140021210269440 logging_writer.py:48] [96800] global_step=96800, grad_norm=3.897735357284546, loss=2.542476177215576
I0307 13:39:32.821061 140021218662144 logging_writer.py:48] [96900] global_step=96900, grad_norm=3.7273449897766113, loss=2.5087594985961914
I0307 13:39:32.831136 140178276385984 spec.py:321] Evaluating on the training split.
I0307 13:39:45.206679 140178276385984 spec.py:333] Evaluating on the validation split.
I0307 13:40:01.231586 140178276385984 spec.py:349] Evaluating on the test split.
I0307 13:40:03.010253 140178276385984 submission_runner.py:469] Time since start: 40644.05s, 	Step: 96901, 	{'train/accuracy': 0.7095025181770325, 'train/loss': 1.2492046356201172, 'validation/accuracy': 0.6546799540519714, 'validation/loss': 1.5062360763549805, 'validation/num_examples': 50000, 'test/accuracy': 0.5243000388145447, 'test/loss': 2.1919305324554443, 'test/num_examples': 10000, 'score': 37795.1165561676, 'total_duration': 40644.05375957489, 'accumulated_submission_time': 37795.1165561676, 'accumulated_eval_time': 2829.8101563453674, 'accumulated_logging_time': 8.787564992904663}
I0307 13:40:03.098377 140021210269440 logging_writer.py:48] [96901] accumulated_eval_time=2829.81, accumulated_logging_time=8.78756, accumulated_submission_time=37795.1, global_step=96901, preemption_count=0, score=37795.1, test/accuracy=0.5243, test/loss=2.19193, test/num_examples=10000, total_duration=40644.1, train/accuracy=0.709503, train/loss=1.2492, validation/accuracy=0.65468, validation/loss=1.50624, validation/num_examples=50000
I0307 13:40:42.310371 140021218662144 logging_writer.py:48] [97000] global_step=97000, grad_norm=3.4918053150177, loss=2.4791321754455566
I0307 13:41:22.098258 140021210269440 logging_writer.py:48] [97100] global_step=97100, grad_norm=3.415229320526123, loss=2.478334903717041
I0307 13:42:01.924134 140021218662144 logging_writer.py:48] [97200] global_step=97200, grad_norm=3.723179578781128, loss=2.52184796333313
I0307 13:42:41.500416 140021210269440 logging_writer.py:48] [97300] global_step=97300, grad_norm=3.7685444355010986, loss=2.461954116821289
I0307 13:43:20.293299 140021218662144 logging_writer.py:48] [97400] global_step=97400, grad_norm=3.8611459732055664, loss=2.5458149909973145
I0307 13:44:00.271043 140021210269440 logging_writer.py:48] [97500] global_step=97500, grad_norm=3.729794502258301, loss=2.437676429748535
I0307 13:44:38.887278 140021218662144 logging_writer.py:48] [97600] global_step=97600, grad_norm=3.6438653469085693, loss=2.486156463623047
I0307 13:45:16.944847 140021210269440 logging_writer.py:48] [97700] global_step=97700, grad_norm=4.047133445739746, loss=2.4885764122009277
I0307 13:45:56.532721 140021218662144 logging_writer.py:48] [97800] global_step=97800, grad_norm=3.751929998397827, loss=2.5623180866241455
I0307 13:46:36.027460 140021210269440 logging_writer.py:48] [97900] global_step=97900, grad_norm=3.6661553382873535, loss=2.5177836418151855
I0307 13:47:16.001784 140021218662144 logging_writer.py:48] [98000] global_step=98000, grad_norm=3.6924779415130615, loss=2.579741954803467
I0307 13:47:55.563909 140021210269440 logging_writer.py:48] [98100] global_step=98100, grad_norm=3.7521767616271973, loss=2.4653303623199463
I0307 13:48:33.050609 140178276385984 spec.py:321] Evaluating on the training split.
I0307 13:48:45.174705 140178276385984 spec.py:333] Evaluating on the validation split.
I0307 13:49:04.714308 140178276385984 spec.py:349] Evaluating on the test split.
I0307 13:49:06.444095 140178276385984 submission_runner.py:469] Time since start: 41187.49s, 	Step: 98196, 	{'train/accuracy': 0.7195471525192261, 'train/loss': 1.2031362056732178, 'validation/accuracy': 0.6633399724960327, 'validation/loss': 1.463394045829773, 'validation/num_examples': 50000, 'test/accuracy': 0.5338000059127808, 'test/loss': 2.1390933990478516, 'test/num_examples': 10000, 'score': 38304.8749256134, 'total_duration': 41187.48756098747, 'accumulated_submission_time': 38304.8749256134, 'accumulated_eval_time': 2863.2034561634064, 'accumulated_logging_time': 8.915769577026367}
I0307 13:49:06.548228 140021218662144 logging_writer.py:48] [98196] accumulated_eval_time=2863.2, accumulated_logging_time=8.91577, accumulated_submission_time=38304.9, global_step=98196, preemption_count=0, score=38304.9, test/accuracy=0.5338, test/loss=2.13909, test/num_examples=10000, total_duration=41187.5, train/accuracy=0.719547, train/loss=1.20314, validation/accuracy=0.66334, validation/loss=1.46339, validation/num_examples=50000
I0307 13:49:08.375301 140021210269440 logging_writer.py:48] [98200] global_step=98200, grad_norm=3.7668519020080566, loss=2.4752426147460938
I0307 13:49:47.849271 140021218662144 logging_writer.py:48] [98300] global_step=98300, grad_norm=3.766885757446289, loss=2.5433363914489746
I0307 13:50:27.811709 140021210269440 logging_writer.py:48] [98400] global_step=98400, grad_norm=3.6427369117736816, loss=2.406670093536377
I0307 13:51:07.975115 140021218662144 logging_writer.py:48] [98500] global_step=98500, grad_norm=3.5383405685424805, loss=2.592813491821289
I0307 13:51:47.598797 140021210269440 logging_writer.py:48] [98600] global_step=98600, grad_norm=3.2551207542419434, loss=2.5373759269714355
I0307 13:52:26.800000 140021218662144 logging_writer.py:48] [98700] global_step=98700, grad_norm=3.7381718158721924, loss=2.504077911376953
I0307 13:53:05.702399 140021210269440 logging_writer.py:48] [98800] global_step=98800, grad_norm=3.8678388595581055, loss=2.449146270751953
2025-03-07 13:53:13.377938: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 13:54:28.823587 140021218662144 logging_writer.py:48] [98900] global_step=98900, grad_norm=3.9142303466796875, loss=2.439816474914551
I0307 13:55:09.797555 140021210269440 logging_writer.py:48] [99000] global_step=99000, grad_norm=3.557222366333008, loss=2.6135365962982178
I0307 13:55:50.083196 140021218662144 logging_writer.py:48] [99100] global_step=99100, grad_norm=3.649296283721924, loss=2.5295512676239014
I0307 13:56:30.170992 140021210269440 logging_writer.py:48] [99200] global_step=99200, grad_norm=3.5788464546203613, loss=2.526745557785034
I0307 13:57:09.100072 140021218662144 logging_writer.py:48] [99300] global_step=99300, grad_norm=3.7349581718444824, loss=2.4537434577941895
I0307 13:57:36.871706 140178276385984 spec.py:321] Evaluating on the training split.
I0307 13:57:49.580691 140178276385984 spec.py:333] Evaluating on the validation split.
I0307 13:58:09.488154 140178276385984 spec.py:349] Evaluating on the test split.
I0307 13:58:11.250243 140178276385984 submission_runner.py:469] Time since start: 41732.29s, 	Step: 99369, 	{'train/accuracy': 0.7308474183082581, 'train/loss': 1.1716368198394775, 'validation/accuracy': 0.6662200093269348, 'validation/loss': 1.4630690813064575, 'validation/num_examples': 50000, 'test/accuracy': 0.5388000011444092, 'test/loss': 2.1034204959869385, 'test/num_examples': 10000, 'score': 38815.03141593933, 'total_duration': 41732.293737888336, 'accumulated_submission_time': 38815.03141593933, 'accumulated_eval_time': 2897.5818300247192, 'accumulated_logging_time': 9.04389762878418}
I0307 13:58:11.326692 140021210269440 logging_writer.py:48] [99369] accumulated_eval_time=2897.58, accumulated_logging_time=9.0439, accumulated_submission_time=38815, global_step=99369, preemption_count=0, score=38815, test/accuracy=0.5388, test/loss=2.10342, test/num_examples=10000, total_duration=41732.3, train/accuracy=0.730847, train/loss=1.17164, validation/accuracy=0.66622, validation/loss=1.46307, validation/num_examples=50000
I0307 13:58:23.893968 140021218662144 logging_writer.py:48] [99400] global_step=99400, grad_norm=3.842672824859619, loss=2.5082805156707764
I0307 13:59:03.511337 140021210269440 logging_writer.py:48] [99500] global_step=99500, grad_norm=3.713475465774536, loss=2.5322394371032715
I0307 13:59:43.169433 140021218662144 logging_writer.py:48] [99600] global_step=99600, grad_norm=3.6451339721679688, loss=2.5158259868621826
I0307 14:00:23.578584 140021210269440 logging_writer.py:48] [99700] global_step=99700, grad_norm=3.74432635307312, loss=2.5033483505249023
I0307 14:01:02.688682 140021218662144 logging_writer.py:48] [99800] global_step=99800, grad_norm=3.6310958862304688, loss=2.456695079803467
I0307 14:01:43.274218 140021210269440 logging_writer.py:48] [99900] global_step=99900, grad_norm=3.9570372104644775, loss=2.5562398433685303
I0307 14:02:27.565542 140021218662144 logging_writer.py:48] [100000] global_step=100000, grad_norm=3.7485225200653076, loss=2.509711265563965
I0307 14:03:06.469647 140021210269440 logging_writer.py:48] [100100] global_step=100100, grad_norm=3.867396116256714, loss=2.6513612270355225
I0307 14:03:44.366290 140021218662144 logging_writer.py:48] [100200] global_step=100200, grad_norm=3.7404391765594482, loss=2.4489753246307373
I0307 14:04:22.647176 140021210269440 logging_writer.py:48] [100300] global_step=100300, grad_norm=3.672058582305908, loss=2.482330560684204
I0307 14:05:02.174166 140021218662144 logging_writer.py:48] [100400] global_step=100400, grad_norm=3.952989339828491, loss=2.541563034057617
I0307 14:05:40.569449 140021210269440 logging_writer.py:48] [100500] global_step=100500, grad_norm=3.774421453475952, loss=2.569737434387207
I0307 14:06:18.475403 140021218662144 logging_writer.py:48] [100600] global_step=100600, grad_norm=4.021496772766113, loss=2.5705108642578125
I0307 14:06:41.414495 140178276385984 spec.py:321] Evaluating on the training split.
I0307 14:06:53.769212 140178276385984 spec.py:333] Evaluating on the validation split.
I0307 14:07:17.568254 140178276385984 spec.py:349] Evaluating on the test split.
I0307 14:07:19.300123 140178276385984 submission_runner.py:469] Time since start: 42280.34s, 	Step: 100662, 	{'train/accuracy': 0.7101601958274841, 'train/loss': 1.2412019968032837, 'validation/accuracy': 0.6539199948310852, 'validation/loss': 1.508751392364502, 'validation/num_examples': 50000, 'test/accuracy': 0.5207000374794006, 'test/loss': 2.2001237869262695, 'test/num_examples': 10000, 'score': 39324.945012807846, 'total_duration': 42280.34361362457, 'accumulated_submission_time': 39324.945012807846, 'accumulated_eval_time': 2935.467300415039, 'accumulated_logging_time': 9.144246339797974}
I0307 14:07:19.406854 140021210269440 logging_writer.py:48] [100662] accumulated_eval_time=2935.47, accumulated_logging_time=9.14425, accumulated_submission_time=39324.9, global_step=100662, preemption_count=0, score=39324.9, test/accuracy=0.5207, test/loss=2.20012, test/num_examples=10000, total_duration=42280.3, train/accuracy=0.71016, train/loss=1.2412, validation/accuracy=0.65392, validation/loss=1.50875, validation/num_examples=50000
I0307 14:07:34.720812 140021218662144 logging_writer.py:48] [100700] global_step=100700, grad_norm=4.105618476867676, loss=2.540888786315918
I0307 14:08:13.608558 140021210269440 logging_writer.py:48] [100800] global_step=100800, grad_norm=3.5254902839660645, loss=2.471242904663086
I0307 14:08:52.182636 140021218662144 logging_writer.py:48] [100900] global_step=100900, grad_norm=3.5424482822418213, loss=2.4465675354003906
I0307 14:09:29.976115 140021210269440 logging_writer.py:48] [101000] global_step=101000, grad_norm=4.115224361419678, loss=2.47052264213562
I0307 14:10:08.421774 140021218662144 logging_writer.py:48] [101100] global_step=101100, grad_norm=4.022703170776367, loss=2.4101967811584473
I0307 14:10:52.707094 140021210269440 logging_writer.py:48] [101200] global_step=101200, grad_norm=4.221113204956055, loss=2.538909912109375
I0307 14:11:35.344984 140021218662144 logging_writer.py:48] [101300] global_step=101300, grad_norm=3.9073293209075928, loss=2.442272901535034
I0307 14:12:16.485200 140021210269440 logging_writer.py:48] [101400] global_step=101400, grad_norm=3.859569787979126, loss=2.500490665435791
I0307 14:12:56.634931 140021218662144 logging_writer.py:48] [101500] global_step=101500, grad_norm=3.665339946746826, loss=2.463717460632324
I0307 14:13:36.335410 140021210269440 logging_writer.py:48] [101600] global_step=101600, grad_norm=3.984773874282837, loss=2.581007719039917
I0307 14:14:16.484487 140021218662144 logging_writer.py:48] [101700] global_step=101700, grad_norm=3.6822986602783203, loss=2.524036407470703
I0307 14:14:56.990765 140021210269440 logging_writer.py:48] [101800] global_step=101800, grad_norm=3.7259867191314697, loss=2.4268057346343994
I0307 14:15:36.888190 140021218662144 logging_writer.py:48] [101900] global_step=101900, grad_norm=3.877082586288452, loss=2.4390640258789062
I0307 14:15:49.497102 140178276385984 spec.py:321] Evaluating on the training split.
I0307 14:16:02.845639 140178276385984 spec.py:333] Evaluating on the validation split.
I0307 14:16:26.871072 140178276385984 spec.py:349] Evaluating on the test split.
I0307 14:16:28.599140 140178276385984 submission_runner.py:469] Time since start: 42829.64s, 	Step: 101932, 	{'train/accuracy': 0.7271006107330322, 'train/loss': 1.1802823543548584, 'validation/accuracy': 0.664139986038208, 'validation/loss': 1.456663727760315, 'validation/num_examples': 50000, 'test/accuracy': 0.5317000150680542, 'test/loss': 2.14662766456604, 'test/num_examples': 10000, 'score': 39834.85927581787, 'total_duration': 42829.64259696007, 'accumulated_submission_time': 39834.85927581787, 'accumulated_eval_time': 2974.5691351890564, 'accumulated_logging_time': 9.28302550315857}
I0307 14:16:28.680764 140021210269440 logging_writer.py:48] [101932] accumulated_eval_time=2974.57, accumulated_logging_time=9.28303, accumulated_submission_time=39834.9, global_step=101932, preemption_count=0, score=39834.9, test/accuracy=0.5317, test/loss=2.14663, test/num_examples=10000, total_duration=42829.6, train/accuracy=0.727101, train/loss=1.18028, validation/accuracy=0.66414, validation/loss=1.45666, validation/num_examples=50000
I0307 14:16:56.034312 140021218662144 logging_writer.py:48] [102000] global_step=102000, grad_norm=5.058086395263672, loss=2.5805795192718506
I0307 14:17:42.582704 140021210269440 logging_writer.py:48] [102100] global_step=102100, grad_norm=3.5729284286499023, loss=2.612124443054199
I0307 14:18:28.872009 140021218662144 logging_writer.py:48] [102200] global_step=102200, grad_norm=4.119240760803223, loss=2.4535329341888428
I0307 14:19:11.542108 140021210269440 logging_writer.py:48] [102300] global_step=102300, grad_norm=3.9044079780578613, loss=2.545400381088257
I0307 14:19:52.296808 140021218662144 logging_writer.py:48] [102400] global_step=102400, grad_norm=3.5487136840820312, loss=2.4320626258850098
I0307 14:20:35.148170 140021210269440 logging_writer.py:48] [102500] global_step=102500, grad_norm=3.460733652114868, loss=2.627551794052124
I0307 14:21:15.659076 140021218662144 logging_writer.py:48] [102600] global_step=102600, grad_norm=3.7496984004974365, loss=2.5100769996643066
I0307 14:21:55.553570 140021210269440 logging_writer.py:48] [102700] global_step=102700, grad_norm=4.090237617492676, loss=2.439732789993286
I0307 14:22:37.914613 140021218662144 logging_writer.py:48] [102800] global_step=102800, grad_norm=3.8375470638275146, loss=2.394465446472168
I0307 14:23:24.251081 140021210269440 logging_writer.py:48] [102900] global_step=102900, grad_norm=3.8630881309509277, loss=2.5015299320220947
I0307 14:24:05.803063 140021218662144 logging_writer.py:48] [103000] global_step=103000, grad_norm=3.9110755920410156, loss=2.5073556900024414
I0307 14:24:49.254889 140021210269440 logging_writer.py:48] [103100] global_step=103100, grad_norm=4.130645275115967, loss=2.5024731159210205
I0307 14:24:58.643162 140178276385984 spec.py:321] Evaluating on the training split.
I0307 14:25:11.700022 140178276385984 spec.py:333] Evaluating on the validation split.
I0307 14:25:29.056633 140178276385984 spec.py:349] Evaluating on the test split.
I0307 14:25:30.790941 140178276385984 submission_runner.py:469] Time since start: 43371.83s, 	Step: 103121, 	{'train/accuracy': 0.7332987785339355, 'train/loss': 1.1505793333053589, 'validation/accuracy': 0.6702600121498108, 'validation/loss': 1.4429877996444702, 'validation/num_examples': 50000, 'test/accuracy': 0.5444000363349915, 'test/loss': 2.080712080001831, 'test/num_examples': 10000, 'score': 40344.64529109001, 'total_duration': 43371.83443522453, 'accumulated_submission_time': 40344.64529109001, 'accumulated_eval_time': 3006.7167494297028, 'accumulated_logging_time': 9.402681112289429}
I0307 14:25:30.877703 140021218662144 logging_writer.py:48] [103121] accumulated_eval_time=3006.72, accumulated_logging_time=9.40268, accumulated_submission_time=40344.6, global_step=103121, preemption_count=0, score=40344.6, test/accuracy=0.5444, test/loss=2.08071, test/num_examples=10000, total_duration=43371.8, train/accuracy=0.733299, train/loss=1.15058, validation/accuracy=0.67026, validation/loss=1.44299, validation/num_examples=50000
I0307 14:26:02.461739 140021210269440 logging_writer.py:48] [103200] global_step=103200, grad_norm=4.460268497467041, loss=2.5001320838928223
I0307 14:26:41.781204 140021218662144 logging_writer.py:48] [103300] global_step=103300, grad_norm=3.633087396621704, loss=2.5322659015655518
I0307 14:27:21.300752 140021210269440 logging_writer.py:48] [103400] global_step=103400, grad_norm=4.31453800201416, loss=2.483417272567749
I0307 14:28:00.555078 140021218662144 logging_writer.py:48] [103500] global_step=103500, grad_norm=3.591230869293213, loss=2.4996466636657715
I0307 14:28:40.141040 140021210269440 logging_writer.py:48] [103600] global_step=103600, grad_norm=3.8214547634124756, loss=2.4907543659210205
I0307 14:29:19.296230 140021218662144 logging_writer.py:48] [103700] global_step=103700, grad_norm=3.924900531768799, loss=2.5705718994140625
I0307 14:30:07.090203 140021210269440 logging_writer.py:48] [103800] global_step=103800, grad_norm=3.7806098461151123, loss=2.485992431640625
I0307 14:31:32.262550 140021218662144 logging_writer.py:48] [103900] global_step=103900, grad_norm=4.52998685836792, loss=2.515814781188965
I0307 14:32:14.362107 140021210269440 logging_writer.py:48] [104000] global_step=104000, grad_norm=3.9599170684814453, loss=2.4799511432647705
I0307 14:32:57.885502 140021218662144 logging_writer.py:48] [104100] global_step=104100, grad_norm=3.7406086921691895, loss=2.5445635318756104
I0307 14:33:49.493077 140021210269440 logging_writer.py:48] [104200] global_step=104200, grad_norm=3.9669671058654785, loss=2.4678893089294434
I0307 14:34:01.006035 140178276385984 spec.py:321] Evaluating on the training split.
I0307 14:34:15.524065 140178276385984 spec.py:333] Evaluating on the validation split.
I0307 14:34:36.167863 140178276385984 spec.py:349] Evaluating on the test split.
I0307 14:34:37.896424 140178276385984 submission_runner.py:469] Time since start: 43918.94s, 	Step: 104221, 	{'train/accuracy': 0.7423269748687744, 'train/loss': 1.1134259700775146, 'validation/accuracy': 0.6682400107383728, 'validation/loss': 1.4516679048538208, 'validation/num_examples': 50000, 'test/accuracy': 0.5444000363349915, 'test/loss': 2.122929096221924, 'test/num_examples': 10000, 'score': 40854.62970161438, 'total_duration': 43918.93988966942, 'accumulated_submission_time': 40854.62970161438, 'accumulated_eval_time': 3043.606955051422, 'accumulated_logging_time': 9.508100986480713}
I0307 14:34:38.018954 140021218662144 logging_writer.py:48] [104221] accumulated_eval_time=3043.61, accumulated_logging_time=9.5081, accumulated_submission_time=40854.6, global_step=104221, preemption_count=0, score=40854.6, test/accuracy=0.5444, test/loss=2.12293, test/num_examples=10000, total_duration=43918.9, train/accuracy=0.742327, train/loss=1.11343, validation/accuracy=0.66824, validation/loss=1.45167, validation/num_examples=50000
I0307 14:35:09.319353 140021210269440 logging_writer.py:48] [104300] global_step=104300, grad_norm=3.774383068084717, loss=2.5250020027160645
I0307 14:36:01.805273 140021218662144 logging_writer.py:48] [104400] global_step=104400, grad_norm=3.869633913040161, loss=2.537971019744873
I0307 14:36:50.718421 140021210269440 logging_writer.py:48] [104500] global_step=104500, grad_norm=3.9453725814819336, loss=2.467255115509033
I0307 14:37:37.173318 140021218662144 logging_writer.py:48] [104600] global_step=104600, grad_norm=3.8184385299682617, loss=2.509887456893921
I0307 14:38:22.474517 140021210269440 logging_writer.py:48] [104700] global_step=104700, grad_norm=3.504441499710083, loss=2.4303691387176514
I0307 14:39:00.925017 140021218662144 logging_writer.py:48] [104800] global_step=104800, grad_norm=4.266619682312012, loss=2.459594964981079
I0307 14:39:41.163209 140021210269440 logging_writer.py:48] [104900] global_step=104900, grad_norm=3.777775287628174, loss=2.470620632171631
I0307 14:40:26.809543 140021218662144 logging_writer.py:48] [105000] global_step=105000, grad_norm=3.5522854328155518, loss=2.410304546356201
I0307 14:41:06.926288 140021210269440 logging_writer.py:48] [105100] global_step=105100, grad_norm=3.5684540271759033, loss=2.456188678741455
I0307 14:41:44.849940 140021218662144 logging_writer.py:48] [105200] global_step=105200, grad_norm=3.6220033168792725, loss=2.500645875930786
I0307 14:42:22.385057 140021210269440 logging_writer.py:48] [105300] global_step=105300, grad_norm=4.093290328979492, loss=2.53184175491333
I0307 14:43:00.340785 140021218662144 logging_writer.py:48] [105400] global_step=105400, grad_norm=3.862520217895508, loss=2.630026340484619
I0307 14:43:08.198452 140178276385984 spec.py:321] Evaluating on the training split.
I0307 14:43:21.244093 140178276385984 spec.py:333] Evaluating on the validation split.
I0307 14:43:40.907681 140178276385984 spec.py:349] Evaluating on the test split.
I0307 14:43:42.809163 140178276385984 submission_runner.py:469] Time since start: 44463.85s, 	Step: 105422, 	{'train/accuracy': 0.7566964030265808, 'train/loss': 1.0610973834991455, 'validation/accuracy': 0.6690199971199036, 'validation/loss': 1.450792670249939, 'validation/num_examples': 50000, 'test/accuracy': 0.5405000448226929, 'test/loss': 2.126655340194702, 'test/num_examples': 10000, 'score': 41364.64756274223, 'total_duration': 44463.8526802063, 'accumulated_submission_time': 41364.64756274223, 'accumulated_eval_time': 3078.2175245285034, 'accumulated_logging_time': 9.650720834732056}
I0307 14:43:42.884699 140021210269440 logging_writer.py:48] [105422] accumulated_eval_time=3078.22, accumulated_logging_time=9.65072, accumulated_submission_time=41364.6, global_step=105422, preemption_count=0, score=41364.6, test/accuracy=0.5405, test/loss=2.12666, test/num_examples=10000, total_duration=44463.9, train/accuracy=0.756696, train/loss=1.0611, validation/accuracy=0.66902, validation/loss=1.45079, validation/num_examples=50000
I0307 14:44:13.601805 140021218662144 logging_writer.py:48] [105500] global_step=105500, grad_norm=3.9731862545013428, loss=2.44744610786438
I0307 14:44:53.286641 140021210269440 logging_writer.py:48] [105600] global_step=105600, grad_norm=3.6600096225738525, loss=2.4452145099639893
I0307 14:45:33.091839 140021218662144 logging_writer.py:48] [105700] global_step=105700, grad_norm=3.6825263500213623, loss=2.385692834854126
I0307 14:46:14.292887 140021210269440 logging_writer.py:48] [105800] global_step=105800, grad_norm=4.054088592529297, loss=2.475109577178955
I0307 14:47:06.394522 140021218662144 logging_writer.py:48] [105900] global_step=105900, grad_norm=4.0910491943359375, loss=2.4680864810943604
I0307 14:47:47.056453 140021210269440 logging_writer.py:48] [106000] global_step=106000, grad_norm=4.028474807739258, loss=2.4744515419006348
I0307 14:48:25.583083 140021218662144 logging_writer.py:48] [106100] global_step=106100, grad_norm=4.657578468322754, loss=2.555588960647583
I0307 14:49:08.615933 140021210269440 logging_writer.py:48] [106200] global_step=106200, grad_norm=4.006453990936279, loss=2.4706552028656006
I0307 14:49:50.133162 140021218662144 logging_writer.py:48] [106300] global_step=106300, grad_norm=4.110212802886963, loss=2.4545705318450928
I0307 14:50:28.915801 140021210269440 logging_writer.py:48] [106400] global_step=106400, grad_norm=3.609041929244995, loss=2.521764039993286
I0307 14:51:07.974491 140021218662144 logging_writer.py:48] [106500] global_step=106500, grad_norm=4.757110118865967, loss=2.5625040531158447
I0307 14:51:47.959416 140021210269440 logging_writer.py:48] [106600] global_step=106600, grad_norm=4.0811285972595215, loss=2.5301194190979004
I0307 14:52:12.857164 140178276385984 spec.py:321] Evaluating on the training split.
I0307 14:52:25.877373 140178276385984 spec.py:333] Evaluating on the validation split.
I0307 14:52:45.051219 140178276385984 spec.py:349] Evaluating on the test split.
I0307 14:52:46.779363 140178276385984 submission_runner.py:469] Time since start: 45007.82s, 	Step: 106665, 	{'train/accuracy': 0.7568159699440002, 'train/loss': 1.0291569232940674, 'validation/accuracy': 0.6678599715232849, 'validation/loss': 1.4241039752960205, 'validation/num_examples': 50000, 'test/accuracy': 0.544700026512146, 'test/loss': 2.0712826251983643, 'test/num_examples': 10000, 'score': 41874.44925785065, 'total_duration': 45007.82282829285, 'accumulated_submission_time': 41874.44925785065, 'accumulated_eval_time': 3112.1395366191864, 'accumulated_logging_time': 9.751102924346924}
I0307 14:52:46.857166 140021218662144 logging_writer.py:48] [106665] accumulated_eval_time=3112.14, accumulated_logging_time=9.7511, accumulated_submission_time=41874.4, global_step=106665, preemption_count=0, score=41874.4, test/accuracy=0.5447, test/loss=2.07128, test/num_examples=10000, total_duration=45007.8, train/accuracy=0.756816, train/loss=1.02916, validation/accuracy=0.66786, validation/loss=1.4241, validation/num_examples=50000
I0307 14:53:01.228204 140021210269440 logging_writer.py:48] [106700] global_step=106700, grad_norm=3.710883378982544, loss=2.413405656814575
I0307 14:53:48.255558 140021218662144 logging_writer.py:48] [106800] global_step=106800, grad_norm=3.7813539505004883, loss=2.4180521965026855
I0307 14:54:36.494669 140021210269440 logging_writer.py:48] [106900] global_step=106900, grad_norm=3.990823268890381, loss=2.48992919921875
I0307 14:55:16.398045 140021218662144 logging_writer.py:48] [107000] global_step=107000, grad_norm=3.600170373916626, loss=2.4483437538146973
I0307 14:55:54.903227 140021210269440 logging_writer.py:48] [107100] global_step=107100, grad_norm=3.7074053287506104, loss=2.4591197967529297
I0307 14:56:34.850598 140021218662144 logging_writer.py:48] [107200] global_step=107200, grad_norm=4.118967533111572, loss=2.531632423400879
I0307 14:57:15.193866 140021210269440 logging_writer.py:48] [107300] global_step=107300, grad_norm=4.019218444824219, loss=2.426849365234375
I0307 14:57:55.932954 140021218662144 logging_writer.py:48] [107400] global_step=107400, grad_norm=3.6917450428009033, loss=2.4660913944244385
I0307 14:58:38.912072 140021210269440 logging_writer.py:48] [107500] global_step=107500, grad_norm=3.7368435859680176, loss=2.458862066268921
I0307 14:59:17.881720 140021218662144 logging_writer.py:48] [107600] global_step=107600, grad_norm=4.648158073425293, loss=2.5286903381347656
I0307 14:59:56.238891 140021210269440 logging_writer.py:48] [107700] global_step=107700, grad_norm=4.155536651611328, loss=2.4473166465759277
I0307 15:00:35.962321 140021218662144 logging_writer.py:48] [107800] global_step=107800, grad_norm=4.382911682128906, loss=2.479264974594116
I0307 15:01:15.935084 140021210269440 logging_writer.py:48] [107900] global_step=107900, grad_norm=3.84413743019104, loss=2.4886138439178467
I0307 15:01:17.055797 140178276385984 spec.py:321] Evaluating on the training split.
I0307 15:01:31.209637 140178276385984 spec.py:333] Evaluating on the validation split.
I0307 15:01:50.429111 140178276385984 spec.py:349] Evaluating on the test split.
I0307 15:01:52.156997 140178276385984 submission_runner.py:469] Time since start: 45553.20s, 	Step: 107904, 	{'train/accuracy': 0.7282366156578064, 'train/loss': 1.162413477897644, 'validation/accuracy': 0.6685199737548828, 'validation/loss': 1.4427968263626099, 'validation/num_examples': 50000, 'test/accuracy': 0.5442000031471252, 'test/loss': 2.1069281101226807, 'test/num_examples': 10000, 'score': 42384.473433971405, 'total_duration': 45553.20047688484, 'accumulated_submission_time': 42384.473433971405, 'accumulated_eval_time': 3147.2405631542206, 'accumulated_logging_time': 9.86249566078186}
I0307 15:01:52.237205 140021218662144 logging_writer.py:48] [107904] accumulated_eval_time=3147.24, accumulated_logging_time=9.8625, accumulated_submission_time=42384.5, global_step=107904, preemption_count=0, score=42384.5, test/accuracy=0.5442, test/loss=2.10693, test/num_examples=10000, total_duration=45553.2, train/accuracy=0.728237, train/loss=1.16241, validation/accuracy=0.66852, validation/loss=1.4428, validation/num_examples=50000
I0307 15:02:33.676242 140021210269440 logging_writer.py:48] [108000] global_step=108000, grad_norm=4.25822639465332, loss=2.4800186157226562
I0307 15:03:12.746336 140021218662144 logging_writer.py:48] [108100] global_step=108100, grad_norm=4.780063629150391, loss=2.521345376968384
I0307 15:03:54.743436 140021210269440 logging_writer.py:48] [108200] global_step=108200, grad_norm=3.908118724822998, loss=2.3919291496276855
I0307 15:04:37.954382 140021218662144 logging_writer.py:48] [108300] global_step=108300, grad_norm=4.492244720458984, loss=2.4878602027893066
I0307 15:05:24.298104 140021210269440 logging_writer.py:48] [108400] global_step=108400, grad_norm=4.568363189697266, loss=2.539377212524414
I0307 15:06:20.413704 140021218662144 logging_writer.py:48] [108500] global_step=108500, grad_norm=3.9429898262023926, loss=2.4985544681549072
I0307 15:07:11.639214 140021210269440 logging_writer.py:48] [108600] global_step=108600, grad_norm=4.642932891845703, loss=2.436596155166626
I0307 15:08:13.926421 140021218662144 logging_writer.py:48] [108700] global_step=108700, grad_norm=3.9428906440734863, loss=2.435758352279663
I0307 15:09:14.302573 140021210269440 logging_writer.py:48] [108800] global_step=108800, grad_norm=3.975147008895874, loss=2.477015733718872
I0307 15:09:59.860036 140021218662144 logging_writer.py:48] [108900] global_step=108900, grad_norm=3.9053494930267334, loss=2.416923761367798
I0307 15:10:22.465568 140178276385984 spec.py:321] Evaluating on the training split.
I0307 15:10:34.570227 140178276385984 spec.py:333] Evaluating on the validation split.
I0307 15:10:52.962068 140178276385984 spec.py:349] Evaluating on the test split.
I0307 15:10:54.722303 140178276385984 submission_runner.py:469] Time since start: 46095.77s, 	Step: 108958, 	{'train/accuracy': 0.725027859210968, 'train/loss': 1.1865731477737427, 'validation/accuracy': 0.6681199669837952, 'validation/loss': 1.4408783912658691, 'validation/num_examples': 50000, 'test/accuracy': 0.537600040435791, 'test/loss': 2.1079046726226807, 'test/num_examples': 10000, 'score': 42894.51804852486, 'total_duration': 46095.765778541565, 'accumulated_submission_time': 42894.51804852486, 'accumulated_eval_time': 3179.497117996216, 'accumulated_logging_time': 10.004640579223633}
I0307 15:10:54.878381 140021210269440 logging_writer.py:48] [108958] accumulated_eval_time=3179.5, accumulated_logging_time=10.0046, accumulated_submission_time=42894.5, global_step=108958, preemption_count=0, score=42894.5, test/accuracy=0.5376, test/loss=2.1079, test/num_examples=10000, total_duration=46095.8, train/accuracy=0.725028, train/loss=1.18657, validation/accuracy=0.66812, validation/loss=1.44088, validation/num_examples=50000
I0307 15:11:12.033587 140021218662144 logging_writer.py:48] [109000] global_step=109000, grad_norm=3.903156042098999, loss=2.4530839920043945
I0307 15:11:51.739583 140021210269440 logging_writer.py:48] [109100] global_step=109100, grad_norm=3.9287335872650146, loss=2.575862407684326
I0307 15:12:31.315156 140021218662144 logging_writer.py:48] [109200] global_step=109200, grad_norm=4.013214111328125, loss=2.46327543258667
I0307 15:13:17.101862 140021210269440 logging_writer.py:48] [109300] global_step=109300, grad_norm=3.9386348724365234, loss=2.3490829467773438
I0307 15:14:02.573799 140021218662144 logging_writer.py:48] [109400] global_step=109400, grad_norm=3.5793418884277344, loss=2.4498491287231445
I0307 15:14:47.691892 140021210269440 logging_writer.py:48] [109500] global_step=109500, grad_norm=3.907841205596924, loss=2.464242696762085
I0307 15:15:35.631298 140021218662144 logging_writer.py:48] [109600] global_step=109600, grad_norm=3.9310050010681152, loss=2.412886142730713
I0307 15:16:28.907534 140021210269440 logging_writer.py:48] [109700] global_step=109700, grad_norm=4.3798828125, loss=2.471116542816162
I0307 15:17:10.076700 140021218662144 logging_writer.py:48] [109800] global_step=109800, grad_norm=4.010058403015137, loss=2.395064115524292
I0307 15:18:47.993442 140021210269440 logging_writer.py:48] [109900] global_step=109900, grad_norm=3.785264015197754, loss=2.4112141132354736
I0307 15:19:24.982407 140178276385984 spec.py:321] Evaluating on the training split.
I0307 15:19:39.292413 140178276385984 spec.py:333] Evaluating on the validation split.
I0307 15:20:00.282090 140178276385984 spec.py:349] Evaluating on the test split.
I0307 15:20:02.011758 140178276385984 submission_runner.py:469] Time since start: 46643.06s, 	Step: 109941, 	{'train/accuracy': 0.7311065196990967, 'train/loss': 1.148652195930481, 'validation/accuracy': 0.6694200038909912, 'validation/loss': 1.426511526107788, 'validation/num_examples': 50000, 'test/accuracy': 0.5468000173568726, 'test/loss': 2.093478202819824, 'test/num_examples': 10000, 'score': 43404.46943116188, 'total_duration': 46643.05524802208, 'accumulated_submission_time': 43404.46943116188, 'accumulated_eval_time': 3216.5263097286224, 'accumulated_logging_time': 10.200197219848633}
I0307 15:20:02.209345 140021218662144 logging_writer.py:48] [109941] accumulated_eval_time=3216.53, accumulated_logging_time=10.2002, accumulated_submission_time=43404.5, global_step=109941, preemption_count=0, score=43404.5, test/accuracy=0.5468, test/loss=2.09348, test/num_examples=10000, total_duration=46643.1, train/accuracy=0.731107, train/loss=1.14865, validation/accuracy=0.66942, validation/loss=1.42651, validation/num_examples=50000
I0307 15:20:26.420076 140021210269440 logging_writer.py:48] [110000] global_step=110000, grad_norm=4.2513861656188965, loss=2.3791816234588623
I0307 15:21:07.003427 140021218662144 logging_writer.py:48] [110100] global_step=110100, grad_norm=3.559548854827881, loss=2.435744524002075
I0307 15:21:45.242690 140021210269440 logging_writer.py:48] [110200] global_step=110200, grad_norm=4.763662338256836, loss=2.4758052825927734
I0307 15:22:26.356626 140021218662144 logging_writer.py:48] [110300] global_step=110300, grad_norm=4.154496192932129, loss=2.3981592655181885
I0307 15:23:09.068987 140021210269440 logging_writer.py:48] [110400] global_step=110400, grad_norm=3.780726194381714, loss=2.455644369125366
I0307 15:23:51.901314 140021218662144 logging_writer.py:48] [110500] global_step=110500, grad_norm=3.9826576709747314, loss=2.465116500854492
I0307 15:24:39.966940 140021210269440 logging_writer.py:48] [110600] global_step=110600, grad_norm=4.392353057861328, loss=2.4459943771362305
I0307 15:25:34.818210 140021218662144 logging_writer.py:48] [110700] global_step=110700, grad_norm=4.011219024658203, loss=2.535487174987793
I0307 15:26:28.649296 140021210269440 logging_writer.py:48] [110800] global_step=110800, grad_norm=4.779915809631348, loss=2.5042383670806885
I0307 15:27:21.704838 140021218662144 logging_writer.py:48] [110900] global_step=110900, grad_norm=3.8707096576690674, loss=2.3855323791503906
I0307 15:28:12.189895 140021210269440 logging_writer.py:48] [111000] global_step=111000, grad_norm=4.184224605560303, loss=2.5044713020324707
I0307 15:28:32.343120 140178276385984 spec.py:321] Evaluating on the training split.
I0307 15:28:45.881580 140178276385984 spec.py:333] Evaluating on the validation split.
I0307 15:29:03.547310 140178276385984 spec.py:349] Evaluating on the test split.
I0307 15:29:05.272495 140178276385984 submission_runner.py:469] Time since start: 47186.32s, 	Step: 111037, 	{'train/accuracy': 0.7291334271430969, 'train/loss': 1.175467610359192, 'validation/accuracy': 0.665399968624115, 'validation/loss': 1.4693171977996826, 'validation/num_examples': 50000, 'test/accuracy': 0.5387000441551208, 'test/loss': 2.137195348739624, 'test/num_examples': 10000, 'score': 43914.41852641106, 'total_duration': 47186.31592774391, 'accumulated_submission_time': 43914.41852641106, 'accumulated_eval_time': 3249.455461025238, 'accumulated_logging_time': 10.457334041595459}
I0307 15:29:05.371615 140021218662144 logging_writer.py:48] [111037] accumulated_eval_time=3249.46, accumulated_logging_time=10.4573, accumulated_submission_time=43914.4, global_step=111037, preemption_count=0, score=43914.4, test/accuracy=0.5387, test/loss=2.1372, test/num_examples=10000, total_duration=47186.3, train/accuracy=0.729133, train/loss=1.17547, validation/accuracy=0.6654, validation/loss=1.46932, validation/num_examples=50000
I0307 15:29:39.346059 140021210269440 logging_writer.py:48] [111100] global_step=111100, grad_norm=4.560915470123291, loss=2.4676353931427
I0307 15:30:36.020756 140021218662144 logging_writer.py:48] [111200] global_step=111200, grad_norm=4.260076522827148, loss=2.470252275466919
I0307 15:31:17.603765 140021210269440 logging_writer.py:48] [111300] global_step=111300, grad_norm=3.726578950881958, loss=2.467766284942627
2025-03-07 15:31:35.940234: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 15:32:06.560109 140021218662144 logging_writer.py:48] [111400] global_step=111400, grad_norm=4.596580505371094, loss=2.4194419384002686
I0307 15:32:47.909738 140021210269440 logging_writer.py:48] [111500] global_step=111500, grad_norm=4.26365327835083, loss=2.452582836151123
I0307 15:33:32.160497 140021218662144 logging_writer.py:48] [111600] global_step=111600, grad_norm=4.02116584777832, loss=2.4898440837860107
I0307 15:34:51.912763 140021210269440 logging_writer.py:48] [111700] global_step=111700, grad_norm=4.019611835479736, loss=2.420964241027832
I0307 15:35:49.873637 140021218662144 logging_writer.py:48] [111800] global_step=111800, grad_norm=3.7219510078430176, loss=2.479037046432495
I0307 15:36:35.839164 140021210269440 logging_writer.py:48] [111900] global_step=111900, grad_norm=4.346735000610352, loss=2.3758668899536133
I0307 15:37:18.179824 140021218662144 logging_writer.py:48] [112000] global_step=112000, grad_norm=4.155086994171143, loss=2.541834592819214
I0307 15:37:35.919466 140178276385984 spec.py:321] Evaluating on the training split.
I0307 15:37:49.205695 140178276385984 spec.py:333] Evaluating on the validation split.
I0307 15:38:07.978356 140178276385984 spec.py:349] Evaluating on the test split.
I0307 15:38:09.745462 140178276385984 submission_runner.py:469] Time since start: 47730.79s, 	Step: 112040, 	{'train/accuracy': 0.7534478306770325, 'train/loss': 1.070895791053772, 'validation/accuracy': 0.6733999848365784, 'validation/loss': 1.4278932809829712, 'validation/num_examples': 50000, 'test/accuracy': 0.5451000332832336, 'test/loss': 2.084998846054077, 'test/num_examples': 10000, 'score': 44424.831065654755, 'total_duration': 47730.78890681267, 'accumulated_submission_time': 44424.831065654755, 'accumulated_eval_time': 3283.28125166893, 'accumulated_logging_time': 10.579755067825317}
I0307 15:38:09.806930 140021210269440 logging_writer.py:48] [112040] accumulated_eval_time=3283.28, accumulated_logging_time=10.5798, accumulated_submission_time=44424.8, global_step=112040, preemption_count=0, score=44424.8, test/accuracy=0.5451, test/loss=2.085, test/num_examples=10000, total_duration=47730.8, train/accuracy=0.753448, train/loss=1.0709, validation/accuracy=0.6734, validation/loss=1.42789, validation/num_examples=50000
I0307 15:38:42.188150 140021218662144 logging_writer.py:48] [112100] global_step=112100, grad_norm=3.731088399887085, loss=2.4122314453125
I0307 15:39:49.781061 140021210269440 logging_writer.py:48] [112200] global_step=112200, grad_norm=3.7219274044036865, loss=2.4484591484069824
I0307 15:40:43.510201 140021218662144 logging_writer.py:48] [112300] global_step=112300, grad_norm=4.335674285888672, loss=2.471909761428833
I0307 15:41:39.545394 140021210269440 logging_writer.py:48] [112400] global_step=112400, grad_norm=3.632695436477661, loss=2.4057695865631104
I0307 15:42:40.949727 140021218662144 logging_writer.py:48] [112500] global_step=112500, grad_norm=3.7610905170440674, loss=2.4023382663726807
2025-03-07 15:43:13.047928: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 15:43:19.461077 140021210269440 logging_writer.py:48] [112600] global_step=112600, grad_norm=4.210799217224121, loss=2.3974177837371826
I0307 15:43:59.141385 140021218662144 logging_writer.py:48] [112700] global_step=112700, grad_norm=4.116985321044922, loss=2.4273300170898438
I0307 15:44:43.588278 140021210269440 logging_writer.py:48] [112800] global_step=112800, grad_norm=4.4273362159729, loss=2.466947317123413
I0307 15:45:30.660777 140021218662144 logging_writer.py:48] [112900] global_step=112900, grad_norm=3.900080442428589, loss=2.452059268951416
I0307 15:46:11.962148 140021210269440 logging_writer.py:48] [113000] global_step=113000, grad_norm=4.259366512298584, loss=2.437351703643799
I0307 15:46:39.786769 140178276385984 spec.py:321] Evaluating on the training split.
I0307 15:46:53.169286 140178276385984 spec.py:333] Evaluating on the validation split.
I0307 15:47:10.682428 140178276385984 spec.py:349] Evaluating on the test split.
I0307 15:47:12.457262 140178276385984 submission_runner.py:469] Time since start: 48273.50s, 	Step: 113048, 	{'train/accuracy': 0.7387396097183228, 'train/loss': 1.133206844329834, 'validation/accuracy': 0.6808800101280212, 'validation/loss': 1.3862591981887817, 'validation/num_examples': 50000, 'test/accuracy': 0.555400013923645, 'test/loss': 2.0340847969055176, 'test/num_examples': 10000, 'score': 44934.65943312645, 'total_duration': 48273.50072813034, 'accumulated_submission_time': 44934.65943312645, 'accumulated_eval_time': 3315.951554298401, 'accumulated_logging_time': 10.680310487747192}
I0307 15:47:12.537849 140021218662144 logging_writer.py:48] [113048] accumulated_eval_time=3315.95, accumulated_logging_time=10.6803, accumulated_submission_time=44934.7, global_step=113048, preemption_count=0, score=44934.7, test/accuracy=0.5554, test/loss=2.03408, test/num_examples=10000, total_duration=48273.5, train/accuracy=0.73874, train/loss=1.13321, validation/accuracy=0.68088, validation/loss=1.38626, validation/num_examples=50000
I0307 15:47:51.811706 140021210269440 logging_writer.py:48] [113100] global_step=113100, grad_norm=4.106022357940674, loss=2.5367586612701416
I0307 15:48:50.715662 140021218662144 logging_writer.py:48] [113200] global_step=113200, grad_norm=4.174262523651123, loss=2.45238995552063
I0307 15:49:39.286412 140021210269440 logging_writer.py:48] [113300] global_step=113300, grad_norm=4.340237140655518, loss=2.3393144607543945
I0307 15:50:33.128984 140021218662144 logging_writer.py:48] [113400] global_step=113400, grad_norm=4.56193733215332, loss=2.4795737266540527
I0307 15:51:18.836446 140021210269440 logging_writer.py:48] [113500] global_step=113500, grad_norm=4.179628372192383, loss=2.5495388507843018
I0307 15:52:09.044574 140021218662144 logging_writer.py:48] [113600] global_step=113600, grad_norm=4.178701400756836, loss=2.452533483505249
I0307 15:53:26.557370 140021210269440 logging_writer.py:48] [113700] global_step=113700, grad_norm=4.335212230682373, loss=2.4182286262512207
I0307 15:54:12.738458 140021218662144 logging_writer.py:48] [113800] global_step=113800, grad_norm=3.861013412475586, loss=2.433454990386963
I0307 15:54:54.865950 140021210269440 logging_writer.py:48] [113900] global_step=113900, grad_norm=4.196821689605713, loss=2.3845877647399902
I0307 15:55:34.750041 140021218662144 logging_writer.py:48] [114000] global_step=114000, grad_norm=4.888717174530029, loss=2.4357223510742188
I0307 15:55:42.669867 140178276385984 spec.py:321] Evaluating on the training split.
I0307 15:55:55.156743 140178276385984 spec.py:333] Evaluating on the validation split.
I0307 15:56:22.092443 140178276385984 spec.py:349] Evaluating on the test split.
I0307 15:56:23.817224 140178276385984 submission_runner.py:469] Time since start: 48824.86s, 	Step: 114021, 	{'train/accuracy': 0.7410913705825806, 'train/loss': 1.1247379779815674, 'validation/accuracy': 0.6778799891471863, 'validation/loss': 1.4026718139648438, 'validation/num_examples': 50000, 'test/accuracy': 0.5468000173568726, 'test/loss': 2.063849925994873, 'test/num_examples': 10000, 'score': 45444.641672849655, 'total_duration': 48824.86071610451, 'accumulated_submission_time': 45444.641672849655, 'accumulated_eval_time': 3357.098758459091, 'accumulated_logging_time': 10.801420450210571}
I0307 15:56:23.885933 140021210269440 logging_writer.py:48] [114021] accumulated_eval_time=3357.1, accumulated_logging_time=10.8014, accumulated_submission_time=45444.6, global_step=114021, preemption_count=0, score=45444.6, test/accuracy=0.5468, test/loss=2.06385, test/num_examples=10000, total_duration=48824.9, train/accuracy=0.741091, train/loss=1.12474, validation/accuracy=0.67788, validation/loss=1.40267, validation/num_examples=50000
I0307 15:56:55.388670 140021218662144 logging_writer.py:48] [114100] global_step=114100, grad_norm=4.459516525268555, loss=2.441143035888672
I0307 15:57:56.655406 140021210269440 logging_writer.py:48] [114200] global_step=114200, grad_norm=4.114686489105225, loss=2.3785438537597656
I0307 15:58:55.696758 140021218662144 logging_writer.py:48] [114300] global_step=114300, grad_norm=4.193714141845703, loss=2.4793083667755127
I0307 15:59:46.487494 140021210269440 logging_writer.py:48] [114400] global_step=114400, grad_norm=4.517076015472412, loss=2.381770133972168
I0307 16:00:43.565278 140021218662144 logging_writer.py:48] [114500] global_step=114500, grad_norm=4.523565292358398, loss=2.4061615467071533
I0307 16:01:49.086416 140021210269440 logging_writer.py:48] [114600] global_step=114600, grad_norm=4.0606279373168945, loss=2.4124507904052734
I0307 16:02:57.960772 140021218662144 logging_writer.py:48] [114700] global_step=114700, grad_norm=3.8918397426605225, loss=2.3838839530944824
I0307 16:03:55.125316 140021210269440 logging_writer.py:48] [114800] global_step=114800, grad_norm=3.7858939170837402, loss=2.4464309215545654
I0307 16:04:50.705827 140021218662144 logging_writer.py:48] [114900] global_step=114900, grad_norm=3.911658763885498, loss=2.398324728012085
I0307 16:04:54.300500 140178276385984 spec.py:321] Evaluating on the training split.
I0307 16:05:08.519936 140178276385984 spec.py:333] Evaluating on the validation split.
I0307 16:05:29.089157 140178276385984 spec.py:349] Evaluating on the test split.
I0307 16:05:30.790441 140178276385984 submission_runner.py:469] Time since start: 49371.83s, 	Step: 114906, 	{'train/accuracy': 0.7554408311843872, 'train/loss': 1.054111123085022, 'validation/accuracy': 0.6805399656295776, 'validation/loss': 1.3793418407440186, 'validation/num_examples': 50000, 'test/accuracy': 0.5561000108718872, 'test/loss': 2.0117599964141846, 'test/num_examples': 10000, 'score': 45954.92693686485, 'total_duration': 49371.83388233185, 'accumulated_submission_time': 45954.92693686485, 'accumulated_eval_time': 3393.5884919166565, 'accumulated_logging_time': 10.899273157119751}
I0307 16:05:30.879730 140021210269440 logging_writer.py:48] [114906] accumulated_eval_time=3393.59, accumulated_logging_time=10.8993, accumulated_submission_time=45954.9, global_step=114906, preemption_count=0, score=45954.9, test/accuracy=0.5561, test/loss=2.01176, test/num_examples=10000, total_duration=49371.8, train/accuracy=0.755441, train/loss=1.05411, validation/accuracy=0.68054, validation/loss=1.37934, validation/num_examples=50000
I0307 16:06:16.095959 140021218662144 logging_writer.py:48] [115000] global_step=115000, grad_norm=3.8392250537872314, loss=2.3576085567474365
I0307 16:07:05.551057 140021210269440 logging_writer.py:48] [115100] global_step=115100, grad_norm=3.7961597442626953, loss=2.357693910598755
I0307 16:07:48.354047 140021218662144 logging_writer.py:48] [115200] global_step=115200, grad_norm=4.6308817863464355, loss=2.4386942386627197
I0307 16:08:33.474648 140021210269440 logging_writer.py:48] [115300] global_step=115300, grad_norm=4.79678201675415, loss=2.525270938873291
I0307 16:09:30.784927 140021218662144 logging_writer.py:48] [115400] global_step=115400, grad_norm=4.198040008544922, loss=2.515247106552124
I0307 16:10:34.417257 140021210269440 logging_writer.py:48] [115500] global_step=115500, grad_norm=4.137088775634766, loss=2.4950149059295654
I0307 16:11:52.347422 140021218662144 logging_writer.py:48] [115600] global_step=115600, grad_norm=4.639920234680176, loss=2.37174654006958
I0307 16:12:55.074562 140021210269440 logging_writer.py:48] [115700] global_step=115700, grad_norm=4.554230690002441, loss=2.374814510345459
I0307 16:14:01.240878 140178276385984 spec.py:321] Evaluating on the training split.
I0307 16:14:14.321887 140178276385984 spec.py:333] Evaluating on the validation split.
I0307 16:14:31.196424 140178276385984 spec.py:349] Evaluating on the test split.
I0307 16:14:32.936276 140178276385984 submission_runner.py:469] Time since start: 49913.98s, 	Step: 115798, 	{'train/accuracy': 0.7488042116165161, 'train/loss': 1.0620816946029663, 'validation/accuracy': 0.685699999332428, 'validation/loss': 1.3478524684906006, 'validation/num_examples': 50000, 'test/accuracy': 0.5600000023841858, 'test/loss': 1.978164792060852, 'test/num_examples': 10000, 'score': 46465.16021347046, 'total_duration': 49913.97975707054, 'accumulated_submission_time': 46465.16021347046, 'accumulated_eval_time': 3425.283721446991, 'accumulated_logging_time': 11.017818450927734}
I0307 16:14:33.061152 140021218662144 logging_writer.py:48] [115798] accumulated_eval_time=3425.28, accumulated_logging_time=11.0178, accumulated_submission_time=46465.2, global_step=115798, preemption_count=0, score=46465.2, test/accuracy=0.56, test/loss=1.97816, test/num_examples=10000, total_duration=49914, train/accuracy=0.748804, train/loss=1.06208, validation/accuracy=0.6857, validation/loss=1.34785, validation/num_examples=50000
I0307 16:14:34.163192 140021210269440 logging_writer.py:48] [115800] global_step=115800, grad_norm=4.391664505004883, loss=2.4372591972351074
I0307 16:15:34.714323 140021218662144 logging_writer.py:48] [115900] global_step=115900, grad_norm=4.178345680236816, loss=2.422464609146118
I0307 16:16:24.736306 140021210269440 logging_writer.py:48] [116000] global_step=116000, grad_norm=4.332716464996338, loss=2.406808376312256
I0307 16:17:34.966244 140021218662144 logging_writer.py:48] [116100] global_step=116100, grad_norm=4.13125467300415, loss=2.4766998291015625
I0307 16:18:35.056531 140021210269440 logging_writer.py:48] [116200] global_step=116200, grad_norm=4.078596115112305, loss=2.4159364700317383
I0307 16:19:24.660626 140021218662144 logging_writer.py:48] [116300] global_step=116300, grad_norm=4.446693420410156, loss=2.3670783042907715
I0307 16:20:34.366325 140021210269440 logging_writer.py:48] [116400] global_step=116400, grad_norm=4.317556858062744, loss=2.396850109100342
I0307 16:21:27.401884 140021218662144 logging_writer.py:48] [116500] global_step=116500, grad_norm=4.728106498718262, loss=2.4375288486480713
I0307 16:23:03.484482 140178276385984 spec.py:321] Evaluating on the training split.
I0307 16:23:15.767109 140178276385984 spec.py:333] Evaluating on the validation split.
I0307 16:23:34.177238 140178276385984 spec.py:349] Evaluating on the test split.
I0307 16:23:35.896053 140178276385984 submission_runner.py:469] Time since start: 50456.94s, 	Step: 116580, 	{'train/accuracy': 0.74613356590271, 'train/loss': 1.099212646484375, 'validation/accuracy': 0.6809999942779541, 'validation/loss': 1.3919414281845093, 'validation/num_examples': 50000, 'test/accuracy': 0.5516000390052795, 'test/loss': 2.0558104515075684, 'test/num_examples': 10000, 'score': 46975.46970319748, 'total_duration': 50456.93953156471, 'accumulated_submission_time': 46975.46970319748, 'accumulated_eval_time': 3457.695133447647, 'accumulated_logging_time': 11.170905351638794}
I0307 16:23:35.995305 140021210269440 logging_writer.py:48] [116580] accumulated_eval_time=3457.7, accumulated_logging_time=11.1709, accumulated_submission_time=46975.5, global_step=116580, preemption_count=0, score=46975.5, test/accuracy=0.5516, test/loss=2.05581, test/num_examples=10000, total_duration=50456.9, train/accuracy=0.746134, train/loss=1.09921, validation/accuracy=0.681, validation/loss=1.39194, validation/num_examples=50000
I0307 16:23:44.240552 140021218662144 logging_writer.py:48] [116600] global_step=116600, grad_norm=4.724347114562988, loss=2.5372250080108643
I0307 16:25:05.922659 140021210269440 logging_writer.py:48] [116700] global_step=116700, grad_norm=4.083262920379639, loss=2.302780866622925
I0307 16:26:26.368681 140021218662144 logging_writer.py:48] [116800] global_step=116800, grad_norm=4.502959251403809, loss=2.373220920562744
I0307 16:27:34.957248 140021210269440 logging_writer.py:48] [116900] global_step=116900, grad_norm=4.266458988189697, loss=2.46069598197937
I0307 16:28:27.711450 140021218662144 logging_writer.py:48] [117000] global_step=117000, grad_norm=4.139820575714111, loss=2.4935333728790283
I0307 16:29:25.810752 140021210269440 logging_writer.py:48] [117100] global_step=117100, grad_norm=3.86492657661438, loss=2.34721302986145
I0307 16:30:37.407243 140021218662144 logging_writer.py:48] [117200] global_step=117200, grad_norm=3.9798545837402344, loss=2.3463194370269775
I0307 16:31:42.211353 140021210269440 logging_writer.py:48] [117300] global_step=117300, grad_norm=4.273728370666504, loss=2.4699015617370605
I0307 16:32:06.132472 140178276385984 spec.py:321] Evaluating on the training split.
I0307 16:32:18.190246 140178276385984 spec.py:333] Evaluating on the validation split.
I0307 16:32:35.287240 140178276385984 spec.py:349] Evaluating on the test split.
I0307 16:32:37.028715 140178276385984 submission_runner.py:469] Time since start: 50998.04s, 	Step: 117330, 	{'train/accuracy': 0.7751116156578064, 'train/loss': 0.952418327331543, 'validation/accuracy': 0.6855199933052063, 'validation/loss': 1.340497612953186, 'validation/num_examples': 50000, 'test/accuracy': 0.5654000043869019, 'test/loss': 1.9599807262420654, 'test/num_examples': 10000, 'score': 47485.47335219383, 'total_duration': 50998.04479455948, 'accumulated_submission_time': 47485.47335219383, 'accumulated_eval_time': 3488.5638015270233, 'accumulated_logging_time': 11.319461584091187}
I0307 16:32:37.108348 140021218662144 logging_writer.py:48] [117330] accumulated_eval_time=3488.56, accumulated_logging_time=11.3195, accumulated_submission_time=47485.5, global_step=117330, preemption_count=0, score=47485.5, test/accuracy=0.5654, test/loss=1.95998, test/num_examples=10000, total_duration=50998, train/accuracy=0.775112, train/loss=0.952418, validation/accuracy=0.68552, validation/loss=1.3405, validation/num_examples=50000
I0307 16:33:26.708693 140021210269440 logging_writer.py:48] [117400] global_step=117400, grad_norm=4.044320106506348, loss=2.4139416217803955
I0307 16:34:43.001952 140021218662144 logging_writer.py:48] [117500] global_step=117500, grad_norm=4.050271034240723, loss=2.3642046451568604
I0307 16:35:33.630660 140021210269440 logging_writer.py:48] [117600] global_step=117600, grad_norm=4.471168518066406, loss=2.444146156311035
I0307 16:36:17.909687 140021218662144 logging_writer.py:48] [117700] global_step=117700, grad_norm=4.881441116333008, loss=2.5224435329437256
I0307 16:37:02.298253 140021210269440 logging_writer.py:48] [117800] global_step=117800, grad_norm=4.899360656738281, loss=2.462860107421875
I0307 16:38:20.773401 140021218662144 logging_writer.py:48] [117900] global_step=117900, grad_norm=4.0820794105529785, loss=2.3850436210632324
I0307 16:39:31.997861 140021210269440 logging_writer.py:48] [118000] global_step=118000, grad_norm=4.578685283660889, loss=2.5218563079833984
I0307 16:40:34.439042 140021218662144 logging_writer.py:48] [118100] global_step=118100, grad_norm=4.529832363128662, loss=2.382720947265625
I0307 16:41:07.152222 140178276385984 spec.py:321] Evaluating on the training split.
I0307 16:41:19.088192 140178276385984 spec.py:333] Evaluating on the validation split.
I0307 16:41:39.074408 140178276385984 spec.py:349] Evaluating on the test split.
I0307 16:41:40.805066 140178276385984 submission_runner.py:469] Time since start: 51541.83s, 	Step: 118141, 	{'train/accuracy': 0.7511559128761292, 'train/loss': 1.0472253561019897, 'validation/accuracy': 0.6897799968719482, 'validation/loss': 1.3284671306610107, 'validation/num_examples': 50000, 'test/accuracy': 0.5694000124931335, 'test/loss': 1.9590842723846436, 'test/num_examples': 10000, 'score': 47995.369913339615, 'total_duration': 51541.8345823288, 'accumulated_submission_time': 47995.369913339615, 'accumulated_eval_time': 3522.202502012253, 'accumulated_logging_time': 11.456263303756714}
I0307 16:41:40.910494 140021210269440 logging_writer.py:48] [118141] accumulated_eval_time=3522.2, accumulated_logging_time=11.4563, accumulated_submission_time=47995.4, global_step=118141, preemption_count=0, score=47995.4, test/accuracy=0.5694, test/loss=1.95908, test/num_examples=10000, total_duration=51541.8, train/accuracy=0.751156, train/loss=1.04723, validation/accuracy=0.68978, validation/loss=1.32847, validation/num_examples=50000
I0307 16:42:22.269151 140021218662144 logging_writer.py:48] [118200] global_step=118200, grad_norm=4.103855133056641, loss=2.4707908630371094
I0307 16:43:20.563629 140021210269440 logging_writer.py:48] [118300] global_step=118300, grad_norm=4.370814800262451, loss=2.406343698501587
I0307 16:44:31.772148 140021218662144 logging_writer.py:48] [118400] global_step=118400, grad_norm=4.635220527648926, loss=2.434757709503174
I0307 16:45:29.358464 140021210269440 logging_writer.py:48] [118500] global_step=118500, grad_norm=4.604172229766846, loss=2.331209182739258
I0307 16:46:15.520774 140021218662144 logging_writer.py:48] [118600] global_step=118600, grad_norm=4.681460857391357, loss=2.457951068878174
I0307 16:47:36.997398 140021210269440 logging_writer.py:48] [118700] global_step=118700, grad_norm=4.407124996185303, loss=2.413693428039551
I0307 16:48:42.577288 140021218662144 logging_writer.py:48] [118800] global_step=118800, grad_norm=5.071987628936768, loss=2.46574068069458
I0307 16:50:11.694355 140178276385984 spec.py:321] Evaluating on the training split.
I0307 16:50:22.738616 140178276385984 spec.py:333] Evaluating on the validation split.
I0307 16:50:39.534975 140178276385984 spec.py:349] Evaluating on the test split.
I0307 16:50:41.309857 140178276385984 submission_runner.py:469] Time since start: 52082.34s, 	Step: 118874, 	{'train/accuracy': 0.7639707922935486, 'train/loss': 1.0124475955963135, 'validation/accuracy': 0.6890599727630615, 'validation/loss': 1.3436685800552368, 'validation/num_examples': 50000, 'test/accuracy': 0.5657000541687012, 'test/loss': 1.9665162563323975, 'test/num_examples': 10000, 'score': 48506.04863142967, 'total_duration': 52082.33760142326, 'accumulated_submission_time': 48506.04863142967, 'accumulated_eval_time': 3551.8020944595337, 'accumulated_logging_time': 11.587366580963135}
I0307 16:50:41.416658 140021210269440 logging_writer.py:48] [118874] accumulated_eval_time=3551.8, accumulated_logging_time=11.5874, accumulated_submission_time=48506, global_step=118874, preemption_count=0, score=48506, test/accuracy=0.5657, test/loss=1.96652, test/num_examples=10000, total_duration=52082.3, train/accuracy=0.763971, train/loss=1.01245, validation/accuracy=0.68906, validation/loss=1.34367, validation/num_examples=50000
I0307 16:50:52.293637 140021218662144 logging_writer.py:48] [118900] global_step=118900, grad_norm=4.150154113769531, loss=2.385361433029175
I0307 16:51:58.901114 140021210269440 logging_writer.py:48] [119000] global_step=119000, grad_norm=4.523176670074463, loss=2.3983306884765625
I0307 16:53:18.627086 140021218662144 logging_writer.py:48] [119100] global_step=119100, grad_norm=4.363110542297363, loss=2.4372363090515137
I0307 16:56:26.647900 140021210269440 logging_writer.py:48] [119200] global_step=119200, grad_norm=4.354248046875, loss=2.4135539531707764
I0307 16:58:48.007280 140021218662144 logging_writer.py:48] [119300] global_step=119300, grad_norm=4.069833278656006, loss=2.452660322189331
I0307 16:59:12.372259 140178276385984 spec.py:321] Evaluating on the training split.
I0307 16:59:23.928068 140178276385984 spec.py:333] Evaluating on the validation split.
I0307 16:59:43.003113 140178276385984 spec.py:349] Evaluating on the test split.
I0307 16:59:44.777218 140178276385984 submission_runner.py:469] Time since start: 52625.82s, 	Step: 119323, 	{'train/accuracy': 0.7469706535339355, 'train/loss': 1.082323431968689, 'validation/accuracy': 0.6826399564743042, 'validation/loss': 1.3713523149490356, 'validation/num_examples': 50000, 'test/accuracy': 0.5574000477790833, 'test/loss': 2.0278215408325195, 'test/num_examples': 10000, 'score': 49016.9319229126, 'total_duration': 52625.82083821297, 'accumulated_submission_time': 49016.9319229126, 'accumulated_eval_time': 3584.207018852234, 'accumulated_logging_time': 11.715851783752441}
I0307 16:59:44.836085 140021210269440 logging_writer.py:48] [119323] accumulated_eval_time=3584.21, accumulated_logging_time=11.7159, accumulated_submission_time=49016.9, global_step=119323, preemption_count=0, score=49016.9, test/accuracy=0.5574, test/loss=2.02782, test/num_examples=10000, total_duration=52625.8, train/accuracy=0.746971, train/loss=1.08232, validation/accuracy=0.68264, validation/loss=1.37135, validation/num_examples=50000
I0307 17:00:37.818212 140021218662144 logging_writer.py:48] [119400] global_step=119400, grad_norm=4.512617111206055, loss=2.3961281776428223
I0307 17:02:21.650426 140021210269440 logging_writer.py:48] [119500] global_step=119500, grad_norm=4.521822929382324, loss=2.3892898559570312
I0307 17:03:16.581314 140021218662144 logging_writer.py:48] [119600] global_step=119600, grad_norm=4.093968391418457, loss=2.3698267936706543
I0307 17:04:12.835692 140021210269440 logging_writer.py:48] [119700] global_step=119700, grad_norm=4.4390997886657715, loss=2.3148155212402344
I0307 17:05:01.417589 140021218662144 logging_writer.py:48] [119800] global_step=119800, grad_norm=4.145303249359131, loss=2.461573362350464
I0307 17:05:54.383666 140021210269440 logging_writer.py:48] [119900] global_step=119900, grad_norm=4.190496444702148, loss=2.406238079071045
I0307 17:06:53.381904 140021218662144 logging_writer.py:48] [120000] global_step=120000, grad_norm=4.129903793334961, loss=2.410275936126709
I0307 17:07:48.253348 140021210269440 logging_writer.py:48] [120100] global_step=120100, grad_norm=4.487710475921631, loss=2.3961994647979736
I0307 17:08:15.244574 140178276385984 spec.py:321] Evaluating on the training split.
I0307 17:08:26.642541 140178276385984 spec.py:333] Evaluating on the validation split.
I0307 17:08:43.646907 140178276385984 spec.py:349] Evaluating on the test split.
I0307 17:08:45.410916 140178276385984 submission_runner.py:469] Time since start: 53166.45s, 	Step: 120148, 	{'train/accuracy': 0.7698700428009033, 'train/loss': 0.9811974763870239, 'validation/accuracy': 0.6880399584770203, 'validation/loss': 1.3295722007751465, 'validation/num_examples': 50000, 'test/accuracy': 0.5639000535011292, 'test/loss': 1.9627537727355957, 'test/num_examples': 10000, 'score': 49527.23885369301, 'total_duration': 53166.45454239845, 'accumulated_submission_time': 49527.23885369301, 'accumulated_eval_time': 3614.373338699341, 'accumulated_logging_time': 11.782408237457275}
I0307 17:08:45.458704 140021218662144 logging_writer.py:48] [120148] accumulated_eval_time=3614.37, accumulated_logging_time=11.7824, accumulated_submission_time=49527.2, global_step=120148, preemption_count=0, score=49527.2, test/accuracy=0.5639, test/loss=1.96275, test/num_examples=10000, total_duration=53166.5, train/accuracy=0.76987, train/loss=0.981197, validation/accuracy=0.68804, validation/loss=1.32957, validation/num_examples=50000
I0307 17:09:17.950951 140021210269440 logging_writer.py:48] [120200] global_step=120200, grad_norm=4.36783504486084, loss=2.4381330013275146
I0307 17:10:15.949796 140021218662144 logging_writer.py:48] [120300] global_step=120300, grad_norm=4.304844856262207, loss=2.51991605758667
I0307 17:11:14.413684 140021210269440 logging_writer.py:48] [120400] global_step=120400, grad_norm=4.497788429260254, loss=2.4384753704071045
I0307 17:12:12.708316 140021218662144 logging_writer.py:48] [120500] global_step=120500, grad_norm=4.730883598327637, loss=2.36391544342041
I0307 17:13:09.313093 140021210269440 logging_writer.py:48] [120600] global_step=120600, grad_norm=4.394157409667969, loss=2.39912486076355
I0307 17:14:08.793302 140021218662144 logging_writer.py:48] [120700] global_step=120700, grad_norm=4.408565521240234, loss=2.360960006713867
I0307 17:15:31.663310 140021210269440 logging_writer.py:48] [120800] global_step=120800, grad_norm=4.560052871704102, loss=2.4347481727600098
I0307 17:16:41.177375 140021218662144 logging_writer.py:48] [120900] global_step=120900, grad_norm=4.176675796508789, loss=2.3131299018859863
I0307 17:17:15.708693 140178276385984 spec.py:321] Evaluating on the training split.
I0307 17:17:27.502650 140178276385984 spec.py:333] Evaluating on the validation split.
I0307 17:17:44.669413 140178276385984 spec.py:349] Evaluating on the test split.
I0307 17:17:46.418220 140178276385984 submission_runner.py:469] Time since start: 53707.46s, 	Step: 120962, 	{'train/accuracy': 0.7498205900192261, 'train/loss': 1.0659445524215698, 'validation/accuracy': 0.687999963760376, 'validation/loss': 1.3525488376617432, 'validation/num_examples': 50000, 'test/accuracy': 0.5611000061035156, 'test/loss': 2.005923271179199, 'test/num_examples': 10000, 'score': 50037.370426893234, 'total_duration': 53707.46183490753, 'accumulated_submission_time': 50037.370426893234, 'accumulated_eval_time': 3645.0828211307526, 'accumulated_logging_time': 11.857717514038086}
I0307 17:17:46.499127 140021210269440 logging_writer.py:48] [120962] accumulated_eval_time=3645.08, accumulated_logging_time=11.8577, accumulated_submission_time=50037.4, global_step=120962, preemption_count=0, score=50037.4, test/accuracy=0.5611, test/loss=2.00592, test/num_examples=10000, total_duration=53707.5, train/accuracy=0.749821, train/loss=1.06594, validation/accuracy=0.688, validation/loss=1.35255, validation/num_examples=50000
I0307 17:18:02.435080 140021218662144 logging_writer.py:48] [121000] global_step=121000, grad_norm=4.359014511108398, loss=2.3191490173339844
I0307 17:18:58.868765 140021210269440 logging_writer.py:48] [121100] global_step=121100, grad_norm=4.731285095214844, loss=2.459202766418457
I0307 17:19:58.491405 140021218662144 logging_writer.py:48] [121200] global_step=121200, grad_norm=4.255087375640869, loss=2.3404462337493896
I0307 17:21:10.302967 140021210269440 logging_writer.py:48] [121300] global_step=121300, grad_norm=4.414248466491699, loss=2.3473446369171143
I0307 17:22:21.902799 140021218662144 logging_writer.py:48] [121400] global_step=121400, grad_norm=4.913337707519531, loss=2.448720693588257
I0307 17:23:33.263200 140021210269440 logging_writer.py:48] [121500] global_step=121500, grad_norm=4.372837543487549, loss=2.385526418685913
I0307 17:24:27.691060 140021218662144 logging_writer.py:48] [121600] global_step=121600, grad_norm=4.172939777374268, loss=2.351478099822998
I0307 17:25:16.986433 140021210269440 logging_writer.py:48] [121700] global_step=121700, grad_norm=4.034252166748047, loss=2.3129453659057617
I0307 17:26:17.265420 140178276385984 spec.py:321] Evaluating on the training split.
I0307 17:26:27.893205 140178276385984 spec.py:333] Evaluating on the validation split.
I0307 17:26:51.026375 140178276385984 spec.py:349] Evaluating on the test split.
I0307 17:26:52.832238 140178276385984 submission_runner.py:469] Time since start: 54253.88s, 	Step: 121775, 	{'train/accuracy': 0.7587292790412903, 'train/loss': 1.0377458333969116, 'validation/accuracy': 0.6874399781227112, 'validation/loss': 1.3576257228851318, 'validation/num_examples': 50000, 'test/accuracy': 0.5678000450134277, 'test/loss': 1.9920352697372437, 'test/num_examples': 10000, 'score': 50548.03465914726, 'total_duration': 54253.87586045265, 'accumulated_submission_time': 50548.03465914726, 'accumulated_eval_time': 3680.649604320526, 'accumulated_logging_time': 11.94773817062378}
I0307 17:26:52.892872 140021218662144 logging_writer.py:48] [121775] accumulated_eval_time=3680.65, accumulated_logging_time=11.9477, accumulated_submission_time=50548, global_step=121775, preemption_count=0, score=50548, test/accuracy=0.5678, test/loss=1.99204, test/num_examples=10000, total_duration=54253.9, train/accuracy=0.758729, train/loss=1.03775, validation/accuracy=0.68744, validation/loss=1.35763, validation/num_examples=50000
I0307 17:28:17.989230 140021210269440 logging_writer.py:48] [121800] global_step=121800, grad_norm=4.918080806732178, loss=2.3746390342712402
I0307 17:32:59.243205 140021218662144 logging_writer.py:48] [121900] global_step=121900, grad_norm=4.584743499755859, loss=2.4436001777648926
I0307 17:34:18.044376 140021210269440 logging_writer.py:48] [122000] global_step=122000, grad_norm=4.1844916343688965, loss=2.4314827919006348
I0307 17:35:26.968947 140178276385984 spec.py:321] Evaluating on the training split.
I0307 17:35:36.457228 140178276385984 spec.py:333] Evaluating on the validation split.
I0307 17:36:01.172336 140178276385984 spec.py:349] Evaluating on the test split.
I0307 17:36:02.992511 140178276385984 submission_runner.py:469] Time since start: 54804.04s, 	Step: 122062, 	{'train/accuracy': 0.7622368931770325, 'train/loss': 1.0118249654769897, 'validation/accuracy': 0.6948399543762207, 'validation/loss': 1.315991997718811, 'validation/num_examples': 50000, 'test/accuracy': 0.5697000026702881, 'test/loss': 1.973698377609253, 'test/num_examples': 10000, 'score': 51062.07180595398, 'total_duration': 54804.03612494469, 'accumulated_submission_time': 51062.07180595398, 'accumulated_eval_time': 3716.6731264591217, 'accumulated_logging_time': 12.015897512435913}
I0307 17:36:03.019019 140021218662144 logging_writer.py:48] [122062] accumulated_eval_time=3716.67, accumulated_logging_time=12.0159, accumulated_submission_time=51062.1, global_step=122062, preemption_count=0, score=51062.1, test/accuracy=0.5697, test/loss=1.9737, test/num_examples=10000, total_duration=54804, train/accuracy=0.762237, train/loss=1.01182, validation/accuracy=0.69484, validation/loss=1.31599, validation/num_examples=50000
I0307 17:38:31.454167 140021210269440 logging_writer.py:48] [122100] global_step=122100, grad_norm=4.452723026275635, loss=2.279717206954956
I0307 17:40:10.566772 140021218662144 logging_writer.py:48] [122200] global_step=122200, grad_norm=4.893710613250732, loss=2.3347644805908203
I0307 17:41:07.204717 140021210269440 logging_writer.py:48] [122300] global_step=122300, grad_norm=4.21392822265625, loss=2.4370012283325195
I0307 17:42:29.182837 140021218662144 logging_writer.py:48] [122400] global_step=122400, grad_norm=4.163221836090088, loss=2.4035181999206543
I0307 17:43:51.552031 140021210269440 logging_writer.py:48] [122500] global_step=122500, grad_norm=4.290151596069336, loss=2.369046688079834
I0307 17:44:34.614755 140178276385984 spec.py:321] Evaluating on the training split.
I0307 17:44:44.495192 140178276385984 spec.py:333] Evaluating on the validation split.
I0307 17:45:07.351599 140178276385984 spec.py:349] Evaluating on the test split.
I0307 17:45:09.115138 140178276385984 submission_runner.py:469] Time since start: 55350.16s, 	Step: 122554, 	{'train/accuracy': 0.7384207248687744, 'train/loss': 1.1291208267211914, 'validation/accuracy': 0.6741200089454651, 'validation/loss': 1.4252798557281494, 'validation/num_examples': 50000, 'test/accuracy': 0.5497000217437744, 'test/loss': 2.1041948795318604, 'test/num_examples': 10000, 'score': 51573.56299877167, 'total_duration': 55350.1587536335, 'accumulated_submission_time': 51573.56299877167, 'accumulated_eval_time': 3751.17346906662, 'accumulated_logging_time': 12.092714786529541}
I0307 17:45:09.188796 140021218662144 logging_writer.py:48] [122554] accumulated_eval_time=3751.17, accumulated_logging_time=12.0927, accumulated_submission_time=51573.6, global_step=122554, preemption_count=0, score=51573.6, test/accuracy=0.5497, test/loss=2.10419, test/num_examples=10000, total_duration=55350.2, train/accuracy=0.738421, train/loss=1.12912, validation/accuracy=0.67412, validation/loss=1.42528, validation/num_examples=50000
I0307 17:46:24.229506 140021210269440 logging_writer.py:48] [122600] global_step=122600, grad_norm=4.592160701751709, loss=2.4024605751037598
I0307 17:48:07.807407 140021218662144 logging_writer.py:48] [122700] global_step=122700, grad_norm=4.517537593841553, loss=2.341834545135498
I0307 17:49:28.003062 140021210269440 logging_writer.py:48] [122800] global_step=122800, grad_norm=4.285645008087158, loss=2.375412940979004
I0307 17:50:39.249077 140021218662144 logging_writer.py:48] [122900] global_step=122900, grad_norm=4.562771797180176, loss=2.446387767791748
I0307 17:51:44.278586 140021210269440 logging_writer.py:48] [123000] global_step=123000, grad_norm=4.456568241119385, loss=2.402458429336548
I0307 17:52:40.890470 140021218662144 logging_writer.py:48] [123100] global_step=123100, grad_norm=4.532917022705078, loss=2.476290464401245
I0307 17:53:38.437310 140021210269440 logging_writer.py:48] [123200] global_step=123200, grad_norm=4.096871852874756, loss=2.32249116897583
I0307 17:53:39.215895 140178276385984 spec.py:321] Evaluating on the training split.
I0307 17:53:50.587376 140178276385984 spec.py:333] Evaluating on the validation split.
I0307 17:54:06.979552 140178276385984 spec.py:349] Evaluating on the test split.
I0307 17:54:08.729519 140178276385984 submission_runner.py:469] Time since start: 55889.77s, 	Step: 123202, 	{'train/accuracy': 0.7606425285339355, 'train/loss': 1.024558663368225, 'validation/accuracy': 0.689520001411438, 'validation/loss': 1.3395723104476929, 'validation/num_examples': 50000, 'test/accuracy': 0.5610000491142273, 'test/loss': 1.9768309593200684, 'test/num_examples': 10000, 'score': 52083.50978779793, 'total_duration': 55889.77313542366, 'accumulated_submission_time': 52083.50978779793, 'accumulated_eval_time': 3780.687047958374, 'accumulated_logging_time': 12.175379753112793}
I0307 17:54:08.782472 140021218662144 logging_writer.py:48] [123202] accumulated_eval_time=3780.69, accumulated_logging_time=12.1754, accumulated_submission_time=52083.5, global_step=123202, preemption_count=0, score=52083.5, test/accuracy=0.561, test/loss=1.97683, test/num_examples=10000, total_duration=55889.8, train/accuracy=0.760643, train/loss=1.02456, validation/accuracy=0.68952, validation/loss=1.33957, validation/num_examples=50000
I0307 17:56:00.913965 140021210269440 logging_writer.py:48] [123300] global_step=123300, grad_norm=4.2546844482421875, loss=2.328594923019409
I0307 17:57:38.748629 140021218662144 logging_writer.py:48] [123400] global_step=123400, grad_norm=4.658676624298096, loss=2.437013626098633
I0307 17:58:45.584630 140021210269440 logging_writer.py:48] [123500] global_step=123500, grad_norm=4.233829975128174, loss=2.359814167022705
I0307 17:59:52.989559 140021218662144 logging_writer.py:48] [123600] global_step=123600, grad_norm=4.086675643920898, loss=2.352658987045288
I0307 18:01:21.161443 140021210269440 logging_writer.py:48] [123700] global_step=123700, grad_norm=4.32795524597168, loss=2.3359005451202393
I0307 18:02:39.458682 140178276385984 spec.py:321] Evaluating on the training split.
I0307 18:02:49.772322 140178276385984 spec.py:333] Evaluating on the validation split.
I0307 18:03:09.099773 140178276385984 spec.py:349] Evaluating on the test split.
I0307 18:03:10.871146 140178276385984 submission_runner.py:469] Time since start: 56431.91s, 	Step: 123790, 	{'train/accuracy': 0.7522122263908386, 'train/loss': 1.0723891258239746, 'validation/accuracy': 0.6873399615287781, 'validation/loss': 1.355772614479065, 'validation/num_examples': 50000, 'test/accuracy': 0.5653000473976135, 'test/loss': 2.0078916549682617, 'test/num_examples': 10000, 'score': 52594.11408638954, 'total_duration': 56431.91476607323, 'accumulated_submission_time': 52594.11408638954, 'accumulated_eval_time': 3812.099485397339, 'accumulated_logging_time': 12.236084699630737}
I0307 18:03:10.963283 140021218662144 logging_writer.py:48] [123790] accumulated_eval_time=3812.1, accumulated_logging_time=12.2361, accumulated_submission_time=52594.1, global_step=123790, preemption_count=0, score=52594.1, test/accuracy=0.5653, test/loss=2.00789, test/num_examples=10000, total_duration=56431.9, train/accuracy=0.752212, train/loss=1.07239, validation/accuracy=0.68734, validation/loss=1.35577, validation/num_examples=50000
I0307 18:03:15.234448 140021210269440 logging_writer.py:48] [123800] global_step=123800, grad_norm=4.37379264831543, loss=2.4691131114959717
I0307 18:04:57.896911 140021218662144 logging_writer.py:48] [123900] global_step=123900, grad_norm=4.269192695617676, loss=2.3630824089050293
I0307 18:07:05.096354 140021210269440 logging_writer.py:48] [124000] global_step=124000, grad_norm=4.63950777053833, loss=2.4095475673675537
I0307 18:09:06.663743 140021218662144 logging_writer.py:48] [124100] global_step=124100, grad_norm=4.541469573974609, loss=2.344667911529541
I0307 18:10:34.591262 140021210269440 logging_writer.py:48] [124200] global_step=124200, grad_norm=4.25639533996582, loss=2.346031427383423
I0307 18:11:42.756702 140178276385984 spec.py:321] Evaluating on the training split.
I0307 18:11:52.928834 140178276385984 spec.py:333] Evaluating on the validation split.
I0307 18:12:09.435723 140178276385984 spec.py:349] Evaluating on the test split.
I0307 18:12:11.229895 140178276385984 submission_runner.py:469] Time since start: 56972.27s, 	Step: 124258, 	{'train/accuracy': 0.7708665132522583, 'train/loss': 0.9882678389549255, 'validation/accuracy': 0.6894199848175049, 'validation/loss': 1.336884617805481, 'validation/num_examples': 50000, 'test/accuracy': 0.5653000473976135, 'test/loss': 1.967854380607605, 'test/num_examples': 10000, 'score': 53105.847051143646, 'total_duration': 56972.273503780365, 'accumulated_submission_time': 53105.847051143646, 'accumulated_eval_time': 3840.572640657425, 'accumulated_logging_time': 12.336040258407593}
I0307 18:12:11.272260 140021218662144 logging_writer.py:48] [124258] accumulated_eval_time=3840.57, accumulated_logging_time=12.336, accumulated_submission_time=53105.8, global_step=124258, preemption_count=0, score=53105.8, test/accuracy=0.5653, test/loss=1.96785, test/num_examples=10000, total_duration=56972.3, train/accuracy=0.770867, train/loss=0.988268, validation/accuracy=0.68942, validation/loss=1.33688, validation/num_examples=50000
I0307 18:13:03.697324 140021210269440 logging_writer.py:48] [124300] global_step=124300, grad_norm=5.087466716766357, loss=2.400928497314453
I0307 18:16:16.265777 140021218662144 logging_writer.py:48] [124400] global_step=124400, grad_norm=4.442193508148193, loss=2.306797742843628
I0307 18:18:24.408512 140021210269440 logging_writer.py:48] [124500] global_step=124500, grad_norm=4.418076515197754, loss=2.319413661956787
I0307 18:19:50.989443 140021218662144 logging_writer.py:48] [124600] global_step=124600, grad_norm=4.383347988128662, loss=2.3687362670898438
I0307 18:20:41.277576 140178276385984 spec.py:321] Evaluating on the training split.
I0307 18:20:51.796261 140178276385984 spec.py:333] Evaluating on the validation split.
I0307 18:21:09.700765 140178276385984 spec.py:349] Evaluating on the test split.
I0307 18:21:11.480282 140178276385984 submission_runner.py:469] Time since start: 57512.52s, 	Step: 124653, 	{'train/accuracy': 0.7533880472183228, 'train/loss': 1.049121379852295, 'validation/accuracy': 0.6859999895095825, 'validation/loss': 1.35211980342865, 'validation/num_examples': 50000, 'test/accuracy': 0.5587000250816345, 'test/loss': 2.0085337162017822, 'test/num_examples': 10000, 'score': 53615.73741841316, 'total_duration': 57512.523903131485, 'accumulated_submission_time': 53615.73741841316, 'accumulated_eval_time': 3870.7753100395203, 'accumulated_logging_time': 12.449589729309082}
I0307 18:21:11.522867 140021210269440 logging_writer.py:48] [124653] accumulated_eval_time=3870.78, accumulated_logging_time=12.4496, accumulated_submission_time=53615.7, global_step=124653, preemption_count=0, score=53615.7, test/accuracy=0.5587, test/loss=2.00853, test/num_examples=10000, total_duration=57512.5, train/accuracy=0.753388, train/loss=1.04912, validation/accuracy=0.686, validation/loss=1.35212, validation/num_examples=50000
I0307 18:21:44.976051 140021218662144 logging_writer.py:48] [124700] global_step=124700, grad_norm=4.911704063415527, loss=2.3353400230407715
I0307 18:22:49.008234 140021210269440 logging_writer.py:48] [124800] global_step=124800, grad_norm=4.3189849853515625, loss=2.299729347229004
I0307 18:24:08.424325 140021218662144 logging_writer.py:48] [124900] global_step=124900, grad_norm=4.436464786529541, loss=2.3745505809783936
I0307 18:26:06.649121 140021210269440 logging_writer.py:48] [125000] global_step=125000, grad_norm=4.525859832763672, loss=2.355400323867798
I0307 18:27:54.488894 140021218662144 logging_writer.py:48] [125100] global_step=125100, grad_norm=4.451974868774414, loss=2.3328027725219727
I0307 18:29:42.884403 140178276385984 spec.py:321] Evaluating on the training split.
I0307 18:29:52.974289 140178276385984 spec.py:333] Evaluating on the validation split.
I0307 18:30:13.674475 140178276385984 spec.py:349] Evaluating on the test split.
I0307 18:30:15.462097 140178276385984 submission_runner.py:469] Time since start: 58056.51s, 	Step: 125165, 	{'train/accuracy': 0.7603236436843872, 'train/loss': 1.0306915044784546, 'validation/accuracy': 0.6949799656867981, 'validation/loss': 1.3209543228149414, 'validation/num_examples': 50000, 'test/accuracy': 0.5672000050544739, 'test/loss': 1.9698246717453003, 'test/num_examples': 10000, 'score': 54127.0356400013, 'total_duration': 58056.505707740784, 'accumulated_submission_time': 54127.0356400013, 'accumulated_eval_time': 3903.352961540222, 'accumulated_logging_time': 12.500242471694946}
I0307 18:30:15.544071 140021210269440 logging_writer.py:48] [125165] accumulated_eval_time=3903.35, accumulated_logging_time=12.5002, accumulated_submission_time=54127, global_step=125165, preemption_count=0, score=54127, test/accuracy=0.5672, test/loss=1.96982, test/num_examples=10000, total_duration=58056.5, train/accuracy=0.760324, train/loss=1.03069, validation/accuracy=0.69498, validation/loss=1.32095, validation/num_examples=50000
I0307 18:30:58.284858 140021218662144 logging_writer.py:48] [125200] global_step=125200, grad_norm=4.353389263153076, loss=2.4039528369903564
I0307 18:32:16.303699 140021210269440 logging_writer.py:48] [125300] global_step=125300, grad_norm=4.1402411460876465, loss=2.3130099773406982
I0307 18:35:35.768391 140021218662144 logging_writer.py:48] [125400] global_step=125400, grad_norm=4.570847988128662, loss=2.3513779640197754
I0307 18:38:06.988112 140021210269440 logging_writer.py:48] [125500] global_step=125500, grad_norm=4.438723087310791, loss=2.345241069793701
I0307 18:38:45.961331 140178276385984 spec.py:321] Evaluating on the training split.
I0307 18:38:56.806799 140178276385984 spec.py:333] Evaluating on the validation split.
I0307 18:39:17.051257 140178276385984 spec.py:349] Evaluating on the test split.
I0307 18:39:18.837827 140178276385984 submission_runner.py:469] Time since start: 58599.88s, 	Step: 125545, 	{'train/accuracy': 0.7852359414100647, 'train/loss': 0.9306259155273438, 'validation/accuracy': 0.6957199573516846, 'validation/loss': 1.3137327432632446, 'validation/num_examples': 50000, 'test/accuracy': 0.5723000168800354, 'test/loss': 1.9529039859771729, 'test/num_examples': 10000, 'score': 54637.392612457275, 'total_duration': 58599.88144183159, 'accumulated_submission_time': 54637.392612457275, 'accumulated_eval_time': 3936.2294318675995, 'accumulated_logging_time': 12.601667642593384}
I0307 18:39:18.893648 140021218662144 logging_writer.py:48] [125545] accumulated_eval_time=3936.23, accumulated_logging_time=12.6017, accumulated_submission_time=54637.4, global_step=125545, preemption_count=0, score=54637.4, test/accuracy=0.5723, test/loss=1.9529, test/num_examples=10000, total_duration=58599.9, train/accuracy=0.785236, train/loss=0.930626, validation/accuracy=0.69572, validation/loss=1.31373, validation/num_examples=50000
I0307 18:40:00.953311 140021210269440 logging_writer.py:48] [125600] global_step=125600, grad_norm=4.116955280303955, loss=2.308337688446045
I0307 18:43:13.010856 140021218662144 logging_writer.py:48] [125700] global_step=125700, grad_norm=4.210690021514893, loss=2.309065103530884
I0307 18:47:50.727771 140178276385984 spec.py:321] Evaluating on the training split.
I0307 18:48:02.079895 140178276385984 spec.py:333] Evaluating on the validation split.
I0307 18:48:22.366881 140178276385984 spec.py:349] Evaluating on the test split.
I0307 18:48:24.142059 140178276385984 submission_runner.py:469] Time since start: 59145.19s, 	Step: 125780, 	{'train/accuracy': 0.7738161683082581, 'train/loss': 0.9807087182998657, 'validation/accuracy': 0.6979799866676331, 'validation/loss': 1.3162760734558105, 'validation/num_examples': 50000, 'test/accuracy': 0.5725000500679016, 'test/loss': 1.9774309396743774, 'test/num_examples': 10000, 'score': 55149.19099855423, 'total_duration': 59145.185670137405, 'accumulated_submission_time': 55149.19099855423, 'accumulated_eval_time': 3969.6436800956726, 'accumulated_logging_time': 12.66632628440857}
I0307 18:48:24.166006 140021210269440 logging_writer.py:48] [125780] accumulated_eval_time=3969.64, accumulated_logging_time=12.6663, accumulated_submission_time=55149.2, global_step=125780, preemption_count=0, score=55149.2, test/accuracy=0.5725, test/loss=1.97743, test/num_examples=10000, total_duration=59145.2, train/accuracy=0.773816, train/loss=0.980709, validation/accuracy=0.69798, validation/loss=1.31628, validation/num_examples=50000
I0307 18:48:53.133365 140021218662144 logging_writer.py:48] [125800] global_step=125800, grad_norm=4.423753261566162, loss=2.398366928100586
I0307 18:51:50.256779 140021210269440 logging_writer.py:48] [125900] global_step=125900, grad_norm=4.516689777374268, loss=2.306617021560669
I0307 18:53:15.662186 140021218662144 logging_writer.py:48] [126000] global_step=126000, grad_norm=4.1627326011657715, loss=2.352128505706787
I0307 18:55:08.063730 140021210269440 logging_writer.py:48] [126100] global_step=126100, grad_norm=4.773721694946289, loss=2.417940139770508
I0307 18:56:55.762305 140178276385984 spec.py:321] Evaluating on the training split.
I0307 18:57:05.840464 140178276385984 spec.py:333] Evaluating on the validation split.
I0307 18:57:31.096692 140178276385984 spec.py:349] Evaluating on the test split.
I0307 18:57:32.827223 140178276385984 submission_runner.py:469] Time since start: 59693.87s, 	Step: 126130, 	{'train/accuracy': 0.7628148794174194, 'train/loss': 1.0199297666549683, 'validation/accuracy': 0.6890000104904175, 'validation/loss': 1.3471249341964722, 'validation/num_examples': 50000, 'test/accuracy': 0.5642000436782837, 'test/loss': 2.0088818073272705, 'test/num_examples': 10000, 'score': 55660.742210149765, 'total_duration': 59693.870842933655, 'accumulated_submission_time': 55660.742210149765, 'accumulated_eval_time': 4006.7085659503937, 'accumulated_logging_time': 12.697842359542847}
I0307 18:57:32.861819 140021218662144 logging_writer.py:48] [126130] accumulated_eval_time=4006.71, accumulated_logging_time=12.6978, accumulated_submission_time=55660.7, global_step=126130, preemption_count=0, score=55660.7, test/accuracy=0.5642, test/loss=2.00888, test/num_examples=10000, total_duration=59693.9, train/accuracy=0.762815, train/loss=1.01993, validation/accuracy=0.689, validation/loss=1.34712, validation/num_examples=50000
I0307 19:00:52.693547 140021210269440 logging_writer.py:48] [126200] global_step=126200, grad_norm=4.614162921905518, loss=2.321547031402588
I0307 19:03:06.779601 140021218662144 logging_writer.py:48] [126300] global_step=126300, grad_norm=4.530542373657227, loss=2.352221965789795
I0307 19:05:25.143458 140021210269440 logging_writer.py:48] [126400] global_step=126400, grad_norm=4.392982006072998, loss=2.3727993965148926
I0307 19:06:02.847873 140178276385984 spec.py:321] Evaluating on the training split.
I0307 19:06:14.874562 140178276385984 spec.py:333] Evaluating on the validation split.
I0307 19:06:33.452053 140178276385984 spec.py:349] Evaluating on the test split.
I0307 19:06:35.174291 140178276385984 submission_runner.py:469] Time since start: 60236.22s, 	Step: 126443, 	{'train/accuracy': 0.7667211294174194, 'train/loss': 0.9877922534942627, 'validation/accuracy': 0.6985999941825867, 'validation/loss': 1.2874317169189453, 'validation/num_examples': 50000, 'test/accuracy': 0.572700023651123, 'test/loss': 1.9590399265289307, 'test/num_examples': 10000, 'score': 56170.68586349487, 'total_duration': 60236.21789479256, 'accumulated_submission_time': 56170.68586349487, 'accumulated_eval_time': 4039.034947156906, 'accumulated_logging_time': 12.740835666656494}
I0307 19:06:35.216791 140021218662144 logging_writer.py:48] [126443] accumulated_eval_time=4039.03, accumulated_logging_time=12.7408, accumulated_submission_time=56170.7, global_step=126443, preemption_count=0, score=56170.7, test/accuracy=0.5727, test/loss=1.95904, test/num_examples=10000, total_duration=60236.2, train/accuracy=0.766721, train/loss=0.987792, validation/accuracy=0.6986, validation/loss=1.28743, validation/num_examples=50000
I0307 19:07:19.110942 140021210269440 logging_writer.py:48] [126500] global_step=126500, grad_norm=4.586423873901367, loss=2.3465094566345215
I0307 19:08:54.399145 140021218662144 logging_writer.py:48] [126600] global_step=126600, grad_norm=4.569180965423584, loss=2.3102192878723145
I0307 19:10:18.031155 140021210269440 logging_writer.py:48] [126700] global_step=126700, grad_norm=4.469590187072754, loss=2.3433761596679688
I0307 19:11:47.779569 140021218662144 logging_writer.py:48] [126800] global_step=126800, grad_norm=4.7221832275390625, loss=2.3552305698394775
I0307 19:13:17.219559 140021210269440 logging_writer.py:48] [126900] global_step=126900, grad_norm=5.548332214355469, loss=2.3310792446136475
I0307 19:14:48.842162 140021218662144 logging_writer.py:48] [127000] global_step=127000, grad_norm=5.27589225769043, loss=2.3307199478149414
I0307 19:15:05.436968 140178276385984 spec.py:321] Evaluating on the training split.
I0307 19:15:16.631228 140178276385984 spec.py:333] Evaluating on the validation split.
I0307 19:15:34.875868 140178276385984 spec.py:349] Evaluating on the test split.
I0307 19:15:36.609632 140178276385984 submission_runner.py:469] Time since start: 60777.65s, 	Step: 127019, 	{'train/accuracy': 0.781668484210968, 'train/loss': 0.9370632171630859, 'validation/accuracy': 0.6987999677658081, 'validation/loss': 1.3069871664047241, 'validation/num_examples': 50000, 'test/accuracy': 0.5697000026702881, 'test/loss': 1.9576137065887451, 'test/num_examples': 10000, 'score': 56680.822405815125, 'total_duration': 60777.653245687485, 'accumulated_submission_time': 56680.822405815125, 'accumulated_eval_time': 4070.207568883896, 'accumulated_logging_time': 12.80076813697815}
I0307 19:15:36.695354 140021210269440 logging_writer.py:48] [127019] accumulated_eval_time=4070.21, accumulated_logging_time=12.8008, accumulated_submission_time=56680.8, global_step=127019, preemption_count=0, score=56680.8, test/accuracy=0.5697, test/loss=1.95761, test/num_examples=10000, total_duration=60777.7, train/accuracy=0.781668, train/loss=0.937063, validation/accuracy=0.6988, validation/loss=1.30699, validation/num_examples=50000
I0307 19:20:39.748255 140021218662144 logging_writer.py:48] [127100] global_step=127100, grad_norm=4.657258033752441, loss=2.2886557579040527
I0307 19:24:07.413769 140178276385984 spec.py:321] Evaluating on the training split.
I0307 19:24:17.485625 140178276385984 spec.py:333] Evaluating on the validation split.
I0307 19:24:34.100129 140178276385984 spec.py:349] Evaluating on the test split.
I0307 19:24:35.865056 140178276385984 submission_runner.py:469] Time since start: 61316.91s, 	Step: 127189, 	{'train/accuracy': 0.7752909660339355, 'train/loss': 0.9475078582763672, 'validation/accuracy': 0.6964399814605713, 'validation/loss': 1.3045929670333862, 'validation/num_examples': 50000, 'test/accuracy': 0.5751000046730042, 'test/loss': 1.9500045776367188, 'test/num_examples': 10000, 'score': 57191.51372337341, 'total_duration': 61316.90868020058, 'accumulated_submission_time': 57191.51372337341, 'accumulated_eval_time': 4098.65883231163, 'accumulated_logging_time': 12.894912719726562}
I0307 19:24:35.889481 140021210269440 logging_writer.py:48] [127189] accumulated_eval_time=4098.66, accumulated_logging_time=12.8949, accumulated_submission_time=57191.5, global_step=127189, preemption_count=0, score=57191.5, test/accuracy=0.5751, test/loss=1.95, test/num_examples=10000, total_duration=61316.9, train/accuracy=0.775291, train/loss=0.947508, validation/accuracy=0.69644, validation/loss=1.30459, validation/num_examples=50000
I0307 19:24:40.554507 140021218662144 logging_writer.py:48] [127200] global_step=127200, grad_norm=4.745351314544678, loss=2.3630034923553467
I0307 19:25:39.771209 140021210269440 logging_writer.py:48] [127300] global_step=127300, grad_norm=4.773383617401123, loss=2.372084856033325
I0307 19:26:40.265785 140021218662144 logging_writer.py:48] [127400] global_step=127400, grad_norm=4.591069221496582, loss=2.302276134490967
I0307 19:27:48.336856 140021210269440 logging_writer.py:48] [127500] global_step=127500, grad_norm=4.803142070770264, loss=2.345198631286621
2025-03-07 19:28:51.211696: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 19:28:52.573660 140021218662144 logging_writer.py:48] [127600] global_step=127600, grad_norm=4.5388054847717285, loss=2.3305180072784424
I0307 19:30:15.721472 140021210269440 logging_writer.py:48] [127700] global_step=127700, grad_norm=4.417258262634277, loss=2.2967946529388428
I0307 19:32:03.674212 140021218662144 logging_writer.py:48] [127800] global_step=127800, grad_norm=4.5740885734558105, loss=2.2344789505004883
I0307 19:33:07.928783 140178276385984 spec.py:321] Evaluating on the training split.
I0307 19:33:19.396527 140178276385984 spec.py:333] Evaluating on the validation split.
I0307 19:33:41.545120 140178276385984 spec.py:349] Evaluating on the test split.
I0307 19:33:43.328645 140178276385984 submission_runner.py:469] Time since start: 61864.37s, 	Step: 127829, 	{'train/accuracy': 0.7615194320678711, 'train/loss': 1.0176628828048706, 'validation/accuracy': 0.6960799694061279, 'validation/loss': 1.3095643520355225, 'validation/num_examples': 50000, 'test/accuracy': 0.5751000046730042, 'test/loss': 1.9625041484832764, 'test/num_examples': 10000, 'score': 57703.47299528122, 'total_duration': 61864.37225365639, 'accumulated_submission_time': 57703.47299528122, 'accumulated_eval_time': 4134.058648347855, 'accumulated_logging_time': 12.92718505859375}
I0307 19:33:43.368669 140021210269440 logging_writer.py:48] [127829] accumulated_eval_time=4134.06, accumulated_logging_time=12.9272, accumulated_submission_time=57703.5, global_step=127829, preemption_count=0, score=57703.5, test/accuracy=0.5751, test/loss=1.9625, test/num_examples=10000, total_duration=61864.4, train/accuracy=0.761519, train/loss=1.01766, validation/accuracy=0.69608, validation/loss=1.30956, validation/num_examples=50000
I0307 19:38:32.946571 140021218662144 logging_writer.py:48] [127900] global_step=127900, grad_norm=5.057363986968994, loss=2.465123176574707
I0307 19:42:13.371221 140178276385984 spec.py:321] Evaluating on the training split.
I0307 19:42:24.358443 140178276385984 spec.py:333] Evaluating on the validation split.
I0307 19:42:45.963118 140178276385984 spec.py:349] Evaluating on the test split.
I0307 19:42:47.751911 140178276385984 submission_runner.py:469] Time since start: 62408.80s, 	Step: 127976, 	{'train/accuracy': 0.7653658986091614, 'train/loss': 1.011232614517212, 'validation/accuracy': 0.6974799633026123, 'validation/loss': 1.303728461265564, 'validation/num_examples': 50000, 'test/accuracy': 0.5696000456809998, 'test/loss': 1.984849452972412, 'test/num_examples': 10000, 'score': 58213.44985294342, 'total_duration': 62408.79552268982, 'accumulated_submission_time': 58213.44985294342, 'accumulated_eval_time': 4168.439317703247, 'accumulated_logging_time': 12.975699663162231}
I0307 19:42:47.780216 140021210269440 logging_writer.py:48] [127976] accumulated_eval_time=4168.44, accumulated_logging_time=12.9757, accumulated_submission_time=58213.4, global_step=127976, preemption_count=0, score=58213.4, test/accuracy=0.5696, test/loss=1.98485, test/num_examples=10000, total_duration=62408.8, train/accuracy=0.765366, train/loss=1.01123, validation/accuracy=0.69748, validation/loss=1.30373, validation/num_examples=50000
I0307 19:43:11.316314 140021218662144 logging_writer.py:48] [128000] global_step=128000, grad_norm=4.811152935028076, loss=2.3433303833007812
I0307 19:45:35.836204 140021210269440 logging_writer.py:48] [128100] global_step=128100, grad_norm=4.934779644012451, loss=2.3280322551727295
I0307 19:50:17.102365 140021218662144 logging_writer.py:48] [128200] global_step=128200, grad_norm=5.048886775970459, loss=2.37611722946167
I0307 19:51:18.038238 140178276385984 spec.py:321] Evaluating on the training split.
I0307 19:51:27.968979 140178276385984 spec.py:333] Evaluating on the validation split.
I0307 19:51:47.807584 140178276385984 spec.py:349] Evaluating on the test split.
I0307 19:51:49.593302 140178276385984 submission_runner.py:469] Time since start: 62950.64s, 	Step: 128229, 	{'train/accuracy': 0.7700892686843872, 'train/loss': 0.9973018765449524, 'validation/accuracy': 0.698199987411499, 'validation/loss': 1.3144395351409912, 'validation/num_examples': 50000, 'test/accuracy': 0.5705000162124634, 'test/loss': 1.9554637670516968, 'test/num_examples': 10000, 'score': 58723.67125630379, 'total_duration': 62950.63692235947, 'accumulated_submission_time': 58723.67125630379, 'accumulated_eval_time': 4199.994345664978, 'accumulated_logging_time': 13.013160705566406}
I0307 19:51:49.618520 140021210269440 logging_writer.py:48] [128229] accumulated_eval_time=4199.99, accumulated_logging_time=13.0132, accumulated_submission_time=58723.7, global_step=128229, preemption_count=0, score=58723.7, test/accuracy=0.5705, test/loss=1.95546, test/num_examples=10000, total_duration=62950.6, train/accuracy=0.770089, train/loss=0.997302, validation/accuracy=0.6982, validation/loss=1.31444, validation/num_examples=50000
I0307 19:55:37.395154 140021218662144 logging_writer.py:48] [128300] global_step=128300, grad_norm=4.955848693847656, loss=2.40213680267334
I0307 19:58:41.918560 140021210269440 logging_writer.py:48] [128400] global_step=128400, grad_norm=4.688241004943848, loss=2.191169261932373
I0307 20:00:05.165978 140021218662144 logging_writer.py:48] [128500] global_step=128500, grad_norm=4.8845343589782715, loss=2.3532824516296387
I0307 20:00:19.833604 140178276385984 spec.py:321] Evaluating on the training split.
I0307 20:00:31.274410 140178276385984 spec.py:333] Evaluating on the validation split.
I0307 20:00:49.734580 140178276385984 spec.py:349] Evaluating on the test split.
I0307 20:00:51.490895 140178276385984 submission_runner.py:469] Time since start: 63492.53s, 	Step: 128518, 	{'train/accuracy': 0.7856544852256775, 'train/loss': 0.9454336762428284, 'validation/accuracy': 0.6955199837684631, 'validation/loss': 1.3293514251708984, 'validation/num_examples': 50000, 'test/accuracy': 0.5753000378608704, 'test/loss': 1.9687272310256958, 'test/num_examples': 10000, 'score': 59233.84620428085, 'total_duration': 63492.53452038765, 'accumulated_submission_time': 59233.84620428085, 'accumulated_eval_time': 4231.651604175568, 'accumulated_logging_time': 13.046807050704956}
I0307 20:00:51.514070 140021210269440 logging_writer.py:48] [128518] accumulated_eval_time=4231.65, accumulated_logging_time=13.0468, accumulated_submission_time=59233.8, global_step=128518, preemption_count=0, score=59233.8, test/accuracy=0.5753, test/loss=1.96873, test/num_examples=10000, total_duration=63492.5, train/accuracy=0.785654, train/loss=0.945434, validation/accuracy=0.69552, validation/loss=1.32935, validation/num_examples=50000
I0307 20:01:55.489157 140021218662144 logging_writer.py:48] [128600] global_step=128600, grad_norm=4.68742561340332, loss=2.345445156097412
I0307 20:03:41.836843 140021210269440 logging_writer.py:48] [128700] global_step=128700, grad_norm=4.752773284912109, loss=2.3272790908813477
I0307 20:07:00.567645 140021218662144 logging_writer.py:48] [128800] global_step=128800, grad_norm=4.7710957527160645, loss=2.3255743980407715
I0307 20:09:23.670933 140178276385984 spec.py:321] Evaluating on the training split.
I0307 20:09:33.393274 140178276385984 spec.py:333] Evaluating on the validation split.
I0307 20:09:54.691484 140178276385984 spec.py:349] Evaluating on the test split.
I0307 20:09:56.420850 140178276385984 submission_runner.py:469] Time since start: 64037.46s, 	Step: 128889, 	{'train/accuracy': 0.7693518400192261, 'train/loss': 0.9701951146125793, 'validation/accuracy': 0.6976000070571899, 'validation/loss': 1.2956416606903076, 'validation/num_examples': 50000, 'test/accuracy': 0.5720000267028809, 'test/loss': 1.9348167181015015, 'test/num_examples': 10000, 'score': 59745.95229887962, 'total_duration': 64037.46446585655, 'accumulated_submission_time': 59745.95229887962, 'accumulated_eval_time': 4264.401487827301, 'accumulated_logging_time': 13.077680587768555}
I0307 20:09:56.465499 140021210269440 logging_writer.py:48] [128889] accumulated_eval_time=4264.4, accumulated_logging_time=13.0777, accumulated_submission_time=59746, global_step=128889, preemption_count=0, score=59746, test/accuracy=0.572, test/loss=1.93482, test/num_examples=10000, total_duration=64037.5, train/accuracy=0.769352, train/loss=0.970195, validation/accuracy=0.6976, validation/loss=1.29564, validation/num_examples=50000
I0307 20:10:25.305129 140021218662144 logging_writer.py:48] [128900] global_step=128900, grad_norm=4.882024765014648, loss=2.4136276245117188
I0307 20:13:53.114728 140021210269440 logging_writer.py:48] [129000] global_step=129000, grad_norm=4.977973461151123, loss=2.2808549404144287
I0307 20:18:29.271724 140178276385984 spec.py:321] Evaluating on the training split.
I0307 20:18:39.263397 140178276385984 spec.py:333] Evaluating on the validation split.
I0307 20:18:59.279638 140178276385984 spec.py:349] Evaluating on the test split.
I0307 20:19:01.061905 140178276385984 submission_runner.py:469] Time since start: 64582.11s, 	Step: 129099, 	{'train/accuracy': 0.7744140625, 'train/loss': 0.9605424404144287, 'validation/accuracy': 0.702739953994751, 'validation/loss': 1.2833123207092285, 'validation/num_examples': 50000, 'test/accuracy': 0.5743000507354736, 'test/loss': 1.9408903121948242, 'test/num_examples': 10000, 'score': 60258.72899079323, 'total_duration': 64582.10551524162, 'accumulated_submission_time': 60258.72899079323, 'accumulated_eval_time': 4296.1916353702545, 'accumulated_logging_time': 13.13055419921875}
I0307 20:19:01.089840 140021218662144 logging_writer.py:48] [129099] accumulated_eval_time=4296.19, accumulated_logging_time=13.1306, accumulated_submission_time=60258.7, global_step=129099, preemption_count=0, score=60258.7, test/accuracy=0.5743, test/loss=1.94089, test/num_examples=10000, total_duration=64582.1, train/accuracy=0.774414, train/loss=0.960542, validation/accuracy=0.70274, validation/loss=1.28331, validation/num_examples=50000
I0307 20:19:01.832284 140021210269440 logging_writer.py:48] [129100] global_step=129100, grad_norm=4.560554027557373, loss=2.379732847213745
I0307 20:25:42.298614 140021218662144 logging_writer.py:48] [129200] global_step=129200, grad_norm=4.3050971031188965, loss=2.2529940605163574
I0307 20:27:34.629802 140178276385984 spec.py:321] Evaluating on the training split.
I0307 20:27:44.297413 140178276385984 spec.py:333] Evaluating on the validation split.
I0307 20:28:07.048540 140178276385984 spec.py:349] Evaluating on the test split.
I0307 20:28:08.814098 140178276385984 submission_runner.py:469] Time since start: 65129.86s, 	Step: 129236, 	{'train/accuracy': 0.7612404227256775, 'train/loss': 1.0304335355758667, 'validation/accuracy': 0.6899600028991699, 'validation/loss': 1.3441596031188965, 'validation/num_examples': 50000, 'test/accuracy': 0.5599000453948975, 'test/loss': 2.0078001022338867, 'test/num_examples': 10000, 'score': 60772.240236759186, 'total_duration': 65129.85771942139, 'accumulated_submission_time': 60772.240236759186, 'accumulated_eval_time': 4330.375913858414, 'accumulated_logging_time': 13.172591209411621}
I0307 20:28:08.838638 140021210269440 logging_writer.py:48] [129236] accumulated_eval_time=4330.38, accumulated_logging_time=13.1726, accumulated_submission_time=60772.2, global_step=129236, preemption_count=0, score=60772.2, test/accuracy=0.5599, test/loss=2.0078, test/num_examples=10000, total_duration=65129.9, train/accuracy=0.76124, train/loss=1.03043, validation/accuracy=0.68996, validation/loss=1.34416, validation/num_examples=50000
I0307 20:32:29.234137 140021218662144 logging_writer.py:48] [129300] global_step=129300, grad_norm=4.81223201751709, loss=2.2800328731536865
I0307 20:36:41.769869 140178276385984 spec.py:321] Evaluating on the training split.
I0307 20:36:52.217600 140178276385984 spec.py:333] Evaluating on the validation split.
I0307 20:37:11.502650 140178276385984 spec.py:349] Evaluating on the test split.
I0307 20:37:13.278621 140178276385984 submission_runner.py:469] Time since start: 65674.32s, 	Step: 129360, 	{'train/accuracy': 0.7726601958274841, 'train/loss': 0.9548271894454956, 'validation/accuracy': 0.700719952583313, 'validation/loss': 1.2781521081924438, 'validation/num_examples': 50000, 'test/accuracy': 0.5826000571250916, 'test/loss': 1.9059375524520874, 'test/num_examples': 10000, 'score': 61285.149243593216, 'total_duration': 65674.32224273682, 'accumulated_submission_time': 61285.149243593216, 'accumulated_eval_time': 4361.884636878967, 'accumulated_logging_time': 13.20588207244873}
I0307 20:37:13.302936 140021210269440 logging_writer.py:48] [129360] accumulated_eval_time=4361.88, accumulated_logging_time=13.2059, accumulated_submission_time=61285.1, global_step=129360, preemption_count=0, score=61285.1, test/accuracy=0.5826, test/loss=1.90594, test/num_examples=10000, total_duration=65674.3, train/accuracy=0.77266, train/loss=0.954827, validation/accuracy=0.70072, validation/loss=1.27815, validation/num_examples=50000
I0307 20:39:50.612625 140021218662144 logging_writer.py:48] [129400] global_step=129400, grad_norm=5.01300048828125, loss=2.281342029571533
I0307 20:44:08.236331 140021210269440 logging_writer.py:48] [129500] global_step=129500, grad_norm=4.7638936042785645, loss=2.2724905014038086
I0307 20:45:43.736212 140178276385984 spec.py:321] Evaluating on the training split.
I0307 20:45:53.886224 140178276385984 spec.py:333] Evaluating on the validation split.
I0307 20:46:13.151465 140178276385984 spec.py:349] Evaluating on the test split.
I0307 20:46:14.911954 140178276385984 submission_runner.py:469] Time since start: 66215.96s, 	Step: 129556, 	{'train/accuracy': 0.7672193646430969, 'train/loss': 1.0069751739501953, 'validation/accuracy': 0.6964799761772156, 'validation/loss': 1.3204766511917114, 'validation/num_examples': 50000, 'test/accuracy': 0.5740000009536743, 'test/loss': 1.957993507385254, 'test/num_examples': 10000, 'score': 61795.55415034294, 'total_duration': 66215.95557427406, 'accumulated_submission_time': 61795.55415034294, 'accumulated_eval_time': 4393.0603449344635, 'accumulated_logging_time': 13.238199472427368}
I0307 20:46:14.938105 140021218662144 logging_writer.py:48] [129556] accumulated_eval_time=4393.06, accumulated_logging_time=13.2382, accumulated_submission_time=61795.6, global_step=129556, preemption_count=0, score=61795.6, test/accuracy=0.574, test/loss=1.95799, test/num_examples=10000, total_duration=66216, train/accuracy=0.767219, train/loss=1.00698, validation/accuracy=0.69648, validation/loss=1.32048, validation/num_examples=50000
I0307 20:47:11.741308 140021210269440 logging_writer.py:48] [129600] global_step=129600, grad_norm=5.45410680770874, loss=2.323291301727295
I0307 20:50:14.159696 140021218662144 logging_writer.py:48] [129700] global_step=129700, grad_norm=4.900008201599121, loss=2.2637174129486084
I0307 20:51:50.486645 140021210269440 logging_writer.py:48] [129800] global_step=129800, grad_norm=4.690382957458496, loss=2.2956578731536865
I0307 20:53:22.252555 140021218662144 logging_writer.py:48] [129900] global_step=129900, grad_norm=4.831620216369629, loss=2.4206132888793945
I0307 20:54:45.561952 140178276385984 spec.py:321] Evaluating on the training split.
I0307 20:54:55.880961 140178276385984 spec.py:333] Evaluating on the validation split.
I0307 20:55:15.201852 140178276385984 spec.py:349] Evaluating on the test split.
I0307 20:55:16.943614 140178276385984 submission_runner.py:469] Time since start: 66757.99s, 	Step: 129973, 	{'train/accuracy': 0.7987882494926453, 'train/loss': 0.8608719706535339, 'validation/accuracy': 0.6976999640464783, 'validation/loss': 1.2853513956069946, 'validation/num_examples': 50000, 'test/accuracy': 0.5726000070571899, 'test/loss': 1.942155361175537, 'test/num_examples': 10000, 'score': 62306.12463569641, 'total_duration': 66757.98723316193, 'accumulated_submission_time': 62306.12463569641, 'accumulated_eval_time': 4424.4419713020325, 'accumulated_logging_time': 13.272434711456299}
I0307 20:55:16.995975 140021210269440 logging_writer.py:48] [129973] accumulated_eval_time=4424.44, accumulated_logging_time=13.2724, accumulated_submission_time=62306.1, global_step=129973, preemption_count=0, score=62306.1, test/accuracy=0.5726, test/loss=1.94216, test/num_examples=10000, total_duration=66758, train/accuracy=0.798788, train/loss=0.860872, validation/accuracy=0.6977, validation/loss=1.28535, validation/num_examples=50000
I0307 20:56:02.349135 140021218662144 logging_writer.py:48] [130000] global_step=130000, grad_norm=4.534821033477783, loss=2.2831015586853027
I0307 21:00:59.183691 140021210269440 logging_writer.py:48] [130100] global_step=130100, grad_norm=4.541548252105713, loss=2.2528154850006104
I0307 21:03:49.217055 140178276385984 spec.py:321] Evaluating on the training split.
I0307 21:03:59.313885 140178276385984 spec.py:333] Evaluating on the validation split.
I0307 21:04:24.276792 140178276385984 spec.py:349] Evaluating on the test split.
I0307 21:04:26.198986 140178276385984 submission_runner.py:469] Time since start: 67307.24s, 	Step: 130141, 	{'train/accuracy': 0.7819674611091614, 'train/loss': 0.9341273903846741, 'validation/accuracy': 0.6918799877166748, 'validation/loss': 1.3201701641082764, 'validation/num_examples': 50000, 'test/accuracy': 0.5683000087738037, 'test/loss': 1.9836262464523315, 'test/num_examples': 10000, 'score': 62818.319324970245, 'total_duration': 67307.24260640144, 'accumulated_submission_time': 62818.319324970245, 'accumulated_eval_time': 4461.423869848251, 'accumulated_logging_time': 13.334020376205444}
I0307 21:04:26.224441 140021218662144 logging_writer.py:48] [130141] accumulated_eval_time=4461.42, accumulated_logging_time=13.334, accumulated_submission_time=62818.3, global_step=130141, preemption_count=0, score=62818.3, test/accuracy=0.5683, test/loss=1.98363, test/num_examples=10000, total_duration=67307.2, train/accuracy=0.781967, train/loss=0.934127, validation/accuracy=0.69188, validation/loss=1.32017, validation/num_examples=50000
I0307 21:06:55.499255 140021210269440 logging_writer.py:48] [130200] global_step=130200, grad_norm=4.818380832672119, loss=2.3879916667938232
I0307 21:10:30.813436 140021218662144 logging_writer.py:48] [130300] global_step=130300, grad_norm=5.42177152633667, loss=2.302504062652588
I0307 21:12:56.520710 140178276385984 spec.py:321] Evaluating on the training split.
I0307 21:13:06.694392 140178276385984 spec.py:333] Evaluating on the validation split.
I0307 21:13:25.268630 140178276385984 spec.py:349] Evaluating on the test split.
I0307 21:13:27.021384 140178276385984 submission_runner.py:469] Time since start: 67848.06s, 	Step: 130369, 	{'train/accuracy': 0.7807716727256775, 'train/loss': 0.9305323958396912, 'validation/accuracy': 0.6992200016975403, 'validation/loss': 1.288451910018921, 'validation/num_examples': 50000, 'test/accuracy': 0.5716000199317932, 'test/loss': 1.9218629598617554, 'test/num_examples': 10000, 'score': 63328.58293747902, 'total_duration': 67848.06496286392, 'accumulated_submission_time': 63328.58293747902, 'accumulated_eval_time': 4491.924478530884, 'accumulated_logging_time': 13.367144584655762}
I0307 21:13:27.047342 140021210269440 logging_writer.py:48] [130369] accumulated_eval_time=4491.92, accumulated_logging_time=13.3671, accumulated_submission_time=63328.6, global_step=130369, preemption_count=0, score=63328.6, test/accuracy=0.5716, test/loss=1.92186, test/num_examples=10000, total_duration=67848.1, train/accuracy=0.780772, train/loss=0.930532, validation/accuracy=0.69922, validation/loss=1.28845, validation/num_examples=50000
I0307 21:14:21.652438 140021218662144 logging_writer.py:48] [130400] global_step=130400, grad_norm=4.534915447235107, loss=2.405611276626587
I0307 21:19:27.748811 140021210269440 logging_writer.py:48] [130500] global_step=130500, grad_norm=5.053667068481445, loss=2.275947332382202
I0307 21:22:00.534166 140178276385984 spec.py:321] Evaluating on the training split.
I0307 21:22:10.704849 140178276385984 spec.py:333] Evaluating on the validation split.
I0307 21:22:31.565910 140178276385984 spec.py:349] Evaluating on the test split.
I0307 21:22:33.338942 140178276385984 submission_runner.py:469] Time since start: 68394.38s, 	Step: 130537, 	{'train/accuracy': 0.7791174650192261, 'train/loss': 0.9595478177070618, 'validation/accuracy': 0.6994799971580505, 'validation/loss': 1.303695559501648, 'validation/num_examples': 50000, 'test/accuracy': 0.574400007724762, 'test/loss': 1.9433953762054443, 'test/num_examples': 10000, 'score': 63842.0419113636, 'total_duration': 68394.38256573677, 'accumulated_submission_time': 63842.0419113636, 'accumulated_eval_time': 4524.7292404174805, 'accumulated_logging_time': 13.401313781738281}
I0307 21:22:33.363297 140021218662144 logging_writer.py:48] [130537] accumulated_eval_time=4524.73, accumulated_logging_time=13.4013, accumulated_submission_time=63842, global_step=130537, preemption_count=0, score=63842, test/accuracy=0.5744, test/loss=1.9434, test/num_examples=10000, total_duration=68394.4, train/accuracy=0.779117, train/loss=0.959548, validation/accuracy=0.69948, validation/loss=1.3037, validation/num_examples=50000
I0307 21:26:50.006365 140021210269440 logging_writer.py:48] [130600] global_step=130600, grad_norm=4.611855506896973, loss=2.3316636085510254
I0307 21:31:05.360726 140178276385984 spec.py:321] Evaluating on the training split.
I0307 21:31:15.141030 140178276385984 spec.py:333] Evaluating on the validation split.
I0307 21:31:35.484002 140178276385984 spec.py:349] Evaluating on the test split.
I0307 21:31:37.224134 140178276385984 submission_runner.py:469] Time since start: 68938.27s, 	Step: 130661, 	{'train/accuracy': 0.7713049650192261, 'train/loss': 0.9815318584442139, 'validation/accuracy': 0.6945799589157104, 'validation/loss': 1.320129156112671, 'validation/num_examples': 50000, 'test/accuracy': 0.5695000290870667, 'test/loss': 1.9612842798233032, 'test/num_examples': 10000, 'score': 64354.01810669899, 'total_duration': 68938.26775503159, 'accumulated_submission_time': 64354.01810669899, 'accumulated_eval_time': 4556.5926167964935, 'accumulated_logging_time': 13.433764457702637}
I0307 21:31:37.247581 140021218662144 logging_writer.py:48] [130661] accumulated_eval_time=4556.59, accumulated_logging_time=13.4338, accumulated_submission_time=64354, global_step=130661, preemption_count=0, score=64354, test/accuracy=0.5695, test/loss=1.96128, test/num_examples=10000, total_duration=68938.3, train/accuracy=0.771305, train/loss=0.981532, validation/accuracy=0.69458, validation/loss=1.32013, validation/num_examples=50000
I0307 21:34:09.722140 140021210269440 logging_writer.py:48] [130700] global_step=130700, grad_norm=4.42726469039917, loss=2.3419744968414307
I0307 21:40:10.147732 140178276385984 spec.py:321] Evaluating on the training split.
I0307 21:40:20.721635 140178276385984 spec.py:333] Evaluating on the validation split.
I0307 21:40:43.160560 140178276385984 spec.py:349] Evaluating on the test split.
I0307 21:40:44.929077 140178276385984 submission_runner.py:469] Time since start: 69485.97s, 	Step: 130786, 	{'train/accuracy': 0.7755300998687744, 'train/loss': 0.9319085478782654, 'validation/accuracy': 0.7029399871826172, 'validation/loss': 1.2618303298950195, 'validation/num_examples': 50000, 'test/accuracy': 0.5763000249862671, 'test/loss': 1.9140115976333618, 'test/num_examples': 10000, 'score': 64866.89805006981, 'total_duration': 69485.97267341614, 'accumulated_submission_time': 64866.89805006981, 'accumulated_eval_time': 4591.373907804489, 'accumulated_logging_time': 13.464891910552979}
I0307 21:40:44.954532 140021218662144 logging_writer.py:48] [130786] accumulated_eval_time=4591.37, accumulated_logging_time=13.4649, accumulated_submission_time=64866.9, global_step=130786, preemption_count=0, score=64866.9, test/accuracy=0.5763, test/loss=1.91401, test/num_examples=10000, total_duration=69486, train/accuracy=0.77553, train/loss=0.931909, validation/accuracy=0.70294, validation/loss=1.26183, validation/num_examples=50000
I0307 21:41:32.027515 140021210269440 logging_writer.py:48] [130800] global_step=130800, grad_norm=5.0857977867126465, loss=2.3332090377807617
I0307 21:48:35.477509 140021218662144 logging_writer.py:48] [130900] global_step=130900, grad_norm=4.5886335372924805, loss=2.311514139175415
I0307 21:49:17.510604 140178276385984 spec.py:321] Evaluating on the training split.
I0307 21:49:27.490608 140178276385984 spec.py:333] Evaluating on the validation split.
I0307 21:49:47.911399 140178276385984 spec.py:349] Evaluating on the test split.
I0307 21:49:49.640369 140178276385984 submission_runner.py:469] Time since start: 70030.68s, 	Step: 130911, 	{'train/accuracy': 0.7705675959587097, 'train/loss': 0.9718958735466003, 'validation/accuracy': 0.6991999745368958, 'validation/loss': 1.2852084636688232, 'validation/num_examples': 50000, 'test/accuracy': 0.5732000470161438, 'test/loss': 1.934641718864441, 'test/num_examples': 10000, 'score': 65379.43269944191, 'total_duration': 70030.68398976326, 'accumulated_submission_time': 65379.43269944191, 'accumulated_eval_time': 4623.503634691238, 'accumulated_logging_time': 13.499063491821289}
I0307 21:49:49.664853 140021210269440 logging_writer.py:48] [130911] accumulated_eval_time=4623.5, accumulated_logging_time=13.4991, accumulated_submission_time=65379.4, global_step=130911, preemption_count=0, score=65379.4, test/accuracy=0.5732, test/loss=1.93464, test/num_examples=10000, total_duration=70030.7, train/accuracy=0.770568, train/loss=0.971896, validation/accuracy=0.6992, validation/loss=1.28521, validation/num_examples=50000
I0307 21:53:07.447697 140021218662144 logging_writer.py:48] [131000] global_step=131000, grad_norm=4.844226837158203, loss=2.199058771133423
I0307 21:56:25.382179 140021210269440 logging_writer.py:48] [131100] global_step=131100, grad_norm=5.065819263458252, loss=2.3916759490966797
I0307 21:58:21.236792 140178276385984 spec.py:321] Evaluating on the training split.
I0307 21:58:31.149207 140178276385984 spec.py:333] Evaluating on the validation split.
I0307 21:58:50.108951 140178276385984 spec.py:349] Evaluating on the test split.
I0307 21:58:51.847865 140178276385984 submission_runner.py:469] Time since start: 70572.89s, 	Step: 131155, 	{'train/accuracy': 0.7739756107330322, 'train/loss': 0.971443235874176, 'validation/accuracy': 0.699400007724762, 'validation/loss': 1.290891170501709, 'validation/num_examples': 50000, 'test/accuracy': 0.5782999992370605, 'test/loss': 1.9318766593933105, 'test/num_examples': 10000, 'score': 65890.97120189667, 'total_duration': 70572.89148759842, 'accumulated_submission_time': 65890.97120189667, 'accumulated_eval_time': 4654.114678621292, 'accumulated_logging_time': 13.53047513961792}
I0307 21:58:51.872677 140021218662144 logging_writer.py:48] [131155] accumulated_eval_time=4654.11, accumulated_logging_time=13.5305, accumulated_submission_time=65891, global_step=131155, preemption_count=0, score=65891, test/accuracy=0.5783, test/loss=1.93188, test/num_examples=10000, total_duration=70572.9, train/accuracy=0.773976, train/loss=0.971443, validation/accuracy=0.6994, validation/loss=1.29089, validation/num_examples=50000
I0307 22:00:18.360917 140021210269440 logging_writer.py:48] [131200] global_step=131200, grad_norm=4.528295040130615, loss=2.2897677421569824
I0307 22:03:54.811504 140021218662144 logging_writer.py:48] [131300] global_step=131300, grad_norm=4.895885944366455, loss=2.2830402851104736
I0307 22:07:23.905797 140021210269440 logging_writer.py:48] [131398] global_step=131398, preemption_count=0, score=66402.9
I0307 22:07:25.488952 140178276385984 submission_runner.py:646] Tuning trial 1/5
I0307 22:07:25.502889 140178276385984 submission_runner.py:647] Hyperparameters: Hyperparameters(dropout_rate=0.0, label_smoothing=0.1, learning_rate=0.001308209823469072, one_minus_beta1=0.02686663061, beta2=0.9981232922116359, weight_decay=0.16375311233774334, warmup_factor=0.1)
I0307 22:07:25.507027 140178276385984 submission_runner.py:648] Metrics: {'eval_results': [(1, {'train/accuracy': 0.000996492337435484, 'train/loss': 6.913202285766602, 'validation/accuracy': 0.0012599999317899346, 'validation/loss': 6.912908554077148, 'validation/num_examples': 50000, 'test/accuracy': 0.0014000000664964318, 'test/loss': 6.912501335144043, 'test/num_examples': 10000, 'score': 58.73045778274536, 'total_duration': 184.43373084068298, 'accumulated_submission_time': 58.73045778274536, 'accumulated_eval_time': 125.70307064056396, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1364, {'train/accuracy': 0.0675223171710968, 'train/loss': 5.486059188842773, 'validation/accuracy': 0.06019999831914902, 'validation/loss': 5.564065456390381, 'validation/num_examples': 50000, 'test/accuracy': 0.04190000146627426, 'test/loss': 5.753705978393555, 'test/num_examples': 10000, 'score': 568.6404049396515, 'total_duration': 726.6521196365356, 'accumulated_submission_time': 568.6404049396515, 'accumulated_eval_time': 157.75989651679993, 'accumulated_logging_time': 0.04564213752746582, 'global_step': 1364, 'preemption_count': 0}), (2714, {'train/accuracy': 0.15852199494838715, 'train/loss': 4.433286190032959, 'validation/accuracy': 0.13793998956680298, 'validation/loss': 4.577001571655273, 'validation/num_examples': 50000, 'test/accuracy': 0.09740000218153, 'test/loss': 4.945664882659912, 'test/num_examples': 10000, 'score': 1078.768221616745, 'total_duration': 1265.8174669742584, 'accumulated_submission_time': 1078.768221616745, 'accumulated_eval_time': 186.58173441886902, 'accumulated_logging_time': 0.07293891906738281, 'global_step': 2714, 'preemption_count': 0}), (4046, {'train/accuracy': 0.25958624482154846, 'train/loss': 3.62833571434021, 'validation/accuracy': 0.22817999124526978, 'validation/loss': 3.8408868312835693, 'validation/num_examples': 50000, 'test/accuracy': 0.16610001027584076, 'test/loss': 4.363452434539795, 'test/num_examples': 10000, 'score': 1588.6849162578583, 'total_duration': 1812.0272243022919, 'accumulated_submission_time': 1588.6849162578583, 'accumulated_eval_time': 222.6708743572235, 'accumulated_logging_time': 0.11372518539428711, 'global_step': 4046, 'preemption_count': 0}), (5354, {'train/accuracy': 0.3443279564380646, 'train/loss': 3.1209475994110107, 'validation/accuracy': 0.3086400032043457, 'validation/loss': 3.3249173164367676, 'validation/num_examples': 50000, 'test/accuracy': 0.2297000139951706, 'test/loss': 3.9242238998413086, 'test/num_examples': 10000, 'score': 2098.7594044208527, 'total_duration': 2353.8241600990295, 'accumulated_submission_time': 2098.7594044208527, 'accumulated_eval_time': 254.20077347755432, 'accumulated_logging_time': 0.1502981185913086, 'global_step': 5354, 'preemption_count': 0}), (6676, {'train/accuracy': 0.4188257157802582, 'train/loss': 2.6895928382873535, 'validation/accuracy': 0.37417998909950256, 'validation/loss': 2.9200215339660645, 'validation/num_examples': 50000, 'test/accuracy': 0.2785000205039978, 'test/loss': 3.572272777557373, 'test/num_examples': 10000, 'score': 2608.8417177200317, 'total_duration': 2895.933042526245, 'accumulated_submission_time': 2608.8417177200317, 'accumulated_eval_time': 286.05760288238525, 'accumulated_logging_time': 0.19629478454589844, 'global_step': 6676, 'preemption_count': 0}), (7939, {'train/accuracy': 0.4820830523967743, 'train/loss': 2.3453423976898193, 'validation/accuracy': 0.4251599907875061, 'validation/loss': 2.626068592071533, 'validation/num_examples': 50000, 'test/accuracy': 0.32830002903938293, 'test/loss': 3.2708113193511963, 'test/num_examples': 10000, 'score': 3118.9543426036835, 'total_duration': 3439.132836341858, 'accumulated_submission_time': 3118.9543426036835, 'accumulated_eval_time': 318.89340806007385, 'accumulated_logging_time': 0.31638145446777344, 'global_step': 7939, 'preemption_count': 0}), (9273, {'train/accuracy': 0.5176777839660645, 'train/loss': 2.1896069049835205, 'validation/accuracy': 0.46333998441696167, 'validation/loss': 2.463589668273926, 'validation/num_examples': 50000, 'test/accuracy': 0.3579000234603882, 'test/loss': 3.092560291290283, 'test/num_examples': 10000, 'score': 3628.821580171585, 'total_duration': 3982.388798236847, 'accumulated_submission_time': 3628.821580171585, 'accumulated_eval_time': 352.04007840156555, 'accumulated_logging_time': 0.40958476066589355, 'global_step': 9273, 'preemption_count': 0}), (10596, {'train/accuracy': 0.5426697731018066, 'train/loss': 2.040057897567749, 'validation/accuracy': 0.4890799820423126, 'validation/loss': 2.310072660446167, 'validation/num_examples': 50000, 'test/accuracy': 0.37550002336502075, 'test/loss': 2.9704039096832275, 'test/num_examples': 10000, 'score': 4138.6773002147675, 'total_duration': 4531.155784845352, 'accumulated_submission_time': 4138.6773002147675, 'accumulated_eval_time': 390.76485323905945, 'accumulated_logging_time': 0.453249454498291, 'global_step': 10596, 'preemption_count': 0}), (11919, {'train/accuracy': 0.5757134556770325, 'train/loss': 1.8782466650009155, 'validation/accuracy': 0.5187399983406067, 'validation/loss': 2.147243022918701, 'validation/num_examples': 50000, 'test/accuracy': 0.39900001883506775, 'test/loss': 2.8007309436798096, 'test/num_examples': 10000, 'score': 4648.66397023201, 'total_duration': 5075.199770689011, 'accumulated_submission_time': 4648.66397023201, 'accumulated_eval_time': 424.641606092453, 'accumulated_logging_time': 0.49742960929870605, 'global_step': 11919, 'preemption_count': 0}), (13243, {'train/accuracy': 0.5942283272743225, 'train/loss': 1.7778406143188477, 'validation/accuracy': 0.536899983882904, 'validation/loss': 2.0516693592071533, 'validation/num_examples': 50000, 'test/accuracy': 0.4289000332355499, 'test/loss': 2.710283041000366, 'test/num_examples': 10000, 'score': 5158.840147972107, 'total_duration': 5624.270761728287, 'accumulated_submission_time': 5158.840147972107, 'accumulated_eval_time': 463.343022108078, 'accumulated_logging_time': 0.5522055625915527, 'global_step': 13243, 'preemption_count': 0}), (14560, {'train/accuracy': 0.6015425324440002, 'train/loss': 1.7793742418289185, 'validation/accuracy': 0.5474199652671814, 'validation/loss': 2.0388450622558594, 'validation/num_examples': 50000, 'test/accuracy': 0.4229000210762024, 'test/loss': 2.727372646331787, 'test/num_examples': 10000, 'score': 5668.890580654144, 'total_duration': 6177.888833761215, 'accumulated_submission_time': 5668.890580654144, 'accumulated_eval_time': 506.6967315673828, 'accumulated_logging_time': 0.6243801116943359, 'global_step': 14560, 'preemption_count': 0}), (15878, {'train/accuracy': 0.6091358065605164, 'train/loss': 1.7038657665252686, 'validation/accuracy': 0.5549799799919128, 'validation/loss': 1.9675432443618774, 'validation/num_examples': 50000, 'test/accuracy': 0.43970000743865967, 'test/loss': 2.628751754760742, 'test/num_examples': 10000, 'score': 6178.918691635132, 'total_duration': 6728.452962398529, 'accumulated_submission_time': 6178.918691635132, 'accumulated_eval_time': 546.9710538387299, 'accumulated_logging_time': 0.7399330139160156, 'global_step': 15878, 'preemption_count': 0}), (17199, {'train/accuracy': 0.6215322017669678, 'train/loss': 1.6433767080307007, 'validation/accuracy': 0.5615800023078918, 'validation/loss': 1.9205811023712158, 'validation/num_examples': 50000, 'test/accuracy': 0.4382000267505646, 'test/loss': 2.6020491123199463, 'test/num_examples': 10000, 'score': 6688.884175777435, 'total_duration': 7277.159107208252, 'accumulated_submission_time': 6688.884175777435, 'accumulated_eval_time': 585.464373588562, 'accumulated_logging_time': 0.8444693088531494, 'global_step': 17199, 'preemption_count': 0}), (18518, {'train/accuracy': 0.6163504123687744, 'train/loss': 1.6868990659713745, 'validation/accuracy': 0.5631600022315979, 'validation/loss': 1.9450334310531616, 'validation/num_examples': 50000, 'test/accuracy': 0.44350001215934753, 'test/loss': 2.576486825942993, 'test/num_examples': 10000, 'score': 7198.761182785034, 'total_duration': 7822.632341384888, 'accumulated_submission_time': 7198.761182785034, 'accumulated_eval_time': 620.834246635437, 'accumulated_logging_time': 0.942030668258667, 'global_step': 18518, 'preemption_count': 0}), (19845, {'train/accuracy': 0.6201769709587097, 'train/loss': 1.6656979322433472, 'validation/accuracy': 0.5651000142097473, 'validation/loss': 1.926265001296997, 'validation/num_examples': 50000, 'test/accuracy': 0.43790000677108765, 'test/loss': 2.6407697200775146, 'test/num_examples': 10000, 'score': 7708.849365949631, 'total_duration': 8372.323499679565, 'accumulated_submission_time': 7708.849365949631, 'accumulated_eval_time': 660.2645676136017, 'accumulated_logging_time': 0.9778788089752197, 'global_step': 19845, 'preemption_count': 0}), (21164, {'train/accuracy': 0.6351442933082581, 'train/loss': 1.6175873279571533, 'validation/accuracy': 0.5804600119590759, 'validation/loss': 1.8682241439819336, 'validation/num_examples': 50000, 'test/accuracy': 0.4603000283241272, 'test/loss': 2.5139081478118896, 'test/num_examples': 10000, 'score': 8218.854027748108, 'total_duration': 8919.967081785202, 'accumulated_submission_time': 8218.854027748108, 'accumulated_eval_time': 697.6881215572357, 'accumulated_logging_time': 1.0596222877502441, 'global_step': 21164, 'preemption_count': 0}), (22486, {'train/accuracy': 0.6356425285339355, 'train/loss': 1.5772041082382202, 'validation/accuracy': 0.5849599838256836, 'validation/loss': 1.812549352645874, 'validation/num_examples': 50000, 'test/accuracy': 0.4580000340938568, 'test/loss': 2.490973711013794, 'test/num_examples': 10000, 'score': 8729.067692518234, 'total_duration': 9471.169401407242, 'accumulated_submission_time': 8729.067692518234, 'accumulated_eval_time': 738.5037069320679, 'accumulated_logging_time': 1.0967564582824707, 'global_step': 22486, 'preemption_count': 0}), (23803, {'train/accuracy': 0.6248405575752258, 'train/loss': 1.6467496156692505, 'validation/accuracy': 0.5753799676895142, 'validation/loss': 1.8854948282241821, 'validation/num_examples': 50000, 'test/accuracy': 0.45680001378059387, 'test/loss': 2.5530502796173096, 'test/num_examples': 10000, 'score': 9238.982190132141, 'total_duration': 10019.38933634758, 'accumulated_submission_time': 9238.982190132141, 'accumulated_eval_time': 776.6177186965942, 'accumulated_logging_time': 1.1485655307769775, 'global_step': 23803, 'preemption_count': 0}), (25121, {'train/accuracy': 0.626953125, 'train/loss': 1.6276590824127197, 'validation/accuracy': 0.5781199932098389, 'validation/loss': 1.8583282232284546, 'validation/num_examples': 50000, 'test/accuracy': 0.45020002126693726, 'test/loss': 2.554035186767578, 'test/num_examples': 10000, 'score': 9748.885342121124, 'total_duration': 10566.616530179977, 'accumulated_submission_time': 9748.885342121124, 'accumulated_eval_time': 813.7401127815247, 'accumulated_logging_time': 1.2130684852600098, 'global_step': 25121, 'preemption_count': 0}), (26436, {'train/accuracy': 0.6262555718421936, 'train/loss': 1.6446563005447388, 'validation/accuracy': 0.5775600075721741, 'validation/loss': 1.8790220022201538, 'validation/num_examples': 50000, 'test/accuracy': 0.45600003004074097, 'test/loss': 2.5358023643493652, 'test/num_examples': 10000, 'score': 10259.000326633453, 'total_duration': 11117.328831672668, 'accumulated_submission_time': 10259.000326633453, 'accumulated_eval_time': 854.0970220565796, 'accumulated_logging_time': 1.3244616985321045, 'global_step': 26436, 'preemption_count': 0}), (27754, {'train/accuracy': 0.6438336968421936, 'train/loss': 1.5523779392242432, 'validation/accuracy': 0.5927199721336365, 'validation/loss': 1.7915163040161133, 'validation/num_examples': 50000, 'test/accuracy': 0.46890002489089966, 'test/loss': 2.48091197013855, 'test/num_examples': 10000, 'score': 10768.678529977798, 'total_duration': 11668.561493635178, 'accumulated_submission_time': 10768.678529977798, 'accumulated_eval_time': 895.0297713279724, 'accumulated_logging_time': 1.811082363128662, 'global_step': 27754, 'preemption_count': 0}), (29072, {'train/accuracy': 0.643953263759613, 'train/loss': 1.5473839044570923, 'validation/accuracy': 0.5926399827003479, 'validation/loss': 1.8019572496414185, 'validation/num_examples': 50000, 'test/accuracy': 0.47220003604888916, 'test/loss': 2.455782890319824, 'test/num_examples': 10000, 'score': 11278.552919864655, 'total_duration': 12216.936494588852, 'accumulated_submission_time': 11278.552919864655, 'accumulated_eval_time': 933.3212687969208, 'accumulated_logging_time': 1.8892240524291992, 'global_step': 29072, 'preemption_count': 0}), (30387, {'train/accuracy': 0.6468630433082581, 'train/loss': 1.554618000984192, 'validation/accuracy': 0.5940999984741211, 'validation/loss': 1.7913073301315308, 'validation/num_examples': 50000, 'test/accuracy': 0.46870002150535583, 'test/loss': 2.4587674140930176, 'test/num_examples': 10000, 'score': 11788.623572349548, 'total_duration': 12771.77317738533, 'accumulated_submission_time': 11788.623572349548, 'accumulated_eval_time': 977.775559425354, 'accumulated_logging_time': 2.0716392993927, 'global_step': 30387, 'preemption_count': 0}), (31703, {'train/accuracy': 0.6502909660339355, 'train/loss': 1.5120701789855957, 'validation/accuracy': 0.5998199582099915, 'validation/loss': 1.7403184175491333, 'validation/num_examples': 50000, 'test/accuracy': 0.47450003027915955, 'test/loss': 2.4055850505828857, 'test/num_examples': 10000, 'score': 12298.59297657013, 'total_duration': 13318.485968828201, 'accumulated_submission_time': 12298.59297657013, 'accumulated_eval_time': 1014.29518699646, 'accumulated_logging_time': 2.158447265625, 'global_step': 31703, 'preemption_count': 0}), (33018, {'train/accuracy': 0.6458067297935486, 'train/loss': 1.522133469581604, 'validation/accuracy': 0.6018399596214294, 'validation/loss': 1.7434443235397339, 'validation/num_examples': 50000, 'test/accuracy': 0.4806000292301178, 'test/loss': 2.4198873043060303, 'test/num_examples': 10000, 'score': 12808.712173938751, 'total_duration': 13868.920781612396, 'accumulated_submission_time': 12808.712173938751, 'accumulated_eval_time': 1054.3969042301178, 'accumulated_logging_time': 2.239515542984009, 'global_step': 33018, 'preemption_count': 0}), (34335, {'train/accuracy': 0.6493741869926453, 'train/loss': 1.547829270362854, 'validation/accuracy': 0.5983799695968628, 'validation/loss': 1.7821993827819824, 'validation/num_examples': 50000, 'test/accuracy': 0.47460001707077026, 'test/loss': 2.423468589782715, 'test/num_examples': 10000, 'score': 13318.718039989471, 'total_duration': 14415.638063907623, 'accumulated_submission_time': 13318.718039989471, 'accumulated_eval_time': 1090.8937418460846, 'accumulated_logging_time': 2.3205432891845703, 'global_step': 34335, 'preemption_count': 0}), (35651, {'train/accuracy': 0.6356824040412903, 'train/loss': 1.5992987155914307, 'validation/accuracy': 0.5906000137329102, 'validation/loss': 1.8113071918487549, 'validation/num_examples': 50000, 'test/accuracy': 0.46390002965927124, 'test/loss': 2.5130488872528076, 'test/num_examples': 10000, 'score': 13828.68073129654, 'total_duration': 14965.26175737381, 'accumulated_submission_time': 13828.68073129654, 'accumulated_eval_time': 1130.2455112934113, 'accumulated_logging_time': 2.496436595916748, 'global_step': 35651, 'preemption_count': 0}), (36964, {'train/accuracy': 0.6471619606018066, 'train/loss': 1.5200525522232056, 'validation/accuracy': 0.5983200073242188, 'validation/loss': 1.7462304830551147, 'validation/num_examples': 50000, 'test/accuracy': 0.47470003366470337, 'test/loss': 2.412858009338379, 'test/num_examples': 10000, 'score': 14338.458849191666, 'total_duration': 15509.033997297287, 'accumulated_submission_time': 14338.458849191666, 'accumulated_eval_time': 1163.9601674079895, 'accumulated_logging_time': 2.6400506496429443, 'global_step': 36964, 'preemption_count': 0}), (38276, {'train/accuracy': 0.6376355290412903, 'train/loss': 1.5618999004364014, 'validation/accuracy': 0.5956400036811829, 'validation/loss': 1.7637141942977905, 'validation/num_examples': 50000, 'test/accuracy': 0.4739000201225281, 'test/loss': 2.428598642349243, 'test/num_examples': 10000, 'score': 14848.297893047333, 'total_duration': 16051.706707715988, 'accumulated_submission_time': 14848.297893047333, 'accumulated_eval_time': 1196.538817167282, 'accumulated_logging_time': 2.7606732845306396, 'global_step': 38276, 'preemption_count': 0}), (39589, {'train/accuracy': 0.6508290767669678, 'train/loss': 1.5039514303207397, 'validation/accuracy': 0.6037600040435791, 'validation/loss': 1.7200927734375, 'validation/num_examples': 50000, 'test/accuracy': 0.4755000174045563, 'test/loss': 2.4143035411834717, 'test/num_examples': 10000, 'score': 15358.123179197311, 'total_duration': 16602.310455322266, 'accumulated_submission_time': 15358.123179197311, 'accumulated_eval_time': 1237.0061221122742, 'accumulated_logging_time': 2.9374351501464844, 'global_step': 39589, 'preemption_count': 0}), (40903, {'train/accuracy': 0.6486367583274841, 'train/loss': 1.5019538402557373, 'validation/accuracy': 0.6007800102233887, 'validation/loss': 1.7279860973358154, 'validation/num_examples': 50000, 'test/accuracy': 0.46870002150535583, 'test/loss': 2.4473395347595215, 'test/num_examples': 10000, 'score': 15867.841248989105, 'total_duration': 17152.022185325623, 'accumulated_submission_time': 15867.841248989105, 'accumulated_eval_time': 1276.7072124481201, 'accumulated_logging_time': 3.0963544845581055, 'global_step': 40903, 'preemption_count': 0}), (42213, {'train/accuracy': 0.649832546710968, 'train/loss': 1.5172303915023804, 'validation/accuracy': 0.6005799770355225, 'validation/loss': 1.753570795059204, 'validation/num_examples': 50000, 'test/accuracy': 0.4838000237941742, 'test/loss': 2.393526792526245, 'test/num_examples': 10000, 'score': 16378.066253900528, 'total_duration': 17700.466415166855, 'accumulated_submission_time': 16378.066253900528, 'accumulated_eval_time': 1314.67906832695, 'accumulated_logging_time': 3.2084603309631348, 'global_step': 42213, 'preemption_count': 0}), (43526, {'train/accuracy': 0.660574734210968, 'train/loss': 1.4861271381378174, 'validation/accuracy': 0.6094200015068054, 'validation/loss': 1.7295567989349365, 'validation/num_examples': 50000, 'test/accuracy': 0.4872000217437744, 'test/loss': 2.392664909362793, 'test/num_examples': 10000, 'score': 16887.953792095184, 'total_duration': 18245.37069272995, 'accumulated_submission_time': 16887.953792095184, 'accumulated_eval_time': 1349.4510612487793, 'accumulated_logging_time': 3.3152430057525635, 'global_step': 43526, 'preemption_count': 0}), (44841, {'train/accuracy': 0.6543566584587097, 'train/loss': 1.4867870807647705, 'validation/accuracy': 0.608299970626831, 'validation/loss': 1.704209327697754, 'validation/num_examples': 50000, 'test/accuracy': 0.48590001463890076, 'test/loss': 2.38734769821167, 'test/num_examples': 10000, 'score': 17398.072768211365, 'total_duration': 18790.966079711914, 'accumulated_submission_time': 17398.072768211365, 'accumulated_eval_time': 1384.7077555656433, 'accumulated_logging_time': 3.398590326309204, 'global_step': 44841, 'preemption_count': 0}), (46154, {'train/accuracy': 0.6581433415412903, 'train/loss': 1.5149755477905273, 'validation/accuracy': 0.607479989528656, 'validation/loss': 1.7512638568878174, 'validation/num_examples': 50000, 'test/accuracy': 0.49010002613067627, 'test/loss': 2.3885762691497803, 'test/num_examples': 10000, 'score': 17908.021098852158, 'total_duration': 19340.73963212967, 'accumulated_submission_time': 17908.021098852158, 'accumulated_eval_time': 1424.3016283512115, 'accumulated_logging_time': 3.4924120903015137, 'global_step': 46154, 'preemption_count': 0}), (47467, {'train/accuracy': 0.6602957248687744, 'train/loss': 1.4626154899597168, 'validation/accuracy': 0.6132799983024597, 'validation/loss': 1.67678701877594, 'validation/num_examples': 50000, 'test/accuracy': 0.4881000220775604, 'test/loss': 2.350416421890259, 'test/num_examples': 10000, 'score': 18418.08557868004, 'total_duration': 19888.50702190399, 'accumulated_submission_time': 18418.08557868004, 'accumulated_eval_time': 1461.7590398788452, 'accumulated_logging_time': 3.5983238220214844, 'global_step': 47467, 'preemption_count': 0}), (48781, {'train/accuracy': 0.6590800285339355, 'train/loss': 1.482071876525879, 'validation/accuracy': 0.6136199831962585, 'validation/loss': 1.6933525800704956, 'validation/num_examples': 50000, 'test/accuracy': 0.489300012588501, 'test/loss': 2.389267683029175, 'test/num_examples': 10000, 'score': 18927.877472162247, 'total_duration': 20438.90229868889, 'accumulated_submission_time': 18927.877472162247, 'accumulated_eval_time': 1502.112146615982, 'accumulated_logging_time': 3.713918685913086, 'global_step': 48781, 'preemption_count': 0}), (50099, {'train/accuracy': 0.6592195630073547, 'train/loss': 1.4656013250350952, 'validation/accuracy': 0.6134999990463257, 'validation/loss': 1.6840779781341553, 'validation/num_examples': 50000, 'test/accuracy': 0.48600003123283386, 'test/loss': 2.373300075531006, 'test/num_examples': 10000, 'score': 19437.684385299683, 'total_duration': 20988.74840116501, 'accumulated_submission_time': 19437.684385299683, 'accumulated_eval_time': 1541.8967187404633, 'accumulated_logging_time': 3.835242986679077, 'global_step': 50099, 'preemption_count': 0}), (51377, {'train/accuracy': 0.6623485088348389, 'train/loss': 1.4459306001663208, 'validation/accuracy': 0.6115399599075317, 'validation/loss': 1.678749680519104, 'validation/num_examples': 50000, 'test/accuracy': 0.48840001225471497, 'test/loss': 2.377931594848633, 'test/num_examples': 10000, 'score': 19947.51396727562, 'total_duration': 21537.916051387787, 'accumulated_submission_time': 19947.51396727562, 'accumulated_eval_time': 1580.963044166565, 'accumulated_logging_time': 3.9767110347747803, 'global_step': 51377, 'preemption_count': 0}), (52687, {'train/accuracy': 0.6687459945678711, 'train/loss': 1.454114556312561, 'validation/accuracy': 0.617859959602356, 'validation/loss': 1.6900898218154907, 'validation/num_examples': 50000, 'test/accuracy': 0.5021000504493713, 'test/loss': 2.3211350440979004, 'test/num_examples': 10000, 'score': 20457.49803853035, 'total_duration': 22086.88534140587, 'accumulated_submission_time': 20457.49803853035, 'accumulated_eval_time': 1619.67600274086, 'accumulated_logging_time': 4.113572359085083, 'global_step': 52687, 'preemption_count': 0}), (54010, {'train/accuracy': 0.6610730290412903, 'train/loss': 1.4648584127426147, 'validation/accuracy': 0.6103399991989136, 'validation/loss': 1.694023609161377, 'validation/num_examples': 50000, 'test/accuracy': 0.48420003056526184, 'test/loss': 2.402348756790161, 'test/num_examples': 10000, 'score': 20967.674545049667, 'total_duration': 22634.503627300262, 'accumulated_submission_time': 20967.674545049667, 'accumulated_eval_time': 1656.8635902404785, 'accumulated_logging_time': 4.229868412017822, 'global_step': 54010, 'preemption_count': 0}), (55321, {'train/accuracy': 0.6611726880073547, 'train/loss': 1.4714429378509521, 'validation/accuracy': 0.6130599975585938, 'validation/loss': 1.701870083808899, 'validation/num_examples': 50000, 'test/accuracy': 0.4928000271320343, 'test/loss': 2.3565683364868164, 'test/num_examples': 10000, 'score': 21477.61782360077, 'total_duration': 23179.13675260544, 'accumulated_submission_time': 21477.61782360077, 'accumulated_eval_time': 1691.293715953827, 'accumulated_logging_time': 4.355632781982422, 'global_step': 55321, 'preemption_count': 0}), (56633, {'train/accuracy': 0.6633051633834839, 'train/loss': 1.4805388450622559, 'validation/accuracy': 0.6139599680900574, 'validation/loss': 1.7096501588821411, 'validation/num_examples': 50000, 'test/accuracy': 0.4856000244617462, 'test/loss': 2.3927533626556396, 'test/num_examples': 10000, 'score': 21987.49871826172, 'total_duration': 23725.494714975357, 'accumulated_submission_time': 21987.49871826172, 'accumulated_eval_time': 1727.506727218628, 'accumulated_logging_time': 4.483761548995972, 'global_step': 56633, 'preemption_count': 0}), (57942, {'train/accuracy': 0.6635243892669678, 'train/loss': 1.4601473808288574, 'validation/accuracy': 0.6194800138473511, 'validation/loss': 1.6809638738632202, 'validation/num_examples': 50000, 'test/accuracy': 0.49490001797676086, 'test/loss': 2.341952085494995, 'test/num_examples': 10000, 'score': 22497.533754587173, 'total_duration': 24267.5276324749, 'accumulated_submission_time': 22497.533754587173, 'accumulated_eval_time': 1759.198405265808, 'accumulated_logging_time': 4.650980472564697, 'global_step': 57942, 'preemption_count': 0}), (59250, {'train/accuracy': 0.6815210580825806, 'train/loss': 1.407950520515442, 'validation/accuracy': 0.6273199915885925, 'validation/loss': 1.64316725730896, 'validation/num_examples': 50000, 'test/accuracy': 0.5069000124931335, 'test/loss': 2.278907299041748, 'test/num_examples': 10000, 'score': 23007.503632068634, 'total_duration': 24813.594052791595, 'accumulated_submission_time': 23007.503632068634, 'accumulated_eval_time': 1795.0393946170807, 'accumulated_logging_time': 4.769937992095947, 'global_step': 59250, 'preemption_count': 0}), (60564, {'train/accuracy': 0.6716955900192261, 'train/loss': 1.43076491355896, 'validation/accuracy': 0.6256999969482422, 'validation/loss': 1.652513027191162, 'validation/num_examples': 50000, 'test/accuracy': 0.5027000308036804, 'test/loss': 2.324721097946167, 'test/num_examples': 10000, 'score': 23517.36927509308, 'total_duration': 25359.12310576439, 'accumulated_submission_time': 23517.36927509308, 'accumulated_eval_time': 1830.45521402359, 'accumulated_logging_time': 4.889114618301392, 'global_step': 60564, 'preemption_count': 0}), (61881, {'train/accuracy': 0.671875, 'train/loss': 1.4209444522857666, 'validation/accuracy': 0.6280800104141235, 'validation/loss': 1.634539008140564, 'validation/num_examples': 50000, 'test/accuracy': 0.5053000450134277, 'test/loss': 2.3042047023773193, 'test/num_examples': 10000, 'score': 24027.429548978806, 'total_duration': 25905.54525899887, 'accumulated_submission_time': 24027.429548978806, 'accumulated_eval_time': 1866.5778954029083, 'accumulated_logging_time': 4.998018741607666, 'global_step': 61881, 'preemption_count': 0}), (63193, {'train/accuracy': 0.6750239133834839, 'train/loss': 1.3740328550338745, 'validation/accuracy': 0.6272000074386597, 'validation/loss': 1.6051738262176514, 'validation/num_examples': 50000, 'test/accuracy': 0.5029000043869019, 'test/loss': 2.2685818672180176, 'test/num_examples': 10000, 'score': 24537.16818213463, 'total_duration': 26455.723151683807, 'accumulated_submission_time': 24537.16818213463, 'accumulated_eval_time': 1906.7139070034027, 'accumulated_logging_time': 5.164235591888428, 'global_step': 63193, 'preemption_count': 0}), (64499, {'train/accuracy': 0.6745654940605164, 'train/loss': 1.4092012643814087, 'validation/accuracy': 0.6245399713516235, 'validation/loss': 1.6473835706710815, 'validation/num_examples': 50000, 'test/accuracy': 0.5029000043869019, 'test/loss': 2.3262922763824463, 'test/num_examples': 10000, 'score': 25046.89953494072, 'total_duration': 26998.881210803986, 'accumulated_submission_time': 25046.89953494072, 'accumulated_eval_time': 1939.8683335781097, 'accumulated_logging_time': 5.304022312164307, 'global_step': 64499, 'preemption_count': 0}), (65815, {'train/accuracy': 0.6670320630073547, 'train/loss': 1.4286961555480957, 'validation/accuracy': 0.6154599785804749, 'validation/loss': 1.6613672971725464, 'validation/num_examples': 50000, 'test/accuracy': 0.49710002541542053, 'test/loss': 2.3397810459136963, 'test/num_examples': 10000, 'score': 25556.913395404816, 'total_duration': 27543.49799132347, 'accumulated_submission_time': 25556.913395404816, 'accumulated_eval_time': 1974.1832756996155, 'accumulated_logging_time': 5.456932783126831, 'global_step': 65815, 'preemption_count': 0}), (67124, {'train/accuracy': 0.6716158986091614, 'train/loss': 1.4147002696990967, 'validation/accuracy': 0.6239399909973145, 'validation/loss': 1.6442785263061523, 'validation/num_examples': 50000, 'test/accuracy': 0.5065000057220459, 'test/loss': 2.2860329151153564, 'test/num_examples': 10000, 'score': 26066.837782144547, 'total_duration': 28085.895067214966, 'accumulated_submission_time': 26066.837782144547, 'accumulated_eval_time': 2006.33753824234, 'accumulated_logging_time': 5.6403584480285645, 'global_step': 67124, 'preemption_count': 0}), (68434, {'train/accuracy': 0.6814811825752258, 'train/loss': 1.375356674194336, 'validation/accuracy': 0.6348400115966797, 'validation/loss': 1.5984749794006348, 'validation/num_examples': 50000, 'test/accuracy': 0.5063000321388245, 'test/loss': 2.266976833343506, 'test/num_examples': 10000, 'score': 26576.572456598282, 'total_duration': 28633.104213237762, 'accumulated_submission_time': 26576.572456598282, 'accumulated_eval_time': 2043.5450999736786, 'accumulated_logging_time': 5.771882772445679, 'global_step': 68434, 'preemption_count': 0}), (69742, {'train/accuracy': 0.6739277839660645, 'train/loss': 1.4021824598312378, 'validation/accuracy': 0.6243199706077576, 'validation/loss': 1.6376807689666748, 'validation/num_examples': 50000, 'test/accuracy': 0.4970000088214874, 'test/loss': 2.3001816272735596, 'test/num_examples': 10000, 'score': 27086.497886657715, 'total_duration': 29175.9358522892, 'accumulated_submission_time': 27086.497886657715, 'accumulated_eval_time': 2076.1965646743774, 'accumulated_logging_time': 5.893910884857178, 'global_step': 69742, 'preemption_count': 0}), (71053, {'train/accuracy': 0.6735690236091614, 'train/loss': 1.3958847522735596, 'validation/accuracy': 0.6253199577331543, 'validation/loss': 1.6287626028060913, 'validation/num_examples': 50000, 'test/accuracy': 0.49810001254081726, 'test/loss': 2.2989299297332764, 'test/num_examples': 10000, 'score': 27596.4241232872, 'total_duration': 29722.662075042725, 'accumulated_submission_time': 27596.4241232872, 'accumulated_eval_time': 2112.6913816928864, 'accumulated_logging_time': 6.063194513320923, 'global_step': 71053, 'preemption_count': 0}), (72365, {'train/accuracy': 0.6788703799247742, 'train/loss': 1.4000656604766846, 'validation/accuracy': 0.6317600011825562, 'validation/loss': 1.621607780456543, 'validation/num_examples': 50000, 'test/accuracy': 0.5060000419616699, 'test/loss': 2.3053808212280273, 'test/num_examples': 10000, 'score': 28106.429681777954, 'total_duration': 30268.88716864586, 'accumulated_submission_time': 28106.429681777954, 'accumulated_eval_time': 2148.618767261505, 'accumulated_logging_time': 6.223567008972168, 'global_step': 72365, 'preemption_count': 0}), (73678, {'train/accuracy': 0.6807637214660645, 'train/loss': 1.412006139755249, 'validation/accuracy': 0.6336399912834167, 'validation/loss': 1.632641315460205, 'validation/num_examples': 50000, 'test/accuracy': 0.5109000205993652, 'test/loss': 2.3001463413238525, 'test/num_examples': 10000, 'score': 28616.465846061707, 'total_duration': 30815.07935500145, 'accumulated_submission_time': 28616.465846061707, 'accumulated_eval_time': 2184.4910113811493, 'accumulated_logging_time': 6.375152826309204, 'global_step': 73678, 'preemption_count': 0}), (74960, {'train/accuracy': 0.6818598508834839, 'train/loss': 1.3522303104400635, 'validation/accuracy': 0.6339399814605713, 'validation/loss': 1.579234004020691, 'validation/num_examples': 50000, 'test/accuracy': 0.5148000121116638, 'test/loss': 2.217301607131958, 'test/num_examples': 10000, 'score': 29126.742819070816, 'total_duration': 31360.731994628906, 'accumulated_submission_time': 29126.742819070816, 'accumulated_eval_time': 2219.580649614334, 'accumulated_logging_time': 6.529333591461182, 'global_step': 74960, 'preemption_count': 0}), (76280, {'train/accuracy': 0.692402720451355, 'train/loss': 1.3357133865356445, 'validation/accuracy': 0.6426999568939209, 'validation/loss': 1.573549509048462, 'validation/num_examples': 50000, 'test/accuracy': 0.5139000415802002, 'test/loss': 2.238517999649048, 'test/num_examples': 10000, 'score': 29636.832682847977, 'total_duration': 31912.44214630127, 'accumulated_submission_time': 29636.832682847977, 'accumulated_eval_time': 2260.9373409748077, 'accumulated_logging_time': 6.660996913909912, 'global_step': 76280, 'preemption_count': 0}), (77598, {'train/accuracy': 0.6854472160339355, 'train/loss': 1.3634953498840332, 'validation/accuracy': 0.6365999579429626, 'validation/loss': 1.5826818943023682, 'validation/num_examples': 50000, 'test/accuracy': 0.5142000317573547, 'test/loss': 2.2404658794403076, 'test/num_examples': 10000, 'score': 30146.72769021988, 'total_duration': 32453.902106523514, 'accumulated_submission_time': 30146.72769021988, 'accumulated_eval_time': 2292.289056301117, 'accumulated_logging_time': 6.738872289657593, 'global_step': 77598, 'preemption_count': 0}), (78912, {'train/accuracy': 0.6892936825752258, 'train/loss': 1.352744221687317, 'validation/accuracy': 0.6391800045967102, 'validation/loss': 1.577341079711914, 'validation/num_examples': 50000, 'test/accuracy': 0.5157000422477722, 'test/loss': 2.2470035552978516, 'test/num_examples': 10000, 'score': 30656.435901641846, 'total_duration': 33009.96707415581, 'accumulated_submission_time': 30656.435901641846, 'accumulated_eval_time': 2338.3528513908386, 'accumulated_logging_time': 6.89769172668457, 'global_step': 78912, 'preemption_count': 0}), (80232, {'train/accuracy': 0.6902303695678711, 'train/loss': 1.3328542709350586, 'validation/accuracy': 0.6419599652290344, 'validation/loss': 1.5575660467147827, 'validation/num_examples': 50000, 'test/accuracy': 0.5175000429153442, 'test/loss': 2.2019965648651123, 'test/num_examples': 10000, 'score': 31166.220273017883, 'total_duration': 33554.590924978256, 'accumulated_submission_time': 31166.220273017883, 'accumulated_eval_time': 2372.933177947998, 'accumulated_logging_time': 7.0192365646362305, 'global_step': 80232, 'preemption_count': 0}), (81550, {'train/accuracy': 0.6890544891357422, 'train/loss': 1.3389873504638672, 'validation/accuracy': 0.6426799893379211, 'validation/loss': 1.5595519542694092, 'validation/num_examples': 50000, 'test/accuracy': 0.5101000070571899, 'test/loss': 2.2419626712799072, 'test/num_examples': 10000, 'score': 31676.05092048645, 'total_duration': 34101.459647893906, 'accumulated_submission_time': 31676.05092048645, 'accumulated_eval_time': 2409.7041358947754, 'accumulated_logging_time': 7.154574871063232, 'global_step': 81550, 'preemption_count': 0}), (82866, {'train/accuracy': 0.6954519748687744, 'train/loss': 1.317766785621643, 'validation/accuracy': 0.643559992313385, 'validation/loss': 1.5462788343429565, 'validation/num_examples': 50000, 'test/accuracy': 0.5209000110626221, 'test/loss': 2.200854778289795, 'test/num_examples': 10000, 'score': 32185.880859375, 'total_duration': 34645.37012696266, 'accumulated_submission_time': 32185.880859375, 'accumulated_eval_time': 2443.5153691768646, 'accumulated_logging_time': 7.285172462463379, 'global_step': 82866, 'preemption_count': 0}), (84177, {'train/accuracy': 0.6924425959587097, 'train/loss': 1.3435760736465454, 'validation/accuracy': 0.6429600119590759, 'validation/loss': 1.5658118724822998, 'validation/num_examples': 50000, 'test/accuracy': 0.5148000121116638, 'test/loss': 2.2513833045959473, 'test/num_examples': 10000, 'score': 32695.633306741714, 'total_duration': 35190.62775874138, 'accumulated_submission_time': 32695.633306741714, 'accumulated_eval_time': 2478.750470161438, 'accumulated_logging_time': 7.41370701789856, 'global_step': 84177, 'preemption_count': 0}), (85481, {'train/accuracy': 0.6950533986091614, 'train/loss': 1.3141634464263916, 'validation/accuracy': 0.6448000073432922, 'validation/loss': 1.5536383390426636, 'validation/num_examples': 50000, 'test/accuracy': 0.5141000151634216, 'test/loss': 2.2255003452301025, 'test/num_examples': 10000, 'score': 33205.53166937828, 'total_duration': 35734.82333302498, 'accumulated_submission_time': 33205.53166937828, 'accumulated_eval_time': 2512.784499645233, 'accumulated_logging_time': 7.529149770736694, 'global_step': 85481, 'preemption_count': 0}), (86815, {'train/accuracy': 0.6929807066917419, 'train/loss': 1.336341381072998, 'validation/accuracy': 0.644540011882782, 'validation/loss': 1.5634878873825073, 'validation/num_examples': 50000, 'test/accuracy': 0.5164999961853027, 'test/loss': 2.2240328788757324, 'test/num_examples': 10000, 'score': 33715.46340942383, 'total_duration': 36290.17020225525, 'accumulated_submission_time': 33715.46340942383, 'accumulated_eval_time': 2557.8713188171387, 'accumulated_logging_time': 7.696270704269409, 'global_step': 86815, 'preemption_count': 0}), (88015, {'train/accuracy': 0.6919841766357422, 'train/loss': 1.3221924304962158, 'validation/accuracy': 0.6390199661254883, 'validation/loss': 1.5645782947540283, 'validation/num_examples': 50000, 'test/accuracy': 0.5217000246047974, 'test/loss': 2.2037353515625, 'test/num_examples': 10000, 'score': 34225.476650476456, 'total_duration': 36836.33246874809, 'accumulated_submission_time': 34225.476650476456, 'accumulated_eval_time': 2593.7514803409576, 'accumulated_logging_time': 7.820662021636963, 'global_step': 88015, 'preemption_count': 0}), (89241, {'train/accuracy': 0.7044602632522583, 'train/loss': 1.2716877460479736, 'validation/accuracy': 0.6524800062179565, 'validation/loss': 1.5168160200119019, 'validation/num_examples': 50000, 'test/accuracy': 0.5270000100135803, 'test/loss': 2.167029619216919, 'test/num_examples': 10000, 'score': 34736.05078983307, 'total_duration': 37380.87375640869, 'accumulated_submission_time': 34736.05078983307, 'accumulated_eval_time': 2627.4473009109497, 'accumulated_logging_time': 7.944999933242798, 'global_step': 89241, 'preemption_count': 0}), (90537, {'train/accuracy': 0.7014309763908386, 'train/loss': 1.2846760749816895, 'validation/accuracy': 0.6474999785423279, 'validation/loss': 1.5295343399047852, 'validation/num_examples': 50000, 'test/accuracy': 0.5282000303268433, 'test/loss': 2.18021821975708, 'test/num_examples': 10000, 'score': 35246.08141589165, 'total_duration': 37927.631106853485, 'accumulated_submission_time': 35246.08141589165, 'accumulated_eval_time': 2663.8959975242615, 'accumulated_logging_time': 8.067214012145996, 'global_step': 90537, 'preemption_count': 0}), (91840, {'train/accuracy': 0.7085259556770325, 'train/loss': 1.2437835931777954, 'validation/accuracy': 0.6565999984741211, 'validation/loss': 1.4826949834823608, 'validation/num_examples': 50000, 'test/accuracy': 0.5263000130653381, 'test/loss': 2.1592321395874023, 'test/num_examples': 10000, 'score': 35755.80262589455, 'total_duration': 38471.47956061363, 'accumulated_submission_time': 35755.80262589455, 'accumulated_eval_time': 2697.737674474716, 'accumulated_logging_time': 8.196099519729614, 'global_step': 91840, 'preemption_count': 0}), (93106, {'train/accuracy': 0.7000358700752258, 'train/loss': 1.2826281785964966, 'validation/accuracy': 0.646340012550354, 'validation/loss': 1.5347332954406738, 'validation/num_examples': 50000, 'test/accuracy': 0.5135000348091125, 'test/loss': 2.214576244354248, 'test/num_examples': 10000, 'score': 36265.714277505875, 'total_duration': 39015.148689985275, 'accumulated_submission_time': 36265.714277505875, 'accumulated_eval_time': 2731.237590789795, 'accumulated_logging_time': 8.304457187652588, 'global_step': 93106, 'preemption_count': 0}), (94363, {'train/accuracy': 0.7093231678009033, 'train/loss': 1.2625337839126587, 'validation/accuracy': 0.656719982624054, 'validation/loss': 1.515025019645691, 'validation/num_examples': 50000, 'test/accuracy': 0.5198000073432922, 'test/loss': 2.1848230361938477, 'test/num_examples': 10000, 'score': 36775.48110985756, 'total_duration': 39560.606474876404, 'accumulated_submission_time': 36775.48110985756, 'accumulated_eval_time': 2766.6549985408783, 'accumulated_logging_time': 8.430238723754883, 'global_step': 94363, 'preemption_count': 0}), (95639, {'train/accuracy': 0.7106584906578064, 'train/loss': 1.2461391687393188, 'validation/accuracy': 0.653499960899353, 'validation/loss': 1.4984668493270874, 'validation/num_examples': 50000, 'test/accuracy': 0.526199996471405, 'test/loss': 2.1798980236053467, 'test/num_examples': 10000, 'score': 37285.27674341202, 'total_duration': 40103.7116959095, 'accumulated_submission_time': 37285.27674341202, 'accumulated_eval_time': 2799.631216287613, 'accumulated_logging_time': 8.614967107772827, 'global_step': 95639, 'preemption_count': 0}), (96901, {'train/accuracy': 0.7095025181770325, 'train/loss': 1.2492046356201172, 'validation/accuracy': 0.6546799540519714, 'validation/loss': 1.5062360763549805, 'validation/num_examples': 50000, 'test/accuracy': 0.5243000388145447, 'test/loss': 2.1919305324554443, 'test/num_examples': 10000, 'score': 37795.1165561676, 'total_duration': 40644.05375957489, 'accumulated_submission_time': 37795.1165561676, 'accumulated_eval_time': 2829.8101563453674, 'accumulated_logging_time': 8.787564992904663, 'global_step': 96901, 'preemption_count': 0}), (98196, {'train/accuracy': 0.7195471525192261, 'train/loss': 1.2031362056732178, 'validation/accuracy': 0.6633399724960327, 'validation/loss': 1.463394045829773, 'validation/num_examples': 50000, 'test/accuracy': 0.5338000059127808, 'test/loss': 2.1390933990478516, 'test/num_examples': 10000, 'score': 38304.8749256134, 'total_duration': 41187.48756098747, 'accumulated_submission_time': 38304.8749256134, 'accumulated_eval_time': 2863.2034561634064, 'accumulated_logging_time': 8.915769577026367, 'global_step': 98196, 'preemption_count': 0}), (99369, {'train/accuracy': 0.7308474183082581, 'train/loss': 1.1716368198394775, 'validation/accuracy': 0.6662200093269348, 'validation/loss': 1.4630690813064575, 'validation/num_examples': 50000, 'test/accuracy': 0.5388000011444092, 'test/loss': 2.1034204959869385, 'test/num_examples': 10000, 'score': 38815.03141593933, 'total_duration': 41732.293737888336, 'accumulated_submission_time': 38815.03141593933, 'accumulated_eval_time': 2897.5818300247192, 'accumulated_logging_time': 9.04389762878418, 'global_step': 99369, 'preemption_count': 0}), (100662, {'train/accuracy': 0.7101601958274841, 'train/loss': 1.2412019968032837, 'validation/accuracy': 0.6539199948310852, 'validation/loss': 1.508751392364502, 'validation/num_examples': 50000, 'test/accuracy': 0.5207000374794006, 'test/loss': 2.2001237869262695, 'test/num_examples': 10000, 'score': 39324.945012807846, 'total_duration': 42280.34361362457, 'accumulated_submission_time': 39324.945012807846, 'accumulated_eval_time': 2935.467300415039, 'accumulated_logging_time': 9.144246339797974, 'global_step': 100662, 'preemption_count': 0}), (101932, {'train/accuracy': 0.7271006107330322, 'train/loss': 1.1802823543548584, 'validation/accuracy': 0.664139986038208, 'validation/loss': 1.456663727760315, 'validation/num_examples': 50000, 'test/accuracy': 0.5317000150680542, 'test/loss': 2.14662766456604, 'test/num_examples': 10000, 'score': 39834.85927581787, 'total_duration': 42829.64259696007, 'accumulated_submission_time': 39834.85927581787, 'accumulated_eval_time': 2974.5691351890564, 'accumulated_logging_time': 9.28302550315857, 'global_step': 101932, 'preemption_count': 0}), (103121, {'train/accuracy': 0.7332987785339355, 'train/loss': 1.1505793333053589, 'validation/accuracy': 0.6702600121498108, 'validation/loss': 1.4429877996444702, 'validation/num_examples': 50000, 'test/accuracy': 0.5444000363349915, 'test/loss': 2.080712080001831, 'test/num_examples': 10000, 'score': 40344.64529109001, 'total_duration': 43371.83443522453, 'accumulated_submission_time': 40344.64529109001, 'accumulated_eval_time': 3006.7167494297028, 'accumulated_logging_time': 9.402681112289429, 'global_step': 103121, 'preemption_count': 0}), (104221, {'train/accuracy': 0.7423269748687744, 'train/loss': 1.1134259700775146, 'validation/accuracy': 0.6682400107383728, 'validation/loss': 1.4516679048538208, 'validation/num_examples': 50000, 'test/accuracy': 0.5444000363349915, 'test/loss': 2.122929096221924, 'test/num_examples': 10000, 'score': 40854.62970161438, 'total_duration': 43918.93988966942, 'accumulated_submission_time': 40854.62970161438, 'accumulated_eval_time': 3043.606955051422, 'accumulated_logging_time': 9.508100986480713, 'global_step': 104221, 'preemption_count': 0}), (105422, {'train/accuracy': 0.7566964030265808, 'train/loss': 1.0610973834991455, 'validation/accuracy': 0.6690199971199036, 'validation/loss': 1.450792670249939, 'validation/num_examples': 50000, 'test/accuracy': 0.5405000448226929, 'test/loss': 2.126655340194702, 'test/num_examples': 10000, 'score': 41364.64756274223, 'total_duration': 44463.8526802063, 'accumulated_submission_time': 41364.64756274223, 'accumulated_eval_time': 3078.2175245285034, 'accumulated_logging_time': 9.650720834732056, 'global_step': 105422, 'preemption_count': 0}), (106665, {'train/accuracy': 0.7568159699440002, 'train/loss': 1.0291569232940674, 'validation/accuracy': 0.6678599715232849, 'validation/loss': 1.4241039752960205, 'validation/num_examples': 50000, 'test/accuracy': 0.544700026512146, 'test/loss': 2.0712826251983643, 'test/num_examples': 10000, 'score': 41874.44925785065, 'total_duration': 45007.82282829285, 'accumulated_submission_time': 41874.44925785065, 'accumulated_eval_time': 3112.1395366191864, 'accumulated_logging_time': 9.751102924346924, 'global_step': 106665, 'preemption_count': 0}), (107904, {'train/accuracy': 0.7282366156578064, 'train/loss': 1.162413477897644, 'validation/accuracy': 0.6685199737548828, 'validation/loss': 1.4427968263626099, 'validation/num_examples': 50000, 'test/accuracy': 0.5442000031471252, 'test/loss': 2.1069281101226807, 'test/num_examples': 10000, 'score': 42384.473433971405, 'total_duration': 45553.20047688484, 'accumulated_submission_time': 42384.473433971405, 'accumulated_eval_time': 3147.2405631542206, 'accumulated_logging_time': 9.86249566078186, 'global_step': 107904, 'preemption_count': 0}), (108958, {'train/accuracy': 0.725027859210968, 'train/loss': 1.1865731477737427, 'validation/accuracy': 0.6681199669837952, 'validation/loss': 1.4408783912658691, 'validation/num_examples': 50000, 'test/accuracy': 0.537600040435791, 'test/loss': 2.1079046726226807, 'test/num_examples': 10000, 'score': 42894.51804852486, 'total_duration': 46095.765778541565, 'accumulated_submission_time': 42894.51804852486, 'accumulated_eval_time': 3179.497117996216, 'accumulated_logging_time': 10.004640579223633, 'global_step': 108958, 'preemption_count': 0}), (109941, {'train/accuracy': 0.7311065196990967, 'train/loss': 1.148652195930481, 'validation/accuracy': 0.6694200038909912, 'validation/loss': 1.426511526107788, 'validation/num_examples': 50000, 'test/accuracy': 0.5468000173568726, 'test/loss': 2.093478202819824, 'test/num_examples': 10000, 'score': 43404.46943116188, 'total_duration': 46643.05524802208, 'accumulated_submission_time': 43404.46943116188, 'accumulated_eval_time': 3216.5263097286224, 'accumulated_logging_time': 10.200197219848633, 'global_step': 109941, 'preemption_count': 0}), (111037, {'train/accuracy': 0.7291334271430969, 'train/loss': 1.175467610359192, 'validation/accuracy': 0.665399968624115, 'validation/loss': 1.4693171977996826, 'validation/num_examples': 50000, 'test/accuracy': 0.5387000441551208, 'test/loss': 2.137195348739624, 'test/num_examples': 10000, 'score': 43914.41852641106, 'total_duration': 47186.31592774391, 'accumulated_submission_time': 43914.41852641106, 'accumulated_eval_time': 3249.455461025238, 'accumulated_logging_time': 10.457334041595459, 'global_step': 111037, 'preemption_count': 0}), (112040, {'train/accuracy': 0.7534478306770325, 'train/loss': 1.070895791053772, 'validation/accuracy': 0.6733999848365784, 'validation/loss': 1.4278932809829712, 'validation/num_examples': 50000, 'test/accuracy': 0.5451000332832336, 'test/loss': 2.084998846054077, 'test/num_examples': 10000, 'score': 44424.831065654755, 'total_duration': 47730.78890681267, 'accumulated_submission_time': 44424.831065654755, 'accumulated_eval_time': 3283.28125166893, 'accumulated_logging_time': 10.579755067825317, 'global_step': 112040, 'preemption_count': 0}), (113048, {'train/accuracy': 0.7387396097183228, 'train/loss': 1.133206844329834, 'validation/accuracy': 0.6808800101280212, 'validation/loss': 1.3862591981887817, 'validation/num_examples': 50000, 'test/accuracy': 0.555400013923645, 'test/loss': 2.0340847969055176, 'test/num_examples': 10000, 'score': 44934.65943312645, 'total_duration': 48273.50072813034, 'accumulated_submission_time': 44934.65943312645, 'accumulated_eval_time': 3315.951554298401, 'accumulated_logging_time': 10.680310487747192, 'global_step': 113048, 'preemption_count': 0}), (114021, {'train/accuracy': 0.7410913705825806, 'train/loss': 1.1247379779815674, 'validation/accuracy': 0.6778799891471863, 'validation/loss': 1.4026718139648438, 'validation/num_examples': 50000, 'test/accuracy': 0.5468000173568726, 'test/loss': 2.063849925994873, 'test/num_examples': 10000, 'score': 45444.641672849655, 'total_duration': 48824.86071610451, 'accumulated_submission_time': 45444.641672849655, 'accumulated_eval_time': 3357.098758459091, 'accumulated_logging_time': 10.801420450210571, 'global_step': 114021, 'preemption_count': 0}), (114906, {'train/accuracy': 0.7554408311843872, 'train/loss': 1.054111123085022, 'validation/accuracy': 0.6805399656295776, 'validation/loss': 1.3793418407440186, 'validation/num_examples': 50000, 'test/accuracy': 0.5561000108718872, 'test/loss': 2.0117599964141846, 'test/num_examples': 10000, 'score': 45954.92693686485, 'total_duration': 49371.83388233185, 'accumulated_submission_time': 45954.92693686485, 'accumulated_eval_time': 3393.5884919166565, 'accumulated_logging_time': 10.899273157119751, 'global_step': 114906, 'preemption_count': 0}), (115798, {'train/accuracy': 0.7488042116165161, 'train/loss': 1.0620816946029663, 'validation/accuracy': 0.685699999332428, 'validation/loss': 1.3478524684906006, 'validation/num_examples': 50000, 'test/accuracy': 0.5600000023841858, 'test/loss': 1.978164792060852, 'test/num_examples': 10000, 'score': 46465.16021347046, 'total_duration': 49913.97975707054, 'accumulated_submission_time': 46465.16021347046, 'accumulated_eval_time': 3425.283721446991, 'accumulated_logging_time': 11.017818450927734, 'global_step': 115798, 'preemption_count': 0}), (116580, {'train/accuracy': 0.74613356590271, 'train/loss': 1.099212646484375, 'validation/accuracy': 0.6809999942779541, 'validation/loss': 1.3919414281845093, 'validation/num_examples': 50000, 'test/accuracy': 0.5516000390052795, 'test/loss': 2.0558104515075684, 'test/num_examples': 10000, 'score': 46975.46970319748, 'total_duration': 50456.93953156471, 'accumulated_submission_time': 46975.46970319748, 'accumulated_eval_time': 3457.695133447647, 'accumulated_logging_time': 11.170905351638794, 'global_step': 116580, 'preemption_count': 0}), (117330, {'train/accuracy': 0.7751116156578064, 'train/loss': 0.952418327331543, 'validation/accuracy': 0.6855199933052063, 'validation/loss': 1.340497612953186, 'validation/num_examples': 50000, 'test/accuracy': 0.5654000043869019, 'test/loss': 1.9599807262420654, 'test/num_examples': 10000, 'score': 47485.47335219383, 'total_duration': 50998.04479455948, 'accumulated_submission_time': 47485.47335219383, 'accumulated_eval_time': 3488.5638015270233, 'accumulated_logging_time': 11.319461584091187, 'global_step': 117330, 'preemption_count': 0}), (118141, {'train/accuracy': 0.7511559128761292, 'train/loss': 1.0472253561019897, 'validation/accuracy': 0.6897799968719482, 'validation/loss': 1.3284671306610107, 'validation/num_examples': 50000, 'test/accuracy': 0.5694000124931335, 'test/loss': 1.9590842723846436, 'test/num_examples': 10000, 'score': 47995.369913339615, 'total_duration': 51541.8345823288, 'accumulated_submission_time': 47995.369913339615, 'accumulated_eval_time': 3522.202502012253, 'accumulated_logging_time': 11.456263303756714, 'global_step': 118141, 'preemption_count': 0}), (118874, {'train/accuracy': 0.7639707922935486, 'train/loss': 1.0124475955963135, 'validation/accuracy': 0.6890599727630615, 'validation/loss': 1.3436685800552368, 'validation/num_examples': 50000, 'test/accuracy': 0.5657000541687012, 'test/loss': 1.9665162563323975, 'test/num_examples': 10000, 'score': 48506.04863142967, 'total_duration': 52082.33760142326, 'accumulated_submission_time': 48506.04863142967, 'accumulated_eval_time': 3551.8020944595337, 'accumulated_logging_time': 11.587366580963135, 'global_step': 118874, 'preemption_count': 0}), (119323, {'train/accuracy': 0.7469706535339355, 'train/loss': 1.082323431968689, 'validation/accuracy': 0.6826399564743042, 'validation/loss': 1.3713523149490356, 'validation/num_examples': 50000, 'test/accuracy': 0.5574000477790833, 'test/loss': 2.0278215408325195, 'test/num_examples': 10000, 'score': 49016.9319229126, 'total_duration': 52625.82083821297, 'accumulated_submission_time': 49016.9319229126, 'accumulated_eval_time': 3584.207018852234, 'accumulated_logging_time': 11.715851783752441, 'global_step': 119323, 'preemption_count': 0}), (120148, {'train/accuracy': 0.7698700428009033, 'train/loss': 0.9811974763870239, 'validation/accuracy': 0.6880399584770203, 'validation/loss': 1.3295722007751465, 'validation/num_examples': 50000, 'test/accuracy': 0.5639000535011292, 'test/loss': 1.9627537727355957, 'test/num_examples': 10000, 'score': 49527.23885369301, 'total_duration': 53166.45454239845, 'accumulated_submission_time': 49527.23885369301, 'accumulated_eval_time': 3614.373338699341, 'accumulated_logging_time': 11.782408237457275, 'global_step': 120148, 'preemption_count': 0}), (120962, {'train/accuracy': 0.7498205900192261, 'train/loss': 1.0659445524215698, 'validation/accuracy': 0.687999963760376, 'validation/loss': 1.3525488376617432, 'validation/num_examples': 50000, 'test/accuracy': 0.5611000061035156, 'test/loss': 2.005923271179199, 'test/num_examples': 10000, 'score': 50037.370426893234, 'total_duration': 53707.46183490753, 'accumulated_submission_time': 50037.370426893234, 'accumulated_eval_time': 3645.0828211307526, 'accumulated_logging_time': 11.857717514038086, 'global_step': 120962, 'preemption_count': 0}), (121775, {'train/accuracy': 0.7587292790412903, 'train/loss': 1.0377458333969116, 'validation/accuracy': 0.6874399781227112, 'validation/loss': 1.3576257228851318, 'validation/num_examples': 50000, 'test/accuracy': 0.5678000450134277, 'test/loss': 1.9920352697372437, 'test/num_examples': 10000, 'score': 50548.03465914726, 'total_duration': 54253.87586045265, 'accumulated_submission_time': 50548.03465914726, 'accumulated_eval_time': 3680.649604320526, 'accumulated_logging_time': 11.94773817062378, 'global_step': 121775, 'preemption_count': 0}), (122062, {'train/accuracy': 0.7622368931770325, 'train/loss': 1.0118249654769897, 'validation/accuracy': 0.6948399543762207, 'validation/loss': 1.315991997718811, 'validation/num_examples': 50000, 'test/accuracy': 0.5697000026702881, 'test/loss': 1.973698377609253, 'test/num_examples': 10000, 'score': 51062.07180595398, 'total_duration': 54804.03612494469, 'accumulated_submission_time': 51062.07180595398, 'accumulated_eval_time': 3716.6731264591217, 'accumulated_logging_time': 12.015897512435913, 'global_step': 122062, 'preemption_count': 0}), (122554, {'train/accuracy': 0.7384207248687744, 'train/loss': 1.1291208267211914, 'validation/accuracy': 0.6741200089454651, 'validation/loss': 1.4252798557281494, 'validation/num_examples': 50000, 'test/accuracy': 0.5497000217437744, 'test/loss': 2.1041948795318604, 'test/num_examples': 10000, 'score': 51573.56299877167, 'total_duration': 55350.1587536335, 'accumulated_submission_time': 51573.56299877167, 'accumulated_eval_time': 3751.17346906662, 'accumulated_logging_time': 12.092714786529541, 'global_step': 122554, 'preemption_count': 0}), (123202, {'train/accuracy': 0.7606425285339355, 'train/loss': 1.024558663368225, 'validation/accuracy': 0.689520001411438, 'validation/loss': 1.3395723104476929, 'validation/num_examples': 50000, 'test/accuracy': 0.5610000491142273, 'test/loss': 1.9768309593200684, 'test/num_examples': 10000, 'score': 52083.50978779793, 'total_duration': 55889.77313542366, 'accumulated_submission_time': 52083.50978779793, 'accumulated_eval_time': 3780.687047958374, 'accumulated_logging_time': 12.175379753112793, 'global_step': 123202, 'preemption_count': 0}), (123790, {'train/accuracy': 0.7522122263908386, 'train/loss': 1.0723891258239746, 'validation/accuracy': 0.6873399615287781, 'validation/loss': 1.355772614479065, 'validation/num_examples': 50000, 'test/accuracy': 0.5653000473976135, 'test/loss': 2.0078916549682617, 'test/num_examples': 10000, 'score': 52594.11408638954, 'total_duration': 56431.91476607323, 'accumulated_submission_time': 52594.11408638954, 'accumulated_eval_time': 3812.099485397339, 'accumulated_logging_time': 12.236084699630737, 'global_step': 123790, 'preemption_count': 0}), (124258, {'train/accuracy': 0.7708665132522583, 'train/loss': 0.9882678389549255, 'validation/accuracy': 0.6894199848175049, 'validation/loss': 1.336884617805481, 'validation/num_examples': 50000, 'test/accuracy': 0.5653000473976135, 'test/loss': 1.967854380607605, 'test/num_examples': 10000, 'score': 53105.847051143646, 'total_duration': 56972.273503780365, 'accumulated_submission_time': 53105.847051143646, 'accumulated_eval_time': 3840.572640657425, 'accumulated_logging_time': 12.336040258407593, 'global_step': 124258, 'preemption_count': 0}), (124653, {'train/accuracy': 0.7533880472183228, 'train/loss': 1.049121379852295, 'validation/accuracy': 0.6859999895095825, 'validation/loss': 1.35211980342865, 'validation/num_examples': 50000, 'test/accuracy': 0.5587000250816345, 'test/loss': 2.0085337162017822, 'test/num_examples': 10000, 'score': 53615.73741841316, 'total_duration': 57512.523903131485, 'accumulated_submission_time': 53615.73741841316, 'accumulated_eval_time': 3870.7753100395203, 'accumulated_logging_time': 12.449589729309082, 'global_step': 124653, 'preemption_count': 0}), (125165, {'train/accuracy': 0.7603236436843872, 'train/loss': 1.0306915044784546, 'validation/accuracy': 0.6949799656867981, 'validation/loss': 1.3209543228149414, 'validation/num_examples': 50000, 'test/accuracy': 0.5672000050544739, 'test/loss': 1.9698246717453003, 'test/num_examples': 10000, 'score': 54127.0356400013, 'total_duration': 58056.505707740784, 'accumulated_submission_time': 54127.0356400013, 'accumulated_eval_time': 3903.352961540222, 'accumulated_logging_time': 12.500242471694946, 'global_step': 125165, 'preemption_count': 0}), (125545, {'train/accuracy': 0.7852359414100647, 'train/loss': 0.9306259155273438, 'validation/accuracy': 0.6957199573516846, 'validation/loss': 1.3137327432632446, 'validation/num_examples': 50000, 'test/accuracy': 0.5723000168800354, 'test/loss': 1.9529039859771729, 'test/num_examples': 10000, 'score': 54637.392612457275, 'total_duration': 58599.88144183159, 'accumulated_submission_time': 54637.392612457275, 'accumulated_eval_time': 3936.2294318675995, 'accumulated_logging_time': 12.601667642593384, 'global_step': 125545, 'preemption_count': 0}), (125780, {'train/accuracy': 0.7738161683082581, 'train/loss': 0.9807087182998657, 'validation/accuracy': 0.6979799866676331, 'validation/loss': 1.3162760734558105, 'validation/num_examples': 50000, 'test/accuracy': 0.5725000500679016, 'test/loss': 1.9774309396743774, 'test/num_examples': 10000, 'score': 55149.19099855423, 'total_duration': 59145.185670137405, 'accumulated_submission_time': 55149.19099855423, 'accumulated_eval_time': 3969.6436800956726, 'accumulated_logging_time': 12.66632628440857, 'global_step': 125780, 'preemption_count': 0}), (126130, {'train/accuracy': 0.7628148794174194, 'train/loss': 1.0199297666549683, 'validation/accuracy': 0.6890000104904175, 'validation/loss': 1.3471249341964722, 'validation/num_examples': 50000, 'test/accuracy': 0.5642000436782837, 'test/loss': 2.0088818073272705, 'test/num_examples': 10000, 'score': 55660.742210149765, 'total_duration': 59693.870842933655, 'accumulated_submission_time': 55660.742210149765, 'accumulated_eval_time': 4006.7085659503937, 'accumulated_logging_time': 12.697842359542847, 'global_step': 126130, 'preemption_count': 0}), (126443, {'train/accuracy': 0.7667211294174194, 'train/loss': 0.9877922534942627, 'validation/accuracy': 0.6985999941825867, 'validation/loss': 1.2874317169189453, 'validation/num_examples': 50000, 'test/accuracy': 0.572700023651123, 'test/loss': 1.9590399265289307, 'test/num_examples': 10000, 'score': 56170.68586349487, 'total_duration': 60236.21789479256, 'accumulated_submission_time': 56170.68586349487, 'accumulated_eval_time': 4039.034947156906, 'accumulated_logging_time': 12.740835666656494, 'global_step': 126443, 'preemption_count': 0}), (127019, {'train/accuracy': 0.781668484210968, 'train/loss': 0.9370632171630859, 'validation/accuracy': 0.6987999677658081, 'validation/loss': 1.3069871664047241, 'validation/num_examples': 50000, 'test/accuracy': 0.5697000026702881, 'test/loss': 1.9576137065887451, 'test/num_examples': 10000, 'score': 56680.822405815125, 'total_duration': 60777.653245687485, 'accumulated_submission_time': 56680.822405815125, 'accumulated_eval_time': 4070.207568883896, 'accumulated_logging_time': 12.80076813697815, 'global_step': 127019, 'preemption_count': 0}), (127189, {'train/accuracy': 0.7752909660339355, 'train/loss': 0.9475078582763672, 'validation/accuracy': 0.6964399814605713, 'validation/loss': 1.3045929670333862, 'validation/num_examples': 50000, 'test/accuracy': 0.5751000046730042, 'test/loss': 1.9500045776367188, 'test/num_examples': 10000, 'score': 57191.51372337341, 'total_duration': 61316.90868020058, 'accumulated_submission_time': 57191.51372337341, 'accumulated_eval_time': 4098.65883231163, 'accumulated_logging_time': 12.894912719726562, 'global_step': 127189, 'preemption_count': 0}), (127829, {'train/accuracy': 0.7615194320678711, 'train/loss': 1.0176628828048706, 'validation/accuracy': 0.6960799694061279, 'validation/loss': 1.3095643520355225, 'validation/num_examples': 50000, 'test/accuracy': 0.5751000046730042, 'test/loss': 1.9625041484832764, 'test/num_examples': 10000, 'score': 57703.47299528122, 'total_duration': 61864.37225365639, 'accumulated_submission_time': 57703.47299528122, 'accumulated_eval_time': 4134.058648347855, 'accumulated_logging_time': 12.92718505859375, 'global_step': 127829, 'preemption_count': 0}), (127976, {'train/accuracy': 0.7653658986091614, 'train/loss': 1.011232614517212, 'validation/accuracy': 0.6974799633026123, 'validation/loss': 1.303728461265564, 'validation/num_examples': 50000, 'test/accuracy': 0.5696000456809998, 'test/loss': 1.984849452972412, 'test/num_examples': 10000, 'score': 58213.44985294342, 'total_duration': 62408.79552268982, 'accumulated_submission_time': 58213.44985294342, 'accumulated_eval_time': 4168.439317703247, 'accumulated_logging_time': 12.975699663162231, 'global_step': 127976, 'preemption_count': 0}), (128229, {'train/accuracy': 0.7700892686843872, 'train/loss': 0.9973018765449524, 'validation/accuracy': 0.698199987411499, 'validation/loss': 1.3144395351409912, 'validation/num_examples': 50000, 'test/accuracy': 0.5705000162124634, 'test/loss': 1.9554637670516968, 'test/num_examples': 10000, 'score': 58723.67125630379, 'total_duration': 62950.63692235947, 'accumulated_submission_time': 58723.67125630379, 'accumulated_eval_time': 4199.994345664978, 'accumulated_logging_time': 13.013160705566406, 'global_step': 128229, 'preemption_count': 0}), (128518, {'train/accuracy': 0.7856544852256775, 'train/loss': 0.9454336762428284, 'validation/accuracy': 0.6955199837684631, 'validation/loss': 1.3293514251708984, 'validation/num_examples': 50000, 'test/accuracy': 0.5753000378608704, 'test/loss': 1.9687272310256958, 'test/num_examples': 10000, 'score': 59233.84620428085, 'total_duration': 63492.53452038765, 'accumulated_submission_time': 59233.84620428085, 'accumulated_eval_time': 4231.651604175568, 'accumulated_logging_time': 13.046807050704956, 'global_step': 128518, 'preemption_count': 0}), (128889, {'train/accuracy': 0.7693518400192261, 'train/loss': 0.9701951146125793, 'validation/accuracy': 0.6976000070571899, 'validation/loss': 1.2956416606903076, 'validation/num_examples': 50000, 'test/accuracy': 0.5720000267028809, 'test/loss': 1.9348167181015015, 'test/num_examples': 10000, 'score': 59745.95229887962, 'total_duration': 64037.46446585655, 'accumulated_submission_time': 59745.95229887962, 'accumulated_eval_time': 4264.401487827301, 'accumulated_logging_time': 13.077680587768555, 'global_step': 128889, 'preemption_count': 0}), (129099, {'train/accuracy': 0.7744140625, 'train/loss': 0.9605424404144287, 'validation/accuracy': 0.702739953994751, 'validation/loss': 1.2833123207092285, 'validation/num_examples': 50000, 'test/accuracy': 0.5743000507354736, 'test/loss': 1.9408903121948242, 'test/num_examples': 10000, 'score': 60258.72899079323, 'total_duration': 64582.10551524162, 'accumulated_submission_time': 60258.72899079323, 'accumulated_eval_time': 4296.1916353702545, 'accumulated_logging_time': 13.13055419921875, 'global_step': 129099, 'preemption_count': 0}), (129236, {'train/accuracy': 0.7612404227256775, 'train/loss': 1.0304335355758667, 'validation/accuracy': 0.6899600028991699, 'validation/loss': 1.3441596031188965, 'validation/num_examples': 50000, 'test/accuracy': 0.5599000453948975, 'test/loss': 2.0078001022338867, 'test/num_examples': 10000, 'score': 60772.240236759186, 'total_duration': 65129.85771942139, 'accumulated_submission_time': 60772.240236759186, 'accumulated_eval_time': 4330.375913858414, 'accumulated_logging_time': 13.172591209411621, 'global_step': 129236, 'preemption_count': 0}), (129360, {'train/accuracy': 0.7726601958274841, 'train/loss': 0.9548271894454956, 'validation/accuracy': 0.700719952583313, 'validation/loss': 1.2781521081924438, 'validation/num_examples': 50000, 'test/accuracy': 0.5826000571250916, 'test/loss': 1.9059375524520874, 'test/num_examples': 10000, 'score': 61285.149243593216, 'total_duration': 65674.32224273682, 'accumulated_submission_time': 61285.149243593216, 'accumulated_eval_time': 4361.884636878967, 'accumulated_logging_time': 13.20588207244873, 'global_step': 129360, 'preemption_count': 0}), (129556, {'train/accuracy': 0.7672193646430969, 'train/loss': 1.0069751739501953, 'validation/accuracy': 0.6964799761772156, 'validation/loss': 1.3204766511917114, 'validation/num_examples': 50000, 'test/accuracy': 0.5740000009536743, 'test/loss': 1.957993507385254, 'test/num_examples': 10000, 'score': 61795.55415034294, 'total_duration': 66215.95557427406, 'accumulated_submission_time': 61795.55415034294, 'accumulated_eval_time': 4393.0603449344635, 'accumulated_logging_time': 13.238199472427368, 'global_step': 129556, 'preemption_count': 0}), (129973, {'train/accuracy': 0.7987882494926453, 'train/loss': 0.8608719706535339, 'validation/accuracy': 0.6976999640464783, 'validation/loss': 1.2853513956069946, 'validation/num_examples': 50000, 'test/accuracy': 0.5726000070571899, 'test/loss': 1.942155361175537, 'test/num_examples': 10000, 'score': 62306.12463569641, 'total_duration': 66757.98723316193, 'accumulated_submission_time': 62306.12463569641, 'accumulated_eval_time': 4424.4419713020325, 'accumulated_logging_time': 13.272434711456299, 'global_step': 129973, 'preemption_count': 0}), (130141, {'train/accuracy': 0.7819674611091614, 'train/loss': 0.9341273903846741, 'validation/accuracy': 0.6918799877166748, 'validation/loss': 1.3201701641082764, 'validation/num_examples': 50000, 'test/accuracy': 0.5683000087738037, 'test/loss': 1.9836262464523315, 'test/num_examples': 10000, 'score': 62818.319324970245, 'total_duration': 67307.24260640144, 'accumulated_submission_time': 62818.319324970245, 'accumulated_eval_time': 4461.423869848251, 'accumulated_logging_time': 13.334020376205444, 'global_step': 130141, 'preemption_count': 0}), (130369, {'train/accuracy': 0.7807716727256775, 'train/loss': 0.9305323958396912, 'validation/accuracy': 0.6992200016975403, 'validation/loss': 1.288451910018921, 'validation/num_examples': 50000, 'test/accuracy': 0.5716000199317932, 'test/loss': 1.9218629598617554, 'test/num_examples': 10000, 'score': 63328.58293747902, 'total_duration': 67848.06496286392, 'accumulated_submission_time': 63328.58293747902, 'accumulated_eval_time': 4491.924478530884, 'accumulated_logging_time': 13.367144584655762, 'global_step': 130369, 'preemption_count': 0}), (130537, {'train/accuracy': 0.7791174650192261, 'train/loss': 0.9595478177070618, 'validation/accuracy': 0.6994799971580505, 'validation/loss': 1.303695559501648, 'validation/num_examples': 50000, 'test/accuracy': 0.574400007724762, 'test/loss': 1.9433953762054443, 'test/num_examples': 10000, 'score': 63842.0419113636, 'total_duration': 68394.38256573677, 'accumulated_submission_time': 63842.0419113636, 'accumulated_eval_time': 4524.7292404174805, 'accumulated_logging_time': 13.401313781738281, 'global_step': 130537, 'preemption_count': 0}), (130661, {'train/accuracy': 0.7713049650192261, 'train/loss': 0.9815318584442139, 'validation/accuracy': 0.6945799589157104, 'validation/loss': 1.320129156112671, 'validation/num_examples': 50000, 'test/accuracy': 0.5695000290870667, 'test/loss': 1.9612842798233032, 'test/num_examples': 10000, 'score': 64354.01810669899, 'total_duration': 68938.26775503159, 'accumulated_submission_time': 64354.01810669899, 'accumulated_eval_time': 4556.5926167964935, 'accumulated_logging_time': 13.433764457702637, 'global_step': 130661, 'preemption_count': 0}), (130786, {'train/accuracy': 0.7755300998687744, 'train/loss': 0.9319085478782654, 'validation/accuracy': 0.7029399871826172, 'validation/loss': 1.2618303298950195, 'validation/num_examples': 50000, 'test/accuracy': 0.5763000249862671, 'test/loss': 1.9140115976333618, 'test/num_examples': 10000, 'score': 64866.89805006981, 'total_duration': 69485.97267341614, 'accumulated_submission_time': 64866.89805006981, 'accumulated_eval_time': 4591.373907804489, 'accumulated_logging_time': 13.464891910552979, 'global_step': 130786, 'preemption_count': 0}), (130911, {'train/accuracy': 0.7705675959587097, 'train/loss': 0.9718958735466003, 'validation/accuracy': 0.6991999745368958, 'validation/loss': 1.2852084636688232, 'validation/num_examples': 50000, 'test/accuracy': 0.5732000470161438, 'test/loss': 1.934641718864441, 'test/num_examples': 10000, 'score': 65379.43269944191, 'total_duration': 70030.68398976326, 'accumulated_submission_time': 65379.43269944191, 'accumulated_eval_time': 4623.503634691238, 'accumulated_logging_time': 13.499063491821289, 'global_step': 130911, 'preemption_count': 0}), (131155, {'train/accuracy': 0.7739756107330322, 'train/loss': 0.971443235874176, 'validation/accuracy': 0.699400007724762, 'validation/loss': 1.290891170501709, 'validation/num_examples': 50000, 'test/accuracy': 0.5782999992370605, 'test/loss': 1.9318766593933105, 'test/num_examples': 10000, 'score': 65890.97120189667, 'total_duration': 70572.89148759842, 'accumulated_submission_time': 65890.97120189667, 'accumulated_eval_time': 4654.114678621292, 'accumulated_logging_time': 13.53047513961792, 'global_step': 131155, 'preemption_count': 0})], 'global_step': 131398}
I0307 22:07:25.507308 140178276385984 submission_runner.py:649] Timing: 66402.94897890091
I0307 22:07:25.507354 140178276385984 submission_runner.py:651] Total number of evals: 130
I0307 22:07:25.507384 140178276385984 submission_runner.py:652] ====================
I0307 22:07:25.507628 140178276385984 submission_runner.py:750] Final imagenet_resnet score: 0
