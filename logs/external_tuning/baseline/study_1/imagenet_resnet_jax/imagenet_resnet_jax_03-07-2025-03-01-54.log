python submission_runner.py --framework=jax --workload=imagenet_resnet --submission_path=prize_qualification_baselines/external_tuning/jax_nadamw_full_budget.py --data_dir=/data/imagenet/jax --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=submissions/rolling_leaderboard/external_tuning/baseline/study_1 --overwrite=True --save_checkpoints=False --rng_seed=511729757 --imagenet_v2_data_dir=/data/imagenet/jax --tuning_ruleset=external --tuning_search_space=prize_qualification_baselines/external_tuning/tuning_search_space.json --num_tuning_trials=5 --hparam_start_index=3 --hparam_end_index=4 2>&1 | tee -a /logs/imagenet_resnet_jax_03-07-2025-03-01-54.log
2025-03-07 03:02:11.497103: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1741316532.025004       9 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1741316532.174601       9 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
I0307 03:02:59.640272 140050941662400 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_1/imagenet_resnet_jax.
I0307 03:03:02.599938 140050941662400 xla_bridge.py:884] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA
I0307 03:03:02.603469 140050941662400 xla_bridge.py:884] Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory
I0307 03:03:02.606113 140050941662400 submission_runner.py:606] Using RNG seed 511729757
I0307 03:03:08.603157 140050941662400 submission_runner.py:615] --- Tuning run 4/5 ---
I0307 03:03:08.603384 140050941662400 submission_runner.py:620] Creating tuning directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_1/imagenet_resnet_jax/trial_4.
I0307 03:03:08.603586 140050941662400 logger_utils.py:97] Saving hparams to /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_1/imagenet_resnet_jax/trial_4/hparams.json.
I0307 03:03:08.846808 140050941662400 submission_runner.py:218] Initializing dataset.
I0307 03:03:10.553167 140050941662400 dataset_info.py:690] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0307 03:03:10.901344 140050941662400 reader.py:261] Creating a tf.data.Dataset reading 1024 files located in folders: /data/imagenet/jax/imagenet2012/5.1.0.
I0307 03:03:11.242335 140050941662400 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split train, from /data/imagenet/jax/imagenet2012/5.1.0
I0307 03:03:13.082726 140050941662400 submission_runner.py:229] Initializing model.
I0307 03:03:36.779456 140050941662400 submission_runner.py:272] Initializing optimizer.
I0307 03:03:37.924701 140050941662400 submission_runner.py:279] Initializing metrics bundle.
I0307 03:03:37.924964 140050941662400 submission_runner.py:301] Initializing checkpoint and logger.
I0307 03:03:37.926108 140050941662400 checkpoints.py:1101] Found no checkpoint files in /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_1/imagenet_resnet_jax/trial_4 with prefix checkpoint_
I0307 03:03:37.926213 140050941662400 submission_runner.py:321] Saving meta data to /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_1/imagenet_resnet_jax/trial_4/meta_data_0.json.
I0307 03:03:38.621838 140050941662400 submission_runner.py:325] Saving flags to /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_1/imagenet_resnet_jax/trial_4/flags_0.json.
I0307 03:03:38.903200 140050941662400 submission_runner.py:337] Starting training loop.
I0307 03:04:35.872913 139914596816640 logging_writer.py:48] [0] global_step=0, grad_norm=0.6763489842414856, loss=6.932281970977783
I0307 03:04:36.171169 140050941662400 spec.py:321] Evaluating on the training split.
I0307 03:04:36.632081 140050941662400 dataset_info.py:690] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0307 03:04:36.656286 140050941662400 reader.py:261] Creating a tf.data.Dataset reading 1024 files located in folders: /data/imagenet/jax/imagenet2012/5.1.0.
I0307 03:04:36.698375 140050941662400 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split train, from /data/imagenet/jax/imagenet2012/5.1.0
I0307 03:04:55.749006 140050941662400 spec.py:333] Evaluating on the validation split.
I0307 03:04:56.305936 140050941662400 dataset_info.py:690] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0307 03:04:56.338064 140050941662400 reader.py:261] Creating a tf.data.Dataset reading 64 files located in folders: /data/imagenet/jax/imagenet2012/5.1.0.
I0307 03:04:56.512418 140050941662400 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split validation, from /data/imagenet/jax/imagenet2012/5.1.0
I0307 03:05:38.429162 140050941662400 spec.py:349] Evaluating on the test split.
I0307 03:05:38.912147 140050941662400 dataset_info.py:690] Load dataset info from /data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0
I0307 03:05:38.971862 140050941662400 reader.py:261] Creating a tf.data.Dataset reading 16 files located in folders: /data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0.
I0307 03:05:39.068557 140050941662400 logging_logger.py:49] Constructing tf.data.Dataset imagenet_v2 for split test, from /data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0
I0307 03:05:58.074061 140050941662400 submission_runner.py:469] Time since start: 139.17s, 	Step: 1, 	{'train/accuracy': 0.0012755101779475808, 'train/loss': 6.914036273956299, 'validation/accuracy': 0.001339999958872795, 'validation/loss': 6.914280891418457, 'validation/num_examples': 50000, 'test/accuracy': 0.0013000001199543476, 'test/loss': 6.913465976715088, 'test/num_examples': 10000, 'score': 57.26776361465454, 'total_duration': 139.17078804969788, 'accumulated_submission_time': 57.26776361465454, 'accumulated_eval_time': 81.9028148651123, 'accumulated_logging_time': 0}
I0307 03:05:58.101595 139894550664960 logging_writer.py:48] [1] accumulated_eval_time=81.9028, accumulated_logging_time=0, accumulated_submission_time=57.2678, global_step=1, preemption_count=0, score=57.2678, test/accuracy=0.0013, test/loss=6.91347, test/num_examples=10000, total_duration=139.171, train/accuracy=0.00127551, train/loss=6.91404, validation/accuracy=0.00134, validation/loss=6.91428, validation/num_examples=50000
I0307 03:06:34.445091 139894466803456 logging_writer.py:48] [100] global_step=100, grad_norm=0.7921770811080933, loss=6.647430419921875
I0307 03:07:10.868544 139894550664960 logging_writer.py:48] [200] global_step=200, grad_norm=0.9406744241714478, loss=6.2947821617126465
I0307 03:07:47.895873 139894466803456 logging_writer.py:48] [300] global_step=300, grad_norm=1.8690584897994995, loss=6.038278102874756
I0307 03:08:25.636070 139894550664960 logging_writer.py:48] [400] global_step=400, grad_norm=3.2987003326416016, loss=5.800247669219971
I0307 03:09:03.272838 139894466803456 logging_writer.py:48] [500] global_step=500, grad_norm=3.8682079315185547, loss=5.519508361816406
I0307 03:09:40.448041 139894550664960 logging_writer.py:48] [600] global_step=600, grad_norm=3.3264691829681396, loss=5.387824535369873
I0307 03:10:17.966927 139894466803456 logging_writer.py:48] [700] global_step=700, grad_norm=2.4872477054595947, loss=5.14946174621582
I0307 03:10:55.732415 139894550664960 logging_writer.py:48] [800] global_step=800, grad_norm=3.588956832885742, loss=5.056656360626221
I0307 03:11:33.665712 139894466803456 logging_writer.py:48] [900] global_step=900, grad_norm=2.56290864944458, loss=4.907764434814453
I0307 03:12:11.025216 139894550664960 logging_writer.py:48] [1000] global_step=1000, grad_norm=3.2912018299102783, loss=4.677592754364014
I0307 03:12:48.706712 139894466803456 logging_writer.py:48] [1100] global_step=1100, grad_norm=3.610295534133911, loss=4.572935104370117
I0307 03:13:26.371197 139894550664960 logging_writer.py:48] [1200] global_step=1200, grad_norm=3.5086677074432373, loss=4.411126613616943
I0307 03:14:03.864854 139894466803456 logging_writer.py:48] [1300] global_step=1300, grad_norm=1.8770438432693481, loss=4.140071392059326
I0307 03:14:28.145866 140050941662400 spec.py:321] Evaluating on the training split.
I0307 03:14:40.582484 140050941662400 spec.py:333] Evaluating on the validation split.
I0307 03:15:00.363480 140050941662400 spec.py:349] Evaluating on the test split.
I0307 03:15:02.277550 140050941662400 submission_runner.py:469] Time since start: 683.37s, 	Step: 1366, 	{'train/accuracy': 0.19393733143806458, 'train/loss': 4.09002161026001, 'validation/accuracy': 0.15984000265598297, 'validation/loss': 4.396219253540039, 'validation/num_examples': 50000, 'test/accuracy': 0.1251000016927719, 'test/loss': 4.906614303588867, 'test/num_examples': 10000, 'score': 567.101241350174, 'total_duration': 683.3743026256561, 'accumulated_submission_time': 567.101241350174, 'accumulated_eval_time': 116.03449773788452, 'accumulated_logging_time': 0.03821897506713867}
I0307 03:15:02.305108 139894559057664 logging_writer.py:48] [1366] accumulated_eval_time=116.034, accumulated_logging_time=0.038219, accumulated_submission_time=567.101, global_step=1366, preemption_count=0, score=567.101, test/accuracy=0.1251, test/loss=4.90661, test/num_examples=10000, total_duration=683.374, train/accuracy=0.193937, train/loss=4.09002, validation/accuracy=0.15984, validation/loss=4.39622, validation/num_examples=50000
I0307 03:15:15.583246 139894567450368 logging_writer.py:48] [1400] global_step=1400, grad_norm=3.775437593460083, loss=4.261959075927734
I0307 03:15:53.754316 139894559057664 logging_writer.py:48] [1500] global_step=1500, grad_norm=2.593994617462158, loss=4.199550151824951
I0307 03:16:30.856428 139894567450368 logging_writer.py:48] [1600] global_step=1600, grad_norm=4.306083679199219, loss=3.89699387550354
I0307 03:17:08.200527 139894559057664 logging_writer.py:48] [1700] global_step=1700, grad_norm=2.8429698944091797, loss=3.8359620571136475
I0307 03:17:45.779598 139894567450368 logging_writer.py:48] [1800] global_step=1800, grad_norm=2.0537564754486084, loss=3.8878660202026367
I0307 03:18:23.131018 139894559057664 logging_writer.py:48] [1900] global_step=1900, grad_norm=2.366311550140381, loss=3.7230358123779297
I0307 03:19:00.988949 139894567450368 logging_writer.py:48] [2000] global_step=2000, grad_norm=2.018460512161255, loss=3.527324914932251
I0307 03:19:38.978528 139894559057664 logging_writer.py:48] [2100] global_step=2100, grad_norm=1.948050856590271, loss=3.5157101154327393
I0307 03:20:16.670764 139894567450368 logging_writer.py:48] [2200] global_step=2200, grad_norm=2.1412391662597656, loss=3.5915184020996094
I0307 03:20:54.534941 139894559057664 logging_writer.py:48] [2300] global_step=2300, grad_norm=1.3226262331008911, loss=3.200000047683716
I0307 03:21:32.018479 139894567450368 logging_writer.py:48] [2400] global_step=2400, grad_norm=1.8487364053726196, loss=3.4244589805603027
I0307 03:22:09.016732 139894559057664 logging_writer.py:48] [2500] global_step=2500, grad_norm=1.544244408607483, loss=3.4118826389312744
I0307 03:22:45.403566 139894567450368 logging_writer.py:48] [2600] global_step=2600, grad_norm=2.2078042030334473, loss=3.318814754486084
I0307 03:23:23.060318 139894559057664 logging_writer.py:48] [2700] global_step=2700, grad_norm=1.7068939208984375, loss=3.29123854637146
I0307 03:23:32.335875 140050941662400 spec.py:321] Evaluating on the training split.
I0307 03:23:43.817212 140050941662400 spec.py:333] Evaluating on the validation split.
I0307 03:24:06.798081 140050941662400 spec.py:349] Evaluating on the test split.
I0307 03:24:08.604996 140050941662400 submission_runner.py:469] Time since start: 1229.70s, 	Step: 2726, 	{'train/accuracy': 0.3436702787876129, 'train/loss': 3.0623178482055664, 'validation/accuracy': 0.30357998609542847, 'validation/loss': 3.3523764610290527, 'validation/num_examples': 50000, 'test/accuracy': 0.22940000891685486, 'test/loss': 3.9628517627716064, 'test/num_examples': 10000, 'score': 1076.922331571579, 'total_duration': 1229.7017509937286, 'accumulated_submission_time': 1076.922331571579, 'accumulated_eval_time': 152.30359268188477, 'accumulated_logging_time': 0.07364583015441895}
I0307 03:24:08.637923 139894567450368 logging_writer.py:48] [2726] accumulated_eval_time=152.304, accumulated_logging_time=0.0736458, accumulated_submission_time=1076.92, global_step=2726, preemption_count=0, score=1076.92, test/accuracy=0.2294, test/loss=3.96285, test/num_examples=10000, total_duration=1229.7, train/accuracy=0.34367, train/loss=3.06232, validation/accuracy=0.30358, validation/loss=3.35238, validation/num_examples=50000
I0307 03:24:36.679106 139894559057664 logging_writer.py:48] [2800] global_step=2800, grad_norm=1.4904863834381104, loss=3.145021677017212
I0307 03:25:14.580421 139894567450368 logging_writer.py:48] [2900] global_step=2900, grad_norm=1.3028151988983154, loss=3.0736069679260254
I0307 03:25:52.709201 139894559057664 logging_writer.py:48] [3000] global_step=3000, grad_norm=1.2832415103912354, loss=3.1020405292510986
I0307 03:26:30.737095 139894567450368 logging_writer.py:48] [3100] global_step=3100, grad_norm=1.4565778970718384, loss=3.0425429344177246
I0307 03:27:08.593677 139894559057664 logging_writer.py:48] [3200] global_step=3200, grad_norm=1.7385964393615723, loss=3.0286526679992676
I0307 03:27:47.059982 139894567450368 logging_writer.py:48] [3300] global_step=3300, grad_norm=1.0887088775634766, loss=3.0730533599853516
I0307 03:28:25.221071 139894559057664 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.9435563087463379, loss=3.0871810913085938
I0307 03:29:03.306030 139894567450368 logging_writer.py:48] [3500] global_step=3500, grad_norm=1.2439404726028442, loss=3.0295917987823486
I0307 03:29:41.441288 139894559057664 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.8847976326942444, loss=2.9663422107696533
I0307 03:30:19.652957 139894567450368 logging_writer.py:48] [3700] global_step=3700, grad_norm=1.065894365310669, loss=3.108900308609009
I0307 03:30:57.517439 139894559057664 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.9945256114006042, loss=3.1543736457824707
I0307 03:31:35.681029 139894567450368 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.9644548892974854, loss=3.0361249446868896
I0307 03:32:13.859165 139894559057664 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.8358186483383179, loss=3.0488333702087402
I0307 03:32:38.977686 140050941662400 spec.py:321] Evaluating on the training split.
I0307 03:32:50.231913 140050941662400 spec.py:333] Evaluating on the validation split.
I0307 03:33:08.980435 140050941662400 spec.py:349] Evaluating on the test split.
I0307 03:33:10.838685 140050941662400 submission_runner.py:469] Time since start: 1771.94s, 	Step: 4067, 	{'train/accuracy': 0.3639189898967743, 'train/loss': 2.980862617492676, 'validation/accuracy': 0.33131998777389526, 'validation/loss': 3.2336480617523193, 'validation/num_examples': 50000, 'test/accuracy': 0.2485000044107437, 'test/loss': 4.009598731994629, 'test/num_examples': 10000, 'score': 1587.0840775966644, 'total_duration': 1771.9354231357574, 'accumulated_submission_time': 1587.0840775966644, 'accumulated_eval_time': 184.16454124450684, 'accumulated_logging_time': 0.11448264122009277}
I0307 03:33:10.869693 139894567450368 logging_writer.py:48] [4067] accumulated_eval_time=184.165, accumulated_logging_time=0.114483, accumulated_submission_time=1587.08, global_step=4067, preemption_count=0, score=1587.08, test/accuracy=0.2485, test/loss=4.0096, test/num_examples=10000, total_duration=1771.94, train/accuracy=0.363919, train/loss=2.98086, validation/accuracy=0.33132, validation/loss=3.23365, validation/num_examples=50000
I0307 03:33:23.935202 139894559057664 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.8347514867782593, loss=2.842373847961426
I0307 03:34:01.475748 139894567450368 logging_writer.py:48] [4200] global_step=4200, grad_norm=1.0370999574661255, loss=2.9377059936523438
I0307 03:34:39.213873 139894559057664 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.9503316283226013, loss=2.8743252754211426
I0307 03:35:16.809604 139894567450368 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.9965775012969971, loss=2.865525722503662
I0307 03:35:54.401292 139894559057664 logging_writer.py:48] [4500] global_step=4500, grad_norm=1.0479398965835571, loss=2.8362932205200195
I0307 03:36:32.623197 139894567450368 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.8085862398147583, loss=2.649357318878174
I0307 03:37:10.349121 139894559057664 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.9562653303146362, loss=2.8529863357543945
I0307 03:37:48.481621 139894567450368 logging_writer.py:48] [4800] global_step=4800, grad_norm=1.061912178993225, loss=2.7679758071899414
I0307 03:38:26.616614 139894559057664 logging_writer.py:48] [4900] global_step=4900, grad_norm=1.1671669483184814, loss=2.75917911529541
I0307 03:39:04.461121 139894567450368 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.8313535451889038, loss=2.6673643589019775
I0307 03:39:42.145639 139894559057664 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.8715870976448059, loss=2.6678218841552734
I0307 03:40:19.573615 139894567450368 logging_writer.py:48] [5200] global_step=5200, grad_norm=1.0861340761184692, loss=2.675926923751831
I0307 03:40:57.320451 139894559057664 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.9728308916091919, loss=2.73616623878479
I0307 03:41:35.660263 139894567450368 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.8957443833351135, loss=2.6802010536193848
I0307 03:41:40.885369 140050941662400 spec.py:321] Evaluating on the training split.
I0307 03:41:52.197283 140050941662400 spec.py:333] Evaluating on the validation split.
I0307 03:42:13.955337 140050941662400 spec.py:349] Evaluating on the test split.
I0307 03:42:15.738464 140050941662400 submission_runner.py:469] Time since start: 2316.84s, 	Step: 5415, 	{'train/accuracy': 0.35977357625961304, 'train/loss': 2.959120988845825, 'validation/accuracy': 0.33541998267173767, 'validation/loss': 3.1407690048217773, 'validation/num_examples': 50000, 'test/accuracy': 0.2524000108242035, 'test/loss': 3.831477165222168, 'test/num_examples': 10000, 'score': 2096.9187150001526, 'total_duration': 2316.8352077007294, 'accumulated_submission_time': 2096.9187150001526, 'accumulated_eval_time': 219.01759910583496, 'accumulated_logging_time': 0.1541295051574707}
I0307 03:42:15.767196 139894559057664 logging_writer.py:48] [5415] accumulated_eval_time=219.018, accumulated_logging_time=0.15413, accumulated_submission_time=2096.92, global_step=5415, preemption_count=0, score=2096.92, test/accuracy=0.2524, test/loss=3.83148, test/num_examples=10000, total_duration=2316.84, train/accuracy=0.359774, train/loss=2.95912, validation/accuracy=0.33542, validation/loss=3.14077, validation/num_examples=50000
I0307 03:42:48.710823 139894567450368 logging_writer.py:48] [5500] global_step=5500, grad_norm=1.020349383354187, loss=2.775015115737915
I0307 03:43:26.543786 139894559057664 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.9181557893753052, loss=2.6239309310913086
I0307 03:44:04.419186 139894567450368 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.8307871222496033, loss=2.7626078128814697
I0307 03:44:42.558753 139894559057664 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.798823893070221, loss=2.6761765480041504
I0307 03:45:20.598450 139894567450368 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.9068708419799805, loss=2.5537781715393066
I0307 03:45:58.305871 139894559057664 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.8111592531204224, loss=2.5138187408447266
I0307 03:46:36.072669 139894567450368 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.8657621145248413, loss=2.6365163326263428
I0307 03:47:13.924674 139894559057664 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.8315244913101196, loss=2.495579719543457
I0307 03:47:51.706233 139894567450368 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.9562776684761047, loss=2.459616184234619
I0307 03:48:29.575618 139894559057664 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.9302310943603516, loss=2.6456217765808105
I0307 03:49:07.487210 139894567450368 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.8151006102561951, loss=2.667210578918457
I0307 03:49:45.627276 139894559057664 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.9958447813987732, loss=2.422079563140869
I0307 03:50:23.298402 139894567450368 logging_writer.py:48] [6700] global_step=6700, grad_norm=1.1479763984680176, loss=2.6102375984191895
I0307 03:50:45.982797 140050941662400 spec.py:321] Evaluating on the training split.
I0307 03:50:57.420843 140050941662400 spec.py:333] Evaluating on the validation split.
I0307 03:51:18.449579 140050941662400 spec.py:349] Evaluating on the test split.
I0307 03:51:20.272737 140050941662400 submission_runner.py:469] Time since start: 2861.37s, 	Step: 6761, 	{'train/accuracy': 0.36045119166374207, 'train/loss': 3.0153629779815674, 'validation/accuracy': 0.33069998025894165, 'validation/loss': 3.2171270847320557, 'validation/num_examples': 50000, 'test/accuracy': 0.24490001797676086, 'test/loss': 3.9743778705596924, 'test/num_examples': 10000, 'score': 2606.9692907333374, 'total_duration': 2861.3694908618927, 'accumulated_submission_time': 2606.9692907333374, 'accumulated_eval_time': 253.3075098991394, 'accumulated_logging_time': 0.19097661972045898}
I0307 03:51:20.295912 139894559057664 logging_writer.py:48] [6761] accumulated_eval_time=253.308, accumulated_logging_time=0.190977, accumulated_submission_time=2606.97, global_step=6761, preemption_count=0, score=2606.97, test/accuracy=0.2449, test/loss=3.97438, test/num_examples=10000, total_duration=2861.37, train/accuracy=0.360451, train/loss=3.01536, validation/accuracy=0.3307, validation/loss=3.21713, validation/num_examples=50000
I0307 03:51:35.672450 139894567450368 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.9618680477142334, loss=2.560732364654541
I0307 03:52:13.792148 139894559057664 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.9512386918067932, loss=2.480971336364746
I0307 03:52:51.407366 139894567450368 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.948553740978241, loss=2.5300939083099365
I0307 03:53:28.588934 139894559057664 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.980897068977356, loss=2.584904193878174
I0307 03:54:06.412076 139894567450368 logging_writer.py:48] [7200] global_step=7200, grad_norm=1.2016410827636719, loss=2.559415817260742
I0307 03:54:43.810781 139894559057664 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.9751027226448059, loss=2.5323898792266846
I0307 03:55:21.428791 139894567450368 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.9068699479103088, loss=2.603942394256592
I0307 03:55:59.027103 139894559057664 logging_writer.py:48] [7500] global_step=7500, grad_norm=1.11686372756958, loss=2.483196496963501
I0307 03:56:36.942991 139894567450368 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.8890681266784668, loss=2.5591697692871094
I0307 03:57:15.090174 139894559057664 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.9635341167449951, loss=2.749823570251465
I0307 03:57:52.999509 139894567450368 logging_writer.py:48] [7800] global_step=7800, grad_norm=1.095629334449768, loss=2.52781343460083
I0307 03:58:30.827238 139894559057664 logging_writer.py:48] [7900] global_step=7900, grad_norm=1.0842998027801514, loss=2.4830641746520996
I0307 03:59:08.648642 139894567450368 logging_writer.py:48] [8000] global_step=8000, grad_norm=1.0643609762191772, loss=2.5874078273773193
I0307 03:59:46.444386 139894559057664 logging_writer.py:48] [8100] global_step=8100, grad_norm=1.0267008543014526, loss=2.551234006881714
I0307 03:59:50.349736 140050941662400 spec.py:321] Evaluating on the training split.
I0307 04:00:01.545858 140050941662400 spec.py:333] Evaluating on the validation split.
I0307 04:00:26.267118 140050941662400 spec.py:349] Evaluating on the test split.
I0307 04:00:28.055595 140050941662400 submission_runner.py:469] Time since start: 3409.15s, 	Step: 8111, 	{'train/accuracy': 0.2926100194454193, 'train/loss': 3.4803214073181152, 'validation/accuracy': 0.264739990234375, 'validation/loss': 3.670687437057495, 'validation/num_examples': 50000, 'test/accuracy': 0.20230001211166382, 'test/loss': 4.283381938934326, 'test/num_examples': 10000, 'score': 3116.845325946808, 'total_duration': 3409.1523463726044, 'accumulated_submission_time': 3116.845325946808, 'accumulated_eval_time': 291.01332998275757, 'accumulated_logging_time': 0.2355666160583496}
I0307 04:00:28.084419 139894567450368 logging_writer.py:48] [8111] accumulated_eval_time=291.013, accumulated_logging_time=0.235567, accumulated_submission_time=3116.85, global_step=8111, preemption_count=0, score=3116.85, test/accuracy=0.2023, test/loss=4.28338, test/num_examples=10000, total_duration=3409.15, train/accuracy=0.29261, train/loss=3.48032, validation/accuracy=0.26474, validation/loss=3.67069, validation/num_examples=50000
I0307 04:01:02.264595 139894559057664 logging_writer.py:48] [8200] global_step=8200, grad_norm=1.0624794960021973, loss=2.4873576164245605
I0307 04:01:39.859549 139894567450368 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.8702369332313538, loss=2.4864535331726074
I0307 04:02:17.444017 139894559057664 logging_writer.py:48] [8400] global_step=8400, grad_norm=1.0602753162384033, loss=2.6140084266662598
I0307 04:02:55.730961 139894567450368 logging_writer.py:48] [8500] global_step=8500, grad_norm=1.3420307636260986, loss=2.596942663192749
I0307 04:03:33.970137 139894559057664 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.9065752625465393, loss=2.518449068069458
I0307 04:04:11.915609 139894567450368 logging_writer.py:48] [8700] global_step=8700, grad_norm=1.0244197845458984, loss=2.3882155418395996
I0307 04:04:49.681026 139894559057664 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.930496871471405, loss=2.420903444290161
I0307 04:05:27.472868 139894567450368 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.988411009311676, loss=2.509584426879883
I0307 04:06:05.163081 139894559057664 logging_writer.py:48] [9000] global_step=9000, grad_norm=1.1164886951446533, loss=2.539048194885254
I0307 04:06:43.296526 139894567450368 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.9377300143241882, loss=2.4184799194335938
I0307 04:07:20.799193 139894559057664 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.9250001311302185, loss=2.4451661109924316
I0307 04:07:59.074900 139894567450368 logging_writer.py:48] [9300] global_step=9300, grad_norm=1.2002867460250854, loss=2.387315511703491
I0307 04:08:36.641272 139894559057664 logging_writer.py:48] [9400] global_step=9400, grad_norm=1.0200846195220947, loss=2.567840576171875
I0307 04:08:58.245083 140050941662400 spec.py:321] Evaluating on the training split.
I0307 04:09:09.368637 140050941662400 spec.py:333] Evaluating on the validation split.
I0307 04:09:32.541226 140050941662400 spec.py:349] Evaluating on the test split.
I0307 04:09:34.354404 140050941662400 submission_runner.py:469] Time since start: 3955.45s, 	Step: 9458, 	{'train/accuracy': 0.4260004758834839, 'train/loss': 2.5971479415893555, 'validation/accuracy': 0.39563998579978943, 'validation/loss': 2.8102974891662598, 'validation/num_examples': 50000, 'test/accuracy': 0.30170002579689026, 'test/loss': 3.5090537071228027, 'test/num_examples': 10000, 'score': 3626.829142808914, 'total_duration': 3955.451154947281, 'accumulated_submission_time': 3626.829142808914, 'accumulated_eval_time': 327.12261295318604, 'accumulated_logging_time': 0.2720615863800049}
I0307 04:09:34.392987 139894567450368 logging_writer.py:48] [9458] accumulated_eval_time=327.123, accumulated_logging_time=0.272062, accumulated_submission_time=3626.83, global_step=9458, preemption_count=0, score=3626.83, test/accuracy=0.3017, test/loss=3.50905, test/num_examples=10000, total_duration=3955.45, train/accuracy=0.426, train/loss=2.59715, validation/accuracy=0.39564, validation/loss=2.8103, validation/num_examples=50000
I0307 04:09:50.628661 139894559057664 logging_writer.py:48] [9500] global_step=9500, grad_norm=1.1332919597625732, loss=2.473090648651123
I0307 04:10:28.734358 139894567450368 logging_writer.py:48] [9600] global_step=9600, grad_norm=1.1415975093841553, loss=2.4690451622009277
I0307 04:11:07.016066 139894559057664 logging_writer.py:48] [9700] global_step=9700, grad_norm=1.1070674657821655, loss=2.444582939147949
I0307 04:11:45.706940 139894567450368 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.9821980595588684, loss=2.521512508392334
I0307 04:12:24.044666 139894559057664 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.9841157793998718, loss=2.4924046993255615
I0307 04:13:02.532749 139894567450368 logging_writer.py:48] [10000] global_step=10000, grad_norm=1.0179779529571533, loss=2.4906232357025146
I0307 04:13:39.748309 139894559057664 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.9717625379562378, loss=2.3961679935455322
I0307 04:14:17.017720 139894567450368 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.9737105965614319, loss=2.479417562484741
I0307 04:14:53.957665 139894559057664 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.9817509651184082, loss=2.4876933097839355
I0307 04:15:31.058847 139894567450368 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.9304100275039673, loss=2.573199987411499
I0307 04:16:07.881281 139894559057664 logging_writer.py:48] [10500] global_step=10500, grad_norm=1.1516053676605225, loss=2.5508899688720703
I0307 04:16:45.495685 139894567450368 logging_writer.py:48] [10600] global_step=10600, grad_norm=1.145656943321228, loss=2.3349015712738037
I0307 04:17:24.112103 139894559057664 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.9853513836860657, loss=2.3664777278900146
I0307 04:18:02.312648 139894567450368 logging_writer.py:48] [10800] global_step=10800, grad_norm=1.159298300743103, loss=2.570425510406494
I0307 04:18:04.632697 140050941662400 spec.py:321] Evaluating on the training split.
I0307 04:18:17.048464 140050941662400 spec.py:333] Evaluating on the validation split.
I0307 04:18:38.256240 140050941662400 spec.py:349] Evaluating on the test split.
I0307 04:18:40.059128 140050941662400 submission_runner.py:469] Time since start: 4501.16s, 	Step: 10807, 	{'train/accuracy': 0.29348692297935486, 'train/loss': 3.6059610843658447, 'validation/accuracy': 0.2662999927997589, 'validation/loss': 3.812204360961914, 'validation/num_examples': 50000, 'test/accuracy': 0.1859000027179718, 'test/loss': 4.66912841796875, 'test/num_examples': 10000, 'score': 4136.896089792252, 'total_duration': 4501.155874967575, 'accumulated_submission_time': 4136.896089792252, 'accumulated_eval_time': 362.54900217056274, 'accumulated_logging_time': 0.3199045658111572}
I0307 04:18:40.154928 139894559057664 logging_writer.py:48] [10807] accumulated_eval_time=362.549, accumulated_logging_time=0.319905, accumulated_submission_time=4136.9, global_step=10807, preemption_count=0, score=4136.9, test/accuracy=0.1859, test/loss=4.66913, test/num_examples=10000, total_duration=4501.16, train/accuracy=0.293487, train/loss=3.60596, validation/accuracy=0.2663, validation/loss=3.8122, validation/num_examples=50000
I0307 04:19:15.995769 139894567450368 logging_writer.py:48] [10900] global_step=10900, grad_norm=1.0536913871765137, loss=2.433712959289551
I0307 04:19:54.116434 139894559057664 logging_writer.py:48] [11000] global_step=11000, grad_norm=1.1241456270217896, loss=2.4040229320526123
I0307 04:20:32.514060 139894567450368 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.8777346014976501, loss=2.395653486251831
I0307 04:21:10.960039 139894559057664 logging_writer.py:48] [11200] global_step=11200, grad_norm=1.093430995941162, loss=2.3132193088531494
I0307 04:21:49.275435 139894567450368 logging_writer.py:48] [11300] global_step=11300, grad_norm=1.2390598058700562, loss=2.364769458770752
I0307 04:22:27.631168 139894559057664 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.9965201020240784, loss=2.5711212158203125
I0307 04:23:05.719868 139894567450368 logging_writer.py:48] [11500] global_step=11500, grad_norm=1.0686746835708618, loss=2.467886447906494
I0307 04:23:44.197592 139894559057664 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.981048047542572, loss=2.3704583644866943
I0307 04:24:22.624226 139894567450368 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.8812399506568909, loss=2.4431662559509277
I0307 04:25:00.742260 139894559057664 logging_writer.py:48] [11800] global_step=11800, grad_norm=1.0622515678405762, loss=2.4496898651123047
I0307 04:25:39.165158 139894567450368 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.9870331883430481, loss=2.3590736389160156
I0307 04:26:17.628857 139894559057664 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.9899832606315613, loss=2.4297432899475098
I0307 04:26:55.798368 139894567450368 logging_writer.py:48] [12100] global_step=12100, grad_norm=0.9687281847000122, loss=2.474677085876465
I0307 04:27:10.224791 140050941662400 spec.py:321] Evaluating on the training split.
I0307 04:27:21.134022 140050941662400 spec.py:333] Evaluating on the validation split.
I0307 04:27:42.815040 140050941662400 spec.py:349] Evaluating on the test split.
I0307 04:27:44.654525 140050941662400 submission_runner.py:469] Time since start: 5045.75s, 	Step: 12139, 	{'train/accuracy': 0.27130499482154846, 'train/loss': 3.673388719558716, 'validation/accuracy': 0.2638799846172333, 'validation/loss': 3.7236242294311523, 'validation/num_examples': 50000, 'test/accuracy': 0.1868000030517578, 'test/loss': 4.45974063873291, 'test/num_examples': 10000, 'score': 4646.833619832993, 'total_duration': 5045.751278400421, 'accumulated_submission_time': 4646.833619832993, 'accumulated_eval_time': 396.9786994457245, 'accumulated_logging_time': 0.42337989807128906}
I0307 04:27:44.687037 139894559057664 logging_writer.py:48] [12139] accumulated_eval_time=396.979, accumulated_logging_time=0.42338, accumulated_submission_time=4646.83, global_step=12139, preemption_count=0, score=4646.83, test/accuracy=0.1868, test/loss=4.45974, test/num_examples=10000, total_duration=5045.75, train/accuracy=0.271305, train/loss=3.67339, validation/accuracy=0.26388, validation/loss=3.72362, validation/num_examples=50000
I0307 04:28:08.524112 139894567450368 logging_writer.py:48] [12200] global_step=12200, grad_norm=0.9734624028205872, loss=2.4951837062835693
I0307 04:28:47.070214 139894559057664 logging_writer.py:48] [12300] global_step=12300, grad_norm=1.1187440156936646, loss=2.4408740997314453
I0307 04:29:25.195300 139894567450368 logging_writer.py:48] [12400] global_step=12400, grad_norm=1.1044684648513794, loss=2.487591505050659
I0307 04:30:03.498722 139894559057664 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.9831269979476929, loss=2.4793057441711426
I0307 04:30:41.605647 139894567450368 logging_writer.py:48] [12600] global_step=12600, grad_norm=1.0509965419769287, loss=2.5041897296905518
I0307 04:31:19.952198 139894559057664 logging_writer.py:48] [12700] global_step=12700, grad_norm=0.958448052406311, loss=2.398344039916992
I0307 04:31:58.070293 139894567450368 logging_writer.py:48] [12800] global_step=12800, grad_norm=0.9672693014144897, loss=2.326082468032837
I0307 04:32:36.102729 139894559057664 logging_writer.py:48] [12900] global_step=12900, grad_norm=0.9918724894523621, loss=2.380021572113037
I0307 04:33:14.204471 139894567450368 logging_writer.py:48] [13000] global_step=13000, grad_norm=1.1363558769226074, loss=2.495687484741211
I0307 04:33:52.377074 139894559057664 logging_writer.py:48] [13100] global_step=13100, grad_norm=1.0274368524551392, loss=2.5004987716674805
I0307 04:34:30.390137 139894567450368 logging_writer.py:48] [13200] global_step=13200, grad_norm=1.0107921361923218, loss=2.4175448417663574
I0307 04:35:08.808643 139894559057664 logging_writer.py:48] [13300] global_step=13300, grad_norm=0.9723122119903564, loss=2.353586435317993
I0307 04:35:47.050929 139894567450368 logging_writer.py:48] [13400] global_step=13400, grad_norm=1.2282124757766724, loss=2.414170265197754
I0307 04:36:14.791396 140050941662400 spec.py:321] Evaluating on the training split.
I0307 04:36:27.410002 140050941662400 spec.py:333] Evaluating on the validation split.
I0307 04:36:51.058269 140050941662400 spec.py:349] Evaluating on the test split.
I0307 04:36:52.885064 140050941662400 submission_runner.py:469] Time since start: 5593.98s, 	Step: 13474, 	{'train/accuracy': 0.3152901828289032, 'train/loss': 3.298755407333374, 'validation/accuracy': 0.2900799810886383, 'validation/loss': 3.493664503097534, 'validation/num_examples': 50000, 'test/accuracy': 0.2142000049352646, 'test/loss': 4.238362789154053, 'test/num_examples': 10000, 'score': 5156.796553850174, 'total_duration': 5593.981817483902, 'accumulated_submission_time': 5156.796553850174, 'accumulated_eval_time': 435.0723292827606, 'accumulated_logging_time': 0.46392059326171875}
I0307 04:36:52.936442 139894559057664 logging_writer.py:48] [13474] accumulated_eval_time=435.072, accumulated_logging_time=0.463921, accumulated_submission_time=5156.8, global_step=13474, preemption_count=0, score=5156.8, test/accuracy=0.2142, test/loss=4.23836, test/num_examples=10000, total_duration=5593.98, train/accuracy=0.31529, train/loss=3.29876, validation/accuracy=0.29008, validation/loss=3.49366, validation/num_examples=50000
I0307 04:37:03.398065 139894567450368 logging_writer.py:48] [13500] global_step=13500, grad_norm=1.1368533372879028, loss=2.424908399581909
I0307 04:37:41.750705 139894559057664 logging_writer.py:48] [13600] global_step=13600, grad_norm=1.0545169115066528, loss=2.3709113597869873
I0307 04:38:19.741163 139894567450368 logging_writer.py:48] [13700] global_step=13700, grad_norm=1.0353646278381348, loss=2.3782739639282227
I0307 04:38:58.471954 139894559057664 logging_writer.py:48] [13800] global_step=13800, grad_norm=0.9897748827934265, loss=2.474616765975952
I0307 04:39:36.702248 139894567450368 logging_writer.py:48] [13900] global_step=13900, grad_norm=1.026857614517212, loss=2.3165581226348877
I0307 04:40:14.960069 139894559057664 logging_writer.py:48] [14000] global_step=14000, grad_norm=1.0428963899612427, loss=2.4130239486694336
I0307 04:40:53.196042 139894567450368 logging_writer.py:48] [14100] global_step=14100, grad_norm=1.1471134424209595, loss=2.3968546390533447
I0307 04:41:31.261363 139894559057664 logging_writer.py:48] [14200] global_step=14200, grad_norm=0.9496210217475891, loss=2.3129067420959473
I0307 04:42:09.441512 139894567450368 logging_writer.py:48] [14300] global_step=14300, grad_norm=1.007496953010559, loss=2.3167295455932617
I0307 04:42:47.421712 139894559057664 logging_writer.py:48] [14400] global_step=14400, grad_norm=1.011608362197876, loss=2.277566909790039
I0307 04:43:25.453435 139894567450368 logging_writer.py:48] [14500] global_step=14500, grad_norm=1.14246666431427, loss=2.3931045532226562
I0307 04:44:06.131294 139894559057664 logging_writer.py:48] [14600] global_step=14600, grad_norm=1.0143146514892578, loss=2.4154303073883057
I0307 04:44:44.566094 139894567450368 logging_writer.py:48] [14700] global_step=14700, grad_norm=1.0385069847106934, loss=2.4352235794067383
I0307 04:45:22.606665 139894559057664 logging_writer.py:48] [14800] global_step=14800, grad_norm=0.9889237880706787, loss=2.3400630950927734
I0307 04:45:23.005450 140050941662400 spec.py:321] Evaluating on the training split.
I0307 04:45:39.029386 140050941662400 spec.py:333] Evaluating on the validation split.
I0307 04:46:00.860184 140050941662400 spec.py:349] Evaluating on the test split.
I0307 04:46:02.685063 140050941662400 submission_runner.py:469] Time since start: 6143.78s, 	Step: 14802, 	{'train/accuracy': 0.11832349747419357, 'train/loss': 5.744869232177734, 'validation/accuracy': 0.10957999527454376, 'validation/loss': 5.901554107666016, 'validation/num_examples': 50000, 'test/accuracy': 0.08180000633001328, 'test/loss': 6.412503242492676, 'test/num_examples': 10000, 'score': 5666.712646484375, 'total_duration': 6143.781819105148, 'accumulated_submission_time': 5666.712646484375, 'accumulated_eval_time': 474.75190806388855, 'accumulated_logging_time': 0.5230445861816406}
I0307 04:46:02.738379 139894567450368 logging_writer.py:48] [14802] accumulated_eval_time=474.752, accumulated_logging_time=0.523045, accumulated_submission_time=5666.71, global_step=14802, preemption_count=0, score=5666.71, test/accuracy=0.0818, test/loss=6.4125, test/num_examples=10000, total_duration=6143.78, train/accuracy=0.118323, train/loss=5.74487, validation/accuracy=0.10958, validation/loss=5.90155, validation/num_examples=50000
I0307 04:46:40.540374 139894559057664 logging_writer.py:48] [14900] global_step=14900, grad_norm=1.0016837120056152, loss=2.3366570472717285
I0307 04:47:18.843165 139894567450368 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.9821677207946777, loss=2.432093858718872
I0307 04:47:57.103666 139894559057664 logging_writer.py:48] [15100] global_step=15100, grad_norm=1.0266263484954834, loss=2.3783843517303467
I0307 04:48:35.194911 139894567450368 logging_writer.py:48] [15200] global_step=15200, grad_norm=1.0147806406021118, loss=2.3708157539367676
I0307 04:49:13.334006 139894559057664 logging_writer.py:48] [15300] global_step=15300, grad_norm=1.0308105945587158, loss=2.5253331661224365
I0307 04:49:51.640809 139894567450368 logging_writer.py:48] [15400] global_step=15400, grad_norm=1.0346704721450806, loss=2.3166093826293945
I0307 04:50:30.112280 139894559057664 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.9851934313774109, loss=2.407503128051758
I0307 04:51:08.502171 139894567450368 logging_writer.py:48] [15600] global_step=15600, grad_norm=1.0659898519515991, loss=2.3375320434570312
I0307 04:51:47.137063 139894559057664 logging_writer.py:48] [15700] global_step=15700, grad_norm=0.9961195588111877, loss=2.450528621673584
I0307 04:52:25.422956 139894567450368 logging_writer.py:48] [15800] global_step=15800, grad_norm=1.0749989748001099, loss=2.3760485649108887
I0307 04:53:03.663124 139894559057664 logging_writer.py:48] [15900] global_step=15900, grad_norm=1.086265206336975, loss=2.290060043334961
I0307 04:53:41.722041 139894567450368 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.9793040752410889, loss=2.336364507675171
I0307 04:54:20.104474 139894559057664 logging_writer.py:48] [16100] global_step=16100, grad_norm=1.1209479570388794, loss=2.2875027656555176
I0307 04:54:32.714546 140050941662400 spec.py:321] Evaluating on the training split.
I0307 04:54:52.392097 140050941662400 spec.py:333] Evaluating on the validation split.
I0307 04:55:16.356384 140050941662400 spec.py:349] Evaluating on the test split.
I0307 04:55:18.173389 140050941662400 submission_runner.py:469] Time since start: 6699.27s, 	Step: 16134, 	{'train/accuracy': 0.3648357689380646, 'train/loss': 2.9762790203094482, 'validation/accuracy': 0.3432199954986572, 'validation/loss': 3.141003131866455, 'validation/num_examples': 50000, 'test/accuracy': 0.24480001628398895, 'test/loss': 3.940446376800537, 'test/num_examples': 10000, 'score': 6176.533273935318, 'total_duration': 6699.270140171051, 'accumulated_submission_time': 6176.533273935318, 'accumulated_eval_time': 520.2107214927673, 'accumulated_logging_time': 0.5842971801757812}
I0307 04:55:18.301683 139894567450368 logging_writer.py:48] [16134] accumulated_eval_time=520.211, accumulated_logging_time=0.584297, accumulated_submission_time=6176.53, global_step=16134, preemption_count=0, score=6176.53, test/accuracy=0.2448, test/loss=3.94045, test/num_examples=10000, total_duration=6699.27, train/accuracy=0.364836, train/loss=2.97628, validation/accuracy=0.34322, validation/loss=3.141, validation/num_examples=50000
I0307 04:55:44.136885 139894559057664 logging_writer.py:48] [16200] global_step=16200, grad_norm=0.9941810965538025, loss=2.2879343032836914
I0307 04:56:22.044237 139894567450368 logging_writer.py:48] [16300] global_step=16300, grad_norm=1.2519818544387817, loss=2.421863555908203
I0307 04:57:00.447751 139894559057664 logging_writer.py:48] [16400] global_step=16400, grad_norm=0.9947088956832886, loss=2.438753843307495
I0307 04:57:38.970489 139894567450368 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.9744309186935425, loss=2.3369195461273193
I0307 04:58:17.349229 139894559057664 logging_writer.py:48] [16600] global_step=16600, grad_norm=0.9919343590736389, loss=2.2516088485717773
I0307 04:58:55.661821 139894567450368 logging_writer.py:48] [16700] global_step=16700, grad_norm=1.0804466009140015, loss=2.3976738452911377
I0307 04:59:33.982339 139894559057664 logging_writer.py:48] [16800] global_step=16800, grad_norm=0.9881694316864014, loss=2.5635976791381836
I0307 05:00:12.398837 139894567450368 logging_writer.py:48] [16900] global_step=16900, grad_norm=0.9934679865837097, loss=2.2312889099121094
I0307 05:00:50.766917 139894559057664 logging_writer.py:48] [17000] global_step=17000, grad_norm=1.1368578672409058, loss=2.340808391571045
I0307 05:01:29.004215 139894567450368 logging_writer.py:48] [17100] global_step=17100, grad_norm=0.9579364657402039, loss=2.3704676628112793
I0307 05:02:07.093783 139894559057664 logging_writer.py:48] [17200] global_step=17200, grad_norm=1.0687921047210693, loss=2.375948429107666
I0307 05:02:45.115774 139894567450368 logging_writer.py:48] [17300] global_step=17300, grad_norm=1.0104262828826904, loss=2.487370252609253
I0307 05:03:23.171725 139894559057664 logging_writer.py:48] [17400] global_step=17400, grad_norm=1.1000206470489502, loss=2.3277835845947266
I0307 05:03:48.453820 140050941662400 spec.py:321] Evaluating on the training split.
I0307 05:04:03.792496 140050941662400 spec.py:333] Evaluating on the validation split.
I0307 05:04:27.454771 140050941662400 spec.py:349] Evaluating on the test split.
I0307 05:04:29.259861 140050941662400 submission_runner.py:469] Time since start: 7250.36s, 	Step: 17467, 	{'train/accuracy': 0.25554049015045166, 'train/loss': 3.8944926261901855, 'validation/accuracy': 0.23291999101638794, 'validation/loss': 4.114696502685547, 'validation/num_examples': 50000, 'test/accuracy': 0.17550000548362732, 'test/loss': 4.802067279815674, 'test/num_examples': 10000, 'score': 6686.518086671829, 'total_duration': 7250.356608629227, 'accumulated_submission_time': 6686.518086671829, 'accumulated_eval_time': 561.0167219638824, 'accumulated_logging_time': 0.7289402484893799}
I0307 05:04:29.369433 139894567450368 logging_writer.py:48] [17467] accumulated_eval_time=561.017, accumulated_logging_time=0.72894, accumulated_submission_time=6686.52, global_step=17467, preemption_count=0, score=6686.52, test/accuracy=0.1755, test/loss=4.80207, test/num_examples=10000, total_duration=7250.36, train/accuracy=0.25554, train/loss=3.89449, validation/accuracy=0.23292, validation/loss=4.1147, validation/num_examples=50000
I0307 05:04:42.436212 139894559057664 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.9021374583244324, loss=2.2782583236694336
I0307 05:05:20.835432 139894567450368 logging_writer.py:48] [17600] global_step=17600, grad_norm=1.0325536727905273, loss=2.310776472091675
I0307 05:05:58.752008 139894559057664 logging_writer.py:48] [17700] global_step=17700, grad_norm=0.9869093298912048, loss=2.3240582942962646
I0307 05:06:36.695543 139894567450368 logging_writer.py:48] [17800] global_step=17800, grad_norm=1.003347396850586, loss=2.4336981773376465
I0307 05:07:15.047078 139894559057664 logging_writer.py:48] [17900] global_step=17900, grad_norm=0.9818551540374756, loss=2.453449249267578
I0307 05:07:53.191076 139894567450368 logging_writer.py:48] [18000] global_step=18000, grad_norm=1.2138092517852783, loss=2.4702775478363037
I0307 05:08:31.799559 139894559057664 logging_writer.py:48] [18100] global_step=18100, grad_norm=0.9910742044448853, loss=2.307461977005005
I0307 05:09:10.274852 139894567450368 logging_writer.py:48] [18200] global_step=18200, grad_norm=0.9561779499053955, loss=2.3272507190704346
I0307 05:09:48.709084 139894559057664 logging_writer.py:48] [18300] global_step=18300, grad_norm=1.067020058631897, loss=2.3474292755126953
I0307 05:10:26.887518 139894567450368 logging_writer.py:48] [18400] global_step=18400, grad_norm=1.1067512035369873, loss=2.456993818283081
I0307 05:11:04.861095 139894559057664 logging_writer.py:48] [18500] global_step=18500, grad_norm=1.1587063074111938, loss=2.449699878692627
I0307 05:11:43.129851 139894567450368 logging_writer.py:48] [18600] global_step=18600, grad_norm=1.0497461557388306, loss=2.351961851119995
I0307 05:12:21.471091 139894559057664 logging_writer.py:48] [18700] global_step=18700, grad_norm=1.239113450050354, loss=2.5286788940429688
I0307 05:12:59.609160 140050941662400 spec.py:321] Evaluating on the training split.
I0307 05:13:18.456268 140050941662400 spec.py:333] Evaluating on the validation split.
I0307 05:13:40.778698 140050941662400 spec.py:349] Evaluating on the test split.
I0307 05:13:42.619000 140050941662400 submission_runner.py:469] Time since start: 7803.72s, 	Step: 18800, 	{'train/accuracy': 0.3449457883834839, 'train/loss': 3.1866557598114014, 'validation/accuracy': 0.32194000482559204, 'validation/loss': 3.358506441116333, 'validation/num_examples': 50000, 'test/accuracy': 0.24070000648498535, 'test/loss': 4.1043314933776855, 'test/num_examples': 10000, 'score': 7196.599380970001, 'total_duration': 7803.715749025345, 'accumulated_submission_time': 7196.599380970001, 'accumulated_eval_time': 604.0265378952026, 'accumulated_logging_time': 0.8469357490539551}
I0307 05:13:42.675536 139894567450368 logging_writer.py:48] [18800] accumulated_eval_time=604.027, accumulated_logging_time=0.846936, accumulated_submission_time=7196.6, global_step=18800, preemption_count=0, score=7196.6, test/accuracy=0.2407, test/loss=4.10433, test/num_examples=10000, total_duration=7803.72, train/accuracy=0.344946, train/loss=3.18666, validation/accuracy=0.32194, validation/loss=3.35851, validation/num_examples=50000
I0307 05:13:43.032345 139894559057664 logging_writer.py:48] [18800] global_step=18800, grad_norm=1.1868407726287842, loss=2.3575363159179688
I0307 05:14:21.262951 139894567450368 logging_writer.py:48] [18900] global_step=18900, grad_norm=1.0060033798217773, loss=2.3510494232177734
I0307 05:14:59.430909 139894559057664 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.9713447690010071, loss=2.434864044189453
I0307 05:15:37.673764 139894567450368 logging_writer.py:48] [19100] global_step=19100, grad_norm=1.043377161026001, loss=2.2896854877471924
I0307 05:16:16.025861 139894559057664 logging_writer.py:48] [19200] global_step=19200, grad_norm=1.0052310228347778, loss=2.40228271484375
I0307 05:16:54.401480 139894567450368 logging_writer.py:48] [19300] global_step=19300, grad_norm=1.2245049476623535, loss=2.248971462249756
I0307 05:17:32.762467 139894559057664 logging_writer.py:48] [19400] global_step=19400, grad_norm=0.9693904519081116, loss=2.3519983291625977
I0307 05:18:10.786432 139894567450368 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.9643229246139526, loss=2.29967999458313
I0307 05:18:48.951325 139894559057664 logging_writer.py:48] [19600] global_step=19600, grad_norm=0.9772276282310486, loss=2.291325807571411
I0307 05:19:27.673112 139894567450368 logging_writer.py:48] [19700] global_step=19700, grad_norm=1.0223513841629028, loss=2.1296560764312744
I0307 05:20:05.633500 139894559057664 logging_writer.py:48] [19800] global_step=19800, grad_norm=1.0064029693603516, loss=2.222182512283325
I0307 05:20:43.271876 139894567450368 logging_writer.py:48] [19900] global_step=19900, grad_norm=1.1198288202285767, loss=2.413891315460205
I0307 05:21:21.380723 139894559057664 logging_writer.py:48] [20000] global_step=20000, grad_norm=1.1840931177139282, loss=2.289578437805176
I0307 05:21:59.984930 139894567450368 logging_writer.py:48] [20100] global_step=20100, grad_norm=0.986721396446228, loss=2.3206164836883545
I0307 05:22:12.773389 140050941662400 spec.py:321] Evaluating on the training split.
I0307 05:22:28.002227 140050941662400 spec.py:333] Evaluating on the validation split.
I0307 05:22:50.033573 140050941662400 spec.py:349] Evaluating on the test split.
I0307 05:22:51.827293 140050941662400 submission_runner.py:469] Time since start: 8352.92s, 	Step: 20135, 	{'train/accuracy': 0.19192442297935486, 'train/loss': 4.728091716766357, 'validation/accuracy': 0.17817999422550201, 'validation/loss': 4.861440658569336, 'validation/num_examples': 50000, 'test/accuracy': 0.13180001080036163, 'test/loss': 5.658026695251465, 'test/num_examples': 10000, 'score': 7706.53710603714, 'total_duration': 8352.924048423767, 'accumulated_submission_time': 7706.53710603714, 'accumulated_eval_time': 643.0804052352905, 'accumulated_logging_time': 0.9112529754638672}
I0307 05:22:51.887280 139894559057664 logging_writer.py:48] [20135] accumulated_eval_time=643.08, accumulated_logging_time=0.911253, accumulated_submission_time=7706.54, global_step=20135, preemption_count=0, score=7706.54, test/accuracy=0.1318, test/loss=5.65803, test/num_examples=10000, total_duration=8352.92, train/accuracy=0.191924, train/loss=4.72809, validation/accuracy=0.17818, validation/loss=4.86144, validation/num_examples=50000
I0307 05:23:17.336645 139894567450368 logging_writer.py:48] [20200] global_step=20200, grad_norm=1.0091191530227661, loss=2.3094992637634277
I0307 05:23:55.682187 139894559057664 logging_writer.py:48] [20300] global_step=20300, grad_norm=1.0632983446121216, loss=2.3715293407440186
I0307 05:24:34.309445 139894567450368 logging_writer.py:48] [20400] global_step=20400, grad_norm=0.9826139211654663, loss=2.2259445190429688
I0307 05:25:12.972383 139894559057664 logging_writer.py:48] [20500] global_step=20500, grad_norm=1.0591182708740234, loss=2.322232484817505
I0307 05:25:51.322567 139894567450368 logging_writer.py:48] [20600] global_step=20600, grad_norm=1.1253230571746826, loss=2.394385576248169
I0307 05:26:29.290741 139894559057664 logging_writer.py:48] [20700] global_step=20700, grad_norm=1.07526695728302, loss=2.3182427883148193
I0307 05:27:07.557451 139894567450368 logging_writer.py:48] [20800] global_step=20800, grad_norm=1.0155553817749023, loss=2.2563681602478027
I0307 05:27:45.857127 139894559057664 logging_writer.py:48] [20900] global_step=20900, grad_norm=1.1833575963974, loss=2.3157107830047607
I0307 05:28:23.942492 139894567450368 logging_writer.py:48] [21000] global_step=21000, grad_norm=1.0777108669281006, loss=2.250072717666626
I0307 05:29:02.410488 139894559057664 logging_writer.py:48] [21100] global_step=21100, grad_norm=0.998018205165863, loss=2.3284876346588135
I0307 05:29:40.690510 139894567450368 logging_writer.py:48] [21200] global_step=21200, grad_norm=1.1335780620574951, loss=2.360318660736084
I0307 05:30:19.544968 139894559057664 logging_writer.py:48] [21300] global_step=21300, grad_norm=1.0608265399932861, loss=2.2237699031829834
I0307 05:30:57.622372 139894567450368 logging_writer.py:48] [21400] global_step=21400, grad_norm=1.0967159271240234, loss=2.3789355754852295
I0307 05:31:21.831759 140050941662400 spec.py:321] Evaluating on the training split.
I0307 05:31:34.381947 140050941662400 spec.py:333] Evaluating on the validation split.
I0307 05:31:58.531452 140050941662400 spec.py:349] Evaluating on the test split.
I0307 05:32:00.365719 140050941662400 submission_runner.py:469] Time since start: 8901.45s, 	Step: 21464, 	{'train/accuracy': 0.32599249482154846, 'train/loss': 3.305140495300293, 'validation/accuracy': 0.30480000376701355, 'validation/loss': 3.441506862640381, 'validation/num_examples': 50000, 'test/accuracy': 0.21940000355243683, 'test/loss': 4.304385185241699, 'test/num_examples': 10000, 'score': 8216.33454298973, 'total_duration': 8901.447535037994, 'accumulated_submission_time': 8216.33454298973, 'accumulated_eval_time': 681.5993916988373, 'accumulated_logging_time': 0.9790341854095459}
I0307 05:32:00.457081 139894559057664 logging_writer.py:48] [21464] accumulated_eval_time=681.599, accumulated_logging_time=0.979034, accumulated_submission_time=8216.33, global_step=21464, preemption_count=0, score=8216.33, test/accuracy=0.2194, test/loss=4.30439, test/num_examples=10000, total_duration=8901.45, train/accuracy=0.325992, train/loss=3.30514, validation/accuracy=0.3048, validation/loss=3.44151, validation/num_examples=50000
I0307 05:32:14.670719 139894567450368 logging_writer.py:48] [21500] global_step=21500, grad_norm=1.0618557929992676, loss=2.2941231727600098
I0307 05:32:53.681742 139894559057664 logging_writer.py:48] [21600] global_step=21600, grad_norm=1.0076738595962524, loss=2.3271901607513428
I0307 05:33:32.223903 139894567450368 logging_writer.py:48] [21700] global_step=21700, grad_norm=1.0374152660369873, loss=2.4064574241638184
I0307 05:34:11.348460 139894559057664 logging_writer.py:48] [21800] global_step=21800, grad_norm=1.112290859222412, loss=2.332034111022949
I0307 05:34:49.562756 139894567450368 logging_writer.py:48] [21900] global_step=21900, grad_norm=0.9520704746246338, loss=2.2806429862976074
I0307 05:35:28.122986 139894559057664 logging_writer.py:48] [22000] global_step=22000, grad_norm=1.0633329153060913, loss=2.321305751800537
I0307 05:36:06.584065 139894567450368 logging_writer.py:48] [22100] global_step=22100, grad_norm=1.1178553104400635, loss=2.206465244293213
I0307 05:36:45.003450 139894559057664 logging_writer.py:48] [22200] global_step=22200, grad_norm=1.0838061571121216, loss=2.3255083560943604
I0307 05:37:23.239834 139894567450368 logging_writer.py:48] [22300] global_step=22300, grad_norm=0.9756140112876892, loss=2.302400588989258
I0307 05:38:01.565800 139894559057664 logging_writer.py:48] [22400] global_step=22400, grad_norm=1.0685147047042847, loss=2.2855539321899414
I0307 05:38:40.111622 139894567450368 logging_writer.py:48] [22500] global_step=22500, grad_norm=1.073805809020996, loss=2.2372426986694336
I0307 05:39:18.512482 139894559057664 logging_writer.py:48] [22600] global_step=22600, grad_norm=1.109052062034607, loss=2.304201126098633
I0307 05:39:57.037460 139894567450368 logging_writer.py:48] [22700] global_step=22700, grad_norm=1.5465580224990845, loss=2.324345111846924
I0307 05:40:30.583067 140050941662400 spec.py:321] Evaluating on the training split.
I0307 05:40:47.543267 140050941662400 spec.py:333] Evaluating on the validation split.
I0307 05:41:11.463144 140050941662400 spec.py:349] Evaluating on the test split.
I0307 05:41:13.448060 140050941662400 submission_runner.py:469] Time since start: 9454.54s, 	Step: 22788, 	{'train/accuracy': 0.2582111060619354, 'train/loss': 4.174224853515625, 'validation/accuracy': 0.2453799992799759, 'validation/loss': 4.2462334632873535, 'validation/num_examples': 50000, 'test/accuracy': 0.18440000712871552, 'test/loss': 5.009335517883301, 'test/num_examples': 10000, 'score': 8726.244793891907, 'total_duration': 9454.54481267929, 'accumulated_submission_time': 8726.244793891907, 'accumulated_eval_time': 724.4643535614014, 'accumulated_logging_time': 1.149904489517212}
I0307 05:41:13.494215 139894559057664 logging_writer.py:48] [22788] accumulated_eval_time=724.464, accumulated_logging_time=1.1499, accumulated_submission_time=8726.24, global_step=22788, preemption_count=0, score=8726.24, test/accuracy=0.1844, test/loss=5.00934, test/num_examples=10000, total_duration=9454.54, train/accuracy=0.258211, train/loss=4.17422, validation/accuracy=0.24538, validation/loss=4.24623, validation/num_examples=50000
I0307 05:41:18.527932 139894567450368 logging_writer.py:48] [22800] global_step=22800, grad_norm=1.1023285388946533, loss=2.3393752574920654
I0307 05:41:57.135022 139894559057664 logging_writer.py:48] [22900] global_step=22900, grad_norm=0.9862951636314392, loss=2.3850791454315186
I0307 05:42:35.408571 139894567450368 logging_writer.py:48] [23000] global_step=23000, grad_norm=1.0445432662963867, loss=2.3543272018432617
I0307 05:43:13.741314 139894559057664 logging_writer.py:48] [23100] global_step=23100, grad_norm=1.015389323234558, loss=2.400479555130005
I0307 05:43:52.413611 139894567450368 logging_writer.py:48] [23200] global_step=23200, grad_norm=1.0214289426803589, loss=2.2865898609161377
I0307 05:44:30.912227 139894559057664 logging_writer.py:48] [23300] global_step=23300, grad_norm=1.1694014072418213, loss=2.349949598312378
I0307 05:45:08.760762 139894567450368 logging_writer.py:48] [23400] global_step=23400, grad_norm=1.0163363218307495, loss=2.259878396987915
I0307 05:45:47.336924 139894559057664 logging_writer.py:48] [23500] global_step=23500, grad_norm=1.072477102279663, loss=2.2317795753479004
I0307 05:46:25.663418 139894567450368 logging_writer.py:48] [23600] global_step=23600, grad_norm=1.085263729095459, loss=2.297187566757202
I0307 05:47:04.092729 139894559057664 logging_writer.py:48] [23700] global_step=23700, grad_norm=0.9958827495574951, loss=2.340632677078247
I0307 05:47:42.856801 139894567450368 logging_writer.py:48] [23800] global_step=23800, grad_norm=1.1519311666488647, loss=2.3815579414367676
I0307 05:48:21.570728 139894559057664 logging_writer.py:48] [23900] global_step=23900, grad_norm=1.1489001512527466, loss=2.2440521717071533
I0307 05:49:00.050845 139894567450368 logging_writer.py:48] [24000] global_step=24000, grad_norm=1.0214076042175293, loss=2.4405710697174072
I0307 05:49:38.757583 139894559057664 logging_writer.py:48] [24100] global_step=24100, grad_norm=1.081100344657898, loss=2.339733123779297
I0307 05:49:43.469710 140050941662400 spec.py:321] Evaluating on the training split.
I0307 05:50:01.220722 140050941662400 spec.py:333] Evaluating on the validation split.
I0307 05:50:23.480795 140050941662400 spec.py:349] Evaluating on the test split.
I0307 05:50:25.276990 140050941662400 submission_runner.py:469] Time since start: 10006.37s, 	Step: 24113, 	{'train/accuracy': 0.1472417116165161, 'train/loss': 6.062775135040283, 'validation/accuracy': 0.13793998956680298, 'validation/loss': 6.144471645355225, 'validation/num_examples': 50000, 'test/accuracy': 0.10030000656843185, 'test/loss': 6.940281391143799, 'test/num_examples': 10000, 'score': 9236.071083068848, 'total_duration': 10006.373598098755, 'accumulated_submission_time': 9236.071083068848, 'accumulated_eval_time': 766.2714502811432, 'accumulated_logging_time': 1.204176425933838}
I0307 05:50:25.378293 139894567450368 logging_writer.py:48] [24113] accumulated_eval_time=766.271, accumulated_logging_time=1.20418, accumulated_submission_time=9236.07, global_step=24113, preemption_count=0, score=9236.07, test/accuracy=0.1003, test/loss=6.94028, test/num_examples=10000, total_duration=10006.4, train/accuracy=0.147242, train/loss=6.06278, validation/accuracy=0.13794, validation/loss=6.14447, validation/num_examples=50000
I0307 05:50:59.105211 139894559057664 logging_writer.py:48] [24200] global_step=24200, grad_norm=1.0848844051361084, loss=2.180169105529785
I0307 05:51:37.392727 139894567450368 logging_writer.py:48] [24300] global_step=24300, grad_norm=1.021553874015808, loss=2.2834606170654297
I0307 05:52:15.492778 139894559057664 logging_writer.py:48] [24400] global_step=24400, grad_norm=1.1114170551300049, loss=2.35555100440979
I0307 05:52:53.437970 139894567450368 logging_writer.py:48] [24500] global_step=24500, grad_norm=1.0174673795700073, loss=2.244267463684082
I0307 05:53:31.441097 139894559057664 logging_writer.py:48] [24600] global_step=24600, grad_norm=1.2878257036209106, loss=2.2327113151550293
I0307 05:54:09.686035 139894567450368 logging_writer.py:48] [24700] global_step=24700, grad_norm=1.0391496419906616, loss=2.349119186401367
I0307 05:54:47.809737 139894559057664 logging_writer.py:48] [24800] global_step=24800, grad_norm=0.9992663264274597, loss=2.263849973678589
I0307 05:55:25.887566 139894567450368 logging_writer.py:48] [24900] global_step=24900, grad_norm=1.1874887943267822, loss=2.284330368041992
I0307 05:56:04.428506 139894559057664 logging_writer.py:48] [25000] global_step=25000, grad_norm=1.053929328918457, loss=2.3259220123291016
I0307 05:56:43.363555 139894567450368 logging_writer.py:48] [25100] global_step=25100, grad_norm=1.0671448707580566, loss=2.2333106994628906
I0307 05:57:21.517622 139894559057664 logging_writer.py:48] [25200] global_step=25200, grad_norm=1.1776015758514404, loss=2.3217782974243164
I0307 05:57:59.694219 139894567450368 logging_writer.py:48] [25300] global_step=25300, grad_norm=1.1768426895141602, loss=2.257056474685669
I0307 05:58:38.121469 139894559057664 logging_writer.py:48] [25400] global_step=25400, grad_norm=1.2535923719406128, loss=2.30475115776062
I0307 05:58:55.605174 140050941662400 spec.py:321] Evaluating on the training split.
I0307 05:59:10.927371 140050941662400 spec.py:333] Evaluating on the validation split.
I0307 05:59:33.410086 140050941662400 spec.py:349] Evaluating on the test split.
I0307 05:59:35.176961 140050941662400 submission_runner.py:469] Time since start: 10556.27s, 	Step: 25446, 	{'train/accuracy': 0.15433673560619354, 'train/loss': 5.109786510467529, 'validation/accuracy': 0.14671999216079712, 'validation/loss': 5.193364143371582, 'validation/num_examples': 50000, 'test/accuracy': 0.10590000450611115, 'test/loss': 6.017796516418457, 'test/num_examples': 10000, 'score': 9746.119248628616, 'total_duration': 10556.273713827133, 'accumulated_submission_time': 9746.119248628616, 'accumulated_eval_time': 805.8432006835938, 'accumulated_logging_time': 1.3381574153900146}
I0307 05:59:35.253709 139894567450368 logging_writer.py:48] [25446] accumulated_eval_time=805.843, accumulated_logging_time=1.33816, accumulated_submission_time=9746.12, global_step=25446, preemption_count=0, score=9746.12, test/accuracy=0.1059, test/loss=6.0178, test/num_examples=10000, total_duration=10556.3, train/accuracy=0.154337, train/loss=5.10979, validation/accuracy=0.14672, validation/loss=5.19336, validation/num_examples=50000
I0307 05:59:56.431988 139894559057664 logging_writer.py:48] [25500] global_step=25500, grad_norm=1.270888328552246, loss=2.4555864334106445
I0307 06:00:34.601071 139894567450368 logging_writer.py:48] [25600] global_step=25600, grad_norm=1.0278123617172241, loss=2.280322790145874
I0307 06:01:13.070646 139894559057664 logging_writer.py:48] [25700] global_step=25700, grad_norm=1.027296543121338, loss=2.4033522605895996
I0307 06:01:51.354532 139894567450368 logging_writer.py:48] [25800] global_step=25800, grad_norm=0.9440353512763977, loss=2.3187966346740723
I0307 06:02:29.343075 139894559057664 logging_writer.py:48] [25900] global_step=25900, grad_norm=1.0571002960205078, loss=2.2557384967803955
I0307 06:03:07.631555 139894567450368 logging_writer.py:48] [26000] global_step=26000, grad_norm=1.120927333831787, loss=2.2898917198181152
I0307 06:03:46.596710 139894559057664 logging_writer.py:48] [26100] global_step=26100, grad_norm=1.0657671689987183, loss=2.275235652923584
I0307 06:04:24.829146 139894567450368 logging_writer.py:48] [26200] global_step=26200, grad_norm=1.1578354835510254, loss=2.3586416244506836
I0307 06:05:04.321231 139894559057664 logging_writer.py:48] [26300] global_step=26300, grad_norm=1.1946958303451538, loss=2.280917167663574
I0307 06:05:42.901377 139894567450368 logging_writer.py:48] [26400] global_step=26400, grad_norm=1.023742437362671, loss=2.27669620513916
I0307 06:06:21.565580 139894559057664 logging_writer.py:48] [26500] global_step=26500, grad_norm=1.164751648902893, loss=2.4225316047668457
I0307 06:07:00.177016 139894567450368 logging_writer.py:48] [26600] global_step=26600, grad_norm=1.0946475267410278, loss=2.361246109008789
I0307 06:07:38.862202 139894559057664 logging_writer.py:48] [26700] global_step=26700, grad_norm=1.087630271911621, loss=2.4450082778930664
I0307 06:08:05.371352 140050941662400 spec.py:321] Evaluating on the training split.
I0307 06:08:21.297569 140050941662400 spec.py:333] Evaluating on the validation split.
I0307 06:08:48.745191 140050941662400 spec.py:349] Evaluating on the test split.
I0307 06:08:50.532897 140050941662400 submission_runner.py:469] Time since start: 11111.61s, 	Step: 26770, 	{'train/accuracy': 0.25420519709587097, 'train/loss': 3.8675661087036133, 'validation/accuracy': 0.23813998699188232, 'validation/loss': 3.992392063140869, 'validation/num_examples': 50000, 'test/accuracy': 0.178600013256073, 'test/loss': 4.676533222198486, 'test/num_examples': 10000, 'score': 10256.0816552639, 'total_duration': 11111.612708330154, 'accumulated_submission_time': 10256.0816552639, 'accumulated_eval_time': 850.9877662658691, 'accumulated_logging_time': 1.4228200912475586}
I0307 06:08:50.566155 139894567450368 logging_writer.py:48] [26770] accumulated_eval_time=850.988, accumulated_logging_time=1.42282, accumulated_submission_time=10256.1, global_step=26770, preemption_count=0, score=10256.1, test/accuracy=0.1786, test/loss=4.67653, test/num_examples=10000, total_duration=11111.6, train/accuracy=0.254205, train/loss=3.86757, validation/accuracy=0.23814, validation/loss=3.99239, validation/num_examples=50000
I0307 06:09:02.615280 139894559057664 logging_writer.py:48] [26800] global_step=26800, grad_norm=1.027451515197754, loss=2.4067795276641846
I0307 06:09:40.938650 139894567450368 logging_writer.py:48] [26900] global_step=26900, grad_norm=1.188254714012146, loss=2.2582921981811523
I0307 06:10:19.725604 139894559057664 logging_writer.py:48] [27000] global_step=27000, grad_norm=1.2917207479476929, loss=2.2720956802368164
I0307 06:10:57.796931 139894567450368 logging_writer.py:48] [27100] global_step=27100, grad_norm=1.0987958908081055, loss=2.4159059524536133
I0307 06:11:36.351223 139894559057664 logging_writer.py:48] [27200] global_step=27200, grad_norm=1.0603229999542236, loss=2.3357925415039062
I0307 06:12:14.581386 139894567450368 logging_writer.py:48] [27300] global_step=27300, grad_norm=1.1065967082977295, loss=2.2871193885803223
I0307 06:12:53.392262 139894559057664 logging_writer.py:48] [27400] global_step=27400, grad_norm=1.0897001028060913, loss=2.295900583267212
I0307 06:13:32.238223 139894567450368 logging_writer.py:48] [27500] global_step=27500, grad_norm=1.1842069625854492, loss=2.2767677307128906
I0307 06:14:10.628427 139894559057664 logging_writer.py:48] [27600] global_step=27600, grad_norm=1.0890685319900513, loss=2.1899144649505615
I0307 06:14:49.049799 139894567450368 logging_writer.py:48] [27700] global_step=27700, grad_norm=1.0711244344711304, loss=2.3814303874969482
I0307 06:15:27.814842 139894559057664 logging_writer.py:48] [27800] global_step=27800, grad_norm=1.150638461112976, loss=2.3206965923309326
I0307 06:16:06.037840 139894567450368 logging_writer.py:48] [27900] global_step=27900, grad_norm=1.2300403118133545, loss=2.1521265506744385
I0307 06:16:44.043581 139894559057664 logging_writer.py:48] [28000] global_step=28000, grad_norm=1.013938069343567, loss=2.2770984172821045
I0307 06:17:20.874837 140050941662400 spec.py:321] Evaluating on the training split.
I0307 06:17:35.919955 140050941662400 spec.py:333] Evaluating on the validation split.
I0307 06:18:00.010204 140050941662400 spec.py:349] Evaluating on the test split.
I0307 06:18:01.804806 140050941662400 submission_runner.py:469] Time since start: 11662.90s, 	Step: 28097, 	{'train/accuracy': 0.10090481489896774, 'train/loss': 6.6337690353393555, 'validation/accuracy': 0.09415999799966812, 'validation/loss': 6.824917793273926, 'validation/num_examples': 50000, 'test/accuracy': 0.06680000573396683, 'test/loss': 7.5064167976379395, 'test/num_examples': 10000, 'score': 10766.232750177383, 'total_duration': 11662.901554107666, 'accumulated_submission_time': 10766.232750177383, 'accumulated_eval_time': 891.9176917076111, 'accumulated_logging_time': 1.4651060104370117}
I0307 06:18:01.859217 139894567450368 logging_writer.py:48] [28097] accumulated_eval_time=891.918, accumulated_logging_time=1.46511, accumulated_submission_time=10766.2, global_step=28097, preemption_count=0, score=10766.2, test/accuracy=0.0668, test/loss=7.50642, test/num_examples=10000, total_duration=11662.9, train/accuracy=0.100905, train/loss=6.63377, validation/accuracy=0.09416, validation/loss=6.82492, validation/num_examples=50000
I0307 06:18:03.437578 139894559057664 logging_writer.py:48] [28100] global_step=28100, grad_norm=1.3120472431182861, loss=2.525622844696045
I0307 06:18:41.565083 139894567450368 logging_writer.py:48] [28200] global_step=28200, grad_norm=1.105129361152649, loss=2.4174623489379883
I0307 06:19:19.729853 139894559057664 logging_writer.py:48] [28300] global_step=28300, grad_norm=1.0724289417266846, loss=2.0989573001861572
I0307 06:19:57.980407 139894567450368 logging_writer.py:48] [28400] global_step=28400, grad_norm=1.2023390531539917, loss=2.2939510345458984
I0307 06:20:35.963107 139894559057664 logging_writer.py:48] [28500] global_step=28500, grad_norm=1.1126307249069214, loss=2.3152194023132324
I0307 06:21:14.463387 139894567450368 logging_writer.py:48] [28600] global_step=28600, grad_norm=1.0908890962600708, loss=2.225691556930542
I0307 06:21:52.695064 139894559057664 logging_writer.py:48] [28700] global_step=28700, grad_norm=1.1370049715042114, loss=2.4046711921691895
I0307 06:22:31.458198 139894567450368 logging_writer.py:48] [28800] global_step=28800, grad_norm=1.2052258253097534, loss=2.248777389526367
I0307 06:23:10.085067 139894559057664 logging_writer.py:48] [28900] global_step=28900, grad_norm=1.0448247194290161, loss=2.400325298309326
I0307 06:23:48.411932 139894567450368 logging_writer.py:48] [29000] global_step=29000, grad_norm=1.1846024990081787, loss=2.314352035522461
I0307 06:24:26.983307 139894559057664 logging_writer.py:48] [29100] global_step=29100, grad_norm=0.9934895634651184, loss=2.191324234008789
I0307 06:25:05.014995 139894567450368 logging_writer.py:48] [29200] global_step=29200, grad_norm=1.2097113132476807, loss=2.4455738067626953
I0307 06:25:43.040308 139894559057664 logging_writer.py:48] [29300] global_step=29300, grad_norm=1.0809811353683472, loss=2.2873663902282715
I0307 06:26:21.491608 139894567450368 logging_writer.py:48] [29400] global_step=29400, grad_norm=1.1742315292358398, loss=2.263305187225342
I0307 06:26:31.835907 140050941662400 spec.py:321] Evaluating on the training split.
I0307 06:26:46.029981 140050941662400 spec.py:333] Evaluating on the validation split.
I0307 06:27:09.455635 140050941662400 spec.py:349] Evaluating on the test split.
I0307 06:27:11.251477 140050941662400 submission_runner.py:469] Time since start: 12212.35s, 	Step: 29428, 	{'train/accuracy': 0.25207269191741943, 'train/loss': 3.8476932048797607, 'validation/accuracy': 0.23503999412059784, 'validation/loss': 3.985804319381714, 'validation/num_examples': 50000, 'test/accuracy': 0.1706000119447708, 'test/loss': 4.736830234527588, 'test/num_examples': 10000, 'score': 11276.032188892365, 'total_duration': 12212.34822511673, 'accumulated_submission_time': 11276.032188892365, 'accumulated_eval_time': 931.333226442337, 'accumulated_logging_time': 1.5485587120056152}
I0307 06:27:11.309164 139894559057664 logging_writer.py:48] [29428] accumulated_eval_time=931.333, accumulated_logging_time=1.54856, accumulated_submission_time=11276, global_step=29428, preemption_count=0, score=11276, test/accuracy=0.1706, test/loss=4.73683, test/num_examples=10000, total_duration=12212.3, train/accuracy=0.252073, train/loss=3.84769, validation/accuracy=0.23504, validation/loss=3.9858, validation/num_examples=50000
I0307 06:27:39.468867 139894567450368 logging_writer.py:48] [29500] global_step=29500, grad_norm=1.0387192964553833, loss=2.300950050354004
I0307 06:28:17.799043 139894559057664 logging_writer.py:48] [29600] global_step=29600, grad_norm=1.1591875553131104, loss=2.3150033950805664
I0307 06:28:56.426371 139894567450368 logging_writer.py:48] [29700] global_step=29700, grad_norm=1.0881677865982056, loss=2.248018264770508
I0307 06:29:34.971835 139894559057664 logging_writer.py:48] [29800] global_step=29800, grad_norm=0.9732418060302734, loss=2.3112545013427734
I0307 06:30:13.291920 139894567450368 logging_writer.py:48] [29900] global_step=29900, grad_norm=1.1354420185089111, loss=2.3775668144226074
I0307 06:30:51.853899 139894559057664 logging_writer.py:48] [30000] global_step=30000, grad_norm=1.1018224954605103, loss=2.2979657649993896
I0307 06:31:30.946513 139894567450368 logging_writer.py:48] [30100] global_step=30100, grad_norm=1.1624870300292969, loss=2.230795383453369
I0307 06:32:09.166065 139894559057664 logging_writer.py:48] [30200] global_step=30200, grad_norm=1.0915377140045166, loss=2.290933609008789
I0307 06:32:47.521028 139894567450368 logging_writer.py:48] [30300] global_step=30300, grad_norm=1.257317304611206, loss=2.300886631011963
I0307 06:33:26.367576 139894559057664 logging_writer.py:48] [30400] global_step=30400, grad_norm=1.1833187341690063, loss=2.193342447280884
I0307 06:34:04.592307 139894567450368 logging_writer.py:48] [30500] global_step=30500, grad_norm=1.0865100622177124, loss=2.250744104385376
I0307 06:34:42.998187 139894559057664 logging_writer.py:48] [30600] global_step=30600, grad_norm=0.9807301163673401, loss=2.285818099975586
I0307 06:35:21.415556 139894567450368 logging_writer.py:48] [30700] global_step=30700, grad_norm=1.0881962776184082, loss=2.305708885192871
I0307 06:35:41.372982 140050941662400 spec.py:321] Evaluating on the training split.
I0307 06:35:56.606708 140050941662400 spec.py:333] Evaluating on the validation split.
I0307 06:36:18.809426 140050941662400 spec.py:349] Evaluating on the test split.
I0307 06:36:20.598850 140050941662400 submission_runner.py:469] Time since start: 12761.70s, 	Step: 30753, 	{'train/accuracy': 0.25310903787612915, 'train/loss': 3.8119935989379883, 'validation/accuracy': 0.23891998827457428, 'validation/loss': 3.9384870529174805, 'validation/num_examples': 50000, 'test/accuracy': 0.17190000414848328, 'test/loss': 4.64061164855957, 'test/num_examples': 10000, 'score': 11785.914880037308, 'total_duration': 12761.69545340538, 'accumulated_submission_time': 11785.914880037308, 'accumulated_eval_time': 970.5589113235474, 'accumulated_logging_time': 1.6435976028442383}
I0307 06:36:20.637780 139894559057664 logging_writer.py:48] [30753] accumulated_eval_time=970.559, accumulated_logging_time=1.6436, accumulated_submission_time=11785.9, global_step=30753, preemption_count=0, score=11785.9, test/accuracy=0.1719, test/loss=4.64061, test/num_examples=10000, total_duration=12761.7, train/accuracy=0.253109, train/loss=3.81199, validation/accuracy=0.23892, validation/loss=3.93849, validation/num_examples=50000
I0307 06:36:39.186141 139894567450368 logging_writer.py:48] [30800] global_step=30800, grad_norm=1.2472007274627686, loss=2.4061474800109863
I0307 06:37:18.154444 139894559057664 logging_writer.py:48] [30900] global_step=30900, grad_norm=1.366829514503479, loss=2.2070629596710205
I0307 06:37:56.154173 139894567450368 logging_writer.py:48] [31000] global_step=31000, grad_norm=1.0128166675567627, loss=2.2935945987701416
I0307 06:38:34.710301 139894559057664 logging_writer.py:48] [31100] global_step=31100, grad_norm=1.192713975906372, loss=2.4028568267822266
I0307 06:39:12.987066 139894567450368 logging_writer.py:48] [31200] global_step=31200, grad_norm=1.1860792636871338, loss=2.35536789894104
I0307 06:39:51.698904 139894559057664 logging_writer.py:48] [31300] global_step=31300, grad_norm=1.1363029479980469, loss=2.338332176208496
I0307 06:40:29.668882 139894567450368 logging_writer.py:48] [31400] global_step=31400, grad_norm=1.1114330291748047, loss=2.3855907917022705
I0307 06:41:08.005654 139894559057664 logging_writer.py:48] [31500] global_step=31500, grad_norm=1.1580554246902466, loss=2.282078504562378
I0307 06:41:46.388231 139894567450368 logging_writer.py:48] [31600] global_step=31600, grad_norm=1.059538722038269, loss=2.2233333587646484
I0307 06:42:24.824053 139894559057664 logging_writer.py:48] [31700] global_step=31700, grad_norm=1.2087912559509277, loss=2.310245990753174
I0307 06:43:02.965550 139894567450368 logging_writer.py:48] [31800] global_step=31800, grad_norm=0.9925183057785034, loss=2.3408203125
I0307 06:43:41.550457 139894559057664 logging_writer.py:48] [31900] global_step=31900, grad_norm=0.9797525405883789, loss=2.181745767593384
I0307 06:44:20.221727 139894567450368 logging_writer.py:48] [32000] global_step=32000, grad_norm=1.1519887447357178, loss=2.2260353565216064
I0307 06:44:50.616474 140050941662400 spec.py:321] Evaluating on the training split.
I0307 06:45:08.578148 140050941662400 spec.py:333] Evaluating on the validation split.
I0307 06:45:28.993183 140050941662400 spec.py:349] Evaluating on the test split.
I0307 06:45:30.772226 140050941662400 submission_runner.py:469] Time since start: 13311.87s, 	Step: 32079, 	{'train/accuracy': 0.28916212916374207, 'train/loss': 3.596898317337036, 'validation/accuracy': 0.2730799913406372, 'validation/loss': 3.7396838665008545, 'validation/num_examples': 50000, 'test/accuracy': 0.20500001311302185, 'test/loss': 4.513715744018555, 'test/num_examples': 10000, 'score': 12295.721064329147, 'total_duration': 13311.868789434433, 'accumulated_submission_time': 12295.721064329147, 'accumulated_eval_time': 1010.7144453525543, 'accumulated_logging_time': 1.710144281387329}
I0307 06:45:30.897182 139894559057664 logging_writer.py:48] [32079] accumulated_eval_time=1010.71, accumulated_logging_time=1.71014, accumulated_submission_time=12295.7, global_step=32079, preemption_count=0, score=12295.7, test/accuracy=0.205, test/loss=4.51372, test/num_examples=10000, total_duration=13311.9, train/accuracy=0.289162, train/loss=3.5969, validation/accuracy=0.27308, validation/loss=3.73968, validation/num_examples=50000
I0307 06:45:39.385230 139894567450368 logging_writer.py:48] [32100] global_step=32100, grad_norm=1.1552948951721191, loss=2.321965217590332
I0307 06:46:18.013864 139894559057664 logging_writer.py:48] [32200] global_step=32200, grad_norm=1.0275914669036865, loss=2.318932056427002
I0307 06:46:56.296799 139894567450368 logging_writer.py:48] [32300] global_step=32300, grad_norm=1.0543338060379028, loss=2.2529609203338623
I0307 06:47:34.440813 139894559057664 logging_writer.py:48] [32400] global_step=32400, grad_norm=1.1075644493103027, loss=2.3261966705322266
I0307 06:48:12.770379 139894567450368 logging_writer.py:48] [32500] global_step=32500, grad_norm=1.1371248960494995, loss=2.3993759155273438
I0307 06:48:51.548717 139894559057664 logging_writer.py:48] [32600] global_step=32600, grad_norm=1.0590760707855225, loss=2.1816229820251465
I0307 06:49:29.821968 139894567450368 logging_writer.py:48] [32700] global_step=32700, grad_norm=1.1757190227508545, loss=2.362708330154419
I0307 06:50:08.529556 139894559057664 logging_writer.py:48] [32800] global_step=32800, grad_norm=1.047160029411316, loss=2.180835008621216
I0307 06:50:47.140595 139894567450368 logging_writer.py:48] [32900] global_step=32900, grad_norm=1.198624610900879, loss=2.3152902126312256
I0307 06:51:25.693148 139894559057664 logging_writer.py:48] [33000] global_step=33000, grad_norm=1.1019560098648071, loss=2.258131980895996
I0307 06:52:04.030936 139894567450368 logging_writer.py:48] [33100] global_step=33100, grad_norm=1.1567984819412231, loss=2.2776248455047607
I0307 06:52:42.468922 139894559057664 logging_writer.py:48] [33200] global_step=33200, grad_norm=1.1744641065597534, loss=2.251871109008789
I0307 06:53:21.274899 139894567450368 logging_writer.py:48] [33300] global_step=33300, grad_norm=1.16378915309906, loss=2.3260927200317383
I0307 06:53:59.738072 139894559057664 logging_writer.py:48] [33400] global_step=33400, grad_norm=1.1176866292953491, loss=2.298124313354492
I0307 06:54:00.950824 140050941662400 spec.py:321] Evaluating on the training split.
I0307 06:54:13.455425 140050941662400 spec.py:333] Evaluating on the validation split.
I0307 06:54:38.942667 140050941662400 spec.py:349] Evaluating on the test split.
I0307 06:54:40.716367 140050941662400 submission_runner.py:469] Time since start: 13861.81s, 	Step: 33404, 	{'train/accuracy': 0.18865592777729034, 'train/loss': 5.211587905883789, 'validation/accuracy': 0.16941998898983002, 'validation/loss': 5.43745756149292, 'validation/num_examples': 50000, 'test/accuracy': 0.13600000739097595, 'test/loss': 5.970217227935791, 'test/num_examples': 10000, 'score': 12805.604042291641, 'total_duration': 13861.812965631485, 'accumulated_submission_time': 12805.604042291641, 'accumulated_eval_time': 1050.4797942638397, 'accumulated_logging_time': 1.8638660907745361}
I0307 06:54:40.814343 139894567450368 logging_writer.py:48] [33404] accumulated_eval_time=1050.48, accumulated_logging_time=1.86387, accumulated_submission_time=12805.6, global_step=33404, preemption_count=0, score=12805.6, test/accuracy=0.136, test/loss=5.97022, test/num_examples=10000, total_duration=13861.8, train/accuracy=0.188656, train/loss=5.21159, validation/accuracy=0.16942, validation/loss=5.43746, validation/num_examples=50000
I0307 06:55:17.949596 139894559057664 logging_writer.py:48] [33500] global_step=33500, grad_norm=1.089307188987732, loss=2.359978437423706
I0307 06:55:56.333225 139894567450368 logging_writer.py:48] [33600] global_step=33600, grad_norm=1.1178818941116333, loss=2.301694393157959
I0307 06:56:34.286865 139894559057664 logging_writer.py:48] [33700] global_step=33700, grad_norm=1.0595061779022217, loss=2.2552103996276855
I0307 06:57:12.933521 139894567450368 logging_writer.py:48] [33800] global_step=33800, grad_norm=1.0947579145431519, loss=2.3796122074127197
I0307 06:57:51.157087 139894559057664 logging_writer.py:48] [33900] global_step=33900, grad_norm=1.0883135795593262, loss=2.2592203617095947
I0307 06:58:29.250359 139894567450368 logging_writer.py:48] [34000] global_step=34000, grad_norm=1.1871238946914673, loss=2.1961848735809326
I0307 06:59:07.614800 139894559057664 logging_writer.py:48] [34100] global_step=34100, grad_norm=1.1164637804031372, loss=2.31986927986145
I0307 06:59:45.970462 139894567450368 logging_writer.py:48] [34200] global_step=34200, grad_norm=1.2099037170410156, loss=2.211326837539673
I0307 07:00:24.445490 139894559057664 logging_writer.py:48] [34300] global_step=34300, grad_norm=1.2107527256011963, loss=2.302053689956665
I0307 07:01:02.714569 139894567450368 logging_writer.py:48] [34400] global_step=34400, grad_norm=1.1918162107467651, loss=2.2334933280944824
I0307 07:01:41.182771 139894559057664 logging_writer.py:48] [34500] global_step=34500, grad_norm=1.0699536800384521, loss=2.328113079071045
I0307 07:02:19.721772 139894567450368 logging_writer.py:48] [34600] global_step=34600, grad_norm=1.151877760887146, loss=2.234940528869629
I0307 07:02:58.378615 139894559057664 logging_writer.py:48] [34700] global_step=34700, grad_norm=1.0731409788131714, loss=2.242295026779175
I0307 07:03:11.060249 140050941662400 spec.py:321] Evaluating on the training split.
I0307 07:03:23.656623 140050941662400 spec.py:333] Evaluating on the validation split.
I0307 07:03:52.942674 140050941662400 spec.py:349] Evaluating on the test split.
I0307 07:03:54.682335 140050941662400 submission_runner.py:469] Time since start: 14415.78s, 	Step: 34734, 	{'train/accuracy': 0.26773756742477417, 'train/loss': 3.8897037506103516, 'validation/accuracy': 0.24743999540805817, 'validation/loss': 4.053437232971191, 'validation/num_examples': 50000, 'test/accuracy': 0.18640001118183136, 'test/loss': 4.783019065856934, 'test/num_examples': 10000, 'score': 13315.685696840286, 'total_duration': 14415.778901576996, 'accumulated_submission_time': 13315.685696840286, 'accumulated_eval_time': 1094.1016657352448, 'accumulated_logging_time': 1.9842908382415771}
I0307 07:03:54.704402 139894567450368 logging_writer.py:48] [34734] accumulated_eval_time=1094.1, accumulated_logging_time=1.98429, accumulated_submission_time=13315.7, global_step=34734, preemption_count=0, score=13315.7, test/accuracy=0.1864, test/loss=4.78302, test/num_examples=10000, total_duration=14415.8, train/accuracy=0.267738, train/loss=3.8897, validation/accuracy=0.24744, validation/loss=4.05344, validation/num_examples=50000
I0307 07:04:20.492400 139894559057664 logging_writer.py:48] [34800] global_step=34800, grad_norm=1.136650800704956, loss=2.275129556655884
I0307 07:04:58.749764 139894567450368 logging_writer.py:48] [34900] global_step=34900, grad_norm=1.2243506908416748, loss=2.4462838172912598
I0307 07:05:37.365008 139894559057664 logging_writer.py:48] [35000] global_step=35000, grad_norm=1.0907996892929077, loss=2.3030474185943604
I0307 07:06:15.825237 139894567450368 logging_writer.py:48] [35100] global_step=35100, grad_norm=1.1398365497589111, loss=2.257748603820801
I0307 07:06:54.436299 139894559057664 logging_writer.py:48] [35200] global_step=35200, grad_norm=1.0865570306777954, loss=2.266843557357788
I0307 07:07:32.753154 139894567450368 logging_writer.py:48] [35300] global_step=35300, grad_norm=1.1850003004074097, loss=2.2784366607666016
I0307 07:08:11.193593 139894559057664 logging_writer.py:48] [35400] global_step=35400, grad_norm=1.2953511476516724, loss=2.2847540378570557
I0307 07:08:49.816979 139894567450368 logging_writer.py:48] [35500] global_step=35500, grad_norm=1.2166627645492554, loss=2.3091511726379395
I0307 07:09:28.293481 139894559057664 logging_writer.py:48] [35600] global_step=35600, grad_norm=1.3576407432556152, loss=2.2870662212371826
I0307 07:10:06.412529 139894567450368 logging_writer.py:48] [35700] global_step=35700, grad_norm=1.1297106742858887, loss=2.3148720264434814
I0307 07:10:44.569683 139894559057664 logging_writer.py:48] [35800] global_step=35800, grad_norm=1.1284548044204712, loss=2.3144798278808594
I0307 07:11:22.513892 139894567450368 logging_writer.py:48] [35900] global_step=35900, grad_norm=1.1908916234970093, loss=2.216789960861206
I0307 07:12:01.093306 139894559057664 logging_writer.py:48] [36000] global_step=36000, grad_norm=1.2593631744384766, loss=2.241398811340332
I0307 07:12:24.774214 140050941662400 spec.py:321] Evaluating on the training split.
I0307 07:12:37.287152 140050941662400 spec.py:333] Evaluating on the validation split.
I0307 07:13:02.072055 140050941662400 spec.py:349] Evaluating on the test split.
I0307 07:13:03.827691 140050941662400 submission_runner.py:469] Time since start: 14964.92s, 	Step: 36063, 	{'train/accuracy': 0.15132732689380646, 'train/loss': 5.1248602867126465, 'validation/accuracy': 0.1421400010585785, 'validation/loss': 5.193818092346191, 'validation/num_examples': 50000, 'test/accuracy': 0.10250000655651093, 'test/loss': 5.777795791625977, 'test/num_examples': 10000, 'score': 13825.573055028915, 'total_duration': 14964.924278497696, 'accumulated_submission_time': 13825.573055028915, 'accumulated_eval_time': 1133.1549396514893, 'accumulated_logging_time': 2.0456316471099854}
I0307 07:13:03.895488 139894567450368 logging_writer.py:48] [36063] accumulated_eval_time=1133.15, accumulated_logging_time=2.04563, accumulated_submission_time=13825.6, global_step=36063, preemption_count=0, score=13825.6, test/accuracy=0.1025, test/loss=5.7778, test/num_examples=10000, total_duration=14964.9, train/accuracy=0.151327, train/loss=5.12486, validation/accuracy=0.14214, validation/loss=5.19382, validation/num_examples=50000
I0307 07:13:18.418200 139894559057664 logging_writer.py:48] [36100] global_step=36100, grad_norm=1.014528751373291, loss=2.1673171520233154
I0307 07:13:56.838659 139894567450368 logging_writer.py:48] [36200] global_step=36200, grad_norm=1.0967799425125122, loss=2.2592124938964844
I0307 07:14:35.696967 139894559057664 logging_writer.py:48] [36300] global_step=36300, grad_norm=1.1568701267242432, loss=2.2558939456939697
I0307 07:15:14.217533 139894567450368 logging_writer.py:48] [36400] global_step=36400, grad_norm=1.1123859882354736, loss=2.259472370147705
I0307 07:15:52.945166 139894559057664 logging_writer.py:48] [36500] global_step=36500, grad_norm=1.1583123207092285, loss=2.2014243602752686
I0307 07:16:30.859673 139894567450368 logging_writer.py:48] [36600] global_step=36600, grad_norm=1.0364142656326294, loss=2.312298059463501
I0307 07:17:08.778413 139894559057664 logging_writer.py:48] [36700] global_step=36700, grad_norm=1.1957813501358032, loss=2.3411662578582764
I0307 07:17:47.529213 139894567450368 logging_writer.py:48] [36800] global_step=36800, grad_norm=1.118493914604187, loss=2.1923818588256836
I0307 07:18:25.942659 139894559057664 logging_writer.py:48] [36900] global_step=36900, grad_norm=1.1538703441619873, loss=2.301787853240967
I0307 07:19:04.115173 139894567450368 logging_writer.py:48] [37000] global_step=37000, grad_norm=1.0216549634933472, loss=2.2323851585388184
I0307 07:19:42.396852 139894559057664 logging_writer.py:48] [37100] global_step=37100, grad_norm=1.3843945264816284, loss=2.3666696548461914
I0307 07:20:21.089085 139894567450368 logging_writer.py:48] [37200] global_step=37200, grad_norm=1.0774048566818237, loss=2.324596643447876
I0307 07:20:59.571705 139894559057664 logging_writer.py:48] [37300] global_step=37300, grad_norm=1.113315224647522, loss=2.237410306930542
I0307 07:21:34.180187 140050941662400 spec.py:321] Evaluating on the training split.
I0307 07:21:45.875489 140050941662400 spec.py:333] Evaluating on the validation split.
I0307 07:22:10.197546 140050941662400 spec.py:349] Evaluating on the test split.
I0307 07:22:11.934880 140050941662400 submission_runner.py:469] Time since start: 15513.03s, 	Step: 37392, 	{'train/accuracy': 0.23624840378761292, 'train/loss': 4.087423324584961, 'validation/accuracy': 0.22645999491214752, 'validation/loss': 4.208317279815674, 'validation/num_examples': 50000, 'test/accuracy': 0.17590001225471497, 'test/loss': 4.745566368103027, 'test/num_examples': 10000, 'score': 14335.688372373581, 'total_duration': 15513.031492233276, 'accumulated_submission_time': 14335.688372373581, 'accumulated_eval_time': 1170.9094579219818, 'accumulated_logging_time': 2.1393346786499023}
I0307 07:22:11.999762 139894567450368 logging_writer.py:48] [37392] accumulated_eval_time=1170.91, accumulated_logging_time=2.13933, accumulated_submission_time=14335.7, global_step=37392, preemption_count=0, score=14335.7, test/accuracy=0.1759, test/loss=4.74557, test/num_examples=10000, total_duration=15513, train/accuracy=0.236248, train/loss=4.08742, validation/accuracy=0.22646, validation/loss=4.20832, validation/num_examples=50000
I0307 07:22:15.488206 139894559057664 logging_writer.py:48] [37400] global_step=37400, grad_norm=1.1227831840515137, loss=2.3635947704315186
I0307 07:22:54.051738 139894567450368 logging_writer.py:48] [37500] global_step=37500, grad_norm=1.1761733293533325, loss=2.2138099670410156
I0307 07:23:33.196989 139894559057664 logging_writer.py:48] [37600] global_step=37600, grad_norm=1.2479314804077148, loss=2.2315561771392822
I0307 07:24:11.516375 139894567450368 logging_writer.py:48] [37700] global_step=37700, grad_norm=1.2454380989074707, loss=2.314539670944214
I0307 07:24:49.952922 139894559057664 logging_writer.py:48] [37800] global_step=37800, grad_norm=1.2698493003845215, loss=2.244342803955078
I0307 07:25:28.646012 139894567450368 logging_writer.py:48] [37900] global_step=37900, grad_norm=1.1904278993606567, loss=2.3132362365722656
I0307 07:26:08.149363 139894559057664 logging_writer.py:48] [38000] global_step=38000, grad_norm=1.0791910886764526, loss=2.2263312339782715
I0307 07:26:46.702144 139894567450368 logging_writer.py:48] [38100] global_step=38100, grad_norm=1.1767057180404663, loss=2.2876124382019043
I0307 07:27:24.892960 139894559057664 logging_writer.py:48] [38200] global_step=38200, grad_norm=1.2266451120376587, loss=2.2600014209747314
I0307 07:28:03.071465 139894567450368 logging_writer.py:48] [38300] global_step=38300, grad_norm=1.235511064529419, loss=2.3548641204833984
I0307 07:28:41.297252 139894559057664 logging_writer.py:48] [38400] global_step=38400, grad_norm=1.126296877861023, loss=2.2638485431671143
I0307 07:29:19.709349 139894567450368 logging_writer.py:48] [38500] global_step=38500, grad_norm=1.342045545578003, loss=2.3076796531677246
I0307 07:29:57.899869 139894559057664 logging_writer.py:48] [38600] global_step=38600, grad_norm=1.254743218421936, loss=2.3892271518707275
I0307 07:30:36.215464 139894567450368 logging_writer.py:48] [38700] global_step=38700, grad_norm=1.1545042991638184, loss=2.1986570358276367
I0307 07:30:42.003651 140050941662400 spec.py:321] Evaluating on the training split.
I0307 07:30:54.194211 140050941662400 spec.py:333] Evaluating on the validation split.
I0307 07:31:18.081717 140050941662400 spec.py:349] Evaluating on the test split.
I0307 07:31:19.840859 140050941662400 submission_runner.py:469] Time since start: 16060.94s, 	Step: 38716, 	{'train/accuracy': 0.18307557702064514, 'train/loss': 4.588757038116455, 'validation/accuracy': 0.17389999330043793, 'validation/loss': 4.6564531326293945, 'validation/num_examples': 50000, 'test/accuracy': 0.1193000078201294, 'test/loss': 5.400364875793457, 'test/num_examples': 10000, 'score': 14845.524495840073, 'total_duration': 16060.937438488007, 'accumulated_submission_time': 14845.524495840073, 'accumulated_eval_time': 1208.7464561462402, 'accumulated_logging_time': 2.2261641025543213}
I0307 07:31:19.898341 139894559057664 logging_writer.py:48] [38716] accumulated_eval_time=1208.75, accumulated_logging_time=2.22616, accumulated_submission_time=14845.5, global_step=38716, preemption_count=0, score=14845.5, test/accuracy=0.1193, test/loss=5.40036, test/num_examples=10000, total_duration=16060.9, train/accuracy=0.183076, train/loss=4.58876, validation/accuracy=0.1739, validation/loss=4.65645, validation/num_examples=50000
I0307 07:31:52.882889 139894567450368 logging_writer.py:48] [38800] global_step=38800, grad_norm=1.2292104959487915, loss=2.2328503131866455
I0307 07:32:31.289851 139894559057664 logging_writer.py:48] [38900] global_step=38900, grad_norm=1.2155520915985107, loss=2.2804880142211914
I0307 07:33:09.561590 139894567450368 logging_writer.py:48] [39000] global_step=39000, grad_norm=1.0950639247894287, loss=2.2722883224487305
I0307 07:33:48.450126 139894559057664 logging_writer.py:48] [39100] global_step=39100, grad_norm=1.1404109001159668, loss=2.3167004585266113
I0307 07:34:26.706568 139894567450368 logging_writer.py:48] [39200] global_step=39200, grad_norm=1.118316888809204, loss=2.3930399417877197
I0307 07:35:04.895880 139894559057664 logging_writer.py:48] [39300] global_step=39300, grad_norm=1.1838009357452393, loss=2.213387966156006
I0307 07:35:43.097966 139894567450368 logging_writer.py:48] [39400] global_step=39400, grad_norm=1.103015661239624, loss=2.260118246078491
I0307 07:36:21.465173 139894559057664 logging_writer.py:48] [39500] global_step=39500, grad_norm=1.1486828327178955, loss=2.30513596534729
I0307 07:36:59.730845 139894567450368 logging_writer.py:48] [39600] global_step=39600, grad_norm=1.1816450357437134, loss=2.371305465698242
I0307 07:37:37.910526 139894559057664 logging_writer.py:48] [39700] global_step=39700, grad_norm=1.2292903661727905, loss=2.3216605186462402
I0307 07:38:16.320974 139894567450368 logging_writer.py:48] [39800] global_step=39800, grad_norm=1.114029884338379, loss=2.2781457901000977
I0307 07:38:54.676167 139894559057664 logging_writer.py:48] [39900] global_step=39900, grad_norm=1.0482653379440308, loss=2.227970838546753
I0307 07:39:34.017219 139894567450368 logging_writer.py:48] [40000] global_step=40000, grad_norm=1.2545702457427979, loss=2.404780864715576
I0307 07:39:50.022100 140050941662400 spec.py:321] Evaluating on the training split.
I0307 07:40:02.258333 140050941662400 spec.py:333] Evaluating on the validation split.
I0307 07:40:32.980641 140050941662400 spec.py:349] Evaluating on the test split.
I0307 07:40:35.071659 140050941662400 submission_runner.py:469] Time since start: 16616.17s, 	Step: 40042, 	{'train/accuracy': 0.21982620656490326, 'train/loss': 4.917595386505127, 'validation/accuracy': 0.20999999344348907, 'validation/loss': 5.031191825866699, 'validation/num_examples': 50000, 'test/accuracy': 0.1551000028848648, 'test/loss': 5.816312789916992, 'test/num_examples': 10000, 'score': 15355.480728149414, 'total_duration': 16616.16822552681, 'accumulated_submission_time': 15355.480728149414, 'accumulated_eval_time': 1253.7957904338837, 'accumulated_logging_time': 2.3082756996154785}
I0307 07:40:35.140478 139894559057664 logging_writer.py:48] [40042] accumulated_eval_time=1253.8, accumulated_logging_time=2.30828, accumulated_submission_time=15355.5, global_step=40042, preemption_count=0, score=15355.5, test/accuracy=0.1551, test/loss=5.81631, test/num_examples=10000, total_duration=16616.2, train/accuracy=0.219826, train/loss=4.9176, validation/accuracy=0.21, validation/loss=5.03119, validation/num_examples=50000
I0307 07:40:58.018143 139894567450368 logging_writer.py:48] [40100] global_step=40100, grad_norm=1.2467581033706665, loss=2.3586156368255615
I0307 07:41:36.505831 139894559057664 logging_writer.py:48] [40200] global_step=40200, grad_norm=1.1968070268630981, loss=2.289679527282715
I0307 07:42:15.046125 139894567450368 logging_writer.py:48] [40300] global_step=40300, grad_norm=1.2654677629470825, loss=2.245209217071533
I0307 07:42:53.508154 139894559057664 logging_writer.py:48] [40400] global_step=40400, grad_norm=1.1561661958694458, loss=2.169612407684326
I0307 07:43:31.533719 139894567450368 logging_writer.py:48] [40500] global_step=40500, grad_norm=1.1309102773666382, loss=2.2020533084869385
I0307 07:44:09.892169 139894559057664 logging_writer.py:48] [40600] global_step=40600, grad_norm=1.1879520416259766, loss=2.39507794380188
I0307 07:44:48.337604 139894567450368 logging_writer.py:48] [40700] global_step=40700, grad_norm=1.2476240396499634, loss=2.387514114379883
I0307 07:45:26.782992 139894559057664 logging_writer.py:48] [40800] global_step=40800, grad_norm=1.2366794347763062, loss=2.3531339168548584
I0307 07:46:04.921742 139894567450368 logging_writer.py:48] [40900] global_step=40900, grad_norm=1.0999194383621216, loss=2.293537139892578
I0307 07:46:43.415540 139894559057664 logging_writer.py:48] [41000] global_step=41000, grad_norm=1.3230483531951904, loss=2.3390731811523438
I0307 07:47:21.957420 139894567450368 logging_writer.py:48] [41100] global_step=41100, grad_norm=1.1018495559692383, loss=2.2610268592834473
I0307 07:48:00.223118 139894559057664 logging_writer.py:48] [41200] global_step=41200, grad_norm=1.2208900451660156, loss=2.2203869819641113
I0307 07:48:38.621264 139894567450368 logging_writer.py:48] [41300] global_step=41300, grad_norm=1.0878132581710815, loss=2.2941317558288574
I0307 07:49:05.092895 140050941662400 spec.py:321] Evaluating on the training split.
I0307 07:49:17.502465 140050941662400 spec.py:333] Evaluating on the validation split.
I0307 07:49:49.062577 140050941662400 spec.py:349] Evaluating on the test split.
I0307 07:49:50.796994 140050941662400 submission_runner.py:469] Time since start: 17171.89s, 	Step: 41370, 	{'train/accuracy': 0.1408442258834839, 'train/loss': 5.489231586456299, 'validation/accuracy': 0.12661999464035034, 'validation/loss': 5.6430206298828125, 'validation/num_examples': 50000, 'test/accuracy': 0.09790000319480896, 'test/loss': 6.098194599151611, 'test/num_examples': 10000, 'score': 15865.256662607193, 'total_duration': 17171.893587112427, 'accumulated_submission_time': 15865.256662607193, 'accumulated_eval_time': 1299.4996938705444, 'accumulated_logging_time': 2.4108331203460693}
I0307 07:49:50.896048 139894559057664 logging_writer.py:48] [41370] accumulated_eval_time=1299.5, accumulated_logging_time=2.41083, accumulated_submission_time=15865.3, global_step=41370, preemption_count=0, score=15865.3, test/accuracy=0.0979, test/loss=6.09819, test/num_examples=10000, total_duration=17171.9, train/accuracy=0.140844, train/loss=5.48923, validation/accuracy=0.12662, validation/loss=5.64302, validation/num_examples=50000
I0307 07:50:02.998360 139894567450368 logging_writer.py:48] [41400] global_step=41400, grad_norm=1.1440165042877197, loss=2.1920719146728516
I0307 07:50:41.737052 139894559057664 logging_writer.py:48] [41500] global_step=41500, grad_norm=1.1156635284423828, loss=2.293513774871826
I0307 07:51:20.197715 139894567450368 logging_writer.py:48] [41600] global_step=41600, grad_norm=1.0765283107757568, loss=2.3551676273345947
I0307 07:51:59.050187 139894559057664 logging_writer.py:48] [41700] global_step=41700, grad_norm=1.1801830530166626, loss=2.4038634300231934
I0307 07:52:37.646709 139894567450368 logging_writer.py:48] [41800] global_step=41800, grad_norm=1.1413884162902832, loss=2.308220624923706
I0307 07:53:15.831381 139894559057664 logging_writer.py:48] [41900] global_step=41900, grad_norm=1.2473474740982056, loss=2.3779056072235107
I0307 07:53:54.486061 139894567450368 logging_writer.py:48] [42000] global_step=42000, grad_norm=1.2124282121658325, loss=2.3164050579071045
I0307 07:54:32.678222 139894559057664 logging_writer.py:48] [42100] global_step=42100, grad_norm=1.211229681968689, loss=2.2130801677703857
I0307 07:55:11.008999 139894567450368 logging_writer.py:48] [42200] global_step=42200, grad_norm=1.1456905603408813, loss=2.325260877609253
I0307 07:55:49.653198 139894559057664 logging_writer.py:48] [42300] global_step=42300, grad_norm=1.145592451095581, loss=2.3131837844848633
I0307 07:56:27.958366 139894567450368 logging_writer.py:48] [42400] global_step=42400, grad_norm=1.1806246042251587, loss=2.2732763290405273
I0307 07:57:06.395668 139894559057664 logging_writer.py:48] [42500] global_step=42500, grad_norm=1.2350871562957764, loss=2.3080427646636963
I0307 07:57:45.030478 139894567450368 logging_writer.py:48] [42600] global_step=42600, grad_norm=1.173778772354126, loss=2.2442116737365723
I0307 07:58:20.852698 140050941662400 spec.py:321] Evaluating on the training split.
I0307 07:58:32.597937 140050941662400 spec.py:333] Evaluating on the validation split.
I0307 07:58:58.961876 140050941662400 spec.py:349] Evaluating on the test split.
I0307 07:59:00.704683 140050941662400 submission_runner.py:469] Time since start: 17721.80s, 	Step: 42694, 	{'train/accuracy': 0.2802136540412903, 'train/loss': 3.61694073677063, 'validation/accuracy': 0.2674599885940552, 'validation/loss': 3.719081163406372, 'validation/num_examples': 50000, 'test/accuracy': 0.20180000364780426, 'test/loss': 4.413468360900879, 'test/num_examples': 10000, 'score': 16375.031787872314, 'total_duration': 17721.80126619339, 'accumulated_submission_time': 16375.031787872314, 'accumulated_eval_time': 1339.3514766693115, 'accumulated_logging_time': 2.5451548099517822}
I0307 07:59:00.795917 139894559057664 logging_writer.py:48] [42694] accumulated_eval_time=1339.35, accumulated_logging_time=2.54515, accumulated_submission_time=16375, global_step=42694, preemption_count=0, score=16375, test/accuracy=0.2018, test/loss=4.41347, test/num_examples=10000, total_duration=17721.8, train/accuracy=0.280214, train/loss=3.61694, validation/accuracy=0.26746, validation/loss=3.71908, validation/num_examples=50000
I0307 07:59:03.452199 139894567450368 logging_writer.py:48] [42700] global_step=42700, grad_norm=1.1760002374649048, loss=2.2151529788970947
I0307 07:59:41.437348 139894559057664 logging_writer.py:48] [42800] global_step=42800, grad_norm=1.2736245393753052, loss=2.3067667484283447
I0307 08:00:19.647649 139894567450368 logging_writer.py:48] [42900] global_step=42900, grad_norm=1.2787803411483765, loss=2.421537399291992
I0307 08:00:58.310079 139894559057664 logging_writer.py:48] [43000] global_step=43000, grad_norm=1.203137755393982, loss=2.3210835456848145
I0307 08:01:36.554085 139894567450368 logging_writer.py:48] [43100] global_step=43100, grad_norm=1.5609313249588013, loss=2.354613780975342
I0307 08:02:14.348990 139894559057664 logging_writer.py:48] [43200] global_step=43200, grad_norm=1.1048941612243652, loss=2.2084765434265137
I0307 08:02:53.080184 139894567450368 logging_writer.py:48] [43300] global_step=43300, grad_norm=1.289975881576538, loss=2.3843135833740234
I0307 08:03:31.854484 139894559057664 logging_writer.py:48] [43400] global_step=43400, grad_norm=1.0725878477096558, loss=2.265408754348755
I0307 08:04:10.103979 139894567450368 logging_writer.py:48] [43500] global_step=43500, grad_norm=1.2636098861694336, loss=2.2706387042999268
I0307 08:04:47.909218 139894559057664 logging_writer.py:48] [43600] global_step=43600, grad_norm=1.1281484365463257, loss=2.257158041000366
I0307 08:05:25.924336 139894567450368 logging_writer.py:48] [43700] global_step=43700, grad_norm=1.1163347959518433, loss=2.247647762298584
I0307 08:06:05.030327 139894559057664 logging_writer.py:48] [43800] global_step=43800, grad_norm=1.1305880546569824, loss=2.23345685005188
I0307 08:06:43.549192 139894567450368 logging_writer.py:48] [43900] global_step=43900, grad_norm=1.1750049591064453, loss=2.192983627319336
I0307 08:07:22.147177 139894559057664 logging_writer.py:48] [44000] global_step=44000, grad_norm=1.0919468402862549, loss=2.2350118160247803
I0307 08:07:30.956029 140050941662400 spec.py:321] Evaluating on the training split.
I0307 08:07:43.129852 140050941662400 spec.py:333] Evaluating on the validation split.
I0307 08:08:07.783316 140050941662400 spec.py:349] Evaluating on the test split.
I0307 08:08:09.569188 140050941662400 submission_runner.py:469] Time since start: 18270.67s, 	Step: 44024, 	{'train/accuracy': 0.2174944132566452, 'train/loss': 4.659315586090088, 'validation/accuracy': 0.20833998918533325, 'validation/loss': 4.689654350280762, 'validation/num_examples': 50000, 'test/accuracy': 0.15220001339912415, 'test/loss': 5.475283622741699, 'test/num_examples': 10000, 'score': 16885.010546445847, 'total_duration': 18270.6658346653, 'accumulated_submission_time': 16885.010546445847, 'accumulated_eval_time': 1377.9644961357117, 'accumulated_logging_time': 2.672539472579956}
I0307 08:08:09.655895 139894567450368 logging_writer.py:48] [44024] accumulated_eval_time=1377.96, accumulated_logging_time=2.67254, accumulated_submission_time=16885, global_step=44024, preemption_count=0, score=16885, test/accuracy=0.1522, test/loss=5.47528, test/num_examples=10000, total_duration=18270.7, train/accuracy=0.217494, train/loss=4.65932, validation/accuracy=0.20834, validation/loss=4.68965, validation/num_examples=50000
I0307 08:08:39.176848 139894559057664 logging_writer.py:48] [44100] global_step=44100, grad_norm=1.263676643371582, loss=2.181992292404175
I0307 08:09:17.538525 139894567450368 logging_writer.py:48] [44200] global_step=44200, grad_norm=1.1761046648025513, loss=2.2952022552490234
I0307 08:09:55.849328 139894559057664 logging_writer.py:48] [44300] global_step=44300, grad_norm=1.1476528644561768, loss=2.1948366165161133
I0307 08:10:34.779340 139894567450368 logging_writer.py:48] [44400] global_step=44400, grad_norm=1.051640272140503, loss=2.1199917793273926
I0307 08:11:14.387310 139894559057664 logging_writer.py:48] [44500] global_step=44500, grad_norm=1.249017357826233, loss=2.173124074935913
I0307 08:11:53.019897 139894567450368 logging_writer.py:48] [44600] global_step=44600, grad_norm=1.2912960052490234, loss=2.2571468353271484
I0307 08:12:31.283421 139894559057664 logging_writer.py:48] [44700] global_step=44700, grad_norm=1.1737481355667114, loss=2.182060480117798
I0307 08:13:09.619418 139894567450368 logging_writer.py:48] [44800] global_step=44800, grad_norm=1.1402817964553833, loss=2.233790397644043
I0307 08:13:47.883429 139894559057664 logging_writer.py:48] [44900] global_step=44900, grad_norm=1.1603518724441528, loss=2.244708776473999
I0307 08:14:26.808834 139894567450368 logging_writer.py:48] [45000] global_step=45000, grad_norm=1.2195842266082764, loss=2.3684306144714355
I0307 08:15:05.900849 139894559057664 logging_writer.py:48] [45100] global_step=45100, grad_norm=1.1432912349700928, loss=2.3867063522338867
I0307 08:15:44.458688 139894567450368 logging_writer.py:48] [45200] global_step=45200, grad_norm=1.2687128782272339, loss=2.339500665664673
I0307 08:16:22.980471 139894559057664 logging_writer.py:48] [45300] global_step=45300, grad_norm=1.1032404899597168, loss=2.3225417137145996
I0307 08:16:39.650918 140050941662400 spec.py:321] Evaluating on the training split.
I0307 08:16:51.581945 140050941662400 spec.py:333] Evaluating on the validation split.
I0307 08:17:16.627347 140050941662400 spec.py:349] Evaluating on the test split.
I0307 08:17:18.408211 140050941662400 submission_runner.py:469] Time since start: 18819.50s, 	Step: 45345, 	{'train/accuracy': 0.3126992881298065, 'train/loss': 3.4420769214630127, 'validation/accuracy': 0.2979799807071686, 'validation/loss': 3.526496171951294, 'validation/num_examples': 50000, 'test/accuracy': 0.22420001029968262, 'test/loss': 4.273254871368408, 'test/num_examples': 10000, 'score': 17394.830267190933, 'total_duration': 18819.504824399948, 'accumulated_submission_time': 17394.830267190933, 'accumulated_eval_time': 1416.7216205596924, 'accumulated_logging_time': 2.7885525226593018}
I0307 08:17:18.474362 139894567450368 logging_writer.py:48] [45345] accumulated_eval_time=1416.72, accumulated_logging_time=2.78855, accumulated_submission_time=17394.8, global_step=45345, preemption_count=0, score=17394.8, test/accuracy=0.2242, test/loss=4.27325, test/num_examples=10000, total_duration=18819.5, train/accuracy=0.312699, train/loss=3.44208, validation/accuracy=0.29798, validation/loss=3.5265, validation/num_examples=50000
I0307 08:17:40.261526 139894559057664 logging_writer.py:48] [45400] global_step=45400, grad_norm=1.1457996368408203, loss=2.255751132965088
I0307 08:18:19.143659 139894567450368 logging_writer.py:48] [45500] global_step=45500, grad_norm=1.0775842666625977, loss=2.226210117340088
I0307 08:18:57.719083 139894559057664 logging_writer.py:48] [45600] global_step=45600, grad_norm=1.1571075916290283, loss=2.244368314743042
I0307 08:19:36.463984 139894567450368 logging_writer.py:48] [45700] global_step=45700, grad_norm=1.2266639471054077, loss=2.319666862487793
I0307 08:20:14.837143 139894559057664 logging_writer.py:48] [45800] global_step=45800, grad_norm=1.1753807067871094, loss=2.183767318725586
I0307 08:20:53.173402 139894567450368 logging_writer.py:48] [45900] global_step=45900, grad_norm=1.1537072658538818, loss=2.1768763065338135
I0307 08:21:31.552314 139894559057664 logging_writer.py:48] [46000] global_step=46000, grad_norm=1.232357144355774, loss=2.184549331665039
I0307 08:22:09.448214 139894567450368 logging_writer.py:48] [46100] global_step=46100, grad_norm=1.1768395900726318, loss=2.2251133918762207
I0307 08:22:48.328080 139894559057664 logging_writer.py:48] [46200] global_step=46200, grad_norm=1.150811791419983, loss=2.21543025970459
I0307 08:23:27.385484 139894567450368 logging_writer.py:48] [46300] global_step=46300, grad_norm=1.3814868927001953, loss=2.2186684608459473
I0307 08:24:05.633529 139894559057664 logging_writer.py:48] [46400] global_step=46400, grad_norm=1.1209248304367065, loss=2.3703293800354004
I0307 08:24:43.848914 139894567450368 logging_writer.py:48] [46500] global_step=46500, grad_norm=1.1352776288986206, loss=2.255754232406616
I0307 08:25:22.581101 139894559057664 logging_writer.py:48] [46600] global_step=46600, grad_norm=1.2712029218673706, loss=2.2288591861724854
I0307 08:25:48.636454 140050941662400 spec.py:321] Evaluating on the training split.
I0307 08:26:01.066817 140050941662400 spec.py:333] Evaluating on the validation split.
I0307 08:26:26.149003 140050941662400 spec.py:349] Evaluating on the test split.
I0307 08:26:27.960411 140050941662400 submission_runner.py:469] Time since start: 19369.06s, 	Step: 46669, 	{'train/accuracy': 0.30709901452064514, 'train/loss': 3.4897403717041016, 'validation/accuracy': 0.2933200001716614, 'validation/loss': 3.5928421020507812, 'validation/num_examples': 50000, 'test/accuracy': 0.2217000126838684, 'test/loss': 4.303364276885986, 'test/num_examples': 10000, 'score': 17904.821108341217, 'total_duration': 19369.057031154633, 'accumulated_submission_time': 17904.821108341217, 'accumulated_eval_time': 1456.0454235076904, 'accumulated_logging_time': 2.88271427154541}
I0307 08:26:28.017930 139894567450368 logging_writer.py:48] [46669] accumulated_eval_time=1456.05, accumulated_logging_time=2.88271, accumulated_submission_time=17904.8, global_step=46669, preemption_count=0, score=17904.8, test/accuracy=0.2217, test/loss=4.30336, test/num_examples=10000, total_duration=19369.1, train/accuracy=0.307099, train/loss=3.48974, validation/accuracy=0.29332, validation/loss=3.59284, validation/num_examples=50000
I0307 08:26:40.285608 139894559057664 logging_writer.py:48] [46700] global_step=46700, grad_norm=1.2017422914505005, loss=2.2988340854644775
I0307 08:27:18.462667 139894567450368 logging_writer.py:48] [46800] global_step=46800, grad_norm=1.4069087505340576, loss=2.118370294570923
I0307 08:27:56.966935 139894559057664 logging_writer.py:48] [46900] global_step=46900, grad_norm=1.225313425064087, loss=2.364515781402588
I0307 08:28:35.450914 139894567450368 logging_writer.py:48] [47000] global_step=47000, grad_norm=1.6230497360229492, loss=2.297189474105835
I0307 08:29:13.724875 139894559057664 logging_writer.py:48] [47100] global_step=47100, grad_norm=1.1853348016738892, loss=2.3248181343078613
I0307 08:29:52.364856 139894567450368 logging_writer.py:48] [47200] global_step=47200, grad_norm=1.15992271900177, loss=2.173567056655884
I0307 08:30:31.081802 139894559057664 logging_writer.py:48] [47300] global_step=47300, grad_norm=1.237436056137085, loss=2.288276195526123
I0307 08:31:09.678432 139894567450368 logging_writer.py:48] [47400] global_step=47400, grad_norm=1.3114711046218872, loss=2.3366546630859375
I0307 08:31:48.299892 139894559057664 logging_writer.py:48] [47500] global_step=47500, grad_norm=1.418251872062683, loss=2.200411796569824
I0307 08:32:27.441025 139894567450368 logging_writer.py:48] [47600] global_step=47600, grad_norm=1.289832592010498, loss=2.163252592086792
I0307 08:33:05.714195 139894559057664 logging_writer.py:48] [47700] global_step=47700, grad_norm=1.2334824800491333, loss=2.223240852355957
I0307 08:33:44.446156 139894567450368 logging_writer.py:48] [47800] global_step=47800, grad_norm=1.1700507402420044, loss=2.354011297225952
I0307 08:34:22.890541 139894559057664 logging_writer.py:48] [47900] global_step=47900, grad_norm=1.1656858921051025, loss=2.2659778594970703
I0307 08:34:57.996124 140050941662400 spec.py:321] Evaluating on the training split.
I0307 08:35:10.462469 140050941662400 spec.py:333] Evaluating on the validation split.
I0307 08:35:39.185814 140050941662400 spec.py:349] Evaluating on the test split.
I0307 08:35:40.931953 140050941662400 submission_runner.py:469] Time since start: 19922.03s, 	Step: 47992, 	{'train/accuracy': 0.21053889393806458, 'train/loss': 4.542476177215576, 'validation/accuracy': 0.20093999803066254, 'validation/loss': 4.677862167358398, 'validation/num_examples': 50000, 'test/accuracy': 0.15160000324249268, 'test/loss': 5.391515731811523, 'test/num_examples': 10000, 'score': 18414.586498975754, 'total_duration': 19922.02856040001, 'accumulated_submission_time': 18414.586498975754, 'accumulated_eval_time': 1498.9810802936554, 'accumulated_logging_time': 3.0106141567230225}
I0307 08:35:41.013920 139894567450368 logging_writer.py:48] [47992] accumulated_eval_time=1498.98, accumulated_logging_time=3.01061, accumulated_submission_time=18414.6, global_step=47992, preemption_count=0, score=18414.6, test/accuracy=0.1516, test/loss=5.39152, test/num_examples=10000, total_duration=19922, train/accuracy=0.210539, train/loss=4.54248, validation/accuracy=0.20094, validation/loss=4.67786, validation/num_examples=50000
I0307 08:35:44.712339 139894559057664 logging_writer.py:48] [48000] global_step=48000, grad_norm=1.0868977308273315, loss=2.166837453842163
I0307 08:36:23.179887 139894567450368 logging_writer.py:48] [48100] global_step=48100, grad_norm=1.169707179069519, loss=2.2804555892944336
I0307 08:37:02.029643 139894559057664 logging_writer.py:48] [48200] global_step=48200, grad_norm=1.1677721738815308, loss=2.2575042247772217
I0307 08:37:40.368766 139894567450368 logging_writer.py:48] [48300] global_step=48300, grad_norm=1.278738021850586, loss=2.3408212661743164
I0307 08:38:18.523936 139894559057664 logging_writer.py:48] [48400] global_step=48400, grad_norm=1.4398962259292603, loss=2.2894716262817383
I0307 08:38:56.906384 139894567450368 logging_writer.py:48] [48500] global_step=48500, grad_norm=1.2093803882598877, loss=2.2722153663635254
I0307 08:39:35.415257 139894559057664 logging_writer.py:48] [48600] global_step=48600, grad_norm=1.2147115468978882, loss=2.311309814453125
I0307 08:40:14.674646 139894567450368 logging_writer.py:48] [48700] global_step=48700, grad_norm=1.1073312759399414, loss=2.1615917682647705
I0307 08:40:54.246143 139894559057664 logging_writer.py:48] [48800] global_step=48800, grad_norm=1.2791186571121216, loss=2.3381383419036865
I0307 08:41:32.720441 139894567450368 logging_writer.py:48] [48900] global_step=48900, grad_norm=1.1792776584625244, loss=2.250170946121216
I0307 08:42:11.412230 139894559057664 logging_writer.py:48] [49000] global_step=49000, grad_norm=1.3067344427108765, loss=2.3297884464263916
I0307 08:42:50.021116 139894567450368 logging_writer.py:48] [49100] global_step=49100, grad_norm=1.2160134315490723, loss=2.163264274597168
I0307 08:43:28.178501 139894559057664 logging_writer.py:48] [49200] global_step=49200, grad_norm=1.281369924545288, loss=2.240612745285034
I0307 08:44:06.333828 139894567450368 logging_writer.py:48] [49300] global_step=49300, grad_norm=1.476157307624817, loss=2.3979859352111816
I0307 08:44:11.242959 140050941662400 spec.py:321] Evaluating on the training split.
I0307 08:44:23.366145 140050941662400 spec.py:333] Evaluating on the validation split.
I0307 08:44:49.479203 140050941662400 spec.py:349] Evaluating on the test split.
I0307 08:44:51.224889 140050941662400 submission_runner.py:469] Time since start: 20472.32s, 	Step: 49314, 	{'train/accuracy': 0.28946107625961304, 'train/loss': 3.677434206008911, 'validation/accuracy': 0.27730000019073486, 'validation/loss': 3.8016085624694824, 'validation/num_examples': 50000, 'test/accuracy': 0.21630001068115234, 'test/loss': 4.441237926483154, 'test/num_examples': 10000, 'score': 18924.61717748642, 'total_duration': 20472.321487426758, 'accumulated_submission_time': 18924.61717748642, 'accumulated_eval_time': 1538.9628190994263, 'accumulated_logging_time': 3.146548271179199}
I0307 08:44:51.345337 139894559057664 logging_writer.py:48] [49314] accumulated_eval_time=1538.96, accumulated_logging_time=3.14655, accumulated_submission_time=18924.6, global_step=49314, preemption_count=0, score=18924.6, test/accuracy=0.2163, test/loss=4.44124, test/num_examples=10000, total_duration=20472.3, train/accuracy=0.289461, train/loss=3.67743, validation/accuracy=0.2773, validation/loss=3.80161, validation/num_examples=50000
I0307 08:45:24.845818 139894567450368 logging_writer.py:48] [49400] global_step=49400, grad_norm=1.2181005477905273, loss=2.2677531242370605
I0307 08:46:02.989751 139894559057664 logging_writer.py:48] [49500] global_step=49500, grad_norm=1.3209198713302612, loss=2.297023057937622
I0307 08:46:41.644729 139894567450368 logging_writer.py:48] [49600] global_step=49600, grad_norm=1.1713513135910034, loss=2.173475503921509
I0307 08:47:19.846651 139894559057664 logging_writer.py:48] [49700] global_step=49700, grad_norm=1.2878280878067017, loss=2.3195481300354004
I0307 08:47:57.707397 139894567450368 logging_writer.py:48] [49800] global_step=49800, grad_norm=1.3071637153625488, loss=2.344088554382324
I0307 08:48:35.665980 139894559057664 logging_writer.py:48] [49900] global_step=49900, grad_norm=1.241607904434204, loss=2.230499505996704
I0307 08:49:15.072516 139894567450368 logging_writer.py:48] [50000] global_step=50000, grad_norm=1.159147024154663, loss=2.1886496543884277
I0307 08:49:54.459056 139894559057664 logging_writer.py:48] [50100] global_step=50100, grad_norm=1.193468689918518, loss=2.2081868648529053
I0307 08:50:33.954319 139894567450368 logging_writer.py:48] [50200] global_step=50200, grad_norm=1.199344515800476, loss=2.3027124404907227
I0307 08:51:12.556905 139894559057664 logging_writer.py:48] [50300] global_step=50300, grad_norm=1.2647477388381958, loss=2.320330858230591
I0307 08:51:51.088626 139894567450368 logging_writer.py:48] [50400] global_step=50400, grad_norm=1.2953439950942993, loss=2.0862574577331543
I0307 08:52:29.399237 139894559057664 logging_writer.py:48] [50500] global_step=50500, grad_norm=1.234279751777649, loss=2.2426934242248535
I0307 08:53:08.190706 139894567450368 logging_writer.py:48] [50600] global_step=50600, grad_norm=1.3911551237106323, loss=2.198115348815918
I0307 08:53:21.573811 140050941662400 spec.py:321] Evaluating on the training split.
I0307 08:53:33.653389 140050941662400 spec.py:333] Evaluating on the validation split.
I0307 08:53:57.434320 140050941662400 spec.py:349] Evaluating on the test split.
I0307 08:53:59.173372 140050941662400 submission_runner.py:469] Time since start: 21020.27s, 	Step: 50636, 	{'train/accuracy': 0.37236925959587097, 'train/loss': 2.983518600463867, 'validation/accuracy': 0.35429999232292175, 'validation/loss': 3.113117218017578, 'validation/num_examples': 50000, 'test/accuracy': 0.25690001249313354, 'test/loss': 3.925462007522583, 'test/num_examples': 10000, 'score': 19434.668936252594, 'total_duration': 21020.26998066902, 'accumulated_submission_time': 19434.668936252594, 'accumulated_eval_time': 1576.5622019767761, 'accumulated_logging_time': 3.294395685195923}
I0307 08:53:59.263574 139894559057664 logging_writer.py:48] [50636] accumulated_eval_time=1576.56, accumulated_logging_time=3.2944, accumulated_submission_time=19434.7, global_step=50636, preemption_count=0, score=19434.7, test/accuracy=0.2569, test/loss=3.92546, test/num_examples=10000, total_duration=21020.3, train/accuracy=0.372369, train/loss=2.98352, validation/accuracy=0.3543, validation/loss=3.11312, validation/num_examples=50000
I0307 08:54:24.140413 139894567450368 logging_writer.py:48] [50700] global_step=50700, grad_norm=1.1913989782333374, loss=2.3983285427093506
I0307 08:55:02.999953 139894559057664 logging_writer.py:48] [50800] global_step=50800, grad_norm=1.1804367303848267, loss=2.1556520462036133
I0307 08:55:41.524340 139894567450368 logging_writer.py:48] [50900] global_step=50900, grad_norm=1.293273687362671, loss=2.217007875442505
I0307 08:56:20.098373 139894559057664 logging_writer.py:48] [51000] global_step=51000, grad_norm=1.2370012998580933, loss=2.1692159175872803
I0307 08:56:58.696993 139894567450368 logging_writer.py:48] [51100] global_step=51100, grad_norm=1.1926782131195068, loss=2.285778760910034
I0307 08:57:37.361451 139894559057664 logging_writer.py:48] [51200] global_step=51200, grad_norm=1.333595633506775, loss=2.2299976348876953
I0307 08:58:16.711763 139894567450368 logging_writer.py:48] [51300] global_step=51300, grad_norm=1.2541279792785645, loss=2.3007290363311768
I0307 08:58:55.380726 139894559057664 logging_writer.py:48] [51400] global_step=51400, grad_norm=1.2817931175231934, loss=2.2595958709716797
I0307 08:59:34.102139 139894567450368 logging_writer.py:48] [51500] global_step=51500, grad_norm=1.140533685684204, loss=2.2415871620178223
I0307 09:00:12.222729 139894559057664 logging_writer.py:48] [51600] global_step=51600, grad_norm=1.193652868270874, loss=2.1225786209106445
I0307 09:00:50.457710 139894567450368 logging_writer.py:48] [51700] global_step=51700, grad_norm=1.3672468662261963, loss=2.3949575424194336
I0307 09:01:28.780260 139894559057664 logging_writer.py:48] [51800] global_step=51800, grad_norm=1.2114976644515991, loss=2.3255202770233154
I0307 09:02:07.320415 139894567450368 logging_writer.py:48] [51900] global_step=51900, grad_norm=1.231303334236145, loss=2.1628615856170654
I0307 09:02:29.179630 140050941662400 spec.py:321] Evaluating on the training split.
I0307 09:02:41.567973 140050941662400 spec.py:333] Evaluating on the validation split.
I0307 09:03:08.313142 140050941662400 spec.py:349] Evaluating on the test split.
I0307 09:03:10.093816 140050941662400 submission_runner.py:469] Time since start: 21571.19s, 	Step: 51958, 	{'train/accuracy': 0.24587450921535492, 'train/loss': 3.937427282333374, 'validation/accuracy': 0.2337999939918518, 'validation/loss': 4.068167209625244, 'validation/num_examples': 50000, 'test/accuracy': 0.1697000116109848, 'test/loss': 4.802371025085449, 'test/num_examples': 10000, 'score': 19944.413109779358, 'total_duration': 21571.19043803215, 'accumulated_submission_time': 19944.413109779358, 'accumulated_eval_time': 1617.4762258529663, 'accumulated_logging_time': 3.415945291519165}
I0307 09:03:10.174496 139894559057664 logging_writer.py:48] [51958] accumulated_eval_time=1617.48, accumulated_logging_time=3.41595, accumulated_submission_time=19944.4, global_step=51958, preemption_count=0, score=19944.4, test/accuracy=0.1697, test/loss=4.80237, test/num_examples=10000, total_duration=21571.2, train/accuracy=0.245875, train/loss=3.93743, validation/accuracy=0.2338, validation/loss=4.06817, validation/num_examples=50000
I0307 09:03:26.964943 139894567450368 logging_writer.py:48] [52000] global_step=52000, grad_norm=1.1938837766647339, loss=2.3031351566314697
I0307 09:04:05.833121 139894559057664 logging_writer.py:48] [52100] global_step=52100, grad_norm=1.2759454250335693, loss=2.2552340030670166
I0307 09:04:44.316674 139894567450368 logging_writer.py:48] [52200] global_step=52200, grad_norm=1.1329550743103027, loss=2.2224833965301514
I0307 09:05:23.086534 139894559057664 logging_writer.py:48] [52300] global_step=52300, grad_norm=1.1658865213394165, loss=2.2380971908569336
I0307 09:06:01.630236 139894567450368 logging_writer.py:48] [52400] global_step=52400, grad_norm=1.247033715248108, loss=2.397562026977539
I0307 09:06:40.406058 139894559057664 logging_writer.py:48] [52500] global_step=52500, grad_norm=1.270201325416565, loss=2.372476816177368
I0307 09:07:20.200505 139894567450368 logging_writer.py:48] [52600] global_step=52600, grad_norm=1.2400223016738892, loss=2.178501844406128
I0307 09:07:59.089859 139894559057664 logging_writer.py:48] [52700] global_step=52700, grad_norm=1.2384873628616333, loss=2.2857799530029297
I0307 09:08:37.899234 139894567450368 logging_writer.py:48] [52800] global_step=52800, grad_norm=1.4351723194122314, loss=2.3189175128936768
I0307 09:09:16.507051 139894559057664 logging_writer.py:48] [52900] global_step=52900, grad_norm=1.5588505268096924, loss=2.2217276096343994
I0307 09:09:55.261233 139894567450368 logging_writer.py:48] [53000] global_step=53000, grad_norm=1.3045417070388794, loss=2.2989351749420166
I0307 09:10:33.800677 139894559057664 logging_writer.py:48] [53100] global_step=53100, grad_norm=1.2268493175506592, loss=2.132882833480835
I0307 09:11:12.257516 139894567450368 logging_writer.py:48] [53200] global_step=53200, grad_norm=1.2942166328430176, loss=2.296192169189453
I0307 09:11:40.391922 140050941662400 spec.py:321] Evaluating on the training split.
I0307 09:11:52.733114 140050941662400 spec.py:333] Evaluating on the validation split.
I0307 09:12:14.192928 140050941662400 spec.py:349] Evaluating on the test split.
I0307 09:12:15.959187 140050941662400 submission_runner.py:469] Time since start: 22117.06s, 	Step: 53275, 	{'train/accuracy': 0.2149832546710968, 'train/loss': 4.184123516082764, 'validation/accuracy': 0.19365999102592468, 'validation/loss': 4.402884006500244, 'validation/num_examples': 50000, 'test/accuracy': 0.16030000150203705, 'test/loss': 4.8527398109436035, 'test/num_examples': 10000, 'score': 20454.460059404373, 'total_duration': 22117.05579972267, 'accumulated_submission_time': 20454.460059404373, 'accumulated_eval_time': 1653.0433180332184, 'accumulated_logging_time': 3.5210657119750977}
I0307 09:12:16.036740 139894559057664 logging_writer.py:48] [53275] accumulated_eval_time=1653.04, accumulated_logging_time=3.52107, accumulated_submission_time=20454.5, global_step=53275, preemption_count=0, score=20454.5, test/accuracy=0.1603, test/loss=4.85274, test/num_examples=10000, total_duration=22117.1, train/accuracy=0.214983, train/loss=4.18412, validation/accuracy=0.19366, validation/loss=4.40288, validation/num_examples=50000
I0307 09:12:26.219157 139894567450368 logging_writer.py:48] [53300] global_step=53300, grad_norm=1.2938388586044312, loss=2.259737014770508
I0307 09:13:04.454730 139894559057664 logging_writer.py:48] [53400] global_step=53400, grad_norm=1.2693284749984741, loss=2.2214789390563965
I0307 09:13:43.173414 139894567450368 logging_writer.py:48] [53500] global_step=53500, grad_norm=1.223671317100525, loss=2.298576831817627
I0307 09:14:21.739153 139894559057664 logging_writer.py:48] [53600] global_step=53600, grad_norm=1.488084316253662, loss=2.23380184173584
I0307 09:15:00.470534 139894567450368 logging_writer.py:48] [53700] global_step=53700, grad_norm=1.4706482887268066, loss=2.2474682331085205
I0307 09:15:39.691928 139894559057664 logging_writer.py:48] [53800] global_step=53800, grad_norm=1.3074429035186768, loss=2.307429075241089
I0307 09:16:19.303335 139894567450368 logging_writer.py:48] [53900] global_step=53900, grad_norm=1.3696483373641968, loss=2.2806894779205322
I0307 09:16:58.745817 139894559057664 logging_writer.py:48] [54000] global_step=54000, grad_norm=1.1980531215667725, loss=2.213749885559082
I0307 09:17:37.156574 139894567450368 logging_writer.py:48] [54100] global_step=54100, grad_norm=1.398216962814331, loss=2.223567008972168
I0307 09:18:15.240419 139894559057664 logging_writer.py:48] [54200] global_step=54200, grad_norm=1.2014912366867065, loss=2.1651906967163086
I0307 09:18:54.148143 139894567450368 logging_writer.py:48] [54300] global_step=54300, grad_norm=1.1923707723617554, loss=2.1401820182800293
I0307 09:19:32.500622 139894559057664 logging_writer.py:48] [54400] global_step=54400, grad_norm=1.1935598850250244, loss=2.2895326614379883
I0307 09:20:11.009229 139894567450368 logging_writer.py:48] [54500] global_step=54500, grad_norm=1.2550466060638428, loss=2.1921870708465576
I0307 09:20:46.068781 140050941662400 spec.py:321] Evaluating on the training split.
I0307 09:20:58.466869 140050941662400 spec.py:333] Evaluating on the validation split.
I0307 09:21:23.321165 140050941662400 spec.py:349] Evaluating on the test split.
I0307 09:21:25.072436 140050941662400 submission_runner.py:469] Time since start: 22666.17s, 	Step: 54592, 	{'train/accuracy': 0.18975207209587097, 'train/loss': 5.093667030334473, 'validation/accuracy': 0.1624400019645691, 'validation/loss': 5.454000949859619, 'validation/num_examples': 50000, 'test/accuracy': 0.12160000205039978, 'test/loss': 6.084766387939453, 'test/num_examples': 10000, 'score': 20964.308794498444, 'total_duration': 22666.169044733047, 'accumulated_submission_time': 20964.308794498444, 'accumulated_eval_time': 1692.0468056201935, 'accumulated_logging_time': 3.6348958015441895}
I0307 09:21:25.128732 139894559057664 logging_writer.py:48] [54592] accumulated_eval_time=1692.05, accumulated_logging_time=3.6349, accumulated_submission_time=20964.3, global_step=54592, preemption_count=0, score=20964.3, test/accuracy=0.1216, test/loss=6.08477, test/num_examples=10000, total_duration=22666.2, train/accuracy=0.189752, train/loss=5.09367, validation/accuracy=0.16244, validation/loss=5.454, validation/num_examples=50000
I0307 09:21:28.646788 139894567450368 logging_writer.py:48] [54600] global_step=54600, grad_norm=1.1241216659545898, loss=2.282491445541382
I0307 09:22:07.124614 139894559057664 logging_writer.py:48] [54700] global_step=54700, grad_norm=1.2152962684631348, loss=2.1897835731506348
I0307 09:22:45.752900 139894567450368 logging_writer.py:48] [54800] global_step=54800, grad_norm=1.1990004777908325, loss=2.2094197273254395
I0307 09:23:24.005086 139894559057664 logging_writer.py:48] [54900] global_step=54900, grad_norm=1.2923123836517334, loss=2.1979689598083496
I0307 09:24:02.334812 139894567450368 logging_writer.py:48] [55000] global_step=55000, grad_norm=1.3690569400787354, loss=2.3503096103668213
I0307 09:24:41.990060 139894559057664 logging_writer.py:48] [55100] global_step=55100, grad_norm=1.1675634384155273, loss=2.239222764968872
I0307 09:25:21.181794 139894567450368 logging_writer.py:48] [55200] global_step=55200, grad_norm=1.123561978340149, loss=2.1555867195129395
I0307 09:25:59.635608 139894559057664 logging_writer.py:48] [55300] global_step=55300, grad_norm=1.2764570713043213, loss=2.1693506240844727
I0307 09:26:37.945822 139894567450368 logging_writer.py:48] [55400] global_step=55400, grad_norm=1.1510825157165527, loss=2.204540252685547
I0307 09:27:16.415012 139894559057664 logging_writer.py:48] [55500] global_step=55500, grad_norm=1.139998435974121, loss=2.310112476348877
I0307 09:27:54.985418 139894567450368 logging_writer.py:48] [55600] global_step=55600, grad_norm=1.0986884832382202, loss=2.166069746017456
I0307 09:28:33.681303 139894559057664 logging_writer.py:48] [55700] global_step=55700, grad_norm=1.399674654006958, loss=2.1462860107421875
I0307 09:29:12.077878 139894567450368 logging_writer.py:48] [55800] global_step=55800, grad_norm=1.3629859685897827, loss=2.202731132507324
I0307 09:29:50.585349 139894559057664 logging_writer.py:48] [55900] global_step=55900, grad_norm=1.1833863258361816, loss=2.2587943077087402
I0307 09:29:55.211173 140050941662400 spec.py:321] Evaluating on the training split.
I0307 09:30:07.488968 140050941662400 spec.py:333] Evaluating on the validation split.
I0307 09:30:31.750594 140050941662400 spec.py:349] Evaluating on the test split.
I0307 09:30:33.486919 140050941662400 submission_runner.py:469] Time since start: 23214.58s, 	Step: 55913, 	{'train/accuracy': 0.18837690353393555, 'train/loss': 4.547784328460693, 'validation/accuracy': 0.16555999219417572, 'validation/loss': 4.7818603515625, 'validation/num_examples': 50000, 'test/accuracy': 0.12230000644922256, 'test/loss': 5.451502323150635, 'test/num_examples': 10000, 'score': 21474.225752592087, 'total_duration': 23214.583297729492, 'accumulated_submission_time': 21474.225752592087, 'accumulated_eval_time': 1730.3221430778503, 'accumulated_logging_time': 3.7142038345336914}
I0307 09:30:33.583588 139894567450368 logging_writer.py:48] [55913] accumulated_eval_time=1730.32, accumulated_logging_time=3.7142, accumulated_submission_time=21474.2, global_step=55913, preemption_count=0, score=21474.2, test/accuracy=0.1223, test/loss=5.4515, test/num_examples=10000, total_duration=23214.6, train/accuracy=0.188377, train/loss=4.54778, validation/accuracy=0.16556, validation/loss=4.78186, validation/num_examples=50000
I0307 09:31:07.465120 139894559057664 logging_writer.py:48] [56000] global_step=56000, grad_norm=1.292161226272583, loss=2.3473427295684814
I0307 09:31:46.608930 139894567450368 logging_writer.py:48] [56100] global_step=56100, grad_norm=1.28006911277771, loss=2.304962396621704
I0307 09:32:25.318588 139894559057664 logging_writer.py:48] [56200] global_step=56200, grad_norm=1.3078960180282593, loss=2.33290696144104
I0307 09:33:04.842102 139894567450368 logging_writer.py:48] [56300] global_step=56300, grad_norm=1.2830063104629517, loss=2.211318016052246
I0307 09:33:43.785314 139894559057664 logging_writer.py:48] [56400] global_step=56400, grad_norm=1.313116192817688, loss=2.285537004470825
I0307 09:34:22.215165 139894567450368 logging_writer.py:48] [56500] global_step=56500, grad_norm=1.3932242393493652, loss=2.180807113647461
I0307 09:35:00.489919 139894559057664 logging_writer.py:48] [56600] global_step=56600, grad_norm=1.4244053363800049, loss=2.2673635482788086
I0307 09:35:39.137017 139894567450368 logging_writer.py:48] [56700] global_step=56700, grad_norm=1.2745150327682495, loss=2.24525785446167
I0307 09:36:17.471939 139894559057664 logging_writer.py:48] [56800] global_step=56800, grad_norm=1.2344303131103516, loss=2.1463541984558105
I0307 09:36:55.933441 139894567450368 logging_writer.py:48] [56900] global_step=56900, grad_norm=1.2336589097976685, loss=2.2409229278564453
I0307 09:37:34.390144 139894559057664 logging_writer.py:48] [57000] global_step=57000, grad_norm=1.174811601638794, loss=2.2187981605529785
I0307 09:38:12.746495 139894567450368 logging_writer.py:48] [57100] global_step=57100, grad_norm=1.4548884630203247, loss=2.403782367706299
I0307 09:38:50.920061 139894559057664 logging_writer.py:48] [57200] global_step=57200, grad_norm=1.22739839553833, loss=2.1048507690429688
I0307 09:39:03.595534 140050941662400 spec.py:321] Evaluating on the training split.
I0307 09:39:15.703647 140050941662400 spec.py:333] Evaluating on the validation split.
I0307 09:39:41.796017 140050941662400 spec.py:349] Evaluating on the test split.
I0307 09:39:43.583485 140050941662400 submission_runner.py:469] Time since start: 23764.68s, 	Step: 57234, 	{'train/accuracy': 0.13877151906490326, 'train/loss': 5.732127666473389, 'validation/accuracy': 0.12139999866485596, 'validation/loss': 5.944208145141602, 'validation/num_examples': 50000, 'test/accuracy': 0.09410000592470169, 'test/loss': 6.54478645324707, 'test/num_examples': 10000, 'score': 21984.058734178543, 'total_duration': 23764.680055618286, 'accumulated_submission_time': 21984.058734178543, 'accumulated_eval_time': 1770.3098804950714, 'accumulated_logging_time': 3.842374801635742}
I0307 09:39:43.632555 139894567450368 logging_writer.py:48] [57234] accumulated_eval_time=1770.31, accumulated_logging_time=3.84237, accumulated_submission_time=21984.1, global_step=57234, preemption_count=0, score=21984.1, test/accuracy=0.0941, test/loss=6.54479, test/num_examples=10000, total_duration=23764.7, train/accuracy=0.138772, train/loss=5.73213, validation/accuracy=0.1214, validation/loss=5.94421, validation/num_examples=50000
I0307 09:40:09.433059 139894559057664 logging_writer.py:48] [57300] global_step=57300, grad_norm=1.3014030456542969, loss=2.2404682636260986
I0307 09:40:48.211613 139894567450368 logging_writer.py:48] [57400] global_step=57400, grad_norm=1.1931896209716797, loss=2.180325746536255
I0307 09:41:26.933032 139894559057664 logging_writer.py:48] [57500] global_step=57500, grad_norm=1.2611867189407349, loss=2.195796012878418
I0307 09:42:05.566741 139894567450368 logging_writer.py:48] [57600] global_step=57600, grad_norm=1.1820417642593384, loss=2.233438014984131
I0307 09:42:43.831570 139894559057664 logging_writer.py:48] [57700] global_step=57700, grad_norm=1.501562237739563, loss=2.1790506839752197
I0307 09:43:22.668234 139894567450368 logging_writer.py:48] [57800] global_step=57800, grad_norm=1.1789706945419312, loss=2.1719555854797363
I0307 09:44:01.930348 139894559057664 logging_writer.py:48] [57900] global_step=57900, grad_norm=1.2102510929107666, loss=2.1316237449645996
I0307 09:44:40.401198 139894567450368 logging_writer.py:48] [58000] global_step=58000, grad_norm=1.3168476819992065, loss=2.1966471672058105
I0307 09:45:18.800559 139894559057664 logging_writer.py:48] [58100] global_step=58100, grad_norm=1.2588932514190674, loss=2.2897865772247314
I0307 09:45:57.256451 139894567450368 logging_writer.py:48] [58200] global_step=58200, grad_norm=1.2342162132263184, loss=2.1812660694122314
I0307 09:46:35.963918 139894559057664 logging_writer.py:48] [58300] global_step=58300, grad_norm=1.1569873094558716, loss=2.285195827484131
I0307 09:47:14.830836 139894567450368 logging_writer.py:48] [58400] global_step=58400, grad_norm=1.3482472896575928, loss=2.2545664310455322
I0307 09:47:53.223588 139894559057664 logging_writer.py:48] [58500] global_step=58500, grad_norm=1.2593632936477661, loss=2.1369948387145996
I0307 09:48:13.632471 140050941662400 spec.py:321] Evaluating on the training split.
I0307 09:48:25.821782 140050941662400 spec.py:333] Evaluating on the validation split.
I0307 09:48:48.527486 140050941662400 spec.py:349] Evaluating on the test split.
I0307 09:48:50.272382 140050941662400 submission_runner.py:469] Time since start: 24311.37s, 	Step: 58554, 	{'train/accuracy': 0.18496890366077423, 'train/loss': 4.7120232582092285, 'validation/accuracy': 0.17031998932361603, 'validation/loss': 4.847750663757324, 'validation/num_examples': 50000, 'test/accuracy': 0.12400000542402267, 'test/loss': 5.619679927825928, 'test/num_examples': 10000, 'score': 22493.88467645645, 'total_duration': 24311.368948698044, 'accumulated_submission_time': 22493.88467645645, 'accumulated_eval_time': 1806.9495697021484, 'accumulated_logging_time': 3.9172229766845703}
I0307 09:48:50.368101 139894567450368 logging_writer.py:48] [58554] accumulated_eval_time=1806.95, accumulated_logging_time=3.91722, accumulated_submission_time=22493.9, global_step=58554, preemption_count=0, score=22493.9, test/accuracy=0.124, test/loss=5.61968, test/num_examples=10000, total_duration=24311.4, train/accuracy=0.184969, train/loss=4.71202, validation/accuracy=0.17032, validation/loss=4.84775, validation/num_examples=50000
I0307 09:49:08.928073 139894559057664 logging_writer.py:48] [58600] global_step=58600, grad_norm=1.3340394496917725, loss=2.2694671154022217
I0307 09:49:49.026227 139894567450368 logging_writer.py:48] [58700] global_step=58700, grad_norm=1.2527273893356323, loss=2.327254056930542
I0307 09:50:28.396858 139894559057664 logging_writer.py:48] [58800] global_step=58800, grad_norm=1.1726300716400146, loss=2.140320301055908
I0307 09:51:07.525320 139894567450368 logging_writer.py:48] [58900] global_step=58900, grad_norm=1.2423533201217651, loss=2.103947401046753
I0307 09:51:47.035319 139894559057664 logging_writer.py:48] [59000] global_step=59000, grad_norm=1.3601137399673462, loss=2.215128183364868
I0307 09:52:26.847078 139894567450368 logging_writer.py:48] [59100] global_step=59100, grad_norm=1.2676581144332886, loss=2.305492877960205
I0307 09:53:07.055552 139894559057664 logging_writer.py:48] [59200] global_step=59200, grad_norm=1.3450332880020142, loss=2.2756967544555664
I0307 09:53:46.927259 139894567450368 logging_writer.py:48] [59300] global_step=59300, grad_norm=1.1915414333343506, loss=2.1746816635131836
I0307 09:54:26.751978 139894559057664 logging_writer.py:48] [59400] global_step=59400, grad_norm=1.192186951637268, loss=2.196408987045288
I0307 09:55:06.388812 139894567450368 logging_writer.py:48] [59500] global_step=59500, grad_norm=1.246173620223999, loss=2.1908013820648193
I0307 09:55:46.184897 139894559057664 logging_writer.py:48] [59600] global_step=59600, grad_norm=1.3216145038604736, loss=2.2744367122650146
I0307 09:56:37.541957 139894567450368 logging_writer.py:48] [59700] global_step=59700, grad_norm=1.3167502880096436, loss=2.3039662837982178
I0307 09:57:20.294236 140050941662400 spec.py:321] Evaluating on the training split.
I0307 09:57:33.589913 140050941662400 spec.py:333] Evaluating on the validation split.
I0307 09:57:56.295389 140050941662400 spec.py:349] Evaluating on the test split.
I0307 09:57:58.083682 140050941662400 submission_runner.py:469] Time since start: 24859.18s, 	Step: 59740, 	{'train/accuracy': 0.15517377853393555, 'train/loss': 5.4521613121032715, 'validation/accuracy': 0.1453000009059906, 'validation/loss': 5.564300537109375, 'validation/num_examples': 50000, 'test/accuracy': 0.1145000085234642, 'test/loss': 6.183597564697266, 'test/num_examples': 10000, 'score': 23003.635682582855, 'total_duration': 24859.180291175842, 'accumulated_submission_time': 23003.635682582855, 'accumulated_eval_time': 1844.7388427257538, 'accumulated_logging_time': 4.038300514221191}
I0307 09:57:58.147528 139894559057664 logging_writer.py:48] [59740] accumulated_eval_time=1844.74, accumulated_logging_time=4.0383, accumulated_submission_time=23003.6, global_step=59740, preemption_count=0, score=23003.6, test/accuracy=0.1145, test/loss=6.1836, test/num_examples=10000, total_duration=24859.2, train/accuracy=0.155174, train/loss=5.45216, validation/accuracy=0.1453, validation/loss=5.5643, validation/num_examples=50000
I0307 09:58:21.890735 139894567450368 logging_writer.py:48] [59800] global_step=59800, grad_norm=1.349516749382019, loss=2.2914388179779053
I0307 09:59:01.813269 139894559057664 logging_writer.py:48] [59900] global_step=59900, grad_norm=1.3726122379302979, loss=2.2346343994140625
I0307 09:59:41.282999 139894567450368 logging_writer.py:48] [60000] global_step=60000, grad_norm=1.258955955505371, loss=2.2319037914276123
2025-03-07 09:59:55.605969: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 10:00:20.759805 139894559057664 logging_writer.py:48] [60100] global_step=60100, grad_norm=1.2650409936904907, loss=2.182316780090332
I0307 10:01:00.141141 139894567450368 logging_writer.py:48] [60200] global_step=60200, grad_norm=1.2364225387573242, loss=2.143789529800415
I0307 10:01:39.628169 139894559057664 logging_writer.py:48] [60300] global_step=60300, grad_norm=1.4092074632644653, loss=2.096043586730957
I0307 10:02:19.344704 139894567450368 logging_writer.py:48] [60400] global_step=60400, grad_norm=1.2434213161468506, loss=2.188208818435669
I0307 10:02:58.579761 139894559057664 logging_writer.py:48] [60500] global_step=60500, grad_norm=1.2973006963729858, loss=2.1205577850341797
I0307 10:03:38.291383 139894567450368 logging_writer.py:48] [60600] global_step=60600, grad_norm=1.2224067449569702, loss=2.2519357204437256
I0307 10:04:17.627993 139894559057664 logging_writer.py:48] [60700] global_step=60700, grad_norm=1.2535938024520874, loss=2.147886276245117
I0307 10:04:57.358641 139894567450368 logging_writer.py:48] [60800] global_step=60800, grad_norm=1.3150943517684937, loss=2.318934202194214
I0307 10:05:36.847187 139894559057664 logging_writer.py:48] [60900] global_step=60900, grad_norm=1.2466000318527222, loss=2.135838031768799
I0307 10:06:16.225038 139894567450368 logging_writer.py:48] [61000] global_step=61000, grad_norm=1.2171164751052856, loss=2.147077798843384
I0307 10:06:28.250837 140050941662400 spec.py:321] Evaluating on the training split.
I0307 10:06:41.076102 140050941662400 spec.py:333] Evaluating on the validation split.
I0307 10:07:06.702266 140050941662400 spec.py:349] Evaluating on the test split.
I0307 10:07:08.437318 140050941662400 submission_runner.py:469] Time since start: 25409.53s, 	Step: 61031, 	{'train/accuracy': 0.20531727373600006, 'train/loss': 4.719232082366943, 'validation/accuracy': 0.19779999554157257, 'validation/loss': 4.821146011352539, 'validation/num_examples': 50000, 'test/accuracy': 0.14430001378059387, 'test/loss': 5.658450126647949, 'test/num_examples': 10000, 'score': 23513.539635419846, 'total_duration': 25409.53394317627, 'accumulated_submission_time': 23513.539635419846, 'accumulated_eval_time': 1884.925163269043, 'accumulated_logging_time': 4.141346216201782}
I0307 10:07:08.492028 139894559057664 logging_writer.py:48] [61031] accumulated_eval_time=1884.93, accumulated_logging_time=4.14135, accumulated_submission_time=23513.5, global_step=61031, preemption_count=0, score=23513.5, test/accuracy=0.1443, test/loss=5.65845, test/num_examples=10000, total_duration=25409.5, train/accuracy=0.205317, train/loss=4.71923, validation/accuracy=0.1978, validation/loss=4.82115, validation/num_examples=50000
I0307 10:07:36.307244 139894567450368 logging_writer.py:48] [61100] global_step=61100, grad_norm=1.4930174350738525, loss=2.3016481399536133
I0307 10:08:15.630896 139894559057664 logging_writer.py:48] [61200] global_step=61200, grad_norm=1.2751373052597046, loss=2.1794466972351074
I0307 10:08:55.145721 139894567450368 logging_writer.py:48] [61300] global_step=61300, grad_norm=1.2659469842910767, loss=2.306842803955078
I0307 10:09:35.265129 139894559057664 logging_writer.py:48] [61400] global_step=61400, grad_norm=1.3892569541931152, loss=2.3146920204162598
I0307 10:10:14.323073 139894567450368 logging_writer.py:48] [61500] global_step=61500, grad_norm=1.196533203125, loss=2.2477011680603027
I0307 10:10:53.754742 139894559057664 logging_writer.py:48] [61600] global_step=61600, grad_norm=1.2236425876617432, loss=2.186100959777832
I0307 10:11:33.691935 139894567450368 logging_writer.py:48] [61700] global_step=61700, grad_norm=1.5389904975891113, loss=2.1799840927124023
I0307 10:12:13.217983 139894559057664 logging_writer.py:48] [61800] global_step=61800, grad_norm=1.225013017654419, loss=2.3093326091766357
I0307 10:12:52.755265 139894567450368 logging_writer.py:48] [61900] global_step=61900, grad_norm=1.2703466415405273, loss=2.1214728355407715
I0307 10:13:32.296481 139894559057664 logging_writer.py:48] [62000] global_step=62000, grad_norm=1.3566079139709473, loss=2.19264554977417
I0307 10:14:11.917298 139894567450368 logging_writer.py:48] [62100] global_step=62100, grad_norm=1.2946619987487793, loss=2.2434661388397217
I0307 10:14:51.887282 139894559057664 logging_writer.py:48] [62200] global_step=62200, grad_norm=1.290554165840149, loss=2.3396079540252686
I0307 10:15:31.622933 139894567450368 logging_writer.py:48] [62300] global_step=62300, grad_norm=1.288862705230713, loss=2.1930742263793945
I0307 10:15:38.691156 140050941662400 spec.py:321] Evaluating on the training split.
I0307 10:15:51.206477 140050941662400 spec.py:333] Evaluating on the validation split.
I0307 10:16:13.489852 140050941662400 spec.py:349] Evaluating on the test split.
I0307 10:16:15.222223 140050941662400 submission_runner.py:469] Time since start: 25956.32s, 	Step: 62319, 	{'train/accuracy': 0.0460379458963871, 'train/loss': 7.943126678466797, 'validation/accuracy': 0.04471999779343605, 'validation/loss': 8.036075592041016, 'validation/num_examples': 50000, 'test/accuracy': 0.031700000166893005, 'test/loss': 8.588732719421387, 'test/num_examples': 10000, 'score': 24023.54101920128, 'total_duration': 25956.318823814392, 'accumulated_submission_time': 24023.54101920128, 'accumulated_eval_time': 1921.456042289734, 'accumulated_logging_time': 4.233725070953369}
I0307 10:16:15.336330 139894559057664 logging_writer.py:48] [62319] accumulated_eval_time=1921.46, accumulated_logging_time=4.23373, accumulated_submission_time=24023.5, global_step=62319, preemption_count=0, score=24023.5, test/accuracy=0.0317, test/loss=8.58873, test/num_examples=10000, total_duration=25956.3, train/accuracy=0.0460379, train/loss=7.94313, validation/accuracy=0.04472, validation/loss=8.03608, validation/num_examples=50000
I0307 10:16:47.907268 139894567450368 logging_writer.py:48] [62400] global_step=62400, grad_norm=1.3062138557434082, loss=2.21787166595459
I0307 10:17:27.901341 139894559057664 logging_writer.py:48] [62500] global_step=62500, grad_norm=1.3196802139282227, loss=2.2749087810516357
I0307 10:18:06.827387 139894567450368 logging_writer.py:48] [62600] global_step=62600, grad_norm=1.4138903617858887, loss=2.1223700046539307
I0307 10:18:44.834806 139894559057664 logging_writer.py:48] [62700] global_step=62700, grad_norm=1.2688688039779663, loss=2.2290878295898438
I0307 10:19:23.617090 139894567450368 logging_writer.py:48] [62800] global_step=62800, grad_norm=1.3786365985870361, loss=2.204399585723877
I0307 10:20:02.428721 139894559057664 logging_writer.py:48] [62900] global_step=62900, grad_norm=1.2854994535446167, loss=2.174036979675293
I0307 10:20:39.714692 139894567450368 logging_writer.py:48] [63000] global_step=63000, grad_norm=1.234081506729126, loss=2.148439884185791
I0307 10:21:17.693537 139894559057664 logging_writer.py:48] [63100] global_step=63100, grad_norm=1.2544581890106201, loss=2.3398194313049316
I0307 10:21:55.472016 139894567450368 logging_writer.py:48] [63200] global_step=63200, grad_norm=1.1844639778137207, loss=2.1652801036834717
I0307 10:22:35.104875 139894559057664 logging_writer.py:48] [63300] global_step=63300, grad_norm=1.5264865159988403, loss=2.275851249694824
I0307 10:23:14.292933 139894567450368 logging_writer.py:48] [63400] global_step=63400, grad_norm=1.2614595890045166, loss=2.270565986633301
I0307 10:23:53.788095 139894559057664 logging_writer.py:48] [63500] global_step=63500, grad_norm=1.3539793491363525, loss=2.1565892696380615
I0307 10:24:32.945572 139894567450368 logging_writer.py:48] [63600] global_step=63600, grad_norm=1.2219599485397339, loss=2.184731960296631
I0307 10:24:45.491913 140050941662400 spec.py:321] Evaluating on the training split.
I0307 10:24:57.759898 140050941662400 spec.py:333] Evaluating on the validation split.
I0307 10:25:20.400331 140050941662400 spec.py:349] Evaluating on the test split.
I0307 10:25:22.173402 140050941662400 submission_runner.py:469] Time since start: 26503.27s, 	Step: 63632, 	{'train/accuracy': 0.3078164756298065, 'train/loss': 3.433074951171875, 'validation/accuracy': 0.28933998942375183, 'validation/loss': 3.587611675262451, 'validation/num_examples': 50000, 'test/accuracy': 0.21560001373291016, 'test/loss': 4.357187271118164, 'test/num_examples': 10000, 'score': 24533.494884490967, 'total_duration': 26503.26997280121, 'accumulated_submission_time': 24533.494884490967, 'accumulated_eval_time': 1958.1373147964478, 'accumulated_logging_time': 4.3803229331970215}
I0307 10:25:22.242347 139894559057664 logging_writer.py:48] [63632] accumulated_eval_time=1958.14, accumulated_logging_time=4.38032, accumulated_submission_time=24533.5, global_step=63632, preemption_count=0, score=24533.5, test/accuracy=0.2156, test/loss=4.35719, test/num_examples=10000, total_duration=26503.3, train/accuracy=0.307816, train/loss=3.43307, validation/accuracy=0.28934, validation/loss=3.58761, validation/num_examples=50000
I0307 10:25:49.852992 139894567450368 logging_writer.py:48] [63700] global_step=63700, grad_norm=1.4139800071716309, loss=2.2964577674865723
I0307 10:27:01.086944 139894559057664 logging_writer.py:48] [63800] global_step=63800, grad_norm=1.3232171535491943, loss=2.216156005859375
I0307 10:27:50.432632 139894567450368 logging_writer.py:48] [63900] global_step=63900, grad_norm=1.257733702659607, loss=2.24727463722229
I0307 10:28:30.727982 139894559057664 logging_writer.py:48] [64000] global_step=64000, grad_norm=1.2726383209228516, loss=2.249880790710449
I0307 10:29:11.065456 139894567450368 logging_writer.py:48] [64100] global_step=64100, grad_norm=1.2201364040374756, loss=2.077108144760132
I0307 10:29:54.267547 139894559057664 logging_writer.py:48] [64200] global_step=64200, grad_norm=1.2315857410430908, loss=2.3045589923858643
I0307 10:30:34.296016 139894567450368 logging_writer.py:48] [64300] global_step=64300, grad_norm=1.254176378250122, loss=2.302578926086426
I0307 10:31:14.657559 139894559057664 logging_writer.py:48] [64400] global_step=64400, grad_norm=1.3054757118225098, loss=2.0922553539276123
I0307 10:31:55.043441 139894567450368 logging_writer.py:48] [64500] global_step=64500, grad_norm=1.4313606023788452, loss=2.1181583404541016
I0307 10:32:34.287668 139894559057664 logging_writer.py:48] [64600] global_step=64600, grad_norm=1.2864642143249512, loss=2.1979336738586426
I0307 10:33:13.986578 139894567450368 logging_writer.py:48] [64700] global_step=64700, grad_norm=1.2963305711746216, loss=2.0572516918182373
I0307 10:33:52.429010 140050941662400 spec.py:321] Evaluating on the training split.
I0307 10:34:04.567351 140050941662400 spec.py:333] Evaluating on the validation split.
I0307 10:34:29.438851 140050941662400 spec.py:349] Evaluating on the test split.
I0307 10:34:31.203170 140050941662400 submission_runner.py:469] Time since start: 27052.30s, 	Step: 64797, 	{'train/accuracy': 0.24125079810619354, 'train/loss': 4.432473659515381, 'validation/accuracy': 0.2279599905014038, 'validation/loss': 4.5382304191589355, 'validation/num_examples': 50000, 'test/accuracy': 0.16600000858306885, 'test/loss': 5.430295467376709, 'test/num_examples': 10000, 'score': 25043.484233379364, 'total_duration': 27052.299745321274, 'accumulated_submission_time': 25043.484233379364, 'accumulated_eval_time': 1996.9112660884857, 'accumulated_logging_time': 4.504631280899048}
I0307 10:34:31.262627 139894559057664 logging_writer.py:48] [64797] accumulated_eval_time=1996.91, accumulated_logging_time=4.50463, accumulated_submission_time=25043.5, global_step=64797, preemption_count=0, score=25043.5, test/accuracy=0.166, test/loss=5.4303, test/num_examples=10000, total_duration=27052.3, train/accuracy=0.241251, train/loss=4.43247, validation/accuracy=0.22796, validation/loss=4.53823, validation/num_examples=50000
I0307 10:34:32.842587 139894567450368 logging_writer.py:48] [64800] global_step=64800, grad_norm=1.1737680435180664, loss=2.1260087490081787
I0307 10:35:12.995604 139894559057664 logging_writer.py:48] [64900] global_step=64900, grad_norm=1.371490478515625, loss=2.124340295791626
I0307 10:35:53.000711 139894567450368 logging_writer.py:48] [65000] global_step=65000, grad_norm=1.3796025514602661, loss=2.2471249103546143
2025-03-07 10:36:08.296748: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 10:36:32.745800 139894559057664 logging_writer.py:48] [65100] global_step=65100, grad_norm=1.2245886325836182, loss=2.195171594619751
I0307 10:37:12.775246 139894567450368 logging_writer.py:48] [65200] global_step=65200, grad_norm=1.354882001876831, loss=2.1360201835632324
I0307 10:37:52.606312 139894559057664 logging_writer.py:48] [65300] global_step=65300, grad_norm=1.431118369102478, loss=2.2782928943634033
I0307 10:38:32.692653 139894567450368 logging_writer.py:48] [65400] global_step=65400, grad_norm=1.4843260049819946, loss=2.1715729236602783
I0307 10:39:12.575169 139894559057664 logging_writer.py:48] [65500] global_step=65500, grad_norm=1.3250092267990112, loss=2.151655912399292
I0307 10:39:52.340464 139894567450368 logging_writer.py:48] [65600] global_step=65600, grad_norm=1.3422117233276367, loss=2.211982488632202
I0307 10:41:05.294989 139894559057664 logging_writer.py:48] [65700] global_step=65700, grad_norm=1.6013494729995728, loss=2.085331678390503
I0307 10:41:52.440940 139894567450368 logging_writer.py:48] [65800] global_step=65800, grad_norm=1.2693414688110352, loss=2.064291477203369
I0307 10:42:32.750096 139894559057664 logging_writer.py:48] [65900] global_step=65900, grad_norm=1.39312744140625, loss=2.2280256748199463
I0307 10:43:01.445488 140050941662400 spec.py:321] Evaluating on the training split.
I0307 10:43:13.600288 140050941662400 spec.py:333] Evaluating on the validation split.
I0307 10:43:36.208771 140050941662400 spec.py:349] Evaluating on the test split.
I0307 10:43:37.957268 140050941662400 submission_runner.py:469] Time since start: 27599.05s, 	Step: 65972, 	{'train/accuracy': 0.16557715833187103, 'train/loss': 4.766014099121094, 'validation/accuracy': 0.15997999906539917, 'validation/loss': 4.814699649810791, 'validation/num_examples': 50000, 'test/accuracy': 0.10930000245571136, 'test/loss': 5.580372333526611, 'test/num_examples': 10000, 'score': 25553.503398895264, 'total_duration': 27599.053845643997, 'accumulated_submission_time': 25553.503398895264, 'accumulated_eval_time': 2033.4228341579437, 'accumulated_logging_time': 4.587366104125977}
I0307 10:43:38.052291 139894567450368 logging_writer.py:48] [65972] accumulated_eval_time=2033.42, accumulated_logging_time=4.58737, accumulated_submission_time=25553.5, global_step=65972, preemption_count=0, score=25553.5, test/accuracy=0.1093, test/loss=5.58037, test/num_examples=10000, total_duration=27599.1, train/accuracy=0.165577, train/loss=4.76601, validation/accuracy=0.15998, validation/loss=4.8147, validation/num_examples=50000
I0307 10:43:49.596305 139894559057664 logging_writer.py:48] [66000] global_step=66000, grad_norm=1.3560460805892944, loss=2.1430327892303467
I0307 10:44:29.270763 139894567450368 logging_writer.py:48] [66100] global_step=66100, grad_norm=1.4916638135910034, loss=2.2755136489868164
I0307 10:45:09.342872 139894559057664 logging_writer.py:48] [66200] global_step=66200, grad_norm=1.3095850944519043, loss=2.190652847290039
I0307 10:45:49.400368 139894567450368 logging_writer.py:48] [66300] global_step=66300, grad_norm=1.3355894088745117, loss=2.3565967082977295
I0307 10:46:29.055926 139894559057664 logging_writer.py:48] [66400] global_step=66400, grad_norm=1.3626857995986938, loss=2.217207670211792
I0307 10:47:08.915612 139894567450368 logging_writer.py:48] [66500] global_step=66500, grad_norm=1.334877848625183, loss=2.103023052215576
I0307 10:47:49.061561 139894559057664 logging_writer.py:48] [66600] global_step=66600, grad_norm=1.4456372261047363, loss=2.118980646133423
I0307 10:48:29.057355 139894567450368 logging_writer.py:48] [66700] global_step=66700, grad_norm=1.2854050397872925, loss=2.1256234645843506
I0307 10:49:13.869580 139894559057664 logging_writer.py:48] [66800] global_step=66800, grad_norm=1.2693464756011963, loss=2.0702548027038574
I0307 10:49:53.495568 139894567450368 logging_writer.py:48] [66900] global_step=66900, grad_norm=1.5215222835540771, loss=2.0592041015625
I0307 10:50:35.058650 139894559057664 logging_writer.py:48] [67000] global_step=67000, grad_norm=1.4078000783920288, loss=2.1862611770629883
I0307 10:51:15.864704 139894567450368 logging_writer.py:48] [67100] global_step=67100, grad_norm=1.3103381395339966, loss=2.1986212730407715
I0307 10:52:02.109542 139894559057664 logging_writer.py:48] [67200] global_step=67200, grad_norm=1.3751989603042603, loss=2.328040599822998
I0307 10:52:07.970537 140050941662400 spec.py:321] Evaluating on the training split.
I0307 10:52:20.484762 140050941662400 spec.py:333] Evaluating on the validation split.
I0307 10:52:40.083192 140050941662400 spec.py:349] Evaluating on the test split.
I0307 10:52:41.939745 140050941662400 submission_runner.py:469] Time since start: 28143.04s, 	Step: 67214, 	{'train/accuracy': 0.15351960062980652, 'train/loss': 5.711238861083984, 'validation/accuracy': 0.14441999793052673, 'validation/loss': 5.792862415313721, 'validation/num_examples': 50000, 'test/accuracy': 0.10620000213384628, 'test/loss': 6.442117691040039, 'test/num_examples': 10000, 'score': 26063.22305393219, 'total_duration': 28143.036039590836, 'accumulated_submission_time': 26063.22305393219, 'accumulated_eval_time': 2067.3915452957153, 'accumulated_logging_time': 4.727046966552734}
I0307 10:52:42.113486 139894567450368 logging_writer.py:48] [67214] accumulated_eval_time=2067.39, accumulated_logging_time=4.72705, accumulated_submission_time=26063.2, global_step=67214, preemption_count=0, score=26063.2, test/accuracy=0.1062, test/loss=6.44212, test/num_examples=10000, total_duration=28143, train/accuracy=0.15352, train/loss=5.71124, validation/accuracy=0.14442, validation/loss=5.79286, validation/num_examples=50000
I0307 10:53:17.530425 139894559057664 logging_writer.py:48] [67300] global_step=67300, grad_norm=1.1999837160110474, loss=2.193183660507202
I0307 10:53:59.828189 139894567450368 logging_writer.py:48] [67400] global_step=67400, grad_norm=1.2771965265274048, loss=2.1142995357513428
I0307 10:54:42.698876 139894559057664 logging_writer.py:48] [67500] global_step=67500, grad_norm=1.5443254709243774, loss=2.2742178440093994
I0307 10:55:30.555871 139894567450368 logging_writer.py:48] [67600] global_step=67600, grad_norm=1.316444754600525, loss=2.219109535217285
I0307 10:56:10.959339 139894559057664 logging_writer.py:48] [67700] global_step=67700, grad_norm=1.3965364694595337, loss=2.07476544380188
I0307 10:56:53.057995 139894567450368 logging_writer.py:48] [67800] global_step=67800, grad_norm=1.341259479522705, loss=2.0773348808288574
I0307 10:57:37.735409 139894559057664 logging_writer.py:48] [67900] global_step=67900, grad_norm=1.349061369895935, loss=2.135352611541748
I0307 10:58:24.559089 139894567450368 logging_writer.py:48] [68000] global_step=68000, grad_norm=1.3778274059295654, loss=2.3079795837402344
I0307 10:59:08.446200 139894559057664 logging_writer.py:48] [68100] global_step=68100, grad_norm=1.3835951089859009, loss=2.300205707550049
I0307 10:59:48.765802 139894567450368 logging_writer.py:48] [68200] global_step=68200, grad_norm=1.3593213558197021, loss=2.108264207839966
I0307 11:00:28.699615 139894559057664 logging_writer.py:48] [68300] global_step=68300, grad_norm=1.317816138267517, loss=2.1766602993011475
I0307 11:01:12.009408 140050941662400 spec.py:321] Evaluating on the training split.
I0307 11:01:24.470696 140050941662400 spec.py:333] Evaluating on the validation split.
I0307 11:01:44.894680 140050941662400 spec.py:349] Evaluating on the test split.
I0307 11:01:46.682371 140050941662400 submission_runner.py:469] Time since start: 28687.78s, 	Step: 68387, 	{'train/accuracy': 0.20952247083187103, 'train/loss': 4.308839797973633, 'validation/accuracy': 0.19845999777317047, 'validation/loss': 4.430704116821289, 'validation/num_examples': 50000, 'test/accuracy': 0.1503000110387802, 'test/loss': 5.009849548339844, 'test/num_examples': 10000, 'score': 26572.947857618332, 'total_duration': 28687.77893590927, 'accumulated_submission_time': 26572.947857618332, 'accumulated_eval_time': 2102.0642869472504, 'accumulated_logging_time': 4.9262425899505615}
I0307 11:01:46.851382 139894567450368 logging_writer.py:48] [68387] accumulated_eval_time=2102.06, accumulated_logging_time=4.92624, accumulated_submission_time=26572.9, global_step=68387, preemption_count=0, score=26572.9, test/accuracy=0.1503, test/loss=5.00985, test/num_examples=10000, total_duration=28687.8, train/accuracy=0.209522, train/loss=4.30884, validation/accuracy=0.19846, validation/loss=4.4307, validation/num_examples=50000
I0307 11:01:52.387788 139894559057664 logging_writer.py:48] [68400] global_step=68400, grad_norm=1.410339117050171, loss=2.18003511428833
I0307 11:02:31.626096 139894567450368 logging_writer.py:48] [68500] global_step=68500, grad_norm=1.2776120901107788, loss=2.0513968467712402
I0307 11:03:22.025362 139894559057664 logging_writer.py:48] [68600] global_step=68600, grad_norm=1.3888565301895142, loss=2.2011685371398926
I0307 11:04:09.236698 139894567450368 logging_writer.py:48] [68700] global_step=68700, grad_norm=1.2528605461120605, loss=2.0808870792388916
I0307 11:04:49.963662 139894559057664 logging_writer.py:48] [68800] global_step=68800, grad_norm=1.3329652547836304, loss=2.174792766571045
I0307 11:05:28.940454 139894567450368 logging_writer.py:48] [68900] global_step=68900, grad_norm=1.3690457344055176, loss=2.1386518478393555
I0307 11:06:08.499909 139894559057664 logging_writer.py:48] [69000] global_step=69000, grad_norm=1.3692877292633057, loss=2.2800533771514893
I0307 11:06:48.259471 139894567450368 logging_writer.py:48] [69100] global_step=69100, grad_norm=1.4404575824737549, loss=2.1467795372009277
I0307 11:07:27.763793 139894559057664 logging_writer.py:48] [69200] global_step=69200, grad_norm=1.3290150165557861, loss=2.212244749069214
I0307 11:08:07.374192 139894567450368 logging_writer.py:48] [69300] global_step=69300, grad_norm=1.2756091356277466, loss=2.2651901245117188
I0307 11:08:47.121747 139894559057664 logging_writer.py:48] [69400] global_step=69400, grad_norm=1.3737258911132812, loss=2.167963743209839
I0307 11:09:26.914800 139894567450368 logging_writer.py:48] [69500] global_step=69500, grad_norm=1.3833718299865723, loss=2.1036956310272217
I0307 11:10:06.574100 139894559057664 logging_writer.py:48] [69600] global_step=69600, grad_norm=1.3802499771118164, loss=2.0385959148406982
I0307 11:10:16.877523 140050941662400 spec.py:321] Evaluating on the training split.
I0307 11:10:29.156371 140050941662400 spec.py:333] Evaluating on the validation split.
I0307 11:10:56.012041 140050941662400 spec.py:349] Evaluating on the test split.
I0307 11:10:57.744068 140050941662400 submission_runner.py:469] Time since start: 29238.84s, 	Step: 69627, 	{'train/accuracy': 0.2120535671710968, 'train/loss': 4.696142196655273, 'validation/accuracy': 0.20768000185489655, 'validation/loss': 4.728293418884277, 'validation/num_examples': 50000, 'test/accuracy': 0.14900000393390656, 'test/loss': 5.663735866546631, 'test/num_examples': 10000, 'score': 27082.757184505463, 'total_duration': 29238.840626955032, 'accumulated_submission_time': 27082.757184505463, 'accumulated_eval_time': 2142.9305984973907, 'accumulated_logging_time': 5.160403728485107}
I0307 11:10:57.811576 139894567450368 logging_writer.py:48] [69627] accumulated_eval_time=2142.93, accumulated_logging_time=5.1604, accumulated_submission_time=27082.8, global_step=69627, preemption_count=0, score=27082.8, test/accuracy=0.149, test/loss=5.66374, test/num_examples=10000, total_duration=29238.8, train/accuracy=0.212054, train/loss=4.69614, validation/accuracy=0.20768, validation/loss=4.72829, validation/num_examples=50000
I0307 11:11:36.441404 139894559057664 logging_writer.py:48] [69700] global_step=69700, grad_norm=1.226419448852539, loss=2.1500964164733887
I0307 11:12:16.216915 139894567450368 logging_writer.py:48] [69800] global_step=69800, grad_norm=1.4206429719924927, loss=2.1391215324401855
I0307 11:12:56.185637 139894559057664 logging_writer.py:48] [69900] global_step=69900, grad_norm=1.219199538230896, loss=2.040290117263794
I0307 11:13:34.899566 139894567450368 logging_writer.py:48] [70000] global_step=70000, grad_norm=1.2603117227554321, loss=2.132871150970459
I0307 11:14:16.293678 139894559057664 logging_writer.py:48] [70100] global_step=70100, grad_norm=1.4041215181350708, loss=2.171807289123535
I0307 11:14:54.357825 139894567450368 logging_writer.py:48] [70200] global_step=70200, grad_norm=1.3339645862579346, loss=2.251530170440674
I0307 11:15:32.756928 139894559057664 logging_writer.py:48] [70300] global_step=70300, grad_norm=1.4744714498519897, loss=2.2538809776306152
I0307 11:16:11.647018 139894567450368 logging_writer.py:48] [70400] global_step=70400, grad_norm=1.36495840549469, loss=2.1363630294799805
I0307 11:16:50.284797 139894559057664 logging_writer.py:48] [70500] global_step=70500, grad_norm=1.4094457626342773, loss=2.1701321601867676
I0307 11:17:28.894117 139894567450368 logging_writer.py:48] [70600] global_step=70600, grad_norm=1.3245755434036255, loss=2.113656997680664
I0307 11:18:08.140440 139894559057664 logging_writer.py:48] [70700] global_step=70700, grad_norm=1.3122999668121338, loss=1.984817385673523
I0307 11:18:46.226578 139894567450368 logging_writer.py:48] [70800] global_step=70800, grad_norm=1.3224972486495972, loss=2.216885566711426
I0307 11:19:25.358023 139894559057664 logging_writer.py:48] [70900] global_step=70900, grad_norm=1.4220434427261353, loss=2.1361613273620605
I0307 11:19:28.018810 140050941662400 spec.py:321] Evaluating on the training split.
I0307 11:19:40.789893 140050941662400 spec.py:333] Evaluating on the validation split.
I0307 11:20:04.486484 140050941662400 spec.py:349] Evaluating on the test split.
I0307 11:20:06.268711 140050941662400 submission_runner.py:469] Time since start: 29787.37s, 	Step: 70908, 	{'train/accuracy': 0.35726243257522583, 'train/loss': 3.080620527267456, 'validation/accuracy': 0.33701997995376587, 'validation/loss': 3.2168705463409424, 'validation/num_examples': 50000, 'test/accuracy': 0.24370001256465912, 'test/loss': 4.009713649749756, 'test/num_examples': 10000, 'score': 27592.759934663773, 'total_duration': 29787.365347623825, 'accumulated_submission_time': 27592.759934663773, 'accumulated_eval_time': 2181.1803455352783, 'accumulated_logging_time': 5.276264905929565}
I0307 11:20:06.386592 139894567450368 logging_writer.py:48] [70908] accumulated_eval_time=2181.18, accumulated_logging_time=5.27626, accumulated_submission_time=27592.8, global_step=70908, preemption_count=0, score=27592.8, test/accuracy=0.2437, test/loss=4.00971, test/num_examples=10000, total_duration=29787.4, train/accuracy=0.357262, train/loss=3.08062, validation/accuracy=0.33702, validation/loss=3.21687, validation/num_examples=50000
I0307 11:20:42.708683 139894559057664 logging_writer.py:48] [71000] global_step=71000, grad_norm=1.2159638404846191, loss=2.1444356441497803
I0307 11:21:22.641708 139894567450368 logging_writer.py:48] [71100] global_step=71100, grad_norm=1.2972798347473145, loss=2.0645318031311035
I0307 11:22:07.490680 139894559057664 logging_writer.py:48] [71200] global_step=71200, grad_norm=1.3589324951171875, loss=2.207468032836914
I0307 11:22:49.383277 139894567450368 logging_writer.py:48] [71300] global_step=71300, grad_norm=1.3168872594833374, loss=2.0685031414031982
I0307 11:23:28.034832 139894559057664 logging_writer.py:48] [71400] global_step=71400, grad_norm=1.4467029571533203, loss=2.204792022705078
I0307 11:24:07.705549 139894567450368 logging_writer.py:48] [71500] global_step=71500, grad_norm=1.3277755975723267, loss=2.14412522315979
I0307 11:24:47.616671 139894559057664 logging_writer.py:48] [71600] global_step=71600, grad_norm=1.3127074241638184, loss=2.1599340438842773
I0307 11:25:27.278953 139894567450368 logging_writer.py:48] [71700] global_step=71700, grad_norm=1.4545910358428955, loss=2.1481516361236572
I0307 11:26:06.728049 139894559057664 logging_writer.py:48] [71800] global_step=71800, grad_norm=1.3830684423446655, loss=2.1196038722991943
I0307 11:26:46.539767 139894567450368 logging_writer.py:48] [71900] global_step=71900, grad_norm=1.3254575729370117, loss=2.1804556846618652
I0307 11:27:26.415281 139894559057664 logging_writer.py:48] [72000] global_step=72000, grad_norm=1.3959101438522339, loss=2.2114744186401367
I0307 11:28:05.934264 139894567450368 logging_writer.py:48] [72100] global_step=72100, grad_norm=1.4027879238128662, loss=2.0775957107543945
I0307 11:28:36.621387 140050941662400 spec.py:321] Evaluating on the training split.
I0307 11:28:49.707187 140050941662400 spec.py:333] Evaluating on the validation split.
I0307 11:29:11.482052 140050941662400 spec.py:349] Evaluating on the test split.
I0307 11:29:13.251723 140050941662400 submission_runner.py:469] Time since start: 30334.35s, 	Step: 72179, 	{'train/accuracy': 0.21033960580825806, 'train/loss': 4.6485748291015625, 'validation/accuracy': 0.1946999877691269, 'validation/loss': 4.786005973815918, 'validation/num_examples': 50000, 'test/accuracy': 0.13990001380443573, 'test/loss': 5.396419048309326, 'test/num_examples': 10000, 'score': 28102.796191453934, 'total_duration': 30334.34835910797, 'accumulated_submission_time': 28102.796191453934, 'accumulated_eval_time': 2217.810533285141, 'accumulated_logging_time': 5.437184810638428}
I0307 11:29:13.331994 139894559057664 logging_writer.py:48] [72179] accumulated_eval_time=2217.81, accumulated_logging_time=5.43718, accumulated_submission_time=28102.8, global_step=72179, preemption_count=0, score=28102.8, test/accuracy=0.1399, test/loss=5.39642, test/num_examples=10000, total_duration=30334.3, train/accuracy=0.21034, train/loss=4.64857, validation/accuracy=0.1947, validation/loss=4.78601, validation/num_examples=50000
I0307 11:29:22.325103 139894567450368 logging_writer.py:48] [72200] global_step=72200, grad_norm=1.2955673933029175, loss=2.1376352310180664
I0307 11:30:03.619252 139894559057664 logging_writer.py:48] [72300] global_step=72300, grad_norm=1.3898566961288452, loss=2.2522077560424805
I0307 11:30:56.776188 139894567450368 logging_writer.py:48] [72400] global_step=72400, grad_norm=1.4129358530044556, loss=2.087242364883423
I0307 11:31:43.858835 139894559057664 logging_writer.py:48] [72500] global_step=72500, grad_norm=1.3172760009765625, loss=1.9970567226409912
I0307 11:33:05.608080 139894567450368 logging_writer.py:48] [72600] global_step=72600, grad_norm=1.4759889841079712, loss=2.2761054039001465
I0307 11:33:52.421832 139894559057664 logging_writer.py:48] [72700] global_step=72700, grad_norm=1.4414693117141724, loss=2.216675043106079
I0307 11:34:32.481590 139894567450368 logging_writer.py:48] [72800] global_step=72800, grad_norm=1.3985947370529175, loss=2.058201551437378
I0307 11:35:13.186183 139894559057664 logging_writer.py:48] [72900] global_step=72900, grad_norm=1.4205889701843262, loss=2.1488842964172363
I0307 11:35:52.189902 139894567450368 logging_writer.py:48] [73000] global_step=73000, grad_norm=1.388178825378418, loss=2.066124200820923
I0307 11:36:31.270097 139894559057664 logging_writer.py:48] [73100] global_step=73100, grad_norm=1.3497123718261719, loss=2.209174633026123
I0307 11:37:10.194303 139894567450368 logging_writer.py:48] [73200] global_step=73200, grad_norm=1.456436038017273, loss=2.091299533843994
I0307 11:37:43.562643 140050941662400 spec.py:321] Evaluating on the training split.
I0307 11:37:56.418376 140050941662400 spec.py:333] Evaluating on the validation split.
I0307 11:38:19.598852 140050941662400 spec.py:349] Evaluating on the test split.
I0307 11:38:21.340523 140050941662400 submission_runner.py:469] Time since start: 30882.44s, 	Step: 73287, 	{'train/accuracy': 0.21671715378761292, 'train/loss': 4.643174171447754, 'validation/accuracy': 0.20819999277591705, 'validation/loss': 4.7156758308410645, 'validation/num_examples': 50000, 'test/accuracy': 0.14160001277923584, 'test/loss': 5.669127464294434, 'test/num_examples': 10000, 'score': 28612.852231264114, 'total_duration': 30882.437122821808, 'accumulated_submission_time': 28612.852231264114, 'accumulated_eval_time': 2255.5882289409637, 'accumulated_logging_time': 5.556652784347534}
I0307 11:38:21.408310 139894559057664 logging_writer.py:48] [73287] accumulated_eval_time=2255.59, accumulated_logging_time=5.55665, accumulated_submission_time=28612.9, global_step=73287, preemption_count=0, score=28612.9, test/accuracy=0.1416, test/loss=5.66913, test/num_examples=10000, total_duration=30882.4, train/accuracy=0.216717, train/loss=4.64317, validation/accuracy=0.2082, validation/loss=4.71568, validation/num_examples=50000
I0307 11:38:26.964483 139894567450368 logging_writer.py:48] [73300] global_step=73300, grad_norm=1.377181053161621, loss=2.1143875122070312
I0307 11:39:14.950275 139894559057664 logging_writer.py:48] [73400] global_step=73400, grad_norm=1.474626898765564, loss=2.0744364261627197
I0307 11:39:54.423156 139894567450368 logging_writer.py:48] [73500] global_step=73500, grad_norm=1.4035186767578125, loss=2.1272990703582764
I0307 11:40:33.118459 139894559057664 logging_writer.py:48] [73600] global_step=73600, grad_norm=1.3464637994766235, loss=2.0516583919525146
I0307 11:41:14.791438 139894567450368 logging_writer.py:48] [73700] global_step=73700, grad_norm=1.3733010292053223, loss=2.147512435913086
I0307 11:41:54.932682 139894559057664 logging_writer.py:48] [73800] global_step=73800, grad_norm=1.3621200323104858, loss=2.1751768589019775
I0307 11:42:33.039513 139894567450368 logging_writer.py:48] [73900] global_step=73900, grad_norm=1.4009846448898315, loss=2.2031686305999756
I0307 11:43:12.781501 139894559057664 logging_writer.py:48] [74000] global_step=74000, grad_norm=1.4269616603851318, loss=2.2220091819763184
I0307 11:43:52.401923 139894567450368 logging_writer.py:48] [74100] global_step=74100, grad_norm=1.3318607807159424, loss=2.0564770698547363
I0307 11:44:32.228818 139894559057664 logging_writer.py:48] [74200] global_step=74200, grad_norm=1.3628700971603394, loss=2.1123502254486084
I0307 11:45:12.044110 139894567450368 logging_writer.py:48] [74300] global_step=74300, grad_norm=1.4197973012924194, loss=2.184480905532837
I0307 11:45:50.799200 139894559057664 logging_writer.py:48] [74400] global_step=74400, grad_norm=1.3693159818649292, loss=1.9911737442016602
I0307 11:46:33.622851 139894567450368 logging_writer.py:48] [74500] global_step=74500, grad_norm=1.414110779762268, loss=2.076504945755005
I0307 11:46:51.543895 140050941662400 spec.py:321] Evaluating on the training split.
I0307 11:47:04.595443 140050941662400 spec.py:333] Evaluating on the validation split.
I0307 11:47:29.448551 140050941662400 spec.py:349] Evaluating on the test split.
I0307 11:47:31.170943 140050941662400 submission_runner.py:469] Time since start: 31432.27s, 	Step: 74541, 	{'train/accuracy': 0.17902980744838715, 'train/loss': 5.017524242401123, 'validation/accuracy': 0.16808000206947327, 'validation/loss': 5.142340183258057, 'validation/num_examples': 50000, 'test/accuracy': 0.10930000245571136, 'test/loss': 6.125587463378906, 'test/num_examples': 10000, 'score': 29122.80700802803, 'total_duration': 31432.2675075531, 'accumulated_submission_time': 29122.80700802803, 'accumulated_eval_time': 2295.2150542736053, 'accumulated_logging_time': 5.657249927520752}
I0307 11:47:31.326009 139894559057664 logging_writer.py:48] [74541] accumulated_eval_time=2295.22, accumulated_logging_time=5.65725, accumulated_submission_time=29122.8, global_step=74541, preemption_count=0, score=29122.8, test/accuracy=0.1093, test/loss=6.12559, test/num_examples=10000, total_duration=31432.3, train/accuracy=0.17903, train/loss=5.01752, validation/accuracy=0.16808, validation/loss=5.14234, validation/num_examples=50000
I0307 11:47:58.096388 139894567450368 logging_writer.py:48] [74600] global_step=74600, grad_norm=1.4282658100128174, loss=2.0912768840789795
I0307 11:48:48.126428 139894559057664 logging_writer.py:48] [74700] global_step=74700, grad_norm=1.3683501482009888, loss=2.187546491622925
I0307 11:49:32.229068 139894567450368 logging_writer.py:48] [74800] global_step=74800, grad_norm=1.5258055925369263, loss=2.040982484817505
I0307 11:50:14.747269 139894559057664 logging_writer.py:48] [74900] global_step=74900, grad_norm=1.422481894493103, loss=2.1408345699310303
I0307 11:51:03.658497 139894567450368 logging_writer.py:48] [75000] global_step=75000, grad_norm=1.4142621755599976, loss=2.0536365509033203
I0307 11:52:22.752972 139894559057664 logging_writer.py:48] [75100] global_step=75100, grad_norm=1.3526355028152466, loss=2.193685531616211
I0307 11:53:19.725227 139894567450368 logging_writer.py:48] [75200] global_step=75200, grad_norm=1.3167933225631714, loss=2.129627227783203
I0307 11:54:08.347118 139894559057664 logging_writer.py:48] [75300] global_step=75300, grad_norm=1.705992341041565, loss=2.205888032913208
I0307 11:55:05.220408 139894567450368 logging_writer.py:48] [75400] global_step=75400, grad_norm=1.3530749082565308, loss=1.9474985599517822
I0307 11:55:52.946426 139894559057664 logging_writer.py:48] [75500] global_step=75500, grad_norm=1.4003052711486816, loss=2.1628851890563965
I0307 11:56:01.420605 140050941662400 spec.py:321] Evaluating on the training split.
I0307 11:56:15.003628 140050941662400 spec.py:333] Evaluating on the validation split.
I0307 11:56:35.668739 140050941662400 spec.py:349] Evaluating on the test split.
I0307 11:56:37.443640 140050941662400 submission_runner.py:469] Time since start: 31978.54s, 	Step: 75519, 	{'train/accuracy': 0.2323620766401291, 'train/loss': 4.253023147583008, 'validation/accuracy': 0.20306000113487244, 'validation/loss': 4.5004963874816895, 'validation/num_examples': 50000, 'test/accuracy': 0.1535000056028366, 'test/loss': 5.129334449768066, 'test/num_examples': 10000, 'score': 29632.28448987007, 'total_duration': 31978.54022550583, 'accumulated_submission_time': 29632.28448987007, 'accumulated_eval_time': 2331.2378821372986, 'accumulated_logging_time': 6.314392805099487}
I0307 11:56:37.604099 139894567450368 logging_writer.py:48] [75519] accumulated_eval_time=2331.24, accumulated_logging_time=6.31439, accumulated_submission_time=29632.3, global_step=75519, preemption_count=0, score=29632.3, test/accuracy=0.1535, test/loss=5.12933, test/num_examples=10000, total_duration=31978.5, train/accuracy=0.232362, train/loss=4.25302, validation/accuracy=0.20306, validation/loss=4.5005, validation/num_examples=50000
I0307 11:57:18.689651 139894559057664 logging_writer.py:48] [75600] global_step=75600, grad_norm=1.3479890823364258, loss=2.1232500076293945
I0307 11:58:13.614605 139894567450368 logging_writer.py:48] [75700] global_step=75700, grad_norm=1.6020604372024536, loss=2.106106758117676
I0307 11:59:11.617678 139894559057664 logging_writer.py:48] [75800] global_step=75800, grad_norm=1.4237614870071411, loss=2.2268662452697754
I0307 12:00:09.280027 139894567450368 logging_writer.py:48] [75900] global_step=75900, grad_norm=1.3497164249420166, loss=2.0928211212158203
I0307 12:00:51.530182 139894559057664 logging_writer.py:48] [76000] global_step=76000, grad_norm=1.347009539604187, loss=2.1943376064300537
I0307 12:01:32.410984 139894567450368 logging_writer.py:48] [76100] global_step=76100, grad_norm=1.4710330963134766, loss=2.1081924438476562
I0307 12:02:14.408248 139894559057664 logging_writer.py:48] [76200] global_step=76200, grad_norm=1.416114330291748, loss=2.212076187133789
I0307 12:02:56.263425 139894567450368 logging_writer.py:48] [76300] global_step=76300, grad_norm=1.3679636716842651, loss=2.251007318496704
2025-03-07 12:02:58.006620: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 12:03:36.917414 139894559057664 logging_writer.py:48] [76400] global_step=76400, grad_norm=1.4187852144241333, loss=2.2470309734344482
I0307 12:04:18.700074 139894567450368 logging_writer.py:48] [76500] global_step=76500, grad_norm=1.5902255773544312, loss=2.265134572982788
I0307 12:05:07.699153 140050941662400 spec.py:321] Evaluating on the training split.
I0307 12:05:20.351056 140050941662400 spec.py:333] Evaluating on the validation split.
I0307 12:05:39.039489 140050941662400 spec.py:349] Evaluating on the test split.
I0307 12:05:40.850379 140050941662400 submission_runner.py:469] Time since start: 32521.95s, 	Step: 76595, 	{'train/accuracy': 0.20774872601032257, 'train/loss': 4.750784873962402, 'validation/accuracy': 0.1917800009250641, 'validation/loss': 4.861920356750488, 'validation/num_examples': 50000, 'test/accuracy': 0.14430001378059387, 'test/loss': 5.6282453536987305, 'test/num_examples': 10000, 'score': 30142.22321844101, 'total_duration': 32521.946982860565, 'accumulated_submission_time': 30142.22321844101, 'accumulated_eval_time': 2364.388926744461, 'accumulated_logging_time': 6.501705169677734}
I0307 12:05:40.931293 139894559057664 logging_writer.py:48] [76595] accumulated_eval_time=2364.39, accumulated_logging_time=6.50171, accumulated_submission_time=30142.2, global_step=76595, preemption_count=0, score=30142.2, test/accuracy=0.1443, test/loss=5.62825, test/num_examples=10000, total_duration=32521.9, train/accuracy=0.207749, train/loss=4.75078, validation/accuracy=0.19178, validation/loss=4.86192, validation/num_examples=50000
I0307 12:05:43.312210 139894567450368 logging_writer.py:48] [76600] global_step=76600, grad_norm=1.3848860263824463, loss=2.206122875213623
I0307 12:06:30.894360 139894559057664 logging_writer.py:48] [76700] global_step=76700, grad_norm=1.4490543603897095, loss=2.1696877479553223
I0307 12:07:34.282136 139894567450368 logging_writer.py:48] [76800] global_step=76800, grad_norm=1.415839672088623, loss=2.1169145107269287
I0307 12:08:52.164146 139894559057664 logging_writer.py:48] [76900] global_step=76900, grad_norm=1.2654575109481812, loss=2.022247076034546
I0307 12:09:50.245120 139894567450368 logging_writer.py:48] [77000] global_step=77000, grad_norm=1.4294391870498657, loss=2.1444504261016846
I0307 12:10:41.895135 139894559057664 logging_writer.py:48] [77100] global_step=77100, grad_norm=1.4017794132232666, loss=2.1416473388671875
I0307 12:11:45.276204 139894567450368 logging_writer.py:48] [77200] global_step=77200, grad_norm=1.4609748125076294, loss=2.077423334121704
I0307 12:12:58.866758 139894559057664 logging_writer.py:48] [77300] global_step=77300, grad_norm=1.4288924932479858, loss=2.2284836769104004
I0307 12:13:55.074606 139894567450368 logging_writer.py:48] [77400] global_step=77400, grad_norm=1.4862570762634277, loss=2.165144205093384
I0307 12:14:11.092455 140050941662400 spec.py:321] Evaluating on the training split.
I0307 12:14:23.365758 140050941662400 spec.py:333] Evaluating on the validation split.
I0307 12:14:43.048043 140050941662400 spec.py:349] Evaluating on the test split.
I0307 12:14:44.850183 140050941662400 submission_runner.py:469] Time since start: 33065.95s, 	Step: 77432, 	{'train/accuracy': 0.3491310477256775, 'train/loss': 3.3808252811431885, 'validation/accuracy': 0.33229997754096985, 'validation/loss': 3.5293385982513428, 'validation/num_examples': 50000, 'test/accuracy': 0.24970000982284546, 'test/loss': 4.375136375427246, 'test/num_examples': 10000, 'score': 30652.220622062683, 'total_duration': 33065.946811914444, 'accumulated_submission_time': 30652.220622062683, 'accumulated_eval_time': 2398.146493911743, 'accumulated_logging_time': 6.64655327796936}
I0307 12:14:44.929424 139894559057664 logging_writer.py:48] [77432] accumulated_eval_time=2398.15, accumulated_logging_time=6.64655, accumulated_submission_time=30652.2, global_step=77432, preemption_count=0, score=30652.2, test/accuracy=0.2497, test/loss=4.37514, test/num_examples=10000, total_duration=33065.9, train/accuracy=0.349131, train/loss=3.38083, validation/accuracy=0.3323, validation/loss=3.52934, validation/num_examples=50000
I0307 12:15:12.201213 139894567450368 logging_writer.py:48] [77500] global_step=77500, grad_norm=1.2565408945083618, loss=2.190380811691284
I0307 12:16:10.059235 139894559057664 logging_writer.py:48] [77600] global_step=77600, grad_norm=1.3397184610366821, loss=2.229518413543701
I0307 12:16:55.453681 139894567450368 logging_writer.py:48] [77700] global_step=77700, grad_norm=1.4943870306015015, loss=2.115741729736328
I0307 12:18:01.775841 139894559057664 logging_writer.py:48] [77800] global_step=77800, grad_norm=1.3741339445114136, loss=2.0565621852874756
I0307 12:19:19.387326 139894567450368 logging_writer.py:48] [77900] global_step=77900, grad_norm=1.2884604930877686, loss=2.060399293899536
I0307 12:20:04.730325 139894559057664 logging_writer.py:48] [78000] global_step=78000, grad_norm=1.5331610441207886, loss=2.1564273834228516
I0307 12:21:00.881792 139894567450368 logging_writer.py:48] [78100] global_step=78100, grad_norm=1.3767646551132202, loss=2.020677328109741
I0307 12:21:51.760401 139894559057664 logging_writer.py:48] [78200] global_step=78200, grad_norm=1.3955289125442505, loss=2.0975682735443115
I0307 12:22:43.239004 139894567450368 logging_writer.py:48] [78300] global_step=78300, grad_norm=1.334646463394165, loss=2.042292594909668
I0307 12:23:15.318403 140050941662400 spec.py:321] Evaluating on the training split.
I0307 12:23:28.117171 140050941662400 spec.py:333] Evaluating on the validation split.
I0307 12:23:50.571830 140050941662400 spec.py:349] Evaluating on the test split.
I0307 12:23:52.641854 140050941662400 submission_runner.py:469] Time since start: 33613.74s, 	Step: 78365, 	{'train/accuracy': 0.16234852373600006, 'train/loss': 4.925436496734619, 'validation/accuracy': 0.15129999816417694, 'validation/loss': 5.087410926818848, 'validation/num_examples': 50000, 'test/accuracy': 0.11520000547170639, 'test/loss': 5.64840841293335, 'test/num_examples': 10000, 'score': 31162.470264196396, 'total_duration': 33613.73833322525, 'accumulated_submission_time': 31162.470264196396, 'accumulated_eval_time': 2435.46963763237, 'accumulated_logging_time': 6.755829095840454}
I0307 12:23:52.754718 139894559057664 logging_writer.py:48] [78365] accumulated_eval_time=2435.47, accumulated_logging_time=6.75583, accumulated_submission_time=31162.5, global_step=78365, preemption_count=0, score=31162.5, test/accuracy=0.1152, test/loss=5.64841, test/num_examples=10000, total_duration=33613.7, train/accuracy=0.162349, train/loss=4.92544, validation/accuracy=0.1513, validation/loss=5.08741, validation/num_examples=50000
I0307 12:24:07.437801 139894567450368 logging_writer.py:48] [78400] global_step=78400, grad_norm=1.3241313695907593, loss=2.0330986976623535
I0307 12:24:50.139835 139894559057664 logging_writer.py:48] [78500] global_step=78500, grad_norm=1.3445987701416016, loss=2.1709721088409424
I0307 12:25:37.027712 139894567450368 logging_writer.py:48] [78600] global_step=78600, grad_norm=1.6718838214874268, loss=2.095643997192383
I0307 12:26:31.063372 139894559057664 logging_writer.py:48] [78700] global_step=78700, grad_norm=1.4712401628494263, loss=2.0732319355010986
I0307 12:27:21.770778 139894567450368 logging_writer.py:48] [78800] global_step=78800, grad_norm=1.5262324810028076, loss=2.11889910697937
I0307 12:28:06.183733 139894559057664 logging_writer.py:48] [78900] global_step=78900, grad_norm=1.4668781757354736, loss=1.98121178150177
I0307 12:28:50.009280 139894567450368 logging_writer.py:48] [79000] global_step=79000, grad_norm=1.5374572277069092, loss=2.227816581726074
I0307 12:30:23.255725 139894559057664 logging_writer.py:48] [79100] global_step=79100, grad_norm=1.4739248752593994, loss=2.0819191932678223
I0307 12:31:11.324352 139894567450368 logging_writer.py:48] [79200] global_step=79200, grad_norm=1.3787614107131958, loss=2.1695175170898438
I0307 12:32:10.685136 139894559057664 logging_writer.py:48] [79300] global_step=79300, grad_norm=1.4661920070648193, loss=2.16977596282959
I0307 12:32:23.072501 140050941662400 spec.py:321] Evaluating on the training split.
I0307 12:32:36.246089 140050941662400 spec.py:333] Evaluating on the validation split.
I0307 12:32:58.213227 140050941662400 spec.py:349] Evaluating on the test split.
I0307 12:33:00.035878 140050941662400 submission_runner.py:469] Time since start: 34161.13s, 	Step: 79319, 	{'train/accuracy': 0.38926976919174194, 'train/loss': 2.869124174118042, 'validation/accuracy': 0.3698999881744385, 'validation/loss': 2.9974637031555176, 'validation/num_examples': 50000, 'test/accuracy': 0.28140002489089966, 'test/loss': 3.785993814468384, 'test/num_examples': 10000, 'score': 31672.649310350418, 'total_duration': 34161.132488012314, 'accumulated_submission_time': 31672.649310350418, 'accumulated_eval_time': 2472.4328339099884, 'accumulated_logging_time': 6.895248889923096}
I0307 12:33:00.096282 139894567450368 logging_writer.py:48] [79319] accumulated_eval_time=2472.43, accumulated_logging_time=6.89525, accumulated_submission_time=31672.6, global_step=79319, preemption_count=0, score=31672.6, test/accuracy=0.2814, test/loss=3.78599, test/num_examples=10000, total_duration=34161.1, train/accuracy=0.38927, train/loss=2.86912, validation/accuracy=0.3699, validation/loss=2.99746, validation/num_examples=50000
I0307 12:33:54.691939 139894559057664 logging_writer.py:48] [79400] global_step=79400, grad_norm=1.3398866653442383, loss=2.117518663406372
I0307 12:35:14.081751 139894567450368 logging_writer.py:48] [79500] global_step=79500, grad_norm=1.4003924131393433, loss=2.1643238067626953
I0307 12:36:06.293619 139894559057664 logging_writer.py:48] [79600] global_step=79600, grad_norm=1.5261328220367432, loss=2.1973183155059814
I0307 12:37:22.569868 139894567450368 logging_writer.py:48] [79700] global_step=79700, grad_norm=1.4570367336273193, loss=2.140580415725708
I0307 12:38:10.338089 139894559057664 logging_writer.py:48] [79800] global_step=79800, grad_norm=1.5868592262268066, loss=2.2525079250335693
I0307 12:39:09.633461 139894567450368 logging_writer.py:48] [79900] global_step=79900, grad_norm=1.4267892837524414, loss=2.187309741973877
I0307 12:40:10.209747 139894559057664 logging_writer.py:48] [80000] global_step=80000, grad_norm=1.541936993598938, loss=1.989824891090393
2025-03-07 12:41:03.328929: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 12:41:26.701274 139894567450368 logging_writer.py:48] [80100] global_step=80100, grad_norm=1.415865421295166, loss=2.0810959339141846
I0307 12:41:30.151072 140050941662400 spec.py:321] Evaluating on the training split.
I0307 12:41:43.057039 140050941662400 spec.py:333] Evaluating on the validation split.
2025-03-07 12:41:43.981504: E external/xla/xla/stream_executor/cuda/cuda_driver.cc:1635] failed to alloc 1073741824 bytes on host: CUDA_ERROR_INVALID_VALUE: invalid argument
2025-03-07 12:41:44.034277: E external/xla/xla/stream_executor/cuda/cuda_driver.cc:1635] failed to alloc 966367744 bytes on host: CUDA_ERROR_INVALID_VALUE: invalid argument
2025-03-07 12:41:44.048006: E external/xla/xla/stream_executor/cuda/cuda_driver.cc:1635] failed to alloc 869731072 bytes on host: CUDA_ERROR_INVALID_VALUE: invalid argument
2025-03-07 12:41:44.204403: E external/xla/xla/stream_executor/cuda/cuda_driver.cc:1635] failed to alloc 782758144 bytes on host: CUDA_ERROR_INVALID_VALUE: invalid argument
2025-03-07 12:41:44.218700: E external/xla/xla/stream_executor/cuda/cuda_driver.cc:1635] failed to alloc 704482304 bytes on host: CUDA_ERROR_INVALID_VALUE: invalid argument
2025-03-07 12:41:44.253314: E external/xla/xla/stream_executor/cuda/cuda_driver.cc:1635] failed to alloc 634034176 bytes on host: CUDA_ERROR_INVALID_VALUE: invalid argument
2025-03-07 12:41:44.272990: E external/xla/xla/stream_executor/cuda/cuda_driver.cc:1635] failed to alloc 570630912 bytes on host: CUDA_ERROR_INVALID_VALUE: invalid argument
I0307 12:42:08.336385 140050941662400 spec.py:349] Evaluating on the test split.
I0307 12:42:10.120560 140050941662400 submission_runner.py:469] Time since start: 34711.22s, 	Step: 80109, 	{'train/accuracy': 0.20804767310619354, 'train/loss': 4.664608478546143, 'validation/accuracy': 0.19643999636173248, 'validation/loss': 4.822405815124512, 'validation/num_examples': 50000, 'test/accuracy': 0.14690001308918, 'test/loss': 5.479805946350098, 'test/num_examples': 10000, 'score': 32182.58517575264, 'total_duration': 34711.21717429161, 'accumulated_submission_time': 32182.58517575264, 'accumulated_eval_time': 2512.4021470546722, 'accumulated_logging_time': 6.981983423233032}
I0307 12:42:10.180490 139894559057664 logging_writer.py:48] [80109] accumulated_eval_time=2512.4, accumulated_logging_time=6.98198, accumulated_submission_time=32182.6, global_step=80109, preemption_count=0, score=32182.6, test/accuracy=0.1469, test/loss=5.47981, test/num_examples=10000, total_duration=34711.2, train/accuracy=0.208048, train/loss=4.66461, validation/accuracy=0.19644, validation/loss=4.82241, validation/num_examples=50000
I0307 12:42:50.015157 139894567450368 logging_writer.py:48] [80200] global_step=80200, grad_norm=1.4349987506866455, loss=2.2115142345428467
I0307 12:43:32.567975 139894559057664 logging_writer.py:48] [80300] global_step=80300, grad_norm=1.5138167142868042, loss=2.132235050201416
I0307 12:44:17.404509 139894567450368 logging_writer.py:48] [80400] global_step=80400, grad_norm=1.3780169486999512, loss=2.161484956741333
I0307 12:45:12.723319 139894559057664 logging_writer.py:48] [80500] global_step=80500, grad_norm=1.555789828300476, loss=2.1641886234283447
I0307 12:46:05.624572 139894567450368 logging_writer.py:48] [80600] global_step=80600, grad_norm=1.48097562789917, loss=2.1536242961883545
I0307 12:47:37.442565 139894559057664 logging_writer.py:48] [80700] global_step=80700, grad_norm=1.4075253009796143, loss=2.1397719383239746
I0307 12:48:29.097919 139894567450368 logging_writer.py:48] [80800] global_step=80800, grad_norm=1.4503377676010132, loss=2.052358627319336
I0307 12:49:24.741723 139894559057664 logging_writer.py:48] [80900] global_step=80900, grad_norm=1.4684447050094604, loss=2.0738301277160645
I0307 12:50:12.606410 139894567450368 logging_writer.py:48] [81000] global_step=81000, grad_norm=1.451767921447754, loss=2.12126088142395
I0307 12:50:40.614762 140050941662400 spec.py:321] Evaluating on the training split.
I0307 12:50:53.626708 140050941662400 spec.py:333] Evaluating on the validation split.
I0307 12:51:13.674761 140050941662400 spec.py:349] Evaluating on the test split.
I0307 12:51:15.485685 140050941662400 submission_runner.py:469] Time since start: 35256.58s, 	Step: 81050, 	{'train/accuracy': 0.3246372640132904, 'train/loss': 3.411705493927002, 'validation/accuracy': 0.29892000555992126, 'validation/loss': 3.6232857704162598, 'validation/num_examples': 50000, 'test/accuracy': 0.22350001335144043, 'test/loss': 4.455986976623535, 'test/num_examples': 10000, 'score': 32692.882498264313, 'total_duration': 35256.58227753639, 'accumulated_submission_time': 32692.882498264313, 'accumulated_eval_time': 2547.272887945175, 'accumulated_logging_time': 7.0693488121032715}
I0307 12:51:15.542705 139894559057664 logging_writer.py:48] [81050] accumulated_eval_time=2547.27, accumulated_logging_time=7.06935, accumulated_submission_time=32692.9, global_step=81050, preemption_count=0, score=32692.9, test/accuracy=0.2235, test/loss=4.45599, test/num_examples=10000, total_duration=35256.6, train/accuracy=0.324637, train/loss=3.41171, validation/accuracy=0.29892, validation/loss=3.62329, validation/num_examples=50000
I0307 12:51:40.072766 139894567450368 logging_writer.py:48] [81100] global_step=81100, grad_norm=1.4129828214645386, loss=2.2008633613586426
I0307 12:52:41.720349 139894559057664 logging_writer.py:48] [81200] global_step=81200, grad_norm=1.414312481880188, loss=2.0291829109191895
I0307 12:54:06.917242 139894567450368 logging_writer.py:48] [81300] global_step=81300, grad_norm=1.4842792749404907, loss=2.1702091693878174
I0307 12:55:23.736294 139894559057664 logging_writer.py:48] [81400] global_step=81400, grad_norm=1.3671908378601074, loss=2.051832437515259
I0307 12:56:19.152193 139894567450368 logging_writer.py:48] [81500] global_step=81500, grad_norm=1.5086456537246704, loss=2.204735517501831
I0307 12:57:41.085326 139894559057664 logging_writer.py:48] [81600] global_step=81600, grad_norm=1.4261285066604614, loss=2.0792059898376465
I0307 12:58:48.141896 139894567450368 logging_writer.py:48] [81700] global_step=81700, grad_norm=1.630538821220398, loss=2.0802483558654785
I0307 12:59:41.537690 139894559057664 logging_writer.py:48] [81800] global_step=81800, grad_norm=1.5600619316101074, loss=2.264719247817993
I0307 12:59:45.519741 140050941662400 spec.py:321] Evaluating on the training split.
I0307 12:59:56.960731 140050941662400 spec.py:333] Evaluating on the validation split.
I0307 13:00:18.299165 140050941662400 spec.py:349] Evaluating on the test split.
I0307 13:00:20.089418 140050941662400 submission_runner.py:469] Time since start: 35801.16s, 	Step: 81808, 	{'train/accuracy': 0.29530054330825806, 'train/loss': 3.5512404441833496, 'validation/accuracy': 0.28011998534202576, 'validation/loss': 3.6820178031921387, 'validation/num_examples': 50000, 'test/accuracy': 0.2005000114440918, 'test/loss': 4.468143939971924, 'test/num_examples': 10000, 'score': 33202.73929190636, 'total_duration': 35801.161251306534, 'accumulated_submission_time': 33202.73929190636, 'accumulated_eval_time': 2581.8176136016846, 'accumulated_logging_time': 7.157943487167358}
I0307 13:00:20.143906 139894567450368 logging_writer.py:48] [81808] accumulated_eval_time=2581.82, accumulated_logging_time=7.15794, accumulated_submission_time=33202.7, global_step=81808, preemption_count=0, score=33202.7, test/accuracy=0.2005, test/loss=4.46814, test/num_examples=10000, total_duration=35801.2, train/accuracy=0.295301, train/loss=3.55124, validation/accuracy=0.28012, validation/loss=3.68202, validation/num_examples=50000
I0307 13:01:08.730346 139894559057664 logging_writer.py:48] [81900] global_step=81900, grad_norm=1.422179102897644, loss=2.063999891281128
I0307 13:02:42.442253 139894567450368 logging_writer.py:48] [82000] global_step=82000, grad_norm=1.4193875789642334, loss=2.0679452419281006
I0307 13:03:52.349565 139894559057664 logging_writer.py:48] [82100] global_step=82100, grad_norm=1.3572394847869873, loss=2.0039539337158203
I0307 13:04:48.302627 139894567450368 logging_writer.py:48] [82200] global_step=82200, grad_norm=1.409098744392395, loss=2.1469480991363525
I0307 13:05:31.360577 139894559057664 logging_writer.py:48] [82300] global_step=82300, grad_norm=1.4616636037826538, loss=2.161783456802368
I0307 13:06:14.439069 139894567450368 logging_writer.py:48] [82400] global_step=82400, grad_norm=1.5835071802139282, loss=2.106513738632202
I0307 13:07:05.285496 139894559057664 logging_writer.py:48] [82500] global_step=82500, grad_norm=1.480486273765564, loss=2.1560919284820557
I0307 13:08:07.145308 139894567450368 logging_writer.py:48] [82600] global_step=82600, grad_norm=1.7194950580596924, loss=2.199493885040283
I0307 13:08:50.081002 140050941662400 spec.py:321] Evaluating on the training split.
I0307 13:09:00.979954 140050941662400 spec.py:333] Evaluating on the validation split.
I0307 13:09:22.451634 140050941662400 spec.py:349] Evaluating on the test split.
I0307 13:09:24.264206 140050941662400 submission_runner.py:469] Time since start: 36345.31s, 	Step: 82690, 	{'train/accuracy': 0.2952008843421936, 'train/loss': 3.861325979232788, 'validation/accuracy': 0.2816599905490875, 'validation/loss': 3.9740242958068848, 'validation/num_examples': 50000, 'test/accuracy': 0.20420001447200775, 'test/loss': 4.859318256378174, 'test/num_examples': 10000, 'score': 33712.5399518013, 'total_duration': 36345.314138650894, 'accumulated_submission_time': 33712.5399518013, 'accumulated_eval_time': 2615.953969478607, 'accumulated_logging_time': 7.246772766113281}
I0307 13:09:24.326693 139894559057664 logging_writer.py:48] [82690] accumulated_eval_time=2615.95, accumulated_logging_time=7.24677, accumulated_submission_time=33712.5, global_step=82690, preemption_count=0, score=33712.5, test/accuracy=0.2042, test/loss=4.85932, test/num_examples=10000, total_duration=36345.3, train/accuracy=0.295201, train/loss=3.86133, validation/accuracy=0.28166, validation/loss=3.97402, validation/num_examples=50000
I0307 13:09:28.707061 139894567450368 logging_writer.py:48] [82700] global_step=82700, grad_norm=1.5169416666030884, loss=2.131770372390747
I0307 13:10:21.765218 139894559057664 logging_writer.py:48] [82800] global_step=82800, grad_norm=1.3789279460906982, loss=2.0682241916656494
I0307 13:11:15.098215 139894567450368 logging_writer.py:48] [82900] global_step=82900, grad_norm=1.4992378950119019, loss=2.140958070755005
I0307 13:12:16.098197 139894559057664 logging_writer.py:48] [83000] global_step=83000, grad_norm=1.3916120529174805, loss=2.098397731781006
I0307 13:14:54.041332 139894567450368 logging_writer.py:48] [83100] global_step=83100, grad_norm=1.5037742853164673, loss=2.0717921257019043
I0307 13:15:49.392381 139894559057664 logging_writer.py:48] [83200] global_step=83200, grad_norm=1.476768970489502, loss=2.047849416732788
I0307 13:16:44.265791 139894567450368 logging_writer.py:48] [83300] global_step=83300, grad_norm=1.403111219406128, loss=2.0734128952026367
I0307 13:17:47.294389 139894559057664 logging_writer.py:48] [83400] global_step=83400, grad_norm=1.5856027603149414, loss=2.0125443935394287
I0307 13:17:54.523510 140050941662400 spec.py:321] Evaluating on the training split.
I0307 13:18:05.492884 140050941662400 spec.py:333] Evaluating on the validation split.
I0307 13:18:26.992730 140050941662400 spec.py:349] Evaluating on the test split.
I0307 13:18:28.719616 140050941662400 submission_runner.py:469] Time since start: 36889.80s, 	Step: 83415, 	{'train/accuracy': 0.1780133843421936, 'train/loss': 4.704196453094482, 'validation/accuracy': 0.15817999839782715, 'validation/loss': 4.9603424072265625, 'validation/num_examples': 50000, 'test/accuracy': 0.12460000813007355, 'test/loss': 5.451750755310059, 'test/num_examples': 10000, 'score': 34222.599754571915, 'total_duration': 36889.79902672768, 'accumulated_submission_time': 34222.599754571915, 'accumulated_eval_time': 2650.1326994895935, 'accumulated_logging_time': 7.36224627494812}
I0307 13:18:28.764406 139894567450368 logging_writer.py:48] [83415] accumulated_eval_time=2650.13, accumulated_logging_time=7.36225, accumulated_submission_time=34222.6, global_step=83415, preemption_count=0, score=34222.6, test/accuracy=0.1246, test/loss=5.45175, test/num_examples=10000, total_duration=36889.8, train/accuracy=0.178013, train/loss=4.7042, validation/accuracy=0.15818, validation/loss=4.96034, validation/num_examples=50000
I0307 13:19:05.773458 139894559057664 logging_writer.py:48] [83500] global_step=83500, grad_norm=1.3705207109451294, loss=2.0245416164398193
I0307 13:20:02.995304 139894567450368 logging_writer.py:48] [83600] global_step=83600, grad_norm=1.4959708452224731, loss=2.099189281463623
I0307 13:21:38.303546 139894559057664 logging_writer.py:48] [83700] global_step=83700, grad_norm=1.5505993366241455, loss=2.1217594146728516
I0307 13:22:28.964958 139894567450368 logging_writer.py:48] [83800] global_step=83800, grad_norm=1.6415225267410278, loss=2.0100619792938232
I0307 13:23:24.882074 139894559057664 logging_writer.py:48] [83900] global_step=83900, grad_norm=1.5566191673278809, loss=2.1705198287963867
I0307 13:24:31.375535 139894567450368 logging_writer.py:48] [84000] global_step=84000, grad_norm=1.3630372285842896, loss=2.1655831336975098
I0307 13:25:50.326212 139894559057664 logging_writer.py:48] [84100] global_step=84100, grad_norm=1.4387381076812744, loss=2.1622743606567383
I0307 13:26:49.926208 139894567450368 logging_writer.py:48] [84200] global_step=84200, grad_norm=1.4432718753814697, loss=2.080198049545288
I0307 13:26:58.761121 140050941662400 spec.py:321] Evaluating on the training split.
I0307 13:27:10.463173 140050941662400 spec.py:333] Evaluating on the validation split.
I0307 13:27:41.305365 140050941662400 spec.py:349] Evaluating on the test split.
I0307 13:27:43.038056 140050941662400 submission_runner.py:469] Time since start: 37444.12s, 	Step: 84215, 	{'train/accuracy': 0.34066087007522583, 'train/loss': 3.2218375205993652, 'validation/accuracy': 0.32148000597953796, 'validation/loss': 3.3773317337036133, 'validation/num_examples': 50000, 'test/accuracy': 0.24160000681877136, 'test/loss': 4.1262640953063965, 'test/num_examples': 10000, 'score': 34732.472373485565, 'total_duration': 37444.11557793617, 'accumulated_submission_time': 34732.472373485565, 'accumulated_eval_time': 2694.390380382538, 'accumulated_logging_time': 7.43683123588562}
I0307 13:27:43.085162 139894559057664 logging_writer.py:48] [84215] accumulated_eval_time=2694.39, accumulated_logging_time=7.43683, accumulated_submission_time=34732.5, global_step=84215, preemption_count=0, score=34732.5, test/accuracy=0.2416, test/loss=4.12626, test/num_examples=10000, total_duration=37444.1, train/accuracy=0.340661, train/loss=3.22184, validation/accuracy=0.32148, validation/loss=3.37733, validation/num_examples=50000
I0307 13:29:21.529900 139894567450368 logging_writer.py:48] [84300] global_step=84300, grad_norm=1.392256736755371, loss=2.0813162326812744
I0307 13:31:46.047309 139894559057664 logging_writer.py:48] [84400] global_step=84400, grad_norm=1.444082260131836, loss=2.0638744831085205
I0307 13:33:01.262900 139894567450368 logging_writer.py:48] [84500] global_step=84500, grad_norm=1.4126628637313843, loss=2.132784605026245
I0307 13:34:22.063732 139894559057664 logging_writer.py:48] [84600] global_step=84600, grad_norm=1.5692195892333984, loss=2.1212730407714844
I0307 13:35:29.594224 139894567450368 logging_writer.py:48] [84700] global_step=84700, grad_norm=1.5458388328552246, loss=2.1917884349823
I0307 13:36:10.800491 139894559057664 logging_writer.py:48] [84800] global_step=84800, grad_norm=1.495451807975769, loss=1.974186658859253
I0307 13:36:13.022720 140050941662400 spec.py:321] Evaluating on the training split.
I0307 13:36:24.306464 140050941662400 spec.py:333] Evaluating on the validation split.
I0307 13:36:47.899562 140050941662400 spec.py:349] Evaluating on the test split.
I0307 13:36:49.617179 140050941662400 submission_runner.py:469] Time since start: 37990.71s, 	Step: 84807, 	{'train/accuracy': 0.34225526452064514, 'train/loss': 3.1803669929504395, 'validation/accuracy': 0.30991998314857483, 'validation/loss': 3.4767887592315674, 'validation/num_examples': 50000, 'test/accuracy': 0.2323000133037567, 'test/loss': 4.207357883453369, 'test/num_examples': 10000, 'score': 35242.308685302734, 'total_duration': 37990.713916778564, 'accumulated_submission_time': 35242.308685302734, 'accumulated_eval_time': 2730.9847898483276, 'accumulated_logging_time': 7.516220808029175}
I0307 13:36:49.666169 139894567450368 logging_writer.py:48] [84807] accumulated_eval_time=2730.98, accumulated_logging_time=7.51622, accumulated_submission_time=35242.3, global_step=84807, preemption_count=0, score=35242.3, test/accuracy=0.2323, test/loss=4.20736, test/num_examples=10000, total_duration=37990.7, train/accuracy=0.342255, train/loss=3.18037, validation/accuracy=0.30992, validation/loss=3.47679, validation/num_examples=50000
I0307 13:38:06.109069 139894559057664 logging_writer.py:48] [84900] global_step=84900, grad_norm=1.504460096359253, loss=2.0789883136749268
I0307 13:39:27.639351 139894567450368 logging_writer.py:48] [85000] global_step=85000, grad_norm=1.4564646482467651, loss=2.045199155807495
I0307 13:41:06.809375 139894559057664 logging_writer.py:48] [85100] global_step=85100, grad_norm=1.3922669887542725, loss=2.0505640506744385
I0307 13:42:01.591782 139894567450368 logging_writer.py:48] [85200] global_step=85200, grad_norm=1.4295226335525513, loss=2.080888509750366
I0307 13:42:50.848857 139894559057664 logging_writer.py:48] [85300] global_step=85300, grad_norm=1.6695618629455566, loss=2.0312352180480957
I0307 13:44:19.189057 139894567450368 logging_writer.py:48] [85400] global_step=85400, grad_norm=1.4198373556137085, loss=2.0132408142089844
I0307 13:45:20.239641 140050941662400 spec.py:321] Evaluating on the training split.
I0307 13:45:30.693917 140050941662400 spec.py:333] Evaluating on the validation split.
I0307 13:45:50.403930 140050941662400 spec.py:349] Evaluating on the test split.
I0307 13:45:52.162227 140050941662400 submission_runner.py:469] Time since start: 38533.26s, 	Step: 85476, 	{'train/accuracy': 0.3448660671710968, 'train/loss': 3.225349187850952, 'validation/accuracy': 0.32486000657081604, 'validation/loss': 3.352023124694824, 'validation/num_examples': 50000, 'test/accuracy': 0.23770001530647278, 'test/loss': 4.146182060241699, 'test/num_examples': 10000, 'score': 35752.79736351967, 'total_duration': 38533.25898337364, 'accumulated_submission_time': 35752.79736351967, 'accumulated_eval_time': 2762.9073498249054, 'accumulated_logging_time': 7.573133230209351}
I0307 13:45:52.216969 139894559057664 logging_writer.py:48] [85476] accumulated_eval_time=2762.91, accumulated_logging_time=7.57313, accumulated_submission_time=35752.8, global_step=85476, preemption_count=0, score=35752.8, test/accuracy=0.2377, test/loss=4.14618, test/num_examples=10000, total_duration=38533.3, train/accuracy=0.344866, train/loss=3.22535, validation/accuracy=0.32486, validation/loss=3.35202, validation/num_examples=50000
I0307 13:46:02.876844 139894567450368 logging_writer.py:48] [85500] global_step=85500, grad_norm=1.536569356918335, loss=2.0996193885803223
I0307 13:47:31.611190 139894559057664 logging_writer.py:48] [85600] global_step=85600, grad_norm=1.5326329469680786, loss=2.1713104248046875
I0307 13:48:25.852370 139894567450368 logging_writer.py:48] [85700] global_step=85700, grad_norm=1.389556646347046, loss=2.0726733207702637
I0307 13:49:23.162903 139894559057664 logging_writer.py:48] [85800] global_step=85800, grad_norm=1.5367600917816162, loss=2.058541774749756
I0307 13:50:20.896168 139894567450368 logging_writer.py:48] [85900] global_step=85900, grad_norm=1.5520999431610107, loss=2.0760772228240967
I0307 13:51:35.393492 139894559057664 logging_writer.py:48] [86000] global_step=86000, grad_norm=1.4099698066711426, loss=1.9917564392089844
I0307 13:52:59.779882 139894567450368 logging_writer.py:48] [86100] global_step=86100, grad_norm=1.4738868474960327, loss=2.0904390811920166
I0307 13:54:23.019581 140050941662400 spec.py:321] Evaluating on the training split.
I0307 13:54:33.153157 140050941662400 spec.py:333] Evaluating on the validation split.
I0307 13:54:53.707673 140050941662400 spec.py:349] Evaluating on the test split.
I0307 13:54:55.419954 140050941662400 submission_runner.py:469] Time since start: 39076.52s, 	Step: 86199, 	{'train/accuracy': 0.3011399805545807, 'train/loss': 3.5227794647216797, 'validation/accuracy': 0.27539998292922974, 'validation/loss': 3.7204880714416504, 'validation/num_examples': 50000, 'test/accuracy': 0.1859000027179718, 'test/loss': 4.593204975128174, 'test/num_examples': 10000, 'score': 36263.49385714531, 'total_duration': 39076.5167093277, 'accumulated_submission_time': 36263.49385714531, 'accumulated_eval_time': 2795.307690382004, 'accumulated_logging_time': 7.65012788772583}
I0307 13:54:55.531107 139894559057664 logging_writer.py:48] [86199] accumulated_eval_time=2795.31, accumulated_logging_time=7.65013, accumulated_submission_time=36263.5, global_step=86199, preemption_count=0, score=36263.5, test/accuracy=0.1859, test/loss=4.5932, test/num_examples=10000, total_duration=39076.5, train/accuracy=0.30114, train/loss=3.52278, validation/accuracy=0.2754, validation/loss=3.72049, validation/num_examples=50000
I0307 13:54:56.245851 139894567450368 logging_writer.py:48] [86200] global_step=86200, grad_norm=1.6402190923690796, loss=2.090681552886963
I0307 13:57:15.519351 139894559057664 logging_writer.py:48] [86300] global_step=86300, grad_norm=1.3364241123199463, loss=2.018799304962158
I0307 13:58:44.053490 139894567450368 logging_writer.py:48] [86400] global_step=86400, grad_norm=1.3805421590805054, loss=2.087270736694336
I0307 13:59:41.257467 139894559057664 logging_writer.py:48] [86500] global_step=86500, grad_norm=1.5306856632232666, loss=2.0812597274780273
I0307 14:00:40.056394 139894567450368 logging_writer.py:48] [86600] global_step=86600, grad_norm=1.5074937343597412, loss=2.054316282272339
I0307 14:01:43.077777 139894559057664 logging_writer.py:48] [86700] global_step=86700, grad_norm=1.5630422830581665, loss=1.998255968093872
I0307 14:03:07.750154 139894567450368 logging_writer.py:48] [86800] global_step=86800, grad_norm=1.4594544172286987, loss=2.086127281188965
I0307 14:03:26.077817 140050941662400 spec.py:321] Evaluating on the training split.
I0307 14:03:36.530758 140050941662400 spec.py:333] Evaluating on the validation split.
I0307 14:03:56.543379 140050941662400 spec.py:349] Evaluating on the test split.
I0307 14:03:58.262781 140050941662400 submission_runner.py:469] Time since start: 39619.36s, 	Step: 86823, 	{'train/accuracy': 0.2626155912876129, 'train/loss': 4.037708282470703, 'validation/accuracy': 0.24619999527931213, 'validation/loss': 4.217474937438965, 'validation/num_examples': 50000, 'test/accuracy': 0.18850000202655792, 'test/loss': 4.856085300445557, 'test/num_examples': 10000, 'score': 36773.94080400467, 'total_duration': 39619.359533786774, 'accumulated_submission_time': 36773.94080400467, 'accumulated_eval_time': 2827.4926176071167, 'accumulated_logging_time': 7.788488864898682}
I0307 14:03:58.316370 139894559057664 logging_writer.py:48] [86823] accumulated_eval_time=2827.49, accumulated_logging_time=7.78849, accumulated_submission_time=36773.9, global_step=86823, preemption_count=0, score=36773.9, test/accuracy=0.1885, test/loss=4.85609, test/num_examples=10000, total_duration=39619.4, train/accuracy=0.262616, train/loss=4.03771, validation/accuracy=0.2462, validation/loss=4.21747, validation/num_examples=50000
I0307 14:04:58.521070 139894567450368 logging_writer.py:48] [86900] global_step=86900, grad_norm=1.5635230541229248, loss=1.9945547580718994
I0307 14:06:28.533459 139894559057664 logging_writer.py:48] [87000] global_step=87000, grad_norm=1.426567792892456, loss=2.0264291763305664
I0307 14:08:15.079087 139894567450368 logging_writer.py:48] [87100] global_step=87100, grad_norm=1.5074821710586548, loss=2.0640616416931152
I0307 14:09:29.074960 139894559057664 logging_writer.py:48] [87200] global_step=87200, grad_norm=1.526578426361084, loss=2.0452752113342285
I0307 14:10:09.660257 139894567450368 logging_writer.py:48] [87300] global_step=87300, grad_norm=1.5654208660125732, loss=2.0454602241516113
I0307 14:11:12.119372 139894559057664 logging_writer.py:48] [87400] global_step=87400, grad_norm=1.5157086849212646, loss=1.9709511995315552
I0307 14:12:30.105504 140050941662400 spec.py:321] Evaluating on the training split.
I0307 14:12:40.239677 140050941662400 spec.py:333] Evaluating on the validation split.
I0307 14:13:00.210132 140050941662400 spec.py:349] Evaluating on the test split.
I0307 14:13:01.930708 140050941662400 submission_runner.py:469] Time since start: 40163.03s, 	Step: 87483, 	{'train/accuracy': 0.5042649507522583, 'train/loss': 2.1560163497924805, 'validation/accuracy': 0.4483399987220764, 'validation/loss': 2.5270583629608154, 'validation/num_examples': 50000, 'test/accuracy': 0.3337000012397766, 'test/loss': 3.355020523071289, 'test/num_examples': 10000, 'score': 37285.644619464874, 'total_duration': 40163.02745461464, 'accumulated_submission_time': 37285.644619464874, 'accumulated_eval_time': 2859.317789077759, 'accumulated_logging_time': 7.850671052932739}
I0307 14:13:01.985856 139894567450368 logging_writer.py:48] [87483] accumulated_eval_time=2859.32, accumulated_logging_time=7.85067, accumulated_submission_time=37285.6, global_step=87483, preemption_count=0, score=37285.6, test/accuracy=0.3337, test/loss=3.35502, test/num_examples=10000, total_duration=40163, train/accuracy=0.504265, train/loss=2.15602, validation/accuracy=0.44834, validation/loss=2.52706, validation/num_examples=50000
I0307 14:13:17.731401 139894559057664 logging_writer.py:48] [87500] global_step=87500, grad_norm=1.4218369722366333, loss=2.058279037475586
I0307 14:15:51.870571 139894567450368 logging_writer.py:48] [87600] global_step=87600, grad_norm=1.5004209280014038, loss=2.230398654937744
I0307 14:17:04.217104 139894559057664 logging_writer.py:48] [87700] global_step=87700, grad_norm=1.442574143409729, loss=1.9984723329544067
I0307 14:18:28.908501 139894567450368 logging_writer.py:48] [87800] global_step=87800, grad_norm=1.485154628753662, loss=2.100761651992798
I0307 14:19:52.030788 139894559057664 logging_writer.py:48] [87900] global_step=87900, grad_norm=1.4741514921188354, loss=2.0382072925567627
I0307 14:21:14.688936 139894567450368 logging_writer.py:48] [88000] global_step=88000, grad_norm=1.4981887340545654, loss=2.128361701965332
I0307 14:21:32.384174 140050941662400 spec.py:321] Evaluating on the training split.
I0307 14:21:43.977010 140050941662400 spec.py:333] Evaluating on the validation split.
I0307 14:22:11.053118 140050941662400 spec.py:349] Evaluating on the test split.
I0307 14:22:12.760204 140050941662400 submission_runner.py:469] Time since start: 40713.86s, 	Step: 88025, 	{'train/accuracy': 0.36758607625961304, 'train/loss': 3.0777347087860107, 'validation/accuracy': 0.3454799950122833, 'validation/loss': 3.21311616897583, 'validation/num_examples': 50000, 'test/accuracy': 0.2629000246524811, 'test/loss': 3.9372456073760986, 'test/num_examples': 10000, 'score': 37795.97331047058, 'total_duration': 40713.85692739487, 'accumulated_submission_time': 37795.97331047058, 'accumulated_eval_time': 2899.6937775611877, 'accumulated_logging_time': 7.913533687591553}
I0307 14:22:12.793097 139894559057664 logging_writer.py:48] [88025] accumulated_eval_time=2899.69, accumulated_logging_time=7.91353, accumulated_submission_time=37796, global_step=88025, preemption_count=0, score=37796, test/accuracy=0.2629, test/loss=3.93725, test/num_examples=10000, total_duration=40713.9, train/accuracy=0.367586, train/loss=3.07773, validation/accuracy=0.34548, validation/loss=3.21312, validation/num_examples=50000
I0307 14:22:59.451671 139894567450368 logging_writer.py:48] [88100] global_step=88100, grad_norm=1.6057143211364746, loss=2.146674633026123
I0307 14:24:23.624753 139894559057664 logging_writer.py:48] [88200] global_step=88200, grad_norm=1.6297683715820312, loss=2.1244759559631348
I0307 14:26:00.213991 139894567450368 logging_writer.py:48] [88300] global_step=88300, grad_norm=1.5690315961837769, loss=2.1154489517211914
I0307 14:27:46.627223 139894559057664 logging_writer.py:48] [88400] global_step=88400, grad_norm=1.5064045190811157, loss=2.0098788738250732
I0307 14:28:57.491499 139894567450368 logging_writer.py:48] [88500] global_step=88500, grad_norm=1.6257872581481934, loss=2.178618907928467
I0307 14:30:12.717880 139894559057664 logging_writer.py:48] [88600] global_step=88600, grad_norm=1.3990414142608643, loss=1.9711353778839111
I0307 14:30:43.457040 140050941662400 spec.py:321] Evaluating on the training split.
I0307 14:30:54.208332 140050941662400 spec.py:333] Evaluating on the validation split.
I0307 14:31:12.903303 140050941662400 spec.py:349] Evaluating on the test split.
I0307 14:31:14.659262 140050941662400 submission_runner.py:469] Time since start: 41255.76s, 	Step: 88637, 	{'train/accuracy': 0.27425462007522583, 'train/loss': 3.7851626873016357, 'validation/accuracy': 0.2615799903869629, 'validation/loss': 3.885563611984253, 'validation/num_examples': 50000, 'test/accuracy': 0.20090000331401825, 'test/loss': 4.547630310058594, 'test/num_examples': 10000, 'score': 38306.55653214455, 'total_duration': 41255.75601744652, 'accumulated_submission_time': 38306.55653214455, 'accumulated_eval_time': 2930.895970106125, 'accumulated_logging_time': 7.954211950302124}
I0307 14:31:14.715405 139894567450368 logging_writer.py:48] [88637] accumulated_eval_time=2930.9, accumulated_logging_time=7.95421, accumulated_submission_time=38306.6, global_step=88637, preemption_count=0, score=38306.6, test/accuracy=0.2009, test/loss=4.54763, test/num_examples=10000, total_duration=41255.8, train/accuracy=0.274255, train/loss=3.78516, validation/accuracy=0.26158, validation/loss=3.88556, validation/num_examples=50000
I0307 14:31:57.305899 139894559057664 logging_writer.py:48] [88700] global_step=88700, grad_norm=1.4409643411636353, loss=1.9858458042144775
I0307 14:33:16.038433 139894567450368 logging_writer.py:48] [88800] global_step=88800, grad_norm=1.493299961090088, loss=2.057358980178833
I0307 14:34:49.793849 139894559057664 logging_writer.py:48] [88900] global_step=88900, grad_norm=1.5362439155578613, loss=2.100393533706665
I0307 14:36:02.896995 139894567450368 logging_writer.py:48] [89000] global_step=89000, grad_norm=1.406412124633789, loss=2.0358636379241943
I0307 14:37:21.229820 139894559057664 logging_writer.py:48] [89100] global_step=89100, grad_norm=1.4920355081558228, loss=1.984524130821228
I0307 14:38:25.867062 139894567450368 logging_writer.py:48] [89200] global_step=89200, grad_norm=1.5220324993133545, loss=2.0061659812927246
I0307 14:39:46.339789 140050941662400 spec.py:321] Evaluating on the training split.
I0307 14:39:56.976425 140050941662400 spec.py:333] Evaluating on the validation split.
I0307 14:40:21.227092 140050941662400 spec.py:349] Evaluating on the test split.
I0307 14:40:22.964449 140050941662400 submission_runner.py:469] Time since start: 41804.06s, 	Step: 89240, 	{'train/accuracy': 0.39190050959587097, 'train/loss': 2.837898015975952, 'validation/accuracy': 0.3554999828338623, 'validation/loss': 3.0690979957580566, 'validation/num_examples': 50000, 'test/accuracy': 0.27060002088546753, 'test/loss': 3.7492642402648926, 'test/num_examples': 10000, 'score': 38818.10015010834, 'total_duration': 41804.061200380325, 'accumulated_submission_time': 38818.10015010834, 'accumulated_eval_time': 2967.5205914974213, 'accumulated_logging_time': 8.019020318984985}
I0307 14:40:22.998853 139894559057664 logging_writer.py:48] [89240] accumulated_eval_time=2967.52, accumulated_logging_time=8.01902, accumulated_submission_time=38818.1, global_step=89240, preemption_count=0, score=38818.1, test/accuracy=0.2706, test/loss=3.74926, test/num_examples=10000, total_duration=41804.1, train/accuracy=0.391901, train/loss=2.8379, validation/accuracy=0.3555, validation/loss=3.0691, validation/num_examples=50000
I0307 14:42:13.413652 139894567450368 logging_writer.py:48] [89300] global_step=89300, grad_norm=1.660686731338501, loss=2.166140079498291
I0307 14:44:29.124748 139894559057664 logging_writer.py:48] [89400] global_step=89400, grad_norm=1.6214107275009155, loss=2.2269723415374756
I0307 14:46:34.672473 139894567450368 logging_writer.py:48] [89500] global_step=89500, grad_norm=1.785675287246704, loss=2.008058786392212
I0307 14:48:54.395350 140050941662400 spec.py:321] Evaluating on the training split.
I0307 14:49:05.218720 140050941662400 spec.py:333] Evaluating on the validation split.
I0307 14:49:27.956928 140050941662400 spec.py:349] Evaluating on the test split.
I0307 14:49:29.721446 140050941662400 submission_runner.py:469] Time since start: 42350.82s, 	Step: 89556, 	{'train/accuracy': 0.26658162474632263, 'train/loss': 3.921617031097412, 'validation/accuracy': 0.25562000274658203, 'validation/loss': 3.994880437850952, 'validation/num_examples': 50000, 'test/accuracy': 0.1736000031232834, 'test/loss': 4.9081010818481445, 'test/num_examples': 10000, 'score': 39329.451273441315, 'total_duration': 42350.818180799484, 'accumulated_submission_time': 39329.451273441315, 'accumulated_eval_time': 3002.8466506004333, 'accumulated_logging_time': 8.061416625976562}
I0307 14:49:29.765350 139894559057664 logging_writer.py:48] [89556] accumulated_eval_time=3002.85, accumulated_logging_time=8.06142, accumulated_submission_time=39329.5, global_step=89556, preemption_count=0, score=39329.5, test/accuracy=0.1736, test/loss=4.9081, test/num_examples=10000, total_duration=42350.8, train/accuracy=0.266582, train/loss=3.92162, validation/accuracy=0.25562, validation/loss=3.99488, validation/num_examples=50000
I0307 14:50:27.754384 139894567450368 logging_writer.py:48] [89600] global_step=89600, grad_norm=1.625809669494629, loss=2.118854284286499
I0307 14:51:40.145146 139894559057664 logging_writer.py:48] [89700] global_step=89700, grad_norm=1.5033708810806274, loss=2.0169777870178223
I0307 14:52:37.417880 139894567450368 logging_writer.py:48] [89800] global_step=89800, grad_norm=1.5498018264770508, loss=2.18652606010437
I0307 14:53:32.586158 139894559057664 logging_writer.py:48] [89900] global_step=89900, grad_norm=1.484636664390564, loss=2.0093464851379395
I0307 14:54:40.309104 139894567450368 logging_writer.py:48] [90000] global_step=90000, grad_norm=1.5337761640548706, loss=2.0479660034179688
I0307 14:57:14.318647 139894559057664 logging_writer.py:48] [90100] global_step=90100, grad_norm=1.4186010360717773, loss=1.9794552326202393
I0307 14:58:00.898380 140050941662400 spec.py:321] Evaluating on the training split.
I0307 14:58:11.376177 140050941662400 spec.py:333] Evaluating on the validation split.
I0307 14:58:31.593433 140050941662400 spec.py:349] Evaluating on the test split.
I0307 14:58:33.306449 140050941662400 submission_runner.py:469] Time since start: 42894.40s, 	Step: 90125, 	{'train/accuracy': 0.43134167790412903, 'train/loss': 2.5786147117614746, 'validation/accuracy': 0.4067800045013428, 'validation/loss': 2.743133306503296, 'validation/num_examples': 50000, 'test/accuracy': 0.31070002913475037, 'test/loss': 3.488515853881836, 'test/num_examples': 10000, 'score': 39840.51077198982, 'total_duration': 42894.403200149536, 'accumulated_submission_time': 39840.51077198982, 'accumulated_eval_time': 3035.2546877861023, 'accumulated_logging_time': 8.114333868026733}
I0307 14:58:33.353145 139894567450368 logging_writer.py:48] [90125] accumulated_eval_time=3035.25, accumulated_logging_time=8.11433, accumulated_submission_time=39840.5, global_step=90125, preemption_count=0, score=39840.5, test/accuracy=0.3107, test/loss=3.48852, test/num_examples=10000, total_duration=42894.4, train/accuracy=0.431342, train/loss=2.57861, validation/accuracy=0.40678, validation/loss=2.74313, validation/num_examples=50000
I0307 14:59:18.146516 139894559057664 logging_writer.py:48] [90200] global_step=90200, grad_norm=1.6287068128585815, loss=2.144469976425171
I0307 15:00:50.126415 139894567450368 logging_writer.py:48] [90300] global_step=90300, grad_norm=1.6413005590438843, loss=2.0815694332122803
I0307 15:02:53.112304 139894559057664 logging_writer.py:48] [90400] global_step=90400, grad_norm=1.5333640575408936, loss=2.173682451248169
I0307 15:04:13.825463 139894567450368 logging_writer.py:48] [90500] global_step=90500, grad_norm=1.599915862083435, loss=2.1291260719299316
I0307 15:05:34.491435 139894559057664 logging_writer.py:48] [90600] global_step=90600, grad_norm=1.5807597637176514, loss=2.105727434158325
I0307 15:07:04.213694 140050941662400 spec.py:321] Evaluating on the training split.
I0307 15:07:14.760239 140050941662400 spec.py:333] Evaluating on the validation split.
I0307 15:07:35.915369 140050941662400 spec.py:349] Evaluating on the test split.
I0307 15:07:37.643043 140050941662400 submission_runner.py:469] Time since start: 43438.74s, 	Step: 90688, 	{'train/accuracy': 0.3365154564380646, 'train/loss': 3.448878288269043, 'validation/accuracy': 0.3193399906158447, 'validation/loss': 3.5757927894592285, 'validation/num_examples': 50000, 'test/accuracy': 0.2378000169992447, 'test/loss': 4.406792640686035, 'test/num_examples': 10000, 'score': 40351.29848766327, 'total_duration': 43438.73979139328, 'accumulated_submission_time': 40351.29848766327, 'accumulated_eval_time': 3068.6840081214905, 'accumulated_logging_time': 8.168935775756836}
I0307 15:07:37.704289 139894567450368 logging_writer.py:48] [90688] accumulated_eval_time=3068.68, accumulated_logging_time=8.16894, accumulated_submission_time=40351.3, global_step=90688, preemption_count=0, score=40351.3, test/accuracy=0.2378, test/loss=4.40679, test/num_examples=10000, total_duration=43438.7, train/accuracy=0.336515, train/loss=3.44888, validation/accuracy=0.31934, validation/loss=3.57579, validation/num_examples=50000
I0307 15:07:42.760641 139894559057664 logging_writer.py:48] [90700] global_step=90700, grad_norm=1.554202675819397, loss=2.0814273357391357
I0307 15:10:25.732946 139894567450368 logging_writer.py:48] [90800] global_step=90800, grad_norm=1.5765222311019897, loss=2.0487313270568848
I0307 15:12:13.923296 139894559057664 logging_writer.py:48] [90900] global_step=90900, grad_norm=1.5800459384918213, loss=1.9687918424606323
I0307 15:14:10.810575 139894567450368 logging_writer.py:48] [91000] global_step=91000, grad_norm=1.7247741222381592, loss=2.1165308952331543
I0307 15:16:08.026161 140050941662400 spec.py:321] Evaluating on the training split.
I0307 15:16:19.084769 140050941662400 spec.py:333] Evaluating on the validation split.
I0307 15:16:44.916830 140050941662400 spec.py:349] Evaluating on the test split.
I0307 15:16:46.670849 140050941662400 submission_runner.py:469] Time since start: 43987.77s, 	Step: 91095, 	{'train/accuracy': 0.4096181392669678, 'train/loss': 2.7284374237060547, 'validation/accuracy': 0.3902999758720398, 'validation/loss': 2.8480730056762695, 'validation/num_examples': 50000, 'test/accuracy': 0.2891000211238861, 'test/loss': 3.645298957824707, 'test/num_examples': 10000, 'score': 40861.563559532166, 'total_duration': 43987.76760482788, 'accumulated_submission_time': 40861.563559532166, 'accumulated_eval_time': 3107.3286712169647, 'accumulated_logging_time': 8.238538265228271}
I0307 15:16:46.761095 139894559057664 logging_writer.py:48] [91095] accumulated_eval_time=3107.33, accumulated_logging_time=8.23854, accumulated_submission_time=40861.6, global_step=91095, preemption_count=0, score=40861.6, test/accuracy=0.2891, test/loss=3.6453, test/num_examples=10000, total_duration=43987.8, train/accuracy=0.409618, train/loss=2.72844, validation/accuracy=0.3903, validation/loss=2.84807, validation/num_examples=50000
I0307 15:16:49.064203 139894567450368 logging_writer.py:48] [91100] global_step=91100, grad_norm=1.6757149696350098, loss=2.0161163806915283
I0307 15:18:44.888421 139894559057664 logging_writer.py:48] [91200] global_step=91200, grad_norm=1.6071979999542236, loss=1.955496072769165
I0307 15:21:06.905881 139894567450368 logging_writer.py:48] [91300] global_step=91300, grad_norm=1.4504317045211792, loss=1.9508249759674072
I0307 15:22:08.130576 139894559057664 logging_writer.py:48] [91400] global_step=91400, grad_norm=1.6352980136871338, loss=2.080497980117798
I0307 15:24:49.022280 139894567450368 logging_writer.py:48] [91500] global_step=91500, grad_norm=1.7454484701156616, loss=2.061896324157715
I0307 15:25:17.374355 140050941662400 spec.py:321] Evaluating on the training split.
I0307 15:25:28.022098 140050941662400 spec.py:333] Evaluating on the validation split.
I0307 15:25:49.072300 140050941662400 spec.py:349] Evaluating on the test split.
I0307 15:25:50.786937 140050941662400 submission_runner.py:469] Time since start: 44531.88s, 	Step: 91521, 	{'train/accuracy': 0.398457407951355, 'train/loss': 2.791975259780884, 'validation/accuracy': 0.37741997838020325, 'validation/loss': 2.8960323333740234, 'validation/num_examples': 50000, 'test/accuracy': 0.2897000014781952, 'test/loss': 3.5762834548950195, 'test/num_examples': 10000, 'score': 41372.10156774521, 'total_duration': 44531.883685827255, 'accumulated_submission_time': 41372.10156774521, 'accumulated_eval_time': 3140.7412152290344, 'accumulated_logging_time': 8.353724479675293}
I0307 15:25:50.857154 139894559057664 logging_writer.py:48] [91521] accumulated_eval_time=3140.74, accumulated_logging_time=8.35372, accumulated_submission_time=41372.1, global_step=91521, preemption_count=0, score=41372.1, test/accuracy=0.2897, test/loss=3.57628, test/num_examples=10000, total_duration=44531.9, train/accuracy=0.398457, train/loss=2.79198, validation/accuracy=0.37742, validation/loss=2.89603, validation/num_examples=50000
I0307 15:27:07.472900 139894567450368 logging_writer.py:48] [91600] global_step=91600, grad_norm=1.5591354370117188, loss=2.0755422115325928
I0307 15:28:54.262246 139894559057664 logging_writer.py:48] [91700] global_step=91700, grad_norm=1.4896730184555054, loss=1.957820177078247
I0307 15:34:24.674873 140050941662400 spec.py:321] Evaluating on the training split.
I0307 15:34:35.158222 140050941662400 spec.py:333] Evaluating on the validation split.
I0307 15:35:01.135696 140050941662400 spec.py:349] Evaluating on the test split.
I0307 15:35:02.859980 140050941662400 submission_runner.py:469] Time since start: 45083.96s, 	Step: 91796, 	{'train/accuracy': 0.4407086968421936, 'train/loss': 2.493875503540039, 'validation/accuracy': 0.4074999988079071, 'validation/loss': 2.738919734954834, 'validation/num_examples': 50000, 'test/accuracy': 0.31300002336502075, 'test/loss': 3.5155811309814453, 'test/num_examples': 10000, 'score': 41885.878187179565, 'total_duration': 45083.95673274994, 'accumulated_submission_time': 41885.878187179565, 'accumulated_eval_time': 3178.9262914657593, 'accumulated_logging_time': 8.432494878768921}
I0307 15:35:02.879243 139894567450368 logging_writer.py:48] [91796] accumulated_eval_time=3178.93, accumulated_logging_time=8.43249, accumulated_submission_time=41885.9, global_step=91796, preemption_count=0, score=41885.9, test/accuracy=0.313, test/loss=3.51558, test/num_examples=10000, total_duration=45084, train/accuracy=0.440709, train/loss=2.49388, validation/accuracy=0.4075, validation/loss=2.73892, validation/num_examples=50000
I0307 15:35:04.811498 139894559057664 logging_writer.py:48] [91800] global_step=91800, grad_norm=1.6891751289367676, loss=1.9002633094787598
I0307 15:39:18.512928 139894567450368 logging_writer.py:48] [91900] global_step=91900, grad_norm=1.5294195413589478, loss=2.094146966934204
I0307 15:42:55.601983 139894559057664 logging_writer.py:48] [92000] global_step=92000, grad_norm=1.5622782707214355, loss=2.1098108291625977
I0307 15:43:34.935386 140050941662400 spec.py:321] Evaluating on the training split.
I0307 15:43:45.622153 140050941662400 spec.py:333] Evaluating on the validation split.
I0307 15:44:07.805520 140050941662400 spec.py:349] Evaluating on the test split.
I0307 15:44:09.570750 140050941662400 submission_runner.py:469] Time since start: 45630.67s, 	Step: 92019, 	{'train/accuracy': 0.4006098508834839, 'train/loss': 2.8551719188690186, 'validation/accuracy': 0.3722800016403198, 'validation/loss': 3.0422046184539795, 'validation/num_examples': 50000, 'test/accuracy': 0.2639999985694885, 'test/loss': 3.9146673679351807, 'test/num_examples': 10000, 'score': 42397.89996480942, 'total_duration': 45630.667496204376, 'accumulated_submission_time': 42397.89996480942, 'accumulated_eval_time': 3213.561616420746, 'accumulated_logging_time': 8.459996938705444}
I0307 15:44:09.592742 139894567450368 logging_writer.py:48] [92019] accumulated_eval_time=3213.56, accumulated_logging_time=8.46, accumulated_submission_time=42397.9, global_step=92019, preemption_count=0, score=42397.9, test/accuracy=0.264, test/loss=3.91467, test/num_examples=10000, total_duration=45630.7, train/accuracy=0.40061, train/loss=2.85517, validation/accuracy=0.37228, validation/loss=3.0422, validation/num_examples=50000
I0307 15:46:43.287236 139894559057664 logging_writer.py:48] [92100] global_step=92100, grad_norm=1.713746190071106, loss=2.156345844268799
I0307 15:50:47.534549 139894567450368 logging_writer.py:48] [92200] global_step=92200, grad_norm=1.5817627906799316, loss=2.1372506618499756
I0307 15:52:31.929142 139894559057664 logging_writer.py:48] [92300] global_step=92300, grad_norm=1.540490746498108, loss=2.078181028366089
I0307 15:52:39.702186 140050941662400 spec.py:321] Evaluating on the training split.
I0307 15:52:50.038705 140050941662400 spec.py:333] Evaluating on the validation split.
I0307 15:53:09.534070 140050941662400 spec.py:349] Evaluating on the test split.
I0307 15:53:11.288383 140050941662400 submission_runner.py:469] Time since start: 46172.39s, 	Step: 92313, 	{'train/accuracy': 0.3252550959587097, 'train/loss': 3.3468902111053467, 'validation/accuracy': 0.31182000041007996, 'validation/loss': 3.4744107723236084, 'validation/num_examples': 50000, 'test/accuracy': 0.2233000099658966, 'test/loss': 4.218242168426514, 'test/num_examples': 10000, 'score': 42907.967683553696, 'total_duration': 46172.38513112068, 'accumulated_submission_time': 42907.967683553696, 'accumulated_eval_time': 3245.147781610489, 'accumulated_logging_time': 8.490842342376709}
I0307 15:53:11.332355 139894567450368 logging_writer.py:48] [92313] accumulated_eval_time=3245.15, accumulated_logging_time=8.49084, accumulated_submission_time=42908, global_step=92313, preemption_count=0, score=42908, test/accuracy=0.2233, test/loss=4.21824, test/num_examples=10000, total_duration=46172.4, train/accuracy=0.325255, train/loss=3.34689, validation/accuracy=0.31182, validation/loss=3.47441, validation/num_examples=50000
I0307 15:54:14.042506 139894559057664 logging_writer.py:48] [92400] global_step=92400, grad_norm=1.5774258375167847, loss=2.0842745304107666
I0307 15:57:44.696569 139894567450368 logging_writer.py:48] [92500] global_step=92500, grad_norm=1.5611941814422607, loss=2.123671293258667
I0307 16:01:44.239736 140050941662400 spec.py:321] Evaluating on the training split.
I0307 16:01:54.857963 140050941662400 spec.py:333] Evaluating on the validation split.
I0307 16:02:22.837181 140050941662400 spec.py:349] Evaluating on the test split.
I0307 16:02:24.608381 140050941662400 submission_runner.py:469] Time since start: 46725.71s, 	Step: 92599, 	{'train/accuracy': 0.3634207546710968, 'train/loss': 3.0162556171417236, 'validation/accuracy': 0.3368600010871887, 'validation/loss': 3.1696557998657227, 'validation/num_examples': 50000, 'test/accuracy': 0.25600001215934753, 'test/loss': 3.8993678092956543, 'test/num_examples': 10000, 'score': 43420.83335399628, 'total_duration': 46725.70512652397, 'accumulated_submission_time': 43420.83335399628, 'accumulated_eval_time': 3285.5164034366608, 'accumulated_logging_time': 8.543708086013794}
I0307 16:02:24.628518 139894559057664 logging_writer.py:48] [92599] accumulated_eval_time=3285.52, accumulated_logging_time=8.54371, accumulated_submission_time=43420.8, global_step=92599, preemption_count=0, score=43420.8, test/accuracy=0.256, test/loss=3.89937, test/num_examples=10000, total_duration=46725.7, train/accuracy=0.363421, train/loss=3.01626, validation/accuracy=0.33686, validation/loss=3.16966, validation/num_examples=50000
I0307 16:02:25.362190 139894567450368 logging_writer.py:48] [92600] global_step=92600, grad_norm=1.6257867813110352, loss=1.9759933948516846
I0307 16:05:18.569674 139894559057664 logging_writer.py:48] [92700] global_step=92700, grad_norm=1.7201898097991943, loss=2.0581114292144775
I0307 16:06:38.220540 139894567450368 logging_writer.py:48] [92800] global_step=92800, grad_norm=1.562147855758667, loss=1.9912620782852173
I0307 16:08:01.049886 139894559057664 logging_writer.py:48] [92900] global_step=92900, grad_norm=1.5925542116165161, loss=1.9045460224151611
I0307 16:09:28.157599 139894567450368 logging_writer.py:48] [93000] global_step=93000, grad_norm=1.4975905418395996, loss=2.0294911861419678
I0307 16:10:55.499643 140050941662400 spec.py:321] Evaluating on the training split.
I0307 16:11:06.496834 140050941662400 spec.py:333] Evaluating on the validation split.
I0307 16:11:30.877889 140050941662400 spec.py:349] Evaluating on the test split.
I0307 16:11:32.609867 140050941662400 submission_runner.py:469] Time since start: 47273.71s, 	Step: 93099, 	{'train/accuracy': 0.3887316584587097, 'train/loss': 2.867079019546509, 'validation/accuracy': 0.3426799774169922, 'validation/loss': 3.1945977210998535, 'validation/num_examples': 50000, 'test/accuracy': 0.25440001487731934, 'test/loss': 3.9839487075805664, 'test/num_examples': 10000, 'score': 43931.63784909248, 'total_duration': 47273.70661854744, 'accumulated_submission_time': 43931.63784909248, 'accumulated_eval_time': 3322.626597881317, 'accumulated_logging_time': 8.57212781906128}
I0307 16:11:32.655009 139894559057664 logging_writer.py:48] [93099] accumulated_eval_time=3322.63, accumulated_logging_time=8.57213, accumulated_submission_time=43931.6, global_step=93099, preemption_count=0, score=43931.6, test/accuracy=0.2544, test/loss=3.98395, test/num_examples=10000, total_duration=47273.7, train/accuracy=0.388732, train/loss=2.86708, validation/accuracy=0.34268, validation/loss=3.1946, validation/num_examples=50000
I0307 16:11:33.414317 139894567450368 logging_writer.py:48] [93100] global_step=93100, grad_norm=1.5720503330230713, loss=1.9939334392547607
I0307 16:13:51.963311 139894559057664 logging_writer.py:48] [93200] global_step=93200, grad_norm=1.7031079530715942, loss=2.0432732105255127
I0307 16:18:34.438528 139894567450368 logging_writer.py:48] [93300] global_step=93300, grad_norm=1.7017487287521362, loss=2.003030300140381
I0307 16:20:04.516399 140050941662400 spec.py:321] Evaluating on the training split.
I0307 16:20:14.371052 140050941662400 spec.py:333] Evaluating on the validation split.
I0307 16:20:54.585739 140050941662400 spec.py:349] Evaluating on the test split.
I0307 16:20:56.367223 140050941662400 submission_runner.py:469] Time since start: 47837.46s, 	Step: 93322, 	{'train/accuracy': 0.3686622977256775, 'train/loss': 3.066848039627075, 'validation/accuracy': 0.3373199999332428, 'validation/loss': 3.329524278640747, 'validation/num_examples': 50000, 'test/accuracy': 0.25380000472068787, 'test/loss': 4.109381198883057, 'test/num_examples': 10000, 'score': 44443.43331313133, 'total_duration': 47837.46397590637, 'accumulated_submission_time': 44443.43331313133, 'accumulated_eval_time': 3374.477387905121, 'accumulated_logging_time': 8.656093835830688}
I0307 16:20:56.387106 139894559057664 logging_writer.py:48] [93322] accumulated_eval_time=3374.48, accumulated_logging_time=8.65609, accumulated_submission_time=44443.4, global_step=93322, preemption_count=0, score=44443.4, test/accuracy=0.2538, test/loss=4.10938, test/num_examples=10000, total_duration=47837.5, train/accuracy=0.368662, train/loss=3.06685, validation/accuracy=0.33732, validation/loss=3.32952, validation/num_examples=50000
I0307 16:22:10.286488 139894567450368 logging_writer.py:48] [93400] global_step=93400, grad_norm=1.755196452140808, loss=2.080881118774414
I0307 16:24:36.067050 139894559057664 logging_writer.py:48] [93500] global_step=93500, grad_norm=1.5695364475250244, loss=2.065258026123047
I0307 16:27:05.195892 139894567450368 logging_writer.py:48] [93600] global_step=93600, grad_norm=1.6005377769470215, loss=2.2029075622558594
I0307 16:29:26.468358 140050941662400 spec.py:321] Evaluating on the training split.
I0307 16:29:37.447575 140050941662400 spec.py:333] Evaluating on the validation split.
I0307 16:29:57.343976 140050941662400 spec.py:349] Evaluating on the test split.
I0307 16:29:59.099557 140050941662400 submission_runner.py:469] Time since start: 48380.20s, 	Step: 93699, 	{'train/accuracy': 0.4760044515132904, 'train/loss': 2.3018131256103516, 'validation/accuracy': 0.44551998376846313, 'validation/loss': 2.5106801986694336, 'validation/num_examples': 50000, 'test/accuracy': 0.3330000042915344, 'test/loss': 3.297938108444214, 'test/num_examples': 10000, 'score': 44953.46182656288, 'total_duration': 48380.19630932808, 'accumulated_submission_time': 44953.46182656288, 'accumulated_eval_time': 3407.1085555553436, 'accumulated_logging_time': 8.683910369873047}
I0307 16:29:59.135860 139894559057664 logging_writer.py:48] [93699] accumulated_eval_time=3407.11, accumulated_logging_time=8.68391, accumulated_submission_time=44953.5, global_step=93699, preemption_count=0, score=44953.5, test/accuracy=0.333, test/loss=3.29794, test/num_examples=10000, total_duration=48380.2, train/accuracy=0.476004, train/loss=2.30181, validation/accuracy=0.44552, validation/loss=2.51068, validation/num_examples=50000
I0307 16:29:59.855028 139894567450368 logging_writer.py:48] [93700] global_step=93700, grad_norm=1.437036156654358, loss=2.040618658065796
I0307 16:35:53.302457 139894559057664 logging_writer.py:48] [93800] global_step=93800, grad_norm=1.496271014213562, loss=2.0192089080810547
I0307 16:38:32.331512 140050941662400 spec.py:321] Evaluating on the training split.
I0307 16:38:42.827363 140050941662400 spec.py:333] Evaluating on the validation split.
I0307 16:39:09.120080 140050941662400 spec.py:349] Evaluating on the test split.
I0307 16:39:10.891467 140050941662400 submission_runner.py:469] Time since start: 48931.99s, 	Step: 93838, 	{'train/accuracy': 0.27736368775367737, 'train/loss': 3.8772177696228027, 'validation/accuracy': 0.2561199963092804, 'validation/loss': 4.074773788452148, 'validation/num_examples': 50000, 'test/accuracy': 0.19770000874996185, 'test/loss': 4.7148871421813965, 'test/num_examples': 10000, 'score': 45466.63313412666, 'total_duration': 48931.9882144928, 'accumulated_submission_time': 45466.63313412666, 'accumulated_eval_time': 3445.668481826782, 'accumulated_logging_time': 8.729026556015015}
I0307 16:39:10.913218 139894567450368 logging_writer.py:48] [93838] accumulated_eval_time=3445.67, accumulated_logging_time=8.72903, accumulated_submission_time=45466.6, global_step=93838, preemption_count=0, score=45466.6, test/accuracy=0.1977, test/loss=4.71489, test/num_examples=10000, total_duration=48932, train/accuracy=0.277364, train/loss=3.87722, validation/accuracy=0.25612, validation/loss=4.07477, validation/num_examples=50000
I0307 16:42:38.172156 139894559057664 logging_writer.py:48] [93900] global_step=93900, grad_norm=1.5783298015594482, loss=2.0490224361419678
I0307 16:45:17.357164 139894567450368 logging_writer.py:48] [94000] global_step=94000, grad_norm=1.6163173913955688, loss=1.882460117340088
I0307 16:47:42.026665 140050941662400 spec.py:321] Evaluating on the training split.
I0307 16:47:52.739345 140050941662400 spec.py:333] Evaluating on the validation split.
I0307 16:48:11.308287 140050941662400 spec.py:349] Evaluating on the test split.
I0307 16:48:13.032895 140050941662400 submission_runner.py:469] Time since start: 49474.13s, 	Step: 94089, 	{'train/accuracy': 0.26658162474632263, 'train/loss': 3.9404215812683105, 'validation/accuracy': 0.25373998284339905, 'validation/loss': 4.04727029800415, 'validation/num_examples': 50000, 'test/accuracy': 0.18300001323223114, 'test/loss': 4.778444766998291, 'test/num_examples': 10000, 'score': 45977.70986509323, 'total_duration': 49474.12964987755, 'accumulated_submission_time': 45977.70986509323, 'accumulated_eval_time': 3476.6746854782104, 'accumulated_logging_time': 8.758967638015747}
I0307 16:48:13.052419 139894559057664 logging_writer.py:48] [94089] accumulated_eval_time=3476.67, accumulated_logging_time=8.75897, accumulated_submission_time=45977.7, global_step=94089, preemption_count=0, score=45977.7, test/accuracy=0.183, test/loss=4.77844, test/num_examples=10000, total_duration=49474.1, train/accuracy=0.266582, train/loss=3.94042, validation/accuracy=0.25374, validation/loss=4.04727, validation/num_examples=50000
I0307 16:48:17.819829 139894567450368 logging_writer.py:48] [94100] global_step=94100, grad_norm=1.5500985383987427, loss=2.121516704559326
I0307 16:51:49.241229 139894559057664 logging_writer.py:48] [94200] global_step=94200, grad_norm=1.7176812887191772, loss=2.053892135620117
I0307 16:55:25.936325 139894567450368 logging_writer.py:48] [94300] global_step=94300, grad_norm=1.4276448488235474, loss=1.9317543506622314
I0307 16:56:44.841147 140050941662400 spec.py:321] Evaluating on the training split.
I0307 16:56:55.448750 140050941662400 spec.py:333] Evaluating on the validation split.
I0307 16:57:17.657064 140050941662400 spec.py:349] Evaluating on the test split.
I0307 16:57:19.406934 140050941662400 submission_runner.py:469] Time since start: 50020.50s, 	Step: 94338, 	{'train/accuracy': 0.41011637449264526, 'train/loss': 2.7966175079345703, 'validation/accuracy': 0.3874000012874603, 'validation/loss': 2.931044816970825, 'validation/num_examples': 50000, 'test/accuracy': 0.28620001673698425, 'test/loss': 3.7599411010742188, 'test/num_examples': 10000, 'score': 46489.38882517815, 'total_duration': 50020.503683805466, 'accumulated_submission_time': 46489.38882517815, 'accumulated_eval_time': 3511.2404432296753, 'accumulated_logging_time': 8.859675168991089}
I0307 16:57:19.428261 139894559057664 logging_writer.py:48] [94338] accumulated_eval_time=3511.24, accumulated_logging_time=8.85968, accumulated_submission_time=46489.4, global_step=94338, preemption_count=0, score=46489.4, test/accuracy=0.2862, test/loss=3.75994, test/num_examples=10000, total_duration=50020.5, train/accuracy=0.410116, train/loss=2.79662, validation/accuracy=0.3874, validation/loss=2.93104, validation/num_examples=50000
I0307 16:59:13.170603 139894567450368 logging_writer.py:48] [94400] global_step=94400, grad_norm=1.6773545742034912, loss=1.9920045137405396
I0307 17:02:45.697839 139894559057664 logging_writer.py:48] [94500] global_step=94500, grad_norm=1.668480634689331, loss=2.0768723487854004
I0307 17:05:49.848659 140050941662400 spec.py:321] Evaluating on the training split.
I0307 17:06:00.314512 140050941662400 spec.py:333] Evaluating on the validation split.
I0307 17:06:19.051903 140050941662400 spec.py:349] Evaluating on the test split.
I0307 17:06:20.827813 140050941662400 submission_runner.py:469] Time since start: 50561.92s, 	Step: 94586, 	{'train/accuracy': 0.4093989133834839, 'train/loss': 2.7300002574920654, 'validation/accuracy': 0.38165998458862305, 'validation/loss': 2.9146668910980225, 'validation/num_examples': 50000, 'test/accuracy': 0.27730000019073486, 'test/loss': 3.7323670387268066, 'test/num_examples': 10000, 'score': 46999.771547317505, 'total_duration': 50561.9245467186, 'accumulated_submission_time': 46999.771547317505, 'accumulated_eval_time': 3542.219552755356, 'accumulated_logging_time': 8.889421224594116}
I0307 17:06:20.849234 139894567450368 logging_writer.py:48] [94586] accumulated_eval_time=3542.22, accumulated_logging_time=8.88942, accumulated_submission_time=46999.8, global_step=94586, preemption_count=0, score=46999.8, test/accuracy=0.2773, test/loss=3.73237, test/num_examples=10000, total_duration=50561.9, train/accuracy=0.409399, train/loss=2.73, validation/accuracy=0.38166, validation/loss=2.91467, validation/num_examples=50000
I0307 17:06:26.803644 139894559057664 logging_writer.py:48] [94600] global_step=94600, grad_norm=1.5652297735214233, loss=2.1236116886138916
I0307 17:08:54.191932 139894567450368 logging_writer.py:48] [94700] global_step=94700, grad_norm=1.5271867513656616, loss=2.116253137588501
I0307 17:11:58.073856 139894559057664 logging_writer.py:48] [94800] global_step=94800, grad_norm=1.5896137952804565, loss=2.077432155609131
I0307 17:14:52.010331 140050941662400 spec.py:321] Evaluating on the training split.
I0307 17:15:02.589705 140050941662400 spec.py:333] Evaluating on the validation split.
I0307 17:15:21.668687 140050941662400 spec.py:349] Evaluating on the test split.
I0307 17:15:23.442447 140050941662400 submission_runner.py:469] Time since start: 51104.54s, 	Step: 94876, 	{'train/accuracy': 0.1908482164144516, 'train/loss': 4.752130031585693, 'validation/accuracy': 0.17771999537944794, 'validation/loss': 4.92516565322876, 'validation/num_examples': 50000, 'test/accuracy': 0.12200000882148743, 'test/loss': 5.721824645996094, 'test/num_examples': 10000, 'score': 47510.89162516594, 'total_duration': 51104.539194345474, 'accumulated_submission_time': 47510.89162516594, 'accumulated_eval_time': 3573.6516485214233, 'accumulated_logging_time': 8.919398307800293}
I0307 17:15:23.496511 139894567450368 logging_writer.py:48] [94876] accumulated_eval_time=3573.65, accumulated_logging_time=8.9194, accumulated_submission_time=47510.9, global_step=94876, preemption_count=0, score=47510.9, test/accuracy=0.122, test/loss=5.72182, test/num_examples=10000, total_duration=51104.5, train/accuracy=0.190848, train/loss=4.75213, validation/accuracy=0.17772, validation/loss=4.92517, validation/num_examples=50000
I0307 17:15:56.714110 139894559057664 logging_writer.py:48] [94900] global_step=94900, grad_norm=1.8408598899841309, loss=2.1734490394592285
I0307 17:19:36.374007 139894567450368 logging_writer.py:48] [95000] global_step=95000, grad_norm=1.6044056415557861, loss=2.100942373275757
2025-03-07 17:22:07.550956: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 17:23:14.941510 139894559057664 logging_writer.py:48] [95100] global_step=95100, grad_norm=1.9121203422546387, loss=2.1781158447265625
I0307 17:23:53.462234 140050941662400 spec.py:321] Evaluating on the training split.
I0307 17:24:04.496765 140050941662400 spec.py:333] Evaluating on the validation split.
I0307 17:24:28.779947 140050941662400 spec.py:349] Evaluating on the test split.
I0307 17:24:30.517992 140050941662400 submission_runner.py:469] Time since start: 51651.61s, 	Step: 95119, 	{'train/accuracy': 0.4665975570678711, 'train/loss': 2.3335580825805664, 'validation/accuracy': 0.4346199929714203, 'validation/loss': 2.5399229526519775, 'validation/num_examples': 50000, 'test/accuracy': 0.3294000029563904, 'test/loss': 3.31132435798645, 'test/num_examples': 10000, 'score': 48020.814947366714, 'total_duration': 51651.61473989487, 'accumulated_submission_time': 48020.814947366714, 'accumulated_eval_time': 3610.7073690891266, 'accumulated_logging_time': 8.99016284942627}
I0307 17:24:30.561421 139894567450368 logging_writer.py:48] [95119] accumulated_eval_time=3610.71, accumulated_logging_time=8.99016, accumulated_submission_time=48020.8, global_step=95119, preemption_count=0, score=48020.8, test/accuracy=0.3294, test/loss=3.31132, test/num_examples=10000, total_duration=51651.6, train/accuracy=0.466598, train/loss=2.33356, validation/accuracy=0.43462, validation/loss=2.53992, validation/num_examples=50000
I0307 17:27:07.371438 139894559057664 logging_writer.py:48] [95200] global_step=95200, grad_norm=1.4468153715133667, loss=2.1038050651550293
I0307 17:30:40.601682 139894567450368 logging_writer.py:48] [95300] global_step=95300, grad_norm=1.6450719833374023, loss=2.0511672496795654
I0307 17:33:01.935547 140050941662400 spec.py:321] Evaluating on the training split.
I0307 17:33:12.626638 140050941662400 spec.py:333] Evaluating on the validation split.
I0307 17:33:35.229383 140050941662400 spec.py:349] Evaluating on the test split.
I0307 17:33:36.961620 140050941662400 submission_runner.py:469] Time since start: 52198.06s, 	Step: 95367, 	{'train/accuracy': 0.2788584232330322, 'train/loss': 3.994102954864502, 'validation/accuracy': 0.26231998205184937, 'validation/loss': 4.125890731811523, 'validation/num_examples': 50000, 'test/accuracy': 0.1980000138282776, 'test/loss': 4.870264053344727, 'test/num_examples': 10000, 'score': 48532.13700556755, 'total_duration': 52198.0583691597, 'accumulated_submission_time': 48532.13700556755, 'accumulated_eval_time': 3645.7334113121033, 'accumulated_logging_time': 9.056554079055786}
I0307 17:33:37.004769 139894559057664 logging_writer.py:48] [95367] accumulated_eval_time=3645.73, accumulated_logging_time=9.05655, accumulated_submission_time=48532.1, global_step=95367, preemption_count=0, score=48532.1, test/accuracy=0.198, test/loss=4.87026, test/num_examples=10000, total_duration=52198.1, train/accuracy=0.278858, train/loss=3.9941, validation/accuracy=0.26232, validation/loss=4.12589, validation/num_examples=50000
I0307 17:34:28.586869 139894567450368 logging_writer.py:48] [95400] global_step=95400, grad_norm=1.6239229440689087, loss=1.9734673500061035
I0307 17:38:01.628869 139894559057664 logging_writer.py:48] [95500] global_step=95500, grad_norm=1.6611820459365845, loss=2.180170774459839
I0307 17:41:36.137638 139894567450368 logging_writer.py:48] [95600] global_step=95600, grad_norm=1.5683362483978271, loss=1.993605613708496
I0307 17:42:08.453717 140050941662400 spec.py:321] Evaluating on the training split.
I0307 17:42:18.832609 140050941662400 spec.py:333] Evaluating on the validation split.
I0307 17:42:38.727709 140050941662400 spec.py:349] Evaluating on the test split.
I0307 17:42:40.498220 140050941662400 submission_runner.py:469] Time since start: 52741.59s, 	Step: 95616, 	{'train/accuracy': 0.44696667790412903, 'train/loss': 2.4894931316375732, 'validation/accuracy': 0.4175799787044525, 'validation/loss': 2.6646511554718018, 'validation/num_examples': 50000, 'test/accuracy': 0.30560001730918884, 'test/loss': 3.5345771312713623, 'test/num_examples': 10000, 'score': 49043.5319917202, 'total_duration': 52741.59497094154, 'accumulated_submission_time': 49043.5319917202, 'accumulated_eval_time': 3677.777877807617, 'accumulated_logging_time': 9.12613034248352}
I0307 17:42:40.518949 139894559057664 logging_writer.py:48] [95616] accumulated_eval_time=3677.78, accumulated_logging_time=9.12613, accumulated_submission_time=49043.5, global_step=95616, preemption_count=0, score=49043.5, test/accuracy=0.3056, test/loss=3.53458, test/num_examples=10000, total_duration=52741.6, train/accuracy=0.446967, train/loss=2.48949, validation/accuracy=0.41758, validation/loss=2.66465, validation/num_examples=50000
I0307 17:45:22.163157 139894567450368 logging_writer.py:48] [95700] global_step=95700, grad_norm=1.746586561203003, loss=2.0747921466827393
I0307 17:48:55.866180 139894559057664 logging_writer.py:48] [95800] global_step=95800, grad_norm=1.388751745223999, loss=1.9048972129821777
I0307 17:51:10.657248 140050941662400 spec.py:321] Evaluating on the training split.
I0307 17:51:21.221284 140050941662400 spec.py:333] Evaluating on the validation split.
I0307 17:51:39.910489 140050941662400 spec.py:349] Evaluating on the test split.
I0307 17:51:41.686575 140050941662400 submission_runner.py:469] Time since start: 53282.78s, 	Step: 95864, 	{'train/accuracy': 0.3290417790412903, 'train/loss': 3.4775454998016357, 'validation/accuracy': 0.3049599826335907, 'validation/loss': 3.60438871383667, 'validation/num_examples': 50000, 'test/accuracy': 0.23030000925064087, 'test/loss': 4.359577655792236, 'test/num_examples': 10000, 'score': 49553.63417387009, 'total_duration': 53282.78332543373, 'accumulated_submission_time': 49553.63417387009, 'accumulated_eval_time': 3708.807176589966, 'accumulated_logging_time': 9.154602527618408}
I0307 17:51:41.707986 139894567450368 logging_writer.py:48] [95864] accumulated_eval_time=3708.81, accumulated_logging_time=9.1546, accumulated_submission_time=49553.6, global_step=95864, preemption_count=0, score=49553.6, test/accuracy=0.2303, test/loss=4.35958, test/num_examples=10000, total_duration=53282.8, train/accuracy=0.329042, train/loss=3.47755, validation/accuracy=0.30496, validation/loss=3.60439, validation/num_examples=50000
I0307 17:52:40.269663 139894559057664 logging_writer.py:48] [95900] global_step=95900, grad_norm=1.6595654487609863, loss=1.951378583908081
I0307 17:54:50.309898 139894567450368 logging_writer.py:48] [96000] global_step=96000, grad_norm=1.592821478843689, loss=2.112410306930542
I0307 17:58:26.387334 139894559057664 logging_writer.py:48] [96100] global_step=96100, grad_norm=1.5613758563995361, loss=2.0439488887786865
I0307 18:00:12.837136 140050941662400 spec.py:321] Evaluating on the training split.
I0307 18:00:23.873856 140050941662400 spec.py:333] Evaluating on the validation split.
I0307 18:00:48.825338 140050941662400 spec.py:349] Evaluating on the test split.
I0307 18:00:50.612243 140050941662400 submission_runner.py:469] Time since start: 53831.71s, 	Step: 96150, 	{'train/accuracy': 0.2837611436843872, 'train/loss': 4.332614898681641, 'validation/accuracy': 0.2609599828720093, 'validation/loss': 4.480642318725586, 'validation/num_examples': 50000, 'test/accuracy': 0.17550000548362732, 'test/loss': 5.640755653381348, 'test/num_examples': 10000, 'score': 50064.72196316719, 'total_duration': 53831.70897316933, 'accumulated_submission_time': 50064.72196316719, 'accumulated_eval_time': 3746.5822303295135, 'accumulated_logging_time': 9.18456244468689}
I0307 18:00:50.662481 139894567450368 logging_writer.py:48] [96150] accumulated_eval_time=3746.58, accumulated_logging_time=9.18456, accumulated_submission_time=50064.7, global_step=96150, preemption_count=0, score=50064.7, test/accuracy=0.1755, test/loss=5.64076, test/num_examples=10000, total_duration=53831.7, train/accuracy=0.283761, train/loss=4.33261, validation/accuracy=0.26096, validation/loss=4.48064, validation/num_examples=50000
I0307 18:02:21.929062 139894559057664 logging_writer.py:48] [96200] global_step=96200, grad_norm=1.5734952688217163, loss=2.0000650882720947
I0307 18:06:02.496079 139894567450368 logging_writer.py:48] [96300] global_step=96300, grad_norm=1.592929482460022, loss=1.992746353149414
I0307 18:09:24.004390 140050941662400 spec.py:321] Evaluating on the training split.
I0307 18:09:33.574131 140050941662400 spec.py:333] Evaluating on the validation split.
I0307 18:09:57.380600 140050941662400 spec.py:349] Evaluating on the test split.
I0307 18:09:59.157124 140050941662400 submission_runner.py:469] Time since start: 54380.25s, 	Step: 96399, 	{'train/accuracy': 0.2892020046710968, 'train/loss': 3.81107497215271, 'validation/accuracy': 0.2626200020313263, 'validation/loss': 4.066309452056885, 'validation/num_examples': 50000, 'test/accuracy': 0.1972000151872635, 'test/loss': 4.829308032989502, 'test/num_examples': 10000, 'score': 50578.02612900734, 'total_duration': 54380.25386428833, 'accumulated_submission_time': 50578.02612900734, 'accumulated_eval_time': 3781.734937429428, 'accumulated_logging_time': 9.243118524551392}
I0307 18:09:59.178064 139894559057664 logging_writer.py:48] [96399] accumulated_eval_time=3781.73, accumulated_logging_time=9.24312, accumulated_submission_time=50578, global_step=96399, preemption_count=0, score=50578, test/accuracy=0.1972, test/loss=4.82931, test/num_examples=10000, total_duration=54380.3, train/accuracy=0.289202, train/loss=3.81107, validation/accuracy=0.26262, validation/loss=4.06631, validation/num_examples=50000
I0307 18:09:59.910699 139894567450368 logging_writer.py:48] [96400] global_step=96400, grad_norm=1.8458837270736694, loss=1.9223755598068237
I0307 18:16:53.731570 139894559057664 logging_writer.py:48] [96500] global_step=96500, grad_norm=1.6512494087219238, loss=2.1514503955841064
I0307 18:18:31.634112 140050941662400 spec.py:321] Evaluating on the training split.
I0307 18:18:41.357168 140050941662400 spec.py:333] Evaluating on the validation split.
I0307 18:19:08.823131 140050941662400 spec.py:349] Evaluating on the test split.
I0307 18:19:10.596738 140050941662400 submission_runner.py:469] Time since start: 54931.69s, 	Step: 96524, 	{'train/accuracy': 0.3335459232330322, 'train/loss': 3.3491058349609375, 'validation/accuracy': 0.30723997950553894, 'validation/loss': 3.561947822570801, 'validation/num_examples': 50000, 'test/accuracy': 0.22380000352859497, 'test/loss': 4.454788684844971, 'test/num_examples': 10000, 'score': 51090.460377693176, 'total_duration': 54931.69348812103, 'accumulated_submission_time': 51090.460377693176, 'accumulated_eval_time': 3820.6975333690643, 'accumulated_logging_time': 9.272489070892334}
I0307 18:19:10.619693 139894567450368 logging_writer.py:48] [96524] accumulated_eval_time=3820.7, accumulated_logging_time=9.27249, accumulated_submission_time=51090.5, global_step=96524, preemption_count=0, score=51090.5, test/accuracy=0.2238, test/loss=4.45479, test/num_examples=10000, total_duration=54931.7, train/accuracy=0.333546, train/loss=3.34911, validation/accuracy=0.30724, validation/loss=3.56195, validation/num_examples=50000
I0307 18:24:18.091884 139894559057664 logging_writer.py:48] [96600] global_step=96600, grad_norm=1.598954439163208, loss=2.121457576751709
I0307 18:26:08.297087 139894567450368 logging_writer.py:48] [96700] global_step=96700, grad_norm=1.554540753364563, loss=2.033222198486328
I0307 18:27:41.603942 140050941662400 spec.py:321] Evaluating on the training split.
I0307 18:27:51.982076 140050941662400 spec.py:333] Evaluating on the validation split.
I0307 18:28:12.780739 140050941662400 spec.py:349] Evaluating on the test split.
I0307 18:28:14.502771 140050941662400 submission_runner.py:469] Time since start: 55475.60s, 	Step: 96786, 	{'train/accuracy': 0.19234295189380646, 'train/loss': 4.600100517272949, 'validation/accuracy': 0.17433999478816986, 'validation/loss': 4.79287052154541, 'validation/num_examples': 50000, 'test/accuracy': 0.12790000438690186, 'test/loss': 5.336237907409668, 'test/num_examples': 10000, 'score': 51601.40718150139, 'total_duration': 55475.59950566292, 'accumulated_submission_time': 51601.40718150139, 'accumulated_eval_time': 3853.596316576004, 'accumulated_logging_time': 9.303135871887207}
I0307 18:28:14.561216 139894559057664 logging_writer.py:48] [96786] accumulated_eval_time=3853.6, accumulated_logging_time=9.30314, accumulated_submission_time=51601.4, global_step=96786, preemption_count=0, score=51601.4, test/accuracy=0.1279, test/loss=5.33624, test/num_examples=10000, total_duration=55475.6, train/accuracy=0.192343, train/loss=4.6001, validation/accuracy=0.17434, validation/loss=4.79287, validation/num_examples=50000
I0307 18:28:20.676202 139894567450368 logging_writer.py:48] [96800] global_step=96800, grad_norm=1.6171079874038696, loss=2.0616965293884277
I0307 18:30:10.889550 139894559057664 logging_writer.py:48] [96900] global_step=96900, grad_norm=1.6279652118682861, loss=2.099247694015503
I0307 18:36:46.520027 140050941662400 spec.py:321] Evaluating on the training split.
I0307 18:36:56.924302 140050941662400 spec.py:333] Evaluating on the validation split.
I0307 18:37:24.329241 140050941662400 spec.py:349] Evaluating on the test split.
I0307 18:37:26.101705 140050941662400 submission_runner.py:469] Time since start: 56027.20s, 	Step: 96999, 	{'train/accuracy': 0.4685506820678711, 'train/loss': 2.384838581085205, 'validation/accuracy': 0.43925997614860535, 'validation/loss': 2.610322952270508, 'validation/num_examples': 50000, 'test/accuracy': 0.3265000283718109, 'test/loss': 3.4863107204437256, 'test/num_examples': 10000, 'score': 52113.30556297302, 'total_duration': 56027.1984539032, 'accumulated_submission_time': 52113.30556297302, 'accumulated_eval_time': 3893.1779639720917, 'accumulated_logging_time': 9.396708965301514}
I0307 18:37:26.139695 139894567450368 logging_writer.py:48] [96999] accumulated_eval_time=3893.18, accumulated_logging_time=9.39671, accumulated_submission_time=52113.3, global_step=96999, preemption_count=0, score=52113.3, test/accuracy=0.3265, test/loss=3.48631, test/num_examples=10000, total_duration=56027.2, train/accuracy=0.468551, train/loss=2.38484, validation/accuracy=0.43926, validation/loss=2.61032, validation/num_examples=50000
I0307 18:37:26.880785 139894559057664 logging_writer.py:48] [97000] global_step=97000, grad_norm=1.6890223026275635, loss=1.9731820821762085
I0307 18:44:19.310365 139894567450368 logging_writer.py:48] [97100] global_step=97100, grad_norm=1.599434494972229, loss=2.216658592224121
I0307 18:45:56.868500 140050941662400 spec.py:321] Evaluating on the training split.
I0307 18:46:07.603306 140050941662400 spec.py:333] Evaluating on the validation split.
I0307 18:46:35.531987 140050941662400 spec.py:349] Evaluating on the test split.
I0307 18:46:37.257000 140050941662400 submission_runner.py:469] Time since start: 56578.35s, 	Step: 97124, 	{'train/accuracy': 0.37886637449264526, 'train/loss': 2.9933929443359375, 'validation/accuracy': 0.3570999801158905, 'validation/loss': 3.1460907459259033, 'validation/num_examples': 50000, 'test/accuracy': 0.2606000006198883, 'test/loss': 3.912824869155884, 'test/num_examples': 10000, 'score': 52624.01125764847, 'total_duration': 56578.353753089905, 'accumulated_submission_time': 52624.01125764847, 'accumulated_eval_time': 3933.5664355754852, 'accumulated_logging_time': 9.442660570144653}
I0307 18:46:37.278193 139894559057664 logging_writer.py:48] [97124] accumulated_eval_time=3933.57, accumulated_logging_time=9.44266, accumulated_submission_time=52624, global_step=97124, preemption_count=0, score=52624, test/accuracy=0.2606, test/loss=3.91282, test/num_examples=10000, total_duration=56578.4, train/accuracy=0.378866, train/loss=2.99339, validation/accuracy=0.3571, validation/loss=3.14609, validation/num_examples=50000
I0307 18:49:48.896127 139894567450368 logging_writer.py:48] [97200] global_step=97200, grad_norm=1.6996296644210815, loss=2.0698494911193848
I0307 18:51:28.867805 139894559057664 logging_writer.py:48] [97300] global_step=97300, grad_norm=1.5949753522872925, loss=1.9435369968414307
I0307 18:55:04.514494 139894567450368 logging_writer.py:48] [97400] global_step=97400, grad_norm=1.604335069656372, loss=2.0281336307525635
I0307 18:55:08.815833 140050941662400 spec.py:321] Evaluating on the training split.
I0307 18:55:19.422713 140050941662400 spec.py:333] Evaluating on the validation split.
I0307 18:55:42.449984 140050941662400 spec.py:349] Evaluating on the test split.
I0307 18:55:44.196690 140050941662400 submission_runner.py:469] Time since start: 57125.29s, 	Step: 97403, 	{'train/accuracy': 0.45796793699264526, 'train/loss': 2.4078145027160645, 'validation/accuracy': 0.4359000027179718, 'validation/loss': 2.5587518215179443, 'validation/num_examples': 50000, 'test/accuracy': 0.3305000066757202, 'test/loss': 3.3374814987182617, 'test/num_examples': 10000, 'score': 53135.47499227524, 'total_duration': 57125.29343342781, 'accumulated_submission_time': 53135.47499227524, 'accumulated_eval_time': 3968.947244644165, 'accumulated_logging_time': 9.50511646270752}
I0307 18:55:44.279075 139894559057664 logging_writer.py:48] [97403] accumulated_eval_time=3968.95, accumulated_logging_time=9.50512, accumulated_submission_time=53135.5, global_step=97403, preemption_count=0, score=53135.5, test/accuracy=0.3305, test/loss=3.33748, test/num_examples=10000, total_duration=57125.3, train/accuracy=0.457968, train/loss=2.40781, validation/accuracy=0.4359, validation/loss=2.55875, validation/num_examples=50000
I0307 18:59:34.445460 139894567450368 logging_writer.py:48] [97500] global_step=97500, grad_norm=1.6403170824050903, loss=2.0078065395355225
I0307 19:04:17.002620 140050941662400 spec.py:321] Evaluating on the training split.
I0307 19:04:27.533261 140050941662400 spec.py:333] Evaluating on the validation split.
2025-03-07 19:04:48.659503: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 19:04:49.679536 140050941662400 spec.py:349] Evaluating on the test split.
I0307 19:04:51.409496 140050941662400 submission_runner.py:469] Time since start: 57672.51s, 	Step: 97567, 	{'train/accuracy': 0.4033800959587097, 'train/loss': 2.771019697189331, 'validation/accuracy': 0.38189998269081116, 'validation/loss': 2.9103806018829346, 'validation/num_examples': 50000, 'test/accuracy': 0.2744000256061554, 'test/loss': 3.683316946029663, 'test/num_examples': 10000, 'score': 53648.170491695404, 'total_duration': 57672.50623750687, 'accumulated_submission_time': 53648.170491695404, 'accumulated_eval_time': 4003.354075908661, 'accumulated_logging_time': 9.596047639846802}
I0307 19:04:51.431163 139894559057664 logging_writer.py:48] [97567] accumulated_eval_time=4003.35, accumulated_logging_time=9.59605, accumulated_submission_time=53648.2, global_step=97567, preemption_count=0, score=53648.2, test/accuracy=0.2744, test/loss=3.68332, test/num_examples=10000, total_duration=57672.5, train/accuracy=0.40338, train/loss=2.77102, validation/accuracy=0.3819, validation/loss=2.91038, validation/num_examples=50000
I0307 19:06:56.892783 139894567450368 logging_writer.py:48] [97600] global_step=97600, grad_norm=1.6060363054275513, loss=1.9699510335922241
I0307 19:13:23.875261 140050941662400 spec.py:321] Evaluating on the training split.
I0307 19:13:34.637852 140050941662400 spec.py:333] Evaluating on the validation split.
I0307 19:13:59.284877 140050941662400 spec.py:349] Evaluating on the test split.
I0307 19:14:01.010468 140050941662400 submission_runner.py:469] Time since start: 58222.11s, 	Step: 97692, 	{'train/accuracy': 0.37587690353393555, 'train/loss': 3.1253855228424072, 'validation/accuracy': 0.3586599826812744, 'validation/loss': 3.2295265197753906, 'validation/num_examples': 50000, 'test/accuracy': 0.2818000018596649, 'test/loss': 3.9076220989227295, 'test/num_examples': 10000, 'score': 54160.59295082092, 'total_duration': 58222.10721516609, 'accumulated_submission_time': 54160.59295082092, 'accumulated_eval_time': 4040.4892613887787, 'accumulated_logging_time': 9.625763654708862}
I0307 19:14:01.031060 139894559057664 logging_writer.py:48] [97692] accumulated_eval_time=4040.49, accumulated_logging_time=9.62576, accumulated_submission_time=54160.6, global_step=97692, preemption_count=0, score=54160.6, test/accuracy=0.2818, test/loss=3.90762, test/num_examples=10000, total_duration=58222.1, train/accuracy=0.375877, train/loss=3.12539, validation/accuracy=0.35866, validation/loss=3.22953, validation/num_examples=50000
I0307 19:14:16.678121 139894567450368 logging_writer.py:48] [97700] global_step=97700, grad_norm=1.8755593299865723, loss=1.9555197954177856
I0307 19:21:25.056507 139894559057664 logging_writer.py:48] [97800] global_step=97800, grad_norm=1.6037737131118774, loss=2.0151567459106445
I0307 19:22:33.222089 140050941662400 spec.py:321] Evaluating on the training split.
I0307 19:22:43.932151 140050941662400 spec.py:333] Evaluating on the validation split.
I0307 19:23:08.778261 140050941662400 spec.py:349] Evaluating on the test split.
I0307 19:23:10.546673 140050941662400 submission_runner.py:469] Time since start: 58771.64s, 	Step: 97817, 	{'train/accuracy': 0.4442562162876129, 'train/loss': 2.507519483566284, 'validation/accuracy': 0.41481998562812805, 'validation/loss': 2.7050468921661377, 'validation/num_examples': 50000, 'test/accuracy': 0.3221000134944916, 'test/loss': 3.419222354888916, 'test/num_examples': 10000, 'score': 54672.76234912872, 'total_duration': 58771.643421173096, 'accumulated_submission_time': 54672.76234912872, 'accumulated_eval_time': 4077.8138122558594, 'accumulated_logging_time': 9.654458284378052}
I0307 19:23:10.568491 139894567450368 logging_writer.py:48] [97817] accumulated_eval_time=4077.81, accumulated_logging_time=9.65446, accumulated_submission_time=54672.8, global_step=97817, preemption_count=0, score=54672.8, test/accuracy=0.3221, test/loss=3.41922, test/num_examples=10000, total_duration=58771.6, train/accuracy=0.444256, train/loss=2.50752, validation/accuracy=0.41482, validation/loss=2.70505, validation/num_examples=50000
I0307 19:28:47.564535 139894559057664 logging_writer.py:48] [97900] global_step=97900, grad_norm=1.5540796518325806, loss=2.156604766845703
I0307 19:31:43.370608 140050941662400 spec.py:321] Evaluating on the training split.
I0307 19:31:54.019263 140050941662400 spec.py:333] Evaluating on the validation split.
I0307 19:32:23.539063 140050941662400 spec.py:349] Evaluating on the test split.
I0307 19:32:25.305673 140050941662400 submission_runner.py:469] Time since start: 59326.40s, 	Step: 97942, 	{'train/accuracy': 0.4513113796710968, 'train/loss': 2.5114939212799072, 'validation/accuracy': 0.3983599841594696, 'validation/loss': 2.913064956665039, 'validation/num_examples': 50000, 'test/accuracy': 0.31800001859664917, 'test/loss': 3.6161446571350098, 'test/num_examples': 10000, 'score': 55185.54320383072, 'total_duration': 59326.4024181366, 'accumulated_submission_time': 55185.54320383072, 'accumulated_eval_time': 4119.748840332031, 'accumulated_logging_time': 9.684316635131836}
I0307 19:32:25.328994 139894567450368 logging_writer.py:48] [97942] accumulated_eval_time=4119.75, accumulated_logging_time=9.68432, accumulated_submission_time=55185.5, global_step=97942, preemption_count=0, score=55185.5, test/accuracy=0.318, test/loss=3.61614, test/num_examples=10000, total_duration=59326.4, train/accuracy=0.451311, train/loss=2.51149, validation/accuracy=0.39836, validation/loss=2.91306, validation/num_examples=50000
I0307 19:36:15.450131 139894559057664 logging_writer.py:48] [98000] global_step=98000, grad_norm=1.6091970205307007, loss=2.0473504066467285
I0307 19:40:58.547212 140050941662400 spec.py:321] Evaluating on the training split.
I0307 19:41:08.573565 140050941662400 spec.py:333] Evaluating on the validation split.
I0307 19:41:35.191900 140050941662400 spec.py:349] Evaluating on the test split.
I0307 19:41:36.954160 140050941662400 submission_runner.py:469] Time since start: 59878.05s, 	Step: 98067, 	{'train/accuracy': 0.35096460580825806, 'train/loss': 3.17452335357666, 'validation/accuracy': 0.31613999605178833, 'validation/loss': 3.428107738494873, 'validation/num_examples': 50000, 'test/accuracy': 0.2322000116109848, 'test/loss': 4.226362705230713, 'test/num_examples': 10000, 'score': 55698.739948511124, 'total_duration': 59878.05089855194, 'accumulated_submission_time': 55698.739948511124, 'accumulated_eval_time': 4158.155746936798, 'accumulated_logging_time': 9.715595722198486}
I0307 19:41:36.976804 139894567450368 logging_writer.py:48] [98067] accumulated_eval_time=4158.16, accumulated_logging_time=9.7156, accumulated_submission_time=55698.7, global_step=98067, preemption_count=0, score=55698.7, test/accuracy=0.2322, test/loss=4.22636, test/num_examples=10000, total_duration=59878.1, train/accuracy=0.350965, train/loss=3.17452, validation/accuracy=0.31614, validation/loss=3.42811, validation/num_examples=50000
I0307 19:43:41.696645 139894559057664 logging_writer.py:48] [98100] global_step=98100, grad_norm=1.6712056398391724, loss=2.003007650375366
I0307 19:50:08.791929 140050941662400 spec.py:321] Evaluating on the training split.
I0307 19:50:19.512605 140050941662400 spec.py:333] Evaluating on the validation split.
I0307 19:50:44.445875 140050941662400 spec.py:349] Evaluating on the test split.
I0307 19:50:46.215597 140050941662400 submission_runner.py:469] Time since start: 60427.31s, 	Step: 98192, 	{'train/accuracy': 0.3402024805545807, 'train/loss': 3.2356255054473877, 'validation/accuracy': 0.306799978017807, 'validation/loss': 3.4616589546203613, 'validation/num_examples': 50000, 'test/accuracy': 0.21880000829696655, 'test/loss': 4.24287748336792, 'test/num_examples': 10000, 'score': 56210.51002764702, 'total_duration': 60427.31234240532, 'accumulated_submission_time': 56210.51002764702, 'accumulated_eval_time': 4195.579383850098, 'accumulated_logging_time': 9.769680500030518}
I0307 19:50:46.237357 139894567450368 logging_writer.py:48] [98192] accumulated_eval_time=4195.58, accumulated_logging_time=9.76968, accumulated_submission_time=56210.5, global_step=98192, preemption_count=0, score=56210.5, test/accuracy=0.2188, test/loss=4.24288, test/num_examples=10000, total_duration=60427.3, train/accuracy=0.340202, train/loss=3.23563, validation/accuracy=0.3068, validation/loss=3.46166, validation/num_examples=50000
I0307 19:51:02.006611 139894559057664 logging_writer.py:48] [98200] global_step=98200, grad_norm=1.7333078384399414, loss=2.0330216884613037
I0307 19:58:09.820683 139894567450368 logging_writer.py:48] [98300] global_step=98300, grad_norm=1.5468562841415405, loss=2.0288443565368652
I0307 19:59:18.157228 140050941662400 spec.py:321] Evaluating on the training split.
I0307 19:59:28.356246 140050941662400 spec.py:333] Evaluating on the validation split.
I0307 19:59:52.367933 140050941662400 spec.py:349] Evaluating on the test split.
I0307 19:59:54.100005 140050941662400 submission_runner.py:469] Time since start: 60975.20s, 	Step: 98317, 	{'train/accuracy': 0.3770328462123871, 'train/loss': 2.968074083328247, 'validation/accuracy': 0.3532399833202362, 'validation/loss': 3.1530468463897705, 'validation/num_examples': 50000, 'test/accuracy': 0.2581000030040741, 'test/loss': 3.973919630050659, 'test/num_examples': 10000, 'score': 56722.40761208534, 'total_duration': 60975.19676017761, 'accumulated_submission_time': 56722.40761208534, 'accumulated_eval_time': 4231.522141456604, 'accumulated_logging_time': 9.79978895187378}
I0307 19:59:54.121335 139894559057664 logging_writer.py:48] [98317] accumulated_eval_time=4231.52, accumulated_logging_time=9.79979, accumulated_submission_time=56722.4, global_step=98317, preemption_count=0, score=56722.4, test/accuracy=0.2581, test/loss=3.97392, test/num_examples=10000, total_duration=60975.2, train/accuracy=0.377033, train/loss=2.96807, validation/accuracy=0.35324, validation/loss=3.15305, validation/num_examples=50000
I0307 20:03:38.491715 139894567450368 logging_writer.py:48] [98400] global_step=98400, grad_norm=1.7674649953842163, loss=2.0414316654205322
I0307 20:06:43.064904 139894559057664 logging_writer.py:48] [98500] global_step=98500, grad_norm=1.695957899093628, loss=1.9190582036972046
I0307 20:08:24.489382 140050941662400 spec.py:321] Evaluating on the training split.
I0307 20:08:35.762249 140050941662400 spec.py:333] Evaluating on the validation split.
I0307 20:08:56.587994 140050941662400 spec.py:349] Evaluating on the test split.
I0307 20:08:58.294808 140050941662400 submission_runner.py:469] Time since start: 61519.39s, 	Step: 98593, 	{'train/accuracy': 0.4070272445678711, 'train/loss': 2.8445615768432617, 'validation/accuracy': 0.37605997920036316, 'validation/loss': 3.051832675933838, 'validation/num_examples': 50000, 'test/accuracy': 0.2827000021934509, 'test/loss': 3.8926615715026855, 'test/num_examples': 10000, 'score': 57232.735802173615, 'total_duration': 61519.39156007767, 'accumulated_submission_time': 57232.735802173615, 'accumulated_eval_time': 4265.327535390854, 'accumulated_logging_time': 9.82894253730774}
I0307 20:08:58.385923 139894567450368 logging_writer.py:48] [98593] accumulated_eval_time=4265.33, accumulated_logging_time=9.82894, accumulated_submission_time=57232.7, global_step=98593, preemption_count=0, score=57232.7, test/accuracy=0.2827, test/loss=3.89266, test/num_examples=10000, total_duration=61519.4, train/accuracy=0.407027, train/loss=2.84456, validation/accuracy=0.37606, validation/loss=3.05183, validation/num_examples=50000
I0307 20:09:01.511623 139894559057664 logging_writer.py:48] [98600] global_step=98600, grad_norm=1.7529937028884888, loss=1.957579493522644
I0307 20:11:14.556650 139894567450368 logging_writer.py:48] [98700] global_step=98700, grad_norm=1.756277084350586, loss=2.1084659099578857
I0307 20:14:50.702704 139894559057664 logging_writer.py:48] [98800] global_step=98800, grad_norm=1.5827761888504028, loss=2.0511422157287598
2025-03-07 20:15:48.392063: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 20:17:30.336680 140050941662400 spec.py:321] Evaluating on the training split.
I0307 20:17:40.606899 140050941662400 spec.py:333] Evaluating on the validation split.
I0307 20:18:00.262350 140050941662400 spec.py:349] Evaluating on the test split.
I0307 20:18:02.034076 140050941662400 submission_runner.py:469] Time since start: 62063.13s, 	Step: 98870, 	{'train/accuracy': 0.22417090833187103, 'train/loss': 4.18853759765625, 'validation/accuracy': 0.21130000054836273, 'validation/loss': 4.328762531280518, 'validation/num_examples': 50000, 'test/accuracy': 0.16260001063346863, 'test/loss': 4.904516696929932, 'test/num_examples': 10000, 'score': 57744.64808678627, 'total_duration': 62063.1308221817, 'accumulated_submission_time': 57744.64808678627, 'accumulated_eval_time': 4297.024901866913, 'accumulated_logging_time': 9.9279944896698}
I0307 20:18:02.110309 139894567450368 logging_writer.py:48] [98870] accumulated_eval_time=4297.02, accumulated_logging_time=9.92799, accumulated_submission_time=57744.6, global_step=98870, preemption_count=0, score=57744.6, test/accuracy=0.1626, test/loss=4.90452, test/num_examples=10000, total_duration=62063.1, train/accuracy=0.224171, train/loss=4.18854, validation/accuracy=0.2113, validation/loss=4.32876, validation/num_examples=50000
I0307 20:18:48.764196 139894559057664 logging_writer.py:48] [98900] global_step=98900, grad_norm=1.6435558795928955, loss=2.0660386085510254
I0307 20:22:23.907702 139894567450368 logging_writer.py:48] [99000] global_step=99000, grad_norm=1.8378099203109741, loss=1.9295389652252197
I0307 20:25:59.104605 139894559057664 logging_writer.py:48] [99100] global_step=99100, grad_norm=1.6461015939712524, loss=2.020387649536133
I0307 20:26:33.779547 140050941662400 spec.py:321] Evaluating on the training split.
I0307 20:26:44.851457 140050941662400 spec.py:333] Evaluating on the validation split.
I0307 20:27:07.001439 140050941662400 spec.py:349] Evaluating on the test split.
I0307 20:27:08.777988 140050941662400 submission_runner.py:469] Time since start: 62609.87s, 	Step: 99117, 	{'train/accuracy': 0.5038065910339355, 'train/loss': 2.155428409576416, 'validation/accuracy': 0.4748999774456024, 'validation/loss': 2.3396787643432617, 'validation/num_examples': 50000, 'test/accuracy': 0.3717000186443329, 'test/loss': 3.053255319595337, 'test/num_examples': 10000, 'score': 58256.280631542206, 'total_duration': 62609.874737262726, 'accumulated_submission_time': 58256.280631542206, 'accumulated_eval_time': 4332.023305177689, 'accumulated_logging_time': 10.012439966201782}
I0307 20:27:08.800772 139894567450368 logging_writer.py:48] [99117] accumulated_eval_time=4332.02, accumulated_logging_time=10.0124, accumulated_submission_time=58256.3, global_step=99117, preemption_count=0, score=58256.3, test/accuracy=0.3717, test/loss=3.05326, test/num_examples=10000, total_duration=62609.9, train/accuracy=0.503807, train/loss=2.15543, validation/accuracy=0.4749, validation/loss=2.33968, validation/num_examples=50000
I0307 20:29:49.113227 139894559057664 logging_writer.py:48] [99200] global_step=99200, grad_norm=1.7397894859313965, loss=1.9586520195007324
I0307 20:33:22.890854 139894567450368 logging_writer.py:48] [99300] global_step=99300, grad_norm=1.7013949155807495, loss=2.180449962615967
I0307 20:35:39.622383 140050941662400 spec.py:321] Evaluating on the training split.
I0307 20:35:50.095724 140050941662400 spec.py:333] Evaluating on the validation split.
I0307 20:36:10.739096 140050941662400 spec.py:349] Evaluating on the test split.
I0307 20:36:12.481427 140050941662400 submission_runner.py:469] Time since start: 63153.58s, 	Step: 99365, 	{'train/accuracy': 0.2962571680545807, 'train/loss': 3.5831289291381836, 'validation/accuracy': 0.27978000044822693, 'validation/loss': 3.736872673034668, 'validation/num_examples': 50000, 'test/accuracy': 0.21620000898838043, 'test/loss': 4.370543003082275, 'test/num_examples': 10000, 'score': 58767.06583619118, 'total_duration': 63153.5781788826, 'accumulated_submission_time': 58767.06583619118, 'accumulated_eval_time': 4364.882323741913, 'accumulated_logging_time': 10.043159008026123}
I0307 20:36:12.503953 139894559057664 logging_writer.py:48] [99365] accumulated_eval_time=4364.88, accumulated_logging_time=10.0432, accumulated_submission_time=58767.1, global_step=99365, preemption_count=0, score=58767.1, test/accuracy=0.2162, test/loss=4.37054, test/num_examples=10000, total_duration=63153.6, train/accuracy=0.296257, train/loss=3.58313, validation/accuracy=0.27978, validation/loss=3.73687, validation/num_examples=50000
I0307 20:37:08.500740 139894567450368 logging_writer.py:48] [99400] global_step=99400, grad_norm=1.638885736465454, loss=2.0283913612365723
I0307 20:40:40.895766 139894559057664 logging_writer.py:48] [99500] global_step=99500, grad_norm=1.518559455871582, loss=1.9630851745605469
I0307 20:44:13.017701 139894567450368 logging_writer.py:48] [99600] global_step=99600, grad_norm=1.8116267919540405, loss=1.9851053953170776
I0307 20:44:42.796896 140050941662400 spec.py:321] Evaluating on the training split.
I0307 20:44:53.619422 140050941662400 spec.py:333] Evaluating on the validation split.
I0307 20:45:15.118581 140050941662400 spec.py:349] Evaluating on the test split.
I0307 20:45:16.857206 140050941662400 submission_runner.py:469] Time since start: 63697.95s, 	Step: 99615, 	{'train/accuracy': 0.2230747789144516, 'train/loss': 4.642064094543457, 'validation/accuracy': 0.201679989695549, 'validation/loss': 4.863243103027344, 'validation/num_examples': 50000, 'test/accuracy': 0.1486000120639801, 'test/loss': 5.556511878967285, 'test/num_examples': 10000, 'score': 59277.32102560997, 'total_duration': 63697.95394778252, 'accumulated_submission_time': 59277.32102560997, 'accumulated_eval_time': 4398.942591667175, 'accumulated_logging_time': 10.074144124984741}
I0307 20:45:16.905240 139894559057664 logging_writer.py:48] [99615] accumulated_eval_time=4398.94, accumulated_logging_time=10.0741, accumulated_submission_time=59277.3, global_step=99615, preemption_count=0, score=59277.3, test/accuracy=0.1486, test/loss=5.55651, test/num_examples=10000, total_duration=63698, train/accuracy=0.223075, train/loss=4.64206, validation/accuracy=0.20168, validation/loss=4.86324, validation/num_examples=50000
I0307 20:48:01.996346 139894567450368 logging_writer.py:48] [99700] global_step=99700, grad_norm=1.7192203998565674, loss=2.08422589302063
I0307 20:51:35.367360 139894559057664 logging_writer.py:48] [99800] global_step=99800, grad_norm=1.6372452974319458, loss=2.011902332305908
I0307 20:53:48.210814 140050941662400 spec.py:321] Evaluating on the training split.
I0307 20:53:58.754218 140050941662400 spec.py:333] Evaluating on the validation split.
I0307 20:54:17.877082 140050941662400 spec.py:349] Evaluating on the test split.
I0307 20:54:19.654027 140050941662400 submission_runner.py:469] Time since start: 64240.75s, 	Step: 99863, 	{'train/accuracy': 0.41039541363716125, 'train/loss': 2.7273073196411133, 'validation/accuracy': 0.37588000297546387, 'validation/loss': 2.9621357917785645, 'validation/num_examples': 50000, 'test/accuracy': 0.29010000824928284, 'test/loss': 3.6996870040893555, 'test/num_examples': 10000, 'score': 59788.59088373184, 'total_duration': 64240.75077223778, 'accumulated_submission_time': 59788.59088373184, 'accumulated_eval_time': 4430.385771989822, 'accumulated_logging_time': 10.129989624023438}
I0307 20:54:19.687603 139894567450368 logging_writer.py:48] [99863] accumulated_eval_time=4430.39, accumulated_logging_time=10.13, accumulated_submission_time=59788.6, global_step=99863, preemption_count=0, score=59788.6, test/accuracy=0.2901, test/loss=3.69969, test/num_examples=10000, total_duration=64240.8, train/accuracy=0.410395, train/loss=2.72731, validation/accuracy=0.37588, validation/loss=2.96214, validation/num_examples=50000
I0307 20:55:22.749240 139894559057664 logging_writer.py:48] [99900] global_step=99900, grad_norm=1.8074874877929688, loss=2.0097007751464844
I0307 20:58:55.272269 139894567450368 logging_writer.py:48] [100000] global_step=100000, grad_norm=1.6624529361724854, loss=1.9904897212982178
2025-03-07 21:01:23.467020: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 21:01:39.610707 139894559057664 logging_writer.py:48] [100100] global_step=100100, grad_norm=1.6481131315231323, loss=2.0247669219970703
I0307 21:02:38.250197 139894567450368 logging_writer.py:48] [100200] global_step=100200, grad_norm=1.769195318222046, loss=2.089789867401123
I0307 21:02:49.949114 140050941662400 spec.py:321] Evaluating on the training split.
I0307 21:03:01.327762 140050941662400 spec.py:333] Evaluating on the validation split.
I0307 21:03:36.768461 140050941662400 spec.py:349] Evaluating on the test split.
I0307 21:03:38.482454 140050941662400 submission_runner.py:469] Time since start: 64799.58s, 	Step: 100221, 	{'train/accuracy': 0.32718828320503235, 'train/loss': 3.3538715839385986, 'validation/accuracy': 0.3027999997138977, 'validation/loss': 3.5211598873138428, 'validation/num_examples': 50000, 'test/accuracy': 0.22200001776218414, 'test/loss': 4.285613536834717, 'test/num_examples': 10000, 'score': 60298.80384635925, 'total_duration': 64799.57920575142, 'accumulated_submission_time': 60298.80384635925, 'accumulated_eval_time': 4478.919073104858, 'accumulated_logging_time': 10.171766519546509}
I0307 21:03:38.544282 139894559057664 logging_writer.py:48] [100221] accumulated_eval_time=4478.92, accumulated_logging_time=10.1718, accumulated_submission_time=60298.8, global_step=100221, preemption_count=0, score=60298.8, test/accuracy=0.222, test/loss=4.28561, test/num_examples=10000, total_duration=64799.6, train/accuracy=0.327188, train/loss=3.35387, validation/accuracy=0.3028, validation/loss=3.52116, validation/num_examples=50000
I0307 21:04:21.423916 139894567450368 logging_writer.py:48] [100300] global_step=100300, grad_norm=1.6947373151779175, loss=2.1599111557006836
I0307 21:05:19.469229 139894559057664 logging_writer.py:48] [100400] global_step=100400, grad_norm=1.8416486978530884, loss=1.9231510162353516
I0307 21:06:17.793470 139894567450368 logging_writer.py:48] [100500] global_step=100500, grad_norm=1.7158769369125366, loss=2.0218589305877686
I0307 21:07:16.225079 139894559057664 logging_writer.py:48] [100600] global_step=100600, grad_norm=1.6448369026184082, loss=1.937706470489502
I0307 21:08:14.470984 139894567450368 logging_writer.py:48] [100700] global_step=100700, grad_norm=1.6702733039855957, loss=1.883057713508606
I0307 21:09:12.853467 139894559057664 logging_writer.py:48] [100800] global_step=100800, grad_norm=1.675201177597046, loss=2.0897982120513916
I0307 21:10:11.287961 139894567450368 logging_writer.py:48] [100900] global_step=100900, grad_norm=1.5524550676345825, loss=1.9026811122894287
I0307 21:11:09.494368 139894559057664 logging_writer.py:48] [101000] global_step=101000, grad_norm=1.7453255653381348, loss=1.9134016036987305
I0307 21:12:07.616893 139894567450368 logging_writer.py:48] [101100] global_step=101100, grad_norm=1.800580620765686, loss=2.0326318740844727
I0307 21:12:08.734600 140050941662400 spec.py:321] Evaluating on the training split.
I0307 21:12:19.614019 140050941662400 spec.py:333] Evaluating on the validation split.
I0307 21:12:41.559181 140050941662400 spec.py:349] Evaluating on the test split.
I0307 21:12:43.273001 140050941662400 submission_runner.py:469] Time since start: 65344.37s, 	Step: 101103, 	{'train/accuracy': 0.2980707883834839, 'train/loss': 3.5459394454956055, 'validation/accuracy': 0.2823199927806854, 'validation/loss': 3.706989049911499, 'validation/num_examples': 50000, 'test/accuracy': 0.20900000631809235, 'test/loss': 4.410770893096924, 'test/num_examples': 10000, 'score': 60808.884316921234, 'total_duration': 65344.369733572006, 'accumulated_submission_time': 60808.884316921234, 'accumulated_eval_time': 4513.45741724968, 'accumulated_logging_time': 10.241794347763062}
I0307 21:12:43.340158 139894559057664 logging_writer.py:48] [101103] accumulated_eval_time=4513.46, accumulated_logging_time=10.2418, accumulated_submission_time=60808.9, global_step=101103, preemption_count=0, score=60808.9, test/accuracy=0.209, test/loss=4.41077, test/num_examples=10000, total_duration=65344.4, train/accuracy=0.298071, train/loss=3.54594, validation/accuracy=0.28232, validation/loss=3.70699, validation/num_examples=50000
I0307 21:13:35.140973 139894567450368 logging_writer.py:48] [101200] global_step=101200, grad_norm=1.8996338844299316, loss=1.9680930376052856
I0307 21:14:33.837766 139894559057664 logging_writer.py:48] [101300] global_step=101300, grad_norm=1.6869360208511353, loss=1.9830341339111328
2025-03-07 21:14:49.823845: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 21:15:34.046830 139894567450368 logging_writer.py:48] [101400] global_step=101400, grad_norm=1.8946905136108398, loss=2.206022024154663
I0307 21:16:32.494136 139894559057664 logging_writer.py:48] [101500] global_step=101500, grad_norm=1.6723620891571045, loss=1.9868860244750977
I0307 21:17:30.472777 139894567450368 logging_writer.py:48] [101600] global_step=101600, grad_norm=1.7117700576782227, loss=2.0822505950927734
I0307 21:18:28.715466 139894559057664 logging_writer.py:48] [101700] global_step=101700, grad_norm=1.683817982673645, loss=1.8802967071533203
I0307 21:19:26.970433 139894567450368 logging_writer.py:48] [101800] global_step=101800, grad_norm=1.6781059503555298, loss=1.9314626455307007
I0307 21:20:25.305525 139894559057664 logging_writer.py:48] [101900] global_step=101900, grad_norm=1.7123150825500488, loss=1.9854300022125244
I0307 21:21:13.468361 140050941662400 spec.py:321] Evaluating on the training split.
I0307 21:21:23.754383 140050941662400 spec.py:333] Evaluating on the validation split.
I0307 21:21:46.523774 140050941662400 spec.py:349] Evaluating on the test split.
I0307 21:21:48.307344 140050941662400 submission_runner.py:469] Time since start: 65889.40s, 	Step: 101984, 	{'train/accuracy': 0.30674025416374207, 'train/loss': 3.577120542526245, 'validation/accuracy': 0.2876800000667572, 'validation/loss': 3.7334482669830322, 'validation/num_examples': 50000, 'test/accuracy': 0.2175000160932541, 'test/loss': 4.442253112792969, 'test/num_examples': 10000, 'score': 61318.90201878548, 'total_duration': 65889.40408539772, 'accumulated_submission_time': 61318.90201878548, 'accumulated_eval_time': 4548.296358346939, 'accumulated_logging_time': 10.318055152893066}
I0307 21:21:48.359194 139894567450368 logging_writer.py:48] [101984] accumulated_eval_time=4548.3, accumulated_logging_time=10.3181, accumulated_submission_time=61318.9, global_step=101984, preemption_count=0, score=61318.9, test/accuracy=0.2175, test/loss=4.44225, test/num_examples=10000, total_duration=65889.4, train/accuracy=0.30674, train/loss=3.57712, validation/accuracy=0.28768, validation/loss=3.73345, validation/num_examples=50000
I0307 21:22:27.377280 139894559057664 logging_writer.py:48] [102000] global_step=102000, grad_norm=1.7026665210723877, loss=1.8163549900054932
I0307 21:29:32.429683 139894567450368 logging_writer.py:48] [102100] global_step=102100, grad_norm=1.72221839427948, loss=1.967873454093933
I0307 21:30:18.544048 140050941662400 spec.py:321] Evaluating on the training split.
I0307 21:30:29.058073 140050941662400 spec.py:333] Evaluating on the validation split.
I0307 21:30:55.713259 140050941662400 spec.py:349] Evaluating on the test split.
I0307 21:30:57.493674 140050941662400 submission_runner.py:469] Time since start: 66438.59s, 	Step: 102112, 	{'train/accuracy': 0.3849848508834839, 'train/loss': 2.9460418224334717, 'validation/accuracy': 0.3582199811935425, 'validation/loss': 3.1243159770965576, 'validation/num_examples': 50000, 'test/accuracy': 0.27060002088546753, 'test/loss': 3.927813768386841, 'test/num_examples': 10000, 'score': 61829.06133508682, 'total_duration': 66438.59041166306, 'accumulated_submission_time': 61829.06133508682, 'accumulated_eval_time': 4587.245951652527, 'accumulated_logging_time': 10.378966331481934}
I0307 21:30:57.520087 139894559057664 logging_writer.py:48] [102112] accumulated_eval_time=4587.25, accumulated_logging_time=10.379, accumulated_submission_time=61829.1, global_step=102112, preemption_count=0, score=61829.1, test/accuracy=0.2706, test/loss=3.92781, test/num_examples=10000, total_duration=66438.6, train/accuracy=0.384985, train/loss=2.94604, validation/accuracy=0.35822, validation/loss=3.12432, validation/num_examples=50000
I0307 21:36:06.915816 139894567450368 logging_writer.py:48] [102200] global_step=102200, grad_norm=1.65451979637146, loss=1.9767106771469116
I0307 21:38:35.876582 139894559057664 logging_writer.py:48] [102300] global_step=102300, grad_norm=1.868635654449463, loss=1.9395097494125366
I0307 21:39:27.832752 140050941662400 spec.py:321] Evaluating on the training split.
I0307 21:39:38.659428 140050941662400 spec.py:333] Evaluating on the validation split.
I0307 21:39:57.777850 140050941662400 spec.py:349] Evaluating on the test split.
I0307 21:39:59.527920 140050941662400 submission_runner.py:469] Time since start: 66980.62s, 	Step: 102337, 	{'train/accuracy': 0.3403419852256775, 'train/loss': 3.2988839149475098, 'validation/accuracy': 0.30395999550819397, 'validation/loss': 3.5761818885803223, 'validation/num_examples': 50000, 'test/accuracy': 0.22350001335144043, 'test/loss': 4.403056621551514, 'test/num_examples': 10000, 'score': 62339.34055161476, 'total_duration': 66980.62467432022, 'accumulated_submission_time': 62339.34055161476, 'accumulated_eval_time': 4618.941087007523, 'accumulated_logging_time': 10.413552045822144}
I0307 21:39:59.549641 139894567450368 logging_writer.py:48] [102337] accumulated_eval_time=4618.94, accumulated_logging_time=10.4136, accumulated_submission_time=62339.3, global_step=102337, preemption_count=0, score=62339.3, test/accuracy=0.2235, test/loss=4.40306, test/num_examples=10000, total_duration=66980.6, train/accuracy=0.340342, train/loss=3.29888, validation/accuracy=0.30396, validation/loss=3.57618, validation/num_examples=50000
I0307 21:41:18.329416 139894559057664 logging_writer.py:48] [102400] global_step=102400, grad_norm=1.6637418270111084, loss=1.9904178380966187
I0307 21:43:41.149437 139894567450368 logging_writer.py:48] [102500] global_step=102500, grad_norm=1.584404468536377, loss=1.973595142364502
I0307 21:46:07.676989 139894559057664 logging_writer.py:48] [102600] global_step=102600, grad_norm=1.7203975915908813, loss=1.9749908447265625
I0307 21:48:30.658449 140050941662400 spec.py:321] Evaluating on the training split.
I0307 21:48:41.248042 140050941662400 spec.py:333] Evaluating on the validation split.
I0307 21:49:02.508610 140050941662400 spec.py:349] Evaluating on the test split.
I0307 21:49:04.233874 140050941662400 submission_runner.py:469] Time since start: 67525.33s, 	Step: 102700, 	{'train/accuracy': 0.4902941584587097, 'train/loss': 2.2255101203918457, 'validation/accuracy': 0.45367997884750366, 'validation/loss': 2.4762954711914062, 'validation/num_examples': 50000, 'test/accuracy': 0.3379000127315521, 'test/loss': 3.356130599975586, 'test/num_examples': 10000, 'score': 62850.36156988144, 'total_duration': 67525.33063077927, 'accumulated_submission_time': 62850.36156988144, 'accumulated_eval_time': 4652.516487121582, 'accumulated_logging_time': 10.481064081192017}
I0307 21:49:04.273834 139894567450368 logging_writer.py:48] [102700] accumulated_eval_time=4652.52, accumulated_logging_time=10.4811, accumulated_submission_time=62850.4, global_step=102700, preemption_count=0, score=62850.4, test/accuracy=0.3379, test/loss=3.35613, test/num_examples=10000, total_duration=67525.3, train/accuracy=0.490294, train/loss=2.22551, validation/accuracy=0.45368, validation/loss=2.4763, validation/num_examples=50000
I0307 21:49:04.636183 139894559057664 logging_writer.py:48] [102700] global_step=102700, grad_norm=1.7732921838760376, loss=2.0451488494873047
I0307 21:51:17.884566 139894567450368 logging_writer.py:48] [102800] global_step=102800, grad_norm=1.743957757949829, loss=1.873510718345642
I0307 21:53:41.160374 139894559057664 logging_writer.py:48] [102900] global_step=102900, grad_norm=1.5974191427230835, loss=1.9138116836547852
I0307 21:56:04.226667 139894567450368 logging_writer.py:48] [103000] global_step=103000, grad_norm=1.7818868160247803, loss=1.9886529445648193
I0307 21:57:35.552489 140050941662400 spec.py:321] Evaluating on the training split.
I0307 21:57:46.139217 140050941662400 spec.py:333] Evaluating on the validation split.
I0307 21:58:06.959423 140050941662400 spec.py:349] Evaluating on the test split.
I0307 21:58:08.689943 140050941662400 submission_runner.py:469] Time since start: 68069.79s, 	Step: 103065, 	{'train/accuracy': 0.4428810477256775, 'train/loss': 2.5138564109802246, 'validation/accuracy': 0.4138000011444092, 'validation/loss': 2.736682653427124, 'validation/num_examples': 50000, 'test/accuracy': 0.3174000084400177, 'test/loss': 3.503340721130371, 'test/num_examples': 10000, 'score': 63361.57276439667, 'total_duration': 68069.78669524193, 'accumulated_submission_time': 63361.57276439667, 'accumulated_eval_time': 4685.653913021088, 'accumulated_logging_time': 10.547064065933228}
I0307 21:58:08.751244 139894559057664 logging_writer.py:48] [103065] accumulated_eval_time=4685.65, accumulated_logging_time=10.5471, accumulated_submission_time=63361.6, global_step=103065, preemption_count=0, score=63361.6, test/accuracy=0.3174, test/loss=3.50334, test/num_examples=10000, total_duration=68069.8, train/accuracy=0.442881, train/loss=2.51386, validation/accuracy=0.4138, validation/loss=2.73668, validation/num_examples=50000
I0307 21:58:46.368973 139894567450368 logging_writer.py:48] [103100] global_step=103100, grad_norm=1.6817530393600464, loss=1.9187712669372559
I0307 22:01:10.701211 139894559057664 logging_writer.py:48] [103200] global_step=103200, grad_norm=1.7051416635513306, loss=1.8721339702606201
I0307 22:03:33.801947 139894567450368 logging_writer.py:48] [103300] global_step=103300, grad_norm=1.6720515489578247, loss=2.0269875526428223
I0307 22:05:56.868405 139894559057664 logging_writer.py:48] [103400] global_step=103400, grad_norm=1.743980050086975, loss=1.9700736999511719
I0307 22:06:39.977248 140050941662400 spec.py:321] Evaluating on the training split.
I0307 22:06:50.303137 140050941662400 spec.py:333] Evaluating on the validation split.
I0307 22:07:11.651402 140050941662400 spec.py:349] Evaluating on the test split.
I0307 22:07:13.366532 140050941662400 submission_runner.py:469] Time since start: 68614.46s, 	Step: 103431, 	{'train/accuracy': 0.31449297070503235, 'train/loss': 3.3983209133148193, 'validation/accuracy': 0.2950199842453003, 'validation/loss': 3.5421199798583984, 'validation/num_examples': 50000, 'test/accuracy': 0.22750000655651093, 'test/loss': 4.212413787841797, 'test/num_examples': 10000, 'score': 63872.73085141182, 'total_duration': 68614.4632871151, 'accumulated_submission_time': 63872.73085141182, 'accumulated_eval_time': 4719.043176651001, 'accumulated_logging_time': 10.635022163391113}
I0307 22:07:13.413834 139894567450368 logging_writer.py:48] [103431] accumulated_eval_time=4719.04, accumulated_logging_time=10.635, accumulated_submission_time=63872.7, global_step=103431, preemption_count=0, score=63872.7, test/accuracy=0.2275, test/loss=4.21241, test/num_examples=10000, total_duration=68614.5, train/accuracy=0.314493, train/loss=3.39832, validation/accuracy=0.29502, validation/loss=3.54212, validation/num_examples=50000
I0307 22:08:50.723177 139894559057664 logging_writer.py:48] [103500] global_step=103500, grad_norm=1.6790721416473389, loss=2.001753807067871
I0307 22:11:33.902306 139894567450368 logging_writer.py:48] [103600] global_step=103600, grad_norm=1.665488362312317, loss=1.9753053188323975
I0307 22:14:06.805641 139894559057664 logging_writer.py:48] [103700] global_step=103700, grad_norm=1.6429495811462402, loss=1.9829368591308594
I0307 22:15:44.651367 140050941662400 spec.py:321] Evaluating on the training split.
I0307 22:15:55.182363 140050941662400 spec.py:333] Evaluating on the validation split.
I0307 22:16:14.871508 140050941662400 spec.py:349] Evaluating on the test split.
I0307 22:16:16.594494 140050941662400 submission_runner.py:469] Time since start: 69157.69s, 	Step: 103761, 	{'train/accuracy': 0.45344385504722595, 'train/loss': 2.4801504611968994, 'validation/accuracy': 0.4223399758338928, 'validation/loss': 2.708570957183838, 'validation/num_examples': 50000, 'test/accuracy': 0.3102000057697296, 'test/loss': 3.5214643478393555, 'test/num_examples': 10000, 'score': 64383.92204976082, 'total_duration': 69157.69124484062, 'accumulated_submission_time': 64383.92204976082, 'accumulated_eval_time': 4750.986273527145, 'accumulated_logging_time': 10.690326690673828}
I0307 22:16:16.660945 139894567450368 logging_writer.py:48] [103761] accumulated_eval_time=4750.99, accumulated_logging_time=10.6903, accumulated_submission_time=64383.9, global_step=103761, preemption_count=0, score=64383.9, test/accuracy=0.3102, test/loss=3.52146, test/num_examples=10000, total_duration=69157.7, train/accuracy=0.453444, train/loss=2.48015, validation/accuracy=0.42234, validation/loss=2.70857, validation/num_examples=50000
I0307 22:17:04.428228 139894559057664 logging_writer.py:48] [103800] global_step=103800, grad_norm=1.7132822275161743, loss=2.033003091812134
I0307 22:19:46.054541 139894567450368 logging_writer.py:48] [103900] global_step=103900, grad_norm=1.712829351425171, loss=1.970695972442627
I0307 22:22:09.636600 139894559057664 logging_writer.py:48] [104000] global_step=104000, grad_norm=1.7051684856414795, loss=1.898166537284851
I0307 22:24:33.920698 139894567450368 logging_writer.py:48] [104100] global_step=104100, grad_norm=1.6931707859039307, loss=2.1270554065704346
I0307 22:24:47.096523 140050941662400 spec.py:321] Evaluating on the training split.
I0307 22:24:57.715444 140050941662400 spec.py:333] Evaluating on the validation split.
I0307 22:25:18.700759 140050941662400 spec.py:349] Evaluating on the test split.
I0307 22:25:20.424575 140050941662400 submission_runner.py:469] Time since start: 69701.52s, 	Step: 104110, 	{'train/accuracy': 0.3767338991165161, 'train/loss': 3.0262625217437744, 'validation/accuracy': 0.3522599935531616, 'validation/loss': 3.256324291229248, 'validation/num_examples': 50000, 'test/accuracy': 0.2578999996185303, 'test/loss': 4.11581563949585, 'test/num_examples': 10000, 'score': 64894.31019735336, 'total_duration': 69701.52133011818, 'accumulated_submission_time': 64894.31019735336, 'accumulated_eval_time': 4784.314296245575, 'accumulated_logging_time': 10.7651047706604}
I0307 22:25:20.470869 139894559057664 logging_writer.py:48] [104110] accumulated_eval_time=4784.31, accumulated_logging_time=10.7651, accumulated_submission_time=64894.3, global_step=104110, preemption_count=0, score=64894.3, test/accuracy=0.2579, test/loss=4.11582, test/num_examples=10000, total_duration=69701.5, train/accuracy=0.376734, train/loss=3.02626, validation/accuracy=0.35226, validation/loss=3.25632, validation/num_examples=50000
I0307 22:27:21.169631 139894567450368 logging_writer.py:48] [104200] global_step=104200, grad_norm=1.6659131050109863, loss=1.8561363220214844
I0307 22:29:45.119673 139894559057664 logging_writer.py:48] [104300] global_step=104300, grad_norm=1.776837706565857, loss=1.9138290882110596
I0307 22:32:08.701535 139894567450368 logging_writer.py:48] [104400] global_step=104400, grad_norm=1.7635433673858643, loss=1.9871708154678345
I0307 22:33:50.601749 140050941662400 spec.py:321] Evaluating on the training split.
I0307 22:34:01.454493 140050941662400 spec.py:333] Evaluating on the validation split.
I0307 22:34:22.208909 140050941662400 spec.py:349] Evaluating on the test split.
I0307 22:34:23.958566 140050941662400 submission_runner.py:469] Time since start: 70245.06s, 	Step: 104472, 	{'train/accuracy': 0.4209582209587097, 'train/loss': 2.674959659576416, 'validation/accuracy': 0.3912400007247925, 'validation/loss': 2.8653271198272705, 'validation/num_examples': 50000, 'test/accuracy': 0.3004000186920166, 'test/loss': 3.550882577896118, 'test/num_examples': 10000, 'score': 65404.39068698883, 'total_duration': 70245.05532002449, 'accumulated_submission_time': 65404.39068698883, 'accumulated_eval_time': 4817.671086549759, 'accumulated_logging_time': 10.819352149963379}
I0307 22:34:24.024290 139894559057664 logging_writer.py:48] [104472] accumulated_eval_time=4817.67, accumulated_logging_time=10.8194, accumulated_submission_time=65404.4, global_step=104472, preemption_count=0, score=65404.4, test/accuracy=0.3004, test/loss=3.55088, test/num_examples=10000, total_duration=70245.1, train/accuracy=0.420958, train/loss=2.67496, validation/accuracy=0.39124, validation/loss=2.86533, validation/num_examples=50000
I0307 22:34:48.730319 139894567450368 logging_writer.py:48] [104500] global_step=104500, grad_norm=1.7158269882202148, loss=2.020781993865967
I0307 22:37:16.909979 139894559057664 logging_writer.py:48] [104600] global_step=104600, grad_norm=1.777860403060913, loss=1.958822250366211
I0307 22:39:34.956164 139894567450368 logging_writer.py:48] [104700] global_step=104700, grad_norm=1.7482050657272339, loss=2.0836925506591797
I0307 22:41:24.205218 139894559057664 logging_writer.py:48] [104800] global_step=104800, grad_norm=1.8189011812210083, loss=2.0069620609283447
I0307 22:42:54.500728 140050941662400 spec.py:321] Evaluating on the training split.
I0307 22:43:05.908763 140050941662400 spec.py:333] Evaluating on the validation split.
I0307 22:43:27.889528 140050941662400 spec.py:349] Evaluating on the test split.
I0307 22:43:29.644263 140050941662400 submission_runner.py:469] Time since start: 70790.74s, 	Step: 104883, 	{'train/accuracy': 0.33380499482154846, 'train/loss': 3.273742914199829, 'validation/accuracy': 0.3133399784564972, 'validation/loss': 3.427790880203247, 'validation/num_examples': 50000, 'test/accuracy': 0.2192000150680542, 'test/loss': 4.2581281661987305, 'test/num_examples': 10000, 'score': 65914.81075906754, 'total_duration': 70790.74099683762, 'accumulated_submission_time': 65914.81075906754, 'accumulated_eval_time': 4852.814576387405, 'accumulated_logging_time': 10.892746448516846}
I0307 22:43:29.681415 139894567450368 logging_writer.py:48] [104883] accumulated_eval_time=4852.81, accumulated_logging_time=10.8927, accumulated_submission_time=65914.8, global_step=104883, preemption_count=0, score=65914.8, test/accuracy=0.2192, test/loss=4.25813, test/num_examples=10000, total_duration=70790.7, train/accuracy=0.333805, train/loss=3.27374, validation/accuracy=0.31334, validation/loss=3.42779, validation/num_examples=50000
I0307 22:43:37.984845 139894559057664 logging_writer.py:48] [104900] global_step=104900, grad_norm=1.6556533575057983, loss=1.925655484199524
I0307 22:45:34.155694 139894567450368 logging_writer.py:48] [105000] global_step=105000, grad_norm=1.9488078355789185, loss=1.937301754951477
2025-03-07 22:47:02.039529: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 22:47:26.158820 139894559057664 logging_writer.py:48] [105100] global_step=105100, grad_norm=1.7648123502731323, loss=1.9454727172851562
I0307 22:49:12.969705 139894567450368 logging_writer.py:48] [105200] global_step=105200, grad_norm=1.642040729522705, loss=1.8294203281402588
I0307 22:51:00.012264 139894559057664 logging_writer.py:48] [105300] global_step=105300, grad_norm=1.737354040145874, loss=1.8813928365707397
I0307 22:51:59.783194 139894567450368 logging_writer.py:48] [105357] global_step=105357, preemption_count=0, score=66423.5
I0307 22:52:01.150497 140050941662400 submission_runner.py:646] Tuning trial 4/5
I0307 22:52:01.184751 140050941662400 submission_runner.py:647] Hyperparameters: Hyperparameters(dropout_rate=0.0, label_smoothing=0.0, learning_rate=0.004958460849689891, one_minus_beta1=0.13625575743, beta2=0.6291854735396584, weight_decay=0.1147386261512052, warmup_factor=0.02)
I0307 22:52:01.189988 140050941662400 submission_runner.py:648] Metrics: {'eval_results': [(1, {'train/accuracy': 0.0012755101779475808, 'train/loss': 6.914036273956299, 'validation/accuracy': 0.001339999958872795, 'validation/loss': 6.914280891418457, 'validation/num_examples': 50000, 'test/accuracy': 0.0013000001199543476, 'test/loss': 6.913465976715088, 'test/num_examples': 10000, 'score': 57.26776361465454, 'total_duration': 139.17078804969788, 'accumulated_submission_time': 57.26776361465454, 'accumulated_eval_time': 81.9028148651123, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1366, {'train/accuracy': 0.19393733143806458, 'train/loss': 4.09002161026001, 'validation/accuracy': 0.15984000265598297, 'validation/loss': 4.396219253540039, 'validation/num_examples': 50000, 'test/accuracy': 0.1251000016927719, 'test/loss': 4.906614303588867, 'test/num_examples': 10000, 'score': 567.101241350174, 'total_duration': 683.3743026256561, 'accumulated_submission_time': 567.101241350174, 'accumulated_eval_time': 116.03449773788452, 'accumulated_logging_time': 0.03821897506713867, 'global_step': 1366, 'preemption_count': 0}), (2726, {'train/accuracy': 0.3436702787876129, 'train/loss': 3.0623178482055664, 'validation/accuracy': 0.30357998609542847, 'validation/loss': 3.3523764610290527, 'validation/num_examples': 50000, 'test/accuracy': 0.22940000891685486, 'test/loss': 3.9628517627716064, 'test/num_examples': 10000, 'score': 1076.922331571579, 'total_duration': 1229.7017509937286, 'accumulated_submission_time': 1076.922331571579, 'accumulated_eval_time': 152.30359268188477, 'accumulated_logging_time': 0.07364583015441895, 'global_step': 2726, 'preemption_count': 0}), (4067, {'train/accuracy': 0.3639189898967743, 'train/loss': 2.980862617492676, 'validation/accuracy': 0.33131998777389526, 'validation/loss': 3.2336480617523193, 'validation/num_examples': 50000, 'test/accuracy': 0.2485000044107437, 'test/loss': 4.009598731994629, 'test/num_examples': 10000, 'score': 1587.0840775966644, 'total_duration': 1771.9354231357574, 'accumulated_submission_time': 1587.0840775966644, 'accumulated_eval_time': 184.16454124450684, 'accumulated_logging_time': 0.11448264122009277, 'global_step': 4067, 'preemption_count': 0}), (5415, {'train/accuracy': 0.35977357625961304, 'train/loss': 2.959120988845825, 'validation/accuracy': 0.33541998267173767, 'validation/loss': 3.1407690048217773, 'validation/num_examples': 50000, 'test/accuracy': 0.2524000108242035, 'test/loss': 3.831477165222168, 'test/num_examples': 10000, 'score': 2096.9187150001526, 'total_duration': 2316.8352077007294, 'accumulated_submission_time': 2096.9187150001526, 'accumulated_eval_time': 219.01759910583496, 'accumulated_logging_time': 0.1541295051574707, 'global_step': 5415, 'preemption_count': 0}), (6761, {'train/accuracy': 0.36045119166374207, 'train/loss': 3.0153629779815674, 'validation/accuracy': 0.33069998025894165, 'validation/loss': 3.2171270847320557, 'validation/num_examples': 50000, 'test/accuracy': 0.24490001797676086, 'test/loss': 3.9743778705596924, 'test/num_examples': 10000, 'score': 2606.9692907333374, 'total_duration': 2861.3694908618927, 'accumulated_submission_time': 2606.9692907333374, 'accumulated_eval_time': 253.3075098991394, 'accumulated_logging_time': 0.19097661972045898, 'global_step': 6761, 'preemption_count': 0}), (8111, {'train/accuracy': 0.2926100194454193, 'train/loss': 3.4803214073181152, 'validation/accuracy': 0.264739990234375, 'validation/loss': 3.670687437057495, 'validation/num_examples': 50000, 'test/accuracy': 0.20230001211166382, 'test/loss': 4.283381938934326, 'test/num_examples': 10000, 'score': 3116.845325946808, 'total_duration': 3409.1523463726044, 'accumulated_submission_time': 3116.845325946808, 'accumulated_eval_time': 291.01332998275757, 'accumulated_logging_time': 0.2355666160583496, 'global_step': 8111, 'preemption_count': 0}), (9458, {'train/accuracy': 0.4260004758834839, 'train/loss': 2.5971479415893555, 'validation/accuracy': 0.39563998579978943, 'validation/loss': 2.8102974891662598, 'validation/num_examples': 50000, 'test/accuracy': 0.30170002579689026, 'test/loss': 3.5090537071228027, 'test/num_examples': 10000, 'score': 3626.829142808914, 'total_duration': 3955.451154947281, 'accumulated_submission_time': 3626.829142808914, 'accumulated_eval_time': 327.12261295318604, 'accumulated_logging_time': 0.2720615863800049, 'global_step': 9458, 'preemption_count': 0}), (10807, {'train/accuracy': 0.29348692297935486, 'train/loss': 3.6059610843658447, 'validation/accuracy': 0.2662999927997589, 'validation/loss': 3.812204360961914, 'validation/num_examples': 50000, 'test/accuracy': 0.1859000027179718, 'test/loss': 4.66912841796875, 'test/num_examples': 10000, 'score': 4136.896089792252, 'total_duration': 4501.155874967575, 'accumulated_submission_time': 4136.896089792252, 'accumulated_eval_time': 362.54900217056274, 'accumulated_logging_time': 0.3199045658111572, 'global_step': 10807, 'preemption_count': 0}), (12139, {'train/accuracy': 0.27130499482154846, 'train/loss': 3.673388719558716, 'validation/accuracy': 0.2638799846172333, 'validation/loss': 3.7236242294311523, 'validation/num_examples': 50000, 'test/accuracy': 0.1868000030517578, 'test/loss': 4.45974063873291, 'test/num_examples': 10000, 'score': 4646.833619832993, 'total_duration': 5045.751278400421, 'accumulated_submission_time': 4646.833619832993, 'accumulated_eval_time': 396.9786994457245, 'accumulated_logging_time': 0.42337989807128906, 'global_step': 12139, 'preemption_count': 0}), (13474, {'train/accuracy': 0.3152901828289032, 'train/loss': 3.298755407333374, 'validation/accuracy': 0.2900799810886383, 'validation/loss': 3.493664503097534, 'validation/num_examples': 50000, 'test/accuracy': 0.2142000049352646, 'test/loss': 4.238362789154053, 'test/num_examples': 10000, 'score': 5156.796553850174, 'total_duration': 5593.981817483902, 'accumulated_submission_time': 5156.796553850174, 'accumulated_eval_time': 435.0723292827606, 'accumulated_logging_time': 0.46392059326171875, 'global_step': 13474, 'preemption_count': 0}), (14802, {'train/accuracy': 0.11832349747419357, 'train/loss': 5.744869232177734, 'validation/accuracy': 0.10957999527454376, 'validation/loss': 5.901554107666016, 'validation/num_examples': 50000, 'test/accuracy': 0.08180000633001328, 'test/loss': 6.412503242492676, 'test/num_examples': 10000, 'score': 5666.712646484375, 'total_duration': 6143.781819105148, 'accumulated_submission_time': 5666.712646484375, 'accumulated_eval_time': 474.75190806388855, 'accumulated_logging_time': 0.5230445861816406, 'global_step': 14802, 'preemption_count': 0}), (16134, {'train/accuracy': 0.3648357689380646, 'train/loss': 2.9762790203094482, 'validation/accuracy': 0.3432199954986572, 'validation/loss': 3.141003131866455, 'validation/num_examples': 50000, 'test/accuracy': 0.24480001628398895, 'test/loss': 3.940446376800537, 'test/num_examples': 10000, 'score': 6176.533273935318, 'total_duration': 6699.270140171051, 'accumulated_submission_time': 6176.533273935318, 'accumulated_eval_time': 520.2107214927673, 'accumulated_logging_time': 0.5842971801757812, 'global_step': 16134, 'preemption_count': 0}), (17467, {'train/accuracy': 0.25554049015045166, 'train/loss': 3.8944926261901855, 'validation/accuracy': 0.23291999101638794, 'validation/loss': 4.114696502685547, 'validation/num_examples': 50000, 'test/accuracy': 0.17550000548362732, 'test/loss': 4.802067279815674, 'test/num_examples': 10000, 'score': 6686.518086671829, 'total_duration': 7250.356608629227, 'accumulated_submission_time': 6686.518086671829, 'accumulated_eval_time': 561.0167219638824, 'accumulated_logging_time': 0.7289402484893799, 'global_step': 17467, 'preemption_count': 0}), (18800, {'train/accuracy': 0.3449457883834839, 'train/loss': 3.1866557598114014, 'validation/accuracy': 0.32194000482559204, 'validation/loss': 3.358506441116333, 'validation/num_examples': 50000, 'test/accuracy': 0.24070000648498535, 'test/loss': 4.1043314933776855, 'test/num_examples': 10000, 'score': 7196.599380970001, 'total_duration': 7803.715749025345, 'accumulated_submission_time': 7196.599380970001, 'accumulated_eval_time': 604.0265378952026, 'accumulated_logging_time': 0.8469357490539551, 'global_step': 18800, 'preemption_count': 0}), (20135, {'train/accuracy': 0.19192442297935486, 'train/loss': 4.728091716766357, 'validation/accuracy': 0.17817999422550201, 'validation/loss': 4.861440658569336, 'validation/num_examples': 50000, 'test/accuracy': 0.13180001080036163, 'test/loss': 5.658026695251465, 'test/num_examples': 10000, 'score': 7706.53710603714, 'total_duration': 8352.924048423767, 'accumulated_submission_time': 7706.53710603714, 'accumulated_eval_time': 643.0804052352905, 'accumulated_logging_time': 0.9112529754638672, 'global_step': 20135, 'preemption_count': 0}), (21464, {'train/accuracy': 0.32599249482154846, 'train/loss': 3.305140495300293, 'validation/accuracy': 0.30480000376701355, 'validation/loss': 3.441506862640381, 'validation/num_examples': 50000, 'test/accuracy': 0.21940000355243683, 'test/loss': 4.304385185241699, 'test/num_examples': 10000, 'score': 8216.33454298973, 'total_duration': 8901.447535037994, 'accumulated_submission_time': 8216.33454298973, 'accumulated_eval_time': 681.5993916988373, 'accumulated_logging_time': 0.9790341854095459, 'global_step': 21464, 'preemption_count': 0}), (22788, {'train/accuracy': 0.2582111060619354, 'train/loss': 4.174224853515625, 'validation/accuracy': 0.2453799992799759, 'validation/loss': 4.2462334632873535, 'validation/num_examples': 50000, 'test/accuracy': 0.18440000712871552, 'test/loss': 5.009335517883301, 'test/num_examples': 10000, 'score': 8726.244793891907, 'total_duration': 9454.54481267929, 'accumulated_submission_time': 8726.244793891907, 'accumulated_eval_time': 724.4643535614014, 'accumulated_logging_time': 1.149904489517212, 'global_step': 22788, 'preemption_count': 0}), (24113, {'train/accuracy': 0.1472417116165161, 'train/loss': 6.062775135040283, 'validation/accuracy': 0.13793998956680298, 'validation/loss': 6.144471645355225, 'validation/num_examples': 50000, 'test/accuracy': 0.10030000656843185, 'test/loss': 6.940281391143799, 'test/num_examples': 10000, 'score': 9236.071083068848, 'total_duration': 10006.373598098755, 'accumulated_submission_time': 9236.071083068848, 'accumulated_eval_time': 766.2714502811432, 'accumulated_logging_time': 1.204176425933838, 'global_step': 24113, 'preemption_count': 0}), (25446, {'train/accuracy': 0.15433673560619354, 'train/loss': 5.109786510467529, 'validation/accuracy': 0.14671999216079712, 'validation/loss': 5.193364143371582, 'validation/num_examples': 50000, 'test/accuracy': 0.10590000450611115, 'test/loss': 6.017796516418457, 'test/num_examples': 10000, 'score': 9746.119248628616, 'total_duration': 10556.273713827133, 'accumulated_submission_time': 9746.119248628616, 'accumulated_eval_time': 805.8432006835938, 'accumulated_logging_time': 1.3381574153900146, 'global_step': 25446, 'preemption_count': 0}), (26770, {'train/accuracy': 0.25420519709587097, 'train/loss': 3.8675661087036133, 'validation/accuracy': 0.23813998699188232, 'validation/loss': 3.992392063140869, 'validation/num_examples': 50000, 'test/accuracy': 0.178600013256073, 'test/loss': 4.676533222198486, 'test/num_examples': 10000, 'score': 10256.0816552639, 'total_duration': 11111.612708330154, 'accumulated_submission_time': 10256.0816552639, 'accumulated_eval_time': 850.9877662658691, 'accumulated_logging_time': 1.4228200912475586, 'global_step': 26770, 'preemption_count': 0}), (28097, {'train/accuracy': 0.10090481489896774, 'train/loss': 6.6337690353393555, 'validation/accuracy': 0.09415999799966812, 'validation/loss': 6.824917793273926, 'validation/num_examples': 50000, 'test/accuracy': 0.06680000573396683, 'test/loss': 7.5064167976379395, 'test/num_examples': 10000, 'score': 10766.232750177383, 'total_duration': 11662.901554107666, 'accumulated_submission_time': 10766.232750177383, 'accumulated_eval_time': 891.9176917076111, 'accumulated_logging_time': 1.4651060104370117, 'global_step': 28097, 'preemption_count': 0}), (29428, {'train/accuracy': 0.25207269191741943, 'train/loss': 3.8476932048797607, 'validation/accuracy': 0.23503999412059784, 'validation/loss': 3.985804319381714, 'validation/num_examples': 50000, 'test/accuracy': 0.1706000119447708, 'test/loss': 4.736830234527588, 'test/num_examples': 10000, 'score': 11276.032188892365, 'total_duration': 12212.34822511673, 'accumulated_submission_time': 11276.032188892365, 'accumulated_eval_time': 931.333226442337, 'accumulated_logging_time': 1.5485587120056152, 'global_step': 29428, 'preemption_count': 0}), (30753, {'train/accuracy': 0.25310903787612915, 'train/loss': 3.8119935989379883, 'validation/accuracy': 0.23891998827457428, 'validation/loss': 3.9384870529174805, 'validation/num_examples': 50000, 'test/accuracy': 0.17190000414848328, 'test/loss': 4.64061164855957, 'test/num_examples': 10000, 'score': 11785.914880037308, 'total_duration': 12761.69545340538, 'accumulated_submission_time': 11785.914880037308, 'accumulated_eval_time': 970.5589113235474, 'accumulated_logging_time': 1.6435976028442383, 'global_step': 30753, 'preemption_count': 0}), (32079, {'train/accuracy': 0.28916212916374207, 'train/loss': 3.596898317337036, 'validation/accuracy': 0.2730799913406372, 'validation/loss': 3.7396838665008545, 'validation/num_examples': 50000, 'test/accuracy': 0.20500001311302185, 'test/loss': 4.513715744018555, 'test/num_examples': 10000, 'score': 12295.721064329147, 'total_duration': 13311.868789434433, 'accumulated_submission_time': 12295.721064329147, 'accumulated_eval_time': 1010.7144453525543, 'accumulated_logging_time': 1.710144281387329, 'global_step': 32079, 'preemption_count': 0}), (33404, {'train/accuracy': 0.18865592777729034, 'train/loss': 5.211587905883789, 'validation/accuracy': 0.16941998898983002, 'validation/loss': 5.43745756149292, 'validation/num_examples': 50000, 'test/accuracy': 0.13600000739097595, 'test/loss': 5.970217227935791, 'test/num_examples': 10000, 'score': 12805.604042291641, 'total_duration': 13861.812965631485, 'accumulated_submission_time': 12805.604042291641, 'accumulated_eval_time': 1050.4797942638397, 'accumulated_logging_time': 1.8638660907745361, 'global_step': 33404, 'preemption_count': 0}), (34734, {'train/accuracy': 0.26773756742477417, 'train/loss': 3.8897037506103516, 'validation/accuracy': 0.24743999540805817, 'validation/loss': 4.053437232971191, 'validation/num_examples': 50000, 'test/accuracy': 0.18640001118183136, 'test/loss': 4.783019065856934, 'test/num_examples': 10000, 'score': 13315.685696840286, 'total_duration': 14415.778901576996, 'accumulated_submission_time': 13315.685696840286, 'accumulated_eval_time': 1094.1016657352448, 'accumulated_logging_time': 1.9842908382415771, 'global_step': 34734, 'preemption_count': 0}), (36063, {'train/accuracy': 0.15132732689380646, 'train/loss': 5.1248602867126465, 'validation/accuracy': 0.1421400010585785, 'validation/loss': 5.193818092346191, 'validation/num_examples': 50000, 'test/accuracy': 0.10250000655651093, 'test/loss': 5.777795791625977, 'test/num_examples': 10000, 'score': 13825.573055028915, 'total_duration': 14964.924278497696, 'accumulated_submission_time': 13825.573055028915, 'accumulated_eval_time': 1133.1549396514893, 'accumulated_logging_time': 2.0456316471099854, 'global_step': 36063, 'preemption_count': 0}), (37392, {'train/accuracy': 0.23624840378761292, 'train/loss': 4.087423324584961, 'validation/accuracy': 0.22645999491214752, 'validation/loss': 4.208317279815674, 'validation/num_examples': 50000, 'test/accuracy': 0.17590001225471497, 'test/loss': 4.745566368103027, 'test/num_examples': 10000, 'score': 14335.688372373581, 'total_duration': 15513.031492233276, 'accumulated_submission_time': 14335.688372373581, 'accumulated_eval_time': 1170.9094579219818, 'accumulated_logging_time': 2.1393346786499023, 'global_step': 37392, 'preemption_count': 0}), (38716, {'train/accuracy': 0.18307557702064514, 'train/loss': 4.588757038116455, 'validation/accuracy': 0.17389999330043793, 'validation/loss': 4.6564531326293945, 'validation/num_examples': 50000, 'test/accuracy': 0.1193000078201294, 'test/loss': 5.400364875793457, 'test/num_examples': 10000, 'score': 14845.524495840073, 'total_duration': 16060.937438488007, 'accumulated_submission_time': 14845.524495840073, 'accumulated_eval_time': 1208.7464561462402, 'accumulated_logging_time': 2.2261641025543213, 'global_step': 38716, 'preemption_count': 0}), (40042, {'train/accuracy': 0.21982620656490326, 'train/loss': 4.917595386505127, 'validation/accuracy': 0.20999999344348907, 'validation/loss': 5.031191825866699, 'validation/num_examples': 50000, 'test/accuracy': 0.1551000028848648, 'test/loss': 5.816312789916992, 'test/num_examples': 10000, 'score': 15355.480728149414, 'total_duration': 16616.16822552681, 'accumulated_submission_time': 15355.480728149414, 'accumulated_eval_time': 1253.7957904338837, 'accumulated_logging_time': 2.3082756996154785, 'global_step': 40042, 'preemption_count': 0}), (41370, {'train/accuracy': 0.1408442258834839, 'train/loss': 5.489231586456299, 'validation/accuracy': 0.12661999464035034, 'validation/loss': 5.6430206298828125, 'validation/num_examples': 50000, 'test/accuracy': 0.09790000319480896, 'test/loss': 6.098194599151611, 'test/num_examples': 10000, 'score': 15865.256662607193, 'total_duration': 17171.893587112427, 'accumulated_submission_time': 15865.256662607193, 'accumulated_eval_time': 1299.4996938705444, 'accumulated_logging_time': 2.4108331203460693, 'global_step': 41370, 'preemption_count': 0}), (42694, {'train/accuracy': 0.2802136540412903, 'train/loss': 3.61694073677063, 'validation/accuracy': 0.2674599885940552, 'validation/loss': 3.719081163406372, 'validation/num_examples': 50000, 'test/accuracy': 0.20180000364780426, 'test/loss': 4.413468360900879, 'test/num_examples': 10000, 'score': 16375.031787872314, 'total_duration': 17721.80126619339, 'accumulated_submission_time': 16375.031787872314, 'accumulated_eval_time': 1339.3514766693115, 'accumulated_logging_time': 2.5451548099517822, 'global_step': 42694, 'preemption_count': 0}), (44024, {'train/accuracy': 0.2174944132566452, 'train/loss': 4.659315586090088, 'validation/accuracy': 0.20833998918533325, 'validation/loss': 4.689654350280762, 'validation/num_examples': 50000, 'test/accuracy': 0.15220001339912415, 'test/loss': 5.475283622741699, 'test/num_examples': 10000, 'score': 16885.010546445847, 'total_duration': 18270.6658346653, 'accumulated_submission_time': 16885.010546445847, 'accumulated_eval_time': 1377.9644961357117, 'accumulated_logging_time': 2.672539472579956, 'global_step': 44024, 'preemption_count': 0}), (45345, {'train/accuracy': 0.3126992881298065, 'train/loss': 3.4420769214630127, 'validation/accuracy': 0.2979799807071686, 'validation/loss': 3.526496171951294, 'validation/num_examples': 50000, 'test/accuracy': 0.22420001029968262, 'test/loss': 4.273254871368408, 'test/num_examples': 10000, 'score': 17394.830267190933, 'total_duration': 18819.504824399948, 'accumulated_submission_time': 17394.830267190933, 'accumulated_eval_time': 1416.7216205596924, 'accumulated_logging_time': 2.7885525226593018, 'global_step': 45345, 'preemption_count': 0}), (46669, {'train/accuracy': 0.30709901452064514, 'train/loss': 3.4897403717041016, 'validation/accuracy': 0.2933200001716614, 'validation/loss': 3.5928421020507812, 'validation/num_examples': 50000, 'test/accuracy': 0.2217000126838684, 'test/loss': 4.303364276885986, 'test/num_examples': 10000, 'score': 17904.821108341217, 'total_duration': 19369.057031154633, 'accumulated_submission_time': 17904.821108341217, 'accumulated_eval_time': 1456.0454235076904, 'accumulated_logging_time': 2.88271427154541, 'global_step': 46669, 'preemption_count': 0}), (47992, {'train/accuracy': 0.21053889393806458, 'train/loss': 4.542476177215576, 'validation/accuracy': 0.20093999803066254, 'validation/loss': 4.677862167358398, 'validation/num_examples': 50000, 'test/accuracy': 0.15160000324249268, 'test/loss': 5.391515731811523, 'test/num_examples': 10000, 'score': 18414.586498975754, 'total_duration': 19922.02856040001, 'accumulated_submission_time': 18414.586498975754, 'accumulated_eval_time': 1498.9810802936554, 'accumulated_logging_time': 3.0106141567230225, 'global_step': 47992, 'preemption_count': 0}), (49314, {'train/accuracy': 0.28946107625961304, 'train/loss': 3.677434206008911, 'validation/accuracy': 0.27730000019073486, 'validation/loss': 3.8016085624694824, 'validation/num_examples': 50000, 'test/accuracy': 0.21630001068115234, 'test/loss': 4.441237926483154, 'test/num_examples': 10000, 'score': 18924.61717748642, 'total_duration': 20472.321487426758, 'accumulated_submission_time': 18924.61717748642, 'accumulated_eval_time': 1538.9628190994263, 'accumulated_logging_time': 3.146548271179199, 'global_step': 49314, 'preemption_count': 0}), (50636, {'train/accuracy': 0.37236925959587097, 'train/loss': 2.983518600463867, 'validation/accuracy': 0.35429999232292175, 'validation/loss': 3.113117218017578, 'validation/num_examples': 50000, 'test/accuracy': 0.25690001249313354, 'test/loss': 3.925462007522583, 'test/num_examples': 10000, 'score': 19434.668936252594, 'total_duration': 21020.26998066902, 'accumulated_submission_time': 19434.668936252594, 'accumulated_eval_time': 1576.5622019767761, 'accumulated_logging_time': 3.294395685195923, 'global_step': 50636, 'preemption_count': 0}), (51958, {'train/accuracy': 0.24587450921535492, 'train/loss': 3.937427282333374, 'validation/accuracy': 0.2337999939918518, 'validation/loss': 4.068167209625244, 'validation/num_examples': 50000, 'test/accuracy': 0.1697000116109848, 'test/loss': 4.802371025085449, 'test/num_examples': 10000, 'score': 19944.413109779358, 'total_duration': 21571.19043803215, 'accumulated_submission_time': 19944.413109779358, 'accumulated_eval_time': 1617.4762258529663, 'accumulated_logging_time': 3.415945291519165, 'global_step': 51958, 'preemption_count': 0}), (53275, {'train/accuracy': 0.2149832546710968, 'train/loss': 4.184123516082764, 'validation/accuracy': 0.19365999102592468, 'validation/loss': 4.402884006500244, 'validation/num_examples': 50000, 'test/accuracy': 0.16030000150203705, 'test/loss': 4.8527398109436035, 'test/num_examples': 10000, 'score': 20454.460059404373, 'total_duration': 22117.05579972267, 'accumulated_submission_time': 20454.460059404373, 'accumulated_eval_time': 1653.0433180332184, 'accumulated_logging_time': 3.5210657119750977, 'global_step': 53275, 'preemption_count': 0}), (54592, {'train/accuracy': 0.18975207209587097, 'train/loss': 5.093667030334473, 'validation/accuracy': 0.1624400019645691, 'validation/loss': 5.454000949859619, 'validation/num_examples': 50000, 'test/accuracy': 0.12160000205039978, 'test/loss': 6.084766387939453, 'test/num_examples': 10000, 'score': 20964.308794498444, 'total_duration': 22666.169044733047, 'accumulated_submission_time': 20964.308794498444, 'accumulated_eval_time': 1692.0468056201935, 'accumulated_logging_time': 3.6348958015441895, 'global_step': 54592, 'preemption_count': 0}), (55913, {'train/accuracy': 0.18837690353393555, 'train/loss': 4.547784328460693, 'validation/accuracy': 0.16555999219417572, 'validation/loss': 4.7818603515625, 'validation/num_examples': 50000, 'test/accuracy': 0.12230000644922256, 'test/loss': 5.451502323150635, 'test/num_examples': 10000, 'score': 21474.225752592087, 'total_duration': 23214.583297729492, 'accumulated_submission_time': 21474.225752592087, 'accumulated_eval_time': 1730.3221430778503, 'accumulated_logging_time': 3.7142038345336914, 'global_step': 55913, 'preemption_count': 0}), (57234, {'train/accuracy': 0.13877151906490326, 'train/loss': 5.732127666473389, 'validation/accuracy': 0.12139999866485596, 'validation/loss': 5.944208145141602, 'validation/num_examples': 50000, 'test/accuracy': 0.09410000592470169, 'test/loss': 6.54478645324707, 'test/num_examples': 10000, 'score': 21984.058734178543, 'total_duration': 23764.680055618286, 'accumulated_submission_time': 21984.058734178543, 'accumulated_eval_time': 1770.3098804950714, 'accumulated_logging_time': 3.842374801635742, 'global_step': 57234, 'preemption_count': 0}), (58554, {'train/accuracy': 0.18496890366077423, 'train/loss': 4.7120232582092285, 'validation/accuracy': 0.17031998932361603, 'validation/loss': 4.847750663757324, 'validation/num_examples': 50000, 'test/accuracy': 0.12400000542402267, 'test/loss': 5.619679927825928, 'test/num_examples': 10000, 'score': 22493.88467645645, 'total_duration': 24311.368948698044, 'accumulated_submission_time': 22493.88467645645, 'accumulated_eval_time': 1806.9495697021484, 'accumulated_logging_time': 3.9172229766845703, 'global_step': 58554, 'preemption_count': 0}), (59740, {'train/accuracy': 0.15517377853393555, 'train/loss': 5.4521613121032715, 'validation/accuracy': 0.1453000009059906, 'validation/loss': 5.564300537109375, 'validation/num_examples': 50000, 'test/accuracy': 0.1145000085234642, 'test/loss': 6.183597564697266, 'test/num_examples': 10000, 'score': 23003.635682582855, 'total_duration': 24859.180291175842, 'accumulated_submission_time': 23003.635682582855, 'accumulated_eval_time': 1844.7388427257538, 'accumulated_logging_time': 4.038300514221191, 'global_step': 59740, 'preemption_count': 0}), (61031, {'train/accuracy': 0.20531727373600006, 'train/loss': 4.719232082366943, 'validation/accuracy': 0.19779999554157257, 'validation/loss': 4.821146011352539, 'validation/num_examples': 50000, 'test/accuracy': 0.14430001378059387, 'test/loss': 5.658450126647949, 'test/num_examples': 10000, 'score': 23513.539635419846, 'total_duration': 25409.53394317627, 'accumulated_submission_time': 23513.539635419846, 'accumulated_eval_time': 1884.925163269043, 'accumulated_logging_time': 4.141346216201782, 'global_step': 61031, 'preemption_count': 0}), (62319, {'train/accuracy': 0.0460379458963871, 'train/loss': 7.943126678466797, 'validation/accuracy': 0.04471999779343605, 'validation/loss': 8.036075592041016, 'validation/num_examples': 50000, 'test/accuracy': 0.031700000166893005, 'test/loss': 8.588732719421387, 'test/num_examples': 10000, 'score': 24023.54101920128, 'total_duration': 25956.318823814392, 'accumulated_submission_time': 24023.54101920128, 'accumulated_eval_time': 1921.456042289734, 'accumulated_logging_time': 4.233725070953369, 'global_step': 62319, 'preemption_count': 0}), (63632, {'train/accuracy': 0.3078164756298065, 'train/loss': 3.433074951171875, 'validation/accuracy': 0.28933998942375183, 'validation/loss': 3.587611675262451, 'validation/num_examples': 50000, 'test/accuracy': 0.21560001373291016, 'test/loss': 4.357187271118164, 'test/num_examples': 10000, 'score': 24533.494884490967, 'total_duration': 26503.26997280121, 'accumulated_submission_time': 24533.494884490967, 'accumulated_eval_time': 1958.1373147964478, 'accumulated_logging_time': 4.3803229331970215, 'global_step': 63632, 'preemption_count': 0}), (64797, {'train/accuracy': 0.24125079810619354, 'train/loss': 4.432473659515381, 'validation/accuracy': 0.2279599905014038, 'validation/loss': 4.5382304191589355, 'validation/num_examples': 50000, 'test/accuracy': 0.16600000858306885, 'test/loss': 5.430295467376709, 'test/num_examples': 10000, 'score': 25043.484233379364, 'total_duration': 27052.299745321274, 'accumulated_submission_time': 25043.484233379364, 'accumulated_eval_time': 1996.9112660884857, 'accumulated_logging_time': 4.504631280899048, 'global_step': 64797, 'preemption_count': 0}), (65972, {'train/accuracy': 0.16557715833187103, 'train/loss': 4.766014099121094, 'validation/accuracy': 0.15997999906539917, 'validation/loss': 4.814699649810791, 'validation/num_examples': 50000, 'test/accuracy': 0.10930000245571136, 'test/loss': 5.580372333526611, 'test/num_examples': 10000, 'score': 25553.503398895264, 'total_duration': 27599.053845643997, 'accumulated_submission_time': 25553.503398895264, 'accumulated_eval_time': 2033.4228341579437, 'accumulated_logging_time': 4.587366104125977, 'global_step': 65972, 'preemption_count': 0}), (67214, {'train/accuracy': 0.15351960062980652, 'train/loss': 5.711238861083984, 'validation/accuracy': 0.14441999793052673, 'validation/loss': 5.792862415313721, 'validation/num_examples': 50000, 'test/accuracy': 0.10620000213384628, 'test/loss': 6.442117691040039, 'test/num_examples': 10000, 'score': 26063.22305393219, 'total_duration': 28143.036039590836, 'accumulated_submission_time': 26063.22305393219, 'accumulated_eval_time': 2067.3915452957153, 'accumulated_logging_time': 4.727046966552734, 'global_step': 67214, 'preemption_count': 0}), (68387, {'train/accuracy': 0.20952247083187103, 'train/loss': 4.308839797973633, 'validation/accuracy': 0.19845999777317047, 'validation/loss': 4.430704116821289, 'validation/num_examples': 50000, 'test/accuracy': 0.1503000110387802, 'test/loss': 5.009849548339844, 'test/num_examples': 10000, 'score': 26572.947857618332, 'total_duration': 28687.77893590927, 'accumulated_submission_time': 26572.947857618332, 'accumulated_eval_time': 2102.0642869472504, 'accumulated_logging_time': 4.9262425899505615, 'global_step': 68387, 'preemption_count': 0}), (69627, {'train/accuracy': 0.2120535671710968, 'train/loss': 4.696142196655273, 'validation/accuracy': 0.20768000185489655, 'validation/loss': 4.728293418884277, 'validation/num_examples': 50000, 'test/accuracy': 0.14900000393390656, 'test/loss': 5.663735866546631, 'test/num_examples': 10000, 'score': 27082.757184505463, 'total_duration': 29238.840626955032, 'accumulated_submission_time': 27082.757184505463, 'accumulated_eval_time': 2142.9305984973907, 'accumulated_logging_time': 5.160403728485107, 'global_step': 69627, 'preemption_count': 0}), (70908, {'train/accuracy': 0.35726243257522583, 'train/loss': 3.080620527267456, 'validation/accuracy': 0.33701997995376587, 'validation/loss': 3.2168705463409424, 'validation/num_examples': 50000, 'test/accuracy': 0.24370001256465912, 'test/loss': 4.009713649749756, 'test/num_examples': 10000, 'score': 27592.759934663773, 'total_duration': 29787.365347623825, 'accumulated_submission_time': 27592.759934663773, 'accumulated_eval_time': 2181.1803455352783, 'accumulated_logging_time': 5.276264905929565, 'global_step': 70908, 'preemption_count': 0}), (72179, {'train/accuracy': 0.21033960580825806, 'train/loss': 4.6485748291015625, 'validation/accuracy': 0.1946999877691269, 'validation/loss': 4.786005973815918, 'validation/num_examples': 50000, 'test/accuracy': 0.13990001380443573, 'test/loss': 5.396419048309326, 'test/num_examples': 10000, 'score': 28102.796191453934, 'total_duration': 30334.34835910797, 'accumulated_submission_time': 28102.796191453934, 'accumulated_eval_time': 2217.810533285141, 'accumulated_logging_time': 5.437184810638428, 'global_step': 72179, 'preemption_count': 0}), (73287, {'train/accuracy': 0.21671715378761292, 'train/loss': 4.643174171447754, 'validation/accuracy': 0.20819999277591705, 'validation/loss': 4.7156758308410645, 'validation/num_examples': 50000, 'test/accuracy': 0.14160001277923584, 'test/loss': 5.669127464294434, 'test/num_examples': 10000, 'score': 28612.852231264114, 'total_duration': 30882.437122821808, 'accumulated_submission_time': 28612.852231264114, 'accumulated_eval_time': 2255.5882289409637, 'accumulated_logging_time': 5.556652784347534, 'global_step': 73287, 'preemption_count': 0}), (74541, {'train/accuracy': 0.17902980744838715, 'train/loss': 5.017524242401123, 'validation/accuracy': 0.16808000206947327, 'validation/loss': 5.142340183258057, 'validation/num_examples': 50000, 'test/accuracy': 0.10930000245571136, 'test/loss': 6.125587463378906, 'test/num_examples': 10000, 'score': 29122.80700802803, 'total_duration': 31432.2675075531, 'accumulated_submission_time': 29122.80700802803, 'accumulated_eval_time': 2295.2150542736053, 'accumulated_logging_time': 5.657249927520752, 'global_step': 74541, 'preemption_count': 0}), (75519, {'train/accuracy': 0.2323620766401291, 'train/loss': 4.253023147583008, 'validation/accuracy': 0.20306000113487244, 'validation/loss': 4.5004963874816895, 'validation/num_examples': 50000, 'test/accuracy': 0.1535000056028366, 'test/loss': 5.129334449768066, 'test/num_examples': 10000, 'score': 29632.28448987007, 'total_duration': 31978.54022550583, 'accumulated_submission_time': 29632.28448987007, 'accumulated_eval_time': 2331.2378821372986, 'accumulated_logging_time': 6.314392805099487, 'global_step': 75519, 'preemption_count': 0}), (76595, {'train/accuracy': 0.20774872601032257, 'train/loss': 4.750784873962402, 'validation/accuracy': 0.1917800009250641, 'validation/loss': 4.861920356750488, 'validation/num_examples': 50000, 'test/accuracy': 0.14430001378059387, 'test/loss': 5.6282453536987305, 'test/num_examples': 10000, 'score': 30142.22321844101, 'total_duration': 32521.946982860565, 'accumulated_submission_time': 30142.22321844101, 'accumulated_eval_time': 2364.388926744461, 'accumulated_logging_time': 6.501705169677734, 'global_step': 76595, 'preemption_count': 0}), (77432, {'train/accuracy': 0.3491310477256775, 'train/loss': 3.3808252811431885, 'validation/accuracy': 0.33229997754096985, 'validation/loss': 3.5293385982513428, 'validation/num_examples': 50000, 'test/accuracy': 0.24970000982284546, 'test/loss': 4.375136375427246, 'test/num_examples': 10000, 'score': 30652.220622062683, 'total_duration': 33065.946811914444, 'accumulated_submission_time': 30652.220622062683, 'accumulated_eval_time': 2398.146493911743, 'accumulated_logging_time': 6.64655327796936, 'global_step': 77432, 'preemption_count': 0}), (78365, {'train/accuracy': 0.16234852373600006, 'train/loss': 4.925436496734619, 'validation/accuracy': 0.15129999816417694, 'validation/loss': 5.087410926818848, 'validation/num_examples': 50000, 'test/accuracy': 0.11520000547170639, 'test/loss': 5.64840841293335, 'test/num_examples': 10000, 'score': 31162.470264196396, 'total_duration': 33613.73833322525, 'accumulated_submission_time': 31162.470264196396, 'accumulated_eval_time': 2435.46963763237, 'accumulated_logging_time': 6.755829095840454, 'global_step': 78365, 'preemption_count': 0}), (79319, {'train/accuracy': 0.38926976919174194, 'train/loss': 2.869124174118042, 'validation/accuracy': 0.3698999881744385, 'validation/loss': 2.9974637031555176, 'validation/num_examples': 50000, 'test/accuracy': 0.28140002489089966, 'test/loss': 3.785993814468384, 'test/num_examples': 10000, 'score': 31672.649310350418, 'total_duration': 34161.132488012314, 'accumulated_submission_time': 31672.649310350418, 'accumulated_eval_time': 2472.4328339099884, 'accumulated_logging_time': 6.895248889923096, 'global_step': 79319, 'preemption_count': 0}), (80109, {'train/accuracy': 0.20804767310619354, 'train/loss': 4.664608478546143, 'validation/accuracy': 0.19643999636173248, 'validation/loss': 4.822405815124512, 'validation/num_examples': 50000, 'test/accuracy': 0.14690001308918, 'test/loss': 5.479805946350098, 'test/num_examples': 10000, 'score': 32182.58517575264, 'total_duration': 34711.21717429161, 'accumulated_submission_time': 32182.58517575264, 'accumulated_eval_time': 2512.4021470546722, 'accumulated_logging_time': 6.981983423233032, 'global_step': 80109, 'preemption_count': 0}), (81050, {'train/accuracy': 0.3246372640132904, 'train/loss': 3.411705493927002, 'validation/accuracy': 0.29892000555992126, 'validation/loss': 3.6232857704162598, 'validation/num_examples': 50000, 'test/accuracy': 0.22350001335144043, 'test/loss': 4.455986976623535, 'test/num_examples': 10000, 'score': 32692.882498264313, 'total_duration': 35256.58227753639, 'accumulated_submission_time': 32692.882498264313, 'accumulated_eval_time': 2547.272887945175, 'accumulated_logging_time': 7.0693488121032715, 'global_step': 81050, 'preemption_count': 0}), (81808, {'train/accuracy': 0.29530054330825806, 'train/loss': 3.5512404441833496, 'validation/accuracy': 0.28011998534202576, 'validation/loss': 3.6820178031921387, 'validation/num_examples': 50000, 'test/accuracy': 0.2005000114440918, 'test/loss': 4.468143939971924, 'test/num_examples': 10000, 'score': 33202.73929190636, 'total_duration': 35801.161251306534, 'accumulated_submission_time': 33202.73929190636, 'accumulated_eval_time': 2581.8176136016846, 'accumulated_logging_time': 7.157943487167358, 'global_step': 81808, 'preemption_count': 0}), (82690, {'train/accuracy': 0.2952008843421936, 'train/loss': 3.861325979232788, 'validation/accuracy': 0.2816599905490875, 'validation/loss': 3.9740242958068848, 'validation/num_examples': 50000, 'test/accuracy': 0.20420001447200775, 'test/loss': 4.859318256378174, 'test/num_examples': 10000, 'score': 33712.5399518013, 'total_duration': 36345.314138650894, 'accumulated_submission_time': 33712.5399518013, 'accumulated_eval_time': 2615.953969478607, 'accumulated_logging_time': 7.246772766113281, 'global_step': 82690, 'preemption_count': 0}), (83415, {'train/accuracy': 0.1780133843421936, 'train/loss': 4.704196453094482, 'validation/accuracy': 0.15817999839782715, 'validation/loss': 4.9603424072265625, 'validation/num_examples': 50000, 'test/accuracy': 0.12460000813007355, 'test/loss': 5.451750755310059, 'test/num_examples': 10000, 'score': 34222.599754571915, 'total_duration': 36889.79902672768, 'accumulated_submission_time': 34222.599754571915, 'accumulated_eval_time': 2650.1326994895935, 'accumulated_logging_time': 7.36224627494812, 'global_step': 83415, 'preemption_count': 0}), (84215, {'train/accuracy': 0.34066087007522583, 'train/loss': 3.2218375205993652, 'validation/accuracy': 0.32148000597953796, 'validation/loss': 3.3773317337036133, 'validation/num_examples': 50000, 'test/accuracy': 0.24160000681877136, 'test/loss': 4.1262640953063965, 'test/num_examples': 10000, 'score': 34732.472373485565, 'total_duration': 37444.11557793617, 'accumulated_submission_time': 34732.472373485565, 'accumulated_eval_time': 2694.390380382538, 'accumulated_logging_time': 7.43683123588562, 'global_step': 84215, 'preemption_count': 0}), (84807, {'train/accuracy': 0.34225526452064514, 'train/loss': 3.1803669929504395, 'validation/accuracy': 0.30991998314857483, 'validation/loss': 3.4767887592315674, 'validation/num_examples': 50000, 'test/accuracy': 0.2323000133037567, 'test/loss': 4.207357883453369, 'test/num_examples': 10000, 'score': 35242.308685302734, 'total_duration': 37990.713916778564, 'accumulated_submission_time': 35242.308685302734, 'accumulated_eval_time': 2730.9847898483276, 'accumulated_logging_time': 7.516220808029175, 'global_step': 84807, 'preemption_count': 0}), (85476, {'train/accuracy': 0.3448660671710968, 'train/loss': 3.225349187850952, 'validation/accuracy': 0.32486000657081604, 'validation/loss': 3.352023124694824, 'validation/num_examples': 50000, 'test/accuracy': 0.23770001530647278, 'test/loss': 4.146182060241699, 'test/num_examples': 10000, 'score': 35752.79736351967, 'total_duration': 38533.25898337364, 'accumulated_submission_time': 35752.79736351967, 'accumulated_eval_time': 2762.9073498249054, 'accumulated_logging_time': 7.573133230209351, 'global_step': 85476, 'preemption_count': 0}), (86199, {'train/accuracy': 0.3011399805545807, 'train/loss': 3.5227794647216797, 'validation/accuracy': 0.27539998292922974, 'validation/loss': 3.7204880714416504, 'validation/num_examples': 50000, 'test/accuracy': 0.1859000027179718, 'test/loss': 4.593204975128174, 'test/num_examples': 10000, 'score': 36263.49385714531, 'total_duration': 39076.5167093277, 'accumulated_submission_time': 36263.49385714531, 'accumulated_eval_time': 2795.307690382004, 'accumulated_logging_time': 7.65012788772583, 'global_step': 86199, 'preemption_count': 0}), (86823, {'train/accuracy': 0.2626155912876129, 'train/loss': 4.037708282470703, 'validation/accuracy': 0.24619999527931213, 'validation/loss': 4.217474937438965, 'validation/num_examples': 50000, 'test/accuracy': 0.18850000202655792, 'test/loss': 4.856085300445557, 'test/num_examples': 10000, 'score': 36773.94080400467, 'total_duration': 39619.359533786774, 'accumulated_submission_time': 36773.94080400467, 'accumulated_eval_time': 2827.4926176071167, 'accumulated_logging_time': 7.788488864898682, 'global_step': 86823, 'preemption_count': 0}), (87483, {'train/accuracy': 0.5042649507522583, 'train/loss': 2.1560163497924805, 'validation/accuracy': 0.4483399987220764, 'validation/loss': 2.5270583629608154, 'validation/num_examples': 50000, 'test/accuracy': 0.3337000012397766, 'test/loss': 3.355020523071289, 'test/num_examples': 10000, 'score': 37285.644619464874, 'total_duration': 40163.02745461464, 'accumulated_submission_time': 37285.644619464874, 'accumulated_eval_time': 2859.317789077759, 'accumulated_logging_time': 7.850671052932739, 'global_step': 87483, 'preemption_count': 0}), (88025, {'train/accuracy': 0.36758607625961304, 'train/loss': 3.0777347087860107, 'validation/accuracy': 0.3454799950122833, 'validation/loss': 3.21311616897583, 'validation/num_examples': 50000, 'test/accuracy': 0.2629000246524811, 'test/loss': 3.9372456073760986, 'test/num_examples': 10000, 'score': 37795.97331047058, 'total_duration': 40713.85692739487, 'accumulated_submission_time': 37795.97331047058, 'accumulated_eval_time': 2899.6937775611877, 'accumulated_logging_time': 7.913533687591553, 'global_step': 88025, 'preemption_count': 0}), (88637, {'train/accuracy': 0.27425462007522583, 'train/loss': 3.7851626873016357, 'validation/accuracy': 0.2615799903869629, 'validation/loss': 3.885563611984253, 'validation/num_examples': 50000, 'test/accuracy': 0.20090000331401825, 'test/loss': 4.547630310058594, 'test/num_examples': 10000, 'score': 38306.55653214455, 'total_duration': 41255.75601744652, 'accumulated_submission_time': 38306.55653214455, 'accumulated_eval_time': 2930.895970106125, 'accumulated_logging_time': 7.954211950302124, 'global_step': 88637, 'preemption_count': 0}), (89240, {'train/accuracy': 0.39190050959587097, 'train/loss': 2.837898015975952, 'validation/accuracy': 0.3554999828338623, 'validation/loss': 3.0690979957580566, 'validation/num_examples': 50000, 'test/accuracy': 0.27060002088546753, 'test/loss': 3.7492642402648926, 'test/num_examples': 10000, 'score': 38818.10015010834, 'total_duration': 41804.061200380325, 'accumulated_submission_time': 38818.10015010834, 'accumulated_eval_time': 2967.5205914974213, 'accumulated_logging_time': 8.019020318984985, 'global_step': 89240, 'preemption_count': 0}), (89556, {'train/accuracy': 0.26658162474632263, 'train/loss': 3.921617031097412, 'validation/accuracy': 0.25562000274658203, 'validation/loss': 3.994880437850952, 'validation/num_examples': 50000, 'test/accuracy': 0.1736000031232834, 'test/loss': 4.9081010818481445, 'test/num_examples': 10000, 'score': 39329.451273441315, 'total_duration': 42350.818180799484, 'accumulated_submission_time': 39329.451273441315, 'accumulated_eval_time': 3002.8466506004333, 'accumulated_logging_time': 8.061416625976562, 'global_step': 89556, 'preemption_count': 0}), (90125, {'train/accuracy': 0.43134167790412903, 'train/loss': 2.5786147117614746, 'validation/accuracy': 0.4067800045013428, 'validation/loss': 2.743133306503296, 'validation/num_examples': 50000, 'test/accuracy': 0.31070002913475037, 'test/loss': 3.488515853881836, 'test/num_examples': 10000, 'score': 39840.51077198982, 'total_duration': 42894.403200149536, 'accumulated_submission_time': 39840.51077198982, 'accumulated_eval_time': 3035.2546877861023, 'accumulated_logging_time': 8.114333868026733, 'global_step': 90125, 'preemption_count': 0}), (90688, {'train/accuracy': 0.3365154564380646, 'train/loss': 3.448878288269043, 'validation/accuracy': 0.3193399906158447, 'validation/loss': 3.5757927894592285, 'validation/num_examples': 50000, 'test/accuracy': 0.2378000169992447, 'test/loss': 4.406792640686035, 'test/num_examples': 10000, 'score': 40351.29848766327, 'total_duration': 43438.73979139328, 'accumulated_submission_time': 40351.29848766327, 'accumulated_eval_time': 3068.6840081214905, 'accumulated_logging_time': 8.168935775756836, 'global_step': 90688, 'preemption_count': 0}), (91095, {'train/accuracy': 0.4096181392669678, 'train/loss': 2.7284374237060547, 'validation/accuracy': 0.3902999758720398, 'validation/loss': 2.8480730056762695, 'validation/num_examples': 50000, 'test/accuracy': 0.2891000211238861, 'test/loss': 3.645298957824707, 'test/num_examples': 10000, 'score': 40861.563559532166, 'total_duration': 43987.76760482788, 'accumulated_submission_time': 40861.563559532166, 'accumulated_eval_time': 3107.3286712169647, 'accumulated_logging_time': 8.238538265228271, 'global_step': 91095, 'preemption_count': 0}), (91521, {'train/accuracy': 0.398457407951355, 'train/loss': 2.791975259780884, 'validation/accuracy': 0.37741997838020325, 'validation/loss': 2.8960323333740234, 'validation/num_examples': 50000, 'test/accuracy': 0.2897000014781952, 'test/loss': 3.5762834548950195, 'test/num_examples': 10000, 'score': 41372.10156774521, 'total_duration': 44531.883685827255, 'accumulated_submission_time': 41372.10156774521, 'accumulated_eval_time': 3140.7412152290344, 'accumulated_logging_time': 8.353724479675293, 'global_step': 91521, 'preemption_count': 0}), (91796, {'train/accuracy': 0.4407086968421936, 'train/loss': 2.493875503540039, 'validation/accuracy': 0.4074999988079071, 'validation/loss': 2.738919734954834, 'validation/num_examples': 50000, 'test/accuracy': 0.31300002336502075, 'test/loss': 3.5155811309814453, 'test/num_examples': 10000, 'score': 41885.878187179565, 'total_duration': 45083.95673274994, 'accumulated_submission_time': 41885.878187179565, 'accumulated_eval_time': 3178.9262914657593, 'accumulated_logging_time': 8.432494878768921, 'global_step': 91796, 'preemption_count': 0}), (92019, {'train/accuracy': 0.4006098508834839, 'train/loss': 2.8551719188690186, 'validation/accuracy': 0.3722800016403198, 'validation/loss': 3.0422046184539795, 'validation/num_examples': 50000, 'test/accuracy': 0.2639999985694885, 'test/loss': 3.9146673679351807, 'test/num_examples': 10000, 'score': 42397.89996480942, 'total_duration': 45630.667496204376, 'accumulated_submission_time': 42397.89996480942, 'accumulated_eval_time': 3213.561616420746, 'accumulated_logging_time': 8.459996938705444, 'global_step': 92019, 'preemption_count': 0}), (92313, {'train/accuracy': 0.3252550959587097, 'train/loss': 3.3468902111053467, 'validation/accuracy': 0.31182000041007996, 'validation/loss': 3.4744107723236084, 'validation/num_examples': 50000, 'test/accuracy': 0.2233000099658966, 'test/loss': 4.218242168426514, 'test/num_examples': 10000, 'score': 42907.967683553696, 'total_duration': 46172.38513112068, 'accumulated_submission_time': 42907.967683553696, 'accumulated_eval_time': 3245.147781610489, 'accumulated_logging_time': 8.490842342376709, 'global_step': 92313, 'preemption_count': 0}), (92599, {'train/accuracy': 0.3634207546710968, 'train/loss': 3.0162556171417236, 'validation/accuracy': 0.3368600010871887, 'validation/loss': 3.1696557998657227, 'validation/num_examples': 50000, 'test/accuracy': 0.25600001215934753, 'test/loss': 3.8993678092956543, 'test/num_examples': 10000, 'score': 43420.83335399628, 'total_duration': 46725.70512652397, 'accumulated_submission_time': 43420.83335399628, 'accumulated_eval_time': 3285.5164034366608, 'accumulated_logging_time': 8.543708086013794, 'global_step': 92599, 'preemption_count': 0}), (93099, {'train/accuracy': 0.3887316584587097, 'train/loss': 2.867079019546509, 'validation/accuracy': 0.3426799774169922, 'validation/loss': 3.1945977210998535, 'validation/num_examples': 50000, 'test/accuracy': 0.25440001487731934, 'test/loss': 3.9839487075805664, 'test/num_examples': 10000, 'score': 43931.63784909248, 'total_duration': 47273.70661854744, 'accumulated_submission_time': 43931.63784909248, 'accumulated_eval_time': 3322.626597881317, 'accumulated_logging_time': 8.57212781906128, 'global_step': 93099, 'preemption_count': 0}), (93322, {'train/accuracy': 0.3686622977256775, 'train/loss': 3.066848039627075, 'validation/accuracy': 0.3373199999332428, 'validation/loss': 3.329524278640747, 'validation/num_examples': 50000, 'test/accuracy': 0.25380000472068787, 'test/loss': 4.109381198883057, 'test/num_examples': 10000, 'score': 44443.43331313133, 'total_duration': 47837.46397590637, 'accumulated_submission_time': 44443.43331313133, 'accumulated_eval_time': 3374.477387905121, 'accumulated_logging_time': 8.656093835830688, 'global_step': 93322, 'preemption_count': 0}), (93699, {'train/accuracy': 0.4760044515132904, 'train/loss': 2.3018131256103516, 'validation/accuracy': 0.44551998376846313, 'validation/loss': 2.5106801986694336, 'validation/num_examples': 50000, 'test/accuracy': 0.3330000042915344, 'test/loss': 3.297938108444214, 'test/num_examples': 10000, 'score': 44953.46182656288, 'total_duration': 48380.19630932808, 'accumulated_submission_time': 44953.46182656288, 'accumulated_eval_time': 3407.1085555553436, 'accumulated_logging_time': 8.683910369873047, 'global_step': 93699, 'preemption_count': 0}), (93838, {'train/accuracy': 0.27736368775367737, 'train/loss': 3.8772177696228027, 'validation/accuracy': 0.2561199963092804, 'validation/loss': 4.074773788452148, 'validation/num_examples': 50000, 'test/accuracy': 0.19770000874996185, 'test/loss': 4.7148871421813965, 'test/num_examples': 10000, 'score': 45466.63313412666, 'total_duration': 48931.9882144928, 'accumulated_submission_time': 45466.63313412666, 'accumulated_eval_time': 3445.668481826782, 'accumulated_logging_time': 8.729026556015015, 'global_step': 93838, 'preemption_count': 0}), (94089, {'train/accuracy': 0.26658162474632263, 'train/loss': 3.9404215812683105, 'validation/accuracy': 0.25373998284339905, 'validation/loss': 4.04727029800415, 'validation/num_examples': 50000, 'test/accuracy': 0.18300001323223114, 'test/loss': 4.778444766998291, 'test/num_examples': 10000, 'score': 45977.70986509323, 'total_duration': 49474.12964987755, 'accumulated_submission_time': 45977.70986509323, 'accumulated_eval_time': 3476.6746854782104, 'accumulated_logging_time': 8.758967638015747, 'global_step': 94089, 'preemption_count': 0}), (94338, {'train/accuracy': 0.41011637449264526, 'train/loss': 2.7966175079345703, 'validation/accuracy': 0.3874000012874603, 'validation/loss': 2.931044816970825, 'validation/num_examples': 50000, 'test/accuracy': 0.28620001673698425, 'test/loss': 3.7599411010742188, 'test/num_examples': 10000, 'score': 46489.38882517815, 'total_duration': 50020.503683805466, 'accumulated_submission_time': 46489.38882517815, 'accumulated_eval_time': 3511.2404432296753, 'accumulated_logging_time': 8.859675168991089, 'global_step': 94338, 'preemption_count': 0}), (94586, {'train/accuracy': 0.4093989133834839, 'train/loss': 2.7300002574920654, 'validation/accuracy': 0.38165998458862305, 'validation/loss': 2.9146668910980225, 'validation/num_examples': 50000, 'test/accuracy': 0.27730000019073486, 'test/loss': 3.7323670387268066, 'test/num_examples': 10000, 'score': 46999.771547317505, 'total_duration': 50561.9245467186, 'accumulated_submission_time': 46999.771547317505, 'accumulated_eval_time': 3542.219552755356, 'accumulated_logging_time': 8.889421224594116, 'global_step': 94586, 'preemption_count': 0}), (94876, {'train/accuracy': 0.1908482164144516, 'train/loss': 4.752130031585693, 'validation/accuracy': 0.17771999537944794, 'validation/loss': 4.92516565322876, 'validation/num_examples': 50000, 'test/accuracy': 0.12200000882148743, 'test/loss': 5.721824645996094, 'test/num_examples': 10000, 'score': 47510.89162516594, 'total_duration': 51104.539194345474, 'accumulated_submission_time': 47510.89162516594, 'accumulated_eval_time': 3573.6516485214233, 'accumulated_logging_time': 8.919398307800293, 'global_step': 94876, 'preemption_count': 0}), (95119, {'train/accuracy': 0.4665975570678711, 'train/loss': 2.3335580825805664, 'validation/accuracy': 0.4346199929714203, 'validation/loss': 2.5399229526519775, 'validation/num_examples': 50000, 'test/accuracy': 0.3294000029563904, 'test/loss': 3.31132435798645, 'test/num_examples': 10000, 'score': 48020.814947366714, 'total_duration': 51651.61473989487, 'accumulated_submission_time': 48020.814947366714, 'accumulated_eval_time': 3610.7073690891266, 'accumulated_logging_time': 8.99016284942627, 'global_step': 95119, 'preemption_count': 0}), (95367, {'train/accuracy': 0.2788584232330322, 'train/loss': 3.994102954864502, 'validation/accuracy': 0.26231998205184937, 'validation/loss': 4.125890731811523, 'validation/num_examples': 50000, 'test/accuracy': 0.1980000138282776, 'test/loss': 4.870264053344727, 'test/num_examples': 10000, 'score': 48532.13700556755, 'total_duration': 52198.0583691597, 'accumulated_submission_time': 48532.13700556755, 'accumulated_eval_time': 3645.7334113121033, 'accumulated_logging_time': 9.056554079055786, 'global_step': 95367, 'preemption_count': 0}), (95616, {'train/accuracy': 0.44696667790412903, 'train/loss': 2.4894931316375732, 'validation/accuracy': 0.4175799787044525, 'validation/loss': 2.6646511554718018, 'validation/num_examples': 50000, 'test/accuracy': 0.30560001730918884, 'test/loss': 3.5345771312713623, 'test/num_examples': 10000, 'score': 49043.5319917202, 'total_duration': 52741.59497094154, 'accumulated_submission_time': 49043.5319917202, 'accumulated_eval_time': 3677.777877807617, 'accumulated_logging_time': 9.12613034248352, 'global_step': 95616, 'preemption_count': 0}), (95864, {'train/accuracy': 0.3290417790412903, 'train/loss': 3.4775454998016357, 'validation/accuracy': 0.3049599826335907, 'validation/loss': 3.60438871383667, 'validation/num_examples': 50000, 'test/accuracy': 0.23030000925064087, 'test/loss': 4.359577655792236, 'test/num_examples': 10000, 'score': 49553.63417387009, 'total_duration': 53282.78332543373, 'accumulated_submission_time': 49553.63417387009, 'accumulated_eval_time': 3708.807176589966, 'accumulated_logging_time': 9.154602527618408, 'global_step': 95864, 'preemption_count': 0}), (96150, {'train/accuracy': 0.2837611436843872, 'train/loss': 4.332614898681641, 'validation/accuracy': 0.2609599828720093, 'validation/loss': 4.480642318725586, 'validation/num_examples': 50000, 'test/accuracy': 0.17550000548362732, 'test/loss': 5.640755653381348, 'test/num_examples': 10000, 'score': 50064.72196316719, 'total_duration': 53831.70897316933, 'accumulated_submission_time': 50064.72196316719, 'accumulated_eval_time': 3746.5822303295135, 'accumulated_logging_time': 9.18456244468689, 'global_step': 96150, 'preemption_count': 0}), (96399, {'train/accuracy': 0.2892020046710968, 'train/loss': 3.81107497215271, 'validation/accuracy': 0.2626200020313263, 'validation/loss': 4.066309452056885, 'validation/num_examples': 50000, 'test/accuracy': 0.1972000151872635, 'test/loss': 4.829308032989502, 'test/num_examples': 10000, 'score': 50578.02612900734, 'total_duration': 54380.25386428833, 'accumulated_submission_time': 50578.02612900734, 'accumulated_eval_time': 3781.734937429428, 'accumulated_logging_time': 9.243118524551392, 'global_step': 96399, 'preemption_count': 0}), (96524, {'train/accuracy': 0.3335459232330322, 'train/loss': 3.3491058349609375, 'validation/accuracy': 0.30723997950553894, 'validation/loss': 3.561947822570801, 'validation/num_examples': 50000, 'test/accuracy': 0.22380000352859497, 'test/loss': 4.454788684844971, 'test/num_examples': 10000, 'score': 51090.460377693176, 'total_duration': 54931.69348812103, 'accumulated_submission_time': 51090.460377693176, 'accumulated_eval_time': 3820.6975333690643, 'accumulated_logging_time': 9.272489070892334, 'global_step': 96524, 'preemption_count': 0}), (96786, {'train/accuracy': 0.19234295189380646, 'train/loss': 4.600100517272949, 'validation/accuracy': 0.17433999478816986, 'validation/loss': 4.79287052154541, 'validation/num_examples': 50000, 'test/accuracy': 0.12790000438690186, 'test/loss': 5.336237907409668, 'test/num_examples': 10000, 'score': 51601.40718150139, 'total_duration': 55475.59950566292, 'accumulated_submission_time': 51601.40718150139, 'accumulated_eval_time': 3853.596316576004, 'accumulated_logging_time': 9.303135871887207, 'global_step': 96786, 'preemption_count': 0}), (96999, {'train/accuracy': 0.4685506820678711, 'train/loss': 2.384838581085205, 'validation/accuracy': 0.43925997614860535, 'validation/loss': 2.610322952270508, 'validation/num_examples': 50000, 'test/accuracy': 0.3265000283718109, 'test/loss': 3.4863107204437256, 'test/num_examples': 10000, 'score': 52113.30556297302, 'total_duration': 56027.1984539032, 'accumulated_submission_time': 52113.30556297302, 'accumulated_eval_time': 3893.1779639720917, 'accumulated_logging_time': 9.396708965301514, 'global_step': 96999, 'preemption_count': 0}), (97124, {'train/accuracy': 0.37886637449264526, 'train/loss': 2.9933929443359375, 'validation/accuracy': 0.3570999801158905, 'validation/loss': 3.1460907459259033, 'validation/num_examples': 50000, 'test/accuracy': 0.2606000006198883, 'test/loss': 3.912824869155884, 'test/num_examples': 10000, 'score': 52624.01125764847, 'total_duration': 56578.353753089905, 'accumulated_submission_time': 52624.01125764847, 'accumulated_eval_time': 3933.5664355754852, 'accumulated_logging_time': 9.442660570144653, 'global_step': 97124, 'preemption_count': 0}), (97403, {'train/accuracy': 0.45796793699264526, 'train/loss': 2.4078145027160645, 'validation/accuracy': 0.4359000027179718, 'validation/loss': 2.5587518215179443, 'validation/num_examples': 50000, 'test/accuracy': 0.3305000066757202, 'test/loss': 3.3374814987182617, 'test/num_examples': 10000, 'score': 53135.47499227524, 'total_duration': 57125.29343342781, 'accumulated_submission_time': 53135.47499227524, 'accumulated_eval_time': 3968.947244644165, 'accumulated_logging_time': 9.50511646270752, 'global_step': 97403, 'preemption_count': 0}), (97567, {'train/accuracy': 0.4033800959587097, 'train/loss': 2.771019697189331, 'validation/accuracy': 0.38189998269081116, 'validation/loss': 2.9103806018829346, 'validation/num_examples': 50000, 'test/accuracy': 0.2744000256061554, 'test/loss': 3.683316946029663, 'test/num_examples': 10000, 'score': 53648.170491695404, 'total_duration': 57672.50623750687, 'accumulated_submission_time': 53648.170491695404, 'accumulated_eval_time': 4003.354075908661, 'accumulated_logging_time': 9.596047639846802, 'global_step': 97567, 'preemption_count': 0}), (97692, {'train/accuracy': 0.37587690353393555, 'train/loss': 3.1253855228424072, 'validation/accuracy': 0.3586599826812744, 'validation/loss': 3.2295265197753906, 'validation/num_examples': 50000, 'test/accuracy': 0.2818000018596649, 'test/loss': 3.9076220989227295, 'test/num_examples': 10000, 'score': 54160.59295082092, 'total_duration': 58222.10721516609, 'accumulated_submission_time': 54160.59295082092, 'accumulated_eval_time': 4040.4892613887787, 'accumulated_logging_time': 9.625763654708862, 'global_step': 97692, 'preemption_count': 0}), (97817, {'train/accuracy': 0.4442562162876129, 'train/loss': 2.507519483566284, 'validation/accuracy': 0.41481998562812805, 'validation/loss': 2.7050468921661377, 'validation/num_examples': 50000, 'test/accuracy': 0.3221000134944916, 'test/loss': 3.419222354888916, 'test/num_examples': 10000, 'score': 54672.76234912872, 'total_duration': 58771.643421173096, 'accumulated_submission_time': 54672.76234912872, 'accumulated_eval_time': 4077.8138122558594, 'accumulated_logging_time': 9.654458284378052, 'global_step': 97817, 'preemption_count': 0}), (97942, {'train/accuracy': 0.4513113796710968, 'train/loss': 2.5114939212799072, 'validation/accuracy': 0.3983599841594696, 'validation/loss': 2.913064956665039, 'validation/num_examples': 50000, 'test/accuracy': 0.31800001859664917, 'test/loss': 3.6161446571350098, 'test/num_examples': 10000, 'score': 55185.54320383072, 'total_duration': 59326.4024181366, 'accumulated_submission_time': 55185.54320383072, 'accumulated_eval_time': 4119.748840332031, 'accumulated_logging_time': 9.684316635131836, 'global_step': 97942, 'preemption_count': 0}), (98067, {'train/accuracy': 0.35096460580825806, 'train/loss': 3.17452335357666, 'validation/accuracy': 0.31613999605178833, 'validation/loss': 3.428107738494873, 'validation/num_examples': 50000, 'test/accuracy': 0.2322000116109848, 'test/loss': 4.226362705230713, 'test/num_examples': 10000, 'score': 55698.739948511124, 'total_duration': 59878.05089855194, 'accumulated_submission_time': 55698.739948511124, 'accumulated_eval_time': 4158.155746936798, 'accumulated_logging_time': 9.715595722198486, 'global_step': 98067, 'preemption_count': 0}), (98192, {'train/accuracy': 0.3402024805545807, 'train/loss': 3.2356255054473877, 'validation/accuracy': 0.306799978017807, 'validation/loss': 3.4616589546203613, 'validation/num_examples': 50000, 'test/accuracy': 0.21880000829696655, 'test/loss': 4.24287748336792, 'test/num_examples': 10000, 'score': 56210.51002764702, 'total_duration': 60427.31234240532, 'accumulated_submission_time': 56210.51002764702, 'accumulated_eval_time': 4195.579383850098, 'accumulated_logging_time': 9.769680500030518, 'global_step': 98192, 'preemption_count': 0}), (98317, {'train/accuracy': 0.3770328462123871, 'train/loss': 2.968074083328247, 'validation/accuracy': 0.3532399833202362, 'validation/loss': 3.1530468463897705, 'validation/num_examples': 50000, 'test/accuracy': 0.2581000030040741, 'test/loss': 3.973919630050659, 'test/num_examples': 10000, 'score': 56722.40761208534, 'total_duration': 60975.19676017761, 'accumulated_submission_time': 56722.40761208534, 'accumulated_eval_time': 4231.522141456604, 'accumulated_logging_time': 9.79978895187378, 'global_step': 98317, 'preemption_count': 0}), (98593, {'train/accuracy': 0.4070272445678711, 'train/loss': 2.8445615768432617, 'validation/accuracy': 0.37605997920036316, 'validation/loss': 3.051832675933838, 'validation/num_examples': 50000, 'test/accuracy': 0.2827000021934509, 'test/loss': 3.8926615715026855, 'test/num_examples': 10000, 'score': 57232.735802173615, 'total_duration': 61519.39156007767, 'accumulated_submission_time': 57232.735802173615, 'accumulated_eval_time': 4265.327535390854, 'accumulated_logging_time': 9.82894253730774, 'global_step': 98593, 'preemption_count': 0}), (98870, {'train/accuracy': 0.22417090833187103, 'train/loss': 4.18853759765625, 'validation/accuracy': 0.21130000054836273, 'validation/loss': 4.328762531280518, 'validation/num_examples': 50000, 'test/accuracy': 0.16260001063346863, 'test/loss': 4.904516696929932, 'test/num_examples': 10000, 'score': 57744.64808678627, 'total_duration': 62063.1308221817, 'accumulated_submission_time': 57744.64808678627, 'accumulated_eval_time': 4297.024901866913, 'accumulated_logging_time': 9.9279944896698, 'global_step': 98870, 'preemption_count': 0}), (99117, {'train/accuracy': 0.5038065910339355, 'train/loss': 2.155428409576416, 'validation/accuracy': 0.4748999774456024, 'validation/loss': 2.3396787643432617, 'validation/num_examples': 50000, 'test/accuracy': 0.3717000186443329, 'test/loss': 3.053255319595337, 'test/num_examples': 10000, 'score': 58256.280631542206, 'total_duration': 62609.874737262726, 'accumulated_submission_time': 58256.280631542206, 'accumulated_eval_time': 4332.023305177689, 'accumulated_logging_time': 10.012439966201782, 'global_step': 99117, 'preemption_count': 0}), (99365, {'train/accuracy': 0.2962571680545807, 'train/loss': 3.5831289291381836, 'validation/accuracy': 0.27978000044822693, 'validation/loss': 3.736872673034668, 'validation/num_examples': 50000, 'test/accuracy': 0.21620000898838043, 'test/loss': 4.370543003082275, 'test/num_examples': 10000, 'score': 58767.06583619118, 'total_duration': 63153.5781788826, 'accumulated_submission_time': 58767.06583619118, 'accumulated_eval_time': 4364.882323741913, 'accumulated_logging_time': 10.043159008026123, 'global_step': 99365, 'preemption_count': 0}), (99615, {'train/accuracy': 0.2230747789144516, 'train/loss': 4.642064094543457, 'validation/accuracy': 0.201679989695549, 'validation/loss': 4.863243103027344, 'validation/num_examples': 50000, 'test/accuracy': 0.1486000120639801, 'test/loss': 5.556511878967285, 'test/num_examples': 10000, 'score': 59277.32102560997, 'total_duration': 63697.95394778252, 'accumulated_submission_time': 59277.32102560997, 'accumulated_eval_time': 4398.942591667175, 'accumulated_logging_time': 10.074144124984741, 'global_step': 99615, 'preemption_count': 0}), (99863, {'train/accuracy': 0.41039541363716125, 'train/loss': 2.7273073196411133, 'validation/accuracy': 0.37588000297546387, 'validation/loss': 2.9621357917785645, 'validation/num_examples': 50000, 'test/accuracy': 0.29010000824928284, 'test/loss': 3.6996870040893555, 'test/num_examples': 10000, 'score': 59788.59088373184, 'total_duration': 64240.75077223778, 'accumulated_submission_time': 59788.59088373184, 'accumulated_eval_time': 4430.385771989822, 'accumulated_logging_time': 10.129989624023438, 'global_step': 99863, 'preemption_count': 0}), (100221, {'train/accuracy': 0.32718828320503235, 'train/loss': 3.3538715839385986, 'validation/accuracy': 0.3027999997138977, 'validation/loss': 3.5211598873138428, 'validation/num_examples': 50000, 'test/accuracy': 0.22200001776218414, 'test/loss': 4.285613536834717, 'test/num_examples': 10000, 'score': 60298.80384635925, 'total_duration': 64799.57920575142, 'accumulated_submission_time': 60298.80384635925, 'accumulated_eval_time': 4478.919073104858, 'accumulated_logging_time': 10.171766519546509, 'global_step': 100221, 'preemption_count': 0}), (101103, {'train/accuracy': 0.2980707883834839, 'train/loss': 3.5459394454956055, 'validation/accuracy': 0.2823199927806854, 'validation/loss': 3.706989049911499, 'validation/num_examples': 50000, 'test/accuracy': 0.20900000631809235, 'test/loss': 4.410770893096924, 'test/num_examples': 10000, 'score': 60808.884316921234, 'total_duration': 65344.369733572006, 'accumulated_submission_time': 60808.884316921234, 'accumulated_eval_time': 4513.45741724968, 'accumulated_logging_time': 10.241794347763062, 'global_step': 101103, 'preemption_count': 0}), (101984, {'train/accuracy': 0.30674025416374207, 'train/loss': 3.577120542526245, 'validation/accuracy': 0.2876800000667572, 'validation/loss': 3.7334482669830322, 'validation/num_examples': 50000, 'test/accuracy': 0.2175000160932541, 'test/loss': 4.442253112792969, 'test/num_examples': 10000, 'score': 61318.90201878548, 'total_duration': 65889.40408539772, 'accumulated_submission_time': 61318.90201878548, 'accumulated_eval_time': 4548.296358346939, 'accumulated_logging_time': 10.318055152893066, 'global_step': 101984, 'preemption_count': 0}), (102112, {'train/accuracy': 0.3849848508834839, 'train/loss': 2.9460418224334717, 'validation/accuracy': 0.3582199811935425, 'validation/loss': 3.1243159770965576, 'validation/num_examples': 50000, 'test/accuracy': 0.27060002088546753, 'test/loss': 3.927813768386841, 'test/num_examples': 10000, 'score': 61829.06133508682, 'total_duration': 66438.59041166306, 'accumulated_submission_time': 61829.06133508682, 'accumulated_eval_time': 4587.245951652527, 'accumulated_logging_time': 10.378966331481934, 'global_step': 102112, 'preemption_count': 0}), (102337, {'train/accuracy': 0.3403419852256775, 'train/loss': 3.2988839149475098, 'validation/accuracy': 0.30395999550819397, 'validation/loss': 3.5761818885803223, 'validation/num_examples': 50000, 'test/accuracy': 0.22350001335144043, 'test/loss': 4.403056621551514, 'test/num_examples': 10000, 'score': 62339.34055161476, 'total_duration': 66980.62467432022, 'accumulated_submission_time': 62339.34055161476, 'accumulated_eval_time': 4618.941087007523, 'accumulated_logging_time': 10.413552045822144, 'global_step': 102337, 'preemption_count': 0}), (102700, {'train/accuracy': 0.4902941584587097, 'train/loss': 2.2255101203918457, 'validation/accuracy': 0.45367997884750366, 'validation/loss': 2.4762954711914062, 'validation/num_examples': 50000, 'test/accuracy': 0.3379000127315521, 'test/loss': 3.356130599975586, 'test/num_examples': 10000, 'score': 62850.36156988144, 'total_duration': 67525.33063077927, 'accumulated_submission_time': 62850.36156988144, 'accumulated_eval_time': 4652.516487121582, 'accumulated_logging_time': 10.481064081192017, 'global_step': 102700, 'preemption_count': 0}), (103065, {'train/accuracy': 0.4428810477256775, 'train/loss': 2.5138564109802246, 'validation/accuracy': 0.4138000011444092, 'validation/loss': 2.736682653427124, 'validation/num_examples': 50000, 'test/accuracy': 0.3174000084400177, 'test/loss': 3.503340721130371, 'test/num_examples': 10000, 'score': 63361.57276439667, 'total_duration': 68069.78669524193, 'accumulated_submission_time': 63361.57276439667, 'accumulated_eval_time': 4685.653913021088, 'accumulated_logging_time': 10.547064065933228, 'global_step': 103065, 'preemption_count': 0}), (103431, {'train/accuracy': 0.31449297070503235, 'train/loss': 3.3983209133148193, 'validation/accuracy': 0.2950199842453003, 'validation/loss': 3.5421199798583984, 'validation/num_examples': 50000, 'test/accuracy': 0.22750000655651093, 'test/loss': 4.212413787841797, 'test/num_examples': 10000, 'score': 63872.73085141182, 'total_duration': 68614.4632871151, 'accumulated_submission_time': 63872.73085141182, 'accumulated_eval_time': 4719.043176651001, 'accumulated_logging_time': 10.635022163391113, 'global_step': 103431, 'preemption_count': 0}), (103761, {'train/accuracy': 0.45344385504722595, 'train/loss': 2.4801504611968994, 'validation/accuracy': 0.4223399758338928, 'validation/loss': 2.708570957183838, 'validation/num_examples': 50000, 'test/accuracy': 0.3102000057697296, 'test/loss': 3.5214643478393555, 'test/num_examples': 10000, 'score': 64383.92204976082, 'total_duration': 69157.69124484062, 'accumulated_submission_time': 64383.92204976082, 'accumulated_eval_time': 4750.986273527145, 'accumulated_logging_time': 10.690326690673828, 'global_step': 103761, 'preemption_count': 0}), (104110, {'train/accuracy': 0.3767338991165161, 'train/loss': 3.0262625217437744, 'validation/accuracy': 0.3522599935531616, 'validation/loss': 3.256324291229248, 'validation/num_examples': 50000, 'test/accuracy': 0.2578999996185303, 'test/loss': 4.11581563949585, 'test/num_examples': 10000, 'score': 64894.31019735336, 'total_duration': 69701.52133011818, 'accumulated_submission_time': 64894.31019735336, 'accumulated_eval_time': 4784.314296245575, 'accumulated_logging_time': 10.7651047706604, 'global_step': 104110, 'preemption_count': 0}), (104472, {'train/accuracy': 0.4209582209587097, 'train/loss': 2.674959659576416, 'validation/accuracy': 0.3912400007247925, 'validation/loss': 2.8653271198272705, 'validation/num_examples': 50000, 'test/accuracy': 0.3004000186920166, 'test/loss': 3.550882577896118, 'test/num_examples': 10000, 'score': 65404.39068698883, 'total_duration': 70245.05532002449, 'accumulated_submission_time': 65404.39068698883, 'accumulated_eval_time': 4817.671086549759, 'accumulated_logging_time': 10.819352149963379, 'global_step': 104472, 'preemption_count': 0}), (104883, {'train/accuracy': 0.33380499482154846, 'train/loss': 3.273742914199829, 'validation/accuracy': 0.3133399784564972, 'validation/loss': 3.427790880203247, 'validation/num_examples': 50000, 'test/accuracy': 0.2192000150680542, 'test/loss': 4.2581281661987305, 'test/num_examples': 10000, 'score': 65914.81075906754, 'total_duration': 70790.74099683762, 'accumulated_submission_time': 65914.81075906754, 'accumulated_eval_time': 4852.814576387405, 'accumulated_logging_time': 10.892746448516846, 'global_step': 104883, 'preemption_count': 0})], 'global_step': 105357}
I0307 22:52:01.190274 140050941662400 submission_runner.py:649] Timing: 66423.4827477932
I0307 22:52:01.190324 140050941662400 submission_runner.py:651] Total number of evals: 130
I0307 22:52:01.190353 140050941662400 submission_runner.py:652] ====================
I0307 22:52:01.190596 140050941662400 submission_runner.py:750] Final imagenet_resnet score: 3
