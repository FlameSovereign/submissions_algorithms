python submission_runner.py --framework=jax --workload=imagenet_resnet --submission_path=prize_qualification_baselines/external_tuning/jax_nadamw_full_budget.py --data_dir=/data/imagenet/jax --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=submissions/rolling_leaderboard/external_tuning/baseline/study_1 --overwrite=True --save_checkpoints=False --rng_seed=-1825857388 --imagenet_v2_data_dir=/data/imagenet/jax --tuning_ruleset=external --tuning_search_space=prize_qualification_baselines/external_tuning/tuning_search_space.json --num_tuning_trials=5 --hparam_start_index=1 --hparam_end_index=2 2>&1 | tee -a /logs/imagenet_resnet_jax_03-07-2025-02-25-12.log
2025-03-07 02:25:29.756315: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1741314330.175435       9 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1741314330.308473       9 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
I0307 02:26:21.480913 140191611933888 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_1/imagenet_resnet_jax.
I0307 02:26:24.849191 140191611933888 xla_bridge.py:884] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA
I0307 02:26:24.852386 140191611933888 xla_bridge.py:884] Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory
I0307 02:26:24.867054 140191611933888 submission_runner.py:606] Using RNG seed -1825857388
I0307 02:26:30.311785 140191611933888 submission_runner.py:615] --- Tuning run 2/5 ---
I0307 02:26:30.311994 140191611933888 submission_runner.py:620] Creating tuning directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_1/imagenet_resnet_jax/trial_2.
I0307 02:26:30.312190 140191611933888 logger_utils.py:97] Saving hparams to /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_1/imagenet_resnet_jax/trial_2/hparams.json.
I0307 02:26:30.557680 140191611933888 submission_runner.py:218] Initializing dataset.
I0307 02:26:32.294034 140191611933888 dataset_info.py:690] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0307 02:26:32.637233 140191611933888 reader.py:261] Creating a tf.data.Dataset reading 1024 files located in folders: /data/imagenet/jax/imagenet2012/5.1.0.
I0307 02:26:32.974839 140191611933888 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split train, from /data/imagenet/jax/imagenet2012/5.1.0
I0307 02:26:34.629006 140191611933888 submission_runner.py:229] Initializing model.
I0307 02:26:58.204215 140191611933888 submission_runner.py:272] Initializing optimizer.
I0307 02:26:59.340091 140191611933888 submission_runner.py:279] Initializing metrics bundle.
I0307 02:26:59.340338 140191611933888 submission_runner.py:301] Initializing checkpoint and logger.
I0307 02:26:59.341414 140191611933888 checkpoints.py:1101] Found no checkpoint files in /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_1/imagenet_resnet_jax/trial_2 with prefix checkpoint_
I0307 02:26:59.341522 140191611933888 submission_runner.py:321] Saving meta data to /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_1/imagenet_resnet_jax/trial_2/meta_data_0.json.
I0307 02:26:59.893730 140191611933888 submission_runner.py:325] Saving flags to /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_1/imagenet_resnet_jax/trial_2/flags_0.json.
I0307 02:27:00.346185 140191611933888 submission_runner.py:337] Starting training loop.
2025-03-07 02:27:29.870781: E external/xla/xla/service/slow_operation_alarm.cc:65] Trying algorithm eng0{} for conv (f32[128,512,15,15]{3,2,1,0}, u8[0]{0}) custom-call(f32[128,512,7,7]{3,2,1,0}, f32[512,512,3,3]{3,2,1,0}), window={size=3x3 stride=2x2}, dim_labels=bf01_oi01->bf01, custom_call_target="__cudnn$convBackwardInput", backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"cudnn_conv_backend_config":{"conv_result_scale":1,"activation_mode":"kNone","side_input_scale":0,"leakyrelu_alpha":0},"force_earliest_schedule":false} is taking a while...
2025-03-07 02:27:29.889722: E external/xla/xla/service/slow_operation_alarm.cc:133] The operation took 1.019145567s
Trying algorithm eng0{} for conv (f32[128,512,15,15]{3,2,1,0}, u8[0]{0}) custom-call(f32[128,512,7,7]{3,2,1,0}, f32[512,512,3,3]{3,2,1,0}), window={size=3x3 stride=2x2}, dim_labels=bf01_oi01->bf01, custom_call_target="__cudnn$convBackwardInput", backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"cudnn_conv_backend_config":{"conv_result_scale":1,"activation_mode":"kNone","side_input_scale":0,"leakyrelu_alpha":0},"force_earliest_schedule":false} is taking a while...
I0307 02:28:00.157745 140053938415360 logging_writer.py:48] [0] global_step=0, grad_norm=0.5363162755966187, loss=6.925543785095215
I0307 02:28:00.550755 140191611933888 spec.py:321] Evaluating on the training split.
I0307 02:28:01.034610 140191611933888 dataset_info.py:690] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0307 02:28:01.061676 140191611933888 reader.py:261] Creating a tf.data.Dataset reading 1024 files located in folders: /data/imagenet/jax/imagenet2012/5.1.0.
I0307 02:28:01.108410 140191611933888 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split train, from /data/imagenet/jax/imagenet2012/5.1.0
I0307 02:28:20.530665 140191611933888 spec.py:333] Evaluating on the validation split.
I0307 02:28:20.984352 140191611933888 dataset_info.py:690] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0307 02:28:20.992952 140191611933888 reader.py:261] Creating a tf.data.Dataset reading 64 files located in folders: /data/imagenet/jax/imagenet2012/5.1.0.
I0307 02:28:21.213542 140191611933888 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split validation, from /data/imagenet/jax/imagenet2012/5.1.0
I0307 02:29:07.329034 140191611933888 spec.py:349] Evaluating on the test split.
I0307 02:29:07.788702 140191611933888 dataset_info.py:690] Load dataset info from /data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0
I0307 02:29:07.821632 140191611933888 reader.py:261] Creating a tf.data.Dataset reading 16 files located in folders: /data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0.
I0307 02:29:07.858392 140191611933888 logging_logger.py:49] Constructing tf.data.Dataset imagenet_v2 for split test, from /data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0
I0307 02:29:31.860101 140191611933888 submission_runner.py:469] Time since start: 151.51s, 	Step: 1, 	{'train/accuracy': 0.0006776147638447583, 'train/loss': 6.91315221786499, 'validation/accuracy': 0.0007599999662488699, 'validation/loss': 6.913278102874756, 'validation/num_examples': 50000, 'test/accuracy': 0.0005000000237487257, 'test/loss': 6.913549423217773, 'test/num_examples': 10000, 'score': 60.20431733131409, 'total_duration': 151.5138657093048, 'accumulated_submission_time': 60.20431733131409, 'accumulated_eval_time': 91.3093032836914, 'accumulated_logging_time': 0}
I0307 02:29:31.933630 140036183938816 logging_writer.py:48] [1] accumulated_eval_time=91.3093, accumulated_logging_time=0, accumulated_submission_time=60.2043, global_step=1, preemption_count=0, score=60.2043, test/accuracy=0.0005, test/loss=6.91355, test/num_examples=10000, total_duration=151.514, train/accuracy=0.000677615, train/loss=6.91315, validation/accuracy=0.00076, validation/loss=6.91328, validation/num_examples=50000
I0307 02:30:08.264034 140036175546112 logging_writer.py:48] [100] global_step=100, grad_norm=0.5263246893882751, loss=6.904157638549805
I0307 02:30:44.478475 140036183938816 logging_writer.py:48] [200] global_step=200, grad_norm=0.5452143549919128, loss=6.858860492706299
I0307 02:31:22.054406 140036175546112 logging_writer.py:48] [300] global_step=300, grad_norm=0.5726307034492493, loss=6.774511814117432
I0307 02:31:59.343392 140036183938816 logging_writer.py:48] [400] global_step=400, grad_norm=0.6028873324394226, loss=6.697347640991211
I0307 02:32:36.873236 140036175546112 logging_writer.py:48] [500] global_step=500, grad_norm=0.6429052948951721, loss=6.597597599029541
I0307 02:33:14.655928 140036183938816 logging_writer.py:48] [600] global_step=600, grad_norm=0.6718041896820068, loss=6.540067195892334
I0307 02:33:52.581778 140036175546112 logging_writer.py:48] [700] global_step=700, grad_norm=0.8370829224586487, loss=6.452940464019775
I0307 02:34:30.999669 140036183938816 logging_writer.py:48] [800] global_step=800, grad_norm=1.1230778694152832, loss=6.382935523986816
I0307 02:35:09.124138 140036175546112 logging_writer.py:48] [900] global_step=900, grad_norm=1.6974806785583496, loss=6.3427019119262695
I0307 02:35:46.945024 140036183938816 logging_writer.py:48] [1000] global_step=1000, grad_norm=1.7095680236816406, loss=6.215790748596191
I0307 02:36:24.920375 140036175546112 logging_writer.py:48] [1100] global_step=1100, grad_norm=1.4780957698822021, loss=6.177435874938965
I0307 02:37:03.460832 140036183938816 logging_writer.py:48] [1200] global_step=1200, grad_norm=1.1794995069503784, loss=6.082265853881836
I0307 02:37:41.053992 140036175546112 logging_writer.py:48] [1300] global_step=1300, grad_norm=1.7547328472137451, loss=6.068624973297119
I0307 02:38:02.179690 140191611933888 spec.py:321] Evaluating on the training split.
I0307 02:38:14.572522 140191611933888 spec.py:333] Evaluating on the validation split.
I0307 02:38:45.257599 140191611933888 spec.py:349] Evaluating on the test split.
I0307 02:38:47.225964 140191611933888 submission_runner.py:469] Time since start: 706.88s, 	Step: 1356, 	{'train/accuracy': 0.07834422588348389, 'train/loss': 5.371662139892578, 'validation/accuracy': 0.06749999523162842, 'validation/loss': 5.458141803741455, 'validation/num_examples': 50000, 'test/accuracy': 0.04960000142455101, 'test/loss': 5.669939994812012, 'test/num_examples': 10000, 'score': 570.2540411949158, 'total_duration': 706.8797223567963, 'accumulated_submission_time': 570.2540411949158, 'accumulated_eval_time': 136.35554671287537, 'accumulated_logging_time': 0.08300662040710449}
I0307 02:38:47.250526 140036192331520 logging_writer.py:48] [1356] accumulated_eval_time=136.356, accumulated_logging_time=0.0830066, accumulated_submission_time=570.254, global_step=1356, preemption_count=0, score=570.254, test/accuracy=0.0496, test/loss=5.66994, test/num_examples=10000, total_duration=706.88, train/accuracy=0.0783442, train/loss=5.37166, validation/accuracy=0.0675, validation/loss=5.45814, validation/num_examples=50000
I0307 02:39:04.631171 140036200724224 logging_writer.py:48] [1400] global_step=1400, grad_norm=2.3088607788085938, loss=5.994607925415039
I0307 02:39:42.988226 140036192331520 logging_writer.py:48] [1500] global_step=1500, grad_norm=2.2522172927856445, loss=5.956236362457275
I0307 02:40:21.121518 140036200724224 logging_writer.py:48] [1600] global_step=1600, grad_norm=2.9319348335266113, loss=5.881945610046387
I0307 02:40:59.410005 140036192331520 logging_writer.py:48] [1700] global_step=1700, grad_norm=2.5135996341705322, loss=5.822481155395508
I0307 02:41:37.985287 140036200724224 logging_writer.py:48] [1800] global_step=1800, grad_norm=3.7587153911590576, loss=5.888894557952881
I0307 02:42:16.286067 140036192331520 logging_writer.py:48] [1900] global_step=1900, grad_norm=3.712792158126831, loss=5.757574558258057
I0307 02:42:54.527872 140036200724224 logging_writer.py:48] [2000] global_step=2000, grad_norm=3.3276522159576416, loss=5.7324538230896
I0307 02:43:33.084656 140036192331520 logging_writer.py:48] [2100] global_step=2100, grad_norm=6.351052284240723, loss=5.65494966506958
I0307 02:44:11.654418 140036200724224 logging_writer.py:48] [2200] global_step=2200, grad_norm=3.517469644546509, loss=5.600826740264893
I0307 02:44:50.255152 140036192331520 logging_writer.py:48] [2300] global_step=2300, grad_norm=2.1927247047424316, loss=5.5669097900390625
I0307 02:45:28.370210 140036200724224 logging_writer.py:48] [2400] global_step=2400, grad_norm=4.591664791107178, loss=5.540990352630615
I0307 02:46:06.600864 140036192331520 logging_writer.py:48] [2500] global_step=2500, grad_norm=2.7703428268432617, loss=5.47314453125
I0307 02:46:44.809300 140036200724224 logging_writer.py:48] [2600] global_step=2600, grad_norm=3.539109945297241, loss=5.47080135345459
I0307 02:47:17.340053 140191611933888 spec.py:321] Evaluating on the training split.
I0307 02:47:28.528060 140191611933888 spec.py:333] Evaluating on the validation split.
I0307 02:47:48.717785 140191611933888 spec.py:349] Evaluating on the test split.
I0307 02:47:50.575452 140191611933888 submission_runner.py:469] Time since start: 1250.23s, 	Step: 2686, 	{'train/accuracy': 0.1771763414144516, 'train/loss': 4.352410316467285, 'validation/accuracy': 0.1540599912405014, 'validation/loss': 4.5127787590026855, 'validation/num_examples': 50000, 'test/accuracy': 0.11310000717639923, 'test/loss': 4.896148681640625, 'test/num_examples': 10000, 'score': 1080.14115691185, 'total_duration': 1250.2292065620422, 'accumulated_submission_time': 1080.14115691185, 'accumulated_eval_time': 169.59089183807373, 'accumulated_logging_time': 0.14267945289611816}
I0307 02:47:50.639534 140036192331520 logging_writer.py:48] [2686] accumulated_eval_time=169.591, accumulated_logging_time=0.142679, accumulated_submission_time=1080.14, global_step=2686, preemption_count=0, score=1080.14, test/accuracy=0.1131, test/loss=4.89615, test/num_examples=10000, total_duration=1250.23, train/accuracy=0.177176, train/loss=4.35241, validation/accuracy=0.15406, validation/loss=4.51278, validation/num_examples=50000
I0307 02:47:56.552747 140036200724224 logging_writer.py:48] [2700] global_step=2700, grad_norm=5.015771389007568, loss=5.414680480957031
I0307 02:48:35.118481 140036192331520 logging_writer.py:48] [2800] global_step=2800, grad_norm=3.7848827838897705, loss=5.361686706542969
I0307 02:49:13.566938 140036200724224 logging_writer.py:48] [2900] global_step=2900, grad_norm=4.141910552978516, loss=5.301449775695801
I0307 02:49:52.271998 140036192331520 logging_writer.py:48] [3000] global_step=3000, grad_norm=3.1390435695648193, loss=5.279829025268555
I0307 02:50:31.048178 140036200724224 logging_writer.py:48] [3100] global_step=3100, grad_norm=3.5699334144592285, loss=5.236952781677246
I0307 02:51:09.626220 140036192331520 logging_writer.py:48] [3200] global_step=3200, grad_norm=2.373777151107788, loss=5.195975303649902
I0307 02:51:48.237451 140036200724224 logging_writer.py:48] [3300] global_step=3300, grad_norm=4.541661262512207, loss=5.1279826164245605
I0307 02:52:26.835272 140036192331520 logging_writer.py:48] [3400] global_step=3400, grad_norm=4.128343105316162, loss=5.1711835861206055
I0307 02:53:05.609077 140036200724224 logging_writer.py:48] [3500] global_step=3500, grad_norm=2.9203646183013916, loss=5.1025543212890625
I0307 02:53:44.106554 140036192331520 logging_writer.py:48] [3600] global_step=3600, grad_norm=3.8691012859344482, loss=5.064062118530273
I0307 02:54:22.532595 140036200724224 logging_writer.py:48] [3700] global_step=3700, grad_norm=4.549208164215088, loss=4.982385635375977
I0307 02:55:00.934155 140036192331520 logging_writer.py:48] [3800] global_step=3800, grad_norm=4.443924427032471, loss=4.893694877624512
I0307 02:55:39.421738 140036200724224 logging_writer.py:48] [3900] global_step=3900, grad_norm=3.7115845680236816, loss=5.092080593109131
I0307 02:56:17.896780 140036192331520 logging_writer.py:48] [4000] global_step=4000, grad_norm=3.8874754905700684, loss=4.888167381286621
I0307 02:56:20.641465 140191611933888 spec.py:321] Evaluating on the training split.
I0307 02:56:32.334214 140191611933888 spec.py:333] Evaluating on the validation split.
I0307 02:56:54.279831 140191611933888 spec.py:349] Evaluating on the test split.
I0307 02:56:56.072488 140191611933888 submission_runner.py:469] Time since start: 1795.73s, 	Step: 4008, 	{'train/accuracy': 0.2733577787876129, 'train/loss': 3.6192002296447754, 'validation/accuracy': 0.2360599935054779, 'validation/loss': 3.839977979660034, 'validation/num_examples': 50000, 'test/accuracy': 0.17180000245571136, 'test/loss': 4.346663951873779, 'test/num_examples': 10000, 'score': 1589.9829618930817, 'total_duration': 1795.7262625694275, 'accumulated_submission_time': 1589.9829618930817, 'accumulated_eval_time': 205.02187991142273, 'accumulated_logging_time': 0.21509766578674316}
I0307 02:56:56.106026 140036200724224 logging_writer.py:48] [4008] accumulated_eval_time=205.022, accumulated_logging_time=0.215098, accumulated_submission_time=1589.98, global_step=4008, preemption_count=0, score=1589.98, test/accuracy=0.1718, test/loss=4.34666, test/num_examples=10000, total_duration=1795.73, train/accuracy=0.273358, train/loss=3.6192, validation/accuracy=0.23606, validation/loss=3.83998, validation/num_examples=50000
I0307 02:57:31.252526 140036192331520 logging_writer.py:48] [4100] global_step=4100, grad_norm=4.6667799949646, loss=4.908332824707031
I0307 02:58:09.412179 140036200724224 logging_writer.py:48] [4200] global_step=4200, grad_norm=4.953874111175537, loss=4.8442583084106445
I0307 02:58:47.704074 140036192331520 logging_writer.py:48] [4300] global_step=4300, grad_norm=3.459587812423706, loss=4.877981185913086
I0307 02:59:26.741813 140036200724224 logging_writer.py:48] [4400] global_step=4400, grad_norm=4.335048675537109, loss=4.7184739112854
I0307 03:00:05.274753 140036192331520 logging_writer.py:48] [4500] global_step=4500, grad_norm=5.261767864227295, loss=4.7223381996154785
I0307 03:00:44.277466 140036200724224 logging_writer.py:48] [4600] global_step=4600, grad_norm=3.7314698696136475, loss=4.729707717895508
I0307 03:01:22.819410 140036192331520 logging_writer.py:48] [4700] global_step=4700, grad_norm=4.508379936218262, loss=4.699368476867676
I0307 03:02:01.439270 140036200724224 logging_writer.py:48] [4800] global_step=4800, grad_norm=3.8411529064178467, loss=4.697479248046875
I0307 03:02:40.227639 140036192331520 logging_writer.py:48] [4900] global_step=4900, grad_norm=3.5033769607543945, loss=4.677803993225098
I0307 03:03:18.796554 140036200724224 logging_writer.py:48] [5000] global_step=5000, grad_norm=3.2238245010375977, loss=4.557092189788818
I0307 03:03:57.223177 140036192331520 logging_writer.py:48] [5100] global_step=5100, grad_norm=3.640209913253784, loss=4.5575480461120605
I0307 03:04:35.813014 140036200724224 logging_writer.py:48] [5200] global_step=5200, grad_norm=3.729351758956909, loss=4.616296291351318
I0307 03:05:14.341951 140036192331520 logging_writer.py:48] [5300] global_step=5300, grad_norm=4.894742012023926, loss=4.6182146072387695
I0307 03:05:26.154919 140191611933888 spec.py:321] Evaluating on the training split.
I0307 03:05:39.479376 140191611933888 spec.py:333] Evaluating on the validation split.
I0307 03:06:06.673443 140191611933888 spec.py:349] Evaluating on the test split.
I0307 03:06:08.472738 140191611933888 submission_runner.py:469] Time since start: 2348.13s, 	Step: 5333, 	{'train/accuracy': 0.36828362941741943, 'train/loss': 3.1484599113464355, 'validation/accuracy': 0.32999998331069946, 'validation/loss': 3.3475239276885986, 'validation/num_examples': 50000, 'test/accuracy': 0.24570001661777496, 'test/loss': 3.890148639678955, 'test/num_examples': 10000, 'score': 2099.870808839798, 'total_duration': 2348.1265008449554, 'accumulated_submission_time': 2099.870808839798, 'accumulated_eval_time': 247.33965063095093, 'accumulated_logging_time': 0.2575819492340088}
I0307 03:06:08.491835 140036200724224 logging_writer.py:48] [5333] accumulated_eval_time=247.34, accumulated_logging_time=0.257582, accumulated_submission_time=2099.87, global_step=5333, preemption_count=0, score=2099.87, test/accuracy=0.2457, test/loss=3.89015, test/num_examples=10000, total_duration=2348.13, train/accuracy=0.368284, train/loss=3.14846, validation/accuracy=0.33, validation/loss=3.34752, validation/num_examples=50000
I0307 03:06:34.241478 140036192331520 logging_writer.py:48] [5400] global_step=5400, grad_norm=3.479266881942749, loss=4.522871494293213
I0307 03:07:12.791265 140036200724224 logging_writer.py:48] [5500] global_step=5500, grad_norm=3.04756236076355, loss=4.476747035980225
I0307 03:07:51.515614 140036192331520 logging_writer.py:48] [5600] global_step=5600, grad_norm=2.8515143394470215, loss=4.475589275360107
I0307 03:08:30.052026 140036200724224 logging_writer.py:48] [5700] global_step=5700, grad_norm=3.6291158199310303, loss=4.462716102600098
I0307 03:09:08.403387 140036192331520 logging_writer.py:48] [5800] global_step=5800, grad_norm=2.8528454303741455, loss=4.481630325317383
I0307 03:09:46.577690 140036200724224 logging_writer.py:48] [5900] global_step=5900, grad_norm=5.419063568115234, loss=4.4176716804504395
I0307 03:10:25.092378 140036192331520 logging_writer.py:48] [6000] global_step=6000, grad_norm=2.6618452072143555, loss=4.373298645019531
I0307 03:11:03.183067 140036200724224 logging_writer.py:48] [6100] global_step=6100, grad_norm=3.365699291229248, loss=4.407833099365234
I0307 03:11:41.478938 140036192331520 logging_writer.py:48] [6200] global_step=6200, grad_norm=3.5988662242889404, loss=4.357047080993652
I0307 03:12:19.708276 140036200724224 logging_writer.py:48] [6300] global_step=6300, grad_norm=2.4360969066619873, loss=4.332468032836914
I0307 03:12:57.498117 140036192331520 logging_writer.py:48] [6400] global_step=6400, grad_norm=3.1367721557617188, loss=4.355591773986816
I0307 03:13:35.948719 140036200724224 logging_writer.py:48] [6500] global_step=6500, grad_norm=2.0944087505340576, loss=4.285213947296143
I0307 03:14:14.205309 140036192331520 logging_writer.py:48] [6600] global_step=6600, grad_norm=3.330852508544922, loss=4.313295364379883
I0307 03:14:38.569731 140191611933888 spec.py:321] Evaluating on the training split.
I0307 03:14:51.731389 140191611933888 spec.py:333] Evaluating on the validation split.
I0307 03:15:13.564468 140191611933888 spec.py:349] Evaluating on the test split.
I0307 03:15:15.409164 140191611933888 submission_runner.py:469] Time since start: 2895.06s, 	Step: 6665, 	{'train/accuracy': 0.4065887928009033, 'train/loss': 2.868271827697754, 'validation/accuracy': 0.359279990196228, 'validation/loss': 3.1143975257873535, 'validation/num_examples': 50000, 'test/accuracy': 0.2720000147819519, 'test/loss': 3.6712520122528076, 'test/num_examples': 10000, 'score': 2609.7945597171783, 'total_duration': 2895.062940120697, 'accumulated_submission_time': 2609.7945597171783, 'accumulated_eval_time': 284.1790518760681, 'accumulated_logging_time': 0.2852938175201416}
I0307 03:15:15.434775 140036200724224 logging_writer.py:48] [6665] accumulated_eval_time=284.179, accumulated_logging_time=0.285294, accumulated_submission_time=2609.79, global_step=6665, preemption_count=0, score=2609.79, test/accuracy=0.272, test/loss=3.67125, test/num_examples=10000, total_duration=2895.06, train/accuracy=0.406589, train/loss=2.86827, validation/accuracy=0.35928, validation/loss=3.1144, validation/num_examples=50000
I0307 03:15:29.356260 140036192331520 logging_writer.py:48] [6700] global_step=6700, grad_norm=3.1445908546447754, loss=4.267122268676758
I0307 03:16:07.648746 140036200724224 logging_writer.py:48] [6800] global_step=6800, grad_norm=4.344742298126221, loss=4.259903907775879
I0307 03:16:46.071690 140036192331520 logging_writer.py:48] [6900] global_step=6900, grad_norm=3.333773374557495, loss=4.2258124351501465
I0307 03:17:24.415545 140036200724224 logging_writer.py:48] [7000] global_step=7000, grad_norm=3.470156669616699, loss=4.275632381439209
I0307 03:18:03.066459 140036192331520 logging_writer.py:48] [7100] global_step=7100, grad_norm=2.6125969886779785, loss=4.177297115325928
I0307 03:18:41.324060 140036200724224 logging_writer.py:48] [7200] global_step=7200, grad_norm=2.803495407104492, loss=4.139085292816162
I0307 03:19:19.810467 140036192331520 logging_writer.py:48] [7300] global_step=7300, grad_norm=2.545564651489258, loss=4.129161834716797
I0307 03:19:58.021084 140036200724224 logging_writer.py:48] [7400] global_step=7400, grad_norm=2.0819506645202637, loss=4.201379776000977
I0307 03:20:36.345363 140036192331520 logging_writer.py:48] [7500] global_step=7500, grad_norm=2.740638494491577, loss=4.114597320556641
I0307 03:21:14.663886 140036200724224 logging_writer.py:48] [7600] global_step=7600, grad_norm=2.170297384262085, loss=4.082126140594482
I0307 03:21:52.850292 140036192331520 logging_writer.py:48] [7700] global_step=7700, grad_norm=2.86049485206604, loss=4.106783866882324
I0307 03:22:31.135117 140036200724224 logging_writer.py:48] [7800] global_step=7800, grad_norm=2.9356496334075928, loss=4.116986274719238
I0307 03:23:10.436455 140036192331520 logging_writer.py:48] [7900] global_step=7900, grad_norm=2.3141510486602783, loss=4.150826454162598
I0307 03:23:45.423559 140191611933888 spec.py:321] Evaluating on the training split.
I0307 03:23:57.529525 140191611933888 spec.py:333] Evaluating on the validation split.
I0307 03:24:21.563038 140191611933888 spec.py:349] Evaluating on the test split.
I0307 03:24:23.369955 140191611933888 submission_runner.py:469] Time since start: 3443.02s, 	Step: 7992, 	{'train/accuracy': 0.4759247303009033, 'train/loss': 2.4640872478485107, 'validation/accuracy': 0.4297599792480469, 'validation/loss': 2.7139670848846436, 'validation/num_examples': 50000, 'test/accuracy': 0.3273000121116638, 'test/loss': 3.340256452560425, 'test/num_examples': 10000, 'score': 3119.5938155651093, 'total_duration': 3443.023730993271, 'accumulated_submission_time': 3119.5938155651093, 'accumulated_eval_time': 322.1254200935364, 'accumulated_logging_time': 0.36200976371765137}
I0307 03:24:23.391798 140036200724224 logging_writer.py:48] [7992] accumulated_eval_time=322.125, accumulated_logging_time=0.36201, accumulated_submission_time=3119.59, global_step=7992, preemption_count=0, score=3119.59, test/accuracy=0.3273, test/loss=3.34026, test/num_examples=10000, total_duration=3443.02, train/accuracy=0.475925, train/loss=2.46409, validation/accuracy=0.42976, validation/loss=2.71397, validation/num_examples=50000
I0307 03:24:26.954599 140036192331520 logging_writer.py:48] [8000] global_step=8000, grad_norm=3.9970693588256836, loss=4.086134910583496
I0307 03:25:05.157282 140036200724224 logging_writer.py:48] [8100] global_step=8100, grad_norm=2.5513744354248047, loss=4.001094818115234
I0307 03:25:43.587528 140036192331520 logging_writer.py:48] [8200] global_step=8200, grad_norm=2.2775492668151855, loss=4.002427101135254
I0307 03:26:21.983864 140036200724224 logging_writer.py:48] [8300] global_step=8300, grad_norm=2.2960002422332764, loss=4.070313453674316
I0307 03:27:00.488895 140036192331520 logging_writer.py:48] [8400] global_step=8400, grad_norm=2.306790590286255, loss=4.096363544464111
I0307 03:27:38.850175 140036200724224 logging_writer.py:48] [8500] global_step=8500, grad_norm=3.167844533920288, loss=4.04467248916626
I0307 03:28:17.375490 140036192331520 logging_writer.py:48] [8600] global_step=8600, grad_norm=2.2558352947235107, loss=3.9696433544158936
I0307 03:28:55.946952 140036200724224 logging_writer.py:48] [8700] global_step=8700, grad_norm=2.0680596828460693, loss=4.005458354949951
I0307 03:29:33.663103 140036192331520 logging_writer.py:48] [8800] global_step=8800, grad_norm=2.0584793090820312, loss=4.016546726226807
I0307 03:30:12.048685 140036200724224 logging_writer.py:48] [8900] global_step=8900, grad_norm=2.5725040435791016, loss=3.8838324546813965
I0307 03:30:50.453520 140036192331520 logging_writer.py:48] [9000] global_step=9000, grad_norm=2.403407096862793, loss=3.940847396850586
I0307 03:31:28.506337 140036200724224 logging_writer.py:48] [9100] global_step=9100, grad_norm=3.037539005279541, loss=3.9421474933624268
I0307 03:32:06.863259 140036192331520 logging_writer.py:48] [9200] global_step=9200, grad_norm=2.495893716812134, loss=3.888284921646118
I0307 03:32:45.178431 140036200724224 logging_writer.py:48] [9300] global_step=9300, grad_norm=2.074838161468506, loss=3.891106605529785
I0307 03:32:53.669145 140191611933888 spec.py:321] Evaluating on the training split.
I0307 03:33:05.347131 140191611933888 spec.py:333] Evaluating on the validation split.
I0307 03:33:27.930124 140191611933888 spec.py:349] Evaluating on the test split.
I0307 03:33:29.758497 140191611933888 submission_runner.py:469] Time since start: 3989.41s, 	Step: 9323, 	{'train/accuracy': 0.5236966013908386, 'train/loss': 2.300750255584717, 'validation/accuracy': 0.47435998916625977, 'validation/loss': 2.542717218399048, 'validation/num_examples': 50000, 'test/accuracy': 0.35450002551078796, 'test/loss': 3.2271807193756104, 'test/num_examples': 10000, 'score': 3629.714698076248, 'total_duration': 3989.412269592285, 'accumulated_submission_time': 3629.714698076248, 'accumulated_eval_time': 358.2147421836853, 'accumulated_logging_time': 0.3919539451599121}
I0307 03:33:29.878256 140036192331520 logging_writer.py:48] [9323] accumulated_eval_time=358.215, accumulated_logging_time=0.391954, accumulated_submission_time=3629.71, global_step=9323, preemption_count=0, score=3629.71, test/accuracy=0.3545, test/loss=3.22718, test/num_examples=10000, total_duration=3989.41, train/accuracy=0.523697, train/loss=2.30075, validation/accuracy=0.47436, validation/loss=2.54272, validation/num_examples=50000
I0307 03:34:00.227810 140036200724224 logging_writer.py:48] [9400] global_step=9400, grad_norm=2.1078293323516846, loss=3.847348928451538
I0307 03:34:38.861411 140036192331520 logging_writer.py:48] [9500] global_step=9500, grad_norm=2.7224326133728027, loss=3.9138286113739014
I0307 03:35:17.381588 140036200724224 logging_writer.py:48] [9600] global_step=9600, grad_norm=1.8556970357894897, loss=3.89467453956604
I0307 03:35:55.847652 140036192331520 logging_writer.py:48] [9700] global_step=9700, grad_norm=1.9907442331314087, loss=3.916609764099121
I0307 03:36:34.100308 140036200724224 logging_writer.py:48] [9800] global_step=9800, grad_norm=2.3427581787109375, loss=3.8376388549804688
I0307 03:37:12.449912 140036192331520 logging_writer.py:48] [9900] global_step=9900, grad_norm=2.2483139038085938, loss=3.786006450653076
I0307 03:37:50.766877 140036200724224 logging_writer.py:48] [10000] global_step=10000, grad_norm=2.563636302947998, loss=3.9010231494903564
I0307 03:38:29.446640 140036192331520 logging_writer.py:48] [10100] global_step=10100, grad_norm=2.173856019973755, loss=3.93076229095459
I0307 03:39:07.583450 140036200724224 logging_writer.py:48] [10200] global_step=10200, grad_norm=2.7762935161590576, loss=3.9331860542297363
I0307 03:39:45.914172 140036192331520 logging_writer.py:48] [10300] global_step=10300, grad_norm=1.7288554906845093, loss=3.8666176795959473
I0307 03:40:24.690118 140036200724224 logging_writer.py:48] [10400] global_step=10400, grad_norm=2.2597928047180176, loss=3.838034152984619
I0307 03:41:03.394732 140036192331520 logging_writer.py:48] [10500] global_step=10500, grad_norm=3.099905014038086, loss=3.8291635513305664
I0307 03:41:41.859065 140036200724224 logging_writer.py:48] [10600] global_step=10600, grad_norm=1.9380639791488647, loss=3.81459903717041
I0307 03:41:59.860717 140191611933888 spec.py:321] Evaluating on the training split.
I0307 03:42:12.304366 140191611933888 spec.py:333] Evaluating on the validation split.
I0307 03:42:42.094746 140191611933888 spec.py:349] Evaluating on the test split.
I0307 03:42:43.867996 140191611933888 submission_runner.py:469] Time since start: 4543.52s, 	Step: 10648, 	{'train/accuracy': 0.5633171200752258, 'train/loss': 2.0104331970214844, 'validation/accuracy': 0.511900007724762, 'validation/loss': 2.268040180206299, 'validation/num_examples': 50000, 'test/accuracy': 0.39490002393722534, 'test/loss': 2.933913230895996, 'test/num_examples': 10000, 'score': 4139.542843580246, 'total_duration': 4543.521776437759, 'accumulated_submission_time': 4139.542843580246, 'accumulated_eval_time': 402.2219989299774, 'accumulated_logging_time': 0.5196027755737305}
I0307 03:42:43.921884 140036192331520 logging_writer.py:48] [10648] accumulated_eval_time=402.222, accumulated_logging_time=0.519603, accumulated_submission_time=4139.54, global_step=10648, preemption_count=0, score=4139.54, test/accuracy=0.3949, test/loss=2.93391, test/num_examples=10000, total_duration=4543.52, train/accuracy=0.563317, train/loss=2.01043, validation/accuracy=0.5119, validation/loss=2.26804, validation/num_examples=50000
I0307 03:43:04.360507 140036200724224 logging_writer.py:48] [10700] global_step=10700, grad_norm=1.885413408279419, loss=3.764720916748047
I0307 03:43:42.936440 140036192331520 logging_writer.py:48] [10800] global_step=10800, grad_norm=2.597121000289917, loss=3.779052257537842
I0307 03:44:21.466837 140036200724224 logging_writer.py:48] [10900] global_step=10900, grad_norm=1.6344395875930786, loss=3.729153633117676
I0307 03:44:59.948011 140036192331520 logging_writer.py:48] [11000] global_step=11000, grad_norm=2.0838680267333984, loss=3.7136616706848145
I0307 03:45:38.484030 140036200724224 logging_writer.py:48] [11100] global_step=11100, grad_norm=1.861375331878662, loss=3.7696659564971924
I0307 03:46:16.858942 140036192331520 logging_writer.py:48] [11200] global_step=11200, grad_norm=2.2195355892181396, loss=3.7640082836151123
I0307 03:46:55.351903 140036200724224 logging_writer.py:48] [11300] global_step=11300, grad_norm=1.501923680305481, loss=3.752131462097168
I0307 03:47:33.900619 140036192331520 logging_writer.py:48] [11400] global_step=11400, grad_norm=1.8825186491012573, loss=3.76539945602417
I0307 03:48:12.121160 140036200724224 logging_writer.py:48] [11500] global_step=11500, grad_norm=1.4597488641738892, loss=3.788114070892334
I0307 03:48:50.416529 140036192331520 logging_writer.py:48] [11600] global_step=11600, grad_norm=3.0653576850891113, loss=3.698296546936035
I0307 03:49:29.122423 140036200724224 logging_writer.py:48] [11700] global_step=11700, grad_norm=2.142606258392334, loss=3.6682796478271484
I0307 03:50:07.802087 140036192331520 logging_writer.py:48] [11800] global_step=11800, grad_norm=1.3155945539474487, loss=3.673804759979248
I0307 03:50:46.360534 140036200724224 logging_writer.py:48] [11900] global_step=11900, grad_norm=1.4776026010513306, loss=3.752147674560547
I0307 03:51:14.083948 140191611933888 spec.py:321] Evaluating on the training split.
I0307 03:51:26.083505 140191611933888 spec.py:333] Evaluating on the validation split.
I0307 03:51:52.467499 140191611933888 spec.py:349] Evaluating on the test split.
I0307 03:51:54.255777 140191611933888 submission_runner.py:469] Time since start: 5093.91s, 	Step: 11973, 	{'train/accuracy': 0.5934112071990967, 'train/loss': 1.9632686376571655, 'validation/accuracy': 0.535860002040863, 'validation/loss': 2.2366771697998047, 'validation/num_examples': 50000, 'test/accuracy': 0.41540002822875977, 'test/loss': 2.877023935317993, 'test/num_examples': 10000, 'score': 4649.554701566696, 'total_duration': 5093.909548282623, 'accumulated_submission_time': 4649.554701566696, 'accumulated_eval_time': 442.3937928676605, 'accumulated_logging_time': 0.5811457633972168}
I0307 03:51:54.301312 140036192331520 logging_writer.py:48] [11973] accumulated_eval_time=442.394, accumulated_logging_time=0.581146, accumulated_submission_time=4649.55, global_step=11973, preemption_count=0, score=4649.55, test/accuracy=0.4154, test/loss=2.87702, test/num_examples=10000, total_duration=5093.91, train/accuracy=0.593411, train/loss=1.96327, validation/accuracy=0.53586, validation/loss=2.23668, validation/num_examples=50000
I0307 03:52:04.975269 140036200724224 logging_writer.py:48] [12000] global_step=12000, grad_norm=1.4694745540618896, loss=3.6312646865844727
I0307 03:52:43.326161 140036192331520 logging_writer.py:48] [12100] global_step=12100, grad_norm=1.670767068862915, loss=3.6893134117126465
I0307 03:53:21.924592 140036200724224 logging_writer.py:48] [12200] global_step=12200, grad_norm=1.6394885778427124, loss=3.71997332572937
I0307 03:54:00.416876 140036192331520 logging_writer.py:48] [12300] global_step=12300, grad_norm=1.469251036643982, loss=3.6489696502685547
I0307 03:54:38.637026 140036200724224 logging_writer.py:48] [12400] global_step=12400, grad_norm=1.450506329536438, loss=3.658404588699341
I0307 03:55:16.906928 140036192331520 logging_writer.py:48] [12500] global_step=12500, grad_norm=2.2116498947143555, loss=3.7053418159484863
I0307 03:55:55.148201 140036200724224 logging_writer.py:48] [12600] global_step=12600, grad_norm=1.676550269126892, loss=3.660844564437866
I0307 03:56:33.224689 140036192331520 logging_writer.py:48] [12700] global_step=12700, grad_norm=1.7198905944824219, loss=3.6372733116149902
I0307 03:57:11.795070 140036200724224 logging_writer.py:48] [12800] global_step=12800, grad_norm=1.4067338705062866, loss=3.6795129776000977
I0307 03:57:50.446146 140036192331520 logging_writer.py:48] [12900] global_step=12900, grad_norm=1.6234830617904663, loss=3.6594252586364746
I0307 03:58:28.771945 140036200724224 logging_writer.py:48] [13000] global_step=13000, grad_norm=2.0723893642425537, loss=3.689337968826294
I0307 03:59:07.162721 140036192331520 logging_writer.py:48] [13100] global_step=13100, grad_norm=1.8614778518676758, loss=3.617971658706665
I0307 03:59:45.739366 140036200724224 logging_writer.py:48] [13200] global_step=13200, grad_norm=1.5673953294754028, loss=3.557488203048706
I0307 04:00:24.386518 140036192331520 logging_writer.py:48] [13300] global_step=13300, grad_norm=1.4236916303634644, loss=3.642599582672119
I0307 04:00:24.399867 140191611933888 spec.py:321] Evaluating on the training split.
I0307 04:00:41.051970 140191611933888 spec.py:333] Evaluating on the validation split.
I0307 04:01:03.223231 140191611933888 spec.py:349] Evaluating on the test split.
I0307 04:01:05.009186 140191611933888 submission_runner.py:469] Time since start: 5644.66s, 	Step: 13301, 	{'train/accuracy': 0.6180843114852905, 'train/loss': 1.8578495979309082, 'validation/accuracy': 0.5574399828910828, 'validation/loss': 2.124974012374878, 'validation/num_examples': 50000, 'test/accuracy': 0.4326000213623047, 'test/loss': 2.7924184799194336, 'test/num_examples': 10000, 'score': 5159.494928598404, 'total_duration': 5644.662960529327, 'accumulated_submission_time': 5159.494928598404, 'accumulated_eval_time': 483.0030674934387, 'accumulated_logging_time': 0.6356997489929199}
I0307 04:01:05.066875 140036200724224 logging_writer.py:48] [13301] accumulated_eval_time=483.003, accumulated_logging_time=0.6357, accumulated_submission_time=5159.49, global_step=13301, preemption_count=0, score=5159.49, test/accuracy=0.4326, test/loss=2.79242, test/num_examples=10000, total_duration=5644.66, train/accuracy=0.618084, train/loss=1.85785, validation/accuracy=0.55744, validation/loss=2.12497, validation/num_examples=50000
I0307 04:01:45.385561 140036192331520 logging_writer.py:48] [13400] global_step=13400, grad_norm=1.522973656654358, loss=3.5974316596984863
I0307 04:02:23.849205 140036200724224 logging_writer.py:48] [13500] global_step=13500, grad_norm=1.789359450340271, loss=3.527378797531128
I0307 04:03:02.233345 140036192331520 logging_writer.py:48] [13600] global_step=13600, grad_norm=2.1070127487182617, loss=3.651927947998047
I0307 04:03:40.661332 140036200724224 logging_writer.py:48] [13700] global_step=13700, grad_norm=1.644269585609436, loss=3.659564733505249
I0307 04:04:19.073518 140036192331520 logging_writer.py:48] [13800] global_step=13800, grad_norm=1.8656797409057617, loss=3.534735918045044
I0307 04:04:57.693402 140036200724224 logging_writer.py:48] [13900] global_step=13900, grad_norm=1.72306489944458, loss=3.615593194961548
I0307 04:05:36.556755 140036192331520 logging_writer.py:48] [14000] global_step=14000, grad_norm=1.5957869291305542, loss=3.5720725059509277
I0307 04:06:15.141328 140036200724224 logging_writer.py:48] [14100] global_step=14100, grad_norm=1.7639329433441162, loss=3.6147758960723877
I0307 04:06:53.871687 140036192331520 logging_writer.py:48] [14200] global_step=14200, grad_norm=1.381666660308838, loss=3.541506052017212
I0307 04:07:32.385641 140036200724224 logging_writer.py:48] [14300] global_step=14300, grad_norm=1.831949234008789, loss=3.553877353668213
I0307 04:08:11.030767 140036192331520 logging_writer.py:48] [14400] global_step=14400, grad_norm=1.629577398300171, loss=3.5870020389556885
I0307 04:08:49.734849 140036200724224 logging_writer.py:48] [14500] global_step=14500, grad_norm=1.3190827369689941, loss=3.5895276069641113
I0307 04:09:28.117760 140036192331520 logging_writer.py:48] [14600] global_step=14600, grad_norm=1.4630398750305176, loss=3.473054885864258
I0307 04:09:35.028793 140191611933888 spec.py:321] Evaluating on the training split.
I0307 04:09:55.404558 140191611933888 spec.py:333] Evaluating on the validation split.
I0307 04:10:17.910114 140191611933888 spec.py:349] Evaluating on the test split.
I0307 04:10:19.724913 140191611933888 submission_runner.py:469] Time since start: 6199.38s, 	Step: 14619, 	{'train/accuracy': 0.6365991830825806, 'train/loss': 1.7211112976074219, 'validation/accuracy': 0.5773800015449524, 'validation/loss': 1.9946941137313843, 'validation/num_examples': 50000, 'test/accuracy': 0.45330002903938293, 'test/loss': 2.6611523628234863, 'test/num_examples': 10000, 'score': 5669.304041862488, 'total_duration': 6199.378535747528, 'accumulated_submission_time': 5669.304041862488, 'accumulated_eval_time': 527.6989982128143, 'accumulated_logging_time': 0.7011928558349609}
I0307 04:10:19.806910 140036200724224 logging_writer.py:48] [14619] accumulated_eval_time=527.699, accumulated_logging_time=0.701193, accumulated_submission_time=5669.3, global_step=14619, preemption_count=0, score=5669.3, test/accuracy=0.4533, test/loss=2.66115, test/num_examples=10000, total_duration=6199.38, train/accuracy=0.636599, train/loss=1.72111, validation/accuracy=0.57738, validation/loss=1.99469, validation/num_examples=50000
I0307 04:10:51.299346 140036192331520 logging_writer.py:48] [14700] global_step=14700, grad_norm=1.3927749395370483, loss=3.5384089946746826
I0307 04:11:29.735754 140036200724224 logging_writer.py:48] [14800] global_step=14800, grad_norm=1.3446542024612427, loss=3.564129114151001
I0307 04:12:08.585826 140036192331520 logging_writer.py:48] [14900] global_step=14900, grad_norm=1.57904052734375, loss=3.5249054431915283
I0307 04:12:46.964761 140036200724224 logging_writer.py:48] [15000] global_step=15000, grad_norm=1.9806663990020752, loss=3.4000508785247803
I0307 04:13:25.559947 140036192331520 logging_writer.py:48] [15100] global_step=15100, grad_norm=1.5622854232788086, loss=3.5159237384796143
I0307 04:14:04.100043 140036200724224 logging_writer.py:48] [15200] global_step=15200, grad_norm=1.206261396408081, loss=3.452484607696533
I0307 04:14:42.669838 140036192331520 logging_writer.py:48] [15300] global_step=15300, grad_norm=1.4842170476913452, loss=3.578845977783203
I0307 04:15:21.189795 140036200724224 logging_writer.py:48] [15400] global_step=15400, grad_norm=1.6687729358673096, loss=3.5625150203704834
I0307 04:15:59.564914 140036192331520 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.9587823152542114, loss=3.474473714828491
I0307 04:16:37.646103 140036200724224 logging_writer.py:48] [15600] global_step=15600, grad_norm=1.7116172313690186, loss=3.5469987392425537
I0307 04:17:15.028883 140036192331520 logging_writer.py:48] [15700] global_step=15700, grad_norm=1.2811250686645508, loss=3.5525214672088623
I0307 04:17:53.429126 140036200724224 logging_writer.py:48] [15800] global_step=15800, grad_norm=1.362079381942749, loss=3.4519779682159424
I0307 04:18:32.035084 140036192331520 logging_writer.py:48] [15900] global_step=15900, grad_norm=1.5775315761566162, loss=3.5566506385803223
I0307 04:18:50.011590 140191611933888 spec.py:321] Evaluating on the training split.
I0307 04:19:07.167798 140191611933888 spec.py:333] Evaluating on the validation split.
I0307 04:19:29.535408 140191611933888 spec.py:349] Evaluating on the test split.
I0307 04:19:31.335492 140191611933888 submission_runner.py:469] Time since start: 6750.99s, 	Step: 15948, 	{'train/accuracy': 0.6457669138908386, 'train/loss': 1.6908912658691406, 'validation/accuracy': 0.583139955997467, 'validation/loss': 1.9813586473464966, 'validation/num_examples': 50000, 'test/accuracy': 0.4610000252723694, 'test/loss': 2.633626937866211, 'test/num_examples': 10000, 'score': 6179.355831623077, 'total_duration': 6750.9891312122345, 'accumulated_submission_time': 6179.355831623077, 'accumulated_eval_time': 569.0227386951447, 'accumulated_logging_time': 0.7907683849334717}
I0307 04:19:31.404288 140036200724224 logging_writer.py:48] [15948] accumulated_eval_time=569.023, accumulated_logging_time=0.790768, accumulated_submission_time=6179.36, global_step=15948, preemption_count=0, score=6179.36, test/accuracy=0.461, test/loss=2.63363, test/num_examples=10000, total_duration=6750.99, train/accuracy=0.645767, train/loss=1.69089, validation/accuracy=0.58314, validation/loss=1.98136, validation/num_examples=50000
I0307 04:19:51.994573 140036192331520 logging_writer.py:48] [16000] global_step=16000, grad_norm=1.3044284582138062, loss=3.428739547729492
I0307 04:20:30.511039 140036200724224 logging_writer.py:48] [16100] global_step=16100, grad_norm=1.4109264612197876, loss=3.5399272441864014
I0307 04:21:08.811270 140036192331520 logging_writer.py:48] [16200] global_step=16200, grad_norm=1.8417890071868896, loss=3.4243178367614746
I0307 04:21:47.387102 140036200724224 logging_writer.py:48] [16300] global_step=16300, grad_norm=1.246263861656189, loss=3.510789394378662
I0307 04:22:26.091983 140036192331520 logging_writer.py:48] [16400] global_step=16400, grad_norm=1.7060661315917969, loss=3.4566116333007812
I0307 04:23:04.426446 140036200724224 logging_writer.py:48] [16500] global_step=16500, grad_norm=1.3617829084396362, loss=3.450746536254883
I0307 04:23:43.148207 140036192331520 logging_writer.py:48] [16600] global_step=16600, grad_norm=1.4384347200393677, loss=3.432069778442383
I0307 04:24:21.643424 140036200724224 logging_writer.py:48] [16700] global_step=16700, grad_norm=1.3974932432174683, loss=3.558115243911743
I0307 04:25:00.125246 140036192331520 logging_writer.py:48] [16800] global_step=16800, grad_norm=1.4862970113754272, loss=3.5014700889587402
I0307 04:25:38.679666 140036200724224 logging_writer.py:48] [16900] global_step=16900, grad_norm=1.4272782802581787, loss=3.5319042205810547
I0307 04:26:17.480180 140036192331520 logging_writer.py:48] [17000] global_step=17000, grad_norm=1.8694496154785156, loss=3.497028350830078
I0307 04:26:56.339145 140036200724224 logging_writer.py:48] [17100] global_step=17100, grad_norm=1.3884773254394531, loss=3.482933521270752
I0307 04:27:34.739793 140036192331520 logging_writer.py:48] [17200] global_step=17200, grad_norm=1.3001354932785034, loss=3.4778847694396973
I0307 04:28:01.505985 140191611933888 spec.py:321] Evaluating on the training split.
I0307 04:28:16.225039 140191611933888 spec.py:333] Evaluating on the validation split.
I0307 04:28:39.061647 140191611933888 spec.py:349] Evaluating on the test split.
I0307 04:28:40.872052 140191611933888 submission_runner.py:469] Time since start: 7300.53s, 	Step: 17270, 	{'train/accuracy': 0.666434109210968, 'train/loss': 1.5680752992630005, 'validation/accuracy': 0.5974599719047546, 'validation/loss': 1.8717472553253174, 'validation/num_examples': 50000, 'test/accuracy': 0.4780000150203705, 'test/loss': 2.5294103622436523, 'test/num_examples': 10000, 'score': 6689.291130781174, 'total_duration': 7300.525658369064, 'accumulated_submission_time': 6689.291130781174, 'accumulated_eval_time': 608.3886113166809, 'accumulated_logging_time': 0.8840675354003906}
I0307 04:28:40.992958 140036200724224 logging_writer.py:48] [17270] accumulated_eval_time=608.389, accumulated_logging_time=0.884068, accumulated_submission_time=6689.29, global_step=17270, preemption_count=0, score=6689.29, test/accuracy=0.478, test/loss=2.52941, test/num_examples=10000, total_duration=7300.53, train/accuracy=0.666434, train/loss=1.56808, validation/accuracy=0.59746, validation/loss=1.87175, validation/num_examples=50000
I0307 04:28:52.897233 140036192331520 logging_writer.py:48] [17300] global_step=17300, grad_norm=1.1941033601760864, loss=3.4168500900268555
I0307 04:29:31.607912 140036200724224 logging_writer.py:48] [17400] global_step=17400, grad_norm=1.484962821006775, loss=3.453120708465576
I0307 04:30:10.041374 140036192331520 logging_writer.py:48] [17500] global_step=17500, grad_norm=1.2904698848724365, loss=3.4489026069641113
I0307 04:30:48.747785 140036200724224 logging_writer.py:48] [17600] global_step=17600, grad_norm=1.8744659423828125, loss=3.423198699951172
I0307 04:31:27.520785 140036192331520 logging_writer.py:48] [17700] global_step=17700, grad_norm=1.126861333847046, loss=3.4676647186279297
I0307 04:32:05.894764 140036200724224 logging_writer.py:48] [17800] global_step=17800, grad_norm=1.8511770963668823, loss=3.4755494594573975
I0307 04:32:44.800172 140036192331520 logging_writer.py:48] [17900] global_step=17900, grad_norm=1.4762121438980103, loss=3.517420768737793
I0307 04:33:23.060478 140036200724224 logging_writer.py:48] [18000] global_step=18000, grad_norm=1.3815003633499146, loss=3.3994128704071045
I0307 04:34:01.797721 140036192331520 logging_writer.py:48] [18100] global_step=18100, grad_norm=1.5785210132598877, loss=3.477219343185425
I0307 04:34:40.379778 140036200724224 logging_writer.py:48] [18200] global_step=18200, grad_norm=2.033268451690674, loss=3.43778657913208
I0307 04:35:18.681279 140036192331520 logging_writer.py:48] [18300] global_step=18300, grad_norm=1.6294723749160767, loss=3.428238868713379
I0307 04:35:57.302473 140036200724224 logging_writer.py:48] [18400] global_step=18400, grad_norm=1.4254804849624634, loss=3.494140863418579
I0307 04:36:35.698693 140036192331520 logging_writer.py:48] [18500] global_step=18500, grad_norm=1.195441484451294, loss=3.3979897499084473
I0307 04:37:11.120837 140191611933888 spec.py:321] Evaluating on the training split.
I0307 04:37:28.661502 140191611933888 spec.py:333] Evaluating on the validation split.
I0307 04:37:51.454771 140191611933888 spec.py:349] Evaluating on the test split.
I0307 04:37:53.225412 140191611933888 submission_runner.py:469] Time since start: 7852.88s, 	Step: 18593, 	{'train/accuracy': 0.6752431392669678, 'train/loss': 1.545620322227478, 'validation/accuracy': 0.612019956111908, 'validation/loss': 1.8358662128448486, 'validation/num_examples': 50000, 'test/accuracy': 0.482200026512146, 'test/loss': 2.5183663368225098, 'test/num_examples': 10000, 'score': 7199.263134479523, 'total_duration': 7852.879142284393, 'accumulated_submission_time': 7199.263134479523, 'accumulated_eval_time': 650.4931201934814, 'accumulated_logging_time': 1.0294301509857178}
I0307 04:37:53.246193 140036200724224 logging_writer.py:48] [18593] accumulated_eval_time=650.493, accumulated_logging_time=1.02943, accumulated_submission_time=7199.26, global_step=18593, preemption_count=0, score=7199.26, test/accuracy=0.4822, test/loss=2.51837, test/num_examples=10000, total_duration=7852.88, train/accuracy=0.675243, train/loss=1.54562, validation/accuracy=0.61202, validation/loss=1.83587, validation/num_examples=50000
I0307 04:37:56.309980 140036192331520 logging_writer.py:48] [18600] global_step=18600, grad_norm=2.0260720252990723, loss=3.4502506256103516
I0307 04:38:34.745281 140036200724224 logging_writer.py:48] [18700] global_step=18700, grad_norm=1.3735325336456299, loss=3.4612767696380615
I0307 04:39:13.340192 140036192331520 logging_writer.py:48] [18800] global_step=18800, grad_norm=1.7096322774887085, loss=3.4390740394592285
I0307 04:39:51.921741 140036200724224 logging_writer.py:48] [18900] global_step=18900, grad_norm=1.387485384941101, loss=3.4580469131469727
I0307 04:40:30.508407 140036192331520 logging_writer.py:48] [19000] global_step=19000, grad_norm=1.4109710454940796, loss=3.411494255065918
I0307 04:41:09.010601 140036200724224 logging_writer.py:48] [19100] global_step=19100, grad_norm=1.374036431312561, loss=3.392219305038452
I0307 04:41:47.482259 140036192331520 logging_writer.py:48] [19200] global_step=19200, grad_norm=1.586050033569336, loss=3.4153337478637695
I0307 04:42:26.221735 140036200724224 logging_writer.py:48] [19300] global_step=19300, grad_norm=1.4396804571151733, loss=3.464311361312866
I0307 04:43:04.964411 140036192331520 logging_writer.py:48] [19400] global_step=19400, grad_norm=1.250253438949585, loss=3.423095941543579
I0307 04:43:43.084904 140036200724224 logging_writer.py:48] [19500] global_step=19500, grad_norm=1.7723495960235596, loss=3.457179307937622
I0307 04:44:21.404968 140036192331520 logging_writer.py:48] [19600] global_step=19600, grad_norm=1.4695197343826294, loss=3.3811323642730713
I0307 04:44:59.658776 140036200724224 logging_writer.py:48] [19700] global_step=19700, grad_norm=1.2634730339050293, loss=3.4108901023864746
I0307 04:45:38.140743 140036192331520 logging_writer.py:48] [19800] global_step=19800, grad_norm=1.30242121219635, loss=3.4180421829223633
I0307 04:46:16.868030 140036200724224 logging_writer.py:48] [19900] global_step=19900, grad_norm=1.253908395767212, loss=3.347277879714966
I0307 04:46:23.364074 140191611933888 spec.py:321] Evaluating on the training split.
I0307 04:46:38.394040 140191611933888 spec.py:333] Evaluating on the validation split.
I0307 04:47:02.182829 140191611933888 spec.py:349] Evaluating on the test split.
I0307 04:47:03.980720 140191611933888 submission_runner.py:469] Time since start: 8403.63s, 	Step: 19918, 	{'train/accuracy': 0.6723931431770325, 'train/loss': 1.562750220298767, 'validation/accuracy': 0.6068999767303467, 'validation/loss': 1.8614797592163086, 'validation/num_examples': 50000, 'test/accuracy': 0.48350003361701965, 'test/loss': 2.5008797645568848, 'test/num_examples': 10000, 'score': 7709.2389793396, 'total_duration': 8403.634334564209, 'accumulated_submission_time': 7709.2389793396, 'accumulated_eval_time': 691.109569311142, 'accumulated_logging_time': 1.0585978031158447}
I0307 04:47:04.136373 140036192331520 logging_writer.py:48] [19918] accumulated_eval_time=691.11, accumulated_logging_time=1.0586, accumulated_submission_time=7709.24, global_step=19918, preemption_count=0, score=7709.24, test/accuracy=0.4835, test/loss=2.50088, test/num_examples=10000, total_duration=8403.63, train/accuracy=0.672393, train/loss=1.56275, validation/accuracy=0.6069, validation/loss=1.86148, validation/num_examples=50000
I0307 04:47:36.237517 140036200724224 logging_writer.py:48] [20000] global_step=20000, grad_norm=1.3200730085372925, loss=3.428525924682617
I0307 04:48:14.984828 140036192331520 logging_writer.py:48] [20100] global_step=20100, grad_norm=1.6749789714813232, loss=3.4514660835266113
I0307 04:48:53.382371 140036200724224 logging_writer.py:48] [20200] global_step=20200, grad_norm=1.5092980861663818, loss=3.352949857711792
I0307 04:49:32.140382 140036192331520 logging_writer.py:48] [20300] global_step=20300, grad_norm=1.401427984237671, loss=3.4702601432800293
I0307 04:50:11.091223 140036200724224 logging_writer.py:48] [20400] global_step=20400, grad_norm=1.1719355583190918, loss=3.399341344833374
I0307 04:50:50.007132 140036192331520 logging_writer.py:48] [20500] global_step=20500, grad_norm=1.4701865911483765, loss=3.382437229156494
I0307 04:51:29.264116 140036200724224 logging_writer.py:48] [20600] global_step=20600, grad_norm=1.548081636428833, loss=3.4248530864715576
I0307 04:52:07.844726 140036192331520 logging_writer.py:48] [20700] global_step=20700, grad_norm=1.4624123573303223, loss=3.3774943351745605
I0307 04:52:46.078315 140036200724224 logging_writer.py:48] [20800] global_step=20800, grad_norm=1.1275110244750977, loss=3.396177291870117
I0307 04:53:24.780845 140036192331520 logging_writer.py:48] [20900] global_step=20900, grad_norm=1.5247825384140015, loss=3.390007495880127
I0307 04:54:03.116887 140036200724224 logging_writer.py:48] [21000] global_step=21000, grad_norm=1.037402629852295, loss=3.3646202087402344
I0307 04:54:41.745494 140036192331520 logging_writer.py:48] [21100] global_step=21100, grad_norm=1.2579853534698486, loss=3.4200308322906494
I0307 04:55:20.491607 140036200724224 logging_writer.py:48] [21200] global_step=21200, grad_norm=1.5601801872253418, loss=3.3776984214782715
I0307 04:55:34.263547 140191611933888 spec.py:321] Evaluating on the training split.
I0307 04:55:47.254694 140191611933888 spec.py:333] Evaluating on the validation split.
I0307 04:56:12.142067 140191611933888 spec.py:349] Evaluating on the test split.
I0307 04:56:13.930557 140191611933888 submission_runner.py:469] Time since start: 8953.58s, 	Step: 21237, 	{'train/accuracy': 0.6915059089660645, 'train/loss': 1.4633668661117554, 'validation/accuracy': 0.6233599781990051, 'validation/loss': 1.7683180570602417, 'validation/num_examples': 50000, 'test/accuracy': 0.49150002002716064, 'test/loss': 2.439638376235962, 'test/num_examples': 10000, 'score': 8219.197752714157, 'total_duration': 8953.584324121475, 'accumulated_submission_time': 8219.197752714157, 'accumulated_eval_time': 730.7765364646912, 'accumulated_logging_time': 1.2414052486419678}
I0307 04:56:14.001808 140036192331520 logging_writer.py:48] [21237] accumulated_eval_time=730.777, accumulated_logging_time=1.24141, accumulated_submission_time=8219.2, global_step=21237, preemption_count=0, score=8219.2, test/accuracy=0.4915, test/loss=2.43964, test/num_examples=10000, total_duration=8953.58, train/accuracy=0.691506, train/loss=1.46337, validation/accuracy=0.62336, validation/loss=1.76832, validation/num_examples=50000
I0307 04:56:38.836271 140036200724224 logging_writer.py:48] [21300] global_step=21300, grad_norm=1.8202155828475952, loss=3.3659470081329346
I0307 04:57:17.080710 140036192331520 logging_writer.py:48] [21400] global_step=21400, grad_norm=1.6901342868804932, loss=3.4534595012664795
I0307 04:57:56.003793 140036200724224 logging_writer.py:48] [21500] global_step=21500, grad_norm=1.1524490118026733, loss=3.3277831077575684
I0307 04:58:35.330892 140036192331520 logging_writer.py:48] [21600] global_step=21600, grad_norm=1.6124101877212524, loss=3.377481698989868
I0307 04:59:13.951226 140036200724224 logging_writer.py:48] [21700] global_step=21700, grad_norm=1.6416786909103394, loss=3.3655900955200195
I0307 04:59:52.890358 140036192331520 logging_writer.py:48] [21800] global_step=21800, grad_norm=1.352308750152588, loss=3.451887607574463
I0307 05:00:32.288010 140036200724224 logging_writer.py:48] [21900] global_step=21900, grad_norm=1.5733404159545898, loss=3.3739449977874756
I0307 05:01:11.164921 140036192331520 logging_writer.py:48] [22000] global_step=22000, grad_norm=1.5677202939987183, loss=3.407329797744751
I0307 05:01:49.500940 140036200724224 logging_writer.py:48] [22100] global_step=22100, grad_norm=1.120554804801941, loss=3.430172920227051
I0307 05:02:27.778558 140036192331520 logging_writer.py:48] [22200] global_step=22200, grad_norm=1.51198410987854, loss=3.2611098289489746
I0307 05:03:06.205637 140036200724224 logging_writer.py:48] [22300] global_step=22300, grad_norm=1.7761387825012207, loss=3.456840991973877
I0307 05:03:44.823812 140036192331520 logging_writer.py:48] [22400] global_step=22400, grad_norm=1.6149556636810303, loss=3.3178133964538574
I0307 05:04:23.221402 140036200724224 logging_writer.py:48] [22500] global_step=22500, grad_norm=1.3341721296310425, loss=3.3071115016937256
I0307 05:04:44.217104 140191611933888 spec.py:321] Evaluating on the training split.
I0307 05:04:59.482557 140191611933888 spec.py:333] Evaluating on the validation split.
I0307 05:05:23.259878 140191611933888 spec.py:349] Evaluating on the test split.
I0307 05:05:25.032336 140191611933888 submission_runner.py:469] Time since start: 9504.69s, 	Step: 22555, 	{'train/accuracy': 0.6957908272743225, 'train/loss': 1.4110486507415771, 'validation/accuracy': 0.6284199953079224, 'validation/loss': 1.7138735055923462, 'validation/num_examples': 50000, 'test/accuracy': 0.49490001797676086, 'test/loss': 2.3887875080108643, 'test/num_examples': 10000, 'score': 8729.263293981552, 'total_duration': 9504.685961723328, 'accumulated_submission_time': 8729.263293981552, 'accumulated_eval_time': 771.5915892124176, 'accumulated_logging_time': 1.3209893703460693}
I0307 05:05:25.131520 140036192331520 logging_writer.py:48] [22555] accumulated_eval_time=771.592, accumulated_logging_time=1.32099, accumulated_submission_time=8729.26, global_step=22555, preemption_count=0, score=8729.26, test/accuracy=0.4949, test/loss=2.38879, test/num_examples=10000, total_duration=9504.69, train/accuracy=0.695791, train/loss=1.41105, validation/accuracy=0.62842, validation/loss=1.71387, validation/num_examples=50000
I0307 05:05:42.613127 140036200724224 logging_writer.py:48] [22600] global_step=22600, grad_norm=1.2613638639450073, loss=3.3769893646240234
I0307 05:06:20.976696 140036192331520 logging_writer.py:48] [22700] global_step=22700, grad_norm=1.1573430299758911, loss=3.227654457092285
I0307 05:07:00.064180 140036200724224 logging_writer.py:48] [22800] global_step=22800, grad_norm=1.0833908319473267, loss=3.3600049018859863
I0307 05:07:38.452292 140036192331520 logging_writer.py:48] [22900] global_step=22900, grad_norm=1.1719541549682617, loss=3.2716996669769287
I0307 05:08:17.530298 140036200724224 logging_writer.py:48] [23000] global_step=23000, grad_norm=1.3859758377075195, loss=3.3605189323425293
I0307 05:08:55.856371 140036192331520 logging_writer.py:48] [23100] global_step=23100, grad_norm=1.5341061353683472, loss=3.340337038040161
I0307 05:09:37.198214 140036200724224 logging_writer.py:48] [23200] global_step=23200, grad_norm=1.3348357677459717, loss=3.2949092388153076
I0307 05:10:16.707993 140036192331520 logging_writer.py:48] [23300] global_step=23300, grad_norm=1.2033910751342773, loss=3.297572612762451
I0307 05:10:55.285676 140036200724224 logging_writer.py:48] [23400] global_step=23400, grad_norm=1.1989858150482178, loss=3.3320207595825195
I0307 05:11:33.926941 140036192331520 logging_writer.py:48] [23500] global_step=23500, grad_norm=1.5995897054672241, loss=3.3966774940490723
I0307 05:12:12.074235 140036200724224 logging_writer.py:48] [23600] global_step=23600, grad_norm=1.2595466375350952, loss=3.274839162826538
I0307 05:12:50.368892 140036192331520 logging_writer.py:48] [23700] global_step=23700, grad_norm=1.3326281309127808, loss=3.4846935272216797
I0307 05:13:28.991200 140036200724224 logging_writer.py:48] [23800] global_step=23800, grad_norm=1.406126856803894, loss=3.3297483921051025
I0307 05:13:55.331249 140191611933888 spec.py:321] Evaluating on the training split.
I0307 05:14:12.572720 140191611933888 spec.py:333] Evaluating on the validation split.
I0307 05:14:37.040951 140191611933888 spec.py:349] Evaluating on the test split.
I0307 05:14:38.804685 140191611933888 submission_runner.py:469] Time since start: 10058.46s, 	Step: 23869, 	{'train/accuracy': 0.698640763759613, 'train/loss': 1.3857132196426392, 'validation/accuracy': 0.6312599778175354, 'validation/loss': 1.689432978630066, 'validation/num_examples': 50000, 'test/accuracy': 0.5024999976158142, 'test/loss': 2.361210346221924, 'test/num_examples': 10000, 'score': 9239.309756040573, 'total_duration': 10058.458269357681, 'accumulated_submission_time': 9239.309756040573, 'accumulated_eval_time': 815.0648167133331, 'accumulated_logging_time': 1.4297010898590088}
I0307 05:14:38.904195 140036192331520 logging_writer.py:48] [23869] accumulated_eval_time=815.065, accumulated_logging_time=1.4297, accumulated_submission_time=9239.31, global_step=23869, preemption_count=0, score=9239.31, test/accuracy=0.5025, test/loss=2.36121, test/num_examples=10000, total_duration=10058.5, train/accuracy=0.698641, train/loss=1.38571, validation/accuracy=0.63126, validation/loss=1.68943, validation/num_examples=50000
I0307 05:14:51.316561 140036200724224 logging_writer.py:48] [23900] global_step=23900, grad_norm=1.4682544469833374, loss=3.3550870418548584
I0307 05:15:29.901172 140036192331520 logging_writer.py:48] [24000] global_step=24000, grad_norm=1.426011562347412, loss=3.3233189582824707
I0307 05:16:08.463000 140036200724224 logging_writer.py:48] [24100] global_step=24100, grad_norm=1.590532660484314, loss=3.2692058086395264
I0307 05:16:47.174236 140036192331520 logging_writer.py:48] [24200] global_step=24200, grad_norm=1.6156820058822632, loss=3.313363552093506
I0307 05:17:26.016958 140036200724224 logging_writer.py:48] [24300] global_step=24300, grad_norm=1.520996332168579, loss=3.3085246086120605
I0307 05:18:04.707250 140036192331520 logging_writer.py:48] [24400] global_step=24400, grad_norm=1.230391025543213, loss=3.336820602416992
I0307 05:18:44.179855 140036200724224 logging_writer.py:48] [24500] global_step=24500, grad_norm=1.4312373399734497, loss=3.325852394104004
I0307 05:19:22.840780 140036192331520 logging_writer.py:48] [24600] global_step=24600, grad_norm=1.3126869201660156, loss=3.276707649230957
I0307 05:20:01.632064 140036200724224 logging_writer.py:48] [24700] global_step=24700, grad_norm=1.5139973163604736, loss=3.339402198791504
I0307 05:20:40.434582 140036192331520 logging_writer.py:48] [24800] global_step=24800, grad_norm=1.2792260646820068, loss=3.355989933013916
I0307 05:21:18.602704 140036200724224 logging_writer.py:48] [24900] global_step=24900, grad_norm=1.4393740892410278, loss=3.314746379852295
I0307 05:21:57.211306 140036192331520 logging_writer.py:48] [25000] global_step=25000, grad_norm=1.3925034999847412, loss=3.315506935119629
I0307 05:22:36.095737 140036200724224 logging_writer.py:48] [25100] global_step=25100, grad_norm=1.249179720878601, loss=3.2953743934631348
I0307 05:23:08.919595 140191611933888 spec.py:321] Evaluating on the training split.
I0307 05:23:23.485161 140191611933888 spec.py:333] Evaluating on the validation split.
I0307 05:23:44.844652 140191611933888 spec.py:349] Evaluating on the test split.
I0307 05:23:46.625104 140191611933888 submission_runner.py:469] Time since start: 10606.28s, 	Step: 25186, 	{'train/accuracy': 0.7036431431770325, 'train/loss': 1.4194320440292358, 'validation/accuracy': 0.6347999572753906, 'validation/loss': 1.7313907146453857, 'validation/num_examples': 50000, 'test/accuracy': 0.5056000351905823, 'test/loss': 2.386014223098755, 'test/num_examples': 10000, 'score': 9749.113762140274, 'total_duration': 10606.27887749672, 'accumulated_submission_time': 9749.113762140274, 'accumulated_eval_time': 852.7702946662903, 'accumulated_logging_time': 1.6018095016479492}
I0307 05:23:46.664855 140036192331520 logging_writer.py:48] [25186] accumulated_eval_time=852.77, accumulated_logging_time=1.60181, accumulated_submission_time=9749.11, global_step=25186, preemption_count=0, score=9749.11, test/accuracy=0.5056, test/loss=2.38601, test/num_examples=10000, total_duration=10606.3, train/accuracy=0.703643, train/loss=1.41943, validation/accuracy=0.6348, validation/loss=1.73139, validation/num_examples=50000
I0307 05:23:52.475859 140036200724224 logging_writer.py:48] [25200] global_step=25200, grad_norm=1.5593427419662476, loss=3.384403944015503
I0307 05:24:31.023851 140036192331520 logging_writer.py:48] [25300] global_step=25300, grad_norm=1.6194490194320679, loss=3.319037437438965
I0307 05:25:10.129744 140036200724224 logging_writer.py:48] [25400] global_step=25400, grad_norm=1.3109577894210815, loss=3.298447847366333
I0307 05:25:48.861011 140036192331520 logging_writer.py:48] [25500] global_step=25500, grad_norm=1.5043977499008179, loss=3.297226667404175
I0307 05:26:27.650831 140036200724224 logging_writer.py:48] [25600] global_step=25600, grad_norm=1.3370232582092285, loss=3.251824378967285
I0307 05:27:06.411982 140036192331520 logging_writer.py:48] [25700] global_step=25700, grad_norm=1.546189546585083, loss=3.3699474334716797
I0307 05:27:44.727089 140036200724224 logging_writer.py:48] [25800] global_step=25800, grad_norm=1.433082103729248, loss=3.227509021759033
I0307 05:28:23.837872 140036192331520 logging_writer.py:48] [25900] global_step=25900, grad_norm=1.4756247997283936, loss=3.3333098888397217
I0307 05:29:02.608032 140036200724224 logging_writer.py:48] [26000] global_step=26000, grad_norm=1.6333613395690918, loss=3.3218636512756348
I0307 05:29:41.268227 140036192331520 logging_writer.py:48] [26100] global_step=26100, grad_norm=2.1150381565093994, loss=3.316539764404297
I0307 05:30:19.916275 140036200724224 logging_writer.py:48] [26200] global_step=26200, grad_norm=1.2570596933364868, loss=3.284325361251831
I0307 05:30:58.384039 140036192331520 logging_writer.py:48] [26300] global_step=26300, grad_norm=1.3335001468658447, loss=3.374314546585083
I0307 05:31:36.757464 140036200724224 logging_writer.py:48] [26400] global_step=26400, grad_norm=1.2362017631530762, loss=3.3348116874694824
I0307 05:32:15.102807 140036192331520 logging_writer.py:48] [26500] global_step=26500, grad_norm=1.3273844718933105, loss=3.3675312995910645
I0307 05:32:16.668770 140191611933888 spec.py:321] Evaluating on the training split.
I0307 05:32:33.471402 140191611933888 spec.py:333] Evaluating on the validation split.
I0307 05:32:57.510169 140191611933888 spec.py:349] Evaluating on the test split.
I0307 05:32:59.325062 140191611933888 submission_runner.py:469] Time since start: 11158.98s, 	Step: 26504, 	{'train/accuracy': 0.7095224857330322, 'train/loss': 1.4008926153182983, 'validation/accuracy': 0.6384599804878235, 'validation/loss': 1.7122982740402222, 'validation/num_examples': 50000, 'test/accuracy': 0.5063000321388245, 'test/loss': 2.375081777572632, 'test/num_examples': 10000, 'score': 10258.938409090042, 'total_duration': 11158.97870516777, 'accumulated_submission_time': 10258.938409090042, 'accumulated_eval_time': 895.4264299869537, 'accumulated_logging_time': 1.6848649978637695}
I0307 05:32:59.404195 140036200724224 logging_writer.py:48] [26504] accumulated_eval_time=895.426, accumulated_logging_time=1.68486, accumulated_submission_time=10258.9, global_step=26504, preemption_count=0, score=10258.9, test/accuracy=0.5063, test/loss=2.37508, test/num_examples=10000, total_duration=11159, train/accuracy=0.709522, train/loss=1.40089, validation/accuracy=0.63846, validation/loss=1.7123, validation/num_examples=50000
I0307 05:33:37.167426 140036192331520 logging_writer.py:48] [26600] global_step=26600, grad_norm=1.428009033203125, loss=3.265859365463257
I0307 05:34:14.617005 140036200724224 logging_writer.py:48] [26700] global_step=26700, grad_norm=1.2215690612792969, loss=3.285726547241211
I0307 05:34:53.153349 140036192331520 logging_writer.py:48] [26800] global_step=26800, grad_norm=1.3867416381835938, loss=3.248175621032715
I0307 05:35:31.695359 140036200724224 logging_writer.py:48] [26900] global_step=26900, grad_norm=1.1403495073318481, loss=3.2120590209960938
I0307 05:36:10.067464 140036192331520 logging_writer.py:48] [27000] global_step=27000, grad_norm=1.2740427255630493, loss=3.29480242729187
I0307 05:36:48.847900 140036200724224 logging_writer.py:48] [27100] global_step=27100, grad_norm=1.8696829080581665, loss=3.254866123199463
I0307 05:37:27.295711 140036192331520 logging_writer.py:48] [27200] global_step=27200, grad_norm=1.1899832487106323, loss=3.3156065940856934
I0307 05:38:06.044541 140036200724224 logging_writer.py:48] [27300] global_step=27300, grad_norm=1.3071731328964233, loss=3.221863031387329
I0307 05:38:44.681594 140036192331520 logging_writer.py:48] [27400] global_step=27400, grad_norm=1.4490578174591064, loss=3.3035929203033447
I0307 05:39:22.962237 140036200724224 logging_writer.py:48] [27500] global_step=27500, grad_norm=1.197638988494873, loss=3.2907521724700928
I0307 05:40:01.289163 140036192331520 logging_writer.py:48] [27600] global_step=27600, grad_norm=1.3551908731460571, loss=3.4211132526397705
I0307 05:40:39.767660 140036200724224 logging_writer.py:48] [27700] global_step=27700, grad_norm=1.4215524196624756, loss=3.290428876876831
I0307 05:41:18.611412 140036192331520 logging_writer.py:48] [27800] global_step=27800, grad_norm=1.372640609741211, loss=3.284139633178711
I0307 05:41:29.376315 140191611933888 spec.py:321] Evaluating on the training split.
I0307 05:41:44.136938 140191611933888 spec.py:333] Evaluating on the validation split.
I0307 05:42:07.852566 140191611933888 spec.py:349] Evaluating on the test split.
I0307 05:42:09.664431 140191611933888 submission_runner.py:469] Time since start: 11709.32s, 	Step: 27829, 	{'train/accuracy': 0.7002949714660645, 'train/loss': 1.4403825998306274, 'validation/accuracy': 0.6310799717903137, 'validation/loss': 1.7473212480545044, 'validation/num_examples': 50000, 'test/accuracy': 0.5012000203132629, 'test/loss': 2.433413505554199, 'test/num_examples': 10000, 'score': 10768.737848758698, 'total_duration': 11709.318193435669, 'accumulated_submission_time': 10768.737848758698, 'accumulated_eval_time': 935.7145071029663, 'accumulated_logging_time': 1.7973113059997559}
I0307 05:42:09.729366 140036200724224 logging_writer.py:48] [27829] accumulated_eval_time=935.715, accumulated_logging_time=1.79731, accumulated_submission_time=10768.7, global_step=27829, preemption_count=0, score=10768.7, test/accuracy=0.5012, test/loss=2.43341, test/num_examples=10000, total_duration=11709.3, train/accuracy=0.700295, train/loss=1.44038, validation/accuracy=0.63108, validation/loss=1.74732, validation/num_examples=50000
I0307 05:42:37.352772 140036192331520 logging_writer.py:48] [27900] global_step=27900, grad_norm=1.4047236442565918, loss=3.2906482219696045
I0307 05:43:16.179876 140036200724224 logging_writer.py:48] [28000] global_step=28000, grad_norm=1.1950581073760986, loss=3.2464520931243896
I0307 05:43:55.377484 140036192331520 logging_writer.py:48] [28100] global_step=28100, grad_norm=1.3665121793746948, loss=3.2238850593566895
I0307 05:44:33.760365 140036200724224 logging_writer.py:48] [28200] global_step=28200, grad_norm=1.5565389394760132, loss=3.3060946464538574
I0307 05:45:12.419020 140036192331520 logging_writer.py:48] [28300] global_step=28300, grad_norm=1.352344036102295, loss=3.304015636444092
I0307 05:45:50.844310 140036200724224 logging_writer.py:48] [28400] global_step=28400, grad_norm=1.4088033437728882, loss=3.252368927001953
I0307 05:46:29.417283 140036192331520 logging_writer.py:48] [28500] global_step=28500, grad_norm=1.2049565315246582, loss=3.2717504501342773
I0307 05:47:08.142743 140036200724224 logging_writer.py:48] [28600] global_step=28600, grad_norm=1.2578541040420532, loss=3.269862413406372
I0307 05:47:46.745869 140036192331520 logging_writer.py:48] [28700] global_step=28700, grad_norm=1.2704344987869263, loss=3.3493003845214844
I0307 05:48:25.493362 140036200724224 logging_writer.py:48] [28800] global_step=28800, grad_norm=1.347767949104309, loss=3.2984066009521484
I0307 05:49:04.076486 140036192331520 logging_writer.py:48] [28900] global_step=28900, grad_norm=1.3592034578323364, loss=3.3114500045776367
I0307 05:49:42.798235 140036200724224 logging_writer.py:48] [29000] global_step=29000, grad_norm=1.5479601621627808, loss=3.2614946365356445
I0307 05:50:21.184481 140036192331520 logging_writer.py:48] [29100] global_step=29100, grad_norm=1.5627734661102295, loss=3.2265396118164062
I0307 05:50:39.724136 140191611933888 spec.py:321] Evaluating on the training split.
I0307 05:50:54.305288 140191611933888 spec.py:333] Evaluating on the validation split.
I0307 05:51:17.659769 140191611933888 spec.py:349] Evaluating on the test split.
I0307 05:51:19.471225 140191611933888 submission_runner.py:469] Time since start: 12259.13s, 	Step: 29149, 	{'train/accuracy': 0.7065330147743225, 'train/loss': 1.4145169258117676, 'validation/accuracy': 0.6378799676895142, 'validation/loss': 1.7297440767288208, 'validation/num_examples': 50000, 'test/accuracy': 0.513200044631958, 'test/loss': 2.3744258880615234, 'test/num_examples': 10000, 'score': 11278.56044459343, 'total_duration': 12259.125003576279, 'accumulated_submission_time': 11278.56044459343, 'accumulated_eval_time': 975.4615819454193, 'accumulated_logging_time': 1.8986070156097412}
I0307 05:51:19.550688 140036200724224 logging_writer.py:48] [29149] accumulated_eval_time=975.462, accumulated_logging_time=1.89861, accumulated_submission_time=11278.6, global_step=29149, preemption_count=0, score=11278.6, test/accuracy=0.5132, test/loss=2.37443, test/num_examples=10000, total_duration=12259.1, train/accuracy=0.706533, train/loss=1.41452, validation/accuracy=0.63788, validation/loss=1.72974, validation/num_examples=50000
I0307 05:51:39.384615 140036192331520 logging_writer.py:48] [29200] global_step=29200, grad_norm=1.6248283386230469, loss=3.287179946899414
I0307 05:52:18.045819 140036200724224 logging_writer.py:48] [29300] global_step=29300, grad_norm=1.3676716089248657, loss=3.243373155593872
I0307 05:52:56.506673 140036192331520 logging_writer.py:48] [29400] global_step=29400, grad_norm=1.5229182243347168, loss=3.3921666145324707
I0307 05:53:34.909719 140036200724224 logging_writer.py:48] [29500] global_step=29500, grad_norm=1.1528183221817017, loss=3.1784372329711914
I0307 05:54:13.399658 140036192331520 logging_writer.py:48] [29600] global_step=29600, grad_norm=1.4147617816925049, loss=3.280066967010498
I0307 05:54:51.920377 140036200724224 logging_writer.py:48] [29700] global_step=29700, grad_norm=1.4503451585769653, loss=3.2269692420959473
I0307 05:55:30.571267 140036192331520 logging_writer.py:48] [29800] global_step=29800, grad_norm=1.7261343002319336, loss=3.3328614234924316
I0307 05:56:08.863928 140036200724224 logging_writer.py:48] [29900] global_step=29900, grad_norm=1.3483047485351562, loss=3.295266628265381
I0307 05:56:47.768855 140036192331520 logging_writer.py:48] [30000] global_step=30000, grad_norm=1.690677523612976, loss=3.2815282344818115
I0307 05:57:26.780306 140036200724224 logging_writer.py:48] [30100] global_step=30100, grad_norm=1.5607503652572632, loss=3.2975759506225586
I0307 05:58:05.240954 140036192331520 logging_writer.py:48] [30200] global_step=30200, grad_norm=1.4437460899353027, loss=3.253857135772705
I0307 05:58:43.695486 140036200724224 logging_writer.py:48] [30300] global_step=30300, grad_norm=1.2226176261901855, loss=3.2803139686584473
I0307 05:59:22.350091 140036192331520 logging_writer.py:48] [30400] global_step=30400, grad_norm=1.4889352321624756, loss=3.2550861835479736
I0307 05:59:49.640106 140191611933888 spec.py:321] Evaluating on the training split.
I0307 06:00:07.385027 140191611933888 spec.py:333] Evaluating on the validation split.
I0307 06:00:30.799168 140191611933888 spec.py:349] Evaluating on the test split.
I0307 06:00:32.613527 140191611933888 submission_runner.py:469] Time since start: 12812.27s, 	Step: 30472, 	{'train/accuracy': 0.7128108739852905, 'train/loss': 1.3705154657363892, 'validation/accuracy': 0.6441999673843384, 'validation/loss': 1.6669092178344727, 'validation/num_examples': 50000, 'test/accuracy': 0.5151000022888184, 'test/loss': 2.3454983234405518, 'test/num_examples': 10000, 'score': 11788.487871646881, 'total_duration': 12812.267133235931, 'accumulated_submission_time': 11788.487871646881, 'accumulated_eval_time': 1018.4347972869873, 'accumulated_logging_time': 2.006068229675293}
I0307 06:00:32.757164 140036200724224 logging_writer.py:48] [30472] accumulated_eval_time=1018.43, accumulated_logging_time=2.00607, accumulated_submission_time=11788.5, global_step=30472, preemption_count=0, score=11788.5, test/accuracy=0.5151, test/loss=2.3455, test/num_examples=10000, total_duration=12812.3, train/accuracy=0.712811, train/loss=1.37052, validation/accuracy=0.6442, validation/loss=1.66691, validation/num_examples=50000
I0307 06:00:44.156777 140036192331520 logging_writer.py:48] [30500] global_step=30500, grad_norm=1.3530712127685547, loss=3.262519121170044
I0307 06:01:22.610603 140036200724224 logging_writer.py:48] [30600] global_step=30600, grad_norm=1.5853856801986694, loss=3.2842280864715576
I0307 06:02:01.363958 140036192331520 logging_writer.py:48] [30700] global_step=30700, grad_norm=1.2653831243515015, loss=3.1976232528686523
I0307 06:02:39.975530 140036200724224 logging_writer.py:48] [30800] global_step=30800, grad_norm=1.5274467468261719, loss=3.281505584716797
I0307 06:03:18.500910 140036192331520 logging_writer.py:48] [30900] global_step=30900, grad_norm=1.2775195837020874, loss=3.2165398597717285
I0307 06:03:57.244173 140036200724224 logging_writer.py:48] [31000] global_step=31000, grad_norm=1.5269031524658203, loss=3.218602180480957
I0307 06:04:35.716663 140036192331520 logging_writer.py:48] [31100] global_step=31100, grad_norm=1.6163350343704224, loss=3.2643818855285645
I0307 06:05:14.332635 140036200724224 logging_writer.py:48] [31200] global_step=31200, grad_norm=1.5576529502868652, loss=3.321460485458374
I0307 06:05:53.106913 140036192331520 logging_writer.py:48] [31300] global_step=31300, grad_norm=1.5185651779174805, loss=3.2115638256073
I0307 06:06:32.049585 140036200724224 logging_writer.py:48] [31400] global_step=31400, grad_norm=1.3495302200317383, loss=3.302342176437378
I0307 06:07:10.622563 140036192331520 logging_writer.py:48] [31500] global_step=31500, grad_norm=1.397619605064392, loss=3.225209951400757
I0307 06:07:49.283953 140036200724224 logging_writer.py:48] [31600] global_step=31600, grad_norm=1.516729712486267, loss=3.2340095043182373
I0307 06:08:27.906152 140036192331520 logging_writer.py:48] [31700] global_step=31700, grad_norm=1.3077524900436401, loss=3.3231406211853027
I0307 06:09:02.618623 140191611933888 spec.py:321] Evaluating on the training split.
I0307 06:09:18.360399 140191611933888 spec.py:333] Evaluating on the validation split.
I0307 06:09:39.023420 140191611933888 spec.py:349] Evaluating on the test split.
I0307 06:09:40.850511 140191611933888 submission_runner.py:469] Time since start: 13360.50s, 	Step: 31790, 	{'train/accuracy': 0.7202845811843872, 'train/loss': 1.324790358543396, 'validation/accuracy': 0.6470400094985962, 'validation/loss': 1.6521518230438232, 'validation/num_examples': 50000, 'test/accuracy': 0.5204000473022461, 'test/loss': 2.290083408355713, 'test/num_examples': 10000, 'score': 12298.158679246902, 'total_duration': 13360.504140853882, 'accumulated_submission_time': 12298.158679246902, 'accumulated_eval_time': 1056.6665122509003, 'accumulated_logging_time': 2.202118158340454}
I0307 06:09:40.949564 140036200724224 logging_writer.py:48] [31790] accumulated_eval_time=1056.67, accumulated_logging_time=2.20212, accumulated_submission_time=12298.2, global_step=31790, preemption_count=0, score=12298.2, test/accuracy=0.5204, test/loss=2.29008, test/num_examples=10000, total_duration=13360.5, train/accuracy=0.720285, train/loss=1.32479, validation/accuracy=0.64704, validation/loss=1.65215, validation/num_examples=50000
I0307 06:09:45.240899 140036192331520 logging_writer.py:48] [31800] global_step=31800, grad_norm=1.447715401649475, loss=3.2481069564819336
I0307 06:10:23.535132 140036200724224 logging_writer.py:48] [31900] global_step=31900, grad_norm=1.3214973211288452, loss=3.255127191543579
I0307 06:11:02.308734 140036192331520 logging_writer.py:48] [32000] global_step=32000, grad_norm=1.2978079319000244, loss=3.1882681846618652
I0307 06:11:41.250128 140036200724224 logging_writer.py:48] [32100] global_step=32100, grad_norm=1.585828185081482, loss=3.2670257091522217
I0307 06:12:19.770612 140036192331520 logging_writer.py:48] [32200] global_step=32200, grad_norm=1.79640793800354, loss=3.196126937866211
I0307 06:12:58.196295 140036200724224 logging_writer.py:48] [32300] global_step=32300, grad_norm=1.2839725017547607, loss=3.187983751296997
I0307 06:13:36.830393 140036192331520 logging_writer.py:48] [32400] global_step=32400, grad_norm=1.4357829093933105, loss=3.2528152465820312
I0307 06:14:14.941317 140036200724224 logging_writer.py:48] [32500] global_step=32500, grad_norm=1.2344510555267334, loss=3.2330446243286133
I0307 06:14:54.201462 140036192331520 logging_writer.py:48] [32600] global_step=32600, grad_norm=1.4208039045333862, loss=3.243896961212158
I0307 06:15:32.662297 140036200724224 logging_writer.py:48] [32700] global_step=32700, grad_norm=1.502383828163147, loss=3.2693920135498047
I0307 06:16:11.156325 140036192331520 logging_writer.py:48] [32800] global_step=32800, grad_norm=1.2301965951919556, loss=3.2800230979919434
I0307 06:16:49.754418 140036200724224 logging_writer.py:48] [32900] global_step=32900, grad_norm=1.3366930484771729, loss=3.315955638885498
I0307 06:17:28.450644 140036192331520 logging_writer.py:48] [33000] global_step=33000, grad_norm=1.3452516794204712, loss=3.233577251434326
I0307 06:18:07.085622 140036200724224 logging_writer.py:48] [33100] global_step=33100, grad_norm=1.3248602151870728, loss=3.2621705532073975
I0307 06:18:10.925842 140191611933888 spec.py:321] Evaluating on the training split.
I0307 06:18:23.401092 140191611933888 spec.py:333] Evaluating on the validation split.
I0307 06:18:49.689002 140191611933888 spec.py:349] Evaluating on the test split.
I0307 06:18:51.465562 140191611933888 submission_runner.py:469] Time since start: 13911.12s, 	Step: 33111, 	{'train/accuracy': 0.7161989808082581, 'train/loss': 1.3585830926895142, 'validation/accuracy': 0.6484799981117249, 'validation/loss': 1.6654702425003052, 'validation/num_examples': 50000, 'test/accuracy': 0.5112000107765198, 'test/loss': 2.335038661956787, 'test/num_examples': 10000, 'score': 12807.9684612751, 'total_duration': 13911.119186401367, 'accumulated_submission_time': 12807.9684612751, 'accumulated_eval_time': 1097.2060430049896, 'accumulated_logging_time': 2.3264379501342773}
I0307 06:18:51.552023 140036192331520 logging_writer.py:48] [33111] accumulated_eval_time=1097.21, accumulated_logging_time=2.32644, accumulated_submission_time=12808, global_step=33111, preemption_count=0, score=12808, test/accuracy=0.5112, test/loss=2.33504, test/num_examples=10000, total_duration=13911.1, train/accuracy=0.716199, train/loss=1.35858, validation/accuracy=0.64848, validation/loss=1.66547, validation/num_examples=50000
I0307 06:19:26.498077 140036200724224 logging_writer.py:48] [33200] global_step=33200, grad_norm=1.8282420635223389, loss=3.2999496459960938
I0307 06:20:05.273423 140036192331520 logging_writer.py:48] [33300] global_step=33300, grad_norm=1.270007848739624, loss=3.2544939517974854
I0307 06:20:44.180850 140036200724224 logging_writer.py:48] [33400] global_step=33400, grad_norm=1.5097697973251343, loss=3.245060920715332
I0307 06:21:22.552622 140036192331520 logging_writer.py:48] [33500] global_step=33500, grad_norm=1.326148271560669, loss=3.2525908946990967
I0307 06:22:01.428426 140036200724224 logging_writer.py:48] [33600] global_step=33600, grad_norm=1.2852121591567993, loss=3.1728029251098633
I0307 06:22:40.118598 140036192331520 logging_writer.py:48] [33700] global_step=33700, grad_norm=1.4912058115005493, loss=3.2315967082977295
I0307 06:23:19.000840 140036200724224 logging_writer.py:48] [33800] global_step=33800, grad_norm=1.576027274131775, loss=3.2825894355773926
I0307 06:23:57.654197 140036192331520 logging_writer.py:48] [33900] global_step=33900, grad_norm=1.3204638957977295, loss=3.234616756439209
I0307 06:24:36.382105 140036200724224 logging_writer.py:48] [34000] global_step=34000, grad_norm=1.391108751296997, loss=3.2735815048217773
I0307 06:25:15.229141 140036192331520 logging_writer.py:48] [34100] global_step=34100, grad_norm=1.4249566793441772, loss=3.206531524658203
I0307 06:25:53.653589 140036200724224 logging_writer.py:48] [34200] global_step=34200, grad_norm=1.4161806106567383, loss=3.279122829437256
I0307 06:26:32.265246 140036192331520 logging_writer.py:48] [34300] global_step=34300, grad_norm=1.7536993026733398, loss=3.1935062408447266
I0307 06:27:10.750054 140036200724224 logging_writer.py:48] [34400] global_step=34400, grad_norm=1.5068484544754028, loss=3.237888813018799
I0307 06:27:21.770167 140191611933888 spec.py:321] Evaluating on the training split.
I0307 06:27:34.534765 140191611933888 spec.py:333] Evaluating on the validation split.
I0307 06:28:03.888411 140191611933888 spec.py:349] Evaluating on the test split.
I0307 06:28:05.709437 140191611933888 submission_runner.py:469] Time since start: 14465.36s, 	Step: 34429, 	{'train/accuracy': 0.7191286683082581, 'train/loss': 1.3470344543457031, 'validation/accuracy': 0.6523199677467346, 'validation/loss': 1.656497597694397, 'validation/num_examples': 50000, 'test/accuracy': 0.5245000123977661, 'test/loss': 2.3005402088165283, 'test/num_examples': 10000, 'score': 13318.015569448471, 'total_duration': 14465.363008975983, 'accumulated_submission_time': 13318.015569448471, 'accumulated_eval_time': 1141.145075082779, 'accumulated_logging_time': 2.4435434341430664}
I0307 06:28:05.829840 140036192331520 logging_writer.py:48] [34429] accumulated_eval_time=1141.15, accumulated_logging_time=2.44354, accumulated_submission_time=13318, global_step=34429, preemption_count=0, score=13318, test/accuracy=0.5245, test/loss=2.30054, test/num_examples=10000, total_duration=14465.4, train/accuracy=0.719129, train/loss=1.34703, validation/accuracy=0.65232, validation/loss=1.6565, validation/num_examples=50000
I0307 06:28:33.888011 140036200724224 logging_writer.py:48] [34500] global_step=34500, grad_norm=1.4273452758789062, loss=3.2078628540039062
I0307 06:29:12.177845 140036192331520 logging_writer.py:48] [34600] global_step=34600, grad_norm=1.4694772958755493, loss=3.228135585784912
I0307 06:29:50.659566 140036200724224 logging_writer.py:48] [34700] global_step=34700, grad_norm=1.2625596523284912, loss=3.256260395050049
I0307 06:30:29.315294 140036192331520 logging_writer.py:48] [34800] global_step=34800, grad_norm=1.3940972089767456, loss=3.208103656768799
I0307 06:31:08.092248 140036200724224 logging_writer.py:48] [34900] global_step=34900, grad_norm=1.6045277118682861, loss=3.2621541023254395
I0307 06:31:46.682552 140036192331520 logging_writer.py:48] [35000] global_step=35000, grad_norm=1.506996512413025, loss=3.1843953132629395
I0307 06:32:25.511069 140036200724224 logging_writer.py:48] [35100] global_step=35100, grad_norm=1.5129870176315308, loss=3.2570724487304688
I0307 06:33:04.016791 140036192331520 logging_writer.py:48] [35200] global_step=35200, grad_norm=1.4155373573303223, loss=3.2453267574310303
I0307 06:33:42.716934 140036200724224 logging_writer.py:48] [35300] global_step=35300, grad_norm=1.485770344734192, loss=3.2769792079925537
I0307 06:34:21.595338 140036192331520 logging_writer.py:48] [35400] global_step=35400, grad_norm=1.4448763132095337, loss=3.2788074016571045
I0307 06:35:00.415849 140036200724224 logging_writer.py:48] [35500] global_step=35500, grad_norm=1.6511361598968506, loss=3.198298454284668
I0307 06:35:39.069180 140036192331520 logging_writer.py:48] [35600] global_step=35600, grad_norm=1.5231201648712158, loss=3.2537522315979004
I0307 06:36:17.647083 140036200724224 logging_writer.py:48] [35700] global_step=35700, grad_norm=1.4561628103256226, loss=3.226823329925537
I0307 06:36:35.919754 140191611933888 spec.py:321] Evaluating on the training split.
I0307 06:36:48.061743 140191611933888 spec.py:333] Evaluating on the validation split.
I0307 06:37:12.663690 140191611933888 spec.py:349] Evaluating on the test split.
I0307 06:37:14.505555 140191611933888 submission_runner.py:469] Time since start: 15014.16s, 	Step: 35748, 	{'train/accuracy': 0.7219985723495483, 'train/loss': 1.3317444324493408, 'validation/accuracy': 0.6523199677467346, 'validation/loss': 1.6439622640609741, 'validation/num_examples': 50000, 'test/accuracy': 0.5229000449180603, 'test/loss': 2.297809600830078, 'test/num_examples': 10000, 'score': 13827.9128844738, 'total_duration': 15014.159200668335, 'accumulated_submission_time': 13827.9128844738, 'accumulated_eval_time': 1179.7307233810425, 'accumulated_logging_time': 2.615875482559204}
I0307 06:37:14.596274 140036192331520 logging_writer.py:48] [35748] accumulated_eval_time=1179.73, accumulated_logging_time=2.61588, accumulated_submission_time=13827.9, global_step=35748, preemption_count=0, score=13827.9, test/accuracy=0.5229, test/loss=2.29781, test/num_examples=10000, total_duration=15014.2, train/accuracy=0.721999, train/loss=1.33174, validation/accuracy=0.65232, validation/loss=1.64396, validation/num_examples=50000
I0307 06:37:35.240103 140036200724224 logging_writer.py:48] [35800] global_step=35800, grad_norm=1.3121751546859741, loss=3.101227283477783
I0307 06:38:14.030735 140036192331520 logging_writer.py:48] [35900] global_step=35900, grad_norm=1.5179636478424072, loss=3.140472412109375
I0307 06:38:53.110454 140036200724224 logging_writer.py:48] [36000] global_step=36000, grad_norm=1.4425221681594849, loss=3.282013177871704
I0307 06:39:31.780975 140036192331520 logging_writer.py:48] [36100] global_step=36100, grad_norm=1.4287152290344238, loss=3.243175745010376
I0307 06:40:10.081251 140036200724224 logging_writer.py:48] [36200] global_step=36200, grad_norm=1.5175902843475342, loss=3.2439591884613037
I0307 06:40:48.848865 140036192331520 logging_writer.py:48] [36300] global_step=36300, grad_norm=1.5626990795135498, loss=3.260300874710083
I0307 06:41:27.505520 140036200724224 logging_writer.py:48] [36400] global_step=36400, grad_norm=1.424469232559204, loss=3.226532459259033
I0307 06:42:06.289956 140036192331520 logging_writer.py:48] [36500] global_step=36500, grad_norm=1.2553945779800415, loss=3.187983512878418
I0307 06:42:45.194031 140036200724224 logging_writer.py:48] [36600] global_step=36600, grad_norm=1.479649305343628, loss=3.2235279083251953
I0307 06:43:23.782709 140036192331520 logging_writer.py:48] [36700] global_step=36700, grad_norm=1.3908395767211914, loss=3.2840476036071777
I0307 06:44:02.585382 140036200724224 logging_writer.py:48] [36800] global_step=36800, grad_norm=1.4622218608856201, loss=3.2334060668945312
I0307 06:44:41.104352 140036192331520 logging_writer.py:48] [36900] global_step=36900, grad_norm=2.474630117416382, loss=3.259674072265625
I0307 06:45:19.502255 140036200724224 logging_writer.py:48] [37000] global_step=37000, grad_norm=1.5521314144134521, loss=3.2758302688598633
I0307 06:45:44.629318 140191611933888 spec.py:321] Evaluating on the training split.
I0307 06:45:57.013052 140191611933888 spec.py:333] Evaluating on the validation split.
I0307 06:46:23.438834 140191611933888 spec.py:349] Evaluating on the test split.
I0307 06:46:25.198693 140191611933888 submission_runner.py:469] Time since start: 15564.85s, 	Step: 37066, 	{'train/accuracy': 0.702547013759613, 'train/loss': 1.403496265411377, 'validation/accuracy': 0.6398199796676636, 'validation/loss': 1.6943724155426025, 'validation/num_examples': 50000, 'test/accuracy': 0.5054000020027161, 'test/loss': 2.3812694549560547, 'test/num_examples': 10000, 'score': 14337.776632785797, 'total_duration': 15564.852329492569, 'accumulated_submission_time': 14337.776632785797, 'accumulated_eval_time': 1220.2999331951141, 'accumulated_logging_time': 2.7399470806121826}
I0307 06:46:25.305345 140036192331520 logging_writer.py:48] [37066] accumulated_eval_time=1220.3, accumulated_logging_time=2.73995, accumulated_submission_time=14337.8, global_step=37066, preemption_count=0, score=14337.8, test/accuracy=0.5054, test/loss=2.38127, test/num_examples=10000, total_duration=15564.9, train/accuracy=0.702547, train/loss=1.4035, validation/accuracy=0.63982, validation/loss=1.69437, validation/num_examples=50000
I0307 06:46:38.922033 140036200724224 logging_writer.py:48] [37100] global_step=37100, grad_norm=1.4215319156646729, loss=3.186333179473877
I0307 06:47:17.661255 140036192331520 logging_writer.py:48] [37200] global_step=37200, grad_norm=1.5124447345733643, loss=3.182478666305542
I0307 06:47:56.015989 140036200724224 logging_writer.py:48] [37300] global_step=37300, grad_norm=1.6760971546173096, loss=3.2848479747772217
I0307 06:48:34.793289 140036192331520 logging_writer.py:48] [37400] global_step=37400, grad_norm=1.564285159111023, loss=3.230112075805664
I0307 06:49:13.525678 140036200724224 logging_writer.py:48] [37500] global_step=37500, grad_norm=1.4440139532089233, loss=3.222440719604492
I0307 06:49:52.122194 140036192331520 logging_writer.py:48] [37600] global_step=37600, grad_norm=1.4483110904693604, loss=3.1907854080200195
I0307 06:50:30.881337 140036200724224 logging_writer.py:48] [37700] global_step=37700, grad_norm=1.656699299812317, loss=3.2240822315216064
I0307 06:51:09.495608 140036192331520 logging_writer.py:48] [37800] global_step=37800, grad_norm=1.6311055421829224, loss=3.2574126720428467
I0307 06:51:48.170368 140036200724224 logging_writer.py:48] [37900] global_step=37900, grad_norm=1.6125096082687378, loss=3.2155866622924805
I0307 06:52:27.529545 140036192331520 logging_writer.py:48] [38000] global_step=38000, grad_norm=1.3804255723953247, loss=3.1366355419158936
I0307 06:53:06.620218 140036200724224 logging_writer.py:48] [38100] global_step=38100, grad_norm=1.4870613813400269, loss=3.187654972076416
I0307 06:53:45.330606 140036192331520 logging_writer.py:48] [38200] global_step=38200, grad_norm=1.4835573434829712, loss=3.2416162490844727
I0307 06:54:24.106073 140036200724224 logging_writer.py:48] [38300] global_step=38300, grad_norm=1.485965371131897, loss=3.271228313446045
I0307 06:54:55.464394 140191611933888 spec.py:321] Evaluating on the training split.
I0307 06:55:07.513326 140191611933888 spec.py:333] Evaluating on the validation split.
I0307 06:55:34.350333 140191611933888 spec.py:349] Evaluating on the test split.
I0307 06:55:36.152165 140191611933888 submission_runner.py:469] Time since start: 16115.81s, 	Step: 38383, 	{'train/accuracy': 0.7105189561843872, 'train/loss': 1.391722321510315, 'validation/accuracy': 0.6444799900054932, 'validation/loss': 1.6981041431427002, 'validation/num_examples': 50000, 'test/accuracy': 0.5157000422477722, 'test/loss': 2.336155652999878, 'test/num_examples': 10000, 'score': 14847.772614955902, 'total_duration': 16115.805789232254, 'accumulated_submission_time': 14847.772614955902, 'accumulated_eval_time': 1260.9875202178955, 'accumulated_logging_time': 2.8697681427001953}
I0307 06:55:36.240036 140036192331520 logging_writer.py:48] [38383] accumulated_eval_time=1260.99, accumulated_logging_time=2.86977, accumulated_submission_time=14847.8, global_step=38383, preemption_count=0, score=14847.8, test/accuracy=0.5157, test/loss=2.33616, test/num_examples=10000, total_duration=16115.8, train/accuracy=0.710519, train/loss=1.39172, validation/accuracy=0.64448, validation/loss=1.6981, validation/num_examples=50000
I0307 06:55:43.082157 140036200724224 logging_writer.py:48] [38400] global_step=38400, grad_norm=1.5260204076766968, loss=3.2261152267456055
I0307 06:56:21.146745 140036192331520 logging_writer.py:48] [38500] global_step=38500, grad_norm=1.4989078044891357, loss=3.213733434677124
I0307 06:56:59.943433 140036200724224 logging_writer.py:48] [38600] global_step=38600, grad_norm=1.6417657136917114, loss=3.2358615398406982
I0307 06:57:38.725991 140036192331520 logging_writer.py:48] [38700] global_step=38700, grad_norm=1.482025146484375, loss=3.2163941860198975
I0307 06:58:17.469402 140036200724224 logging_writer.py:48] [38800] global_step=38800, grad_norm=1.7701990604400635, loss=3.2474584579467773
I0307 06:58:56.142683 140036192331520 logging_writer.py:48] [38900] global_step=38900, grad_norm=1.6323872804641724, loss=3.241133213043213
I0307 06:59:34.788713 140036200724224 logging_writer.py:48] [39000] global_step=39000, grad_norm=1.618009090423584, loss=3.154435634613037
I0307 07:00:13.669686 140036192331520 logging_writer.py:48] [39100] global_step=39100, grad_norm=1.8236809968948364, loss=3.2687628269195557
I0307 07:00:52.332680 140036200724224 logging_writer.py:48] [39200] global_step=39200, grad_norm=1.5551761388778687, loss=3.250056505203247
I0307 07:01:31.303157 140036192331520 logging_writer.py:48] [39300] global_step=39300, grad_norm=1.6196775436401367, loss=3.2429075241088867
I0307 07:02:09.888262 140036200724224 logging_writer.py:48] [39400] global_step=39400, grad_norm=1.5712262392044067, loss=3.157752275466919
I0307 07:02:48.513567 140036192331520 logging_writer.py:48] [39500] global_step=39500, grad_norm=1.489190936088562, loss=3.1626124382019043
I0307 07:03:27.035017 140036200724224 logging_writer.py:48] [39600] global_step=39600, grad_norm=1.6341068744659424, loss=3.209904432296753
I0307 07:04:05.614318 140036192331520 logging_writer.py:48] [39700] global_step=39700, grad_norm=1.6602486371994019, loss=3.2633073329925537
I0307 07:04:06.353282 140191611933888 spec.py:321] Evaluating on the training split.
I0307 07:04:19.087541 140191611933888 spec.py:333] Evaluating on the validation split.
I0307 07:04:42.535980 140191611933888 spec.py:349] Evaluating on the test split.
I0307 07:04:44.355594 140191611933888 submission_runner.py:469] Time since start: 16664.01s, 	Step: 39703, 	{'train/accuracy': 0.7277383208274841, 'train/loss': 1.3032801151275635, 'validation/accuracy': 0.6576200127601624, 'validation/loss': 1.611594319343567, 'validation/num_examples': 50000, 'test/accuracy': 0.5355000495910645, 'test/loss': 2.250380754470825, 'test/num_examples': 10000, 'score': 15357.718620061874, 'total_duration': 16664.009169578552, 'accumulated_submission_time': 15357.718620061874, 'accumulated_eval_time': 1298.9895973205566, 'accumulated_logging_time': 2.985231637954712}
I0307 07:04:44.490282 140036200724224 logging_writer.py:48] [39703] accumulated_eval_time=1298.99, accumulated_logging_time=2.98523, accumulated_submission_time=15357.7, global_step=39703, preemption_count=0, score=15357.7, test/accuracy=0.5355, test/loss=2.25038, test/num_examples=10000, total_duration=16664, train/accuracy=0.727738, train/loss=1.30328, validation/accuracy=0.65762, validation/loss=1.61159, validation/num_examples=50000
I0307 07:05:22.375828 140036192331520 logging_writer.py:48] [39800] global_step=39800, grad_norm=1.7042487859725952, loss=3.2255635261535645
I0307 07:06:00.941799 140036200724224 logging_writer.py:48] [39900] global_step=39900, grad_norm=1.610329031944275, loss=3.2446210384368896
I0307 07:06:39.591257 140036192331520 logging_writer.py:48] [40000] global_step=40000, grad_norm=1.6502830982208252, loss=3.165917158126831
I0307 07:07:18.559419 140036200724224 logging_writer.py:48] [40100] global_step=40100, grad_norm=1.406318187713623, loss=3.0949137210845947
I0307 07:07:57.208967 140036192331520 logging_writer.py:48] [40200] global_step=40200, grad_norm=1.5541406869888306, loss=3.2494728565216064
I0307 07:08:36.073386 140036200724224 logging_writer.py:48] [40300] global_step=40300, grad_norm=1.5289688110351562, loss=3.188103199005127
I0307 07:09:14.902778 140036192331520 logging_writer.py:48] [40400] global_step=40400, grad_norm=1.5408350229263306, loss=3.209494113922119
I0307 07:09:53.410365 140036200724224 logging_writer.py:48] [40500] global_step=40500, grad_norm=1.562666416168213, loss=3.2872698307037354
I0307 07:10:32.086027 140036192331520 logging_writer.py:48] [40600] global_step=40600, grad_norm=1.622896671295166, loss=3.1933512687683105
I0307 07:11:10.456342 140036200724224 logging_writer.py:48] [40700] global_step=40700, grad_norm=1.6068757772445679, loss=3.177532196044922
I0307 07:11:49.318367 140036192331520 logging_writer.py:48] [40800] global_step=40800, grad_norm=1.4877870082855225, loss=3.2847089767456055
I0307 07:12:27.540590 140036200724224 logging_writer.py:48] [40900] global_step=40900, grad_norm=1.5638561248779297, loss=3.262723207473755
I0307 07:13:05.944509 140036192331520 logging_writer.py:48] [41000] global_step=41000, grad_norm=1.6024880409240723, loss=3.2667720317840576
I0307 07:13:14.397582 140191611933888 spec.py:321] Evaluating on the training split.
I0307 07:13:27.049586 140191611933888 spec.py:333] Evaluating on the validation split.
I0307 07:13:51.085598 140191611933888 spec.py:349] Evaluating on the test split.
I0307 07:13:52.849487 140191611933888 submission_runner.py:469] Time since start: 17212.50s, 	Step: 41023, 	{'train/accuracy': 0.7084861397743225, 'train/loss': 1.3757389783859253, 'validation/accuracy': 0.640720009803772, 'validation/loss': 1.6938326358795166, 'validation/num_examples': 50000, 'test/accuracy': 0.5135000348091125, 'test/loss': 2.345468759536743, 'test/num_examples': 10000, 'score': 15867.44130897522, 'total_duration': 17212.503080129623, 'accumulated_submission_time': 15867.44130897522, 'accumulated_eval_time': 1337.4413006305695, 'accumulated_logging_time': 3.16410231590271}
I0307 07:13:52.997915 140036200724224 logging_writer.py:48] [41023] accumulated_eval_time=1337.44, accumulated_logging_time=3.1641, accumulated_submission_time=15867.4, global_step=41023, preemption_count=0, score=15867.4, test/accuracy=0.5135, test/loss=2.34547, test/num_examples=10000, total_duration=17212.5, train/accuracy=0.708486, train/loss=1.37574, validation/accuracy=0.64072, validation/loss=1.69383, validation/num_examples=50000
I0307 07:14:23.056277 140036192331520 logging_writer.py:48] [41100] global_step=41100, grad_norm=1.9022374153137207, loss=3.209200859069824
I0307 07:15:01.554419 140036200724224 logging_writer.py:48] [41200] global_step=41200, grad_norm=1.5283422470092773, loss=3.2603421211242676
I0307 07:15:40.471518 140036192331520 logging_writer.py:48] [41300] global_step=41300, grad_norm=1.5859922170639038, loss=3.1554512977600098
I0307 07:16:19.165798 140036200724224 logging_writer.py:48] [41400] global_step=41400, grad_norm=1.6715247631072998, loss=3.193570852279663
I0307 07:16:58.376661 140036192331520 logging_writer.py:48] [41500] global_step=41500, grad_norm=1.629082202911377, loss=3.1641108989715576
I0307 07:17:37.326384 140036200724224 logging_writer.py:48] [41600] global_step=41600, grad_norm=1.4332993030548096, loss=3.182931900024414
I0307 07:18:16.026727 140036192331520 logging_writer.py:48] [41700] global_step=41700, grad_norm=1.5061737298965454, loss=3.1912918090820312
I0307 07:18:54.694687 140036200724224 logging_writer.py:48] [41800] global_step=41800, grad_norm=1.5951989889144897, loss=3.121260166168213
I0307 07:19:33.589091 140036192331520 logging_writer.py:48] [41900] global_step=41900, grad_norm=1.6432760953903198, loss=3.2578585147857666
I0307 07:20:12.446036 140036200724224 logging_writer.py:48] [42000] global_step=42000, grad_norm=1.7713816165924072, loss=3.2235918045043945
I0307 07:20:51.156353 140036192331520 logging_writer.py:48] [42100] global_step=42100, grad_norm=1.6603245735168457, loss=3.133868455886841
I0307 07:21:30.122848 140036200724224 logging_writer.py:48] [42200] global_step=42200, grad_norm=1.656724214553833, loss=3.1646385192871094
I0307 07:22:08.408401 140036192331520 logging_writer.py:48] [42300] global_step=42300, grad_norm=1.6405487060546875, loss=3.1916208267211914
I0307 07:22:23.202791 140191611933888 spec.py:321] Evaluating on the training split.
I0307 07:22:35.535259 140191611933888 spec.py:333] Evaluating on the validation split.
I0307 07:22:54.483920 140191611933888 spec.py:349] Evaluating on the test split.
I0307 07:22:56.291754 140191611933888 submission_runner.py:469] Time since start: 17755.95s, 	Step: 42339, 	{'train/accuracy': 0.7323620915412903, 'train/loss': 1.2590270042419434, 'validation/accuracy': 0.6604799628257751, 'validation/loss': 1.5863053798675537, 'validation/num_examples': 50000, 'test/accuracy': 0.5314000248908997, 'test/loss': 2.2374258041381836, 'test/num_examples': 10000, 'score': 16377.471191883087, 'total_duration': 17755.94539618492, 'accumulated_submission_time': 16377.471191883087, 'accumulated_eval_time': 1370.5300965309143, 'accumulated_logging_time': 3.3480923175811768}
I0307 07:22:56.392967 140036200724224 logging_writer.py:48] [42339] accumulated_eval_time=1370.53, accumulated_logging_time=3.34809, accumulated_submission_time=16377.5, global_step=42339, preemption_count=0, score=16377.5, test/accuracy=0.5314, test/loss=2.23743, test/num_examples=10000, total_duration=17755.9, train/accuracy=0.732362, train/loss=1.25903, validation/accuracy=0.66048, validation/loss=1.58631, validation/num_examples=50000
I0307 07:23:20.489790 140036192331520 logging_writer.py:48] [42400] global_step=42400, grad_norm=1.7614760398864746, loss=3.1707940101623535
I0307 07:23:59.077044 140036200724224 logging_writer.py:48] [42500] global_step=42500, grad_norm=1.7941255569458008, loss=3.2086021900177
I0307 07:24:38.302927 140036192331520 logging_writer.py:48] [42600] global_step=42600, grad_norm=1.6407643556594849, loss=3.1295204162597656
I0307 07:25:16.557721 140036200724224 logging_writer.py:48] [42700] global_step=42700, grad_norm=1.8641737699508667, loss=3.2060372829437256
I0307 07:25:55.444288 140036192331520 logging_writer.py:48] [42800] global_step=42800, grad_norm=1.7536451816558838, loss=3.109592914581299
I0307 07:26:34.501271 140036200724224 logging_writer.py:48] [42900] global_step=42900, grad_norm=2.2367100715637207, loss=3.23789119720459
I0307 07:27:13.300973 140036192331520 logging_writer.py:48] [43000] global_step=43000, grad_norm=1.6196131706237793, loss=3.2034518718719482
I0307 07:27:52.006428 140036200724224 logging_writer.py:48] [43100] global_step=43100, grad_norm=1.6032474040985107, loss=3.2071259021759033
I0307 07:28:31.220523 140036192331520 logging_writer.py:48] [43200] global_step=43200, grad_norm=1.6293565034866333, loss=3.1689553260803223
I0307 07:29:09.933960 140036200724224 logging_writer.py:48] [43300] global_step=43300, grad_norm=1.546296238899231, loss=3.202963352203369
I0307 07:29:48.482553 140036192331520 logging_writer.py:48] [43400] global_step=43400, grad_norm=1.5557501316070557, loss=3.1767959594726562
I0307 07:30:27.227861 140036200724224 logging_writer.py:48] [43500] global_step=43500, grad_norm=1.6112470626831055, loss=3.202669143676758
I0307 07:31:06.086359 140036192331520 logging_writer.py:48] [43600] global_step=43600, grad_norm=1.7031941413879395, loss=3.185391902923584
I0307 07:31:26.459158 140191611933888 spec.py:321] Evaluating on the training split.
I0307 07:31:38.795567 140191611933888 spec.py:333] Evaluating on the validation split.
I0307 07:32:03.963680 140191611933888 spec.py:349] Evaluating on the test split.
I0307 07:32:05.770327 140191611933888 submission_runner.py:469] Time since start: 18305.42s, 	Step: 43654, 	{'train/accuracy': 0.7234932780265808, 'train/loss': 1.3088228702545166, 'validation/accuracy': 0.6539799571037292, 'validation/loss': 1.6147361993789673, 'validation/num_examples': 50000, 'test/accuracy': 0.5294000506401062, 'test/loss': 2.283883810043335, 'test/num_examples': 10000, 'score': 16887.367341518402, 'total_duration': 18305.42393732071, 'accumulated_submission_time': 16887.367341518402, 'accumulated_eval_time': 1409.8410658836365, 'accumulated_logging_time': 3.4818544387817383}
I0307 07:32:05.860036 140036200724224 logging_writer.py:48] [43654] accumulated_eval_time=1409.84, accumulated_logging_time=3.48185, accumulated_submission_time=16887.4, global_step=43654, preemption_count=0, score=16887.4, test/accuracy=0.5294, test/loss=2.28388, test/num_examples=10000, total_duration=18305.4, train/accuracy=0.723493, train/loss=1.30882, validation/accuracy=0.65398, validation/loss=1.61474, validation/num_examples=50000
I0307 07:32:24.009893 140036192331520 logging_writer.py:48] [43700] global_step=43700, grad_norm=1.52550208568573, loss=3.1122705936431885
I0307 07:33:03.106440 140036200724224 logging_writer.py:48] [43800] global_step=43800, grad_norm=1.4774724245071411, loss=3.1962437629699707
I0307 07:33:41.774830 140036192331520 logging_writer.py:48] [43900] global_step=43900, grad_norm=2.0692105293273926, loss=3.2091009616851807
I0307 07:34:20.477417 140036200724224 logging_writer.py:48] [44000] global_step=44000, grad_norm=1.7623586654663086, loss=3.1515936851501465
I0307 07:34:59.200489 140036192331520 logging_writer.py:48] [44100] global_step=44100, grad_norm=1.679580569267273, loss=3.1554179191589355
I0307 07:35:37.822182 140036200724224 logging_writer.py:48] [44200] global_step=44200, grad_norm=2.025454044342041, loss=3.2387001514434814
I0307 07:36:16.879557 140036192331520 logging_writer.py:48] [44300] global_step=44300, grad_norm=1.5121195316314697, loss=3.146170139312744
I0307 07:36:55.353536 140036200724224 logging_writer.py:48] [44400] global_step=44400, grad_norm=1.5844653844833374, loss=3.105318546295166
I0307 07:37:34.142654 140036192331520 logging_writer.py:48] [44500] global_step=44500, grad_norm=1.6848700046539307, loss=3.2097764015197754
I0307 07:38:13.023732 140036200724224 logging_writer.py:48] [44600] global_step=44600, grad_norm=1.5840715169906616, loss=3.1156680583953857
I0307 07:38:51.693505 140036192331520 logging_writer.py:48] [44700] global_step=44700, grad_norm=1.6636368036270142, loss=3.214519739151001
I0307 07:39:30.190383 140036200724224 logging_writer.py:48] [44800] global_step=44800, grad_norm=1.536956548690796, loss=3.2068910598754883
I0307 07:40:08.871912 140036192331520 logging_writer.py:48] [44900] global_step=44900, grad_norm=1.726633906364441, loss=3.187601327896118
I0307 07:40:35.973877 140191611933888 spec.py:321] Evaluating on the training split.
I0307 07:40:48.294946 140191611933888 spec.py:333] Evaluating on the validation split.
I0307 07:41:04.767488 140191611933888 spec.py:349] Evaluating on the test split.
I0307 07:41:06.602255 140191611933888 submission_runner.py:469] Time since start: 18846.26s, 	Step: 44970, 	{'train/accuracy': 0.7120336294174194, 'train/loss': 1.3774102926254272, 'validation/accuracy': 0.6472799777984619, 'validation/loss': 1.6748263835906982, 'validation/num_examples': 50000, 'test/accuracy': 0.5196000337600708, 'test/loss': 2.306929588317871, 'test/num_examples': 10000, 'score': 17397.292685747147, 'total_duration': 18846.25577688217, 'accumulated_submission_time': 17397.292685747147, 'accumulated_eval_time': 1440.4691541194916, 'accumulated_logging_time': 3.6180477142333984}
I0307 07:41:06.700531 140036200724224 logging_writer.py:48] [44970] accumulated_eval_time=1440.47, accumulated_logging_time=3.61805, accumulated_submission_time=17397.3, global_step=44970, preemption_count=0, score=17397.3, test/accuracy=0.5196, test/loss=2.30693, test/num_examples=10000, total_duration=18846.3, train/accuracy=0.712034, train/loss=1.37741, validation/accuracy=0.64728, validation/loss=1.67483, validation/num_examples=50000
I0307 07:41:18.946856 140036192331520 logging_writer.py:48] [45000] global_step=45000, grad_norm=1.5705327987670898, loss=3.218174934387207
I0307 07:41:57.440506 140036200724224 logging_writer.py:48] [45100] global_step=45100, grad_norm=1.627770185470581, loss=3.1621742248535156
I0307 07:42:35.483278 140036192331520 logging_writer.py:48] [45200] global_step=45200, grad_norm=2.066087484359741, loss=3.196347713470459
I0307 07:43:14.181934 140036200724224 logging_writer.py:48] [45300] global_step=45300, grad_norm=1.6982686519622803, loss=3.2058846950531006
I0307 07:43:52.736082 140036192331520 logging_writer.py:48] [45400] global_step=45400, grad_norm=2.0279383659362793, loss=3.230710506439209
I0307 07:44:31.354216 140036200724224 logging_writer.py:48] [45500] global_step=45500, grad_norm=1.6819664239883423, loss=3.1569833755493164
I0307 07:45:09.852701 140036192331520 logging_writer.py:48] [45600] global_step=45600, grad_norm=1.7210570573806763, loss=3.2037980556488037
I0307 07:45:48.447942 140036200724224 logging_writer.py:48] [45700] global_step=45700, grad_norm=1.7921862602233887, loss=3.2146806716918945
I0307 07:46:27.399472 140036192331520 logging_writer.py:48] [45800] global_step=45800, grad_norm=1.866062879562378, loss=3.189056873321533
I0307 07:47:05.788227 140036200724224 logging_writer.py:48] [45900] global_step=45900, grad_norm=1.680734634399414, loss=3.1948232650756836
I0307 07:47:44.572330 140036192331520 logging_writer.py:48] [46000] global_step=46000, grad_norm=1.794775128364563, loss=3.178947687149048
I0307 07:48:23.573318 140036200724224 logging_writer.py:48] [46100] global_step=46100, grad_norm=1.629611849784851, loss=3.084951162338257
I0307 07:49:02.817993 140036192331520 logging_writer.py:48] [46200] global_step=46200, grad_norm=1.732460379600525, loss=3.224369525909424
I0307 07:49:36.960452 140191611933888 spec.py:321] Evaluating on the training split.
I0307 07:49:49.256872 140191611933888 spec.py:333] Evaluating on the validation split.
I0307 07:50:09.075870 140191611933888 spec.py:349] Evaluating on the test split.
I0307 07:50:10.873320 140191611933888 submission_runner.py:469] Time since start: 19390.53s, 	Step: 46289, 	{'train/accuracy': 0.7318239808082581, 'train/loss': 1.2762997150421143, 'validation/accuracy': 0.6612799763679504, 'validation/loss': 1.5900646448135376, 'validation/num_examples': 50000, 'test/accuracy': 0.5356000065803528, 'test/loss': 2.2268800735473633, 'test/num_examples': 10000, 'score': 17907.36415863037, 'total_duration': 19390.526972055435, 'accumulated_submission_time': 17907.36415863037, 'accumulated_eval_time': 1474.3818657398224, 'accumulated_logging_time': 3.764610528945923}
I0307 07:50:10.996420 140036200724224 logging_writer.py:48] [46289] accumulated_eval_time=1474.38, accumulated_logging_time=3.76461, accumulated_submission_time=17907.4, global_step=46289, preemption_count=0, score=17907.4, test/accuracy=0.5356, test/loss=2.22688, test/num_examples=10000, total_duration=19390.5, train/accuracy=0.731824, train/loss=1.2763, validation/accuracy=0.66128, validation/loss=1.59006, validation/num_examples=50000
I0307 07:50:15.721483 140036192331520 logging_writer.py:48] [46300] global_step=46300, grad_norm=1.928688645362854, loss=3.1733944416046143
I0307 07:50:54.540425 140036200724224 logging_writer.py:48] [46400] global_step=46400, grad_norm=1.6280758380889893, loss=3.1584625244140625
I0307 07:51:33.644701 140036192331520 logging_writer.py:48] [46500] global_step=46500, grad_norm=1.593964695930481, loss=3.092939615249634
I0307 07:52:12.432628 140036200724224 logging_writer.py:48] [46600] global_step=46600, grad_norm=1.6181670427322388, loss=3.108785390853882
I0307 07:52:50.840380 140036192331520 logging_writer.py:48] [46700] global_step=46700, grad_norm=1.9333869218826294, loss=3.2391998767852783
I0307 07:53:29.594032 140036200724224 logging_writer.py:48] [46800] global_step=46800, grad_norm=1.5272436141967773, loss=3.1342415809631348
I0307 07:54:07.835325 140036192331520 logging_writer.py:48] [46900] global_step=46900, grad_norm=1.6552925109863281, loss=3.155608654022217
I0307 07:54:46.917942 140036200724224 logging_writer.py:48] [47000] global_step=47000, grad_norm=1.5742545127868652, loss=3.104038715362549
I0307 07:55:25.644666 140036192331520 logging_writer.py:48] [47100] global_step=47100, grad_norm=1.8542243242263794, loss=3.1990039348602295
I0307 07:56:04.467509 140036200724224 logging_writer.py:48] [47200] global_step=47200, grad_norm=1.5857261419296265, loss=3.211944103240967
I0307 07:56:43.475315 140036192331520 logging_writer.py:48] [47300] global_step=47300, grad_norm=1.8527911901474, loss=3.1888973712921143
I0307 07:57:22.677119 140036200724224 logging_writer.py:48] [47400] global_step=47400, grad_norm=1.714701771736145, loss=3.1755177974700928
I0307 07:58:01.864104 140036192331520 logging_writer.py:48] [47500] global_step=47500, grad_norm=1.823763132095337, loss=3.1381962299346924
I0307 07:58:40.970776 140036200724224 logging_writer.py:48] [47600] global_step=47600, grad_norm=1.5955997705459595, loss=3.163402557373047
I0307 07:58:40.981178 140191611933888 spec.py:321] Evaluating on the training split.
I0307 07:58:53.061487 140191611933888 spec.py:333] Evaluating on the validation split.
I0307 07:59:14.682342 140191611933888 spec.py:349] Evaluating on the test split.
I0307 07:59:16.460681 140191611933888 submission_runner.py:469] Time since start: 19936.11s, 	Step: 47601, 	{'train/accuracy': 0.7213807106018066, 'train/loss': 1.3244725465774536, 'validation/accuracy': 0.6558399796485901, 'validation/loss': 1.6315107345581055, 'validation/num_examples': 50000, 'test/accuracy': 0.518500030040741, 'test/loss': 2.3189094066619873, 'test/num_examples': 10000, 'score': 18417.17841887474, 'total_duration': 19936.114319086075, 'accumulated_submission_time': 18417.17841887474, 'accumulated_eval_time': 1509.861181974411, 'accumulated_logging_time': 3.912874698638916}
I0307 07:59:16.535457 140036192331520 logging_writer.py:48] [47601] accumulated_eval_time=1509.86, accumulated_logging_time=3.91287, accumulated_submission_time=18417.2, global_step=47601, preemption_count=0, score=18417.2, test/accuracy=0.5185, test/loss=2.31891, test/num_examples=10000, total_duration=19936.1, train/accuracy=0.721381, train/loss=1.32447, validation/accuracy=0.65584, validation/loss=1.63151, validation/num_examples=50000
I0307 07:59:55.346742 140036200724224 logging_writer.py:48] [47700] global_step=47700, grad_norm=1.7744078636169434, loss=3.190429449081421
I0307 08:00:34.065058 140036192331520 logging_writer.py:48] [47800] global_step=47800, grad_norm=1.6292994022369385, loss=3.1831977367401123
I0307 08:01:12.965952 140036200724224 logging_writer.py:48] [47900] global_step=47900, grad_norm=1.8034563064575195, loss=3.121448516845703
I0307 08:01:52.016320 140036192331520 logging_writer.py:48] [48000] global_step=48000, grad_norm=1.6793941259384155, loss=3.1539201736450195
I0307 08:02:30.700404 140036200724224 logging_writer.py:48] [48100] global_step=48100, grad_norm=1.6389552354812622, loss=3.188077449798584
I0307 08:03:09.558206 140036192331520 logging_writer.py:48] [48200] global_step=48200, grad_norm=1.6910641193389893, loss=3.1409339904785156
I0307 08:03:47.934121 140036200724224 logging_writer.py:48] [48300] global_step=48300, grad_norm=1.9054126739501953, loss=3.1849775314331055
I0307 08:04:26.496070 140036192331520 logging_writer.py:48] [48400] global_step=48400, grad_norm=1.6333823204040527, loss=3.1126983165740967
I0307 08:05:05.425694 140036200724224 logging_writer.py:48] [48500] global_step=48500, grad_norm=1.9642642736434937, loss=3.105619192123413
I0307 08:05:44.595389 140036192331520 logging_writer.py:48] [48600] global_step=48600, grad_norm=1.8155303001403809, loss=3.1833786964416504
I0307 08:06:24.064906 140036200724224 logging_writer.py:48] [48700] global_step=48700, grad_norm=1.629144549369812, loss=3.230956554412842
I0307 08:07:04.325690 140036192331520 logging_writer.py:48] [48800] global_step=48800, grad_norm=1.6889313459396362, loss=3.1129260063171387
I0307 08:07:43.211369 140036200724224 logging_writer.py:48] [48900] global_step=48900, grad_norm=1.6734123229980469, loss=3.172825813293457
I0307 08:07:46.763814 140191611933888 spec.py:321] Evaluating on the training split.
I0307 08:07:58.921582 140191611933888 spec.py:333] Evaluating on the validation split.
I0307 08:08:21.866712 140191611933888 spec.py:349] Evaluating on the test split.
I0307 08:08:23.681270 140191611933888 submission_runner.py:469] Time since start: 20483.33s, 	Step: 48910, 	{'train/accuracy': 0.7327407598495483, 'train/loss': 1.3355664014816284, 'validation/accuracy': 0.6626600027084351, 'validation/loss': 1.6429983377456665, 'validation/num_examples': 50000, 'test/accuracy': 0.5272000432014465, 'test/loss': 2.3307063579559326, 'test/num_examples': 10000, 'score': 18927.227395296097, 'total_duration': 20483.33490729332, 'accumulated_submission_time': 18927.227395296097, 'accumulated_eval_time': 1546.7784659862518, 'accumulated_logging_time': 4.030738115310669}
I0307 08:08:23.738228 140036192331520 logging_writer.py:48] [48910] accumulated_eval_time=1546.78, accumulated_logging_time=4.03074, accumulated_submission_time=18927.2, global_step=48910, preemption_count=0, score=18927.2, test/accuracy=0.5272, test/loss=2.33071, test/num_examples=10000, total_duration=20483.3, train/accuracy=0.732741, train/loss=1.33557, validation/accuracy=0.66266, validation/loss=1.643, validation/num_examples=50000
I0307 08:08:59.868881 140036200724224 logging_writer.py:48] [49000] global_step=49000, grad_norm=1.8503773212432861, loss=3.254610300064087
I0307 08:09:38.908335 140036192331520 logging_writer.py:48] [49100] global_step=49100, grad_norm=1.871462106704712, loss=3.1701509952545166
I0307 08:10:17.828523 140036200724224 logging_writer.py:48] [49200] global_step=49200, grad_norm=1.700163722038269, loss=3.141028642654419
I0307 08:10:56.573382 140036192331520 logging_writer.py:48] [49300] global_step=49300, grad_norm=1.960983157157898, loss=3.176555871963501
I0307 08:11:35.573507 140036200724224 logging_writer.py:48] [49400] global_step=49400, grad_norm=1.765122890472412, loss=3.2223260402679443
I0307 08:12:14.438127 140036192331520 logging_writer.py:48] [49500] global_step=49500, grad_norm=2.0383293628692627, loss=3.1159420013427734
I0307 08:12:53.101246 140036200724224 logging_writer.py:48] [49600] global_step=49600, grad_norm=1.968211054801941, loss=3.139756679534912
I0307 08:13:32.157393 140036192331520 logging_writer.py:48] [49700] global_step=49700, grad_norm=1.6758843660354614, loss=3.1672165393829346
I0307 08:14:11.535513 140036200724224 logging_writer.py:48] [49800] global_step=49800, grad_norm=1.8488675355911255, loss=3.1992852687835693
I0307 08:14:51.591498 140036192331520 logging_writer.py:48] [49900] global_step=49900, grad_norm=1.906499981880188, loss=3.12839412689209
I0307 08:15:31.587733 140036200724224 logging_writer.py:48] [50000] global_step=50000, grad_norm=1.6089107990264893, loss=3.1544504165649414
I0307 08:16:10.808954 140036192331520 logging_writer.py:48] [50100] global_step=50100, grad_norm=1.8701841831207275, loss=3.1364989280700684
I0307 08:16:50.069353 140036200724224 logging_writer.py:48] [50200] global_step=50200, grad_norm=1.8980748653411865, loss=3.158536672592163
I0307 08:16:53.825157 140191611933888 spec.py:321] Evaluating on the training split.
I0307 08:17:06.372667 140191611933888 spec.py:333] Evaluating on the validation split.
I0307 08:17:33.281543 140191611933888 spec.py:349] Evaluating on the test split.
I0307 08:17:35.041837 140191611933888 submission_runner.py:469] Time since start: 21034.70s, 	Step: 50211, 	{'train/accuracy': 0.7366868257522583, 'train/loss': 1.2299232482910156, 'validation/accuracy': 0.6650999784469604, 'validation/loss': 1.5512888431549072, 'validation/num_examples': 50000, 'test/accuracy': 0.5398000478744507, 'test/loss': 2.1954569816589355, 'test/num_examples': 10000, 'score': 19437.131939172745, 'total_duration': 21034.695421218872, 'accumulated_submission_time': 19437.131939172745, 'accumulated_eval_time': 1587.9949188232422, 'accumulated_logging_time': 4.12834095954895}
I0307 08:17:35.144543 140036192331520 logging_writer.py:48] [50211] accumulated_eval_time=1587.99, accumulated_logging_time=4.12834, accumulated_submission_time=19437.1, global_step=50211, preemption_count=0, score=19437.1, test/accuracy=0.5398, test/loss=2.19546, test/num_examples=10000, total_duration=21034.7, train/accuracy=0.736687, train/loss=1.22992, validation/accuracy=0.6651, validation/loss=1.55129, validation/num_examples=50000
I0307 08:18:09.814591 140036200724224 logging_writer.py:48] [50300] global_step=50300, grad_norm=1.9355114698410034, loss=3.1997227668762207
I0307 08:18:49.035974 140036192331520 logging_writer.py:48] [50400] global_step=50400, grad_norm=1.747436761856079, loss=3.199627161026001
I0307 08:19:27.568286 140036200724224 logging_writer.py:48] [50500] global_step=50500, grad_norm=1.8895683288574219, loss=3.1288788318634033
I0307 08:20:06.324019 140036192331520 logging_writer.py:48] [50600] global_step=50600, grad_norm=1.6401983499526978, loss=3.1509041786193848
I0307 08:20:44.985545 140036200724224 logging_writer.py:48] [50700] global_step=50700, grad_norm=1.974726676940918, loss=3.146662950515747
I0307 08:21:24.470111 140036192331520 logging_writer.py:48] [50800] global_step=50800, grad_norm=1.7091730833053589, loss=3.2206311225891113
I0307 08:22:03.607015 140036200724224 logging_writer.py:48] [50900] global_step=50900, grad_norm=1.7145576477050781, loss=3.167593479156494
I0307 08:22:42.614946 140036192331520 logging_writer.py:48] [51000] global_step=51000, grad_norm=2.0342116355895996, loss=3.234703779220581
I0307 08:23:22.431094 140036200724224 logging_writer.py:48] [51100] global_step=51100, grad_norm=1.7432665824890137, loss=3.213446617126465
I0307 08:24:02.704534 140036192331520 logging_writer.py:48] [51200] global_step=51200, grad_norm=1.7464463710784912, loss=3.1634600162506104
I0307 08:24:42.049443 140036200724224 logging_writer.py:48] [51300] global_step=51300, grad_norm=1.8576005697250366, loss=3.050759792327881
I0307 08:25:20.725554 140036192331520 logging_writer.py:48] [51400] global_step=51400, grad_norm=1.7550151348114014, loss=3.106656312942505
I0307 08:25:59.948673 140036200724224 logging_writer.py:48] [51500] global_step=51500, grad_norm=1.8068503141403198, loss=3.1059439182281494
I0307 08:26:05.359683 140191611933888 spec.py:321] Evaluating on the training split.
I0307 08:26:17.687497 140191611933888 spec.py:333] Evaluating on the validation split.
I0307 08:26:40.280334 140191611933888 spec.py:349] Evaluating on the test split.
I0307 08:26:42.068332 140191611933888 submission_runner.py:469] Time since start: 21581.72s, 	Step: 51515, 	{'train/accuracy': 0.7292928695678711, 'train/loss': 1.2655597925186157, 'validation/accuracy': 0.6610599756240845, 'validation/loss': 1.5827680826187134, 'validation/num_examples': 50000, 'test/accuracy': 0.5331000089645386, 'test/loss': 2.243577241897583, 'test/num_examples': 10000, 'score': 19947.166453123093, 'total_duration': 21581.721972703934, 'accumulated_submission_time': 19947.166453123093, 'accumulated_eval_time': 1624.703408241272, 'accumulated_logging_time': 4.268998384475708}
I0307 08:26:42.149294 140036192331520 logging_writer.py:48] [51515] accumulated_eval_time=1624.7, accumulated_logging_time=4.269, accumulated_submission_time=19947.2, global_step=51515, preemption_count=0, score=19947.2, test/accuracy=0.5331, test/loss=2.24358, test/num_examples=10000, total_duration=21581.7, train/accuracy=0.729293, train/loss=1.26556, validation/accuracy=0.66106, validation/loss=1.58277, validation/num_examples=50000
I0307 08:27:16.578023 140036200724224 logging_writer.py:48] [51600] global_step=51600, grad_norm=1.6980631351470947, loss=3.0691397190093994
I0307 08:27:56.397200 140036192331520 logging_writer.py:48] [51700] global_step=51700, grad_norm=1.752669095993042, loss=3.2683517932891846
I0307 08:28:35.569706 140036200724224 logging_writer.py:48] [51800] global_step=51800, grad_norm=1.7814582586288452, loss=3.126950740814209
I0307 08:29:15.362671 140036192331520 logging_writer.py:48] [51900] global_step=51900, grad_norm=1.8222687244415283, loss=3.123978853225708
I0307 08:29:55.528182 140036200724224 logging_writer.py:48] [52000] global_step=52000, grad_norm=1.9421167373657227, loss=3.1819136142730713
I0307 08:30:35.468359 140036192331520 logging_writer.py:48] [52100] global_step=52100, grad_norm=1.8242803812026978, loss=3.098921060562134
I0307 08:31:15.781729 140036200724224 logging_writer.py:48] [52200] global_step=52200, grad_norm=1.7892296314239502, loss=3.1293787956237793
I0307 08:31:55.490474 140036192331520 logging_writer.py:48] [52300] global_step=52300, grad_norm=1.8806536197662354, loss=3.267202138900757
I0307 08:32:50.063891 140036200724224 logging_writer.py:48] [52400] global_step=52400, grad_norm=1.6386985778808594, loss=3.0349576473236084
I0307 08:33:30.065499 140036192331520 logging_writer.py:48] [52500] global_step=52500, grad_norm=1.8151640892028809, loss=3.110822916030884
2025-03-07 08:33:38.392515: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 08:34:10.471148 140036200724224 logging_writer.py:48] [52600] global_step=52600, grad_norm=1.8895118236541748, loss=3.159183979034424
I0307 08:34:50.934880 140036192331520 logging_writer.py:48] [52700] global_step=52700, grad_norm=1.8992629051208496, loss=3.1729676723480225
I0307 08:35:12.283613 140191611933888 spec.py:321] Evaluating on the training split.
I0307 08:35:24.461482 140191611933888 spec.py:333] Evaluating on the validation split.
I0307 08:35:45.947440 140191611933888 spec.py:349] Evaluating on the test split.
I0307 08:35:47.745952 140191611933888 submission_runner.py:469] Time since start: 22127.40s, 	Step: 52754, 	{'train/accuracy': 0.741629421710968, 'train/loss': 1.247258186340332, 'validation/accuracy': 0.6700400114059448, 'validation/loss': 1.5683972835540771, 'validation/num_examples': 50000, 'test/accuracy': 0.5378000140190125, 'test/loss': 2.2395901679992676, 'test/num_examples': 10000, 'score': 20457.109797239304, 'total_duration': 22127.39958548546, 'accumulated_submission_time': 20457.109797239304, 'accumulated_eval_time': 1660.1655759811401, 'accumulated_logging_time': 4.396728038787842}
I0307 08:35:47.885421 140036200724224 logging_writer.py:48] [52754] accumulated_eval_time=1660.17, accumulated_logging_time=4.39673, accumulated_submission_time=20457.1, global_step=52754, preemption_count=0, score=20457.1, test/accuracy=0.5378, test/loss=2.23959, test/num_examples=10000, total_duration=22127.4, train/accuracy=0.741629, train/loss=1.24726, validation/accuracy=0.67004, validation/loss=1.5684, validation/num_examples=50000
I0307 08:36:06.770372 140036192331520 logging_writer.py:48] [52800] global_step=52800, grad_norm=1.7691609859466553, loss=3.1359596252441406
I0307 08:36:47.414717 140036200724224 logging_writer.py:48] [52900] global_step=52900, grad_norm=1.7785032987594604, loss=3.2155580520629883
I0307 08:37:27.939177 140036192331520 logging_writer.py:48] [53000] global_step=53000, grad_norm=1.7835904359817505, loss=3.2528700828552246
I0307 08:38:08.571953 140036200724224 logging_writer.py:48] [53100] global_step=53100, grad_norm=1.7508881092071533, loss=3.1714634895324707
I0307 08:38:49.191106 140036192331520 logging_writer.py:48] [53200] global_step=53200, grad_norm=1.8869378566741943, loss=3.265096664428711
I0307 08:39:30.666119 140036200724224 logging_writer.py:48] [53300] global_step=53300, grad_norm=1.8139671087265015, loss=3.1441569328308105
I0307 08:40:12.083110 140036192331520 logging_writer.py:48] [53400] global_step=53400, grad_norm=1.8129711151123047, loss=3.1314399242401123
I0307 08:40:52.215363 140036200724224 logging_writer.py:48] [53500] global_step=53500, grad_norm=1.8405996561050415, loss=3.1156203746795654
I0307 08:41:39.956320 140036192331520 logging_writer.py:48] [53600] global_step=53600, grad_norm=2.227163076400757, loss=3.129753351211548
I0307 08:42:30.596613 140036200724224 logging_writer.py:48] [53700] global_step=53700, grad_norm=1.769960880279541, loss=3.1887786388397217
I0307 08:43:11.562571 140036192331520 logging_writer.py:48] [53800] global_step=53800, grad_norm=1.8810979127883911, loss=3.165715456008911
I0307 08:43:51.404699 140036200724224 logging_writer.py:48] [53900] global_step=53900, grad_norm=1.863127589225769, loss=3.186863660812378
I0307 08:44:18.118936 140191611933888 spec.py:321] Evaluating on the training split.
I0307 08:44:30.065795 140191611933888 spec.py:333] Evaluating on the validation split.
I0307 08:44:54.471138 140191611933888 spec.py:349] Evaluating on the test split.
I0307 08:44:56.247849 140191611933888 submission_runner.py:469] Time since start: 22675.90s, 	Step: 53967, 	{'train/accuracy': 0.7393972873687744, 'train/loss': 1.2260617017745972, 'validation/accuracy': 0.6633599996566772, 'validation/loss': 1.5694737434387207, 'validation/num_examples': 50000, 'test/accuracy': 0.5368000268936157, 'test/loss': 2.219684362411499, 'test/num_examples': 10000, 'score': 20967.162478923798, 'total_duration': 22675.901480913162, 'accumulated_submission_time': 20967.162478923798, 'accumulated_eval_time': 1698.2943212985992, 'accumulated_logging_time': 4.577424764633179}
I0307 08:44:56.331073 140036192331520 logging_writer.py:48] [53967] accumulated_eval_time=1698.29, accumulated_logging_time=4.57742, accumulated_submission_time=20967.2, global_step=53967, preemption_count=0, score=20967.2, test/accuracy=0.5368, test/loss=2.21968, test/num_examples=10000, total_duration=22675.9, train/accuracy=0.739397, train/loss=1.22606, validation/accuracy=0.66336, validation/loss=1.56947, validation/num_examples=50000
I0307 08:45:10.047022 140036200724224 logging_writer.py:48] [54000] global_step=54000, grad_norm=1.8127490282058716, loss=3.1476120948791504
I0307 08:45:50.292417 140036192331520 logging_writer.py:48] [54100] global_step=54100, grad_norm=1.81476891040802, loss=3.1072676181793213
I0307 08:46:30.762517 140036200724224 logging_writer.py:48] [54200] global_step=54200, grad_norm=1.8339341878890991, loss=3.129082679748535
I0307 08:47:11.417356 140036192331520 logging_writer.py:48] [54300] global_step=54300, grad_norm=1.6758015155792236, loss=3.0686657428741455
I0307 08:47:52.120164 140036200724224 logging_writer.py:48] [54400] global_step=54400, grad_norm=1.910441517829895, loss=3.167316198348999
I0307 08:48:31.922387 140036192331520 logging_writer.py:48] [54500] global_step=54500, grad_norm=1.7017492055892944, loss=3.126890182495117
I0307 08:49:13.952572 140036200724224 logging_writer.py:48] [54600] global_step=54600, grad_norm=1.8846938610076904, loss=3.160696268081665
I0307 08:49:54.767897 140036192331520 logging_writer.py:48] [54700] global_step=54700, grad_norm=1.754833459854126, loss=3.0820937156677246
I0307 08:50:33.862822 140036200724224 logging_writer.py:48] [54800] global_step=54800, grad_norm=1.8669028282165527, loss=3.1664764881134033
I0307 08:51:16.971004 140036192331520 logging_writer.py:48] [54900] global_step=54900, grad_norm=1.9337477684020996, loss=3.1521832942962646
I0307 08:51:57.011857 140036200724224 logging_writer.py:48] [55000] global_step=55000, grad_norm=1.8573448657989502, loss=3.1398963928222656
I0307 08:52:37.178986 140036192331520 logging_writer.py:48] [55100] global_step=55100, grad_norm=1.885849952697754, loss=3.1324398517608643
I0307 08:53:17.494596 140036200724224 logging_writer.py:48] [55200] global_step=55200, grad_norm=1.9611103534698486, loss=3.201672077178955
I0307 08:53:26.611194 140191611933888 spec.py:321] Evaluating on the training split.
I0307 08:53:38.648015 140191611933888 spec.py:333] Evaluating on the validation split.
I0307 08:53:59.247320 140191611933888 spec.py:349] Evaluating on the test split.
I0307 08:54:01.031234 140191611933888 submission_runner.py:469] Time since start: 23220.68s, 	Step: 55224, 	{'train/accuracy': 0.7392578125, 'train/loss': 1.2593649625778198, 'validation/accuracy': 0.663379967212677, 'validation/loss': 1.5964614152908325, 'validation/num_examples': 50000, 'test/accuracy': 0.5317000150680542, 'test/loss': 2.276043653488159, 'test/num_examples': 10000, 'score': 21477.251506567, 'total_duration': 23220.684890031815, 'accumulated_submission_time': 21477.251506567, 'accumulated_eval_time': 1732.7142033576965, 'accumulated_logging_time': 4.702350616455078}
I0307 08:54:01.112678 140036192331520 logging_writer.py:48] [55224] accumulated_eval_time=1732.71, accumulated_logging_time=4.70235, accumulated_submission_time=21477.3, global_step=55224, preemption_count=0, score=21477.3, test/accuracy=0.5317, test/loss=2.27604, test/num_examples=10000, total_duration=23220.7, train/accuracy=0.739258, train/loss=1.25936, validation/accuracy=0.66338, validation/loss=1.59646, validation/num_examples=50000
I0307 08:54:31.754531 140036200724224 logging_writer.py:48] [55300] global_step=55300, grad_norm=1.8569920063018799, loss=3.136094093322754
I0307 08:55:12.009041 140036192331520 logging_writer.py:48] [55400] global_step=55400, grad_norm=1.8660095930099487, loss=3.0974559783935547
I0307 08:55:52.401430 140036200724224 logging_writer.py:48] [55500] global_step=55500, grad_norm=2.0456063747406006, loss=3.1022565364837646
I0307 08:56:32.749734 140036192331520 logging_writer.py:48] [55600] global_step=55600, grad_norm=1.7197532653808594, loss=3.149444103240967
I0307 08:57:12.858998 140036200724224 logging_writer.py:48] [55700] global_step=55700, grad_norm=1.8561828136444092, loss=3.1459174156188965
I0307 08:57:53.329745 140036192331520 logging_writer.py:48] [55800] global_step=55800, grad_norm=2.036968231201172, loss=3.101922035217285
I0307 08:58:34.139662 140036200724224 logging_writer.py:48] [55900] global_step=55900, grad_norm=2.0401525497436523, loss=3.121892213821411
I0307 08:59:14.076943 140036192331520 logging_writer.py:48] [56000] global_step=56000, grad_norm=1.7763506174087524, loss=3.1072628498077393
I0307 08:59:55.755604 140036200724224 logging_writer.py:48] [56100] global_step=56100, grad_norm=1.8446953296661377, loss=3.138298273086548
I0307 09:00:51.452593 140036192331520 logging_writer.py:48] [56200] global_step=56200, grad_norm=1.9348350763320923, loss=3.1170215606689453
I0307 09:01:49.799448 140036200724224 logging_writer.py:48] [56300] global_step=56300, grad_norm=1.8133418560028076, loss=3.123444080352783
I0307 09:02:31.863162 140191611933888 spec.py:321] Evaluating on the training split.
I0307 09:02:45.602651 140191611933888 spec.py:333] Evaluating on the validation split.
I0307 09:03:05.621419 140191611933888 spec.py:349] Evaluating on the test split.
I0307 09:03:07.397544 140191611933888 submission_runner.py:469] Time since start: 23767.05s, 	Step: 56328, 	{'train/accuracy': 0.750398576259613, 'train/loss': 1.2322849035263062, 'validation/accuracy': 0.6684600114822388, 'validation/loss': 1.5972193479537964, 'validation/num_examples': 50000, 'test/accuracy': 0.5393000245094299, 'test/loss': 2.245885133743286, 'test/num_examples': 10000, 'score': 21987.854191303253, 'total_duration': 23767.051189899445, 'accumulated_submission_time': 21987.854191303253, 'accumulated_eval_time': 1768.2484197616577, 'accumulated_logging_time': 4.803948163986206}
I0307 09:03:07.504197 140036192331520 logging_writer.py:48] [56328] accumulated_eval_time=1768.25, accumulated_logging_time=4.80395, accumulated_submission_time=21987.9, global_step=56328, preemption_count=0, score=21987.9, test/accuracy=0.5393, test/loss=2.24589, test/num_examples=10000, total_duration=23767.1, train/accuracy=0.750399, train/loss=1.23228, validation/accuracy=0.66846, validation/loss=1.59722, validation/num_examples=50000
I0307 09:03:36.501049 140036200724224 logging_writer.py:48] [56400] global_step=56400, grad_norm=1.7593992948532104, loss=3.140389919281006
I0307 09:04:16.288299 140036192331520 logging_writer.py:48] [56500] global_step=56500, grad_norm=2.0551271438598633, loss=3.0261995792388916
I0307 09:05:00.303867 140036200724224 logging_writer.py:48] [56600] global_step=56600, grad_norm=1.991762399673462, loss=3.159548044204712
I0307 09:05:50.987547 140036192331520 logging_writer.py:48] [56700] global_step=56700, grad_norm=1.74550199508667, loss=3.161444902420044
I0307 09:06:33.121697 140036200724224 logging_writer.py:48] [56800] global_step=56800, grad_norm=1.9391344785690308, loss=3.13594388961792
I0307 09:07:18.963025 140036192331520 logging_writer.py:48] [56900] global_step=56900, grad_norm=1.9793366193771362, loss=3.0689611434936523
I0307 09:08:01.210857 140036200724224 logging_writer.py:48] [57000] global_step=57000, grad_norm=1.9717528820037842, loss=3.1107749938964844
I0307 09:08:41.302643 140036192331520 logging_writer.py:48] [57100] global_step=57100, grad_norm=2.2446417808532715, loss=3.17879056930542
I0307 09:09:28.376637 140036200724224 logging_writer.py:48] [57200] global_step=57200, grad_norm=1.877581000328064, loss=3.092285394668579
I0307 09:10:19.996418 140036192331520 logging_writer.py:48] [57300] global_step=57300, grad_norm=1.9786916971206665, loss=3.1375575065612793
I0307 09:11:23.995876 140036200724224 logging_writer.py:48] [57400] global_step=57400, grad_norm=2.1799519062042236, loss=3.1026041507720947
I0307 09:11:37.797023 140191611933888 spec.py:321] Evaluating on the training split.
I0307 09:11:51.284211 140191611933888 spec.py:333] Evaluating on the validation split.
I0307 09:12:14.998279 140191611933888 spec.py:349] Evaluating on the test split.
I0307 09:12:16.749066 140191611933888 submission_runner.py:469] Time since start: 24316.40s, 	Step: 57432, 	{'train/accuracy': 0.7433235049247742, 'train/loss': 1.2292670011520386, 'validation/accuracy': 0.6522200107574463, 'validation/loss': 1.6248177289962769, 'validation/num_examples': 50000, 'test/accuracy': 0.5308000445365906, 'test/loss': 2.26238751411438, 'test/num_examples': 10000, 'score': 22497.98379802704, 'total_duration': 24316.402676343918, 'accumulated_submission_time': 22497.98379802704, 'accumulated_eval_time': 1807.200285434723, 'accumulated_logging_time': 4.943390369415283}
I0307 09:12:16.832024 140036192331520 logging_writer.py:48] [57432] accumulated_eval_time=1807.2, accumulated_logging_time=4.94339, accumulated_submission_time=22498, global_step=57432, preemption_count=0, score=22498, test/accuracy=0.5308, test/loss=2.26239, test/num_examples=10000, total_duration=24316.4, train/accuracy=0.743324, train/loss=1.22927, validation/accuracy=0.65222, validation/loss=1.62482, validation/num_examples=50000
I0307 09:12:44.164864 140036200724224 logging_writer.py:48] [57500] global_step=57500, grad_norm=1.979993224143982, loss=3.1645431518554688
I0307 09:13:25.723012 140036192331520 logging_writer.py:48] [57600] global_step=57600, grad_norm=1.8595688343048096, loss=3.083462715148926
I0307 09:14:06.804615 140036200724224 logging_writer.py:48] [57700] global_step=57700, grad_norm=1.9525339603424072, loss=3.1337578296661377
I0307 09:14:46.506929 140036192331520 logging_writer.py:48] [57800] global_step=57800, grad_norm=1.8824902772903442, loss=3.1538236141204834
I0307 09:15:27.666224 140036200724224 logging_writer.py:48] [57900] global_step=57900, grad_norm=1.855063796043396, loss=3.008816719055176
I0307 09:16:08.258258 140036192331520 logging_writer.py:48] [58000] global_step=58000, grad_norm=2.0233314037323, loss=3.1592841148376465
I0307 09:16:45.910815 140036200724224 logging_writer.py:48] [58100] global_step=58100, grad_norm=1.9667469263076782, loss=3.1052210330963135
I0307 09:17:24.207362 140036192331520 logging_writer.py:48] [58200] global_step=58200, grad_norm=2.0770576000213623, loss=3.1196296215057373
I0307 09:18:02.500849 140036200724224 logging_writer.py:48] [58300] global_step=58300, grad_norm=1.8632785081863403, loss=3.0843019485473633
I0307 09:18:40.670324 140036192331520 logging_writer.py:48] [58400] global_step=58400, grad_norm=1.9473209381103516, loss=3.145622491836548
I0307 09:19:19.007839 140036200724224 logging_writer.py:48] [58500] global_step=58500, grad_norm=1.9083267450332642, loss=3.1559696197509766
I0307 09:19:59.840748 140036192331520 logging_writer.py:48] [58600] global_step=58600, grad_norm=1.8051646947860718, loss=3.077989339828491
I0307 09:20:43.015848 140036200724224 logging_writer.py:48] [58700] global_step=58700, grad_norm=1.9453086853027344, loss=3.1562371253967285
I0307 09:20:46.931158 140191611933888 spec.py:321] Evaluating on the training split.
I0307 09:20:59.749398 140191611933888 spec.py:333] Evaluating on the validation split.
I0307 09:21:18.455236 140191611933888 spec.py:349] Evaluating on the test split.
I0307 09:21:20.245577 140191611933888 submission_runner.py:469] Time since start: 24859.90s, 	Step: 58711, 	{'train/accuracy': 0.7578921914100647, 'train/loss': 1.2047712802886963, 'validation/accuracy': 0.6647199988365173, 'validation/loss': 1.5992448329925537, 'validation/num_examples': 50000, 'test/accuracy': 0.5400000214576721, 'test/loss': 2.246666431427002, 'test/num_examples': 10000, 'score': 23007.906752824783, 'total_duration': 24859.899211645126, 'accumulated_submission_time': 23007.906752824783, 'accumulated_eval_time': 1840.514530658722, 'accumulated_logging_time': 5.05224871635437}
I0307 09:21:20.379625 140036192331520 logging_writer.py:48] [58711] accumulated_eval_time=1840.51, accumulated_logging_time=5.05225, accumulated_submission_time=23007.9, global_step=58711, preemption_count=0, score=23007.9, test/accuracy=0.54, test/loss=2.24667, test/num_examples=10000, total_duration=24859.9, train/accuracy=0.757892, train/loss=1.20477, validation/accuracy=0.66472, validation/loss=1.59924, validation/num_examples=50000
2025-03-07 09:21:50.891746: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 09:21:56.716317 140036200724224 logging_writer.py:48] [58800] global_step=58800, grad_norm=1.8608527183532715, loss=3.1088767051696777
I0307 09:22:35.610583 140036192331520 logging_writer.py:48] [58900] global_step=58900, grad_norm=1.872820258140564, loss=3.121432065963745
I0307 09:23:15.463942 140036200724224 logging_writer.py:48] [59000] global_step=59000, grad_norm=1.933131217956543, loss=3.1609387397766113
I0307 09:23:53.309180 140036192331520 logging_writer.py:48] [59100] global_step=59100, grad_norm=1.9460679292678833, loss=3.1177494525909424
I0307 09:24:31.534147 140036200724224 logging_writer.py:48] [59200] global_step=59200, grad_norm=1.8769782781600952, loss=3.1261446475982666
I0307 09:25:40.118362 140036192331520 logging_writer.py:48] [59300] global_step=59300, grad_norm=2.0269088745117188, loss=3.0272321701049805
I0307 09:26:35.500531 140036200724224 logging_writer.py:48] [59400] global_step=59400, grad_norm=1.774403691291809, loss=3.13759446144104
I0307 09:27:20.932830 140036192331520 logging_writer.py:48] [59500] global_step=59500, grad_norm=1.9229848384857178, loss=3.1835708618164062
I0307 09:28:03.544813 140036200724224 logging_writer.py:48] [59600] global_step=59600, grad_norm=2.0891542434692383, loss=3.1317615509033203
I0307 09:28:44.669514 140036192331520 logging_writer.py:48] [59700] global_step=59700, grad_norm=1.9058088064193726, loss=3.087697744369507
I0307 09:29:25.004628 140036200724224 logging_writer.py:48] [59800] global_step=59800, grad_norm=2.135218381881714, loss=3.095057487487793
I0307 09:29:50.532600 140191611933888 spec.py:321] Evaluating on the training split.
I0307 09:30:04.452119 140191611933888 spec.py:333] Evaluating on the validation split.
I0307 09:30:22.714370 140191611933888 spec.py:349] Evaluating on the test split.
I0307 09:30:24.497698 140191611933888 submission_runner.py:469] Time since start: 25404.15s, 	Step: 59852, 	{'train/accuracy': 0.7736766338348389, 'train/loss': 1.0858649015426636, 'validation/accuracy': 0.6649999618530273, 'validation/loss': 1.5545696020126343, 'validation/num_examples': 50000, 'test/accuracy': 0.5404000282287598, 'test/loss': 2.2117249965667725, 'test/num_examples': 10000, 'score': 23517.882717609406, 'total_duration': 25404.1510720253, 'accumulated_submission_time': 23517.882717609406, 'accumulated_eval_time': 1874.4791932106018, 'accumulated_logging_time': 5.232440233230591}
I0307 09:30:24.621075 140036192331520 logging_writer.py:48] [59852] accumulated_eval_time=1874.48, accumulated_logging_time=5.23244, accumulated_submission_time=23517.9, global_step=59852, preemption_count=0, score=23517.9, test/accuracy=0.5404, test/loss=2.21172, test/num_examples=10000, total_duration=25404.2, train/accuracy=0.773677, train/loss=1.08586, validation/accuracy=0.665, validation/loss=1.55457, validation/num_examples=50000
I0307 09:30:45.121282 140036200724224 logging_writer.py:48] [59900] global_step=59900, grad_norm=2.0599400997161865, loss=3.158055543899536
I0307 09:31:24.130089 140036192331520 logging_writer.py:48] [60000] global_step=60000, grad_norm=1.9242439270019531, loss=3.146754264831543
I0307 09:32:06.019880 140036200724224 logging_writer.py:48] [60100] global_step=60100, grad_norm=1.9865105152130127, loss=3.2040750980377197
I0307 09:32:44.938260 140036192331520 logging_writer.py:48] [60200] global_step=60200, grad_norm=1.907341718673706, loss=3.132591724395752
I0307 09:33:25.439438 140036200724224 logging_writer.py:48] [60300] global_step=60300, grad_norm=1.8771840333938599, loss=3.1610445976257324
I0307 09:34:07.926769 140036192331520 logging_writer.py:48] [60400] global_step=60400, grad_norm=2.023411273956299, loss=3.161221981048584
I0307 09:34:59.052463 140036200724224 logging_writer.py:48] [60500] global_step=60500, grad_norm=1.9171273708343506, loss=3.103693962097168
I0307 09:35:51.440311 140036192331520 logging_writer.py:48] [60600] global_step=60600, grad_norm=1.957785725593567, loss=3.0606164932250977
I0307 09:36:37.097967 140036200724224 logging_writer.py:48] [60700] global_step=60700, grad_norm=1.8995637893676758, loss=3.04779314994812
I0307 09:37:23.256254 140036192331520 logging_writer.py:48] [60800] global_step=60800, grad_norm=1.8633170127868652, loss=3.1024770736694336
I0307 09:38:08.308658 140036200724224 logging_writer.py:48] [60900] global_step=60900, grad_norm=1.9046449661254883, loss=3.1893093585968018
I0307 09:38:53.003780 140036192331520 logging_writer.py:48] [61000] global_step=61000, grad_norm=1.8852766752243042, loss=3.155717372894287
I0307 09:38:54.610386 140191611933888 spec.py:321] Evaluating on the training split.
I0307 09:39:07.802578 140191611933888 spec.py:333] Evaluating on the validation split.
I0307 09:39:25.853580 140191611933888 spec.py:349] Evaluating on the test split.
I0307 09:39:27.658664 140191611933888 submission_runner.py:469] Time since start: 25947.31s, 	Step: 61005, 	{'train/accuracy': 0.7468510866165161, 'train/loss': 1.2288507223129272, 'validation/accuracy': 0.6705399751663208, 'validation/loss': 1.5565474033355713, 'validation/num_examples': 50000, 'test/accuracy': 0.549500048160553, 'test/loss': 2.184891700744629, 'test/num_examples': 10000, 'score': 24027.689358711243, 'total_duration': 25947.31233048439, 'accumulated_submission_time': 24027.689358711243, 'accumulated_eval_time': 1907.5273263454437, 'accumulated_logging_time': 5.403896331787109}
I0307 09:39:27.751563 140036200724224 logging_writer.py:48] [61005] accumulated_eval_time=1907.53, accumulated_logging_time=5.4039, accumulated_submission_time=24027.7, global_step=61005, preemption_count=0, score=24027.7, test/accuracy=0.5495, test/loss=2.18489, test/num_examples=10000, total_duration=25947.3, train/accuracy=0.746851, train/loss=1.22885, validation/accuracy=0.67054, validation/loss=1.55655, validation/num_examples=50000
I0307 09:40:12.900323 140036192331520 logging_writer.py:48] [61100] global_step=61100, grad_norm=2.065922975540161, loss=3.13960337638855
I0307 09:41:02.227735 140036200724224 logging_writer.py:48] [61200] global_step=61200, grad_norm=1.9924513101577759, loss=3.021160364151001
2025-03-07 09:41:34.393641: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 09:41:43.012046 140036192331520 logging_writer.py:48] [61300] global_step=61300, grad_norm=1.9706414937973022, loss=3.1004185676574707
I0307 09:42:23.280863 140036200724224 logging_writer.py:48] [61400] global_step=61400, grad_norm=1.8893438577651978, loss=3.1275832653045654
I0307 09:43:03.049344 140036192331520 logging_writer.py:48] [61500] global_step=61500, grad_norm=2.001224994659424, loss=3.142090320587158
I0307 09:43:42.871146 140036200724224 logging_writer.py:48] [61600] global_step=61600, grad_norm=1.925745964050293, loss=3.1188440322875977
I0307 09:44:23.117711 140036192331520 logging_writer.py:48] [61700] global_step=61700, grad_norm=1.9040391445159912, loss=3.089799642562866
I0307 09:45:12.510761 140036200724224 logging_writer.py:48] [61800] global_step=61800, grad_norm=1.9236606359481812, loss=3.1038942337036133
I0307 09:46:01.908133 140036192331520 logging_writer.py:48] [61900] global_step=61900, grad_norm=2.0498270988464355, loss=3.152245044708252
I0307 09:47:16.695570 140036200724224 logging_writer.py:48] [62000] global_step=62000, grad_norm=2.163689613342285, loss=3.1373298168182373
I0307 09:47:57.761106 140191611933888 spec.py:321] Evaluating on the training split.
I0307 09:48:11.299689 140191611933888 spec.py:333] Evaluating on the validation split.
I0307 09:48:28.363387 140191611933888 spec.py:349] Evaluating on the test split.
I0307 09:48:30.146324 140191611933888 submission_runner.py:469] Time since start: 26489.80s, 	Step: 62058, 	{'train/accuracy': 0.7456353306770325, 'train/loss': 1.2305089235305786, 'validation/accuracy': 0.6714400053024292, 'validation/loss': 1.5547122955322266, 'validation/num_examples': 50000, 'test/accuracy': 0.5471000075340271, 'test/loss': 2.1882545948028564, 'test/num_examples': 10000, 'score': 24537.553835868835, 'total_duration': 26489.79996585846, 'accumulated_submission_time': 24537.553835868835, 'accumulated_eval_time': 1939.9123837947845, 'accumulated_logging_time': 5.52220892906189}
I0307 09:48:30.202512 140036192331520 logging_writer.py:48] [62058] accumulated_eval_time=1939.91, accumulated_logging_time=5.52221, accumulated_submission_time=24537.6, global_step=62058, preemption_count=0, score=24537.6, test/accuracy=0.5471, test/loss=2.18825, test/num_examples=10000, total_duration=26489.8, train/accuracy=0.745635, train/loss=1.23051, validation/accuracy=0.67144, validation/loss=1.55471, validation/num_examples=50000
I0307 09:48:47.439078 140036200724224 logging_writer.py:48] [62100] global_step=62100, grad_norm=2.1408488750457764, loss=3.144274950027466
I0307 09:49:27.251240 140036192331520 logging_writer.py:48] [62200] global_step=62200, grad_norm=1.9068725109100342, loss=3.075742244720459
I0307 09:50:22.690562 140036200724224 logging_writer.py:48] [62300] global_step=62300, grad_norm=2.0171380043029785, loss=3.1524460315704346
I0307 09:51:20.313351 140036192331520 logging_writer.py:48] [62400] global_step=62400, grad_norm=1.9711920022964478, loss=3.0660006999969482
I0307 09:52:04.801089 140036200724224 logging_writer.py:48] [62500] global_step=62500, grad_norm=2.3134567737579346, loss=3.0581507682800293
I0307 09:52:53.535567 140036192331520 logging_writer.py:48] [62600] global_step=62600, grad_norm=1.9331172704696655, loss=3.000835418701172
I0307 09:53:39.360248 140036200724224 logging_writer.py:48] [62700] global_step=62700, grad_norm=2.2016828060150146, loss=3.1944773197174072
I0307 09:54:23.730453 140036192331520 logging_writer.py:48] [62800] global_step=62800, grad_norm=1.8915835618972778, loss=3.141993999481201
I0307 09:55:04.011050 140036200724224 logging_writer.py:48] [62900] global_step=62900, grad_norm=1.8493015766143799, loss=3.093071222305298
I0307 09:55:49.823421 140036192331520 logging_writer.py:48] [63000] global_step=63000, grad_norm=1.9816466569900513, loss=3.1242480278015137
I0307 09:56:54.938717 140036200724224 logging_writer.py:48] [63100] global_step=63100, grad_norm=2.0528783798217773, loss=3.1035799980163574
I0307 09:57:00.248692 140191611933888 spec.py:321] Evaluating on the training split.
I0307 09:57:13.961134 140191611933888 spec.py:333] Evaluating on the validation split.
I0307 09:57:31.595902 140191611933888 spec.py:349] Evaluating on the test split.
I0307 09:57:33.378004 140191611933888 submission_runner.py:469] Time since start: 27033.03s, 	Step: 63111, 	{'train/accuracy': 0.7518534660339355, 'train/loss': 1.1656666994094849, 'validation/accuracy': 0.671779990196228, 'validation/loss': 1.5303453207015991, 'validation/num_examples': 50000, 'test/accuracy': 0.5543000102043152, 'test/loss': 2.1460766792297363, 'test/num_examples': 10000, 'score': 25047.454434156418, 'total_duration': 27033.031614780426, 'accumulated_submission_time': 25047.454434156418, 'accumulated_eval_time': 1973.0414950847626, 'accumulated_logging_time': 5.605474472045898}
I0307 09:57:33.475342 140036192331520 logging_writer.py:48] [63111] accumulated_eval_time=1973.04, accumulated_logging_time=5.60547, accumulated_submission_time=25047.5, global_step=63111, preemption_count=0, score=25047.5, test/accuracy=0.5543, test/loss=2.14608, test/num_examples=10000, total_duration=27033, train/accuracy=0.751853, train/loss=1.16567, validation/accuracy=0.67178, validation/loss=1.53035, validation/num_examples=50000
I0307 09:58:10.364327 140036200724224 logging_writer.py:48] [63200] global_step=63200, grad_norm=1.8038933277130127, loss=3.123014450073242
I0307 09:58:55.963815 140036192331520 logging_writer.py:48] [63300] global_step=63300, grad_norm=2.117335081100464, loss=3.0961458683013916
I0307 09:59:40.143818 140036200724224 logging_writer.py:48] [63400] global_step=63400, grad_norm=2.088305950164795, loss=3.0999693870544434
I0307 10:00:20.416030 140036192331520 logging_writer.py:48] [63500] global_step=63500, grad_norm=1.977154016494751, loss=3.0797433853149414
I0307 10:01:08.329128 140036200724224 logging_writer.py:48] [63600] global_step=63600, grad_norm=1.8380329608917236, loss=3.146437406539917
I0307 10:01:58.642341 140036192331520 logging_writer.py:48] [63700] global_step=63700, grad_norm=1.998011827468872, loss=3.1164588928222656
I0307 10:02:40.737762 140036200724224 logging_writer.py:48] [63800] global_step=63800, grad_norm=2.0239200592041016, loss=3.0859339237213135
I0307 10:03:25.044529 140036192331520 logging_writer.py:48] [63900] global_step=63900, grad_norm=2.184295892715454, loss=3.1169252395629883
I0307 10:04:10.077472 140036200724224 logging_writer.py:48] [64000] global_step=64000, grad_norm=2.0372209548950195, loss=3.1697933673858643
I0307 10:05:01.325363 140036192331520 logging_writer.py:48] [64100] global_step=64100, grad_norm=2.264608383178711, loss=3.0921173095703125
I0307 10:05:57.162138 140036200724224 logging_writer.py:48] [64200] global_step=64200, grad_norm=1.9110221862792969, loss=3.0976858139038086
I0307 10:06:03.527361 140191611933888 spec.py:321] Evaluating on the training split.
I0307 10:06:17.433772 140191611933888 spec.py:333] Evaluating on the validation split.
I0307 10:06:33.198786 140191611933888 spec.py:349] Evaluating on the test split.
I0307 10:06:34.995306 140191611933888 submission_runner.py:469] Time since start: 27574.65s, 	Step: 64213, 	{'train/accuracy': 0.7583904266357422, 'train/loss': 1.1838148832321167, 'validation/accuracy': 0.674780011177063, 'validation/loss': 1.5550591945648193, 'validation/num_examples': 50000, 'test/accuracy': 0.5503000020980835, 'test/loss': 2.1836345195770264, 'test/num_examples': 10000, 'score': 25557.352769374847, 'total_duration': 27574.648943424225, 'accumulated_submission_time': 25557.352769374847, 'accumulated_eval_time': 2004.5092668533325, 'accumulated_logging_time': 5.728713035583496}
I0307 10:06:35.176986 140036192331520 logging_writer.py:48] [64213] accumulated_eval_time=2004.51, accumulated_logging_time=5.72871, accumulated_submission_time=25557.4, global_step=64213, preemption_count=0, score=25557.4, test/accuracy=0.5503, test/loss=2.18363, test/num_examples=10000, total_duration=27574.6, train/accuracy=0.75839, train/loss=1.18381, validation/accuracy=0.67478, validation/loss=1.55506, validation/num_examples=50000
I0307 10:07:23.336241 140036200724224 logging_writer.py:48] [64300] global_step=64300, grad_norm=2.080127716064453, loss=3.1616485118865967
I0307 10:08:09.116276 140036192331520 logging_writer.py:48] [64400] global_step=64400, grad_norm=2.04306960105896, loss=3.1076736450195312
I0307 10:08:52.990601 140036200724224 logging_writer.py:48] [64500] global_step=64500, grad_norm=1.9652963876724243, loss=3.0919270515441895
I0307 10:09:39.866081 140036192331520 logging_writer.py:48] [64600] global_step=64600, grad_norm=1.972427487373352, loss=3.0211844444274902
I0307 10:10:35.023267 140036200724224 logging_writer.py:48] [64700] global_step=64700, grad_norm=2.141444683074951, loss=3.108544111251831
I0307 10:11:33.797096 140036192331520 logging_writer.py:48] [64800] global_step=64800, grad_norm=2.3220431804656982, loss=3.0797529220581055
I0307 10:12:38.599630 140036200724224 logging_writer.py:48] [64900] global_step=64900, grad_norm=1.9941655397415161, loss=3.023207426071167
I0307 10:13:19.625998 140036192331520 logging_writer.py:48] [65000] global_step=65000, grad_norm=1.9740849733352661, loss=3.0996525287628174
I0307 10:14:01.585754 140036200724224 logging_writer.py:48] [65100] global_step=65100, grad_norm=2.0974650382995605, loss=3.0239357948303223
I0307 10:14:41.956949 140036192331520 logging_writer.py:48] [65200] global_step=65200, grad_norm=2.2110304832458496, loss=3.06606388092041
I0307 10:15:05.610323 140191611933888 spec.py:321] Evaluating on the training split.
I0307 10:15:20.176600 140191611933888 spec.py:333] Evaluating on the validation split.
I0307 10:15:36.082018 140191611933888 spec.py:349] Evaluating on the test split.
I0307 10:15:38.030066 140191611933888 submission_runner.py:469] Time since start: 28117.68s, 	Step: 65259, 	{'train/accuracy': 0.7678372263908386, 'train/loss': 1.1303789615631104, 'validation/accuracy': 0.6765599846839905, 'validation/loss': 1.5321184396743774, 'validation/num_examples': 50000, 'test/accuracy': 0.5440000295639038, 'test/loss': 2.196814775466919, 'test/num_examples': 10000, 'score': 26067.63009238243, 'total_duration': 28117.683656454086, 'accumulated_submission_time': 26067.63009238243, 'accumulated_eval_time': 2036.9287922382355, 'accumulated_logging_time': 5.9465131759643555}
I0307 10:15:38.174429 140036200724224 logging_writer.py:48] [65259] accumulated_eval_time=2036.93, accumulated_logging_time=5.94651, accumulated_submission_time=26067.6, global_step=65259, preemption_count=0, score=26067.6, test/accuracy=0.544, test/loss=2.19681, test/num_examples=10000, total_duration=28117.7, train/accuracy=0.767837, train/loss=1.13038, validation/accuracy=0.67656, validation/loss=1.53212, validation/num_examples=50000
I0307 10:15:55.163017 140036192331520 logging_writer.py:48] [65300] global_step=65300, grad_norm=2.026542901992798, loss=3.1524817943573
I0307 10:16:40.452202 140036200724224 logging_writer.py:48] [65400] global_step=65400, grad_norm=1.9075957536697388, loss=3.0505857467651367
I0307 10:17:36.891430 140036192331520 logging_writer.py:48] [65500] global_step=65500, grad_norm=2.02506947517395, loss=3.1450161933898926
I0307 10:18:35.547502 140036200724224 logging_writer.py:48] [65600] global_step=65600, grad_norm=1.9612092971801758, loss=3.181553363800049
I0307 10:19:34.111565 140036192331520 logging_writer.py:48] [65700] global_step=65700, grad_norm=1.9105206727981567, loss=3.0923008918762207
I0307 10:20:20.233596 140036200724224 logging_writer.py:48] [65800] global_step=65800, grad_norm=2.2441930770874023, loss=3.137576103210449
I0307 10:21:13.722485 140036192331520 logging_writer.py:48] [65900] global_step=65900, grad_norm=2.0340452194213867, loss=3.107656478881836
I0307 10:22:04.157638 140036200724224 logging_writer.py:48] [66000] global_step=66000, grad_norm=1.955904245376587, loss=3.06411075592041
I0307 10:22:56.623252 140036192331520 logging_writer.py:48] [66100] global_step=66100, grad_norm=2.180478096008301, loss=3.0789597034454346
I0307 10:23:54.872585 140036200724224 logging_writer.py:48] [66200] global_step=66200, grad_norm=2.2065582275390625, loss=3.106194496154785
I0307 10:24:08.578784 140191611933888 spec.py:321] Evaluating on the training split.
I0307 10:24:22.685837 140191611933888 spec.py:333] Evaluating on the validation split.
I0307 10:24:37.525475 140191611933888 spec.py:349] Evaluating on the test split.
I0307 10:24:39.323813 140191611933888 submission_runner.py:469] Time since start: 28658.98s, 	Step: 66224, 	{'train/accuracy': 0.7456353306770325, 'train/loss': 1.2098084688186646, 'validation/accuracy': 0.6709199547767639, 'validation/loss': 1.533651351928711, 'validation/num_examples': 50000, 'test/accuracy': 0.5453000068664551, 'test/loss': 2.1791269779205322, 'test/num_examples': 10000, 'score': 26577.853446483612, 'total_duration': 28658.977437496185, 'accumulated_submission_time': 26577.853446483612, 'accumulated_eval_time': 2067.6736421585083, 'accumulated_logging_time': 6.165448904037476}
I0307 10:24:39.400643 140036192331520 logging_writer.py:48] [66224] accumulated_eval_time=2067.67, accumulated_logging_time=6.16545, accumulated_submission_time=26577.9, global_step=66224, preemption_count=0, score=26577.9, test/accuracy=0.5453, test/loss=2.17913, test/num_examples=10000, total_duration=28659, train/accuracy=0.745635, train/loss=1.20981, validation/accuracy=0.67092, validation/loss=1.53365, validation/num_examples=50000
I0307 10:25:25.618898 140036200724224 logging_writer.py:48] [66300] global_step=66300, grad_norm=2.228501558303833, loss=3.045475721359253
I0307 10:26:26.858748 140036192331520 logging_writer.py:48] [66400] global_step=66400, grad_norm=1.9748438596725464, loss=3.104794979095459
I0307 10:27:14.227141 140036200724224 logging_writer.py:48] [66500] global_step=66500, grad_norm=2.1997740268707275, loss=3.1382930278778076
I0307 10:28:21.085057 140036192331520 logging_writer.py:48] [66600] global_step=66600, grad_norm=1.9151790142059326, loss=3.053372383117676
I0307 10:29:45.972718 140036200724224 logging_writer.py:48] [66700] global_step=66700, grad_norm=2.0448267459869385, loss=3.0794854164123535
I0307 10:30:33.630799 140036192331520 logging_writer.py:48] [66800] global_step=66800, grad_norm=1.92536199092865, loss=3.0414113998413086
I0307 10:31:43.253808 140036200724224 logging_writer.py:48] [66900] global_step=66900, grad_norm=2.0353424549102783, loss=3.051922082901001
I0307 10:32:25.701238 140036192331520 logging_writer.py:48] [67000] global_step=67000, grad_norm=2.040846824645996, loss=3.190782308578491
I0307 10:33:09.453796 140191611933888 spec.py:321] Evaluating on the training split.
I0307 10:33:23.613518 140191611933888 spec.py:333] Evaluating on the validation split.
I0307 10:33:36.147558 140191611933888 spec.py:349] Evaluating on the test split.
I0307 10:33:37.942851 140191611933888 submission_runner.py:469] Time since start: 29197.60s, 	Step: 67071, 	{'train/accuracy': 0.7593869566917419, 'train/loss': 1.1656389236450195, 'validation/accuracy': 0.6778799891471863, 'validation/loss': 1.5219621658325195, 'validation/num_examples': 50000, 'test/accuracy': 0.54830002784729, 'test/loss': 2.184826612472534, 'test/num_examples': 10000, 'score': 27087.778123617172, 'total_duration': 29197.59646296501, 'accumulated_submission_time': 27087.778123617172, 'accumulated_eval_time': 2096.162500143051, 'accumulated_logging_time': 6.274480819702148}
I0307 10:33:38.047168 140036200724224 logging_writer.py:48] [67071] accumulated_eval_time=2096.16, accumulated_logging_time=6.27448, accumulated_submission_time=27087.8, global_step=67071, preemption_count=0, score=27087.8, test/accuracy=0.5483, test/loss=2.18483, test/num_examples=10000, total_duration=29197.6, train/accuracy=0.759387, train/loss=1.16564, validation/accuracy=0.67788, validation/loss=1.52196, validation/num_examples=50000
I0307 10:33:50.404455 140036192331520 logging_writer.py:48] [67100] global_step=67100, grad_norm=1.9606680870056152, loss=3.0903282165527344
I0307 10:34:43.539452 140036200724224 logging_writer.py:48] [67200] global_step=67200, grad_norm=2.147507429122925, loss=3.1336886882781982
I0307 10:35:32.767646 140036192331520 logging_writer.py:48] [67300] global_step=67300, grad_norm=1.972212314605713, loss=3.1449594497680664
I0307 10:36:42.478361 140036200724224 logging_writer.py:48] [67400] global_step=67400, grad_norm=1.8965418338775635, loss=3.100651979446411
I0307 10:37:26.173087 140036192331520 logging_writer.py:48] [67500] global_step=67500, grad_norm=2.1315438747406006, loss=3.0465474128723145
I0307 10:38:50.275519 140036200724224 logging_writer.py:48] [67600] global_step=67600, grad_norm=1.9815107583999634, loss=3.0659737586975098
I0307 10:39:59.353159 140036192331520 logging_writer.py:48] [67700] global_step=67700, grad_norm=2.063345432281494, loss=3.082374095916748
I0307 10:40:52.615784 140036200724224 logging_writer.py:48] [67800] global_step=67800, grad_norm=2.0386242866516113, loss=3.0828897953033447
I0307 10:41:38.604233 140036192331520 logging_writer.py:48] [67900] global_step=67900, grad_norm=2.105067014694214, loss=2.9877145290374756
I0307 10:42:08.125428 140191611933888 spec.py:321] Evaluating on the training split.
I0307 10:42:21.295393 140191611933888 spec.py:333] Evaluating on the validation split.
I0307 10:42:43.415546 140191611933888 spec.py:349] Evaluating on the test split.
I0307 10:42:45.191667 140191611933888 submission_runner.py:469] Time since start: 29744.85s, 	Step: 67952, 	{'train/accuracy': 0.7694913744926453, 'train/loss': 1.0812344551086426, 'validation/accuracy': 0.6800400018692017, 'validation/loss': 1.4644702672958374, 'validation/num_examples': 50000, 'test/accuracy': 0.5522000193595886, 'test/loss': 2.1288936138153076, 'test/num_examples': 10000, 'score': 27597.71247267723, 'total_duration': 29744.845315933228, 'accumulated_submission_time': 27597.71247267723, 'accumulated_eval_time': 2133.2285804748535, 'accumulated_logging_time': 6.420159578323364}
I0307 10:42:45.292637 140036200724224 logging_writer.py:48] [67952] accumulated_eval_time=2133.23, accumulated_logging_time=6.42016, accumulated_submission_time=27597.7, global_step=67952, preemption_count=0, score=27597.7, test/accuracy=0.5522, test/loss=2.12889, test/num_examples=10000, total_duration=29744.8, train/accuracy=0.769491, train/loss=1.08123, validation/accuracy=0.68004, validation/loss=1.46447, validation/num_examples=50000
I0307 10:43:29.299848 140036192331520 logging_writer.py:48] [68000] global_step=68000, grad_norm=2.05169939994812, loss=3.0621728897094727
I0307 10:44:29.164386 140036200724224 logging_writer.py:48] [68100] global_step=68100, grad_norm=1.9847224950790405, loss=3.1111674308776855
I0307 10:46:06.720211 140036192331520 logging_writer.py:48] [68200] global_step=68200, grad_norm=1.9995819330215454, loss=3.143681049346924
I0307 10:46:56.882766 140036200724224 logging_writer.py:48] [68300] global_step=68300, grad_norm=1.931876301765442, loss=3.080627202987671
I0307 10:47:37.620668 140036192331520 logging_writer.py:48] [68400] global_step=68400, grad_norm=1.939975619316101, loss=3.084113121032715
I0307 10:48:18.483786 140036200724224 logging_writer.py:48] [68500] global_step=68500, grad_norm=1.9050521850585938, loss=2.961348295211792
I0307 10:49:20.898679 140036192331520 logging_writer.py:48] [68600] global_step=68600, grad_norm=1.8780006170272827, loss=3.0036494731903076
I0307 10:50:25.475132 140036200724224 logging_writer.py:48] [68700] global_step=68700, grad_norm=2.1792588233947754, loss=3.104522943496704
I0307 10:51:11.627725 140036192331520 logging_writer.py:48] [68800] global_step=68800, grad_norm=2.074605941772461, loss=3.0276196002960205
I0307 10:51:15.526705 140191611933888 spec.py:321] Evaluating on the training split.
I0307 10:51:29.192370 140191611933888 spec.py:333] Evaluating on the validation split.
I0307 10:51:46.549243 140191611933888 spec.py:349] Evaluating on the test split.
I0307 10:51:48.355817 140191611933888 submission_runner.py:469] Time since start: 30288.01s, 	Step: 68810, 	{'train/accuracy': 0.7471300959587097, 'train/loss': 1.2173529863357544, 'validation/accuracy': 0.6756599545478821, 'validation/loss': 1.5367326736450195, 'validation/num_examples': 50000, 'test/accuracy': 0.5509999990463257, 'test/loss': 2.1745712757110596, 'test/num_examples': 10000, 'score': 28107.8175573349, 'total_duration': 30288.00942182541, 'accumulated_submission_time': 28107.8175573349, 'accumulated_eval_time': 2166.0574867725372, 'accumulated_logging_time': 6.551817417144775}
I0307 10:51:48.490516 140036200724224 logging_writer.py:48] [68810] accumulated_eval_time=2166.06, accumulated_logging_time=6.55182, accumulated_submission_time=28107.8, global_step=68810, preemption_count=0, score=28107.8, test/accuracy=0.551, test/loss=2.17457, test/num_examples=10000, total_duration=30288, train/accuracy=0.74713, train/loss=1.21735, validation/accuracy=0.67566, validation/loss=1.53673, validation/num_examples=50000
I0307 10:52:27.248813 140036192331520 logging_writer.py:48] [68900] global_step=68900, grad_norm=2.0229525566101074, loss=3.112894296646118
I0307 10:53:14.227305 140036200724224 logging_writer.py:48] [69000] global_step=69000, grad_norm=2.499398708343506, loss=3.0767698287963867
I0307 10:54:22.786033 140036192331520 logging_writer.py:48] [69100] global_step=69100, grad_norm=2.162513256072998, loss=3.119546413421631
I0307 10:55:45.311229 140036200724224 logging_writer.py:48] [69200] global_step=69200, grad_norm=2.081805944442749, loss=3.036073923110962
I0307 10:57:37.524948 140036192331520 logging_writer.py:48] [69300] global_step=69300, grad_norm=2.1626975536346436, loss=3.090669631958008
I0307 10:58:42.069463 140036200724224 logging_writer.py:48] [69400] global_step=69400, grad_norm=2.1812119483947754, loss=3.0876219272613525
I0307 10:59:30.264294 140036192331520 logging_writer.py:48] [69500] global_step=69500, grad_norm=1.9159375429153442, loss=3.0632994174957275
I0307 11:00:18.561898 140191611933888 spec.py:321] Evaluating on the training split.
I0307 11:00:32.190587 140191611933888 spec.py:333] Evaluating on the validation split.
I0307 11:00:50.691025 140191611933888 spec.py:349] Evaluating on the test split.
I0307 11:00:52.487734 140191611933888 submission_runner.py:469] Time since start: 30832.14s, 	Step: 69584, 	{'train/accuracy': 0.7616987824440002, 'train/loss': 1.179900050163269, 'validation/accuracy': 0.6801599860191345, 'validation/loss': 1.5314499139785767, 'validation/num_examples': 50000, 'test/accuracy': 0.5567000508308411, 'test/loss': 2.184511423110962, 'test/num_examples': 10000, 'score': 28617.75919365883, 'total_duration': 30832.141397476196, 'accumulated_submission_time': 28617.75919365883, 'accumulated_eval_time': 2199.983179330826, 'accumulated_logging_time': 6.727322340011597}
I0307 11:00:52.609897 140036200724224 logging_writer.py:48] [69584] accumulated_eval_time=2199.98, accumulated_logging_time=6.72732, accumulated_submission_time=28617.8, global_step=69584, preemption_count=0, score=28617.8, test/accuracy=0.5567, test/loss=2.18451, test/num_examples=10000, total_duration=30832.1, train/accuracy=0.761699, train/loss=1.1799, validation/accuracy=0.68016, validation/loss=1.53145, validation/num_examples=50000
I0307 11:00:59.402954 140036192331520 logging_writer.py:48] [69600] global_step=69600, grad_norm=2.1903553009033203, loss=3.132695436477661
I0307 11:01:47.673158 140036200724224 logging_writer.py:48] [69700] global_step=69700, grad_norm=1.9424750804901123, loss=3.1062264442443848
I0307 11:02:35.494689 140036192331520 logging_writer.py:48] [69800] global_step=69800, grad_norm=2.2522153854370117, loss=3.1437976360321045
I0307 11:03:45.200631 140036200724224 logging_writer.py:48] [69900] global_step=69900, grad_norm=2.1782305240631104, loss=3.090827226638794
I0307 11:04:27.480873 140036192331520 logging_writer.py:48] [70000] global_step=70000, grad_norm=2.1259357929229736, loss=3.039839267730713
I0307 11:05:10.187843 140036200724224 logging_writer.py:48] [70100] global_step=70100, grad_norm=2.27193021774292, loss=3.052218198776245
I0307 11:05:55.958788 140036192331520 logging_writer.py:48] [70200] global_step=70200, grad_norm=1.9963982105255127, loss=3.0867271423339844
I0307 11:06:47.928724 140036200724224 logging_writer.py:48] [70300] global_step=70300, grad_norm=1.9285253286361694, loss=3.0762970447540283
I0307 11:07:36.619101 140036192331520 logging_writer.py:48] [70400] global_step=70400, grad_norm=2.148547410964966, loss=3.1099112033843994
I0307 11:08:28.030301 140036200724224 logging_writer.py:48] [70500] global_step=70500, grad_norm=2.1862869262695312, loss=3.049354076385498
I0307 11:09:22.634115 140191611933888 spec.py:321] Evaluating on the training split.
I0307 11:09:36.169270 140191611933888 spec.py:333] Evaluating on the validation split.
I0307 11:09:55.812924 140191611933888 spec.py:349] Evaluating on the test split.
I0307 11:09:58.011685 140191611933888 submission_runner.py:469] Time since start: 31377.67s, 	Step: 70581, 	{'train/accuracy': 0.7714046239852905, 'train/loss': 1.0927977561950684, 'validation/accuracy': 0.6775999665260315, 'validation/loss': 1.5057133436203003, 'validation/num_examples': 50000, 'test/accuracy': 0.5499000549316406, 'test/loss': 2.1518149375915527, 'test/num_examples': 10000, 'score': 29127.641743183136, 'total_duration': 31377.665235757828, 'accumulated_submission_time': 29127.641743183136, 'accumulated_eval_time': 2235.36049079895, 'accumulated_logging_time': 6.8783769607543945}
I0307 11:09:58.071996 140036192331520 logging_writer.py:48] [70581] accumulated_eval_time=2235.36, accumulated_logging_time=6.87838, accumulated_submission_time=29127.6, global_step=70581, preemption_count=0, score=29127.6, test/accuracy=0.5499, test/loss=2.15181, test/num_examples=10000, total_duration=31377.7, train/accuracy=0.771405, train/loss=1.0928, validation/accuracy=0.6776, validation/loss=1.50571, validation/num_examples=50000
I0307 11:10:05.993569 140036200724224 logging_writer.py:48] [70600] global_step=70600, grad_norm=2.0742013454437256, loss=3.047257900238037
I0307 11:11:06.695188 140036192331520 logging_writer.py:48] [70700] global_step=70700, grad_norm=2.1912317276000977, loss=3.042426824569702
I0307 11:12:15.638230 140036200724224 logging_writer.py:48] [70800] global_step=70800, grad_norm=2.160851240158081, loss=3.107684850692749
I0307 11:13:17.435614 140036192331520 logging_writer.py:48] [70900] global_step=70900, grad_norm=2.173306703567505, loss=3.0713858604431152
I0307 11:14:13.274624 140036200724224 logging_writer.py:48] [71000] global_step=71000, grad_norm=1.9567773342132568, loss=3.1009514331817627
I0307 11:15:15.730108 140036192331520 logging_writer.py:48] [71100] global_step=71100, grad_norm=1.9228461980819702, loss=3.0824923515319824
I0307 11:16:16.152597 140036200724224 logging_writer.py:48] [71200] global_step=71200, grad_norm=2.046231508255005, loss=2.9933974742889404
I0307 11:18:17.279265 140036192331520 logging_writer.py:48] [71300] global_step=71300, grad_norm=2.143876791000366, loss=3.0395236015319824
I0307 11:18:28.784867 140191611933888 spec.py:321] Evaluating on the training split.
I0307 11:18:40.431326 140191611933888 spec.py:333] Evaluating on the validation split.
I0307 11:19:02.948124 140191611933888 spec.py:349] Evaluating on the test split.
I0307 11:19:04.772558 140191611933888 submission_runner.py:469] Time since start: 31924.39s, 	Step: 71314, 	{'train/accuracy': 0.7598652839660645, 'train/loss': 1.1756232976913452, 'validation/accuracy': 0.6831600069999695, 'validation/loss': 1.513314962387085, 'validation/num_examples': 50000, 'test/accuracy': 0.5561000108718872, 'test/loss': 2.155890703201294, 'test/num_examples': 10000, 'score': 29638.18580365181, 'total_duration': 31924.391001224518, 'accumulated_submission_time': 29638.18580365181, 'accumulated_eval_time': 2271.3128185272217, 'accumulated_logging_time': 7.024794340133667}
I0307 11:19:04.892433 140036200724224 logging_writer.py:48] [71314] accumulated_eval_time=2271.31, accumulated_logging_time=7.02479, accumulated_submission_time=29638.2, global_step=71314, preemption_count=0, score=29638.2, test/accuracy=0.5561, test/loss=2.15589, test/num_examples=10000, total_duration=31924.4, train/accuracy=0.759865, train/loss=1.17562, validation/accuracy=0.68316, validation/loss=1.51331, validation/num_examples=50000
I0307 11:19:50.845361 140036192331520 logging_writer.py:48] [71400] global_step=71400, grad_norm=2.056298017501831, loss=3.085303783416748
I0307 11:21:10.205279 140036200724224 logging_writer.py:48] [71500] global_step=71500, grad_norm=2.2751076221466064, loss=3.244319200515747
I0307 11:22:14.927019 140036192331520 logging_writer.py:48] [71600] global_step=71600, grad_norm=2.035226583480835, loss=3.0996041297912598
I0307 11:23:24.545096 140036200724224 logging_writer.py:48] [71700] global_step=71700, grad_norm=2.0672903060913086, loss=3.0670459270477295
I0307 11:24:43.425628 140036192331520 logging_writer.py:48] [71800] global_step=71800, grad_norm=2.134753942489624, loss=3.054394245147705
I0307 11:26:09.604922 140036200724224 logging_writer.py:48] [71900] global_step=71900, grad_norm=2.038299322128296, loss=3.040271759033203
I0307 11:27:33.788090 140036192331520 logging_writer.py:48] [72000] global_step=72000, grad_norm=2.0713679790496826, loss=3.1038248538970947
I0307 11:27:35.417659 140191611933888 spec.py:321] Evaluating on the training split.
I0307 11:27:47.017054 140191611933888 spec.py:333] Evaluating on the validation split.
I0307 11:28:06.664461 140191611933888 spec.py:349] Evaluating on the test split.
I0307 11:28:08.458569 140191611933888 submission_runner.py:469] Time since start: 32468.11s, 	Step: 72003, 	{'train/accuracy': 0.770527720451355, 'train/loss': 1.127446174621582, 'validation/accuracy': 0.6768999695777893, 'validation/loss': 1.5289407968521118, 'validation/num_examples': 50000, 'test/accuracy': 0.5512000322341919, 'test/loss': 2.18680477142334, 'test/num_examples': 10000, 'score': 30148.604938983917, 'total_duration': 32468.112343788147, 'accumulated_submission_time': 30148.604938983917, 'accumulated_eval_time': 2304.3536944389343, 'accumulated_logging_time': 7.173162937164307}
I0307 11:28:08.514232 140036200724224 logging_writer.py:48] [72003] accumulated_eval_time=2304.35, accumulated_logging_time=7.17316, accumulated_submission_time=30148.6, global_step=72003, preemption_count=0, score=30148.6, test/accuracy=0.5512, test/loss=2.1868, test/num_examples=10000, total_duration=32468.1, train/accuracy=0.770528, train/loss=1.12745, validation/accuracy=0.6769, validation/loss=1.52894, validation/num_examples=50000
I0307 11:28:52.329933 140036192331520 logging_writer.py:48] [72100] global_step=72100, grad_norm=2.0631182193756104, loss=3.093292713165283
I0307 11:29:39.218810 140036200724224 logging_writer.py:48] [72200] global_step=72200, grad_norm=2.2950422763824463, loss=3.0790841579437256
I0307 11:30:44.881541 140036192331520 logging_writer.py:48] [72300] global_step=72300, grad_norm=1.9732292890548706, loss=3.0556631088256836
I0307 11:32:31.602056 140036200724224 logging_writer.py:48] [72400] global_step=72400, grad_norm=1.981315016746521, loss=3.0546436309814453
I0307 11:33:26.968458 140036192331520 logging_writer.py:48] [72500] global_step=72500, grad_norm=2.1085898876190186, loss=3.1446146965026855
I0307 11:34:41.323066 140036200724224 logging_writer.py:48] [72600] global_step=72600, grad_norm=2.0186190605163574, loss=3.0860400199890137
I0307 11:35:34.343235 140036192331520 logging_writer.py:48] [72700] global_step=72700, grad_norm=2.4417061805725098, loss=3.101717472076416
I0307 11:36:32.890891 140036200724224 logging_writer.py:48] [72800] global_step=72800, grad_norm=2.0803170204162598, loss=3.181708335876465
I0307 11:36:38.787702 140191611933888 spec.py:321] Evaluating on the training split.
I0307 11:36:50.360636 140191611933888 spec.py:333] Evaluating on the validation split.
I0307 11:37:15.375423 140191611933888 spec.py:349] Evaluating on the test split.
I0307 11:37:17.110276 140191611933888 submission_runner.py:469] Time since start: 33016.76s, 	Step: 72808, 	{'train/accuracy': 0.7454559803009033, 'train/loss': 1.2438143491744995, 'validation/accuracy': 0.6727399826049805, 'validation/loss': 1.5702115297317505, 'validation/num_examples': 50000, 'test/accuracy': 0.5467000007629395, 'test/loss': 2.2195987701416016, 'test/num_examples': 10000, 'score': 30658.778817415237, 'total_duration': 33016.76405334473, 'accumulated_submission_time': 30658.778817415237, 'accumulated_eval_time': 2342.676242828369, 'accumulated_logging_time': 7.236826181411743}
I0307 11:37:17.145753 140036192331520 logging_writer.py:48] [72808] accumulated_eval_time=2342.68, accumulated_logging_time=7.23683, accumulated_submission_time=30658.8, global_step=72808, preemption_count=0, score=30658.8, test/accuracy=0.5467, test/loss=2.2196, test/num_examples=10000, total_duration=33016.8, train/accuracy=0.745456, train/loss=1.24381, validation/accuracy=0.67274, validation/loss=1.57021, validation/num_examples=50000
I0307 11:38:30.727046 140036200724224 logging_writer.py:48] [72900] global_step=72900, grad_norm=2.462110996246338, loss=3.0891475677490234
I0307 11:39:23.576421 140036192331520 logging_writer.py:48] [73000] global_step=73000, grad_norm=2.0238139629364014, loss=3.0153968334198
I0307 11:40:19.578372 140036200724224 logging_writer.py:48] [73100] global_step=73100, grad_norm=2.1795222759246826, loss=3.059405565261841
I0307 11:41:17.221946 140036192331520 logging_writer.py:48] [73200] global_step=73200, grad_norm=2.0779457092285156, loss=3.050590991973877
I0307 11:43:03.518805 140036200724224 logging_writer.py:48] [73300] global_step=73300, grad_norm=1.9798396825790405, loss=3.0089123249053955
I0307 11:44:23.771635 140036192331520 logging_writer.py:48] [73400] global_step=73400, grad_norm=2.25156831741333, loss=3.037480354309082
I0307 11:45:16.282947 140036200724224 logging_writer.py:48] [73500] global_step=73500, grad_norm=2.220935344696045, loss=3.0905184745788574
I0307 11:45:47.515605 140191611933888 spec.py:321] Evaluating on the training split.
I0307 11:45:58.134471 140191611933888 spec.py:333] Evaluating on the validation split.
I0307 11:46:18.475311 140191611933888 spec.py:349] Evaluating on the test split.
I0307 11:46:20.248054 140191611933888 submission_runner.py:469] Time since start: 33559.90s, 	Step: 73550, 	{'train/accuracy': 0.7712252736091614, 'train/loss': 1.0943078994750977, 'validation/accuracy': 0.6843000054359436, 'validation/loss': 1.4587708711624146, 'validation/num_examples': 50000, 'test/accuracy': 0.5604000091552734, 'test/loss': 2.1090142726898193, 'test/num_examples': 10000, 'score': 31168.35579061508, 'total_duration': 33559.90183258057, 'accumulated_submission_time': 31168.35579061508, 'accumulated_eval_time': 2375.4086627960205, 'accumulated_logging_time': 7.977468013763428}
I0307 11:46:20.294667 140036192331520 logging_writer.py:48] [73550] accumulated_eval_time=2375.41, accumulated_logging_time=7.97747, accumulated_submission_time=31168.4, global_step=73550, preemption_count=0, score=31168.4, test/accuracy=0.5604, test/loss=2.10901, test/num_examples=10000, total_duration=33559.9, train/accuracy=0.771225, train/loss=1.09431, validation/accuracy=0.6843, validation/loss=1.45877, validation/num_examples=50000
I0307 11:46:57.273080 140036200724224 logging_writer.py:48] [73600] global_step=73600, grad_norm=2.0714473724365234, loss=3.0783870220184326
I0307 11:48:32.535180 140036192331520 logging_writer.py:48] [73700] global_step=73700, grad_norm=2.0783472061157227, loss=3.0594146251678467
I0307 11:49:24.478298 140036200724224 logging_writer.py:48] [73800] global_step=73800, grad_norm=2.175365447998047, loss=3.065622091293335
I0307 11:50:25.453884 140036192331520 logging_writer.py:48] [73900] global_step=73900, grad_norm=1.9517115354537964, loss=3.0464680194854736
I0307 11:51:56.014666 140036200724224 logging_writer.py:48] [74000] global_step=74000, grad_norm=2.190351724624634, loss=3.070185661315918
I0307 11:53:18.274646 140036192331520 logging_writer.py:48] [74100] global_step=74100, grad_norm=2.4802629947662354, loss=3.109375476837158
I0307 11:54:15.920944 140036200724224 logging_writer.py:48] [74200] global_step=74200, grad_norm=2.1868605613708496, loss=3.050236225128174
I0307 11:54:50.657889 140191611933888 spec.py:321] Evaluating on the training split.
I0307 11:55:03.020566 140191611933888 spec.py:333] Evaluating on the validation split.
I0307 11:55:20.959729 140191611933888 spec.py:349] Evaluating on the test split.
I0307 11:55:22.713996 140191611933888 submission_runner.py:469] Time since start: 34102.37s, 	Step: 74261, 	{'train/accuracy': 0.7502591013908386, 'train/loss': 1.2099504470825195, 'validation/accuracy': 0.6813200116157532, 'validation/loss': 1.518631100654602, 'validation/num_examples': 50000, 'test/accuracy': 0.5591000318527222, 'test/loss': 2.152308225631714, 'test/num_examples': 10000, 'score': 31678.629658460617, 'total_duration': 34102.36777091026, 'accumulated_submission_time': 31678.629658460617, 'accumulated_eval_time': 2407.464739561081, 'accumulated_logging_time': 8.031905889511108}
I0307 11:55:22.749162 140036192331520 logging_writer.py:48] [74261] accumulated_eval_time=2407.46, accumulated_logging_time=8.03191, accumulated_submission_time=31678.6, global_step=74261, preemption_count=0, score=31678.6, test/accuracy=0.5591, test/loss=2.15231, test/num_examples=10000, total_duration=34102.4, train/accuracy=0.750259, train/loss=1.20995, validation/accuracy=0.68132, validation/loss=1.51863, validation/num_examples=50000
I0307 11:55:46.127570 140036200724224 logging_writer.py:48] [74300] global_step=74300, grad_norm=2.2381303310394287, loss=3.091201066970825
I0307 11:57:32.769250 140036192331520 logging_writer.py:48] [74400] global_step=74400, grad_norm=2.168954610824585, loss=3.0487112998962402
I0307 11:59:11.375005 140036200724224 logging_writer.py:48] [74500] global_step=74500, grad_norm=2.021042585372925, loss=3.094367027282715
I0307 12:00:39.149458 140036192331520 logging_writer.py:48] [74600] global_step=74600, grad_norm=2.138545036315918, loss=3.053319215774536
I0307 12:01:51.877770 140036200724224 logging_writer.py:48] [74700] global_step=74700, grad_norm=2.154832601547241, loss=3.10504150390625
I0307 12:02:51.425400 140036192331520 logging_writer.py:48] [74800] global_step=74800, grad_norm=2.2520453929901123, loss=3.0838911533355713
I0307 12:03:53.324382 140191611933888 spec.py:321] Evaluating on the training split.
I0307 12:04:03.635341 140191611933888 spec.py:333] Evaluating on the validation split.
I0307 12:04:24.689660 140191611933888 spec.py:349] Evaluating on the test split.
I0307 12:04:26.419431 140191611933888 submission_runner.py:469] Time since start: 34646.07s, 	Step: 74862, 	{'train/accuracy': 0.7745934128761292, 'train/loss': 1.097033143043518, 'validation/accuracy': 0.6853199601173401, 'validation/loss': 1.4846985340118408, 'validation/num_examples': 50000, 'test/accuracy': 0.555400013923645, 'test/loss': 2.1179192066192627, 'test/num_examples': 10000, 'score': 32189.127655506134, 'total_duration': 34646.07320523262, 'accumulated_submission_time': 32189.127655506134, 'accumulated_eval_time': 2440.5597558021545, 'accumulated_logging_time': 8.074886322021484}
I0307 12:04:26.506386 140036200724224 logging_writer.py:48] [74862] accumulated_eval_time=2440.56, accumulated_logging_time=8.07489, accumulated_submission_time=32189.1, global_step=74862, preemption_count=0, score=32189.1, test/accuracy=0.5554, test/loss=2.11792, test/num_examples=10000, total_duration=34646.1, train/accuracy=0.774593, train/loss=1.09703, validation/accuracy=0.68532, validation/loss=1.4847, validation/num_examples=50000
I0307 12:04:54.051913 140036192331520 logging_writer.py:48] [74900] global_step=74900, grad_norm=2.120062828063965, loss=3.024491786956787
I0307 12:06:11.193511 140036200724224 logging_writer.py:48] [75000] global_step=75000, grad_norm=2.4546751976013184, loss=3.0397348403930664
I0307 12:07:29.018008 140036192331520 logging_writer.py:48] [75100] global_step=75100, grad_norm=2.004080295562744, loss=3.0816574096679688
I0307 12:08:45.018830 140036200724224 logging_writer.py:48] [75200] global_step=75200, grad_norm=2.231907367706299, loss=3.112220287322998
I0307 12:09:38.716736 140036192331520 logging_writer.py:48] [75300] global_step=75300, grad_norm=2.3471198081970215, loss=3.0765275955200195
I0307 12:10:33.575231 140036200724224 logging_writer.py:48] [75400] global_step=75400, grad_norm=2.0685620307922363, loss=3.0182995796203613
I0307 12:11:17.976950 140036192331520 logging_writer.py:48] [75500] global_step=75500, grad_norm=2.247708320617676, loss=3.0732438564300537
I0307 12:12:02.609770 140036200724224 logging_writer.py:48] [75600] global_step=75600, grad_norm=1.955213189125061, loss=3.070624589920044
I0307 12:12:58.090905 140191611933888 spec.py:321] Evaluating on the training split.
I0307 12:13:08.829994 140191611933888 spec.py:333] Evaluating on the validation split.
I0307 12:13:28.851085 140191611933888 spec.py:349] Evaluating on the test split.
I0307 12:13:30.663966 140191611933888 submission_runner.py:469] Time since start: 35190.32s, 	Step: 75680, 	{'train/accuracy': 0.7627750039100647, 'train/loss': 1.1549268960952759, 'validation/accuracy': 0.6815800070762634, 'validation/loss': 1.5061566829681396, 'validation/num_examples': 50000, 'test/accuracy': 0.5516000390052795, 'test/loss': 2.1618735790252686, 'test/num_examples': 10000, 'score': 32700.60781764984, 'total_duration': 35190.317730903625, 'accumulated_submission_time': 32700.60781764984, 'accumulated_eval_time': 2473.132777452469, 'accumulated_logging_time': 8.169992685317993}
I0307 12:13:30.720494 140036192331520 logging_writer.py:48] [75680] accumulated_eval_time=2473.13, accumulated_logging_time=8.16999, accumulated_submission_time=32700.6, global_step=75680, preemption_count=0, score=32700.6, test/accuracy=0.5516, test/loss=2.16187, test/num_examples=10000, total_duration=35190.3, train/accuracy=0.762775, train/loss=1.15493, validation/accuracy=0.68158, validation/loss=1.50616, validation/num_examples=50000
I0307 12:13:51.140562 140036200724224 logging_writer.py:48] [75700] global_step=75700, grad_norm=2.3751935958862305, loss=3.0460433959960938
I0307 12:15:26.548198 140036192331520 logging_writer.py:48] [75800] global_step=75800, grad_norm=2.0266706943511963, loss=3.089553117752075
I0307 12:18:36.966905 140036200724224 logging_writer.py:48] [75900] global_step=75900, grad_norm=1.986741542816162, loss=3.0136168003082275
I0307 12:19:35.508317 140036192331520 logging_writer.py:48] [76000] global_step=76000, grad_norm=1.9953056573867798, loss=3.0344743728637695
I0307 12:22:01.262797 140191611933888 spec.py:321] Evaluating on the training split.
I0307 12:22:12.469092 140191611933888 spec.py:333] Evaluating on the validation split.
I0307 12:22:34.708327 140191611933888 spec.py:349] Evaluating on the test split.
I0307 12:22:36.499695 140191611933888 submission_runner.py:469] Time since start: 35736.15s, 	Step: 76081, 	{'train/accuracy': 0.7795559763908386, 'train/loss': 1.05972158908844, 'validation/accuracy': 0.6872400045394897, 'validation/loss': 1.4586371183395386, 'validation/num_examples': 50000, 'test/accuracy': 0.5582000017166138, 'test/loss': 2.101837158203125, 'test/num_examples': 10000, 'score': 33211.09546780586, 'total_duration': 35736.15346813202, 'accumulated_submission_time': 33211.09546780586, 'accumulated_eval_time': 2508.369644641876, 'accumulated_logging_time': 8.234814167022705}
I0307 12:22:36.539196 140036200724224 logging_writer.py:48] [76081] accumulated_eval_time=2508.37, accumulated_logging_time=8.23481, accumulated_submission_time=33211.1, global_step=76081, preemption_count=0, score=33211.1, test/accuracy=0.5582, test/loss=2.10184, test/num_examples=10000, total_duration=35736.2, train/accuracy=0.779556, train/loss=1.05972, validation/accuracy=0.68724, validation/loss=1.45864, validation/num_examples=50000
I0307 12:22:45.451153 140036192331520 logging_writer.py:48] [76100] global_step=76100, grad_norm=2.185492515563965, loss=3.087214946746826
I0307 12:24:19.007717 140036200724224 logging_writer.py:48] [76200] global_step=76200, grad_norm=2.0947988033294678, loss=3.1010589599609375
I0307 12:26:03.507834 140036192331520 logging_writer.py:48] [76300] global_step=76300, grad_norm=2.1631696224212646, loss=3.0576107501983643
I0307 12:28:11.624629 140036200724224 logging_writer.py:48] [76400] global_step=76400, grad_norm=2.1518280506134033, loss=3.019235610961914
I0307 12:29:28.578686 140036192331520 logging_writer.py:48] [76500] global_step=76500, grad_norm=2.168933629989624, loss=3.1400389671325684
I0307 12:30:51.037283 140036200724224 logging_writer.py:48] [76600] global_step=76600, grad_norm=2.242379665374756, loss=3.0654847621917725
I0307 12:31:06.635468 140191611933888 spec.py:321] Evaluating on the training split.
I0307 12:31:18.582891 140191611933888 spec.py:333] Evaluating on the validation split.
I0307 12:31:41.045231 140191611933888 spec.py:349] Evaluating on the test split.
I0307 12:31:42.823633 140191611933888 submission_runner.py:469] Time since start: 36282.48s, 	Step: 76622, 	{'train/accuracy': 0.7625358700752258, 'train/loss': 1.1496362686157227, 'validation/accuracy': 0.6817399859428406, 'validation/loss': 1.5017591714859009, 'validation/num_examples': 50000, 'test/accuracy': 0.5541000366210938, 'test/loss': 2.1414129734039307, 'test/num_examples': 10000, 'score': 33721.12235498428, 'total_duration': 36282.47741103172, 'accumulated_submission_time': 33721.12235498428, 'accumulated_eval_time': 2544.557785511017, 'accumulated_logging_time': 8.282521486282349}
I0307 12:31:42.905928 140036192331520 logging_writer.py:48] [76622] accumulated_eval_time=2544.56, accumulated_logging_time=8.28252, accumulated_submission_time=33721.1, global_step=76622, preemption_count=0, score=33721.1, test/accuracy=0.5541, test/loss=2.14141, test/num_examples=10000, total_duration=36282.5, train/accuracy=0.762536, train/loss=1.14964, validation/accuracy=0.68174, validation/loss=1.50176, validation/num_examples=50000
I0307 12:32:34.095899 140036200724224 logging_writer.py:48] [76700] global_step=76700, grad_norm=2.0983030796051025, loss=3.0690975189208984
I0307 12:35:20.968773 140036192331520 logging_writer.py:48] [76800] global_step=76800, grad_norm=2.1442527770996094, loss=3.08361554145813
I0307 12:37:55.359043 140036200724224 logging_writer.py:48] [76900] global_step=76900, grad_norm=2.3015494346618652, loss=3.001905679702759
I0307 12:39:30.101541 140036192331520 logging_writer.py:48] [77000] global_step=77000, grad_norm=2.044938325881958, loss=3.033205032348633
I0307 12:40:13.463732 140191611933888 spec.py:321] Evaluating on the training split.
I0307 12:40:25.074030 140191611933888 spec.py:333] Evaluating on the validation split.
I0307 12:40:46.825008 140191611933888 spec.py:349] Evaluating on the test split.
I0307 12:40:48.561271 140191611933888 submission_runner.py:469] Time since start: 36828.22s, 	Step: 77042, 	{'train/accuracy': 0.7663623690605164, 'train/loss': 1.134068250656128, 'validation/accuracy': 0.6872000098228455, 'validation/loss': 1.4789130687713623, 'validation/num_examples': 50000, 'test/accuracy': 0.5644000172615051, 'test/loss': 2.108720064163208, 'test/num_examples': 10000, 'score': 34231.62412691116, 'total_duration': 36828.21502995491, 'accumulated_submission_time': 34231.62412691116, 'accumulated_eval_time': 2579.6552743911743, 'accumulated_logging_time': 8.372639179229736}
I0307 12:40:48.605133 140036200724224 logging_writer.py:48] [77042] accumulated_eval_time=2579.66, accumulated_logging_time=8.37264, accumulated_submission_time=34231.6, global_step=77042, preemption_count=0, score=34231.6, test/accuracy=0.5644, test/loss=2.10872, test/num_examples=10000, total_duration=36828.2, train/accuracy=0.766362, train/loss=1.13407, validation/accuracy=0.6872, validation/loss=1.47891, validation/num_examples=50000
I0307 12:42:00.666972 140036192331520 logging_writer.py:48] [77100] global_step=77100, grad_norm=2.088168144226074, loss=3.0715579986572266
I0307 12:43:26.837368 140036200724224 logging_writer.py:48] [77200] global_step=77200, grad_norm=2.122084617614746, loss=3.041111469268799
I0307 12:44:56.595110 140036192331520 logging_writer.py:48] [77300] global_step=77300, grad_norm=2.2095603942871094, loss=3.060256004333496
I0307 12:47:11.024962 140036200724224 logging_writer.py:48] [77400] global_step=77400, grad_norm=2.3273067474365234, loss=3.0279626846313477
I0307 12:48:59.197473 140036192331520 logging_writer.py:48] [77500] global_step=77500, grad_norm=2.3302931785583496, loss=3.122943878173828
I0307 12:49:19.309663 140191611933888 spec.py:321] Evaluating on the training split.
I0307 12:49:31.408146 140191611933888 spec.py:333] Evaluating on the validation split.
I0307 12:49:58.715889 140191611933888 spec.py:349] Evaluating on the test split.
I0307 12:50:00.469930 140191611933888 submission_runner.py:469] Time since start: 37380.12s, 	Step: 77525, 	{'train/accuracy': 0.7770846486091614, 'train/loss': 1.0971626043319702, 'validation/accuracy': 0.687720000743866, 'validation/loss': 1.4916902780532837, 'validation/num_examples': 50000, 'test/accuracy': 0.5563000440597534, 'test/loss': 2.146676540374756, 'test/num_examples': 10000, 'score': 34742.2627761364, 'total_duration': 37380.123707294464, 'accumulated_submission_time': 34742.2627761364, 'accumulated_eval_time': 2620.815510034561, 'accumulated_logging_time': 8.424813270568848}
I0307 12:50:00.566400 140036200724224 logging_writer.py:48] [77525] accumulated_eval_time=2620.82, accumulated_logging_time=8.42481, accumulated_submission_time=34742.3, global_step=77525, preemption_count=0, score=34742.3, test/accuracy=0.5563, test/loss=2.14668, test/num_examples=10000, total_duration=37380.1, train/accuracy=0.777085, train/loss=1.09716, validation/accuracy=0.68772, validation/loss=1.49169, validation/num_examples=50000
I0307 12:50:34.962930 140036192331520 logging_writer.py:48] [77600] global_step=77600, grad_norm=2.112788438796997, loss=3.03999662399292
I0307 12:51:59.990172 140036200724224 logging_writer.py:48] [77700] global_step=77700, grad_norm=2.2028603553771973, loss=3.0436630249023438
I0307 12:53:26.045199 140036192331520 logging_writer.py:48] [77800] global_step=77800, grad_norm=2.164777994155884, loss=3.013885498046875
I0307 12:56:14.397876 140036200724224 logging_writer.py:48] [77900] global_step=77900, grad_norm=2.287315845489502, loss=3.031813859939575
I0307 12:57:34.236974 140036192331520 logging_writer.py:48] [78000] global_step=78000, grad_norm=2.027773380279541, loss=3.0042788982391357
I0307 12:58:31.237115 140191611933888 spec.py:321] Evaluating on the training split.
I0307 12:58:42.961852 140191611933888 spec.py:333] Evaluating on the validation split.
I0307 12:59:07.393825 140191611933888 spec.py:349] Evaluating on the test split.
I0307 12:59:09.131465 140191611933888 submission_runner.py:469] Time since start: 37928.79s, 	Step: 78067, 	{'train/accuracy': 0.7648875713348389, 'train/loss': 1.116260051727295, 'validation/accuracy': 0.6841599941253662, 'validation/loss': 1.473738431930542, 'validation/num_examples': 50000, 'test/accuracy': 0.5575000047683716, 'test/loss': 2.1474058628082275, 'test/num_examples': 10000, 'score': 35252.850377082825, 'total_duration': 37928.78523039818, 'accumulated_submission_time': 35252.850377082825, 'accumulated_eval_time': 2658.7098178863525, 'accumulated_logging_time': 8.541117668151855}
I0307 12:59:09.221225 140036200724224 logging_writer.py:48] [78067] accumulated_eval_time=2658.71, accumulated_logging_time=8.54112, accumulated_submission_time=35252.9, global_step=78067, preemption_count=0, score=35252.9, test/accuracy=0.5575, test/loss=2.14741, test/num_examples=10000, total_duration=37928.8, train/accuracy=0.764888, train/loss=1.11626, validation/accuracy=0.68416, validation/loss=1.47374, validation/num_examples=50000
I0307 12:59:32.791244 140036192331520 logging_writer.py:48] [78100] global_step=78100, grad_norm=2.2571749687194824, loss=3.0907905101776123
I0307 13:01:13.482774 140036200724224 logging_writer.py:48] [78200] global_step=78200, grad_norm=2.142768383026123, loss=3.042839765548706
I0307 13:04:03.399230 140036192331520 logging_writer.py:48] [78300] global_step=78300, grad_norm=2.0741240978240967, loss=3.0308406352996826
I0307 13:05:51.627565 140036200724224 logging_writer.py:48] [78400] global_step=78400, grad_norm=2.258521795272827, loss=3.113797426223755
I0307 13:07:40.020516 140191611933888 spec.py:321] Evaluating on the training split.
I0307 13:07:51.207085 140191611933888 spec.py:333] Evaluating on the validation split.
I0307 13:08:11.632028 140191611933888 spec.py:349] Evaluating on the test split.
I0307 13:08:13.387259 140191611933888 submission_runner.py:469] Time since start: 38473.04s, 	Step: 78475, 	{'train/accuracy': 0.7654057741165161, 'train/loss': 1.111968755722046, 'validation/accuracy': 0.686519980430603, 'validation/loss': 1.4623723030090332, 'validation/num_examples': 50000, 'test/accuracy': 0.5558000206947327, 'test/loss': 2.1241090297698975, 'test/num_examples': 10000, 'score': 35763.59578490257, 'total_duration': 38473.041018009186, 'accumulated_submission_time': 35763.59578490257, 'accumulated_eval_time': 2692.076517343521, 'accumulated_logging_time': 8.639185667037964}
I0307 13:08:13.477055 140036192331520 logging_writer.py:48] [78475] accumulated_eval_time=2692.08, accumulated_logging_time=8.63919, accumulated_submission_time=35763.6, global_step=78475, preemption_count=0, score=35763.6, test/accuracy=0.5558, test/loss=2.12411, test/num_examples=10000, total_duration=38473, train/accuracy=0.765406, train/loss=1.11197, validation/accuracy=0.68652, validation/loss=1.46237, validation/num_examples=50000
I0307 13:08:39.693200 140036200724224 logging_writer.py:48] [78500] global_step=78500, grad_norm=2.1255455017089844, loss=3.07857608795166
I0307 13:11:36.160500 140036192331520 logging_writer.py:48] [78600] global_step=78600, grad_norm=2.3073668479919434, loss=3.0400390625
I0307 13:15:35.575125 140036200724224 logging_writer.py:48] [78700] global_step=78700, grad_norm=2.062352418899536, loss=2.9901864528656006
I0307 13:16:44.822521 140191611933888 spec.py:321] Evaluating on the training split.
I0307 13:16:54.970010 140191611933888 spec.py:333] Evaluating on the validation split.
I0307 13:17:17.070992 140191611933888 spec.py:349] Evaluating on the test split.
I0307 13:17:18.872176 140191611933888 submission_runner.py:469] Time since start: 39018.53s, 	Step: 78734, 	{'train/accuracy': 0.7793965339660645, 'train/loss': 1.0718291997909546, 'validation/accuracy': 0.678119957447052, 'validation/loss': 1.5102078914642334, 'validation/num_examples': 50000, 'test/accuracy': 0.5527000427246094, 'test/loss': 2.1734182834625244, 'test/num_examples': 10000, 'score': 36274.880365371704, 'total_duration': 39018.525948762894, 'accumulated_submission_time': 36274.880365371704, 'accumulated_eval_time': 2726.1261394023895, 'accumulated_logging_time': 8.76133680343628}
I0307 13:17:18.891232 140036192331520 logging_writer.py:48] [78734] accumulated_eval_time=2726.13, accumulated_logging_time=8.76134, accumulated_submission_time=36274.9, global_step=78734, preemption_count=0, score=36274.9, test/accuracy=0.5527, test/loss=2.17342, test/num_examples=10000, total_duration=39018.5, train/accuracy=0.779397, train/loss=1.07183, validation/accuracy=0.67812, validation/loss=1.51021, validation/num_examples=50000
I0307 13:19:22.431636 140036200724224 logging_writer.py:48] [78800] global_step=78800, grad_norm=2.097743511199951, loss=3.0966012477874756
2025-03-07 13:19:31.401402: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 13:22:44.436512 140036192331520 logging_writer.py:48] [78900] global_step=78900, grad_norm=2.154529571533203, loss=2.9974353313446045
I0307 13:24:22.550075 140036200724224 logging_writer.py:48] [79000] global_step=79000, grad_norm=2.175177574157715, loss=3.019313335418701
I0307 13:25:49.700771 140191611933888 spec.py:321] Evaluating on the training split.
I0307 13:26:01.004332 140191611933888 spec.py:333] Evaluating on the validation split.
I0307 13:26:22.091201 140191611933888 spec.py:349] Evaluating on the test split.
I0307 13:26:23.833498 140191611933888 submission_runner.py:469] Time since start: 39563.49s, 	Step: 79093, 	{'train/accuracy': 0.7680763602256775, 'train/loss': 1.1448367834091187, 'validation/accuracy': 0.6817599534988403, 'validation/loss': 1.5266982316970825, 'validation/num_examples': 50000, 'test/accuracy': 0.5530000329017639, 'test/loss': 2.19062876701355, 'test/num_examples': 10000, 'score': 36785.64168906212, 'total_duration': 39563.48726439476, 'accumulated_submission_time': 36785.64168906212, 'accumulated_eval_time': 2760.2588329315186, 'accumulated_logging_time': 8.787997245788574}
I0307 13:26:23.903645 140036192331520 logging_writer.py:48] [79093] accumulated_eval_time=2760.26, accumulated_logging_time=8.788, accumulated_submission_time=36785.6, global_step=79093, preemption_count=0, score=36785.6, test/accuracy=0.553, test/loss=2.19063, test/num_examples=10000, total_duration=39563.5, train/accuracy=0.768076, train/loss=1.14484, validation/accuracy=0.68176, validation/loss=1.5267, validation/num_examples=50000
I0307 13:26:27.020887 140036200724224 logging_writer.py:48] [79100] global_step=79100, grad_norm=2.172761917114258, loss=3.02091121673584
I0307 13:28:03.857923 140036192331520 logging_writer.py:48] [79200] global_step=79200, grad_norm=2.271535634994507, loss=3.081188917160034
I0307 13:29:24.548849 140036200724224 logging_writer.py:48] [79300] global_step=79300, grad_norm=2.185544013977051, loss=3.0019114017486572
I0307 13:30:42.159292 140036192331520 logging_writer.py:48] [79400] global_step=79400, grad_norm=2.340911865234375, loss=3.097306728363037
I0307 13:32:33.873546 140036200724224 logging_writer.py:48] [79500] global_step=79500, grad_norm=2.190521717071533, loss=2.995356559753418
I0307 13:34:55.377851 140191611933888 spec.py:321] Evaluating on the training split.
I0307 13:35:06.364446 140191611933888 spec.py:333] Evaluating on the validation split.
I0307 13:35:28.205382 140191611933888 spec.py:349] Evaluating on the test split.
I0307 13:35:30.012876 140191611933888 submission_runner.py:469] Time since start: 40109.67s, 	Step: 79569, 	{'train/accuracy': 0.7649872303009033, 'train/loss': 1.1422159671783447, 'validation/accuracy': 0.685699999332428, 'validation/loss': 1.4956337213516235, 'validation/num_examples': 50000, 'test/accuracy': 0.5616000294685364, 'test/loss': 2.1342687606811523, 'test/num_examples': 10000, 'score': 37297.03584074974, 'total_duration': 40109.666603565216, 'accumulated_submission_time': 37297.03584074974, 'accumulated_eval_time': 2794.893789291382, 'accumulated_logging_time': 8.882561683654785}
I0307 13:35:30.056783 140036192331520 logging_writer.py:48] [79569] accumulated_eval_time=2794.89, accumulated_logging_time=8.88256, accumulated_submission_time=37297, global_step=79569, preemption_count=0, score=37297, test/accuracy=0.5616, test/loss=2.13427, test/num_examples=10000, total_duration=40109.7, train/accuracy=0.764987, train/loss=1.14222, validation/accuracy=0.6857, validation/loss=1.49563, validation/num_examples=50000
I0307 13:35:54.746797 140036200724224 logging_writer.py:48] [79600] global_step=79600, grad_norm=2.2430341243743896, loss=3.035074234008789
I0307 13:37:21.265650 140036192331520 logging_writer.py:48] [79700] global_step=79700, grad_norm=2.304863452911377, loss=2.9667110443115234
I0307 13:38:49.932105 140036200724224 logging_writer.py:48] [79800] global_step=79800, grad_norm=2.194925546646118, loss=3.0407018661499023
I0307 13:40:34.324766 140036192331520 logging_writer.py:48] [79900] global_step=79900, grad_norm=2.132699489593506, loss=3.08622407913208
I0307 13:42:01.854678 140036200724224 logging_writer.py:48] [80000] global_step=80000, grad_norm=2.3427083492279053, loss=3.0247645378112793
I0307 13:43:29.622747 140036192331520 logging_writer.py:48] [80100] global_step=80100, grad_norm=2.2638659477233887, loss=3.085488796234131
I0307 13:44:00.954793 140191611933888 spec.py:321] Evaluating on the training split.
I0307 13:44:12.264212 140191611933888 spec.py:333] Evaluating on the validation split.
I0307 13:44:32.932441 140191611933888 spec.py:349] Evaluating on the test split.
I0307 13:44:34.667551 140191611933888 submission_runner.py:469] Time since start: 40654.32s, 	Step: 80132, 	{'train/accuracy': 0.7879264950752258, 'train/loss': 1.0406149625778198, 'validation/accuracy': 0.6904799938201904, 'validation/loss': 1.4685161113739014, 'validation/num_examples': 50000, 'test/accuracy': 0.5601000189781189, 'test/loss': 2.1198103427886963, 'test/num_examples': 10000, 'score': 37807.86044502258, 'total_duration': 40654.321321725845, 'accumulated_submission_time': 37807.86044502258, 'accumulated_eval_time': 2828.606508255005, 'accumulated_logging_time': 8.934717655181885}
I0307 13:44:34.719414 140036200724224 logging_writer.py:48] [80132] accumulated_eval_time=2828.61, accumulated_logging_time=8.93472, accumulated_submission_time=37807.9, global_step=80132, preemption_count=0, score=37807.9, test/accuracy=0.5601, test/loss=2.11981, test/num_examples=10000, total_duration=40654.3, train/accuracy=0.787926, train/loss=1.04061, validation/accuracy=0.69048, validation/loss=1.46852, validation/num_examples=50000
I0307 13:47:34.665986 140036192331520 logging_writer.py:48] [80200] global_step=80200, grad_norm=2.072791337966919, loss=2.980802536010742
I0307 13:52:19.998851 140036200724224 logging_writer.py:48] [80300] global_step=80300, grad_norm=2.187808036804199, loss=3.1189231872558594
I0307 13:53:05.294731 140191611933888 spec.py:321] Evaluating on the training split.
I0307 13:53:16.142941 140191611933888 spec.py:333] Evaluating on the validation split.
I0307 13:53:35.156496 140191611933888 spec.py:349] Evaluating on the test split.
I0307 13:53:36.959402 140191611933888 submission_runner.py:469] Time since start: 41196.61s, 	Step: 80343, 	{'train/accuracy': 0.7824856638908386, 'train/loss': 1.0652188062667847, 'validation/accuracy': 0.691379964351654, 'validation/loss': 1.4680545330047607, 'validation/num_examples': 50000, 'test/accuracy': 0.5629000067710876, 'test/loss': 2.094031810760498, 'test/num_examples': 10000, 'score': 38318.40378689766, 'total_duration': 41196.6131772995, 'accumulated_submission_time': 38318.40378689766, 'accumulated_eval_time': 2860.2711555957794, 'accumulated_logging_time': 8.99477767944336}
I0307 13:53:36.977637 140036192331520 logging_writer.py:48] [80343] accumulated_eval_time=2860.27, accumulated_logging_time=8.99478, accumulated_submission_time=38318.4, global_step=80343, preemption_count=0, score=38318.4, test/accuracy=0.5629, test/loss=2.09403, test/num_examples=10000, total_duration=41196.6, train/accuracy=0.782486, train/loss=1.06522, validation/accuracy=0.69138, validation/loss=1.46805, validation/num_examples=50000
I0307 13:54:27.263105 140036200724224 logging_writer.py:48] [80400] global_step=80400, grad_norm=2.0162265300750732, loss=2.979703903198242
I0307 13:55:55.457972 140036192331520 logging_writer.py:48] [80500] global_step=80500, grad_norm=2.289745569229126, loss=3.0823071002960205
I0307 13:57:21.697531 140036200724224 logging_writer.py:48] [80600] global_step=80600, grad_norm=2.066617012023926, loss=3.0818030834198
I0307 13:58:48.220088 140036192331520 logging_writer.py:48] [80700] global_step=80700, grad_norm=2.4885122776031494, loss=3.0851893424987793
I0307 14:01:08.451811 140036200724224 logging_writer.py:48] [80800] global_step=80800, grad_norm=2.2079195976257324, loss=3.021052598953247
I0307 14:02:08.347844 140191611933888 spec.py:321] Evaluating on the training split.
I0307 14:02:19.247976 140191611933888 spec.py:333] Evaluating on the validation split.
I0307 14:02:40.285147 140191611933888 spec.py:349] Evaluating on the test split.
I0307 14:02:42.079461 140191611933888 submission_runner.py:469] Time since start: 41741.73s, 	Step: 80830, 	{'train/accuracy': 0.7667809128761292, 'train/loss': 1.119646668434143, 'validation/accuracy': 0.6874799728393555, 'validation/loss': 1.4800801277160645, 'validation/num_examples': 50000, 'test/accuracy': 0.5625, 'test/loss': 2.12665057182312, 'test/num_examples': 10000, 'score': 38829.7118229866, 'total_duration': 41741.733221292496, 'accumulated_submission_time': 38829.7118229866, 'accumulated_eval_time': 2894.0027215480804, 'accumulated_logging_time': 9.021174907684326}
I0307 14:02:42.167899 140036192331520 logging_writer.py:48] [80830] accumulated_eval_time=2894, accumulated_logging_time=9.02117, accumulated_submission_time=38829.7, global_step=80830, preemption_count=0, score=38829.7, test/accuracy=0.5625, test/loss=2.12665, test/num_examples=10000, total_duration=41741.7, train/accuracy=0.766781, train/loss=1.11965, validation/accuracy=0.68748, validation/loss=1.48008, validation/num_examples=50000
I0307 14:04:41.441587 140036200724224 logging_writer.py:48] [80900] global_step=80900, grad_norm=2.1175575256347656, loss=2.9769792556762695
I0307 14:06:11.601746 140036192331520 logging_writer.py:48] [81000] global_step=81000, grad_norm=2.2681519985198975, loss=3.075115203857422
I0307 14:07:53.154123 140036200724224 logging_writer.py:48] [81100] global_step=81100, grad_norm=2.3107402324676514, loss=3.0812132358551025
I0307 14:10:08.890832 140036192331520 logging_writer.py:48] [81200] global_step=81200, grad_norm=2.3328449726104736, loss=3.0789103507995605
I0307 14:11:12.952683 140191611933888 spec.py:321] Evaluating on the training split.
I0307 14:11:23.527850 140191611933888 spec.py:333] Evaluating on the validation split.
I0307 14:11:44.683106 140191611933888 spec.py:349] Evaluating on the test split.
I0307 14:11:46.449956 140191611933888 submission_runner.py:469] Time since start: 42286.10s, 	Step: 81235, 	{'train/accuracy': 0.7640106678009033, 'train/loss': 1.1098157167434692, 'validation/accuracy': 0.6887199878692627, 'validation/loss': 1.4445910453796387, 'validation/num_examples': 50000, 'test/accuracy': 0.5681000351905823, 'test/loss': 2.076789379119873, 'test/num_examples': 10000, 'score': 39340.443491220474, 'total_duration': 42286.10372829437, 'accumulated_submission_time': 39340.443491220474, 'accumulated_eval_time': 2927.4999630451202, 'accumulated_logging_time': 9.118421792984009}
I0307 14:11:46.502936 140036200724224 logging_writer.py:48] [81235] accumulated_eval_time=2927.5, accumulated_logging_time=9.11842, accumulated_submission_time=39340.4, global_step=81235, preemption_count=0, score=39340.4, test/accuracy=0.5681, test/loss=2.07679, test/num_examples=10000, total_duration=42286.1, train/accuracy=0.764011, train/loss=1.10982, validation/accuracy=0.68872, validation/loss=1.44459, validation/num_examples=50000
I0307 14:13:45.427672 140036192331520 logging_writer.py:48] [81300] global_step=81300, grad_norm=2.269287586212158, loss=3.0518245697021484
I0307 14:17:07.879616 140036200724224 logging_writer.py:48] [81400] global_step=81400, grad_norm=2.2026913166046143, loss=2.994035243988037
I0307 14:19:51.946216 140036192331520 logging_writer.py:48] [81500] global_step=81500, grad_norm=2.1651015281677246, loss=3.0191240310668945
I0307 14:20:17.999957 140191611933888 spec.py:321] Evaluating on the training split.
I0307 14:20:28.843359 140191611933888 spec.py:333] Evaluating on the validation split.
I0307 14:20:51.879210 140191611933888 spec.py:349] Evaluating on the test split.
I0307 14:20:53.645881 140191611933888 submission_runner.py:469] Time since start: 42833.30s, 	Step: 81518, 	{'train/accuracy': 0.7999840378761292, 'train/loss': 0.9816750884056091, 'validation/accuracy': 0.6929000020027161, 'validation/loss': 1.4409170150756836, 'validation/num_examples': 50000, 'test/accuracy': 0.5675000548362732, 'test/loss': 2.091183662414551, 'test/num_examples': 10000, 'score': 39851.90071582794, 'total_duration': 42833.29964828491, 'accumulated_submission_time': 39851.90071582794, 'accumulated_eval_time': 2963.145844221115, 'accumulated_logging_time': 9.179704427719116}
I0307 14:20:53.719311 140036200724224 logging_writer.py:48] [81518] accumulated_eval_time=2963.15, accumulated_logging_time=9.1797, accumulated_submission_time=39851.9, global_step=81518, preemption_count=0, score=39851.9, test/accuracy=0.5675, test/loss=2.09118, test/num_examples=10000, total_duration=42833.3, train/accuracy=0.799984, train/loss=0.981675, validation/accuracy=0.6929, validation/loss=1.44092, validation/num_examples=50000
I0307 14:25:40.526569 140036192331520 logging_writer.py:48] [81600] global_step=81600, grad_norm=2.4192757606506348, loss=3.031456470489502
I0307 14:28:48.988439 140036200724224 logging_writer.py:48] [81700] global_step=81700, grad_norm=2.21061110496521, loss=3.0632715225219727
I0307 14:29:24.533888 140191611933888 spec.py:321] Evaluating on the training split.
I0307 14:29:35.371184 140191611933888 spec.py:333] Evaluating on the validation split.
I0307 14:29:58.222874 140191611933888 spec.py:349] Evaluating on the test split.
I0307 14:29:59.991032 140191611933888 submission_runner.py:469] Time since start: 43379.64s, 	Step: 81718, 	{'train/accuracy': 0.7785395383834839, 'train/loss': 1.1193634271621704, 'validation/accuracy': 0.6834799647331238, 'validation/loss': 1.5222110748291016, 'validation/num_examples': 50000, 'test/accuracy': 0.5539000034332275, 'test/loss': 2.1843812465667725, 'test/num_examples': 10000, 'score': 40362.663738012314, 'total_duration': 43379.644800662994, 'accumulated_submission_time': 40362.663738012314, 'accumulated_eval_time': 2998.602949142456, 'accumulated_logging_time': 9.281748056411743}
I0307 14:30:00.010699 140036192331520 logging_writer.py:48] [81718] accumulated_eval_time=2998.6, accumulated_logging_time=9.28175, accumulated_submission_time=40362.7, global_step=81718, preemption_count=0, score=40362.7, test/accuracy=0.5539, test/loss=2.18438, test/num_examples=10000, total_duration=43379.6, train/accuracy=0.77854, train/loss=1.11936, validation/accuracy=0.68348, validation/loss=1.52221, validation/num_examples=50000
I0307 14:32:34.648006 140036200724224 logging_writer.py:48] [81800] global_step=81800, grad_norm=2.1553494930267334, loss=3.0242631435394287
I0307 14:36:55.778220 140036192331520 logging_writer.py:48] [81900] global_step=81900, grad_norm=2.018449068069458, loss=2.9917492866516113
I0307 14:38:30.399132 140191611933888 spec.py:321] Evaluating on the training split.
I0307 14:38:41.570244 140191611933888 spec.py:333] Evaluating on the validation split.
I0307 14:39:03.711289 140191611933888 spec.py:349] Evaluating on the test split.
I0307 14:39:05.433668 140191611933888 submission_runner.py:469] Time since start: 43925.09s, 	Step: 81967, 	{'train/accuracy': 0.7727598547935486, 'train/loss': 1.1218955516815186, 'validation/accuracy': 0.6892200112342834, 'validation/loss': 1.494043231010437, 'validation/num_examples': 50000, 'test/accuracy': 0.5591000318527222, 'test/loss': 2.132713556289673, 'test/num_examples': 10000, 'score': 40873.015095710754, 'total_duration': 43925.087421417236, 'accumulated_submission_time': 40873.015095710754, 'accumulated_eval_time': 3033.637434244156, 'accumulated_logging_time': 9.309917449951172}
I0307 14:39:05.454199 140036200724224 logging_writer.py:48] [81967] accumulated_eval_time=3033.64, accumulated_logging_time=9.30992, accumulated_submission_time=40873, global_step=81967, preemption_count=0, score=40873, test/accuracy=0.5591, test/loss=2.13271, test/num_examples=10000, total_duration=43925.1, train/accuracy=0.77276, train/loss=1.1219, validation/accuracy=0.68922, validation/loss=1.49404, validation/num_examples=50000
I0307 14:39:38.129044 140036192331520 logging_writer.py:48] [82000] global_step=82000, grad_norm=2.2363064289093018, loss=2.999295473098755
I0307 14:42:10.253636 140036200724224 logging_writer.py:48] [82100] global_step=82100, grad_norm=2.3075878620147705, loss=3.0919711589813232
I0307 14:44:38.161907 140036192331520 logging_writer.py:48] [82200] global_step=82200, grad_norm=2.2659010887145996, loss=3.018101692199707
I0307 14:46:12.982304 140036200724224 logging_writer.py:48] [82300] global_step=82300, grad_norm=2.4309470653533936, loss=3.072053909301758
I0307 14:47:37.517939 140191611933888 spec.py:321] Evaluating on the training split.
I0307 14:47:48.046558 140191611933888 spec.py:333] Evaluating on the validation split.
I0307 14:48:11.354944 140191611933888 spec.py:349] Evaluating on the test split.
I0307 14:48:13.162039 140191611933888 submission_runner.py:469] Time since start: 44472.82s, 	Step: 82337, 	{'train/accuracy': 0.7665218114852905, 'train/loss': 1.1272982358932495, 'validation/accuracy': 0.686199963092804, 'validation/loss': 1.4827165603637695, 'validation/num_examples': 50000, 'test/accuracy': 0.5581000447273254, 'test/loss': 2.1117899417877197, 'test/num_examples': 10000, 'score': 41385.028460502625, 'total_duration': 44472.81581091881, 'accumulated_submission_time': 41385.028460502625, 'accumulated_eval_time': 3069.281506061554, 'accumulated_logging_time': 9.33889389038086}
I0307 14:48:13.210639 140036192331520 logging_writer.py:48] [82337] accumulated_eval_time=3069.28, accumulated_logging_time=9.33889, accumulated_submission_time=41385, global_step=82337, preemption_count=0, score=41385, test/accuracy=0.5581, test/loss=2.11179, test/num_examples=10000, total_duration=44472.8, train/accuracy=0.766522, train/loss=1.1273, validation/accuracy=0.6862, validation/loss=1.48272, validation/num_examples=50000
I0307 14:52:24.519548 140036200724224 logging_writer.py:48] [82400] global_step=82400, grad_norm=2.1684529781341553, loss=3.0599417686462402
I0307 14:56:43.454092 140191611933888 spec.py:321] Evaluating on the training split.
I0307 14:56:54.157755 140191611933888 spec.py:333] Evaluating on the validation split.
I0307 14:57:14.650861 140191611933888 spec.py:349] Evaluating on the test split.
I0307 14:57:16.380381 140191611933888 submission_runner.py:469] Time since start: 45016.03s, 	Step: 82488, 	{'train/accuracy': 0.7665815949440002, 'train/loss': 1.107668161392212, 'validation/accuracy': 0.6884199976921082, 'validation/loss': 1.451607584953308, 'validation/num_examples': 50000, 'test/accuracy': 0.5640000104904175, 'test/loss': 2.0964252948760986, 'test/num_examples': 10000, 'score': 41895.230098962784, 'total_duration': 45016.03415679932, 'accumulated_submission_time': 41895.230098962784, 'accumulated_eval_time': 3102.2077696323395, 'accumulated_logging_time': 9.411880016326904}
I0307 14:57:16.398711 140036192331520 logging_writer.py:48] [82488] accumulated_eval_time=3102.21, accumulated_logging_time=9.41188, accumulated_submission_time=41895.2, global_step=82488, preemption_count=0, score=41895.2, test/accuracy=0.564, test/loss=2.09643, test/num_examples=10000, total_duration=45016, train/accuracy=0.766582, train/loss=1.10767, validation/accuracy=0.68842, validation/loss=1.45161, validation/num_examples=50000
I0307 14:57:21.483476 140036200724224 logging_writer.py:48] [82500] global_step=82500, grad_norm=2.3717997074127197, loss=3.034179210662842
I0307 14:59:42.062643 140036192331520 logging_writer.py:48] [82600] global_step=82600, grad_norm=2.1761109828948975, loss=3.0307562351226807
I0307 15:01:29.042149 140036200724224 logging_writer.py:48] [82700] global_step=82700, grad_norm=2.235426425933838, loss=3.002842903137207
I0307 15:05:10.828199 140036192331520 logging_writer.py:48] [82800] global_step=82800, grad_norm=2.2964718341827393, loss=3.035883903503418
I0307 15:05:48.606810 140191611933888 spec.py:321] Evaluating on the training split.
I0307 15:05:58.953921 140191611933888 spec.py:333] Evaluating on the validation split.
I0307 15:06:22.543043 140191611933888 spec.py:349] Evaluating on the test split.
I0307 15:06:24.338263 140191611933888 submission_runner.py:469] Time since start: 45563.99s, 	Step: 82810, 	{'train/accuracy': 0.7723811864852905, 'train/loss': 1.0965019464492798, 'validation/accuracy': 0.6918799877166748, 'validation/loss': 1.449447512626648, 'validation/num_examples': 50000, 'test/accuracy': 0.5680000185966492, 'test/loss': 2.0799481868743896, 'test/num_examples': 10000, 'score': 42407.39200472832, 'total_duration': 45563.992037534714, 'accumulated_submission_time': 42407.39200472832, 'accumulated_eval_time': 3137.9391853809357, 'accumulated_logging_time': 9.438167810440063}
I0307 15:06:24.404420 140036200724224 logging_writer.py:48] [82810] accumulated_eval_time=3137.94, accumulated_logging_time=9.43817, accumulated_submission_time=42407.4, global_step=82810, preemption_count=0, score=42407.4, test/accuracy=0.568, test/loss=2.07995, test/num_examples=10000, total_duration=45564, train/accuracy=0.772381, train/loss=1.0965, validation/accuracy=0.69188, validation/loss=1.44945, validation/num_examples=50000
I0307 15:09:35.456001 140036192331520 logging_writer.py:48] [82900] global_step=82900, grad_norm=2.145968198776245, loss=2.9934518337249756
I0307 15:11:24.341666 140036200724224 logging_writer.py:48] [83000] global_step=83000, grad_norm=2.400991201400757, loss=3.0657265186309814
I0307 15:13:13.051023 140036192331520 logging_writer.py:48] [83100] global_step=83100, grad_norm=2.2465784549713135, loss=3.0302252769470215
I0307 15:14:54.556934 140191611933888 spec.py:321] Evaluating on the training split.
I0307 15:15:05.743021 140191611933888 spec.py:333] Evaluating on the validation split.
I0307 15:15:28.695995 140191611933888 spec.py:349] Evaluating on the test split.
I0307 15:15:30.451174 140191611933888 submission_runner.py:469] Time since start: 46110.10s, 	Step: 83195, 	{'train/accuracy': 0.78617262840271, 'train/loss': 1.0235415697097778, 'validation/accuracy': 0.6933799982070923, 'validation/loss': 1.4387099742889404, 'validation/num_examples': 50000, 'test/accuracy': 0.5683000087738037, 'test/loss': 2.0722548961639404, 'test/num_examples': 10000, 'score': 42917.456682920456, 'total_duration': 46110.10495042801, 'accumulated_submission_time': 42917.456682920456, 'accumulated_eval_time': 3173.8333978652954, 'accumulated_logging_time': 9.54940414428711}
I0307 15:15:30.525366 140036200724224 logging_writer.py:48] [83195] accumulated_eval_time=3173.83, accumulated_logging_time=9.5494, accumulated_submission_time=42917.5, global_step=83195, preemption_count=0, score=42917.5, test/accuracy=0.5683, test/loss=2.07225, test/num_examples=10000, total_duration=46110.1, train/accuracy=0.786173, train/loss=1.02354, validation/accuracy=0.69338, validation/loss=1.43871, validation/num_examples=50000
I0307 15:15:32.814106 140036192331520 logging_writer.py:48] [83200] global_step=83200, grad_norm=2.256406307220459, loss=3.0344364643096924
I0307 15:17:12.962103 140036200724224 logging_writer.py:48] [83300] global_step=83300, grad_norm=2.3654749393463135, loss=3.0586986541748047
I0307 15:21:57.478313 140036192331520 logging_writer.py:48] [83400] global_step=83400, grad_norm=2.3660104274749756, loss=3.047516345977783
I0307 15:23:39.430190 140036200724224 logging_writer.py:48] [83500] global_step=83500, grad_norm=2.272214889526367, loss=3.0890276432037354
I0307 15:24:00.585016 140191611933888 spec.py:321] Evaluating on the training split.
I0307 15:24:12.323126 140191611933888 spec.py:333] Evaluating on the validation split.
I0307 15:24:35.936130 140191611933888 spec.py:349] Evaluating on the test split.
I0307 15:24:37.711165 140191611933888 submission_runner.py:469] Time since start: 46657.36s, 	Step: 83531, 	{'train/accuracy': 0.7734972834587097, 'train/loss': 1.0873615741729736, 'validation/accuracy': 0.6864599585533142, 'validation/loss': 1.4675266742706299, 'validation/num_examples': 50000, 'test/accuracy': 0.5550000071525574, 'test/loss': 2.133202314376831, 'test/num_examples': 10000, 'score': 43427.468729496, 'total_duration': 46657.36492609978, 'accumulated_submission_time': 43427.468729496, 'accumulated_eval_time': 3210.959508419037, 'accumulated_logging_time': 9.631896018981934}
I0307 15:24:37.768633 140036192331520 logging_writer.py:48] [83531] accumulated_eval_time=3210.96, accumulated_logging_time=9.6319, accumulated_submission_time=43427.5, global_step=83531, preemption_count=0, score=43427.5, test/accuracy=0.555, test/loss=2.1332, test/num_examples=10000, total_duration=46657.4, train/accuracy=0.773497, train/loss=1.08736, validation/accuracy=0.68646, validation/loss=1.46753, validation/num_examples=50000
I0307 15:25:30.475545 140036200724224 logging_writer.py:48] [83600] global_step=83600, grad_norm=2.239062786102295, loss=2.9689323902130127
I0307 15:27:47.404174 140036192331520 logging_writer.py:48] [83700] global_step=83700, grad_norm=2.427264451980591, loss=3.0181756019592285
I0307 15:31:01.022392 140036200724224 logging_writer.py:48] [83800] global_step=83800, grad_norm=2.4011547565460205, loss=2.9868860244750977
I0307 15:32:35.724530 140036192331520 logging_writer.py:48] [83900] global_step=83900, grad_norm=2.253457546234131, loss=3.051910400390625
I0307 15:33:08.071084 140191611933888 spec.py:321] Evaluating on the training split.
I0307 15:33:20.233193 140191611933888 spec.py:333] Evaluating on the validation split.
I0307 15:33:45.612102 140191611933888 spec.py:349] Evaluating on the test split.
I0307 15:33:47.390187 140191611933888 submission_runner.py:469] Time since start: 47207.04s, 	Step: 83951, 	{'train/accuracy': 0.76859450340271, 'train/loss': 1.1439391374588013, 'validation/accuracy': 0.6890000104904175, 'validation/loss': 1.4934748411178589, 'validation/num_examples': 50000, 'test/accuracy': 0.556600034236908, 'test/loss': 2.1601579189300537, 'test/num_examples': 10000, 'score': 43937.71469449997, 'total_duration': 47207.04396414757, 'accumulated_submission_time': 43937.71469449997, 'accumulated_eval_time': 3250.27858042717, 'accumulated_logging_time': 9.697559833526611}
I0307 15:33:47.433375 140036200724224 logging_writer.py:48] [83951] accumulated_eval_time=3250.28, accumulated_logging_time=9.69756, accumulated_submission_time=43937.7, global_step=83951, preemption_count=0, score=43937.7, test/accuracy=0.5566, test/loss=2.16016, test/num_examples=10000, total_duration=47207, train/accuracy=0.768595, train/loss=1.14394, validation/accuracy=0.689, validation/loss=1.49347, validation/num_examples=50000
I0307 15:34:12.763296 140036192331520 logging_writer.py:48] [84000] global_step=84000, grad_norm=2.293907403945923, loss=3.060049295425415
I0307 15:35:18.085995 140036200724224 logging_writer.py:48] [84100] global_step=84100, grad_norm=2.42498779296875, loss=2.986926794052124
I0307 15:36:21.902614 140036192331520 logging_writer.py:48] [84200] global_step=84200, grad_norm=2.4285340309143066, loss=3.0957696437835693
I0307 15:37:25.932617 140036200724224 logging_writer.py:48] [84300] global_step=84300, grad_norm=2.2808070182800293, loss=2.9245612621307373
I0307 15:38:29.589089 140036192331520 logging_writer.py:48] [84400] global_step=84400, grad_norm=2.1816513538360596, loss=3.0535356998443604
I0307 15:39:33.754015 140036200724224 logging_writer.py:48] [84500] global_step=84500, grad_norm=2.3158469200134277, loss=2.975116729736328
I0307 15:40:37.760962 140036192331520 logging_writer.py:48] [84600] global_step=84600, grad_norm=2.328136444091797, loss=3.037628173828125
I0307 15:41:51.890851 140036200724224 logging_writer.py:48] [84700] global_step=84700, grad_norm=2.2740731239318848, loss=2.979853868484497
I0307 15:42:18.554847 140191611933888 spec.py:321] Evaluating on the training split.
I0307 15:42:29.385521 140191611933888 spec.py:333] Evaluating on the validation split.
I0307 15:42:53.581820 140191611933888 spec.py:349] Evaluating on the test split.
I0307 15:42:55.330785 140191611933888 submission_runner.py:469] Time since start: 47754.98s, 	Step: 84714, 	{'train/accuracy': 0.784199595451355, 'train/loss': 1.0420020818710327, 'validation/accuracy': 0.6931799650192261, 'validation/loss': 1.4441362619400024, 'validation/num_examples': 50000, 'test/accuracy': 0.5631000399589539, 'test/loss': 2.1002256870269775, 'test/num_examples': 10000, 'score': 44448.74419569969, 'total_duration': 47754.984545469284, 'accumulated_submission_time': 44448.74419569969, 'accumulated_eval_time': 3287.054470539093, 'accumulated_logging_time': 9.748671770095825}
I0307 15:42:55.374763 140036192331520 logging_writer.py:48] [84714] accumulated_eval_time=3287.05, accumulated_logging_time=9.74867, accumulated_submission_time=44448.7, global_step=84714, preemption_count=0, score=44448.7, test/accuracy=0.5631, test/loss=2.10023, test/num_examples=10000, total_duration=47755, train/accuracy=0.7842, train/loss=1.042, validation/accuracy=0.69318, validation/loss=1.44414, validation/num_examples=50000
I0307 15:45:40.185143 140036200724224 logging_writer.py:48] [84800] global_step=84800, grad_norm=2.1275041103363037, loss=3.040168285369873
I0307 15:49:26.183318 140036192331520 logging_writer.py:48] [84900] global_step=84900, grad_norm=2.2413394451141357, loss=3.072253704071045
I0307 15:51:27.126732 140191611933888 spec.py:321] Evaluating on the training split.
I0307 15:51:38.036472 140191611933888 spec.py:333] Evaluating on the validation split.
I0307 15:51:57.247238 140191611933888 spec.py:349] Evaluating on the test split.
I0307 15:51:58.994348 140191611933888 submission_runner.py:469] Time since start: 48298.65s, 	Step: 84954, 	{'train/accuracy': 0.7774633169174194, 'train/loss': 1.0720586776733398, 'validation/accuracy': 0.6904799938201904, 'validation/loss': 1.4531588554382324, 'validation/num_examples': 50000, 'test/accuracy': 0.5653000473976135, 'test/loss': 2.1090221405029297, 'test/num_examples': 10000, 'score': 44960.462553977966, 'total_duration': 48298.648114442825, 'accumulated_submission_time': 44960.462553977966, 'accumulated_eval_time': 3318.9220542907715, 'accumulated_logging_time': 9.800751209259033}
I0307 15:51:59.013601 140036200724224 logging_writer.py:48] [84954] accumulated_eval_time=3318.92, accumulated_logging_time=9.80075, accumulated_submission_time=44960.5, global_step=84954, preemption_count=0, score=44960.5, test/accuracy=0.5653, test/loss=2.10902, test/num_examples=10000, total_duration=48298.6, train/accuracy=0.777463, train/loss=1.07206, validation/accuracy=0.69048, validation/loss=1.45316, validation/num_examples=50000
I0307 15:53:16.156672 140036192331520 logging_writer.py:48] [85000] global_step=85000, grad_norm=2.289492607116699, loss=3.070138931274414
I0307 15:56:51.440286 140036200724224 logging_writer.py:48] [85100] global_step=85100, grad_norm=2.1502223014831543, loss=3.006349563598633
I0307 16:00:30.149923 140191611933888 spec.py:321] Evaluating on the training split.
I0307 16:00:40.977623 140191611933888 spec.py:333] Evaluating on the validation split.
I0307 16:01:02.728540 140191611933888 spec.py:349] Evaluating on the test split.
I0307 16:01:04.469897 140191611933888 submission_runner.py:469] Time since start: 48844.12s, 	Step: 85164, 	{'train/accuracy': 0.7782605290412903, 'train/loss': 1.1029448509216309, 'validation/accuracy': 0.6894999742507935, 'validation/loss': 1.4759265184402466, 'validation/num_examples': 50000, 'test/accuracy': 0.5706000328063965, 'test/loss': 2.1005172729492188, 'test/num_examples': 10000, 'score': 45471.5216255188, 'total_duration': 48844.12367153168, 'accumulated_submission_time': 45471.5216255188, 'accumulated_eval_time': 3353.2419979572296, 'accumulated_logging_time': 9.873937129974365}
I0307 16:01:04.489246 140036192331520 logging_writer.py:48] [85164] accumulated_eval_time=3353.24, accumulated_logging_time=9.87394, accumulated_submission_time=45471.5, global_step=85164, preemption_count=0, score=45471.5, test/accuracy=0.5706, test/loss=2.10052, test/num_examples=10000, total_duration=48844.1, train/accuracy=0.778261, train/loss=1.10294, validation/accuracy=0.6895, validation/loss=1.47593, validation/num_examples=50000
I0307 16:03:15.399610 140036200724224 logging_writer.py:48] [85200] global_step=85200, grad_norm=2.323482036590576, loss=2.9763758182525635
I0307 16:09:36.321131 140191611933888 spec.py:321] Evaluating on the training split.
I0307 16:09:47.455953 140191611933888 spec.py:333] Evaluating on the validation split.
I0307 16:10:11.130074 140191611933888 spec.py:349] Evaluating on the test split.
I0307 16:10:12.917628 140191611933888 submission_runner.py:469] Time since start: 49392.57s, 	Step: 85292, 	{'train/accuracy': 0.7707669138908386, 'train/loss': 1.1187443733215332, 'validation/accuracy': 0.6912199854850769, 'validation/loss': 1.475957989692688, 'validation/num_examples': 50000, 'test/accuracy': 0.5631999969482422, 'test/loss': 2.113006114959717, 'test/num_examples': 10000, 'score': 45983.33199429512, 'total_duration': 49392.57139849663, 'accumulated_submission_time': 45983.33199429512, 'accumulated_eval_time': 3389.838460922241, 'accumulated_logging_time': 9.901540756225586}
I0307 16:10:12.937435 140036192331520 logging_writer.py:48] [85292] accumulated_eval_time=3389.84, accumulated_logging_time=9.90154, accumulated_submission_time=45983.3, global_step=85292, preemption_count=0, score=45983.3, test/accuracy=0.5632, test/loss=2.11301, test/num_examples=10000, total_duration=49392.6, train/accuracy=0.770767, train/loss=1.11874, validation/accuracy=0.69122, validation/loss=1.47596, validation/num_examples=50000
I0307 16:10:26.193540 140036200724224 logging_writer.py:48] [85300] global_step=85300, grad_norm=2.233579397201538, loss=2.9731221199035645
I0307 16:17:29.183959 140036192331520 logging_writer.py:48] [85400] global_step=85400, grad_norm=2.336674928665161, loss=3.009268045425415
I0307 16:18:44.240889 140191611933888 spec.py:321] Evaluating on the training split.
I0307 16:18:55.275524 140191611933888 spec.py:333] Evaluating on the validation split.
I0307 16:19:16.262795 140191611933888 spec.py:349] Evaluating on the test split.
I0307 16:19:18.031826 140191611933888 submission_runner.py:469] Time since start: 49937.69s, 	Step: 85422, 	{'train/accuracy': 0.7719427347183228, 'train/loss': 1.1172235012054443, 'validation/accuracy': 0.6912800073623657, 'validation/loss': 1.4774099588394165, 'validation/num_examples': 50000, 'test/accuracy': 0.5648000240325928, 'test/loss': 2.113751173019409, 'test/num_examples': 10000, 'score': 46494.613300323486, 'total_duration': 49937.6855866909, 'accumulated_submission_time': 46494.613300323486, 'accumulated_eval_time': 3423.6293530464172, 'accumulated_logging_time': 9.92949914932251}
I0307 16:19:18.050855 140036200724224 logging_writer.py:48] [85422] accumulated_eval_time=3423.63, accumulated_logging_time=9.9295, accumulated_submission_time=46494.6, global_step=85422, preemption_count=0, score=46494.6, test/accuracy=0.5648, test/loss=2.11375, test/num_examples=10000, total_duration=49937.7, train/accuracy=0.771943, train/loss=1.11722, validation/accuracy=0.69128, validation/loss=1.47741, validation/num_examples=50000
I0307 16:20:54.783656 140036192331520 logging_writer.py:48] [85500] global_step=85500, grad_norm=2.4926624298095703, loss=3.0645153522491455
I0307 16:23:15.859745 140036200724224 logging_writer.py:48] [85600] global_step=85600, grad_norm=2.332707643508911, loss=3.0718839168548584
I0307 16:25:36.610571 140036192331520 logging_writer.py:48] [85700] global_step=85700, grad_norm=2.252138614654541, loss=3.066462993621826
I0307 16:27:49.239568 140191611933888 spec.py:321] Evaluating on the training split.
I0307 16:28:00.193585 140191611933888 spec.py:333] Evaluating on the validation split.
I0307 16:28:21.095819 140191611933888 spec.py:349] Evaluating on the test split.
I0307 16:28:22.860208 140191611933888 submission_runner.py:469] Time since start: 50482.51s, 	Step: 85795, 	{'train/accuracy': 0.7743343114852905, 'train/loss': 1.0845028162002563, 'validation/accuracy': 0.6970599889755249, 'validation/loss': 1.4260917901992798, 'validation/num_examples': 50000, 'test/accuracy': 0.5698000192642212, 'test/loss': 2.080115795135498, 'test/num_examples': 10000, 'score': 47005.75315093994, 'total_duration': 50482.513984680176, 'accumulated_submission_time': 47005.75315093994, 'accumulated_eval_time': 3457.249967813492, 'accumulated_logging_time': 9.956375122070312}
I0307 16:28:22.931676 140036200724224 logging_writer.py:48] [85795] accumulated_eval_time=3457.25, accumulated_logging_time=9.95638, accumulated_submission_time=47005.8, global_step=85795, preemption_count=0, score=47005.8, test/accuracy=0.5698, test/loss=2.08012, test/num_examples=10000, total_duration=50482.5, train/accuracy=0.774334, train/loss=1.0845, validation/accuracy=0.69706, validation/loss=1.42609, validation/num_examples=50000
I0307 16:28:25.219867 140036192331520 logging_writer.py:48] [85800] global_step=85800, grad_norm=2.2077407836914062, loss=3.0493505001068115
I0307 16:30:41.215809 140036200724224 logging_writer.py:48] [85900] global_step=85900, grad_norm=2.2609968185424805, loss=3.0059924125671387
I0307 16:32:42.296547 140036192331520 logging_writer.py:48] [86000] global_step=86000, grad_norm=2.563185930252075, loss=2.965993881225586
I0307 16:34:30.937636 140036200724224 logging_writer.py:48] [86100] global_step=86100, grad_norm=2.189613103866577, loss=3.0037379264831543
I0307 16:36:20.866510 140036192331520 logging_writer.py:48] [86200] global_step=86200, grad_norm=2.3447999954223633, loss=3.1133673191070557
I0307 16:36:53.002405 140191611933888 spec.py:321] Evaluating on the training split.
I0307 16:37:03.465098 140191611933888 spec.py:333] Evaluating on the validation split.
I0307 16:37:25.368848 140191611933888 spec.py:349] Evaluating on the test split.
I0307 16:37:27.156467 140191611933888 submission_runner.py:469] Time since start: 51026.81s, 	Step: 86220, 	{'train/accuracy': 0.7840401530265808, 'train/loss': 1.088598370552063, 'validation/accuracy': 0.6858999729156494, 'validation/loss': 1.5026551485061646, 'validation/num_examples': 50000, 'test/accuracy': 0.5688000321388245, 'test/loss': 2.1097285747528076, 'test/num_examples': 10000, 'score': 47515.768565654755, 'total_duration': 51026.81024312973, 'accumulated_submission_time': 47515.768565654755, 'accumulated_eval_time': 3491.4040021896362, 'accumulated_logging_time': 10.035655975341797}
I0307 16:37:27.225232 140036200724224 logging_writer.py:48] [86220] accumulated_eval_time=3491.4, accumulated_logging_time=10.0357, accumulated_submission_time=47515.8, global_step=86220, preemption_count=0, score=47515.8, test/accuracy=0.5688, test/loss=2.10973, test/num_examples=10000, total_duration=51026.8, train/accuracy=0.78404, train/loss=1.0886, validation/accuracy=0.6859, validation/loss=1.50266, validation/num_examples=50000
I0307 16:42:48.456191 140036192331520 logging_writer.py:48] [86300] global_step=86300, grad_norm=2.3597235679626465, loss=2.994063377380371
I0307 16:45:57.197239 140191611933888 spec.py:321] Evaluating on the training split.
I0307 16:46:07.754479 140191611933888 spec.py:333] Evaluating on the validation split.
I0307 16:46:32.771616 140191611933888 spec.py:349] Evaluating on the test split.
I0307 16:46:34.562290 140191611933888 submission_runner.py:469] Time since start: 51574.22s, 	Step: 86345, 	{'train/accuracy': 0.785554826259613, 'train/loss': 1.009905457496643, 'validation/accuracy': 0.6980599761009216, 'validation/loss': 1.4055782556533813, 'validation/num_examples': 50000, 'test/accuracy': 0.5692000389099121, 'test/loss': 2.048983573913574, 'test/num_examples': 10000, 'score': 48025.72045445442, 'total_duration': 51574.21605324745, 'accumulated_submission_time': 48025.72045445442, 'accumulated_eval_time': 3528.7690114974976, 'accumulated_logging_time': 10.112008571624756}
I0307 16:46:34.583678 140036200724224 logging_writer.py:48] [86345] accumulated_eval_time=3528.77, accumulated_logging_time=10.112, accumulated_submission_time=48025.7, global_step=86345, preemption_count=0, score=48025.7, test/accuracy=0.5692, test/loss=2.04898, test/num_examples=10000, total_duration=51574.2, train/accuracy=0.785555, train/loss=1.00991, validation/accuracy=0.69806, validation/loss=1.40558, validation/num_examples=50000
I0307 16:50:09.268700 140036192331520 logging_writer.py:48] [86400] global_step=86400, grad_norm=2.2133281230926514, loss=3.108854293823242
I0307 16:55:06.656155 140191611933888 spec.py:321] Evaluating on the training split.
I0307 16:55:17.244548 140191611933888 spec.py:333] Evaluating on the validation split.
I0307 16:55:40.756396 140191611933888 spec.py:349] Evaluating on the test split.
I0307 16:55:42.559819 140191611933888 submission_runner.py:469] Time since start: 52122.21s, 	Step: 86471, 	{'train/accuracy': 0.7827646732330322, 'train/loss': 1.0541810989379883, 'validation/accuracy': 0.6919800043106079, 'validation/loss': 1.4586546421051025, 'validation/num_examples': 50000, 'test/accuracy': 0.5665000081062317, 'test/loss': 2.108456611633301, 'test/num_examples': 10000, 'score': 48537.77165770531, 'total_duration': 52122.21358251572, 'accumulated_submission_time': 48537.77165770531, 'accumulated_eval_time': 3564.672637462616, 'accumulated_logging_time': 10.141455173492432}
I0307 16:55:42.581539 140036200724224 logging_writer.py:48] [86471] accumulated_eval_time=3564.67, accumulated_logging_time=10.1415, accumulated_submission_time=48537.8, global_step=86471, preemption_count=0, score=48537.8, test/accuracy=0.5665, test/loss=2.10846, test/num_examples=10000, total_duration=52122.2, train/accuracy=0.782765, train/loss=1.05418, validation/accuracy=0.69198, validation/loss=1.45865, validation/num_examples=50000
I0307 16:57:25.556148 140036192331520 logging_writer.py:48] [86500] global_step=86500, grad_norm=2.2603859901428223, loss=3.006453037261963
I0307 17:04:03.842933 140036200724224 logging_writer.py:48] [86600] global_step=86600, grad_norm=2.2567238807678223, loss=3.0282678604125977
I0307 17:04:14.620553 140191611933888 spec.py:321] Evaluating on the training split.
I0307 17:04:25.613934 140191611933888 spec.py:333] Evaluating on the validation split.
I0307 17:04:47.514953 140191611933888 spec.py:349] Evaluating on the test split.
I0307 17:04:49.313576 140191611933888 submission_runner.py:469] Time since start: 52668.97s, 	Step: 86606, 	{'train/accuracy': 0.77445387840271, 'train/loss': 1.0693210363388062, 'validation/accuracy': 0.6883999705314636, 'validation/loss': 1.460637092590332, 'validation/num_examples': 50000, 'test/accuracy': 0.5678000450134277, 'test/loss': 2.085423707962036, 'test/num_examples': 10000, 'score': 49049.787569761276, 'total_duration': 52668.96734929085, 'accumulated_submission_time': 49049.787569761276, 'accumulated_eval_time': 3599.365636110306, 'accumulated_logging_time': 10.17133641242981}
I0307 17:04:49.333892 140036192331520 logging_writer.py:48] [86606] accumulated_eval_time=3599.37, accumulated_logging_time=10.1713, accumulated_submission_time=49049.8, global_step=86606, preemption_count=0, score=49049.8, test/accuracy=0.5678, test/loss=2.08542, test/num_examples=10000, total_duration=52669, train/accuracy=0.774454, train/loss=1.06932, validation/accuracy=0.6884, validation/loss=1.46064, validation/num_examples=50000
I0307 17:07:49.865725 140036200724224 logging_writer.py:48] [86700] global_step=86700, grad_norm=2.3805325031280518, loss=2.9262852668762207
I0307 17:11:21.898002 140036192331520 logging_writer.py:48] [86800] global_step=86800, grad_norm=2.2787723541259766, loss=2.999607563018799
I0307 17:13:20.943408 140191611933888 spec.py:321] Evaluating on the training split.
I0307 17:13:31.088659 140191611933888 spec.py:333] Evaluating on the validation split.
I0307 17:13:50.092462 140191611933888 spec.py:349] Evaluating on the test split.
I0307 17:13:51.903691 140191611933888 submission_runner.py:469] Time since start: 53211.56s, 	Step: 86857, 	{'train/accuracy': 0.7752710580825806, 'train/loss': 1.1167206764221191, 'validation/accuracy': 0.6937199831008911, 'validation/loss': 1.47046959400177, 'validation/num_examples': 50000, 'test/accuracy': 0.5664000511169434, 'test/loss': 2.1299562454223633, 'test/num_examples': 10000, 'score': 49561.36015033722, 'total_duration': 53211.557465553284, 'accumulated_submission_time': 49561.36015033722, 'accumulated_eval_time': 3630.325887441635, 'accumulated_logging_time': 10.199901819229126}
I0307 17:13:51.924202 140036200724224 logging_writer.py:48] [86857] accumulated_eval_time=3630.33, accumulated_logging_time=10.1999, accumulated_submission_time=49561.4, global_step=86857, preemption_count=0, score=49561.4, test/accuracy=0.5664, test/loss=2.12996, test/num_examples=10000, total_duration=53211.6, train/accuracy=0.775271, train/loss=1.11672, validation/accuracy=0.69372, validation/loss=1.47047, validation/num_examples=50000
I0307 17:15:05.692484 140036192331520 logging_writer.py:48] [86900] global_step=86900, grad_norm=2.2766430377960205, loss=2.951958179473877
I0307 17:18:38.149138 140036200724224 logging_writer.py:48] [87000] global_step=87000, grad_norm=2.2847118377685547, loss=2.9772865772247314
I0307 17:22:11.265778 140036192331520 logging_writer.py:48] [87100] global_step=87100, grad_norm=2.241025924682617, loss=3.069758653640747
I0307 17:22:21.957942 140191611933888 spec.py:321] Evaluating on the training split.
I0307 17:22:32.916863 140191611933888 spec.py:333] Evaluating on the validation split.
I0307 17:22:53.311463 140191611933888 spec.py:349] Evaluating on the test split.
I0307 17:22:55.083615 140191611933888 submission_runner.py:469] Time since start: 53754.74s, 	Step: 87106, 	{'train/accuracy': 0.7701091766357422, 'train/loss': 1.1027480363845825, 'validation/accuracy': 0.6889599561691284, 'validation/loss': 1.4552830457687378, 'validation/num_examples': 50000, 'test/accuracy': 0.5586000084877014, 'test/loss': 2.1209237575531006, 'test/num_examples': 10000, 'score': 50071.35767006874, 'total_duration': 53754.737388134, 'accumulated_submission_time': 50071.35767006874, 'accumulated_eval_time': 3663.4515252113342, 'accumulated_logging_time': 10.228155136108398}
I0307 17:22:55.194445 140036200724224 logging_writer.py:48] [87106] accumulated_eval_time=3663.45, accumulated_logging_time=10.2282, accumulated_submission_time=50071.4, global_step=87106, preemption_count=0, score=50071.4, test/accuracy=0.5586, test/loss=2.12092, test/num_examples=10000, total_duration=53754.7, train/accuracy=0.770109, train/loss=1.10275, validation/accuracy=0.68896, validation/loss=1.45528, validation/num_examples=50000
I0307 17:25:16.485138 140036192331520 logging_writer.py:48] [87200] global_step=87200, grad_norm=2.328047275543213, loss=2.955705165863037
I0307 17:27:04.282435 140036200724224 logging_writer.py:48] [87300] global_step=87300, grad_norm=2.101754665374756, loss=2.960179328918457
I0307 17:28:53.990675 140036192331520 logging_writer.py:48] [87400] global_step=87400, grad_norm=2.321446657180786, loss=2.9797298908233643
I0307 17:30:43.406966 140036200724224 logging_writer.py:48] [87500] global_step=87500, grad_norm=2.433208703994751, loss=3.0079448223114014
I0307 17:31:25.512588 140191611933888 spec.py:321] Evaluating on the training split.
I0307 17:31:36.893750 140191611933888 spec.py:333] Evaluating on the validation split.
I0307 17:31:58.795284 140191611933888 spec.py:349] Evaluating on the test split.
I0307 17:32:00.570263 140191611933888 submission_runner.py:469] Time since start: 54300.22s, 	Step: 87540, 	{'train/accuracy': 0.8050462007522583, 'train/loss': 0.9416555762290955, 'validation/accuracy': 0.6919400095939636, 'validation/loss': 1.427805781364441, 'validation/num_examples': 50000, 'test/accuracy': 0.5595000386238098, 'test/loss': 2.0933897495269775, 'test/num_examples': 10000, 'score': 50581.6177277565, 'total_duration': 54300.22401833534, 'accumulated_submission_time': 50581.6177277565, 'accumulated_eval_time': 3698.5091445446014, 'accumulated_logging_time': 10.347389459609985}
I0307 17:32:00.611177 140036192331520 logging_writer.py:48] [87540] accumulated_eval_time=3698.51, accumulated_logging_time=10.3474, accumulated_submission_time=50581.6, global_step=87540, preemption_count=0, score=50581.6, test/accuracy=0.5595, test/loss=2.09339, test/num_examples=10000, total_duration=54300.2, train/accuracy=0.805046, train/loss=0.941656, validation/accuracy=0.69194, validation/loss=1.42781, validation/num_examples=50000
I0307 17:32:58.455200 140036200724224 logging_writer.py:48] [87600] global_step=87600, grad_norm=2.3554847240448, loss=2.9986302852630615
I0307 17:34:45.595084 140036192331520 logging_writer.py:48] [87700] global_step=87700, grad_norm=2.5226149559020996, loss=3.0131168365478516
I0307 17:36:32.872464 140036200724224 logging_writer.py:48] [87800] global_step=87800, grad_norm=2.196364402770996, loss=2.97436261177063
I0307 17:38:20.552968 140036192331520 logging_writer.py:48] [87900] global_step=87900, grad_norm=2.406751871109009, loss=2.983182191848755
I0307 17:40:08.095517 140036200724224 logging_writer.py:48] [88000] global_step=88000, grad_norm=2.426849126815796, loss=3.0721116065979004
I0307 17:40:30.768609 140191611933888 spec.py:321] Evaluating on the training split.
I0307 17:40:42.159749 140191611933888 spec.py:333] Evaluating on the validation split.
I0307 17:41:04.162207 140191611933888 spec.py:349] Evaluating on the test split.
I0307 17:41:05.893054 140191611933888 submission_runner.py:469] Time since start: 54845.55s, 	Step: 88022, 	{'train/accuracy': 0.7840800285339355, 'train/loss': 1.0858005285263062, 'validation/accuracy': 0.6907399892807007, 'validation/loss': 1.4813494682312012, 'validation/num_examples': 50000, 'test/accuracy': 0.5687000155448914, 'test/loss': 2.1247341632843018, 'test/num_examples': 10000, 'score': 51091.7116689682, 'total_duration': 54845.54681921005, 'accumulated_submission_time': 51091.7116689682, 'accumulated_eval_time': 3733.63356256485, 'accumulated_logging_time': 10.396383285522461}
I0307 17:41:05.953915 140036192331520 logging_writer.py:48] [88022] accumulated_eval_time=3733.63, accumulated_logging_time=10.3964, accumulated_submission_time=51091.7, global_step=88022, preemption_count=0, score=51091.7, test/accuracy=0.5687, test/loss=2.12473, test/num_examples=10000, total_duration=54845.5, train/accuracy=0.78408, train/loss=1.0858, validation/accuracy=0.69074, validation/loss=1.48135, validation/num_examples=50000
I0307 17:42:21.411282 140036200724224 logging_writer.py:48] [88100] global_step=88100, grad_norm=2.2395126819610596, loss=3.078343629837036
I0307 17:44:09.294444 140036192331520 logging_writer.py:48] [88200] global_step=88200, grad_norm=2.237333059310913, loss=3.0112767219543457
I0307 17:45:56.643649 140036200724224 logging_writer.py:48] [88300] global_step=88300, grad_norm=2.3496286869049072, loss=3.0101406574249268
I0307 17:47:44.086844 140036192331520 logging_writer.py:48] [88400] global_step=88400, grad_norm=2.231402635574341, loss=3.048006534576416
I0307 17:49:38.039856 140191611933888 spec.py:321] Evaluating on the training split.
I0307 17:49:48.468459 140191611933888 spec.py:333] Evaluating on the validation split.
I0307 17:50:09.039194 140191611933888 spec.py:349] Evaluating on the test split.
I0307 17:50:10.810817 140191611933888 submission_runner.py:469] Time since start: 55390.46s, 	Step: 88465, 	{'train/accuracy': 0.7799345850944519, 'train/loss': 1.0690176486968994, 'validation/accuracy': 0.6926400065422058, 'validation/loss': 1.4479008913040161, 'validation/num_examples': 50000, 'test/accuracy': 0.5670000314712524, 'test/loss': 2.0885019302368164, 'test/num_examples': 10000, 'score': 51603.74013900757, 'total_duration': 55390.46458911896, 'accumulated_submission_time': 51603.74013900757, 'accumulated_eval_time': 3766.4044890403748, 'accumulated_logging_time': 10.465767621994019}
I0307 17:50:10.864487 140036200724224 logging_writer.py:48] [88465] accumulated_eval_time=3766.4, accumulated_logging_time=10.4658, accumulated_submission_time=51603.7, global_step=88465, preemption_count=0, score=51603.7, test/accuracy=0.567, test/loss=2.0885, test/num_examples=10000, total_duration=55390.5, train/accuracy=0.779935, train/loss=1.06902, validation/accuracy=0.69264, validation/loss=1.4479, validation/num_examples=50000
I0307 17:52:19.941032 140036192331520 logging_writer.py:48] [88500] global_step=88500, grad_norm=2.2855310440063477, loss=3.0051944255828857
I0307 17:58:41.749456 140191611933888 spec.py:321] Evaluating on the training split.
I0307 17:58:52.491414 140191611933888 spec.py:333] Evaluating on the validation split.
I0307 17:59:15.894992 140191611933888 spec.py:349] Evaluating on the test split.
I0307 17:59:18.105060 140191611933888 submission_runner.py:469] Time since start: 55937.76s, 	Step: 88591, 	{'train/accuracy': 0.7720224857330322, 'train/loss': 1.083070158958435, 'validation/accuracy': 0.6910200119018555, 'validation/loss': 1.4420679807662964, 'validation/num_examples': 50000, 'test/accuracy': 0.5648000240325928, 'test/loss': 2.076608419418335, 'test/num_examples': 10000, 'score': 52114.60315442085, 'total_duration': 55937.75881958008, 'accumulated_submission_time': 52114.60315442085, 'accumulated_eval_time': 3802.760051012039, 'accumulated_logging_time': 10.527741432189941}
I0307 17:59:18.127695 140036200724224 logging_writer.py:48] [88591] accumulated_eval_time=3802.76, accumulated_logging_time=10.5277, accumulated_submission_time=52114.6, global_step=88591, preemption_count=0, score=52114.6, test/accuracy=0.5648, test/loss=2.07661, test/num_examples=10000, total_duration=55937.8, train/accuracy=0.772022, train/loss=1.08307, validation/accuracy=0.69102, validation/loss=1.44207, validation/num_examples=50000
I0307 17:59:35.908975 140036192331520 logging_writer.py:48] [88600] global_step=88600, grad_norm=2.2530202865600586, loss=2.9990713596343994
I0307 18:06:42.308106 140036200724224 logging_writer.py:48] [88700] global_step=88700, grad_norm=2.1406006813049316, loss=2.98445200920105
I0307 18:07:50.114623 140191611933888 spec.py:321] Evaluating on the training split.
I0307 18:08:00.738092 140191611933888 spec.py:333] Evaluating on the validation split.
I0307 18:08:23.883037 140191611933888 spec.py:349] Evaluating on the test split.
I0307 18:08:25.630091 140191611933888 submission_runner.py:469] Time since start: 56485.28s, 	Step: 88717, 	{'train/accuracy': 0.7752112150192261, 'train/loss': 1.1010602712631226, 'validation/accuracy': 0.6972599625587463, 'validation/loss': 1.4462090730667114, 'validation/num_examples': 50000, 'test/accuracy': 0.5683000087738037, 'test/loss': 2.0938825607299805, 'test/num_examples': 10000, 'score': 52626.56941008568, 'total_duration': 56485.283853530884, 'accumulated_submission_time': 52626.56941008568, 'accumulated_eval_time': 3838.2754712104797, 'accumulated_logging_time': 10.558876276016235}
I0307 18:08:25.652106 140036192331520 logging_writer.py:48] [88717] accumulated_eval_time=3838.28, accumulated_logging_time=10.5589, accumulated_submission_time=52626.6, global_step=88717, preemption_count=0, score=52626.6, test/accuracy=0.5683, test/loss=2.09388, test/num_examples=10000, total_duration=56485.3, train/accuracy=0.775211, train/loss=1.10106, validation/accuracy=0.69726, validation/loss=1.44621, validation/num_examples=50000
I0307 18:13:43.733528 140036200724224 logging_writer.py:48] [88800] global_step=88800, grad_norm=2.5564801692962646, loss=3.0890860557556152
I0307 18:15:41.155305 140036192331520 logging_writer.py:48] [88900] global_step=88900, grad_norm=2.477959394454956, loss=2.9907546043395996
I0307 18:16:55.684156 140191611933888 spec.py:321] Evaluating on the training split.
I0307 18:17:07.116547 140191611933888 spec.py:333] Evaluating on the validation split.
I0307 18:17:29.756539 140191611933888 spec.py:349] Evaluating on the test split.
I0307 18:17:31.528796 140191611933888 submission_runner.py:469] Time since start: 57031.18s, 	Step: 88969, 	{'train/accuracy': 0.8073580861091614, 'train/loss': 0.9568255543708801, 'validation/accuracy': 0.6888200044631958, 'validation/loss': 1.4602144956588745, 'validation/num_examples': 50000, 'test/accuracy': 0.5629000067710876, 'test/loss': 2.1097867488861084, 'test/num_examples': 10000, 'score': 53136.56471943855, 'total_duration': 57031.182565927505, 'accumulated_submission_time': 53136.56471943855, 'accumulated_eval_time': 3874.120075941086, 'accumulated_logging_time': 10.589030504226685}
I0307 18:17:31.549290 140036200724224 logging_writer.py:48] [88969] accumulated_eval_time=3874.12, accumulated_logging_time=10.589, accumulated_submission_time=53136.6, global_step=88969, preemption_count=0, score=53136.6, test/accuracy=0.5629, test/loss=2.10979, test/num_examples=10000, total_duration=57031.2, train/accuracy=0.807358, train/loss=0.956826, validation/accuracy=0.68882, validation/loss=1.46021, validation/num_examples=50000
I0307 18:17:54.211323 140036192331520 logging_writer.py:48] [89000] global_step=89000, grad_norm=2.2751693725585938, loss=2.9347620010375977
I0307 18:20:06.951130 140036200724224 logging_writer.py:48] [89100] global_step=89100, grad_norm=2.46563982963562, loss=3.057821273803711
I0307 18:26:02.198025 140191611933888 spec.py:321] Evaluating on the training split.
I0307 18:26:12.952664 140191611933888 spec.py:333] Evaluating on the validation split.
I0307 18:26:35.848309 140191611933888 spec.py:349] Evaluating on the test split.
I0307 18:26:37.658988 140191611933888 submission_runner.py:469] Time since start: 57577.31s, 	Step: 89186, 	{'train/accuracy': 0.793387234210968, 'train/loss': 1.0224562883377075, 'validation/accuracy': 0.6946399807929993, 'validation/loss': 1.4530787467956543, 'validation/num_examples': 50000, 'test/accuracy': 0.5619000196456909, 'test/loss': 2.1328399181365967, 'test/num_examples': 10000, 'score': 53647.18182229996, 'total_duration': 57577.31274533272, 'accumulated_submission_time': 53647.18182229996, 'accumulated_eval_time': 3909.580999851227, 'accumulated_logging_time': 10.617105722427368}
I0307 18:26:37.680135 140036192331520 logging_writer.py:48] [89186] accumulated_eval_time=3909.58, accumulated_logging_time=10.6171, accumulated_submission_time=53647.2, global_step=89186, preemption_count=0, score=53647.2, test/accuracy=0.5619, test/loss=2.13284, test/num_examples=10000, total_duration=57577.3, train/accuracy=0.793387, train/loss=1.02246, validation/accuracy=0.69464, validation/loss=1.45308, validation/num_examples=50000
I0307 18:27:19.292617 140036200724224 logging_writer.py:48] [89200] global_step=89200, grad_norm=2.3913228511810303, loss=3.0852842330932617
I0307 18:34:23.035538 140036192331520 logging_writer.py:48] [89300] global_step=89300, grad_norm=2.3479089736938477, loss=2.952822685241699
I0307 18:35:10.026041 140191611933888 spec.py:321] Evaluating on the training split.
I0307 18:35:20.790494 140191611933888 spec.py:333] Evaluating on the validation split.
I0307 18:35:45.135220 140191611933888 spec.py:349] Evaluating on the test split.
I0307 18:35:46.916727 140191611933888 submission_runner.py:469] Time since start: 58126.57s, 	Step: 89312, 	{'train/accuracy': 0.7902582883834839, 'train/loss': 1.0390269756317139, 'validation/accuracy': 0.6925199627876282, 'validation/loss': 1.4560710191726685, 'validation/num_examples': 50000, 'test/accuracy': 0.5738000273704529, 'test/loss': 2.091273546218872, 'test/num_examples': 10000, 'score': 54159.506237745285, 'total_duration': 58126.57048201561, 'accumulated_submission_time': 54159.506237745285, 'accumulated_eval_time': 3946.4716336727142, 'accumulated_logging_time': 10.646672010421753}
I0307 18:35:46.937617 140036200724224 logging_writer.py:48] [89312] accumulated_eval_time=3946.47, accumulated_logging_time=10.6467, accumulated_submission_time=54159.5, global_step=89312, preemption_count=0, score=54159.5, test/accuracy=0.5738, test/loss=2.09127, test/num_examples=10000, total_duration=58126.6, train/accuracy=0.790258, train/loss=1.03903, validation/accuracy=0.69252, validation/loss=1.45607, validation/num_examples=50000
I0307 18:41:41.958902 140036192331520 logging_writer.py:48] [89400] global_step=89400, grad_norm=2.438626289367676, loss=3.123687267303467
I0307 18:44:18.327314 140191611933888 spec.py:321] Evaluating on the training split.
I0307 18:44:29.027153 140191611933888 spec.py:333] Evaluating on the validation split.
I0307 18:44:52.366220 140191611933888 spec.py:349] Evaluating on the test split.
I0307 18:44:54.149481 140191611933888 submission_runner.py:469] Time since start: 58673.80s, 	Step: 89438, 	{'train/accuracy': 0.7804129123687744, 'train/loss': 1.0518841743469238, 'validation/accuracy': 0.6904999613761902, 'validation/loss': 1.4516704082489014, 'validation/num_examples': 50000, 'test/accuracy': 0.5580000281333923, 'test/loss': 2.1310157775878906, 'test/num_examples': 10000, 'score': 54670.87477970123, 'total_duration': 58673.803253889084, 'accumulated_submission_time': 54670.87477970123, 'accumulated_eval_time': 3982.29376578331, 'accumulated_logging_time': 10.675749063491821}
I0307 18:44:54.170525 140036200724224 logging_writer.py:48] [89438] accumulated_eval_time=3982.29, accumulated_logging_time=10.6757, accumulated_submission_time=54670.9, global_step=89438, preemption_count=0, score=54670.9, test/accuracy=0.558, test/loss=2.13102, test/num_examples=10000, total_duration=58673.8, train/accuracy=0.780413, train/loss=1.05188, validation/accuracy=0.6905, validation/loss=1.45167, validation/num_examples=50000
I0307 18:48:58.571011 140036192331520 logging_writer.py:48] [89500] global_step=89500, grad_norm=2.274749994277954, loss=3.007772922515869
I0307 18:53:26.751743 140191611933888 spec.py:321] Evaluating on the training split.
I0307 18:53:37.615740 140191611933888 spec.py:333] Evaluating on the validation split.
I0307 18:53:59.334105 140191611933888 spec.py:349] Evaluating on the test split.
I0307 18:54:01.092919 140191611933888 submission_runner.py:469] Time since start: 59220.75s, 	Step: 89564, 	{'train/accuracy': 0.7720623016357422, 'train/loss': 1.096530556678772, 'validation/accuracy': 0.6793799996376038, 'validation/loss': 1.505624532699585, 'validation/num_examples': 50000, 'test/accuracy': 0.5509999990463257, 'test/loss': 2.172883987426758, 'test/num_examples': 10000, 'score': 55183.433886766434, 'total_duration': 59220.74669146538, 'accumulated_submission_time': 55183.433886766434, 'accumulated_eval_time': 4016.6349081993103, 'accumulated_logging_time': 10.704985857009888}
I0307 18:54:01.114517 140036200724224 logging_writer.py:48] [89564] accumulated_eval_time=4016.63, accumulated_logging_time=10.705, accumulated_submission_time=55183.4, global_step=89564, preemption_count=0, score=55183.4, test/accuracy=0.551, test/loss=2.17288, test/num_examples=10000, total_duration=59220.7, train/accuracy=0.772062, train/loss=1.09653, validation/accuracy=0.67938, validation/loss=1.50562, validation/num_examples=50000
I0307 18:56:14.559729 140036192331520 logging_writer.py:48] [89600] global_step=89600, grad_norm=2.678953170776367, loss=3.0658371448516846
I0307 19:02:28.269274 140036200724224 logging_writer.py:48] [89700] global_step=89700, grad_norm=2.1764614582061768, loss=2.963261842727661
I0307 19:02:32.481856 140191611933888 spec.py:321] Evaluating on the training split.
I0307 19:02:43.584939 140191611933888 spec.py:333] Evaluating on the validation split.
I0307 19:03:04.063046 140191611933888 spec.py:349] Evaluating on the test split.
I0307 19:03:05.855925 140191611933888 submission_runner.py:469] Time since start: 59765.51s, 	Step: 89703, 	{'train/accuracy': 0.7728993892669678, 'train/loss': 1.0548344850540161, 'validation/accuracy': 0.6851399540901184, 'validation/loss': 1.4541112184524536, 'validation/num_examples': 50000, 'test/accuracy': 0.5579000115394592, 'test/loss': 2.1299798488616943, 'test/num_examples': 10000, 'score': 55694.77769470215, 'total_duration': 59765.50969982147, 'accumulated_submission_time': 55694.77769470215, 'accumulated_eval_time': 4050.008941411972, 'accumulated_logging_time': 10.734372854232788}
I0307 19:03:05.877174 140036192331520 logging_writer.py:48] [89703] accumulated_eval_time=4050.01, accumulated_logging_time=10.7344, accumulated_submission_time=55694.8, global_step=89703, preemption_count=0, score=55694.8, test/accuracy=0.5579, test/loss=2.12998, test/num_examples=10000, total_duration=59765.5, train/accuracy=0.772899, train/loss=1.05483, validation/accuracy=0.68514, validation/loss=1.45411, validation/num_examples=50000
I0307 19:06:12.730335 140036200724224 logging_writer.py:48] [89800] global_step=89800, grad_norm=2.257722854614258, loss=2.9228241443634033
I0307 19:09:54.580049 140036192331520 logging_writer.py:48] [89900] global_step=89900, grad_norm=2.1814584732055664, loss=2.9594757556915283
I0307 19:11:37.940967 140191611933888 spec.py:321] Evaluating on the training split.
I0307 19:11:48.959020 140191611933888 spec.py:333] Evaluating on the validation split.
I0307 19:12:08.358393 140191611933888 spec.py:349] Evaluating on the test split.
I0307 19:12:10.164089 140191611933888 submission_runner.py:469] Time since start: 60309.82s, 	Step: 89946, 	{'train/accuracy': 0.7827646732330322, 'train/loss': 1.0659565925598145, 'validation/accuracy': 0.6953999996185303, 'validation/loss': 1.4379647970199585, 'validation/num_examples': 50000, 'test/accuracy': 0.5689000487327576, 'test/loss': 2.0999844074249268, 'test/num_examples': 10000, 'score': 56206.807099580765, 'total_duration': 60309.817862033844, 'accumulated_submission_time': 56206.807099580765, 'accumulated_eval_time': 4082.232030391693, 'accumulated_logging_time': 10.763605833053589}
I0307 19:12:10.185299 140036200724224 logging_writer.py:48] [89946] accumulated_eval_time=4082.23, accumulated_logging_time=10.7636, accumulated_submission_time=56206.8, global_step=89946, preemption_count=0, score=56206.8, test/accuracy=0.5689, test/loss=2.09998, test/num_examples=10000, total_duration=60309.8, train/accuracy=0.782765, train/loss=1.06596, validation/accuracy=0.6954, validation/loss=1.43796, validation/num_examples=50000
I0307 19:13:44.245656 140036192331520 logging_writer.py:48] [90000] global_step=90000, grad_norm=2.194446325302124, loss=2.889547824859619
I0307 19:17:16.429260 140036200724224 logging_writer.py:48] [90100] global_step=90100, grad_norm=2.4315898418426514, loss=2.983884811401367
I0307 19:20:41.240042 140191611933888 spec.py:321] Evaluating on the training split.
I0307 19:20:52.301509 140191611933888 spec.py:333] Evaluating on the validation split.
I0307 19:21:14.192757 140191611933888 spec.py:349] Evaluating on the test split.
I0307 19:21:15.951931 140191611933888 submission_runner.py:469] Time since start: 60855.61s, 	Step: 90197, 	{'train/accuracy': 0.7810905575752258, 'train/loss': 1.0500255823135376, 'validation/accuracy': 0.6977199912071228, 'validation/loss': 1.4182322025299072, 'validation/num_examples': 50000, 'test/accuracy': 0.573900043964386, 'test/loss': 2.066542625427246, 'test/num_examples': 10000, 'score': 56717.82669019699, 'total_duration': 60855.6057035923, 'accumulated_submission_time': 56717.82669019699, 'accumulated_eval_time': 4116.943884849548, 'accumulated_logging_time': 10.793275356292725}
I0307 19:21:15.990530 140036192331520 logging_writer.py:48] [90197] accumulated_eval_time=4116.94, accumulated_logging_time=10.7933, accumulated_submission_time=56717.8, global_step=90197, preemption_count=0, score=56717.8, test/accuracy=0.5739, test/loss=2.06654, test/num_examples=10000, total_duration=60855.6, train/accuracy=0.781091, train/loss=1.05003, validation/accuracy=0.69772, validation/loss=1.41823, validation/num_examples=50000
I0307 19:21:17.518749 140036200724224 logging_writer.py:48] [90200] global_step=90200, grad_norm=2.2788350582122803, loss=3.023336410522461
I0307 19:24:32.305981 140036192331520 logging_writer.py:48] [90300] global_step=90300, grad_norm=2.454129934310913, loss=3.0755362510681152
I0307 19:28:04.025254 140036200724224 logging_writer.py:48] [90400] global_step=90400, grad_norm=2.4947659969329834, loss=2.9695074558258057
I0307 19:29:47.763026 140191611933888 spec.py:321] Evaluating on the training split.
I0307 19:29:58.663072 140191611933888 spec.py:333] Evaluating on the validation split.
I0307 19:30:20.797681 140191611933888 spec.py:349] Evaluating on the test split.
I0307 19:30:22.587996 140191611933888 submission_runner.py:469] Time since start: 61402.24s, 	Step: 90450, 	{'train/accuracy': 0.7820671200752258, 'train/loss': 1.095510482788086, 'validation/accuracy': 0.6970799565315247, 'validation/loss': 1.4553059339523315, 'validation/num_examples': 50000, 'test/accuracy': 0.5752000212669373, 'test/loss': 2.0966646671295166, 'test/num_examples': 10000, 'score': 57229.56269788742, 'total_duration': 61402.24177098274, 'accumulated_submission_time': 57229.56269788742, 'accumulated_eval_time': 4151.768825769424, 'accumulated_logging_time': 10.840416431427002}
I0307 19:30:22.609457 140036192331520 logging_writer.py:48] [90450] accumulated_eval_time=4151.77, accumulated_logging_time=10.8404, accumulated_submission_time=57229.6, global_step=90450, preemption_count=0, score=57229.6, test/accuracy=0.5752, test/loss=2.09666, test/num_examples=10000, total_duration=61402.2, train/accuracy=0.782067, train/loss=1.09551, validation/accuracy=0.69708, validation/loss=1.45531, validation/num_examples=50000
I0307 19:31:49.821347 140036200724224 logging_writer.py:48] [90500] global_step=90500, grad_norm=2.2251198291778564, loss=2.9702677726745605
I0307 19:35:21.561823 140036192331520 logging_writer.py:48] [90600] global_step=90600, grad_norm=2.2016899585723877, loss=2.964334011077881
I0307 19:38:52.846204 140191611933888 spec.py:321] Evaluating on the training split.
I0307 19:39:04.005853 140191611933888 spec.py:333] Evaluating on the validation split.
I0307 19:39:25.896713 140191611933888 spec.py:349] Evaluating on the test split.
I0307 19:39:27.647863 140191611933888 submission_runner.py:469] Time since start: 61947.30s, 	Step: 90700, 	{'train/accuracy': 0.8140744566917419, 'train/loss': 0.9294105768203735, 'validation/accuracy': 0.6945399641990662, 'validation/loss': 1.4238859415054321, 'validation/num_examples': 50000, 'test/accuracy': 0.5665000081062317, 'test/loss': 2.08419132232666, 'test/num_examples': 10000, 'score': 57739.76331591606, 'total_duration': 61947.30162739754, 'accumulated_submission_time': 57739.76331591606, 'accumulated_eval_time': 4186.570454835892, 'accumulated_logging_time': 10.86995792388916}
I0307 19:39:27.669833 140036200724224 logging_writer.py:48] [90700] accumulated_eval_time=4186.57, accumulated_logging_time=10.87, accumulated_submission_time=57739.8, global_step=90700, preemption_count=0, score=57739.8, test/accuracy=0.5665, test/loss=2.08419, test/num_examples=10000, total_duration=61947.3, train/accuracy=0.814074, train/loss=0.929411, validation/accuracy=0.69454, validation/loss=1.42389, validation/num_examples=50000
I0307 19:39:28.031132 140036192331520 logging_writer.py:48] [90700] global_step=90700, grad_norm=2.4023327827453613, loss=3.013216018676758
I0307 19:42:38.752195 140036200724224 logging_writer.py:48] [90800] global_step=90800, grad_norm=2.4843432903289795, loss=2.9857418537139893
I0307 19:46:10.486569 140036192331520 logging_writer.py:48] [90900] global_step=90900, grad_norm=2.3950886726379395, loss=3.0021328926086426
I0307 19:47:57.891292 140191611933888 spec.py:321] Evaluating on the training split.
I0307 19:48:09.990418 140191611933888 spec.py:333] Evaluating on the validation split.
I0307 19:48:31.340416 140191611933888 spec.py:349] Evaluating on the test split.
I0307 19:48:33.213442 140191611933888 submission_runner.py:469] Time since start: 62492.87s, 	Step: 90966, 	{'train/accuracy': 0.7899593114852905, 'train/loss': 1.0642435550689697, 'validation/accuracy': 0.6909399628639221, 'validation/loss': 1.480566382408142, 'validation/num_examples': 50000, 'test/accuracy': 0.5604000091552734, 'test/loss': 2.151770830154419, 'test/num_examples': 10000, 'score': 58249.934388399124, 'total_duration': 62492.867218732834, 'accumulated_submission_time': 58249.934388399124, 'accumulated_eval_time': 4221.892584562302, 'accumulated_logging_time': 10.913435220718384}
I0307 19:48:33.267351 140036200724224 logging_writer.py:48] [90966] accumulated_eval_time=4221.89, accumulated_logging_time=10.9134, accumulated_submission_time=58249.9, global_step=90966, preemption_count=0, score=58249.9, test/accuracy=0.5604, test/loss=2.15177, test/num_examples=10000, total_duration=62492.9, train/accuracy=0.789959, train/loss=1.06424, validation/accuracy=0.69094, validation/loss=1.48057, validation/num_examples=50000
I0307 19:48:53.260927 140036192331520 logging_writer.py:48] [91000] global_step=91000, grad_norm=2.318690776824951, loss=3.009225845336914
I0307 19:50:22.835906 140036200724224 logging_writer.py:48] [91100] global_step=91100, grad_norm=2.3525807857513428, loss=2.9380807876586914
I0307 19:51:55.066903 140036192331520 logging_writer.py:48] [91200] global_step=91200, grad_norm=2.3071579933166504, loss=2.9874022006988525
I0307 19:53:20.175536 140036200724224 logging_writer.py:48] [91300] global_step=91300, grad_norm=2.3598694801330566, loss=2.9539763927459717
I0307 19:54:37.570353 140036192331520 logging_writer.py:48] [91400] global_step=91400, grad_norm=2.427117347717285, loss=2.9661219120025635
I0307 19:55:51.846119 140036200724224 logging_writer.py:48] [91500] global_step=91500, grad_norm=2.4880802631378174, loss=3.0968265533447266
I0307 19:57:03.870388 140191611933888 spec.py:321] Evaluating on the training split.
I0307 19:57:15.794814 140191611933888 spec.py:333] Evaluating on the validation split.
I0307 19:57:41.476640 140191611933888 spec.py:349] Evaluating on the test split.
I0307 19:57:43.243352 140191611933888 submission_runner.py:469] Time since start: 63042.90s, 	Step: 91597, 	{'train/accuracy': 0.7754902839660645, 'train/loss': 1.0823265314102173, 'validation/accuracy': 0.685759961605072, 'validation/loss': 1.4598616361618042, 'validation/num_examples': 50000, 'test/accuracy': 0.5568000078201294, 'test/loss': 2.1180851459503174, 'test/num_examples': 10000, 'score': 58760.41983580589, 'total_duration': 63042.89712166786, 'accumulated_submission_time': 58760.41983580589, 'accumulated_eval_time': 4261.265513181686, 'accumulated_logging_time': 11.013250589370728}
I0307 19:57:43.276932 140036192331520 logging_writer.py:48] [91597] accumulated_eval_time=4261.27, accumulated_logging_time=11.0133, accumulated_submission_time=58760.4, global_step=91597, preemption_count=0, score=58760.4, test/accuracy=0.5568, test/loss=2.11809, test/num_examples=10000, total_duration=63042.9, train/accuracy=0.77549, train/loss=1.08233, validation/accuracy=0.68576, validation/loss=1.45986, validation/num_examples=50000
I0307 19:57:44.805425 140036200724224 logging_writer.py:48] [91600] global_step=91600, grad_norm=2.289630174636841, loss=3.0455265045166016
I0307 19:58:54.069618 140036192331520 logging_writer.py:48] [91700] global_step=91700, grad_norm=2.3003809452056885, loss=3.0108273029327393
I0307 20:00:08.329891 140036200724224 logging_writer.py:48] [91800] global_step=91800, grad_norm=2.376413345336914, loss=2.948307514190674
I0307 20:01:22.234107 140036192331520 logging_writer.py:48] [91900] global_step=91900, grad_norm=2.2049639225006104, loss=2.985006809234619
I0307 20:02:36.108887 140036200724224 logging_writer.py:48] [92000] global_step=92000, grad_norm=2.344261884689331, loss=3.034743309020996
I0307 20:03:50.289077 140036192331520 logging_writer.py:48] [92100] global_step=92100, grad_norm=2.393824577331543, loss=2.9855000972747803
I0307 20:06:07.082966 140036200724224 logging_writer.py:48] [92200] global_step=92200, grad_norm=2.3263258934020996, loss=2.940214157104492
I0307 20:06:13.986531 140191611933888 spec.py:321] Evaluating on the training split.
I0307 20:06:25.071595 140191611933888 spec.py:333] Evaluating on the validation split.
I0307 20:06:48.247432 140191611933888 spec.py:349] Evaluating on the test split.
I0307 20:06:49.977481 140191611933888 submission_runner.py:469] Time since start: 63589.63s, 	Step: 92206, 	{'train/accuracy': 0.802176296710968, 'train/loss': 0.9829091429710388, 'validation/accuracy': 0.6957199573516846, 'validation/loss': 1.429304838180542, 'validation/num_examples': 50000, 'test/accuracy': 0.5660000443458557, 'test/loss': 2.0917460918426514, 'test/num_examples': 10000, 'score': 59271.054829359055, 'total_duration': 63589.63125872612, 'accumulated_submission_time': 59271.054829359055, 'accumulated_eval_time': 4297.256429433823, 'accumulated_logging_time': 11.054826736450195}
I0307 20:06:50.021433 140036192331520 logging_writer.py:48] [92206] accumulated_eval_time=4297.26, accumulated_logging_time=11.0548, accumulated_submission_time=59271.1, global_step=92206, preemption_count=0, score=59271.1, test/accuracy=0.566, test/loss=2.09175, test/num_examples=10000, total_duration=63589.6, train/accuracy=0.802176, train/loss=0.982909, validation/accuracy=0.69572, validation/loss=1.4293, validation/num_examples=50000
I0307 20:08:50.241018 140036200724224 logging_writer.py:48] [92300] global_step=92300, grad_norm=2.2754571437835693, loss=3.0229618549346924
I0307 20:11:13.053777 140036192331520 logging_writer.py:48] [92400] global_step=92400, grad_norm=2.472254991531372, loss=3.0145375728607178
I0307 20:13:35.539032 140036200724224 logging_writer.py:48] [92500] global_step=92500, grad_norm=2.4257290363311768, loss=2.952989101409912
2025-03-07 20:15:10.166466: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 20:15:21.412686 140191611933888 spec.py:321] Evaluating on the training split.
I0307 20:15:32.322095 140191611933888 spec.py:333] Evaluating on the validation split.
I0307 20:15:53.686074 140191611933888 spec.py:349] Evaluating on the test split.
I0307 20:15:55.437568 140191611933888 submission_runner.py:469] Time since start: 64135.09s, 	Step: 92575, 	{'train/accuracy': 0.7879663705825806, 'train/loss': 1.0225143432617188, 'validation/accuracy': 0.6972599625587463, 'validation/loss': 1.414861798286438, 'validation/num_examples': 50000, 'test/accuracy': 0.5729000568389893, 'test/loss': 2.061185359954834, 'test/num_examples': 10000, 'score': 59782.395458221436, 'total_duration': 64135.09134721756, 'accumulated_submission_time': 59782.395458221436, 'accumulated_eval_time': 4331.281293392181, 'accumulated_logging_time': 11.108251333236694}
I0307 20:15:55.515562 140036192331520 logging_writer.py:48] [92575] accumulated_eval_time=4331.28, accumulated_logging_time=11.1083, accumulated_submission_time=59782.4, global_step=92575, preemption_count=0, score=59782.4, test/accuracy=0.5729, test/loss=2.06119, test/num_examples=10000, total_duration=64135.1, train/accuracy=0.787966, train/loss=1.02251, validation/accuracy=0.69726, validation/loss=1.41486, validation/num_examples=50000
I0307 20:16:19.740660 140036200724224 logging_writer.py:48] [92600] global_step=92600, grad_norm=2.1946823596954346, loss=2.99143385887146
I0307 20:18:42.278746 140036192331520 logging_writer.py:48] [92700] global_step=92700, grad_norm=2.4923133850097656, loss=2.989677906036377
I0307 20:21:04.438577 140036200724224 logging_writer.py:48] [92800] global_step=92800, grad_norm=2.598376750946045, loss=3.011561632156372
I0307 20:23:26.752731 140036192331520 logging_writer.py:48] [92900] global_step=92900, grad_norm=2.482602119445801, loss=3.014910936355591
I0307 20:24:26.344499 140191611933888 spec.py:321] Evaluating on the training split.
I0307 20:24:37.284003 140191611933888 spec.py:333] Evaluating on the validation split.
I0307 20:24:58.428760 140191611933888 spec.py:349] Evaluating on the test split.
I0307 20:25:00.214436 140191611933888 submission_runner.py:469] Time since start: 64679.87s, 	Step: 92943, 	{'train/accuracy': 0.7697703838348389, 'train/loss': 1.1075923442840576, 'validation/accuracy': 0.6868599653244019, 'validation/loss': 1.4848952293395996, 'validation/num_examples': 50000, 'test/accuracy': 0.5653000473976135, 'test/loss': 2.10247540473938, 'test/num_examples': 10000, 'score': 60293.17581152916, 'total_duration': 64679.86819791794, 'accumulated_submission_time': 60293.17581152916, 'accumulated_eval_time': 4365.151185512543, 'accumulated_logging_time': 11.19473147392273}
I0307 20:25:00.273922 140036200724224 logging_writer.py:48] [92943] accumulated_eval_time=4365.15, accumulated_logging_time=11.1947, accumulated_submission_time=60293.2, global_step=92943, preemption_count=0, score=60293.2, test/accuracy=0.5653, test/loss=2.10248, test/num_examples=10000, total_duration=64679.9, train/accuracy=0.76977, train/loss=1.10759, validation/accuracy=0.68686, validation/loss=1.4849, validation/num_examples=50000
I0307 20:26:08.600271 140036192331520 logging_writer.py:48] [93000] global_step=93000, grad_norm=2.471630811691284, loss=2.990337610244751
I0307 20:28:31.146260 140036200724224 logging_writer.py:48] [93100] global_step=93100, grad_norm=2.4160921573638916, loss=3.0002760887145996
I0307 20:30:53.901612 140036192331520 logging_writer.py:48] [93200] global_step=93200, grad_norm=2.52115797996521, loss=2.9388585090637207
I0307 20:33:16.760407 140036200724224 logging_writer.py:48] [93300] global_step=93300, grad_norm=2.325603723526001, loss=2.9280922412872314
I0307 20:33:31.052386 140191611933888 spec.py:321] Evaluating on the training split.
I0307 20:33:41.964796 140191611933888 spec.py:333] Evaluating on the validation split.
I0307 20:34:03.707109 140191611933888 spec.py:349] Evaluating on the test split.
I0307 20:34:05.492447 140191611933888 submission_runner.py:469] Time since start: 65225.15s, 	Step: 93311, 	{'train/accuracy': 0.7724011540412903, 'train/loss': 1.106764316558838, 'validation/accuracy': 0.6890999674797058, 'validation/loss': 1.47303307056427, 'validation/num_examples': 50000, 'test/accuracy': 0.5609000325202942, 'test/loss': 2.1311542987823486, 'test/num_examples': 10000, 'score': 60803.90449619293, 'total_duration': 65225.14622235298, 'accumulated_submission_time': 60803.90449619293, 'accumulated_eval_time': 4399.5912129879, 'accumulated_logging_time': 11.262699842453003}
I0307 20:34:05.535431 140036192331520 logging_writer.py:48] [93311] accumulated_eval_time=4399.59, accumulated_logging_time=11.2627, accumulated_submission_time=60803.9, global_step=93311, preemption_count=0, score=60803.9, test/accuracy=0.5609, test/loss=2.13115, test/num_examples=10000, total_duration=65225.1, train/accuracy=0.772401, train/loss=1.10676, validation/accuracy=0.6891, validation/loss=1.47303, validation/num_examples=50000
I0307 20:35:59.120537 140036200724224 logging_writer.py:48] [93400] global_step=93400, grad_norm=2.300658941268921, loss=2.9540815353393555
I0307 20:39:10.123106 140036192331520 logging_writer.py:48] [93500] global_step=93500, grad_norm=2.362187385559082, loss=2.8529350757598877
I0307 20:42:37.311702 140191611933888 spec.py:321] Evaluating on the training split.
I0307 20:42:48.010303 140191611933888 spec.py:333] Evaluating on the validation split.
I0307 20:43:08.455236 140191611933888 spec.py:349] Evaluating on the test split.
I0307 20:43:10.209716 140191611933888 submission_runner.py:469] Time since start: 65769.86s, 	Step: 93599, 	{'train/accuracy': 0.8108258843421936, 'train/loss': 0.9398601651191711, 'validation/accuracy': 0.6992799639701843, 'validation/loss': 1.4104418754577637, 'validation/num_examples': 50000, 'test/accuracy': 0.5777000188827515, 'test/loss': 2.0372133255004883, 'test/num_examples': 10000, 'score': 61315.64055657387, 'total_duration': 65769.86347270012, 'accumulated_submission_time': 61315.64055657387, 'accumulated_eval_time': 4432.489187717438, 'accumulated_logging_time': 11.314046859741211}
I0307 20:43:10.250833 140036200724224 logging_writer.py:48] [93599] accumulated_eval_time=4432.49, accumulated_logging_time=11.314, accumulated_submission_time=61315.6, global_step=93599, preemption_count=0, score=61315.6, test/accuracy=0.5777, test/loss=2.03721, test/num_examples=10000, total_duration=65769.9, train/accuracy=0.810826, train/loss=0.93986, validation/accuracy=0.69928, validation/loss=1.41044, validation/num_examples=50000
I0307 20:43:11.024189 140036192331520 logging_writer.py:48] [93600] global_step=93600, grad_norm=2.4769649505615234, loss=3.0211682319641113
I0307 20:46:20.536497 140036200724224 logging_writer.py:48] [93700] global_step=93700, grad_norm=2.299302339553833, loss=2.9673194885253906
I0307 20:49:50.883446 140036192331520 logging_writer.py:48] [93800] global_step=93800, grad_norm=2.280926465988159, loss=3.002794027328491
2025-03-07 20:50:28.618473: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 20:51:40.295736 140191611933888 spec.py:321] Evaluating on the training split.
I0307 20:51:51.309063 140191611933888 spec.py:333] Evaluating on the validation split.
I0307 20:52:10.344819 140191611933888 spec.py:349] Evaluating on the test split.
I0307 20:52:12.107603 140191611933888 submission_runner.py:469] Time since start: 66311.76s, 	Step: 93852, 	{'train/accuracy': 0.7962173223495483, 'train/loss': 1.0093600749969482, 'validation/accuracy': 0.6981599926948547, 'validation/loss': 1.4332609176635742, 'validation/num_examples': 50000, 'test/accuracy': 0.5707000494003296, 'test/loss': 2.074535369873047, 'test/num_examples': 10000, 'score': 61825.60519719124, 'total_duration': 66311.76137256622, 'accumulated_submission_time': 61825.60519719124, 'accumulated_eval_time': 4464.301021337509, 'accumulated_logging_time': 11.407354354858398}
I0307 20:52:12.158321 140036200724224 logging_writer.py:48] [93852] accumulated_eval_time=4464.3, accumulated_logging_time=11.4074, accumulated_submission_time=61825.6, global_step=93852, preemption_count=0, score=61825.6, test/accuracy=0.5707, test/loss=2.07454, test/num_examples=10000, total_duration=66311.8, train/accuracy=0.796217, train/loss=1.00936, validation/accuracy=0.69816, validation/loss=1.43326, validation/num_examples=50000
I0307 20:53:33.009351 140036192331520 logging_writer.py:48] [93900] global_step=93900, grad_norm=2.366565465927124, loss=2.921311378479004
I0307 20:57:03.166780 140036200724224 logging_writer.py:48] [94000] global_step=94000, grad_norm=2.554785966873169, loss=3.104123115539551
I0307 21:00:34.785892 140036192331520 logging_writer.py:48] [94100] global_step=94100, grad_norm=2.4787769317626953, loss=2.9811251163482666
I0307 21:00:43.236267 140191611933888 spec.py:321] Evaluating on the training split.
I0307 21:00:54.155250 140191611933888 spec.py:333] Evaluating on the validation split.
I0307 21:01:14.197446 140191611933888 spec.py:349] Evaluating on the test split.
I0307 21:01:15.991791 140191611933888 submission_runner.py:469] Time since start: 66855.65s, 	Step: 94105, 	{'train/accuracy': 0.7833226919174194, 'train/loss': 1.0635826587677002, 'validation/accuracy': 0.6948599815368652, 'validation/loss': 1.4533624649047852, 'validation/num_examples': 50000, 'test/accuracy': 0.5742000341415405, 'test/loss': 2.0841734409332275, 'test/num_examples': 10000, 'score': 62336.62378716469, 'total_duration': 66855.64555573463, 'accumulated_submission_time': 62336.62378716469, 'accumulated_eval_time': 4497.056501150131, 'accumulated_logging_time': 11.490060091018677}
I0307 21:01:16.051039 140036200724224 logging_writer.py:48] [94105] accumulated_eval_time=4497.06, accumulated_logging_time=11.4901, accumulated_submission_time=62336.6, global_step=94105, preemption_count=0, score=62336.6, test/accuracy=0.5742, test/loss=2.08417, test/num_examples=10000, total_duration=66855.6, train/accuracy=0.783323, train/loss=1.06358, validation/accuracy=0.69486, validation/loss=1.45336, validation/num_examples=50000
I0307 21:04:16.782925 140036192331520 logging_writer.py:48] [94200] global_step=94200, grad_norm=2.322194814682007, loss=2.954946756362915
I0307 21:07:47.749243 140036200724224 logging_writer.py:48] [94300] global_step=94300, grad_norm=2.3713009357452393, loss=2.9553475379943848
I0307 21:09:47.065551 140191611933888 spec.py:321] Evaluating on the training split.
I0307 21:09:58.193033 140191611933888 spec.py:333] Evaluating on the validation split.
I0307 21:10:17.022100 140191611933888 spec.py:349] Evaluating on the test split.
I0307 21:10:18.816646 140191611933888 submission_runner.py:469] Time since start: 67398.47s, 	Step: 94357, 	{'train/accuracy': 0.7841199040412903, 'train/loss': 1.0602374076843262, 'validation/accuracy': 0.6938799619674683, 'validation/loss': 1.4602330923080444, 'validation/num_examples': 50000, 'test/accuracy': 0.5738000273704529, 'test/loss': 2.077045202255249, 'test/num_examples': 10000, 'score': 62847.5939848423, 'total_duration': 67398.47040963173, 'accumulated_submission_time': 62847.5939848423, 'accumulated_eval_time': 4528.807563781738, 'accumulated_logging_time': 11.564785242080688}
I0307 21:10:18.842437 140036192331520 logging_writer.py:48] [94357] accumulated_eval_time=4528.81, accumulated_logging_time=11.5648, accumulated_submission_time=62847.6, global_step=94357, preemption_count=0, score=62847.6, test/accuracy=0.5738, test/loss=2.07705, test/num_examples=10000, total_duration=67398.5, train/accuracy=0.78412, train/loss=1.06024, validation/accuracy=0.69388, validation/loss=1.46023, validation/num_examples=50000
I0307 21:11:28.164506 140036200724224 logging_writer.py:48] [94400] global_step=94400, grad_norm=2.470686912536621, loss=3.100148916244507
I0307 21:14:59.034249 140036192331520 logging_writer.py:48] [94500] global_step=94500, grad_norm=2.583728313446045, loss=3.0341405868530273
I0307 21:18:31.805421 140036200724224 logging_writer.py:48] [94600] global_step=94600, grad_norm=2.4021718502044678, loss=3.0703847408294678
I0307 21:18:50.828716 140191611933888 spec.py:321] Evaluating on the training split.
I0307 21:19:01.944217 140191611933888 spec.py:333] Evaluating on the validation split.
I0307 21:19:23.938270 140191611933888 spec.py:349] Evaluating on the test split.
I0307 21:19:25.735008 140191611933888 submission_runner.py:469] Time since start: 67945.39s, 	Step: 94610, 	{'train/accuracy': 0.7872090339660645, 'train/loss': 1.0548466444015503, 'validation/accuracy': 0.7005400061607361, 'validation/loss': 1.423754334449768, 'validation/num_examples': 50000, 'test/accuracy': 0.5754000544548035, 'test/loss': 2.073009490966797, 'test/num_examples': 10000, 'score': 63359.5439684391, 'total_duration': 67945.38875842094, 'accumulated_submission_time': 63359.5439684391, 'accumulated_eval_time': 4563.713807344437, 'accumulated_logging_time': 11.59960651397705}
I0307 21:19:25.804300 140036192331520 logging_writer.py:48] [94610] accumulated_eval_time=4563.71, accumulated_logging_time=11.5996, accumulated_submission_time=63359.5, global_step=94610, preemption_count=0, score=63359.5, test/accuracy=0.5754, test/loss=2.07301, test/num_examples=10000, total_duration=67945.4, train/accuracy=0.787209, train/loss=1.05485, validation/accuracy=0.70054, validation/loss=1.42375, validation/num_examples=50000
I0307 21:22:16.531690 140036200724224 logging_writer.py:48] [94700] global_step=94700, grad_norm=2.579131841659546, loss=2.9610958099365234
I0307 21:25:47.945472 140036192331520 logging_writer.py:48] [94800] global_step=94800, grad_norm=2.413015127182007, loss=2.9936158657073975
I0307 21:27:57.726695 140191611933888 spec.py:321] Evaluating on the training split.
I0307 21:28:08.833693 140191611933888 spec.py:333] Evaluating on the validation split.
I0307 21:28:30.381265 140191611933888 spec.py:349] Evaluating on the test split.
I0307 21:28:32.291696 140191611933888 submission_runner.py:469] Time since start: 68491.95s, 	Step: 94862, 	{'train/accuracy': 0.7851163744926453, 'train/loss': 1.0493755340576172, 'validation/accuracy': 0.7021999955177307, 'validation/loss': 1.429882287979126, 'validation/num_examples': 50000, 'test/accuracy': 0.5784000158309937, 'test/loss': 2.059344530105591, 'test/num_examples': 10000, 'score': 63871.428904771805, 'total_duration': 68491.94546818733, 'accumulated_submission_time': 63871.428904771805, 'accumulated_eval_time': 4598.2787845134735, 'accumulated_logging_time': 11.677581071853638}
I0307 21:28:32.379040 140036200724224 logging_writer.py:48] [94862] accumulated_eval_time=4598.28, accumulated_logging_time=11.6776, accumulated_submission_time=63871.4, global_step=94862, preemption_count=0, score=63871.4, test/accuracy=0.5784, test/loss=2.05934, test/num_examples=10000, total_duration=68491.9, train/accuracy=0.785116, train/loss=1.04938, validation/accuracy=0.7022, validation/loss=1.42988, validation/num_examples=50000
I0307 21:29:33.100873 140036192331520 logging_writer.py:48] [94900] global_step=94900, grad_norm=2.4800117015838623, loss=2.9714913368225098
I0307 21:33:03.992044 140036200724224 logging_writer.py:48] [95000] global_step=95000, grad_norm=2.31615948677063, loss=2.978649377822876
2025-03-07 21:35:30.523456: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 91453 bytes after encountering the first element of size 91453 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
I0307 21:36:37.936001 140036192331520 logging_writer.py:48] [95100] global_step=95100, grad_norm=2.4947118759155273, loss=3.0299742221832275
I0307 21:37:03.652130 140191611933888 spec.py:321] Evaluating on the training split.
I0307 21:37:14.907548 140191611933888 spec.py:333] Evaluating on the validation split.
I0307 21:37:36.209545 140191611933888 spec.py:349] Evaluating on the test split.
I0307 21:37:37.996143 140191611933888 submission_runner.py:469] Time since start: 69037.65s, 	Step: 95113, 	{'train/accuracy': 0.8137555718421936, 'train/loss': 0.9395840764045715, 'validation/accuracy': 0.6972399950027466, 'validation/loss': 1.4292817115783691, 'validation/num_examples': 50000, 'test/accuracy': 0.5726000070571899, 'test/loss': 2.0850026607513428, 'test/num_examples': 10000, 'score': 64382.65586090088, 'total_duration': 69037.64989924431, 'accumulated_submission_time': 64382.65586090088, 'accumulated_eval_time': 4632.622745990753, 'accumulated_logging_time': 11.78269362449646}
I0307 21:37:38.017783 140036200724224 logging_writer.py:48] [95113] accumulated_eval_time=4632.62, accumulated_logging_time=11.7827, accumulated_submission_time=64382.7, global_step=95113, preemption_count=0, score=64382.7, test/accuracy=0.5726, test/loss=2.085, test/num_examples=10000, total_duration=69037.6, train/accuracy=0.813756, train/loss=0.939584, validation/accuracy=0.69724, validation/loss=1.42928, validation/num_examples=50000
I0307 21:40:20.788950 140036192331520 logging_writer.py:48] [95200] global_step=95200, grad_norm=2.2126147747039795, loss=2.8941590785980225
I0307 21:43:51.941356 140036200724224 logging_writer.py:48] [95300] global_step=95300, grad_norm=2.5452892780303955, loss=3.018857955932617
I0307 21:46:09.461503 140191611933888 spec.py:321] Evaluating on the training split.
I0307 21:46:20.813507 140191611933888 spec.py:333] Evaluating on the validation split.
I0307 21:46:42.990048 140191611933888 spec.py:349] Evaluating on the test split.
I0307 21:46:44.788304 140191611933888 submission_runner.py:469] Time since start: 69584.44s, 	Step: 95366, 	{'train/accuracy': 0.8052654266357422, 'train/loss': 0.9614248275756836, 'validation/accuracy': 0.7026399970054626, 'validation/loss': 1.4003020524978638, 'validation/num_examples': 50000, 'test/accuracy': 0.5766000151634216, 'test/loss': 2.0446157455444336, 'test/num_examples': 10000, 'score': 64894.06465935707, 'total_duration': 69584.44207715988, 'accumulated_submission_time': 64894.06465935707, 'accumulated_eval_time': 4667.94951748848, 'accumulated_logging_time': 11.812203407287598}
I0307 21:46:44.866099 140036192331520 logging_writer.py:48] [95366] accumulated_eval_time=4667.95, accumulated_logging_time=11.8122, accumulated_submission_time=64894.1, global_step=95366, preemption_count=0, score=64894.1, test/accuracy=0.5766, test/loss=2.04462, test/num_examples=10000, total_duration=69584.4, train/accuracy=0.805265, train/loss=0.961425, validation/accuracy=0.70264, validation/loss=1.4003, validation/num_examples=50000
I0307 21:47:37.043295 140036200724224 logging_writer.py:48] [95400] global_step=95400, grad_norm=2.3445611000061035, loss=2.999207019805908
I0307 21:51:06.819550 140036192331520 logging_writer.py:48] [95500] global_step=95500, grad_norm=2.3759567737579346, loss=2.9573521614074707
I0307 21:54:38.565411 140036200724224 logging_writer.py:48] [95600] global_step=95600, grad_norm=2.545027732849121, loss=2.9903738498687744
I0307 21:55:16.434572 140191611933888 spec.py:321] Evaluating on the training split.
I0307 21:55:27.635628 140191611933888 spec.py:333] Evaluating on the validation split.
I0307 21:55:46.886232 140191611933888 spec.py:349] Evaluating on the test split.
I0307 21:55:48.681760 140191611933888 submission_runner.py:469] Time since start: 70128.34s, 	Step: 95619, 	{'train/accuracy': 0.7876673936843872, 'train/loss': 1.0385938882827759, 'validation/accuracy': 0.6993599534034729, 'validation/loss': 1.4322893619537354, 'validation/num_examples': 50000, 'test/accuracy': 0.5670000314712524, 'test/loss': 2.0767104625701904, 'test/num_examples': 10000, 'score': 65405.598452329636, 'total_duration': 70128.33552503586, 'accumulated_submission_time': 65405.598452329636, 'accumulated_eval_time': 4700.196660518646, 'accumulated_logging_time': 11.898047924041748}
I0307 21:55:48.704829 140036192331520 logging_writer.py:48] [95619] accumulated_eval_time=4700.2, accumulated_logging_time=11.898, accumulated_submission_time=65405.6, global_step=95619, preemption_count=0, score=65405.6, test/accuracy=0.567, test/loss=2.07671, test/num_examples=10000, total_duration=70128.3, train/accuracy=0.787667, train/loss=1.03859, validation/accuracy=0.69936, validation/loss=1.43229, validation/num_examples=50000
I0307 21:58:19.803757 140036200724224 logging_writer.py:48] [95700] global_step=95700, grad_norm=2.555065393447876, loss=3.0500290393829346
I0307 22:01:50.968888 140036192331520 logging_writer.py:48] [95800] global_step=95800, grad_norm=2.618891477584839, loss=3.0107038021087646
I0307 22:04:19.726135 140191611933888 spec.py:321] Evaluating on the training split.
I0307 22:04:30.425978 140191611933888 spec.py:333] Evaluating on the validation split.
I0307 22:04:51.351595 140191611933888 spec.py:349] Evaluating on the test split.
I0307 22:04:53.146669 140191611933888 submission_runner.py:469] Time since start: 70672.80s, 	Step: 95871, 	{'train/accuracy': 0.7921516299247742, 'train/loss': 1.0169832706451416, 'validation/accuracy': 0.7007399797439575, 'validation/loss': 1.408028244972229, 'validation/num_examples': 50000, 'test/accuracy': 0.5724000334739685, 'test/loss': 2.058804512023926, 'test/num_examples': 10000, 'score': 65916.58423781395, 'total_duration': 70672.80043172836, 'accumulated_submission_time': 65916.58423781395, 'accumulated_eval_time': 4733.617154598236, 'accumulated_logging_time': 11.928902864456177}
I0307 22:04:53.190469 140036200724224 logging_writer.py:48] [95871] accumulated_eval_time=4733.62, accumulated_logging_time=11.9289, accumulated_submission_time=65916.6, global_step=95871, preemption_count=0, score=65916.6, test/accuracy=0.5724, test/loss=2.0588, test/num_examples=10000, total_duration=70672.8, train/accuracy=0.792152, train/loss=1.01698, validation/accuracy=0.70074, validation/loss=1.40803, validation/num_examples=50000
I0307 22:05:35.145601 140036192331520 logging_writer.py:48] [95900] global_step=95900, grad_norm=2.433067798614502, loss=2.9540019035339355
I0307 22:08:17.404387 140036200724224 logging_writer.py:48] [96000] global_step=96000, grad_norm=2.528177499771118, loss=2.9407567977905273
I0307 22:10:40.241283 140036192331520 logging_writer.py:48] [96100] global_step=96100, grad_norm=2.3404791355133057, loss=2.9363813400268555
I0307 22:13:04.439553 140036200724224 logging_writer.py:48] [96200] global_step=96200, grad_norm=2.4209861755371094, loss=2.9722201824188232
I0307 22:13:24.556385 140036192331520 logging_writer.py:48] [96215] global_step=96215, preemption_count=0, score=66427.9
I0307 22:13:26.404253 140191611933888 submission_runner.py:646] Tuning trial 2/5
I0307 22:13:26.421001 140191611933888 submission_runner.py:647] Hyperparameters: Hyperparameters(dropout_rate=0.0, label_smoothing=0.2, learning_rate=0.0008445074561975979, one_minus_beta1=0.11042418465, beta2=0.9978504782314613, weight_decay=0.08135402759553023, warmup_factor=0.05)
I0307 22:13:26.425483 140191611933888 submission_runner.py:648] Metrics: {'eval_results': [(1, {'train/accuracy': 0.0006776147638447583, 'train/loss': 6.91315221786499, 'validation/accuracy': 0.0007599999662488699, 'validation/loss': 6.913278102874756, 'validation/num_examples': 50000, 'test/accuracy': 0.0005000000237487257, 'test/loss': 6.913549423217773, 'test/num_examples': 10000, 'score': 60.20431733131409, 'total_duration': 151.5138657093048, 'accumulated_submission_time': 60.20431733131409, 'accumulated_eval_time': 91.3093032836914, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1356, {'train/accuracy': 0.07834422588348389, 'train/loss': 5.371662139892578, 'validation/accuracy': 0.06749999523162842, 'validation/loss': 5.458141803741455, 'validation/num_examples': 50000, 'test/accuracy': 0.04960000142455101, 'test/loss': 5.669939994812012, 'test/num_examples': 10000, 'score': 570.2540411949158, 'total_duration': 706.8797223567963, 'accumulated_submission_time': 570.2540411949158, 'accumulated_eval_time': 136.35554671287537, 'accumulated_logging_time': 0.08300662040710449, 'global_step': 1356, 'preemption_count': 0}), (2686, {'train/accuracy': 0.1771763414144516, 'train/loss': 4.352410316467285, 'validation/accuracy': 0.1540599912405014, 'validation/loss': 4.5127787590026855, 'validation/num_examples': 50000, 'test/accuracy': 0.11310000717639923, 'test/loss': 4.896148681640625, 'test/num_examples': 10000, 'score': 1080.14115691185, 'total_duration': 1250.2292065620422, 'accumulated_submission_time': 1080.14115691185, 'accumulated_eval_time': 169.59089183807373, 'accumulated_logging_time': 0.14267945289611816, 'global_step': 2686, 'preemption_count': 0}), (4008, {'train/accuracy': 0.2733577787876129, 'train/loss': 3.6192002296447754, 'validation/accuracy': 0.2360599935054779, 'validation/loss': 3.839977979660034, 'validation/num_examples': 50000, 'test/accuracy': 0.17180000245571136, 'test/loss': 4.346663951873779, 'test/num_examples': 10000, 'score': 1589.9829618930817, 'total_duration': 1795.7262625694275, 'accumulated_submission_time': 1589.9829618930817, 'accumulated_eval_time': 205.02187991142273, 'accumulated_logging_time': 0.21509766578674316, 'global_step': 4008, 'preemption_count': 0}), (5333, {'train/accuracy': 0.36828362941741943, 'train/loss': 3.1484599113464355, 'validation/accuracy': 0.32999998331069946, 'validation/loss': 3.3475239276885986, 'validation/num_examples': 50000, 'test/accuracy': 0.24570001661777496, 'test/loss': 3.890148639678955, 'test/num_examples': 10000, 'score': 2099.870808839798, 'total_duration': 2348.1265008449554, 'accumulated_submission_time': 2099.870808839798, 'accumulated_eval_time': 247.33965063095093, 'accumulated_logging_time': 0.2575819492340088, 'global_step': 5333, 'preemption_count': 0}), (6665, {'train/accuracy': 0.4065887928009033, 'train/loss': 2.868271827697754, 'validation/accuracy': 0.359279990196228, 'validation/loss': 3.1143975257873535, 'validation/num_examples': 50000, 'test/accuracy': 0.2720000147819519, 'test/loss': 3.6712520122528076, 'test/num_examples': 10000, 'score': 2609.7945597171783, 'total_duration': 2895.062940120697, 'accumulated_submission_time': 2609.7945597171783, 'accumulated_eval_time': 284.1790518760681, 'accumulated_logging_time': 0.2852938175201416, 'global_step': 6665, 'preemption_count': 0}), (7992, {'train/accuracy': 0.4759247303009033, 'train/loss': 2.4640872478485107, 'validation/accuracy': 0.4297599792480469, 'validation/loss': 2.7139670848846436, 'validation/num_examples': 50000, 'test/accuracy': 0.3273000121116638, 'test/loss': 3.340256452560425, 'test/num_examples': 10000, 'score': 3119.5938155651093, 'total_duration': 3443.023730993271, 'accumulated_submission_time': 3119.5938155651093, 'accumulated_eval_time': 322.1254200935364, 'accumulated_logging_time': 0.36200976371765137, 'global_step': 7992, 'preemption_count': 0}), (9323, {'train/accuracy': 0.5236966013908386, 'train/loss': 2.300750255584717, 'validation/accuracy': 0.47435998916625977, 'validation/loss': 2.542717218399048, 'validation/num_examples': 50000, 'test/accuracy': 0.35450002551078796, 'test/loss': 3.2271807193756104, 'test/num_examples': 10000, 'score': 3629.714698076248, 'total_duration': 3989.412269592285, 'accumulated_submission_time': 3629.714698076248, 'accumulated_eval_time': 358.2147421836853, 'accumulated_logging_time': 0.3919539451599121, 'global_step': 9323, 'preemption_count': 0}), (10648, {'train/accuracy': 0.5633171200752258, 'train/loss': 2.0104331970214844, 'validation/accuracy': 0.511900007724762, 'validation/loss': 2.268040180206299, 'validation/num_examples': 50000, 'test/accuracy': 0.39490002393722534, 'test/loss': 2.933913230895996, 'test/num_examples': 10000, 'score': 4139.542843580246, 'total_duration': 4543.521776437759, 'accumulated_submission_time': 4139.542843580246, 'accumulated_eval_time': 402.2219989299774, 'accumulated_logging_time': 0.5196027755737305, 'global_step': 10648, 'preemption_count': 0}), (11973, {'train/accuracy': 0.5934112071990967, 'train/loss': 1.9632686376571655, 'validation/accuracy': 0.535860002040863, 'validation/loss': 2.2366771697998047, 'validation/num_examples': 50000, 'test/accuracy': 0.41540002822875977, 'test/loss': 2.877023935317993, 'test/num_examples': 10000, 'score': 4649.554701566696, 'total_duration': 5093.909548282623, 'accumulated_submission_time': 4649.554701566696, 'accumulated_eval_time': 442.3937928676605, 'accumulated_logging_time': 0.5811457633972168, 'global_step': 11973, 'preemption_count': 0}), (13301, {'train/accuracy': 0.6180843114852905, 'train/loss': 1.8578495979309082, 'validation/accuracy': 0.5574399828910828, 'validation/loss': 2.124974012374878, 'validation/num_examples': 50000, 'test/accuracy': 0.4326000213623047, 'test/loss': 2.7924184799194336, 'test/num_examples': 10000, 'score': 5159.494928598404, 'total_duration': 5644.662960529327, 'accumulated_submission_time': 5159.494928598404, 'accumulated_eval_time': 483.0030674934387, 'accumulated_logging_time': 0.6356997489929199, 'global_step': 13301, 'preemption_count': 0}), (14619, {'train/accuracy': 0.6365991830825806, 'train/loss': 1.7211112976074219, 'validation/accuracy': 0.5773800015449524, 'validation/loss': 1.9946941137313843, 'validation/num_examples': 50000, 'test/accuracy': 0.45330002903938293, 'test/loss': 2.6611523628234863, 'test/num_examples': 10000, 'score': 5669.304041862488, 'total_duration': 6199.378535747528, 'accumulated_submission_time': 5669.304041862488, 'accumulated_eval_time': 527.6989982128143, 'accumulated_logging_time': 0.7011928558349609, 'global_step': 14619, 'preemption_count': 0}), (15948, {'train/accuracy': 0.6457669138908386, 'train/loss': 1.6908912658691406, 'validation/accuracy': 0.583139955997467, 'validation/loss': 1.9813586473464966, 'validation/num_examples': 50000, 'test/accuracy': 0.4610000252723694, 'test/loss': 2.633626937866211, 'test/num_examples': 10000, 'score': 6179.355831623077, 'total_duration': 6750.9891312122345, 'accumulated_submission_time': 6179.355831623077, 'accumulated_eval_time': 569.0227386951447, 'accumulated_logging_time': 0.7907683849334717, 'global_step': 15948, 'preemption_count': 0}), (17270, {'train/accuracy': 0.666434109210968, 'train/loss': 1.5680752992630005, 'validation/accuracy': 0.5974599719047546, 'validation/loss': 1.8717472553253174, 'validation/num_examples': 50000, 'test/accuracy': 0.4780000150203705, 'test/loss': 2.5294103622436523, 'test/num_examples': 10000, 'score': 6689.291130781174, 'total_duration': 7300.525658369064, 'accumulated_submission_time': 6689.291130781174, 'accumulated_eval_time': 608.3886113166809, 'accumulated_logging_time': 0.8840675354003906, 'global_step': 17270, 'preemption_count': 0}), (18593, {'train/accuracy': 0.6752431392669678, 'train/loss': 1.545620322227478, 'validation/accuracy': 0.612019956111908, 'validation/loss': 1.8358662128448486, 'validation/num_examples': 50000, 'test/accuracy': 0.482200026512146, 'test/loss': 2.5183663368225098, 'test/num_examples': 10000, 'score': 7199.263134479523, 'total_duration': 7852.879142284393, 'accumulated_submission_time': 7199.263134479523, 'accumulated_eval_time': 650.4931201934814, 'accumulated_logging_time': 1.0294301509857178, 'global_step': 18593, 'preemption_count': 0}), (19918, {'train/accuracy': 0.6723931431770325, 'train/loss': 1.562750220298767, 'validation/accuracy': 0.6068999767303467, 'validation/loss': 1.8614797592163086, 'validation/num_examples': 50000, 'test/accuracy': 0.48350003361701965, 'test/loss': 2.5008797645568848, 'test/num_examples': 10000, 'score': 7709.2389793396, 'total_duration': 8403.634334564209, 'accumulated_submission_time': 7709.2389793396, 'accumulated_eval_time': 691.109569311142, 'accumulated_logging_time': 1.0585978031158447, 'global_step': 19918, 'preemption_count': 0}), (21237, {'train/accuracy': 0.6915059089660645, 'train/loss': 1.4633668661117554, 'validation/accuracy': 0.6233599781990051, 'validation/loss': 1.7683180570602417, 'validation/num_examples': 50000, 'test/accuracy': 0.49150002002716064, 'test/loss': 2.439638376235962, 'test/num_examples': 10000, 'score': 8219.197752714157, 'total_duration': 8953.584324121475, 'accumulated_submission_time': 8219.197752714157, 'accumulated_eval_time': 730.7765364646912, 'accumulated_logging_time': 1.2414052486419678, 'global_step': 21237, 'preemption_count': 0}), (22555, {'train/accuracy': 0.6957908272743225, 'train/loss': 1.4110486507415771, 'validation/accuracy': 0.6284199953079224, 'validation/loss': 1.7138735055923462, 'validation/num_examples': 50000, 'test/accuracy': 0.49490001797676086, 'test/loss': 2.3887875080108643, 'test/num_examples': 10000, 'score': 8729.263293981552, 'total_duration': 9504.685961723328, 'accumulated_submission_time': 8729.263293981552, 'accumulated_eval_time': 771.5915892124176, 'accumulated_logging_time': 1.3209893703460693, 'global_step': 22555, 'preemption_count': 0}), (23869, {'train/accuracy': 0.698640763759613, 'train/loss': 1.3857132196426392, 'validation/accuracy': 0.6312599778175354, 'validation/loss': 1.689432978630066, 'validation/num_examples': 50000, 'test/accuracy': 0.5024999976158142, 'test/loss': 2.361210346221924, 'test/num_examples': 10000, 'score': 9239.309756040573, 'total_duration': 10058.458269357681, 'accumulated_submission_time': 9239.309756040573, 'accumulated_eval_time': 815.0648167133331, 'accumulated_logging_time': 1.4297010898590088, 'global_step': 23869, 'preemption_count': 0}), (25186, {'train/accuracy': 0.7036431431770325, 'train/loss': 1.4194320440292358, 'validation/accuracy': 0.6347999572753906, 'validation/loss': 1.7313907146453857, 'validation/num_examples': 50000, 'test/accuracy': 0.5056000351905823, 'test/loss': 2.386014223098755, 'test/num_examples': 10000, 'score': 9749.113762140274, 'total_duration': 10606.27887749672, 'accumulated_submission_time': 9749.113762140274, 'accumulated_eval_time': 852.7702946662903, 'accumulated_logging_time': 1.6018095016479492, 'global_step': 25186, 'preemption_count': 0}), (26504, {'train/accuracy': 0.7095224857330322, 'train/loss': 1.4008926153182983, 'validation/accuracy': 0.6384599804878235, 'validation/loss': 1.7122982740402222, 'validation/num_examples': 50000, 'test/accuracy': 0.5063000321388245, 'test/loss': 2.375081777572632, 'test/num_examples': 10000, 'score': 10258.938409090042, 'total_duration': 11158.97870516777, 'accumulated_submission_time': 10258.938409090042, 'accumulated_eval_time': 895.4264299869537, 'accumulated_logging_time': 1.6848649978637695, 'global_step': 26504, 'preemption_count': 0}), (27829, {'train/accuracy': 0.7002949714660645, 'train/loss': 1.4403825998306274, 'validation/accuracy': 0.6310799717903137, 'validation/loss': 1.7473212480545044, 'validation/num_examples': 50000, 'test/accuracy': 0.5012000203132629, 'test/loss': 2.433413505554199, 'test/num_examples': 10000, 'score': 10768.737848758698, 'total_duration': 11709.318193435669, 'accumulated_submission_time': 10768.737848758698, 'accumulated_eval_time': 935.7145071029663, 'accumulated_logging_time': 1.7973113059997559, 'global_step': 27829, 'preemption_count': 0}), (29149, {'train/accuracy': 0.7065330147743225, 'train/loss': 1.4145169258117676, 'validation/accuracy': 0.6378799676895142, 'validation/loss': 1.7297440767288208, 'validation/num_examples': 50000, 'test/accuracy': 0.513200044631958, 'test/loss': 2.3744258880615234, 'test/num_examples': 10000, 'score': 11278.56044459343, 'total_duration': 12259.125003576279, 'accumulated_submission_time': 11278.56044459343, 'accumulated_eval_time': 975.4615819454193, 'accumulated_logging_time': 1.8986070156097412, 'global_step': 29149, 'preemption_count': 0}), (30472, {'train/accuracy': 0.7128108739852905, 'train/loss': 1.3705154657363892, 'validation/accuracy': 0.6441999673843384, 'validation/loss': 1.6669092178344727, 'validation/num_examples': 50000, 'test/accuracy': 0.5151000022888184, 'test/loss': 2.3454983234405518, 'test/num_examples': 10000, 'score': 11788.487871646881, 'total_duration': 12812.267133235931, 'accumulated_submission_time': 11788.487871646881, 'accumulated_eval_time': 1018.4347972869873, 'accumulated_logging_time': 2.006068229675293, 'global_step': 30472, 'preemption_count': 0}), (31790, {'train/accuracy': 0.7202845811843872, 'train/loss': 1.324790358543396, 'validation/accuracy': 0.6470400094985962, 'validation/loss': 1.6521518230438232, 'validation/num_examples': 50000, 'test/accuracy': 0.5204000473022461, 'test/loss': 2.290083408355713, 'test/num_examples': 10000, 'score': 12298.158679246902, 'total_duration': 13360.504140853882, 'accumulated_submission_time': 12298.158679246902, 'accumulated_eval_time': 1056.6665122509003, 'accumulated_logging_time': 2.202118158340454, 'global_step': 31790, 'preemption_count': 0}), (33111, {'train/accuracy': 0.7161989808082581, 'train/loss': 1.3585830926895142, 'validation/accuracy': 0.6484799981117249, 'validation/loss': 1.6654702425003052, 'validation/num_examples': 50000, 'test/accuracy': 0.5112000107765198, 'test/loss': 2.335038661956787, 'test/num_examples': 10000, 'score': 12807.9684612751, 'total_duration': 13911.119186401367, 'accumulated_submission_time': 12807.9684612751, 'accumulated_eval_time': 1097.2060430049896, 'accumulated_logging_time': 2.3264379501342773, 'global_step': 33111, 'preemption_count': 0}), (34429, {'train/accuracy': 0.7191286683082581, 'train/loss': 1.3470344543457031, 'validation/accuracy': 0.6523199677467346, 'validation/loss': 1.656497597694397, 'validation/num_examples': 50000, 'test/accuracy': 0.5245000123977661, 'test/loss': 2.3005402088165283, 'test/num_examples': 10000, 'score': 13318.015569448471, 'total_duration': 14465.363008975983, 'accumulated_submission_time': 13318.015569448471, 'accumulated_eval_time': 1141.145075082779, 'accumulated_logging_time': 2.4435434341430664, 'global_step': 34429, 'preemption_count': 0}), (35748, {'train/accuracy': 0.7219985723495483, 'train/loss': 1.3317444324493408, 'validation/accuracy': 0.6523199677467346, 'validation/loss': 1.6439622640609741, 'validation/num_examples': 50000, 'test/accuracy': 0.5229000449180603, 'test/loss': 2.297809600830078, 'test/num_examples': 10000, 'score': 13827.9128844738, 'total_duration': 15014.159200668335, 'accumulated_submission_time': 13827.9128844738, 'accumulated_eval_time': 1179.7307233810425, 'accumulated_logging_time': 2.615875482559204, 'global_step': 35748, 'preemption_count': 0}), (37066, {'train/accuracy': 0.702547013759613, 'train/loss': 1.403496265411377, 'validation/accuracy': 0.6398199796676636, 'validation/loss': 1.6943724155426025, 'validation/num_examples': 50000, 'test/accuracy': 0.5054000020027161, 'test/loss': 2.3812694549560547, 'test/num_examples': 10000, 'score': 14337.776632785797, 'total_duration': 15564.852329492569, 'accumulated_submission_time': 14337.776632785797, 'accumulated_eval_time': 1220.2999331951141, 'accumulated_logging_time': 2.7399470806121826, 'global_step': 37066, 'preemption_count': 0}), (38383, {'train/accuracy': 0.7105189561843872, 'train/loss': 1.391722321510315, 'validation/accuracy': 0.6444799900054932, 'validation/loss': 1.6981041431427002, 'validation/num_examples': 50000, 'test/accuracy': 0.5157000422477722, 'test/loss': 2.336155652999878, 'test/num_examples': 10000, 'score': 14847.772614955902, 'total_duration': 16115.805789232254, 'accumulated_submission_time': 14847.772614955902, 'accumulated_eval_time': 1260.9875202178955, 'accumulated_logging_time': 2.8697681427001953, 'global_step': 38383, 'preemption_count': 0}), (39703, {'train/accuracy': 0.7277383208274841, 'train/loss': 1.3032801151275635, 'validation/accuracy': 0.6576200127601624, 'validation/loss': 1.611594319343567, 'validation/num_examples': 50000, 'test/accuracy': 0.5355000495910645, 'test/loss': 2.250380754470825, 'test/num_examples': 10000, 'score': 15357.718620061874, 'total_duration': 16664.009169578552, 'accumulated_submission_time': 15357.718620061874, 'accumulated_eval_time': 1298.9895973205566, 'accumulated_logging_time': 2.985231637954712, 'global_step': 39703, 'preemption_count': 0}), (41023, {'train/accuracy': 0.7084861397743225, 'train/loss': 1.3757389783859253, 'validation/accuracy': 0.640720009803772, 'validation/loss': 1.6938326358795166, 'validation/num_examples': 50000, 'test/accuracy': 0.5135000348091125, 'test/loss': 2.345468759536743, 'test/num_examples': 10000, 'score': 15867.44130897522, 'total_duration': 17212.503080129623, 'accumulated_submission_time': 15867.44130897522, 'accumulated_eval_time': 1337.4413006305695, 'accumulated_logging_time': 3.16410231590271, 'global_step': 41023, 'preemption_count': 0}), (42339, {'train/accuracy': 0.7323620915412903, 'train/loss': 1.2590270042419434, 'validation/accuracy': 0.6604799628257751, 'validation/loss': 1.5863053798675537, 'validation/num_examples': 50000, 'test/accuracy': 0.5314000248908997, 'test/loss': 2.2374258041381836, 'test/num_examples': 10000, 'score': 16377.471191883087, 'total_duration': 17755.94539618492, 'accumulated_submission_time': 16377.471191883087, 'accumulated_eval_time': 1370.5300965309143, 'accumulated_logging_time': 3.3480923175811768, 'global_step': 42339, 'preemption_count': 0}), (43654, {'train/accuracy': 0.7234932780265808, 'train/loss': 1.3088228702545166, 'validation/accuracy': 0.6539799571037292, 'validation/loss': 1.6147361993789673, 'validation/num_examples': 50000, 'test/accuracy': 0.5294000506401062, 'test/loss': 2.283883810043335, 'test/num_examples': 10000, 'score': 16887.367341518402, 'total_duration': 18305.42393732071, 'accumulated_submission_time': 16887.367341518402, 'accumulated_eval_time': 1409.8410658836365, 'accumulated_logging_time': 3.4818544387817383, 'global_step': 43654, 'preemption_count': 0}), (44970, {'train/accuracy': 0.7120336294174194, 'train/loss': 1.3774102926254272, 'validation/accuracy': 0.6472799777984619, 'validation/loss': 1.6748263835906982, 'validation/num_examples': 50000, 'test/accuracy': 0.5196000337600708, 'test/loss': 2.306929588317871, 'test/num_examples': 10000, 'score': 17397.292685747147, 'total_duration': 18846.25577688217, 'accumulated_submission_time': 17397.292685747147, 'accumulated_eval_time': 1440.4691541194916, 'accumulated_logging_time': 3.6180477142333984, 'global_step': 44970, 'preemption_count': 0}), (46289, {'train/accuracy': 0.7318239808082581, 'train/loss': 1.2762997150421143, 'validation/accuracy': 0.6612799763679504, 'validation/loss': 1.5900646448135376, 'validation/num_examples': 50000, 'test/accuracy': 0.5356000065803528, 'test/loss': 2.2268800735473633, 'test/num_examples': 10000, 'score': 17907.36415863037, 'total_duration': 19390.526972055435, 'accumulated_submission_time': 17907.36415863037, 'accumulated_eval_time': 1474.3818657398224, 'accumulated_logging_time': 3.764610528945923, 'global_step': 46289, 'preemption_count': 0}), (47601, {'train/accuracy': 0.7213807106018066, 'train/loss': 1.3244725465774536, 'validation/accuracy': 0.6558399796485901, 'validation/loss': 1.6315107345581055, 'validation/num_examples': 50000, 'test/accuracy': 0.518500030040741, 'test/loss': 2.3189094066619873, 'test/num_examples': 10000, 'score': 18417.17841887474, 'total_duration': 19936.114319086075, 'accumulated_submission_time': 18417.17841887474, 'accumulated_eval_time': 1509.861181974411, 'accumulated_logging_time': 3.912874698638916, 'global_step': 47601, 'preemption_count': 0}), (48910, {'train/accuracy': 0.7327407598495483, 'train/loss': 1.3355664014816284, 'validation/accuracy': 0.6626600027084351, 'validation/loss': 1.6429983377456665, 'validation/num_examples': 50000, 'test/accuracy': 0.5272000432014465, 'test/loss': 2.3307063579559326, 'test/num_examples': 10000, 'score': 18927.227395296097, 'total_duration': 20483.33490729332, 'accumulated_submission_time': 18927.227395296097, 'accumulated_eval_time': 1546.7784659862518, 'accumulated_logging_time': 4.030738115310669, 'global_step': 48910, 'preemption_count': 0}), (50211, {'train/accuracy': 0.7366868257522583, 'train/loss': 1.2299232482910156, 'validation/accuracy': 0.6650999784469604, 'validation/loss': 1.5512888431549072, 'validation/num_examples': 50000, 'test/accuracy': 0.5398000478744507, 'test/loss': 2.1954569816589355, 'test/num_examples': 10000, 'score': 19437.131939172745, 'total_duration': 21034.695421218872, 'accumulated_submission_time': 19437.131939172745, 'accumulated_eval_time': 1587.9949188232422, 'accumulated_logging_time': 4.12834095954895, 'global_step': 50211, 'preemption_count': 0}), (51515, {'train/accuracy': 0.7292928695678711, 'train/loss': 1.2655597925186157, 'validation/accuracy': 0.6610599756240845, 'validation/loss': 1.5827680826187134, 'validation/num_examples': 50000, 'test/accuracy': 0.5331000089645386, 'test/loss': 2.243577241897583, 'test/num_examples': 10000, 'score': 19947.166453123093, 'total_duration': 21581.721972703934, 'accumulated_submission_time': 19947.166453123093, 'accumulated_eval_time': 1624.703408241272, 'accumulated_logging_time': 4.268998384475708, 'global_step': 51515, 'preemption_count': 0}), (52754, {'train/accuracy': 0.741629421710968, 'train/loss': 1.247258186340332, 'validation/accuracy': 0.6700400114059448, 'validation/loss': 1.5683972835540771, 'validation/num_examples': 50000, 'test/accuracy': 0.5378000140190125, 'test/loss': 2.2395901679992676, 'test/num_examples': 10000, 'score': 20457.109797239304, 'total_duration': 22127.39958548546, 'accumulated_submission_time': 20457.109797239304, 'accumulated_eval_time': 1660.1655759811401, 'accumulated_logging_time': 4.396728038787842, 'global_step': 52754, 'preemption_count': 0}), (53967, {'train/accuracy': 0.7393972873687744, 'train/loss': 1.2260617017745972, 'validation/accuracy': 0.6633599996566772, 'validation/loss': 1.5694737434387207, 'validation/num_examples': 50000, 'test/accuracy': 0.5368000268936157, 'test/loss': 2.219684362411499, 'test/num_examples': 10000, 'score': 20967.162478923798, 'total_duration': 22675.901480913162, 'accumulated_submission_time': 20967.162478923798, 'accumulated_eval_time': 1698.2943212985992, 'accumulated_logging_time': 4.577424764633179, 'global_step': 53967, 'preemption_count': 0}), (55224, {'train/accuracy': 0.7392578125, 'train/loss': 1.2593649625778198, 'validation/accuracy': 0.663379967212677, 'validation/loss': 1.5964614152908325, 'validation/num_examples': 50000, 'test/accuracy': 0.5317000150680542, 'test/loss': 2.276043653488159, 'test/num_examples': 10000, 'score': 21477.251506567, 'total_duration': 23220.684890031815, 'accumulated_submission_time': 21477.251506567, 'accumulated_eval_time': 1732.7142033576965, 'accumulated_logging_time': 4.702350616455078, 'global_step': 55224, 'preemption_count': 0}), (56328, {'train/accuracy': 0.750398576259613, 'train/loss': 1.2322849035263062, 'validation/accuracy': 0.6684600114822388, 'validation/loss': 1.5972193479537964, 'validation/num_examples': 50000, 'test/accuracy': 0.5393000245094299, 'test/loss': 2.245885133743286, 'test/num_examples': 10000, 'score': 21987.854191303253, 'total_duration': 23767.051189899445, 'accumulated_submission_time': 21987.854191303253, 'accumulated_eval_time': 1768.2484197616577, 'accumulated_logging_time': 4.803948163986206, 'global_step': 56328, 'preemption_count': 0}), (57432, {'train/accuracy': 0.7433235049247742, 'train/loss': 1.2292670011520386, 'validation/accuracy': 0.6522200107574463, 'validation/loss': 1.6248177289962769, 'validation/num_examples': 50000, 'test/accuracy': 0.5308000445365906, 'test/loss': 2.26238751411438, 'test/num_examples': 10000, 'score': 22497.98379802704, 'total_duration': 24316.402676343918, 'accumulated_submission_time': 22497.98379802704, 'accumulated_eval_time': 1807.200285434723, 'accumulated_logging_time': 4.943390369415283, 'global_step': 57432, 'preemption_count': 0}), (58711, {'train/accuracy': 0.7578921914100647, 'train/loss': 1.2047712802886963, 'validation/accuracy': 0.6647199988365173, 'validation/loss': 1.5992448329925537, 'validation/num_examples': 50000, 'test/accuracy': 0.5400000214576721, 'test/loss': 2.246666431427002, 'test/num_examples': 10000, 'score': 23007.906752824783, 'total_duration': 24859.899211645126, 'accumulated_submission_time': 23007.906752824783, 'accumulated_eval_time': 1840.514530658722, 'accumulated_logging_time': 5.05224871635437, 'global_step': 58711, 'preemption_count': 0}), (59852, {'train/accuracy': 0.7736766338348389, 'train/loss': 1.0858649015426636, 'validation/accuracy': 0.6649999618530273, 'validation/loss': 1.5545696020126343, 'validation/num_examples': 50000, 'test/accuracy': 0.5404000282287598, 'test/loss': 2.2117249965667725, 'test/num_examples': 10000, 'score': 23517.882717609406, 'total_duration': 25404.1510720253, 'accumulated_submission_time': 23517.882717609406, 'accumulated_eval_time': 1874.4791932106018, 'accumulated_logging_time': 5.232440233230591, 'global_step': 59852, 'preemption_count': 0}), (61005, {'train/accuracy': 0.7468510866165161, 'train/loss': 1.2288507223129272, 'validation/accuracy': 0.6705399751663208, 'validation/loss': 1.5565474033355713, 'validation/num_examples': 50000, 'test/accuracy': 0.549500048160553, 'test/loss': 2.184891700744629, 'test/num_examples': 10000, 'score': 24027.689358711243, 'total_duration': 25947.31233048439, 'accumulated_submission_time': 24027.689358711243, 'accumulated_eval_time': 1907.5273263454437, 'accumulated_logging_time': 5.403896331787109, 'global_step': 61005, 'preemption_count': 0}), (62058, {'train/accuracy': 0.7456353306770325, 'train/loss': 1.2305089235305786, 'validation/accuracy': 0.6714400053024292, 'validation/loss': 1.5547122955322266, 'validation/num_examples': 50000, 'test/accuracy': 0.5471000075340271, 'test/loss': 2.1882545948028564, 'test/num_examples': 10000, 'score': 24537.553835868835, 'total_duration': 26489.79996585846, 'accumulated_submission_time': 24537.553835868835, 'accumulated_eval_time': 1939.9123837947845, 'accumulated_logging_time': 5.52220892906189, 'global_step': 62058, 'preemption_count': 0}), (63111, {'train/accuracy': 0.7518534660339355, 'train/loss': 1.1656666994094849, 'validation/accuracy': 0.671779990196228, 'validation/loss': 1.5303453207015991, 'validation/num_examples': 50000, 'test/accuracy': 0.5543000102043152, 'test/loss': 2.1460766792297363, 'test/num_examples': 10000, 'score': 25047.454434156418, 'total_duration': 27033.031614780426, 'accumulated_submission_time': 25047.454434156418, 'accumulated_eval_time': 1973.0414950847626, 'accumulated_logging_time': 5.605474472045898, 'global_step': 63111, 'preemption_count': 0}), (64213, {'train/accuracy': 0.7583904266357422, 'train/loss': 1.1838148832321167, 'validation/accuracy': 0.674780011177063, 'validation/loss': 1.5550591945648193, 'validation/num_examples': 50000, 'test/accuracy': 0.5503000020980835, 'test/loss': 2.1836345195770264, 'test/num_examples': 10000, 'score': 25557.352769374847, 'total_duration': 27574.648943424225, 'accumulated_submission_time': 25557.352769374847, 'accumulated_eval_time': 2004.5092668533325, 'accumulated_logging_time': 5.728713035583496, 'global_step': 64213, 'preemption_count': 0}), (65259, {'train/accuracy': 0.7678372263908386, 'train/loss': 1.1303789615631104, 'validation/accuracy': 0.6765599846839905, 'validation/loss': 1.5321184396743774, 'validation/num_examples': 50000, 'test/accuracy': 0.5440000295639038, 'test/loss': 2.196814775466919, 'test/num_examples': 10000, 'score': 26067.63009238243, 'total_duration': 28117.683656454086, 'accumulated_submission_time': 26067.63009238243, 'accumulated_eval_time': 2036.9287922382355, 'accumulated_logging_time': 5.9465131759643555, 'global_step': 65259, 'preemption_count': 0}), (66224, {'train/accuracy': 0.7456353306770325, 'train/loss': 1.2098084688186646, 'validation/accuracy': 0.6709199547767639, 'validation/loss': 1.533651351928711, 'validation/num_examples': 50000, 'test/accuracy': 0.5453000068664551, 'test/loss': 2.1791269779205322, 'test/num_examples': 10000, 'score': 26577.853446483612, 'total_duration': 28658.977437496185, 'accumulated_submission_time': 26577.853446483612, 'accumulated_eval_time': 2067.6736421585083, 'accumulated_logging_time': 6.165448904037476, 'global_step': 66224, 'preemption_count': 0}), (67071, {'train/accuracy': 0.7593869566917419, 'train/loss': 1.1656389236450195, 'validation/accuracy': 0.6778799891471863, 'validation/loss': 1.5219621658325195, 'validation/num_examples': 50000, 'test/accuracy': 0.54830002784729, 'test/loss': 2.184826612472534, 'test/num_examples': 10000, 'score': 27087.778123617172, 'total_duration': 29197.59646296501, 'accumulated_submission_time': 27087.778123617172, 'accumulated_eval_time': 2096.162500143051, 'accumulated_logging_time': 6.274480819702148, 'global_step': 67071, 'preemption_count': 0}), (67952, {'train/accuracy': 0.7694913744926453, 'train/loss': 1.0812344551086426, 'validation/accuracy': 0.6800400018692017, 'validation/loss': 1.4644702672958374, 'validation/num_examples': 50000, 'test/accuracy': 0.5522000193595886, 'test/loss': 2.1288936138153076, 'test/num_examples': 10000, 'score': 27597.71247267723, 'total_duration': 29744.845315933228, 'accumulated_submission_time': 27597.71247267723, 'accumulated_eval_time': 2133.2285804748535, 'accumulated_logging_time': 6.420159578323364, 'global_step': 67952, 'preemption_count': 0}), (68810, {'train/accuracy': 0.7471300959587097, 'train/loss': 1.2173529863357544, 'validation/accuracy': 0.6756599545478821, 'validation/loss': 1.5367326736450195, 'validation/num_examples': 50000, 'test/accuracy': 0.5509999990463257, 'test/loss': 2.1745712757110596, 'test/num_examples': 10000, 'score': 28107.8175573349, 'total_duration': 30288.00942182541, 'accumulated_submission_time': 28107.8175573349, 'accumulated_eval_time': 2166.0574867725372, 'accumulated_logging_time': 6.551817417144775, 'global_step': 68810, 'preemption_count': 0}), (69584, {'train/accuracy': 0.7616987824440002, 'train/loss': 1.179900050163269, 'validation/accuracy': 0.6801599860191345, 'validation/loss': 1.5314499139785767, 'validation/num_examples': 50000, 'test/accuracy': 0.5567000508308411, 'test/loss': 2.184511423110962, 'test/num_examples': 10000, 'score': 28617.75919365883, 'total_duration': 30832.141397476196, 'accumulated_submission_time': 28617.75919365883, 'accumulated_eval_time': 2199.983179330826, 'accumulated_logging_time': 6.727322340011597, 'global_step': 69584, 'preemption_count': 0}), (70581, {'train/accuracy': 0.7714046239852905, 'train/loss': 1.0927977561950684, 'validation/accuracy': 0.6775999665260315, 'validation/loss': 1.5057133436203003, 'validation/num_examples': 50000, 'test/accuracy': 0.5499000549316406, 'test/loss': 2.1518149375915527, 'test/num_examples': 10000, 'score': 29127.641743183136, 'total_duration': 31377.665235757828, 'accumulated_submission_time': 29127.641743183136, 'accumulated_eval_time': 2235.36049079895, 'accumulated_logging_time': 6.8783769607543945, 'global_step': 70581, 'preemption_count': 0}), (71314, {'train/accuracy': 0.7598652839660645, 'train/loss': 1.1756232976913452, 'validation/accuracy': 0.6831600069999695, 'validation/loss': 1.513314962387085, 'validation/num_examples': 50000, 'test/accuracy': 0.5561000108718872, 'test/loss': 2.155890703201294, 'test/num_examples': 10000, 'score': 29638.18580365181, 'total_duration': 31924.391001224518, 'accumulated_submission_time': 29638.18580365181, 'accumulated_eval_time': 2271.3128185272217, 'accumulated_logging_time': 7.024794340133667, 'global_step': 71314, 'preemption_count': 0}), (72003, {'train/accuracy': 0.770527720451355, 'train/loss': 1.127446174621582, 'validation/accuracy': 0.6768999695777893, 'validation/loss': 1.5289407968521118, 'validation/num_examples': 50000, 'test/accuracy': 0.5512000322341919, 'test/loss': 2.18680477142334, 'test/num_examples': 10000, 'score': 30148.604938983917, 'total_duration': 32468.112343788147, 'accumulated_submission_time': 30148.604938983917, 'accumulated_eval_time': 2304.3536944389343, 'accumulated_logging_time': 7.173162937164307, 'global_step': 72003, 'preemption_count': 0}), (72808, {'train/accuracy': 0.7454559803009033, 'train/loss': 1.2438143491744995, 'validation/accuracy': 0.6727399826049805, 'validation/loss': 1.5702115297317505, 'validation/num_examples': 50000, 'test/accuracy': 0.5467000007629395, 'test/loss': 2.2195987701416016, 'test/num_examples': 10000, 'score': 30658.778817415237, 'total_duration': 33016.76405334473, 'accumulated_submission_time': 30658.778817415237, 'accumulated_eval_time': 2342.676242828369, 'accumulated_logging_time': 7.236826181411743, 'global_step': 72808, 'preemption_count': 0}), (73550, {'train/accuracy': 0.7712252736091614, 'train/loss': 1.0943078994750977, 'validation/accuracy': 0.6843000054359436, 'validation/loss': 1.4587708711624146, 'validation/num_examples': 50000, 'test/accuracy': 0.5604000091552734, 'test/loss': 2.1090142726898193, 'test/num_examples': 10000, 'score': 31168.35579061508, 'total_duration': 33559.90183258057, 'accumulated_submission_time': 31168.35579061508, 'accumulated_eval_time': 2375.4086627960205, 'accumulated_logging_time': 7.977468013763428, 'global_step': 73550, 'preemption_count': 0}), (74261, {'train/accuracy': 0.7502591013908386, 'train/loss': 1.2099504470825195, 'validation/accuracy': 0.6813200116157532, 'validation/loss': 1.518631100654602, 'validation/num_examples': 50000, 'test/accuracy': 0.5591000318527222, 'test/loss': 2.152308225631714, 'test/num_examples': 10000, 'score': 31678.629658460617, 'total_duration': 34102.36777091026, 'accumulated_submission_time': 31678.629658460617, 'accumulated_eval_time': 2407.464739561081, 'accumulated_logging_time': 8.031905889511108, 'global_step': 74261, 'preemption_count': 0}), (74862, {'train/accuracy': 0.7745934128761292, 'train/loss': 1.097033143043518, 'validation/accuracy': 0.6853199601173401, 'validation/loss': 1.4846985340118408, 'validation/num_examples': 50000, 'test/accuracy': 0.555400013923645, 'test/loss': 2.1179192066192627, 'test/num_examples': 10000, 'score': 32189.127655506134, 'total_duration': 34646.07320523262, 'accumulated_submission_time': 32189.127655506134, 'accumulated_eval_time': 2440.5597558021545, 'accumulated_logging_time': 8.074886322021484, 'global_step': 74862, 'preemption_count': 0}), (75680, {'train/accuracy': 0.7627750039100647, 'train/loss': 1.1549268960952759, 'validation/accuracy': 0.6815800070762634, 'validation/loss': 1.5061566829681396, 'validation/num_examples': 50000, 'test/accuracy': 0.5516000390052795, 'test/loss': 2.1618735790252686, 'test/num_examples': 10000, 'score': 32700.60781764984, 'total_duration': 35190.317730903625, 'accumulated_submission_time': 32700.60781764984, 'accumulated_eval_time': 2473.132777452469, 'accumulated_logging_time': 8.169992685317993, 'global_step': 75680, 'preemption_count': 0}), (76081, {'train/accuracy': 0.7795559763908386, 'train/loss': 1.05972158908844, 'validation/accuracy': 0.6872400045394897, 'validation/loss': 1.4586371183395386, 'validation/num_examples': 50000, 'test/accuracy': 0.5582000017166138, 'test/loss': 2.101837158203125, 'test/num_examples': 10000, 'score': 33211.09546780586, 'total_duration': 35736.15346813202, 'accumulated_submission_time': 33211.09546780586, 'accumulated_eval_time': 2508.369644641876, 'accumulated_logging_time': 8.234814167022705, 'global_step': 76081, 'preemption_count': 0}), (76622, {'train/accuracy': 0.7625358700752258, 'train/loss': 1.1496362686157227, 'validation/accuracy': 0.6817399859428406, 'validation/loss': 1.5017591714859009, 'validation/num_examples': 50000, 'test/accuracy': 0.5541000366210938, 'test/loss': 2.1414129734039307, 'test/num_examples': 10000, 'score': 33721.12235498428, 'total_duration': 36282.47741103172, 'accumulated_submission_time': 33721.12235498428, 'accumulated_eval_time': 2544.557785511017, 'accumulated_logging_time': 8.282521486282349, 'global_step': 76622, 'preemption_count': 0}), (77042, {'train/accuracy': 0.7663623690605164, 'train/loss': 1.134068250656128, 'validation/accuracy': 0.6872000098228455, 'validation/loss': 1.4789130687713623, 'validation/num_examples': 50000, 'test/accuracy': 0.5644000172615051, 'test/loss': 2.108720064163208, 'test/num_examples': 10000, 'score': 34231.62412691116, 'total_duration': 36828.21502995491, 'accumulated_submission_time': 34231.62412691116, 'accumulated_eval_time': 2579.6552743911743, 'accumulated_logging_time': 8.372639179229736, 'global_step': 77042, 'preemption_count': 0}), (77525, {'train/accuracy': 0.7770846486091614, 'train/loss': 1.0971626043319702, 'validation/accuracy': 0.687720000743866, 'validation/loss': 1.4916902780532837, 'validation/num_examples': 50000, 'test/accuracy': 0.5563000440597534, 'test/loss': 2.146676540374756, 'test/num_examples': 10000, 'score': 34742.2627761364, 'total_duration': 37380.123707294464, 'accumulated_submission_time': 34742.2627761364, 'accumulated_eval_time': 2620.815510034561, 'accumulated_logging_time': 8.424813270568848, 'global_step': 77525, 'preemption_count': 0}), (78067, {'train/accuracy': 0.7648875713348389, 'train/loss': 1.116260051727295, 'validation/accuracy': 0.6841599941253662, 'validation/loss': 1.473738431930542, 'validation/num_examples': 50000, 'test/accuracy': 0.5575000047683716, 'test/loss': 2.1474058628082275, 'test/num_examples': 10000, 'score': 35252.850377082825, 'total_duration': 37928.78523039818, 'accumulated_submission_time': 35252.850377082825, 'accumulated_eval_time': 2658.7098178863525, 'accumulated_logging_time': 8.541117668151855, 'global_step': 78067, 'preemption_count': 0}), (78475, {'train/accuracy': 0.7654057741165161, 'train/loss': 1.111968755722046, 'validation/accuracy': 0.686519980430603, 'validation/loss': 1.4623723030090332, 'validation/num_examples': 50000, 'test/accuracy': 0.5558000206947327, 'test/loss': 2.1241090297698975, 'test/num_examples': 10000, 'score': 35763.59578490257, 'total_duration': 38473.041018009186, 'accumulated_submission_time': 35763.59578490257, 'accumulated_eval_time': 2692.076517343521, 'accumulated_logging_time': 8.639185667037964, 'global_step': 78475, 'preemption_count': 0}), (78734, {'train/accuracy': 0.7793965339660645, 'train/loss': 1.0718291997909546, 'validation/accuracy': 0.678119957447052, 'validation/loss': 1.5102078914642334, 'validation/num_examples': 50000, 'test/accuracy': 0.5527000427246094, 'test/loss': 2.1734182834625244, 'test/num_examples': 10000, 'score': 36274.880365371704, 'total_duration': 39018.525948762894, 'accumulated_submission_time': 36274.880365371704, 'accumulated_eval_time': 2726.1261394023895, 'accumulated_logging_time': 8.76133680343628, 'global_step': 78734, 'preemption_count': 0}), (79093, {'train/accuracy': 0.7680763602256775, 'train/loss': 1.1448367834091187, 'validation/accuracy': 0.6817599534988403, 'validation/loss': 1.5266982316970825, 'validation/num_examples': 50000, 'test/accuracy': 0.5530000329017639, 'test/loss': 2.19062876701355, 'test/num_examples': 10000, 'score': 36785.64168906212, 'total_duration': 39563.48726439476, 'accumulated_submission_time': 36785.64168906212, 'accumulated_eval_time': 2760.2588329315186, 'accumulated_logging_time': 8.787997245788574, 'global_step': 79093, 'preemption_count': 0}), (79569, {'train/accuracy': 0.7649872303009033, 'train/loss': 1.1422159671783447, 'validation/accuracy': 0.685699999332428, 'validation/loss': 1.4956337213516235, 'validation/num_examples': 50000, 'test/accuracy': 0.5616000294685364, 'test/loss': 2.1342687606811523, 'test/num_examples': 10000, 'score': 37297.03584074974, 'total_duration': 40109.666603565216, 'accumulated_submission_time': 37297.03584074974, 'accumulated_eval_time': 2794.893789291382, 'accumulated_logging_time': 8.882561683654785, 'global_step': 79569, 'preemption_count': 0}), (80132, {'train/accuracy': 0.7879264950752258, 'train/loss': 1.0406149625778198, 'validation/accuracy': 0.6904799938201904, 'validation/loss': 1.4685161113739014, 'validation/num_examples': 50000, 'test/accuracy': 0.5601000189781189, 'test/loss': 2.1198103427886963, 'test/num_examples': 10000, 'score': 37807.86044502258, 'total_duration': 40654.321321725845, 'accumulated_submission_time': 37807.86044502258, 'accumulated_eval_time': 2828.606508255005, 'accumulated_logging_time': 8.934717655181885, 'global_step': 80132, 'preemption_count': 0}), (80343, {'train/accuracy': 0.7824856638908386, 'train/loss': 1.0652188062667847, 'validation/accuracy': 0.691379964351654, 'validation/loss': 1.4680545330047607, 'validation/num_examples': 50000, 'test/accuracy': 0.5629000067710876, 'test/loss': 2.094031810760498, 'test/num_examples': 10000, 'score': 38318.40378689766, 'total_duration': 41196.6131772995, 'accumulated_submission_time': 38318.40378689766, 'accumulated_eval_time': 2860.2711555957794, 'accumulated_logging_time': 8.99477767944336, 'global_step': 80343, 'preemption_count': 0}), (80830, {'train/accuracy': 0.7667809128761292, 'train/loss': 1.119646668434143, 'validation/accuracy': 0.6874799728393555, 'validation/loss': 1.4800801277160645, 'validation/num_examples': 50000, 'test/accuracy': 0.5625, 'test/loss': 2.12665057182312, 'test/num_examples': 10000, 'score': 38829.7118229866, 'total_duration': 41741.733221292496, 'accumulated_submission_time': 38829.7118229866, 'accumulated_eval_time': 2894.0027215480804, 'accumulated_logging_time': 9.021174907684326, 'global_step': 80830, 'preemption_count': 0}), (81235, {'train/accuracy': 0.7640106678009033, 'train/loss': 1.1098157167434692, 'validation/accuracy': 0.6887199878692627, 'validation/loss': 1.4445910453796387, 'validation/num_examples': 50000, 'test/accuracy': 0.5681000351905823, 'test/loss': 2.076789379119873, 'test/num_examples': 10000, 'score': 39340.443491220474, 'total_duration': 42286.10372829437, 'accumulated_submission_time': 39340.443491220474, 'accumulated_eval_time': 2927.4999630451202, 'accumulated_logging_time': 9.118421792984009, 'global_step': 81235, 'preemption_count': 0}), (81518, {'train/accuracy': 0.7999840378761292, 'train/loss': 0.9816750884056091, 'validation/accuracy': 0.6929000020027161, 'validation/loss': 1.4409170150756836, 'validation/num_examples': 50000, 'test/accuracy': 0.5675000548362732, 'test/loss': 2.091183662414551, 'test/num_examples': 10000, 'score': 39851.90071582794, 'total_duration': 42833.29964828491, 'accumulated_submission_time': 39851.90071582794, 'accumulated_eval_time': 2963.145844221115, 'accumulated_logging_time': 9.179704427719116, 'global_step': 81518, 'preemption_count': 0}), (81718, {'train/accuracy': 0.7785395383834839, 'train/loss': 1.1193634271621704, 'validation/accuracy': 0.6834799647331238, 'validation/loss': 1.5222110748291016, 'validation/num_examples': 50000, 'test/accuracy': 0.5539000034332275, 'test/loss': 2.1843812465667725, 'test/num_examples': 10000, 'score': 40362.663738012314, 'total_duration': 43379.644800662994, 'accumulated_submission_time': 40362.663738012314, 'accumulated_eval_time': 2998.602949142456, 'accumulated_logging_time': 9.281748056411743, 'global_step': 81718, 'preemption_count': 0}), (81967, {'train/accuracy': 0.7727598547935486, 'train/loss': 1.1218955516815186, 'validation/accuracy': 0.6892200112342834, 'validation/loss': 1.494043231010437, 'validation/num_examples': 50000, 'test/accuracy': 0.5591000318527222, 'test/loss': 2.132713556289673, 'test/num_examples': 10000, 'score': 40873.015095710754, 'total_duration': 43925.087421417236, 'accumulated_submission_time': 40873.015095710754, 'accumulated_eval_time': 3033.637434244156, 'accumulated_logging_time': 9.309917449951172, 'global_step': 81967, 'preemption_count': 0}), (82337, {'train/accuracy': 0.7665218114852905, 'train/loss': 1.1272982358932495, 'validation/accuracy': 0.686199963092804, 'validation/loss': 1.4827165603637695, 'validation/num_examples': 50000, 'test/accuracy': 0.5581000447273254, 'test/loss': 2.1117899417877197, 'test/num_examples': 10000, 'score': 41385.028460502625, 'total_duration': 44472.81581091881, 'accumulated_submission_time': 41385.028460502625, 'accumulated_eval_time': 3069.281506061554, 'accumulated_logging_time': 9.33889389038086, 'global_step': 82337, 'preemption_count': 0}), (82488, {'train/accuracy': 0.7665815949440002, 'train/loss': 1.107668161392212, 'validation/accuracy': 0.6884199976921082, 'validation/loss': 1.451607584953308, 'validation/num_examples': 50000, 'test/accuracy': 0.5640000104904175, 'test/loss': 2.0964252948760986, 'test/num_examples': 10000, 'score': 41895.230098962784, 'total_duration': 45016.03415679932, 'accumulated_submission_time': 41895.230098962784, 'accumulated_eval_time': 3102.2077696323395, 'accumulated_logging_time': 9.411880016326904, 'global_step': 82488, 'preemption_count': 0}), (82810, {'train/accuracy': 0.7723811864852905, 'train/loss': 1.0965019464492798, 'validation/accuracy': 0.6918799877166748, 'validation/loss': 1.449447512626648, 'validation/num_examples': 50000, 'test/accuracy': 0.5680000185966492, 'test/loss': 2.0799481868743896, 'test/num_examples': 10000, 'score': 42407.39200472832, 'total_duration': 45563.992037534714, 'accumulated_submission_time': 42407.39200472832, 'accumulated_eval_time': 3137.9391853809357, 'accumulated_logging_time': 9.438167810440063, 'global_step': 82810, 'preemption_count': 0}), (83195, {'train/accuracy': 0.78617262840271, 'train/loss': 1.0235415697097778, 'validation/accuracy': 0.6933799982070923, 'validation/loss': 1.4387099742889404, 'validation/num_examples': 50000, 'test/accuracy': 0.5683000087738037, 'test/loss': 2.0722548961639404, 'test/num_examples': 10000, 'score': 42917.456682920456, 'total_duration': 46110.10495042801, 'accumulated_submission_time': 42917.456682920456, 'accumulated_eval_time': 3173.8333978652954, 'accumulated_logging_time': 9.54940414428711, 'global_step': 83195, 'preemption_count': 0}), (83531, {'train/accuracy': 0.7734972834587097, 'train/loss': 1.0873615741729736, 'validation/accuracy': 0.6864599585533142, 'validation/loss': 1.4675266742706299, 'validation/num_examples': 50000, 'test/accuracy': 0.5550000071525574, 'test/loss': 2.133202314376831, 'test/num_examples': 10000, 'score': 43427.468729496, 'total_duration': 46657.36492609978, 'accumulated_submission_time': 43427.468729496, 'accumulated_eval_time': 3210.959508419037, 'accumulated_logging_time': 9.631896018981934, 'global_step': 83531, 'preemption_count': 0}), (83951, {'train/accuracy': 0.76859450340271, 'train/loss': 1.1439391374588013, 'validation/accuracy': 0.6890000104904175, 'validation/loss': 1.4934748411178589, 'validation/num_examples': 50000, 'test/accuracy': 0.556600034236908, 'test/loss': 2.1601579189300537, 'test/num_examples': 10000, 'score': 43937.71469449997, 'total_duration': 47207.04396414757, 'accumulated_submission_time': 43937.71469449997, 'accumulated_eval_time': 3250.27858042717, 'accumulated_logging_time': 9.697559833526611, 'global_step': 83951, 'preemption_count': 0}), (84714, {'train/accuracy': 0.784199595451355, 'train/loss': 1.0420020818710327, 'validation/accuracy': 0.6931799650192261, 'validation/loss': 1.4441362619400024, 'validation/num_examples': 50000, 'test/accuracy': 0.5631000399589539, 'test/loss': 2.1002256870269775, 'test/num_examples': 10000, 'score': 44448.74419569969, 'total_duration': 47754.984545469284, 'accumulated_submission_time': 44448.74419569969, 'accumulated_eval_time': 3287.054470539093, 'accumulated_logging_time': 9.748671770095825, 'global_step': 84714, 'preemption_count': 0}), (84954, {'train/accuracy': 0.7774633169174194, 'train/loss': 1.0720586776733398, 'validation/accuracy': 0.6904799938201904, 'validation/loss': 1.4531588554382324, 'validation/num_examples': 50000, 'test/accuracy': 0.5653000473976135, 'test/loss': 2.1090221405029297, 'test/num_examples': 10000, 'score': 44960.462553977966, 'total_duration': 48298.648114442825, 'accumulated_submission_time': 44960.462553977966, 'accumulated_eval_time': 3318.9220542907715, 'accumulated_logging_time': 9.800751209259033, 'global_step': 84954, 'preemption_count': 0}), (85164, {'train/accuracy': 0.7782605290412903, 'train/loss': 1.1029448509216309, 'validation/accuracy': 0.6894999742507935, 'validation/loss': 1.4759265184402466, 'validation/num_examples': 50000, 'test/accuracy': 0.5706000328063965, 'test/loss': 2.1005172729492188, 'test/num_examples': 10000, 'score': 45471.5216255188, 'total_duration': 48844.12367153168, 'accumulated_submission_time': 45471.5216255188, 'accumulated_eval_time': 3353.2419979572296, 'accumulated_logging_time': 9.873937129974365, 'global_step': 85164, 'preemption_count': 0}), (85292, {'train/accuracy': 0.7707669138908386, 'train/loss': 1.1187443733215332, 'validation/accuracy': 0.6912199854850769, 'validation/loss': 1.475957989692688, 'validation/num_examples': 50000, 'test/accuracy': 0.5631999969482422, 'test/loss': 2.113006114959717, 'test/num_examples': 10000, 'score': 45983.33199429512, 'total_duration': 49392.57139849663, 'accumulated_submission_time': 45983.33199429512, 'accumulated_eval_time': 3389.838460922241, 'accumulated_logging_time': 9.901540756225586, 'global_step': 85292, 'preemption_count': 0}), (85422, {'train/accuracy': 0.7719427347183228, 'train/loss': 1.1172235012054443, 'validation/accuracy': 0.6912800073623657, 'validation/loss': 1.4774099588394165, 'validation/num_examples': 50000, 'test/accuracy': 0.5648000240325928, 'test/loss': 2.113751173019409, 'test/num_examples': 10000, 'score': 46494.613300323486, 'total_duration': 49937.6855866909, 'accumulated_submission_time': 46494.613300323486, 'accumulated_eval_time': 3423.6293530464172, 'accumulated_logging_time': 9.92949914932251, 'global_step': 85422, 'preemption_count': 0}), (85795, {'train/accuracy': 0.7743343114852905, 'train/loss': 1.0845028162002563, 'validation/accuracy': 0.6970599889755249, 'validation/loss': 1.4260917901992798, 'validation/num_examples': 50000, 'test/accuracy': 0.5698000192642212, 'test/loss': 2.080115795135498, 'test/num_examples': 10000, 'score': 47005.75315093994, 'total_duration': 50482.513984680176, 'accumulated_submission_time': 47005.75315093994, 'accumulated_eval_time': 3457.249967813492, 'accumulated_logging_time': 9.956375122070312, 'global_step': 85795, 'preemption_count': 0}), (86220, {'train/accuracy': 0.7840401530265808, 'train/loss': 1.088598370552063, 'validation/accuracy': 0.6858999729156494, 'validation/loss': 1.5026551485061646, 'validation/num_examples': 50000, 'test/accuracy': 0.5688000321388245, 'test/loss': 2.1097285747528076, 'test/num_examples': 10000, 'score': 47515.768565654755, 'total_duration': 51026.81024312973, 'accumulated_submission_time': 47515.768565654755, 'accumulated_eval_time': 3491.4040021896362, 'accumulated_logging_time': 10.035655975341797, 'global_step': 86220, 'preemption_count': 0}), (86345, {'train/accuracy': 0.785554826259613, 'train/loss': 1.009905457496643, 'validation/accuracy': 0.6980599761009216, 'validation/loss': 1.4055782556533813, 'validation/num_examples': 50000, 'test/accuracy': 0.5692000389099121, 'test/loss': 2.048983573913574, 'test/num_examples': 10000, 'score': 48025.72045445442, 'total_duration': 51574.21605324745, 'accumulated_submission_time': 48025.72045445442, 'accumulated_eval_time': 3528.7690114974976, 'accumulated_logging_time': 10.112008571624756, 'global_step': 86345, 'preemption_count': 0}), (86471, {'train/accuracy': 0.7827646732330322, 'train/loss': 1.0541810989379883, 'validation/accuracy': 0.6919800043106079, 'validation/loss': 1.4586546421051025, 'validation/num_examples': 50000, 'test/accuracy': 0.5665000081062317, 'test/loss': 2.108456611633301, 'test/num_examples': 10000, 'score': 48537.77165770531, 'total_duration': 52122.21358251572, 'accumulated_submission_time': 48537.77165770531, 'accumulated_eval_time': 3564.672637462616, 'accumulated_logging_time': 10.141455173492432, 'global_step': 86471, 'preemption_count': 0}), (86606, {'train/accuracy': 0.77445387840271, 'train/loss': 1.0693210363388062, 'validation/accuracy': 0.6883999705314636, 'validation/loss': 1.460637092590332, 'validation/num_examples': 50000, 'test/accuracy': 0.5678000450134277, 'test/loss': 2.085423707962036, 'test/num_examples': 10000, 'score': 49049.787569761276, 'total_duration': 52668.96734929085, 'accumulated_submission_time': 49049.787569761276, 'accumulated_eval_time': 3599.365636110306, 'accumulated_logging_time': 10.17133641242981, 'global_step': 86606, 'preemption_count': 0}), (86857, {'train/accuracy': 0.7752710580825806, 'train/loss': 1.1167206764221191, 'validation/accuracy': 0.6937199831008911, 'validation/loss': 1.47046959400177, 'validation/num_examples': 50000, 'test/accuracy': 0.5664000511169434, 'test/loss': 2.1299562454223633, 'test/num_examples': 10000, 'score': 49561.36015033722, 'total_duration': 53211.557465553284, 'accumulated_submission_time': 49561.36015033722, 'accumulated_eval_time': 3630.325887441635, 'accumulated_logging_time': 10.199901819229126, 'global_step': 86857, 'preemption_count': 0}), (87106, {'train/accuracy': 0.7701091766357422, 'train/loss': 1.1027480363845825, 'validation/accuracy': 0.6889599561691284, 'validation/loss': 1.4552830457687378, 'validation/num_examples': 50000, 'test/accuracy': 0.5586000084877014, 'test/loss': 2.1209237575531006, 'test/num_examples': 10000, 'score': 50071.35767006874, 'total_duration': 53754.737388134, 'accumulated_submission_time': 50071.35767006874, 'accumulated_eval_time': 3663.4515252113342, 'accumulated_logging_time': 10.228155136108398, 'global_step': 87106, 'preemption_count': 0}), (87540, {'train/accuracy': 0.8050462007522583, 'train/loss': 0.9416555762290955, 'validation/accuracy': 0.6919400095939636, 'validation/loss': 1.427805781364441, 'validation/num_examples': 50000, 'test/accuracy': 0.5595000386238098, 'test/loss': 2.0933897495269775, 'test/num_examples': 10000, 'score': 50581.6177277565, 'total_duration': 54300.22401833534, 'accumulated_submission_time': 50581.6177277565, 'accumulated_eval_time': 3698.5091445446014, 'accumulated_logging_time': 10.347389459609985, 'global_step': 87540, 'preemption_count': 0}), (88022, {'train/accuracy': 0.7840800285339355, 'train/loss': 1.0858005285263062, 'validation/accuracy': 0.6907399892807007, 'validation/loss': 1.4813494682312012, 'validation/num_examples': 50000, 'test/accuracy': 0.5687000155448914, 'test/loss': 2.1247341632843018, 'test/num_examples': 10000, 'score': 51091.7116689682, 'total_duration': 54845.54681921005, 'accumulated_submission_time': 51091.7116689682, 'accumulated_eval_time': 3733.63356256485, 'accumulated_logging_time': 10.396383285522461, 'global_step': 88022, 'preemption_count': 0}), (88465, {'train/accuracy': 0.7799345850944519, 'train/loss': 1.0690176486968994, 'validation/accuracy': 0.6926400065422058, 'validation/loss': 1.4479008913040161, 'validation/num_examples': 50000, 'test/accuracy': 0.5670000314712524, 'test/loss': 2.0885019302368164, 'test/num_examples': 10000, 'score': 51603.74013900757, 'total_duration': 55390.46458911896, 'accumulated_submission_time': 51603.74013900757, 'accumulated_eval_time': 3766.4044890403748, 'accumulated_logging_time': 10.465767621994019, 'global_step': 88465, 'preemption_count': 0}), (88591, {'train/accuracy': 0.7720224857330322, 'train/loss': 1.083070158958435, 'validation/accuracy': 0.6910200119018555, 'validation/loss': 1.4420679807662964, 'validation/num_examples': 50000, 'test/accuracy': 0.5648000240325928, 'test/loss': 2.076608419418335, 'test/num_examples': 10000, 'score': 52114.60315442085, 'total_duration': 55937.75881958008, 'accumulated_submission_time': 52114.60315442085, 'accumulated_eval_time': 3802.760051012039, 'accumulated_logging_time': 10.527741432189941, 'global_step': 88591, 'preemption_count': 0}), (88717, {'train/accuracy': 0.7752112150192261, 'train/loss': 1.1010602712631226, 'validation/accuracy': 0.6972599625587463, 'validation/loss': 1.4462090730667114, 'validation/num_examples': 50000, 'test/accuracy': 0.5683000087738037, 'test/loss': 2.0938825607299805, 'test/num_examples': 10000, 'score': 52626.56941008568, 'total_duration': 56485.283853530884, 'accumulated_submission_time': 52626.56941008568, 'accumulated_eval_time': 3838.2754712104797, 'accumulated_logging_time': 10.558876276016235, 'global_step': 88717, 'preemption_count': 0}), (88969, {'train/accuracy': 0.8073580861091614, 'train/loss': 0.9568255543708801, 'validation/accuracy': 0.6888200044631958, 'validation/loss': 1.4602144956588745, 'validation/num_examples': 50000, 'test/accuracy': 0.5629000067710876, 'test/loss': 2.1097867488861084, 'test/num_examples': 10000, 'score': 53136.56471943855, 'total_duration': 57031.182565927505, 'accumulated_submission_time': 53136.56471943855, 'accumulated_eval_time': 3874.120075941086, 'accumulated_logging_time': 10.589030504226685, 'global_step': 88969, 'preemption_count': 0}), (89186, {'train/accuracy': 0.793387234210968, 'train/loss': 1.0224562883377075, 'validation/accuracy': 0.6946399807929993, 'validation/loss': 1.4530787467956543, 'validation/num_examples': 50000, 'test/accuracy': 0.5619000196456909, 'test/loss': 2.1328399181365967, 'test/num_examples': 10000, 'score': 53647.18182229996, 'total_duration': 57577.31274533272, 'accumulated_submission_time': 53647.18182229996, 'accumulated_eval_time': 3909.580999851227, 'accumulated_logging_time': 10.617105722427368, 'global_step': 89186, 'preemption_count': 0}), (89312, {'train/accuracy': 0.7902582883834839, 'train/loss': 1.0390269756317139, 'validation/accuracy': 0.6925199627876282, 'validation/loss': 1.4560710191726685, 'validation/num_examples': 50000, 'test/accuracy': 0.5738000273704529, 'test/loss': 2.091273546218872, 'test/num_examples': 10000, 'score': 54159.506237745285, 'total_duration': 58126.57048201561, 'accumulated_submission_time': 54159.506237745285, 'accumulated_eval_time': 3946.4716336727142, 'accumulated_logging_time': 10.646672010421753, 'global_step': 89312, 'preemption_count': 0}), (89438, {'train/accuracy': 0.7804129123687744, 'train/loss': 1.0518841743469238, 'validation/accuracy': 0.6904999613761902, 'validation/loss': 1.4516704082489014, 'validation/num_examples': 50000, 'test/accuracy': 0.5580000281333923, 'test/loss': 2.1310157775878906, 'test/num_examples': 10000, 'score': 54670.87477970123, 'total_duration': 58673.803253889084, 'accumulated_submission_time': 54670.87477970123, 'accumulated_eval_time': 3982.29376578331, 'accumulated_logging_time': 10.675749063491821, 'global_step': 89438, 'preemption_count': 0}), (89564, {'train/accuracy': 0.7720623016357422, 'train/loss': 1.096530556678772, 'validation/accuracy': 0.6793799996376038, 'validation/loss': 1.505624532699585, 'validation/num_examples': 50000, 'test/accuracy': 0.5509999990463257, 'test/loss': 2.172883987426758, 'test/num_examples': 10000, 'score': 55183.433886766434, 'total_duration': 59220.74669146538, 'accumulated_submission_time': 55183.433886766434, 'accumulated_eval_time': 4016.6349081993103, 'accumulated_logging_time': 10.704985857009888, 'global_step': 89564, 'preemption_count': 0}), (89703, {'train/accuracy': 0.7728993892669678, 'train/loss': 1.0548344850540161, 'validation/accuracy': 0.6851399540901184, 'validation/loss': 1.4541112184524536, 'validation/num_examples': 50000, 'test/accuracy': 0.5579000115394592, 'test/loss': 2.1299798488616943, 'test/num_examples': 10000, 'score': 55694.77769470215, 'total_duration': 59765.50969982147, 'accumulated_submission_time': 55694.77769470215, 'accumulated_eval_time': 4050.008941411972, 'accumulated_logging_time': 10.734372854232788, 'global_step': 89703, 'preemption_count': 0}), (89946, {'train/accuracy': 0.7827646732330322, 'train/loss': 1.0659565925598145, 'validation/accuracy': 0.6953999996185303, 'validation/loss': 1.4379647970199585, 'validation/num_examples': 50000, 'test/accuracy': 0.5689000487327576, 'test/loss': 2.0999844074249268, 'test/num_examples': 10000, 'score': 56206.807099580765, 'total_duration': 60309.817862033844, 'accumulated_submission_time': 56206.807099580765, 'accumulated_eval_time': 4082.232030391693, 'accumulated_logging_time': 10.763605833053589, 'global_step': 89946, 'preemption_count': 0}), (90197, {'train/accuracy': 0.7810905575752258, 'train/loss': 1.0500255823135376, 'validation/accuracy': 0.6977199912071228, 'validation/loss': 1.4182322025299072, 'validation/num_examples': 50000, 'test/accuracy': 0.573900043964386, 'test/loss': 2.066542625427246, 'test/num_examples': 10000, 'score': 56717.82669019699, 'total_duration': 60855.6057035923, 'accumulated_submission_time': 56717.82669019699, 'accumulated_eval_time': 4116.943884849548, 'accumulated_logging_time': 10.793275356292725, 'global_step': 90197, 'preemption_count': 0}), (90450, {'train/accuracy': 0.7820671200752258, 'train/loss': 1.095510482788086, 'validation/accuracy': 0.6970799565315247, 'validation/loss': 1.4553059339523315, 'validation/num_examples': 50000, 'test/accuracy': 0.5752000212669373, 'test/loss': 2.0966646671295166, 'test/num_examples': 10000, 'score': 57229.56269788742, 'total_duration': 61402.24177098274, 'accumulated_submission_time': 57229.56269788742, 'accumulated_eval_time': 4151.768825769424, 'accumulated_logging_time': 10.840416431427002, 'global_step': 90450, 'preemption_count': 0}), (90700, {'train/accuracy': 0.8140744566917419, 'train/loss': 0.9294105768203735, 'validation/accuracy': 0.6945399641990662, 'validation/loss': 1.4238859415054321, 'validation/num_examples': 50000, 'test/accuracy': 0.5665000081062317, 'test/loss': 2.08419132232666, 'test/num_examples': 10000, 'score': 57739.76331591606, 'total_duration': 61947.30162739754, 'accumulated_submission_time': 57739.76331591606, 'accumulated_eval_time': 4186.570454835892, 'accumulated_logging_time': 10.86995792388916, 'global_step': 90700, 'preemption_count': 0}), (90966, {'train/accuracy': 0.7899593114852905, 'train/loss': 1.0642435550689697, 'validation/accuracy': 0.6909399628639221, 'validation/loss': 1.480566382408142, 'validation/num_examples': 50000, 'test/accuracy': 0.5604000091552734, 'test/loss': 2.151770830154419, 'test/num_examples': 10000, 'score': 58249.934388399124, 'total_duration': 62492.867218732834, 'accumulated_submission_time': 58249.934388399124, 'accumulated_eval_time': 4221.892584562302, 'accumulated_logging_time': 10.913435220718384, 'global_step': 90966, 'preemption_count': 0}), (91597, {'train/accuracy': 0.7754902839660645, 'train/loss': 1.0823265314102173, 'validation/accuracy': 0.685759961605072, 'validation/loss': 1.4598616361618042, 'validation/num_examples': 50000, 'test/accuracy': 0.5568000078201294, 'test/loss': 2.1180851459503174, 'test/num_examples': 10000, 'score': 58760.41983580589, 'total_duration': 63042.89712166786, 'accumulated_submission_time': 58760.41983580589, 'accumulated_eval_time': 4261.265513181686, 'accumulated_logging_time': 11.013250589370728, 'global_step': 91597, 'preemption_count': 0}), (92206, {'train/accuracy': 0.802176296710968, 'train/loss': 0.9829091429710388, 'validation/accuracy': 0.6957199573516846, 'validation/loss': 1.429304838180542, 'validation/num_examples': 50000, 'test/accuracy': 0.5660000443458557, 'test/loss': 2.0917460918426514, 'test/num_examples': 10000, 'score': 59271.054829359055, 'total_duration': 63589.63125872612, 'accumulated_submission_time': 59271.054829359055, 'accumulated_eval_time': 4297.256429433823, 'accumulated_logging_time': 11.054826736450195, 'global_step': 92206, 'preemption_count': 0}), (92575, {'train/accuracy': 0.7879663705825806, 'train/loss': 1.0225143432617188, 'validation/accuracy': 0.6972599625587463, 'validation/loss': 1.414861798286438, 'validation/num_examples': 50000, 'test/accuracy': 0.5729000568389893, 'test/loss': 2.061185359954834, 'test/num_examples': 10000, 'score': 59782.395458221436, 'total_duration': 64135.09134721756, 'accumulated_submission_time': 59782.395458221436, 'accumulated_eval_time': 4331.281293392181, 'accumulated_logging_time': 11.108251333236694, 'global_step': 92575, 'preemption_count': 0}), (92943, {'train/accuracy': 0.7697703838348389, 'train/loss': 1.1075923442840576, 'validation/accuracy': 0.6868599653244019, 'validation/loss': 1.4848952293395996, 'validation/num_examples': 50000, 'test/accuracy': 0.5653000473976135, 'test/loss': 2.10247540473938, 'test/num_examples': 10000, 'score': 60293.17581152916, 'total_duration': 64679.86819791794, 'accumulated_submission_time': 60293.17581152916, 'accumulated_eval_time': 4365.151185512543, 'accumulated_logging_time': 11.19473147392273, 'global_step': 92943, 'preemption_count': 0}), (93311, {'train/accuracy': 0.7724011540412903, 'train/loss': 1.106764316558838, 'validation/accuracy': 0.6890999674797058, 'validation/loss': 1.47303307056427, 'validation/num_examples': 50000, 'test/accuracy': 0.5609000325202942, 'test/loss': 2.1311542987823486, 'test/num_examples': 10000, 'score': 60803.90449619293, 'total_duration': 65225.14622235298, 'accumulated_submission_time': 60803.90449619293, 'accumulated_eval_time': 4399.5912129879, 'accumulated_logging_time': 11.262699842453003, 'global_step': 93311, 'preemption_count': 0}), (93599, {'train/accuracy': 0.8108258843421936, 'train/loss': 0.9398601651191711, 'validation/accuracy': 0.6992799639701843, 'validation/loss': 1.4104418754577637, 'validation/num_examples': 50000, 'test/accuracy': 0.5777000188827515, 'test/loss': 2.0372133255004883, 'test/num_examples': 10000, 'score': 61315.64055657387, 'total_duration': 65769.86347270012, 'accumulated_submission_time': 61315.64055657387, 'accumulated_eval_time': 4432.489187717438, 'accumulated_logging_time': 11.314046859741211, 'global_step': 93599, 'preemption_count': 0}), (93852, {'train/accuracy': 0.7962173223495483, 'train/loss': 1.0093600749969482, 'validation/accuracy': 0.6981599926948547, 'validation/loss': 1.4332609176635742, 'validation/num_examples': 50000, 'test/accuracy': 0.5707000494003296, 'test/loss': 2.074535369873047, 'test/num_examples': 10000, 'score': 61825.60519719124, 'total_duration': 66311.76137256622, 'accumulated_submission_time': 61825.60519719124, 'accumulated_eval_time': 4464.301021337509, 'accumulated_logging_time': 11.407354354858398, 'global_step': 93852, 'preemption_count': 0}), (94105, {'train/accuracy': 0.7833226919174194, 'train/loss': 1.0635826587677002, 'validation/accuracy': 0.6948599815368652, 'validation/loss': 1.4533624649047852, 'validation/num_examples': 50000, 'test/accuracy': 0.5742000341415405, 'test/loss': 2.0841734409332275, 'test/num_examples': 10000, 'score': 62336.62378716469, 'total_duration': 66855.64555573463, 'accumulated_submission_time': 62336.62378716469, 'accumulated_eval_time': 4497.056501150131, 'accumulated_logging_time': 11.490060091018677, 'global_step': 94105, 'preemption_count': 0}), (94357, {'train/accuracy': 0.7841199040412903, 'train/loss': 1.0602374076843262, 'validation/accuracy': 0.6938799619674683, 'validation/loss': 1.4602330923080444, 'validation/num_examples': 50000, 'test/accuracy': 0.5738000273704529, 'test/loss': 2.077045202255249, 'test/num_examples': 10000, 'score': 62847.5939848423, 'total_duration': 67398.47040963173, 'accumulated_submission_time': 62847.5939848423, 'accumulated_eval_time': 4528.807563781738, 'accumulated_logging_time': 11.564785242080688, 'global_step': 94357, 'preemption_count': 0}), (94610, {'train/accuracy': 0.7872090339660645, 'train/loss': 1.0548466444015503, 'validation/accuracy': 0.7005400061607361, 'validation/loss': 1.423754334449768, 'validation/num_examples': 50000, 'test/accuracy': 0.5754000544548035, 'test/loss': 2.073009490966797, 'test/num_examples': 10000, 'score': 63359.5439684391, 'total_duration': 67945.38875842094, 'accumulated_submission_time': 63359.5439684391, 'accumulated_eval_time': 4563.713807344437, 'accumulated_logging_time': 11.59960651397705, 'global_step': 94610, 'preemption_count': 0}), (94862, {'train/accuracy': 0.7851163744926453, 'train/loss': 1.0493755340576172, 'validation/accuracy': 0.7021999955177307, 'validation/loss': 1.429882287979126, 'validation/num_examples': 50000, 'test/accuracy': 0.5784000158309937, 'test/loss': 2.059344530105591, 'test/num_examples': 10000, 'score': 63871.428904771805, 'total_duration': 68491.94546818733, 'accumulated_submission_time': 63871.428904771805, 'accumulated_eval_time': 4598.2787845134735, 'accumulated_logging_time': 11.677581071853638, 'global_step': 94862, 'preemption_count': 0}), (95113, {'train/accuracy': 0.8137555718421936, 'train/loss': 0.9395840764045715, 'validation/accuracy': 0.6972399950027466, 'validation/loss': 1.4292817115783691, 'validation/num_examples': 50000, 'test/accuracy': 0.5726000070571899, 'test/loss': 2.0850026607513428, 'test/num_examples': 10000, 'score': 64382.65586090088, 'total_duration': 69037.64989924431, 'accumulated_submission_time': 64382.65586090088, 'accumulated_eval_time': 4632.622745990753, 'accumulated_logging_time': 11.78269362449646, 'global_step': 95113, 'preemption_count': 0}), (95366, {'train/accuracy': 0.8052654266357422, 'train/loss': 0.9614248275756836, 'validation/accuracy': 0.7026399970054626, 'validation/loss': 1.4003020524978638, 'validation/num_examples': 50000, 'test/accuracy': 0.5766000151634216, 'test/loss': 2.0446157455444336, 'test/num_examples': 10000, 'score': 64894.06465935707, 'total_duration': 69584.44207715988, 'accumulated_submission_time': 64894.06465935707, 'accumulated_eval_time': 4667.94951748848, 'accumulated_logging_time': 11.812203407287598, 'global_step': 95366, 'preemption_count': 0}), (95619, {'train/accuracy': 0.7876673936843872, 'train/loss': 1.0385938882827759, 'validation/accuracy': 0.6993599534034729, 'validation/loss': 1.4322893619537354, 'validation/num_examples': 50000, 'test/accuracy': 0.5670000314712524, 'test/loss': 2.0767104625701904, 'test/num_examples': 10000, 'score': 65405.598452329636, 'total_duration': 70128.33552503586, 'accumulated_submission_time': 65405.598452329636, 'accumulated_eval_time': 4700.196660518646, 'accumulated_logging_time': 11.898047924041748, 'global_step': 95619, 'preemption_count': 0}), (95871, {'train/accuracy': 0.7921516299247742, 'train/loss': 1.0169832706451416, 'validation/accuracy': 0.7007399797439575, 'validation/loss': 1.408028244972229, 'validation/num_examples': 50000, 'test/accuracy': 0.5724000334739685, 'test/loss': 2.058804512023926, 'test/num_examples': 10000, 'score': 65916.58423781395, 'total_duration': 70672.80043172836, 'accumulated_submission_time': 65916.58423781395, 'accumulated_eval_time': 4733.617154598236, 'accumulated_logging_time': 11.928902864456177, 'global_step': 95871, 'preemption_count': 0})], 'global_step': 96215}
I0307 22:13:26.425828 140191611933888 submission_runner.py:649] Timing: 66427.8840637207
I0307 22:13:26.425875 140191611933888 submission_runner.py:651] Total number of evals: 130
I0307 22:13:26.425904 140191611933888 submission_runner.py:652] ====================
I0307 22:13:26.426144 140191611933888 submission_runner.py:750] Final imagenet_resnet score: 1
