python submission_runner.py --framework=jax --workload=fastmri --submission_path=prize_qualification_baselines/external_tuning/jax_nadamw_full_budget.py --data_dir=/data/fastmri --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=submissions/rolling_leaderboard/external_tuning/baseline/study_1 --overwrite=True --save_checkpoints=False --rng_seed=-1381718262 --tuning_ruleset=external --tuning_search_space=prize_qualification_baselines/external_tuning/tuning_search_space.json --num_tuning_trials=5 --hparam_start_index=4 --hparam_end_index=5 2>&1 | tee -a /logs/fastmri_jax_03-07-2025-15-51-50.log
2025-03-07 15:51:56.601110: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1741362716.646955       9 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1741362716.656247       9 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
I0307 15:52:15.672651 140616853095616 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_1/fastmri_jax.
I0307 15:52:16.726375 140616853095616 xla_bridge.py:884] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA
I0307 15:52:16.729549 140616853095616 xla_bridge.py:884] Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory
I0307 15:52:16.731346 140616853095616 submission_runner.py:606] Using RNG seed -1381718262
I0307 15:52:17.366583 140616853095616 submission_runner.py:615] --- Tuning run 5/5 ---
I0307 15:52:17.366797 140616853095616 submission_runner.py:620] Creating tuning directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_1/fastmri_jax/trial_5.
I0307 15:52:17.367002 140616853095616 logger_utils.py:97] Saving hparams to /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_1/fastmri_jax/trial_5/hparams.json.
I0307 15:52:17.608553 140616853095616 submission_runner.py:218] Initializing dataset.
I0307 15:52:22.488995 140616853095616 submission_runner.py:229] Initializing model.
I0307 15:52:33.452674 140616853095616 submission_runner.py:272] Initializing optimizer.
I0307 15:52:33.984611 140616853095616 submission_runner.py:279] Initializing metrics bundle.
I0307 15:52:33.984878 140616853095616 submission_runner.py:301] Initializing checkpoint and logger.
I0307 15:52:33.985692 140616853095616 checkpoints.py:1101] Found no checkpoint files in /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_1/fastmri_jax/trial_5 with prefix checkpoint_
I0307 15:52:33.985811 140616853095616 submission_runner.py:321] Saving meta data to /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_1/fastmri_jax/trial_5/meta_data_0.json.
I0307 15:52:33.986005 140616853095616 logger_utils.py:262] Unable to record workload.train_mean information. Continuing without it.
I0307 15:52:33.986055 140616853095616 logger_utils.py:262] Unable to record workload.train_stddev information. Continuing without it.
I0307 15:52:34.327116 140616853095616 submission_runner.py:325] Saving flags to /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_1/fastmri_jax/trial_5/flags_0.json.
I0307 15:52:34.364075 140616853095616 submission_runner.py:337] Starting training loop.
E0307 15:56:05.029869       9 gpu_timer.cc:183] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.
E0307 15:56:05.239321       9 gpu_timer.cc:183] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.
E0307 15:56:05.649895       9 gpu_timer.cc:183] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.
E0307 15:56:05.858743       9 gpu_timer.cc:183] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.
E0307 15:56:07.772246       9 gpu_timer.cc:183] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.
E0307 15:56:07.981775       9 gpu_timer.cc:183] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.
I0307 15:56:24.687372 140479698028288 logging_writer.py:48] [0] global_step=0, grad_norm=3.7100491523742676, loss=0.9519651532173157
I0307 15:56:24.742350 140616853095616 spec.py:321] Evaluating on the training split.
I0307 15:57:18.549500 140616853095616 spec.py:333] Evaluating on the validation split.
I0307 15:57:42.693247 140616853095616 spec.py:349] Evaluating on the test split.
I0307 15:58:04.666873 140616853095616 submission_runner.py:469] Time since start: 330.30s, 	Step: 1, 	{'train/ssim': 0.18020692893436976, 'train/loss': 0.9958035605294364, 'validation/ssim': 0.1708046521909732, 'validation/loss': 1.0128932216560567, 'validation/num_examples': 3554, 'test/ssim': 0.1926895222561348, 'test/loss': 1.0104085990950502, 'test/num_examples': 3581, 'score': 230.37812995910645, 'total_duration': 330.30274510383606, 'accumulated_submission_time': 230.37812995910645, 'accumulated_eval_time': 99.92445659637451, 'accumulated_logging_time': 0}
I0307 15:58:04.674274 140460756543232 logging_writer.py:48] [1] accumulated_eval_time=99.9245, accumulated_logging_time=0, accumulated_submission_time=230.378, global_step=1, preemption_count=0, score=230.378, test/loss=1.01041, test/num_examples=3581, test/ssim=0.19269, total_duration=330.303, train/loss=0.995804, train/ssim=0.180207, validation/loss=1.01289, validation/num_examples=3554, validation/ssim=0.170805
I0307 15:58:16.858752 140460748150528 logging_writer.py:48] [100] global_step=100, grad_norm=0.15444397926330566, loss=0.4672619104385376
I0307 15:58:34.380049 140460756543232 logging_writer.py:48] [200] global_step=200, grad_norm=0.1878294050693512, loss=0.32166317105293274
I0307 15:58:51.199144 140460748150528 logging_writer.py:48] [300] global_step=300, grad_norm=0.2170846164226532, loss=0.43396803736686707
I0307 15:59:09.458260 140460756543232 logging_writer.py:48] [400] global_step=400, grad_norm=0.10439387708902359, loss=0.36160722374916077
I0307 15:59:24.715998 140616853095616 spec.py:321] Evaluating on the training split.
I0307 15:59:26.297292 140616853095616 spec.py:333] Evaluating on the validation split.
I0307 15:59:27.595282 140616853095616 spec.py:349] Evaluating on the test split.
I0307 15:59:28.890760 140616853095616 submission_runner.py:469] Time since start: 414.53s, 	Step: 481, 	{'train/ssim': 0.7112443787711007, 'train/loss': 0.29891014099121094, 'validation/ssim': 0.6903459487769062, 'validation/loss': 0.3184809644502673, 'validation/num_examples': 3554, 'test/ssim': 0.7077388555701969, 'test/loss': 0.32064090697867215, 'test/num_examples': 3581, 'score': 310.3247218132019, 'total_duration': 414.52664375305176, 'accumulated_submission_time': 310.3247218132019, 'accumulated_eval_time': 104.09919023513794, 'accumulated_logging_time': 0.015844345092773438}
I0307 15:59:28.902073 140460748150528 logging_writer.py:48] [481] accumulated_eval_time=104.099, accumulated_logging_time=0.0158443, accumulated_submission_time=310.325, global_step=481, preemption_count=0, score=310.325, test/loss=0.320641, test/num_examples=3581, test/ssim=0.707739, total_duration=414.527, train/loss=0.29891, train/ssim=0.711244, validation/loss=0.318481, validation/num_examples=3554, validation/ssim=0.690346
I0307 15:59:30.585720 140460756543232 logging_writer.py:48] [500] global_step=500, grad_norm=0.16523075103759766, loss=0.3475453555583954
I0307 15:59:47.526108 140460748150528 logging_writer.py:48] [600] global_step=600, grad_norm=0.2212408483028412, loss=0.36726483702659607
I0307 16:00:06.615274 140460756543232 logging_writer.py:48] [700] global_step=700, grad_norm=0.14353661239147186, loss=0.33109885454177856
I0307 16:00:24.795280 140460748150528 logging_writer.py:48] [800] global_step=800, grad_norm=0.2178443968296051, loss=0.346784383058548
I0307 16:00:43.333446 140460756543232 logging_writer.py:48] [900] global_step=900, grad_norm=0.06729927659034729, loss=0.3359149098396301
I0307 16:00:49.033895 140616853095616 spec.py:321] Evaluating on the training split.
I0307 16:00:50.322035 140616853095616 spec.py:333] Evaluating on the validation split.
I0307 16:00:51.614727 140616853095616 spec.py:349] Evaluating on the test split.
I0307 16:00:52.920070 140616853095616 submission_runner.py:469] Time since start: 498.56s, 	Step: 929, 	{'train/ssim': 0.7268283707754952, 'train/loss': 0.28778001240321566, 'validation/ssim': 0.7044542406003799, 'validation/loss': 0.30737376816834905, 'validation/num_examples': 3554, 'test/ssim': 0.7215728546713558, 'test/loss': 0.30974112912550616, 'test/num_examples': 3581, 'score': 390.3729133605957, 'total_duration': 498.5559594631195, 'accumulated_submission_time': 390.3729133605957, 'accumulated_eval_time': 107.98533248901367, 'accumulated_logging_time': 0.04126453399658203}
I0307 16:00:52.955705 140460748150528 logging_writer.py:48] [929] accumulated_eval_time=107.985, accumulated_logging_time=0.0412645, accumulated_submission_time=390.373, global_step=929, preemption_count=0, score=390.373, test/loss=0.309741, test/num_examples=3581, test/ssim=0.721573, total_duration=498.556, train/loss=0.28778, train/ssim=0.726828, validation/loss=0.307374, validation/num_examples=3554, validation/ssim=0.704454
I0307 16:00:59.672030 140460756543232 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.2879674434661865, loss=0.29502904415130615
I0307 16:01:08.060491 140460748150528 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.16382870078086853, loss=0.33393287658691406
I0307 16:01:16.354798 140460756543232 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.07180699706077576, loss=0.3176918029785156
I0307 16:01:24.660657 140460748150528 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.4115986227989197, loss=0.3029475212097168
I0307 16:01:32.932395 140460756543232 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.13883176445960999, loss=0.30338290333747864
I0307 16:01:41.236697 140460748150528 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.3621924817562103, loss=0.33132264018058777
I0307 16:01:49.553235 140460756543232 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.11970920115709305, loss=0.25100499391555786
I0307 16:01:57.874082 140460748150528 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.277648001909256, loss=0.3053358197212219
I0307 16:02:06.198120 140460756543232 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.29115918278694153, loss=0.2716222107410431
I0307 16:02:12.928942 140616853095616 spec.py:321] Evaluating on the training split.
I0307 16:02:14.221448 140616853095616 spec.py:333] Evaluating on the validation split.
I0307 16:02:15.517883 140616853095616 spec.py:349] Evaluating on the test split.
I0307 16:02:16.822351 140616853095616 submission_runner.py:469] Time since start: 582.46s, 	Step: 1882, 	{'train/ssim': 0.731466565813337, 'train/loss': 0.2809727191925049, 'validation/ssim': 0.7087999986810636, 'validation/loss': 0.3002311436057963, 'validation/num_examples': 3554, 'test/ssim': 0.7259995653056059, 'test/loss': 0.302540957811191, 'test/num_examples': 3581, 'score': 470.26545906066895, 'total_duration': 582.4582343101501, 'accumulated_submission_time': 470.26545906066895, 'accumulated_eval_time': 111.87870740890503, 'accumulated_logging_time': 0.10169720649719238}
I0307 16:02:16.831465 140460748150528 logging_writer.py:48] [1882] accumulated_eval_time=111.879, accumulated_logging_time=0.101697, accumulated_submission_time=470.265, global_step=1882, preemption_count=0, score=470.265, test/loss=0.302541, test/num_examples=3581, test/ssim=0.726, total_duration=582.458, train/loss=0.280973, train/ssim=0.731467, validation/loss=0.300231, validation/num_examples=3554, validation/ssim=0.7088
I0307 16:02:18.422547 140460756543232 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.0665242001414299, loss=0.2501426041126251
I0307 16:02:26.724859 140460748150528 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.447945237159729, loss=0.28879857063293457
I0307 16:02:35.042280 140460756543232 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.14470475912094116, loss=0.3212587237358093
I0307 16:02:43.329115 140460748150528 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.18218518793582916, loss=0.26575297117233276
I0307 16:02:51.589838 140460756543232 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.06611956655979156, loss=0.3187277615070343
I0307 16:02:59.875154 140460748150528 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.0932559221982956, loss=0.29316583275794983
I0307 16:03:08.174695 140460756543232 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.1980884075164795, loss=0.29965487122535706
I0307 16:03:16.468424 140460748150528 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.2489849478006363, loss=0.30858564376831055
I0307 16:03:24.767687 140460756543232 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.09311229735612869, loss=0.3149765431880951
I0307 16:03:33.080117 140460748150528 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.13145865499973297, loss=0.30958837270736694
I0307 16:03:36.905870 140616853095616 spec.py:321] Evaluating on the training split.
I0307 16:03:38.199046 140616853095616 spec.py:333] Evaluating on the validation split.
I0307 16:03:39.495156 140616853095616 spec.py:349] Evaluating on the test split.
I0307 16:03:40.792912 140616853095616 submission_runner.py:469] Time since start: 666.43s, 	Step: 2847, 	{'train/ssim': 0.7361689976283482, 'train/loss': 0.2774186134338379, 'validation/ssim': 0.7132158252585116, 'validation/loss': 0.29716499427361776, 'validation/num_examples': 3554, 'test/ssim': 0.7306273289147585, 'test/loss': 0.29904451772375035, 'test/num_examples': 3581, 'score': 550.2847602367401, 'total_duration': 666.4287843704224, 'accumulated_submission_time': 550.2847602367401, 'accumulated_eval_time': 115.7656946182251, 'accumulated_logging_time': 0.1188802719116211}
I0307 16:03:40.803386 140460756543232 logging_writer.py:48] [2847] accumulated_eval_time=115.766, accumulated_logging_time=0.11888, accumulated_submission_time=550.285, global_step=2847, preemption_count=0, score=550.285, test/loss=0.299045, test/num_examples=3581, test/ssim=0.730627, total_duration=666.429, train/loss=0.277419, train/ssim=0.736169, validation/loss=0.297165, validation/num_examples=3554, validation/ssim=0.713216
I0307 16:03:45.275334 140460748150528 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.21401916444301605, loss=0.2766238749027252
I0307 16:03:53.574559 140460756543232 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.18845224380493164, loss=0.3247987627983093
I0307 16:04:01.861754 140460748150528 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.17601017653942108, loss=0.31518495082855225
I0307 16:04:10.164313 140460756543232 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.16944551467895508, loss=0.3198692500591278
I0307 16:04:18.450112 140460748150528 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.15004032850265503, loss=0.2843021750450134
I0307 16:04:26.725965 140460756543232 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.17745408415794373, loss=0.27965047955513
I0307 16:04:35.035007 140460748150528 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.20658905804157257, loss=0.28934890031814575
I0307 16:04:43.342674 140460756543232 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.3750883936882019, loss=0.353129506111145
I0307 16:04:51.611064 140460748150528 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.2891804575920105, loss=0.3220270872116089
I0307 16:04:59.900768 140460756543232 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.10250263661146164, loss=0.263479620218277
I0307 16:05:00.819591 140616853095616 spec.py:321] Evaluating on the training split.
I0307 16:05:02.111937 140616853095616 spec.py:333] Evaluating on the validation split.
I0307 16:05:03.406690 140616853095616 spec.py:349] Evaluating on the test split.
I0307 16:05:04.700964 140616853095616 submission_runner.py:469] Time since start: 750.34s, 	Step: 3812, 	{'train/ssim': 0.7387965066092355, 'train/loss': 0.2736138275691441, 'validation/ssim': 0.7156405387195766, 'validation/loss': 0.293467884997538, 'validation/num_examples': 3554, 'test/ssim': 0.7329760149050545, 'test/loss': 0.2952618038344736, 'test/num_examples': 3581, 'score': 630.2471811771393, 'total_duration': 750.336847782135, 'accumulated_submission_time': 630.2471811771393, 'accumulated_eval_time': 119.6470217704773, 'accumulated_logging_time': 0.13733768463134766}
I0307 16:05:04.711158 140460748150528 logging_writer.py:48] [3812] accumulated_eval_time=119.647, accumulated_logging_time=0.137338, accumulated_submission_time=630.247, global_step=3812, preemption_count=0, score=630.247, test/loss=0.295262, test/num_examples=3581, test/ssim=0.732976, total_duration=750.337, train/loss=0.273614, train/ssim=0.738797, validation/loss=0.293468, validation/num_examples=3554, validation/ssim=0.715641
I0307 16:05:12.095953 140460756543232 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.32961201667785645, loss=0.329439640045166
I0307 16:05:20.362348 140460748150528 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.32000938057899475, loss=0.25243332982063293
I0307 16:05:28.643990 140460756543232 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.06821306049823761, loss=0.24407659471035004
I0307 16:05:36.909295 140460748150528 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.1658533811569214, loss=0.32064926624298096
I0307 16:05:45.198858 140460756543232 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.2498033493757248, loss=0.3482668995857239
I0307 16:05:53.481893 140460748150528 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.08809658139944077, loss=0.2725801467895508
I0307 16:06:01.739278 140460756543232 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.2061941772699356, loss=0.32062119245529175
I0307 16:06:10.005503 140460748150528 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.08955895900726318, loss=0.24774983525276184
I0307 16:06:18.300904 140460756543232 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.08995583653450012, loss=0.2464093714952469
I0307 16:06:24.781435 140616853095616 spec.py:321] Evaluating on the training split.
I0307 16:06:26.074100 140616853095616 spec.py:333] Evaluating on the validation split.
I0307 16:06:27.369600 140616853095616 spec.py:349] Evaluating on the test split.
I0307 16:06:28.665004 140616853095616 submission_runner.py:469] Time since start: 834.30s, 	Step: 4779, 	{'train/ssim': 0.7389520236424038, 'train/loss': 0.27224622453962055, 'validation/ssim': 0.7155438854108047, 'validation/loss': 0.2918601564698227, 'validation/num_examples': 3554, 'test/ssim': 0.7329728106019617, 'test/loss': 0.29356747744170625, 'test/num_examples': 3581, 'score': 710.2633895874023, 'total_duration': 834.3008890151978, 'accumulated_submission_time': 710.2633895874023, 'accumulated_eval_time': 123.53054666519165, 'accumulated_logging_time': 0.155534029006958}
I0307 16:06:28.674749 140460748150528 logging_writer.py:48] [4779] accumulated_eval_time=123.531, accumulated_logging_time=0.155534, accumulated_submission_time=710.263, global_step=4779, preemption_count=0, score=710.263, test/loss=0.293567, test/num_examples=3581, test/ssim=0.732973, total_duration=834.301, train/loss=0.272246, train/ssim=0.738952, validation/loss=0.29186, validation/num_examples=3554, validation/ssim=0.715544
I0307 16:06:30.515985 140460756543232 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.13052046298980713, loss=0.24490223824977875
I0307 16:06:38.805991 140460748150528 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.0656544491648674, loss=0.29250165820121765
I0307 16:06:47.087119 140460756543232 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.13612474501132965, loss=0.3197023868560791
I0307 16:06:55.372166 140460748150528 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.08336620032787323, loss=0.261615127325058
I0307 16:07:03.665589 140460756543232 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.14626361429691315, loss=0.26441332697868347
I0307 16:07:11.963172 140460748150528 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.13362766802310944, loss=0.320715069770813
I0307 16:07:20.240479 140460756543232 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.12087400257587433, loss=0.2550342381000519
I0307 16:07:28.500790 140460748150528 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.5098118185997009, loss=0.319318562746048
I0307 16:07:36.792879 140460756543232 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.12432616949081421, loss=0.3057217299938202
I0307 16:07:45.097373 140460748150528 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.11378664523363113, loss=0.28574883937835693
I0307 16:07:48.739237 140616853095616 spec.py:321] Evaluating on the training split.
I0307 16:07:50.034855 140616853095616 spec.py:333] Evaluating on the validation split.
I0307 16:07:51.332871 140616853095616 spec.py:349] Evaluating on the test split.
I0307 16:07:52.632234 140616853095616 submission_runner.py:469] Time since start: 918.27s, 	Step: 5745, 	{'train/ssim': 0.7402826717921666, 'train/loss': 0.2725231647491455, 'validation/ssim': 0.7166053544421779, 'validation/loss': 0.2925934370273811, 'validation/num_examples': 3554, 'test/ssim': 0.7338398132068557, 'test/loss': 0.2942429036276529, 'test/num_examples': 3581, 'score': 790.2726905345917, 'total_duration': 918.2681102752686, 'accumulated_submission_time': 790.2726905345917, 'accumulated_eval_time': 127.42349624633789, 'accumulated_logging_time': 0.17377042770385742}
I0307 16:07:52.642370 140460756543232 logging_writer.py:48] [5745] accumulated_eval_time=127.423, accumulated_logging_time=0.17377, accumulated_submission_time=790.273, global_step=5745, preemption_count=0, score=790.273, test/loss=0.294243, test/num_examples=3581, test/ssim=0.73384, total_duration=918.268, train/loss=0.272523, train/ssim=0.740283, validation/loss=0.292593, validation/num_examples=3554, validation/ssim=0.716605
I0307 16:07:57.280725 140460748150528 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.11965921521186829, loss=0.2511409521102905
I0307 16:08:05.543287 140460756543232 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.44016799330711365, loss=0.2803035080432892
I0307 16:08:13.807751 140460748150528 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.36317768692970276, loss=0.3042202293872833
I0307 16:08:22.107614 140460756543232 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.04695424437522888, loss=0.3208494186401367
I0307 16:08:30.376547 140460748150528 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.14680440723896027, loss=0.22854064404964447
I0307 16:08:38.667321 140460756543232 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.1955646127462387, loss=0.2908923923969269
I0307 16:08:46.952327 140460748150528 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.16404841840267181, loss=0.37763068079948425
I0307 16:08:55.231332 140460756543232 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.39781588315963745, loss=0.2629711329936981
I0307 16:09:03.506700 140460748150528 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.09051141142845154, loss=0.2586568593978882
I0307 16:09:11.817250 140460756543232 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.09416934102773666, loss=0.30195218324661255
I0307 16:09:12.643126 140616853095616 spec.py:321] Evaluating on the training split.
I0307 16:09:13.937357 140616853095616 spec.py:333] Evaluating on the validation split.
I0307 16:09:15.231413 140616853095616 spec.py:349] Evaluating on the test split.
I0307 16:09:16.527713 140616853095616 submission_runner.py:469] Time since start: 1002.16s, 	Step: 6711, 	{'train/ssim': 0.741997378213065, 'train/loss': 0.270321113722665, 'validation/ssim': 0.7181727592369513, 'validation/loss': 0.29048557743036013, 'validation/num_examples': 3554, 'test/ssim': 0.7355230949804524, 'test/loss': 0.2921064856730836, 'test/num_examples': 3581, 'score': 870.2188169956207, 'total_duration': 1002.1635987758636, 'accumulated_submission_time': 870.2188169956207, 'accumulated_eval_time': 131.30804777145386, 'accumulated_logging_time': 0.19206547737121582}
I0307 16:09:16.538773 140460748150528 logging_writer.py:48] [6711] accumulated_eval_time=131.308, accumulated_logging_time=0.192065, accumulated_submission_time=870.219, global_step=6711, preemption_count=0, score=870.219, test/loss=0.292106, test/num_examples=3581, test/ssim=0.735523, total_duration=1002.16, train/loss=0.270321, train/ssim=0.741997, validation/loss=0.290486, validation/num_examples=3554, validation/ssim=0.718173
I0307 16:09:24.001518 140460756543232 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.11491339653730392, loss=0.2730256915092468
I0307 16:09:32.286580 140460748150528 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.13373449444770813, loss=0.42385607957839966
I0307 16:09:40.564079 140460756543232 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.2266673743724823, loss=0.26320692896842957
I0307 16:09:48.857532 140460748150528 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.23720750212669373, loss=0.2488856017589569
I0307 16:09:57.150956 140460756543232 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.1752106249332428, loss=0.34463900327682495
I0307 16:10:05.450784 140460748150528 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.0945817306637764, loss=0.37615546584129333
I0307 16:10:13.737510 140460756543232 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.1988568902015686, loss=0.26810264587402344
I0307 16:10:22.025044 140460748150528 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.07957690209150314, loss=0.33595365285873413
I0307 16:10:30.315275 140460756543232 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.31415995955467224, loss=0.27113011479377747
I0307 16:10:36.601724 140616853095616 spec.py:321] Evaluating on the training split.
I0307 16:10:37.895586 140616853095616 spec.py:333] Evaluating on the validation split.
I0307 16:10:39.190565 140616853095616 spec.py:349] Evaluating on the test split.
I0307 16:10:40.491232 140616853095616 submission_runner.py:469] Time since start: 1086.13s, 	Step: 7677, 	{'train/ssim': 0.7426671981811523, 'train/loss': 0.26995488575526644, 'validation/ssim': 0.7193892719910664, 'validation/loss': 0.2897324441320519, 'validation/num_examples': 3554, 'test/ssim': 0.736902513373534, 'test/loss': 0.29124176698635157, 'test/num_examples': 3581, 'score': 950.2277727127075, 'total_duration': 1086.1271166801453, 'accumulated_submission_time': 950.2277727127075, 'accumulated_eval_time': 135.1975119113922, 'accumulated_logging_time': 0.21083450317382812}
I0307 16:10:40.501677 140460748150528 logging_writer.py:48] [7677] accumulated_eval_time=135.198, accumulated_logging_time=0.210835, accumulated_submission_time=950.228, global_step=7677, preemption_count=0, score=950.228, test/loss=0.291242, test/num_examples=3581, test/ssim=0.736903, total_duration=1086.13, train/loss=0.269955, train/ssim=0.742667, validation/loss=0.289732, validation/num_examples=3554, validation/ssim=0.719389
I0307 16:10:42.508345 140460756543232 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.41734299063682556, loss=0.25633201003074646
I0307 16:10:50.777173 140460748150528 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.19637441635131836, loss=0.21168601512908936
I0307 16:10:59.071242 140460756543232 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.14790818095207214, loss=0.3379538059234619
I0307 16:11:07.324816 140460748150528 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.11878359317779541, loss=0.4228181540966034
I0307 16:11:15.599101 140460756543232 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.18997088074684143, loss=0.30391114950180054
I0307 16:11:23.899317 140460748150528 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.09818795323371887, loss=0.3390517830848694
I0307 16:11:32.205343 140460756543232 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.1725575476884842, loss=0.3026820719242096
I0307 16:11:40.494272 140460748150528 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.08319037407636642, loss=0.2684001624584198
I0307 16:11:48.755783 140460756543232 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.1725149005651474, loss=0.3438992500305176
I0307 16:11:57.050348 140460748150528 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.1186792328953743, loss=0.2997920513153076
I0307 16:12:00.534555 140616853095616 spec.py:321] Evaluating on the training split.
I0307 16:12:01.827138 140616853095616 spec.py:333] Evaluating on the validation split.
I0307 16:12:03.125020 140616853095616 spec.py:349] Evaluating on the test split.
I0307 16:12:04.421328 140616853095616 submission_runner.py:469] Time since start: 1170.06s, 	Step: 8643, 	{'train/ssim': 0.7425406319754464, 'train/loss': 0.26960534708840506, 'validation/ssim': 0.719443403339547, 'validation/loss': 0.28923791167302687, 'validation/num_examples': 3554, 'test/ssim': 0.7369913475635297, 'test/loss': 0.29074632718688914, 'test/num_examples': 3581, 'score': 1030.2060573101044, 'total_duration': 1170.057211637497, 'accumulated_submission_time': 1030.2060573101044, 'accumulated_eval_time': 139.08423948287964, 'accumulated_logging_time': 0.22933006286621094}
I0307 16:12:04.431539 140460756543232 logging_writer.py:48] [8643] accumulated_eval_time=139.084, accumulated_logging_time=0.22933, accumulated_submission_time=1030.21, global_step=8643, preemption_count=0, score=1030.21, test/loss=0.290746, test/num_examples=3581, test/ssim=0.736991, total_duration=1170.06, train/loss=0.269605, train/ssim=0.742541, validation/loss=0.289238, validation/num_examples=3554, validation/ssim=0.719443
I0307 16:12:09.232825 140460748150528 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.2408687323331833, loss=0.29956135153770447
I0307 16:12:17.527403 140460756543232 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.1220313310623169, loss=0.4168701171875
I0307 16:12:25.817804 140460748150528 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.13118824362754822, loss=0.21144242584705353
I0307 16:12:34.094837 140460756543232 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.1090242862701416, loss=0.31280747056007385
I0307 16:12:42.355078 140460748150528 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.14382857084274292, loss=0.34828418493270874
I0307 16:12:50.614636 140460756543232 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.21952494978904724, loss=0.2667121887207031
I0307 16:12:58.904838 140460748150528 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.0885763093829155, loss=0.3245593309402466
I0307 16:13:07.176698 140460756543232 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.30193838477134705, loss=0.20169338583946228
I0307 16:13:15.451072 140460748150528 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.11355755478143692, loss=0.2096431851387024
I0307 16:13:23.740805 140460756543232 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.07413429766893387, loss=0.2897965610027313
I0307 16:13:24.491045 140616853095616 spec.py:321] Evaluating on the training split.
I0307 16:13:25.781373 140616853095616 spec.py:333] Evaluating on the validation split.
I0307 16:13:27.076490 140616853095616 spec.py:349] Evaluating on the test split.
I0307 16:13:28.374201 140616853095616 submission_runner.py:469] Time since start: 1254.01s, 	Step: 9610, 	{'train/ssim': 0.743058613368443, 'train/loss': 0.2688384737287249, 'validation/ssim': 0.7195667788495357, 'validation/loss': 0.2885968192749367, 'validation/num_examples': 3554, 'test/ssim': 0.7369914839168529, 'test/loss': 0.2901561559162071, 'test/num_examples': 3581, 'score': 1110.2093303203583, 'total_duration': 1254.0100696086884, 'accumulated_submission_time': 1110.2093303203583, 'accumulated_eval_time': 142.96735429763794, 'accumulated_logging_time': 0.2472386360168457}
I0307 16:13:28.385317 140460748150528 logging_writer.py:48] [9610] accumulated_eval_time=142.967, accumulated_logging_time=0.247239, accumulated_submission_time=1110.21, global_step=9610, preemption_count=0, score=1110.21, test/loss=0.290156, test/num_examples=3581, test/ssim=0.736991, total_duration=1254.01, train/loss=0.268838, train/ssim=0.743059, validation/loss=0.288597, validation/num_examples=3554, validation/ssim=0.719567
I0307 16:13:38.689508 140460756543232 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.11086580157279968, loss=0.3138803541660309
I0307 16:13:47.141337 140460748150528 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.1632014811038971, loss=0.3361673057079315
I0307 16:13:55.516638 140460756543232 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.07084091007709503, loss=0.2593681216239929
I0307 16:14:03.818088 140460748150528 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.059885699301958084, loss=0.26937800645828247
I0307 16:14:12.122677 140460756543232 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.0927172303199768, loss=0.2895408868789673
I0307 16:14:20.412708 140460748150528 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.09407971054315567, loss=0.2758897840976715
I0307 16:14:28.727341 140460756543232 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.13870643079280853, loss=0.35998350381851196
I0307 16:14:37.040846 140460748150528 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.15412507951259613, loss=0.25501102209091187
I0307 16:14:45.314923 140460756543232 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.15674613416194916, loss=0.23597504198551178
I0307 16:14:48.449880 140616853095616 spec.py:321] Evaluating on the training split.
I0307 16:14:49.740441 140616853095616 spec.py:333] Evaluating on the validation split.
I0307 16:14:51.035414 140616853095616 spec.py:349] Evaluating on the test split.
I0307 16:14:52.330743 140616853095616 submission_runner.py:469] Time since start: 1337.97s, 	Step: 10539, 	{'train/ssim': 0.7428361347743443, 'train/loss': 0.26940992900303434, 'validation/ssim': 0.7188806571380838, 'validation/loss': 0.2896745689276168, 'validation/num_examples': 3554, 'test/ssim': 0.7362786969203784, 'test/loss': 0.2911889300736526, 'test/num_examples': 3581, 'score': 1187.4759728908539, 'total_duration': 1337.9666283130646, 'accumulated_submission_time': 1187.4759728908539, 'accumulated_eval_time': 146.84817051887512, 'accumulated_logging_time': 3.00484299659729}
I0307 16:14:52.341711 140460748150528 logging_writer.py:48] [10539] accumulated_eval_time=146.848, accumulated_logging_time=3.00484, accumulated_submission_time=1187.48, global_step=10539, preemption_count=0, score=1187.48, test/loss=0.291189, test/num_examples=3581, test/ssim=0.736279, total_duration=1337.97, train/loss=0.26941, train/ssim=0.742836, validation/loss=0.289675, validation/num_examples=3554, validation/ssim=0.718881
I0307 16:14:57.505122 140460756543232 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.20691195130348206, loss=0.2925111949443817
I0307 16:15:05.805620 140460748150528 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.16229192912578583, loss=0.2987290918827057
I0307 16:15:14.109245 140460756543232 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.11426146328449249, loss=0.31840184330940247
I0307 16:15:22.387913 140460748150528 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.38010433316230774, loss=0.31417152285575867
I0307 16:15:30.668841 140460756543232 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.07643861323595047, loss=0.3334159553050995
I0307 16:15:38.941086 140460748150528 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.07994499057531357, loss=0.20276178419589996
I0307 16:15:47.241446 140460756543232 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.10988413542509079, loss=0.29431745409965515
I0307 16:15:55.529776 140460748150528 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.15850163996219635, loss=0.23345687985420227
I0307 16:16:03.848625 140460756543232 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.15442009270191193, loss=0.2520041763782501
I0307 16:16:12.136945 140460748150528 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.1542184054851532, loss=0.2834184765815735
I0307 16:16:12.389332 140616853095616 spec.py:321] Evaluating on the training split.
I0307 16:16:13.681985 140616853095616 spec.py:333] Evaluating on the validation split.
I0307 16:16:14.974771 140616853095616 spec.py:349] Evaluating on the test split.
I0307 16:16:16.270268 140616853095616 submission_runner.py:469] Time since start: 1421.91s, 	Step: 11504, 	{'train/ssim': 0.7444582666669574, 'train/loss': 0.26833774362291607, 'validation/ssim': 0.7205948623030388, 'validation/loss': 0.28857610785162846, 'validation/num_examples': 3554, 'test/ssim': 0.7380572896668179, 'test/loss': 0.29010556883333916, 'test/num_examples': 3581, 'score': 1267.467027425766, 'total_duration': 1421.9061546325684, 'accumulated_submission_time': 1267.467027425766, 'accumulated_eval_time': 150.72906160354614, 'accumulated_logging_time': 3.024651050567627}
I0307 16:16:16.281275 140460756543232 logging_writer.py:48] [11504] accumulated_eval_time=150.729, accumulated_logging_time=3.02465, accumulated_submission_time=1267.47, global_step=11504, preemption_count=0, score=1267.47, test/loss=0.290106, test/num_examples=3581, test/ssim=0.738057, total_duration=1421.91, train/loss=0.268338, train/ssim=0.744458, validation/loss=0.288576, validation/num_examples=3554, validation/ssim=0.720595
I0307 16:16:24.347784 140460748150528 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.15523837506771088, loss=0.29108765721321106
I0307 16:16:32.665028 140460756543232 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.12462758272886276, loss=0.2516375780105591
I0307 16:16:40.946210 140460748150528 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.25959616899490356, loss=0.2733073830604553
I0307 16:16:49.212556 140460756543232 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.11274872720241547, loss=0.2647724151611328
I0307 16:16:57.495355 140460748150528 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.17744798958301544, loss=0.24266797304153442
I0307 16:17:05.758512 140460756543232 logging_writer.py:48] [12100] global_step=12100, grad_norm=0.10313867777585983, loss=0.310754656791687
I0307 16:17:14.062888 140460748150528 logging_writer.py:48] [12200] global_step=12200, grad_norm=0.10115890204906464, loss=0.2789113521575928
I0307 16:17:22.361202 140460756543232 logging_writer.py:48] [12300] global_step=12300, grad_norm=0.09580251574516296, loss=0.277715265750885
I0307 16:17:30.660221 140460748150528 logging_writer.py:48] [12400] global_step=12400, grad_norm=0.09144790470600128, loss=0.33439382910728455
I0307 16:17:36.277023 140616853095616 spec.py:321] Evaluating on the training split.
I0307 16:17:37.570389 140616853095616 spec.py:333] Evaluating on the validation split.
I0307 16:17:38.867713 140616853095616 spec.py:349] Evaluating on the test split.
I0307 16:17:40.166674 140616853095616 submission_runner.py:469] Time since start: 1505.80s, 	Step: 12469, 	{'train/ssim': 0.7441231182643345, 'train/loss': 0.2687712737492153, 'validation/ssim': 0.7202655403682471, 'validation/loss': 0.28900926168138014, 'validation/num_examples': 3554, 'test/ssim': 0.7374430861229405, 'test/loss': 0.29058089651764524, 'test/num_examples': 3581, 'score': 1347.4073014259338, 'total_duration': 1505.8025550842285, 'accumulated_submission_time': 1347.4073014259338, 'accumulated_eval_time': 154.61867356300354, 'accumulated_logging_time': 3.0438902378082275}
I0307 16:17:40.178385 140460756543232 logging_writer.py:48] [12469] accumulated_eval_time=154.619, accumulated_logging_time=3.04389, accumulated_submission_time=1347.41, global_step=12469, preemption_count=0, score=1347.41, test/loss=0.290581, test/num_examples=3581, test/ssim=0.737443, total_duration=1505.8, train/loss=0.268771, train/ssim=0.744123, validation/loss=0.289009, validation/num_examples=3554, validation/ssim=0.720266
I0307 16:17:42.847741 140460748150528 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.19003941118717194, loss=0.27882158756256104
I0307 16:17:51.140439 140460756543232 logging_writer.py:48] [12600] global_step=12600, grad_norm=0.10067935287952423, loss=0.31492680311203003
I0307 16:17:59.409170 140460748150528 logging_writer.py:48] [12700] global_step=12700, grad_norm=0.05623024329543114, loss=0.2527030408382416
I0307 16:18:07.712525 140460756543232 logging_writer.py:48] [12800] global_step=12800, grad_norm=0.26819631457328796, loss=0.31242111325263977
I0307 16:18:16.007721 140460748150528 logging_writer.py:48] [12900] global_step=12900, grad_norm=0.14265906810760498, loss=0.3494708240032196
I0307 16:18:24.293681 140460756543232 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.19246840476989746, loss=0.30748841166496277
I0307 16:18:32.566715 140460748150528 logging_writer.py:48] [13100] global_step=13100, grad_norm=0.14563828706741333, loss=0.271120548248291
I0307 16:18:40.864827 140460756543232 logging_writer.py:48] [13200] global_step=13200, grad_norm=0.07609459012746811, loss=0.35942357778549194
I0307 16:18:49.178980 140460748150528 logging_writer.py:48] [13300] global_step=13300, grad_norm=0.13403145968914032, loss=0.25452739000320435
I0307 16:18:57.466913 140460756543232 logging_writer.py:48] [13400] global_step=13400, grad_norm=0.06805068999528885, loss=0.35505977272987366
I0307 16:19:00.211186 140616853095616 spec.py:321] Evaluating on the training split.
I0307 16:19:01.501630 140616853095616 spec.py:333] Evaluating on the validation split.
I0307 16:19:02.797491 140616853095616 spec.py:349] Evaluating on the test split.
I0307 16:19:04.093601 140616853095616 submission_runner.py:469] Time since start: 1589.73s, 	Step: 13434, 	{'train/ssim': 0.7460836683000837, 'train/loss': 0.26756627219063894, 'validation/ssim': 0.7224871238833005, 'validation/loss': 0.28790812151580963, 'validation/num_examples': 3554, 'test/ssim': 0.7397983852494066, 'test/loss': 0.2894373352851857, 'test/num_examples': 3581, 'score': 1427.3862013816833, 'total_duration': 1589.7294850349426, 'accumulated_submission_time': 1427.3862013816833, 'accumulated_eval_time': 158.50104570388794, 'accumulated_logging_time': 3.063603162765503}
I0307 16:19:04.105051 140460748150528 logging_writer.py:48] [13434] accumulated_eval_time=158.501, accumulated_logging_time=3.0636, accumulated_submission_time=1427.39, global_step=13434, preemption_count=0, score=1427.39, test/loss=0.289437, test/num_examples=3581, test/ssim=0.739798, total_duration=1589.73, train/loss=0.267566, train/ssim=0.746084, validation/loss=0.287908, validation/num_examples=3554, validation/ssim=0.722487
I0307 16:19:09.671168 140460756543232 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.09790568053722382, loss=0.3149264454841614
I0307 16:19:17.952708 140460748150528 logging_writer.py:48] [13600] global_step=13600, grad_norm=0.07937176525592804, loss=0.3025950789451599
I0307 16:19:26.251599 140460756543232 logging_writer.py:48] [13700] global_step=13700, grad_norm=0.1253773719072342, loss=0.3110075294971466
I0307 16:19:34.533081 140460748150528 logging_writer.py:48] [13800] global_step=13800, grad_norm=0.1858157515525818, loss=0.2620072066783905
I0307 16:19:42.843671 140460756543232 logging_writer.py:48] [13900] global_step=13900, grad_norm=0.17085544764995575, loss=0.2542983293533325
I0307 16:19:51.146983 140460748150528 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.17384915053844452, loss=0.3891623020172119
I0307 16:19:59.449388 140460756543232 logging_writer.py:48] [14100] global_step=14100, grad_norm=0.2289181798696518, loss=0.22607360780239105
I0307 16:20:07.718827 140460748150528 logging_writer.py:48] [14200] global_step=14200, grad_norm=0.14585955440998077, loss=0.23723305761814117
I0307 16:20:15.983260 140460756543232 logging_writer.py:48] [14300] global_step=14300, grad_norm=0.17040453851222992, loss=0.30020618438720703
I0307 16:20:24.175426 140616853095616 spec.py:321] Evaluating on the training split.
I0307 16:20:25.466007 140616853095616 spec.py:333] Evaluating on the validation split.
I0307 16:20:26.759272 140616853095616 spec.py:349] Evaluating on the test split.
I0307 16:20:28.056029 140616853095616 submission_runner.py:469] Time since start: 1673.69s, 	Step: 14400, 	{'train/ssim': 0.746281487601144, 'train/loss': 0.2675477777208601, 'validation/ssim': 0.7224320995049592, 'validation/loss': 0.2878974223311322, 'validation/num_examples': 3554, 'test/ssim': 0.7397602063189402, 'test/loss': 0.2893891343854719, 'test/num_examples': 3581, 'score': 1507.398591041565, 'total_duration': 1673.6918992996216, 'accumulated_submission_time': 1507.398591041565, 'accumulated_eval_time': 162.38159012794495, 'accumulated_logging_time': 3.0839579105377197}
I0307 16:20:28.067399 140460748150528 logging_writer.py:48] [14400] accumulated_eval_time=162.382, accumulated_logging_time=3.08396, accumulated_submission_time=1507.4, global_step=14400, preemption_count=0, score=1507.4, test/loss=0.289389, test/num_examples=3581, test/ssim=0.73976, total_duration=1673.69, train/loss=0.267548, train/ssim=0.746281, validation/loss=0.287897, validation/num_examples=3554, validation/ssim=0.722432
I0307 16:20:28.161890 140460756543232 logging_writer.py:48] [14400] global_step=14400, grad_norm=0.14032837748527527, loss=0.3567747473716736
I0307 16:20:36.434860 140460748150528 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.08442337810993195, loss=0.28723132610321045
I0307 16:20:44.698785 140460756543232 logging_writer.py:48] [14600] global_step=14600, grad_norm=0.09514599293470383, loss=0.237899512052536
I0307 16:20:52.964193 140460748150528 logging_writer.py:48] [14700] global_step=14700, grad_norm=0.08482933044433594, loss=0.3243219554424286
I0307 16:21:01.271081 140460756543232 logging_writer.py:48] [14800] global_step=14800, grad_norm=0.09501530975103378, loss=0.3268662691116333
I0307 16:21:09.569688 140460748150528 logging_writer.py:48] [14900] global_step=14900, grad_norm=0.11635556817054749, loss=0.21963448822498322
I0307 16:21:17.894588 140460756543232 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.22328674793243408, loss=0.26313236355781555
I0307 16:21:26.189735 140460748150528 logging_writer.py:48] [15100] global_step=15100, grad_norm=0.08331237733364105, loss=0.3377932012081146
I0307 16:21:34.454233 140460756543232 logging_writer.py:48] [15200] global_step=15200, grad_norm=0.22532649338245392, loss=0.2746587097644806
I0307 16:21:42.717032 140460748150528 logging_writer.py:48] [15300] global_step=15300, grad_norm=0.12922003865242004, loss=0.313970685005188
I0307 16:21:48.091304 140616853095616 spec.py:321] Evaluating on the training split.
I0307 16:21:49.381721 140616853095616 spec.py:333] Evaluating on the validation split.
I0307 16:21:50.676538 140616853095616 spec.py:349] Evaluating on the test split.
I0307 16:21:51.977411 140616853095616 submission_runner.py:469] Time since start: 1757.61s, 	Step: 15366, 	{'train/ssim': 0.7458127566746303, 'train/loss': 0.26731155599866596, 'validation/ssim': 0.7218966937561551, 'validation/loss': 0.2875665375940841, 'validation/num_examples': 3554, 'test/ssim': 0.7392618349230313, 'test/loss': 0.2890428651214744, 'test/num_examples': 3581, 'score': 1587.3673589229584, 'total_duration': 1757.613296031952, 'accumulated_submission_time': 1587.3673589229584, 'accumulated_eval_time': 166.26765513420105, 'accumulated_logging_time': 3.1034443378448486}
I0307 16:21:51.989235 140460756543232 logging_writer.py:48] [15366] accumulated_eval_time=166.268, accumulated_logging_time=3.10344, accumulated_submission_time=1587.37, global_step=15366, preemption_count=0, score=1587.37, test/loss=0.289043, test/num_examples=3581, test/ssim=0.739262, total_duration=1757.61, train/loss=0.267312, train/ssim=0.745813, validation/loss=0.287567, validation/num_examples=3554, validation/ssim=0.721897
I0307 16:21:54.899598 140460748150528 logging_writer.py:48] [15400] global_step=15400, grad_norm=0.09422703087329865, loss=0.2989628314971924
I0307 16:22:03.199341 140460756543232 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.10414746403694153, loss=0.3360781669616699
I0307 16:22:11.493271 140460748150528 logging_writer.py:48] [15600] global_step=15600, grad_norm=0.08606128394603729, loss=0.347859650850296
I0307 16:22:19.771904 140460756543232 logging_writer.py:48] [15700] global_step=15700, grad_norm=0.12009373307228088, loss=0.3189998269081116
I0307 16:22:28.033313 140460748150528 logging_writer.py:48] [15800] global_step=15800, grad_norm=0.0973488911986351, loss=0.2731768488883972
I0307 16:22:36.339416 140460756543232 logging_writer.py:48] [15900] global_step=15900, grad_norm=0.25840091705322266, loss=0.29787030816078186
I0307 16:22:44.632952 140460748150528 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.10487762838602066, loss=0.23204649984836578
I0307 16:22:52.912813 140460756543232 logging_writer.py:48] [16100] global_step=16100, grad_norm=0.1699897199869156, loss=0.3202435374259949
I0307 16:23:01.194365 140460748150528 logging_writer.py:48] [16200] global_step=16200, grad_norm=0.212453693151474, loss=0.25392046570777893
I0307 16:23:09.468549 140460756543232 logging_writer.py:48] [16300] global_step=16300, grad_norm=0.15724898874759674, loss=0.3560543656349182
I0307 16:23:12.047532 140616853095616 spec.py:321] Evaluating on the training split.
I0307 16:23:13.341884 140616853095616 spec.py:333] Evaluating on the validation split.
I0307 16:23:14.636979 140616853095616 spec.py:349] Evaluating on the test split.
I0307 16:23:15.932875 140616853095616 submission_runner.py:469] Time since start: 1841.57s, 	Step: 16332, 	{'train/ssim': 0.745870862688337, 'train/loss': 0.26725404603140696, 'validation/ssim': 0.7219311097530952, 'validation/loss': 0.28751747247269804, 'validation/num_examples': 3554, 'test/ssim': 0.7392814698015568, 'test/loss': 0.28900516342763893, 'test/num_examples': 3581, 'score': 1667.369958639145, 'total_duration': 1841.5687565803528, 'accumulated_submission_time': 1667.369958639145, 'accumulated_eval_time': 170.15294885635376, 'accumulated_logging_time': 3.1235756874084473}
I0307 16:23:15.945423 140460748150528 logging_writer.py:48] [16332] accumulated_eval_time=170.153, accumulated_logging_time=3.12358, accumulated_submission_time=1667.37, global_step=16332, preemption_count=0, score=1667.37, test/loss=0.289005, test/num_examples=3581, test/ssim=0.739281, total_duration=1841.57, train/loss=0.267254, train/ssim=0.745871, validation/loss=0.287517, validation/num_examples=3554, validation/ssim=0.721931
I0307 16:23:21.655915 140460756543232 logging_writer.py:48] [16400] global_step=16400, grad_norm=0.10108380019664764, loss=0.3498948812484741
I0307 16:23:29.942334 140460748150528 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.19063329696655273, loss=0.23747791349887848
I0307 16:23:38.235762 140460756543232 logging_writer.py:48] [16600] global_step=16600, grad_norm=0.08976755291223526, loss=0.3086709678173065
I0307 16:23:46.550563 140460748150528 logging_writer.py:48] [16700] global_step=16700, grad_norm=0.22099697589874268, loss=0.23466619849205017
I0307 16:23:54.818133 140460756543232 logging_writer.py:48] [16800] global_step=16800, grad_norm=0.16361305117607117, loss=0.2905750274658203
I0307 16:24:03.104619 140460748150528 logging_writer.py:48] [16900] global_step=16900, grad_norm=0.11206155270338058, loss=0.2739627957344055
I0307 16:24:11.415105 140460756543232 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.1325666606426239, loss=0.23516219854354858
I0307 16:24:19.703088 140460748150528 logging_writer.py:48] [17100] global_step=17100, grad_norm=0.19956016540527344, loss=0.264826238155365
I0307 16:24:27.978062 140460756543232 logging_writer.py:48] [17200] global_step=17200, grad_norm=0.2626579701900482, loss=0.2339399755001068
I0307 16:24:35.994495 140616853095616 spec.py:321] Evaluating on the training split.
I0307 16:24:37.286697 140616853095616 spec.py:333] Evaluating on the validation split.
I0307 16:24:38.582011 140616853095616 spec.py:349] Evaluating on the test split.
I0307 16:24:39.877483 140616853095616 submission_runner.py:469] Time since start: 1925.51s, 	Step: 17298, 	{'train/ssim': 0.7457662309919085, 'train/loss': 0.26716082436697824, 'validation/ssim': 0.7217512672780669, 'validation/loss': 0.2875110838744636, 'validation/num_examples': 3554, 'test/ssim': 0.7391123235042586, 'test/loss': 0.2890060838125698, 'test/num_examples': 3581, 'score': 1747.363486289978, 'total_duration': 1925.513364315033, 'accumulated_submission_time': 1747.363486289978, 'accumulated_eval_time': 174.035888671875, 'accumulated_logging_time': 3.1440863609313965}
I0307 16:24:39.889169 140460748150528 logging_writer.py:48] [17298] accumulated_eval_time=174.036, accumulated_logging_time=3.14409, accumulated_submission_time=1747.36, global_step=17298, preemption_count=0, score=1747.36, test/loss=0.289006, test/num_examples=3581, test/ssim=0.739112, total_duration=1925.51, train/loss=0.267161, train/ssim=0.745766, validation/loss=0.287511, validation/num_examples=3554, validation/ssim=0.721751
I0307 16:24:40.151028 140460756543232 logging_writer.py:48] [17300] global_step=17300, grad_norm=0.1032489463686943, loss=0.28881606459617615
I0307 16:24:48.426162 140460748150528 logging_writer.py:48] [17400] global_step=17400, grad_norm=0.1685456484556198, loss=0.2640332579612732
I0307 16:24:56.715233 140460756543232 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.08811049163341522, loss=0.35966891050338745
I0307 16:25:05.012884 140460748150528 logging_writer.py:48] [17600] global_step=17600, grad_norm=0.17897608876228333, loss=0.28761979937553406
I0307 16:25:13.319591 140460756543232 logging_writer.py:48] [17700] global_step=17700, grad_norm=0.12095323950052261, loss=0.3438768982887268
I0307 16:25:21.610347 140460748150528 logging_writer.py:48] [17800] global_step=17800, grad_norm=0.15318067371845245, loss=0.2330874800682068
I0307 16:25:29.877219 140460756543232 logging_writer.py:48] [17900] global_step=17900, grad_norm=0.09881418943405151, loss=0.30359598994255066
I0307 16:25:38.183541 140460748150528 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.14694443345069885, loss=0.29935747385025024
I0307 16:25:46.473199 140460756543232 logging_writer.py:48] [18100] global_step=18100, grad_norm=0.14937536418437958, loss=0.2308170348405838
I0307 16:25:54.767721 140460748150528 logging_writer.py:48] [18200] global_step=18200, grad_norm=0.1265169084072113, loss=0.2915802597999573
I0307 16:25:59.904281 140616853095616 spec.py:321] Evaluating on the training split.
I0307 16:26:01.196070 140616853095616 spec.py:333] Evaluating on the validation split.
I0307 16:26:02.493725 140616853095616 spec.py:349] Evaluating on the test split.
I0307 16:26:03.791897 140616853095616 submission_runner.py:469] Time since start: 2009.43s, 	Step: 18263, 	{'train/ssim': 0.7462079184395927, 'train/loss': 0.26698907784053255, 'validation/ssim': 0.722294229433385, 'validation/loss': 0.28728324104442177, 'validation/num_examples': 3554, 'test/ssim': 0.7396811895682072, 'test/loss': 0.288767329143832, 'test/num_examples': 3581, 'score': 1827.3210127353668, 'total_duration': 2009.4277806282043, 'accumulated_submission_time': 1827.3210127353668, 'accumulated_eval_time': 177.92346382141113, 'accumulated_logging_time': 3.164139986038208}
I0307 16:26:03.804046 140460756543232 logging_writer.py:48] [18263] accumulated_eval_time=177.923, accumulated_logging_time=3.16414, accumulated_submission_time=1827.32, global_step=18263, preemption_count=0, score=1827.32, test/loss=0.288767, test/num_examples=3581, test/ssim=0.739681, total_duration=2009.43, train/loss=0.266989, train/ssim=0.746208, validation/loss=0.287283, validation/num_examples=3554, validation/ssim=0.722294
I0307 16:26:06.969210 140460748150528 logging_writer.py:48] [18300] global_step=18300, grad_norm=0.12888026237487793, loss=0.32429713010787964
I0307 16:26:15.251400 140460756543232 logging_writer.py:48] [18400] global_step=18400, grad_norm=0.25595521926879883, loss=0.26376986503601074
I0307 16:26:23.535411 140460748150528 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.09727255254983902, loss=0.20635062456130981
I0307 16:26:31.842697 140460756543232 logging_writer.py:48] [18600] global_step=18600, grad_norm=0.13476306200027466, loss=0.26453810930252075
I0307 16:26:40.137456 140460748150528 logging_writer.py:48] [18700] global_step=18700, grad_norm=0.09771448373794556, loss=0.3457157015800476
I0307 16:26:48.437063 140460756543232 logging_writer.py:48] [18800] global_step=18800, grad_norm=0.07951771467924118, loss=0.3473629653453827
I0307 16:26:56.738930 140460748150528 logging_writer.py:48] [18900] global_step=18900, grad_norm=0.09023511409759521, loss=0.2365087866783142
I0307 16:27:05.023951 140460756543232 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.06390875577926636, loss=0.32191944122314453
I0307 16:27:13.324605 140460748150528 logging_writer.py:48] [19100] global_step=19100, grad_norm=0.17874668538570404, loss=0.3154800534248352
I0307 16:27:21.592206 140460756543232 logging_writer.py:48] [19200] global_step=19200, grad_norm=0.13167521357536316, loss=0.23088613152503967
I0307 16:27:23.832822 140616853095616 spec.py:321] Evaluating on the training split.
I0307 16:27:25.123492 140616853095616 spec.py:333] Evaluating on the validation split.
I0307 16:27:26.417330 140616853095616 spec.py:349] Evaluating on the test split.
I0307 16:27:27.713903 140616853095616 submission_runner.py:469] Time since start: 2093.35s, 	Step: 19228, 	{'train/ssim': 0.7462079184395927, 'train/loss': 0.26698907784053255, 'validation/ssim': 0.722294229433385, 'validation/loss': 0.28728324104442177, 'validation/num_examples': 3554, 'test/ssim': 0.7396811895682072, 'test/loss': 0.288767329143832, 'test/num_examples': 3581, 'score': 1907.2931859493256, 'total_duration': 2093.3497874736786, 'accumulated_submission_time': 1907.2931859493256, 'accumulated_eval_time': 181.8044993877411, 'accumulated_logging_time': 3.18454647064209}
I0307 16:27:27.726192 140460748150528 logging_writer.py:48] [19228] accumulated_eval_time=181.804, accumulated_logging_time=3.18455, accumulated_submission_time=1907.29, global_step=19228, preemption_count=0, score=1907.29, test/loss=0.288767, test/num_examples=3581, test/ssim=0.739681, total_duration=2093.35, train/loss=0.266989, train/ssim=0.746208, validation/loss=0.287283, validation/num_examples=3554, validation/ssim=0.722294
I0307 16:27:33.801311 140460756543232 logging_writer.py:48] [19300] global_step=19300, grad_norm=0.11310166120529175, loss=0.3166458010673523
I0307 16:27:42.093443 140460748150528 logging_writer.py:48] [19400] global_step=19400, grad_norm=0.1434025764465332, loss=0.2948932647705078
I0307 16:27:50.408166 140460756543232 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.06650938838720322, loss=0.22667738795280457
I0307 16:27:58.679198 140460748150528 logging_writer.py:48] [19600] global_step=19600, grad_norm=0.1335889995098114, loss=0.2610708475112915
I0307 16:28:06.968545 140460756543232 logging_writer.py:48] [19700] global_step=19700, grad_norm=0.06667359173297882, loss=0.2978106737136841
I0307 16:28:15.248671 140460748150528 logging_writer.py:48] [19800] global_step=19800, grad_norm=0.06768810003995895, loss=0.3049072325229645
I0307 16:28:23.547331 140460756543232 logging_writer.py:48] [19900] global_step=19900, grad_norm=0.1106979250907898, loss=0.27813953161239624
I0307 16:28:31.822881 140460748150528 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.06232883036136627, loss=0.4240638017654419
I0307 16:28:40.102303 140460756543232 logging_writer.py:48] [20100] global_step=20100, grad_norm=0.0874996930360794, loss=0.2900107204914093
I0307 16:28:47.717987 140616853095616 spec.py:321] Evaluating on the training split.
I0307 16:28:49.011592 140616853095616 spec.py:333] Evaluating on the validation split.
I0307 16:28:50.308927 140616853095616 spec.py:349] Evaluating on the test split.
I0307 16:28:51.604354 140616853095616 submission_runner.py:469] Time since start: 2177.24s, 	Step: 20193, 	{'train/ssim': 0.7462079184395927, 'train/loss': 0.26698907784053255, 'validation/ssim': 0.722294229433385, 'validation/loss': 0.28728324104442177, 'validation/num_examples': 3554, 'test/ssim': 0.7396811895682072, 'test/loss': 0.288767329143832, 'test/num_examples': 3581, 'score': 1987.2285618782043, 'total_duration': 2177.240236520767, 'accumulated_submission_time': 1987.2285618782043, 'accumulated_eval_time': 185.69081735610962, 'accumulated_logging_time': 3.2041587829589844}
I0307 16:28:51.617463 140460748150528 logging_writer.py:48] [20193] accumulated_eval_time=185.691, accumulated_logging_time=3.20416, accumulated_submission_time=1987.23, global_step=20193, preemption_count=0, score=1987.23, test/loss=0.288767, test/num_examples=3581, test/ssim=0.739681, total_duration=2177.24, train/loss=0.266989, train/ssim=0.746208, validation/loss=0.287283, validation/num_examples=3554, validation/ssim=0.722294
I0307 16:28:52.299199 140460756543232 logging_writer.py:48] [20200] global_step=20200, grad_norm=0.06031334400177002, loss=0.2986580729484558
I0307 16:29:00.594478 140460748150528 logging_writer.py:48] [20300] global_step=20300, grad_norm=0.13769520819187164, loss=0.3432313799858093
I0307 16:29:08.902716 140460756543232 logging_writer.py:48] [20400] global_step=20400, grad_norm=0.24361088871955872, loss=0.2542543411254883
I0307 16:29:17.191719 140460748150528 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.07324611395597458, loss=0.3118550181388855
I0307 16:29:25.458691 140460756543232 logging_writer.py:48] [20600] global_step=20600, grad_norm=0.14865323901176453, loss=0.2712613344192505
I0307 16:29:33.727061 140460748150528 logging_writer.py:48] [20700] global_step=20700, grad_norm=0.10713738203048706, loss=0.2595732808113098
I0307 16:29:42.001525 140460756543232 logging_writer.py:48] [20800] global_step=20800, grad_norm=0.11267159879207611, loss=0.29058802127838135
I0307 16:29:50.244482 140460748150528 logging_writer.py:48] [20900] global_step=20900, grad_norm=0.20721712708473206, loss=0.29306936264038086
I0307 16:29:58.540612 140460756543232 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.09830182045698166, loss=0.35374370217323303
I0307 16:30:06.847706 140460748150528 logging_writer.py:48] [21100] global_step=21100, grad_norm=0.20397986471652985, loss=0.33475935459136963
I0307 16:30:11.644942 140616853095616 spec.py:321] Evaluating on the training split.
I0307 16:30:12.936685 140616853095616 spec.py:333] Evaluating on the validation split.
I0307 16:30:14.232561 140616853095616 spec.py:349] Evaluating on the test split.
I0307 16:30:15.527946 140616853095616 submission_runner.py:469] Time since start: 2261.16s, 	Step: 21159, 	{'train/ssim': 0.7462079184395927, 'train/loss': 0.26698907784053255, 'validation/ssim': 0.722294229433385, 'validation/loss': 0.28728324104442177, 'validation/num_examples': 3554, 'test/ssim': 0.7396811895682072, 'test/loss': 0.288767329143832, 'test/num_examples': 3581, 'score': 2067.2004284858704, 'total_duration': 2261.1638147830963, 'accumulated_submission_time': 2067.2004284858704, 'accumulated_eval_time': 189.57376384735107, 'accumulated_logging_time': 3.2246878147125244}
I0307 16:30:15.543870 140460756543232 logging_writer.py:48] [21159] accumulated_eval_time=189.574, accumulated_logging_time=3.22469, accumulated_submission_time=2067.2, global_step=21159, preemption_count=0, score=2067.2, test/loss=0.288767, test/num_examples=3581, test/ssim=0.739681, total_duration=2261.16, train/loss=0.266989, train/ssim=0.746208, validation/loss=0.287283, validation/num_examples=3554, validation/ssim=0.722294
I0307 16:30:19.038520 140460748150528 logging_writer.py:48] [21200] global_step=21200, grad_norm=0.13953325152397156, loss=0.346481591463089
I0307 16:30:27.327543 140460756543232 logging_writer.py:48] [21300] global_step=21300, grad_norm=0.046693991869688034, loss=0.3191937804222107
I0307 16:30:35.583588 140460748150528 logging_writer.py:48] [21400] global_step=21400, grad_norm=0.0923161506652832, loss=0.2814176678657532
I0307 16:30:43.873098 140460756543232 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.08026425540447235, loss=0.29413923621177673
I0307 16:30:52.149359 140460748150528 logging_writer.py:48] [21600] global_step=21600, grad_norm=0.11541782319545746, loss=0.3683062493801117
I0307 16:31:00.415836 140460756543232 logging_writer.py:48] [21700] global_step=21700, grad_norm=0.09164462238550186, loss=0.24679026007652283
I0307 16:31:08.700726 140460748150528 logging_writer.py:48] [21800] global_step=21800, grad_norm=0.11440204828977585, loss=0.3207103908061981
I0307 16:31:16.976390 140460756543232 logging_writer.py:48] [21900] global_step=21900, grad_norm=0.10799951106309891, loss=0.22863167524337769
I0307 16:31:25.290513 140460748150528 logging_writer.py:48] [22000] global_step=22000, grad_norm=0.0762038305401802, loss=0.26224440336227417
I0307 16:31:33.617569 140460756543232 logging_writer.py:48] [22100] global_step=22100, grad_norm=0.1611170768737793, loss=0.3020767569541931
I0307 16:31:35.534796 140616853095616 spec.py:321] Evaluating on the training split.
I0307 16:31:36.827945 140616853095616 spec.py:333] Evaluating on the validation split.
I0307 16:31:38.124838 140616853095616 spec.py:349] Evaluating on the test split.
I0307 16:31:39.423374 140616853095616 submission_runner.py:469] Time since start: 2345.06s, 	Step: 22124, 	{'train/ssim': 0.7462079184395927, 'train/loss': 0.26698907784053255, 'validation/ssim': 0.722294229433385, 'validation/loss': 0.28728324104442177, 'validation/num_examples': 3554, 'test/ssim': 0.7396811895682072, 'test/loss': 0.288767329143832, 'test/num_examples': 3581, 'score': 2147.1358704566956, 'total_duration': 2345.0592563152313, 'accumulated_submission_time': 2147.1358704566956, 'accumulated_eval_time': 193.4622938632965, 'accumulated_logging_time': 3.2485551834106445}
I0307 16:31:39.435968 140460748150528 logging_writer.py:48] [22124] accumulated_eval_time=193.462, accumulated_logging_time=3.24856, accumulated_submission_time=2147.14, global_step=22124, preemption_count=0, score=2147.14, test/loss=0.288767, test/num_examples=3581, test/ssim=0.739681, total_duration=2345.06, train/loss=0.266989, train/ssim=0.746208, validation/loss=0.287283, validation/num_examples=3554, validation/ssim=0.722294
I0307 16:31:45.824246 140460756543232 logging_writer.py:48] [22200] global_step=22200, grad_norm=0.19127701222896576, loss=0.2517086863517761
I0307 16:31:54.112264 140460748150528 logging_writer.py:48] [22300] global_step=22300, grad_norm=0.24019630253314972, loss=0.28835204243659973
I0307 16:32:02.416998 140460756543232 logging_writer.py:48] [22400] global_step=22400, grad_norm=0.2795540690422058, loss=0.31916189193725586
I0307 16:32:10.689029 140460748150528 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.11190111935138702, loss=0.26420286297798157
I0307 16:32:19.006841 140460756543232 logging_writer.py:48] [22600] global_step=22600, grad_norm=0.1566435992717743, loss=0.2538725733757019
I0307 16:32:27.289480 140460748150528 logging_writer.py:48] [22700] global_step=22700, grad_norm=0.27834194898605347, loss=0.29515212774276733
I0307 16:32:35.558619 140460756543232 logging_writer.py:48] [22800] global_step=22800, grad_norm=0.1311415284872055, loss=0.3592526316642761
I0307 16:32:43.812000 140460748150528 logging_writer.py:48] [22900] global_step=22900, grad_norm=0.13041801750659943, loss=0.2720945477485657
I0307 16:32:52.070514 140460756543232 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.21336717903614044, loss=0.24353422224521637
I0307 16:32:59.432130 140616853095616 spec.py:321] Evaluating on the training split.
I0307 16:33:00.725147 140616853095616 spec.py:333] Evaluating on the validation split.
I0307 16:33:02.021568 140616853095616 spec.py:349] Evaluating on the test split.
I0307 16:33:03.317955 140616853095616 submission_runner.py:469] Time since start: 2428.95s, 	Step: 23090, 	{'train/ssim': 0.7462079184395927, 'train/loss': 0.26698907784053255, 'validation/ssim': 0.722294229433385, 'validation/loss': 0.28728324104442177, 'validation/num_examples': 3554, 'test/ssim': 0.7396811895682072, 'test/loss': 0.288767329143832, 'test/num_examples': 3581, 'score': 2227.076051235199, 'total_duration': 2428.953841686249, 'accumulated_submission_time': 2227.076051235199, 'accumulated_eval_time': 197.34807348251343, 'accumulated_logging_time': 3.268991231918335}
I0307 16:33:03.330507 140460748150528 logging_writer.py:48] [23090] accumulated_eval_time=197.348, accumulated_logging_time=3.26899, accumulated_submission_time=2227.08, global_step=23090, preemption_count=0, score=2227.08, test/loss=0.288767, test/num_examples=3581, test/ssim=0.739681, total_duration=2428.95, train/loss=0.266989, train/ssim=0.746208, validation/loss=0.287283, validation/num_examples=3554, validation/ssim=0.722294
I0307 16:33:04.252211 140460756543232 logging_writer.py:48] [23100] global_step=23100, grad_norm=0.08878064900636673, loss=0.27947142720222473
I0307 16:33:12.542639 140460748150528 logging_writer.py:48] [23200] global_step=23200, grad_norm=0.09509606659412384, loss=0.28518131375312805
I0307 16:33:20.830818 140460756543232 logging_writer.py:48] [23300] global_step=23300, grad_norm=0.1211928278207779, loss=0.29686054587364197
I0307 16:33:29.164433 140460748150528 logging_writer.py:48] [23400] global_step=23400, grad_norm=0.12934409081935883, loss=0.3080942630767822
I0307 16:33:37.433521 140460756543232 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.10097689926624298, loss=0.23765504360198975
I0307 16:33:45.690642 140460748150528 logging_writer.py:48] [23600] global_step=23600, grad_norm=0.16461794078350067, loss=0.24519425630569458
I0307 16:33:53.989415 140460756543232 logging_writer.py:48] [23700] global_step=23700, grad_norm=0.2221037894487381, loss=0.26538917422294617
I0307 16:34:02.277389 140460748150528 logging_writer.py:48] [23800] global_step=23800, grad_norm=0.09033186733722687, loss=0.2820882499217987
I0307 16:34:10.575158 140460756543232 logging_writer.py:48] [23900] global_step=23900, grad_norm=0.10637076944112778, loss=0.2553970217704773
I0307 16:34:18.847781 140460748150528 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.09161572903394699, loss=0.33073970675468445
I0307 16:34:23.321625 140616853095616 spec.py:321] Evaluating on the training split.
I0307 16:34:24.613227 140616853095616 spec.py:333] Evaluating on the validation split.
I0307 16:34:25.911649 140616853095616 spec.py:349] Evaluating on the test split.
I0307 16:34:27.207695 140616853095616 submission_runner.py:469] Time since start: 2512.84s, 	Step: 24055, 	{'train/ssim': 0.7462079184395927, 'train/loss': 0.26698907784053255, 'validation/ssim': 0.722294229433385, 'validation/loss': 0.28728324104442177, 'validation/num_examples': 3554, 'test/ssim': 0.7396811895682072, 'test/loss': 0.288767329143832, 'test/num_examples': 3581, 'score': 2307.010026693344, 'total_duration': 2512.843559026718, 'accumulated_submission_time': 2307.010026693344, 'accumulated_eval_time': 201.23407864570618, 'accumulated_logging_time': 3.2897231578826904}
I0307 16:34:27.224487 140460756543232 logging_writer.py:48] [24055] accumulated_eval_time=201.234, accumulated_logging_time=3.28972, accumulated_submission_time=2307.01, global_step=24055, preemption_count=0, score=2307.01, test/loss=0.288767, test/num_examples=3581, test/ssim=0.739681, total_duration=2512.84, train/loss=0.266989, train/ssim=0.746208, validation/loss=0.287283, validation/num_examples=3554, validation/ssim=0.722294
I0307 16:34:31.047234 140460748150528 logging_writer.py:48] [24100] global_step=24100, grad_norm=0.10180425643920898, loss=0.19987884163856506
I0307 16:34:39.333061 140460756543232 logging_writer.py:48] [24200] global_step=24200, grad_norm=0.13211177289485931, loss=0.23713795840740204
I0307 16:34:47.619508 140460748150528 logging_writer.py:48] [24300] global_step=24300, grad_norm=0.1797223687171936, loss=0.26493459939956665
I0307 16:34:55.925427 140460756543232 logging_writer.py:48] [24400] global_step=24400, grad_norm=0.19838030636310577, loss=0.26799458265304565
I0307 16:35:04.229570 140460748150528 logging_writer.py:48] [24500] global_step=24500, grad_norm=0.10653746873140335, loss=0.34428972005844116
I0307 16:35:12.497329 140460756543232 logging_writer.py:48] [24600] global_step=24600, grad_norm=0.13470569252967834, loss=0.3726814389228821
I0307 16:35:20.790890 140460748150528 logging_writer.py:48] [24700] global_step=24700, grad_norm=0.07097690552473068, loss=0.2550552487373352
I0307 16:35:29.069504 140460756543232 logging_writer.py:48] [24800] global_step=24800, grad_norm=0.15196247398853302, loss=0.34058624505996704
I0307 16:35:37.336903 140460748150528 logging_writer.py:48] [24900] global_step=24900, grad_norm=0.1965661197900772, loss=0.28889164328575134
I0307 16:35:45.609035 140460756543232 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.07554923743009567, loss=0.2432086169719696
I0307 16:35:47.265416 140616853095616 spec.py:321] Evaluating on the training split.
I0307 16:35:48.556003 140616853095616 spec.py:333] Evaluating on the validation split.
I0307 16:35:49.851133 140616853095616 spec.py:349] Evaluating on the test split.
I0307 16:35:51.144743 140616853095616 submission_runner.py:469] Time since start: 2596.78s, 	Step: 25021, 	{'train/ssim': 0.7462079184395927, 'train/loss': 0.26698907784053255, 'validation/ssim': 0.722294229433385, 'validation/loss': 0.28728324104442177, 'validation/num_examples': 3554, 'test/ssim': 0.7396811895682072, 'test/loss': 0.288767329143832, 'test/num_examples': 3581, 'score': 2386.9936690330505, 'total_duration': 2596.7806215286255, 'accumulated_submission_time': 2386.9936690330505, 'accumulated_eval_time': 205.1133553981781, 'accumulated_logging_time': 3.3141512870788574}
I0307 16:35:51.158689 140460748150528 logging_writer.py:48] [25021] accumulated_eval_time=205.113, accumulated_logging_time=3.31415, accumulated_submission_time=2386.99, global_step=25021, preemption_count=0, score=2386.99, test/loss=0.288767, test/num_examples=3581, test/ssim=0.739681, total_duration=2596.78, train/loss=0.266989, train/ssim=0.746208, validation/loss=0.287283, validation/num_examples=3554, validation/ssim=0.722294
I0307 16:35:57.794475 140460756543232 logging_writer.py:48] [25100] global_step=25100, grad_norm=0.11945407092571259, loss=0.3273618519306183
I0307 16:36:06.091157 140460748150528 logging_writer.py:48] [25200] global_step=25200, grad_norm=0.17461243271827698, loss=0.24009041488170624
I0307 16:36:14.403386 140460756543232 logging_writer.py:48] [25300] global_step=25300, grad_norm=0.13299506902694702, loss=0.25687357783317566
I0307 16:36:22.684887 140460748150528 logging_writer.py:48] [25400] global_step=25400, grad_norm=0.08500256389379501, loss=0.24236345291137695
I0307 16:36:30.942221 140460756543232 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.13074342906475067, loss=0.3525317311286926
I0307 16:36:39.219251 140460748150528 logging_writer.py:48] [25600] global_step=25600, grad_norm=0.07614459842443466, loss=0.2588787376880646
I0307 16:36:47.506429 140460756543232 logging_writer.py:48] [25700] global_step=25700, grad_norm=0.09781895577907562, loss=0.20266377925872803
I0307 16:36:55.802498 140460748150528 logging_writer.py:48] [25800] global_step=25800, grad_norm=0.06777399033308029, loss=0.2757490873336792
I0307 16:37:04.075260 140460756543232 logging_writer.py:48] [25900] global_step=25900, grad_norm=0.09126563370227814, loss=0.2971329689025879
I0307 16:37:11.207649 140616853095616 spec.py:321] Evaluating on the training split.
I0307 16:37:12.501261 140616853095616 spec.py:333] Evaluating on the validation split.
I0307 16:37:13.797699 140616853095616 spec.py:349] Evaluating on the test split.
I0307 16:37:15.095647 140616853095616 submission_runner.py:469] Time since start: 2680.73s, 	Step: 25987, 	{'train/ssim': 0.7462079184395927, 'train/loss': 0.26698907784053255, 'validation/ssim': 0.722294229433385, 'validation/loss': 0.28728324104442177, 'validation/num_examples': 3554, 'test/ssim': 0.7396811895682072, 'test/loss': 0.288767329143832, 'test/num_examples': 3581, 'score': 2466.986483812332, 'total_duration': 2680.7314970493317, 'accumulated_submission_time': 2466.986483812332, 'accumulated_eval_time': 209.00127363204956, 'accumulated_logging_time': 3.33595609664917}
I0307 16:37:15.110091 140460748150528 logging_writer.py:48] [25987] accumulated_eval_time=209.001, accumulated_logging_time=3.33596, accumulated_submission_time=2466.99, global_step=25987, preemption_count=0, score=2466.99, test/loss=0.288767, test/num_examples=3581, test/ssim=0.739681, total_duration=2680.73, train/loss=0.266989, train/ssim=0.746208, validation/loss=0.287283, validation/num_examples=3554, validation/ssim=0.722294
I0307 16:37:16.282610 140460756543232 logging_writer.py:48] [26000] global_step=26000, grad_norm=0.18454669415950775, loss=0.2415195107460022
I0307 16:37:24.545196 140460748150528 logging_writer.py:48] [26100] global_step=26100, grad_norm=0.18261925876140594, loss=0.19652633368968964
I0307 16:37:32.803472 140460756543232 logging_writer.py:48] [26200] global_step=26200, grad_norm=0.13931243121623993, loss=0.3547546863555908
I0307 16:37:41.086874 140460748150528 logging_writer.py:48] [26300] global_step=26300, grad_norm=0.10823000222444534, loss=0.28583577275276184
I0307 16:37:49.358478 140460756543232 logging_writer.py:48] [26400] global_step=26400, grad_norm=0.07612968981266022, loss=0.31097277998924255
I0307 16:37:57.624532 140460748150528 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.1974073201417923, loss=0.28564634919166565
I0307 16:38:05.887900 140460756543232 logging_writer.py:48] [26600] global_step=26600, grad_norm=0.18160568177700043, loss=0.28104668855667114
I0307 16:38:14.127202 140460748150528 logging_writer.py:48] [26700] global_step=26700, grad_norm=0.06548070907592773, loss=0.28555843234062195
I0307 16:38:22.397842 140460756543232 logging_writer.py:48] [26800] global_step=26800, grad_norm=0.10458793491125107, loss=0.23779740929603577
I0307 16:38:30.687377 140460748150528 logging_writer.py:48] [26900] global_step=26900, grad_norm=0.1655491590499878, loss=0.2929360866546631
I0307 16:38:35.154628 140616853095616 spec.py:321] Evaluating on the training split.
I0307 16:38:36.444164 140616853095616 spec.py:333] Evaluating on the validation split.
I0307 16:38:37.738418 140616853095616 spec.py:349] Evaluating on the test split.
I0307 16:38:39.035017 140616853095616 submission_runner.py:469] Time since start: 2764.67s, 	Step: 26955, 	{'train/ssim': 0.7462079184395927, 'train/loss': 0.26698907784053255, 'validation/ssim': 0.722294229433385, 'validation/loss': 0.28728324104442177, 'validation/num_examples': 3554, 'test/ssim': 0.7396811895682072, 'test/loss': 0.288767329143832, 'test/num_examples': 3581, 'score': 2546.973133325577, 'total_duration': 2764.670900583267, 'accumulated_submission_time': 2546.973133325577, 'accumulated_eval_time': 212.88161492347717, 'accumulated_logging_time': 3.3591275215148926}
I0307 16:38:39.048521 140460756543232 logging_writer.py:48] [26955] accumulated_eval_time=212.882, accumulated_logging_time=3.35913, accumulated_submission_time=2546.97, global_step=26955, preemption_count=0, score=2546.97, test/loss=0.288767, test/num_examples=3581, test/ssim=0.739681, total_duration=2764.67, train/loss=0.266989, train/ssim=0.746208, validation/loss=0.287283, validation/num_examples=3554, validation/ssim=0.722294
I0307 16:38:42.875925 140460748150528 logging_writer.py:48] [27000] global_step=27000, grad_norm=0.12549296021461487, loss=0.2965640127658844
I0307 16:38:51.136856 140460756543232 logging_writer.py:48] [27100] global_step=27100, grad_norm=0.06680449098348618, loss=0.24522417783737183
I0307 16:38:59.371940 140460748150528 logging_writer.py:48] [27200] global_step=27200, grad_norm=0.11281619966030121, loss=0.23912501335144043
I0307 16:39:07.646847 140460756543232 logging_writer.py:48] [27300] global_step=27300, grad_norm=0.06812998652458191, loss=0.2530297636985779
I0307 16:39:15.933547 140460748150528 logging_writer.py:48] [27400] global_step=27400, grad_norm=0.11403483897447586, loss=0.3002597391605377
I0307 16:39:24.231927 140460756543232 logging_writer.py:48] [27500] global_step=27500, grad_norm=0.12047036737203598, loss=0.2620948553085327
I0307 16:39:32.504610 140460748150528 logging_writer.py:48] [27600] global_step=27600, grad_norm=0.29534339904785156, loss=0.2604721188545227
I0307 16:39:40.784114 140460756543232 logging_writer.py:48] [27700] global_step=27700, grad_norm=0.06162930652499199, loss=0.3201095163822174
I0307 16:39:49.083715 140460748150528 logging_writer.py:48] [27800] global_step=27800, grad_norm=0.1859593391418457, loss=0.35566088557243347
I0307 16:39:57.362561 140460756543232 logging_writer.py:48] [27900] global_step=27900, grad_norm=0.1642586588859558, loss=0.24934178590774536
I0307 16:39:59.106870 140616853095616 spec.py:321] Evaluating on the training split.
I0307 16:40:00.399104 140616853095616 spec.py:333] Evaluating on the validation split.
I0307 16:40:01.697685 140616853095616 spec.py:349] Evaluating on the test split.
I0307 16:40:02.993853 140616853095616 submission_runner.py:469] Time since start: 2848.63s, 	Step: 27922, 	{'train/ssim': 0.7462079184395927, 'train/loss': 0.26698907784053255, 'validation/ssim': 0.722294229433385, 'validation/loss': 0.28728324104442177, 'validation/num_examples': 3554, 'test/ssim': 0.7396811895682072, 'test/loss': 0.288767329143832, 'test/num_examples': 3581, 'score': 2626.9760432243347, 'total_duration': 2848.629737138748, 'accumulated_submission_time': 2626.9760432243347, 'accumulated_eval_time': 216.7685523033142, 'accumulated_logging_time': 3.3802478313446045}
I0307 16:40:03.007252 140460748150528 logging_writer.py:48] [27922] accumulated_eval_time=216.769, accumulated_logging_time=3.38025, accumulated_submission_time=2626.98, global_step=27922, preemption_count=0, score=2626.98, test/loss=0.288767, test/num_examples=3581, test/ssim=0.739681, total_duration=2848.63, train/loss=0.266989, train/ssim=0.746208, validation/loss=0.287283, validation/num_examples=3554, validation/ssim=0.722294
I0307 16:40:09.570506 140460756543232 logging_writer.py:48] [28000] global_step=28000, grad_norm=0.09610875695943832, loss=0.28156208992004395
I0307 16:40:17.854734 140460748150528 logging_writer.py:48] [28100] global_step=28100, grad_norm=0.10287124663591385, loss=0.2845562994480133
I0307 16:40:26.162557 140460756543232 logging_writer.py:48] [28200] global_step=28200, grad_norm=0.12422340363264084, loss=0.21617335081100464
I0307 16:40:34.455290 140460748150528 logging_writer.py:48] [28300] global_step=28300, grad_norm=0.15764111280441284, loss=0.28115060925483704
I0307 16:40:42.723438 140460756543232 logging_writer.py:48] [28400] global_step=28400, grad_norm=0.24191342294216156, loss=0.24255047738552094
I0307 16:40:51.043564 140460748150528 logging_writer.py:48] [28500] global_step=28500, grad_norm=0.0629413053393364, loss=0.30541956424713135
I0307 16:40:59.309407 140460756543232 logging_writer.py:48] [28600] global_step=28600, grad_norm=0.16302087903022766, loss=0.31545352935791016
I0307 16:41:07.585718 140460748150528 logging_writer.py:48] [28700] global_step=28700, grad_norm=0.2673564553260803, loss=0.2935568690299988
I0307 16:41:15.863539 140460756543232 logging_writer.py:48] [28800] global_step=28800, grad_norm=0.1004304587841034, loss=0.29325127601623535
I0307 16:41:23.007428 140616853095616 spec.py:321] Evaluating on the training split.
I0307 16:41:24.301308 140616853095616 spec.py:333] Evaluating on the validation split.
I0307 16:41:25.597028 140616853095616 spec.py:349] Evaluating on the test split.
I0307 16:41:26.890851 140616853095616 submission_runner.py:469] Time since start: 2932.53s, 	Step: 28887, 	{'train/ssim': 0.7462079184395927, 'train/loss': 0.26698907784053255, 'validation/ssim': 0.722294229433385, 'validation/loss': 0.28728324104442177, 'validation/num_examples': 3554, 'test/ssim': 0.7396811895682072, 'test/loss': 0.288767329143832, 'test/num_examples': 3581, 'score': 2706.9189536571503, 'total_duration': 2932.526736021042, 'accumulated_submission_time': 2706.9189536571503, 'accumulated_eval_time': 220.65193033218384, 'accumulated_logging_time': 3.401975393295288}
I0307 16:41:26.903886 140460748150528 logging_writer.py:48] [28887] accumulated_eval_time=220.652, accumulated_logging_time=3.40198, accumulated_submission_time=2706.92, global_step=28887, preemption_count=0, score=2706.92, test/loss=0.288767, test/num_examples=3581, test/ssim=0.739681, total_duration=2932.53, train/loss=0.266989, train/ssim=0.746208, validation/loss=0.287283, validation/num_examples=3554, validation/ssim=0.722294
I0307 16:41:28.076788 140460756543232 logging_writer.py:48] [28900] global_step=28900, grad_norm=0.06589579582214355, loss=0.3082602024078369
I0307 16:41:36.354986 140460748150528 logging_writer.py:48] [29000] global_step=29000, grad_norm=0.10493779182434082, loss=0.24830394983291626
I0307 16:41:44.627739 140460756543232 logging_writer.py:48] [29100] global_step=29100, grad_norm=0.05208052694797516, loss=0.35124140977859497
I0307 16:41:52.891630 140460748150528 logging_writer.py:48] [29200] global_step=29200, grad_norm=0.0897359549999237, loss=0.3168773055076599
I0307 16:42:01.158357 140460756543232 logging_writer.py:48] [29300] global_step=29300, grad_norm=0.07586906850337982, loss=0.27833065390586853
I0307 16:42:09.423261 140460748150528 logging_writer.py:48] [29400] global_step=29400, grad_norm=0.2832854688167572, loss=0.2489573359489441
I0307 16:42:17.704408 140460756543232 logging_writer.py:48] [29500] global_step=29500, grad_norm=0.07181746512651443, loss=0.322231650352478
I0307 16:42:25.992591 140460748150528 logging_writer.py:48] [29600] global_step=29600, grad_norm=0.14601194858551025, loss=0.31971582770347595
I0307 16:42:34.277340 140460756543232 logging_writer.py:48] [29700] global_step=29700, grad_norm=0.1274072825908661, loss=0.22793623805046082
I0307 16:42:42.545744 140460748150528 logging_writer.py:48] [29800] global_step=29800, grad_norm=0.1830022931098938, loss=0.23254525661468506
I0307 16:42:46.937800 140616853095616 spec.py:321] Evaluating on the training split.
I0307 16:42:48.228385 140616853095616 spec.py:333] Evaluating on the validation split.
I0307 16:42:49.523555 140616853095616 spec.py:349] Evaluating on the test split.
I0307 16:42:50.818956 140616853095616 submission_runner.py:469] Time since start: 3016.45s, 	Step: 29854, 	{'train/ssim': 0.7462079184395927, 'train/loss': 0.26698907784053255, 'validation/ssim': 0.722294229433385, 'validation/loss': 0.28728324104442177, 'validation/num_examples': 3554, 'test/ssim': 0.7396811895682072, 'test/loss': 0.288767329143832, 'test/num_examples': 3581, 'score': 2786.8974103927612, 'total_duration': 3016.454841375351, 'accumulated_submission_time': 2786.8974103927612, 'accumulated_eval_time': 224.53304052352905, 'accumulated_logging_time': 3.422224760055542}
I0307 16:42:50.832607 140460756543232 logging_writer.py:48] [29854] accumulated_eval_time=224.533, accumulated_logging_time=3.42222, accumulated_submission_time=2786.9, global_step=29854, preemption_count=0, score=2786.9, test/loss=0.288767, test/num_examples=3581, test/ssim=0.739681, total_duration=3016.45, train/loss=0.266989, train/ssim=0.746208, validation/loss=0.287283, validation/num_examples=3554, validation/ssim=0.722294
I0307 16:42:54.735777 140460748150528 logging_writer.py:48] [29900] global_step=29900, grad_norm=0.07865416258573532, loss=0.33715057373046875
I0307 16:43:03.013379 140460756543232 logging_writer.py:48] [30000] global_step=30000, grad_norm=0.09328974038362503, loss=0.3278094232082367
I0307 16:43:11.297901 140460748150528 logging_writer.py:48] [30100] global_step=30100, grad_norm=0.12381879985332489, loss=0.30746203660964966
I0307 16:43:19.580223 140460756543232 logging_writer.py:48] [30200] global_step=30200, grad_norm=0.231453537940979, loss=0.23505419492721558
I0307 16:43:27.838275 140460748150528 logging_writer.py:48] [30300] global_step=30300, grad_norm=0.09509937465190887, loss=0.3131259083747864
I0307 16:43:36.095507 140460756543232 logging_writer.py:48] [30400] global_step=30400, grad_norm=0.09982191026210785, loss=0.3124026656150818
I0307 16:43:44.374938 140460748150528 logging_writer.py:48] [30500] global_step=30500, grad_norm=0.1542360782623291, loss=0.26495838165283203
I0307 16:43:52.637531 140460756543232 logging_writer.py:48] [30600] global_step=30600, grad_norm=0.08097964525222778, loss=0.3732147812843323
I0307 16:44:00.908194 140460748150528 logging_writer.py:48] [30700] global_step=30700, grad_norm=0.07523173838853836, loss=0.2653229832649231
I0307 16:44:09.162497 140460756543232 logging_writer.py:48] [30800] global_step=30800, grad_norm=0.11294861137866974, loss=0.3140350878238678
I0307 16:44:10.824234 140616853095616 spec.py:321] Evaluating on the training split.
I0307 16:44:12.115929 140616853095616 spec.py:333] Evaluating on the validation split.
I0307 16:44:13.412863 140616853095616 spec.py:349] Evaluating on the test split.
I0307 16:44:14.709290 140616853095616 submission_runner.py:469] Time since start: 3100.35s, 	Step: 30821, 	{'train/ssim': 0.7462079184395927, 'train/loss': 0.26698907784053255, 'validation/ssim': 0.722294229433385, 'validation/loss': 0.28728324104442177, 'validation/num_examples': 3554, 'test/ssim': 0.7396811895682072, 'test/loss': 0.288767329143832, 'test/num_examples': 3581, 'score': 2866.8336820602417, 'total_duration': 3100.3451731204987, 'accumulated_submission_time': 2866.8336820602417, 'accumulated_eval_time': 228.41805696487427, 'accumulated_logging_time': 3.4431910514831543}
I0307 16:44:14.723051 140460748150528 logging_writer.py:48] [30821] accumulated_eval_time=228.418, accumulated_logging_time=3.44319, accumulated_submission_time=2866.83, global_step=30821, preemption_count=0, score=2866.83, test/loss=0.288767, test/num_examples=3581, test/ssim=0.739681, total_duration=3100.35, train/loss=0.266989, train/ssim=0.746208, validation/loss=0.287283, validation/num_examples=3554, validation/ssim=0.722294
I0307 16:44:21.347821 140460756543232 logging_writer.py:48] [30900] global_step=30900, grad_norm=0.09855056554079056, loss=0.2841595411300659
I0307 16:44:29.633266 140460748150528 logging_writer.py:48] [31000] global_step=31000, grad_norm=0.1174376830458641, loss=0.351384699344635
I0307 16:44:37.928868 140460756543232 logging_writer.py:48] [31100] global_step=31100, grad_norm=0.08910805732011795, loss=0.2851235270500183
I0307 16:44:46.178440 140460748150528 logging_writer.py:48] [31200] global_step=31200, grad_norm=0.06759399175643921, loss=0.28499835729599
I0307 16:44:54.462549 140460756543232 logging_writer.py:48] [31300] global_step=31300, grad_norm=0.18802641332149506, loss=0.24995535612106323
I0307 16:45:02.748286 140460748150528 logging_writer.py:48] [31400] global_step=31400, grad_norm=0.1275123804807663, loss=0.328717440366745
I0307 16:45:11.018006 140460756543232 logging_writer.py:48] [31500] global_step=31500, grad_norm=0.13536538183689117, loss=0.2563181221485138
I0307 16:45:19.287939 140460748150528 logging_writer.py:48] [31600] global_step=31600, grad_norm=0.08396707475185394, loss=0.3406551480293274
I0307 16:45:27.558747 140460756543232 logging_writer.py:48] [31700] global_step=31700, grad_norm=0.14284752309322357, loss=0.2048429697751999
I0307 16:45:34.760850 140616853095616 spec.py:321] Evaluating on the training split.
I0307 16:45:36.055157 140616853095616 spec.py:333] Evaluating on the validation split.
I0307 16:45:37.352461 140616853095616 spec.py:349] Evaluating on the test split.
I0307 16:45:38.647842 140616853095616 submission_runner.py:469] Time since start: 3184.28s, 	Step: 31788, 	{'train/ssim': 0.7462079184395927, 'train/loss': 0.26698907784053255, 'validation/ssim': 0.722294229433385, 'validation/loss': 0.28728324104442177, 'validation/num_examples': 3554, 'test/ssim': 0.7396811895682072, 'test/loss': 0.288767329143832, 'test/num_examples': 3581, 'score': 2946.816180706024, 'total_duration': 3184.283727645874, 'accumulated_submission_time': 2946.816180706024, 'accumulated_eval_time': 232.30500388145447, 'accumulated_logging_time': 3.464566946029663}
I0307 16:45:38.661774 140460748150528 logging_writer.py:48] [31788] accumulated_eval_time=232.305, accumulated_logging_time=3.46457, accumulated_submission_time=2946.82, global_step=31788, preemption_count=0, score=2946.82, test/loss=0.288767, test/num_examples=3581, test/ssim=0.739681, total_duration=3184.28, train/loss=0.266989, train/ssim=0.746208, validation/loss=0.287283, validation/num_examples=3554, validation/ssim=0.722294
I0307 16:45:39.755673 140460756543232 logging_writer.py:48] [31800] global_step=31800, grad_norm=0.068897545337677, loss=0.31429412961006165
I0307 16:45:48.045726 140460748150528 logging_writer.py:48] [31900] global_step=31900, grad_norm=0.1300739198923111, loss=0.3209398090839386
I0307 16:45:56.314877 140460756543232 logging_writer.py:48] [32000] global_step=32000, grad_norm=0.18081048130989075, loss=0.20672431588172913
I0307 16:46:04.585465 140460748150528 logging_writer.py:48] [32100] global_step=32100, grad_norm=0.1910230964422226, loss=0.2810724377632141
I0307 16:46:12.870829 140460756543232 logging_writer.py:48] [32200] global_step=32200, grad_norm=0.07347390055656433, loss=0.2262386828660965
I0307 16:46:21.158233 140460748150528 logging_writer.py:48] [32300] global_step=32300, grad_norm=0.07022125273942947, loss=0.23700068891048431
I0307 16:46:29.441253 140460756543232 logging_writer.py:48] [32400] global_step=32400, grad_norm=0.10246328264474869, loss=0.25076332688331604
I0307 16:46:37.709561 140460748150528 logging_writer.py:48] [32500] global_step=32500, grad_norm=0.1494922637939453, loss=0.30481719970703125
I0307 16:46:45.978771 140460756543232 logging_writer.py:48] [32600] global_step=32600, grad_norm=0.06056743487715721, loss=0.23859265446662903
I0307 16:46:54.251257 140460748150528 logging_writer.py:48] [32700] global_step=32700, grad_norm=0.08727777004241943, loss=0.32903727889060974
I0307 16:46:58.653868 140616853095616 spec.py:321] Evaluating on the training split.
I0307 16:46:59.944052 140616853095616 spec.py:333] Evaluating on the validation split.
I0307 16:47:01.239331 140616853095616 spec.py:349] Evaluating on the test split.
I0307 16:47:02.534487 140616853095616 submission_runner.py:469] Time since start: 3268.17s, 	Step: 32754, 	{'train/ssim': 0.7462079184395927, 'train/loss': 0.26698907784053255, 'validation/ssim': 0.722294229433385, 'validation/loss': 0.28728324104442177, 'validation/num_examples': 3554, 'test/ssim': 0.7396811895682072, 'test/loss': 0.288767329143832, 'test/num_examples': 3581, 'score': 3026.7526535987854, 'total_duration': 3268.1703748703003, 'accumulated_submission_time': 3026.7526535987854, 'accumulated_eval_time': 236.1855788230896, 'accumulated_logging_time': 3.4855358600616455}
I0307 16:47:02.548580 140460756543232 logging_writer.py:48] [32754] accumulated_eval_time=236.186, accumulated_logging_time=3.48554, accumulated_submission_time=3026.75, global_step=32754, preemption_count=0, score=3026.75, test/loss=0.288767, test/num_examples=3581, test/ssim=0.739681, total_duration=3268.17, train/loss=0.266989, train/ssim=0.746208, validation/loss=0.287283, validation/num_examples=3554, validation/ssim=0.722294
I0307 16:47:06.446259 140460748150528 logging_writer.py:48] [32800] global_step=32800, grad_norm=0.08622168004512787, loss=0.2426304817199707
I0307 16:47:14.712486 140460756543232 logging_writer.py:48] [32900] global_step=32900, grad_norm=0.061044733971357346, loss=0.39024055004119873
I0307 16:47:22.971048 140460748150528 logging_writer.py:48] [33000] global_step=33000, grad_norm=0.16965970396995544, loss=0.31322917342185974
I0307 16:47:31.233900 140460756543232 logging_writer.py:48] [33100] global_step=33100, grad_norm=0.16506829857826233, loss=0.2938048541545868
I0307 16:47:39.502572 140460748150528 logging_writer.py:48] [33200] global_step=33200, grad_norm=0.21575042605400085, loss=0.2531639635562897
I0307 16:47:47.783528 140460756543232 logging_writer.py:48] [33300] global_step=33300, grad_norm=0.16656383872032166, loss=0.261175274848938
I0307 16:47:56.077926 140460748150528 logging_writer.py:48] [33400] global_step=33400, grad_norm=0.15337133407592773, loss=0.2924903333187103
I0307 16:48:04.346729 140460756543232 logging_writer.py:48] [33500] global_step=33500, grad_norm=0.07041604816913605, loss=0.36071643233299255
I0307 16:48:12.598990 140460748150528 logging_writer.py:48] [33600] global_step=33600, grad_norm=0.08800830692052841, loss=0.33198538422584534
I0307 16:48:20.858834 140460756543232 logging_writer.py:48] [33700] global_step=33700, grad_norm=0.2340737134218216, loss=0.2843771278858185
I0307 16:48:22.608049 140616853095616 spec.py:321] Evaluating on the training split.
I0307 16:48:23.900767 140616853095616 spec.py:333] Evaluating on the validation split.
I0307 16:48:25.196122 140616853095616 spec.py:349] Evaluating on the test split.
I0307 16:48:26.493503 140616853095616 submission_runner.py:469] Time since start: 3352.13s, 	Step: 33722, 	{'train/ssim': 0.7462079184395927, 'train/loss': 0.26698907784053255, 'validation/ssim': 0.722294229433385, 'validation/loss': 0.28728324104442177, 'validation/num_examples': 3554, 'test/ssim': 0.7396811895682072, 'test/loss': 0.288767329143832, 'test/num_examples': 3581, 'score': 3106.756071805954, 'total_duration': 3352.1293897628784, 'accumulated_submission_time': 3106.756071805954, 'accumulated_eval_time': 240.0709900856018, 'accumulated_logging_time': 3.5071394443511963}
I0307 16:48:26.507617 140460748150528 logging_writer.py:48] [33722] accumulated_eval_time=240.071, accumulated_logging_time=3.50714, accumulated_submission_time=3106.76, global_step=33722, preemption_count=0, score=3106.76, test/loss=0.288767, test/num_examples=3581, test/ssim=0.739681, total_duration=3352.13, train/loss=0.266989, train/ssim=0.746208, validation/loss=0.287283, validation/num_examples=3554, validation/ssim=0.722294
I0307 16:48:33.043849 140460756543232 logging_writer.py:48] [33800] global_step=33800, grad_norm=0.10505973547697067, loss=0.3217606246471405
I0307 16:48:41.325304 140460748150528 logging_writer.py:48] [33900] global_step=33900, grad_norm=0.1304364800453186, loss=0.324472039937973
I0307 16:48:49.591328 140460756543232 logging_writer.py:48] [34000] global_step=34000, grad_norm=0.14386910200119019, loss=0.31772080063819885
I0307 16:48:57.868736 140460748150528 logging_writer.py:48] [34100] global_step=34100, grad_norm=0.08793217688798904, loss=0.2637457847595215
I0307 16:49:06.134364 140460756543232 logging_writer.py:48] [34200] global_step=34200, grad_norm=0.1190829873085022, loss=0.26444151997566223
I0307 16:49:14.398055 140460748150528 logging_writer.py:48] [34300] global_step=34300, grad_norm=0.08264666050672531, loss=0.21484577655792236
I0307 16:49:22.661741 140460756543232 logging_writer.py:48] [34400] global_step=34400, grad_norm=0.20370228588581085, loss=0.30081045627593994
I0307 16:49:30.949171 140460748150528 logging_writer.py:48] [34500] global_step=34500, grad_norm=0.07672715932130814, loss=0.2825927138328552
I0307 16:49:39.239911 140460756543232 logging_writer.py:48] [34600] global_step=34600, grad_norm=0.1887272596359253, loss=0.30773985385894775
I0307 16:49:46.500216 140616853095616 spec.py:321] Evaluating on the training split.
I0307 16:49:47.792168 140616853095616 spec.py:333] Evaluating on the validation split.
I0307 16:49:49.087746 140616853095616 spec.py:349] Evaluating on the test split.
I0307 16:49:50.389240 140616853095616 submission_runner.py:469] Time since start: 3436.03s, 	Step: 34689, 	{'train/ssim': 0.7462079184395927, 'train/loss': 0.26698907784053255, 'validation/ssim': 0.722294229433385, 'validation/loss': 0.28728324104442177, 'validation/num_examples': 3554, 'test/ssim': 0.7396811895682072, 'test/loss': 0.288767329143832, 'test/num_examples': 3581, 'score': 3186.69092464447, 'total_duration': 3436.0251252651215, 'accumulated_submission_time': 3186.69092464447, 'accumulated_eval_time': 243.95996713638306, 'accumulated_logging_time': 3.5303633213043213}
I0307 16:49:50.403135 140460748150528 logging_writer.py:48] [34689] accumulated_eval_time=243.96, accumulated_logging_time=3.53036, accumulated_submission_time=3186.69, global_step=34689, preemption_count=0, score=3186.69, test/loss=0.288767, test/num_examples=3581, test/ssim=0.739681, total_duration=3436.03, train/loss=0.266989, train/ssim=0.746208, validation/loss=0.287283, validation/num_examples=3554, validation/ssim=0.722294
I0307 16:49:51.412438 140460756543232 logging_writer.py:48] [34700] global_step=34700, grad_norm=0.21930892765522003, loss=0.32482361793518066
I0307 16:49:59.671862 140460748150528 logging_writer.py:48] [34800] global_step=34800, grad_norm=0.11495523154735565, loss=0.24127478897571564
I0307 16:50:07.921851 140460756543232 logging_writer.py:48] [34900] global_step=34900, grad_norm=0.12413046509027481, loss=0.20060747861862183
I0307 16:50:16.206259 140460748150528 logging_writer.py:48] [35000] global_step=35000, grad_norm=0.16368712484836578, loss=0.27329450845718384
I0307 16:50:24.500575 140460756543232 logging_writer.py:48] [35100] global_step=35100, grad_norm=0.09388137608766556, loss=0.27433592081069946
I0307 16:50:32.773686 140460748150528 logging_writer.py:48] [35200] global_step=35200, grad_norm=0.10479339957237244, loss=0.3882962167263031
I0307 16:50:41.058159 140460756543232 logging_writer.py:48] [35300] global_step=35300, grad_norm=0.06153727322816849, loss=0.27994346618652344
I0307 16:50:49.356520 140460748150528 logging_writer.py:48] [35400] global_step=35400, grad_norm=0.14975211024284363, loss=0.27443036437034607
I0307 16:50:57.634282 140460756543232 logging_writer.py:48] [35500] global_step=35500, grad_norm=0.061357300728559494, loss=0.30547770857810974
I0307 16:51:05.917324 140460748150528 logging_writer.py:48] [35600] global_step=35600, grad_norm=0.12136397510766983, loss=0.2098880559206009
I0307 16:51:10.399252 140616853095616 spec.py:321] Evaluating on the training split.
I0307 16:51:11.690715 140616853095616 spec.py:333] Evaluating on the validation split.
I0307 16:51:12.987432 140616853095616 spec.py:349] Evaluating on the test split.
I0307 16:51:14.284296 140616853095616 submission_runner.py:469] Time since start: 3519.92s, 	Step: 35655, 	{'train/ssim': 0.7462079184395927, 'train/loss': 0.26698907784053255, 'validation/ssim': 0.722294229433385, 'validation/loss': 0.28728324104442177, 'validation/num_examples': 3554, 'test/ssim': 0.7396811895682072, 'test/loss': 0.288767329143832, 'test/num_examples': 3581, 'score': 3266.630596637726, 'total_duration': 3519.920182466507, 'accumulated_submission_time': 3266.630596637726, 'accumulated_eval_time': 247.8449673652649, 'accumulated_logging_time': 3.552405834197998}
I0307 16:51:14.298868 140460756543232 logging_writer.py:48] [35655] accumulated_eval_time=247.845, accumulated_logging_time=3.55241, accumulated_submission_time=3266.63, global_step=35655, preemption_count=0, score=3266.63, test/loss=0.288767, test/num_examples=3581, test/ssim=0.739681, total_duration=3519.92, train/loss=0.266989, train/ssim=0.746208, validation/loss=0.287283, validation/num_examples=3554, validation/ssim=0.722294
I0307 16:51:18.130212 140460748150528 logging_writer.py:48] [35700] global_step=35700, grad_norm=0.056841813027858734, loss=0.341265469789505
I0307 16:51:26.419924 140460756543232 logging_writer.py:48] [35800] global_step=35800, grad_norm=0.10001608729362488, loss=0.2195420265197754
I0307 16:51:34.680462 140460748150528 logging_writer.py:48] [35900] global_step=35900, grad_norm=0.10464098304510117, loss=0.28463372588157654
I0307 16:51:42.932814 140460756543232 logging_writer.py:48] [36000] global_step=36000, grad_norm=0.062048569321632385, loss=0.2616070508956909
I0307 16:51:51.198115 140460748150528 logging_writer.py:48] [36100] global_step=36100, grad_norm=0.06424754858016968, loss=0.24416956305503845
I0307 16:52:09.173566 140460756543232 logging_writer.py:48] [36200] global_step=36200, grad_norm=0.14663970470428467, loss=0.30434542894363403
I0307 16:52:17.482906 140460748150528 logging_writer.py:48] [36300] global_step=36300, grad_norm=0.15716198086738586, loss=0.2869260311126709
I0307 16:52:25.769075 140460756543232 logging_writer.py:48] [36400] global_step=36400, grad_norm=0.13859106600284576, loss=0.2850748300552368
I0307 16:52:34.063599 140460748150528 logging_writer.py:48] [36500] global_step=36500, grad_norm=0.12631288170814514, loss=0.32062363624572754
I0307 16:52:34.316875 140616853095616 spec.py:321] Evaluating on the training split.
I0307 16:52:35.607483 140616853095616 spec.py:333] Evaluating on the validation split.
I0307 16:52:36.901998 140616853095616 spec.py:349] Evaluating on the test split.
I0307 16:52:38.196657 140616853095616 submission_runner.py:469] Time since start: 3603.83s, 	Step: 36504, 	{'train/ssim': 0.7462079184395927, 'train/loss': 0.26698907784053255, 'validation/ssim': 0.722294229433385, 'validation/loss': 0.28728324104442177, 'validation/num_examples': 3554, 'test/ssim': 0.7396811895682072, 'test/loss': 0.288767329143832, 'test/num_examples': 3581, 'score': 3346.599247932434, 'total_duration': 3603.832542657852, 'accumulated_submission_time': 3346.599247932434, 'accumulated_eval_time': 251.72470259666443, 'accumulated_logging_time': 3.574495792388916}
I0307 16:52:38.226500 140460756543232 logging_writer.py:48] [36504] accumulated_eval_time=251.725, accumulated_logging_time=3.5745, accumulated_submission_time=3346.6, global_step=36504, preemption_count=0, score=3346.6, test/loss=0.288767, test/num_examples=3581, test/ssim=0.739681, total_duration=3603.83, train/loss=0.266989, train/ssim=0.746208, validation/loss=0.287283, validation/num_examples=3554, validation/ssim=0.722294
I0307 16:52:46.266453 140460748150528 logging_writer.py:48] [36600] global_step=36600, grad_norm=0.18206088244915009, loss=0.359184592962265
I0307 16:52:54.545967 140460756543232 logging_writer.py:48] [36700] global_step=36700, grad_norm=0.08605780452489853, loss=0.29578354954719543
I0307 16:53:02.836786 140460748150528 logging_writer.py:48] [36800] global_step=36800, grad_norm=0.1076386496424675, loss=0.31116151809692383
I0307 16:53:11.108359 140460756543232 logging_writer.py:48] [36900] global_step=36900, grad_norm=0.10924571007490158, loss=0.24682213366031647
I0307 16:53:19.380139 140460748150528 logging_writer.py:48] [37000] global_step=37000, grad_norm=0.15677857398986816, loss=0.2974754571914673
I0307 16:53:27.675803 140460756543232 logging_writer.py:48] [37100] global_step=37100, grad_norm=0.14793351292610168, loss=0.2140418440103531
I0307 16:53:35.961954 140460748150528 logging_writer.py:48] [37200] global_step=37200, grad_norm=0.20095117390155792, loss=0.24755577743053436
I0307 16:53:44.247952 140460756543232 logging_writer.py:48] [37300] global_step=37300, grad_norm=0.08309832215309143, loss=0.3250236213207245
I0307 16:53:52.513717 140460748150528 logging_writer.py:48] [37400] global_step=37400, grad_norm=0.20912976562976837, loss=0.19872348010540009
I0307 16:53:58.224652 140616853095616 spec.py:321] Evaluating on the training split.
I0307 16:53:59.515545 140616853095616 spec.py:333] Evaluating on the validation split.
I0307 16:54:00.810900 140616853095616 spec.py:349] Evaluating on the test split.
I0307 16:54:02.109198 140616853095616 submission_runner.py:469] Time since start: 3687.75s, 	Step: 37470, 	{'train/ssim': 0.7462079184395927, 'train/loss': 0.26698907784053255, 'validation/ssim': 0.722294229433385, 'validation/loss': 0.28728324104442177, 'validation/num_examples': 3554, 'test/ssim': 0.7396811895682072, 'test/loss': 0.288767329143832, 'test/num_examples': 3581, 'score': 3426.542949438095, 'total_duration': 3687.7450592517853, 'accumulated_submission_time': 3426.542949438095, 'accumulated_eval_time': 255.60917830467224, 'accumulated_logging_time': 3.6115708351135254}
I0307 16:54:02.123819 140460756543232 logging_writer.py:48] [37470] accumulated_eval_time=255.609, accumulated_logging_time=3.61157, accumulated_submission_time=3426.54, global_step=37470, preemption_count=0, score=3426.54, test/loss=0.288767, test/num_examples=3581, test/ssim=0.739681, total_duration=3687.75, train/loss=0.266989, train/ssim=0.746208, validation/loss=0.287283, validation/num_examples=3554, validation/ssim=0.722294
I0307 16:54:04.701213 140460748150528 logging_writer.py:48] [37500] global_step=37500, grad_norm=0.13673432171344757, loss=0.3151887059211731
I0307 16:54:12.973230 140460756543232 logging_writer.py:48] [37600] global_step=37600, grad_norm=0.0956372320652008, loss=0.3326927721500397
I0307 16:54:21.253175 140460748150528 logging_writer.py:48] [37700] global_step=37700, grad_norm=0.06681244820356369, loss=0.3116268515586853
I0307 16:54:29.531996 140460756543232 logging_writer.py:48] [37800] global_step=37800, grad_norm=0.08941257745027542, loss=0.27899718284606934
I0307 16:54:37.801631 140460748150528 logging_writer.py:48] [37900] global_step=37900, grad_norm=0.06476682424545288, loss=0.261192262172699
I0307 16:54:46.064279 140460756543232 logging_writer.py:48] [38000] global_step=38000, grad_norm=0.07659044861793518, loss=0.2326340228319168
I0307 16:54:54.331677 140460748150528 logging_writer.py:48] [38100] global_step=38100, grad_norm=0.09434178471565247, loss=0.2505151033401489
I0307 16:55:02.600812 140460756543232 logging_writer.py:48] [38200] global_step=38200, grad_norm=0.11641214787960052, loss=0.2783266007900238
I0307 16:55:10.893234 140460748150528 logging_writer.py:48] [38300] global_step=38300, grad_norm=0.11050224304199219, loss=0.33099740743637085
I0307 16:55:19.161882 140460756543232 logging_writer.py:48] [38400] global_step=38400, grad_norm=0.17819863557815552, loss=0.28619328141212463
I0307 16:55:22.139155 140616853095616 spec.py:321] Evaluating on the training split.
I0307 16:55:23.434000 140616853095616 spec.py:333] Evaluating on the validation split.
I0307 16:55:24.730554 140616853095616 spec.py:349] Evaluating on the test split.
I0307 16:55:26.028506 140616853095616 submission_runner.py:469] Time since start: 3771.66s, 	Step: 38437, 	{'train/ssim': 0.7462079184395927, 'train/loss': 0.26698907784053255, 'validation/ssim': 0.722294229433385, 'validation/loss': 0.28728324104442177, 'validation/num_examples': 3554, 'test/ssim': 0.7396811895682072, 'test/loss': 0.288767329143832, 'test/num_examples': 3581, 'score': 3506.502942085266, 'total_duration': 3771.6643948554993, 'accumulated_submission_time': 3506.502942085266, 'accumulated_eval_time': 259.4984874725342, 'accumulated_logging_time': 3.6332693099975586}
I0307 16:55:26.043766 140460748150528 logging_writer.py:48] [38437] accumulated_eval_time=259.498, accumulated_logging_time=3.63327, accumulated_submission_time=3506.5, global_step=38437, preemption_count=0, score=3506.5, test/loss=0.288767, test/num_examples=3581, test/ssim=0.739681, total_duration=3771.66, train/loss=0.266989, train/ssim=0.746208, validation/loss=0.287283, validation/num_examples=3554, validation/ssim=0.722294
I0307 16:55:31.360494 140460756543232 logging_writer.py:48] [38500] global_step=38500, grad_norm=0.07577221095561981, loss=0.3425373136997223
I0307 16:55:39.619647 140460748150528 logging_writer.py:48] [38600] global_step=38600, grad_norm=0.07106228172779083, loss=0.31128767132759094
I0307 16:55:47.889342 140460756543232 logging_writer.py:48] [38700] global_step=38700, grad_norm=0.1203780397772789, loss=0.2398764193058014
I0307 16:55:56.143103 140460748150528 logging_writer.py:48] [38800] global_step=38800, grad_norm=0.07880820333957672, loss=0.26307275891304016
I0307 16:56:04.379606 140460756543232 logging_writer.py:48] [38900] global_step=38900, grad_norm=0.07691825181245804, loss=0.26392918825149536
I0307 16:56:12.651420 140460748150528 logging_writer.py:48] [39000] global_step=39000, grad_norm=0.08794544637203217, loss=0.2931084632873535
I0307 16:56:20.915354 140460756543232 logging_writer.py:48] [39100] global_step=39100, grad_norm=0.16093415021896362, loss=0.2623707056045532
I0307 16:56:29.178780 140460748150528 logging_writer.py:48] [39200] global_step=39200, grad_norm=0.21635057032108307, loss=0.3127816915512085
I0307 16:56:37.426235 140460756543232 logging_writer.py:48] [39300] global_step=39300, grad_norm=0.10090736299753189, loss=0.27251797914505005
I0307 16:56:45.721568 140460748150528 logging_writer.py:48] [39400] global_step=39400, grad_norm=0.2505961060523987, loss=0.2397340089082718
I0307 16:56:46.057851 140616853095616 spec.py:321] Evaluating on the training split.
I0307 16:56:47.350931 140616853095616 spec.py:333] Evaluating on the validation split.
I0307 16:56:48.649622 140616853095616 spec.py:349] Evaluating on the test split.
I0307 16:56:49.948802 140616853095616 submission_runner.py:469] Time since start: 3855.58s, 	Step: 39405, 	{'train/ssim': 0.7462079184395927, 'train/loss': 0.26698907784053255, 'validation/ssim': 0.722294229433385, 'validation/loss': 0.28728324104442177, 'validation/num_examples': 3554, 'test/ssim': 0.7396811895682072, 'test/loss': 0.288767329143832, 'test/num_examples': 3581, 'score': 3586.4605283737183, 'total_duration': 3855.58465385437, 'accumulated_submission_time': 3586.4605283737183, 'accumulated_eval_time': 263.3893642425537, 'accumulated_logging_time': 3.655965805053711}
I0307 16:56:49.967075 140460756543232 logging_writer.py:48] [39405] accumulated_eval_time=263.389, accumulated_logging_time=3.65597, accumulated_submission_time=3586.46, global_step=39405, preemption_count=0, score=3586.46, test/loss=0.288767, test/num_examples=3581, test/ssim=0.739681, total_duration=3855.58, train/loss=0.266989, train/ssim=0.746208, validation/loss=0.287283, validation/num_examples=3554, validation/ssim=0.722294
I0307 16:56:57.915113 140460748150528 logging_writer.py:48] [39500] global_step=39500, grad_norm=0.1170060783624649, loss=0.2889472246170044
I0307 16:57:06.183383 140460756543232 logging_writer.py:48] [39600] global_step=39600, grad_norm=0.15967868268489838, loss=0.3654574155807495
I0307 16:57:14.473143 140460748150528 logging_writer.py:48] [39700] global_step=39700, grad_norm=0.08545473963022232, loss=0.29268527030944824
I0307 16:57:22.732037 140460756543232 logging_writer.py:48] [39800] global_step=39800, grad_norm=0.109791100025177, loss=0.2777211666107178
I0307 16:57:31.014973 140460748150528 logging_writer.py:48] [39900] global_step=39900, grad_norm=0.1954227238893509, loss=0.30558204650878906
I0307 16:57:39.303300 140460756543232 logging_writer.py:48] [40000] global_step=40000, grad_norm=0.14229702949523926, loss=0.3373713195323944
I0307 16:57:47.565817 140460748150528 logging_writer.py:48] [40100] global_step=40100, grad_norm=0.05532841384410858, loss=0.35079479217529297
I0307 16:57:55.837404 140460756543232 logging_writer.py:48] [40200] global_step=40200, grad_norm=0.12244201451539993, loss=0.33539313077926636
I0307 16:58:04.102986 140460748150528 logging_writer.py:48] [40300] global_step=40300, grad_norm=0.06236288323998451, loss=0.31959283351898193
I0307 16:58:09.974672 140616853095616 spec.py:321] Evaluating on the training split.
I0307 16:58:11.268323 140616853095616 spec.py:333] Evaluating on the validation split.
I0307 16:58:12.564710 140616853095616 spec.py:349] Evaluating on the test split.
I0307 16:58:13.859715 140616853095616 submission_runner.py:469] Time since start: 3939.50s, 	Step: 40372, 	{'train/ssim': 0.7462079184395927, 'train/loss': 0.26698907784053255, 'validation/ssim': 0.722294229433385, 'validation/loss': 0.28728324104442177, 'validation/num_examples': 3554, 'test/ssim': 0.7396811895682072, 'test/loss': 0.288767329143832, 'test/num_examples': 3581, 'score': 3666.4126176834106, 'total_duration': 3939.4955973625183, 'accumulated_submission_time': 3666.4126176834106, 'accumulated_eval_time': 267.274356842041, 'accumulated_logging_time': 3.682171583175659}
I0307 16:58:13.874611 140460756543232 logging_writer.py:48] [40372] accumulated_eval_time=267.274, accumulated_logging_time=3.68217, accumulated_submission_time=3666.41, global_step=40372, preemption_count=0, score=3666.41, test/loss=0.288767, test/num_examples=3581, test/ssim=0.739681, total_duration=3939.5, train/loss=0.266989, train/ssim=0.746208, validation/loss=0.287283, validation/num_examples=3554, validation/ssim=0.722294
I0307 16:58:16.287376 140460748150528 logging_writer.py:48] [40400] global_step=40400, grad_norm=0.1276136040687561, loss=0.2525918185710907
I0307 16:58:24.568387 140460756543232 logging_writer.py:48] [40500] global_step=40500, grad_norm=0.1496000438928604, loss=0.3725303113460541
I0307 16:58:32.849042 140460748150528 logging_writer.py:48] [40600] global_step=40600, grad_norm=0.11752058565616608, loss=0.2945183217525482
I0307 16:58:41.108565 140460756543232 logging_writer.py:48] [40700] global_step=40700, grad_norm=0.144728884100914, loss=0.2774052619934082
I0307 16:58:49.376947 140460748150528 logging_writer.py:48] [40800] global_step=40800, grad_norm=0.14753742516040802, loss=0.3009684681892395
I0307 16:58:57.636220 140460756543232 logging_writer.py:48] [40900] global_step=40900, grad_norm=0.05939236655831337, loss=0.3026641011238098
I0307 16:59:05.920724 140460748150528 logging_writer.py:48] [41000] global_step=41000, grad_norm=0.21124185621738434, loss=0.3105603754520416
I0307 16:59:14.179064 140460756543232 logging_writer.py:48] [41100] global_step=41100, grad_norm=0.1239214539527893, loss=0.2522297501564026
I0307 16:59:22.461630 140460748150528 logging_writer.py:48] [41200] global_step=41200, grad_norm=0.09553266316652298, loss=0.3724009692668915
I0307 16:59:30.701137 140460756543232 logging_writer.py:48] [41300] global_step=41300, grad_norm=0.12137378007173538, loss=0.2743305563926697
I0307 16:59:33.933950 140616853095616 spec.py:321] Evaluating on the training split.
I0307 16:59:35.225646 140616853095616 spec.py:333] Evaluating on the validation split.
I0307 16:59:36.522332 140616853095616 spec.py:349] Evaluating on the test split.
I0307 16:59:37.819092 140616853095616 submission_runner.py:469] Time since start: 4023.45s, 	Step: 41340, 	{'train/ssim': 0.7462079184395927, 'train/loss': 0.26698907784053255, 'validation/ssim': 0.722294229433385, 'validation/loss': 0.28728324104442177, 'validation/num_examples': 3554, 'test/ssim': 0.7396811895682072, 'test/loss': 0.288767329143832, 'test/num_examples': 3581, 'score': 3746.4170277118683, 'total_duration': 4023.4549787044525, 'accumulated_submission_time': 3746.4170277118683, 'accumulated_eval_time': 271.15945506095886, 'accumulated_logging_time': 3.7042229175567627}
I0307 16:59:37.834058 140460748150528 logging_writer.py:48] [41340] accumulated_eval_time=271.159, accumulated_logging_time=3.70422, accumulated_submission_time=3746.42, global_step=41340, preemption_count=0, score=3746.42, test/loss=0.288767, test/num_examples=3581, test/ssim=0.739681, total_duration=4023.45, train/loss=0.266989, train/ssim=0.746208, validation/loss=0.287283, validation/num_examples=3554, validation/ssim=0.722294
I0307 16:59:42.886390 140460756543232 logging_writer.py:48] [41400] global_step=41400, grad_norm=0.16330644488334656, loss=0.3308134973049164
I0307 16:59:51.162487 140460748150528 logging_writer.py:48] [41500] global_step=41500, grad_norm=0.06940389424562454, loss=0.3675895035266876
I0307 16:59:59.440867 140460756543232 logging_writer.py:48] [41600] global_step=41600, grad_norm=0.26803380250930786, loss=0.2894696593284607
I0307 17:00:07.718228 140460748150528 logging_writer.py:48] [41700] global_step=41700, grad_norm=0.1493951827287674, loss=0.2749629616737366
I0307 17:00:16.022948 140460756543232 logging_writer.py:48] [41800] global_step=41800, grad_norm=0.17722727358341217, loss=0.2609512209892273
I0307 17:00:24.307280 140460748150528 logging_writer.py:48] [41900] global_step=41900, grad_norm=0.09115023910999298, loss=0.23024725914001465
I0307 17:00:32.575200 140460756543232 logging_writer.py:48] [42000] global_step=42000, grad_norm=0.05155477672815323, loss=0.2896384298801422
I0307 17:00:40.858561 140460748150528 logging_writer.py:48] [42100] global_step=42100, grad_norm=0.12544554471969604, loss=0.41657403111457825
I0307 17:00:49.163948 140460756543232 logging_writer.py:48] [42200] global_step=42200, grad_norm=0.05933845415711403, loss=0.28784269094467163
I0307 17:00:57.437993 140460748150528 logging_writer.py:48] [42300] global_step=42300, grad_norm=0.18321867287158966, loss=0.29058289527893066
I0307 17:00:57.855197 140616853095616 spec.py:321] Evaluating on the training split.
I0307 17:00:59.147844 140616853095616 spec.py:333] Evaluating on the validation split.
I0307 17:01:00.442464 140616853095616 spec.py:349] Evaluating on the test split.
I0307 17:01:01.739287 140616853095616 submission_runner.py:469] Time since start: 4107.38s, 	Step: 42306, 	{'train/ssim': 0.7462079184395927, 'train/loss': 0.26698907784053255, 'validation/ssim': 0.722294229433385, 'validation/loss': 0.28728324104442177, 'validation/num_examples': 3554, 'test/ssim': 0.7396811895682072, 'test/loss': 0.288767329143832, 'test/num_examples': 3581, 'score': 3826.3824422359467, 'total_duration': 4107.375155925751, 'accumulated_submission_time': 3826.3824422359467, 'accumulated_eval_time': 275.04347920417786, 'accumulated_logging_time': 3.7267045974731445}
I0307 17:01:01.757507 140460756543232 logging_writer.py:48] [42306] accumulated_eval_time=275.043, accumulated_logging_time=3.7267, accumulated_submission_time=3826.38, global_step=42306, preemption_count=0, score=3826.38, test/loss=0.288767, test/num_examples=3581, test/ssim=0.739681, total_duration=4107.38, train/loss=0.266989, train/ssim=0.746208, validation/loss=0.287283, validation/num_examples=3554, validation/ssim=0.722294
I0307 17:01:09.621580 140460748150528 logging_writer.py:48] [42400] global_step=42400, grad_norm=0.0962882712483406, loss=0.28113898634910583
I0307 17:01:17.881925 140460756543232 logging_writer.py:48] [42500] global_step=42500, grad_norm=0.08752448111772537, loss=0.34040945768356323
I0307 17:01:26.150437 140460748150528 logging_writer.py:48] [42600] global_step=42600, grad_norm=0.06149454042315483, loss=0.22917811572551727
I0307 17:01:34.432678 140460756543232 logging_writer.py:48] [42700] global_step=42700, grad_norm=0.13413164019584656, loss=0.33281606435775757
I0307 17:01:42.692604 140460748150528 logging_writer.py:48] [42800] global_step=42800, grad_norm=0.10772046446800232, loss=0.3011820912361145
I0307 17:01:50.968856 140460756543232 logging_writer.py:48] [42900] global_step=42900, grad_norm=0.08014234900474548, loss=0.28275322914123535
I0307 17:01:59.233009 140460748150528 logging_writer.py:48] [43000] global_step=43000, grad_norm=0.21115291118621826, loss=0.2941909730434418
I0307 17:02:07.469696 140460756543232 logging_writer.py:48] [43100] global_step=43100, grad_norm=0.12771639227867126, loss=0.3022393584251404
I0307 17:02:15.724272 140460748150528 logging_writer.py:48] [43200] global_step=43200, grad_norm=0.16020886600017548, loss=0.28027409315109253
I0307 17:02:21.763844 140616853095616 spec.py:321] Evaluating on the training split.
I0307 17:02:23.056241 140616853095616 spec.py:333] Evaluating on the validation split.
I0307 17:02:24.348128 140616853095616 spec.py:349] Evaluating on the test split.
I0307 17:02:25.645323 140616853095616 submission_runner.py:469] Time since start: 4191.28s, 	Step: 43274, 	{'train/ssim': 0.7462079184395927, 'train/loss': 0.26698907784053255, 'validation/ssim': 0.722294229433385, 'validation/loss': 0.28728324104442177, 'validation/num_examples': 3554, 'test/ssim': 0.7396811895682072, 'test/loss': 0.288767329143832, 'test/num_examples': 3581, 'score': 3906.332620859146, 'total_duration': 4191.28120803833, 'accumulated_submission_time': 3906.332620859146, 'accumulated_eval_time': 278.9249110221863, 'accumulated_logging_time': 3.7526423931121826}
I0307 17:02:25.660707 140460756543232 logging_writer.py:48] [43274] accumulated_eval_time=278.925, accumulated_logging_time=3.75264, accumulated_submission_time=3906.33, global_step=43274, preemption_count=0, score=3906.33, test/loss=0.288767, test/num_examples=3581, test/ssim=0.739681, total_duration=4191.28, train/loss=0.266989, train/ssim=0.746208, validation/loss=0.287283, validation/num_examples=3554, validation/ssim=0.722294
I0307 17:02:27.909220 140460748150528 logging_writer.py:48] [43300] global_step=43300, grad_norm=0.09210605174303055, loss=0.2447091042995453
I0307 17:02:36.157966 140460756543232 logging_writer.py:48] [43400] global_step=43400, grad_norm=0.15646973252296448, loss=0.21778452396392822
I0307 17:02:44.424042 140460748150528 logging_writer.py:48] [43500] global_step=43500, grad_norm=0.17366531491279602, loss=0.2300463765859604
I0307 17:02:52.685726 140460756543232 logging_writer.py:48] [43600] global_step=43600, grad_norm=0.11230875551700592, loss=0.25751903653144836
I0307 17:03:00.963037 140460748150528 logging_writer.py:48] [43700] global_step=43700, grad_norm=0.10343041270971298, loss=0.3581653833389282
I0307 17:03:09.234800 140460756543232 logging_writer.py:48] [43800] global_step=43800, grad_norm=0.1131950318813324, loss=0.2888863682746887
I0307 17:03:17.509288 140460748150528 logging_writer.py:48] [43900] global_step=43900, grad_norm=0.0717298835515976, loss=0.3309658169746399
I0307 17:03:25.779845 140460756543232 logging_writer.py:48] [44000] global_step=44000, grad_norm=0.10418014228343964, loss=0.2800844609737396
I0307 17:03:34.044133 140460748150528 logging_writer.py:48] [44100] global_step=44100, grad_norm=0.06167863309383392, loss=0.3584092855453491
I0307 17:03:42.324096 140460756543232 logging_writer.py:48] [44200] global_step=44200, grad_norm=0.0732308179140091, loss=0.3072109818458557
I0307 17:03:45.717970 140616853095616 spec.py:321] Evaluating on the training split.
I0307 17:03:47.010733 140616853095616 spec.py:333] Evaluating on the validation split.
I0307 17:03:48.307454 140616853095616 spec.py:349] Evaluating on the test split.
I0307 17:03:49.605335 140616853095616 submission_runner.py:469] Time since start: 4275.24s, 	Step: 44242, 	{'train/ssim': 0.7462079184395927, 'train/loss': 0.26698907784053255, 'validation/ssim': 0.722294229433385, 'validation/loss': 0.28728324104442177, 'validation/num_examples': 3554, 'test/ssim': 0.7396811895682072, 'test/loss': 0.288767329143832, 'test/num_examples': 3581, 'score': 3986.3344116210938, 'total_duration': 4275.241221666336, 'accumulated_submission_time': 3986.3344116210938, 'accumulated_eval_time': 282.8122282028198, 'accumulated_logging_time': 3.775925636291504}
I0307 17:03:49.620596 140460748150528 logging_writer.py:48] [44242] accumulated_eval_time=282.812, accumulated_logging_time=3.77593, accumulated_submission_time=3986.33, global_step=44242, preemption_count=0, score=3986.33, test/loss=0.288767, test/num_examples=3581, test/ssim=0.739681, total_duration=4275.24, train/loss=0.266989, train/ssim=0.746208, validation/loss=0.287283, validation/num_examples=3554, validation/ssim=0.722294
I0307 17:03:54.500054 140460756543232 logging_writer.py:48] [44300] global_step=44300, grad_norm=0.15002837777137756, loss=0.2576791048049927
I0307 17:04:02.775546 140460748150528 logging_writer.py:48] [44400] global_step=44400, grad_norm=0.0894218161702156, loss=0.35905566811561584
I0307 17:04:11.030111 140460756543232 logging_writer.py:48] [44500] global_step=44500, grad_norm=0.2112138718366623, loss=0.2693336606025696
I0307 17:04:19.291836 140460748150528 logging_writer.py:48] [44600] global_step=44600, grad_norm=0.1574515402317047, loss=0.26062214374542236
I0307 17:04:27.557826 140460756543232 logging_writer.py:48] [44700] global_step=44700, grad_norm=0.09162996709346771, loss=0.24610644578933716
I0307 17:04:35.824221 140460748150528 logging_writer.py:48] [44800] global_step=44800, grad_norm=0.15757562220096588, loss=0.2779271900653839
I0307 17:04:44.082405 140460756543232 logging_writer.py:48] [44900] global_step=44900, grad_norm=0.12184406816959381, loss=0.2791982889175415
I0307 17:04:52.346037 140460748150528 logging_writer.py:48] [45000] global_step=45000, grad_norm=0.14701969921588898, loss=0.2656731605529785
I0307 17:05:00.640973 140460756543232 logging_writer.py:48] [45100] global_step=45100, grad_norm=0.07054736465215683, loss=0.29099687933921814
I0307 17:05:08.930225 140460748150528 logging_writer.py:48] [45200] global_step=45200, grad_norm=0.11095444113016129, loss=0.36522603034973145
I0307 17:05:09.686120 140616853095616 spec.py:321] Evaluating on the training split.
I0307 17:05:10.977373 140616853095616 spec.py:333] Evaluating on the validation split.
I0307 17:05:12.271729 140616853095616 spec.py:349] Evaluating on the test split.
I0307 17:05:13.568451 140616853095616 submission_runner.py:469] Time since start: 4359.20s, 	Step: 45210, 	{'train/ssim': 0.7462079184395927, 'train/loss': 0.26698907784053255, 'validation/ssim': 0.722294229433385, 'validation/loss': 0.28728324104442177, 'validation/num_examples': 3554, 'test/ssim': 0.7396811895682072, 'test/loss': 0.288767329143832, 'test/num_examples': 3581, 'score': 4066.3453018665314, 'total_duration': 4359.20433306694, 'accumulated_submission_time': 4066.3453018665314, 'accumulated_eval_time': 286.6945128440857, 'accumulated_logging_time': 3.7985146045684814}
I0307 17:05:13.584019 140460756543232 logging_writer.py:48] [45210] accumulated_eval_time=286.695, accumulated_logging_time=3.79851, accumulated_submission_time=4066.35, global_step=45210, preemption_count=0, score=4066.35, test/loss=0.288767, test/num_examples=3581, test/ssim=0.739681, total_duration=4359.2, train/loss=0.266989, train/ssim=0.746208, validation/loss=0.287283, validation/num_examples=3554, validation/ssim=0.722294
I0307 17:05:21.130499 140460748150528 logging_writer.py:48] [45300] global_step=45300, grad_norm=0.060290947556495667, loss=0.22138085961341858
I0307 17:05:29.426879 140460756543232 logging_writer.py:48] [45400] global_step=45400, grad_norm=0.09953544288873672, loss=0.23271608352661133
I0307 17:05:37.684591 140460748150528 logging_writer.py:48] [45500] global_step=45500, grad_norm=0.08189720660448074, loss=0.29774561524391174
I0307 17:05:45.945819 140460756543232 logging_writer.py:48] [45600] global_step=45600, grad_norm=0.1895570605993271, loss=0.23878948390483856
I0307 17:05:54.209247 140460748150528 logging_writer.py:48] [45700] global_step=45700, grad_norm=0.10677263140678406, loss=0.2390914410352707
I0307 17:06:02.474266 140460756543232 logging_writer.py:48] [45800] global_step=45800, grad_norm=0.18710820376873016, loss=0.2585407495498657
I0307 17:06:10.756812 140460748150528 logging_writer.py:48] [45900] global_step=45900, grad_norm=0.06443563848733902, loss=0.37007322907447815
I0307 17:06:19.008278 140460756543232 logging_writer.py:48] [46000] global_step=46000, grad_norm=0.25284433364868164, loss=0.28037378191947937
I0307 17:06:27.283401 140460748150528 logging_writer.py:48] [46100] global_step=46100, grad_norm=0.18808165192604065, loss=0.31231001019477844
I0307 17:06:33.583756 140616853095616 spec.py:321] Evaluating on the training split.
I0307 17:06:34.875062 140616853095616 spec.py:333] Evaluating on the validation split.
I0307 17:06:36.171666 140616853095616 spec.py:349] Evaluating on the test split.
I0307 17:06:37.468125 140616853095616 submission_runner.py:469] Time since start: 4443.10s, 	Step: 46177, 	{'train/ssim': 0.7462079184395927, 'train/loss': 0.26698907784053255, 'validation/ssim': 0.722294229433385, 'validation/loss': 0.28728324104442177, 'validation/num_examples': 3554, 'test/ssim': 0.7396811895682072, 'test/loss': 0.288767329143832, 'test/num_examples': 3581, 'score': 4146.291484117508, 'total_duration': 4443.104011297226, 'accumulated_submission_time': 4146.291484117508, 'accumulated_eval_time': 290.57883977890015, 'accumulated_logging_time': 3.8214449882507324}
I0307 17:06:37.483810 140460756543232 logging_writer.py:48] [46177] accumulated_eval_time=290.579, accumulated_logging_time=3.82144, accumulated_submission_time=4146.29, global_step=46177, preemption_count=0, score=4146.29, test/loss=0.288767, test/num_examples=3581, test/ssim=0.739681, total_duration=4443.1, train/loss=0.266989, train/ssim=0.746208, validation/loss=0.287283, validation/num_examples=3554, validation/ssim=0.722294
I0307 17:06:39.478670 140460748150528 logging_writer.py:48] [46200] global_step=46200, grad_norm=0.2147405743598938, loss=0.32061970233917236
I0307 17:06:47.740411 140460756543232 logging_writer.py:48] [46300] global_step=46300, grad_norm=0.1609332412481308, loss=0.3148368000984192
I0307 17:06:56.004304 140460748150528 logging_writer.py:48] [46400] global_step=46400, grad_norm=0.22094836831092834, loss=0.3230724632740021
I0307 17:07:04.269596 140460756543232 logging_writer.py:48] [46500] global_step=46500, grad_norm=0.1784365475177765, loss=0.2908821105957031
I0307 17:07:12.523203 140460748150528 logging_writer.py:48] [46600] global_step=46600, grad_norm=0.08245906233787537, loss=0.28457075357437134
I0307 17:07:20.819345 140460756543232 logging_writer.py:48] [46700] global_step=46700, grad_norm=0.22593912482261658, loss=0.24942569434642792
I0307 17:07:29.073859 140460748150528 logging_writer.py:48] [46800] global_step=46800, grad_norm=0.14601171016693115, loss=0.3117532730102539
I0307 17:07:37.339234 140460756543232 logging_writer.py:48] [46900] global_step=46900, grad_norm=0.08527856320142746, loss=0.2556627690792084
I0307 17:07:45.617186 140460748150528 logging_writer.py:48] [47000] global_step=47000, grad_norm=0.13803145289421082, loss=0.3156909644603729
I0307 17:07:53.874843 140460756543232 logging_writer.py:48] [47100] global_step=47100, grad_norm=0.06398586928844452, loss=0.2592773139476776
I0307 17:07:57.512924 140616853095616 spec.py:321] Evaluating on the training split.
I0307 17:07:58.803129 140616853095616 spec.py:333] Evaluating on the validation split.
I0307 17:08:00.100801 140616853095616 spec.py:349] Evaluating on the test split.
I0307 17:08:01.398283 140616853095616 submission_runner.py:469] Time since start: 4527.03s, 	Step: 47145, 	{'train/ssim': 0.7462079184395927, 'train/loss': 0.26698907784053255, 'validation/ssim': 0.722294229433385, 'validation/loss': 0.28728324104442177, 'validation/num_examples': 3554, 'test/ssim': 0.7396811895682072, 'test/loss': 0.288767329143832, 'test/num_examples': 3581, 'score': 4226.266886234283, 'total_duration': 4527.034152746201, 'accumulated_submission_time': 4226.266886234283, 'accumulated_eval_time': 294.4641377925873, 'accumulated_logging_time': 3.8446476459503174}
I0307 17:08:01.414299 140460748150528 logging_writer.py:48] [47145] accumulated_eval_time=294.464, accumulated_logging_time=3.84465, accumulated_submission_time=4226.27, global_step=47145, preemption_count=0, score=4226.27, test/loss=0.288767, test/num_examples=3581, test/ssim=0.739681, total_duration=4527.03, train/loss=0.266989, train/ssim=0.746208, validation/loss=0.287283, validation/num_examples=3554, validation/ssim=0.722294
I0307 17:08:06.049170 140460756543232 logging_writer.py:48] [47200] global_step=47200, grad_norm=0.13996869325637817, loss=0.3006317913532257
I0307 17:08:14.322062 140460748150528 logging_writer.py:48] [47300] global_step=47300, grad_norm=0.08938974142074585, loss=0.3335382640361786
I0307 17:08:22.597646 140460756543232 logging_writer.py:48] [47400] global_step=47400, grad_norm=0.20097237825393677, loss=0.1996297687292099
I0307 17:08:30.871900 140460748150528 logging_writer.py:48] [47500] global_step=47500, grad_norm=0.08540066331624985, loss=0.3751259446144104
I0307 17:08:39.122298 140460756543232 logging_writer.py:48] [47600] global_step=47600, grad_norm=0.08058363199234009, loss=0.30995431542396545
I0307 17:08:47.395478 140460748150528 logging_writer.py:48] [47700] global_step=47700, grad_norm=0.13105152547359467, loss=0.31067442893981934
I0307 17:08:55.666718 140460756543232 logging_writer.py:48] [47800] global_step=47800, grad_norm=0.06742578744888306, loss=0.25013267993927
I0307 17:09:03.919931 140460748150528 logging_writer.py:48] [47900] global_step=47900, grad_norm=0.07520877569913864, loss=0.2861286699771881
I0307 17:09:12.196213 140460756543232 logging_writer.py:48] [48000] global_step=48000, grad_norm=0.11953941732645035, loss=0.24027711153030396
I0307 17:09:20.477252 140460748150528 logging_writer.py:48] [48100] global_step=48100, grad_norm=0.05409182235598564, loss=0.3599943220615387
I0307 17:09:21.470466 140616853095616 spec.py:321] Evaluating on the training split.
I0307 17:09:22.764445 140616853095616 spec.py:333] Evaluating on the validation split.
I0307 17:09:24.064098 140616853095616 spec.py:349] Evaluating on the test split.
I0307 17:09:25.361040 140616853095616 submission_runner.py:469] Time since start: 4611.00s, 	Step: 48113, 	{'train/ssim': 0.7462079184395927, 'train/loss': 0.26698907784053255, 'validation/ssim': 0.722294229433385, 'validation/loss': 0.28728324104442177, 'validation/num_examples': 3554, 'test/ssim': 0.7396811895682072, 'test/loss': 0.288767329143832, 'test/num_examples': 3581, 'score': 4306.271164894104, 'total_duration': 4610.996920824051, 'accumulated_submission_time': 4306.271164894104, 'accumulated_eval_time': 298.3546690940857, 'accumulated_logging_time': 3.8678138256073}
I0307 17:09:25.376644 140460756543232 logging_writer.py:48] [48113] accumulated_eval_time=298.355, accumulated_logging_time=3.86781, accumulated_submission_time=4306.27, global_step=48113, preemption_count=0, score=4306.27, test/loss=0.288767, test/num_examples=3581, test/ssim=0.739681, total_duration=4611, train/loss=0.266989, train/ssim=0.746208, validation/loss=0.287283, validation/num_examples=3554, validation/ssim=0.722294
I0307 17:09:32.673241 140460748150528 logging_writer.py:48] [48200] global_step=48200, grad_norm=0.09762324392795563, loss=0.278632253408432
I0307 17:09:40.920080 140460756543232 logging_writer.py:48] [48300] global_step=48300, grad_norm=0.07169299572706223, loss=0.2781875729560852
I0307 17:09:49.176971 140460748150528 logging_writer.py:48] [48400] global_step=48400, grad_norm=0.090005062520504, loss=0.3497034013271332
I0307 17:09:57.454269 140460756543232 logging_writer.py:48] [48500] global_step=48500, grad_norm=0.16758179664611816, loss=0.21032369136810303
I0307 17:10:05.711730 140460748150528 logging_writer.py:48] [48600] global_step=48600, grad_norm=0.14535385370254517, loss=0.275909960269928
I0307 17:10:13.976241 140460756543232 logging_writer.py:48] [48700] global_step=48700, grad_norm=0.07929349690675735, loss=0.3307093381881714
I0307 17:10:22.242908 140460748150528 logging_writer.py:48] [48800] global_step=48800, grad_norm=0.10204028338193893, loss=0.33451712131500244
I0307 17:10:30.476343 140460756543232 logging_writer.py:48] [48900] global_step=48900, grad_norm=0.26433223485946655, loss=0.32688701152801514
I0307 17:10:38.754925 140460748150528 logging_writer.py:48] [49000] global_step=49000, grad_norm=0.0796612873673439, loss=0.280174195766449
I0307 17:10:45.367249 140616853095616 spec.py:321] Evaluating on the training split.
I0307 17:10:46.659545 140616853095616 spec.py:333] Evaluating on the validation split.
I0307 17:10:47.955870 140616853095616 spec.py:349] Evaluating on the test split.
I0307 17:10:49.255845 140616853095616 submission_runner.py:469] Time since start: 4694.89s, 	Step: 49081, 	{'train/ssim': 0.7462079184395927, 'train/loss': 0.26698907784053255, 'validation/ssim': 0.722294229433385, 'validation/loss': 0.28728324104442177, 'validation/num_examples': 3554, 'test/ssim': 0.7396811895682072, 'test/loss': 0.288767329143832, 'test/num_examples': 3581, 'score': 4386.211649894714, 'total_duration': 4694.891733407974, 'accumulated_submission_time': 4386.211649894714, 'accumulated_eval_time': 302.24322533607483, 'accumulated_logging_time': 3.890458106994629}
I0307 17:10:49.272114 140460756543232 logging_writer.py:48] [49081] accumulated_eval_time=302.243, accumulated_logging_time=3.89046, accumulated_submission_time=4386.21, global_step=49081, preemption_count=0, score=4386.21, test/loss=0.288767, test/num_examples=3581, test/ssim=0.739681, total_duration=4694.89, train/loss=0.266989, train/ssim=0.746208, validation/loss=0.287283, validation/num_examples=3554, validation/ssim=0.722294
I0307 17:10:50.941567 140460748150528 logging_writer.py:48] [49100] global_step=49100, grad_norm=0.15770916640758514, loss=0.25184428691864014
I0307 17:10:59.228213 140460756543232 logging_writer.py:48] [49200] global_step=49200, grad_norm=0.13132168352603912, loss=0.2726668119430542
I0307 17:11:07.491367 140460748150528 logging_writer.py:48] [49300] global_step=49300, grad_norm=0.18943898379802704, loss=0.2745971381664276
I0307 17:11:15.760561 140460756543232 logging_writer.py:48] [49400] global_step=49400, grad_norm=0.10688114166259766, loss=0.2761388421058655
I0307 17:11:24.022091 140460748150528 logging_writer.py:48] [49500] global_step=49500, grad_norm=0.1905730664730072, loss=0.3078612685203552
I0307 17:11:32.310244 140460756543232 logging_writer.py:48] [49600] global_step=49600, grad_norm=0.19232775270938873, loss=0.2678386867046356
I0307 17:11:40.583922 140460748150528 logging_writer.py:48] [49700] global_step=49700, grad_norm=0.1449478268623352, loss=0.29617422819137573
I0307 17:11:48.849737 140460756543232 logging_writer.py:48] [49800] global_step=49800, grad_norm=0.09399515390396118, loss=0.39365333318710327
I0307 17:11:57.131952 140460748150528 logging_writer.py:48] [49900] global_step=49900, grad_norm=0.08055250346660614, loss=0.25376075506210327
I0307 17:12:05.407905 140460756543232 logging_writer.py:48] [50000] global_step=50000, grad_norm=0.079666368663311, loss=0.2245751917362213
I0307 17:12:09.311396 140460748150528 logging_writer.py:48] [50048] global_step=50048, preemption_count=0, score=4466.19
I0307 17:12:09.971535 140616853095616 submission_runner.py:646] Tuning trial 5/5
I0307 17:12:09.971745 140616853095616 submission_runner.py:647] Hyperparameters: Hyperparameters(dropout_rate=0.1, label_smoothing=0.0, learning_rate=0.0017486387539278373, one_minus_beta1=0.06733926164, beta2=0.9955159689799007, weight_decay=0.08121616522670176, warmup_factor=0.02)
I0307 17:12:09.973497 140616853095616 submission_runner.py:648] Metrics: {'eval_results': [(1, {'train/ssim': 0.18020692893436976, 'train/loss': 0.9958035605294364, 'validation/ssim': 0.1708046521909732, 'validation/loss': 1.0128932216560567, 'validation/num_examples': 3554, 'test/ssim': 0.1926895222561348, 'test/loss': 1.0104085990950502, 'test/num_examples': 3581, 'score': 230.37812995910645, 'total_duration': 330.30274510383606, 'accumulated_submission_time': 230.37812995910645, 'accumulated_eval_time': 99.92445659637451, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (481, {'train/ssim': 0.7112443787711007, 'train/loss': 0.29891014099121094, 'validation/ssim': 0.6903459487769062, 'validation/loss': 0.3184809644502673, 'validation/num_examples': 3554, 'test/ssim': 0.7077388555701969, 'test/loss': 0.32064090697867215, 'test/num_examples': 3581, 'score': 310.3247218132019, 'total_duration': 414.52664375305176, 'accumulated_submission_time': 310.3247218132019, 'accumulated_eval_time': 104.09919023513794, 'accumulated_logging_time': 0.015844345092773438, 'global_step': 481, 'preemption_count': 0}), (929, {'train/ssim': 0.7268283707754952, 'train/loss': 0.28778001240321566, 'validation/ssim': 0.7044542406003799, 'validation/loss': 0.30737376816834905, 'validation/num_examples': 3554, 'test/ssim': 0.7215728546713558, 'test/loss': 0.30974112912550616, 'test/num_examples': 3581, 'score': 390.3729133605957, 'total_duration': 498.5559594631195, 'accumulated_submission_time': 390.3729133605957, 'accumulated_eval_time': 107.98533248901367, 'accumulated_logging_time': 0.04126453399658203, 'global_step': 929, 'preemption_count': 0}), (1882, {'train/ssim': 0.731466565813337, 'train/loss': 0.2809727191925049, 'validation/ssim': 0.7087999986810636, 'validation/loss': 0.3002311436057963, 'validation/num_examples': 3554, 'test/ssim': 0.7259995653056059, 'test/loss': 0.302540957811191, 'test/num_examples': 3581, 'score': 470.26545906066895, 'total_duration': 582.4582343101501, 'accumulated_submission_time': 470.26545906066895, 'accumulated_eval_time': 111.87870740890503, 'accumulated_logging_time': 0.10169720649719238, 'global_step': 1882, 'preemption_count': 0}), (2847, {'train/ssim': 0.7361689976283482, 'train/loss': 0.2774186134338379, 'validation/ssim': 0.7132158252585116, 'validation/loss': 0.29716499427361776, 'validation/num_examples': 3554, 'test/ssim': 0.7306273289147585, 'test/loss': 0.29904451772375035, 'test/num_examples': 3581, 'score': 550.2847602367401, 'total_duration': 666.4287843704224, 'accumulated_submission_time': 550.2847602367401, 'accumulated_eval_time': 115.7656946182251, 'accumulated_logging_time': 0.1188802719116211, 'global_step': 2847, 'preemption_count': 0}), (3812, {'train/ssim': 0.7387965066092355, 'train/loss': 0.2736138275691441, 'validation/ssim': 0.7156405387195766, 'validation/loss': 0.293467884997538, 'validation/num_examples': 3554, 'test/ssim': 0.7329760149050545, 'test/loss': 0.2952618038344736, 'test/num_examples': 3581, 'score': 630.2471811771393, 'total_duration': 750.336847782135, 'accumulated_submission_time': 630.2471811771393, 'accumulated_eval_time': 119.6470217704773, 'accumulated_logging_time': 0.13733768463134766, 'global_step': 3812, 'preemption_count': 0}), (4779, {'train/ssim': 0.7389520236424038, 'train/loss': 0.27224622453962055, 'validation/ssim': 0.7155438854108047, 'validation/loss': 0.2918601564698227, 'validation/num_examples': 3554, 'test/ssim': 0.7329728106019617, 'test/loss': 0.29356747744170625, 'test/num_examples': 3581, 'score': 710.2633895874023, 'total_duration': 834.3008890151978, 'accumulated_submission_time': 710.2633895874023, 'accumulated_eval_time': 123.53054666519165, 'accumulated_logging_time': 0.155534029006958, 'global_step': 4779, 'preemption_count': 0}), (5745, {'train/ssim': 0.7402826717921666, 'train/loss': 0.2725231647491455, 'validation/ssim': 0.7166053544421779, 'validation/loss': 0.2925934370273811, 'validation/num_examples': 3554, 'test/ssim': 0.7338398132068557, 'test/loss': 0.2942429036276529, 'test/num_examples': 3581, 'score': 790.2726905345917, 'total_duration': 918.2681102752686, 'accumulated_submission_time': 790.2726905345917, 'accumulated_eval_time': 127.42349624633789, 'accumulated_logging_time': 0.17377042770385742, 'global_step': 5745, 'preemption_count': 0}), (6711, {'train/ssim': 0.741997378213065, 'train/loss': 0.270321113722665, 'validation/ssim': 0.7181727592369513, 'validation/loss': 0.29048557743036013, 'validation/num_examples': 3554, 'test/ssim': 0.7355230949804524, 'test/loss': 0.2921064856730836, 'test/num_examples': 3581, 'score': 870.2188169956207, 'total_duration': 1002.1635987758636, 'accumulated_submission_time': 870.2188169956207, 'accumulated_eval_time': 131.30804777145386, 'accumulated_logging_time': 0.19206547737121582, 'global_step': 6711, 'preemption_count': 0}), (7677, {'train/ssim': 0.7426671981811523, 'train/loss': 0.26995488575526644, 'validation/ssim': 0.7193892719910664, 'validation/loss': 0.2897324441320519, 'validation/num_examples': 3554, 'test/ssim': 0.736902513373534, 'test/loss': 0.29124176698635157, 'test/num_examples': 3581, 'score': 950.2277727127075, 'total_duration': 1086.1271166801453, 'accumulated_submission_time': 950.2277727127075, 'accumulated_eval_time': 135.1975119113922, 'accumulated_logging_time': 0.21083450317382812, 'global_step': 7677, 'preemption_count': 0}), (8643, {'train/ssim': 0.7425406319754464, 'train/loss': 0.26960534708840506, 'validation/ssim': 0.719443403339547, 'validation/loss': 0.28923791167302687, 'validation/num_examples': 3554, 'test/ssim': 0.7369913475635297, 'test/loss': 0.29074632718688914, 'test/num_examples': 3581, 'score': 1030.2060573101044, 'total_duration': 1170.057211637497, 'accumulated_submission_time': 1030.2060573101044, 'accumulated_eval_time': 139.08423948287964, 'accumulated_logging_time': 0.22933006286621094, 'global_step': 8643, 'preemption_count': 0}), (9610, {'train/ssim': 0.743058613368443, 'train/loss': 0.2688384737287249, 'validation/ssim': 0.7195667788495357, 'validation/loss': 0.2885968192749367, 'validation/num_examples': 3554, 'test/ssim': 0.7369914839168529, 'test/loss': 0.2901561559162071, 'test/num_examples': 3581, 'score': 1110.2093303203583, 'total_duration': 1254.0100696086884, 'accumulated_submission_time': 1110.2093303203583, 'accumulated_eval_time': 142.96735429763794, 'accumulated_logging_time': 0.2472386360168457, 'global_step': 9610, 'preemption_count': 0}), (10539, {'train/ssim': 0.7428361347743443, 'train/loss': 0.26940992900303434, 'validation/ssim': 0.7188806571380838, 'validation/loss': 0.2896745689276168, 'validation/num_examples': 3554, 'test/ssim': 0.7362786969203784, 'test/loss': 0.2911889300736526, 'test/num_examples': 3581, 'score': 1187.4759728908539, 'total_duration': 1337.9666283130646, 'accumulated_submission_time': 1187.4759728908539, 'accumulated_eval_time': 146.84817051887512, 'accumulated_logging_time': 3.00484299659729, 'global_step': 10539, 'preemption_count': 0}), (11504, {'train/ssim': 0.7444582666669574, 'train/loss': 0.26833774362291607, 'validation/ssim': 0.7205948623030388, 'validation/loss': 0.28857610785162846, 'validation/num_examples': 3554, 'test/ssim': 0.7380572896668179, 'test/loss': 0.29010556883333916, 'test/num_examples': 3581, 'score': 1267.467027425766, 'total_duration': 1421.9061546325684, 'accumulated_submission_time': 1267.467027425766, 'accumulated_eval_time': 150.72906160354614, 'accumulated_logging_time': 3.024651050567627, 'global_step': 11504, 'preemption_count': 0}), (12469, {'train/ssim': 0.7441231182643345, 'train/loss': 0.2687712737492153, 'validation/ssim': 0.7202655403682471, 'validation/loss': 0.28900926168138014, 'validation/num_examples': 3554, 'test/ssim': 0.7374430861229405, 'test/loss': 0.29058089651764524, 'test/num_examples': 3581, 'score': 1347.4073014259338, 'total_duration': 1505.8025550842285, 'accumulated_submission_time': 1347.4073014259338, 'accumulated_eval_time': 154.61867356300354, 'accumulated_logging_time': 3.0438902378082275, 'global_step': 12469, 'preemption_count': 0}), (13434, {'train/ssim': 0.7460836683000837, 'train/loss': 0.26756627219063894, 'validation/ssim': 0.7224871238833005, 'validation/loss': 0.28790812151580963, 'validation/num_examples': 3554, 'test/ssim': 0.7397983852494066, 'test/loss': 0.2894373352851857, 'test/num_examples': 3581, 'score': 1427.3862013816833, 'total_duration': 1589.7294850349426, 'accumulated_submission_time': 1427.3862013816833, 'accumulated_eval_time': 158.50104570388794, 'accumulated_logging_time': 3.063603162765503, 'global_step': 13434, 'preemption_count': 0}), (14400, {'train/ssim': 0.746281487601144, 'train/loss': 0.2675477777208601, 'validation/ssim': 0.7224320995049592, 'validation/loss': 0.2878974223311322, 'validation/num_examples': 3554, 'test/ssim': 0.7397602063189402, 'test/loss': 0.2893891343854719, 'test/num_examples': 3581, 'score': 1507.398591041565, 'total_duration': 1673.6918992996216, 'accumulated_submission_time': 1507.398591041565, 'accumulated_eval_time': 162.38159012794495, 'accumulated_logging_time': 3.0839579105377197, 'global_step': 14400, 'preemption_count': 0}), (15366, {'train/ssim': 0.7458127566746303, 'train/loss': 0.26731155599866596, 'validation/ssim': 0.7218966937561551, 'validation/loss': 0.2875665375940841, 'validation/num_examples': 3554, 'test/ssim': 0.7392618349230313, 'test/loss': 0.2890428651214744, 'test/num_examples': 3581, 'score': 1587.3673589229584, 'total_duration': 1757.613296031952, 'accumulated_submission_time': 1587.3673589229584, 'accumulated_eval_time': 166.26765513420105, 'accumulated_logging_time': 3.1034443378448486, 'global_step': 15366, 'preemption_count': 0}), (16332, {'train/ssim': 0.745870862688337, 'train/loss': 0.26725404603140696, 'validation/ssim': 0.7219311097530952, 'validation/loss': 0.28751747247269804, 'validation/num_examples': 3554, 'test/ssim': 0.7392814698015568, 'test/loss': 0.28900516342763893, 'test/num_examples': 3581, 'score': 1667.369958639145, 'total_duration': 1841.5687565803528, 'accumulated_submission_time': 1667.369958639145, 'accumulated_eval_time': 170.15294885635376, 'accumulated_logging_time': 3.1235756874084473, 'global_step': 16332, 'preemption_count': 0}), (17298, {'train/ssim': 0.7457662309919085, 'train/loss': 0.26716082436697824, 'validation/ssim': 0.7217512672780669, 'validation/loss': 0.2875110838744636, 'validation/num_examples': 3554, 'test/ssim': 0.7391123235042586, 'test/loss': 0.2890060838125698, 'test/num_examples': 3581, 'score': 1747.363486289978, 'total_duration': 1925.513364315033, 'accumulated_submission_time': 1747.363486289978, 'accumulated_eval_time': 174.035888671875, 'accumulated_logging_time': 3.1440863609313965, 'global_step': 17298, 'preemption_count': 0}), (18263, {'train/ssim': 0.7462079184395927, 'train/loss': 0.26698907784053255, 'validation/ssim': 0.722294229433385, 'validation/loss': 0.28728324104442177, 'validation/num_examples': 3554, 'test/ssim': 0.7396811895682072, 'test/loss': 0.288767329143832, 'test/num_examples': 3581, 'score': 1827.3210127353668, 'total_duration': 2009.4277806282043, 'accumulated_submission_time': 1827.3210127353668, 'accumulated_eval_time': 177.92346382141113, 'accumulated_logging_time': 3.164139986038208, 'global_step': 18263, 'preemption_count': 0}), (19228, {'train/ssim': 0.7462079184395927, 'train/loss': 0.26698907784053255, 'validation/ssim': 0.722294229433385, 'validation/loss': 0.28728324104442177, 'validation/num_examples': 3554, 'test/ssim': 0.7396811895682072, 'test/loss': 0.288767329143832, 'test/num_examples': 3581, 'score': 1907.2931859493256, 'total_duration': 2093.3497874736786, 'accumulated_submission_time': 1907.2931859493256, 'accumulated_eval_time': 181.8044993877411, 'accumulated_logging_time': 3.18454647064209, 'global_step': 19228, 'preemption_count': 0}), (20193, {'train/ssim': 0.7462079184395927, 'train/loss': 0.26698907784053255, 'validation/ssim': 0.722294229433385, 'validation/loss': 0.28728324104442177, 'validation/num_examples': 3554, 'test/ssim': 0.7396811895682072, 'test/loss': 0.288767329143832, 'test/num_examples': 3581, 'score': 1987.2285618782043, 'total_duration': 2177.240236520767, 'accumulated_submission_time': 1987.2285618782043, 'accumulated_eval_time': 185.69081735610962, 'accumulated_logging_time': 3.2041587829589844, 'global_step': 20193, 'preemption_count': 0}), (21159, {'train/ssim': 0.7462079184395927, 'train/loss': 0.26698907784053255, 'validation/ssim': 0.722294229433385, 'validation/loss': 0.28728324104442177, 'validation/num_examples': 3554, 'test/ssim': 0.7396811895682072, 'test/loss': 0.288767329143832, 'test/num_examples': 3581, 'score': 2067.2004284858704, 'total_duration': 2261.1638147830963, 'accumulated_submission_time': 2067.2004284858704, 'accumulated_eval_time': 189.57376384735107, 'accumulated_logging_time': 3.2246878147125244, 'global_step': 21159, 'preemption_count': 0}), (22124, {'train/ssim': 0.7462079184395927, 'train/loss': 0.26698907784053255, 'validation/ssim': 0.722294229433385, 'validation/loss': 0.28728324104442177, 'validation/num_examples': 3554, 'test/ssim': 0.7396811895682072, 'test/loss': 0.288767329143832, 'test/num_examples': 3581, 'score': 2147.1358704566956, 'total_duration': 2345.0592563152313, 'accumulated_submission_time': 2147.1358704566956, 'accumulated_eval_time': 193.4622938632965, 'accumulated_logging_time': 3.2485551834106445, 'global_step': 22124, 'preemption_count': 0}), (23090, {'train/ssim': 0.7462079184395927, 'train/loss': 0.26698907784053255, 'validation/ssim': 0.722294229433385, 'validation/loss': 0.28728324104442177, 'validation/num_examples': 3554, 'test/ssim': 0.7396811895682072, 'test/loss': 0.288767329143832, 'test/num_examples': 3581, 'score': 2227.076051235199, 'total_duration': 2428.953841686249, 'accumulated_submission_time': 2227.076051235199, 'accumulated_eval_time': 197.34807348251343, 'accumulated_logging_time': 3.268991231918335, 'global_step': 23090, 'preemption_count': 0}), (24055, {'train/ssim': 0.7462079184395927, 'train/loss': 0.26698907784053255, 'validation/ssim': 0.722294229433385, 'validation/loss': 0.28728324104442177, 'validation/num_examples': 3554, 'test/ssim': 0.7396811895682072, 'test/loss': 0.288767329143832, 'test/num_examples': 3581, 'score': 2307.010026693344, 'total_duration': 2512.843559026718, 'accumulated_submission_time': 2307.010026693344, 'accumulated_eval_time': 201.23407864570618, 'accumulated_logging_time': 3.2897231578826904, 'global_step': 24055, 'preemption_count': 0}), (25021, {'train/ssim': 0.7462079184395927, 'train/loss': 0.26698907784053255, 'validation/ssim': 0.722294229433385, 'validation/loss': 0.28728324104442177, 'validation/num_examples': 3554, 'test/ssim': 0.7396811895682072, 'test/loss': 0.288767329143832, 'test/num_examples': 3581, 'score': 2386.9936690330505, 'total_duration': 2596.7806215286255, 'accumulated_submission_time': 2386.9936690330505, 'accumulated_eval_time': 205.1133553981781, 'accumulated_logging_time': 3.3141512870788574, 'global_step': 25021, 'preemption_count': 0}), (25987, {'train/ssim': 0.7462079184395927, 'train/loss': 0.26698907784053255, 'validation/ssim': 0.722294229433385, 'validation/loss': 0.28728324104442177, 'validation/num_examples': 3554, 'test/ssim': 0.7396811895682072, 'test/loss': 0.288767329143832, 'test/num_examples': 3581, 'score': 2466.986483812332, 'total_duration': 2680.7314970493317, 'accumulated_submission_time': 2466.986483812332, 'accumulated_eval_time': 209.00127363204956, 'accumulated_logging_time': 3.33595609664917, 'global_step': 25987, 'preemption_count': 0}), (26955, {'train/ssim': 0.7462079184395927, 'train/loss': 0.26698907784053255, 'validation/ssim': 0.722294229433385, 'validation/loss': 0.28728324104442177, 'validation/num_examples': 3554, 'test/ssim': 0.7396811895682072, 'test/loss': 0.288767329143832, 'test/num_examples': 3581, 'score': 2546.973133325577, 'total_duration': 2764.670900583267, 'accumulated_submission_time': 2546.973133325577, 'accumulated_eval_time': 212.88161492347717, 'accumulated_logging_time': 3.3591275215148926, 'global_step': 26955, 'preemption_count': 0}), (27922, {'train/ssim': 0.7462079184395927, 'train/loss': 0.26698907784053255, 'validation/ssim': 0.722294229433385, 'validation/loss': 0.28728324104442177, 'validation/num_examples': 3554, 'test/ssim': 0.7396811895682072, 'test/loss': 0.288767329143832, 'test/num_examples': 3581, 'score': 2626.9760432243347, 'total_duration': 2848.629737138748, 'accumulated_submission_time': 2626.9760432243347, 'accumulated_eval_time': 216.7685523033142, 'accumulated_logging_time': 3.3802478313446045, 'global_step': 27922, 'preemption_count': 0}), (28887, {'train/ssim': 0.7462079184395927, 'train/loss': 0.26698907784053255, 'validation/ssim': 0.722294229433385, 'validation/loss': 0.28728324104442177, 'validation/num_examples': 3554, 'test/ssim': 0.7396811895682072, 'test/loss': 0.288767329143832, 'test/num_examples': 3581, 'score': 2706.9189536571503, 'total_duration': 2932.526736021042, 'accumulated_submission_time': 2706.9189536571503, 'accumulated_eval_time': 220.65193033218384, 'accumulated_logging_time': 3.401975393295288, 'global_step': 28887, 'preemption_count': 0}), (29854, {'train/ssim': 0.7462079184395927, 'train/loss': 0.26698907784053255, 'validation/ssim': 0.722294229433385, 'validation/loss': 0.28728324104442177, 'validation/num_examples': 3554, 'test/ssim': 0.7396811895682072, 'test/loss': 0.288767329143832, 'test/num_examples': 3581, 'score': 2786.8974103927612, 'total_duration': 3016.454841375351, 'accumulated_submission_time': 2786.8974103927612, 'accumulated_eval_time': 224.53304052352905, 'accumulated_logging_time': 3.422224760055542, 'global_step': 29854, 'preemption_count': 0}), (30821, {'train/ssim': 0.7462079184395927, 'train/loss': 0.26698907784053255, 'validation/ssim': 0.722294229433385, 'validation/loss': 0.28728324104442177, 'validation/num_examples': 3554, 'test/ssim': 0.7396811895682072, 'test/loss': 0.288767329143832, 'test/num_examples': 3581, 'score': 2866.8336820602417, 'total_duration': 3100.3451731204987, 'accumulated_submission_time': 2866.8336820602417, 'accumulated_eval_time': 228.41805696487427, 'accumulated_logging_time': 3.4431910514831543, 'global_step': 30821, 'preemption_count': 0}), (31788, {'train/ssim': 0.7462079184395927, 'train/loss': 0.26698907784053255, 'validation/ssim': 0.722294229433385, 'validation/loss': 0.28728324104442177, 'validation/num_examples': 3554, 'test/ssim': 0.7396811895682072, 'test/loss': 0.288767329143832, 'test/num_examples': 3581, 'score': 2946.816180706024, 'total_duration': 3184.283727645874, 'accumulated_submission_time': 2946.816180706024, 'accumulated_eval_time': 232.30500388145447, 'accumulated_logging_time': 3.464566946029663, 'global_step': 31788, 'preemption_count': 0}), (32754, {'train/ssim': 0.7462079184395927, 'train/loss': 0.26698907784053255, 'validation/ssim': 0.722294229433385, 'validation/loss': 0.28728324104442177, 'validation/num_examples': 3554, 'test/ssim': 0.7396811895682072, 'test/loss': 0.288767329143832, 'test/num_examples': 3581, 'score': 3026.7526535987854, 'total_duration': 3268.1703748703003, 'accumulated_submission_time': 3026.7526535987854, 'accumulated_eval_time': 236.1855788230896, 'accumulated_logging_time': 3.4855358600616455, 'global_step': 32754, 'preemption_count': 0}), (33722, {'train/ssim': 0.7462079184395927, 'train/loss': 0.26698907784053255, 'validation/ssim': 0.722294229433385, 'validation/loss': 0.28728324104442177, 'validation/num_examples': 3554, 'test/ssim': 0.7396811895682072, 'test/loss': 0.288767329143832, 'test/num_examples': 3581, 'score': 3106.756071805954, 'total_duration': 3352.1293897628784, 'accumulated_submission_time': 3106.756071805954, 'accumulated_eval_time': 240.0709900856018, 'accumulated_logging_time': 3.5071394443511963, 'global_step': 33722, 'preemption_count': 0}), (34689, {'train/ssim': 0.7462079184395927, 'train/loss': 0.26698907784053255, 'validation/ssim': 0.722294229433385, 'validation/loss': 0.28728324104442177, 'validation/num_examples': 3554, 'test/ssim': 0.7396811895682072, 'test/loss': 0.288767329143832, 'test/num_examples': 3581, 'score': 3186.69092464447, 'total_duration': 3436.0251252651215, 'accumulated_submission_time': 3186.69092464447, 'accumulated_eval_time': 243.95996713638306, 'accumulated_logging_time': 3.5303633213043213, 'global_step': 34689, 'preemption_count': 0}), (35655, {'train/ssim': 0.7462079184395927, 'train/loss': 0.26698907784053255, 'validation/ssim': 0.722294229433385, 'validation/loss': 0.28728324104442177, 'validation/num_examples': 3554, 'test/ssim': 0.7396811895682072, 'test/loss': 0.288767329143832, 'test/num_examples': 3581, 'score': 3266.630596637726, 'total_duration': 3519.920182466507, 'accumulated_submission_time': 3266.630596637726, 'accumulated_eval_time': 247.8449673652649, 'accumulated_logging_time': 3.552405834197998, 'global_step': 35655, 'preemption_count': 0}), (36504, {'train/ssim': 0.7462079184395927, 'train/loss': 0.26698907784053255, 'validation/ssim': 0.722294229433385, 'validation/loss': 0.28728324104442177, 'validation/num_examples': 3554, 'test/ssim': 0.7396811895682072, 'test/loss': 0.288767329143832, 'test/num_examples': 3581, 'score': 3346.599247932434, 'total_duration': 3603.832542657852, 'accumulated_submission_time': 3346.599247932434, 'accumulated_eval_time': 251.72470259666443, 'accumulated_logging_time': 3.574495792388916, 'global_step': 36504, 'preemption_count': 0}), (37470, {'train/ssim': 0.7462079184395927, 'train/loss': 0.26698907784053255, 'validation/ssim': 0.722294229433385, 'validation/loss': 0.28728324104442177, 'validation/num_examples': 3554, 'test/ssim': 0.7396811895682072, 'test/loss': 0.288767329143832, 'test/num_examples': 3581, 'score': 3426.542949438095, 'total_duration': 3687.7450592517853, 'accumulated_submission_time': 3426.542949438095, 'accumulated_eval_time': 255.60917830467224, 'accumulated_logging_time': 3.6115708351135254, 'global_step': 37470, 'preemption_count': 0}), (38437, {'train/ssim': 0.7462079184395927, 'train/loss': 0.26698907784053255, 'validation/ssim': 0.722294229433385, 'validation/loss': 0.28728324104442177, 'validation/num_examples': 3554, 'test/ssim': 0.7396811895682072, 'test/loss': 0.288767329143832, 'test/num_examples': 3581, 'score': 3506.502942085266, 'total_duration': 3771.6643948554993, 'accumulated_submission_time': 3506.502942085266, 'accumulated_eval_time': 259.4984874725342, 'accumulated_logging_time': 3.6332693099975586, 'global_step': 38437, 'preemption_count': 0}), (39405, {'train/ssim': 0.7462079184395927, 'train/loss': 0.26698907784053255, 'validation/ssim': 0.722294229433385, 'validation/loss': 0.28728324104442177, 'validation/num_examples': 3554, 'test/ssim': 0.7396811895682072, 'test/loss': 0.288767329143832, 'test/num_examples': 3581, 'score': 3586.4605283737183, 'total_duration': 3855.58465385437, 'accumulated_submission_time': 3586.4605283737183, 'accumulated_eval_time': 263.3893642425537, 'accumulated_logging_time': 3.655965805053711, 'global_step': 39405, 'preemption_count': 0}), (40372, {'train/ssim': 0.7462079184395927, 'train/loss': 0.26698907784053255, 'validation/ssim': 0.722294229433385, 'validation/loss': 0.28728324104442177, 'validation/num_examples': 3554, 'test/ssim': 0.7396811895682072, 'test/loss': 0.288767329143832, 'test/num_examples': 3581, 'score': 3666.4126176834106, 'total_duration': 3939.4955973625183, 'accumulated_submission_time': 3666.4126176834106, 'accumulated_eval_time': 267.274356842041, 'accumulated_logging_time': 3.682171583175659, 'global_step': 40372, 'preemption_count': 0}), (41340, {'train/ssim': 0.7462079184395927, 'train/loss': 0.26698907784053255, 'validation/ssim': 0.722294229433385, 'validation/loss': 0.28728324104442177, 'validation/num_examples': 3554, 'test/ssim': 0.7396811895682072, 'test/loss': 0.288767329143832, 'test/num_examples': 3581, 'score': 3746.4170277118683, 'total_duration': 4023.4549787044525, 'accumulated_submission_time': 3746.4170277118683, 'accumulated_eval_time': 271.15945506095886, 'accumulated_logging_time': 3.7042229175567627, 'global_step': 41340, 'preemption_count': 0}), (42306, {'train/ssim': 0.7462079184395927, 'train/loss': 0.26698907784053255, 'validation/ssim': 0.722294229433385, 'validation/loss': 0.28728324104442177, 'validation/num_examples': 3554, 'test/ssim': 0.7396811895682072, 'test/loss': 0.288767329143832, 'test/num_examples': 3581, 'score': 3826.3824422359467, 'total_duration': 4107.375155925751, 'accumulated_submission_time': 3826.3824422359467, 'accumulated_eval_time': 275.04347920417786, 'accumulated_logging_time': 3.7267045974731445, 'global_step': 42306, 'preemption_count': 0}), (43274, {'train/ssim': 0.7462079184395927, 'train/loss': 0.26698907784053255, 'validation/ssim': 0.722294229433385, 'validation/loss': 0.28728324104442177, 'validation/num_examples': 3554, 'test/ssim': 0.7396811895682072, 'test/loss': 0.288767329143832, 'test/num_examples': 3581, 'score': 3906.332620859146, 'total_duration': 4191.28120803833, 'accumulated_submission_time': 3906.332620859146, 'accumulated_eval_time': 278.9249110221863, 'accumulated_logging_time': 3.7526423931121826, 'global_step': 43274, 'preemption_count': 0}), (44242, {'train/ssim': 0.7462079184395927, 'train/loss': 0.26698907784053255, 'validation/ssim': 0.722294229433385, 'validation/loss': 0.28728324104442177, 'validation/num_examples': 3554, 'test/ssim': 0.7396811895682072, 'test/loss': 0.288767329143832, 'test/num_examples': 3581, 'score': 3986.3344116210938, 'total_duration': 4275.241221666336, 'accumulated_submission_time': 3986.3344116210938, 'accumulated_eval_time': 282.8122282028198, 'accumulated_logging_time': 3.775925636291504, 'global_step': 44242, 'preemption_count': 0}), (45210, {'train/ssim': 0.7462079184395927, 'train/loss': 0.26698907784053255, 'validation/ssim': 0.722294229433385, 'validation/loss': 0.28728324104442177, 'validation/num_examples': 3554, 'test/ssim': 0.7396811895682072, 'test/loss': 0.288767329143832, 'test/num_examples': 3581, 'score': 4066.3453018665314, 'total_duration': 4359.20433306694, 'accumulated_submission_time': 4066.3453018665314, 'accumulated_eval_time': 286.6945128440857, 'accumulated_logging_time': 3.7985146045684814, 'global_step': 45210, 'preemption_count': 0}), (46177, {'train/ssim': 0.7462079184395927, 'train/loss': 0.26698907784053255, 'validation/ssim': 0.722294229433385, 'validation/loss': 0.28728324104442177, 'validation/num_examples': 3554, 'test/ssim': 0.7396811895682072, 'test/loss': 0.288767329143832, 'test/num_examples': 3581, 'score': 4146.291484117508, 'total_duration': 4443.104011297226, 'accumulated_submission_time': 4146.291484117508, 'accumulated_eval_time': 290.57883977890015, 'accumulated_logging_time': 3.8214449882507324, 'global_step': 46177, 'preemption_count': 0}), (47145, {'train/ssim': 0.7462079184395927, 'train/loss': 0.26698907784053255, 'validation/ssim': 0.722294229433385, 'validation/loss': 0.28728324104442177, 'validation/num_examples': 3554, 'test/ssim': 0.7396811895682072, 'test/loss': 0.288767329143832, 'test/num_examples': 3581, 'score': 4226.266886234283, 'total_duration': 4527.034152746201, 'accumulated_submission_time': 4226.266886234283, 'accumulated_eval_time': 294.4641377925873, 'accumulated_logging_time': 3.8446476459503174, 'global_step': 47145, 'preemption_count': 0}), (48113, {'train/ssim': 0.7462079184395927, 'train/loss': 0.26698907784053255, 'validation/ssim': 0.722294229433385, 'validation/loss': 0.28728324104442177, 'validation/num_examples': 3554, 'test/ssim': 0.7396811895682072, 'test/loss': 0.288767329143832, 'test/num_examples': 3581, 'score': 4306.271164894104, 'total_duration': 4610.996920824051, 'accumulated_submission_time': 4306.271164894104, 'accumulated_eval_time': 298.3546690940857, 'accumulated_logging_time': 3.8678138256073, 'global_step': 48113, 'preemption_count': 0}), (49081, {'train/ssim': 0.7462079184395927, 'train/loss': 0.26698907784053255, 'validation/ssim': 0.722294229433385, 'validation/loss': 0.28728324104442177, 'validation/num_examples': 3554, 'test/ssim': 0.7396811895682072, 'test/loss': 0.288767329143832, 'test/num_examples': 3581, 'score': 4386.211649894714, 'total_duration': 4694.891733407974, 'accumulated_submission_time': 4386.211649894714, 'accumulated_eval_time': 302.24322533607483, 'accumulated_logging_time': 3.890458106994629, 'global_step': 49081, 'preemption_count': 0})], 'global_step': 50048}
I0307 17:12:09.973643 140616853095616 submission_runner.py:649] Timing: 4466.187980890274
I0307 17:12:09.973683 140616853095616 submission_runner.py:651] Total number of evals: 53
I0307 17:12:09.973713 140616853095616 submission_runner.py:652] ====================
I0307 17:12:09.973887 140616853095616 submission_runner.py:750] Final fastmri score: 4
