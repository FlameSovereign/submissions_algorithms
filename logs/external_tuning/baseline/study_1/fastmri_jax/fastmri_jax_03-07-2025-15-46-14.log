python submission_runner.py --framework=jax --workload=fastmri --submission_path=prize_qualification_baselines/external_tuning/jax_nadamw_full_budget.py --data_dir=/data/fastmri --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=submissions/rolling_leaderboard/external_tuning/baseline/study_1 --overwrite=True --save_checkpoints=False --rng_seed=1022998887 --tuning_ruleset=external --tuning_search_space=prize_qualification_baselines/external_tuning/tuning_search_space.json --num_tuning_trials=5 --hparam_start_index=0 --hparam_end_index=1 2>&1 | tee -a /logs/fastmri_jax_03-07-2025-15-46-14.log
2025-03-07 15:46:19.345980: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1741362379.372755       9 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1741362379.389506       9 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
I0307 15:46:51.376635 139912252507328 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_1/fastmri_jax.
I0307 15:46:53.115817 139912252507328 xla_bridge.py:884] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA
I0307 15:46:53.119067 139912252507328 xla_bridge.py:884] Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory
I0307 15:46:53.131666 139912252507328 submission_runner.py:606] Using RNG seed 1022998887
I0307 15:46:53.899622 139912252507328 submission_runner.py:615] --- Tuning run 1/5 ---
I0307 15:46:53.899838 139912252507328 submission_runner.py:620] Creating tuning directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_1/fastmri_jax/trial_1.
I0307 15:46:53.900065 139912252507328 logger_utils.py:97] Saving hparams to /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_1/fastmri_jax/trial_1/hparams.json.
I0307 15:46:54.133845 139912252507328 submission_runner.py:218] Initializing dataset.
I0307 15:46:58.698671 139912252507328 submission_runner.py:229] Initializing model.
I0307 15:47:08.671183 139912252507328 submission_runner.py:272] Initializing optimizer.
I0307 15:47:09.150600 139912252507328 submission_runner.py:279] Initializing metrics bundle.
I0307 15:47:09.150818 139912252507328 submission_runner.py:301] Initializing checkpoint and logger.
I0307 15:47:09.151564 139912252507328 checkpoints.py:1101] Found no checkpoint files in /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_1/fastmri_jax/trial_1 with prefix checkpoint_
I0307 15:47:09.151672 139912252507328 submission_runner.py:321] Saving meta data to /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_1/fastmri_jax/trial_1/meta_data_0.json.
I0307 15:47:09.151834 139912252507328 logger_utils.py:262] Unable to record workload.train_mean information. Continuing without it.
I0307 15:47:09.151884 139912252507328 logger_utils.py:262] Unable to record workload.train_stddev information. Continuing without it.
I0307 15:47:09.439917 139912252507328 submission_runner.py:325] Saving flags to /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_1/fastmri_jax/trial_1/flags_0.json.
I0307 15:47:09.616996 139912252507328 submission_runner.py:337] Starting training loop.
E0307 15:51:02.967195       9 gpu_timer.cc:183] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.
E0307 15:51:03.174834       9 gpu_timer.cc:183] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.
E0307 15:51:03.582966       9 gpu_timer.cc:183] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.
E0307 15:51:03.790025       9 gpu_timer.cc:183] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.
E0307 15:51:05.652858       9 gpu_timer.cc:183] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.
E0307 15:51:05.859400       9 gpu_timer.cc:183] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.
I0307 15:51:17.517401 139774006392576 logging_writer.py:48] [0] global_step=0, grad_norm=3.4889917373657227, loss=0.9686681032180786
I0307 15:51:17.601956 139912252507328 spec.py:321] Evaluating on the training split.
I0307 15:52:43.083770 139912252507328 spec.py:333] Evaluating on the validation split.
I0307 15:53:07.085731 139912252507328 spec.py:349] Evaluating on the test split.
I0307 15:53:29.433199 139912252507328 submission_runner.py:469] Time since start: 379.82s, 	Step: 1, 	{'train/ssim': 0.24137336867196219, 'train/loss': 1.0202210971287318, 'validation/ssim': 0.23160899456927933, 'validation/loss': 1.0241995979442178, 'validation/num_examples': 3554, 'test/ssim': 0.2532209552422944, 'test/loss': 1.02164786263439, 'test/num_examples': 3581, 'score': 247.98478198051453, 'total_duration': 379.81613063812256, 'accumulated_submission_time': 247.98478198051453, 'accumulated_eval_time': 131.83117365837097, 'accumulated_logging_time': 0}
I0307 15:53:29.441846 139752690919168 logging_writer.py:48] [1] accumulated_eval_time=131.831, accumulated_logging_time=0, accumulated_submission_time=247.985, global_step=1, preemption_count=0, score=247.985, test/loss=1.02165, test/num_examples=3581, test/ssim=0.253221, total_duration=379.816, train/loss=1.02022, train/ssim=0.241373, validation/loss=1.0242, validation/num_examples=3554, validation/ssim=0.231609
I0307 15:53:41.802018 139752682526464 logging_writer.py:48] [100] global_step=100, grad_norm=0.6078549027442932, loss=0.3998565673828125
I0307 15:53:59.040625 139752690919168 logging_writer.py:48] [200] global_step=200, grad_norm=0.18787910044193268, loss=0.35258781909942627
I0307 15:54:15.784601 139752682526464 logging_writer.py:48] [300] global_step=300, grad_norm=0.12845587730407715, loss=0.38054826855659485
I0307 15:54:34.474888 139752690919168 logging_writer.py:48] [400] global_step=400, grad_norm=0.3233213722705841, loss=0.3543770909309387
I0307 15:54:49.584284 139912252507328 spec.py:321] Evaluating on the training split.
I0307 15:54:51.168003 139912252507328 spec.py:333] Evaluating on the validation split.
I0307 15:54:52.457169 139912252507328 spec.py:349] Evaluating on the test split.
I0307 15:54:53.753224 139912252507328 submission_runner.py:469] Time since start: 464.14s, 	Step: 480, 	{'train/ssim': 0.7047012192862374, 'train/loss': 0.30967794145856586, 'validation/ssim': 0.6842017659459412, 'validation/loss': 0.32224958480980936, 'validation/num_examples': 3554, 'test/ssim': 0.7025727690414688, 'test/loss': 0.32391935219081613, 'test/num_examples': 3581, 'score': 328.04527139663696, 'total_duration': 464.1361770629883, 'accumulated_submission_time': 328.04527139663696, 'accumulated_eval_time': 136.00006890296936, 'accumulated_logging_time': 0.017880678176879883}
I0307 15:54:53.763415 139752682526464 logging_writer.py:48] [480] accumulated_eval_time=136, accumulated_logging_time=0.0178807, accumulated_submission_time=328.045, global_step=480, preemption_count=0, score=328.045, test/loss=0.323919, test/num_examples=3581, test/ssim=0.702573, total_duration=464.136, train/loss=0.309678, train/ssim=0.704701, validation/loss=0.32225, validation/num_examples=3554, validation/ssim=0.684202
I0307 15:54:55.530226 139752690919168 logging_writer.py:48] [500] global_step=500, grad_norm=0.13470645248889923, loss=0.3371794819831848
I0307 15:55:12.351150 139752682526464 logging_writer.py:48] [600] global_step=600, grad_norm=0.29884859919548035, loss=0.3219527006149292
I0307 15:55:32.433357 139752690919168 logging_writer.py:48] [700] global_step=700, grad_norm=0.20428766310214996, loss=0.3093814253807068
I0307 15:55:50.434715 139752682526464 logging_writer.py:48] [800] global_step=800, grad_norm=0.3067514896392822, loss=0.23631222546100616
I0307 15:56:09.094476 139752690919168 logging_writer.py:48] [900] global_step=900, grad_norm=0.13488878309726715, loss=0.2948446273803711
I0307 15:56:13.953342 139912252507328 spec.py:321] Evaluating on the training split.
I0307 15:56:15.237949 139912252507328 spec.py:333] Evaluating on the validation split.
I0307 15:56:16.608604 139912252507328 spec.py:349] Evaluating on the test split.
I0307 15:56:17.906502 139912252507328 submission_runner.py:469] Time since start: 548.29s, 	Step: 926, 	{'train/ssim': 0.7254939079284668, 'train/loss': 0.290813718523298, 'validation/ssim': 0.705199989228686, 'validation/loss': 0.3030659295094436, 'validation/num_examples': 3554, 'test/ssim': 0.7223669764250559, 'test/loss': 0.3052390491919157, 'test/num_examples': 3581, 'score': 408.1468822956085, 'total_duration': 548.2894542217255, 'accumulated_submission_time': 408.1468822956085, 'accumulated_eval_time': 139.9531853199005, 'accumulated_logging_time': 0.04774785041809082}
I0307 15:56:17.946602 139752682526464 logging_writer.py:48] [926] accumulated_eval_time=139.953, accumulated_logging_time=0.0477479, accumulated_submission_time=408.147, global_step=926, preemption_count=0, score=408.147, test/loss=0.305239, test/num_examples=3581, test/ssim=0.722367, total_duration=548.289, train/loss=0.290814, train/ssim=0.725494, validation/loss=0.303066, validation/num_examples=3554, validation/ssim=0.7052
I0307 15:56:25.110880 139752690919168 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.20262987911701202, loss=0.34805092215538025
I0307 15:56:33.412702 139752682526464 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.08333883434534073, loss=0.2447691261768341
I0307 15:56:41.647946 139752690919168 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.17675048112869263, loss=0.38133177161216736
I0307 15:56:49.907580 139752682526464 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.5850726366043091, loss=0.1934025138616562
I0307 15:56:58.188518 139752690919168 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.1714305728673935, loss=0.23308420181274414
I0307 15:57:06.454915 139752682526464 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.13228702545166016, loss=0.2818480432033539
I0307 15:57:14.724529 139752690919168 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.34098631143569946, loss=0.23834645748138428
I0307 15:57:22.978098 139752682526464 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.12231071293354034, loss=0.2760484218597412
I0307 15:57:31.258704 139752690919168 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.21073898673057556, loss=0.23482608795166016
I0307 15:57:37.950411 139912252507328 spec.py:321] Evaluating on the training split.
I0307 15:57:39.241919 139912252507328 spec.py:333] Evaluating on the validation split.
I0307 15:57:40.531513 139912252507328 spec.py:349] Evaluating on the test split.
I0307 15:57:41.831302 139912252507328 submission_runner.py:469] Time since start: 632.21s, 	Step: 1882, 	{'train/ssim': 0.7359660012381417, 'train/loss': 0.2811460154397147, 'validation/ssim': 0.7154753281953433, 'validation/loss': 0.29389801626468415, 'validation/num_examples': 3554, 'test/ssim': 0.7326504031695057, 'test/loss': 0.2956547741116308, 'test/num_examples': 3581, 'score': 488.07775115966797, 'total_duration': 632.2142467498779, 'accumulated_submission_time': 488.07775115966797, 'accumulated_eval_time': 143.83402633666992, 'accumulated_logging_time': 0.10880160331726074}
I0307 15:57:41.840000 139752682526464 logging_writer.py:48] [1882] accumulated_eval_time=143.834, accumulated_logging_time=0.108802, accumulated_submission_time=488.078, global_step=1882, preemption_count=0, score=488.078, test/loss=0.295655, test/num_examples=3581, test/ssim=0.73265, total_duration=632.214, train/loss=0.281146, train/ssim=0.735966, validation/loss=0.293898, validation/num_examples=3554, validation/ssim=0.715475
I0307 15:57:43.420694 139752690919168 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.2220688760280609, loss=0.25062721967697144
I0307 15:57:51.661591 139752682526464 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.3062995970249176, loss=0.2732759118080139
I0307 15:57:59.905576 139752690919168 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.19808360934257507, loss=0.2825685739517212
I0307 15:58:08.124187 139752682526464 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.07099384069442749, loss=0.29348382353782654
I0307 15:58:16.356425 139752690919168 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.3505852222442627, loss=0.24637086689472198
I0307 15:58:24.593517 139752682526464 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.2627018094062805, loss=0.23186162114143372
I0307 15:58:32.853911 139752690919168 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.05738241598010063, loss=0.31402167677879333
I0307 15:58:41.108136 139752682526464 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.2138708084821701, loss=0.3046952784061432
I0307 15:58:49.347259 139752690919168 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.2912696301937103, loss=0.294828325510025
I0307 15:58:57.579097 139752682526464 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.5760137438774109, loss=0.2379029095172882
I0307 15:59:01.865177 139912252507328 spec.py:321] Evaluating on the training split.
I0307 15:59:03.155209 139912252507328 spec.py:333] Evaluating on the validation split.
I0307 15:59:04.446691 139912252507328 spec.py:349] Evaluating on the test split.
I0307 15:59:05.743628 139912252507328 submission_runner.py:469] Time since start: 716.13s, 	Step: 2853, 	{'train/ssim': 0.7400665283203125, 'train/loss': 0.2779160567692348, 'validation/ssim': 0.7186902356939364, 'validation/loss': 0.29128741520338, 'validation/num_examples': 3554, 'test/ssim': 0.735826140241029, 'test/loss': 0.2929528989261903, 'test/num_examples': 3581, 'score': 568.0491299629211, 'total_duration': 716.1265664100647, 'accumulated_submission_time': 568.0491299629211, 'accumulated_eval_time': 147.7124228477478, 'accumulated_logging_time': 0.12505269050598145}
I0307 15:59:05.752768 139752690919168 logging_writer.py:48] [2853] accumulated_eval_time=147.712, accumulated_logging_time=0.125053, accumulated_submission_time=568.049, global_step=2853, preemption_count=0, score=568.049, test/loss=0.292953, test/num_examples=3581, test/ssim=0.735826, total_duration=716.127, train/loss=0.277916, train/ssim=0.740067, validation/loss=0.291287, validation/num_examples=3554, validation/ssim=0.71869
I0307 15:59:09.733654 139752682526464 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.24332435429096222, loss=0.27823615074157715
I0307 15:59:18.024247 139752690919168 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.11925183981657028, loss=0.32429730892181396
I0307 15:59:26.271603 139752682526464 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.41507038474082947, loss=0.22928789258003235
I0307 15:59:34.523453 139752690919168 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.15435005724430084, loss=0.2710839807987213
I0307 15:59:42.786048 139752682526464 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.1433103382587433, loss=0.1876080483198166
I0307 15:59:51.013597 139752690919168 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.21087181568145752, loss=0.2723948359489441
I0307 15:59:59.264400 139752682526464 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.05668932572007179, loss=0.2372955083847046
I0307 16:00:07.500852 139752690919168 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.20375171303749084, loss=0.27846258878707886
I0307 16:00:15.758171 139752682526464 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.5608984231948853, loss=0.2482576072216034
I0307 16:00:23.995915 139752690919168 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.14527039229869843, loss=0.30160441994667053
I0307 16:00:25.814912 139912252507328 spec.py:321] Evaluating on the training split.
I0307 16:00:27.105668 139912252507328 spec.py:333] Evaluating on the validation split.
I0307 16:00:28.398800 139912252507328 spec.py:349] Evaluating on the test split.
I0307 16:00:29.697309 139912252507328 submission_runner.py:469] Time since start: 800.08s, 	Step: 3823, 	{'train/ssim': 0.7421739442007882, 'train/loss': 0.27585242475782124, 'validation/ssim': 0.7206674725001758, 'validation/loss': 0.28941352942986776, 'validation/num_examples': 3554, 'test/ssim': 0.7380208151528903, 'test/loss': 0.2909111783545099, 'test/num_examples': 3581, 'score': 648.0575385093689, 'total_duration': 800.0802555084229, 'accumulated_submission_time': 648.0575385093689, 'accumulated_eval_time': 151.59477138519287, 'accumulated_logging_time': 0.14209413528442383}
I0307 16:00:29.706326 139752682526464 logging_writer.py:48] [3823] accumulated_eval_time=151.595, accumulated_logging_time=0.142094, accumulated_submission_time=648.058, global_step=3823, preemption_count=0, score=648.058, test/loss=0.290911, test/num_examples=3581, test/ssim=0.738021, total_duration=800.08, train/loss=0.275852, train/ssim=0.742174, validation/loss=0.289414, validation/num_examples=3554, validation/ssim=0.720667
I0307 16:00:36.141456 139752690919168 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.13829775154590607, loss=0.3265822231769562
I0307 16:00:44.372322 139752682526464 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.27374541759490967, loss=0.22716565430164337
I0307 16:00:52.602298 139752690919168 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.15167641639709473, loss=0.24405747652053833
I0307 16:01:00.835385 139752682526464 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.3027120530605316, loss=0.30691245198249817
I0307 16:01:09.046899 139752690919168 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.7832428812980652, loss=0.20099914073944092
I0307 16:01:17.278650 139752682526464 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.25042223930358887, loss=0.27550914883613586
I0307 16:01:25.523880 139752690919168 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.11637996882200241, loss=0.2838597595691681
I0307 16:01:33.768460 139752682526464 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.13973407447338104, loss=0.3293250501155853
I0307 16:01:42.004346 139752690919168 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.15162362158298492, loss=0.2659575641155243
I0307 16:01:49.744052 139912252507328 spec.py:321] Evaluating on the training split.
I0307 16:01:51.034366 139912252507328 spec.py:333] Evaluating on the validation split.
I0307 16:01:52.330015 139912252507328 spec.py:349] Evaluating on the test split.
I0307 16:01:53.628675 139912252507328 submission_runner.py:469] Time since start: 884.01s, 	Step: 4795, 	{'train/ssim': 0.7432580675397601, 'train/loss': 0.27445125579833984, 'validation/ssim': 0.7218427684914884, 'validation/loss': 0.2880105451713738, 'validation/num_examples': 3554, 'test/ssim': 0.7390431923694498, 'test/loss': 0.2895097388997487, 'test/num_examples': 3581, 'score': 728.041083574295, 'total_duration': 884.0116300582886, 'accumulated_submission_time': 728.041083574295, 'accumulated_eval_time': 155.47935247421265, 'accumulated_logging_time': 0.15945744514465332}
I0307 16:01:53.638133 139752682526464 logging_writer.py:48] [4795] accumulated_eval_time=155.479, accumulated_logging_time=0.159457, accumulated_submission_time=728.041, global_step=4795, preemption_count=0, score=728.041, test/loss=0.28951, test/num_examples=3581, test/ssim=0.739043, total_duration=884.012, train/loss=0.274451, train/ssim=0.743258, validation/loss=0.288011, validation/num_examples=3554, validation/ssim=0.721843
I0307 16:01:54.149617 139752690919168 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.13070575892925262, loss=0.26141223311424255
I0307 16:02:02.402959 139752682526464 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.0617142952978611, loss=0.2719127833843231
I0307 16:02:10.632752 139752690919168 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.14474697411060333, loss=0.3199039399623871
I0307 16:02:18.871663 139752682526464 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.42428475618362427, loss=0.3401185870170593
I0307 16:02:27.108654 139752690919168 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.11346004903316498, loss=0.23887312412261963
I0307 16:02:35.371684 139752682526464 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.09754455834627151, loss=0.3274743854999542
I0307 16:02:43.617959 139752690919168 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.07725736498832703, loss=0.24685002863407135
I0307 16:02:51.882534 139752682526464 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.1527933031320572, loss=0.2520793080329895
I0307 16:03:00.142532 139752690919168 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.12872320413589478, loss=0.2295224964618683
I0307 16:03:08.372154 139752682526464 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.07518167048692703, loss=0.2780631482601166
I0307 16:03:13.654773 139912252507328 spec.py:321] Evaluating on the training split.
I0307 16:03:14.944880 139912252507328 spec.py:333] Evaluating on the validation split.
I0307 16:03:16.240089 139912252507328 spec.py:349] Evaluating on the test split.
I0307 16:03:17.534406 139912252507328 submission_runner.py:469] Time since start: 967.92s, 	Step: 5765, 	{'train/ssim': 0.7444962092808315, 'train/loss': 0.2742177588599069, 'validation/ssim': 0.7225696947981148, 'validation/loss': 0.2881673749538372, 'validation/num_examples': 3554, 'test/ssim': 0.7397428212702457, 'test/loss': 0.289601811481168, 'test/num_examples': 3581, 'score': 808.0046212673187, 'total_duration': 967.9173603057861, 'accumulated_submission_time': 808.0046212673187, 'accumulated_eval_time': 159.35894298553467, 'accumulated_logging_time': 0.1769089698791504}
I0307 16:03:17.543923 139752690919168 logging_writer.py:48] [5765] accumulated_eval_time=159.359, accumulated_logging_time=0.176909, accumulated_submission_time=808.005, global_step=5765, preemption_count=0, score=808.005, test/loss=0.289602, test/num_examples=3581, test/ssim=0.739743, total_duration=967.917, train/loss=0.274218, train/ssim=0.744496, validation/loss=0.288167, validation/num_examples=3554, validation/ssim=0.72257
I0307 16:03:20.529835 139752682526464 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.25591468811035156, loss=0.3375859260559082
I0307 16:03:28.773461 139752690919168 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.06475020945072174, loss=0.2826387286186218
I0307 16:03:37.020456 139752682526464 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.07813543826341629, loss=0.28196457028388977
I0307 16:03:45.252057 139752690919168 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.12739886343479156, loss=0.2645256519317627
I0307 16:03:53.506159 139752682526464 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.2867945730686188, loss=0.26122429966926575
I0307 16:04:01.770625 139752690919168 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.29978159070014954, loss=0.27362769842147827
I0307 16:04:10.023571 139752682526464 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.09181728214025497, loss=0.22626474499702454
I0307 16:04:18.264928 139752690919168 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.5668622851371765, loss=0.28330689668655396
I0307 16:04:26.496679 139752682526464 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.11600189656019211, loss=0.24129009246826172
I0307 16:04:34.746737 139752690919168 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.10465434938669205, loss=0.25164321064949036
I0307 16:04:37.555874 139912252507328 spec.py:321] Evaluating on the training split.
I0307 16:04:38.843422 139912252507328 spec.py:333] Evaluating on the validation split.
I0307 16:04:40.138265 139912252507328 spec.py:349] Evaluating on the test split.
I0307 16:04:41.434442 139912252507328 submission_runner.py:469] Time since start: 1051.82s, 	Step: 6735, 	{'train/ssim': 0.7451160975864956, 'train/loss': 0.2733462027141026, 'validation/ssim': 0.7232734023283625, 'validation/loss': 0.2873321344292962, 'validation/num_examples': 3554, 'test/ssim': 0.7405440333967467, 'test/loss': 0.2887389335642977, 'test/num_examples': 3581, 'score': 887.9624531269073, 'total_duration': 1051.8173995018005, 'accumulated_submission_time': 887.9624531269073, 'accumulated_eval_time': 163.2374711036682, 'accumulated_logging_time': 0.1942756175994873}
I0307 16:04:41.443866 139752682526464 logging_writer.py:48] [6735] accumulated_eval_time=163.237, accumulated_logging_time=0.194276, accumulated_submission_time=887.962, global_step=6735, preemption_count=0, score=887.962, test/loss=0.288739, test/num_examples=3581, test/ssim=0.740544, total_duration=1051.82, train/loss=0.273346, train/ssim=0.745116, validation/loss=0.287332, validation/num_examples=3554, validation/ssim=0.723273
I0307 16:04:46.895574 139752690919168 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.10133421421051025, loss=0.2633441686630249
I0307 16:04:55.130567 139752682526464 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.13720838725566864, loss=0.3618335723876953
I0307 16:05:03.346349 139752690919168 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.23519961535930634, loss=0.23167866468429565
I0307 16:05:11.588468 139752682526464 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.14624671638011932, loss=0.2648085951805115
I0307 16:05:19.830373 139752690919168 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.07560162991285324, loss=0.29259687662124634
I0307 16:05:28.055066 139752682526464 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.23553752899169922, loss=0.26885151863098145
I0307 16:05:36.286033 139752690919168 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.389074444770813, loss=0.22813260555267334
I0307 16:05:44.535321 139752682526464 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.0925668329000473, loss=0.32668983936309814
I0307 16:05:52.778357 139752690919168 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.20354366302490234, loss=0.25442585349082947
I0307 16:06:01.014657 139752682526464 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.13040633499622345, loss=0.26625150442123413
I0307 16:06:01.516187 139912252507328 spec.py:321] Evaluating on the training split.
I0307 16:06:02.807079 139912252507328 spec.py:333] Evaluating on the validation split.
I0307 16:06:04.098272 139912252507328 spec.py:349] Evaluating on the test split.
I0307 16:06:05.394711 139912252507328 submission_runner.py:469] Time since start: 1135.78s, 	Step: 7707, 	{'train/ssim': 0.7462079865591866, 'train/loss': 0.27277135848999023, 'validation/ssim': 0.7240592686057963, 'validation/loss': 0.2869631585339582, 'validation/num_examples': 3554, 'test/ssim': 0.7412986126867495, 'test/loss': 0.2883611666826655, 'test/num_examples': 3581, 'score': 967.9812605381012, 'total_duration': 1135.7776627540588, 'accumulated_submission_time': 967.9812605381012, 'accumulated_eval_time': 167.1159462928772, 'accumulated_logging_time': 0.21207189559936523}
I0307 16:06:05.405089 139752690919168 logging_writer.py:48] [7707] accumulated_eval_time=167.116, accumulated_logging_time=0.212072, accumulated_submission_time=967.981, global_step=7707, preemption_count=0, score=967.981, test/loss=0.288361, test/num_examples=3581, test/ssim=0.741299, total_duration=1135.78, train/loss=0.272771, train/ssim=0.746208, validation/loss=0.286963, validation/num_examples=3554, validation/ssim=0.724059
I0307 16:06:05.417508 139752682526464 logging_writer.py:48] [7707] global_step=7707, preemption_count=0, score=967.981
I0307 16:06:06.281721 139912252507328 submission_runner.py:646] Tuning trial 1/5
I0307 16:06:06.281918 139912252507328 submission_runner.py:647] Hyperparameters: Hyperparameters(dropout_rate=0.0, label_smoothing=0.1, learning_rate=0.001308209823469072, one_minus_beta1=0.02686663061, beta2=0.9981232922116359, weight_decay=0.16375311233774334, warmup_factor=0.1)
I0307 16:06:06.282601 139912252507328 submission_runner.py:648] Metrics: {'eval_results': [(1, {'train/ssim': 0.24137336867196219, 'train/loss': 1.0202210971287318, 'validation/ssim': 0.23160899456927933, 'validation/loss': 1.0241995979442178, 'validation/num_examples': 3554, 'test/ssim': 0.2532209552422944, 'test/loss': 1.02164786263439, 'test/num_examples': 3581, 'score': 247.98478198051453, 'total_duration': 379.81613063812256, 'accumulated_submission_time': 247.98478198051453, 'accumulated_eval_time': 131.83117365837097, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (480, {'train/ssim': 0.7047012192862374, 'train/loss': 0.30967794145856586, 'validation/ssim': 0.6842017659459412, 'validation/loss': 0.32224958480980936, 'validation/num_examples': 3554, 'test/ssim': 0.7025727690414688, 'test/loss': 0.32391935219081613, 'test/num_examples': 3581, 'score': 328.04527139663696, 'total_duration': 464.1361770629883, 'accumulated_submission_time': 328.04527139663696, 'accumulated_eval_time': 136.00006890296936, 'accumulated_logging_time': 0.017880678176879883, 'global_step': 480, 'preemption_count': 0}), (926, {'train/ssim': 0.7254939079284668, 'train/loss': 0.290813718523298, 'validation/ssim': 0.705199989228686, 'validation/loss': 0.3030659295094436, 'validation/num_examples': 3554, 'test/ssim': 0.7223669764250559, 'test/loss': 0.3052390491919157, 'test/num_examples': 3581, 'score': 408.1468822956085, 'total_duration': 548.2894542217255, 'accumulated_submission_time': 408.1468822956085, 'accumulated_eval_time': 139.9531853199005, 'accumulated_logging_time': 0.04774785041809082, 'global_step': 926, 'preemption_count': 0}), (1882, {'train/ssim': 0.7359660012381417, 'train/loss': 0.2811460154397147, 'validation/ssim': 0.7154753281953433, 'validation/loss': 0.29389801626468415, 'validation/num_examples': 3554, 'test/ssim': 0.7326504031695057, 'test/loss': 0.2956547741116308, 'test/num_examples': 3581, 'score': 488.07775115966797, 'total_duration': 632.2142467498779, 'accumulated_submission_time': 488.07775115966797, 'accumulated_eval_time': 143.83402633666992, 'accumulated_logging_time': 0.10880160331726074, 'global_step': 1882, 'preemption_count': 0}), (2853, {'train/ssim': 0.7400665283203125, 'train/loss': 0.2779160567692348, 'validation/ssim': 0.7186902356939364, 'validation/loss': 0.29128741520338, 'validation/num_examples': 3554, 'test/ssim': 0.735826140241029, 'test/loss': 0.2929528989261903, 'test/num_examples': 3581, 'score': 568.0491299629211, 'total_duration': 716.1265664100647, 'accumulated_submission_time': 568.0491299629211, 'accumulated_eval_time': 147.7124228477478, 'accumulated_logging_time': 0.12505269050598145, 'global_step': 2853, 'preemption_count': 0}), (3823, {'train/ssim': 0.7421739442007882, 'train/loss': 0.27585242475782124, 'validation/ssim': 0.7206674725001758, 'validation/loss': 0.28941352942986776, 'validation/num_examples': 3554, 'test/ssim': 0.7380208151528903, 'test/loss': 0.2909111783545099, 'test/num_examples': 3581, 'score': 648.0575385093689, 'total_duration': 800.0802555084229, 'accumulated_submission_time': 648.0575385093689, 'accumulated_eval_time': 151.59477138519287, 'accumulated_logging_time': 0.14209413528442383, 'global_step': 3823, 'preemption_count': 0}), (4795, {'train/ssim': 0.7432580675397601, 'train/loss': 0.27445125579833984, 'validation/ssim': 0.7218427684914884, 'validation/loss': 0.2880105451713738, 'validation/num_examples': 3554, 'test/ssim': 0.7390431923694498, 'test/loss': 0.2895097388997487, 'test/num_examples': 3581, 'score': 728.041083574295, 'total_duration': 884.0116300582886, 'accumulated_submission_time': 728.041083574295, 'accumulated_eval_time': 155.47935247421265, 'accumulated_logging_time': 0.15945744514465332, 'global_step': 4795, 'preemption_count': 0}), (5765, {'train/ssim': 0.7444962092808315, 'train/loss': 0.2742177588599069, 'validation/ssim': 0.7225696947981148, 'validation/loss': 0.2881673749538372, 'validation/num_examples': 3554, 'test/ssim': 0.7397428212702457, 'test/loss': 0.289601811481168, 'test/num_examples': 3581, 'score': 808.0046212673187, 'total_duration': 967.9173603057861, 'accumulated_submission_time': 808.0046212673187, 'accumulated_eval_time': 159.35894298553467, 'accumulated_logging_time': 0.1769089698791504, 'global_step': 5765, 'preemption_count': 0}), (6735, {'train/ssim': 0.7451160975864956, 'train/loss': 0.2733462027141026, 'validation/ssim': 0.7232734023283625, 'validation/loss': 0.2873321344292962, 'validation/num_examples': 3554, 'test/ssim': 0.7405440333967467, 'test/loss': 0.2887389335642977, 'test/num_examples': 3581, 'score': 887.9624531269073, 'total_duration': 1051.8173995018005, 'accumulated_submission_time': 887.9624531269073, 'accumulated_eval_time': 163.2374711036682, 'accumulated_logging_time': 0.1942756175994873, 'global_step': 6735, 'preemption_count': 0}), (7707, {'train/ssim': 0.7462079865591866, 'train/loss': 0.27277135848999023, 'validation/ssim': 0.7240592686057963, 'validation/loss': 0.2869631585339582, 'validation/num_examples': 3554, 'test/ssim': 0.7412986126867495, 'test/loss': 0.2883611666826655, 'test/num_examples': 3581, 'score': 967.9812605381012, 'total_duration': 1135.7776627540588, 'accumulated_submission_time': 967.9812605381012, 'accumulated_eval_time': 167.1159462928772, 'accumulated_logging_time': 0.21207189559936523, 'global_step': 7707, 'preemption_count': 0})], 'global_step': 7707}
I0307 16:06:06.282681 139912252507328 submission_runner.py:649] Timing: 967.9812605381012
I0307 16:06:06.282716 139912252507328 submission_runner.py:651] Total number of evals: 10
I0307 16:06:06.282747 139912252507328 submission_runner.py:652] ====================
I0307 16:06:06.282848 139912252507328 submission_runner.py:750] Final fastmri score: 0
