python submission_runner.py --framework=jax --workload=fastmri --submission_path=prize_qualification_baselines/external_tuning/jax_nadamw_full_budget.py --data_dir=/data/fastmri --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=submissions/rolling_leaderboard/external_tuning/baseline/study_1 --overwrite=True --save_checkpoints=False --rng_seed=-212674511 --tuning_ruleset=external --tuning_search_space=prize_qualification_baselines/external_tuning/tuning_search_space.json --num_tuning_trials=5 --hparam_start_index=2 --hparam_end_index=3 2>&1 | tee -a /logs/fastmri_jax_03-07-2025-15-47-22.log
2025-03-07 15:47:34.896149: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1741362454.929124       9 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1741362454.940876       9 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
I0307 15:47:54.884631 140313937978560 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_1/fastmri_jax.
I0307 15:47:56.020026 140313937978560 xla_bridge.py:884] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA
I0307 15:47:56.022905 140313937978560 xla_bridge.py:884] Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory
I0307 15:47:56.024633 140313937978560 submission_runner.py:606] Using RNG seed -212674511
I0307 15:47:56.669694 140313937978560 submission_runner.py:615] --- Tuning run 3/5 ---
I0307 15:47:56.669854 140313937978560 submission_runner.py:620] Creating tuning directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_1/fastmri_jax/trial_3.
I0307 15:47:56.670020 140313937978560 logger_utils.py:97] Saving hparams to /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_1/fastmri_jax/trial_3/hparams.json.
I0307 15:47:56.892889 140313937978560 submission_runner.py:218] Initializing dataset.
I0307 15:48:01.356496 140313937978560 submission_runner.py:229] Initializing model.
I0307 15:48:11.249260 140313937978560 submission_runner.py:272] Initializing optimizer.
I0307 15:48:11.726000 140313937978560 submission_runner.py:279] Initializing metrics bundle.
I0307 15:48:11.726205 140313937978560 submission_runner.py:301] Initializing checkpoint and logger.
I0307 15:48:11.726856 140313937978560 checkpoints.py:1101] Found no checkpoint files in /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_1/fastmri_jax/trial_3 with prefix checkpoint_
I0307 15:48:11.726956 140313937978560 submission_runner.py:321] Saving meta data to /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_1/fastmri_jax/trial_3/meta_data_0.json.
I0307 15:48:11.727100 140313937978560 logger_utils.py:262] Unable to record workload.train_mean information. Continuing without it.
I0307 15:48:11.727153 140313937978560 logger_utils.py:262] Unable to record workload.train_stddev information. Continuing without it.
I0307 15:48:12.255463 140313937978560 submission_runner.py:325] Saving flags to /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_1/fastmri_jax/trial_3/flags_0.json.
I0307 15:48:12.287172 140313937978560 submission_runner.py:337] Starting training loop.
E0307 15:51:50.824012       9 gpu_timer.cc:183] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.
E0307 15:51:51.028754       9 gpu_timer.cc:183] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.
E0307 15:51:51.438683       9 gpu_timer.cc:183] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.
E0307 15:51:51.644024       9 gpu_timer.cc:183] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.
E0307 15:51:53.521961       9 gpu_timer.cc:183] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.
E0307 15:51:53.727551       9 gpu_timer.cc:183] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.
I0307 15:52:04.891996 140176928012032 logging_writer.py:48] [0] global_step=0, grad_norm=4.783616542816162, loss=0.9044366478919983
I0307 15:52:04.939481 140313937978560 spec.py:321] Evaluating on the training split.
I0307 15:52:58.340935 140313937978560 spec.py:333] Evaluating on the validation split.
I0307 15:53:22.190425 140313937978560 spec.py:349] Evaluating on the test split.
I0307 15:53:44.208741 140313937978560 submission_runner.py:469] Time since start: 331.92s, 	Step: 1, 	{'train/ssim': 0.26736603464399067, 'train/loss': 0.929619584764753, 'validation/ssim': 0.25792976169017306, 'validation/loss': 0.9332163418419387, 'validation/num_examples': 3554, 'test/ssim': 0.28033953477336987, 'test/loss': 0.931455934423869, 'test/num_examples': 3581, 'score': 232.652197599411, 'total_duration': 331.92153000831604, 'accumulated_submission_time': 232.652197599411, 'accumulated_eval_time': 99.2692015171051, 'accumulated_logging_time': 0}
I0307 15:53:44.215720 140162382141184 logging_writer.py:48] [1] accumulated_eval_time=99.2692, accumulated_logging_time=0, accumulated_submission_time=232.652, global_step=1, preemption_count=0, score=232.652, test/loss=0.931456, test/num_examples=3581, test/ssim=0.28034, total_duration=331.922, train/loss=0.92962, train/ssim=0.267366, validation/loss=0.933216, validation/num_examples=3554, validation/ssim=0.25793
I0307 15:53:56.598369 140161627191040 logging_writer.py:48] [100] global_step=100, grad_norm=0.4637031853199005, loss=0.4899367690086365
I0307 15:54:13.581015 140162382141184 logging_writer.py:48] [200] global_step=200, grad_norm=0.07902216166257858, loss=0.28604674339294434
I0307 15:54:30.159682 140161627191040 logging_writer.py:48] [300] global_step=300, grad_norm=0.17069678008556366, loss=0.34803473949432373
I0307 15:54:48.234787 140162382141184 logging_writer.py:48] [400] global_step=400, grad_norm=0.12457036226987839, loss=0.45290133357048035
I0307 15:55:04.242583 140313937978560 spec.py:321] Evaluating on the training split.
I0307 15:55:05.845892 140313937978560 spec.py:333] Evaluating on the validation split.
I0307 15:55:07.149250 140313937978560 spec.py:349] Evaluating on the test split.
I0307 15:55:08.453547 140313937978560 submission_runner.py:469] Time since start: 416.17s, 	Step: 489, 	{'train/ssim': 0.7061631338936942, 'train/loss': 0.3063300337110247, 'validation/ssim': 0.6853177784934229, 'validation/loss': 0.3213749307558385, 'validation/num_examples': 3554, 'test/ssim': 0.7035206973436191, 'test/loss': 0.32324505091978495, 'test/num_examples': 3581, 'score': 312.5965509414673, 'total_duration': 416.1663374900818, 'accumulated_submission_time': 312.5965509414673, 'accumulated_eval_time': 103.48012113571167, 'accumulated_logging_time': 0.017572879791259766}
I0307 15:55:08.464451 140161627191040 logging_writer.py:48] [489] accumulated_eval_time=103.48, accumulated_logging_time=0.0175729, accumulated_submission_time=312.597, global_step=489, preemption_count=0, score=312.597, test/loss=0.323245, test/num_examples=3581, test/ssim=0.703521, total_duration=416.166, train/loss=0.30633, train/ssim=0.706163, validation/loss=0.321375, validation/num_examples=3554, validation/ssim=0.685318
I0307 15:55:09.485416 140162382141184 logging_writer.py:48] [500] global_step=500, grad_norm=0.17639018595218658, loss=0.38960328698158264
I0307 15:55:25.468551 140161627191040 logging_writer.py:48] [600] global_step=600, grad_norm=0.07009503990411758, loss=0.3333277106285095
I0307 15:55:44.319922 140162382141184 logging_writer.py:48] [700] global_step=700, grad_norm=0.11317391693592072, loss=0.28410035371780396
I0307 15:56:02.444240 140161627191040 logging_writer.py:48] [800] global_step=800, grad_norm=0.09118624776601791, loss=0.2753678560256958
I0307 15:56:20.601761 140162382141184 logging_writer.py:48] [900] global_step=900, grad_norm=0.19068782031536102, loss=0.24556221067905426
I0307 15:56:28.644401 140313937978560 spec.py:321] Evaluating on the training split.
I0307 15:56:29.948966 140313937978560 spec.py:333] Evaluating on the validation split.
I0307 15:56:31.505515 140313937978560 spec.py:349] Evaluating on the test split.
I0307 15:56:32.813819 140313937978560 submission_runner.py:469] Time since start: 500.53s, 	Step: 945, 	{'train/ssim': 0.7261850493294852, 'train/loss': 0.2879002094268799, 'validation/ssim': 0.7057131379255768, 'validation/loss': 0.30256054330288057, 'validation/num_examples': 3554, 'test/ssim': 0.7227346531607791, 'test/loss': 0.3048145812971237, 'test/num_examples': 3581, 'score': 392.6895680427551, 'total_duration': 500.52660846710205, 'accumulated_submission_time': 392.6895680427551, 'accumulated_eval_time': 107.64949321746826, 'accumulated_logging_time': 0.04829716682434082}
I0307 15:56:32.868262 140161627191040 logging_writer.py:48] [945] accumulated_eval_time=107.649, accumulated_logging_time=0.0482972, accumulated_submission_time=392.69, global_step=945, preemption_count=0, score=392.69, test/loss=0.304815, test/num_examples=3581, test/ssim=0.722735, total_duration=500.527, train/loss=0.2879, train/ssim=0.726185, validation/loss=0.302561, validation/num_examples=3554, validation/ssim=0.705713
I0307 15:56:37.612591 140162382141184 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.23717017471790314, loss=0.3405502736568451
I0307 15:56:45.863197 140161627191040 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.14871519804000854, loss=0.2913239598274231
I0307 15:56:54.108124 140162382141184 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.10198868811130524, loss=0.2756442725658417
I0307 15:57:02.372005 140161627191040 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.6868714690208435, loss=0.2261180430650711
I0307 15:57:10.615404 140162382141184 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.2240416407585144, loss=0.3184656500816345
I0307 15:57:18.864647 140161627191040 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.24246199429035187, loss=0.2830498218536377
I0307 15:57:27.113926 140162382141184 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.1309441477060318, loss=0.2609320282936096
I0307 15:57:35.365389 140161627191040 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.13014833629131317, loss=0.26197412610054016
I0307 15:57:43.611416 140162382141184 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.1159319207072258, loss=0.220033198595047
I0307 15:57:51.886413 140161627191040 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.10853271931409836, loss=0.2257755547761917
I0307 15:57:52.881428 140313937978560 spec.py:321] Evaluating on the training split.
I0307 15:57:54.187614 140313937978560 spec.py:333] Evaluating on the validation split.
I0307 15:57:55.495438 140313937978560 spec.py:349] Evaluating on the test split.
I0307 15:57:56.809667 140313937978560 submission_runner.py:469] Time since start: 584.52s, 	Step: 1913, 	{'train/ssim': 0.7368367058890206, 'train/loss': 0.27795352254595074, 'validation/ssim': 0.7158193507755346, 'validation/loss': 0.29314251300251476, 'validation/num_examples': 3554, 'test/ssim': 0.7329012251073374, 'test/loss': 0.2948481419405369, 'test/num_examples': 3581, 'score': 472.63604712486267, 'total_duration': 584.522458076477, 'accumulated_submission_time': 472.63604712486267, 'accumulated_eval_time': 111.5776879787445, 'accumulated_logging_time': 0.11836957931518555}
I0307 15:57:56.818075 140162382141184 logging_writer.py:48] [1913] accumulated_eval_time=111.578, accumulated_logging_time=0.11837, accumulated_submission_time=472.636, global_step=1913, preemption_count=0, score=472.636, test/loss=0.294848, test/num_examples=3581, test/ssim=0.732901, total_duration=584.522, train/loss=0.277954, train/ssim=0.736837, validation/loss=0.293143, validation/num_examples=3554, validation/ssim=0.715819
I0307 15:58:04.103966 140161627191040 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.247813418507576, loss=0.2780074179172516
I0307 15:58:12.360179 140162382141184 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.23504780232906342, loss=0.3934049606323242
I0307 15:58:20.593613 140161627191040 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.19849620759487152, loss=0.24709266424179077
I0307 15:58:28.872189 140162382141184 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.14252690970897675, loss=0.29520758986473083
I0307 15:58:37.108734 140161627191040 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.392429381608963, loss=0.2051895409822464
I0307 15:58:45.354163 140162382141184 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.07275986671447754, loss=0.2728973925113678
I0307 15:58:53.593817 140161627191040 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.4505075216293335, loss=0.31498458981513977
I0307 15:59:01.864639 140162382141184 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.1513463407754898, loss=0.28808867931365967
I0307 15:59:10.131824 140161627191040 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.3014838397502899, loss=0.2923535704612732
I0307 15:59:16.819984 140313937978560 spec.py:321] Evaluating on the training split.
I0307 15:59:18.127941 140313937978560 spec.py:333] Evaluating on the validation split.
I0307 15:59:19.440077 140313937978560 spec.py:349] Evaluating on the test split.
I0307 15:59:20.756000 140313937978560 submission_runner.py:469] Time since start: 668.47s, 	Step: 2882, 	{'train/ssim': 0.737731797354562, 'train/loss': 0.27614474296569824, 'validation/ssim': 0.7174829280168472, 'validation/loss': 0.2910515179309405, 'validation/num_examples': 3554, 'test/ssim': 0.7346044144661058, 'test/loss': 0.292629332490488, 'test/num_examples': 3581, 'score': 552.5840771198273, 'total_duration': 668.4687564373016, 'accumulated_submission_time': 552.5840771198273, 'accumulated_eval_time': 115.51362705230713, 'accumulated_logging_time': 0.1354520320892334}
I0307 15:59:20.767561 140162382141184 logging_writer.py:48] [2882] accumulated_eval_time=115.514, accumulated_logging_time=0.135452, accumulated_submission_time=552.584, global_step=2882, preemption_count=0, score=552.584, test/loss=0.292629, test/num_examples=3581, test/ssim=0.734604, total_duration=668.469, train/loss=0.276145, train/ssim=0.737732, validation/loss=0.291052, validation/num_examples=3554, validation/ssim=0.717483
I0307 15:59:22.348736 140161627191040 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.19349750876426697, loss=0.2591063976287842
I0307 15:59:30.611813 140162382141184 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.11099782586097717, loss=0.2551535665988922
I0307 15:59:38.863322 140161627191040 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.08988434076309204, loss=0.32265347242355347
I0307 15:59:47.116325 140162382141184 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.19236180186271667, loss=0.2514493465423584
I0307 15:59:55.381461 140161627191040 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.22935070097446442, loss=0.22194111347198486
I0307 16:00:03.656201 140162382141184 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.08829781413078308, loss=0.24755974113941193
I0307 16:00:11.898724 140161627191040 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.08318519592285156, loss=0.3171313405036926
I0307 16:00:20.156665 140162382141184 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.09527420997619629, loss=0.28637319803237915
I0307 16:00:28.410105 140161627191040 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.15975862741470337, loss=0.24760659039020538
I0307 16:00:36.682315 140162382141184 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.08749499171972275, loss=0.19651107490062714
I0307 16:00:40.818097 140313937978560 spec.py:321] Evaluating on the training split.
I0307 16:00:42.127761 140313937978560 spec.py:333] Evaluating on the validation split.
I0307 16:00:43.437091 140313937978560 spec.py:349] Evaluating on the test split.
I0307 16:00:44.749838 140313937978560 submission_runner.py:469] Time since start: 752.46s, 	Step: 3851, 	{'train/ssim': 0.7419653620038714, 'train/loss': 0.273246169090271, 'validation/ssim': 0.7206239201208146, 'validation/loss': 0.28882100411727984, 'validation/num_examples': 3554, 'test/ssim': 0.7378638043013473, 'test/loss': 0.2903002472903868, 'test/num_examples': 3581, 'score': 632.5807127952576, 'total_duration': 752.46262550354, 'accumulated_submission_time': 632.5807127952576, 'accumulated_eval_time': 119.4453489780426, 'accumulated_logging_time': 0.15595006942749023}
I0307 16:00:44.758633 140161627191040 logging_writer.py:48] [3851] accumulated_eval_time=119.445, accumulated_logging_time=0.15595, accumulated_submission_time=632.581, global_step=3851, preemption_count=0, score=632.581, test/loss=0.2903, test/num_examples=3581, test/ssim=0.737864, total_duration=752.463, train/loss=0.273246, train/ssim=0.741965, validation/loss=0.288821, validation/num_examples=3554, validation/ssim=0.720624
I0307 16:00:48.904678 140162382141184 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.43214890360832214, loss=0.30088871717453003
I0307 16:00:57.156392 140161627191040 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.12103388458490372, loss=0.20879511535167694
I0307 16:01:05.388215 140162382141184 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.2813716530799866, loss=0.2587839663028717
I0307 16:01:13.659650 140161627191040 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.21018348634243011, loss=0.2962639033794403
I0307 16:01:21.909828 140162382141184 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.15695582330226898, loss=0.2850310206413269
I0307 16:01:30.165049 140161627191040 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.23292548954486847, loss=0.20686116814613342
I0307 16:01:38.402233 140162382141184 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.10015638917684555, loss=0.2802331745624542
I0307 16:01:46.663726 140161627191040 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.2085568904876709, loss=0.24944421648979187
I0307 16:01:54.929801 140162382141184 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.06307274848222733, loss=0.24650143086910248
I0307 16:02:03.193659 140161627191040 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.14039166271686554, loss=0.27274230122566223
I0307 16:02:04.759704 140313937978560 spec.py:321] Evaluating on the training split.
I0307 16:02:06.066834 140313937978560 spec.py:333] Evaluating on the validation split.
I0307 16:02:07.379377 140313937978560 spec.py:349] Evaluating on the test split.
I0307 16:02:08.689899 140313937978560 submission_runner.py:469] Time since start: 836.40s, 	Step: 4820, 	{'train/ssim': 0.7428394726344517, 'train/loss': 0.27235216753823416, 'validation/ssim': 0.7216830535356289, 'validation/loss': 0.2879200572033712, 'validation/num_examples': 3554, 'test/ssim': 0.7388220954560528, 'test/loss': 0.28944950481927184, 'test/num_examples': 3581, 'score': 712.5300538539886, 'total_duration': 836.402693271637, 'accumulated_submission_time': 712.5300538539886, 'accumulated_eval_time': 123.375497341156, 'accumulated_logging_time': 0.17293310165405273}
I0307 16:02:08.699101 140162382141184 logging_writer.py:48] [4820] accumulated_eval_time=123.375, accumulated_logging_time=0.172933, accumulated_submission_time=712.53, global_step=4820, preemption_count=0, score=712.53, test/loss=0.28945, test/num_examples=3581, test/ssim=0.738822, total_duration=836.403, train/loss=0.272352, train/ssim=0.742839, validation/loss=0.28792, validation/num_examples=3554, validation/ssim=0.721683
I0307 16:02:15.396190 140161627191040 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.17011231184005737, loss=0.22174260020256042
I0307 16:02:23.671454 140162382141184 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.047788530588150024, loss=0.2874639928340912
I0307 16:02:31.921833 140161627191040 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.10330767929553986, loss=0.2262265384197235
I0307 16:02:40.172798 140162382141184 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.09263696521520615, loss=0.2172493040561676
I0307 16:02:48.416745 140161627191040 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.24328036606311798, loss=0.34819409251213074
I0307 16:02:56.676465 140162382141184 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.08899853378534317, loss=0.15938855707645416
I0307 16:03:04.924047 140161627191040 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.05708078667521477, loss=0.3095040023326874
I0307 16:03:13.176053 140162382141184 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.1125359833240509, loss=0.26300233602523804
I0307 16:03:21.424746 140161627191040 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.16926392912864685, loss=0.23363952338695526
I0307 16:03:28.771315 140313937978560 spec.py:321] Evaluating on the training split.
I0307 16:03:30.077228 140313937978560 spec.py:333] Evaluating on the validation split.
I0307 16:03:31.387423 140313937978560 spec.py:349] Evaluating on the test split.
I0307 16:03:32.703908 140313937978560 submission_runner.py:469] Time since start: 920.42s, 	Step: 5790, 	{'train/ssim': 0.7448431423732212, 'train/loss': 0.2718174798148019, 'validation/ssim': 0.7230384667803883, 'validation/loss': 0.28774765091929866, 'validation/num_examples': 3554, 'test/ssim': 0.7401929917664409, 'test/loss': 0.2892215561513893, 'test/num_examples': 3581, 'score': 792.5492022037506, 'total_duration': 920.4167010784149, 'accumulated_submission_time': 792.5492022037506, 'accumulated_eval_time': 127.30804657936096, 'accumulated_logging_time': 0.19017314910888672}
I0307 16:03:32.712702 140162382141184 logging_writer.py:48] [5790] accumulated_eval_time=127.308, accumulated_logging_time=0.190173, accumulated_submission_time=792.549, global_step=5790, preemption_count=0, score=792.549, test/loss=0.289222, test/num_examples=3581, test/ssim=0.740193, total_duration=920.417, train/loss=0.271817, train/ssim=0.744843, validation/loss=0.287748, validation/num_examples=3554, validation/ssim=0.723038
I0307 16:03:33.638634 140161627191040 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.1594943255186081, loss=0.3031831979751587
I0307 16:03:41.901766 140162382141184 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.06316939741373062, loss=0.2712952792644501
I0307 16:03:50.166783 140161627191040 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.06904056668281555, loss=0.28319475054740906
I0307 16:03:58.420655 140162382141184 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.11290105432271957, loss=0.26139000058174133
I0307 16:04:06.655557 140161627191040 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.1249551847577095, loss=0.2799522876739502
I0307 16:04:14.902042 140162382141184 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.19318696856498718, loss=0.26959228515625
I0307 16:04:23.188199 140161627191040 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.0715734139084816, loss=0.33176344633102417
I0307 16:04:31.467042 140162382141184 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.06392636895179749, loss=0.24978415668010712
I0307 16:04:39.721264 140161627191040 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.09726930409669876, loss=0.28074172139167786
I0307 16:04:47.966136 140162382141184 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.09318533539772034, loss=0.273928165435791
I0307 16:04:52.745933 140313937978560 spec.py:321] Evaluating on the training split.
I0307 16:04:54.052222 140313937978560 spec.py:333] Evaluating on the validation split.
I0307 16:04:55.362007 140313937978560 spec.py:349] Evaluating on the test split.
I0307 16:04:56.677578 140313937978560 submission_runner.py:469] Time since start: 1004.39s, 	Step: 6759, 	{'train/ssim': 0.744828428540911, 'train/loss': 0.27107180867876324, 'validation/ssim': 0.7227934331255276, 'validation/loss': 0.28734369229653206, 'validation/num_examples': 3554, 'test/ssim': 0.7401501086463278, 'test/loss': 0.2887569322029461, 'test/num_examples': 3581, 'score': 872.5289771556854, 'total_duration': 1004.390373468399, 'accumulated_submission_time': 872.5289771556854, 'accumulated_eval_time': 131.2396547794342, 'accumulated_logging_time': 0.20662641525268555}
I0307 16:04:56.687280 140161627191040 logging_writer.py:48] [6759] accumulated_eval_time=131.24, accumulated_logging_time=0.206626, accumulated_submission_time=872.529, global_step=6759, preemption_count=0, score=872.529, test/loss=0.288757, test/num_examples=3581, test/ssim=0.74015, total_duration=1004.39, train/loss=0.271072, train/ssim=0.744828, validation/loss=0.287344, validation/num_examples=3554, validation/ssim=0.722793
I0307 16:05:00.173574 140162382141184 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.1090691015124321, loss=0.26513001322746277
I0307 16:05:08.433725 140161627191040 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.18586304783821106, loss=0.22903308272361755
I0307 16:05:16.687901 140162382141184 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.32718050479888916, loss=0.25538578629493713
I0307 16:05:24.938947 140161627191040 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.12702377140522003, loss=0.23565907776355743
I0307 16:05:33.207192 140162382141184 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.15418122708797455, loss=0.3566552996635437
I0307 16:05:41.459317 140161627191040 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.24381151795387268, loss=0.3105883300304413
I0307 16:05:49.715698 140162382141184 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.12875297665596008, loss=0.26773014664649963
I0307 16:05:57.963815 140161627191040 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.10867216438055038, loss=0.3812877833843231
I0307 16:06:06.210873 140162382141184 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.19557243585586548, loss=0.23697341978549957
I0307 16:06:14.473760 140161627191040 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.19080382585525513, loss=0.22755472362041473
I0307 16:06:16.693567 140313937978560 spec.py:321] Evaluating on the training split.
I0307 16:06:18.002632 140313937978560 spec.py:333] Evaluating on the validation split.
I0307 16:06:19.313240 140313937978560 spec.py:349] Evaluating on the test split.
I0307 16:06:20.624614 140313937978560 submission_runner.py:469] Time since start: 1088.34s, 	Step: 7728, 	{'train/ssim': 0.7462770598275321, 'train/loss': 0.2705286741256714, 'validation/ssim': 0.7240747935864519, 'validation/loss': 0.28692283480101644, 'validation/num_examples': 3554, 'test/ssim': 0.7412668423624685, 'test/loss': 0.2883198857140987, 'test/num_examples': 3581, 'score': 952.4806084632874, 'total_duration': 1088.3374071121216, 'accumulated_submission_time': 952.4806084632874, 'accumulated_eval_time': 135.17065501213074, 'accumulated_logging_time': 0.22525262832641602}
I0307 16:06:20.633860 140162382141184 logging_writer.py:48] [7728] accumulated_eval_time=135.171, accumulated_logging_time=0.225253, accumulated_submission_time=952.481, global_step=7728, preemption_count=0, score=952.481, test/loss=0.28832, test/num_examples=3581, test/ssim=0.741267, total_duration=1088.34, train/loss=0.270529, train/ssim=0.746277, validation/loss=0.286923, validation/num_examples=3554, validation/ssim=0.724075
I0307 16:06:20.646242 140161627191040 logging_writer.py:48] [7728] global_step=7728, preemption_count=0, score=952.481
I0307 16:06:21.195641 140313937978560 submission_runner.py:646] Tuning trial 3/5
I0307 16:06:21.195827 140313937978560 submission_runner.py:647] Hyperparameters: Hyperparameters(dropout_rate=0.0, label_smoothing=0.0, learning_rate=0.001308209823469072, one_minus_beta1=0.02686663061, beta2=0.9981232922116359, weight_decay=0.16375311233774334, warmup_factor=0.1)
I0307 16:06:21.196490 140313937978560 submission_runner.py:648] Metrics: {'eval_results': [(1, {'train/ssim': 0.26736603464399067, 'train/loss': 0.929619584764753, 'validation/ssim': 0.25792976169017306, 'validation/loss': 0.9332163418419387, 'validation/num_examples': 3554, 'test/ssim': 0.28033953477336987, 'test/loss': 0.931455934423869, 'test/num_examples': 3581, 'score': 232.652197599411, 'total_duration': 331.92153000831604, 'accumulated_submission_time': 232.652197599411, 'accumulated_eval_time': 99.2692015171051, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (489, {'train/ssim': 0.7061631338936942, 'train/loss': 0.3063300337110247, 'validation/ssim': 0.6853177784934229, 'validation/loss': 0.3213749307558385, 'validation/num_examples': 3554, 'test/ssim': 0.7035206973436191, 'test/loss': 0.32324505091978495, 'test/num_examples': 3581, 'score': 312.5965509414673, 'total_duration': 416.1663374900818, 'accumulated_submission_time': 312.5965509414673, 'accumulated_eval_time': 103.48012113571167, 'accumulated_logging_time': 0.017572879791259766, 'global_step': 489, 'preemption_count': 0}), (945, {'train/ssim': 0.7261850493294852, 'train/loss': 0.2879002094268799, 'validation/ssim': 0.7057131379255768, 'validation/loss': 0.30256054330288057, 'validation/num_examples': 3554, 'test/ssim': 0.7227346531607791, 'test/loss': 0.3048145812971237, 'test/num_examples': 3581, 'score': 392.6895680427551, 'total_duration': 500.52660846710205, 'accumulated_submission_time': 392.6895680427551, 'accumulated_eval_time': 107.64949321746826, 'accumulated_logging_time': 0.04829716682434082, 'global_step': 945, 'preemption_count': 0}), (1913, {'train/ssim': 0.7368367058890206, 'train/loss': 0.27795352254595074, 'validation/ssim': 0.7158193507755346, 'validation/loss': 0.29314251300251476, 'validation/num_examples': 3554, 'test/ssim': 0.7329012251073374, 'test/loss': 0.2948481419405369, 'test/num_examples': 3581, 'score': 472.63604712486267, 'total_duration': 584.522458076477, 'accumulated_submission_time': 472.63604712486267, 'accumulated_eval_time': 111.5776879787445, 'accumulated_logging_time': 0.11836957931518555, 'global_step': 1913, 'preemption_count': 0}), (2882, {'train/ssim': 0.737731797354562, 'train/loss': 0.27614474296569824, 'validation/ssim': 0.7174829280168472, 'validation/loss': 0.2910515179309405, 'validation/num_examples': 3554, 'test/ssim': 0.7346044144661058, 'test/loss': 0.292629332490488, 'test/num_examples': 3581, 'score': 552.5840771198273, 'total_duration': 668.4687564373016, 'accumulated_submission_time': 552.5840771198273, 'accumulated_eval_time': 115.51362705230713, 'accumulated_logging_time': 0.1354520320892334, 'global_step': 2882, 'preemption_count': 0}), (3851, {'train/ssim': 0.7419653620038714, 'train/loss': 0.273246169090271, 'validation/ssim': 0.7206239201208146, 'validation/loss': 0.28882100411727984, 'validation/num_examples': 3554, 'test/ssim': 0.7378638043013473, 'test/loss': 0.2903002472903868, 'test/num_examples': 3581, 'score': 632.5807127952576, 'total_duration': 752.46262550354, 'accumulated_submission_time': 632.5807127952576, 'accumulated_eval_time': 119.4453489780426, 'accumulated_logging_time': 0.15595006942749023, 'global_step': 3851, 'preemption_count': 0}), (4820, {'train/ssim': 0.7428394726344517, 'train/loss': 0.27235216753823416, 'validation/ssim': 0.7216830535356289, 'validation/loss': 0.2879200572033712, 'validation/num_examples': 3554, 'test/ssim': 0.7388220954560528, 'test/loss': 0.28944950481927184, 'test/num_examples': 3581, 'score': 712.5300538539886, 'total_duration': 836.402693271637, 'accumulated_submission_time': 712.5300538539886, 'accumulated_eval_time': 123.375497341156, 'accumulated_logging_time': 0.17293310165405273, 'global_step': 4820, 'preemption_count': 0}), (5790, {'train/ssim': 0.7448431423732212, 'train/loss': 0.2718174798148019, 'validation/ssim': 0.7230384667803883, 'validation/loss': 0.28774765091929866, 'validation/num_examples': 3554, 'test/ssim': 0.7401929917664409, 'test/loss': 0.2892215561513893, 'test/num_examples': 3581, 'score': 792.5492022037506, 'total_duration': 920.4167010784149, 'accumulated_submission_time': 792.5492022037506, 'accumulated_eval_time': 127.30804657936096, 'accumulated_logging_time': 0.19017314910888672, 'global_step': 5790, 'preemption_count': 0}), (6759, {'train/ssim': 0.744828428540911, 'train/loss': 0.27107180867876324, 'validation/ssim': 0.7227934331255276, 'validation/loss': 0.28734369229653206, 'validation/num_examples': 3554, 'test/ssim': 0.7401501086463278, 'test/loss': 0.2887569322029461, 'test/num_examples': 3581, 'score': 872.5289771556854, 'total_duration': 1004.390373468399, 'accumulated_submission_time': 872.5289771556854, 'accumulated_eval_time': 131.2396547794342, 'accumulated_logging_time': 0.20662641525268555, 'global_step': 6759, 'preemption_count': 0}), (7728, {'train/ssim': 0.7462770598275321, 'train/loss': 0.2705286741256714, 'validation/ssim': 0.7240747935864519, 'validation/loss': 0.28692283480101644, 'validation/num_examples': 3554, 'test/ssim': 0.7412668423624685, 'test/loss': 0.2883198857140987, 'test/num_examples': 3581, 'score': 952.4806084632874, 'total_duration': 1088.3374071121216, 'accumulated_submission_time': 952.4806084632874, 'accumulated_eval_time': 135.17065501213074, 'accumulated_logging_time': 0.22525262832641602, 'global_step': 7728, 'preemption_count': 0})], 'global_step': 7728}
I0307 16:06:21.196567 140313937978560 submission_runner.py:649] Timing: 952.4806084632874
I0307 16:06:21.196601 140313937978560 submission_runner.py:651] Total number of evals: 10
I0307 16:06:21.196633 140313937978560 submission_runner.py:652] ====================
I0307 16:06:21.196733 140313937978560 submission_runner.py:750] Final fastmri score: 2
