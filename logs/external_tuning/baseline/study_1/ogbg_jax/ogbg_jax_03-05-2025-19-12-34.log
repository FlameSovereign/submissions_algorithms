python submission_runner.py --framework=jax --workload=ogbg --submission_path=prize_qualification_baselines/external_tuning/jax_nadamw_full_budget.py --data_dir=/data/ogbg --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=submissions/rolling_leaderboard/external_tuning/baseline/study_1 --overwrite=True --save_checkpoints=False --rng_seed=-849070187 --tuning_ruleset=external --tuning_search_space=prize_qualification_baselines/external_tuning/tuning_search_space.json --num_tuning_trials=5 --hparam_start_index=1 --hparam_end_index=2 2>&1 | tee -a /logs/ogbg_jax_03-05-2025-19-12-34.log
2025-03-05 19:12:35.043829: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1741201955.066469       9 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1741201955.073417       9 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
I0305 19:12:41.357820 140363820438720 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_1/ogbg_jax.
I0305 19:12:42.371097 140363820438720 xla_bridge.py:884] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA
I0305 19:12:42.373944 140363820438720 xla_bridge.py:884] Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory
I0305 19:12:42.375504 140363820438720 submission_runner.py:606] Using RNG seed -849070187
I0305 19:12:42.961179 140363820438720 submission_runner.py:615] --- Tuning run 2/5 ---
I0305 19:12:42.961355 140363820438720 submission_runner.py:620] Creating tuning directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_1/ogbg_jax/trial_2.
I0305 19:12:42.961561 140363820438720 logger_utils.py:97] Saving hparams to /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_1/ogbg_jax/trial_2/hparams.json.
I0305 19:12:43.191030 140363820438720 submission_runner.py:218] Initializing dataset.
I0305 19:12:43.450857 140363820438720 dataset_info.py:690] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0305 19:12:43.473406 140363820438720 reader.py:261] Creating a tf.data.Dataset reading 8 files located in folders: /data/ogbg/ogbg_molpcba/0.1.3.
WARNING:tensorflow:From /usr/local/lib/python3.11/site-packages/tensorflow_datasets/core/reader.py:101: CounterV2 (from tensorflow.python.data.experimental.ops.counter) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.counter(...)` instead.
W0305 19:12:43.698143 140363820438720 deprecation.py:50] From /usr/local/lib/python3.11/site-packages/tensorflow_datasets/core/reader.py:101: CounterV2 (from tensorflow.python.data.experimental.ops.counter) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.counter(...)` instead.
I0305 19:12:43.748847 140363820438720 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split train, from /data/ogbg/ogbg_molpcba/0.1.3
I0305 19:12:43.780565 140363820438720 submission_runner.py:229] Initializing model.
I0305 19:12:52.064431 140363820438720 submission_runner.py:272] Initializing optimizer.
I0305 19:12:52.478968 140363820438720 submission_runner.py:279] Initializing metrics bundle.
I0305 19:12:52.479180 140363820438720 submission_runner.py:301] Initializing checkpoint and logger.
I0305 19:12:52.479966 140363820438720 checkpoints.py:1101] Found no checkpoint files in /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_1/ogbg_jax/trial_2 with prefix checkpoint_
I0305 19:12:52.480084 140363820438720 submission_runner.py:321] Saving meta data to /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_1/ogbg_jax/trial_2/meta_data_0.json.
I0305 19:12:52.480244 140363820438720 logger_utils.py:262] Unable to record workload.train_mean information. Continuing without it.
I0305 19:12:52.480310 140363820438720 logger_utils.py:262] Unable to record workload.train_stddev information. Continuing without it.
I0305 19:12:52.639532 140363820438720 submission_runner.py:325] Saving flags to /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_1/ogbg_jax/trial_2/flags_0.json.
I0305 19:12:52.671252 140363820438720 submission_runner.py:337] Starting training loop.
I0305 19:13:03.614347 140227684943616 logging_writer.py:48] [0] global_step=0, grad_norm=2.314098834991455, loss=0.6963369250297546
I0305 19:13:03.668343 140363820438720 spec.py:321] Evaluating on the training split.
I0305 19:13:03.671978 140363820438720 dataset_info.py:690] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0305 19:13:03.675535 140363820438720 reader.py:261] Creating a tf.data.Dataset reading 8 files located in folders: /data/ogbg/ogbg_molpcba/0.1.3.
I0305 19:13:03.739461 140363820438720 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split train, from /data/ogbg/ogbg_molpcba/0.1.3
I0305 19:14:19.741556 140363820438720 spec.py:333] Evaluating on the validation split.
I0305 19:14:19.744265 140363820438720 dataset_info.py:690] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0305 19:14:19.747804 140363820438720 reader.py:261] Creating a tf.data.Dataset reading 1 files located in folders: /data/ogbg/ogbg_molpcba/0.1.3.
I0305 19:14:19.810688 140363820438720 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split validation, from /data/ogbg/ogbg_molpcba/0.1.3
I0305 19:15:23.360669 140363820438720 spec.py:349] Evaluating on the test split.
I0305 19:15:23.363411 140363820438720 dataset_info.py:690] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0305 19:15:23.367110 140363820438720 reader.py:261] Creating a tf.data.Dataset reading 1 files located in folders: /data/ogbg/ogbg_molpcba/0.1.3.
I0305 19:15:23.427672 140363820438720 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split test, from /data/ogbg/ogbg_molpcba/0.1.3
I0305 19:16:26.556860 140363820438720 submission_runner.py:469] Time since start: 213.89s, 	Step: 1, 	{'train/accuracy': 0.538699746131897, 'train/loss': 0.6973111033439636, 'train/mean_average_precision': 0.021441355192791046, 'validation/accuracy': 0.5371228456497192, 'validation/loss': 0.6984302997589111, 'validation/mean_average_precision': 0.02601679457974987, 'validation/num_examples': 43793, 'test/accuracy': 0.5367534160614014, 'test/loss': 0.6991291046142578, 'test/mean_average_precision': 0.028082283176050583, 'test/num_examples': 43793, 'score': 10.99699068069458, 'total_duration': 213.88552117347717, 'accumulated_submission_time': 10.99699068069458, 'accumulated_eval_time': 202.88842844963074, 'accumulated_logging_time': 0}
I0305 19:16:26.563823 140222184318720 logging_writer.py:48] [1] accumulated_eval_time=202.888, accumulated_logging_time=0, accumulated_submission_time=10.997, global_step=1, preemption_count=0, score=10.997, test/accuracy=0.536753, test/loss=0.699129, test/mean_average_precision=0.0280823, test/num_examples=43793, total_duration=213.886, train/accuracy=0.5387, train/loss=0.697311, train/mean_average_precision=0.0214414, validation/accuracy=0.537123, validation/loss=0.69843, validation/mean_average_precision=0.0260168, validation/num_examples=43793
I0305 19:16:47.406672 140222192711424 logging_writer.py:48] [100] global_step=100, grad_norm=0.42274630069732666, loss=0.38421547412872314
I0305 19:17:08.491760 140222184318720 logging_writer.py:48] [200] global_step=200, grad_norm=0.2993394732475281, loss=0.2628946304321289
I0305 19:17:29.366577 140222192711424 logging_writer.py:48] [300] global_step=300, grad_norm=0.1849534660577774, loss=0.15606604516506195
I0305 19:17:50.506235 140222184318720 logging_writer.py:48] [400] global_step=400, grad_norm=0.1077609583735466, loss=0.10445903986692429
I0305 19:18:11.577975 140222192711424 logging_writer.py:48] [500] global_step=500, grad_norm=0.07095934450626373, loss=0.07438303530216217
I0305 19:18:32.700448 140222184318720 logging_writer.py:48] [600] global_step=600, grad_norm=0.06637095659971237, loss=0.06999696046113968
I0305 19:18:53.724512 140222754940672 logging_writer.py:48] [700] global_step=700, grad_norm=0.048693157732486725, loss=0.06105310842394829
I0305 19:19:14.667401 140222746547968 logging_writer.py:48] [800] global_step=800, grad_norm=0.03512120246887207, loss=0.056253548711538315
I0305 19:19:35.586107 140222754940672 logging_writer.py:48] [900] global_step=900, grad_norm=0.02088203839957714, loss=0.06026478484272957
I0305 19:19:56.866904 140222746547968 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.061764907091856, loss=0.052713263779878616
I0305 19:20:17.212070 140222754940672 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.03543606400489807, loss=0.05062321573495865
I0305 19:20:26.765952 140363820438720 spec.py:321] Evaluating on the training split.
I0305 19:21:38.176913 140363820438720 spec.py:333] Evaluating on the validation split.
I0305 19:21:40.133270 140363820438720 spec.py:349] Evaluating on the test split.
I0305 19:21:42.014548 140363820438720 submission_runner.py:469] Time since start: 529.34s, 	Step: 1147, 	{'train/accuracy': 0.9867100119590759, 'train/loss': 0.051971886307001114, 'train/mean_average_precision': 0.057989005194681996, 'validation/accuracy': 0.9841219782829285, 'validation/loss': 0.061596449464559555, 'validation/mean_average_precision': 0.05811824640039385, 'validation/num_examples': 43793, 'test/accuracy': 0.9831386804580688, 'test/loss': 0.06487934291362762, 'test/mean_average_precision': 0.0601425657114673, 'test/num_examples': 43793, 'score': 251.1587917804718, 'total_duration': 529.3432381153107, 'accumulated_submission_time': 251.1587917804718, 'accumulated_eval_time': 278.13698744773865, 'accumulated_logging_time': 0.01647210121154785}
I0305 19:21:42.024298 140222746547968 logging_writer.py:48] [1147] accumulated_eval_time=278.137, accumulated_logging_time=0.0164721, accumulated_submission_time=251.159, global_step=1147, preemption_count=0, score=251.159, test/accuracy=0.983139, test/loss=0.0648793, test/mean_average_precision=0.0601426, test/num_examples=43793, total_duration=529.343, train/accuracy=0.98671, train/loss=0.0519719, train/mean_average_precision=0.057989, validation/accuracy=0.984122, validation/loss=0.0615964, validation/mean_average_precision=0.0581182, validation/num_examples=43793
I0305 19:21:53.085161 140222754940672 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.04681072384119034, loss=0.05249501392245293
I0305 19:22:13.799578 140222746547968 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.026413805782794952, loss=0.045093294233083725
I0305 19:22:34.992973 140222754940672 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.0407479889690876, loss=0.04927152767777443
I0305 19:22:55.773537 140222746547968 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.09426160156726837, loss=0.05093277618288994
I0305 19:23:16.586045 140222754940672 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.035180024802684784, loss=0.05020109936594963
I0305 19:23:37.743986 140222746547968 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.023854726925492287, loss=0.05037195235490799
I0305 19:23:58.592290 140222754940672 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.03078806959092617, loss=0.052457045763731
I0305 19:24:19.232072 140222746547968 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.03139791265130043, loss=0.04983876273036003
I0305 19:24:40.010656 140222754940672 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.06036968156695366, loss=0.048098985105752945
I0305 19:25:00.867654 140222746547968 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.0649467334151268, loss=0.04996458441019058
I0305 19:25:21.975477 140222754940672 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.04847615212202072, loss=0.050170574337244034
I0305 19:25:42.160077 140363820438720 spec.py:321] Evaluating on the training split.
I0305 19:26:53.499076 140363820438720 spec.py:333] Evaluating on the validation split.
I0305 19:26:55.407963 140363820438720 spec.py:349] Evaluating on the test split.
I0305 19:26:57.322465 140363820438720 submission_runner.py:469] Time since start: 844.65s, 	Step: 2298, 	{'train/accuracy': 0.9874939322471619, 'train/loss': 0.04541674256324768, 'train/mean_average_precision': 0.1265546478745511, 'validation/accuracy': 0.9847467541694641, 'validation/loss': 0.05470802262425423, 'validation/mean_average_precision': 0.12305261667739999, 'validation/num_examples': 43793, 'test/accuracy': 0.9837380647659302, 'test/loss': 0.0579862967133522, 'test/mean_average_precision': 0.12199999726476862, 'test/num_examples': 43793, 'score': 491.2537651062012, 'total_duration': 844.6511604785919, 'accumulated_submission_time': 491.2537651062012, 'accumulated_eval_time': 353.29932951927185, 'accumulated_logging_time': 0.03597760200500488}
I0305 19:26:57.330945 140222746547968 logging_writer.py:48] [2298] accumulated_eval_time=353.299, accumulated_logging_time=0.0359776, accumulated_submission_time=491.254, global_step=2298, preemption_count=0, score=491.254, test/accuracy=0.983738, test/loss=0.0579863, test/mean_average_precision=0.122, test/num_examples=43793, total_duration=844.651, train/accuracy=0.987494, train/loss=0.0454167, train/mean_average_precision=0.126555, validation/accuracy=0.984747, validation/loss=0.054708, validation/mean_average_precision=0.123053, validation/num_examples=43793
I0305 19:26:57.971917 140222754940672 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.024137308821082115, loss=0.05099151283502579
I0305 19:27:18.694777 140222746547968 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.03652704879641533, loss=0.05109569430351257
I0305 19:27:39.587731 140222754940672 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.037591610103845596, loss=0.04617089405655861
I0305 19:28:00.776993 140222746547968 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.07025668025016785, loss=0.05220688506960869
I0305 19:28:21.661293 140222754940672 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.027580169960856438, loss=0.04743339121341705
I0305 19:28:42.530284 140222746547968 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.03242777660489082, loss=0.04620380327105522
I0305 19:29:03.212950 140222754940672 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.03458437696099281, loss=0.04645976424217224
I0305 19:29:24.211855 140222746547968 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.018151581287384033, loss=0.04537658765912056
I0305 19:29:45.081607 140222754940672 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.020464489236474037, loss=0.044312428683042526
I0305 19:30:05.793232 140222746547968 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.06083446368575096, loss=0.04889216274023056
I0305 19:30:26.605666 140222754940672 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.012757143005728722, loss=0.044672004878520966
I0305 19:30:47.646076 140222746547968 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.02409936860203743, loss=0.04321586340665817
I0305 19:30:57.403444 140363820438720 spec.py:321] Evaluating on the training split.
I0305 19:32:09.671925 140363820438720 spec.py:333] Evaluating on the validation split.
I0305 19:32:11.574454 140363820438720 spec.py:349] Evaluating on the test split.
I0305 19:32:13.427441 140363820438720 submission_runner.py:469] Time since start: 1160.76s, 	Step: 3448, 	{'train/accuracy': 0.9880132675170898, 'train/loss': 0.04221317544579506, 'train/mean_average_precision': 0.18469053420334952, 'validation/accuracy': 0.9852931499481201, 'validation/loss': 0.05145341902971268, 'validation/mean_average_precision': 0.15516940846616792, 'validation/num_examples': 43793, 'test/accuracy': 0.9843774437904358, 'test/loss': 0.05415385589003563, 'test/mean_average_precision': 0.15344928642373062, 'test/num_examples': 43793, 'score': 731.2871816158295, 'total_duration': 1160.7561302185059, 'accumulated_submission_time': 731.2871816158295, 'accumulated_eval_time': 429.3232731819153, 'accumulated_logging_time': 0.05398058891296387}
I0305 19:32:13.437446 140222754940672 logging_writer.py:48] [3448] accumulated_eval_time=429.323, accumulated_logging_time=0.0539806, accumulated_submission_time=731.287, global_step=3448, preemption_count=0, score=731.287, test/accuracy=0.984377, test/loss=0.0541539, test/mean_average_precision=0.153449, test/num_examples=43793, total_duration=1160.76, train/accuracy=0.988013, train/loss=0.0422132, train/mean_average_precision=0.184691, validation/accuracy=0.985293, validation/loss=0.0514534, validation/mean_average_precision=0.155169, validation/num_examples=43793
I0305 19:32:24.632729 140222746547968 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.029951410368084908, loss=0.04265855997800827
I0305 19:32:45.525186 140222754940672 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.015402703545987606, loss=0.042700838297605515
I0305 19:33:06.644331 140222746547968 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.01480808388441801, loss=0.04679737240076065
I0305 19:33:27.349887 140222754940672 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.018229350447654724, loss=0.04173016920685768
I0305 19:33:48.122352 140222746547968 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.019316013902425766, loss=0.046164076775312424
I0305 19:34:09.109953 140222754940672 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.01675325259566307, loss=0.044094864279031754
I0305 19:34:29.632575 140222746547968 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.021485531702637672, loss=0.043007295578718185
I0305 19:34:50.630062 140222754940672 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.015440995804965496, loss=0.044762033969163895
I0305 19:35:11.585040 140222746547968 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.017198363319039345, loss=0.04341525211930275
I0305 19:35:32.457824 140222754940672 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.013450239785015583, loss=0.04795442149043083
I0305 19:35:52.944763 140222746547968 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.011748775839805603, loss=0.04013125225901604
I0305 19:36:13.530483 140363820438720 spec.py:321] Evaluating on the training split.
I0305 19:37:24.877138 140363820438720 spec.py:333] Evaluating on the validation split.
I0305 19:37:26.851495 140363820438720 spec.py:349] Evaluating on the test split.
I0305 19:37:28.724622 140363820438720 submission_runner.py:469] Time since start: 1476.05s, 	Step: 4600, 	{'train/accuracy': 0.9882197976112366, 'train/loss': 0.04027145355939865, 'train/mean_average_precision': 0.20361025530373042, 'validation/accuracy': 0.9855151772499084, 'validation/loss': 0.049441855400800705, 'validation/mean_average_precision': 0.18763431409016312, 'validation/num_examples': 43793, 'test/accuracy': 0.9845859408378601, 'test/loss': 0.052278075367212296, 'test/mean_average_precision': 0.18208174625533277, 'test/num_examples': 43793, 'score': 971.3430013656616, 'total_duration': 1476.0533137321472, 'accumulated_submission_time': 971.3430013656616, 'accumulated_eval_time': 504.51737689971924, 'accumulated_logging_time': 0.07322192192077637}
I0305 19:37:28.734318 140222268335872 logging_writer.py:48] [4600] accumulated_eval_time=504.517, accumulated_logging_time=0.0732219, accumulated_submission_time=971.343, global_step=4600, preemption_count=0, score=971.343, test/accuracy=0.984586, test/loss=0.0522781, test/mean_average_precision=0.182082, test/num_examples=43793, total_duration=1476.05, train/accuracy=0.98822, train/loss=0.0402715, train/mean_average_precision=0.20361, validation/accuracy=0.985515, validation/loss=0.0494419, validation/mean_average_precision=0.187634, validation/num_examples=43793
I0305 19:37:28.950945 140222259943168 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.01560440193861723, loss=0.04359053075313568
I0305 19:37:49.395359 140222268335872 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.011095519177615643, loss=0.04273388534784317
I0305 19:38:09.906672 140222259943168 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.023290546610951424, loss=0.04547926411032677
I0305 19:38:30.576666 140222268335872 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.018895089626312256, loss=0.04754509776830673
I0305 19:38:51.098189 140222259943168 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.01650230586528778, loss=0.04780067503452301
I0305 19:39:11.506011 140222268335872 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.012094799429178238, loss=0.04406541585922241
I0305 19:39:31.860637 140222259943168 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.016702750697731972, loss=0.04214318469166756
I0305 19:39:52.225645 140222268335872 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.01746772974729538, loss=0.042721252888441086
I0305 19:40:13.026515 140222259943168 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.015883464366197586, loss=0.043308328837156296
I0305 19:40:33.781373 140222268335872 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.018088240176439285, loss=0.043181393295526505
I0305 19:40:54.450790 140222259943168 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.027803486213088036, loss=0.0465010330080986
I0305 19:41:15.042955 140222268335872 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.02753489837050438, loss=0.04419398307800293
I0305 19:41:28.765242 140363820438720 spec.py:321] Evaluating on the training split.
I0305 19:42:39.213609 140363820438720 spec.py:333] Evaluating on the validation split.
I0305 19:42:41.169181 140363820438720 spec.py:349] Evaluating on the test split.
I0305 19:42:43.087269 140363820438720 submission_runner.py:469] Time since start: 1790.42s, 	Step: 5767, 	{'train/accuracy': 0.9882570505142212, 'train/loss': 0.04036799445748329, 'train/mean_average_precision': 0.2430359076454796, 'validation/accuracy': 0.9854433536529541, 'validation/loss': 0.05084119737148285, 'validation/mean_average_precision': 0.200664112440204, 'validation/num_examples': 43793, 'test/accuracy': 0.9844895005226135, 'test/loss': 0.05376238748431206, 'test/mean_average_precision': 0.1980760391763428, 'test/num_examples': 43793, 'score': 1211.3304266929626, 'total_duration': 1790.41583943367, 'accumulated_submission_time': 1211.3304266929626, 'accumulated_eval_time': 578.8392398357391, 'accumulated_logging_time': 0.09571456909179688}
I0305 19:42:43.097139 140222259943168 logging_writer.py:48] [5767] accumulated_eval_time=578.839, accumulated_logging_time=0.0957146, accumulated_submission_time=1211.33, global_step=5767, preemption_count=0, score=1211.33, test/accuracy=0.98449, test/loss=0.0537624, test/mean_average_precision=0.198076, test/num_examples=43793, total_duration=1790.42, train/accuracy=0.988257, train/loss=0.040368, train/mean_average_precision=0.243036, validation/accuracy=0.985443, validation/loss=0.0508412, validation/mean_average_precision=0.200664, validation/num_examples=43793
I0305 19:42:50.316286 140222268335872 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.015954844653606415, loss=0.045043885707855225
I0305 19:43:11.021083 140222259943168 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.013604576699435711, loss=0.04285296052694321
I0305 19:43:31.780595 140222268335872 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.018484612926840782, loss=0.04021182656288147
I0305 19:43:52.684991 140222259943168 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.015918025746941566, loss=0.04343380406498909
I0305 19:44:13.366590 140222268335872 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.020520921796560287, loss=0.042383596301078796
I0305 19:44:34.072212 140222259943168 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.013735003769397736, loss=0.04155023396015167
I0305 19:44:54.901069 140222268335872 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.01841709017753601, loss=0.04423307627439499
I0305 19:45:15.818728 140222259943168 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.01692228391766548, loss=0.03969775140285492
I0305 19:45:36.614279 140222268335872 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.015675194561481476, loss=0.04145380109548569
I0305 19:45:57.112125 140222259943168 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.016244513913989067, loss=0.04263673722743988
I0305 19:46:18.072317 140222268335872 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.023453589528799057, loss=0.03974654898047447
I0305 19:46:38.841788 140222259943168 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.02525615505874157, loss=0.040512219071388245
I0305 19:46:43.134905 140363820438720 spec.py:321] Evaluating on the training split.
I0305 19:47:52.591017 140363820438720 spec.py:333] Evaluating on the validation split.
I0305 19:47:54.549880 140363820438720 spec.py:349] Evaluating on the test split.
I0305 19:47:56.554921 140363820438720 submission_runner.py:469] Time since start: 2103.88s, 	Step: 6922, 	{'train/accuracy': 0.9890108704566956, 'train/loss': 0.037540603429079056, 'train/mean_average_precision': 0.26282216084091825, 'validation/accuracy': 0.9861074686050415, 'validation/loss': 0.04751013219356537, 'validation/mean_average_precision': 0.21572292536414434, 'validation/num_examples': 43793, 'test/accuracy': 0.985180675983429, 'test/loss': 0.05034453794360161, 'test/mean_average_precision': 0.2191434815913037, 'test/num_examples': 43793, 'score': 1451.3285562992096, 'total_duration': 2103.8836007118225, 'accumulated_submission_time': 1451.3285562992096, 'accumulated_eval_time': 652.2591917514801, 'accumulated_logging_time': 0.11483597755432129}
I0305 19:47:56.566645 140222268335872 logging_writer.py:48] [6922] accumulated_eval_time=652.259, accumulated_logging_time=0.114836, accumulated_submission_time=1451.33, global_step=6922, preemption_count=0, score=1451.33, test/accuracy=0.985181, test/loss=0.0503445, test/mean_average_precision=0.219143, test/num_examples=43793, total_duration=2103.88, train/accuracy=0.989011, train/loss=0.0375406, train/mean_average_precision=0.262822, validation/accuracy=0.986107, validation/loss=0.0475101, validation/mean_average_precision=0.215723, validation/num_examples=43793
I0305 19:48:13.258899 140222259943168 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.01757267862558365, loss=0.04132785275578499
I0305 19:48:33.777676 140222268335872 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.015439538285136223, loss=0.036825694143772125
I0305 19:48:54.325706 140222259943168 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.014728535898029804, loss=0.040511149913072586
I0305 19:49:15.025161 140222268335872 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.017771797254681587, loss=0.04255209118127823
I0305 19:49:35.722300 140222259943168 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.016097046434879303, loss=0.04337655380368233
I0305 19:49:56.394433 140222268335872 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.022106803953647614, loss=0.03915629908442497
I0305 19:50:17.050400 140222259943168 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.01739261858165264, loss=0.04087533801794052
I0305 19:50:37.452308 140222268335872 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.014919127337634563, loss=0.041044093668460846
I0305 19:50:58.297010 140222259943168 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.015038196928799152, loss=0.0389966256916523
I0305 19:51:19.149080 140222268335872 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.014431902207434177, loss=0.042537909001111984
I0305 19:51:39.617492 140222259943168 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.01818060874938965, loss=0.04109983518719673
I0305 19:51:56.604371 140363820438720 spec.py:321] Evaluating on the training split.
I0305 19:53:10.775305 140363820438720 spec.py:333] Evaluating on the validation split.
I0305 19:53:12.782218 140363820438720 spec.py:349] Evaluating on the test split.
I0305 19:53:14.735432 140363820438720 submission_runner.py:469] Time since start: 2422.06s, 	Step: 8082, 	{'train/accuracy': 0.9892480373382568, 'train/loss': 0.03607333451509476, 'train/mean_average_precision': 0.3052455519620596, 'validation/accuracy': 0.9862073063850403, 'validation/loss': 0.04661579430103302, 'validation/mean_average_precision': 0.22777933137678852, 'validation/num_examples': 43793, 'test/accuracy': 0.9852059483528137, 'test/loss': 0.04947572574019432, 'test/mean_average_precision': 0.22924792326080348, 'test/num_examples': 43793, 'score': 1691.3264362812042, 'total_duration': 2422.0639820098877, 'accumulated_submission_time': 1691.3264362812042, 'accumulated_eval_time': 730.390056848526, 'accumulated_logging_time': 0.13560819625854492}
I0305 19:53:14.744601 140222268335872 logging_writer.py:48] [8082] accumulated_eval_time=730.39, accumulated_logging_time=0.135608, accumulated_submission_time=1691.33, global_step=8082, preemption_count=0, score=1691.33, test/accuracy=0.985206, test/loss=0.0494757, test/mean_average_precision=0.229248, test/num_examples=43793, total_duration=2422.06, train/accuracy=0.989248, train/loss=0.0360733, train/mean_average_precision=0.305246, validation/accuracy=0.986207, validation/loss=0.0466158, validation/mean_average_precision=0.227779, validation/num_examples=43793
I0305 19:53:18.698647 140222259943168 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.02473822422325611, loss=0.04356645047664642
I0305 19:53:39.340598 140222268335872 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.01375754363834858, loss=0.041563887149095535
I0305 19:54:00.229214 140222259943168 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.013889338821172714, loss=0.041732415556907654
I0305 19:54:20.795191 140222268335872 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.014404174871742725, loss=0.04029914736747742
I0305 19:54:40.980367 140222259943168 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.01759517751634121, loss=0.04149375483393669
I0305 19:55:01.680712 140222268335872 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.016757003962993622, loss=0.04219905659556389
I0305 19:55:22.389086 140222259943168 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.01883869804441929, loss=0.03927478566765785
I0305 19:55:43.375678 140222268335872 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.01561580877751112, loss=0.03879019618034363
I0305 19:56:04.255529 140222259943168 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.014511837624013424, loss=0.04065892845392227
I0305 19:56:25.217324 140222268335872 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.01778649352490902, loss=0.03750365227460861
I0305 19:56:46.047476 140222259943168 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.014433750882744789, loss=0.038229260593652725
I0305 19:57:06.756183 140222268335872 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.02142278663814068, loss=0.04097764566540718
I0305 19:57:14.874362 140363820438720 spec.py:321] Evaluating on the training split.
I0305 19:58:26.084140 140363820438720 spec.py:333] Evaluating on the validation split.
I0305 19:58:28.142871 140363820438720 spec.py:349] Evaluating on the test split.
I0305 19:58:30.140837 140363820438720 submission_runner.py:469] Time since start: 2737.47s, 	Step: 9240, 	{'train/accuracy': 0.9897556900978088, 'train/loss': 0.035125937312841415, 'train/mean_average_precision': 0.32176956093950765, 'validation/accuracy': 0.9863181114196777, 'validation/loss': 0.04614488035440445, 'validation/mean_average_precision': 0.2262087337368127, 'validation/num_examples': 43793, 'test/accuracy': 0.9854485392570496, 'test/loss': 0.048699211329221725, 'test/mean_average_precision': 0.23241880039490134, 'test/num_examples': 43793, 'score': 1931.4163098335266, 'total_duration': 2737.4695103168488, 'accumulated_submission_time': 1931.4163098335266, 'accumulated_eval_time': 805.6564626693726, 'accumulated_logging_time': 0.1551961898803711}
I0305 19:58:30.151166 140222259943168 logging_writer.py:48] [9240] accumulated_eval_time=805.656, accumulated_logging_time=0.155196, accumulated_submission_time=1931.42, global_step=9240, preemption_count=0, score=1931.42, test/accuracy=0.985449, test/loss=0.0486992, test/mean_average_precision=0.232419, test/num_examples=43793, total_duration=2737.47, train/accuracy=0.989756, train/loss=0.0351259, train/mean_average_precision=0.32177, validation/accuracy=0.986318, validation/loss=0.0461449, validation/mean_average_precision=0.226209, validation/num_examples=43793
I0305 19:58:42.304248 140222268335872 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.012957270257174969, loss=0.040791139006614685
I0305 19:59:02.297988 140222259943168 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.015445507131516933, loss=0.03882289305329323
I0305 19:59:22.999500 140222268335872 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.02219380810856819, loss=0.038307685405015945
I0305 19:59:43.700266 140222259943168 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.01694619283080101, loss=0.0386803038418293
I0305 20:00:04.388214 140222268335872 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.01661578193306923, loss=0.042791370302438736
I0305 20:00:25.027590 140222259943168 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.020729701966047287, loss=0.0430913008749485
I0305 20:00:45.534917 140222268335872 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.017982956022024155, loss=0.044083066284656525
I0305 20:01:06.512030 140222259943168 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.01888819970190525, loss=0.042123567312955856
I0305 20:01:27.123181 140222268335872 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.015909504145383835, loss=0.037119362503290176
I0305 20:01:47.649434 140222259943168 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.01338426023721695, loss=0.039759840816259384
I0305 20:02:07.964054 140222268335872 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.014545019716024399, loss=0.040033381432294846
I0305 20:02:28.701184 140222259943168 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.014490675181150436, loss=0.0387144573032856
I0305 20:02:30.301870 140363820438720 spec.py:321] Evaluating on the training split.
I0305 20:03:40.259780 140363820438720 spec.py:333] Evaluating on the validation split.
I0305 20:03:42.401456 140363820438720 spec.py:349] Evaluating on the test split.
I0305 20:03:44.287671 140363820438720 submission_runner.py:469] Time since start: 3051.62s, 	Step: 10409, 	{'train/accuracy': 0.9899641871452332, 'train/loss': 0.03337293490767479, 'train/mean_average_precision': 0.3683845504739367, 'validation/accuracy': 0.9864703416824341, 'validation/loss': 0.045791447162628174, 'validation/mean_average_precision': 0.2442675683373668, 'validation/num_examples': 43793, 'test/accuracy': 0.9855790734291077, 'test/loss': 0.04849627614021301, 'test/mean_average_precision': 0.24397161627620667, 'test/num_examples': 43793, 'score': 2171.527834892273, 'total_duration': 3051.6162388324738, 'accumulated_submission_time': 2171.527834892273, 'accumulated_eval_time': 879.642086982727, 'accumulated_logging_time': 0.17492151260375977}
I0305 20:03:44.297001 140222268335872 logging_writer.py:48] [10409] accumulated_eval_time=879.642, accumulated_logging_time=0.174922, accumulated_submission_time=2171.53, global_step=10409, preemption_count=0, score=2171.53, test/accuracy=0.985579, test/loss=0.0484963, test/mean_average_precision=0.243972, test/num_examples=43793, total_duration=3051.62, train/accuracy=0.989964, train/loss=0.0333729, train/mean_average_precision=0.368385, validation/accuracy=0.98647, validation/loss=0.0457914, validation/mean_average_precision=0.244268, validation/num_examples=43793
I0305 20:04:02.902296 140222259943168 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.017795570194721222, loss=0.040768712759017944
I0305 20:04:23.460726 140222268335872 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.020701488479971886, loss=0.041586048901081085
I0305 20:04:44.078060 140222259943168 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.01572294346988201, loss=0.04094528406858444
I0305 20:05:04.554774 140222268335872 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.018998969346284866, loss=0.039222247898578644
I0305 20:05:24.820023 140222259943168 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.015480836853384972, loss=0.039625443518161774
I0305 20:05:45.022397 140222268335872 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.015890786424279213, loss=0.038150206208229065
I0305 20:06:05.382457 140222259943168 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.01638033241033554, loss=0.040247876197099686
I0305 20:06:25.815271 140222268335872 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.022373702377080917, loss=0.041948094964027405
I0305 20:06:45.868989 140222259943168 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.017403192818164825, loss=0.042059801518917084
I0305 20:07:05.916650 140222268335872 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.015376354567706585, loss=0.038608960807323456
I0305 20:07:26.233543 140222259943168 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.017338095232844353, loss=0.03914604336023331
I0305 20:07:44.451055 140363820438720 spec.py:321] Evaluating on the training split.
I0305 20:08:54.914499 140363820438720 spec.py:333] Evaluating on the validation split.
I0305 20:08:56.802765 140363820438720 spec.py:349] Evaluating on the test split.
I0305 20:08:58.692355 140363820438720 submission_runner.py:469] Time since start: 3366.02s, 	Step: 11590, 	{'train/accuracy': 0.9899992346763611, 'train/loss': 0.033259425312280655, 'train/mean_average_precision': 0.3756644475691037, 'validation/accuracy': 0.9865418076515198, 'validation/loss': 0.04566127061843872, 'validation/mean_average_precision': 0.2498464824551247, 'validation/num_examples': 43793, 'test/accuracy': 0.9855167865753174, 'test/loss': 0.048866964876651764, 'test/mean_average_precision': 0.23606531082623217, 'test/num_examples': 43793, 'score': 2411.6436138153076, 'total_duration': 3366.0210485458374, 'accumulated_submission_time': 2411.6436138153076, 'accumulated_eval_time': 953.8833372592926, 'accumulated_logging_time': 0.19335532188415527}
I0305 20:08:58.702698 140222268335872 logging_writer.py:48] [11590] accumulated_eval_time=953.883, accumulated_logging_time=0.193355, accumulated_submission_time=2411.64, global_step=11590, preemption_count=0, score=2411.64, test/accuracy=0.985517, test/loss=0.048867, test/mean_average_precision=0.236065, test/num_examples=43793, total_duration=3366.02, train/accuracy=0.989999, train/loss=0.0332594, train/mean_average_precision=0.375664, validation/accuracy=0.986542, validation/loss=0.0456613, validation/mean_average_precision=0.249846, validation/num_examples=43793
I0305 20:09:00.919569 140222259943168 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.01648731902241707, loss=0.0361139290034771
I0305 20:09:21.001888 140222268335872 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.018445512279868126, loss=0.03858619183301926
I0305 20:09:41.184966 140222259943168 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.016078660264611244, loss=0.0386236272752285
I0305 20:10:01.852402 140222268335872 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.01740143820643425, loss=0.039086926728487015
I0305 20:10:22.420837 140222259943168 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.014078263193368912, loss=0.03738206624984741
I0305 20:10:42.832156 140222268335872 logging_writer.py:48] [12100] global_step=12100, grad_norm=0.018216995522379875, loss=0.03812580183148384
I0305 20:11:03.339204 140222259943168 logging_writer.py:48] [12200] global_step=12200, grad_norm=0.0187421552836895, loss=0.03816784918308258
I0305 20:11:23.781565 140222268335872 logging_writer.py:48] [12300] global_step=12300, grad_norm=0.022484270855784416, loss=0.039032939821481705
I0305 20:11:43.908111 140222259943168 logging_writer.py:48] [12400] global_step=12400, grad_norm=0.0172710120677948, loss=0.03658287227153778
I0305 20:12:04.168474 140222268335872 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.031001942232251167, loss=0.035836294293403625
I0305 20:12:24.538157 140222259943168 logging_writer.py:48] [12600] global_step=12600, grad_norm=0.022112775593996048, loss=0.034917332231998444
I0305 20:12:45.022116 140222268335872 logging_writer.py:48] [12700] global_step=12700, grad_norm=0.018760889768600464, loss=0.04038309305906296
I0305 20:12:58.708945 140363820438720 spec.py:321] Evaluating on the training split.
I0305 20:14:08.924245 140363820438720 spec.py:333] Evaluating on the validation split.
I0305 20:14:10.865737 140363820438720 spec.py:349] Evaluating on the test split.
I0305 20:14:12.745944 140363820438720 submission_runner.py:469] Time since start: 3680.07s, 	Step: 12767, 	{'train/accuracy': 0.9904630184173584, 'train/loss': 0.03145638108253479, 'train/mean_average_precision': 0.4027964395081572, 'validation/accuracy': 0.9865328669548035, 'validation/loss': 0.04559135437011719, 'validation/mean_average_precision': 0.24295518988140477, 'validation/num_examples': 43793, 'test/accuracy': 0.9857884645462036, 'test/loss': 0.048334136605262756, 'test/mean_average_precision': 0.25348927086244094, 'test/num_examples': 43793, 'score': 2651.610383272171, 'total_duration': 3680.0744869709015, 'accumulated_submission_time': 2651.610383272171, 'accumulated_eval_time': 1027.9201366901398, 'accumulated_logging_time': 0.2128150463104248}
I0305 20:14:12.756958 140222259943168 logging_writer.py:48] [12767] accumulated_eval_time=1027.92, accumulated_logging_time=0.212815, accumulated_submission_time=2651.61, global_step=12767, preemption_count=0, score=2651.61, test/accuracy=0.985788, test/loss=0.0483341, test/mean_average_precision=0.253489, test/num_examples=43793, total_duration=3680.07, train/accuracy=0.990463, train/loss=0.0314564, train/mean_average_precision=0.402796, validation/accuracy=0.986533, validation/loss=0.0455914, validation/mean_average_precision=0.242955, validation/num_examples=43793
I0305 20:14:19.606265 140222268335872 logging_writer.py:48] [12800] global_step=12800, grad_norm=0.015544405207037926, loss=0.03579992428421974
I0305 20:14:41.488335 140222259943168 logging_writer.py:48] [12900] global_step=12900, grad_norm=0.0162945669144392, loss=0.040323615074157715
I0305 20:15:01.939306 140222268335872 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.018065081909298897, loss=0.035925086587667465
I0305 20:15:22.451594 140222259943168 logging_writer.py:48] [13100] global_step=13100, grad_norm=0.014930843375623226, loss=0.034282978624105453
I0305 20:15:42.727930 140222268335872 logging_writer.py:48] [13200] global_step=13200, grad_norm=0.019901858642697334, loss=0.037277836352586746
I0305 20:16:03.025234 140222259943168 logging_writer.py:48] [13300] global_step=13300, grad_norm=0.013754447922110558, loss=0.03661211580038071
I0305 20:16:23.225467 140222268335872 logging_writer.py:48] [13400] global_step=13400, grad_norm=0.02087770588696003, loss=0.03666473925113678
I0305 20:16:43.433547 140222259943168 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.01731928624212742, loss=0.03848635032773018
I0305 20:17:03.803771 140222268335872 logging_writer.py:48] [13600] global_step=13600, grad_norm=0.01982492208480835, loss=0.03752652183175087
I0305 20:17:24.139570 140222259943168 logging_writer.py:48] [13700] global_step=13700, grad_norm=0.02181857079267502, loss=0.035364072769880295
I0305 20:17:44.236954 140222268335872 logging_writer.py:48] [13800] global_step=13800, grad_norm=0.01574689894914627, loss=0.03672069311141968
I0305 20:18:04.498773 140222259943168 logging_writer.py:48] [13900] global_step=13900, grad_norm=0.01867794618010521, loss=0.038527458906173706
I0305 20:18:12.825977 140363820438720 spec.py:321] Evaluating on the training split.
I0305 20:19:22.120747 140363820438720 spec.py:333] Evaluating on the validation split.
I0305 20:19:24.024498 140363820438720 spec.py:349] Evaluating on the test split.
I0305 20:19:25.877387 140363820438720 submission_runner.py:469] Time since start: 3993.21s, 	Step: 13942, 	{'train/accuracy': 0.9908450245857239, 'train/loss': 0.03052341751754284, 'train/mean_average_precision': 0.4179890842539039, 'validation/accuracy': 0.9868012070655823, 'validation/loss': 0.04501211643218994, 'validation/mean_average_precision': 0.2664134345530244, 'validation/num_examples': 43793, 'test/accuracy': 0.9858992099761963, 'test/loss': 0.048055361956357956, 'test/mean_average_precision': 0.2509302488676304, 'test/num_examples': 43793, 'score': 2891.6398046016693, 'total_duration': 3993.205961227417, 'accumulated_submission_time': 2891.6398046016693, 'accumulated_eval_time': 1100.9713778495789, 'accumulated_logging_time': 0.2329106330871582}
I0305 20:19:25.887366 140222268335872 logging_writer.py:48] [13942] accumulated_eval_time=1100.97, accumulated_logging_time=0.232911, accumulated_submission_time=2891.64, global_step=13942, preemption_count=0, score=2891.64, test/accuracy=0.985899, test/loss=0.0480554, test/mean_average_precision=0.25093, test/num_examples=43793, total_duration=3993.21, train/accuracy=0.990845, train/loss=0.0305234, train/mean_average_precision=0.417989, validation/accuracy=0.986801, validation/loss=0.0450121, validation/mean_average_precision=0.266413, validation/num_examples=43793
I0305 20:19:38.026074 140222259943168 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.02227182500064373, loss=0.039487142115831375
I0305 20:19:58.544711 140222268335872 logging_writer.py:48] [14100] global_step=14100, grad_norm=0.02118571661412716, loss=0.03747809678316116
I0305 20:20:19.158122 140222259943168 logging_writer.py:48] [14200] global_step=14200, grad_norm=0.02711891010403633, loss=0.03884598985314369
I0305 20:20:39.527833 140222268335872 logging_writer.py:48] [14300] global_step=14300, grad_norm=0.020554831251502037, loss=0.04033345356583595
I0305 20:20:59.655626 140222259943168 logging_writer.py:48] [14400] global_step=14400, grad_norm=0.02273036167025566, loss=0.039754025638103485
I0305 20:21:20.146124 140222268335872 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.018584102392196655, loss=0.03671160712838173
I0305 20:21:40.562086 140222259943168 logging_writer.py:48] [14600] global_step=14600, grad_norm=0.020515499636530876, loss=0.03931298479437828
I0305 20:22:01.065665 140222268335872 logging_writer.py:48] [14700] global_step=14700, grad_norm=0.019977688789367676, loss=0.03522362560033798
I0305 20:22:21.510411 140222259943168 logging_writer.py:48] [14800] global_step=14800, grad_norm=0.020947767421603203, loss=0.03736339509487152
I0305 20:22:41.960691 140222268335872 logging_writer.py:48] [14900] global_step=14900, grad_norm=0.020405147224664688, loss=0.03655713051557541
I0305 20:23:02.300762 140222259943168 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.022442039102315903, loss=0.03711366280913353
I0305 20:23:22.806622 140222268335872 logging_writer.py:48] [15100] global_step=15100, grad_norm=0.025236407294869423, loss=0.03919969126582146
I0305 20:23:25.982737 140363820438720 spec.py:321] Evaluating on the training split.
I0305 20:24:36.229356 140363820438720 spec.py:333] Evaluating on the validation split.
I0305 20:24:38.123321 140363820438720 spec.py:349] Evaluating on the test split.
I0305 20:24:39.975821 140363820438720 submission_runner.py:469] Time since start: 4307.30s, 	Step: 15116, 	{'train/accuracy': 0.9909579753875732, 'train/loss': 0.02959286980330944, 'train/mean_average_precision': 0.4459145046739112, 'validation/accuracy': 0.9867666959762573, 'validation/loss': 0.045350901782512665, 'validation/mean_average_precision': 0.2659193869161913, 'validation/num_examples': 43793, 'test/accuracy': 0.9858490824699402, 'test/loss': 0.048369958996772766, 'test/mean_average_precision': 0.2517446267115604, 'test/num_examples': 43793, 'score': 3131.6940693855286, 'total_duration': 4307.304433345795, 'accumulated_submission_time': 3131.6940693855286, 'accumulated_eval_time': 1174.964334487915, 'accumulated_logging_time': 0.2520325183868408}
I0305 20:24:39.986701 140222259943168 logging_writer.py:48] [15116] accumulated_eval_time=1174.96, accumulated_logging_time=0.252033, accumulated_submission_time=3131.69, global_step=15116, preemption_count=0, score=3131.69, test/accuracy=0.985849, test/loss=0.04837, test/mean_average_precision=0.251745, test/num_examples=43793, total_duration=4307.3, train/accuracy=0.990958, train/loss=0.0295929, train/mean_average_precision=0.445915, validation/accuracy=0.986767, validation/loss=0.0453509, validation/mean_average_precision=0.265919, validation/num_examples=43793
I0305 20:24:57.614273 140222268335872 logging_writer.py:48] [15200] global_step=15200, grad_norm=0.019995734095573425, loss=0.035074710845947266
I0305 20:25:18.128349 140222259943168 logging_writer.py:48] [15300] global_step=15300, grad_norm=0.02329615317285061, loss=0.03743356466293335
I0305 20:25:38.682013 140222268335872 logging_writer.py:48] [15400] global_step=15400, grad_norm=0.025920314714312553, loss=0.037769198417663574
I0305 20:25:59.304918 140222259943168 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.023639600723981857, loss=0.03611624613404274
I0305 20:26:20.158581 140222268335872 logging_writer.py:48] [15600] global_step=15600, grad_norm=0.026119768619537354, loss=0.03756541758775711
I0305 20:26:40.841762 140222259943168 logging_writer.py:48] [15700] global_step=15700, grad_norm=0.025319842621684074, loss=0.035275425761938095
I0305 20:27:01.060220 140222268335872 logging_writer.py:48] [15800] global_step=15800, grad_norm=0.024587275460362434, loss=0.03563666716217995
I0305 20:27:21.283524 140222259943168 logging_writer.py:48] [15900] global_step=15900, grad_norm=0.02186349779367447, loss=0.03702101111412048
I0305 20:27:41.324021 140222268335872 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.021118266507983208, loss=0.03603612631559372
I0305 20:28:01.411477 140222259943168 logging_writer.py:48] [16100] global_step=16100, grad_norm=0.029957227408885956, loss=0.03517512232065201
I0305 20:28:21.997389 140222268335872 logging_writer.py:48] [16200] global_step=16200, grad_norm=0.02389942668378353, loss=0.037247803062200546
I0305 20:28:40.029296 140363820438720 spec.py:321] Evaluating on the training split.
I0305 20:29:51.306572 140363820438720 spec.py:333] Evaluating on the validation split.
I0305 20:29:53.209554 140363820438720 spec.py:349] Evaluating on the test split.
I0305 20:29:55.074785 140363820438720 submission_runner.py:469] Time since start: 4622.40s, 	Step: 16289, 	{'train/accuracy': 0.991315484046936, 'train/loss': 0.02865144982933998, 'train/mean_average_precision': 0.4605190510161351, 'validation/accuracy': 0.9867236614227295, 'validation/loss': 0.045332882553339005, 'validation/mean_average_precision': 0.26645693185329355, 'validation/num_examples': 43793, 'test/accuracy': 0.9859017133712769, 'test/loss': 0.04827573895454407, 'test/mean_average_precision': 0.2597271364786487, 'test/num_examples': 43793, 'score': 3371.693094968796, 'total_duration': 4622.403352975845, 'accumulated_submission_time': 3371.693094968796, 'accumulated_eval_time': 1250.0096497535706, 'accumulated_logging_time': 0.2761106491088867}
I0305 20:29:55.085326 140222259943168 logging_writer.py:48] [16289] accumulated_eval_time=1250.01, accumulated_logging_time=0.276111, accumulated_submission_time=3371.69, global_step=16289, preemption_count=0, score=3371.69, test/accuracy=0.985902, test/loss=0.0482757, test/mean_average_precision=0.259727, test/num_examples=43793, total_duration=4622.4, train/accuracy=0.991315, train/loss=0.0286514, train/mean_average_precision=0.460519, validation/accuracy=0.986724, validation/loss=0.0453329, validation/mean_average_precision=0.266457, validation/num_examples=43793
I0305 20:29:57.508973 140222268335872 logging_writer.py:48] [16300] global_step=16300, grad_norm=0.021293453872203827, loss=0.03228801116347313
I0305 20:30:18.246309 140222259943168 logging_writer.py:48] [16400] global_step=16400, grad_norm=0.02267553098499775, loss=0.037531305104494095
I0305 20:30:38.876271 140222268335872 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.02288319729268551, loss=0.038733527064323425
I0305 20:30:59.632151 140222259943168 logging_writer.py:48] [16600] global_step=16600, grad_norm=0.021723415702581406, loss=0.032948773354291916
I0305 20:31:20.451435 140222268335872 logging_writer.py:48] [16700] global_step=16700, grad_norm=0.02167166769504547, loss=0.0354878231883049
I0305 20:31:41.340498 140222259943168 logging_writer.py:48] [16800] global_step=16800, grad_norm=0.022744322195649147, loss=0.03521861881017685
I0305 20:32:01.792749 140222268335872 logging_writer.py:48] [16900] global_step=16900, grad_norm=0.020245272666215897, loss=0.033812787383794785
I0305 20:32:22.384360 140222259943168 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.023010199889540672, loss=0.034978192299604416
I0305 20:32:43.116385 140222268335872 logging_writer.py:48] [17100] global_step=17100, grad_norm=0.023245546966791153, loss=0.037245240062475204
I0305 20:33:03.999462 140222259943168 logging_writer.py:48] [17200] global_step=17200, grad_norm=0.025118425488471985, loss=0.03311081975698471
I0305 20:33:24.829164 140222268335872 logging_writer.py:48] [17300] global_step=17300, grad_norm=0.022959096357226372, loss=0.03465479239821434
I0305 20:33:45.386813 140222259943168 logging_writer.py:48] [17400] global_step=17400, grad_norm=0.02276587300002575, loss=0.03575168922543526
I0305 20:33:55.212250 140363820438720 spec.py:321] Evaluating on the training split.
I0305 20:35:04.327140 140363820438720 spec.py:333] Evaluating on the validation split.
I0305 20:35:06.224445 140363820438720 spec.py:349] Evaluating on the test split.
I0305 20:35:08.120170 140363820438720 submission_runner.py:469] Time since start: 4935.45s, 	Step: 17449, 	{'train/accuracy': 0.9916352033615112, 'train/loss': 0.027396799996495247, 'train/mean_average_precision': 0.49815429828365054, 'validation/accuracy': 0.9868487119674683, 'validation/loss': 0.04557383060455322, 'validation/mean_average_precision': 0.27047165166112924, 'validation/num_examples': 43793, 'test/accuracy': 0.9859055280685425, 'test/loss': 0.04865854233503342, 'test/mean_average_precision': 0.25498610518335185, 'test/num_examples': 43793, 'score': 3611.7752442359924, 'total_duration': 4935.448865890503, 'accumulated_submission_time': 3611.7752442359924, 'accumulated_eval_time': 1322.9175214767456, 'accumulated_logging_time': 0.30101895332336426}
I0305 20:35:08.130557 140222268335872 logging_writer.py:48] [17449] accumulated_eval_time=1322.92, accumulated_logging_time=0.301019, accumulated_submission_time=3611.78, global_step=17449, preemption_count=0, score=3611.78, test/accuracy=0.985906, test/loss=0.0486585, test/mean_average_precision=0.254986, test/num_examples=43793, total_duration=4935.45, train/accuracy=0.991635, train/loss=0.0273968, train/mean_average_precision=0.498154, validation/accuracy=0.986849, validation/loss=0.0455738, validation/mean_average_precision=0.270472, validation/num_examples=43793
I0305 20:35:18.868559 140222259943168 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.02854357287287712, loss=0.0351024754345417
I0305 20:35:40.080081 140222268335872 logging_writer.py:48] [17600] global_step=17600, grad_norm=0.02331497147679329, loss=0.037061095237731934
I0305 20:36:00.883368 140222259943168 logging_writer.py:48] [17700] global_step=17700, grad_norm=0.019335750490427017, loss=0.03303694725036621
I0305 20:36:21.061254 140222268335872 logging_writer.py:48] [17800] global_step=17800, grad_norm=0.023828715085983276, loss=0.03621334210038185
I0305 20:36:41.326648 140222259943168 logging_writer.py:48] [17900] global_step=17900, grad_norm=0.023779310286045074, loss=0.034445878118276596
I0305 20:37:02.019350 140222268335872 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.024698330089449883, loss=0.03636513650417328
I0305 20:37:22.884385 140222259943168 logging_writer.py:48] [18100] global_step=18100, grad_norm=0.023599354550242424, loss=0.032870933413505554
I0305 20:37:43.326560 140222268335872 logging_writer.py:48] [18200] global_step=18200, grad_norm=0.023725997656583786, loss=0.03585558757185936
I0305 20:38:03.857467 140222259943168 logging_writer.py:48] [18300] global_step=18300, grad_norm=0.029180914163589478, loss=0.03576499596238136
I0305 20:38:24.472143 140222268335872 logging_writer.py:48] [18400] global_step=18400, grad_norm=0.021533606573939323, loss=0.032344378530979156
I0305 20:38:45.117807 140222259943168 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.03268234431743622, loss=0.03833504021167755
I0305 20:39:05.623560 140222268335872 logging_writer.py:48] [18600] global_step=18600, grad_norm=0.02320423536002636, loss=0.03288513049483299
I0305 20:39:08.160895 140363820438720 spec.py:321] Evaluating on the training split.
I0305 20:40:19.652564 140363820438720 spec.py:333] Evaluating on the validation split.
I0305 20:40:21.572219 140363820438720 spec.py:349] Evaluating on the test split.
I0305 20:40:23.489168 140363820438720 submission_runner.py:469] Time since start: 5250.82s, 	Step: 18613, 	{'train/accuracy': 0.9914032220840454, 'train/loss': 0.027891235426068306, 'train/mean_average_precision': 0.4929594016302982, 'validation/accuracy': 0.9868150353431702, 'validation/loss': 0.045948855578899384, 'validation/mean_average_precision': 0.2691772984888078, 'validation/num_examples': 43793, 'test/accuracy': 0.9859210848808289, 'test/loss': 0.04889877513051033, 'test/mean_average_precision': 0.25495938142014885, 'test/num_examples': 43793, 'score': 3851.7677948474884, 'total_duration': 5250.817736387253, 'accumulated_submission_time': 3851.7677948474884, 'accumulated_eval_time': 1398.2456181049347, 'accumulated_logging_time': 0.32051706314086914}
I0305 20:40:23.500835 140222259943168 logging_writer.py:48] [18613] accumulated_eval_time=1398.25, accumulated_logging_time=0.320517, accumulated_submission_time=3851.77, global_step=18613, preemption_count=0, score=3851.77, test/accuracy=0.985921, test/loss=0.0488988, test/mean_average_precision=0.254959, test/num_examples=43793, total_duration=5250.82, train/accuracy=0.991403, train/loss=0.0278912, train/mean_average_precision=0.492959, validation/accuracy=0.986815, validation/loss=0.0459489, validation/mean_average_precision=0.269177, validation/num_examples=43793
I0305 20:40:42.013039 140222268335872 logging_writer.py:48] [18700] global_step=18700, grad_norm=0.03224502503871918, loss=0.03361302241683006
I0305 20:41:02.618361 140222259943168 logging_writer.py:48] [18800] global_step=18800, grad_norm=0.026226390153169632, loss=0.03726601228117943
I0305 20:41:23.399410 140222268335872 logging_writer.py:48] [18900] global_step=18900, grad_norm=0.02582639455795288, loss=0.035431742668151855
I0305 20:41:44.406333 140222259943168 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.022313054651021957, loss=0.030852191150188446
I0305 20:42:04.853184 140222268335872 logging_writer.py:48] [19100] global_step=19100, grad_norm=0.024994967505335808, loss=0.03301478177309036
I0305 20:42:25.665679 140222259943168 logging_writer.py:48] [19200] global_step=19200, grad_norm=0.027313869446516037, loss=0.03472914546728134
I0305 20:42:46.707847 140222268335872 logging_writer.py:48] [19300] global_step=19300, grad_norm=0.025274476036429405, loss=0.03212987631559372
I0305 20:43:07.881675 140222259943168 logging_writer.py:48] [19400] global_step=19400, grad_norm=0.030516242608428, loss=0.032138045877218246
I0305 20:43:28.325112 140222268335872 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.028392789885401726, loss=0.033030323684215546
I0305 20:43:49.305892 140222259943168 logging_writer.py:48] [19600] global_step=19600, grad_norm=0.025012198835611343, loss=0.03388609737157822
I0305 20:44:10.056046 140222268335872 logging_writer.py:48] [19700] global_step=19700, grad_norm=0.02414226159453392, loss=0.03469834476709366
I0305 20:44:23.536628 140363820438720 spec.py:321] Evaluating on the training split.
I0305 20:45:34.889424 140363820438720 spec.py:333] Evaluating on the validation split.
I0305 20:45:36.799248 140363820438720 spec.py:349] Evaluating on the test split.
I0305 20:45:38.681735 140363820438720 submission_runner.py:469] Time since start: 5566.01s, 	Step: 19766, 	{'train/accuracy': 0.9921523928642273, 'train/loss': 0.02557269111275673, 'train/mean_average_precision': 0.5347280590370153, 'validation/accuracy': 0.9867849946022034, 'validation/loss': 0.045625489205121994, 'validation/mean_average_precision': 0.2632517919729572, 'validation/num_examples': 43793, 'test/accuracy': 0.9858638048171997, 'test/loss': 0.04876778647303581, 'test/mean_average_precision': 0.25786114738612004, 'test/num_examples': 43793, 'score': 4091.763608455658, 'total_duration': 5566.010301113129, 'accumulated_submission_time': 4091.763608455658, 'accumulated_eval_time': 1473.3905458450317, 'accumulated_logging_time': 0.34156203269958496}
I0305 20:45:38.693511 140222259943168 logging_writer.py:48] [19766] accumulated_eval_time=1473.39, accumulated_logging_time=0.341562, accumulated_submission_time=4091.76, global_step=19766, preemption_count=0, score=4091.76, test/accuracy=0.985864, test/loss=0.0487678, test/mean_average_precision=0.257861, test/num_examples=43793, total_duration=5566.01, train/accuracy=0.992152, train/loss=0.0255727, train/mean_average_precision=0.534728, validation/accuracy=0.986785, validation/loss=0.0456255, validation/mean_average_precision=0.263252, validation/num_examples=43793
I0305 20:45:45.934224 140222268335872 logging_writer.py:48] [19800] global_step=19800, grad_norm=0.0344153568148613, loss=0.037590399384498596
I0305 20:46:06.468507 140222259943168 logging_writer.py:48] [19900] global_step=19900, grad_norm=0.030055640265345573, loss=0.033936869353055954
I0305 20:46:26.660631 140222268335872 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.026006409898400307, loss=0.034498583525419235
I0305 20:46:46.907646 140222259943168 logging_writer.py:48] [20100] global_step=20100, grad_norm=0.029053395614027977, loss=0.03594028204679489
I0305 20:47:07.400513 140222268335872 logging_writer.py:48] [20200] global_step=20200, grad_norm=0.02626490220427513, loss=0.033096037805080414
I0305 20:47:27.724138 140222259943168 logging_writer.py:48] [20300] global_step=20300, grad_norm=0.02644694782793522, loss=0.03214513882994652
I0305 20:47:48.172375 140222268335872 logging_writer.py:48] [20400] global_step=20400, grad_norm=0.030608128756284714, loss=0.03553716093301773
I0305 20:48:08.808088 140222259943168 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.02699332684278488, loss=0.03177999332547188
I0305 20:48:29.509206 140222268335872 logging_writer.py:48] [20600] global_step=20600, grad_norm=0.037290383130311966, loss=0.03743598237633705
I0305 20:48:50.095516 140222259943168 logging_writer.py:48] [20700] global_step=20700, grad_norm=0.031163861975073814, loss=0.03603970259428024
I0305 20:49:10.779514 140222268335872 logging_writer.py:48] [20800] global_step=20800, grad_norm=0.030005885288119316, loss=0.033812157809734344
I0305 20:49:31.091050 140222259943168 logging_writer.py:48] [20900] global_step=20900, grad_norm=0.0277409665286541, loss=0.030526811257004738
I0305 20:49:38.825307 140363820438720 spec.py:321] Evaluating on the training split.
I0305 20:50:48.675323 140363820438720 spec.py:333] Evaluating on the validation split.
I0305 20:50:50.635199 140363820438720 spec.py:349] Evaluating on the test split.
I0305 20:50:52.548954 140363820438720 submission_runner.py:469] Time since start: 5879.88s, 	Step: 20939, 	{'train/accuracy': 0.9921424388885498, 'train/loss': 0.025625178590416908, 'train/mean_average_precision': 0.5318542426044397, 'validation/accuracy': 0.986750066280365, 'validation/loss': 0.04661884158849716, 'validation/mean_average_precision': 0.25969459497949793, 'validation/num_examples': 43793, 'test/accuracy': 0.9859046936035156, 'test/loss': 0.04983795806765556, 'test/mean_average_precision': 0.2536135266196528, 'test/num_examples': 43793, 'score': 4331.854537010193, 'total_duration': 5879.877614498138, 'accumulated_submission_time': 4331.854537010193, 'accumulated_eval_time': 1547.1141111850739, 'accumulated_logging_time': 0.36356163024902344}
I0305 20:50:52.559933 140222268335872 logging_writer.py:48] [20939] accumulated_eval_time=1547.11, accumulated_logging_time=0.363562, accumulated_submission_time=4331.85, global_step=20939, preemption_count=0, score=4331.85, test/accuracy=0.985905, test/loss=0.049838, test/mean_average_precision=0.253614, test/num_examples=43793, total_duration=5879.88, train/accuracy=0.992142, train/loss=0.0256252, train/mean_average_precision=0.531854, validation/accuracy=0.98675, validation/loss=0.0466188, validation/mean_average_precision=0.259695, validation/num_examples=43793
I0305 20:51:05.240799 140222259943168 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.028065865859389305, loss=0.03345172107219696
I0305 20:51:25.793723 140222268335872 logging_writer.py:48] [21100] global_step=21100, grad_norm=0.0315111018717289, loss=0.033459823578596115
I0305 20:51:46.043204 140222259943168 logging_writer.py:48] [21200] global_step=21200, grad_norm=0.027517879381775856, loss=0.03395833820104599
I0305 20:52:06.810615 140222268335872 logging_writer.py:48] [21300] global_step=21300, grad_norm=0.03094114363193512, loss=0.031880393624305725
I0305 20:52:27.741799 140222259943168 logging_writer.py:48] [21400] global_step=21400, grad_norm=0.027686690911650658, loss=0.03268470615148544
I0305 20:52:48.274364 140222268335872 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.03195375204086304, loss=0.03176458925008774
I0305 20:53:08.732147 140222259943168 logging_writer.py:48] [21600] global_step=21600, grad_norm=0.024733932688832283, loss=0.030696675181388855
I0305 20:53:29.249585 140222268335872 logging_writer.py:48] [21700] global_step=21700, grad_norm=0.036868490278720856, loss=0.03236759454011917
I0305 20:53:49.843845 140222259943168 logging_writer.py:48] [21800] global_step=21800, grad_norm=0.02977568842470646, loss=0.029635142534971237
I0305 20:54:10.469341 140222268335872 logging_writer.py:48] [21900] global_step=21900, grad_norm=0.03061727061867714, loss=0.033726394176483154
I0305 20:54:30.907478 140222259943168 logging_writer.py:48] [22000] global_step=22000, grad_norm=0.029302313923835754, loss=0.03254549577832222
I0305 20:54:51.518669 140222268335872 logging_writer.py:48] [22100] global_step=22100, grad_norm=0.037522170692682266, loss=0.03234531730413437
I0305 20:54:52.712404 140363820438720 spec.py:321] Evaluating on the training split.
I0305 20:56:02.811710 140363820438720 spec.py:333] Evaluating on the validation split.
I0305 20:56:04.868461 140363820438720 spec.py:349] Evaluating on the test split.
I0305 20:56:06.708658 140363820438720 submission_runner.py:469] Time since start: 6194.04s, 	Step: 22107, 	{'train/accuracy': 0.9924676418304443, 'train/loss': 0.024221783503890038, 'train/mean_average_precision': 0.5742881235202819, 'validation/accuracy': 0.9866676330566406, 'validation/loss': 0.04723362997174263, 'validation/mean_average_precision': 0.2563955970690123, 'validation/num_examples': 43793, 'test/accuracy': 0.9858195781707764, 'test/loss': 0.0506041944026947, 'test/mean_average_precision': 0.2497159215150077, 'test/num_examples': 43793, 'score': 4571.967358827591, 'total_duration': 6194.03724861145, 'accumulated_submission_time': 4571.967358827591, 'accumulated_eval_time': 1621.110211610794, 'accumulated_logging_time': 0.38364315032958984}
I0305 20:56:06.719387 140222259943168 logging_writer.py:48] [22107] accumulated_eval_time=1621.11, accumulated_logging_time=0.383643, accumulated_submission_time=4571.97, global_step=22107, preemption_count=0, score=4571.97, test/accuracy=0.98582, test/loss=0.0506042, test/mean_average_precision=0.249716, test/num_examples=43793, total_duration=6194.04, train/accuracy=0.992468, train/loss=0.0242218, train/mean_average_precision=0.574288, validation/accuracy=0.986668, validation/loss=0.0472336, validation/mean_average_precision=0.256396, validation/num_examples=43793
I0305 20:56:26.220909 140222268335872 logging_writer.py:48] [22200] global_step=22200, grad_norm=0.028511622920632362, loss=0.033049728721380234
I0305 20:56:46.735822 140222259943168 logging_writer.py:48] [22300] global_step=22300, grad_norm=0.032247480005025864, loss=0.03360956907272339
I0305 20:57:07.383197 140222268335872 logging_writer.py:48] [22400] global_step=22400, grad_norm=0.03175218775868416, loss=0.03386213257908821
I0305 20:57:28.313481 140222259943168 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.03429533541202545, loss=0.03200182691216469
I0305 20:57:48.842195 140222268335872 logging_writer.py:48] [22600] global_step=22600, grad_norm=0.030786240473389626, loss=0.030099185183644295
I0305 20:58:09.426124 140222259943168 logging_writer.py:48] [22700] global_step=22700, grad_norm=0.03541213274002075, loss=0.03312632441520691
I0305 20:58:30.135607 140222268335872 logging_writer.py:48] [22800] global_step=22800, grad_norm=0.029806269332766533, loss=0.031031299382448196
I0305 20:58:50.827744 140222259943168 logging_writer.py:48] [22900] global_step=22900, grad_norm=0.037054695188999176, loss=0.0345589779317379
I0305 20:59:11.323740 140222268335872 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.0331556499004364, loss=0.03173447400331497
I0305 20:59:31.715907 140222259943168 logging_writer.py:48] [23100] global_step=23100, grad_norm=0.03988827019929886, loss=0.03300855681300163
I0305 20:59:52.147915 140222268335872 logging_writer.py:48] [23200] global_step=23200, grad_norm=0.03187208250164986, loss=0.030306601896882057
I0305 21:00:06.893016 140363820438720 spec.py:321] Evaluating on the training split.
I0305 21:01:17.000195 140363820438720 spec.py:333] Evaluating on the validation split.
I0305 21:01:18.924927 140363820438720 spec.py:349] Evaluating on the test split.
I0305 21:01:20.829258 140363820438720 submission_runner.py:469] Time since start: 6508.16s, 	Step: 23272, 	{'train/accuracy': 0.9925446510314941, 'train/loss': 0.024123508483171463, 'train/mean_average_precision': 0.56754111893297, 'validation/accuracy': 0.9867184162139893, 'validation/loss': 0.047484125941991806, 'validation/mean_average_precision': 0.2529417099267002, 'validation/num_examples': 43793, 'test/accuracy': 0.985888659954071, 'test/loss': 0.05070467293262482, 'test/mean_average_precision': 0.25117025305895657, 'test/num_examples': 43793, 'score': 4812.099123001099, 'total_duration': 6508.157835483551, 'accumulated_submission_time': 4812.099123001099, 'accumulated_eval_time': 1695.0462892055511, 'accumulated_logging_time': 0.403522253036499}
I0305 21:01:20.840962 140222259943168 logging_writer.py:48] [23272] accumulated_eval_time=1695.05, accumulated_logging_time=0.403522, accumulated_submission_time=4812.1, global_step=23272, preemption_count=0, score=4812.1, test/accuracy=0.985889, test/loss=0.0507047, test/mean_average_precision=0.25117, test/num_examples=43793, total_duration=6508.16, train/accuracy=0.992545, train/loss=0.0241235, train/mean_average_precision=0.567541, validation/accuracy=0.986718, validation/loss=0.0474841, validation/mean_average_precision=0.252942, validation/num_examples=43793
I0305 21:01:26.966216 140222268335872 logging_writer.py:48] [23300] global_step=23300, grad_norm=0.037473659962415695, loss=0.03333202749490738
I0305 21:01:47.461746 140222259943168 logging_writer.py:48] [23400] global_step=23400, grad_norm=0.029154924675822258, loss=0.030691200867295265
I0305 21:02:08.120225 140222268335872 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.04064130038022995, loss=0.02969217486679554
I0305 21:02:28.781286 140222259943168 logging_writer.py:48] [23600] global_step=23600, grad_norm=0.04431496188044548, loss=0.030898572877049446
I0305 21:02:49.512649 140222268335872 logging_writer.py:48] [23700] global_step=23700, grad_norm=0.030123544856905937, loss=0.032241884618997574
I0305 21:03:10.163292 140222259943168 logging_writer.py:48] [23800] global_step=23800, grad_norm=0.031181687489151955, loss=0.0303950197994709
I0305 21:03:30.842823 140222268335872 logging_writer.py:48] [23900] global_step=23900, grad_norm=0.04423652961850166, loss=0.03313777968287468
I0305 21:03:51.514672 140222259943168 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.03487280011177063, loss=0.032751910388469696
I0305 21:04:12.057417 140222268335872 logging_writer.py:48] [24100] global_step=24100, grad_norm=0.03656219318509102, loss=0.03121640346944332
I0305 21:04:32.766696 140222259943168 logging_writer.py:48] [24200] global_step=24200, grad_norm=0.034133922308683395, loss=0.027792993932962418
I0305 21:04:53.262619 140222268335872 logging_writer.py:48] [24300] global_step=24300, grad_norm=0.036567263305187225, loss=0.03407270461320877
I0305 21:05:13.829411 140222259943168 logging_writer.py:48] [24400] global_step=24400, grad_norm=0.03511606156826019, loss=0.031409043818712234
I0305 21:05:20.920276 140363820438720 spec.py:321] Evaluating on the training split.
I0305 21:06:31.287329 140363820438720 spec.py:333] Evaluating on the validation split.
I0305 21:06:33.208763 140363820438720 spec.py:349] Evaluating on the test split.
I0305 21:06:35.148373 140363820438720 submission_runner.py:469] Time since start: 6822.48s, 	Step: 24436, 	{'train/accuracy': 0.9931237697601318, 'train/loss': 0.02208654209971428, 'train/mean_average_precision': 0.6171013098708127, 'validation/accuracy': 0.9867033958435059, 'validation/loss': 0.04750249162316322, 'validation/mean_average_precision': 0.25523857886918505, 'validation/num_examples': 43793, 'test/accuracy': 0.985874354839325, 'test/loss': 0.05075371637940407, 'test/mean_average_precision': 0.2533083323572525, 'test/num_examples': 43793, 'score': 5052.138689517975, 'total_duration': 6822.477037191391, 'accumulated_submission_time': 5052.138689517975, 'accumulated_eval_time': 1769.2743089199066, 'accumulated_logging_time': 0.4243290424346924}
I0305 21:06:35.161130 140222268335872 logging_writer.py:48] [24436] accumulated_eval_time=1769.27, accumulated_logging_time=0.424329, accumulated_submission_time=5052.14, global_step=24436, preemption_count=0, score=5052.14, test/accuracy=0.985874, test/loss=0.0507537, test/mean_average_precision=0.253308, test/num_examples=43793, total_duration=6822.48, train/accuracy=0.993124, train/loss=0.0220865, train/mean_average_precision=0.617101, validation/accuracy=0.986703, validation/loss=0.0475025, validation/mean_average_precision=0.255239, validation/num_examples=43793
I0305 21:06:48.714321 140222259943168 logging_writer.py:48] [24500] global_step=24500, grad_norm=0.03344753757119179, loss=0.029363805428147316
I0305 21:07:09.442400 140222268335872 logging_writer.py:48] [24600] global_step=24600, grad_norm=0.041413553059101105, loss=0.03211422264575958
I0305 21:07:30.179953 140222259943168 logging_writer.py:48] [24700] global_step=24700, grad_norm=0.03193650022149086, loss=0.029605859890580177
I0305 21:07:50.884777 140222268335872 logging_writer.py:48] [24800] global_step=24800, grad_norm=0.036992866545915604, loss=0.030776463449001312
I0305 21:08:11.710920 140222259943168 logging_writer.py:48] [24900] global_step=24900, grad_norm=0.03898393735289574, loss=0.03341848775744438
I0305 21:08:32.478656 140222268335872 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.043426480144262314, loss=0.03196626156568527
I0305 21:08:53.126672 140222259943168 logging_writer.py:48] [25100] global_step=25100, grad_norm=0.03982824087142944, loss=0.02916412428021431
I0305 21:09:14.012839 140222268335872 logging_writer.py:48] [25200] global_step=25200, grad_norm=0.03885385021567345, loss=0.03151984512805939
I0305 21:09:34.866355 140222259943168 logging_writer.py:48] [25300] global_step=25300, grad_norm=0.041656386107206345, loss=0.030675116926431656
I0305 21:09:55.648830 140222268335872 logging_writer.py:48] [25400] global_step=25400, grad_norm=0.032865770161151886, loss=0.03081316500902176
I0305 21:10:16.041669 140222259943168 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.04247603565454483, loss=0.03371739014983177
I0305 21:10:35.356705 140363820438720 spec.py:321] Evaluating on the training split.
I0305 21:11:48.374327 140363820438720 spec.py:333] Evaluating on the validation split.
I0305 21:11:50.274805 140363820438720 spec.py:349] Evaluating on the test split.
I0305 21:11:52.234163 140363820438720 submission_runner.py:469] Time since start: 7139.56s, 	Step: 25596, 	{'train/accuracy': 0.9927656054496765, 'train/loss': 0.023470507934689522, 'train/mean_average_precision': 0.5575933764617704, 'validation/accuracy': 0.986524760723114, 'validation/loss': 0.048156507313251495, 'validation/mean_average_precision': 0.2512442215046983, 'validation/num_examples': 43793, 'test/accuracy': 0.9856178760528564, 'test/loss': 0.05143974348902702, 'test/mean_average_precision': 0.24417891179917411, 'test/num_examples': 43793, 'score': 5292.2924609184265, 'total_duration': 7139.5627455711365, 'accumulated_submission_time': 5292.2924609184265, 'accumulated_eval_time': 1846.1516077518463, 'accumulated_logging_time': 0.4477527141571045}
I0305 21:11:52.246910 140222268335872 logging_writer.py:48] [25596] accumulated_eval_time=1846.15, accumulated_logging_time=0.447753, accumulated_submission_time=5292.29, global_step=25596, preemption_count=0, score=5292.29, test/accuracy=0.985618, test/loss=0.0514397, test/mean_average_precision=0.244179, test/num_examples=43793, total_duration=7139.56, train/accuracy=0.992766, train/loss=0.0234705, train/mean_average_precision=0.557593, validation/accuracy=0.986525, validation/loss=0.0481565, validation/mean_average_precision=0.251244, validation/num_examples=43793
I0305 21:11:53.304033 140222259943168 logging_writer.py:48] [25600] global_step=25600, grad_norm=0.03498099371790886, loss=0.029915638267993927
I0305 21:12:14.252083 140222268335872 logging_writer.py:48] [25700] global_step=25700, grad_norm=0.039135005325078964, loss=0.031049778684973717
I0305 21:12:35.063920 140222259943168 logging_writer.py:48] [25800] global_step=25800, grad_norm=0.03658435866236687, loss=0.029774824157357216
I0305 21:12:56.060920 140222268335872 logging_writer.py:48] [25900] global_step=25900, grad_norm=0.048580147325992584, loss=0.03103041462600231
I0305 21:13:16.985746 140222259943168 logging_writer.py:48] [26000] global_step=26000, grad_norm=0.037513069808483124, loss=0.02972833812236786
I0305 21:13:38.035127 140222268335872 logging_writer.py:48] [26100] global_step=26100, grad_norm=0.039110712707042694, loss=0.030083727091550827
I0305 21:13:58.717628 140222259943168 logging_writer.py:48] [26200] global_step=26200, grad_norm=0.03726770356297493, loss=0.03177083283662796
I0305 21:14:19.519526 140222268335872 logging_writer.py:48] [26300] global_step=26300, grad_norm=0.038824621587991714, loss=0.03203094005584717
I0305 21:14:40.288578 140222259943168 logging_writer.py:48] [26400] global_step=26400, grad_norm=0.03957892581820488, loss=0.030087227001786232
I0305 21:15:01.305136 140222268335872 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.041087523102760315, loss=0.03241448104381561
I0305 21:15:22.208833 140222259943168 logging_writer.py:48] [26600] global_step=26600, grad_norm=0.03888610377907753, loss=0.0304983202368021
I0305 21:15:42.882138 140222268335872 logging_writer.py:48] [26700] global_step=26700, grad_norm=0.03935379907488823, loss=0.029457949101924896
I0305 21:15:52.374616 140363820438720 spec.py:321] Evaluating on the training split.
I0305 21:17:03.124101 140363820438720 spec.py:333] Evaluating on the validation split.
I0305 21:17:05.039840 140363820438720 spec.py:349] Evaluating on the test split.
I0305 21:17:06.931143 140363820438720 submission_runner.py:469] Time since start: 7454.26s, 	Step: 26746, 	{'train/accuracy': 0.9936712384223938, 'train/loss': 0.02041718363761902, 'train/mean_average_precision': 0.6470453023222926, 'validation/accuracy': 0.9865689873695374, 'validation/loss': 0.048597317188978195, 'validation/mean_average_precision': 0.2529171942291944, 'validation/num_examples': 43793, 'test/accuracy': 0.9857850670814514, 'test/loss': 0.05172213166952133, 'test/mean_average_precision': 0.2542164261043428, 'test/num_examples': 43793, 'score': 5532.379944801331, 'total_duration': 7454.259835243225, 'accumulated_submission_time': 5532.379944801331, 'accumulated_eval_time': 1920.7080821990967, 'accumulated_logging_time': 0.4693737030029297}
I0305 21:17:06.943092 140222259943168 logging_writer.py:48] [26746] accumulated_eval_time=1920.71, accumulated_logging_time=0.469374, accumulated_submission_time=5532.38, global_step=26746, preemption_count=0, score=5532.38, test/accuracy=0.985785, test/loss=0.0517221, test/mean_average_precision=0.254216, test/num_examples=43793, total_duration=7454.26, train/accuracy=0.993671, train/loss=0.0204172, train/mean_average_precision=0.647045, validation/accuracy=0.986569, validation/loss=0.0485973, validation/mean_average_precision=0.252917, validation/num_examples=43793
I0305 21:17:18.330038 140222268335872 logging_writer.py:48] [26800] global_step=26800, grad_norm=0.04278549551963806, loss=0.032602451741695404
I0305 21:17:39.204146 140222259943168 logging_writer.py:48] [26900] global_step=26900, grad_norm=0.038939468562603, loss=0.02943485789000988
I0305 21:17:59.958854 140222268335872 logging_writer.py:48] [27000] global_step=27000, grad_norm=0.04355195537209511, loss=0.03012564405798912
I0305 21:18:20.476035 140222259943168 logging_writer.py:48] [27100] global_step=27100, grad_norm=0.037952110171318054, loss=0.030889667570590973
I0305 21:18:40.926285 140222268335872 logging_writer.py:48] [27200] global_step=27200, grad_norm=0.04097014293074608, loss=0.029956098645925522
I0305 21:19:01.503406 140222259943168 logging_writer.py:48] [27300] global_step=27300, grad_norm=0.04132302850484848, loss=0.030038688331842422
I0305 21:19:22.338090 140222268335872 logging_writer.py:48] [27400] global_step=27400, grad_norm=0.043583862483501434, loss=0.03173254057765007
I0305 21:19:43.020909 140222259943168 logging_writer.py:48] [27500] global_step=27500, grad_norm=0.04950929433107376, loss=0.02944638952612877
I0305 21:20:03.745326 140222268335872 logging_writer.py:48] [27600] global_step=27600, grad_norm=0.03967510536313057, loss=0.02812727726995945
I0305 21:20:24.845216 140222259943168 logging_writer.py:48] [27700] global_step=27700, grad_norm=0.039077673107385635, loss=0.029149767011404037
I0305 21:20:45.775107 140222268335872 logging_writer.py:48] [27800] global_step=27800, grad_norm=0.04496242105960846, loss=0.029667356982827187
I0305 21:21:06.408246 140222259943168 logging_writer.py:48] [27900] global_step=27900, grad_norm=0.04520013928413391, loss=0.03007480688393116
I0305 21:21:07.025093 140363820438720 spec.py:321] Evaluating on the training split.
I0305 21:22:20.378303 140363820438720 spec.py:333] Evaluating on the validation split.
I0305 21:22:22.273957 140363820438720 spec.py:349] Evaluating on the test split.
I0305 21:22:24.145115 140363820438720 submission_runner.py:469] Time since start: 7771.47s, 	Step: 27904, 	{'train/accuracy': 0.9932960867881775, 'train/loss': 0.021234173327684402, 'train/mean_average_precision': 0.6290925965485867, 'validation/accuracy': 0.9865536093711853, 'validation/loss': 0.04995546489953995, 'validation/mean_average_precision': 0.25205164028418564, 'validation/num_examples': 43793, 'test/accuracy': 0.9856469035148621, 'test/loss': 0.05338302254676819, 'test/mean_average_precision': 0.24693568646084285, 'test/num_examples': 43793, 'score': 5772.423107147217, 'total_duration': 7771.473697900772, 'accumulated_submission_time': 5772.423107147217, 'accumulated_eval_time': 1997.8279411792755, 'accumulated_logging_time': 0.49146437644958496}
I0305 21:22:24.156667 140222268335872 logging_writer.py:48] [27904] accumulated_eval_time=1997.83, accumulated_logging_time=0.491464, accumulated_submission_time=5772.42, global_step=27904, preemption_count=0, score=5772.42, test/accuracy=0.985647, test/loss=0.053383, test/mean_average_precision=0.246936, test/num_examples=43793, total_duration=7771.47, train/accuracy=0.993296, train/loss=0.0212342, train/mean_average_precision=0.629093, validation/accuracy=0.986554, validation/loss=0.0499555, validation/mean_average_precision=0.252052, validation/num_examples=43793
I0305 21:22:44.413051 140222259943168 logging_writer.py:48] [28000] global_step=28000, grad_norm=0.040590107440948486, loss=0.028134172782301903
I0305 21:23:05.435137 140222268335872 logging_writer.py:48] [28100] global_step=28100, grad_norm=0.04578609764575958, loss=0.030658163130283356
I0305 21:23:25.941876 140222259943168 logging_writer.py:48] [28200] global_step=28200, grad_norm=0.03978389874100685, loss=0.02802998200058937
I0305 21:23:46.458830 140222268335872 logging_writer.py:48] [28300] global_step=28300, grad_norm=0.04693347588181496, loss=0.029697773978114128
I0305 21:24:06.937955 140222259943168 logging_writer.py:48] [28400] global_step=28400, grad_norm=0.04191292077302933, loss=0.029409857466816902
I0305 21:24:27.649508 140222268335872 logging_writer.py:48] [28500] global_step=28500, grad_norm=0.04731909558176994, loss=0.028830867260694504
I0305 21:24:48.225298 140222259943168 logging_writer.py:48] [28600] global_step=28600, grad_norm=0.04237601161003113, loss=0.028661904856562614
I0305 21:25:08.899882 140222268335872 logging_writer.py:48] [28700] global_step=28700, grad_norm=0.04672052711248398, loss=0.02845262549817562
I0305 21:25:29.489543 140222259943168 logging_writer.py:48] [28800] global_step=28800, grad_norm=0.04267922788858414, loss=0.028392987325787544
I0305 21:25:49.873564 140222268335872 logging_writer.py:48] [28900] global_step=28900, grad_norm=0.0483236163854599, loss=0.029383201152086258
I0305 21:26:10.334724 140222259943168 logging_writer.py:48] [29000] global_step=29000, grad_norm=0.041758790612220764, loss=0.027484094724059105
I0305 21:26:24.258655 140363820438720 spec.py:321] Evaluating on the training split.
I0305 21:27:32.504210 140363820438720 spec.py:333] Evaluating on the validation split.
I0305 21:27:34.413977 140363820438720 spec.py:349] Evaluating on the test split.
I0305 21:27:36.295909 140363820438720 submission_runner.py:469] Time since start: 8083.62s, 	Step: 29068, 	{'train/accuracy': 0.9945397973060608, 'train/loss': 0.017898812890052795, 'train/mean_average_precision': 0.702509288258941, 'validation/accuracy': 0.9864886403083801, 'validation/loss': 0.05044397711753845, 'validation/mean_average_precision': 0.2551320353944225, 'validation/num_examples': 43793, 'test/accuracy': 0.9855685830116272, 'test/loss': 0.054015256464481354, 'test/mean_average_precision': 0.2448104392682111, 'test/num_examples': 43793, 'score': 6012.278198480606, 'total_duration': 8083.624459981918, 'accumulated_submission_time': 6012.278198480606, 'accumulated_eval_time': 2069.8650031089783, 'accumulated_logging_time': 0.7192282676696777}
I0305 21:27:36.307971 140222268335872 logging_writer.py:48] [29068] accumulated_eval_time=2069.87, accumulated_logging_time=0.719228, accumulated_submission_time=6012.28, global_step=29068, preemption_count=0, score=6012.28, test/accuracy=0.985569, test/loss=0.0540153, test/mean_average_precision=0.24481, test/num_examples=43793, total_duration=8083.62, train/accuracy=0.99454, train/loss=0.0178988, train/mean_average_precision=0.702509, validation/accuracy=0.986489, validation/loss=0.050444, validation/mean_average_precision=0.255132, validation/num_examples=43793
I0305 21:27:43.130970 140222259943168 logging_writer.py:48] [29100] global_step=29100, grad_norm=0.043067168444395065, loss=0.02926950342953205
I0305 21:28:03.827587 140222268335872 logging_writer.py:48] [29200] global_step=29200, grad_norm=0.04440980404615402, loss=0.028475021943449974
I0305 21:28:24.564279 140222259943168 logging_writer.py:48] [29300] global_step=29300, grad_norm=0.04145754501223564, loss=0.02810131199657917
I0305 21:28:45.203037 140222268335872 logging_writer.py:48] [29400] global_step=29400, grad_norm=0.04205615445971489, loss=0.028525689616799355
I0305 21:29:05.770342 140222259943168 logging_writer.py:48] [29500] global_step=29500, grad_norm=0.047604840248823166, loss=0.028616787865757942
I0305 21:29:26.340533 140222268335872 logging_writer.py:48] [29600] global_step=29600, grad_norm=0.04191495478153229, loss=0.028605272993445396
I0305 21:29:46.845547 140222259943168 logging_writer.py:48] [29700] global_step=29700, grad_norm=0.050658609718084335, loss=0.027195435017347336
I0305 21:30:07.666624 140222268335872 logging_writer.py:48] [29800] global_step=29800, grad_norm=0.046128857880830765, loss=0.028070440515875816
I0305 21:30:28.487606 140222259943168 logging_writer.py:48] [29900] global_step=29900, grad_norm=0.04552297666668892, loss=0.028476078063249588
I0305 21:30:49.093369 140222268335872 logging_writer.py:48] [30000] global_step=30000, grad_norm=0.05748474597930908, loss=0.029482131823897362
I0305 21:31:09.938295 140222259943168 logging_writer.py:48] [30100] global_step=30100, grad_norm=0.04697253555059433, loss=0.028310086578130722
I0305 21:31:30.619099 140222268335872 logging_writer.py:48] [30200] global_step=30200, grad_norm=0.05471719801425934, loss=0.0293806791305542
I0305 21:31:36.416956 140363820438720 spec.py:321] Evaluating on the training split.
I0305 21:32:48.078959 140363820438720 spec.py:333] Evaluating on the validation split.
I0305 21:32:50.029997 140363820438720 spec.py:349] Evaluating on the test split.
I0305 21:32:51.877716 140363820438720 submission_runner.py:469] Time since start: 8399.21s, 	Step: 30229, 	{'train/accuracy': 0.9939932823181152, 'train/loss': 0.01932244375348091, 'train/mean_average_precision': 0.6636635893329268, 'validation/accuracy': 0.9864256978034973, 'validation/loss': 0.05072980746626854, 'validation/mean_average_precision': 0.24764399389192257, 'validation/num_examples': 43793, 'test/accuracy': 0.985616147518158, 'test/loss': 0.05419835448265076, 'test/mean_average_precision': 0.2404561881519999, 'test/num_examples': 43793, 'score': 6252.348296880722, 'total_duration': 8399.206347227097, 'accumulated_submission_time': 6252.348296880722, 'accumulated_eval_time': 2145.3256483078003, 'accumulated_logging_time': 0.7407033443450928}
I0305 21:32:51.890273 140222259943168 logging_writer.py:48] [30229] accumulated_eval_time=2145.33, accumulated_logging_time=0.740703, accumulated_submission_time=6252.35, global_step=30229, preemption_count=0, score=6252.35, test/accuracy=0.985616, test/loss=0.0541984, test/mean_average_precision=0.240456, test/num_examples=43793, total_duration=8399.21, train/accuracy=0.993993, train/loss=0.0193224, train/mean_average_precision=0.663664, validation/accuracy=0.986426, validation/loss=0.0507298, validation/mean_average_precision=0.247644, validation/num_examples=43793
I0305 21:33:06.747677 140222268335872 logging_writer.py:48] [30300] global_step=30300, grad_norm=0.050879478454589844, loss=0.028885653242468834
I0305 21:33:27.249446 140222259943168 logging_writer.py:48] [30400] global_step=30400, grad_norm=0.04466795176267624, loss=0.027201225981116295
I0305 21:33:47.803945 140222268335872 logging_writer.py:48] [30500] global_step=30500, grad_norm=0.04784078523516655, loss=0.029331091791391373
I0305 21:34:08.695494 140222259943168 logging_writer.py:48] [30600] global_step=30600, grad_norm=0.04725795239210129, loss=0.02734019234776497
I0305 21:34:29.340616 140222268335872 logging_writer.py:48] [30700] global_step=30700, grad_norm=0.044370412826538086, loss=0.026081770658493042
I0305 21:34:49.948050 140222259943168 logging_writer.py:48] [30800] global_step=30800, grad_norm=0.050368357449769974, loss=0.028468813747167587
I0305 21:35:10.555588 140222268335872 logging_writer.py:48] [30900] global_step=30900, grad_norm=0.05037274584174156, loss=0.028126390650868416
I0305 21:35:31.175663 140222259943168 logging_writer.py:48] [31000] global_step=31000, grad_norm=0.05648922920227051, loss=0.027809157967567444
I0305 21:35:51.865527 140222268335872 logging_writer.py:48] [31100] global_step=31100, grad_norm=0.0505569651722908, loss=0.02662729285657406
I0305 21:36:12.466172 140222259943168 logging_writer.py:48] [31200] global_step=31200, grad_norm=0.041666798293590546, loss=0.026576343923807144
I0305 21:36:33.139867 140222268335872 logging_writer.py:48] [31300] global_step=31300, grad_norm=0.04722047597169876, loss=0.026828253641724586
I0305 21:36:52.069094 140363820438720 spec.py:321] Evaluating on the training split.
I0305 21:38:01.720464 140363820438720 spec.py:333] Evaluating on the validation split.
I0305 21:38:03.694370 140363820438720 spec.py:349] Evaluating on the test split.
I0305 21:38:05.621180 140363820438720 submission_runner.py:469] Time since start: 8712.95s, 	Step: 31391, 	{'train/accuracy': 0.995140552520752, 'train/loss': 0.016331171616911888, 'train/mean_average_precision': 0.7212529360969668, 'validation/accuracy': 0.9863550662994385, 'validation/loss': 0.051335208117961884, 'validation/mean_average_precision': 0.24298281059268267, 'validation/num_examples': 43793, 'test/accuracy': 0.9855344295501709, 'test/loss': 0.05500173196196556, 'test/mean_average_precision': 0.24131749288499924, 'test/num_examples': 43793, 'score': 6492.486770868301, 'total_duration': 8712.949797391891, 'accumulated_submission_time': 6492.486770868301, 'accumulated_eval_time': 2218.877608060837, 'accumulated_logging_time': 0.7624268531799316}
I0305 21:38:05.633222 140222259943168 logging_writer.py:48] [31391] accumulated_eval_time=2218.88, accumulated_logging_time=0.762427, accumulated_submission_time=6492.49, global_step=31391, preemption_count=0, score=6492.49, test/accuracy=0.985534, test/loss=0.0550017, test/mean_average_precision=0.241317, test/num_examples=43793, total_duration=8712.95, train/accuracy=0.995141, train/loss=0.0163312, train/mean_average_precision=0.721253, validation/accuracy=0.986355, validation/loss=0.0513352, validation/mean_average_precision=0.242983, validation/num_examples=43793
I0305 21:38:07.683503 140222268335872 logging_writer.py:48] [31400] global_step=31400, grad_norm=0.046180546283721924, loss=0.026730727404356003
I0305 21:38:30.699828 140222259943168 logging_writer.py:48] [31500] global_step=31500, grad_norm=0.05186603218317032, loss=0.02663985639810562
I0305 21:38:51.109277 140222268335872 logging_writer.py:48] [31600] global_step=31600, grad_norm=0.05249594897031784, loss=0.027940308675169945
I0305 21:39:11.842120 140222259943168 logging_writer.py:48] [31700] global_step=31700, grad_norm=0.06022209674119949, loss=0.028405217453837395
I0305 21:39:32.719917 140222268335872 logging_writer.py:48] [31800] global_step=31800, grad_norm=0.050527650862932205, loss=0.02782166562974453
I0305 21:39:53.476050 140222259943168 logging_writer.py:48] [31900] global_step=31900, grad_norm=0.0505501814186573, loss=0.027565475553274155
I0305 21:40:14.212089 140222268335872 logging_writer.py:48] [32000] global_step=32000, grad_norm=0.05075033754110336, loss=0.028483793139457703
I0305 21:40:35.009084 140222259943168 logging_writer.py:48] [32100] global_step=32100, grad_norm=0.049490898847579956, loss=0.02707161381840706
I0305 21:40:55.863292 140222268335872 logging_writer.py:48] [32200] global_step=32200, grad_norm=0.04812810197472572, loss=0.02636859193444252
I0305 21:41:16.446888 140222259943168 logging_writer.py:48] [32300] global_step=32300, grad_norm=0.04499497637152672, loss=0.02676590345799923
I0305 21:41:37.525517 140222268335872 logging_writer.py:48] [32400] global_step=32400, grad_norm=0.04943818598985672, loss=0.027941852807998657
I0305 21:41:58.130851 140222259943168 logging_writer.py:48] [32500] global_step=32500, grad_norm=0.05427322909235954, loss=0.026528919115662575
I0305 21:42:05.668991 140363820438720 spec.py:321] Evaluating on the training split.
I0305 21:43:16.258533 140363820438720 spec.py:333] Evaluating on the validation split.
I0305 21:43:18.221142 140363820438720 spec.py:349] Evaluating on the test split.
I0305 21:43:20.128153 140363820438720 submission_runner.py:469] Time since start: 9027.46s, 	Step: 32536, 	{'train/accuracy': 0.9941064715385437, 'train/loss': 0.01876217871904373, 'train/mean_average_precision': 0.6691806428191119, 'validation/accuracy': 0.9862353205680847, 'validation/loss': 0.052192412316799164, 'validation/mean_average_precision': 0.2458687938084052, 'validation/num_examples': 43793, 'test/accuracy': 0.985355019569397, 'test/loss': 0.05582445114850998, 'test/mean_average_precision': 0.24176554860146993, 'test/num_examples': 43793, 'score': 6732.483732700348, 'total_duration': 9027.456737041473, 'accumulated_submission_time': 6732.483732700348, 'accumulated_eval_time': 2293.3366119861603, 'accumulated_logging_time': 0.7835638523101807}
I0305 21:43:20.140856 140222268335872 logging_writer.py:48] [32536] accumulated_eval_time=2293.34, accumulated_logging_time=0.783564, accumulated_submission_time=6732.48, global_step=32536, preemption_count=0, score=6732.48, test/accuracy=0.985355, test/loss=0.0558245, test/mean_average_precision=0.241766, test/num_examples=43793, total_duration=9027.46, train/accuracy=0.994106, train/loss=0.0187622, train/mean_average_precision=0.669181, validation/accuracy=0.986235, validation/loss=0.0521924, validation/mean_average_precision=0.245869, validation/num_examples=43793
I0305 21:43:33.573420 140222259943168 logging_writer.py:48] [32600] global_step=32600, grad_norm=0.04868277162313461, loss=0.026614435017108917
I0305 21:43:54.071126 140222268335872 logging_writer.py:48] [32700] global_step=32700, grad_norm=0.05030851438641548, loss=0.02678193897008896
I0305 21:44:14.715842 140222259943168 logging_writer.py:48] [32800] global_step=32800, grad_norm=0.04938558116555214, loss=0.0272674597799778
I0305 21:44:35.494974 140222268335872 logging_writer.py:48] [32900] global_step=32900, grad_norm=0.046264126896858215, loss=0.02548305317759514
I0305 21:44:56.408122 140222259943168 logging_writer.py:48] [33000] global_step=33000, grad_norm=0.05261870473623276, loss=0.027268877252936363
I0305 21:45:17.252128 140222268335872 logging_writer.py:48] [33100] global_step=33100, grad_norm=0.0557895302772522, loss=0.02684136852622032
I0305 21:45:37.832726 140222259943168 logging_writer.py:48] [33200] global_step=33200, grad_norm=0.05181368440389633, loss=0.025588367134332657
I0305 21:45:58.544346 140222268335872 logging_writer.py:48] [33300] global_step=33300, grad_norm=0.04893884062767029, loss=0.02651476114988327
I0305 21:46:19.381247 140222259943168 logging_writer.py:48] [33400] global_step=33400, grad_norm=0.05666279420256615, loss=0.02638879232108593
I0305 21:46:39.847456 140222268335872 logging_writer.py:48] [33500] global_step=33500, grad_norm=0.049860626459121704, loss=0.027032651007175446
I0305 21:47:00.604588 140222259943168 logging_writer.py:48] [33600] global_step=33600, grad_norm=0.05189540609717369, loss=0.026286641135811806
I0305 21:47:20.327615 140363820438720 spec.py:321] Evaluating on the training split.
I0305 21:48:34.107919 140363820438720 spec.py:333] Evaluating on the validation split.
I0305 21:48:36.050732 140363820438720 spec.py:349] Evaluating on the test split.
I0305 21:48:37.930155 140363820438720 submission_runner.py:469] Time since start: 9345.26s, 	Step: 33692, 	{'train/accuracy': 0.995521068572998, 'train/loss': 0.014790726825594902, 'train/mean_average_precision': 0.7565829009105594, 'validation/accuracy': 0.9863092303276062, 'validation/loss': 0.05365309119224548, 'validation/mean_average_precision': 0.2418288608040333, 'validation/num_examples': 43793, 'test/accuracy': 0.9854729771614075, 'test/loss': 0.057275690138339996, 'test/mean_average_precision': 0.23541710163186375, 'test/num_examples': 43793, 'score': 6972.630781650543, 'total_duration': 9345.25880765915, 'accumulated_submission_time': 6972.630781650543, 'accumulated_eval_time': 2370.939062356949, 'accumulated_logging_time': 0.8055140972137451}
I0305 21:48:37.942946 140222268335872 logging_writer.py:48] [33692] accumulated_eval_time=2370.94, accumulated_logging_time=0.805514, accumulated_submission_time=6972.63, global_step=33692, preemption_count=0, score=6972.63, test/accuracy=0.985473, test/loss=0.0572757, test/mean_average_precision=0.235417, test/num_examples=43793, total_duration=9345.26, train/accuracy=0.995521, train/loss=0.0147907, train/mean_average_precision=0.756583, validation/accuracy=0.986309, validation/loss=0.0536531, validation/mean_average_precision=0.241829, validation/num_examples=43793
I0305 21:48:39.770057 140222259943168 logging_writer.py:48] [33700] global_step=33700, grad_norm=0.048042479902505875, loss=0.0269150510430336
I0305 21:49:00.633322 140222268335872 logging_writer.py:48] [33800] global_step=33800, grad_norm=0.05136226490139961, loss=0.026341496035456657
I0305 21:49:21.378051 140222259943168 logging_writer.py:48] [33900] global_step=33900, grad_norm=0.050903551280498505, loss=0.024995679035782814
I0305 21:49:42.177094 140222268335872 logging_writer.py:48] [34000] global_step=34000, grad_norm=0.052032195031642914, loss=0.02589956484735012
I0305 21:50:02.941481 140222259943168 logging_writer.py:48] [34100] global_step=34100, grad_norm=0.053554583340883255, loss=0.025328198447823524
I0305 21:50:23.850175 140222268335872 logging_writer.py:48] [34200] global_step=34200, grad_norm=0.048627156764268875, loss=0.025657057762145996
I0305 21:50:44.437269 140222259943168 logging_writer.py:48] [34300] global_step=34300, grad_norm=0.05477873235940933, loss=0.0263541117310524
I0305 21:51:05.602842 140222268335872 logging_writer.py:48] [34400] global_step=34400, grad_norm=0.049950748682022095, loss=0.025699863210320473
I0305 21:51:26.145701 140222259943168 logging_writer.py:48] [34500] global_step=34500, grad_norm=0.047179728746414185, loss=0.024115709587931633
I0305 21:51:46.812206 140222268335872 logging_writer.py:48] [34600] global_step=34600, grad_norm=0.05377376824617386, loss=0.02384084463119507
I0305 21:52:07.514087 140222259943168 logging_writer.py:48] [34700] global_step=34700, grad_norm=0.05851248279213905, loss=0.025281215086579323
I0305 21:52:28.337691 140222268335872 logging_writer.py:48] [34800] global_step=34800, grad_norm=0.0512784942984581, loss=0.025668252259492874
I0305 21:52:38.073672 140363820438720 spec.py:321] Evaluating on the training split.
I0305 21:53:46.963760 140363820438720 spec.py:333] Evaluating on the validation split.
I0305 21:53:48.979317 140363820438720 spec.py:349] Evaluating on the test split.
I0305 21:53:50.866839 140363820438720 submission_runner.py:469] Time since start: 9658.20s, 	Step: 34848, 	{'train/accuracy': 0.9945482015609741, 'train/loss': 0.01712888292968273, 'train/mean_average_precision': 0.7144302342316529, 'validation/accuracy': 0.9862341284751892, 'validation/loss': 0.0542571134865284, 'validation/mean_average_precision': 0.24011761526995143, 'validation/num_examples': 43793, 'test/accuracy': 0.9854022264480591, 'test/loss': 0.05815305933356285, 'test/mean_average_precision': 0.23598551251244176, 'test/num_examples': 43793, 'score': 7212.722177028656, 'total_duration': 9658.195533275604, 'accumulated_submission_time': 7212.722177028656, 'accumulated_eval_time': 2443.7321813106537, 'accumulated_logging_time': 0.827545166015625}
I0305 21:53:50.879473 140222259943168 logging_writer.py:48] [34848] accumulated_eval_time=2443.73, accumulated_logging_time=0.827545, accumulated_submission_time=7212.72, global_step=34848, preemption_count=0, score=7212.72, test/accuracy=0.985402, test/loss=0.0581531, test/mean_average_precision=0.235986, test/num_examples=43793, total_duration=9658.2, train/accuracy=0.994548, train/loss=0.0171289, train/mean_average_precision=0.71443, validation/accuracy=0.986234, validation/loss=0.0542571, validation/mean_average_precision=0.240118, validation/num_examples=43793
I0305 21:54:01.980708 140222268335872 logging_writer.py:48] [34900] global_step=34900, grad_norm=0.0586201548576355, loss=0.02501990832388401
I0305 21:54:22.743822 140222259943168 logging_writer.py:48] [35000] global_step=35000, grad_norm=0.05931435152888298, loss=0.025660278275609016
I0305 21:54:43.273284 140222268335872 logging_writer.py:48] [35100] global_step=35100, grad_norm=0.056564707309007645, loss=0.025967709720134735
I0305 21:55:03.577759 140222259943168 logging_writer.py:48] [35200] global_step=35200, grad_norm=0.05274796113371849, loss=0.024699345231056213
I0305 21:55:24.060720 140222268335872 logging_writer.py:48] [35300] global_step=35300, grad_norm=0.06668129563331604, loss=0.026973653584718704
I0305 21:55:44.528787 140222259943168 logging_writer.py:48] [35400] global_step=35400, grad_norm=0.0611492358148098, loss=0.026585305109620094
I0305 21:56:05.113020 140222268335872 logging_writer.py:48] [35500] global_step=35500, grad_norm=0.05790180340409279, loss=0.02512444742023945
I0305 21:56:25.896851 140222259943168 logging_writer.py:48] [35600] global_step=35600, grad_norm=0.05863652750849724, loss=0.026402395218610764
I0305 21:56:46.882314 140222268335872 logging_writer.py:48] [35700] global_step=35700, grad_norm=0.05552000179886818, loss=0.026252198964357376
I0305 21:57:07.816561 140222259943168 logging_writer.py:48] [35800] global_step=35800, grad_norm=0.06150461733341217, loss=0.02574557065963745
I0305 21:57:28.840900 140222268335872 logging_writer.py:48] [35900] global_step=35900, grad_norm=0.05647744983434677, loss=0.026281539350748062
I0305 21:57:49.408859 140222259943168 logging_writer.py:48] [36000] global_step=36000, grad_norm=0.05648377537727356, loss=0.0249323807656765
I0305 21:57:51.015674 140363820438720 spec.py:321] Evaluating on the training split.
I0305 21:58:59.662023 140363820438720 spec.py:333] Evaluating on the validation split.
I0305 21:59:01.593377 140363820438720 spec.py:349] Evaluating on the test split.
I0305 21:59:03.504925 140363820438720 submission_runner.py:469] Time since start: 9970.83s, 	Step: 36009, 	{'train/accuracy': 0.9952119588851929, 'train/loss': 0.015240401960909367, 'train/mean_average_precision': 0.7510973779285022, 'validation/accuracy': 0.9861553311347961, 'validation/loss': 0.05528578907251358, 'validation/mean_average_precision': 0.23649006997409588, 'validation/num_examples': 43793, 'test/accuracy': 0.985374391078949, 'test/loss': 0.058966610580682755, 'test/mean_average_precision': 0.2341673271447617, 'test/num_examples': 43793, 'score': 7452.821468114853, 'total_duration': 9970.833562850952, 'accumulated_submission_time': 7452.821468114853, 'accumulated_eval_time': 2516.2213270664215, 'accumulated_logging_time': 0.849318265914917}
I0305 21:59:03.518213 140222268335872 logging_writer.py:48] [36009] accumulated_eval_time=2516.22, accumulated_logging_time=0.849318, accumulated_submission_time=7452.82, global_step=36009, preemption_count=0, score=7452.82, test/accuracy=0.985374, test/loss=0.0589666, test/mean_average_precision=0.234167, test/num_examples=43793, total_duration=9970.83, train/accuracy=0.995212, train/loss=0.0152404, train/mean_average_precision=0.751097, validation/accuracy=0.986155, validation/loss=0.0552858, validation/mean_average_precision=0.23649, validation/num_examples=43793
I0305 21:59:22.524262 140222259943168 logging_writer.py:48] [36100] global_step=36100, grad_norm=0.05049991235136986, loss=0.02367417886853218
I0305 21:59:43.329470 140222268335872 logging_writer.py:48] [36200] global_step=36200, grad_norm=0.05597637966275215, loss=0.025680817663669586
I0305 22:00:04.071166 140222259943168 logging_writer.py:48] [36300] global_step=36300, grad_norm=0.05223606899380684, loss=0.024942513555288315
I0305 22:00:24.621278 140222268335872 logging_writer.py:48] [36400] global_step=36400, grad_norm=0.05424490198493004, loss=0.0247331615537405
I0305 22:00:45.486922 140222259943168 logging_writer.py:48] [36500] global_step=36500, grad_norm=0.0524577833712101, loss=0.02500222623348236
I0305 22:01:05.956836 140222268335872 logging_writer.py:48] [36600] global_step=36600, grad_norm=0.05408116802573204, loss=0.02453739568591118
I0305 22:01:26.958641 140222259943168 logging_writer.py:48] [36700] global_step=36700, grad_norm=0.059219080954790115, loss=0.02569899894297123
I0305 22:01:47.722634 140222268335872 logging_writer.py:48] [36800] global_step=36800, grad_norm=0.04771292209625244, loss=0.023768050596117973
I0305 22:02:08.477110 140222259943168 logging_writer.py:48] [36900] global_step=36900, grad_norm=0.05905986577272415, loss=0.024043433368206024
I0305 22:02:29.566342 140222268335872 logging_writer.py:48] [37000] global_step=37000, grad_norm=0.054642997682094574, loss=0.025345955044031143
I0305 22:02:50.654856 140222259943168 logging_writer.py:48] [37100] global_step=37100, grad_norm=0.061339735984802246, loss=0.024166898801922798
I0305 22:03:03.615054 140363820438720 spec.py:321] Evaluating on the training split.
I0305 22:04:16.913174 140363820438720 spec.py:333] Evaluating on the validation split.
I0305 22:04:19.051495 140363820438720 spec.py:349] Evaluating on the test split.
I0305 22:04:20.939363 140363820438720 submission_runner.py:469] Time since start: 10288.27s, 	Step: 37164, 	{'train/accuracy': 0.9946776628494263, 'train/loss': 0.016332319006323814, 'train/mean_average_precision': 0.7310647975610933, 'validation/accuracy': 0.9862012267112732, 'validation/loss': 0.056744564324617386, 'validation/mean_average_precision': 0.236010592845471, 'validation/num_examples': 43793, 'test/accuracy': 0.985381543636322, 'test/loss': 0.0607730932533741, 'test/mean_average_precision': 0.23049280076958228, 'test/num_examples': 43793, 'score': 7692.8759553432465, 'total_duration': 10288.268055200577, 'accumulated_submission_time': 7692.8759553432465, 'accumulated_eval_time': 2593.545583486557, 'accumulated_logging_time': 0.8719797134399414}
I0305 22:04:20.952741 140222268335872 logging_writer.py:48] [37164] accumulated_eval_time=2593.55, accumulated_logging_time=0.87198, accumulated_submission_time=7692.88, global_step=37164, preemption_count=0, score=7692.88, test/accuracy=0.985382, test/loss=0.0607731, test/mean_average_precision=0.230493, test/num_examples=43793, total_duration=10288.3, train/accuracy=0.994678, train/loss=0.0163323, train/mean_average_precision=0.731065, validation/accuracy=0.986201, validation/loss=0.0567446, validation/mean_average_precision=0.236011, validation/num_examples=43793
I0305 22:04:28.668636 140222259943168 logging_writer.py:48] [37200] global_step=37200, grad_norm=0.05131639167666435, loss=0.024101123213768005
I0305 22:04:49.149755 140222268335872 logging_writer.py:48] [37300] global_step=37300, grad_norm=0.05426296591758728, loss=0.024394802749156952
I0305 22:05:09.747598 140222259943168 logging_writer.py:48] [37400] global_step=37400, grad_norm=0.0562652125954628, loss=0.023777395486831665
I0305 22:05:30.099207 140222268335872 logging_writer.py:48] [37500] global_step=37500, grad_norm=0.05097123980522156, loss=0.023418258875608444
I0305 22:05:50.383843 140222259943168 logging_writer.py:48] [37600] global_step=37600, grad_norm=0.05079621076583862, loss=0.023948857560753822
I0305 22:06:10.834235 140222268335872 logging_writer.py:48] [37700] global_step=37700, grad_norm=0.05612608790397644, loss=0.024345308542251587
I0305 22:06:31.383419 140222259943168 logging_writer.py:48] [37800] global_step=37800, grad_norm=0.05256713554263115, loss=0.023241745308041573
I0305 22:06:51.990823 140222268335872 logging_writer.py:48] [37900] global_step=37900, grad_norm=0.057261474430561066, loss=0.02500763349235058
I0305 22:07:12.606078 140222259943168 logging_writer.py:48] [38000] global_step=38000, grad_norm=0.06125989928841591, loss=0.02497958391904831
I0305 22:07:33.265142 140222268335872 logging_writer.py:48] [38100] global_step=38100, grad_norm=0.05495109781622887, loss=0.024173833429813385
I0305 22:07:53.878178 140222259943168 logging_writer.py:48] [38200] global_step=38200, grad_norm=0.050765544176101685, loss=0.02414815127849579
I0305 22:08:14.322522 140222268335872 logging_writer.py:48] [38300] global_step=38300, grad_norm=0.06031285226345062, loss=0.025670988485217094
I0305 22:08:20.994139 140363820438720 spec.py:321] Evaluating on the training split.
I0305 22:09:28.439449 140363820438720 spec.py:333] Evaluating on the validation split.
I0305 22:09:30.348833 140363820438720 spec.py:349] Evaluating on the test split.
I0305 22:09:32.218281 140363820438720 submission_runner.py:469] Time since start: 10599.55s, 	Step: 38333, 	{'train/accuracy': 0.996945321559906, 'train/loss': 0.011514567770063877, 'train/mean_average_precision': 0.812810160360274, 'validation/accuracy': 0.9862154126167297, 'validation/loss': 0.05728142708539963, 'validation/mean_average_precision': 0.23833612422887548, 'validation/num_examples': 43793, 'test/accuracy': 0.9852665662765503, 'test/loss': 0.06155028194189072, 'test/mean_average_precision': 0.22762399509847517, 'test/num_examples': 43793, 'score': 7932.878902196884, 'total_duration': 10599.546877861023, 'accumulated_submission_time': 7932.878902196884, 'accumulated_eval_time': 2664.76957654953, 'accumulated_logging_time': 0.895087480545044}
I0305 22:09:32.233492 140222259943168 logging_writer.py:48] [38333] accumulated_eval_time=2664.77, accumulated_logging_time=0.895087, accumulated_submission_time=7932.88, global_step=38333, preemption_count=0, score=7932.88, test/accuracy=0.985267, test/loss=0.0615503, test/mean_average_precision=0.227624, test/num_examples=43793, total_duration=10599.5, train/accuracy=0.996945, train/loss=0.0115146, train/mean_average_precision=0.81281, validation/accuracy=0.986215, validation/loss=0.0572814, validation/mean_average_precision=0.238336, validation/num_examples=43793
I0305 22:09:46.234500 140222268335872 logging_writer.py:48] [38400] global_step=38400, grad_norm=0.058191101998090744, loss=0.02456982433795929
I0305 22:10:07.079244 140222259943168 logging_writer.py:48] [38500] global_step=38500, grad_norm=0.06538574397563934, loss=0.024510594084858894
I0305 22:10:27.816929 140222268335872 logging_writer.py:48] [38600] global_step=38600, grad_norm=0.0535433329641819, loss=0.024346796795725822
I0305 22:10:48.297444 140222259943168 logging_writer.py:48] [38700] global_step=38700, grad_norm=0.062384381890296936, loss=0.025008218362927437
I0305 22:11:08.948386 140222268335872 logging_writer.py:48] [38800] global_step=38800, grad_norm=0.05760376155376434, loss=0.02394741214811802
I0305 22:11:29.568098 140222259943168 logging_writer.py:48] [38900] global_step=38900, grad_norm=0.04927155375480652, loss=0.023121550679206848
I0305 22:11:50.296036 140222268335872 logging_writer.py:48] [39000] global_step=39000, grad_norm=0.04868408665060997, loss=0.022503862157464027
I0305 22:12:10.837750 140222259943168 logging_writer.py:48] [39100] global_step=39100, grad_norm=0.05519941449165344, loss=0.025364059954881668
I0305 22:12:31.438741 140222268335872 logging_writer.py:48] [39200] global_step=39200, grad_norm=0.05095640569925308, loss=0.022828299552202225
I0305 22:12:52.238249 140222259943168 logging_writer.py:48] [39300] global_step=39300, grad_norm=0.05768682807683945, loss=0.025354234501719475
I0305 22:13:13.103826 140222268335872 logging_writer.py:48] [39400] global_step=39400, grad_norm=0.05001703277230263, loss=0.023992318660020828
I0305 22:13:32.296085 140363820438720 spec.py:321] Evaluating on the training split.
I0305 22:14:44.009731 140363820438720 spec.py:333] Evaluating on the validation split.
I0305 22:14:45.997393 140363820438720 spec.py:349] Evaluating on the test split.
I0305 22:14:47.962325 140363820438720 submission_runner.py:469] Time since start: 10915.29s, 	Step: 39495, 	{'train/accuracy': 0.9955244660377502, 'train/loss': 0.014057081192731857, 'train/mean_average_precision': 0.7814663194745386, 'validation/accuracy': 0.9861358404159546, 'validation/loss': 0.0582909882068634, 'validation/mean_average_precision': 0.2354027819286958, 'validation/num_examples': 43793, 'test/accuracy': 0.9852425456047058, 'test/loss': 0.06241708993911743, 'test/mean_average_precision': 0.2296379954759312, 'test/num_examples': 43793, 'score': 8172.898724794388, 'total_duration': 10915.291023254395, 'accumulated_submission_time': 8172.898724794388, 'accumulated_eval_time': 2740.435772895813, 'accumulated_logging_time': 0.9203140735626221}
I0305 22:14:47.975645 140222259943168 logging_writer.py:48] [39495] accumulated_eval_time=2740.44, accumulated_logging_time=0.920314, accumulated_submission_time=8172.9, global_step=39495, preemption_count=0, score=8172.9, test/accuracy=0.985243, test/loss=0.0624171, test/mean_average_precision=0.229638, test/num_examples=43793, total_duration=10915.3, train/accuracy=0.995524, train/loss=0.0140571, train/mean_average_precision=0.781466, validation/accuracy=0.986136, validation/loss=0.058291, validation/mean_average_precision=0.235403, validation/num_examples=43793
I0305 22:14:49.226593 140222268335872 logging_writer.py:48] [39500] global_step=39500, grad_norm=0.05016212537884712, loss=0.02315705455839634
I0305 22:15:09.812389 140222259943168 logging_writer.py:48] [39600] global_step=39600, grad_norm=0.04703690856695175, loss=0.022005582228302956
I0305 22:15:30.158156 140222268335872 logging_writer.py:48] [39700] global_step=39700, grad_norm=0.05345943197607994, loss=0.023185038939118385
I0305 22:15:50.498708 140222259943168 logging_writer.py:48] [39800] global_step=39800, grad_norm=0.05974138155579567, loss=0.023196052759885788
I0305 22:16:11.263091 140222268335872 logging_writer.py:48] [39900] global_step=39900, grad_norm=0.05317460000514984, loss=0.02385765127837658
I0305 22:16:32.088268 140222259943168 logging_writer.py:48] [40000] global_step=40000, grad_norm=0.05389324203133583, loss=0.023556195199489594
I0305 22:16:52.759861 140222268335872 logging_writer.py:48] [40100] global_step=40100, grad_norm=0.05589788407087326, loss=0.02312735840678215
I0305 22:17:13.508245 140222259943168 logging_writer.py:48] [40200] global_step=40200, grad_norm=0.048197828233242035, loss=0.022918429225683212
I0305 22:17:34.232843 140222268335872 logging_writer.py:48] [40300] global_step=40300, grad_norm=0.05718018487095833, loss=0.0252617746591568
I0305 22:17:54.830429 140222259943168 logging_writer.py:48] [40400] global_step=40400, grad_norm=0.04766353592276573, loss=0.0228131003677845
I0305 22:18:15.490122 140222268335872 logging_writer.py:48] [40500] global_step=40500, grad_norm=0.06278016418218613, loss=0.02340896986424923
I0305 22:18:36.321081 140222259943168 logging_writer.py:48] [40600] global_step=40600, grad_norm=0.062371768057346344, loss=0.02400965243577957
I0305 22:18:48.104561 140363820438720 spec.py:321] Evaluating on the training split.
I0305 22:19:55.777038 140363820438720 spec.py:333] Evaluating on the validation split.
I0305 22:19:57.720032 140363820438720 spec.py:349] Evaluating on the test split.
I0305 22:19:59.641966 140363820438720 submission_runner.py:469] Time since start: 11226.97s, 	Step: 40658, 	{'train/accuracy': 0.9974278807640076, 'train/loss': 0.010539324954152107, 'train/mean_average_precision': 0.8350180141009163, 'validation/accuracy': 0.986068069934845, 'validation/loss': 0.058774806559085846, 'validation/mean_average_precision': 0.23153708748634433, 'validation/num_examples': 43793, 'test/accuracy': 0.9851983189582825, 'test/loss': 0.06291582435369492, 'test/mean_average_precision': 0.22559891912883753, 'test/num_examples': 43793, 'score': 8412.988114118576, 'total_duration': 11226.97063088417, 'accumulated_submission_time': 8412.988114118576, 'accumulated_eval_time': 2811.973099708557, 'accumulated_logging_time': 0.9436252117156982}
I0305 22:19:59.655162 140222268335872 logging_writer.py:48] [40658] accumulated_eval_time=2811.97, accumulated_logging_time=0.943625, accumulated_submission_time=8412.99, global_step=40658, preemption_count=0, score=8412.99, test/accuracy=0.985198, test/loss=0.0629158, test/mean_average_precision=0.225599, test/num_examples=43793, total_duration=11227, train/accuracy=0.997428, train/loss=0.0105393, train/mean_average_precision=0.835018, validation/accuracy=0.986068, validation/loss=0.0587748, validation/mean_average_precision=0.231537, validation/num_examples=43793
I0305 22:20:08.539269 140222259943168 logging_writer.py:48] [40700] global_step=40700, grad_norm=0.056346360594034195, loss=0.024605093523859978
I0305 22:20:29.076319 140222268335872 logging_writer.py:48] [40800] global_step=40800, grad_norm=0.04798686504364014, loss=0.022451967000961304
I0305 22:20:49.349464 140222259943168 logging_writer.py:48] [40900] global_step=40900, grad_norm=0.06337442994117737, loss=0.023289667442440987
I0305 22:21:10.026096 140222268335872 logging_writer.py:48] [41000] global_step=41000, grad_norm=0.04741181433200836, loss=0.02102944441139698
I0305 22:21:30.674467 140222259943168 logging_writer.py:48] [41100] global_step=41100, grad_norm=0.05251939594745636, loss=0.023105256259441376
I0305 22:21:51.162333 140222268335872 logging_writer.py:48] [41200] global_step=41200, grad_norm=0.0521446093916893, loss=0.022504884749650955
I0305 22:22:11.708159 140222259943168 logging_writer.py:48] [41300] global_step=41300, grad_norm=0.05766863748431206, loss=0.02387446165084839
I0305 22:22:32.356389 140222268335872 logging_writer.py:48] [41400] global_step=41400, grad_norm=0.0563676618039608, loss=0.023990940302610397
I0305 22:22:52.886275 140222259943168 logging_writer.py:48] [41500] global_step=41500, grad_norm=0.04841843247413635, loss=0.021714983507990837
I0305 22:23:13.489908 140222268335872 logging_writer.py:48] [41600] global_step=41600, grad_norm=0.05151623487472534, loss=0.023435767740011215
I0305 22:23:34.186285 140222259943168 logging_writer.py:48] [41700] global_step=41700, grad_norm=0.05128580704331398, loss=0.023838216438889503
I0305 22:23:54.837533 140222268335872 logging_writer.py:48] [41800] global_step=41800, grad_norm=0.05004037171602249, loss=0.022618845105171204
I0305 22:23:59.645536 140363820438720 spec.py:321] Evaluating on the training split.
I0305 22:25:12.829330 140363820438720 spec.py:333] Evaluating on the validation split.
I0305 22:25:14.789672 140363820438720 spec.py:349] Evaluating on the test split.
I0305 22:25:16.689462 140363820438720 submission_runner.py:469] Time since start: 11544.02s, 	Step: 41824, 	{'train/accuracy': 0.9952295422554016, 'train/loss': 0.014390068128705025, 'train/mean_average_precision': 0.7779754447384712, 'validation/accuracy': 0.9861866235733032, 'validation/loss': 0.05978657677769661, 'validation/mean_average_precision': 0.23378385937487411, 'validation/num_examples': 43793, 'test/accuracy': 0.985236644744873, 'test/loss': 0.06418509781360626, 'test/mean_average_precision': 0.22475630918611988, 'test/num_examples': 43793, 'score': 8652.938473463058, 'total_duration': 11544.018153190613, 'accumulated_submission_time': 8652.938473463058, 'accumulated_eval_time': 2889.0169718265533, 'accumulated_logging_time': 0.9666237831115723}
I0305 22:25:16.703806 140222259943168 logging_writer.py:48] [41824] accumulated_eval_time=2889.02, accumulated_logging_time=0.966624, accumulated_submission_time=8652.94, global_step=41824, preemption_count=0, score=8652.94, test/accuracy=0.985237, test/loss=0.0641851, test/mean_average_precision=0.224756, test/num_examples=43793, total_duration=11544, train/accuracy=0.99523, train/loss=0.0143901, train/mean_average_precision=0.777975, validation/accuracy=0.986187, validation/loss=0.0597866, validation/mean_average_precision=0.233784, validation/num_examples=43793
I0305 22:25:32.637417 140222268335872 logging_writer.py:48] [41900] global_step=41900, grad_norm=0.0550331212580204, loss=0.023089170455932617
I0305 22:25:53.336076 140222259943168 logging_writer.py:48] [42000] global_step=42000, grad_norm=0.049357540905475616, loss=0.023066915571689606
I0305 22:26:14.013844 140222268335872 logging_writer.py:48] [42100] global_step=42100, grad_norm=0.04795307293534279, loss=0.02233591303229332
I0305 22:26:34.818977 140222259943168 logging_writer.py:48] [42200] global_step=42200, grad_norm=0.052839476615190506, loss=0.021116826683282852
I0305 22:26:55.762361 140222268335872 logging_writer.py:48] [42300] global_step=42300, grad_norm=0.0450676791369915, loss=0.02242722362279892
I0305 22:27:16.642723 140222259943168 logging_writer.py:48] [42400] global_step=42400, grad_norm=0.04954959452152252, loss=0.022495612502098083
I0305 22:27:37.539463 140222268335872 logging_writer.py:48] [42500] global_step=42500, grad_norm=0.04306576028466225, loss=0.02128256857395172
I0305 22:27:58.296578 140222259943168 logging_writer.py:48] [42600] global_step=42600, grad_norm=0.045741427689790726, loss=0.022328970953822136
I0305 22:28:19.174161 140222268335872 logging_writer.py:48] [42700] global_step=42700, grad_norm=0.05660005658864975, loss=0.0222467090934515
I0305 22:28:40.113227 140222259943168 logging_writer.py:48] [42800] global_step=42800, grad_norm=0.054433781653642654, loss=0.02282702922821045
I0305 22:29:01.009438 140222268335872 logging_writer.py:48] [42900] global_step=42900, grad_norm=0.04590267315506935, loss=0.023053107783198357
I0305 22:29:16.889757 140363820438720 spec.py:321] Evaluating on the training split.
I0305 22:30:26.236139 140363820438720 spec.py:333] Evaluating on the validation split.
I0305 22:30:28.160600 140363820438720 spec.py:349] Evaluating on the test split.
I0305 22:30:30.050101 140363820438720 submission_runner.py:469] Time since start: 11857.38s, 	Step: 42979, 	{'train/accuracy': 0.9969338178634644, 'train/loss': 0.011317688040435314, 'train/mean_average_precision': 0.8315024394536775, 'validation/accuracy': 0.9861127138137817, 'validation/loss': 0.06054950878024101, 'validation/mean_average_precision': 0.23227139473871602, 'validation/num_examples': 43793, 'test/accuracy': 0.9851406216621399, 'test/loss': 0.06507028639316559, 'test/mean_average_precision': 0.22400236830956277, 'test/num_examples': 43793, 'score': 8893.08444738388, 'total_duration': 11857.378795146942, 'accumulated_submission_time': 8893.08444738388, 'accumulated_eval_time': 2962.1772677898407, 'accumulated_logging_time': 0.9897043704986572}
I0305 22:30:30.064751 140222259943168 logging_writer.py:48] [42979] accumulated_eval_time=2962.18, accumulated_logging_time=0.989704, accumulated_submission_time=8893.08, global_step=42979, preemption_count=0, score=8893.08, test/accuracy=0.985141, test/loss=0.0650703, test/mean_average_precision=0.224002, test/num_examples=43793, total_duration=11857.4, train/accuracy=0.996934, train/loss=0.0113177, train/mean_average_precision=0.831502, validation/accuracy=0.986113, validation/loss=0.0605495, validation/mean_average_precision=0.232271, validation/num_examples=43793
I0305 22:30:34.594759 140222268335872 logging_writer.py:48] [43000] global_step=43000, grad_norm=0.0434035062789917, loss=0.02128230594098568
I0305 22:30:55.590917 140222259943168 logging_writer.py:48] [43100] global_step=43100, grad_norm=0.0486283041536808, loss=0.022265706211328506
I0305 22:31:16.284838 140222268335872 logging_writer.py:48] [43200] global_step=43200, grad_norm=0.04707395285367966, loss=0.022374233230948448
I0305 22:31:37.035552 140222259943168 logging_writer.py:48] [43300] global_step=43300, grad_norm=0.047880351543426514, loss=0.021901972591876984
I0305 22:31:57.784892 140222268335872 logging_writer.py:48] [43400] global_step=43400, grad_norm=0.04651505500078201, loss=0.021913794800639153
I0305 22:32:18.306225 140222259943168 logging_writer.py:48] [43500] global_step=43500, grad_norm=0.050275303423404694, loss=0.022651225328445435
I0305 22:32:39.123883 140222268335872 logging_writer.py:48] [43600] global_step=43600, grad_norm=0.04116623476147652, loss=0.021213170140981674
I0305 22:32:59.824688 140222259943168 logging_writer.py:48] [43700] global_step=43700, grad_norm=0.043869297951459885, loss=0.02192111685872078
I0305 22:33:20.434447 140222268335872 logging_writer.py:48] [43800] global_step=43800, grad_norm=0.04110834375023842, loss=0.021232904866337776
I0305 22:33:41.134096 140222259943168 logging_writer.py:48] [43900] global_step=43900, grad_norm=0.049768224358558655, loss=0.021839577704668045
I0305 22:34:02.012260 140222268335872 logging_writer.py:48] [44000] global_step=44000, grad_norm=0.04457276687026024, loss=0.022263552993535995
I0305 22:34:22.311739 140222259943168 logging_writer.py:48] [44100] global_step=44100, grad_norm=0.052338454872369766, loss=0.02372017689049244
I0305 22:34:30.152694 140363820438720 spec.py:321] Evaluating on the training split.
I0305 22:35:42.375948 140363820438720 spec.py:333] Evaluating on the validation split.
I0305 22:35:44.322692 140363820438720 spec.py:349] Evaluating on the test split.
I0305 22:35:46.208708 140363820438720 submission_runner.py:469] Time since start: 12173.54s, 	Step: 44139, 	{'train/accuracy': 0.9955289363861084, 'train/loss': 0.013761293143033981, 'train/mean_average_precision': 0.7845061246579415, 'validation/accuracy': 0.9859479069709778, 'validation/loss': 0.06077152118086815, 'validation/mean_average_precision': 0.229503417208904, 'validation/num_examples': 43793, 'test/accuracy': 0.9850913286209106, 'test/loss': 0.06516489386558533, 'test/mean_average_precision': 0.2217302631520706, 'test/num_examples': 43793, 'score': 9133.135304450989, 'total_duration': 12173.537246704102, 'accumulated_submission_time': 9133.135304450989, 'accumulated_eval_time': 3038.233077287674, 'accumulated_logging_time': 1.0136849880218506}
I0305 22:35:46.222546 140222268335872 logging_writer.py:48] [44139] accumulated_eval_time=3038.23, accumulated_logging_time=1.01368, accumulated_submission_time=9133.14, global_step=44139, preemption_count=0, score=9133.14, test/accuracy=0.985091, test/loss=0.0651649, test/mean_average_precision=0.22173, test/num_examples=43793, total_duration=12173.5, train/accuracy=0.995529, train/loss=0.0137613, train/mean_average_precision=0.784506, validation/accuracy=0.985948, validation/loss=0.0607715, validation/mean_average_precision=0.229503, validation/num_examples=43793
I0305 22:35:59.084483 140222259943168 logging_writer.py:48] [44200] global_step=44200, grad_norm=0.045348070561885834, loss=0.02282736450433731
I0305 22:36:19.704221 140222268335872 logging_writer.py:48] [44300] global_step=44300, grad_norm=0.06801333278417587, loss=0.023696085438132286
I0305 22:36:40.053407 140222259943168 logging_writer.py:48] [44400] global_step=44400, grad_norm=0.04179186001420021, loss=0.02237771637737751
I0305 22:37:00.788722 140222268335872 logging_writer.py:48] [44500] global_step=44500, grad_norm=0.04366239160299301, loss=0.021171150729060173
I0305 22:37:21.852582 140222259943168 logging_writer.py:48] [44600] global_step=44600, grad_norm=0.04480614885687828, loss=0.021341102197766304
I0305 22:37:43.009908 140222268335872 logging_writer.py:48] [44700] global_step=44700, grad_norm=0.04679836705327034, loss=0.021692316979169846
I0305 22:38:03.892438 140222259943168 logging_writer.py:48] [44800] global_step=44800, grad_norm=0.04355379194021225, loss=0.02211209572851658
I0305 22:38:25.049198 140222268335872 logging_writer.py:48] [44900] global_step=44900, grad_norm=0.046298474073410034, loss=0.022337542846798897
I0305 22:38:46.041472 140222259943168 logging_writer.py:48] [45000] global_step=45000, grad_norm=0.06311552971601486, loss=0.022856662049889565
I0305 22:39:07.294936 140222268335872 logging_writer.py:48] [45100] global_step=45100, grad_norm=0.045479461550712585, loss=0.021330537274479866
I0305 22:39:28.224907 140222259943168 logging_writer.py:48] [45200] global_step=45200, grad_norm=0.04307166114449501, loss=0.02260531112551689
I0305 22:39:46.256274 140363820438720 spec.py:321] Evaluating on the training split.
I0305 22:40:56.702181 140363820438720 spec.py:333] Evaluating on the validation split.
I0305 22:40:58.787724 140363820438720 spec.py:349] Evaluating on the test split.
I0305 22:41:00.784937 140363820438720 submission_runner.py:469] Time since start: 12488.11s, 	Step: 45287, 	{'train/accuracy': 0.9982138872146606, 'train/loss': 0.009005549363791943, 'train/mean_average_precision': 0.8757303864250285, 'validation/accuracy': 0.9860749840736389, 'validation/loss': 0.06152097135782242, 'validation/mean_average_precision': 0.23057247557645089, 'validation/num_examples': 43793, 'test/accuracy': 0.985113263130188, 'test/loss': 0.0661998838186264, 'test/mean_average_precision': 0.22124957094384556, 'test/num_examples': 43793, 'score': 9373.128984451294, 'total_duration': 12488.113516569138, 'accumulated_submission_time': 9373.128984451294, 'accumulated_eval_time': 3112.76157951355, 'accumulated_logging_time': 1.0372209548950195}
I0305 22:41:00.799726 140222268335872 logging_writer.py:48] [45287] accumulated_eval_time=3112.76, accumulated_logging_time=1.03722, accumulated_submission_time=9373.13, global_step=45287, preemption_count=0, score=9373.13, test/accuracy=0.985113, test/loss=0.0661999, test/mean_average_precision=0.22125, test/num_examples=43793, total_duration=12488.1, train/accuracy=0.998214, train/loss=0.00900555, train/mean_average_precision=0.87573, validation/accuracy=0.986075, validation/loss=0.061521, validation/mean_average_precision=0.230572, validation/num_examples=43793
I0305 22:41:03.713195 140222259943168 logging_writer.py:48] [45300] global_step=45300, grad_norm=0.04485727846622467, loss=0.021819617599248886
I0305 22:41:24.599285 140222268335872 logging_writer.py:48] [45400] global_step=45400, grad_norm=0.04706510901451111, loss=0.022968465462327003
I0305 22:41:45.478740 140222259943168 logging_writer.py:48] [45500] global_step=45500, grad_norm=0.03968590870499611, loss=0.021422140300273895
I0305 22:42:06.536009 140222268335872 logging_writer.py:48] [45600] global_step=45600, grad_norm=0.044683247804641724, loss=0.024291392415761948
I0305 22:42:27.652979 140222259943168 logging_writer.py:48] [45700] global_step=45700, grad_norm=0.04799607768654823, loss=0.0223389845341444
I0305 22:42:48.513075 140222268335872 logging_writer.py:48] [45800] global_step=45800, grad_norm=0.04026425629854202, loss=0.021580468863248825
I0305 22:43:09.895657 140222259943168 logging_writer.py:48] [45900] global_step=45900, grad_norm=0.0458984449505806, loss=0.02225489355623722
I0305 22:43:30.464412 140222268335872 logging_writer.py:48] [46000] global_step=46000, grad_norm=0.04164479672908783, loss=0.021495502442121506
I0305 22:43:51.100017 140222259943168 logging_writer.py:48] [46100] global_step=46100, grad_norm=0.047454480081796646, loss=0.022572118788957596
I0305 22:44:12.044456 140222268335872 logging_writer.py:48] [46200] global_step=46200, grad_norm=0.043619830161333084, loss=0.02177172526717186
I0305 22:44:32.886076 140222259943168 logging_writer.py:48] [46300] global_step=46300, grad_norm=0.041722144931554794, loss=0.021475791931152344
I0305 22:44:53.959891 140222268335872 logging_writer.py:48] [46400] global_step=46400, grad_norm=0.04811118543148041, loss=0.022045064717531204
I0305 22:45:00.869327 140363820438720 spec.py:321] Evaluating on the training split.
I0305 22:46:10.814095 140363820438720 spec.py:333] Evaluating on the validation split.
I0305 22:46:12.932652 140363820438720 spec.py:349] Evaluating on the test split.
I0305 22:46:14.932249 140363820438720 submission_runner.py:469] Time since start: 12802.26s, 	Step: 46434, 	{'train/accuracy': 0.9959490895271301, 'train/loss': 0.012395849451422691, 'train/mean_average_precision': 0.8200606547448531, 'validation/accuracy': 0.9860343933105469, 'validation/loss': 0.0619334951043129, 'validation/mean_average_precision': 0.2294863399858221, 'validation/num_examples': 43793, 'test/accuracy': 0.9851035475730896, 'test/loss': 0.06656300276517868, 'test/mean_average_precision': 0.2195005036931294, 'test/num_examples': 43793, 'score': 9613.157678604126, 'total_duration': 12802.260908126831, 'accumulated_submission_time': 9613.157678604126, 'accumulated_eval_time': 3186.824418067932, 'accumulated_logging_time': 1.0617547035217285}
I0305 22:46:14.946571 140222259943168 logging_writer.py:48] [46434] accumulated_eval_time=3186.82, accumulated_logging_time=1.06175, accumulated_submission_time=9613.16, global_step=46434, preemption_count=0, score=9613.16, test/accuracy=0.985104, test/loss=0.066563, test/mean_average_precision=0.219501, test/num_examples=43793, total_duration=12802.3, train/accuracy=0.995949, train/loss=0.0123958, train/mean_average_precision=0.820061, validation/accuracy=0.986034, validation/loss=0.0619335, validation/mean_average_precision=0.229486, validation/num_examples=43793
I0305 22:46:28.538498 140222268335872 logging_writer.py:48] [46500] global_step=46500, grad_norm=0.0430213063955307, loss=0.022360706701874733
I0305 22:46:48.817723 140222259943168 logging_writer.py:48] [46600] global_step=46600, grad_norm=0.04342786595225334, loss=0.021612467244267464
I0305 22:47:09.729991 140222268335872 logging_writer.py:48] [46700] global_step=46700, grad_norm=0.04346470534801483, loss=0.022199371829628944
I0305 22:47:30.295161 140222259943168 logging_writer.py:48] [46800] global_step=46800, grad_norm=0.046423498541116714, loss=0.02253378927707672
I0305 22:47:51.017177 140222268335872 logging_writer.py:48] [46900] global_step=46900, grad_norm=0.046544983983039856, loss=0.02196722850203514
I0305 22:48:11.800118 140222259943168 logging_writer.py:48] [47000] global_step=47000, grad_norm=0.03937101736664772, loss=0.02057999186217785
I0305 22:48:32.614274 140222268335872 logging_writer.py:48] [47100] global_step=47100, grad_norm=0.053964897990226746, loss=0.023796850815415382
I0305 22:48:53.210286 140222259943168 logging_writer.py:48] [47200] global_step=47200, grad_norm=0.04412936046719551, loss=0.02160293608903885
I0305 22:49:14.076582 140222268335872 logging_writer.py:48] [47300] global_step=47300, grad_norm=0.044984135776758194, loss=0.021891118958592415
I0305 22:49:34.638171 140222259943168 logging_writer.py:48] [47400] global_step=47400, grad_norm=0.04251252859830856, loss=0.022514689713716507
I0305 22:49:55.109393 140222268335872 logging_writer.py:48] [47500] global_step=47500, grad_norm=0.04661968722939491, loss=0.023193007335066795
I0305 22:50:15.064990 140363820438720 spec.py:321] Evaluating on the training split.
I0305 22:51:25.802390 140363820438720 spec.py:333] Evaluating on the validation split.
I0305 22:51:27.722567 140363820438720 spec.py:349] Evaluating on the test split.
I0305 22:51:29.763736 140363820438720 submission_runner.py:469] Time since start: 13117.09s, 	Step: 47598, 	{'train/accuracy': 0.998333215713501, 'train/loss': 0.008797142654657364, 'train/mean_average_precision': 0.8859533934271745, 'validation/accuracy': 0.9859158396720886, 'validation/loss': 0.06217694655060768, 'validation/mean_average_precision': 0.2274063426240968, 'validation/num_examples': 43793, 'test/accuracy': 0.985042929649353, 'test/loss': 0.06665322184562683, 'test/mean_average_precision': 0.21775207291659265, 'test/num_examples': 43793, 'score': 9853.236314535141, 'total_duration': 13117.092304468155, 'accumulated_submission_time': 9853.236314535141, 'accumulated_eval_time': 3261.52298951149, 'accumulated_logging_time': 1.0859410762786865}
I0305 22:51:29.778915 140222259943168 logging_writer.py:48] [47598] accumulated_eval_time=3261.52, accumulated_logging_time=1.08594, accumulated_submission_time=9853.24, global_step=47598, preemption_count=0, score=9853.24, test/accuracy=0.985043, test/loss=0.0666532, test/mean_average_precision=0.217752, test/num_examples=43793, total_duration=13117.1, train/accuracy=0.998333, train/loss=0.00879714, train/mean_average_precision=0.885953, validation/accuracy=0.985916, validation/loss=0.0621769, validation/mean_average_precision=0.227406, validation/num_examples=43793
I0305 22:51:30.411364 140222268335872 logging_writer.py:48] [47600] global_step=47600, grad_norm=0.044395625591278076, loss=0.02171865478157997
I0305 22:51:50.942956 140222259943168 logging_writer.py:48] [47700] global_step=47700, grad_norm=0.035951122641563416, loss=0.020431874319911003
I0305 22:52:11.419439 140222268335872 logging_writer.py:48] [47800] global_step=47800, grad_norm=0.050130363553762436, loss=0.022945495322346687
I0305 22:52:32.145278 140222259943168 logging_writer.py:48] [47900] global_step=47900, grad_norm=0.04046430066227913, loss=0.02131660468876362
I0305 22:52:52.593684 140222268335872 logging_writer.py:48] [48000] global_step=48000, grad_norm=0.03876530006527901, loss=0.02047472633421421
I0305 22:53:13.217228 140222259943168 logging_writer.py:48] [48100] global_step=48100, grad_norm=0.04113646596670151, loss=0.020710939541459084
I0305 22:53:33.829309 140222268335872 logging_writer.py:48] [48200] global_step=48200, grad_norm=0.04022778943181038, loss=0.020400526002049446
I0305 22:53:54.595403 140222259943168 logging_writer.py:48] [48300] global_step=48300, grad_norm=0.045527420938014984, loss=0.019913703203201294
I0305 22:54:15.321870 140222268335872 logging_writer.py:48] [48400] global_step=48400, grad_norm=0.03484959155321121, loss=0.02128678746521473
I0305 22:54:36.129100 140222259943168 logging_writer.py:48] [48500] global_step=48500, grad_norm=0.04732433706521988, loss=0.021855618804693222
I0305 22:54:56.897626 140222268335872 logging_writer.py:48] [48600] global_step=48600, grad_norm=0.0412590354681015, loss=0.022278495132923126
I0305 22:55:17.644503 140222259943168 logging_writer.py:48] [48700] global_step=48700, grad_norm=0.040398191660642624, loss=0.021971385926008224
I0305 22:55:29.906838 140363820438720 spec.py:321] Evaluating on the training split.
I0305 22:56:38.367027 140363820438720 spec.py:333] Evaluating on the validation split.
I0305 22:56:40.302478 140363820438720 spec.py:349] Evaluating on the test split.
I0305 22:56:42.184072 140363820438720 submission_runner.py:469] Time since start: 13429.51s, 	Step: 48761, 	{'train/accuracy': 0.9959759712219238, 'train/loss': 0.012533287517726421, 'train/mean_average_precision': 0.8183618749058663, 'validation/accuracy': 0.9860132932662964, 'validation/loss': 0.06239403411746025, 'validation/mean_average_precision': 0.22533054546531714, 'validation/num_examples': 43793, 'test/accuracy': 0.9850564002990723, 'test/loss': 0.06707092374563217, 'test/mean_average_precision': 0.21837406252321712, 'test/num_examples': 43793, 'score': 10093.326829195023, 'total_duration': 13429.512689113617, 'accumulated_submission_time': 10093.326829195023, 'accumulated_eval_time': 3333.8000988960266, 'accumulated_logging_time': 1.1108100414276123}
I0305 22:56:42.198131 140222268335872 logging_writer.py:48] [48761] accumulated_eval_time=3333.8, accumulated_logging_time=1.11081, accumulated_submission_time=10093.3, global_step=48761, preemption_count=0, score=10093.3, test/accuracy=0.985056, test/loss=0.0670709, test/mean_average_precision=0.218374, test/num_examples=43793, total_duration=13429.5, train/accuracy=0.995976, train/loss=0.0125333, train/mean_average_precision=0.818362, validation/accuracy=0.986013, validation/loss=0.062394, validation/mean_average_precision=0.225331, validation/num_examples=43793
I0305 22:56:50.477885 140222259943168 logging_writer.py:48] [48800] global_step=48800, grad_norm=0.047956641763448715, loss=0.02289833314716816
I0305 22:57:11.171920 140222268335872 logging_writer.py:48] [48900] global_step=48900, grad_norm=0.04051997512578964, loss=0.021693890914320946
I0305 22:57:31.749240 140222259943168 logging_writer.py:48] [49000] global_step=49000, grad_norm=0.04623629152774811, loss=0.02255364879965782
I0305 22:57:53.097168 140222268335872 logging_writer.py:48] [49100] global_step=49100, grad_norm=0.03909631446003914, loss=0.020712535828351974
I0305 22:58:13.908741 140222259943168 logging_writer.py:48] [49200] global_step=49200, grad_norm=0.052289050072431564, loss=0.022007644176483154
I0305 22:58:34.651700 140222268335872 logging_writer.py:48] [49300] global_step=49300, grad_norm=0.0458127036690712, loss=0.022495457902550697
I0305 22:58:55.370613 140222259943168 logging_writer.py:48] [49400] global_step=49400, grad_norm=0.045983605086803436, loss=0.022244328632950783
I0305 22:59:16.287873 140222268335872 logging_writer.py:48] [49500] global_step=49500, grad_norm=0.039405785501003265, loss=0.02007433958351612
I0305 22:59:37.140794 140222259943168 logging_writer.py:48] [49600] global_step=49600, grad_norm=0.04770699515938759, loss=0.021574240177869797
I0305 22:59:57.838357 140222268335872 logging_writer.py:48] [49700] global_step=49700, grad_norm=0.04929066449403763, loss=0.02210858091711998
I0305 23:00:18.678145 140222259943168 logging_writer.py:48] [49800] global_step=49800, grad_norm=0.04527504742145538, loss=0.021368950605392456
I0305 23:00:39.437606 140222268335872 logging_writer.py:48] [49900] global_step=49900, grad_norm=0.06278195232152939, loss=0.023669308051466942
I0305 23:00:42.365866 140363820438720 spec.py:321] Evaluating on the training split.
I0305 23:01:53.195866 140363820438720 spec.py:333] Evaluating on the validation split.
I0305 23:01:55.150957 140363820438720 spec.py:349] Evaluating on the test split.
I0305 23:01:57.011575 140363820438720 submission_runner.py:469] Time since start: 13744.34s, 	Step: 49915, 	{'train/accuracy': 0.9977337718009949, 'train/loss': 0.009552744217216969, 'train/mean_average_precision': 0.8606569592132653, 'validation/accuracy': 0.9859243631362915, 'validation/loss': 0.06197017431259155, 'validation/mean_average_precision': 0.2292632990368719, 'validation/num_examples': 43793, 'test/accuracy': 0.9850319623947144, 'test/loss': 0.06650514155626297, 'test/mean_average_precision': 0.21890603482027798, 'test/num_examples': 43793, 'score': 10333.45381307602, 'total_duration': 13744.340266227722, 'accumulated_submission_time': 10333.45381307602, 'accumulated_eval_time': 3408.445756673813, 'accumulated_logging_time': 1.1338963508605957}
I0305 23:01:57.026359 140222259943168 logging_writer.py:48] [49915] accumulated_eval_time=3408.45, accumulated_logging_time=1.1339, accumulated_submission_time=10333.5, global_step=49915, preemption_count=0, score=10333.5, test/accuracy=0.985032, test/loss=0.0665051, test/mean_average_precision=0.218906, test/num_examples=43793, total_duration=13744.3, train/accuracy=0.997734, train/loss=0.00955274, train/mean_average_precision=0.860657, validation/accuracy=0.985924, validation/loss=0.0619702, validation/mean_average_precision=0.229263, validation/num_examples=43793
I0305 23:02:14.863867 140222268335872 logging_writer.py:48] [50000] global_step=50000, grad_norm=0.053908564150333405, loss=0.02175283245742321
I0305 23:02:35.855412 140222259943168 logging_writer.py:48] [50100] global_step=50100, grad_norm=0.04986207187175751, loss=0.021023383364081383
I0305 23:02:56.840827 140222268335872 logging_writer.py:48] [50200] global_step=50200, grad_norm=0.05094744265079498, loss=0.022678859531879425
I0305 23:03:17.821895 140222259943168 logging_writer.py:48] [50300] global_step=50300, grad_norm=0.049018096178770065, loss=0.02196352183818817
I0305 23:03:38.801186 140222268335872 logging_writer.py:48] [50400] global_step=50400, grad_norm=0.05494384840130806, loss=0.021831223741173744
I0305 23:03:59.671737 140222259943168 logging_writer.py:48] [50500] global_step=50500, grad_norm=0.05009650066494942, loss=0.021380549296736717
I0305 23:04:20.643090 140222268335872 logging_writer.py:48] [50600] global_step=50600, grad_norm=0.07448098063468933, loss=0.022242078557610512
I0305 23:04:41.743502 140222259943168 logging_writer.py:48] [50700] global_step=50700, grad_norm=0.060012087225914, loss=0.022163569927215576
I0305 23:05:02.754623 140222268335872 logging_writer.py:48] [50800] global_step=50800, grad_norm=0.04973793029785156, loss=0.02210691012442112
I0305 23:05:23.291620 140222259943168 logging_writer.py:48] [50900] global_step=50900, grad_norm=0.05225694179534912, loss=0.022853627800941467
I0305 23:05:44.163419 140222268335872 logging_writer.py:48] [51000] global_step=51000, grad_norm=0.05339047685265541, loss=0.022899340838193893
I0305 23:05:57.162147 140363820438720 spec.py:321] Evaluating on the training split.
I0305 23:07:07.409379 140363820438720 spec.py:333] Evaluating on the validation split.
I0305 23:07:09.362189 140363820438720 spec.py:349] Evaluating on the test split.
I0305 23:07:11.252568 140363820438720 submission_runner.py:469] Time since start: 14058.58s, 	Step: 51063, 	{'train/accuracy': 0.9977223873138428, 'train/loss': 0.009600340388715267, 'train/mean_average_precision': 0.8630774555596852, 'validation/accuracy': 0.9859188795089722, 'validation/loss': 0.062068842351436615, 'validation/mean_average_precision': 0.2247633331852613, 'validation/num_examples': 43793, 'test/accuracy': 0.9850370287895203, 'test/loss': 0.06664708256721497, 'test/mean_average_precision': 0.21677798695773293, 'test/num_examples': 43793, 'score': 10573.550117015839, 'total_duration': 14058.581187009811, 'accumulated_submission_time': 10573.550117015839, 'accumulated_eval_time': 3482.536052942276, 'accumulated_logging_time': 1.1575829982757568}
I0305 23:07:11.267278 140222259943168 logging_writer.py:48] [51063] accumulated_eval_time=3482.54, accumulated_logging_time=1.15758, accumulated_submission_time=10573.6, global_step=51063, preemption_count=0, score=10573.6, test/accuracy=0.985037, test/loss=0.0666471, test/mean_average_precision=0.216778, test/num_examples=43793, total_duration=14058.6, train/accuracy=0.997722, train/loss=0.00960034, train/mean_average_precision=0.863077, validation/accuracy=0.985919, validation/loss=0.0620688, validation/mean_average_precision=0.224763, validation/num_examples=43793
I0305 23:07:19.389341 140222268335872 logging_writer.py:48] [51100] global_step=51100, grad_norm=0.06827308982610703, loss=0.021810222417116165
I0305 23:07:40.100240 140222259943168 logging_writer.py:48] [51200] global_step=51200, grad_norm=0.06989827752113342, loss=0.021835410967469215
I0305 23:08:00.609561 140222268335872 logging_writer.py:48] [51300] global_step=51300, grad_norm=0.08416911214590073, loss=0.023508068174123764
I0305 23:08:21.049360 140222259943168 logging_writer.py:48] [51400] global_step=51400, grad_norm=0.06743975728750229, loss=0.022035595029592514
I0305 23:08:41.495115 140222268335872 logging_writer.py:48] [51500] global_step=51500, grad_norm=0.06810209155082703, loss=0.02248801477253437
I0305 23:09:02.213182 140222259943168 logging_writer.py:48] [51600] global_step=51600, grad_norm=0.05780544877052307, loss=0.022185713052749634
I0305 23:09:22.600234 140222268335872 logging_writer.py:48] [51700] global_step=51700, grad_norm=0.07246538996696472, loss=0.0223397109657526
I0305 23:09:42.910562 140222259943168 logging_writer.py:48] [51800] global_step=51800, grad_norm=0.07742717117071152, loss=0.02199147641658783
I0305 23:10:04.046655 140222268335872 logging_writer.py:48] [51900] global_step=51900, grad_norm=0.10220277309417725, loss=0.022646304219961166
I0305 23:10:24.931985 140222259943168 logging_writer.py:48] [52000] global_step=52000, grad_norm=0.06804124265909195, loss=0.021634526550769806
I0305 23:10:45.799798 140222268335872 logging_writer.py:48] [52100] global_step=52100, grad_norm=0.06672032177448273, loss=0.023136870935559273
I0305 23:11:06.807228 140222259943168 logging_writer.py:48] [52200] global_step=52200, grad_norm=0.060488052666187286, loss=0.02170567773282528
I0305 23:11:11.289640 140363820438720 spec.py:321] Evaluating on the training split.
I0305 23:12:22.197238 140363820438720 spec.py:333] Evaluating on the validation split.
I0305 23:12:24.105036 140363820438720 spec.py:349] Evaluating on the test split.
I0305 23:12:25.969258 140363820438720 submission_runner.py:469] Time since start: 14373.30s, 	Step: 52222, 	{'train/accuracy': 0.9981123805046082, 'train/loss': 0.009071549400687218, 'train/mean_average_precision': 0.8724912362295548, 'validation/accuracy': 0.9859409928321838, 'validation/loss': 0.061929721385240555, 'validation/mean_average_precision': 0.22722660052963015, 'validation/num_examples': 43793, 'test/accuracy': 0.9850564002990723, 'test/loss': 0.06651763617992401, 'test/mean_average_precision': 0.2182488588723709, 'test/num_examples': 43793, 'score': 10813.535950183868, 'total_duration': 14373.297874450684, 'accumulated_submission_time': 10813.535950183868, 'accumulated_eval_time': 3557.2155447006226, 'accumulated_logging_time': 1.1818804740905762}
I0305 23:12:25.984048 140222268335872 logging_writer.py:48] [52222] accumulated_eval_time=3557.22, accumulated_logging_time=1.18188, accumulated_submission_time=10813.5, global_step=52222, preemption_count=0, score=10813.5, test/accuracy=0.985056, test/loss=0.0665176, test/mean_average_precision=0.218249, test/num_examples=43793, total_duration=14373.3, train/accuracy=0.998112, train/loss=0.00907155, train/mean_average_precision=0.872491, validation/accuracy=0.985941, validation/loss=0.0619297, validation/mean_average_precision=0.227227, validation/num_examples=43793
I0305 23:12:42.653660 140222259943168 logging_writer.py:48] [52300] global_step=52300, grad_norm=0.0540599562227726, loss=0.02144508808851242
I0305 23:13:03.478619 140222268335872 logging_writer.py:48] [52400] global_step=52400, grad_norm=0.06357987970113754, loss=0.022245509549975395
I0305 23:13:24.712787 140222259943168 logging_writer.py:48] [52500] global_step=52500, grad_norm=0.08549994230270386, loss=0.023647310212254524
I0305 23:13:45.560817 140222268335872 logging_writer.py:48] [52600] global_step=52600, grad_norm=0.05993092432618141, loss=0.022220691666007042
I0305 23:14:06.270994 140222259943168 logging_writer.py:48] [52700] global_step=52700, grad_norm=0.08104435354471207, loss=0.021928612142801285
I0305 23:14:27.011224 140222268335872 logging_writer.py:48] [52800] global_step=52800, grad_norm=0.06618968397378922, loss=0.021432042121887207
I0305 23:14:47.797434 140222259943168 logging_writer.py:48] [52900] global_step=52900, grad_norm=0.05600513145327568, loss=0.021590281277894974
I0305 23:15:08.758110 140222268335872 logging_writer.py:48] [53000] global_step=53000, grad_norm=0.0535031221807003, loss=0.021478353068232536
I0305 23:15:29.448404 140222259943168 logging_writer.py:48] [53100] global_step=53100, grad_norm=0.06819630414247513, loss=0.020993661135435104
I0305 23:15:50.490226 140222268335872 logging_writer.py:48] [53200] global_step=53200, grad_norm=0.08702628314495087, loss=0.02283932827413082
I0305 23:16:11.226302 140222259943168 logging_writer.py:48] [53300] global_step=53300, grad_norm=0.057994600385427475, loss=0.02139693684875965
I0305 23:16:26.052923 140363820438720 spec.py:321] Evaluating on the training split.
I0305 23:17:33.482938 140363820438720 spec.py:333] Evaluating on the validation split.
I0305 23:17:35.413654 140363820438720 spec.py:349] Evaluating on the test split.
I0305 23:17:37.325352 140363820438720 submission_runner.py:469] Time since start: 14684.65s, 	Step: 53374, 	{'train/accuracy': 0.9979497194290161, 'train/loss': 0.009255867451429367, 'train/mean_average_precision': 0.8831672993461357, 'validation/accuracy': 0.9859409928321838, 'validation/loss': 0.061929721385240555, 'validation/mean_average_precision': 0.22708367427807555, 'validation/num_examples': 43793, 'test/accuracy': 0.9850564002990723, 'test/loss': 0.06651762872934341, 'test/mean_average_precision': 0.2182205122931107, 'test/num_examples': 43793, 'score': 11053.564621686935, 'total_duration': 14684.654006958008, 'accumulated_submission_time': 11053.564621686935, 'accumulated_eval_time': 3628.487885951996, 'accumulated_logging_time': 1.2068958282470703}
I0305 23:17:37.341122 140222268335872 logging_writer.py:48] [53374] accumulated_eval_time=3628.49, accumulated_logging_time=1.2069, accumulated_submission_time=11053.6, global_step=53374, preemption_count=0, score=11053.6, test/accuracy=0.985056, test/loss=0.0665176, test/mean_average_precision=0.218221, test/num_examples=43793, total_duration=14684.7, train/accuracy=0.99795, train/loss=0.00925587, train/mean_average_precision=0.883167, validation/accuracy=0.985941, validation/loss=0.0619297, validation/mean_average_precision=0.227084, validation/num_examples=43793
I0305 23:17:43.119238 140222259943168 logging_writer.py:48] [53400] global_step=53400, grad_norm=0.08376343548297882, loss=0.023477984592318535
I0305 23:18:04.008087 140222268335872 logging_writer.py:48] [53500] global_step=53500, grad_norm=0.07028589397668839, loss=0.022072801366448402
I0305 23:18:24.866086 140222259943168 logging_writer.py:48] [53600] global_step=53600, grad_norm=0.05838165059685707, loss=0.022700490429997444
I0305 23:18:45.772075 140222268335872 logging_writer.py:48] [53700] global_step=53700, grad_norm=0.06589485704898834, loss=0.022908024489879608
I0305 23:19:06.770514 140222259943168 logging_writer.py:48] [53800] global_step=53800, grad_norm=0.07107361406087875, loss=0.023047398775815964
I0305 23:19:28.024949 140222268335872 logging_writer.py:48] [53900] global_step=53900, grad_norm=0.08959047496318817, loss=0.022369183599948883
I0305 23:19:49.215670 140222259943168 logging_writer.py:48] [54000] global_step=54000, grad_norm=0.06846265494823456, loss=0.022044729441404343
I0305 23:20:10.166529 140222268335872 logging_writer.py:48] [54100] global_step=54100, grad_norm=0.06769854575395584, loss=0.02281874045729637
I0305 23:20:31.457720 140222259943168 logging_writer.py:48] [54200] global_step=54200, grad_norm=0.073263019323349, loss=0.0215073823928833
I0305 23:20:52.612190 140222268335872 logging_writer.py:48] [54300] global_step=54300, grad_norm=0.049873579293489456, loss=0.02205151878297329
I0305 23:21:14.001159 140222259943168 logging_writer.py:48] [54400] global_step=54400, grad_norm=0.05381448194384575, loss=0.021882932633161545
I0305 23:21:34.462547 140222268335872 logging_writer.py:48] [54500] global_step=54500, grad_norm=0.06655914336442947, loss=0.021419165655970573
I0305 23:21:37.498143 140363820438720 spec.py:321] Evaluating on the training split.
I0305 23:22:46.283092 140363820438720 spec.py:333] Evaluating on the validation split.
I0305 23:22:48.226667 140363820438720 spec.py:349] Evaluating on the test split.
I0305 23:22:50.143268 140363820438720 submission_runner.py:469] Time since start: 14997.47s, 	Step: 54516, 	{'train/accuracy': 0.997397780418396, 'train/loss': 0.009963732212781906, 'train/mean_average_precision': 0.8576935198336461, 'validation/accuracy': 0.9859409928321838, 'validation/loss': 0.061929721385240555, 'validation/mean_average_precision': 0.22706274587958908, 'validation/num_examples': 43793, 'test/accuracy': 0.9850564002990723, 'test/loss': 0.06651763617992401, 'test/mean_average_precision': 0.2181922863788245, 'test/num_examples': 43793, 'score': 11293.682857751846, 'total_duration': 14997.47192621231, 'accumulated_submission_time': 11293.682857751846, 'accumulated_eval_time': 3701.1329243183136, 'accumulated_logging_time': 1.23175048828125}
I0305 23:22:50.158949 140222259943168 logging_writer.py:48] [54516] accumulated_eval_time=3701.13, accumulated_logging_time=1.23175, accumulated_submission_time=11293.7, global_step=54516, preemption_count=0, score=11293.7, test/accuracy=0.985056, test/loss=0.0665176, test/mean_average_precision=0.218192, test/num_examples=43793, total_duration=14997.5, train/accuracy=0.997398, train/loss=0.00996373, train/mean_average_precision=0.857694, validation/accuracy=0.985941, validation/loss=0.0619297, validation/mean_average_precision=0.227063, validation/num_examples=43793
I0305 23:23:08.076153 140222268335872 logging_writer.py:48] [54600] global_step=54600, grad_norm=0.09036114066839218, loss=0.021715668961405754
I0305 23:23:28.984349 140222259943168 logging_writer.py:48] [54700] global_step=54700, grad_norm=0.06525051593780518, loss=0.02115136757493019
I0305 23:23:50.050095 140222268335872 logging_writer.py:48] [54800] global_step=54800, grad_norm=0.09020976722240448, loss=0.022720059379935265
I0305 23:24:10.911183 140222259943168 logging_writer.py:48] [54900] global_step=54900, grad_norm=0.0731680616736412, loss=0.02287842147052288
I0305 23:24:31.934720 140222268335872 logging_writer.py:48] [55000] global_step=55000, grad_norm=0.058618273586034775, loss=0.022998053580522537
I0305 23:24:53.221609 140222259943168 logging_writer.py:48] [55100] global_step=55100, grad_norm=0.05888214707374573, loss=0.022463686764240265
I0305 23:25:14.540461 140222268335872 logging_writer.py:48] [55200] global_step=55200, grad_norm=0.0713948905467987, loss=0.021586203947663307
I0305 23:25:35.497343 140222259943168 logging_writer.py:48] [55300] global_step=55300, grad_norm=0.07876475900411606, loss=0.02143101952970028
I0305 23:25:56.718540 140222268335872 logging_writer.py:48] [55400] global_step=55400, grad_norm=0.06646014750003815, loss=0.022330831736326218
I0305 23:26:17.994562 140222259943168 logging_writer.py:48] [55500] global_step=55500, grad_norm=0.07842683792114258, loss=0.022321686148643494
I0305 23:26:38.827058 140222268335872 logging_writer.py:48] [55600] global_step=55600, grad_norm=0.06702054291963577, loss=0.021868158131837845
I0305 23:26:50.205194 140363820438720 spec.py:321] Evaluating on the training split.
I0305 23:28:03.471668 140363820438720 spec.py:333] Evaluating on the validation split.
I0305 23:28:05.416921 140363820438720 spec.py:349] Evaluating on the test split.
I0305 23:28:07.293456 140363820438720 submission_runner.py:469] Time since start: 15314.62s, 	Step: 55657, 	{'train/accuracy': 0.9977102875709534, 'train/loss': 0.009582447819411755, 'train/mean_average_precision': 0.8649339771210496, 'validation/accuracy': 0.9859409928321838, 'validation/loss': 0.061929721385240555, 'validation/mean_average_precision': 0.22709213093412073, 'validation/num_examples': 43793, 'test/accuracy': 0.9850564002990723, 'test/loss': 0.06651763617992401, 'test/mean_average_precision': 0.21820152238599252, 'test/num_examples': 43793, 'score': 11533.686092615128, 'total_duration': 15314.622146129608, 'accumulated_submission_time': 11533.686092615128, 'accumulated_eval_time': 3778.221134185791, 'accumulated_logging_time': 1.2575616836547852}
I0305 23:28:07.309181 140222259943168 logging_writer.py:48] [55657] accumulated_eval_time=3778.22, accumulated_logging_time=1.25756, accumulated_submission_time=11533.7, global_step=55657, preemption_count=0, score=11533.7, test/accuracy=0.985056, test/loss=0.0665176, test/mean_average_precision=0.218202, test/num_examples=43793, total_duration=15314.6, train/accuracy=0.99771, train/loss=0.00958245, train/mean_average_precision=0.864934, validation/accuracy=0.985941, validation/loss=0.0619297, validation/mean_average_precision=0.227092, validation/num_examples=43793
I0305 23:28:16.473095 140222268335872 logging_writer.py:48] [55700] global_step=55700, grad_norm=0.05006813630461693, loss=0.021397041156888008
I0305 23:28:37.223561 140222259943168 logging_writer.py:48] [55800] global_step=55800, grad_norm=0.05557231977581978, loss=0.02216089703142643
I0305 23:28:58.074062 140222268335872 logging_writer.py:48] [55900] global_step=55900, grad_norm=0.06968708336353302, loss=0.02255829982459545
I0305 23:29:18.839045 140222259943168 logging_writer.py:48] [56000] global_step=56000, grad_norm=0.10337997227907181, loss=0.022358300164341927
I0305 23:29:39.450940 140222268335872 logging_writer.py:48] [56100] global_step=56100, grad_norm=0.06032012775540352, loss=0.020777275785803795
I0305 23:30:00.356780 140222259943168 logging_writer.py:48] [56200] global_step=56200, grad_norm=0.07554437965154648, loss=0.02259676717221737
I0305 23:30:21.483932 140222268335872 logging_writer.py:48] [56300] global_step=56300, grad_norm=0.06438257545232773, loss=0.022483929991722107
I0305 23:30:42.620476 140222259943168 logging_writer.py:48] [56400] global_step=56400, grad_norm=0.054772961884737015, loss=0.020463399589061737
I0305 23:31:03.391873 140222268335872 logging_writer.py:48] [56500] global_step=56500, grad_norm=0.06281257420778275, loss=0.022122422233223915
I0305 23:31:24.099507 140222259943168 logging_writer.py:48] [56600] global_step=56600, grad_norm=0.07300674170255661, loss=0.021442070603370667
I0305 23:31:45.266372 140222268335872 logging_writer.py:48] [56700] global_step=56700, grad_norm=0.08625108748674393, loss=0.024290140718221664
I0305 23:32:06.075897 140222259943168 logging_writer.py:48] [56800] global_step=56800, grad_norm=0.07078222185373306, loss=0.02312118373811245
I0305 23:32:07.477869 140363820438720 spec.py:321] Evaluating on the training split.
I0305 23:33:15.958744 140363820438720 spec.py:333] Evaluating on the validation split.
I0305 23:33:17.866175 140363820438720 spec.py:349] Evaluating on the test split.
I0305 23:33:19.765472 140363820438720 submission_runner.py:469] Time since start: 15627.09s, 	Step: 56808, 	{'train/accuracy': 0.9976783990859985, 'train/loss': 0.009622287005186081, 'train/mean_average_precision': 0.8595378509006688, 'validation/accuracy': 0.9859409928321838, 'validation/loss': 0.061929721385240555, 'validation/mean_average_precision': 0.2270661563211023, 'validation/num_examples': 43793, 'test/accuracy': 0.9850564002990723, 'test/loss': 0.06651763617992401, 'test/mean_average_precision': 0.21840449659518407, 'test/num_examples': 43793, 'score': 11773.814343690872, 'total_duration': 15627.094087839127, 'accumulated_submission_time': 11773.814343690872, 'accumulated_eval_time': 3850.50860786438, 'accumulated_logging_time': 1.2822446823120117}
I0305 23:33:19.782045 140222268335872 logging_writer.py:48] [56808] accumulated_eval_time=3850.51, accumulated_logging_time=1.28224, accumulated_submission_time=11773.8, global_step=56808, preemption_count=0, score=11773.8, test/accuracy=0.985056, test/loss=0.0665176, test/mean_average_precision=0.218404, test/num_examples=43793, total_duration=15627.1, train/accuracy=0.997678, train/loss=0.00962229, train/mean_average_precision=0.859538, validation/accuracy=0.985941, validation/loss=0.0619297, validation/mean_average_precision=0.227066, validation/num_examples=43793
I0305 23:33:39.037860 140222259943168 logging_writer.py:48] [56900] global_step=56900, grad_norm=0.07555174827575684, loss=0.021846750751137733
I0305 23:34:00.070510 140222268335872 logging_writer.py:48] [57000] global_step=57000, grad_norm=0.05784912034869194, loss=0.02185818925499916
I0305 23:34:21.147589 140222259943168 logging_writer.py:48] [57100] global_step=57100, grad_norm=0.05923585593700409, loss=0.02267356775701046
I0305 23:34:41.617783 140222268335872 logging_writer.py:48] [57200] global_step=57200, grad_norm=0.058868829160928726, loss=0.020912690088152885
I0305 23:35:02.324831 140222259943168 logging_writer.py:48] [57300] global_step=57300, grad_norm=0.08407951146364212, loss=0.021932827308773994
I0305 23:35:23.141694 140222268335872 logging_writer.py:48] [57400] global_step=57400, grad_norm=0.11159069091081619, loss=0.02285836450755596
I0305 23:35:44.237262 140222259943168 logging_writer.py:48] [57500] global_step=57500, grad_norm=0.0666302740573883, loss=0.02213282510638237
I0305 23:36:05.178147 140222268335872 logging_writer.py:48] [57600] global_step=57600, grad_norm=0.08619410544633865, loss=0.022877834737300873
I0305 23:36:26.073469 140222259943168 logging_writer.py:48] [57700] global_step=57700, grad_norm=0.06890102475881577, loss=0.022046340629458427
I0305 23:36:47.220008 140222268335872 logging_writer.py:48] [57800] global_step=57800, grad_norm=0.05650954693555832, loss=0.02302057109773159
I0305 23:37:08.344098 140222259943168 logging_writer.py:48] [57900] global_step=57900, grad_norm=0.06683667004108429, loss=0.02254645712673664
I0305 23:37:19.963140 140222268335872 logging_writer.py:48] [57956] global_step=57956, preemption_count=0, score=12013.9
I0305 23:37:20.114483 140363820438720 submission_runner.py:646] Tuning trial 2/5
I0305 23:37:20.114665 140363820438720 submission_runner.py:647] Hyperparameters: Hyperparameters(dropout_rate=0.0, label_smoothing=0.2, learning_rate=0.0008445074561975979, one_minus_beta1=0.11042418465, beta2=0.9978504782314613, weight_decay=0.08135402759553023, warmup_factor=0.05)
I0305 23:37:20.116259 140363820438720 submission_runner.py:648] Metrics: {'eval_results': [(1, {'train/accuracy': 0.538699746131897, 'train/loss': 0.6973111033439636, 'train/mean_average_precision': 0.021441355192791046, 'validation/accuracy': 0.5371228456497192, 'validation/loss': 0.6984302997589111, 'validation/mean_average_precision': 0.02601679457974987, 'validation/num_examples': 43793, 'test/accuracy': 0.5367534160614014, 'test/loss': 0.6991291046142578, 'test/mean_average_precision': 0.028082283176050583, 'test/num_examples': 43793, 'score': 10.99699068069458, 'total_duration': 213.88552117347717, 'accumulated_submission_time': 10.99699068069458, 'accumulated_eval_time': 202.88842844963074, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1147, {'train/accuracy': 0.9867100119590759, 'train/loss': 0.051971886307001114, 'train/mean_average_precision': 0.057989005194681996, 'validation/accuracy': 0.9841219782829285, 'validation/loss': 0.061596449464559555, 'validation/mean_average_precision': 0.05811824640039385, 'validation/num_examples': 43793, 'test/accuracy': 0.9831386804580688, 'test/loss': 0.06487934291362762, 'test/mean_average_precision': 0.0601425657114673, 'test/num_examples': 43793, 'score': 251.1587917804718, 'total_duration': 529.3432381153107, 'accumulated_submission_time': 251.1587917804718, 'accumulated_eval_time': 278.13698744773865, 'accumulated_logging_time': 0.01647210121154785, 'global_step': 1147, 'preemption_count': 0}), (2298, {'train/accuracy': 0.9874939322471619, 'train/loss': 0.04541674256324768, 'train/mean_average_precision': 0.1265546478745511, 'validation/accuracy': 0.9847467541694641, 'validation/loss': 0.05470802262425423, 'validation/mean_average_precision': 0.12305261667739999, 'validation/num_examples': 43793, 'test/accuracy': 0.9837380647659302, 'test/loss': 0.0579862967133522, 'test/mean_average_precision': 0.12199999726476862, 'test/num_examples': 43793, 'score': 491.2537651062012, 'total_duration': 844.6511604785919, 'accumulated_submission_time': 491.2537651062012, 'accumulated_eval_time': 353.29932951927185, 'accumulated_logging_time': 0.03597760200500488, 'global_step': 2298, 'preemption_count': 0}), (3448, {'train/accuracy': 0.9880132675170898, 'train/loss': 0.04221317544579506, 'train/mean_average_precision': 0.18469053420334952, 'validation/accuracy': 0.9852931499481201, 'validation/loss': 0.05145341902971268, 'validation/mean_average_precision': 0.15516940846616792, 'validation/num_examples': 43793, 'test/accuracy': 0.9843774437904358, 'test/loss': 0.05415385589003563, 'test/mean_average_precision': 0.15344928642373062, 'test/num_examples': 43793, 'score': 731.2871816158295, 'total_duration': 1160.7561302185059, 'accumulated_submission_time': 731.2871816158295, 'accumulated_eval_time': 429.3232731819153, 'accumulated_logging_time': 0.05398058891296387, 'global_step': 3448, 'preemption_count': 0}), (4600, {'train/accuracy': 0.9882197976112366, 'train/loss': 0.04027145355939865, 'train/mean_average_precision': 0.20361025530373042, 'validation/accuracy': 0.9855151772499084, 'validation/loss': 0.049441855400800705, 'validation/mean_average_precision': 0.18763431409016312, 'validation/num_examples': 43793, 'test/accuracy': 0.9845859408378601, 'test/loss': 0.052278075367212296, 'test/mean_average_precision': 0.18208174625533277, 'test/num_examples': 43793, 'score': 971.3430013656616, 'total_duration': 1476.0533137321472, 'accumulated_submission_time': 971.3430013656616, 'accumulated_eval_time': 504.51737689971924, 'accumulated_logging_time': 0.07322192192077637, 'global_step': 4600, 'preemption_count': 0}), (5767, {'train/accuracy': 0.9882570505142212, 'train/loss': 0.04036799445748329, 'train/mean_average_precision': 0.2430359076454796, 'validation/accuracy': 0.9854433536529541, 'validation/loss': 0.05084119737148285, 'validation/mean_average_precision': 0.200664112440204, 'validation/num_examples': 43793, 'test/accuracy': 0.9844895005226135, 'test/loss': 0.05376238748431206, 'test/mean_average_precision': 0.1980760391763428, 'test/num_examples': 43793, 'score': 1211.3304266929626, 'total_duration': 1790.41583943367, 'accumulated_submission_time': 1211.3304266929626, 'accumulated_eval_time': 578.8392398357391, 'accumulated_logging_time': 0.09571456909179688, 'global_step': 5767, 'preemption_count': 0}), (6922, {'train/accuracy': 0.9890108704566956, 'train/loss': 0.037540603429079056, 'train/mean_average_precision': 0.26282216084091825, 'validation/accuracy': 0.9861074686050415, 'validation/loss': 0.04751013219356537, 'validation/mean_average_precision': 0.21572292536414434, 'validation/num_examples': 43793, 'test/accuracy': 0.985180675983429, 'test/loss': 0.05034453794360161, 'test/mean_average_precision': 0.2191434815913037, 'test/num_examples': 43793, 'score': 1451.3285562992096, 'total_duration': 2103.8836007118225, 'accumulated_submission_time': 1451.3285562992096, 'accumulated_eval_time': 652.2591917514801, 'accumulated_logging_time': 0.11483597755432129, 'global_step': 6922, 'preemption_count': 0}), (8082, {'train/accuracy': 0.9892480373382568, 'train/loss': 0.03607333451509476, 'train/mean_average_precision': 0.3052455519620596, 'validation/accuracy': 0.9862073063850403, 'validation/loss': 0.04661579430103302, 'validation/mean_average_precision': 0.22777933137678852, 'validation/num_examples': 43793, 'test/accuracy': 0.9852059483528137, 'test/loss': 0.04947572574019432, 'test/mean_average_precision': 0.22924792326080348, 'test/num_examples': 43793, 'score': 1691.3264362812042, 'total_duration': 2422.0639820098877, 'accumulated_submission_time': 1691.3264362812042, 'accumulated_eval_time': 730.390056848526, 'accumulated_logging_time': 0.13560819625854492, 'global_step': 8082, 'preemption_count': 0}), (9240, {'train/accuracy': 0.9897556900978088, 'train/loss': 0.035125937312841415, 'train/mean_average_precision': 0.32176956093950765, 'validation/accuracy': 0.9863181114196777, 'validation/loss': 0.04614488035440445, 'validation/mean_average_precision': 0.2262087337368127, 'validation/num_examples': 43793, 'test/accuracy': 0.9854485392570496, 'test/loss': 0.048699211329221725, 'test/mean_average_precision': 0.23241880039490134, 'test/num_examples': 43793, 'score': 1931.4163098335266, 'total_duration': 2737.4695103168488, 'accumulated_submission_time': 1931.4163098335266, 'accumulated_eval_time': 805.6564626693726, 'accumulated_logging_time': 0.1551961898803711, 'global_step': 9240, 'preemption_count': 0}), (10409, {'train/accuracy': 0.9899641871452332, 'train/loss': 0.03337293490767479, 'train/mean_average_precision': 0.3683845504739367, 'validation/accuracy': 0.9864703416824341, 'validation/loss': 0.045791447162628174, 'validation/mean_average_precision': 0.2442675683373668, 'validation/num_examples': 43793, 'test/accuracy': 0.9855790734291077, 'test/loss': 0.04849627614021301, 'test/mean_average_precision': 0.24397161627620667, 'test/num_examples': 43793, 'score': 2171.527834892273, 'total_duration': 3051.6162388324738, 'accumulated_submission_time': 2171.527834892273, 'accumulated_eval_time': 879.642086982727, 'accumulated_logging_time': 0.17492151260375977, 'global_step': 10409, 'preemption_count': 0}), (11590, {'train/accuracy': 0.9899992346763611, 'train/loss': 0.033259425312280655, 'train/mean_average_precision': 0.3756644475691037, 'validation/accuracy': 0.9865418076515198, 'validation/loss': 0.04566127061843872, 'validation/mean_average_precision': 0.2498464824551247, 'validation/num_examples': 43793, 'test/accuracy': 0.9855167865753174, 'test/loss': 0.048866964876651764, 'test/mean_average_precision': 0.23606531082623217, 'test/num_examples': 43793, 'score': 2411.6436138153076, 'total_duration': 3366.0210485458374, 'accumulated_submission_time': 2411.6436138153076, 'accumulated_eval_time': 953.8833372592926, 'accumulated_logging_time': 0.19335532188415527, 'global_step': 11590, 'preemption_count': 0}), (12767, {'train/accuracy': 0.9904630184173584, 'train/loss': 0.03145638108253479, 'train/mean_average_precision': 0.4027964395081572, 'validation/accuracy': 0.9865328669548035, 'validation/loss': 0.04559135437011719, 'validation/mean_average_precision': 0.24295518988140477, 'validation/num_examples': 43793, 'test/accuracy': 0.9857884645462036, 'test/loss': 0.048334136605262756, 'test/mean_average_precision': 0.25348927086244094, 'test/num_examples': 43793, 'score': 2651.610383272171, 'total_duration': 3680.0744869709015, 'accumulated_submission_time': 2651.610383272171, 'accumulated_eval_time': 1027.9201366901398, 'accumulated_logging_time': 0.2128150463104248, 'global_step': 12767, 'preemption_count': 0}), (13942, {'train/accuracy': 0.9908450245857239, 'train/loss': 0.03052341751754284, 'train/mean_average_precision': 0.4179890842539039, 'validation/accuracy': 0.9868012070655823, 'validation/loss': 0.04501211643218994, 'validation/mean_average_precision': 0.2664134345530244, 'validation/num_examples': 43793, 'test/accuracy': 0.9858992099761963, 'test/loss': 0.048055361956357956, 'test/mean_average_precision': 0.2509302488676304, 'test/num_examples': 43793, 'score': 2891.6398046016693, 'total_duration': 3993.205961227417, 'accumulated_submission_time': 2891.6398046016693, 'accumulated_eval_time': 1100.9713778495789, 'accumulated_logging_time': 0.2329106330871582, 'global_step': 13942, 'preemption_count': 0}), (15116, {'train/accuracy': 0.9909579753875732, 'train/loss': 0.02959286980330944, 'train/mean_average_precision': 0.4459145046739112, 'validation/accuracy': 0.9867666959762573, 'validation/loss': 0.045350901782512665, 'validation/mean_average_precision': 0.2659193869161913, 'validation/num_examples': 43793, 'test/accuracy': 0.9858490824699402, 'test/loss': 0.048369958996772766, 'test/mean_average_precision': 0.2517446267115604, 'test/num_examples': 43793, 'score': 3131.6940693855286, 'total_duration': 4307.304433345795, 'accumulated_submission_time': 3131.6940693855286, 'accumulated_eval_time': 1174.964334487915, 'accumulated_logging_time': 0.2520325183868408, 'global_step': 15116, 'preemption_count': 0}), (16289, {'train/accuracy': 0.991315484046936, 'train/loss': 0.02865144982933998, 'train/mean_average_precision': 0.4605190510161351, 'validation/accuracy': 0.9867236614227295, 'validation/loss': 0.045332882553339005, 'validation/mean_average_precision': 0.26645693185329355, 'validation/num_examples': 43793, 'test/accuracy': 0.9859017133712769, 'test/loss': 0.04827573895454407, 'test/mean_average_precision': 0.2597271364786487, 'test/num_examples': 43793, 'score': 3371.693094968796, 'total_duration': 4622.403352975845, 'accumulated_submission_time': 3371.693094968796, 'accumulated_eval_time': 1250.0096497535706, 'accumulated_logging_time': 0.2761106491088867, 'global_step': 16289, 'preemption_count': 0}), (17449, {'train/accuracy': 0.9916352033615112, 'train/loss': 0.027396799996495247, 'train/mean_average_precision': 0.49815429828365054, 'validation/accuracy': 0.9868487119674683, 'validation/loss': 0.04557383060455322, 'validation/mean_average_precision': 0.27047165166112924, 'validation/num_examples': 43793, 'test/accuracy': 0.9859055280685425, 'test/loss': 0.04865854233503342, 'test/mean_average_precision': 0.25498610518335185, 'test/num_examples': 43793, 'score': 3611.7752442359924, 'total_duration': 4935.448865890503, 'accumulated_submission_time': 3611.7752442359924, 'accumulated_eval_time': 1322.9175214767456, 'accumulated_logging_time': 0.30101895332336426, 'global_step': 17449, 'preemption_count': 0}), (18613, {'train/accuracy': 0.9914032220840454, 'train/loss': 0.027891235426068306, 'train/mean_average_precision': 0.4929594016302982, 'validation/accuracy': 0.9868150353431702, 'validation/loss': 0.045948855578899384, 'validation/mean_average_precision': 0.2691772984888078, 'validation/num_examples': 43793, 'test/accuracy': 0.9859210848808289, 'test/loss': 0.04889877513051033, 'test/mean_average_precision': 0.25495938142014885, 'test/num_examples': 43793, 'score': 3851.7677948474884, 'total_duration': 5250.817736387253, 'accumulated_submission_time': 3851.7677948474884, 'accumulated_eval_time': 1398.2456181049347, 'accumulated_logging_time': 0.32051706314086914, 'global_step': 18613, 'preemption_count': 0}), (19766, {'train/accuracy': 0.9921523928642273, 'train/loss': 0.02557269111275673, 'train/mean_average_precision': 0.5347280590370153, 'validation/accuracy': 0.9867849946022034, 'validation/loss': 0.045625489205121994, 'validation/mean_average_precision': 0.2632517919729572, 'validation/num_examples': 43793, 'test/accuracy': 0.9858638048171997, 'test/loss': 0.04876778647303581, 'test/mean_average_precision': 0.25786114738612004, 'test/num_examples': 43793, 'score': 4091.763608455658, 'total_duration': 5566.010301113129, 'accumulated_submission_time': 4091.763608455658, 'accumulated_eval_time': 1473.3905458450317, 'accumulated_logging_time': 0.34156203269958496, 'global_step': 19766, 'preemption_count': 0}), (20939, {'train/accuracy': 0.9921424388885498, 'train/loss': 0.025625178590416908, 'train/mean_average_precision': 0.5318542426044397, 'validation/accuracy': 0.986750066280365, 'validation/loss': 0.04661884158849716, 'validation/mean_average_precision': 0.25969459497949793, 'validation/num_examples': 43793, 'test/accuracy': 0.9859046936035156, 'test/loss': 0.04983795806765556, 'test/mean_average_precision': 0.2536135266196528, 'test/num_examples': 43793, 'score': 4331.854537010193, 'total_duration': 5879.877614498138, 'accumulated_submission_time': 4331.854537010193, 'accumulated_eval_time': 1547.1141111850739, 'accumulated_logging_time': 0.36356163024902344, 'global_step': 20939, 'preemption_count': 0}), (22107, {'train/accuracy': 0.9924676418304443, 'train/loss': 0.024221783503890038, 'train/mean_average_precision': 0.5742881235202819, 'validation/accuracy': 0.9866676330566406, 'validation/loss': 0.04723362997174263, 'validation/mean_average_precision': 0.2563955970690123, 'validation/num_examples': 43793, 'test/accuracy': 0.9858195781707764, 'test/loss': 0.0506041944026947, 'test/mean_average_precision': 0.2497159215150077, 'test/num_examples': 43793, 'score': 4571.967358827591, 'total_duration': 6194.03724861145, 'accumulated_submission_time': 4571.967358827591, 'accumulated_eval_time': 1621.110211610794, 'accumulated_logging_time': 0.38364315032958984, 'global_step': 22107, 'preemption_count': 0}), (23272, {'train/accuracy': 0.9925446510314941, 'train/loss': 0.024123508483171463, 'train/mean_average_precision': 0.56754111893297, 'validation/accuracy': 0.9867184162139893, 'validation/loss': 0.047484125941991806, 'validation/mean_average_precision': 0.2529417099267002, 'validation/num_examples': 43793, 'test/accuracy': 0.985888659954071, 'test/loss': 0.05070467293262482, 'test/mean_average_precision': 0.25117025305895657, 'test/num_examples': 43793, 'score': 4812.099123001099, 'total_duration': 6508.157835483551, 'accumulated_submission_time': 4812.099123001099, 'accumulated_eval_time': 1695.0462892055511, 'accumulated_logging_time': 0.403522253036499, 'global_step': 23272, 'preemption_count': 0}), (24436, {'train/accuracy': 0.9931237697601318, 'train/loss': 0.02208654209971428, 'train/mean_average_precision': 0.6171013098708127, 'validation/accuracy': 0.9867033958435059, 'validation/loss': 0.04750249162316322, 'validation/mean_average_precision': 0.25523857886918505, 'validation/num_examples': 43793, 'test/accuracy': 0.985874354839325, 'test/loss': 0.05075371637940407, 'test/mean_average_precision': 0.2533083323572525, 'test/num_examples': 43793, 'score': 5052.138689517975, 'total_duration': 6822.477037191391, 'accumulated_submission_time': 5052.138689517975, 'accumulated_eval_time': 1769.2743089199066, 'accumulated_logging_time': 0.4243290424346924, 'global_step': 24436, 'preemption_count': 0}), (25596, {'train/accuracy': 0.9927656054496765, 'train/loss': 0.023470507934689522, 'train/mean_average_precision': 0.5575933764617704, 'validation/accuracy': 0.986524760723114, 'validation/loss': 0.048156507313251495, 'validation/mean_average_precision': 0.2512442215046983, 'validation/num_examples': 43793, 'test/accuracy': 0.9856178760528564, 'test/loss': 0.05143974348902702, 'test/mean_average_precision': 0.24417891179917411, 'test/num_examples': 43793, 'score': 5292.2924609184265, 'total_duration': 7139.5627455711365, 'accumulated_submission_time': 5292.2924609184265, 'accumulated_eval_time': 1846.1516077518463, 'accumulated_logging_time': 0.4477527141571045, 'global_step': 25596, 'preemption_count': 0}), (26746, {'train/accuracy': 0.9936712384223938, 'train/loss': 0.02041718363761902, 'train/mean_average_precision': 0.6470453023222926, 'validation/accuracy': 0.9865689873695374, 'validation/loss': 0.048597317188978195, 'validation/mean_average_precision': 0.2529171942291944, 'validation/num_examples': 43793, 'test/accuracy': 0.9857850670814514, 'test/loss': 0.05172213166952133, 'test/mean_average_precision': 0.2542164261043428, 'test/num_examples': 43793, 'score': 5532.379944801331, 'total_duration': 7454.259835243225, 'accumulated_submission_time': 5532.379944801331, 'accumulated_eval_time': 1920.7080821990967, 'accumulated_logging_time': 0.4693737030029297, 'global_step': 26746, 'preemption_count': 0}), (27904, {'train/accuracy': 0.9932960867881775, 'train/loss': 0.021234173327684402, 'train/mean_average_precision': 0.6290925965485867, 'validation/accuracy': 0.9865536093711853, 'validation/loss': 0.04995546489953995, 'validation/mean_average_precision': 0.25205164028418564, 'validation/num_examples': 43793, 'test/accuracy': 0.9856469035148621, 'test/loss': 0.05338302254676819, 'test/mean_average_precision': 0.24693568646084285, 'test/num_examples': 43793, 'score': 5772.423107147217, 'total_duration': 7771.473697900772, 'accumulated_submission_time': 5772.423107147217, 'accumulated_eval_time': 1997.8279411792755, 'accumulated_logging_time': 0.49146437644958496, 'global_step': 27904, 'preemption_count': 0}), (29068, {'train/accuracy': 0.9945397973060608, 'train/loss': 0.017898812890052795, 'train/mean_average_precision': 0.702509288258941, 'validation/accuracy': 0.9864886403083801, 'validation/loss': 0.05044397711753845, 'validation/mean_average_precision': 0.2551320353944225, 'validation/num_examples': 43793, 'test/accuracy': 0.9855685830116272, 'test/loss': 0.054015256464481354, 'test/mean_average_precision': 0.2448104392682111, 'test/num_examples': 43793, 'score': 6012.278198480606, 'total_duration': 8083.624459981918, 'accumulated_submission_time': 6012.278198480606, 'accumulated_eval_time': 2069.8650031089783, 'accumulated_logging_time': 0.7192282676696777, 'global_step': 29068, 'preemption_count': 0}), (30229, {'train/accuracy': 0.9939932823181152, 'train/loss': 0.01932244375348091, 'train/mean_average_precision': 0.6636635893329268, 'validation/accuracy': 0.9864256978034973, 'validation/loss': 0.05072980746626854, 'validation/mean_average_precision': 0.24764399389192257, 'validation/num_examples': 43793, 'test/accuracy': 0.985616147518158, 'test/loss': 0.05419835448265076, 'test/mean_average_precision': 0.2404561881519999, 'test/num_examples': 43793, 'score': 6252.348296880722, 'total_duration': 8399.206347227097, 'accumulated_submission_time': 6252.348296880722, 'accumulated_eval_time': 2145.3256483078003, 'accumulated_logging_time': 0.7407033443450928, 'global_step': 30229, 'preemption_count': 0}), (31391, {'train/accuracy': 0.995140552520752, 'train/loss': 0.016331171616911888, 'train/mean_average_precision': 0.7212529360969668, 'validation/accuracy': 0.9863550662994385, 'validation/loss': 0.051335208117961884, 'validation/mean_average_precision': 0.24298281059268267, 'validation/num_examples': 43793, 'test/accuracy': 0.9855344295501709, 'test/loss': 0.05500173196196556, 'test/mean_average_precision': 0.24131749288499924, 'test/num_examples': 43793, 'score': 6492.486770868301, 'total_duration': 8712.949797391891, 'accumulated_submission_time': 6492.486770868301, 'accumulated_eval_time': 2218.877608060837, 'accumulated_logging_time': 0.7624268531799316, 'global_step': 31391, 'preemption_count': 0}), (32536, {'train/accuracy': 0.9941064715385437, 'train/loss': 0.01876217871904373, 'train/mean_average_precision': 0.6691806428191119, 'validation/accuracy': 0.9862353205680847, 'validation/loss': 0.052192412316799164, 'validation/mean_average_precision': 0.2458687938084052, 'validation/num_examples': 43793, 'test/accuracy': 0.985355019569397, 'test/loss': 0.05582445114850998, 'test/mean_average_precision': 0.24176554860146993, 'test/num_examples': 43793, 'score': 6732.483732700348, 'total_duration': 9027.456737041473, 'accumulated_submission_time': 6732.483732700348, 'accumulated_eval_time': 2293.3366119861603, 'accumulated_logging_time': 0.7835638523101807, 'global_step': 32536, 'preemption_count': 0}), (33692, {'train/accuracy': 0.995521068572998, 'train/loss': 0.014790726825594902, 'train/mean_average_precision': 0.7565829009105594, 'validation/accuracy': 0.9863092303276062, 'validation/loss': 0.05365309119224548, 'validation/mean_average_precision': 0.2418288608040333, 'validation/num_examples': 43793, 'test/accuracy': 0.9854729771614075, 'test/loss': 0.057275690138339996, 'test/mean_average_precision': 0.23541710163186375, 'test/num_examples': 43793, 'score': 6972.630781650543, 'total_duration': 9345.25880765915, 'accumulated_submission_time': 6972.630781650543, 'accumulated_eval_time': 2370.939062356949, 'accumulated_logging_time': 0.8055140972137451, 'global_step': 33692, 'preemption_count': 0}), (34848, {'train/accuracy': 0.9945482015609741, 'train/loss': 0.01712888292968273, 'train/mean_average_precision': 0.7144302342316529, 'validation/accuracy': 0.9862341284751892, 'validation/loss': 0.0542571134865284, 'validation/mean_average_precision': 0.24011761526995143, 'validation/num_examples': 43793, 'test/accuracy': 0.9854022264480591, 'test/loss': 0.05815305933356285, 'test/mean_average_precision': 0.23598551251244176, 'test/num_examples': 43793, 'score': 7212.722177028656, 'total_duration': 9658.195533275604, 'accumulated_submission_time': 7212.722177028656, 'accumulated_eval_time': 2443.7321813106537, 'accumulated_logging_time': 0.827545166015625, 'global_step': 34848, 'preemption_count': 0}), (36009, {'train/accuracy': 0.9952119588851929, 'train/loss': 0.015240401960909367, 'train/mean_average_precision': 0.7510973779285022, 'validation/accuracy': 0.9861553311347961, 'validation/loss': 0.05528578907251358, 'validation/mean_average_precision': 0.23649006997409588, 'validation/num_examples': 43793, 'test/accuracy': 0.985374391078949, 'test/loss': 0.058966610580682755, 'test/mean_average_precision': 0.2341673271447617, 'test/num_examples': 43793, 'score': 7452.821468114853, 'total_duration': 9970.833562850952, 'accumulated_submission_time': 7452.821468114853, 'accumulated_eval_time': 2516.2213270664215, 'accumulated_logging_time': 0.849318265914917, 'global_step': 36009, 'preemption_count': 0}), (37164, {'train/accuracy': 0.9946776628494263, 'train/loss': 0.016332319006323814, 'train/mean_average_precision': 0.7310647975610933, 'validation/accuracy': 0.9862012267112732, 'validation/loss': 0.056744564324617386, 'validation/mean_average_precision': 0.236010592845471, 'validation/num_examples': 43793, 'test/accuracy': 0.985381543636322, 'test/loss': 0.0607730932533741, 'test/mean_average_precision': 0.23049280076958228, 'test/num_examples': 43793, 'score': 7692.8759553432465, 'total_duration': 10288.268055200577, 'accumulated_submission_time': 7692.8759553432465, 'accumulated_eval_time': 2593.545583486557, 'accumulated_logging_time': 0.8719797134399414, 'global_step': 37164, 'preemption_count': 0}), (38333, {'train/accuracy': 0.996945321559906, 'train/loss': 0.011514567770063877, 'train/mean_average_precision': 0.812810160360274, 'validation/accuracy': 0.9862154126167297, 'validation/loss': 0.05728142708539963, 'validation/mean_average_precision': 0.23833612422887548, 'validation/num_examples': 43793, 'test/accuracy': 0.9852665662765503, 'test/loss': 0.06155028194189072, 'test/mean_average_precision': 0.22762399509847517, 'test/num_examples': 43793, 'score': 7932.878902196884, 'total_duration': 10599.546877861023, 'accumulated_submission_time': 7932.878902196884, 'accumulated_eval_time': 2664.76957654953, 'accumulated_logging_time': 0.895087480545044, 'global_step': 38333, 'preemption_count': 0}), (39495, {'train/accuracy': 0.9955244660377502, 'train/loss': 0.014057081192731857, 'train/mean_average_precision': 0.7814663194745386, 'validation/accuracy': 0.9861358404159546, 'validation/loss': 0.0582909882068634, 'validation/mean_average_precision': 0.2354027819286958, 'validation/num_examples': 43793, 'test/accuracy': 0.9852425456047058, 'test/loss': 0.06241708993911743, 'test/mean_average_precision': 0.2296379954759312, 'test/num_examples': 43793, 'score': 8172.898724794388, 'total_duration': 10915.291023254395, 'accumulated_submission_time': 8172.898724794388, 'accumulated_eval_time': 2740.435772895813, 'accumulated_logging_time': 0.9203140735626221, 'global_step': 39495, 'preemption_count': 0}), (40658, {'train/accuracy': 0.9974278807640076, 'train/loss': 0.010539324954152107, 'train/mean_average_precision': 0.8350180141009163, 'validation/accuracy': 0.986068069934845, 'validation/loss': 0.058774806559085846, 'validation/mean_average_precision': 0.23153708748634433, 'validation/num_examples': 43793, 'test/accuracy': 0.9851983189582825, 'test/loss': 0.06291582435369492, 'test/mean_average_precision': 0.22559891912883753, 'test/num_examples': 43793, 'score': 8412.988114118576, 'total_duration': 11226.97063088417, 'accumulated_submission_time': 8412.988114118576, 'accumulated_eval_time': 2811.973099708557, 'accumulated_logging_time': 0.9436252117156982, 'global_step': 40658, 'preemption_count': 0}), (41824, {'train/accuracy': 0.9952295422554016, 'train/loss': 0.014390068128705025, 'train/mean_average_precision': 0.7779754447384712, 'validation/accuracy': 0.9861866235733032, 'validation/loss': 0.05978657677769661, 'validation/mean_average_precision': 0.23378385937487411, 'validation/num_examples': 43793, 'test/accuracy': 0.985236644744873, 'test/loss': 0.06418509781360626, 'test/mean_average_precision': 0.22475630918611988, 'test/num_examples': 43793, 'score': 8652.938473463058, 'total_duration': 11544.018153190613, 'accumulated_submission_time': 8652.938473463058, 'accumulated_eval_time': 2889.0169718265533, 'accumulated_logging_time': 0.9666237831115723, 'global_step': 41824, 'preemption_count': 0}), (42979, {'train/accuracy': 0.9969338178634644, 'train/loss': 0.011317688040435314, 'train/mean_average_precision': 0.8315024394536775, 'validation/accuracy': 0.9861127138137817, 'validation/loss': 0.06054950878024101, 'validation/mean_average_precision': 0.23227139473871602, 'validation/num_examples': 43793, 'test/accuracy': 0.9851406216621399, 'test/loss': 0.06507028639316559, 'test/mean_average_precision': 0.22400236830956277, 'test/num_examples': 43793, 'score': 8893.08444738388, 'total_duration': 11857.378795146942, 'accumulated_submission_time': 8893.08444738388, 'accumulated_eval_time': 2962.1772677898407, 'accumulated_logging_time': 0.9897043704986572, 'global_step': 42979, 'preemption_count': 0}), (44139, {'train/accuracy': 0.9955289363861084, 'train/loss': 0.013761293143033981, 'train/mean_average_precision': 0.7845061246579415, 'validation/accuracy': 0.9859479069709778, 'validation/loss': 0.06077152118086815, 'validation/mean_average_precision': 0.229503417208904, 'validation/num_examples': 43793, 'test/accuracy': 0.9850913286209106, 'test/loss': 0.06516489386558533, 'test/mean_average_precision': 0.2217302631520706, 'test/num_examples': 43793, 'score': 9133.135304450989, 'total_duration': 12173.537246704102, 'accumulated_submission_time': 9133.135304450989, 'accumulated_eval_time': 3038.233077287674, 'accumulated_logging_time': 1.0136849880218506, 'global_step': 44139, 'preemption_count': 0}), (45287, {'train/accuracy': 0.9982138872146606, 'train/loss': 0.009005549363791943, 'train/mean_average_precision': 0.8757303864250285, 'validation/accuracy': 0.9860749840736389, 'validation/loss': 0.06152097135782242, 'validation/mean_average_precision': 0.23057247557645089, 'validation/num_examples': 43793, 'test/accuracy': 0.985113263130188, 'test/loss': 0.0661998838186264, 'test/mean_average_precision': 0.22124957094384556, 'test/num_examples': 43793, 'score': 9373.128984451294, 'total_duration': 12488.113516569138, 'accumulated_submission_time': 9373.128984451294, 'accumulated_eval_time': 3112.76157951355, 'accumulated_logging_time': 1.0372209548950195, 'global_step': 45287, 'preemption_count': 0}), (46434, {'train/accuracy': 0.9959490895271301, 'train/loss': 0.012395849451422691, 'train/mean_average_precision': 0.8200606547448531, 'validation/accuracy': 0.9860343933105469, 'validation/loss': 0.0619334951043129, 'validation/mean_average_precision': 0.2294863399858221, 'validation/num_examples': 43793, 'test/accuracy': 0.9851035475730896, 'test/loss': 0.06656300276517868, 'test/mean_average_precision': 0.2195005036931294, 'test/num_examples': 43793, 'score': 9613.157678604126, 'total_duration': 12802.260908126831, 'accumulated_submission_time': 9613.157678604126, 'accumulated_eval_time': 3186.824418067932, 'accumulated_logging_time': 1.0617547035217285, 'global_step': 46434, 'preemption_count': 0}), (47598, {'train/accuracy': 0.998333215713501, 'train/loss': 0.008797142654657364, 'train/mean_average_precision': 0.8859533934271745, 'validation/accuracy': 0.9859158396720886, 'validation/loss': 0.06217694655060768, 'validation/mean_average_precision': 0.2274063426240968, 'validation/num_examples': 43793, 'test/accuracy': 0.985042929649353, 'test/loss': 0.06665322184562683, 'test/mean_average_precision': 0.21775207291659265, 'test/num_examples': 43793, 'score': 9853.236314535141, 'total_duration': 13117.092304468155, 'accumulated_submission_time': 9853.236314535141, 'accumulated_eval_time': 3261.52298951149, 'accumulated_logging_time': 1.0859410762786865, 'global_step': 47598, 'preemption_count': 0}), (48761, {'train/accuracy': 0.9959759712219238, 'train/loss': 0.012533287517726421, 'train/mean_average_precision': 0.8183618749058663, 'validation/accuracy': 0.9860132932662964, 'validation/loss': 0.06239403411746025, 'validation/mean_average_precision': 0.22533054546531714, 'validation/num_examples': 43793, 'test/accuracy': 0.9850564002990723, 'test/loss': 0.06707092374563217, 'test/mean_average_precision': 0.21837406252321712, 'test/num_examples': 43793, 'score': 10093.326829195023, 'total_duration': 13429.512689113617, 'accumulated_submission_time': 10093.326829195023, 'accumulated_eval_time': 3333.8000988960266, 'accumulated_logging_time': 1.1108100414276123, 'global_step': 48761, 'preemption_count': 0}), (49915, {'train/accuracy': 0.9977337718009949, 'train/loss': 0.009552744217216969, 'train/mean_average_precision': 0.8606569592132653, 'validation/accuracy': 0.9859243631362915, 'validation/loss': 0.06197017431259155, 'validation/mean_average_precision': 0.2292632990368719, 'validation/num_examples': 43793, 'test/accuracy': 0.9850319623947144, 'test/loss': 0.06650514155626297, 'test/mean_average_precision': 0.21890603482027798, 'test/num_examples': 43793, 'score': 10333.45381307602, 'total_duration': 13744.340266227722, 'accumulated_submission_time': 10333.45381307602, 'accumulated_eval_time': 3408.445756673813, 'accumulated_logging_time': 1.1338963508605957, 'global_step': 49915, 'preemption_count': 0}), (51063, {'train/accuracy': 0.9977223873138428, 'train/loss': 0.009600340388715267, 'train/mean_average_precision': 0.8630774555596852, 'validation/accuracy': 0.9859188795089722, 'validation/loss': 0.062068842351436615, 'validation/mean_average_precision': 0.2247633331852613, 'validation/num_examples': 43793, 'test/accuracy': 0.9850370287895203, 'test/loss': 0.06664708256721497, 'test/mean_average_precision': 0.21677798695773293, 'test/num_examples': 43793, 'score': 10573.550117015839, 'total_duration': 14058.581187009811, 'accumulated_submission_time': 10573.550117015839, 'accumulated_eval_time': 3482.536052942276, 'accumulated_logging_time': 1.1575829982757568, 'global_step': 51063, 'preemption_count': 0}), (52222, {'train/accuracy': 0.9981123805046082, 'train/loss': 0.009071549400687218, 'train/mean_average_precision': 0.8724912362295548, 'validation/accuracy': 0.9859409928321838, 'validation/loss': 0.061929721385240555, 'validation/mean_average_precision': 0.22722660052963015, 'validation/num_examples': 43793, 'test/accuracy': 0.9850564002990723, 'test/loss': 0.06651763617992401, 'test/mean_average_precision': 0.2182488588723709, 'test/num_examples': 43793, 'score': 10813.535950183868, 'total_duration': 14373.297874450684, 'accumulated_submission_time': 10813.535950183868, 'accumulated_eval_time': 3557.2155447006226, 'accumulated_logging_time': 1.1818804740905762, 'global_step': 52222, 'preemption_count': 0}), (53374, {'train/accuracy': 0.9979497194290161, 'train/loss': 0.009255867451429367, 'train/mean_average_precision': 0.8831672993461357, 'validation/accuracy': 0.9859409928321838, 'validation/loss': 0.061929721385240555, 'validation/mean_average_precision': 0.22708367427807555, 'validation/num_examples': 43793, 'test/accuracy': 0.9850564002990723, 'test/loss': 0.06651762872934341, 'test/mean_average_precision': 0.2182205122931107, 'test/num_examples': 43793, 'score': 11053.564621686935, 'total_duration': 14684.654006958008, 'accumulated_submission_time': 11053.564621686935, 'accumulated_eval_time': 3628.487885951996, 'accumulated_logging_time': 1.2068958282470703, 'global_step': 53374, 'preemption_count': 0}), (54516, {'train/accuracy': 0.997397780418396, 'train/loss': 0.009963732212781906, 'train/mean_average_precision': 0.8576935198336461, 'validation/accuracy': 0.9859409928321838, 'validation/loss': 0.061929721385240555, 'validation/mean_average_precision': 0.22706274587958908, 'validation/num_examples': 43793, 'test/accuracy': 0.9850564002990723, 'test/loss': 0.06651763617992401, 'test/mean_average_precision': 0.2181922863788245, 'test/num_examples': 43793, 'score': 11293.682857751846, 'total_duration': 14997.47192621231, 'accumulated_submission_time': 11293.682857751846, 'accumulated_eval_time': 3701.1329243183136, 'accumulated_logging_time': 1.23175048828125, 'global_step': 54516, 'preemption_count': 0}), (55657, {'train/accuracy': 0.9977102875709534, 'train/loss': 0.009582447819411755, 'train/mean_average_precision': 0.8649339771210496, 'validation/accuracy': 0.9859409928321838, 'validation/loss': 0.061929721385240555, 'validation/mean_average_precision': 0.22709213093412073, 'validation/num_examples': 43793, 'test/accuracy': 0.9850564002990723, 'test/loss': 0.06651763617992401, 'test/mean_average_precision': 0.21820152238599252, 'test/num_examples': 43793, 'score': 11533.686092615128, 'total_duration': 15314.622146129608, 'accumulated_submission_time': 11533.686092615128, 'accumulated_eval_time': 3778.221134185791, 'accumulated_logging_time': 1.2575616836547852, 'global_step': 55657, 'preemption_count': 0}), (56808, {'train/accuracy': 0.9976783990859985, 'train/loss': 0.009622287005186081, 'train/mean_average_precision': 0.8595378509006688, 'validation/accuracy': 0.9859409928321838, 'validation/loss': 0.061929721385240555, 'validation/mean_average_precision': 0.2270661563211023, 'validation/num_examples': 43793, 'test/accuracy': 0.9850564002990723, 'test/loss': 0.06651763617992401, 'test/mean_average_precision': 0.21840449659518407, 'test/num_examples': 43793, 'score': 11773.814343690872, 'total_duration': 15627.094087839127, 'accumulated_submission_time': 11773.814343690872, 'accumulated_eval_time': 3850.50860786438, 'accumulated_logging_time': 1.2822446823120117, 'global_step': 56808, 'preemption_count': 0})], 'global_step': 57956}
I0305 23:37:20.116359 140363820438720 submission_runner.py:649] Timing: 12013.945834159851
I0305 23:37:20.116397 140363820438720 submission_runner.py:651] Total number of evals: 50
I0305 23:37:20.116425 140363820438720 submission_runner.py:652] ====================
I0305 23:37:20.116769 140363820438720 submission_runner.py:750] Final ogbg score: 1
