python submission_runner.py --framework=jax --workload=ogbg --submission_path=prize_qualification_baselines/external_tuning/jax_nadamw_full_budget.py --data_dir=/data/ogbg --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=submissions/rolling_leaderboard/external_tuning/baseline/study_1 --overwrite=True --save_checkpoints=False --rng_seed=-1383925594 --tuning_ruleset=external --tuning_search_space=prize_qualification_baselines/external_tuning/tuning_search_space.json --num_tuning_trials=5 --hparam_start_index=3 --hparam_end_index=4 2>&1 | tee -a /logs/ogbg_jax_03-05-2025-19-13-08.log
2025-03-05 19:13:08.921895: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1741201988.945201       9 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1741201988.952553       9 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
I0305 19:13:15.327244 139988053398720 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_1/ogbg_jax.
I0305 19:13:16.303777 139988053398720 xla_bridge.py:884] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA
I0305 19:13:16.306892 139988053398720 xla_bridge.py:884] Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory
I0305 19:13:16.308678 139988053398720 submission_runner.py:606] Using RNG seed -1383925594
I0305 19:13:16.913704 139988053398720 submission_runner.py:615] --- Tuning run 4/5 ---
I0305 19:13:16.913889 139988053398720 submission_runner.py:620] Creating tuning directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_1/ogbg_jax/trial_4.
I0305 19:13:16.914071 139988053398720 logger_utils.py:97] Saving hparams to /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_1/ogbg_jax/trial_4/hparams.json.
I0305 19:13:17.150565 139988053398720 submission_runner.py:218] Initializing dataset.
I0305 19:13:17.366034 139988053398720 dataset_info.py:690] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0305 19:13:17.413177 139988053398720 reader.py:261] Creating a tf.data.Dataset reading 8 files located in folders: /data/ogbg/ogbg_molpcba/0.1.3.
WARNING:tensorflow:From /usr/local/lib/python3.11/site-packages/tensorflow_datasets/core/reader.py:101: CounterV2 (from tensorflow.python.data.experimental.ops.counter) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.counter(...)` instead.
W0305 19:13:17.637610 139988053398720 deprecation.py:50] From /usr/local/lib/python3.11/site-packages/tensorflow_datasets/core/reader.py:101: CounterV2 (from tensorflow.python.data.experimental.ops.counter) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.counter(...)` instead.
I0305 19:13:17.686569 139988053398720 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split train, from /data/ogbg/ogbg_molpcba/0.1.3
I0305 19:13:17.716810 139988053398720 submission_runner.py:229] Initializing model.
I0305 19:13:25.582194 139988053398720 submission_runner.py:272] Initializing optimizer.
I0305 19:13:25.993447 139988053398720 submission_runner.py:279] Initializing metrics bundle.
I0305 19:13:25.993693 139988053398720 submission_runner.py:301] Initializing checkpoint and logger.
I0305 19:13:25.994585 139988053398720 checkpoints.py:1101] Found no checkpoint files in /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_1/ogbg_jax/trial_4 with prefix checkpoint_
I0305 19:13:25.994700 139988053398720 submission_runner.py:321] Saving meta data to /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_1/ogbg_jax/trial_4/meta_data_0.json.
I0305 19:13:25.994871 139988053398720 logger_utils.py:262] Unable to record workload.train_mean information. Continuing without it.
I0305 19:13:25.994919 139988053398720 logger_utils.py:262] Unable to record workload.train_stddev information. Continuing without it.
I0305 19:13:26.150898 139988053398720 submission_runner.py:325] Saving flags to /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_1/ogbg_jax/trial_4/flags_0.json.
I0305 19:13:26.183394 139988053398720 submission_runner.py:337] Starting training loop.
I0305 19:13:36.851181 139851975984896 logging_writer.py:48] [0] global_step=0, grad_norm=2.9869697093963623, loss=0.755279004573822
I0305 19:13:36.903218 139988053398720 spec.py:321] Evaluating on the training split.
I0305 19:13:36.906671 139988053398720 dataset_info.py:690] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0305 19:13:36.910136 139988053398720 reader.py:261] Creating a tf.data.Dataset reading 8 files located in folders: /data/ogbg/ogbg_molpcba/0.1.3.
I0305 19:13:36.974062 139988053398720 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split train, from /data/ogbg/ogbg_molpcba/0.1.3
I0305 19:14:52.988440 139988053398720 spec.py:333] Evaluating on the validation split.
I0305 19:14:52.991023 139988053398720 dataset_info.py:690] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0305 19:14:52.994566 139988053398720 reader.py:261] Creating a tf.data.Dataset reading 1 files located in folders: /data/ogbg/ogbg_molpcba/0.1.3.
I0305 19:14:53.053805 139988053398720 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split validation, from /data/ogbg/ogbg_molpcba/0.1.3
I0305 19:15:56.117311 139988053398720 spec.py:349] Evaluating on the test split.
I0305 19:15:56.119731 139988053398720 dataset_info.py:690] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0305 19:15:56.123451 139988053398720 reader.py:261] Creating a tf.data.Dataset reading 1 files located in folders: /data/ogbg/ogbg_molpcba/0.1.3.
I0305 19:15:56.182648 139988053398720 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split test, from /data/ogbg/ogbg_molpcba/0.1.3
I0305 19:16:59.794060 139988053398720 submission_runner.py:469] Time since start: 213.61s, 	Step: 1, 	{'train/accuracy': 0.5364514589309692, 'train/loss': 0.7556861639022827, 'train/mean_average_precision': 0.02393405499105412, 'validation/accuracy': 0.5388414263725281, 'validation/loss': 0.7555237412452698, 'validation/mean_average_precision': 0.02782094985628063, 'validation/num_examples': 43793, 'test/accuracy': 0.5390405058860779, 'test/loss': 0.7549278736114502, 'test/mean_average_precision': 0.02926252684263866, 'test/num_examples': 43793, 'score': 10.719719886779785, 'total_duration': 213.61053848266602, 'accumulated_submission_time': 10.719719886779785, 'accumulated_eval_time': 202.89071488380432, 'accumulated_logging_time': 0}
I0305 19:16:59.802275 139846206908160 logging_writer.py:48] [1] accumulated_eval_time=202.891, accumulated_logging_time=0, accumulated_submission_time=10.7197, global_step=1, preemption_count=0, score=10.7197, test/accuracy=0.539041, test/loss=0.754928, test/mean_average_precision=0.0292625, test/num_examples=43793, total_duration=213.611, train/accuracy=0.536451, train/loss=0.755686, train/mean_average_precision=0.0239341, validation/accuracy=0.538841, validation/loss=0.755524, validation/mean_average_precision=0.0278209, validation/num_examples=43793
I0305 19:17:20.816994 139846215300864 logging_writer.py:48] [100] global_step=100, grad_norm=0.025310292840003967, loss=0.06666503846645355
I0305 19:17:41.394668 139846206908160 logging_writer.py:48] [200] global_step=200, grad_norm=0.01141180470585823, loss=0.05270626023411751
I0305 19:18:02.045733 139846215300864 logging_writer.py:48] [300] global_step=300, grad_norm=0.00669875368475914, loss=0.052884381264448166
I0305 19:18:22.587705 139846206908160 logging_writer.py:48] [400] global_step=400, grad_norm=0.013830626383423805, loss=0.05008803308010101
I0305 19:18:43.173479 139846215300864 logging_writer.py:48] [500] global_step=500, grad_norm=0.009474612772464752, loss=0.05531786009669304
I0305 19:19:03.596238 139846206908160 logging_writer.py:48] [600] global_step=600, grad_norm=0.017840493470430374, loss=0.05050157010555267
I0305 19:19:23.995875 139846660114176 logging_writer.py:48] [700] global_step=700, grad_norm=0.013375010341405869, loss=0.05508331581950188
I0305 19:19:44.327030 139846651721472 logging_writer.py:48] [800] global_step=800, grad_norm=0.017917558550834656, loss=0.05278492718935013
I0305 19:20:05.021965 139846660114176 logging_writer.py:48] [900] global_step=900, grad_norm=0.01160196028649807, loss=0.055921636521816254
I0305 19:20:25.405188 139846651721472 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.008143283426761627, loss=0.05541591718792915
I0305 19:20:45.805212 139846660114176 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.004944955464452505, loss=0.055257994681596756
I0305 19:20:59.935623 139988053398720 spec.py:321] Evaluating on the training split.
I0305 19:22:12.244840 139988053398720 spec.py:333] Evaluating on the validation split.
I0305 19:22:14.136040 139988053398720 spec.py:349] Evaluating on the test split.
I0305 19:22:16.012616 139988053398720 submission_runner.py:469] Time since start: 529.83s, 	Step: 1170, 	{'train/accuracy': 0.9867745041847229, 'train/loss': 0.05368809401988983, 'train/mean_average_precision': 0.04189938078535591, 'validation/accuracy': 0.9841674566268921, 'validation/loss': 0.06388798356056213, 'validation/mean_average_precision': 0.03978796145292936, 'validation/num_examples': 43793, 'test/accuracy': 0.9831724166870117, 'test/loss': 0.06711962819099426, 'test/mean_average_precision': 0.041355002685032234, 'test/num_examples': 43793, 'score': 250.8117754459381, 'total_duration': 529.8291528224945, 'accumulated_submission_time': 250.8117754459381, 'accumulated_eval_time': 278.96765327453613, 'accumulated_logging_time': 0.018632888793945312}
I0305 19:22:16.020922 139846651721472 logging_writer.py:48] [1170] accumulated_eval_time=278.968, accumulated_logging_time=0.0186329, accumulated_submission_time=250.812, global_step=1170, preemption_count=0, score=250.812, test/accuracy=0.983172, test/loss=0.0671196, test/mean_average_precision=0.041355, test/num_examples=43793, total_duration=529.829, train/accuracy=0.986775, train/loss=0.0536881, train/mean_average_precision=0.0418994, validation/accuracy=0.984167, validation/loss=0.063888, validation/mean_average_precision=0.039788, validation/num_examples=43793
I0305 19:22:22.427119 139846660114176 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.00933878030627966, loss=0.05522438883781433
I0305 19:22:43.037755 139846651721472 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.004071992356330156, loss=0.054562121629714966
I0305 19:23:03.796892 139846660114176 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.005261576734483242, loss=0.049875400960445404
I0305 19:23:24.549342 139846651721472 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.00559264374896884, loss=0.056519415229558945
I0305 19:23:45.292536 139846660114176 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.0050756605342030525, loss=0.054214607924222946
I0305 19:24:06.793095 139846651721472 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.00697462260723114, loss=0.05731317028403282
I0305 19:24:26.975091 139846660114176 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.01021914929151535, loss=0.05706504359841347
I0305 19:24:47.665578 139846651721472 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.008893309161067009, loss=0.054611511528491974
I0305 19:25:08.675942 139846660114176 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.005347554571926594, loss=0.05290446802973747
I0305 19:25:30.019154 139846651721472 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.006140572484582663, loss=0.05002286657691002
I0305 19:25:50.669783 139846660114176 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.008549983613193035, loss=0.05315866693854332
I0305 19:26:11.360641 139846651721472 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.012267055921256542, loss=0.05408761650323868
I0305 19:26:16.113914 139988053398720 spec.py:321] Evaluating on the training split.
I0305 19:27:28.560669 139988053398720 spec.py:333] Evaluating on the validation split.
I0305 19:27:30.491279 139988053398720 spec.py:349] Evaluating on the test split.
I0305 19:27:32.388285 139988053398720 submission_runner.py:469] Time since start: 846.20s, 	Step: 2324, 	{'train/accuracy': 0.9869147539138794, 'train/loss': 0.05123446136713028, 'train/mean_average_precision': 0.05487954520788056, 'validation/accuracy': 0.984216570854187, 'validation/loss': 0.06142273172736168, 'validation/mean_average_precision': 0.05409831568994666, 'validation/num_examples': 43793, 'test/accuracy': 0.9832220673561096, 'test/loss': 0.06460878252983093, 'test/mean_average_precision': 0.05614455724377822, 'test/num_examples': 43793, 'score': 490.86257910728455, 'total_duration': 846.2048377990723, 'accumulated_submission_time': 490.86257910728455, 'accumulated_eval_time': 355.2419879436493, 'accumulated_logging_time': 0.035897254943847656}
I0305 19:27:32.396641 139846660114176 logging_writer.py:48] [2324] accumulated_eval_time=355.242, accumulated_logging_time=0.0358973, accumulated_submission_time=490.863, global_step=2324, preemption_count=0, score=490.863, test/accuracy=0.983222, test/loss=0.0646088, test/mean_average_precision=0.0561446, test/num_examples=43793, total_duration=846.205, train/accuracy=0.986915, train/loss=0.0512345, train/mean_average_precision=0.0548795, validation/accuracy=0.984217, validation/loss=0.0614227, validation/mean_average_precision=0.0540983, validation/num_examples=43793
I0305 19:27:48.236694 139846651721472 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.02492763288319111, loss=0.055066946893930435
I0305 19:28:09.230440 139846660114176 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.00944947637617588, loss=0.052343402057886124
I0305 19:28:29.702753 139846651721472 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.012128726579248905, loss=0.04818122833967209
I0305 19:28:50.545814 139846660114176 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.009731008671224117, loss=0.04765711724758148
I0305 19:29:11.083589 139846651721472 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.0073327599093317986, loss=0.053276535123586655
I0305 19:29:31.581043 139846660114176 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.01309872791171074, loss=0.05124526843428612
I0305 19:29:51.851037 139846651721472 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.02142811007797718, loss=0.05116335675120354
I0305 19:30:12.298732 139846660114176 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.018957404419779778, loss=0.04864546284079552
I0305 19:30:32.611557 139846651721472 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.021701104938983917, loss=0.05260622128844261
I0305 19:30:52.889652 139846660114176 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.013259449042379856, loss=0.05001838505268097
I0305 19:31:13.167998 139846651721472 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.015930132940411568, loss=0.042192861437797546
I0305 19:31:32.418608 139988053398720 spec.py:321] Evaluating on the training split.
I0305 19:32:43.462580 139988053398720 spec.py:333] Evaluating on the validation split.
I0305 19:32:45.361634 139988053398720 spec.py:349] Evaluating on the test split.
I0305 19:32:47.224519 139988053398720 submission_runner.py:469] Time since start: 1161.04s, 	Step: 3497, 	{'train/accuracy': 0.9871610403060913, 'train/loss': 0.04742623493075371, 'train/mean_average_precision': 0.0965885717008074, 'validation/accuracy': 0.9845405220985413, 'validation/loss': 0.05695180222392082, 'validation/mean_average_precision': 0.09142563291241514, 'validation/num_examples': 43793, 'test/accuracy': 0.9835548400878906, 'test/loss': 0.060451168566942215, 'test/mean_average_precision': 0.09165636591552101, 'test/num_examples': 43793, 'score': 730.8458979129791, 'total_duration': 1161.0410740375519, 'accumulated_submission_time': 730.8458979129791, 'accumulated_eval_time': 430.0478582382202, 'accumulated_logging_time': 0.0531158447265625}
I0305 19:32:47.233120 139846660114176 logging_writer.py:48] [3497] accumulated_eval_time=430.048, accumulated_logging_time=0.0531158, accumulated_submission_time=730.846, global_step=3497, preemption_count=0, score=730.846, test/accuracy=0.983555, test/loss=0.0604512, test/mean_average_precision=0.0916564, test/num_examples=43793, total_duration=1161.04, train/accuracy=0.987161, train/loss=0.0474262, train/mean_average_precision=0.0965886, validation/accuracy=0.984541, validation/loss=0.0569518, validation/mean_average_precision=0.0914256, validation/num_examples=43793
I0305 19:32:48.062248 139846651721472 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.02545856684446335, loss=0.047911662608385086
I0305 19:33:08.469495 139846660114176 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.025032425299286842, loss=0.044235311448574066
I0305 19:33:29.012578 139846651721472 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.021859459578990936, loss=0.043005120009183884
I0305 19:33:49.331942 139846660114176 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.012359289452433586, loss=0.04788291081786156
I0305 19:34:09.347412 139846651721472 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.06168094649910927, loss=0.05153699964284897
I0305 19:34:29.351341 139846660114176 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.02281172387301922, loss=0.04925056919455528
I0305 19:34:49.828902 139846651721472 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.018552005290985107, loss=0.04476097971200943
I0305 19:35:10.508962 139846660114176 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.021908991038799286, loss=0.0453224703669548
I0305 19:35:30.538550 139846651721472 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.02575264684855938, loss=0.04505385458469391
I0305 19:35:50.929053 139846660114176 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.055316321551799774, loss=0.05067436397075653
I0305 19:36:11.434311 139846651721472 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.012109527364373207, loss=0.046241167932748795
I0305 19:36:31.597224 139846660114176 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.016080843284726143, loss=0.04149504005908966
I0305 19:36:47.414743 139988053398720 spec.py:321] Evaluating on the training split.
I0305 19:37:59.712738 139988053398720 spec.py:333] Evaluating on the validation split.
I0305 19:38:01.691313 139988053398720 spec.py:349] Evaluating on the test split.
I0305 19:38:03.569525 139988053398720 submission_runner.py:469] Time since start: 1477.39s, 	Step: 4677, 	{'train/accuracy': 0.9877002239227295, 'train/loss': 0.04505211114883423, 'train/mean_average_precision': 0.12453795896404228, 'validation/accuracy': 0.9848470091819763, 'validation/loss': 0.05384093523025513, 'validation/mean_average_precision': 0.11726347991929706, 'validation/num_examples': 43793, 'test/accuracy': 0.9839099049568176, 'test/loss': 0.0565769337117672, 'test/mean_average_precision': 0.11790388623984789, 'test/num_examples': 43793, 'score': 970.9864010810852, 'total_duration': 1477.3860812187195, 'accumulated_submission_time': 970.9864010810852, 'accumulated_eval_time': 506.2025966644287, 'accumulated_logging_time': 0.07101869583129883}
I0305 19:38:03.578263 139846176491264 logging_writer.py:48] [4677] accumulated_eval_time=506.203, accumulated_logging_time=0.0710187, accumulated_submission_time=970.986, global_step=4677, preemption_count=0, score=970.986, test/accuracy=0.98391, test/loss=0.0565769, test/mean_average_precision=0.117904, test/num_examples=43793, total_duration=1477.39, train/accuracy=0.9877, train/loss=0.0450521, train/mean_average_precision=0.124538, validation/accuracy=0.984847, validation/loss=0.0538409, validation/mean_average_precision=0.117263, validation/num_examples=43793
I0305 19:38:08.407932 139846059546368 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.026968741789460182, loss=0.0431048721075058
I0305 19:38:28.699749 139846176491264 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.05060802027583122, loss=0.04511198028922081
I0305 19:38:49.208932 139846059546368 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.03823544457554817, loss=0.044364575296640396
I0305 19:39:09.850850 139846176491264 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.034496407955884933, loss=0.04364718124270439
I0305 19:39:30.205893 139846059546368 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.0966547429561615, loss=0.044423867017030716
I0305 19:39:50.742129 139846176491264 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.05527232587337494, loss=0.044609878212213516
I0305 19:40:11.385509 139846059546368 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.043703529983758926, loss=0.04626486077904701
I0305 19:40:32.110537 139846176491264 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.01886807009577751, loss=0.04647986963391304
I0305 19:40:52.594383 139846059546368 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.0702863335609436, loss=0.04078489542007446
I0305 19:41:12.823705 139846176491264 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.042269062250852585, loss=0.04402448609471321
I0305 19:41:33.125154 139846059546368 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.030948176980018616, loss=0.050547875463962555
I0305 19:41:53.429754 139846176491264 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.05239582806825638, loss=0.04118480533361435
I0305 19:42:03.597694 139988053398720 spec.py:321] Evaluating on the training split.
I0305 19:43:13.732312 139988053398720 spec.py:333] Evaluating on the validation split.
I0305 19:43:15.668193 139988053398720 spec.py:349] Evaluating on the test split.
I0305 19:43:17.545118 139988053398720 submission_runner.py:469] Time since start: 1791.36s, 	Step: 5851, 	{'train/accuracy': 0.9878891110420227, 'train/loss': 0.0430062934756279, 'train/mean_average_precision': 0.14275915561946215, 'validation/accuracy': 0.984795868396759, 'validation/loss': 0.05284382775425911, 'validation/mean_average_precision': 0.1292960581628193, 'validation/num_examples': 43793, 'test/accuracy': 0.9838420748710632, 'test/loss': 0.05547240376472473, 'test/mean_average_precision': 0.13955263815440438, 'test/num_examples': 43793, 'score': 1210.9637596607208, 'total_duration': 1791.361675977707, 'accumulated_submission_time': 1210.9637596607208, 'accumulated_eval_time': 580.1499774456024, 'accumulated_logging_time': 0.0892038345336914}
I0305 19:43:17.554757 139846059546368 logging_writer.py:48] [5851] accumulated_eval_time=580.15, accumulated_logging_time=0.0892038, accumulated_submission_time=1210.96, global_step=5851, preemption_count=0, score=1210.96, test/accuracy=0.983842, test/loss=0.0554724, test/mean_average_precision=0.139553, test/num_examples=43793, total_duration=1791.36, train/accuracy=0.987889, train/loss=0.0430063, train/mean_average_precision=0.142759, validation/accuracy=0.984796, validation/loss=0.0528438, validation/mean_average_precision=0.129296, validation/num_examples=43793
I0305 19:43:27.620140 139846176491264 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.061945442110300064, loss=0.04490900784730911
I0305 19:43:47.982360 139846059546368 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.03981956094503403, loss=0.04317599534988403
I0305 19:44:08.582400 139846176491264 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.053409378975629807, loss=0.046243760734796524
I0305 19:44:29.242833 139846059546368 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.0603521503508091, loss=0.047759607434272766
I0305 19:44:49.676837 139846176491264 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.0356954000890255, loss=0.036228787153959274
I0305 19:45:10.117759 139846059546368 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.09735851734876633, loss=0.04217915236949921
I0305 19:45:30.549744 139846176491264 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.06483391672372818, loss=0.04158217832446098
I0305 19:45:50.682181 139846059546368 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.031147252768278122, loss=0.04014578089118004
I0305 19:46:11.364441 139846176491264 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.01856004260480404, loss=0.03885259851813316
I0305 19:46:31.906711 139846059546368 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.03191173076629639, loss=0.043204743415117264
I0305 19:46:52.384496 139846176491264 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.08365471661090851, loss=0.04187202453613281
I0305 19:47:12.730660 139846059546368 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.06689897179603577, loss=0.0408148318529129
I0305 19:47:17.546957 139988053398720 spec.py:321] Evaluating on the training split.
I0305 19:48:29.441946 139988053398720 spec.py:333] Evaluating on the validation split.
I0305 19:48:31.369743 139988053398720 spec.py:349] Evaluating on the test split.
I0305 19:48:33.242011 139988053398720 submission_runner.py:469] Time since start: 2107.06s, 	Step: 7025, 	{'train/accuracy': 0.988011360168457, 'train/loss': 0.04190972074866295, 'train/mean_average_precision': 0.16804879014885216, 'validation/accuracy': 0.9851494431495667, 'validation/loss': 0.0512109138071537, 'validation/mean_average_precision': 0.1440025198423024, 'validation/num_examples': 43793, 'test/accuracy': 0.9842514991760254, 'test/loss': 0.05385816842317581, 'test/mean_average_precision': 0.14748185814655518, 'test/num_examples': 43793, 'score': 1450.9160923957825, 'total_duration': 2107.0585041046143, 'accumulated_submission_time': 1450.9160923957825, 'accumulated_eval_time': 655.8449256420135, 'accumulated_logging_time': 0.10793399810791016}
I0305 19:48:33.251096 139846176491264 logging_writer.py:48] [7025] accumulated_eval_time=655.845, accumulated_logging_time=0.107934, accumulated_submission_time=1450.92, global_step=7025, preemption_count=0, score=1450.92, test/accuracy=0.984251, test/loss=0.0538582, test/mean_average_precision=0.147482, test/num_examples=43793, total_duration=2107.06, train/accuracy=0.988011, train/loss=0.0419097, train/mean_average_precision=0.168049, validation/accuracy=0.985149, validation/loss=0.0512109, validation/mean_average_precision=0.144003, validation/num_examples=43793
I0305 19:48:48.869621 139846059546368 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.057039834558963776, loss=0.04254540055990219
I0305 19:49:09.391784 139846176491264 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.05430828407406807, loss=0.03851905092597008
I0305 19:49:29.937213 139846059546368 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.03715359792113304, loss=0.04311960190534592
I0305 19:49:50.808766 139846176491264 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.07229191064834595, loss=0.03957294300198555
I0305 19:50:11.514267 139846059546368 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.09797287732362747, loss=0.042106252163648605
I0305 19:50:31.932301 139846176491264 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.03719441220164299, loss=0.043649058789014816
I0305 19:50:52.507076 139846059546368 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.028812048956751823, loss=0.0384707935154438
I0305 19:51:13.332973 139846176491264 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.02333126589655876, loss=0.039900053292512894
I0305 19:51:33.672263 139846059546368 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.05185541883111, loss=0.0402044877409935
I0305 19:51:54.275198 139846176491264 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.053563740104436874, loss=0.039904993027448654
I0305 19:52:14.788168 139846059546368 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.029174869880080223, loss=0.03799865022301674
I0305 19:52:33.433385 139988053398720 spec.py:321] Evaluating on the training split.
I0305 19:53:42.720682 139988053398720 spec.py:333] Evaluating on the validation split.
I0305 19:53:44.670671 139988053398720 spec.py:349] Evaluating on the test split.
I0305 19:53:46.601730 139988053398720 submission_runner.py:469] Time since start: 2420.42s, 	Step: 8191, 	{'train/accuracy': 0.9882838129997253, 'train/loss': 0.04094330966472626, 'train/mean_average_precision': 0.17232705252527517, 'validation/accuracy': 0.9853893518447876, 'validation/loss': 0.05049822852015495, 'validation/mean_average_precision': 0.15103565161438587, 'validation/num_examples': 43793, 'test/accuracy': 0.9844840168952942, 'test/loss': 0.053228601813316345, 'test/mean_average_precision': 0.15283918803018304, 'test/num_examples': 43793, 'score': 1691.0586531162262, 'total_duration': 2420.4182710647583, 'accumulated_submission_time': 1691.0586531162262, 'accumulated_eval_time': 729.0132117271423, 'accumulated_logging_time': 0.12584805488586426}
I0305 19:53:46.610605 139846176491264 logging_writer.py:48] [8191] accumulated_eval_time=729.013, accumulated_logging_time=0.125848, accumulated_submission_time=1691.06, global_step=8191, preemption_count=0, score=1691.06, test/accuracy=0.984484, test/loss=0.0532286, test/mean_average_precision=0.152839, test/num_examples=43793, total_duration=2420.42, train/accuracy=0.988284, train/loss=0.0409433, train/mean_average_precision=0.172327, validation/accuracy=0.985389, validation/loss=0.0504982, validation/mean_average_precision=0.151036, validation/num_examples=43793
I0305 19:53:48.661869 139846059546368 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.01876601204276085, loss=0.0351775661110878
I0305 19:54:08.966310 139846176491264 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.048489950597286224, loss=0.0388721264898777
I0305 19:54:28.775259 139846059546368 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.07687127590179443, loss=0.047474827617406845
I0305 19:54:48.676053 139846176491264 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.02877577766776085, loss=0.04150160029530525
I0305 19:55:09.517454 139846059546368 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.024426914751529694, loss=0.04156990721821785
I0305 19:55:30.248481 139846176491264 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.06477848440408707, loss=0.04364689439535141
I0305 19:55:50.946777 139846059546368 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.09358488768339157, loss=0.03522323817014694
I0305 19:56:11.701872 139846176491264 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.09975706040859222, loss=0.04120119661092758
I0305 19:56:32.433645 139846059546368 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.055656325072050095, loss=0.04206051304936409
I0305 19:56:53.064309 139846176491264 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.03782947361469269, loss=0.04417799785733223
I0305 19:57:13.371937 139846059546368 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.0993199497461319, loss=0.04347849264740944
I0305 19:57:33.747629 139846176491264 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.03080923669040203, loss=0.03989885002374649
I0305 19:57:46.713839 139988053398720 spec.py:321] Evaluating on the training split.
I0305 19:58:59.157363 139988053398720 spec.py:333] Evaluating on the validation split.
I0305 19:59:01.084291 139988053398720 spec.py:349] Evaluating on the test split.
I0305 19:59:03.013625 139988053398720 submission_runner.py:469] Time since start: 2736.83s, 	Step: 9364, 	{'train/accuracy': 0.9884957075119019, 'train/loss': 0.0406404584646225, 'train/mean_average_precision': 0.18594714553198588, 'validation/accuracy': 0.9852492809295654, 'validation/loss': 0.05092266574501991, 'validation/mean_average_precision': 0.1559057381398793, 'validation/num_examples': 43793, 'test/accuracy': 0.9843230843544006, 'test/loss': 0.053471677005290985, 'test/mean_average_precision': 0.15164588773075652, 'test/num_examples': 43793, 'score': 1931.123347043991, 'total_duration': 2736.8301243782043, 'accumulated_submission_time': 1931.123347043991, 'accumulated_eval_time': 805.3129160404205, 'accumulated_logging_time': 0.1447131633758545}
I0305 19:59:03.023566 139846059546368 logging_writer.py:48] [9364] accumulated_eval_time=805.313, accumulated_logging_time=0.144713, accumulated_submission_time=1931.12, global_step=9364, preemption_count=0, score=1931.12, test/accuracy=0.984323, test/loss=0.0534717, test/mean_average_precision=0.151646, test/num_examples=43793, total_duration=2736.83, train/accuracy=0.988496, train/loss=0.0406405, train/mean_average_precision=0.185947, validation/accuracy=0.985249, validation/loss=0.0509227, validation/mean_average_precision=0.155906, validation/num_examples=43793
I0305 19:59:10.735347 139846176491264 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.06617936491966248, loss=0.042340606451034546
I0305 19:59:31.131962 139846059546368 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.038345761597156525, loss=0.03650866821408272
I0305 19:59:51.424091 139846176491264 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.043053269386291504, loss=0.04408685490489006
I0305 20:00:11.917316 139846059546368 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.028683144599199295, loss=0.03749600797891617
I0305 20:00:32.609235 139846176491264 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.06696522235870361, loss=0.0413629449903965
I0305 20:00:53.468637 139846059546368 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.039930664002895355, loss=0.03801444172859192
I0305 20:01:13.936965 139846176491264 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.028352584689855576, loss=0.03982365503907204
I0305 20:01:34.819587 139846059546368 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.05885397642850876, loss=0.04535626992583275
I0305 20:01:55.705276 139846176491264 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.08834011852741241, loss=0.04195278882980347
I0305 20:02:16.461441 139846059546368 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.04111938178539276, loss=0.046357154846191406
I0305 20:02:37.224101 139846176491264 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.07482141256332397, loss=0.043128348886966705
I0305 20:02:58.029342 139846059546368 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.09183573722839355, loss=0.03712262585759163
I0305 20:03:03.079087 139988053398720 spec.py:321] Evaluating on the training split.
I0305 20:04:14.893694 139988053398720 spec.py:333] Evaluating on the validation split.
I0305 20:04:16.994661 139988053398720 spec.py:349] Evaluating on the test split.
I0305 20:04:19.008061 139988053398720 submission_runner.py:469] Time since start: 3052.82s, 	Step: 10526, 	{'train/accuracy': 0.98842453956604, 'train/loss': 0.040268488228321075, 'train/mean_average_precision': 0.1832961713946283, 'validation/accuracy': 0.9854104518890381, 'validation/loss': 0.0497840940952301, 'validation/mean_average_precision': 0.1578549479765775, 'validation/num_examples': 43793, 'test/accuracy': 0.9844090342521667, 'test/loss': 0.05271275341510773, 'test/mean_average_precision': 0.15375624163507193, 'test/num_examples': 43793, 'score': 2171.1359186172485, 'total_duration': 3052.824548959732, 'accumulated_submission_time': 2171.1359186172485, 'accumulated_eval_time': 881.2417764663696, 'accumulated_logging_time': 0.16405224800109863}
I0305 20:04:19.018100 139846176491264 logging_writer.py:48] [10526] accumulated_eval_time=881.242, accumulated_logging_time=0.164052, accumulated_submission_time=2171.14, global_step=10526, preemption_count=0, score=2171.14, test/accuracy=0.984409, test/loss=0.0527128, test/mean_average_precision=0.153756, test/num_examples=43793, total_duration=3052.82, train/accuracy=0.988425, train/loss=0.0402685, train/mean_average_precision=0.183296, validation/accuracy=0.98541, validation/loss=0.0497841, validation/mean_average_precision=0.157855, validation/num_examples=43793
I0305 20:04:34.002913 139846059546368 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.1046612337231636, loss=0.04082987830042839
I0305 20:04:54.022611 139846176491264 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.0950082391500473, loss=0.042237140238285065
I0305 20:05:14.537502 139846059546368 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.051267195492982864, loss=0.044477108865976334
I0305 20:05:35.222542 139846176491264 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.027337631210684776, loss=0.03834720700979233
I0305 20:05:55.702178 139846059546368 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.03213163837790489, loss=0.039358481764793396
I0305 20:06:16.100461 139846176491264 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.07488397508859634, loss=0.04176412895321846
I0305 20:06:36.453910 139846059546368 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.06061007082462311, loss=0.04209659621119499
I0305 20:06:56.805392 139846176491264 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.0821223333477974, loss=0.043714601546525955
I0305 20:07:17.280245 139846059546368 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.12256232649087906, loss=0.03878897801041603
I0305 20:07:37.881238 139846176491264 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.03241942077875137, loss=0.0379437692463398
I0305 20:07:58.087046 139846059546368 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.1603015959262848, loss=0.03931969031691551
I0305 20:08:18.478158 139846176491264 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.05473501235246658, loss=0.043338265269994736
I0305 20:08:19.077648 139988053398720 spec.py:321] Evaluating on the training split.
I0305 20:09:28.535614 139988053398720 spec.py:333] Evaluating on the validation split.
I0305 20:09:30.484975 139988053398720 spec.py:349] Evaluating on the test split.
I0305 20:09:32.384439 139988053398720 submission_runner.py:469] Time since start: 3366.20s, 	Step: 11704, 	{'train/accuracy': 0.9886184930801392, 'train/loss': 0.039202991873025894, 'train/mean_average_precision': 0.19840598032511872, 'validation/accuracy': 0.9855521321296692, 'validation/loss': 0.04955240711569786, 'validation/mean_average_precision': 0.16933319193913793, 'validation/num_examples': 43793, 'test/accuracy': 0.9846259355545044, 'test/loss': 0.052462127059698105, 'test/mean_average_precision': 0.15793779329496552, 'test/num_examples': 43793, 'score': 2411.153899669647, 'total_duration': 3366.200874567032, 'accumulated_submission_time': 2411.153899669647, 'accumulated_eval_time': 954.5483994483948, 'accumulated_logging_time': 0.18379735946655273}
I0305 20:09:32.394093 139846059546368 logging_writer.py:48] [11704] accumulated_eval_time=954.548, accumulated_logging_time=0.183797, accumulated_submission_time=2411.15, global_step=11704, preemption_count=0, score=2411.15, test/accuracy=0.984626, test/loss=0.0524621, test/mean_average_precision=0.157938, test/num_examples=43793, total_duration=3366.2, train/accuracy=0.988618, train/loss=0.039203, train/mean_average_precision=0.198406, validation/accuracy=0.985552, validation/loss=0.0495524, validation/mean_average_precision=0.169333, validation/num_examples=43793
I0305 20:09:52.256492 139846176491264 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.04534156993031502, loss=0.0433153435587883
I0305 20:10:12.557694 139846059546368 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.028157223016023636, loss=0.04182255640625954
I0305 20:10:32.925699 139846176491264 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.12252496182918549, loss=0.04479580000042915
I0305 20:10:53.296235 139846059546368 logging_writer.py:48] [12100] global_step=12100, grad_norm=0.022448502480983734, loss=0.03811221569776535
I0305 20:11:13.785706 139846176491264 logging_writer.py:48] [12200] global_step=12200, grad_norm=0.1248290091753006, loss=0.03840073198080063
I0305 20:11:34.500429 139846059546368 logging_writer.py:48] [12300] global_step=12300, grad_norm=0.05325287953019142, loss=0.03997324779629707
I0305 20:11:55.074882 139846176491264 logging_writer.py:48] [12400] global_step=12400, grad_norm=0.03138361871242523, loss=0.03847605735063553
I0305 20:12:15.171169 139846059546368 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.05884919315576553, loss=0.0423717126250267
I0305 20:12:35.389847 139846176491264 logging_writer.py:48] [12600] global_step=12600, grad_norm=0.05935800448060036, loss=0.03713913634419441
I0305 20:12:55.851224 139846059546368 logging_writer.py:48] [12700] global_step=12700, grad_norm=0.10550457239151001, loss=0.03816628083586693
I0305 20:13:16.030297 139846176491264 logging_writer.py:48] [12800] global_step=12800, grad_norm=0.08245658129453659, loss=0.0398116409778595
I0305 20:13:32.430250 139988053398720 spec.py:321] Evaluating on the training split.
I0305 20:14:42.849321 139988053398720 spec.py:333] Evaluating on the validation split.
I0305 20:14:44.750511 139988053398720 spec.py:349] Evaluating on the test split.
I0305 20:14:46.638647 139988053398720 submission_runner.py:469] Time since start: 3680.46s, 	Step: 12881, 	{'train/accuracy': 0.9886207580566406, 'train/loss': 0.03934444859623909, 'train/mean_average_precision': 0.19868579731807856, 'validation/accuracy': 0.9856162667274475, 'validation/loss': 0.05002313479781151, 'validation/mean_average_precision': 0.16834695388139903, 'validation/num_examples': 43793, 'test/accuracy': 0.9846171140670776, 'test/loss': 0.0532895028591156, 'test/mean_average_precision': 0.16455377091777112, 'test/num_examples': 43793, 'score': 2651.151495695114, 'total_duration': 3680.455201148987, 'accumulated_submission_time': 2651.151495695114, 'accumulated_eval_time': 1028.7567501068115, 'accumulated_logging_time': 0.2025771141052246}
I0305 20:14:46.648581 139846059546368 logging_writer.py:48] [12881] accumulated_eval_time=1028.76, accumulated_logging_time=0.202577, accumulated_submission_time=2651.15, global_step=12881, preemption_count=0, score=2651.15, test/accuracy=0.984617, test/loss=0.0532895, test/mean_average_precision=0.164554, test/num_examples=43793, total_duration=3680.46, train/accuracy=0.988621, train/loss=0.0393444, train/mean_average_precision=0.198686, validation/accuracy=0.985616, validation/loss=0.0500231, validation/mean_average_precision=0.168347, validation/num_examples=43793
I0305 20:14:50.667056 139846176491264 logging_writer.py:48] [12900] global_step=12900, grad_norm=0.14593718945980072, loss=0.039452530443668365
I0305 20:15:11.006548 139846059546368 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.032486751675605774, loss=0.03963785246014595
I0305 20:15:31.427776 139846176491264 logging_writer.py:48] [13100] global_step=13100, grad_norm=0.08851625770330429, loss=0.040884312242269516
I0305 20:15:51.862982 139846059546368 logging_writer.py:48] [13200] global_step=13200, grad_norm=0.043821562081575394, loss=0.039869699627161026
I0305 20:16:12.319918 139846176491264 logging_writer.py:48] [13300] global_step=13300, grad_norm=0.09740933030843735, loss=0.03986900672316551
I0305 20:16:32.779158 139846059546368 logging_writer.py:48] [13400] global_step=13400, grad_norm=0.16691245138645172, loss=0.04093129187822342
I0305 20:16:53.235481 139846176491264 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.03332981839776039, loss=0.040194328874349594
I0305 20:17:13.657643 139846059546368 logging_writer.py:48] [13600] global_step=13600, grad_norm=0.10471723973751068, loss=0.04362482205033302
I0305 20:17:34.039158 139846176491264 logging_writer.py:48] [13700] global_step=13700, grad_norm=0.19202309846878052, loss=0.0415319986641407
I0305 20:17:54.370127 139846059546368 logging_writer.py:48] [13800] global_step=13800, grad_norm=0.039924461394548416, loss=0.03490569069981575
I0305 20:18:15.030936 139846176491264 logging_writer.py:48] [13900] global_step=13900, grad_norm=0.03352928161621094, loss=0.03744780272245407
I0305 20:18:35.668110 139846059546368 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.13975878059864044, loss=0.036217257380485535
I0305 20:18:46.651953 139988053398720 spec.py:321] Evaluating on the training split.
I0305 20:19:59.203872 139988053398720 spec.py:333] Evaluating on the validation split.
I0305 20:20:01.264735 139988053398720 spec.py:349] Evaluating on the test split.
I0305 20:20:03.205867 139988053398720 submission_runner.py:469] Time since start: 3997.02s, 	Step: 14055, 	{'train/accuracy': 0.9885795712471008, 'train/loss': 0.039418600499629974, 'train/mean_average_precision': 0.21050105707368613, 'validation/accuracy': 0.9854084253311157, 'validation/loss': 0.049398649483919144, 'validation/mean_average_precision': 0.176549990564398, 'validation/num_examples': 43793, 'test/accuracy': 0.9845244288444519, 'test/loss': 0.052014779299497604, 'test/mean_average_precision': 0.16506331970775495, 'test/num_examples': 43793, 'score': 2891.1129665374756, 'total_duration': 3997.022296190262, 'accumulated_submission_time': 2891.1129665374756, 'accumulated_eval_time': 1105.3104910850525, 'accumulated_logging_time': 0.22135496139526367}
I0305 20:20:03.216410 139846176491264 logging_writer.py:48] [14055] accumulated_eval_time=1105.31, accumulated_logging_time=0.221355, accumulated_submission_time=2891.11, global_step=14055, preemption_count=0, score=2891.11, test/accuracy=0.984524, test/loss=0.0520148, test/mean_average_precision=0.165063, test/num_examples=43793, total_duration=3997.02, train/accuracy=0.98858, train/loss=0.0394186, train/mean_average_precision=0.210501, validation/accuracy=0.985408, validation/loss=0.0493986, validation/mean_average_precision=0.17655, validation/num_examples=43793
I0305 20:20:12.592162 139846059546368 logging_writer.py:48] [14100] global_step=14100, grad_norm=0.03424537926912308, loss=0.03941871598362923
I0305 20:20:33.011562 139846176491264 logging_writer.py:48] [14200] global_step=14200, grad_norm=0.05705475062131882, loss=0.03944757953286171
I0305 20:20:53.971641 139846059546368 logging_writer.py:48] [14300] global_step=14300, grad_norm=0.040136922150850296, loss=0.03780027851462364
I0305 20:21:14.755979 139846176491264 logging_writer.py:48] [14400] global_step=14400, grad_norm=0.04169067367911339, loss=0.040684644132852554
I0305 20:21:35.334977 139846059546368 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.04041409119963646, loss=0.04343606159090996
I0305 20:21:55.785845 139846176491264 logging_writer.py:48] [14600] global_step=14600, grad_norm=0.05236922949552536, loss=0.03640303388237953
I0305 20:22:16.316725 139846059546368 logging_writer.py:48] [14700] global_step=14700, grad_norm=0.023742379620671272, loss=0.036581143736839294
I0305 20:22:36.611545 139846176491264 logging_writer.py:48] [14800] global_step=14800, grad_norm=0.03495099022984505, loss=0.0410756841301918
I0305 20:22:56.974505 139846059546368 logging_writer.py:48] [14900] global_step=14900, grad_norm=0.09829725325107574, loss=0.035994455218315125
I0305 20:23:17.358386 139846176491264 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.03864287585020065, loss=0.0406029038131237
I0305 20:23:37.554568 139846059546368 logging_writer.py:48] [15100] global_step=15100, grad_norm=0.028772365301847458, loss=0.04052598401904106
I0305 20:23:57.787598 139846176491264 logging_writer.py:48] [15200] global_step=15200, grad_norm=0.05125291645526886, loss=0.03870804235339165
I0305 20:24:03.210589 139988053398720 spec.py:321] Evaluating on the training split.
I0305 20:25:12.417959 139988053398720 spec.py:333] Evaluating on the validation split.
I0305 20:25:14.345355 139988053398720 spec.py:349] Evaluating on the test split.
I0305 20:25:16.238184 139988053398720 submission_runner.py:469] Time since start: 4310.05s, 	Step: 15227, 	{'train/accuracy': 0.9887218475341797, 'train/loss': 0.038751356303691864, 'train/mean_average_precision': 0.20543755742533734, 'validation/accuracy': 0.9856694340705872, 'validation/loss': 0.048988793045282364, 'validation/mean_average_precision': 0.17542661194392786, 'validation/num_examples': 43793, 'test/accuracy': 0.9846916198730469, 'test/loss': 0.051944900304079056, 'test/mean_average_precision': 0.16663763202110018, 'test/num_examples': 43793, 'score': 3131.066616296768, 'total_duration': 4310.054613828659, 'accumulated_submission_time': 3131.066616296768, 'accumulated_eval_time': 1178.3379123210907, 'accumulated_logging_time': 0.24144697189331055}
I0305 20:25:16.248154 139846059546368 logging_writer.py:48] [15227] accumulated_eval_time=1178.34, accumulated_logging_time=0.241447, accumulated_submission_time=3131.07, global_step=15227, preemption_count=0, score=3131.07, test/accuracy=0.984692, test/loss=0.0519449, test/mean_average_precision=0.166638, test/num_examples=43793, total_duration=4310.05, train/accuracy=0.988722, train/loss=0.0387514, train/mean_average_precision=0.205438, validation/accuracy=0.985669, validation/loss=0.0489888, validation/mean_average_precision=0.175427, validation/num_examples=43793
I0305 20:25:31.463038 139846176491264 logging_writer.py:48] [15300] global_step=15300, grad_norm=0.06179051473736763, loss=0.04150964692234993
I0305 20:25:51.739270 139846059546368 logging_writer.py:48] [15400] global_step=15400, grad_norm=0.09203889966011047, loss=0.04198194667696953
I0305 20:26:12.304515 139846176491264 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.09691459685564041, loss=0.03627626970410347
I0305 20:26:32.781733 139846059546368 logging_writer.py:48] [15600] global_step=15600, grad_norm=0.05000918731093407, loss=0.04082302004098892
I0305 20:26:53.260748 139846176491264 logging_writer.py:48] [15700] global_step=15700, grad_norm=0.0810428336262703, loss=0.03743530437350273
I0305 20:27:13.470947 139846059546368 logging_writer.py:48] [15800] global_step=15800, grad_norm=0.07821053266525269, loss=0.04153354838490486
I0305 20:27:34.193428 139846176491264 logging_writer.py:48] [15900] global_step=15900, grad_norm=0.030931226909160614, loss=0.04194391146302223
I0305 20:27:54.762169 139846059546368 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.03309103101491928, loss=0.03892255946993828
I0305 20:28:15.397373 139846176491264 logging_writer.py:48] [16100] global_step=16100, grad_norm=0.08282765001058578, loss=0.04130129888653755
I0305 20:28:35.484363 139846059546368 logging_writer.py:48] [16200] global_step=16200, grad_norm=0.08777133375406265, loss=0.03872711583971977
I0305 20:28:55.687361 139846176491264 logging_writer.py:48] [16300] global_step=16300, grad_norm=0.039204470813274384, loss=0.04137645661830902
I0305 20:29:16.211796 139846059546368 logging_writer.py:48] [16400] global_step=16400, grad_norm=0.05478470399975777, loss=0.0436241440474987
I0305 20:29:16.429735 139988053398720 spec.py:321] Evaluating on the training split.
I0305 20:30:25.083374 139988053398720 spec.py:333] Evaluating on the validation split.
I0305 20:30:27.022186 139988053398720 spec.py:349] Evaluating on the test split.
I0305 20:30:28.938771 139988053398720 submission_runner.py:469] Time since start: 4622.76s, 	Step: 16402, 	{'train/accuracy': 0.9890241026878357, 'train/loss': 0.03790368139743805, 'train/mean_average_precision': 0.2182404400714565, 'validation/accuracy': 0.985772967338562, 'validation/loss': 0.04821858927607536, 'validation/mean_average_precision': 0.18833870804909034, 'validation/num_examples': 43793, 'test/accuracy': 0.9848487377166748, 'test/loss': 0.05097391828894615, 'test/mean_average_precision': 0.17914398533093895, 'test/num_examples': 43793, 'score': 3371.208928346634, 'total_duration': 4622.755227565765, 'accumulated_submission_time': 3371.208928346634, 'accumulated_eval_time': 1250.8468046188354, 'accumulated_logging_time': 0.2601125240325928}
I0305 20:30:28.949981 139846176491264 logging_writer.py:48] [16402] accumulated_eval_time=1250.85, accumulated_logging_time=0.260113, accumulated_submission_time=3371.21, global_step=16402, preemption_count=0, score=3371.21, test/accuracy=0.984849, test/loss=0.0509739, test/mean_average_precision=0.179144, test/num_examples=43793, total_duration=4622.76, train/accuracy=0.989024, train/loss=0.0379037, train/mean_average_precision=0.21824, validation/accuracy=0.985773, validation/loss=0.0482186, validation/mean_average_precision=0.188339, validation/num_examples=43793
I0305 20:30:49.679870 139846059546368 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.0382595956325531, loss=0.03966507688164711
I0305 20:31:10.555377 139846176491264 logging_writer.py:48] [16600] global_step=16600, grad_norm=0.08309868723154068, loss=0.03716743737459183
I0305 20:31:31.434538 139846059546368 logging_writer.py:48] [16700] global_step=16700, grad_norm=0.13913306593894958, loss=0.04182017594575882
I0305 20:31:52.401684 139846176491264 logging_writer.py:48] [16800] global_step=16800, grad_norm=0.13423557579517365, loss=0.037351515144109726
I0305 20:32:13.018890 139846059546368 logging_writer.py:48] [16900] global_step=16900, grad_norm=0.044479675590991974, loss=0.03938695415854454
I0305 20:32:33.776741 139846176491264 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.03343481197953224, loss=0.036103203892707825
I0305 20:32:54.696592 139846059546368 logging_writer.py:48] [17100] global_step=17100, grad_norm=0.17637866735458374, loss=0.03583722934126854
I0305 20:33:15.482888 139846176491264 logging_writer.py:48] [17200] global_step=17200, grad_norm=0.030041150748729706, loss=0.03966803476214409
I0305 20:33:35.498475 139846059546368 logging_writer.py:48] [17300] global_step=17300, grad_norm=0.04169132187962532, loss=0.04201271012425423
I0305 20:33:55.422965 139846176491264 logging_writer.py:48] [17400] global_step=17400, grad_norm=0.052845802158117294, loss=0.04054107144474983
I0305 20:34:15.590191 139846059546368 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.046134162694215775, loss=0.034798238426446915
I0305 20:34:28.975404 139988053398720 spec.py:321] Evaluating on the training split.
I0305 20:35:40.614434 139988053398720 spec.py:333] Evaluating on the validation split.
I0305 20:35:42.664618 139988053398720 spec.py:349] Evaluating on the test split.
I0305 20:35:44.583668 139988053398720 submission_runner.py:469] Time since start: 4938.40s, 	Step: 17566, 	{'train/accuracy': 0.9888108372688293, 'train/loss': 0.0385148748755455, 'train/mean_average_precision': 0.2188880467168281, 'validation/accuracy': 0.9858338236808777, 'validation/loss': 0.048987001180648804, 'validation/mean_average_precision': 0.18559830936285188, 'validation/num_examples': 43793, 'test/accuracy': 0.9848302006721497, 'test/loss': 0.052273400127887726, 'test/mean_average_precision': 0.17566398495269384, 'test/num_examples': 43793, 'score': 3611.1944913864136, 'total_duration': 4938.400219202042, 'accumulated_submission_time': 3611.1944913864136, 'accumulated_eval_time': 1326.4550292491913, 'accumulated_logging_time': 0.2806370258331299}
I0305 20:35:44.595071 139846176491264 logging_writer.py:48] [17566] accumulated_eval_time=1326.46, accumulated_logging_time=0.280637, accumulated_submission_time=3611.19, global_step=17566, preemption_count=0, score=3611.19, test/accuracy=0.98483, test/loss=0.0522734, test/mean_average_precision=0.175664, test/num_examples=43793, total_duration=4938.4, train/accuracy=0.988811, train/loss=0.0385149, train/mean_average_precision=0.218888, validation/accuracy=0.985834, validation/loss=0.048987, validation/mean_average_precision=0.185598, validation/num_examples=43793
I0305 20:35:51.772332 139846059546368 logging_writer.py:48] [17600] global_step=17600, grad_norm=0.0768219456076622, loss=0.0410357266664505
I0305 20:36:12.136444 139846176491264 logging_writer.py:48] [17700] global_step=17700, grad_norm=0.05337870121002197, loss=0.037132371217012405
I0305 20:36:32.509969 139846059546368 logging_writer.py:48] [17800] global_step=17800, grad_norm=0.03642047196626663, loss=0.03761187940835953
I0305 20:36:52.724722 139846176491264 logging_writer.py:48] [17900] global_step=17900, grad_norm=0.11088345944881439, loss=0.03255071863532066
I0305 20:37:13.136513 139846059546368 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.07571874558925629, loss=0.03915867581963539
I0305 20:37:33.663222 139846176491264 logging_writer.py:48] [18100] global_step=18100, grad_norm=0.05422898009419441, loss=0.04043014720082283
I0305 20:37:54.471115 139846059546368 logging_writer.py:48] [18200] global_step=18200, grad_norm=0.07510649412870407, loss=0.040225788950920105
I0305 20:38:15.073603 139846176491264 logging_writer.py:48] [18300] global_step=18300, grad_norm=0.13415147364139557, loss=0.0415661446750164
I0305 20:38:35.285134 139846059546368 logging_writer.py:48] [18400] global_step=18400, grad_norm=0.0346393920481205, loss=0.03886020556092262
I0305 20:38:55.617286 139846176491264 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.039835743606090546, loss=0.03356228768825531
I0305 20:39:16.194855 139846059546368 logging_writer.py:48] [18600] global_step=18600, grad_norm=0.1702663004398346, loss=0.044140320271253586
I0305 20:39:36.812237 139846176491264 logging_writer.py:48] [18700] global_step=18700, grad_norm=0.19964298605918884, loss=0.04037754237651825
I0305 20:39:44.629162 139988053398720 spec.py:321] Evaluating on the training split.
I0305 20:40:55.022595 139988053398720 spec.py:333] Evaluating on the validation split.
I0305 20:40:56.968871 139988053398720 spec.py:349] Evaluating on the test split.
I0305 20:40:58.855642 139988053398720 submission_runner.py:469] Time since start: 5252.67s, 	Step: 18738, 	{'train/accuracy': 0.9889689087867737, 'train/loss': 0.038197655230760574, 'train/mean_average_precision': 0.21426485858750888, 'validation/accuracy': 0.9857096076011658, 'validation/loss': 0.04854295775294304, 'validation/mean_average_precision': 0.17903267046195645, 'validation/num_examples': 43793, 'test/accuracy': 0.9848066568374634, 'test/loss': 0.0511590912938118, 'test/mean_average_precision': 0.18047951708964088, 'test/num_examples': 43793, 'score': 3851.188839673996, 'total_duration': 5252.672187328339, 'accumulated_submission_time': 3851.188839673996, 'accumulated_eval_time': 1400.681452035904, 'accumulated_logging_time': 0.3010241985321045}
I0305 20:40:58.866146 139846059546368 logging_writer.py:48] [18738] accumulated_eval_time=1400.68, accumulated_logging_time=0.301024, accumulated_submission_time=3851.19, global_step=18738, preemption_count=0, score=3851.19, test/accuracy=0.984807, test/loss=0.0511591, test/mean_average_precision=0.18048, test/num_examples=43793, total_duration=5252.67, train/accuracy=0.988969, train/loss=0.0381977, train/mean_average_precision=0.214265, validation/accuracy=0.98571, validation/loss=0.048543, validation/mean_average_precision=0.179033, validation/num_examples=43793
I0305 20:41:11.990449 139846176491264 logging_writer.py:48] [18800] global_step=18800, grad_norm=0.13365964591503143, loss=0.041278257966041565
I0305 20:41:32.657522 139846059546368 logging_writer.py:48] [18900] global_step=18900, grad_norm=0.08678916841745377, loss=0.04151822626590729
I0305 20:41:53.225643 139846176491264 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.17570188641548157, loss=0.039899829775094986
I0305 20:42:14.145026 139846059546368 logging_writer.py:48] [19100] global_step=19100, grad_norm=0.037716809660196304, loss=0.03500978648662567
I0305 20:42:35.054429 139846176491264 logging_writer.py:48] [19200] global_step=19200, grad_norm=0.0382043793797493, loss=0.03957539051771164
I0305 20:42:55.983594 139846059546368 logging_writer.py:48] [19300] global_step=19300, grad_norm=0.08350656181573868, loss=0.0388830192387104
I0305 20:43:16.857044 139846176491264 logging_writer.py:48] [19400] global_step=19400, grad_norm=0.04332882910966873, loss=0.0343429371714592
I0305 20:43:37.117082 139846059546368 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.08557658642530441, loss=0.04025612026453018
I0305 20:43:57.435890 139846176491264 logging_writer.py:48] [19600] global_step=19600, grad_norm=0.04098791629076004, loss=0.03464840352535248
I0305 20:44:17.706845 139846059546368 logging_writer.py:48] [19700] global_step=19700, grad_norm=0.10553554445505142, loss=0.03509959951043129
I0305 20:44:38.287966 139846176491264 logging_writer.py:48] [19800] global_step=19800, grad_norm=0.061163466423749924, loss=0.03937169164419174
I0305 20:44:58.704006 139846059546368 logging_writer.py:48] [19900] global_step=19900, grad_norm=0.13343040645122528, loss=0.03928619623184204
I0305 20:44:58.910998 139988053398720 spec.py:321] Evaluating on the training split.
I0305 20:46:06.515506 139988053398720 spec.py:333] Evaluating on the validation split.
I0305 20:46:08.522491 139988053398720 spec.py:349] Evaluating on the test split.
I0305 20:46:10.434412 139988053398720 submission_runner.py:469] Time since start: 5564.25s, 	Step: 19902, 	{'train/accuracy': 0.9888707995414734, 'train/loss': 0.038242366164922714, 'train/mean_average_precision': 0.21886045379799754, 'validation/accuracy': 0.9857518672943115, 'validation/loss': 0.04844534397125244, 'validation/mean_average_precision': 0.19113705513416904, 'validation/num_examples': 43793, 'test/accuracy': 0.9848415851593018, 'test/loss': 0.051301259547472, 'test/mean_average_precision': 0.18076723356106775, 'test/num_examples': 43793, 'score': 4091.1910738945007, 'total_duration': 5564.250961780548, 'accumulated_submission_time': 4091.1910738945007, 'accumulated_eval_time': 1472.2048263549805, 'accumulated_logging_time': 0.320357084274292}
I0305 20:46:10.445455 139846176491264 logging_writer.py:48] [19902] accumulated_eval_time=1472.2, accumulated_logging_time=0.320357, accumulated_submission_time=4091.19, global_step=19902, preemption_count=0, score=4091.19, test/accuracy=0.984842, test/loss=0.0513013, test/mean_average_precision=0.180767, test/num_examples=43793, total_duration=5564.25, train/accuracy=0.988871, train/loss=0.0382424, train/mean_average_precision=0.21886, validation/accuracy=0.985752, validation/loss=0.0484453, validation/mean_average_precision=0.191137, validation/num_examples=43793
I0305 20:46:31.313547 139846059546368 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.12714441120624542, loss=0.04211295023560524
I0305 20:46:51.716260 139846176491264 logging_writer.py:48] [20100] global_step=20100, grad_norm=0.04878698289394379, loss=0.0364299938082695
I0305 20:47:12.250216 139846059546368 logging_writer.py:48] [20200] global_step=20200, grad_norm=0.089060939848423, loss=0.03781021386384964
I0305 20:47:32.698931 139846176491264 logging_writer.py:48] [20300] global_step=20300, grad_norm=0.05520091950893402, loss=0.03866397216916084
I0305 20:47:53.630154 139846059546368 logging_writer.py:48] [20400] global_step=20400, grad_norm=0.04477743059396744, loss=0.03628307580947876
I0305 20:48:14.470182 139846176491264 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.11265842616558075, loss=0.03599054366350174
I0305 20:48:35.294535 139846059546368 logging_writer.py:48] [20600] global_step=20600, grad_norm=0.11090109497308731, loss=0.04215293005108833
I0305 20:48:55.974344 139846176491264 logging_writer.py:48] [20700] global_step=20700, grad_norm=0.03394193947315216, loss=0.041120003908872604
I0305 20:49:17.170009 139846059546368 logging_writer.py:48] [20800] global_step=20800, grad_norm=0.045178018510341644, loss=0.03544289618730545
I0305 20:49:37.913752 139846176491264 logging_writer.py:48] [20900] global_step=20900, grad_norm=0.13545434176921844, loss=0.03892897441983223
I0305 20:49:58.809751 139846059546368 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.0690566748380661, loss=0.039654165506362915
I0305 20:50:10.597989 139988053398720 spec.py:321] Evaluating on the training split.
I0305 20:51:23.962998 139988053398720 spec.py:333] Evaluating on the validation split.
I0305 20:51:26.040131 139988053398720 spec.py:349] Evaluating on the test split.
I0305 20:51:28.050407 139988053398720 submission_runner.py:469] Time since start: 5881.87s, 	Step: 21057, 	{'train/accuracy': 0.988986611366272, 'train/loss': 0.03821277618408203, 'train/mean_average_precision': 0.21373003196647805, 'validation/accuracy': 0.9855651259422302, 'validation/loss': 0.048738084733486176, 'validation/mean_average_precision': 0.18276537175967933, 'validation/num_examples': 43793, 'test/accuracy': 0.984652042388916, 'test/loss': 0.05143832042813301, 'test/mean_average_precision': 0.17176403313895092, 'test/num_examples': 43793, 'score': 4331.299253463745, 'total_duration': 5881.8668212890625, 'accumulated_submission_time': 4331.299253463745, 'accumulated_eval_time': 1549.6570563316345, 'accumulated_logging_time': 0.34147047996520996}
I0305 20:51:28.061855 139846176491264 logging_writer.py:48] [21057] accumulated_eval_time=1549.66, accumulated_logging_time=0.34147, accumulated_submission_time=4331.3, global_step=21057, preemption_count=0, score=4331.3, test/accuracy=0.984652, test/loss=0.0514383, test/mean_average_precision=0.171764, test/num_examples=43793, total_duration=5881.87, train/accuracy=0.988987, train/loss=0.0382128, train/mean_average_precision=0.21373, validation/accuracy=0.985565, validation/loss=0.0487381, validation/mean_average_precision=0.182765, validation/num_examples=43793
I0305 20:51:37.041794 139846059546368 logging_writer.py:48] [21100] global_step=21100, grad_norm=0.07460544258356094, loss=0.04223691672086716
I0305 20:51:57.739932 139846176491264 logging_writer.py:48] [21200] global_step=21200, grad_norm=0.06382410228252411, loss=0.03541107848286629
I0305 20:52:18.482669 139846059546368 logging_writer.py:48] [21300] global_step=21300, grad_norm=0.20860446989536285, loss=0.03470081090927124
I0305 20:52:39.380184 139846176491264 logging_writer.py:48] [21400] global_step=21400, grad_norm=0.07204411178827286, loss=0.039596837013959885
I0305 20:53:00.068031 139846059546368 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.03883980214595795, loss=0.037755515426397324
I0305 20:53:21.076570 139846176491264 logging_writer.py:48] [21600] global_step=21600, grad_norm=0.05420256778597832, loss=0.03951956331729889
I0305 20:53:41.806095 139846059546368 logging_writer.py:48] [21700] global_step=21700, grad_norm=0.1582321673631668, loss=0.033913228660821915
I0305 20:54:02.588626 139846176491264 logging_writer.py:48] [21800] global_step=21800, grad_norm=0.04650858789682388, loss=0.03497539088129997
I0305 20:54:23.300626 139846059546368 logging_writer.py:48] [21900] global_step=21900, grad_norm=0.0434730090200901, loss=0.04036355018615723
I0305 20:54:43.957889 139846176491264 logging_writer.py:48] [22000] global_step=22000, grad_norm=0.06013893708586693, loss=0.033530596643686295
I0305 20:55:04.129051 139846059546368 logging_writer.py:48] [22100] global_step=22100, grad_norm=0.09012613445520401, loss=0.03433902934193611
I0305 20:55:24.580466 139846176491264 logging_writer.py:48] [22200] global_step=22200, grad_norm=0.17217455804347992, loss=0.035137057304382324
I0305 20:55:28.240744 139988053398720 spec.py:321] Evaluating on the training split.
I0305 20:56:40.632056 139988053398720 spec.py:333] Evaluating on the validation split.
I0305 20:56:42.690523 139988053398720 spec.py:349] Evaluating on the test split.
I0305 20:56:44.715641 139988053398720 submission_runner.py:469] Time since start: 6198.53s, 	Step: 22219, 	{'train/accuracy': 0.9891259074211121, 'train/loss': 0.0376235656440258, 'train/mean_average_precision': 0.22785225583877694, 'validation/accuracy': 0.9857977032661438, 'validation/loss': 0.04801959916949272, 'validation/mean_average_precision': 0.1813642525270148, 'validation/num_examples': 43793, 'test/accuracy': 0.9848260283470154, 'test/loss': 0.050993192940950394, 'test/mean_average_precision': 0.18087460850697226, 'test/num_examples': 43793, 'score': 4571.439974784851, 'total_duration': 6198.532079219818, 'accumulated_submission_time': 4571.439974784851, 'accumulated_eval_time': 1626.1317908763885, 'accumulated_logging_time': 0.3621251583099365}
I0305 20:56:44.727928 139846059546368 logging_writer.py:48] [22219] accumulated_eval_time=1626.13, accumulated_logging_time=0.362125, accumulated_submission_time=4571.44, global_step=22219, preemption_count=0, score=4571.44, test/accuracy=0.984826, test/loss=0.0509932, test/mean_average_precision=0.180875, test/num_examples=43793, total_duration=6198.53, train/accuracy=0.989126, train/loss=0.0376236, train/mean_average_precision=0.227852, validation/accuracy=0.985798, validation/loss=0.0480196, validation/mean_average_precision=0.181364, validation/num_examples=43793
I0305 20:57:01.577517 139846176491264 logging_writer.py:48] [22300] global_step=22300, grad_norm=0.05887138471007347, loss=0.03510308265686035
I0305 20:57:22.462676 139846059546368 logging_writer.py:48] [22400] global_step=22400, grad_norm=0.06294872611761093, loss=0.03988494351506233
I0305 20:57:43.076446 139846176491264 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.0408736951649189, loss=0.038810987025499344
I0305 20:58:03.795868 139846059546368 logging_writer.py:48] [22600] global_step=22600, grad_norm=0.06068940833210945, loss=0.03516513481736183
I0305 20:58:24.673045 139846176491264 logging_writer.py:48] [22700] global_step=22700, grad_norm=0.1010804995894432, loss=0.037198781967163086
I0305 20:58:45.539223 139846059546368 logging_writer.py:48] [22800] global_step=22800, grad_norm=0.19518981873989105, loss=0.03719596937298775
I0305 20:59:06.139407 139846176491264 logging_writer.py:48] [22900] global_step=22900, grad_norm=0.1010843962430954, loss=0.041551701724529266
I0305 20:59:26.798402 139846059546368 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.06700624525547028, loss=0.03503314033150673
I0305 20:59:47.507279 139846176491264 logging_writer.py:48] [23100] global_step=23100, grad_norm=0.0779755562543869, loss=0.03937673941254616
I0305 21:00:08.059465 139846059546368 logging_writer.py:48] [23200] global_step=23200, grad_norm=0.07780937850475311, loss=0.03588390722870827
I0305 21:00:28.822938 139846176491264 logging_writer.py:48] [23300] global_step=23300, grad_norm=0.05047890543937683, loss=0.03715895116329193
I0305 21:00:44.770370 139988053398720 spec.py:321] Evaluating on the training split.
I0305 21:01:55.412270 139988053398720 spec.py:333] Evaluating on the validation split.
I0305 21:01:57.371571 139988053398720 spec.py:349] Evaluating on the test split.
I0305 21:01:59.229777 139988053398720 submission_runner.py:469] Time since start: 6513.05s, 	Step: 23378, 	{'train/accuracy': 0.9890457987785339, 'train/loss': 0.03764975070953369, 'train/mean_average_precision': 0.22808872299399327, 'validation/accuracy': 0.9858143329620361, 'validation/loss': 0.0485360249876976, 'validation/mean_average_precision': 0.1887411589745791, 'validation/num_examples': 43793, 'test/accuracy': 0.9848390817642212, 'test/loss': 0.051560405641794205, 'test/mean_average_precision': 0.17769738352498465, 'test/num_examples': 43793, 'score': 4811.442640304565, 'total_duration': 6513.0462827682495, 'accumulated_submission_time': 4811.442640304565, 'accumulated_eval_time': 1700.5911021232605, 'accumulated_logging_time': 0.3840799331665039}
I0305 21:01:59.240758 139846059546368 logging_writer.py:48] [23378] accumulated_eval_time=1700.59, accumulated_logging_time=0.38408, accumulated_submission_time=4811.44, global_step=23378, preemption_count=0, score=4811.44, test/accuracy=0.984839, test/loss=0.0515604, test/mean_average_precision=0.177697, test/num_examples=43793, total_duration=6513.05, train/accuracy=0.989046, train/loss=0.0376498, train/mean_average_precision=0.228089, validation/accuracy=0.985814, validation/loss=0.048536, validation/mean_average_precision=0.188741, validation/num_examples=43793
I0305 21:02:04.035944 139846176491264 logging_writer.py:48] [23400] global_step=23400, grad_norm=0.08039121329784393, loss=0.04098526015877724
I0305 21:02:24.721142 139846059546368 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.039995547384023666, loss=0.035628851503133774
I0305 21:02:45.566528 139846176491264 logging_writer.py:48] [23600] global_step=23600, grad_norm=0.04635186865925789, loss=0.040116727352142334
I0305 21:03:06.207145 139846059546368 logging_writer.py:48] [23700] global_step=23700, grad_norm=0.050851933658123016, loss=0.03876067325472832
I0305 21:03:27.055874 139846176491264 logging_writer.py:48] [23800] global_step=23800, grad_norm=0.08651911467313766, loss=0.03201819956302643
I0305 21:03:47.918241 139846059546368 logging_writer.py:48] [23900] global_step=23900, grad_norm=0.05785159394145012, loss=0.038875825703144073
I0305 21:04:08.531052 139846176491264 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.06496789306402206, loss=0.04286215826869011
I0305 21:04:29.254615 139846059546368 logging_writer.py:48] [24100] global_step=24100, grad_norm=0.1391647309064865, loss=0.04031888023018837
I0305 21:04:49.953875 139846176491264 logging_writer.py:48] [24200] global_step=24200, grad_norm=0.08233138173818588, loss=0.0389043353497982
I0305 21:05:10.512429 139846059546368 logging_writer.py:48] [24300] global_step=24300, grad_norm=0.10308724641799927, loss=0.03876698762178421
I0305 21:05:31.097234 139846176491264 logging_writer.py:48] [24400] global_step=24400, grad_norm=0.05652260780334473, loss=0.03760481998324394
I0305 21:05:51.629872 139846059546368 logging_writer.py:48] [24500] global_step=24500, grad_norm=0.09129317104816437, loss=0.03902299329638481
I0305 21:05:59.238848 139988053398720 spec.py:321] Evaluating on the training split.
I0305 21:07:08.432389 139988053398720 spec.py:333] Evaluating on the validation split.
I0305 21:07:10.437968 139988053398720 spec.py:349] Evaluating on the test split.
I0305 21:07:12.354950 139988053398720 submission_runner.py:469] Time since start: 6826.17s, 	Step: 24538, 	{'train/accuracy': 0.9890886545181274, 'train/loss': 0.03725675120949745, 'train/mean_average_precision': 0.22796853059917538, 'validation/accuracy': 0.98587566614151, 'validation/loss': 0.047811109572649, 'validation/mean_average_precision': 0.18768612770648666, 'validation/num_examples': 43793, 'test/accuracy': 0.9849616289138794, 'test/loss': 0.05080631002783775, 'test/mean_average_precision': 0.18156549575173453, 'test/num_examples': 43793, 'score': 5051.400331020355, 'total_duration': 6826.1713988780975, 'accumulated_submission_time': 5051.400331020355, 'accumulated_eval_time': 1773.7070503234863, 'accumulated_logging_time': 0.40456271171569824}
I0305 21:07:12.366737 139846176491264 logging_writer.py:48] [24538] accumulated_eval_time=1773.71, accumulated_logging_time=0.404563, accumulated_submission_time=5051.4, global_step=24538, preemption_count=0, score=5051.4, test/accuracy=0.984962, test/loss=0.0508063, test/mean_average_precision=0.181565, test/num_examples=43793, total_duration=6826.17, train/accuracy=0.989089, train/loss=0.0372568, train/mean_average_precision=0.227969, validation/accuracy=0.985876, validation/loss=0.0478111, validation/mean_average_precision=0.187686, validation/num_examples=43793
I0305 21:07:25.457154 139846059546368 logging_writer.py:48] [24600] global_step=24600, grad_norm=0.0496332123875618, loss=0.0392182394862175
I0305 21:07:46.147822 139846176491264 logging_writer.py:48] [24700] global_step=24700, grad_norm=0.04429124668240547, loss=0.04100384563207626
I0305 21:08:07.072357 139846059546368 logging_writer.py:48] [24800] global_step=24800, grad_norm=0.08477957546710968, loss=0.0345192514359951
I0305 21:08:27.948981 139846176491264 logging_writer.py:48] [24900] global_step=24900, grad_norm=0.047118473798036575, loss=0.03388175740838051
I0305 21:08:48.794957 139846059546368 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.047775059938430786, loss=0.03966010361909866
I0305 21:09:09.815525 139846176491264 logging_writer.py:48] [25100] global_step=25100, grad_norm=0.04887343943119049, loss=0.04043208807706833
I0305 21:09:30.152260 139846059546368 logging_writer.py:48] [25200] global_step=25200, grad_norm=0.07009101659059525, loss=0.039745498448610306
I0305 21:09:50.447653 139846176491264 logging_writer.py:48] [25300] global_step=25300, grad_norm=0.049550868570804596, loss=0.03338473290205002
I0305 21:10:10.713926 139846059546368 logging_writer.py:48] [25400] global_step=25400, grad_norm=0.0599798783659935, loss=0.03783054277300835
I0305 21:10:31.041965 139846176491264 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.07278064638376236, loss=0.033377498388290405
I0305 21:10:51.841034 139846059546368 logging_writer.py:48] [25600] global_step=25600, grad_norm=0.04534781351685524, loss=0.03741036355495453
I0305 21:11:12.567195 139988053398720 spec.py:321] Evaluating on the training split.
I0305 21:12:25.358054 139988053398720 spec.py:333] Evaluating on the validation split.
I0305 21:12:27.310207 139988053398720 spec.py:349] Evaluating on the test split.
I0305 21:12:29.244926 139988053398720 submission_runner.py:469] Time since start: 7143.06s, 	Step: 25700, 	{'train/accuracy': 0.989041268825531, 'train/loss': 0.037431370466947556, 'train/mean_average_precision': 0.2412315383747457, 'validation/accuracy': 0.9858322143554688, 'validation/loss': 0.047944873571395874, 'validation/mean_average_precision': 0.18509168020709726, 'validation/num_examples': 43793, 'test/accuracy': 0.9849346876144409, 'test/loss': 0.05067699775099754, 'test/mean_average_precision': 0.18198308280430534, 'test/num_examples': 43793, 'score': 5291.562994718552, 'total_duration': 7143.0613622665405, 'accumulated_submission_time': 5291.562994718552, 'accumulated_eval_time': 1850.3846244812012, 'accumulated_logging_time': 0.4251689910888672}
I0305 21:12:29.256761 139846176491264 logging_writer.py:48] [25700] accumulated_eval_time=1850.38, accumulated_logging_time=0.425169, accumulated_submission_time=5291.56, global_step=25700, preemption_count=0, score=5291.56, test/accuracy=0.984935, test/loss=0.050677, test/mean_average_precision=0.181983, test/num_examples=43793, total_duration=7143.06, train/accuracy=0.989041, train/loss=0.0374314, train/mean_average_precision=0.241232, validation/accuracy=0.985832, validation/loss=0.0479449, validation/mean_average_precision=0.185092, validation/num_examples=43793
I0305 21:12:29.483566 139846059546368 logging_writer.py:48] [25700] global_step=25700, grad_norm=0.12175055593252182, loss=0.03349701315164566
I0305 21:12:50.345069 139846176491264 logging_writer.py:48] [25800] global_step=25800, grad_norm=0.11741312593221664, loss=0.039516519755125046
I0305 21:13:11.261530 139846059546368 logging_writer.py:48] [25900] global_step=25900, grad_norm=0.11266636103391647, loss=0.033772632479667664
I0305 21:13:32.323977 139846176491264 logging_writer.py:48] [26000] global_step=26000, grad_norm=0.046988241374492645, loss=0.038739610463380814
I0305 21:13:53.362399 139846059546368 logging_writer.py:48] [26100] global_step=26100, grad_norm=0.10838111490011215, loss=0.03625740483403206
I0305 21:14:14.127954 139846176491264 logging_writer.py:48] [26200] global_step=26200, grad_norm=0.03753647953271866, loss=0.035532768815755844
I0305 21:14:34.787038 139846059546368 logging_writer.py:48] [26300] global_step=26300, grad_norm=0.17756253480911255, loss=0.03679393604397774
I0305 21:14:55.682312 139846176491264 logging_writer.py:48] [26400] global_step=26400, grad_norm=0.07578033208847046, loss=0.03984835371375084
I0305 21:15:16.489017 139846059546368 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.04959055408835411, loss=0.03841704502701759
I0305 21:15:37.062384 139846176491264 logging_writer.py:48] [26600] global_step=26600, grad_norm=0.05247429758310318, loss=0.04104601591825485
I0305 21:15:57.964541 139846059546368 logging_writer.py:48] [26700] global_step=26700, grad_norm=0.05717271566390991, loss=0.0351552739739418
I0305 21:16:18.860896 139846176491264 logging_writer.py:48] [26800] global_step=26800, grad_norm=0.10814238339662552, loss=0.03757171705365181
I0305 21:16:29.356369 139988053398720 spec.py:321] Evaluating on the training split.
I0305 21:17:39.022503 139988053398720 spec.py:333] Evaluating on the validation split.
I0305 21:17:40.957858 139988053398720 spec.py:349] Evaluating on the test split.
I0305 21:17:42.896714 139988053398720 submission_runner.py:469] Time since start: 7456.71s, 	Step: 26852, 	{'train/accuracy': 0.989129364490509, 'train/loss': 0.03711039572954178, 'train/mean_average_precision': 0.23456275490941603, 'validation/accuracy': 0.9859040975570679, 'validation/loss': 0.048020441085100174, 'validation/mean_average_precision': 0.19372449818536092, 'validation/num_examples': 43793, 'test/accuracy': 0.9850168228149414, 'test/loss': 0.05087914690375328, 'test/mean_average_precision': 0.18737258356307257, 'test/num_examples': 43793, 'score': 5531.623912334442, 'total_duration': 7456.713232040405, 'accumulated_submission_time': 5531.623912334442, 'accumulated_eval_time': 1923.924885749817, 'accumulated_logging_time': 0.44628334045410156}
I0305 21:17:42.909213 139846059546368 logging_writer.py:48] [26852] accumulated_eval_time=1923.92, accumulated_logging_time=0.446283, accumulated_submission_time=5531.62, global_step=26852, preemption_count=0, score=5531.62, test/accuracy=0.985017, test/loss=0.0508791, test/mean_average_precision=0.187373, test/num_examples=43793, total_duration=7456.71, train/accuracy=0.989129, train/loss=0.0371104, train/mean_average_precision=0.234563, validation/accuracy=0.985904, validation/loss=0.0480204, validation/mean_average_precision=0.193724, validation/num_examples=43793
I0305 21:17:52.975349 139846176491264 logging_writer.py:48] [26900] global_step=26900, grad_norm=0.062113743275403976, loss=0.034984104335308075
I0305 21:18:13.809564 139846059546368 logging_writer.py:48] [27000] global_step=27000, grad_norm=0.20337755978107452, loss=0.040377601981163025
I0305 21:18:34.720873 139846176491264 logging_writer.py:48] [27100] global_step=27100, grad_norm=0.048223935067653656, loss=0.04008076339960098
I0305 21:18:55.410701 139846059546368 logging_writer.py:48] [27200] global_step=27200, grad_norm=0.1926688551902771, loss=0.03760293126106262
I0305 21:19:16.755739 139846176491264 logging_writer.py:48] [27300] global_step=27300, grad_norm=0.09716204553842545, loss=0.03979312628507614
I0305 21:19:37.893490 139846059546368 logging_writer.py:48] [27400] global_step=27400, grad_norm=0.05222870409488678, loss=0.03486480563879013
I0305 21:19:59.016279 139846176491264 logging_writer.py:48] [27500] global_step=27500, grad_norm=0.10266567766666412, loss=0.040322788059711456
I0305 21:20:19.789674 139846059546368 logging_writer.py:48] [27600] global_step=27600, grad_norm=0.13084591925144196, loss=0.0355977788567543
I0305 21:20:40.528411 139846176491264 logging_writer.py:48] [27700] global_step=27700, grad_norm=0.07682988047599792, loss=0.03579094633460045
I0305 21:21:01.128669 139846059546368 logging_writer.py:48] [27800] global_step=27800, grad_norm=0.12788869440555573, loss=0.034975238144397736
I0305 21:21:22.018339 139846176491264 logging_writer.py:48] [27900] global_step=27900, grad_norm=0.06628364324569702, loss=0.03934694826602936
I0305 21:21:43.066903 139988053398720 spec.py:321] Evaluating on the training split.
I0305 21:22:57.549412 139988053398720 spec.py:333] Evaluating on the validation split.
I0305 21:22:59.500861 139988053398720 spec.py:349] Evaluating on the test split.
I0305 21:23:01.422205 139988053398720 submission_runner.py:469] Time since start: 7775.24s, 	Step: 28000, 	{'train/accuracy': 0.9892706274986267, 'train/loss': 0.0367145873606205, 'train/mean_average_precision': 0.24511939084761594, 'validation/accuracy': 0.985965371131897, 'validation/loss': 0.048049941658973694, 'validation/mean_average_precision': 0.1957977600210574, 'validation/num_examples': 43793, 'test/accuracy': 0.9849944710731506, 'test/loss': 0.051086243242025375, 'test/mean_average_precision': 0.19025219529858906, 'test/num_examples': 43793, 'score': 5771.741902112961, 'total_duration': 7775.2387499809265, 'accumulated_submission_time': 5771.741902112961, 'accumulated_eval_time': 2002.2801306247711, 'accumulated_logging_time': 0.4685328006744385}
I0305 21:23:01.435273 139846059546368 logging_writer.py:48] [28000] accumulated_eval_time=2002.28, accumulated_logging_time=0.468533, accumulated_submission_time=5771.74, global_step=28000, preemption_count=0, score=5771.74, test/accuracy=0.984994, test/loss=0.0510862, test/mean_average_precision=0.190252, test/num_examples=43793, total_duration=7775.24, train/accuracy=0.989271, train/loss=0.0367146, train/mean_average_precision=0.245119, validation/accuracy=0.985965, validation/loss=0.0480499, validation/mean_average_precision=0.195798, validation/num_examples=43793
I0305 21:23:01.867197 139846176491264 logging_writer.py:48] [28000] global_step=28000, grad_norm=0.051699697971343994, loss=0.035106904804706573
I0305 21:23:22.954631 139846059546368 logging_writer.py:48] [28100] global_step=28100, grad_norm=0.06617392599582672, loss=0.03644420579075813
I0305 21:23:43.813968 139846176491264 logging_writer.py:48] [28200] global_step=28200, grad_norm=0.047833267599344254, loss=0.039036691188812256
I0305 21:24:04.924344 139846059546368 logging_writer.py:48] [28300] global_step=28300, grad_norm=0.14254410564899445, loss=0.03621036186814308
I0305 21:24:26.231647 139846176491264 logging_writer.py:48] [28400] global_step=28400, grad_norm=0.17105378210544586, loss=0.03548010438680649
I0305 21:24:46.873671 139846059546368 logging_writer.py:48] [28500] global_step=28500, grad_norm=0.1294277310371399, loss=0.03668665885925293
I0305 21:25:07.418563 139846176491264 logging_writer.py:48] [28600] global_step=28600, grad_norm=0.08425002545118332, loss=0.03851079195737839
I0305 21:25:28.216630 139846059546368 logging_writer.py:48] [28700] global_step=28700, grad_norm=0.06794127821922302, loss=0.03287680819630623
I0305 21:25:49.361176 139846176491264 logging_writer.py:48] [28800] global_step=28800, grad_norm=0.07785431295633316, loss=0.03793593868613243
I0305 21:26:10.187637 139846059546368 logging_writer.py:48] [28900] global_step=28900, grad_norm=0.04413127526640892, loss=0.035999879240989685
I0305 21:26:31.256126 139846176491264 logging_writer.py:48] [29000] global_step=29000, grad_norm=0.11454438418149948, loss=0.040756870061159134
I0305 21:26:51.946423 139846059546368 logging_writer.py:48] [29100] global_step=29100, grad_norm=0.0559222437441349, loss=0.03407252952456474
I0305 21:27:01.471399 139988053398720 spec.py:321] Evaluating on the training split.
I0305 21:28:08.552128 139988053398720 spec.py:333] Evaluating on the validation split.
I0305 21:28:10.479435 139988053398720 spec.py:349] Evaluating on the test split.
I0305 21:28:12.349037 139988053398720 submission_runner.py:469] Time since start: 8086.17s, 	Step: 29148, 	{'train/accuracy': 0.9893772006034851, 'train/loss': 0.036354247480630875, 'train/mean_average_precision': 0.24206317312501985, 'validation/accuracy': 0.9857871532440186, 'validation/loss': 0.047526951879262924, 'validation/mean_average_precision': 0.19392047282080271, 'validation/num_examples': 43793, 'test/accuracy': 0.98490309715271, 'test/loss': 0.050181154161691666, 'test/mean_average_precision': 0.19252115434265699, 'test/num_examples': 43793, 'score': 6011.524959087372, 'total_duration': 8086.165498018265, 'accumulated_submission_time': 6011.524959087372, 'accumulated_eval_time': 2073.157628774643, 'accumulated_logging_time': 0.7050776481628418}
I0305 21:28:12.361083 139846176491264 logging_writer.py:48] [29148] accumulated_eval_time=2073.16, accumulated_logging_time=0.705078, accumulated_submission_time=6011.52, global_step=29148, preemption_count=0, score=6011.52, test/accuracy=0.984903, test/loss=0.0501812, test/mean_average_precision=0.192521, test/num_examples=43793, total_duration=8086.17, train/accuracy=0.989377, train/loss=0.0363542, train/mean_average_precision=0.242063, validation/accuracy=0.985787, validation/loss=0.047527, validation/mean_average_precision=0.19392, validation/num_examples=43793
I0305 21:28:23.449885 139846059546368 logging_writer.py:48] [29200] global_step=29200, grad_norm=0.06124180555343628, loss=0.03245077654719353
I0305 21:28:43.941978 139846176491264 logging_writer.py:48] [29300] global_step=29300, grad_norm=0.07130133360624313, loss=0.03839711472392082
I0305 21:29:04.734679 139846059546368 logging_writer.py:48] [29400] global_step=29400, grad_norm=0.06832116842269897, loss=0.03341013193130493
I0305 21:29:25.619138 139846176491264 logging_writer.py:48] [29500] global_step=29500, grad_norm=0.09379422664642334, loss=0.036626655608415604
I0305 21:29:46.561276 139846059546368 logging_writer.py:48] [29600] global_step=29600, grad_norm=0.10529698431491852, loss=0.03915395960211754
I0305 21:30:07.645907 139846176491264 logging_writer.py:48] [29700] global_step=29700, grad_norm=0.0900561586022377, loss=0.036951642483472824
I0305 21:30:28.516615 139846059546368 logging_writer.py:48] [29800] global_step=29800, grad_norm=0.04454116150736809, loss=0.035893894731998444
I0305 21:30:49.250377 139846176491264 logging_writer.py:48] [29900] global_step=29900, grad_norm=0.07926971465349197, loss=0.03484678268432617
I0305 21:31:10.000302 139846059546368 logging_writer.py:48] [30000] global_step=30000, grad_norm=0.14244218170642853, loss=0.040553975850343704
I0305 21:31:30.533433 139846176491264 logging_writer.py:48] [30100] global_step=30100, grad_norm=0.09972179681062698, loss=0.03936268761754036
I0305 21:31:51.145375 139846059546368 logging_writer.py:48] [30200] global_step=30200, grad_norm=0.0799773558974266, loss=0.03803658485412598
I0305 21:32:11.967465 139846176491264 logging_writer.py:48] [30300] global_step=30300, grad_norm=0.10785114765167236, loss=0.0352361798286438
I0305 21:32:12.383527 139988053398720 spec.py:321] Evaluating on the training split.
I0305 21:33:25.630815 139988053398720 spec.py:333] Evaluating on the validation split.
I0305 21:33:27.544340 139988053398720 spec.py:349] Evaluating on the test split.
I0305 21:33:29.419490 139988053398720 submission_runner.py:469] Time since start: 8403.24s, 	Step: 30303, 	{'train/accuracy': 0.9892402291297913, 'train/loss': 0.03663342446088791, 'train/mean_average_precision': 0.25100917700047254, 'validation/accuracy': 0.9858322143554688, 'validation/loss': 0.047663964331150055, 'validation/mean_average_precision': 0.19620843662474877, 'validation/num_examples': 43793, 'test/accuracy': 0.9850509166717529, 'test/loss': 0.050372447818517685, 'test/mean_average_precision': 0.19209507133721085, 'test/num_examples': 43793, 'score': 6251.50754237175, 'total_duration': 8403.235912322998, 'accumulated_submission_time': 6251.50754237175, 'accumulated_eval_time': 2150.1934123039246, 'accumulated_logging_time': 0.7270264625549316}
I0305 21:33:29.432634 139846059546368 logging_writer.py:48] [30303] accumulated_eval_time=2150.19, accumulated_logging_time=0.727026, accumulated_submission_time=6251.51, global_step=30303, preemption_count=0, score=6251.51, test/accuracy=0.985051, test/loss=0.0503724, test/mean_average_precision=0.192095, test/num_examples=43793, total_duration=8403.24, train/accuracy=0.98924, train/loss=0.0366334, train/mean_average_precision=0.251009, validation/accuracy=0.985832, validation/loss=0.047664, validation/mean_average_precision=0.196208, validation/num_examples=43793
I0305 21:33:49.700210 139846176491264 logging_writer.py:48] [30400] global_step=30400, grad_norm=0.048214882612228394, loss=0.034408047795295715
I0305 21:34:10.522577 139846059546368 logging_writer.py:48] [30500] global_step=30500, grad_norm=0.06459177285432816, loss=0.03838871046900749
I0305 21:34:31.409611 139846176491264 logging_writer.py:48] [30600] global_step=30600, grad_norm=0.12926411628723145, loss=0.03708507865667343
I0305 21:34:52.121682 139846059546368 logging_writer.py:48] [30700] global_step=30700, grad_norm=0.06412453949451447, loss=0.03416247293353081
I0305 21:35:12.941499 139846176491264 logging_writer.py:48] [30800] global_step=30800, grad_norm=0.05605160817503929, loss=0.03729682043194771
I0305 21:35:33.540045 139846059546368 logging_writer.py:48] [30900] global_step=30900, grad_norm=0.051088664680719376, loss=0.034071363508701324
I0305 21:35:54.064056 139846176491264 logging_writer.py:48] [31000] global_step=31000, grad_norm=0.07716970890760422, loss=0.03611190617084503
I0305 21:36:14.671279 139846059546368 logging_writer.py:48] [31100] global_step=31100, grad_norm=0.16832061111927032, loss=0.03704061731696129
I0305 21:36:35.537927 139846176491264 logging_writer.py:48] [31200] global_step=31200, grad_norm=0.10783127695322037, loss=0.038558874279260635
I0305 21:36:56.169420 139846059546368 logging_writer.py:48] [31300] global_step=31300, grad_norm=0.06854645907878876, loss=0.03690270334482193
I0305 21:37:16.890970 139846176491264 logging_writer.py:48] [31400] global_step=31400, grad_norm=0.08106110244989395, loss=0.03763725236058235
I0305 21:37:29.522330 139988053398720 spec.py:321] Evaluating on the training split.
I0305 21:38:41.055077 139988053398720 spec.py:333] Evaluating on the validation split.
I0305 21:38:43.142943 139988053398720 spec.py:349] Evaluating on the test split.
I0305 21:38:45.173664 139988053398720 submission_runner.py:469] Time since start: 8718.99s, 	Step: 31462, 	{'train/accuracy': 0.9895560145378113, 'train/loss': 0.03567924723029137, 'train/mean_average_precision': 0.25629327388823, 'validation/accuracy': 0.986055314540863, 'validation/loss': 0.04710887372493744, 'validation/mean_average_precision': 0.1977694902645749, 'validation/num_examples': 43793, 'test/accuracy': 0.985149085521698, 'test/loss': 0.05007486790418625, 'test/mean_average_precision': 0.19130880399459851, 'test/num_examples': 43793, 'score': 6491.557395219803, 'total_duration': 8718.990085363388, 'accumulated_submission_time': 6491.557395219803, 'accumulated_eval_time': 2225.844567298889, 'accumulated_logging_time': 0.7502727508544922}
I0305 21:38:45.186374 139846059546368 logging_writer.py:48] [31462] accumulated_eval_time=2225.84, accumulated_logging_time=0.750273, accumulated_submission_time=6491.56, global_step=31462, preemption_count=0, score=6491.56, test/accuracy=0.985149, test/loss=0.0500749, test/mean_average_precision=0.191309, test/num_examples=43793, total_duration=8718.99, train/accuracy=0.989556, train/loss=0.0356792, train/mean_average_precision=0.256293, validation/accuracy=0.986055, validation/loss=0.0471089, validation/mean_average_precision=0.197769, validation/num_examples=43793
I0305 21:38:53.204898 139846176491264 logging_writer.py:48] [31500] global_step=31500, grad_norm=0.08330447971820831, loss=0.03774730861186981
I0305 21:39:13.617083 139846059546368 logging_writer.py:48] [31600] global_step=31600, grad_norm=0.06990531086921692, loss=0.038225360214710236
I0305 21:39:34.181390 139846176491264 logging_writer.py:48] [31700] global_step=31700, grad_norm=0.05453052371740341, loss=0.03572292625904083
I0305 21:39:54.883896 139846059546368 logging_writer.py:48] [31800] global_step=31800, grad_norm=0.11322630941867828, loss=0.03615407273173332
I0305 21:40:15.162984 139846176491264 logging_writer.py:48] [31900] global_step=31900, grad_norm=0.10366420447826385, loss=0.04086865857243538
I0305 21:40:36.037718 139846059546368 logging_writer.py:48] [32000] global_step=32000, grad_norm=0.06245894357562065, loss=0.03770742937922478
I0305 21:40:56.822218 139846176491264 logging_writer.py:48] [32100] global_step=32100, grad_norm=0.11794276535511017, loss=0.0354854054749012
I0305 21:41:17.473410 139846059546368 logging_writer.py:48] [32200] global_step=32200, grad_norm=0.07978194206953049, loss=0.034240372478961945
I0305 21:41:38.217867 139846176491264 logging_writer.py:48] [32300] global_step=32300, grad_norm=0.08282288908958435, loss=0.03311088681221008
I0305 21:41:59.192777 139846059546368 logging_writer.py:48] [32400] global_step=32400, grad_norm=0.15908914804458618, loss=0.03551892563700676
I0305 21:42:21.194908 139846176491264 logging_writer.py:48] [32500] global_step=32500, grad_norm=0.06549766659736633, loss=0.03618207201361656
I0305 21:42:42.388165 139846059546368 logging_writer.py:48] [32600] global_step=32600, grad_norm=0.04658126458525658, loss=0.03454994037747383
I0305 21:42:45.334355 139988053398720 spec.py:321] Evaluating on the training split.
I0305 21:43:57.900363 139988053398720 spec.py:333] Evaluating on the validation split.
I0305 21:43:59.791687 139988053398720 spec.py:349] Evaluating on the test split.
I0305 21:44:01.727720 139988053398720 submission_runner.py:469] Time since start: 9035.54s, 	Step: 32615, 	{'train/accuracy': 0.989496111869812, 'train/loss': 0.03585892170667648, 'train/mean_average_precision': 0.26011119800579996, 'validation/accuracy': 0.9860924482345581, 'validation/loss': 0.04692311957478523, 'validation/mean_average_precision': 0.20180565323384847, 'validation/num_examples': 43793, 'test/accuracy': 0.9851717948913574, 'test/loss': 0.04963897168636322, 'test/mean_average_precision': 0.19559575880449237, 'test/num_examples': 43793, 'score': 6731.657792329788, 'total_duration': 9035.544276952744, 'accumulated_submission_time': 6731.657792329788, 'accumulated_eval_time': 2302.237907409668, 'accumulated_logging_time': 0.7803506851196289}
I0305 21:44:01.740318 139846176491264 logging_writer.py:48] [32615] accumulated_eval_time=2302.24, accumulated_logging_time=0.780351, accumulated_submission_time=6731.66, global_step=32615, preemption_count=0, score=6731.66, test/accuracy=0.985172, test/loss=0.049639, test/mean_average_precision=0.195596, test/num_examples=43793, total_duration=9035.54, train/accuracy=0.989496, train/loss=0.0358589, train/mean_average_precision=0.260111, validation/accuracy=0.986092, validation/loss=0.0469231, validation/mean_average_precision=0.201806, validation/num_examples=43793
I0305 21:44:19.375811 139846059546368 logging_writer.py:48] [32700] global_step=32700, grad_norm=0.0700390487909317, loss=0.0364566408097744
I0305 21:44:39.731281 139846176491264 logging_writer.py:48] [32800] global_step=32800, grad_norm=0.07080738246440887, loss=0.03630221262574196
I0305 21:45:00.412051 139846059546368 logging_writer.py:48] [32900] global_step=32900, grad_norm=0.07000251114368439, loss=0.038640089333057404
I0305 21:45:20.847189 139846176491264 logging_writer.py:48] [33000] global_step=33000, grad_norm=0.07302148640155792, loss=0.03461689129471779
I0305 21:45:41.151729 139846059546368 logging_writer.py:48] [33100] global_step=33100, grad_norm=0.05535906180739403, loss=0.0380173996090889
I0305 21:46:01.822828 139846176491264 logging_writer.py:48] [33200] global_step=33200, grad_norm=0.05852411314845085, loss=0.036421168595552444
I0305 21:46:22.494225 139846059546368 logging_writer.py:48] [33300] global_step=33300, grad_norm=0.12723952531814575, loss=0.034427858889102936
I0305 21:46:42.947588 139846176491264 logging_writer.py:48] [33400] global_step=33400, grad_norm=0.08480758219957352, loss=0.03419554606080055
I0305 21:47:03.461154 139846059546368 logging_writer.py:48] [33500] global_step=33500, grad_norm=0.10320843011140823, loss=0.03582968935370445
I0305 21:47:24.450363 139846176491264 logging_writer.py:48] [33600] global_step=33600, grad_norm=0.08830578625202179, loss=0.03742670267820358
I0305 21:47:45.188057 139846059546368 logging_writer.py:48] [33700] global_step=33700, grad_norm=0.09411197900772095, loss=0.03831060975790024
I0305 21:48:01.829016 139988053398720 spec.py:321] Evaluating on the training split.
I0305 21:49:11.637851 139988053398720 spec.py:333] Evaluating on the validation split.
I0305 21:49:13.559000 139988053398720 spec.py:349] Evaluating on the test split.
I0305 21:49:15.456363 139988053398720 submission_runner.py:469] Time since start: 9349.27s, 	Step: 33781, 	{'train/accuracy': 0.9896261096000671, 'train/loss': 0.03556752949953079, 'train/mean_average_precision': 0.26079889448724175, 'validation/accuracy': 0.9861310124397278, 'validation/loss': 0.04691952094435692, 'validation/mean_average_precision': 0.19875130409296146, 'validation/num_examples': 43793, 'test/accuracy': 0.985261082649231, 'test/loss': 0.04959997162222862, 'test/mean_average_precision': 0.19764358318239023, 'test/num_examples': 43793, 'score': 6971.704412698746, 'total_duration': 9349.272918462753, 'accumulated_submission_time': 6971.704412698746, 'accumulated_eval_time': 2375.86523103714, 'accumulated_logging_time': 0.8034987449645996}
I0305 21:49:15.469068 139846176491264 logging_writer.py:48] [33781] accumulated_eval_time=2375.87, accumulated_logging_time=0.803499, accumulated_submission_time=6971.7, global_step=33781, preemption_count=0, score=6971.7, test/accuracy=0.985261, test/loss=0.0496, test/mean_average_precision=0.197644, test/num_examples=43793, total_duration=9349.27, train/accuracy=0.989626, train/loss=0.0355675, train/mean_average_precision=0.260799, validation/accuracy=0.986131, validation/loss=0.0469195, validation/mean_average_precision=0.198751, validation/num_examples=43793
I0305 21:49:19.596560 139846059546368 logging_writer.py:48] [33800] global_step=33800, grad_norm=0.1031382828950882, loss=0.03547445684671402
I0305 21:49:40.021644 139846176491264 logging_writer.py:48] [33900] global_step=33900, grad_norm=0.06154508516192436, loss=0.0339730940759182
I0305 21:50:00.721607 139846059546368 logging_writer.py:48] [34000] global_step=34000, grad_norm=0.28088700771331787, loss=0.036515723913908005
I0305 21:50:21.706475 139846176491264 logging_writer.py:48] [34100] global_step=34100, grad_norm=0.13143028318881989, loss=0.0344330258667469
I0305 21:50:42.314750 139846059546368 logging_writer.py:48] [34200] global_step=34200, grad_norm=0.18564438819885254, loss=0.03971633315086365
I0305 21:51:03.045861 139846176491264 logging_writer.py:48] [34300] global_step=34300, grad_norm=0.13145524263381958, loss=0.036290399730205536
I0305 21:51:23.418384 139846059546368 logging_writer.py:48] [34400] global_step=34400, grad_norm=0.06502016633749008, loss=0.037935614585876465
I0305 21:51:43.775249 139846176491264 logging_writer.py:48] [34500] global_step=34500, grad_norm=0.13269144296646118, loss=0.03393867239356041
I0305 21:52:04.508429 139846059546368 logging_writer.py:48] [34600] global_step=34600, grad_norm=0.194731205701828, loss=0.03509531170129776
I0305 21:52:25.315836 139846176491264 logging_writer.py:48] [34700] global_step=34700, grad_norm=0.07438836991786957, loss=0.035552240908145905
I0305 21:52:46.007428 139846059546368 logging_writer.py:48] [34800] global_step=34800, grad_norm=0.13604263961315155, loss=0.03539687767624855
I0305 21:53:06.753669 139846176491264 logging_writer.py:48] [34900] global_step=34900, grad_norm=0.10082677751779556, loss=0.03403379023075104
I0305 21:53:15.518384 139988053398720 spec.py:321] Evaluating on the training split.
I0305 21:54:26.477258 139988053398720 spec.py:333] Evaluating on the validation split.
I0305 21:54:28.409769 139988053398720 spec.py:349] Evaluating on the test split.
I0305 21:54:30.255875 139988053398720 submission_runner.py:469] Time since start: 9664.07s, 	Step: 34943, 	{'train/accuracy': 0.9896506667137146, 'train/loss': 0.03525419905781746, 'train/mean_average_precision': 0.26847102401453926, 'validation/accuracy': 0.9859714508056641, 'validation/loss': 0.04706370458006859, 'validation/mean_average_precision': 0.20236771470246812, 'validation/num_examples': 43793, 'test/accuracy': 0.9851393699645996, 'test/loss': 0.04985686019062996, 'test/mean_average_precision': 0.19588938903801556, 'test/num_examples': 43793, 'score': 7211.712254047394, 'total_duration': 9664.072291851044, 'accumulated_submission_time': 7211.712254047394, 'accumulated_eval_time': 2450.6025428771973, 'accumulated_logging_time': 0.8256773948669434}
I0305 21:54:30.268496 139846059546368 logging_writer.py:48] [34943] accumulated_eval_time=2450.6, accumulated_logging_time=0.825677, accumulated_submission_time=7211.71, global_step=34943, preemption_count=0, score=7211.71, test/accuracy=0.985139, test/loss=0.0498569, test/mean_average_precision=0.195889, test/num_examples=43793, total_duration=9664.07, train/accuracy=0.989651, train/loss=0.0352542, train/mean_average_precision=0.268471, validation/accuracy=0.985971, validation/loss=0.0470637, validation/mean_average_precision=0.202368, validation/num_examples=43793
I0305 21:54:42.029995 139846176491264 logging_writer.py:48] [35000] global_step=35000, grad_norm=0.13554057478904724, loss=0.03639514371752739
I0305 21:55:02.402841 139846059546368 logging_writer.py:48] [35100] global_step=35100, grad_norm=0.21482665836811066, loss=0.03840131312608719
I0305 21:55:23.130200 139846176491264 logging_writer.py:48] [35200] global_step=35200, grad_norm=0.18079020082950592, loss=0.036864545196294785
I0305 21:55:43.988263 139846059546368 logging_writer.py:48] [35300] global_step=35300, grad_norm=0.11617199331521988, loss=0.03574249893426895
I0305 21:56:04.824402 139846176491264 logging_writer.py:48] [35400] global_step=35400, grad_norm=0.056174974888563156, loss=0.03691641613841057
I0305 21:56:25.678553 139846059546368 logging_writer.py:48] [35500] global_step=35500, grad_norm=0.06385155022144318, loss=0.038150593638420105
I0305 21:56:46.621125 139846176491264 logging_writer.py:48] [35600] global_step=35600, grad_norm=0.09306572377681732, loss=0.03677556291222572
I0305 21:57:07.058874 139846059546368 logging_writer.py:48] [35700] global_step=35700, grad_norm=0.08992510288953781, loss=0.03691352531313896
I0305 21:57:27.600722 139846176491264 logging_writer.py:48] [35800] global_step=35800, grad_norm=0.10200357437133789, loss=0.03938684239983559
I0305 21:57:48.152445 139846059546368 logging_writer.py:48] [35900] global_step=35900, grad_norm=0.08686282485723495, loss=0.035490483045578
I0305 21:58:09.106345 139846176491264 logging_writer.py:48] [36000] global_step=36000, grad_norm=0.12464596331119537, loss=0.0385129414498806
I0305 21:58:29.830828 139846059546368 logging_writer.py:48] [36100] global_step=36100, grad_norm=0.24090540409088135, loss=0.0326760895550251
I0305 21:58:30.457100 139988053398720 spec.py:321] Evaluating on the training split.
I0305 21:59:41.230527 139988053398720 spec.py:333] Evaluating on the validation split.
I0305 21:59:43.187586 139988053398720 spec.py:349] Evaluating on the test split.
I0305 21:59:45.074727 139988053398720 submission_runner.py:469] Time since start: 9978.89s, 	Step: 36104, 	{'train/accuracy': 0.9897321462631226, 'train/loss': 0.035038650035858154, 'train/mean_average_precision': 0.27568729045025897, 'validation/accuracy': 0.9859791994094849, 'validation/loss': 0.046800386160612106, 'validation/mean_average_precision': 0.2040558492030353, 'validation/num_examples': 43793, 'test/accuracy': 0.985144853591919, 'test/loss': 0.04945572465658188, 'test/mean_average_precision': 0.19676224304716503, 'test/num_examples': 43793, 'score': 7451.861115217209, 'total_duration': 9978.891277074814, 'accumulated_submission_time': 7451.861115217209, 'accumulated_eval_time': 2525.220118045807, 'accumulated_logging_time': 0.8474617004394531}
I0305 21:59:45.088643 139846176491264 logging_writer.py:48] [36104] accumulated_eval_time=2525.22, accumulated_logging_time=0.847462, accumulated_submission_time=7451.86, global_step=36104, preemption_count=0, score=7451.86, test/accuracy=0.985145, test/loss=0.0494557, test/mean_average_precision=0.196762, test/num_examples=43793, total_duration=9978.89, train/accuracy=0.989732, train/loss=0.0350387, train/mean_average_precision=0.275687, validation/accuracy=0.985979, validation/loss=0.0468004, validation/mean_average_precision=0.204056, validation/num_examples=43793
I0305 22:00:05.218581 139846059546368 logging_writer.py:48] [36200] global_step=36200, grad_norm=0.13828618824481964, loss=0.03632950410246849
I0305 22:00:26.089924 139846176491264 logging_writer.py:48] [36300] global_step=36300, grad_norm=0.0701318010687828, loss=0.03530047833919525
I0305 22:00:46.906230 139846059546368 logging_writer.py:48] [36400] global_step=36400, grad_norm=0.161197692155838, loss=0.034815263003110886
I0305 22:01:07.647356 139846176491264 logging_writer.py:48] [36500] global_step=36500, grad_norm=0.08903968334197998, loss=0.03401268646121025
I0305 22:01:28.486128 139846059546368 logging_writer.py:48] [36600] global_step=36600, grad_norm=0.0817592591047287, loss=0.03492211177945137
I0305 22:01:49.077632 139846176491264 logging_writer.py:48] [36700] global_step=36700, grad_norm=0.23040513694286346, loss=0.03294804319739342
I0305 22:02:09.781727 139846059546368 logging_writer.py:48] [36800] global_step=36800, grad_norm=0.2693716883659363, loss=0.034828152507543564
I0305 22:02:30.721511 139846176491264 logging_writer.py:48] [36900] global_step=36900, grad_norm=0.07640381902456284, loss=0.03441118821501732
I0305 22:02:51.281644 139846059546368 logging_writer.py:48] [37000] global_step=37000, grad_norm=0.09827004373073578, loss=0.031014733016490936
I0305 22:03:11.995708 139846176491264 logging_writer.py:48] [37100] global_step=37100, grad_norm=0.17191387712955475, loss=0.04083075746893883
I0305 22:03:32.384616 139846059546368 logging_writer.py:48] [37200] global_step=37200, grad_norm=0.08458532392978668, loss=0.035774581134319305
I0305 22:03:45.125517 139988053398720 spec.py:321] Evaluating on the training split.
I0305 22:04:54.288021 139988053398720 spec.py:333] Evaluating on the validation split.
I0305 22:04:56.189271 139988053398720 spec.py:349] Evaluating on the test split.
I0305 22:04:58.084180 139988053398720 submission_runner.py:469] Time since start: 10291.90s, 	Step: 37262, 	{'train/accuracy': 0.9898052215576172, 'train/loss': 0.03457319363951683, 'train/mean_average_precision': 0.2811102362806023, 'validation/accuracy': 0.9862211346626282, 'validation/loss': 0.04660797864198685, 'validation/mean_average_precision': 0.21265516007284108, 'validation/num_examples': 43793, 'test/accuracy': 0.9853259921073914, 'test/loss': 0.04942508041858673, 'test/mean_average_precision': 0.20540918149965803, 'test/num_examples': 43793, 'score': 7691.857767343521, 'total_duration': 10291.900625944138, 'accumulated_submission_time': 7691.857767343521, 'accumulated_eval_time': 2598.178633213043, 'accumulated_logging_time': 0.8701357841491699}
I0305 22:04:58.097441 139846176491264 logging_writer.py:48] [37262] accumulated_eval_time=2598.18, accumulated_logging_time=0.870136, accumulated_submission_time=7691.86, global_step=37262, preemption_count=0, score=7691.86, test/accuracy=0.985326, test/loss=0.0494251, test/mean_average_precision=0.205409, test/num_examples=43793, total_duration=10291.9, train/accuracy=0.989805, train/loss=0.0345732, train/mean_average_precision=0.28111, validation/accuracy=0.986221, validation/loss=0.046608, validation/mean_average_precision=0.212655, validation/num_examples=43793
I0305 22:05:06.266410 139846059546368 logging_writer.py:48] [37300] global_step=37300, grad_norm=0.13060437142848969, loss=0.03334017097949982
I0305 22:05:27.116542 139846176491264 logging_writer.py:48] [37400] global_step=37400, grad_norm=0.06292121112346649, loss=0.03161802142858505
I0305 22:05:47.962896 139846059546368 logging_writer.py:48] [37500] global_step=37500, grad_norm=0.09566014260053635, loss=0.035948410630226135
I0305 22:06:08.885215 139846176491264 logging_writer.py:48] [37600] global_step=37600, grad_norm=0.14309802651405334, loss=0.03589402511715889
I0305 22:06:29.846868 139846059546368 logging_writer.py:48] [37700] global_step=37700, grad_norm=0.25113412737846375, loss=0.03306286782026291
I0305 22:06:50.490376 139846176491264 logging_writer.py:48] [37800] global_step=37800, grad_norm=0.12548448145389557, loss=0.034086234867572784
I0305 22:07:11.075437 139846059546368 logging_writer.py:48] [37900] global_step=37900, grad_norm=0.05920141562819481, loss=0.033473554998636246
I0305 22:07:31.868009 139846176491264 logging_writer.py:48] [38000] global_step=38000, grad_norm=0.09115729480981827, loss=0.03181179612874985
I0305 22:07:52.581757 139846059546368 logging_writer.py:48] [38100] global_step=38100, grad_norm=0.14401108026504517, loss=0.03665769472718239
I0305 22:08:13.141786 139846176491264 logging_writer.py:48] [38200] global_step=38200, grad_norm=0.13826683163642883, loss=0.03496113419532776
I0305 22:08:33.988714 139846059546368 logging_writer.py:48] [38300] global_step=38300, grad_norm=0.10981208086013794, loss=0.03617122024297714
I0305 22:08:54.793026 139846176491264 logging_writer.py:48] [38400] global_step=38400, grad_norm=0.15163752436637878, loss=0.03728814050555229
I0305 22:08:58.206790 139988053398720 spec.py:321] Evaluating on the training split.
I0305 22:10:06.335180 139988053398720 spec.py:333] Evaluating on the validation split.
I0305 22:10:08.244809 139988053398720 spec.py:349] Evaluating on the test split.
I0305 22:10:10.162744 139988053398720 submission_runner.py:469] Time since start: 10603.98s, 	Step: 38417, 	{'train/accuracy': 0.9898331761360168, 'train/loss': 0.03427600860595703, 'train/mean_average_precision': 0.2851992314436448, 'validation/accuracy': 0.9862880706787109, 'validation/loss': 0.046690601855516434, 'validation/mean_average_precision': 0.21053127933967072, 'validation/num_examples': 43793, 'test/accuracy': 0.9853900074958801, 'test/loss': 0.04969640448689461, 'test/mean_average_precision': 0.20706954485244747, 'test/num_examples': 43793, 'score': 7931.926340341568, 'total_duration': 10603.97923707962, 'accumulated_submission_time': 7931.926340341568, 'accumulated_eval_time': 2670.134476184845, 'accumulated_logging_time': 0.8924276828765869}
I0305 22:10:10.175583 139846059546368 logging_writer.py:48] [38417] accumulated_eval_time=2670.13, accumulated_logging_time=0.892428, accumulated_submission_time=7931.93, global_step=38417, preemption_count=0, score=7931.93, test/accuracy=0.98539, test/loss=0.0496964, test/mean_average_precision=0.20707, test/num_examples=43793, total_duration=10604, train/accuracy=0.989833, train/loss=0.034276, train/mean_average_precision=0.285199, validation/accuracy=0.986288, validation/loss=0.0466906, validation/mean_average_precision=0.210531, validation/num_examples=43793
I0305 22:10:28.087462 139846176491264 logging_writer.py:48] [38500] global_step=38500, grad_norm=0.12421046197414398, loss=0.03372570127248764
I0305 22:10:49.240966 139846059546368 logging_writer.py:48] [38600] global_step=38600, grad_norm=0.09151843935251236, loss=0.03150532767176628
I0305 22:11:10.393162 139846176491264 logging_writer.py:48] [38700] global_step=38700, grad_norm=0.12903574109077454, loss=0.03471978008747101
I0305 22:11:30.910104 139846059546368 logging_writer.py:48] [38800] global_step=38800, grad_norm=0.08169837296009064, loss=0.03308069705963135
I0305 22:11:52.289543 139846176491264 logging_writer.py:48] [38900] global_step=38900, grad_norm=0.14728358387947083, loss=0.03241888806223869
I0305 22:12:12.637345 139846059546368 logging_writer.py:48] [39000] global_step=39000, grad_norm=0.08498633652925491, loss=0.03501373901963234
I0305 22:12:33.250414 139846176491264 logging_writer.py:48] [39100] global_step=39100, grad_norm=0.12766140699386597, loss=0.03361072391271591
I0305 22:12:54.011896 139846059546368 logging_writer.py:48] [39200] global_step=39200, grad_norm=0.13343055546283722, loss=0.03777334466576576
I0305 22:13:15.029062 139846176491264 logging_writer.py:48] [39300] global_step=39300, grad_norm=0.10728716850280762, loss=0.03164881840348244
I0305 22:13:35.865388 139846059546368 logging_writer.py:48] [39400] global_step=39400, grad_norm=0.12952421605587006, loss=0.03647753596305847
I0305 22:13:56.574318 139846176491264 logging_writer.py:48] [39500] global_step=39500, grad_norm=0.08407139033079147, loss=0.035485152155160904
I0305 22:14:10.292576 139988053398720 spec.py:321] Evaluating on the training split.
I0305 22:15:18.718031 139988053398720 spec.py:333] Evaluating on the validation split.
I0305 22:15:20.788773 139988053398720 spec.py:349] Evaluating on the test split.
I0305 22:15:22.704190 139988053398720 submission_runner.py:469] Time since start: 10916.52s, 	Step: 39567, 	{'train/accuracy': 0.9899880886077881, 'train/loss': 0.034090906381607056, 'train/mean_average_precision': 0.281206312688988, 'validation/accuracy': 0.9862925410270691, 'validation/loss': 0.04625346511602402, 'validation/mean_average_precision': 0.20642464278309228, 'validation/num_examples': 43793, 'test/accuracy': 0.9853954315185547, 'test/loss': 0.049186039716005325, 'test/mean_average_precision': 0.20785768893580164, 'test/num_examples': 43793, 'score': 8172.000776052475, 'total_duration': 10916.520744085312, 'accumulated_submission_time': 8172.000776052475, 'accumulated_eval_time': 2742.5460517406464, 'accumulated_logging_time': 0.9154491424560547}
I0305 22:15:22.717058 139846059546368 logging_writer.py:48] [39567] accumulated_eval_time=2742.55, accumulated_logging_time=0.915449, accumulated_submission_time=8172, global_step=39567, preemption_count=0, score=8172, test/accuracy=0.985395, test/loss=0.049186, test/mean_average_precision=0.207858, test/num_examples=43793, total_duration=10916.5, train/accuracy=0.989988, train/loss=0.0340909, train/mean_average_precision=0.281206, validation/accuracy=0.986293, validation/loss=0.0462535, validation/mean_average_precision=0.206425, validation/num_examples=43793
I0305 22:15:29.828305 139846176491264 logging_writer.py:48] [39600] global_step=39600, grad_norm=0.19693249464035034, loss=0.03228581324219704
I0305 22:15:50.588804 139846059546368 logging_writer.py:48] [39700] global_step=39700, grad_norm=0.08357614278793335, loss=0.03183595463633537
I0305 22:16:11.449842 139846176491264 logging_writer.py:48] [39800] global_step=39800, grad_norm=0.09470011293888092, loss=0.035774942487478256
I0305 22:16:31.902190 139846059546368 logging_writer.py:48] [39900] global_step=39900, grad_norm=0.12331043928861618, loss=0.03378080576658249
I0305 22:16:52.204338 139846176491264 logging_writer.py:48] [40000] global_step=40000, grad_norm=0.11844857037067413, loss=0.03144007921218872
I0305 22:17:12.676635 139846059546368 logging_writer.py:48] [40100] global_step=40100, grad_norm=0.07838001847267151, loss=0.031021686270833015
I0305 22:17:33.602905 139846176491264 logging_writer.py:48] [40200] global_step=40200, grad_norm=0.09883462637662888, loss=0.03231038898229599
I0305 22:17:54.544942 139846059546368 logging_writer.py:48] [40300] global_step=40300, grad_norm=0.12664194405078888, loss=0.03345625847578049
I0305 22:18:15.171777 139846176491264 logging_writer.py:48] [40400] global_step=40400, grad_norm=0.13298490643501282, loss=0.03058897703886032
I0305 22:18:36.120836 139846059546368 logging_writer.py:48] [40500] global_step=40500, grad_norm=0.0941758006811142, loss=0.033607423305511475
I0305 22:18:56.819679 139846176491264 logging_writer.py:48] [40600] global_step=40600, grad_norm=0.10324050486087799, loss=0.03651318699121475
I0305 22:19:17.560706 139846059546368 logging_writer.py:48] [40700] global_step=40700, grad_norm=0.2656872868537903, loss=0.03569731488823891
I0305 22:19:22.750519 139988053398720 spec.py:321] Evaluating on the training split.
I0305 22:20:29.814082 139988053398720 spec.py:333] Evaluating on the validation split.
I0305 22:20:31.752511 139988053398720 spec.py:349] Evaluating on the test split.
I0305 22:20:33.668627 139988053398720 submission_runner.py:469] Time since start: 11227.49s, 	Step: 40726, 	{'train/accuracy': 0.9902454018592834, 'train/loss': 0.03296717628836632, 'train/mean_average_precision': 0.3051772376021655, 'validation/accuracy': 0.9862491488456726, 'validation/loss': 0.046498071402311325, 'validation/mean_average_precision': 0.21170605969588732, 'validation/num_examples': 43793, 'test/accuracy': 0.9853933453559875, 'test/loss': 0.04916827380657196, 'test/mean_average_precision': 0.2044074590811802, 'test/num_examples': 43793, 'score': 8411.995926380157, 'total_duration': 11227.485052347183, 'accumulated_submission_time': 8411.995926380157, 'accumulated_eval_time': 2813.463989496231, 'accumulated_logging_time': 0.9371750354766846}
I0305 22:20:33.683952 139846176491264 logging_writer.py:48] [40726] accumulated_eval_time=2813.46, accumulated_logging_time=0.937175, accumulated_submission_time=8412, global_step=40726, preemption_count=0, score=8412, test/accuracy=0.985393, test/loss=0.0491683, test/mean_average_precision=0.204407, test/num_examples=43793, total_duration=11227.5, train/accuracy=0.990245, train/loss=0.0329672, train/mean_average_precision=0.305177, validation/accuracy=0.986249, validation/loss=0.0464981, validation/mean_average_precision=0.211706, validation/num_examples=43793
I0305 22:20:49.156322 139846059546368 logging_writer.py:48] [40800] global_step=40800, grad_norm=0.20760087668895721, loss=0.03817738592624664
I0305 22:21:09.724317 139846176491264 logging_writer.py:48] [40900] global_step=40900, grad_norm=0.165326327085495, loss=0.032640717923641205
I0305 22:21:30.470057 139846059546368 logging_writer.py:48] [41000] global_step=41000, grad_norm=0.12411489337682724, loss=0.03305429965257645
I0305 22:21:51.319558 139846176491264 logging_writer.py:48] [41100] global_step=41100, grad_norm=0.08597163110971451, loss=0.03169834241271019
I0305 22:22:11.841600 139846059546368 logging_writer.py:48] [41200] global_step=41200, grad_norm=0.15488390624523163, loss=0.03205059468746185
I0305 22:22:32.451215 139846176491264 logging_writer.py:48] [41300] global_step=41300, grad_norm=0.12118636816740036, loss=0.036653824150562286
I0305 22:22:53.328567 139846059546368 logging_writer.py:48] [41400] global_step=41400, grad_norm=0.0901680663228035, loss=0.03184990584850311
I0305 22:23:14.136563 139846176491264 logging_writer.py:48] [41500] global_step=41500, grad_norm=0.11950438469648361, loss=0.03167761489748955
I0305 22:23:34.928483 139846059546368 logging_writer.py:48] [41600] global_step=41600, grad_norm=0.18001948297023773, loss=0.031423188745975494
I0305 22:23:55.745224 139846176491264 logging_writer.py:48] [41700] global_step=41700, grad_norm=0.08099046349525452, loss=0.031192483380436897
I0305 22:24:16.188104 139846059546368 logging_writer.py:48] [41800] global_step=41800, grad_norm=0.07732465863227844, loss=0.03194017708301544
I0305 22:24:33.830427 139988053398720 spec.py:321] Evaluating on the training split.
I0305 22:25:45.641363 139988053398720 spec.py:333] Evaluating on the validation split.
I0305 22:25:47.554497 139988053398720 spec.py:349] Evaluating on the test split.
I0305 22:25:49.445032 139988053398720 submission_runner.py:469] Time since start: 11543.26s, 	Step: 41886, 	{'train/accuracy': 0.9901771545410156, 'train/loss': 0.03334222733974457, 'train/mean_average_precision': 0.301136121843531, 'validation/accuracy': 0.9863836765289307, 'validation/loss': 0.0462188757956028, 'validation/mean_average_precision': 0.2128808637434054, 'validation/num_examples': 43793, 'test/accuracy': 0.9854426383972168, 'test/loss': 0.04929576441645622, 'test/mean_average_precision': 0.2059048086831771, 'test/num_examples': 43793, 'score': 8652.101371049881, 'total_duration': 11543.26158952713, 'accumulated_submission_time': 8652.101371049881, 'accumulated_eval_time': 2889.0785546302795, 'accumulated_logging_time': 0.9625513553619385}
I0305 22:25:49.458413 139846176491264 logging_writer.py:48] [41886] accumulated_eval_time=2889.08, accumulated_logging_time=0.962551, accumulated_submission_time=8652.1, global_step=41886, preemption_count=0, score=8652.1, test/accuracy=0.985443, test/loss=0.0492958, test/mean_average_precision=0.205905, test/num_examples=43793, total_duration=11543.3, train/accuracy=0.990177, train/loss=0.0333422, train/mean_average_precision=0.301136, validation/accuracy=0.986384, validation/loss=0.0462189, validation/mean_average_precision=0.212881, validation/num_examples=43793
I0305 22:25:52.664459 139846059546368 logging_writer.py:48] [41900] global_step=41900, grad_norm=0.17155775427818298, loss=0.035087235271930695
I0305 22:26:13.056949 139846176491264 logging_writer.py:48] [42000] global_step=42000, grad_norm=0.23505139350891113, loss=0.03420928493142128
I0305 22:26:33.638410 139846059546368 logging_writer.py:48] [42100] global_step=42100, grad_norm=0.18982556462287903, loss=0.029830167070031166
I0305 22:26:54.193278 139846176491264 logging_writer.py:48] [42200] global_step=42200, grad_norm=0.14803004264831543, loss=0.03158876299858093
I0305 22:27:15.304888 139846059546368 logging_writer.py:48] [42300] global_step=42300, grad_norm=0.15947581827640533, loss=0.03125210851430893
I0305 22:27:35.914816 139846176491264 logging_writer.py:48] [42400] global_step=42400, grad_norm=0.12836018204689026, loss=0.03399723768234253
I0305 22:27:56.393195 139846059546368 logging_writer.py:48] [42500] global_step=42500, grad_norm=0.2943289279937744, loss=0.03387485072016716
I0305 22:28:16.819876 139846176491264 logging_writer.py:48] [42600] global_step=42600, grad_norm=0.18055258691310883, loss=0.033662982285022736
I0305 22:28:37.476099 139846059546368 logging_writer.py:48] [42700] global_step=42700, grad_norm=0.21575899422168732, loss=0.0340607687830925
I0305 22:28:58.405439 139846176491264 logging_writer.py:48] [42800] global_step=42800, grad_norm=0.11163925379514694, loss=0.03186897188425064
I0305 22:29:19.311269 139846059546368 logging_writer.py:48] [42900] global_step=42900, grad_norm=0.12870535254478455, loss=0.03607664257287979
I0305 22:29:40.065422 139846176491264 logging_writer.py:48] [43000] global_step=43000, grad_norm=0.1054920181632042, loss=0.032686375081539154
I0305 22:29:49.530715 139988053398720 spec.py:321] Evaluating on the training split.
I0305 22:30:57.587132 139988053398720 spec.py:333] Evaluating on the validation split.
I0305 22:30:59.485650 139988053398720 spec.py:349] Evaluating on the test split.
I0305 22:31:01.573848 139988053398720 submission_runner.py:469] Time since start: 11855.39s, 	Step: 43047, 	{'train/accuracy': 0.9904035329818726, 'train/loss': 0.03253759816288948, 'train/mean_average_precision': 0.3252771068780024, 'validation/accuracy': 0.9863542914390564, 'validation/loss': 0.046258874237537384, 'validation/mean_average_precision': 0.21223484665383152, 'validation/num_examples': 43793, 'test/accuracy': 0.9854510426521301, 'test/loss': 0.04899241775274277, 'test/mean_average_precision': 0.20974341672531127, 'test/num_examples': 43793, 'score': 8892.135527133942, 'total_duration': 11855.3903028965, 'accumulated_submission_time': 8892.135527133942, 'accumulated_eval_time': 2961.1215398311615, 'accumulated_logging_time': 0.9855368137359619}
I0305 22:31:01.588452 139846059546368 logging_writer.py:48] [43047] accumulated_eval_time=2961.12, accumulated_logging_time=0.985537, accumulated_submission_time=8892.14, global_step=43047, preemption_count=0, score=8892.14, test/accuracy=0.985451, test/loss=0.0489924, test/mean_average_precision=0.209743, test/num_examples=43793, total_duration=11855.4, train/accuracy=0.990404, train/loss=0.0325376, train/mean_average_precision=0.325277, validation/accuracy=0.986354, validation/loss=0.0462589, validation/mean_average_precision=0.212235, validation/num_examples=43793
I0305 22:31:12.626619 139846176491264 logging_writer.py:48] [43100] global_step=43100, grad_norm=0.12849871814250946, loss=0.032264743000268936
I0305 22:31:33.033263 139846059546368 logging_writer.py:48] [43200] global_step=43200, grad_norm=0.13655078411102295, loss=0.034987956285476685
I0305 22:31:53.936955 139846176491264 logging_writer.py:48] [43300] global_step=43300, grad_norm=0.14661680161952972, loss=0.036829929798841476
I0305 22:32:14.920248 139846059546368 logging_writer.py:48] [43400] global_step=43400, grad_norm=0.159438356757164, loss=0.038562458008527756
I0305 22:32:35.755403 139846176491264 logging_writer.py:48] [43500] global_step=43500, grad_norm=0.13339734077453613, loss=0.03464643284678459
I0305 22:32:56.477081 139846059546368 logging_writer.py:48] [43600] global_step=43600, grad_norm=0.21064431965351105, loss=0.027237771078944206
I0305 22:33:17.228830 139846176491264 logging_writer.py:48] [43700] global_step=43700, grad_norm=0.21788136661052704, loss=0.034622956067323685
I0305 22:33:38.090793 139846059546368 logging_writer.py:48] [43800] global_step=43800, grad_norm=0.15423595905303955, loss=0.029357613995671272
I0305 22:33:58.610056 139846176491264 logging_writer.py:48] [43900] global_step=43900, grad_norm=0.21855619549751282, loss=0.034841835498809814
I0305 22:34:19.157727 139846059546368 logging_writer.py:48] [44000] global_step=44000, grad_norm=0.18590229749679565, loss=0.03267370164394379
I0305 22:34:39.592212 139846176491264 logging_writer.py:48] [44100] global_step=44100, grad_norm=0.15493622422218323, loss=0.036962565034627914
I0305 22:35:00.514956 139846059546368 logging_writer.py:48] [44200] global_step=44200, grad_norm=0.29751214385032654, loss=0.0339653454720974
I0305 22:35:01.782265 139988053398720 spec.py:321] Evaluating on the training split.
I0305 22:36:11.952837 139988053398720 spec.py:333] Evaluating on the validation split.
I0305 22:36:13.962991 139988053398720 spec.py:349] Evaluating on the test split.
I0305 22:36:15.844323 139988053398720 submission_runner.py:469] Time since start: 12169.66s, 	Step: 44207, 	{'train/accuracy': 0.9902641773223877, 'train/loss': 0.03271237760782242, 'train/mean_average_precision': 0.3065861649862181, 'validation/accuracy': 0.9864399433135986, 'validation/loss': 0.04608120396733284, 'validation/mean_average_precision': 0.22112551462443386, 'validation/num_examples': 43793, 'test/accuracy': 0.9855028390884399, 'test/loss': 0.04894053563475609, 'test/mean_average_precision': 0.21316727328209997, 'test/num_examples': 43793, 'score': 9132.289376020432, 'total_duration': 12169.66079378128, 'accumulated_submission_time': 9132.289376020432, 'accumulated_eval_time': 3035.1834678649902, 'accumulated_logging_time': 1.0099751949310303}
I0305 22:36:15.857870 139846176491264 logging_writer.py:48] [44207] accumulated_eval_time=3035.18, accumulated_logging_time=1.00998, accumulated_submission_time=9132.29, global_step=44207, preemption_count=0, score=9132.29, test/accuracy=0.985503, test/loss=0.0489405, test/mean_average_precision=0.213167, test/num_examples=43793, total_duration=12169.7, train/accuracy=0.990264, train/loss=0.0327124, train/mean_average_precision=0.306586, validation/accuracy=0.98644, validation/loss=0.0460812, validation/mean_average_precision=0.221126, validation/num_examples=43793
I0305 22:36:35.521456 139846059546368 logging_writer.py:48] [44300] global_step=44300, grad_norm=0.13874363899230957, loss=0.029865078628063202
I0305 22:36:56.392545 139846176491264 logging_writer.py:48] [44400] global_step=44400, grad_norm=0.1575118750333786, loss=0.031060321256518364
I0305 22:37:16.724908 139846059546368 logging_writer.py:48] [44500] global_step=44500, grad_norm=0.2176513522863388, loss=0.03046899102628231
I0305 22:37:37.264182 139846176491264 logging_writer.py:48] [44600] global_step=44600, grad_norm=0.145145982503891, loss=0.030363207682967186
I0305 22:37:57.961594 139846059546368 logging_writer.py:48] [44700] global_step=44700, grad_norm=0.1527082920074463, loss=0.03536977246403694
I0305 22:38:18.859248 139846176491264 logging_writer.py:48] [44800] global_step=44800, grad_norm=0.16010978817939758, loss=0.035014111548662186
I0305 22:38:39.285615 139846059546368 logging_writer.py:48] [44900] global_step=44900, grad_norm=0.24324077367782593, loss=0.03344985097646713
I0305 22:39:00.297384 139846176491264 logging_writer.py:48] [45000] global_step=45000, grad_norm=0.19324412941932678, loss=0.030522910878062248
I0305 22:39:21.281488 139846059546368 logging_writer.py:48] [45100] global_step=45100, grad_norm=0.17394056916236877, loss=0.033455464988946915
I0305 22:39:42.097399 139846176491264 logging_writer.py:48] [45200] global_step=45200, grad_norm=0.19951942563056946, loss=0.030474282801151276
I0305 22:40:02.906079 139846059546368 logging_writer.py:48] [45300] global_step=45300, grad_norm=0.15753543376922607, loss=0.033814769238233566
I0305 22:40:16.009575 139988053398720 spec.py:321] Evaluating on the training split.
I0305 22:41:26.194705 139988053398720 spec.py:333] Evaluating on the validation split.
I0305 22:41:28.149922 139988053398720 spec.py:349] Evaluating on the test split.
I0305 22:41:30.085224 139988053398720 submission_runner.py:469] Time since start: 12483.90s, 	Step: 45363, 	{'train/accuracy': 0.9905139207839966, 'train/loss': 0.03190862014889717, 'train/mean_average_precision': 0.31863473384338836, 'validation/accuracy': 0.9864845871925354, 'validation/loss': 0.046220190823078156, 'validation/mean_average_precision': 0.21800998610601421, 'validation/num_examples': 43793, 'test/accuracy': 0.9855555295944214, 'test/loss': 0.04915323108434677, 'test/mean_average_precision': 0.212875933131114, 'test/num_examples': 43793, 'score': 9372.402907133102, 'total_duration': 12483.901703119278, 'accumulated_submission_time': 9372.402907133102, 'accumulated_eval_time': 3109.2589938640594, 'accumulated_logging_time': 1.0323193073272705}
I0305 22:41:30.099405 139846176491264 logging_writer.py:48] [45363] accumulated_eval_time=3109.26, accumulated_logging_time=1.03232, accumulated_submission_time=9372.4, global_step=45363, preemption_count=0, score=9372.4, test/accuracy=0.985556, test/loss=0.0491532, test/mean_average_precision=0.212876, test/num_examples=43793, total_duration=12483.9, train/accuracy=0.990514, train/loss=0.0319086, train/mean_average_precision=0.318635, validation/accuracy=0.986485, validation/loss=0.0462202, validation/mean_average_precision=0.21801, validation/num_examples=43793
I0305 22:41:38.217858 139846059546368 logging_writer.py:48] [45400] global_step=45400, grad_norm=0.1273486465215683, loss=0.029761455953121185
I0305 22:41:58.885926 139846176491264 logging_writer.py:48] [45500] global_step=45500, grad_norm=0.1614988148212433, loss=0.03144789859652519
I0305 22:42:20.114494 139846059546368 logging_writer.py:48] [45600] global_step=45600, grad_norm=0.3004899322986603, loss=0.02879737690091133
I0305 22:42:40.959501 139846176491264 logging_writer.py:48] [45700] global_step=45700, grad_norm=0.3740898370742798, loss=0.03316103294491768
I0305 22:43:01.969588 139846059546368 logging_writer.py:48] [45800] global_step=45800, grad_norm=0.15335151553153992, loss=0.03157107159495354
I0305 22:43:22.748742 139846176491264 logging_writer.py:48] [45900] global_step=45900, grad_norm=0.2804492712020874, loss=0.03063926100730896
I0305 22:43:43.803907 139846059546368 logging_writer.py:48] [46000] global_step=46000, grad_norm=0.16878917813301086, loss=0.0365140326321125
I0305 22:44:04.698650 139846176491264 logging_writer.py:48] [46100] global_step=46100, grad_norm=0.12709011137485504, loss=0.031718168407678604
I0305 22:44:25.730117 139846059546368 logging_writer.py:48] [46200] global_step=46200, grad_norm=0.19390162825584412, loss=0.03084433265030384
I0305 22:44:46.385467 139846176491264 logging_writer.py:48] [46300] global_step=46300, grad_norm=0.1946793645620346, loss=0.034815412014722824
I0305 22:45:06.852860 139846059546368 logging_writer.py:48] [46400] global_step=46400, grad_norm=0.13866697251796722, loss=0.03390202298760414
I0305 22:45:27.273942 139846176491264 logging_writer.py:48] [46500] global_step=46500, grad_norm=0.2191532552242279, loss=0.03264179080724716
I0305 22:45:30.111243 139988053398720 spec.py:321] Evaluating on the training split.
I0305 22:46:36.387235 139988053398720 spec.py:333] Evaluating on the validation split.
I0305 22:46:38.552331 139988053398720 spec.py:349] Evaluating on the test split.
I0305 22:46:40.436471 139988053398720 submission_runner.py:469] Time since start: 12794.25s, 	Step: 46515, 	{'train/accuracy': 0.9905454516410828, 'train/loss': 0.03197187930345535, 'train/mean_average_precision': 0.331558288552406, 'validation/accuracy': 0.9864001274108887, 'validation/loss': 0.04629851132631302, 'validation/mean_average_precision': 0.2174071682192422, 'validation/num_examples': 43793, 'test/accuracy': 0.985417366027832, 'test/loss': 0.04922536388039589, 'test/mean_average_precision': 0.21176476129108415, 'test/num_examples': 43793, 'score': 9612.374552726746, 'total_duration': 12794.25294828415, 'accumulated_submission_time': 9612.374552726746, 'accumulated_eval_time': 3179.5840995311737, 'accumulated_logging_time': 1.0555248260498047}
I0305 22:46:40.451617 139846059546368 logging_writer.py:48] [46515] accumulated_eval_time=3179.58, accumulated_logging_time=1.05552, accumulated_submission_time=9612.37, global_step=46515, preemption_count=0, score=9612.37, test/accuracy=0.985417, test/loss=0.0492254, test/mean_average_precision=0.211765, test/num_examples=43793, total_duration=12794.3, train/accuracy=0.990545, train/loss=0.0319719, train/mean_average_precision=0.331558, validation/accuracy=0.9864, validation/loss=0.0462985, validation/mean_average_precision=0.217407, validation/num_examples=43793
I0305 22:46:58.377363 139846176491264 logging_writer.py:48] [46600] global_step=46600, grad_norm=0.10212402790784836, loss=0.028846394270658493
I0305 22:47:19.382133 139846059546368 logging_writer.py:48] [46700] global_step=46700, grad_norm=0.17573966085910797, loss=0.03005284257233143
I0305 22:47:40.383264 139846176491264 logging_writer.py:48] [46800] global_step=46800, grad_norm=0.1904246211051941, loss=0.03129858151078224
I0305 22:48:01.458547 139846059546368 logging_writer.py:48] [46900] global_step=46900, grad_norm=0.1644267588853836, loss=0.03331071883440018
I0305 22:48:22.564546 139846176491264 logging_writer.py:48] [47000] global_step=47000, grad_norm=0.15049245953559875, loss=0.028320953249931335
I0305 22:48:43.373072 139846059546368 logging_writer.py:48] [47100] global_step=47100, grad_norm=0.18806156516075134, loss=0.033489517867565155
I0305 22:49:04.426406 139846176491264 logging_writer.py:48] [47200] global_step=47200, grad_norm=0.16565845906734467, loss=0.02766530215740204
I0305 22:49:24.831094 139846059546368 logging_writer.py:48] [47300] global_step=47300, grad_norm=0.12669910490512848, loss=0.03186078369617462
I0305 22:49:45.165254 139846176491264 logging_writer.py:48] [47400] global_step=47400, grad_norm=0.25487232208251953, loss=0.03409615531563759
I0305 22:50:05.572644 139846059546368 logging_writer.py:48] [47500] global_step=47500, grad_norm=0.3330954611301422, loss=0.03483753278851509
I0305 22:50:26.537169 139846176491264 logging_writer.py:48] [47600] global_step=47600, grad_norm=0.20233209431171417, loss=0.029290437698364258
I0305 22:50:40.640565 139988053398720 spec.py:321] Evaluating on the training split.
I0305 22:51:54.498298 139988053398720 spec.py:333] Evaluating on the validation split.
I0305 22:51:56.474133 139988053398720 spec.py:349] Evaluating on the test split.
I0305 22:51:58.361951 139988053398720 submission_runner.py:469] Time since start: 13112.18s, 	Step: 47668, 	{'train/accuracy': 0.9908093214035034, 'train/loss': 0.031168807297945023, 'train/mean_average_precision': 0.34157119438706895, 'validation/accuracy': 0.9864829182624817, 'validation/loss': 0.04601713642477989, 'validation/mean_average_precision': 0.21942631558991563, 'validation/num_examples': 43793, 'test/accuracy': 0.9854881167411804, 'test/loss': 0.04896024242043495, 'test/mean_average_precision': 0.2149191535565873, 'test/num_examples': 43793, 'score': 9852.525210618973, 'total_duration': 13112.178442001343, 'accumulated_submission_time': 9852.525210618973, 'accumulated_eval_time': 3257.305377483368, 'accumulated_logging_time': 1.0797054767608643}
I0305 22:51:58.375891 139846059546368 logging_writer.py:48] [47668] accumulated_eval_time=3257.31, accumulated_logging_time=1.07971, accumulated_submission_time=9852.53, global_step=47668, preemption_count=0, score=9852.53, test/accuracy=0.985488, test/loss=0.0489602, test/mean_average_precision=0.214919, test/num_examples=43793, total_duration=13112.2, train/accuracy=0.990809, train/loss=0.0311688, train/mean_average_precision=0.341571, validation/accuracy=0.986483, validation/loss=0.0460171, validation/mean_average_precision=0.219426, validation/num_examples=43793
I0305 22:52:05.261080 139846176491264 logging_writer.py:48] [47700] global_step=47700, grad_norm=0.12032123655080795, loss=0.028851637616753578
I0305 22:52:26.621538 139846059546368 logging_writer.py:48] [47800] global_step=47800, grad_norm=0.19302527606487274, loss=0.035467155277729034
I0305 22:52:47.666855 139846176491264 logging_writer.py:48] [47900] global_step=47900, grad_norm=0.18215540051460266, loss=0.030390862375497818
I0305 22:53:08.453440 139846059546368 logging_writer.py:48] [48000] global_step=48000, grad_norm=0.2834845185279846, loss=0.03426463156938553
I0305 22:53:29.594414 139846176491264 logging_writer.py:48] [48100] global_step=48100, grad_norm=0.19628502428531647, loss=0.03170611336827278
I0305 22:53:50.990092 139846059546368 logging_writer.py:48] [48200] global_step=48200, grad_norm=0.3262254595756531, loss=0.03511698544025421
I0305 22:54:11.931786 139846176491264 logging_writer.py:48] [48300] global_step=48300, grad_norm=0.1665097177028656, loss=0.0267136599868536
I0305 22:54:32.786319 139846059546368 logging_writer.py:48] [48400] global_step=48400, grad_norm=0.17869196832180023, loss=0.031858693808317184
I0305 22:54:53.602092 139846176491264 logging_writer.py:48] [48500] global_step=48500, grad_norm=0.12209058552980423, loss=0.03444352373480797
I0305 22:55:14.529702 139846059546368 logging_writer.py:48] [48600] global_step=48600, grad_norm=0.14831413328647614, loss=0.0316605307161808
I0305 22:55:35.305065 139846176491264 logging_writer.py:48] [48700] global_step=48700, grad_norm=0.15246735513210297, loss=0.02901550754904747
I0305 22:55:55.898917 139846059546368 logging_writer.py:48] [48800] global_step=48800, grad_norm=0.13339844346046448, loss=0.030886074528098106
I0305 22:55:58.546404 139988053398720 spec.py:321] Evaluating on the training split.
I0305 22:57:08.410266 139988053398720 spec.py:333] Evaluating on the validation split.
I0305 22:57:10.340647 139988053398720 spec.py:349] Evaluating on the test split.
I0305 22:57:12.227842 139988053398720 submission_runner.py:469] Time since start: 13426.04s, 	Step: 48814, 	{'train/accuracy': 0.9907341599464417, 'train/loss': 0.03130224719643593, 'train/mean_average_precision': 0.3504838553775989, 'validation/accuracy': 0.9865196943283081, 'validation/loss': 0.04623526707291603, 'validation/mean_average_precision': 0.22062492725021093, 'validation/num_examples': 43793, 'test/accuracy': 0.9855133891105652, 'test/loss': 0.04922596365213394, 'test/mean_average_precision': 0.21387498792728132, 'test/num_examples': 43793, 'score': 10092.654266357422, 'total_duration': 13426.044302940369, 'accumulated_submission_time': 10092.654266357422, 'accumulated_eval_time': 3330.9866740703583, 'accumulated_logging_time': 1.103475570678711}
I0305 22:57:12.241927 139846176491264 logging_writer.py:48] [48814] accumulated_eval_time=3330.99, accumulated_logging_time=1.10348, accumulated_submission_time=10092.7, global_step=48814, preemption_count=0, score=10092.7, test/accuracy=0.985513, test/loss=0.049226, test/mean_average_precision=0.213875, test/num_examples=43793, total_duration=13426, train/accuracy=0.990734, train/loss=0.0313022, train/mean_average_precision=0.350484, validation/accuracy=0.98652, validation/loss=0.0462353, validation/mean_average_precision=0.220625, validation/num_examples=43793
I0305 22:57:30.143640 139846059546368 logging_writer.py:48] [48900] global_step=48900, grad_norm=0.16096270084381104, loss=0.03485330194234848
I0305 22:57:51.065258 139846176491264 logging_writer.py:48] [49000] global_step=49000, grad_norm=0.17343375086784363, loss=0.03471285477280617
I0305 22:58:11.448593 139846059546368 logging_writer.py:48] [49100] global_step=49100, grad_norm=0.17657922208309174, loss=0.030484724789857864
I0305 22:58:32.003914 139846176491264 logging_writer.py:48] [49200] global_step=49200, grad_norm=0.19524739682674408, loss=0.028235919773578644
I0305 22:58:52.565080 139846059546368 logging_writer.py:48] [49300] global_step=49300, grad_norm=0.2368072122335434, loss=0.030593231320381165
I0305 22:59:13.385035 139846176491264 logging_writer.py:48] [49400] global_step=49400, grad_norm=0.26364392042160034, loss=0.0322871208190918
I0305 22:59:34.178584 139846059546368 logging_writer.py:48] [49500] global_step=49500, grad_norm=0.14475269615650177, loss=0.0299018956720829
I0305 22:59:54.780585 139846176491264 logging_writer.py:48] [49600] global_step=49600, grad_norm=0.20086225867271423, loss=0.03354170545935631
I0305 23:00:15.217909 139846059546368 logging_writer.py:48] [49700] global_step=49700, grad_norm=0.2264915257692337, loss=0.03006657212972641
I0305 23:00:35.917458 139846176491264 logging_writer.py:48] [49800] global_step=49800, grad_norm=0.15155810117721558, loss=0.0336034819483757
I0305 23:00:56.782977 139846059546368 logging_writer.py:48] [49900] global_step=49900, grad_norm=0.19273915886878967, loss=0.03227459266781807
I0305 23:01:12.325180 139988053398720 spec.py:321] Evaluating on the training split.
I0305 23:02:25.603157 139988053398720 spec.py:333] Evaluating on the validation split.
I0305 23:02:27.540906 139988053398720 spec.py:349] Evaluating on the test split.
I0305 23:02:29.367170 139988053398720 submission_runner.py:469] Time since start: 13743.18s, 	Step: 49977, 	{'train/accuracy': 0.9908154010772705, 'train/loss': 0.030929813161492348, 'train/mean_average_precision': 0.3361994803580365, 'validation/accuracy': 0.986456573009491, 'validation/loss': 0.04615635797381401, 'validation/mean_average_precision': 0.21954298365393962, 'validation/num_examples': 43793, 'test/accuracy': 0.9854493737220764, 'test/loss': 0.04917613044381142, 'test/mean_average_precision': 0.21354416495868084, 'test/num_examples': 43793, 'score': 10332.698824167252, 'total_duration': 13743.183663845062, 'accumulated_submission_time': 10332.698824167252, 'accumulated_eval_time': 3408.0285561084747, 'accumulated_logging_time': 1.1267368793487549}
I0305 23:02:29.381083 139846176491264 logging_writer.py:48] [49977] accumulated_eval_time=3408.03, accumulated_logging_time=1.12674, accumulated_submission_time=10332.7, global_step=49977, preemption_count=0, score=10332.7, test/accuracy=0.985449, test/loss=0.0491761, test/mean_average_precision=0.213544, test/num_examples=43793, total_duration=13743.2, train/accuracy=0.990815, train/loss=0.0309298, train/mean_average_precision=0.336199, validation/accuracy=0.986457, validation/loss=0.0461564, validation/mean_average_precision=0.219543, validation/num_examples=43793
I0305 23:02:34.266875 139846059546368 logging_writer.py:48] [50000] global_step=50000, grad_norm=0.1361822783946991, loss=0.03090774267911911
I0305 23:02:55.481250 139846176491264 logging_writer.py:48] [50100] global_step=50100, grad_norm=0.17488722503185272, loss=0.03283481299877167
I0305 23:03:16.502137 139846059546368 logging_writer.py:48] [50200] global_step=50200, grad_norm=0.17013412714004517, loss=0.030822083353996277
I0305 23:03:37.120771 139846176491264 logging_writer.py:48] [50300] global_step=50300, grad_norm=0.17548806965351105, loss=0.025216663256287575
I0305 23:03:57.722332 139846059546368 logging_writer.py:48] [50400] global_step=50400, grad_norm=0.15524956583976746, loss=0.029894491657614708
I0305 23:04:18.530109 139846176491264 logging_writer.py:48] [50500] global_step=50500, grad_norm=0.23655052483081818, loss=0.029779614880681038
I0305 23:04:39.887933 139846059546368 logging_writer.py:48] [50600] global_step=50600, grad_norm=0.21723230183124542, loss=0.03120127134025097
I0305 23:05:00.712157 139846176491264 logging_writer.py:48] [50700] global_step=50700, grad_norm=0.17795991897583008, loss=0.029958192259073257
I0305 23:05:21.858269 139846059546368 logging_writer.py:48] [50800] global_step=50800, grad_norm=0.11332959681749344, loss=0.03318702429533005
I0305 23:05:42.853830 139846176491264 logging_writer.py:48] [50900] global_step=50900, grad_norm=0.21032360196113586, loss=0.02997634746134281
I0305 23:06:03.520535 139846059546368 logging_writer.py:48] [51000] global_step=51000, grad_norm=0.1964130997657776, loss=0.031528033316135406
I0305 23:06:24.213502 139846176491264 logging_writer.py:48] [51100] global_step=51100, grad_norm=0.17341215908527374, loss=0.030901629477739334
I0305 23:06:29.544294 139988053398720 spec.py:321] Evaluating on the training split.
I0305 23:07:37.409383 139988053398720 spec.py:333] Evaluating on the validation split.
I0305 23:07:39.345111 139988053398720 spec.py:349] Evaluating on the test split.
I0305 23:07:41.244814 139988053398720 submission_runner.py:469] Time since start: 14055.06s, 	Step: 51127, 	{'train/accuracy': 0.9907481074333191, 'train/loss': 0.03128005564212799, 'train/mean_average_precision': 0.3369954197829563, 'validation/accuracy': 0.9864618182182312, 'validation/loss': 0.046143025159835815, 'validation/mean_average_precision': 0.2199924427381768, 'validation/num_examples': 43793, 'test/accuracy': 0.9854733943939209, 'test/loss': 0.0491577610373497, 'test/mean_average_precision': 0.21440788877819716, 'test/num_examples': 43793, 'score': 10572.823588132858, 'total_duration': 14055.061342954636, 'accumulated_submission_time': 10572.823588132858, 'accumulated_eval_time': 3479.729007959366, 'accumulated_logging_time': 1.149550437927246}
I0305 23:07:41.259424 139846059546368 logging_writer.py:48] [51127] accumulated_eval_time=3479.73, accumulated_logging_time=1.14955, accumulated_submission_time=10572.8, global_step=51127, preemption_count=0, score=10572.8, test/accuracy=0.985473, test/loss=0.0491578, test/mean_average_precision=0.214408, test/num_examples=43793, total_duration=14055.1, train/accuracy=0.990748, train/loss=0.0312801, train/mean_average_precision=0.336995, validation/accuracy=0.986462, validation/loss=0.046143, validation/mean_average_precision=0.219992, validation/num_examples=43793
I0305 23:07:57.037745 139846176491264 logging_writer.py:48] [51200] global_step=51200, grad_norm=0.11791706085205078, loss=0.028386492282152176
I0305 23:08:17.952520 139846059546368 logging_writer.py:48] [51300] global_step=51300, grad_norm=0.1830807328224182, loss=0.029704170301556587
I0305 23:08:38.822515 139846176491264 logging_writer.py:48] [51400] global_step=51400, grad_norm=0.17679360508918762, loss=0.03479576110839844
I0305 23:08:59.763945 139846059546368 logging_writer.py:48] [51500] global_step=51500, grad_norm=0.18889164924621582, loss=0.029151996597647667
I0305 23:09:20.794249 139846176491264 logging_writer.py:48] [51600] global_step=51600, grad_norm=0.20542962849140167, loss=0.028314653784036636
I0305 23:09:41.829823 139846059546368 logging_writer.py:48] [51700] global_step=51700, grad_norm=0.18056441843509674, loss=0.029677540063858032
I0305 23:10:02.850906 139846176491264 logging_writer.py:48] [51800] global_step=51800, grad_norm=0.19871166348457336, loss=0.03222689777612686
I0305 23:10:23.101249 139846059546368 logging_writer.py:48] [51900] global_step=51900, grad_norm=0.2748986780643463, loss=0.030659392476081848
I0305 23:10:43.815288 139846176491264 logging_writer.py:48] [52000] global_step=52000, grad_norm=0.14935006201267242, loss=0.03418656438589096
I0305 23:11:04.387427 139846059546368 logging_writer.py:48] [52100] global_step=52100, grad_norm=0.153297558426857, loss=0.03325744345784187
I0305 23:11:25.334601 139846176491264 logging_writer.py:48] [52200] global_step=52200, grad_norm=0.16210761666297913, loss=0.03155871480703354
I0305 23:11:41.288745 139988053398720 spec.py:321] Evaluating on the training split.
I0305 23:12:51.265743 139988053398720 spec.py:333] Evaluating on the validation split.
I0305 23:12:53.214220 139988053398720 spec.py:349] Evaluating on the test split.
I0305 23:12:55.074620 139988053398720 submission_runner.py:469] Time since start: 14368.89s, 	Step: 52278, 	{'train/accuracy': 0.9908356666564941, 'train/loss': 0.030934451147913933, 'train/mean_average_precision': 0.34129104048197956, 'validation/accuracy': 0.986477255821228, 'validation/loss': 0.04616273567080498, 'validation/mean_average_precision': 0.22007124464626387, 'validation/num_examples': 43793, 'test/accuracy': 0.9854763150215149, 'test/loss': 0.049183763563632965, 'test/mean_average_precision': 0.21381964870785214, 'test/num_examples': 43793, 'score': 10812.81077170372, 'total_duration': 14368.891102552414, 'accumulated_submission_time': 10812.81077170372, 'accumulated_eval_time': 3553.5147626399994, 'accumulated_logging_time': 1.1735143661499023}
I0305 23:12:55.090359 139846059546368 logging_writer.py:48] [52278] accumulated_eval_time=3553.51, accumulated_logging_time=1.17351, accumulated_submission_time=10812.8, global_step=52278, preemption_count=0, score=10812.8, test/accuracy=0.985476, test/loss=0.0491838, test/mean_average_precision=0.21382, test/num_examples=43793, total_duration=14368.9, train/accuracy=0.990836, train/loss=0.0309345, train/mean_average_precision=0.341291, validation/accuracy=0.986477, validation/loss=0.0461627, validation/mean_average_precision=0.220071, validation/num_examples=43793
I0305 23:12:59.922816 139846176491264 logging_writer.py:48] [52300] global_step=52300, grad_norm=0.1283724009990692, loss=0.026259327307343483
I0305 23:13:20.955566 139846059546368 logging_writer.py:48] [52400] global_step=52400, grad_norm=0.1925974041223526, loss=0.03154686093330383
I0305 23:13:42.328328 139846176491264 logging_writer.py:48] [52500] global_step=52500, grad_norm=0.17867794632911682, loss=0.03373437374830246
I0305 23:14:03.550795 139846059546368 logging_writer.py:48] [52600] global_step=52600, grad_norm=0.17676778137683868, loss=0.032467320561409
I0305 23:14:24.371314 139846176491264 logging_writer.py:48] [52700] global_step=52700, grad_norm=0.19480806589126587, loss=0.02733272686600685
I0305 23:14:45.163422 139846059546368 logging_writer.py:48] [52800] global_step=52800, grad_norm=0.1982496678829193, loss=0.03546721860766411
I0305 23:15:06.233115 139846176491264 logging_writer.py:48] [52900] global_step=52900, grad_norm=0.14915192127227783, loss=0.03234463557600975
I0305 23:15:27.831396 139846059546368 logging_writer.py:48] [53000] global_step=53000, grad_norm=0.22275125980377197, loss=0.02901809848845005
I0305 23:15:48.794287 139846176491264 logging_writer.py:48] [53100] global_step=53100, grad_norm=0.15400835871696472, loss=0.03204130008816719
I0305 23:16:10.012627 139846059546368 logging_writer.py:48] [53200] global_step=53200, grad_norm=0.25004813075065613, loss=0.033826250582933426
I0305 23:16:31.202241 139846176491264 logging_writer.py:48] [53300] global_step=53300, grad_norm=0.13525348901748657, loss=0.02913334220647812
I0305 23:16:52.236624 139846059546368 logging_writer.py:48] [53400] global_step=53400, grad_norm=0.2707929015159607, loss=0.032972827553749084
I0305 23:16:55.226012 139988053398720 spec.py:321] Evaluating on the training split.
I0305 23:18:06.840394 139988053398720 spec.py:333] Evaluating on the validation split.
I0305 23:18:08.806816 139988053398720 spec.py:349] Evaluating on the test split.
I0305 23:18:10.702799 139988053398720 submission_runner.py:469] Time since start: 14684.52s, 	Step: 53415, 	{'train/accuracy': 0.9909923672676086, 'train/loss': 0.030576353892683983, 'train/mean_average_precision': 0.3423907547306161, 'validation/accuracy': 0.986477255821228, 'validation/loss': 0.04616272449493408, 'validation/mean_average_precision': 0.22005427981555448, 'validation/num_examples': 43793, 'test/accuracy': 0.9854763150215149, 'test/loss': 0.04918377101421356, 'test/mean_average_precision': 0.2139190579217768, 'test/num_examples': 43793, 'score': 11052.907064676285, 'total_duration': 14684.519357681274, 'accumulated_submission_time': 11052.907064676285, 'accumulated_eval_time': 3628.99152469635, 'accumulated_logging_time': 1.1997356414794922}
I0305 23:18:10.717484 139846176491264 logging_writer.py:48] [53415] accumulated_eval_time=3628.99, accumulated_logging_time=1.19974, accumulated_submission_time=11052.9, global_step=53415, preemption_count=0, score=11052.9, test/accuracy=0.985476, test/loss=0.0491838, test/mean_average_precision=0.213919, test/num_examples=43793, total_duration=14684.5, train/accuracy=0.990992, train/loss=0.0305764, train/mean_average_precision=0.342391, validation/accuracy=0.986477, validation/loss=0.0461627, validation/mean_average_precision=0.220054, validation/num_examples=43793
I0305 23:18:28.948245 139846059546368 logging_writer.py:48] [53500] global_step=53500, grad_norm=0.21104982495307922, loss=0.031250569969415665
I0305 23:18:50.045559 139846176491264 logging_writer.py:48] [53600] global_step=53600, grad_norm=0.16714496910572052, loss=0.030230989679694176
I0305 23:19:10.997609 139846059546368 logging_writer.py:48] [53700] global_step=53700, grad_norm=0.2779923379421234, loss=0.029131881892681122
I0305 23:19:32.080108 139846176491264 logging_writer.py:48] [53800] global_step=53800, grad_norm=0.23972342908382416, loss=0.02804463729262352
I0305 23:19:52.821868 139846059546368 logging_writer.py:48] [53900] global_step=53900, grad_norm=0.13295099139213562, loss=0.028570769354701042
I0305 23:20:13.666228 139846176491264 logging_writer.py:48] [54000] global_step=54000, grad_norm=0.34710222482681274, loss=0.03344309329986572
I0305 23:20:34.566531 139846059546368 logging_writer.py:48] [54100] global_step=54100, grad_norm=0.17978577315807343, loss=0.032890427857637405
I0305 23:20:55.537213 139846176491264 logging_writer.py:48] [54200] global_step=54200, grad_norm=0.27241575717926025, loss=0.029759742319583893
I0305 23:21:16.650744 139846059546368 logging_writer.py:48] [54300] global_step=54300, grad_norm=0.31952226161956787, loss=0.03108510747551918
I0305 23:21:37.653127 139846176491264 logging_writer.py:48] [54400] global_step=54400, grad_norm=0.14562350511550903, loss=0.028876276686787605
I0305 23:21:58.843063 139846059546368 logging_writer.py:48] [54500] global_step=54500, grad_norm=0.2677386403083801, loss=0.032697562128305435
I0305 23:22:10.724386 139988053398720 spec.py:321] Evaluating on the training split.
I0305 23:23:20.527934 139988053398720 spec.py:333] Evaluating on the validation split.
I0305 23:23:22.491733 139988053398720 spec.py:349] Evaluating on the test split.
I0305 23:23:24.363331 139988053398720 submission_runner.py:469] Time since start: 14998.18s, 	Step: 54557, 	{'train/accuracy': 0.9907734394073486, 'train/loss': 0.031270235776901245, 'train/mean_average_precision': 0.3482976551199915, 'validation/accuracy': 0.986477255821228, 'validation/loss': 0.04616273567080498, 'validation/mean_average_precision': 0.2200619196401685, 'validation/num_examples': 43793, 'test/accuracy': 0.9854763150215149, 'test/loss': 0.04918377101421356, 'test/mean_average_precision': 0.21395859270510637, 'test/num_examples': 43793, 'score': 11292.872061491013, 'total_duration': 14998.179833173752, 'accumulated_submission_time': 11292.872061491013, 'accumulated_eval_time': 3702.630370616913, 'accumulated_logging_time': 1.2235479354858398}
I0305 23:23:24.377884 139846176491264 logging_writer.py:48] [54557] accumulated_eval_time=3702.63, accumulated_logging_time=1.22355, accumulated_submission_time=11292.9, global_step=54557, preemption_count=0, score=11292.9, test/accuracy=0.985476, test/loss=0.0491838, test/mean_average_precision=0.213959, test/num_examples=43793, total_duration=14998.2, train/accuracy=0.990773, train/loss=0.0312702, train/mean_average_precision=0.348298, validation/accuracy=0.986477, validation/loss=0.0461627, validation/mean_average_precision=0.220062, validation/num_examples=43793
I0305 23:23:33.789762 139846059546368 logging_writer.py:48] [54600] global_step=54600, grad_norm=0.1707439124584198, loss=0.03265441581606865
I0305 23:23:55.011918 139846176491264 logging_writer.py:48] [54700] global_step=54700, grad_norm=0.16400104761123657, loss=0.02498171664774418
I0305 23:24:16.244509 139846059546368 logging_writer.py:48] [54800] global_step=54800, grad_norm=0.16579125821590424, loss=0.02983112819492817
I0305 23:24:36.930715 139846176491264 logging_writer.py:48] [54900] global_step=54900, grad_norm=0.284776508808136, loss=0.03102688118815422
I0305 23:24:57.643134 139846059546368 logging_writer.py:48] [55000] global_step=55000, grad_norm=0.21592722833156586, loss=0.030102744698524475
I0305 23:25:18.824148 139846176491264 logging_writer.py:48] [55100] global_step=55100, grad_norm=0.16140642762184143, loss=0.028024321421980858
I0305 23:25:39.830677 139846059546368 logging_writer.py:48] [55200] global_step=55200, grad_norm=0.21523916721343994, loss=0.029909800738096237
I0305 23:26:00.739862 139846176491264 logging_writer.py:48] [55300] global_step=55300, grad_norm=0.24648749828338623, loss=0.03364962711930275
I0305 23:26:21.996462 139846059546368 logging_writer.py:48] [55400] global_step=55400, grad_norm=0.14277154207229614, loss=0.029225945472717285
I0305 23:26:42.688555 139846176491264 logging_writer.py:48] [55500] global_step=55500, grad_norm=0.15339800715446472, loss=0.029618216678500175
I0305 23:27:03.489751 139846059546368 logging_writer.py:48] [55600] global_step=55600, grad_norm=0.17932116985321045, loss=0.03100435435771942
I0305 23:27:24.360515 139846176491264 logging_writer.py:48] [55700] global_step=55700, grad_norm=0.1471450924873352, loss=0.029555216431617737
I0305 23:27:24.365899 139988053398720 spec.py:321] Evaluating on the training split.
I0305 23:28:37.184427 139988053398720 spec.py:333] Evaluating on the validation split.
I0305 23:28:39.135051 139988053398720 spec.py:349] Evaluating on the test split.
I0305 23:28:41.019167 139988053398720 submission_runner.py:469] Time since start: 15314.84s, 	Step: 55701, 	{'train/accuracy': 0.9908541440963745, 'train/loss': 0.030904466286301613, 'train/mean_average_precision': 0.3425191979481913, 'validation/accuracy': 0.986477255821228, 'validation/loss': 0.04616272822022438, 'validation/mean_average_precision': 0.2201147637976517, 'validation/num_examples': 43793, 'test/accuracy': 0.9854763150215149, 'test/loss': 0.04918375611305237, 'test/mean_average_precision': 0.21386034899082315, 'test/num_examples': 43793, 'score': 11532.818925619125, 'total_duration': 15314.835664510727, 'accumulated_submission_time': 11532.818925619125, 'accumulated_eval_time': 3779.283523082733, 'accumulated_logging_time': 1.2484877109527588}
I0305 23:28:41.034112 139846059546368 logging_writer.py:48] [55701] accumulated_eval_time=3779.28, accumulated_logging_time=1.24849, accumulated_submission_time=11532.8, global_step=55701, preemption_count=0, score=11532.8, test/accuracy=0.985476, test/loss=0.0491838, test/mean_average_precision=0.21386, test/num_examples=43793, total_duration=15314.8, train/accuracy=0.990854, train/loss=0.0309045, train/mean_average_precision=0.342519, validation/accuracy=0.986477, validation/loss=0.0461627, validation/mean_average_precision=0.220115, validation/num_examples=43793
I0305 23:29:02.058055 139846176491264 logging_writer.py:48] [55800] global_step=55800, grad_norm=0.1603015810251236, loss=0.02909688651561737
I0305 23:29:23.125077 139846059546368 logging_writer.py:48] [55900] global_step=55900, grad_norm=0.2763565480709076, loss=0.031702857464551926
I0305 23:29:44.127497 139846176491264 logging_writer.py:48] [56000] global_step=56000, grad_norm=0.37605851888656616, loss=0.02802935428917408
I0305 23:30:05.047318 139846059546368 logging_writer.py:48] [56100] global_step=56100, grad_norm=0.16867221891880035, loss=0.032066941261291504
I0305 23:30:25.755714 139846176491264 logging_writer.py:48] [56200] global_step=56200, grad_norm=0.1700664609670639, loss=0.03106839396059513
I0305 23:30:46.688322 139846059546368 logging_writer.py:48] [56300] global_step=56300, grad_norm=0.21415993571281433, loss=0.03220440074801445
I0305 23:31:07.759004 139846176491264 logging_writer.py:48] [56400] global_step=56400, grad_norm=0.10895011574029922, loss=0.028019005432724953
I0305 23:31:28.791438 139846059546368 logging_writer.py:48] [56500] global_step=56500, grad_norm=0.1889495551586151, loss=0.03418128937482834
I0305 23:31:49.749117 139846176491264 logging_writer.py:48] [56600] global_step=56600, grad_norm=0.1443932205438614, loss=0.03195905312895775
I0305 23:32:10.500067 139846059546368 logging_writer.py:48] [56700] global_step=56700, grad_norm=0.13531003892421722, loss=0.03160678967833519
I0305 23:32:31.894126 139846176491264 logging_writer.py:48] [56800] global_step=56800, grad_norm=0.1793864220380783, loss=0.030875643715262413
I0305 23:32:41.021537 139988053398720 spec.py:321] Evaluating on the training split.
I0305 23:33:49.581524 139988053398720 spec.py:333] Evaluating on the validation split.
I0305 23:33:51.728455 139988053398720 spec.py:349] Evaluating on the test split.
I0305 23:33:53.770915 139988053398720 submission_runner.py:469] Time since start: 15627.59s, 	Step: 56843, 	{'train/accuracy': 0.9906882047653198, 'train/loss': 0.03134234622120857, 'train/mean_average_precision': 0.338537350955308, 'validation/accuracy': 0.986477255821228, 'validation/loss': 0.04616272449493408, 'validation/mean_average_precision': 0.22010040855786156, 'validation/num_examples': 43793, 'test/accuracy': 0.9854763150215149, 'test/loss': 0.04918375611305237, 'test/mean_average_precision': 0.2138619621843709, 'test/num_examples': 43793, 'score': 11772.763296842575, 'total_duration': 15627.58739900589, 'accumulated_submission_time': 11772.763296842575, 'accumulated_eval_time': 3852.0327825546265, 'accumulated_logging_time': 1.2727749347686768}
I0305 23:33:53.787883 139846059546368 logging_writer.py:48] [56843] accumulated_eval_time=3852.03, accumulated_logging_time=1.27277, accumulated_submission_time=11772.8, global_step=56843, preemption_count=0, score=11772.8, test/accuracy=0.985476, test/loss=0.0491838, test/mean_average_precision=0.213862, test/num_examples=43793, total_duration=15627.6, train/accuracy=0.990688, train/loss=0.0313423, train/mean_average_precision=0.338537, validation/accuracy=0.986477, validation/loss=0.0461627, validation/mean_average_precision=0.2201, validation/num_examples=43793
I0305 23:34:06.009772 139846176491264 logging_writer.py:48] [56900] global_step=56900, grad_norm=0.2416808158159256, loss=0.0269891619682312
I0305 23:34:27.476331 139846059546368 logging_writer.py:48] [57000] global_step=57000, grad_norm=0.1877238154411316, loss=0.029546277597546577
I0305 23:34:48.522485 139846176491264 logging_writer.py:48] [57100] global_step=57100, grad_norm=0.2737290561199188, loss=0.03108242154121399
I0305 23:35:09.567946 139846059546368 logging_writer.py:48] [57200] global_step=57200, grad_norm=0.22628192603588104, loss=0.033202044665813446
I0305 23:35:30.624433 139846176491264 logging_writer.py:48] [57300] global_step=57300, grad_norm=0.13567820191383362, loss=0.03218856081366539
I0305 23:35:51.732625 139846059546368 logging_writer.py:48] [57400] global_step=57400, grad_norm=0.134914368391037, loss=0.028276216238737106
I0305 23:36:12.440285 139846176491264 logging_writer.py:48] [57500] global_step=57500, grad_norm=0.1327863484621048, loss=0.032837774604558945
I0305 23:36:33.283940 139846059546368 logging_writer.py:48] [57600] global_step=57600, grad_norm=0.1780575066804886, loss=0.03173588961362839
I0305 23:36:54.145795 139846176491264 logging_writer.py:48] [57700] global_step=57700, grad_norm=0.17741675674915314, loss=0.02909337356686592
I0305 23:37:14.723992 139846059546368 logging_writer.py:48] [57800] global_step=57800, grad_norm=0.14213871955871582, loss=0.03227373957633972
I0305 23:37:35.222315 139846176491264 logging_writer.py:48] [57900] global_step=57900, grad_norm=0.24385063350200653, loss=0.0316862128674984
I0305 23:37:53.827733 139846059546368 logging_writer.py:48] [57991] global_step=57991, preemption_count=0, score=12012.8
I0305 23:37:53.966513 139988053398720 submission_runner.py:646] Tuning trial 4/5
I0305 23:37:53.966689 139988053398720 submission_runner.py:647] Hyperparameters: Hyperparameters(dropout_rate=0.0, label_smoothing=0.0, learning_rate=0.004958460849689891, one_minus_beta1=0.13625575743, beta2=0.6291854735396584, weight_decay=0.1147386261512052, warmup_factor=0.02)
I0305 23:37:53.968271 139988053398720 submission_runner.py:648] Metrics: {'eval_results': [(1, {'train/accuracy': 0.5364514589309692, 'train/loss': 0.7556861639022827, 'train/mean_average_precision': 0.02393405499105412, 'validation/accuracy': 0.5388414263725281, 'validation/loss': 0.7555237412452698, 'validation/mean_average_precision': 0.02782094985628063, 'validation/num_examples': 43793, 'test/accuracy': 0.5390405058860779, 'test/loss': 0.7549278736114502, 'test/mean_average_precision': 0.02926252684263866, 'test/num_examples': 43793, 'score': 10.719719886779785, 'total_duration': 213.61053848266602, 'accumulated_submission_time': 10.719719886779785, 'accumulated_eval_time': 202.89071488380432, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1170, {'train/accuracy': 0.9867745041847229, 'train/loss': 0.05368809401988983, 'train/mean_average_precision': 0.04189938078535591, 'validation/accuracy': 0.9841674566268921, 'validation/loss': 0.06388798356056213, 'validation/mean_average_precision': 0.03978796145292936, 'validation/num_examples': 43793, 'test/accuracy': 0.9831724166870117, 'test/loss': 0.06711962819099426, 'test/mean_average_precision': 0.041355002685032234, 'test/num_examples': 43793, 'score': 250.8117754459381, 'total_duration': 529.8291528224945, 'accumulated_submission_time': 250.8117754459381, 'accumulated_eval_time': 278.96765327453613, 'accumulated_logging_time': 0.018632888793945312, 'global_step': 1170, 'preemption_count': 0}), (2324, {'train/accuracy': 0.9869147539138794, 'train/loss': 0.05123446136713028, 'train/mean_average_precision': 0.05487954520788056, 'validation/accuracy': 0.984216570854187, 'validation/loss': 0.06142273172736168, 'validation/mean_average_precision': 0.05409831568994666, 'validation/num_examples': 43793, 'test/accuracy': 0.9832220673561096, 'test/loss': 0.06460878252983093, 'test/mean_average_precision': 0.05614455724377822, 'test/num_examples': 43793, 'score': 490.86257910728455, 'total_duration': 846.2048377990723, 'accumulated_submission_time': 490.86257910728455, 'accumulated_eval_time': 355.2419879436493, 'accumulated_logging_time': 0.035897254943847656, 'global_step': 2324, 'preemption_count': 0}), (3497, {'train/accuracy': 0.9871610403060913, 'train/loss': 0.04742623493075371, 'train/mean_average_precision': 0.0965885717008074, 'validation/accuracy': 0.9845405220985413, 'validation/loss': 0.05695180222392082, 'validation/mean_average_precision': 0.09142563291241514, 'validation/num_examples': 43793, 'test/accuracy': 0.9835548400878906, 'test/loss': 0.060451168566942215, 'test/mean_average_precision': 0.09165636591552101, 'test/num_examples': 43793, 'score': 730.8458979129791, 'total_duration': 1161.0410740375519, 'accumulated_submission_time': 730.8458979129791, 'accumulated_eval_time': 430.0478582382202, 'accumulated_logging_time': 0.0531158447265625, 'global_step': 3497, 'preemption_count': 0}), (4677, {'train/accuracy': 0.9877002239227295, 'train/loss': 0.04505211114883423, 'train/mean_average_precision': 0.12453795896404228, 'validation/accuracy': 0.9848470091819763, 'validation/loss': 0.05384093523025513, 'validation/mean_average_precision': 0.11726347991929706, 'validation/num_examples': 43793, 'test/accuracy': 0.9839099049568176, 'test/loss': 0.0565769337117672, 'test/mean_average_precision': 0.11790388623984789, 'test/num_examples': 43793, 'score': 970.9864010810852, 'total_duration': 1477.3860812187195, 'accumulated_submission_time': 970.9864010810852, 'accumulated_eval_time': 506.2025966644287, 'accumulated_logging_time': 0.07101869583129883, 'global_step': 4677, 'preemption_count': 0}), (5851, {'train/accuracy': 0.9878891110420227, 'train/loss': 0.0430062934756279, 'train/mean_average_precision': 0.14275915561946215, 'validation/accuracy': 0.984795868396759, 'validation/loss': 0.05284382775425911, 'validation/mean_average_precision': 0.1292960581628193, 'validation/num_examples': 43793, 'test/accuracy': 0.9838420748710632, 'test/loss': 0.05547240376472473, 'test/mean_average_precision': 0.13955263815440438, 'test/num_examples': 43793, 'score': 1210.9637596607208, 'total_duration': 1791.361675977707, 'accumulated_submission_time': 1210.9637596607208, 'accumulated_eval_time': 580.1499774456024, 'accumulated_logging_time': 0.0892038345336914, 'global_step': 5851, 'preemption_count': 0}), (7025, {'train/accuracy': 0.988011360168457, 'train/loss': 0.04190972074866295, 'train/mean_average_precision': 0.16804879014885216, 'validation/accuracy': 0.9851494431495667, 'validation/loss': 0.0512109138071537, 'validation/mean_average_precision': 0.1440025198423024, 'validation/num_examples': 43793, 'test/accuracy': 0.9842514991760254, 'test/loss': 0.05385816842317581, 'test/mean_average_precision': 0.14748185814655518, 'test/num_examples': 43793, 'score': 1450.9160923957825, 'total_duration': 2107.0585041046143, 'accumulated_submission_time': 1450.9160923957825, 'accumulated_eval_time': 655.8449256420135, 'accumulated_logging_time': 0.10793399810791016, 'global_step': 7025, 'preemption_count': 0}), (8191, {'train/accuracy': 0.9882838129997253, 'train/loss': 0.04094330966472626, 'train/mean_average_precision': 0.17232705252527517, 'validation/accuracy': 0.9853893518447876, 'validation/loss': 0.05049822852015495, 'validation/mean_average_precision': 0.15103565161438587, 'validation/num_examples': 43793, 'test/accuracy': 0.9844840168952942, 'test/loss': 0.053228601813316345, 'test/mean_average_precision': 0.15283918803018304, 'test/num_examples': 43793, 'score': 1691.0586531162262, 'total_duration': 2420.4182710647583, 'accumulated_submission_time': 1691.0586531162262, 'accumulated_eval_time': 729.0132117271423, 'accumulated_logging_time': 0.12584805488586426, 'global_step': 8191, 'preemption_count': 0}), (9364, {'train/accuracy': 0.9884957075119019, 'train/loss': 0.0406404584646225, 'train/mean_average_precision': 0.18594714553198588, 'validation/accuracy': 0.9852492809295654, 'validation/loss': 0.05092266574501991, 'validation/mean_average_precision': 0.1559057381398793, 'validation/num_examples': 43793, 'test/accuracy': 0.9843230843544006, 'test/loss': 0.053471677005290985, 'test/mean_average_precision': 0.15164588773075652, 'test/num_examples': 43793, 'score': 1931.123347043991, 'total_duration': 2736.8301243782043, 'accumulated_submission_time': 1931.123347043991, 'accumulated_eval_time': 805.3129160404205, 'accumulated_logging_time': 0.1447131633758545, 'global_step': 9364, 'preemption_count': 0}), (10526, {'train/accuracy': 0.98842453956604, 'train/loss': 0.040268488228321075, 'train/mean_average_precision': 0.1832961713946283, 'validation/accuracy': 0.9854104518890381, 'validation/loss': 0.0497840940952301, 'validation/mean_average_precision': 0.1578549479765775, 'validation/num_examples': 43793, 'test/accuracy': 0.9844090342521667, 'test/loss': 0.05271275341510773, 'test/mean_average_precision': 0.15375624163507193, 'test/num_examples': 43793, 'score': 2171.1359186172485, 'total_duration': 3052.824548959732, 'accumulated_submission_time': 2171.1359186172485, 'accumulated_eval_time': 881.2417764663696, 'accumulated_logging_time': 0.16405224800109863, 'global_step': 10526, 'preemption_count': 0}), (11704, {'train/accuracy': 0.9886184930801392, 'train/loss': 0.039202991873025894, 'train/mean_average_precision': 0.19840598032511872, 'validation/accuracy': 0.9855521321296692, 'validation/loss': 0.04955240711569786, 'validation/mean_average_precision': 0.16933319193913793, 'validation/num_examples': 43793, 'test/accuracy': 0.9846259355545044, 'test/loss': 0.052462127059698105, 'test/mean_average_precision': 0.15793779329496552, 'test/num_examples': 43793, 'score': 2411.153899669647, 'total_duration': 3366.200874567032, 'accumulated_submission_time': 2411.153899669647, 'accumulated_eval_time': 954.5483994483948, 'accumulated_logging_time': 0.18379735946655273, 'global_step': 11704, 'preemption_count': 0}), (12881, {'train/accuracy': 0.9886207580566406, 'train/loss': 0.03934444859623909, 'train/mean_average_precision': 0.19868579731807856, 'validation/accuracy': 0.9856162667274475, 'validation/loss': 0.05002313479781151, 'validation/mean_average_precision': 0.16834695388139903, 'validation/num_examples': 43793, 'test/accuracy': 0.9846171140670776, 'test/loss': 0.0532895028591156, 'test/mean_average_precision': 0.16455377091777112, 'test/num_examples': 43793, 'score': 2651.151495695114, 'total_duration': 3680.455201148987, 'accumulated_submission_time': 2651.151495695114, 'accumulated_eval_time': 1028.7567501068115, 'accumulated_logging_time': 0.2025771141052246, 'global_step': 12881, 'preemption_count': 0}), (14055, {'train/accuracy': 0.9885795712471008, 'train/loss': 0.039418600499629974, 'train/mean_average_precision': 0.21050105707368613, 'validation/accuracy': 0.9854084253311157, 'validation/loss': 0.049398649483919144, 'validation/mean_average_precision': 0.176549990564398, 'validation/num_examples': 43793, 'test/accuracy': 0.9845244288444519, 'test/loss': 0.052014779299497604, 'test/mean_average_precision': 0.16506331970775495, 'test/num_examples': 43793, 'score': 2891.1129665374756, 'total_duration': 3997.022296190262, 'accumulated_submission_time': 2891.1129665374756, 'accumulated_eval_time': 1105.3104910850525, 'accumulated_logging_time': 0.22135496139526367, 'global_step': 14055, 'preemption_count': 0}), (15227, {'train/accuracy': 0.9887218475341797, 'train/loss': 0.038751356303691864, 'train/mean_average_precision': 0.20543755742533734, 'validation/accuracy': 0.9856694340705872, 'validation/loss': 0.048988793045282364, 'validation/mean_average_precision': 0.17542661194392786, 'validation/num_examples': 43793, 'test/accuracy': 0.9846916198730469, 'test/loss': 0.051944900304079056, 'test/mean_average_precision': 0.16663763202110018, 'test/num_examples': 43793, 'score': 3131.066616296768, 'total_duration': 4310.054613828659, 'accumulated_submission_time': 3131.066616296768, 'accumulated_eval_time': 1178.3379123210907, 'accumulated_logging_time': 0.24144697189331055, 'global_step': 15227, 'preemption_count': 0}), (16402, {'train/accuracy': 0.9890241026878357, 'train/loss': 0.03790368139743805, 'train/mean_average_precision': 0.2182404400714565, 'validation/accuracy': 0.985772967338562, 'validation/loss': 0.04821858927607536, 'validation/mean_average_precision': 0.18833870804909034, 'validation/num_examples': 43793, 'test/accuracy': 0.9848487377166748, 'test/loss': 0.05097391828894615, 'test/mean_average_precision': 0.17914398533093895, 'test/num_examples': 43793, 'score': 3371.208928346634, 'total_duration': 4622.755227565765, 'accumulated_submission_time': 3371.208928346634, 'accumulated_eval_time': 1250.8468046188354, 'accumulated_logging_time': 0.2601125240325928, 'global_step': 16402, 'preemption_count': 0}), (17566, {'train/accuracy': 0.9888108372688293, 'train/loss': 0.0385148748755455, 'train/mean_average_precision': 0.2188880467168281, 'validation/accuracy': 0.9858338236808777, 'validation/loss': 0.048987001180648804, 'validation/mean_average_precision': 0.18559830936285188, 'validation/num_examples': 43793, 'test/accuracy': 0.9848302006721497, 'test/loss': 0.052273400127887726, 'test/mean_average_precision': 0.17566398495269384, 'test/num_examples': 43793, 'score': 3611.1944913864136, 'total_duration': 4938.400219202042, 'accumulated_submission_time': 3611.1944913864136, 'accumulated_eval_time': 1326.4550292491913, 'accumulated_logging_time': 0.2806370258331299, 'global_step': 17566, 'preemption_count': 0}), (18738, {'train/accuracy': 0.9889689087867737, 'train/loss': 0.038197655230760574, 'train/mean_average_precision': 0.21426485858750888, 'validation/accuracy': 0.9857096076011658, 'validation/loss': 0.04854295775294304, 'validation/mean_average_precision': 0.17903267046195645, 'validation/num_examples': 43793, 'test/accuracy': 0.9848066568374634, 'test/loss': 0.0511590912938118, 'test/mean_average_precision': 0.18047951708964088, 'test/num_examples': 43793, 'score': 3851.188839673996, 'total_duration': 5252.672187328339, 'accumulated_submission_time': 3851.188839673996, 'accumulated_eval_time': 1400.681452035904, 'accumulated_logging_time': 0.3010241985321045, 'global_step': 18738, 'preemption_count': 0}), (19902, {'train/accuracy': 0.9888707995414734, 'train/loss': 0.038242366164922714, 'train/mean_average_precision': 0.21886045379799754, 'validation/accuracy': 0.9857518672943115, 'validation/loss': 0.04844534397125244, 'validation/mean_average_precision': 0.19113705513416904, 'validation/num_examples': 43793, 'test/accuracy': 0.9848415851593018, 'test/loss': 0.051301259547472, 'test/mean_average_precision': 0.18076723356106775, 'test/num_examples': 43793, 'score': 4091.1910738945007, 'total_duration': 5564.250961780548, 'accumulated_submission_time': 4091.1910738945007, 'accumulated_eval_time': 1472.2048263549805, 'accumulated_logging_time': 0.320357084274292, 'global_step': 19902, 'preemption_count': 0}), (21057, {'train/accuracy': 0.988986611366272, 'train/loss': 0.03821277618408203, 'train/mean_average_precision': 0.21373003196647805, 'validation/accuracy': 0.9855651259422302, 'validation/loss': 0.048738084733486176, 'validation/mean_average_precision': 0.18276537175967933, 'validation/num_examples': 43793, 'test/accuracy': 0.984652042388916, 'test/loss': 0.05143832042813301, 'test/mean_average_precision': 0.17176403313895092, 'test/num_examples': 43793, 'score': 4331.299253463745, 'total_duration': 5881.8668212890625, 'accumulated_submission_time': 4331.299253463745, 'accumulated_eval_time': 1549.6570563316345, 'accumulated_logging_time': 0.34147047996520996, 'global_step': 21057, 'preemption_count': 0}), (22219, {'train/accuracy': 0.9891259074211121, 'train/loss': 0.0376235656440258, 'train/mean_average_precision': 0.22785225583877694, 'validation/accuracy': 0.9857977032661438, 'validation/loss': 0.04801959916949272, 'validation/mean_average_precision': 0.1813642525270148, 'validation/num_examples': 43793, 'test/accuracy': 0.9848260283470154, 'test/loss': 0.050993192940950394, 'test/mean_average_precision': 0.18087460850697226, 'test/num_examples': 43793, 'score': 4571.439974784851, 'total_duration': 6198.532079219818, 'accumulated_submission_time': 4571.439974784851, 'accumulated_eval_time': 1626.1317908763885, 'accumulated_logging_time': 0.3621251583099365, 'global_step': 22219, 'preemption_count': 0}), (23378, {'train/accuracy': 0.9890457987785339, 'train/loss': 0.03764975070953369, 'train/mean_average_precision': 0.22808872299399327, 'validation/accuracy': 0.9858143329620361, 'validation/loss': 0.0485360249876976, 'validation/mean_average_precision': 0.1887411589745791, 'validation/num_examples': 43793, 'test/accuracy': 0.9848390817642212, 'test/loss': 0.051560405641794205, 'test/mean_average_precision': 0.17769738352498465, 'test/num_examples': 43793, 'score': 4811.442640304565, 'total_duration': 6513.0462827682495, 'accumulated_submission_time': 4811.442640304565, 'accumulated_eval_time': 1700.5911021232605, 'accumulated_logging_time': 0.3840799331665039, 'global_step': 23378, 'preemption_count': 0}), (24538, {'train/accuracy': 0.9890886545181274, 'train/loss': 0.03725675120949745, 'train/mean_average_precision': 0.22796853059917538, 'validation/accuracy': 0.98587566614151, 'validation/loss': 0.047811109572649, 'validation/mean_average_precision': 0.18768612770648666, 'validation/num_examples': 43793, 'test/accuracy': 0.9849616289138794, 'test/loss': 0.05080631002783775, 'test/mean_average_precision': 0.18156549575173453, 'test/num_examples': 43793, 'score': 5051.400331020355, 'total_duration': 6826.1713988780975, 'accumulated_submission_time': 5051.400331020355, 'accumulated_eval_time': 1773.7070503234863, 'accumulated_logging_time': 0.40456271171569824, 'global_step': 24538, 'preemption_count': 0}), (25700, {'train/accuracy': 0.989041268825531, 'train/loss': 0.037431370466947556, 'train/mean_average_precision': 0.2412315383747457, 'validation/accuracy': 0.9858322143554688, 'validation/loss': 0.047944873571395874, 'validation/mean_average_precision': 0.18509168020709726, 'validation/num_examples': 43793, 'test/accuracy': 0.9849346876144409, 'test/loss': 0.05067699775099754, 'test/mean_average_precision': 0.18198308280430534, 'test/num_examples': 43793, 'score': 5291.562994718552, 'total_duration': 7143.0613622665405, 'accumulated_submission_time': 5291.562994718552, 'accumulated_eval_time': 1850.3846244812012, 'accumulated_logging_time': 0.4251689910888672, 'global_step': 25700, 'preemption_count': 0}), (26852, {'train/accuracy': 0.989129364490509, 'train/loss': 0.03711039572954178, 'train/mean_average_precision': 0.23456275490941603, 'validation/accuracy': 0.9859040975570679, 'validation/loss': 0.048020441085100174, 'validation/mean_average_precision': 0.19372449818536092, 'validation/num_examples': 43793, 'test/accuracy': 0.9850168228149414, 'test/loss': 0.05087914690375328, 'test/mean_average_precision': 0.18737258356307257, 'test/num_examples': 43793, 'score': 5531.623912334442, 'total_duration': 7456.713232040405, 'accumulated_submission_time': 5531.623912334442, 'accumulated_eval_time': 1923.924885749817, 'accumulated_logging_time': 0.44628334045410156, 'global_step': 26852, 'preemption_count': 0}), (28000, {'train/accuracy': 0.9892706274986267, 'train/loss': 0.0367145873606205, 'train/mean_average_precision': 0.24511939084761594, 'validation/accuracy': 0.985965371131897, 'validation/loss': 0.048049941658973694, 'validation/mean_average_precision': 0.1957977600210574, 'validation/num_examples': 43793, 'test/accuracy': 0.9849944710731506, 'test/loss': 0.051086243242025375, 'test/mean_average_precision': 0.19025219529858906, 'test/num_examples': 43793, 'score': 5771.741902112961, 'total_duration': 7775.2387499809265, 'accumulated_submission_time': 5771.741902112961, 'accumulated_eval_time': 2002.2801306247711, 'accumulated_logging_time': 0.4685328006744385, 'global_step': 28000, 'preemption_count': 0}), (29148, {'train/accuracy': 0.9893772006034851, 'train/loss': 0.036354247480630875, 'train/mean_average_precision': 0.24206317312501985, 'validation/accuracy': 0.9857871532440186, 'validation/loss': 0.047526951879262924, 'validation/mean_average_precision': 0.19392047282080271, 'validation/num_examples': 43793, 'test/accuracy': 0.98490309715271, 'test/loss': 0.050181154161691666, 'test/mean_average_precision': 0.19252115434265699, 'test/num_examples': 43793, 'score': 6011.524959087372, 'total_duration': 8086.165498018265, 'accumulated_submission_time': 6011.524959087372, 'accumulated_eval_time': 2073.157628774643, 'accumulated_logging_time': 0.7050776481628418, 'global_step': 29148, 'preemption_count': 0}), (30303, {'train/accuracy': 0.9892402291297913, 'train/loss': 0.03663342446088791, 'train/mean_average_precision': 0.25100917700047254, 'validation/accuracy': 0.9858322143554688, 'validation/loss': 0.047663964331150055, 'validation/mean_average_precision': 0.19620843662474877, 'validation/num_examples': 43793, 'test/accuracy': 0.9850509166717529, 'test/loss': 0.050372447818517685, 'test/mean_average_precision': 0.19209507133721085, 'test/num_examples': 43793, 'score': 6251.50754237175, 'total_duration': 8403.235912322998, 'accumulated_submission_time': 6251.50754237175, 'accumulated_eval_time': 2150.1934123039246, 'accumulated_logging_time': 0.7270264625549316, 'global_step': 30303, 'preemption_count': 0}), (31462, {'train/accuracy': 0.9895560145378113, 'train/loss': 0.03567924723029137, 'train/mean_average_precision': 0.25629327388823, 'validation/accuracy': 0.986055314540863, 'validation/loss': 0.04710887372493744, 'validation/mean_average_precision': 0.1977694902645749, 'validation/num_examples': 43793, 'test/accuracy': 0.985149085521698, 'test/loss': 0.05007486790418625, 'test/mean_average_precision': 0.19130880399459851, 'test/num_examples': 43793, 'score': 6491.557395219803, 'total_duration': 8718.990085363388, 'accumulated_submission_time': 6491.557395219803, 'accumulated_eval_time': 2225.844567298889, 'accumulated_logging_time': 0.7502727508544922, 'global_step': 31462, 'preemption_count': 0}), (32615, {'train/accuracy': 0.989496111869812, 'train/loss': 0.03585892170667648, 'train/mean_average_precision': 0.26011119800579996, 'validation/accuracy': 0.9860924482345581, 'validation/loss': 0.04692311957478523, 'validation/mean_average_precision': 0.20180565323384847, 'validation/num_examples': 43793, 'test/accuracy': 0.9851717948913574, 'test/loss': 0.04963897168636322, 'test/mean_average_precision': 0.19559575880449237, 'test/num_examples': 43793, 'score': 6731.657792329788, 'total_duration': 9035.544276952744, 'accumulated_submission_time': 6731.657792329788, 'accumulated_eval_time': 2302.237907409668, 'accumulated_logging_time': 0.7803506851196289, 'global_step': 32615, 'preemption_count': 0}), (33781, {'train/accuracy': 0.9896261096000671, 'train/loss': 0.03556752949953079, 'train/mean_average_precision': 0.26079889448724175, 'validation/accuracy': 0.9861310124397278, 'validation/loss': 0.04691952094435692, 'validation/mean_average_precision': 0.19875130409296146, 'validation/num_examples': 43793, 'test/accuracy': 0.985261082649231, 'test/loss': 0.04959997162222862, 'test/mean_average_precision': 0.19764358318239023, 'test/num_examples': 43793, 'score': 6971.704412698746, 'total_duration': 9349.272918462753, 'accumulated_submission_time': 6971.704412698746, 'accumulated_eval_time': 2375.86523103714, 'accumulated_logging_time': 0.8034987449645996, 'global_step': 33781, 'preemption_count': 0}), (34943, {'train/accuracy': 0.9896506667137146, 'train/loss': 0.03525419905781746, 'train/mean_average_precision': 0.26847102401453926, 'validation/accuracy': 0.9859714508056641, 'validation/loss': 0.04706370458006859, 'validation/mean_average_precision': 0.20236771470246812, 'validation/num_examples': 43793, 'test/accuracy': 0.9851393699645996, 'test/loss': 0.04985686019062996, 'test/mean_average_precision': 0.19588938903801556, 'test/num_examples': 43793, 'score': 7211.712254047394, 'total_duration': 9664.072291851044, 'accumulated_submission_time': 7211.712254047394, 'accumulated_eval_time': 2450.6025428771973, 'accumulated_logging_time': 0.8256773948669434, 'global_step': 34943, 'preemption_count': 0}), (36104, {'train/accuracy': 0.9897321462631226, 'train/loss': 0.035038650035858154, 'train/mean_average_precision': 0.27568729045025897, 'validation/accuracy': 0.9859791994094849, 'validation/loss': 0.046800386160612106, 'validation/mean_average_precision': 0.2040558492030353, 'validation/num_examples': 43793, 'test/accuracy': 0.985144853591919, 'test/loss': 0.04945572465658188, 'test/mean_average_precision': 0.19676224304716503, 'test/num_examples': 43793, 'score': 7451.861115217209, 'total_duration': 9978.891277074814, 'accumulated_submission_time': 7451.861115217209, 'accumulated_eval_time': 2525.220118045807, 'accumulated_logging_time': 0.8474617004394531, 'global_step': 36104, 'preemption_count': 0}), (37262, {'train/accuracy': 0.9898052215576172, 'train/loss': 0.03457319363951683, 'train/mean_average_precision': 0.2811102362806023, 'validation/accuracy': 0.9862211346626282, 'validation/loss': 0.04660797864198685, 'validation/mean_average_precision': 0.21265516007284108, 'validation/num_examples': 43793, 'test/accuracy': 0.9853259921073914, 'test/loss': 0.04942508041858673, 'test/mean_average_precision': 0.20540918149965803, 'test/num_examples': 43793, 'score': 7691.857767343521, 'total_duration': 10291.900625944138, 'accumulated_submission_time': 7691.857767343521, 'accumulated_eval_time': 2598.178633213043, 'accumulated_logging_time': 0.8701357841491699, 'global_step': 37262, 'preemption_count': 0}), (38417, {'train/accuracy': 0.9898331761360168, 'train/loss': 0.03427600860595703, 'train/mean_average_precision': 0.2851992314436448, 'validation/accuracy': 0.9862880706787109, 'validation/loss': 0.046690601855516434, 'validation/mean_average_precision': 0.21053127933967072, 'validation/num_examples': 43793, 'test/accuracy': 0.9853900074958801, 'test/loss': 0.04969640448689461, 'test/mean_average_precision': 0.20706954485244747, 'test/num_examples': 43793, 'score': 7931.926340341568, 'total_duration': 10603.97923707962, 'accumulated_submission_time': 7931.926340341568, 'accumulated_eval_time': 2670.134476184845, 'accumulated_logging_time': 0.8924276828765869, 'global_step': 38417, 'preemption_count': 0}), (39567, {'train/accuracy': 0.9899880886077881, 'train/loss': 0.034090906381607056, 'train/mean_average_precision': 0.281206312688988, 'validation/accuracy': 0.9862925410270691, 'validation/loss': 0.04625346511602402, 'validation/mean_average_precision': 0.20642464278309228, 'validation/num_examples': 43793, 'test/accuracy': 0.9853954315185547, 'test/loss': 0.049186039716005325, 'test/mean_average_precision': 0.20785768893580164, 'test/num_examples': 43793, 'score': 8172.000776052475, 'total_duration': 10916.520744085312, 'accumulated_submission_time': 8172.000776052475, 'accumulated_eval_time': 2742.5460517406464, 'accumulated_logging_time': 0.9154491424560547, 'global_step': 39567, 'preemption_count': 0}), (40726, {'train/accuracy': 0.9902454018592834, 'train/loss': 0.03296717628836632, 'train/mean_average_precision': 0.3051772376021655, 'validation/accuracy': 0.9862491488456726, 'validation/loss': 0.046498071402311325, 'validation/mean_average_precision': 0.21170605969588732, 'validation/num_examples': 43793, 'test/accuracy': 0.9853933453559875, 'test/loss': 0.04916827380657196, 'test/mean_average_precision': 0.2044074590811802, 'test/num_examples': 43793, 'score': 8411.995926380157, 'total_duration': 11227.485052347183, 'accumulated_submission_time': 8411.995926380157, 'accumulated_eval_time': 2813.463989496231, 'accumulated_logging_time': 0.9371750354766846, 'global_step': 40726, 'preemption_count': 0}), (41886, {'train/accuracy': 0.9901771545410156, 'train/loss': 0.03334222733974457, 'train/mean_average_precision': 0.301136121843531, 'validation/accuracy': 0.9863836765289307, 'validation/loss': 0.0462188757956028, 'validation/mean_average_precision': 0.2128808637434054, 'validation/num_examples': 43793, 'test/accuracy': 0.9854426383972168, 'test/loss': 0.04929576441645622, 'test/mean_average_precision': 0.2059048086831771, 'test/num_examples': 43793, 'score': 8652.101371049881, 'total_duration': 11543.26158952713, 'accumulated_submission_time': 8652.101371049881, 'accumulated_eval_time': 2889.0785546302795, 'accumulated_logging_time': 0.9625513553619385, 'global_step': 41886, 'preemption_count': 0}), (43047, {'train/accuracy': 0.9904035329818726, 'train/loss': 0.03253759816288948, 'train/mean_average_precision': 0.3252771068780024, 'validation/accuracy': 0.9863542914390564, 'validation/loss': 0.046258874237537384, 'validation/mean_average_precision': 0.21223484665383152, 'validation/num_examples': 43793, 'test/accuracy': 0.9854510426521301, 'test/loss': 0.04899241775274277, 'test/mean_average_precision': 0.20974341672531127, 'test/num_examples': 43793, 'score': 8892.135527133942, 'total_duration': 11855.3903028965, 'accumulated_submission_time': 8892.135527133942, 'accumulated_eval_time': 2961.1215398311615, 'accumulated_logging_time': 0.9855368137359619, 'global_step': 43047, 'preemption_count': 0}), (44207, {'train/accuracy': 0.9902641773223877, 'train/loss': 0.03271237760782242, 'train/mean_average_precision': 0.3065861649862181, 'validation/accuracy': 0.9864399433135986, 'validation/loss': 0.04608120396733284, 'validation/mean_average_precision': 0.22112551462443386, 'validation/num_examples': 43793, 'test/accuracy': 0.9855028390884399, 'test/loss': 0.04894053563475609, 'test/mean_average_precision': 0.21316727328209997, 'test/num_examples': 43793, 'score': 9132.289376020432, 'total_duration': 12169.66079378128, 'accumulated_submission_time': 9132.289376020432, 'accumulated_eval_time': 3035.1834678649902, 'accumulated_logging_time': 1.0099751949310303, 'global_step': 44207, 'preemption_count': 0}), (45363, {'train/accuracy': 0.9905139207839966, 'train/loss': 0.03190862014889717, 'train/mean_average_precision': 0.31863473384338836, 'validation/accuracy': 0.9864845871925354, 'validation/loss': 0.046220190823078156, 'validation/mean_average_precision': 0.21800998610601421, 'validation/num_examples': 43793, 'test/accuracy': 0.9855555295944214, 'test/loss': 0.04915323108434677, 'test/mean_average_precision': 0.212875933131114, 'test/num_examples': 43793, 'score': 9372.402907133102, 'total_duration': 12483.901703119278, 'accumulated_submission_time': 9372.402907133102, 'accumulated_eval_time': 3109.2589938640594, 'accumulated_logging_time': 1.0323193073272705, 'global_step': 45363, 'preemption_count': 0}), (46515, {'train/accuracy': 0.9905454516410828, 'train/loss': 0.03197187930345535, 'train/mean_average_precision': 0.331558288552406, 'validation/accuracy': 0.9864001274108887, 'validation/loss': 0.04629851132631302, 'validation/mean_average_precision': 0.2174071682192422, 'validation/num_examples': 43793, 'test/accuracy': 0.985417366027832, 'test/loss': 0.04922536388039589, 'test/mean_average_precision': 0.21176476129108415, 'test/num_examples': 43793, 'score': 9612.374552726746, 'total_duration': 12794.25294828415, 'accumulated_submission_time': 9612.374552726746, 'accumulated_eval_time': 3179.5840995311737, 'accumulated_logging_time': 1.0555248260498047, 'global_step': 46515, 'preemption_count': 0}), (47668, {'train/accuracy': 0.9908093214035034, 'train/loss': 0.031168807297945023, 'train/mean_average_precision': 0.34157119438706895, 'validation/accuracy': 0.9864829182624817, 'validation/loss': 0.04601713642477989, 'validation/mean_average_precision': 0.21942631558991563, 'validation/num_examples': 43793, 'test/accuracy': 0.9854881167411804, 'test/loss': 0.04896024242043495, 'test/mean_average_precision': 0.2149191535565873, 'test/num_examples': 43793, 'score': 9852.525210618973, 'total_duration': 13112.178442001343, 'accumulated_submission_time': 9852.525210618973, 'accumulated_eval_time': 3257.305377483368, 'accumulated_logging_time': 1.0797054767608643, 'global_step': 47668, 'preemption_count': 0}), (48814, {'train/accuracy': 0.9907341599464417, 'train/loss': 0.03130224719643593, 'train/mean_average_precision': 0.3504838553775989, 'validation/accuracy': 0.9865196943283081, 'validation/loss': 0.04623526707291603, 'validation/mean_average_precision': 0.22062492725021093, 'validation/num_examples': 43793, 'test/accuracy': 0.9855133891105652, 'test/loss': 0.04922596365213394, 'test/mean_average_precision': 0.21387498792728132, 'test/num_examples': 43793, 'score': 10092.654266357422, 'total_duration': 13426.044302940369, 'accumulated_submission_time': 10092.654266357422, 'accumulated_eval_time': 3330.9866740703583, 'accumulated_logging_time': 1.103475570678711, 'global_step': 48814, 'preemption_count': 0}), (49977, {'train/accuracy': 0.9908154010772705, 'train/loss': 0.030929813161492348, 'train/mean_average_precision': 0.3361994803580365, 'validation/accuracy': 0.986456573009491, 'validation/loss': 0.04615635797381401, 'validation/mean_average_precision': 0.21954298365393962, 'validation/num_examples': 43793, 'test/accuracy': 0.9854493737220764, 'test/loss': 0.04917613044381142, 'test/mean_average_precision': 0.21354416495868084, 'test/num_examples': 43793, 'score': 10332.698824167252, 'total_duration': 13743.183663845062, 'accumulated_submission_time': 10332.698824167252, 'accumulated_eval_time': 3408.0285561084747, 'accumulated_logging_time': 1.1267368793487549, 'global_step': 49977, 'preemption_count': 0}), (51127, {'train/accuracy': 0.9907481074333191, 'train/loss': 0.03128005564212799, 'train/mean_average_precision': 0.3369954197829563, 'validation/accuracy': 0.9864618182182312, 'validation/loss': 0.046143025159835815, 'validation/mean_average_precision': 0.2199924427381768, 'validation/num_examples': 43793, 'test/accuracy': 0.9854733943939209, 'test/loss': 0.0491577610373497, 'test/mean_average_precision': 0.21440788877819716, 'test/num_examples': 43793, 'score': 10572.823588132858, 'total_duration': 14055.061342954636, 'accumulated_submission_time': 10572.823588132858, 'accumulated_eval_time': 3479.729007959366, 'accumulated_logging_time': 1.149550437927246, 'global_step': 51127, 'preemption_count': 0}), (52278, {'train/accuracy': 0.9908356666564941, 'train/loss': 0.030934451147913933, 'train/mean_average_precision': 0.34129104048197956, 'validation/accuracy': 0.986477255821228, 'validation/loss': 0.04616273567080498, 'validation/mean_average_precision': 0.22007124464626387, 'validation/num_examples': 43793, 'test/accuracy': 0.9854763150215149, 'test/loss': 0.049183763563632965, 'test/mean_average_precision': 0.21381964870785214, 'test/num_examples': 43793, 'score': 10812.81077170372, 'total_duration': 14368.891102552414, 'accumulated_submission_time': 10812.81077170372, 'accumulated_eval_time': 3553.5147626399994, 'accumulated_logging_time': 1.1735143661499023, 'global_step': 52278, 'preemption_count': 0}), (53415, {'train/accuracy': 0.9909923672676086, 'train/loss': 0.030576353892683983, 'train/mean_average_precision': 0.3423907547306161, 'validation/accuracy': 0.986477255821228, 'validation/loss': 0.04616272449493408, 'validation/mean_average_precision': 0.22005427981555448, 'validation/num_examples': 43793, 'test/accuracy': 0.9854763150215149, 'test/loss': 0.04918377101421356, 'test/mean_average_precision': 0.2139190579217768, 'test/num_examples': 43793, 'score': 11052.907064676285, 'total_duration': 14684.519357681274, 'accumulated_submission_time': 11052.907064676285, 'accumulated_eval_time': 3628.99152469635, 'accumulated_logging_time': 1.1997356414794922, 'global_step': 53415, 'preemption_count': 0}), (54557, {'train/accuracy': 0.9907734394073486, 'train/loss': 0.031270235776901245, 'train/mean_average_precision': 0.3482976551199915, 'validation/accuracy': 0.986477255821228, 'validation/loss': 0.04616273567080498, 'validation/mean_average_precision': 0.2200619196401685, 'validation/num_examples': 43793, 'test/accuracy': 0.9854763150215149, 'test/loss': 0.04918377101421356, 'test/mean_average_precision': 0.21395859270510637, 'test/num_examples': 43793, 'score': 11292.872061491013, 'total_duration': 14998.179833173752, 'accumulated_submission_time': 11292.872061491013, 'accumulated_eval_time': 3702.630370616913, 'accumulated_logging_time': 1.2235479354858398, 'global_step': 54557, 'preemption_count': 0}), (55701, {'train/accuracy': 0.9908541440963745, 'train/loss': 0.030904466286301613, 'train/mean_average_precision': 0.3425191979481913, 'validation/accuracy': 0.986477255821228, 'validation/loss': 0.04616272822022438, 'validation/mean_average_precision': 0.2201147637976517, 'validation/num_examples': 43793, 'test/accuracy': 0.9854763150215149, 'test/loss': 0.04918375611305237, 'test/mean_average_precision': 0.21386034899082315, 'test/num_examples': 43793, 'score': 11532.818925619125, 'total_duration': 15314.835664510727, 'accumulated_submission_time': 11532.818925619125, 'accumulated_eval_time': 3779.283523082733, 'accumulated_logging_time': 1.2484877109527588, 'global_step': 55701, 'preemption_count': 0}), (56843, {'train/accuracy': 0.9906882047653198, 'train/loss': 0.03134234622120857, 'train/mean_average_precision': 0.338537350955308, 'validation/accuracy': 0.986477255821228, 'validation/loss': 0.04616272449493408, 'validation/mean_average_precision': 0.22010040855786156, 'validation/num_examples': 43793, 'test/accuracy': 0.9854763150215149, 'test/loss': 0.04918375611305237, 'test/mean_average_precision': 0.2138619621843709, 'test/num_examples': 43793, 'score': 11772.763296842575, 'total_duration': 15627.58739900589, 'accumulated_submission_time': 11772.763296842575, 'accumulated_eval_time': 3852.0327825546265, 'accumulated_logging_time': 1.2727749347686768, 'global_step': 56843, 'preemption_count': 0})], 'global_step': 57991}
I0305 23:37:53.968377 139988053398720 submission_runner.py:649] Timing: 12012.754350185394
I0305 23:37:53.968419 139988053398720 submission_runner.py:651] Total number of evals: 50
I0305 23:37:53.968447 139988053398720 submission_runner.py:652] ====================
I0305 23:37:53.968807 139988053398720 submission_runner.py:750] Final ogbg score: 3
