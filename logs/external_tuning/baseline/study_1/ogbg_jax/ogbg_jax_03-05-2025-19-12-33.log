python submission_runner.py --framework=jax --workload=ogbg --submission_path=prize_qualification_baselines/external_tuning/jax_nadamw_full_budget.py --data_dir=/data/ogbg --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=submissions/rolling_leaderboard/external_tuning/baseline/study_1 --overwrite=True --save_checkpoints=False --rng_seed=-1529022773 --tuning_ruleset=external --tuning_search_space=prize_qualification_baselines/external_tuning/tuning_search_space.json --num_tuning_trials=5 --hparam_start_index=2 --hparam_end_index=3 2>&1 | tee -a /logs/ogbg_jax_03-05-2025-19-12-33.log
2025-03-05 19:12:34.740971: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1741201954.762962       8 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1741201954.769585       8 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
I0305 19:12:40.989798 139687630427328 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_1/ogbg_jax.
I0305 19:12:41.893989 139687630427328 xla_bridge.py:884] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA
I0305 19:12:41.896974 139687630427328 xla_bridge.py:884] Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory
I0305 19:12:41.898618 139687630427328 submission_runner.py:606] Using RNG seed -1529022773
I0305 19:12:42.500847 139687630427328 submission_runner.py:615] --- Tuning run 3/5 ---
I0305 19:12:42.501038 139687630427328 submission_runner.py:620] Creating tuning directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_1/ogbg_jax/trial_3.
I0305 19:12:42.501228 139687630427328 logger_utils.py:97] Saving hparams to /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_1/ogbg_jax/trial_3/hparams.json.
I0305 19:12:42.734129 139687630427328 submission_runner.py:218] Initializing dataset.
I0305 19:12:43.005576 139687630427328 dataset_info.py:690] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0305 19:12:43.051516 139687630427328 reader.py:261] Creating a tf.data.Dataset reading 8 files located in folders: /data/ogbg/ogbg_molpcba/0.1.3.
WARNING:tensorflow:From /usr/local/lib/python3.11/site-packages/tensorflow_datasets/core/reader.py:101: CounterV2 (from tensorflow.python.data.experimental.ops.counter) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.counter(...)` instead.
W0305 19:12:43.267677 139687630427328 deprecation.py:50] From /usr/local/lib/python3.11/site-packages/tensorflow_datasets/core/reader.py:101: CounterV2 (from tensorflow.python.data.experimental.ops.counter) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.counter(...)` instead.
I0305 19:12:43.314207 139687630427328 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split train, from /data/ogbg/ogbg_molpcba/0.1.3
I0305 19:12:43.343973 139687630427328 submission_runner.py:229] Initializing model.
I0305 19:12:50.486003 139687630427328 submission_runner.py:272] Initializing optimizer.
I0305 19:12:50.881464 139687630427328 submission_runner.py:279] Initializing metrics bundle.
I0305 19:12:50.881677 139687630427328 submission_runner.py:301] Initializing checkpoint and logger.
I0305 19:12:50.882376 139687630427328 checkpoints.py:1101] Found no checkpoint files in /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_1/ogbg_jax/trial_3 with prefix checkpoint_
I0305 19:12:50.882469 139687630427328 submission_runner.py:321] Saving meta data to /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_1/ogbg_jax/trial_3/meta_data_0.json.
I0305 19:12:50.882637 139687630427328 logger_utils.py:262] Unable to record workload.train_mean information. Continuing without it.
I0305 19:12:50.882683 139687630427328 logger_utils.py:262] Unable to record workload.train_stddev information. Continuing without it.
I0305 19:12:51.038926 139687630427328 submission_runner.py:325] Saving flags to /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_1/ogbg_jax/trial_3/flags_0.json.
I0305 19:12:51.071725 139687630427328 submission_runner.py:337] Starting training loop.
I0305 19:13:01.699573 139551353386752 logging_writer.py:48] [0] global_step=0, grad_norm=2.97212553024292, loss=0.7113223075866699
I0305 19:13:01.745841 139687630427328 spec.py:321] Evaluating on the training split.
I0305 19:13:01.749198 139687630427328 dataset_info.py:690] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0305 19:13:01.752658 139687630427328 reader.py:261] Creating a tf.data.Dataset reading 8 files located in folders: /data/ogbg/ogbg_molpcba/0.1.3.
I0305 19:13:01.811139 139687630427328 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split train, from /data/ogbg/ogbg_molpcba/0.1.3
I0305 19:14:15.729528 139687630427328 spec.py:333] Evaluating on the validation split.
I0305 19:14:15.732329 139687630427328 dataset_info.py:690] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0305 19:14:15.736446 139687630427328 reader.py:261] Creating a tf.data.Dataset reading 1 files located in folders: /data/ogbg/ogbg_molpcba/0.1.3.
I0305 19:14:15.794647 139687630427328 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split validation, from /data/ogbg/ogbg_molpcba/0.1.3
I0305 19:15:17.572895 139687630427328 spec.py:349] Evaluating on the test split.
I0305 19:15:17.575179 139687630427328 dataset_info.py:690] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0305 19:15:17.578891 139687630427328 reader.py:261] Creating a tf.data.Dataset reading 1 files located in folders: /data/ogbg/ogbg_molpcba/0.1.3.
I0305 19:15:17.636110 139687630427328 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split test, from /data/ogbg/ogbg_molpcba/0.1.3
I0305 19:16:20.177589 139687630427328 submission_runner.py:469] Time since start: 209.11s, 	Step: 1, 	{'train/accuracy': 0.5324849486351013, 'train/loss': 0.7127916216850281, 'train/mean_average_precision': 0.022772149164080206, 'validation/accuracy': 0.5275315046310425, 'validation/loss': 0.7149298787117004, 'validation/mean_average_precision': 0.026223687348441105, 'validation/num_examples': 43793, 'test/accuracy': 0.5263952016830444, 'test/loss': 0.7148033976554871, 'test/mean_average_precision': 0.028037889592799795, 'test/num_examples': 43793, 'score': 10.67402958869934, 'total_duration': 209.10578536987305, 'accumulated_submission_time': 10.67402958869934, 'accumulated_eval_time': 198.43166184425354, 'accumulated_logging_time': 0}
I0305 19:16:20.185426 139545726985984 logging_writer.py:48] [1] accumulated_eval_time=198.432, accumulated_logging_time=0, accumulated_submission_time=10.674, global_step=1, preemption_count=0, score=10.674, test/accuracy=0.526395, test/loss=0.714803, test/mean_average_precision=0.0280379, test/num_examples=43793, total_duration=209.106, train/accuracy=0.532485, train/loss=0.712792, train/mean_average_precision=0.0227721, validation/accuracy=0.527532, validation/loss=0.71493, validation/mean_average_precision=0.0262237, validation/num_examples=43793
I0305 19:16:39.948298 139545735378688 logging_writer.py:48] [100] global_step=100, grad_norm=0.5080532431602478, loss=0.4262864887714386
I0305 19:17:00.063434 139545726985984 logging_writer.py:48] [200] global_step=200, grad_norm=0.3311135172843933, loss=0.2957404553890228
I0305 19:17:20.001410 139545735378688 logging_writer.py:48] [300] global_step=300, grad_norm=0.20516502857208252, loss=0.1841304451227188
I0305 19:17:39.963427 139545726985984 logging_writer.py:48] [400] global_step=400, grad_norm=0.11180324107408524, loss=0.1185738816857338
I0305 19:17:59.946094 139545735378688 logging_writer.py:48] [500] global_step=500, grad_norm=0.06135037541389465, loss=0.08459004014730453
I0305 19:18:19.901914 139545726985984 logging_writer.py:48] [600] global_step=600, grad_norm=0.041070371866226196, loss=0.06971494108438492
I0305 19:18:39.766518 139546272429824 logging_writer.py:48] [700] global_step=700, grad_norm=0.0825711339712143, loss=0.0652901753783226
I0305 19:18:59.854963 139546264037120 logging_writer.py:48] [800] global_step=800, grad_norm=0.0969204232096672, loss=0.0541032999753952
I0305 19:19:19.815680 139546272429824 logging_writer.py:48] [900] global_step=900, grad_norm=0.06660398840904236, loss=0.05999346822500229
I0305 19:19:39.806787 139546264037120 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.1281801015138626, loss=0.05286802351474762
I0305 19:19:59.649133 139546272429824 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.16739250719547272, loss=0.04623713344335556
I0305 19:20:19.669242 139546264037120 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.28272199630737305, loss=0.050099898129701614
I0305 19:20:20.283564 139687630427328 spec.py:321] Evaluating on the training split.
I0305 19:21:29.705166 139687630427328 spec.py:333] Evaluating on the validation split.
I0305 19:21:31.584713 139687630427328 spec.py:349] Evaluating on the test split.
I0305 19:21:33.429346 139687630427328 submission_runner.py:469] Time since start: 522.36s, 	Step: 1204, 	{'train/accuracy': 0.9871761202812195, 'train/loss': 0.04907609522342682, 'train/mean_average_precision': 0.08047075406191484, 'validation/accuracy': 0.9845871925354004, 'validation/loss': 0.05789482966065407, 'validation/mean_average_precision': 0.08065719290145534, 'validation/num_examples': 43793, 'test/accuracy': 0.983574628829956, 'test/loss': 0.06109181419014931, 'test/mean_average_precision': 0.07969899945991725, 'test/num_examples': 43793, 'score': 250.7315583229065, 'total_duration': 522.3575792312622, 'accumulated_submission_time': 250.7315583229065, 'accumulated_eval_time': 271.5774166584015, 'accumulated_logging_time': 0.017308950424194336}
I0305 19:21:33.437254 139546272429824 logging_writer.py:48] [1204] accumulated_eval_time=271.577, accumulated_logging_time=0.017309, accumulated_submission_time=250.732, global_step=1204, preemption_count=0, score=250.732, test/accuracy=0.983575, test/loss=0.0610918, test/mean_average_precision=0.079699, test/num_examples=43793, total_duration=522.358, train/accuracy=0.987176, train/loss=0.0490761, train/mean_average_precision=0.0804708, validation/accuracy=0.984587, validation/loss=0.0578948, validation/mean_average_precision=0.0806572, validation/num_examples=43793
I0305 19:21:52.765376 139546264037120 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.09247514605522156, loss=0.04666493833065033
I0305 19:22:12.769704 139546272429824 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.11211487650871277, loss=0.04536023736000061
I0305 19:22:32.562277 139546264037120 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.1290862113237381, loss=0.04751149192452431
I0305 19:22:52.660826 139546272429824 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.1434767246246338, loss=0.0494052954018116
I0305 19:23:12.500669 139546264037120 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.10717068612575531, loss=0.04814979434013367
I0305 19:23:32.470773 139546272429824 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.12164580821990967, loss=0.04454982653260231
I0305 19:23:52.430807 139546264037120 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.0864262655377388, loss=0.045205969363451004
I0305 19:24:12.303963 139546272429824 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.3500339388847351, loss=0.04702179506421089
I0305 19:24:32.154654 139546264037120 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.16065950691699982, loss=0.04378752037882805
I0305 19:24:52.001264 139546272429824 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.15287670493125916, loss=0.04718805104494095
I0305 19:25:11.884956 139546264037120 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.15265105664730072, loss=0.03696494176983833
I0305 19:25:31.896895 139546272429824 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.12957456707954407, loss=0.04217527061700821
I0305 19:25:33.519412 139687630427328 spec.py:321] Evaluating on the training split.
I0305 19:26:42.663017 139687630427328 spec.py:333] Evaluating on the validation split.
I0305 19:26:44.509848 139687630427328 spec.py:349] Evaluating on the test split.
I0305 19:26:46.332513 139687630427328 submission_runner.py:469] Time since start: 835.26s, 	Step: 2409, 	{'train/accuracy': 0.9880992770195007, 'train/loss': 0.04199444502592087, 'train/mean_average_precision': 0.16339833442805085, 'validation/accuracy': 0.9852679371833801, 'validation/loss': 0.05172494426369667, 'validation/mean_average_precision': 0.14779307375768755, 'validation/num_examples': 43793, 'test/accuracy': 0.9842839241027832, 'test/loss': 0.054605670273303986, 'test/mean_average_precision': 0.1524877077419345, 'test/num_examples': 43793, 'score': 490.7717094421387, 'total_duration': 835.2607462406158, 'accumulated_submission_time': 490.7717094421387, 'accumulated_eval_time': 344.39047598838806, 'accumulated_logging_time': 0.03360939025878906}
I0305 19:26:46.340509 139546264037120 logging_writer.py:48] [2409] accumulated_eval_time=344.39, accumulated_logging_time=0.0336094, accumulated_submission_time=490.772, global_step=2409, preemption_count=0, score=490.772, test/accuracy=0.984284, test/loss=0.0546057, test/mean_average_precision=0.152488, test/num_examples=43793, total_duration=835.261, train/accuracy=0.988099, train/loss=0.0419944, train/mean_average_precision=0.163398, validation/accuracy=0.985268, validation/loss=0.0517249, validation/mean_average_precision=0.147793, validation/num_examples=43793
I0305 19:27:04.313500 139546272429824 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.08838455379009247, loss=0.04587951675057411
I0305 19:27:23.496493 139546264037120 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.16558286547660828, loss=0.0368119515478611
I0305 19:27:42.565953 139546272429824 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.16493991017341614, loss=0.04394453763961792
I0305 19:28:02.155204 139546264037120 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.23768775165081024, loss=0.04805271700024605
I0305 19:28:21.712741 139546272429824 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.1755746304988861, loss=0.04008451849222183
I0305 19:28:41.185381 139546264037120 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.09892738610506058, loss=0.04538479447364807
I0305 19:29:00.635610 139546272429824 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.03728202357888222, loss=0.03915926069021225
I0305 19:29:20.097379 139546264037120 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.10885485261678696, loss=0.049964264035224915
I0305 19:29:39.681829 139546272429824 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.07193859666585922, loss=0.036986928433179855
I0305 19:29:59.189104 139546264037120 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.07367615401744843, loss=0.038874197751283646
I0305 19:30:18.663674 139546272429824 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.097624771296978, loss=0.043157149106264114
I0305 19:30:38.002585 139546264037120 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.05325445905327797, loss=0.03966725617647171
I0305 19:30:46.512086 139687630427328 spec.py:321] Evaluating on the training split.
I0305 19:31:55.083399 139687630427328 spec.py:333] Evaluating on the validation split.
I0305 19:31:56.932573 139687630427328 spec.py:349] Evaluating on the test split.
I0305 19:31:58.749757 139687630427328 submission_runner.py:469] Time since start: 1147.68s, 	Step: 3645, 	{'train/accuracy': 0.9883466362953186, 'train/loss': 0.04063054174184799, 'train/mean_average_precision': 0.19435524908020002, 'validation/accuracy': 0.9853662252426147, 'validation/loss': 0.05041654780507088, 'validation/mean_average_precision': 0.17264956464667588, 'validation/num_examples': 43793, 'test/accuracy': 0.9844789505004883, 'test/loss': 0.052998971194028854, 'test/mean_average_precision': 0.17195503516865465, 'test/num_examples': 43793, 'score': 730.9027371406555, 'total_duration': 1147.6779839992523, 'accumulated_submission_time': 730.9027371406555, 'accumulated_eval_time': 416.6280953884125, 'accumulated_logging_time': 0.0506742000579834}
I0305 19:31:58.757263 139546272429824 logging_writer.py:48] [3645] accumulated_eval_time=416.628, accumulated_logging_time=0.0506742, accumulated_submission_time=730.903, global_step=3645, preemption_count=0, score=730.903, test/accuracy=0.984479, test/loss=0.052999, test/mean_average_precision=0.171955, test/num_examples=43793, total_duration=1147.68, train/accuracy=0.988347, train/loss=0.0406305, train/mean_average_precision=0.194355, validation/accuracy=0.985366, validation/loss=0.0504165, validation/mean_average_precision=0.17265, validation/num_examples=43793
I0305 19:32:09.668384 139546264037120 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.06065371260046959, loss=0.03921467438340187
I0305 19:32:29.146358 139546272429824 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.08115129917860031, loss=0.040672242641448975
I0305 19:32:48.481302 139546264037120 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.056582190096378326, loss=0.040623098611831665
I0305 19:33:07.979679 139546272429824 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.04502382501959801, loss=0.03594326227903366
I0305 19:33:27.466042 139546264037120 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.0557803139090538, loss=0.0437820702791214
I0305 19:33:46.933043 139546272429824 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.08345608413219452, loss=0.04403253272175789
I0305 19:34:06.490283 139546264037120 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.04238656908273697, loss=0.040930263698101044
I0305 19:34:25.956216 139546272429824 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.045641686767339706, loss=0.04153396934270859
I0305 19:34:45.408461 139546264037120 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.09468548744916916, loss=0.036396998912096024
I0305 19:35:04.946593 139546272429824 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.06559263914823532, loss=0.037322789430618286
I0305 19:35:24.464867 139546264037120 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.048797573894262314, loss=0.04170903563499451
I0305 19:35:43.912249 139546272429824 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.04332209378480911, loss=0.0389041043817997
I0305 19:35:58.817797 139687630427328 spec.py:321] Evaluating on the training split.
I0305 19:37:06.715432 139687630427328 spec.py:333] Evaluating on the validation split.
I0305 19:37:08.555707 139687630427328 spec.py:349] Evaluating on the test split.
I0305 19:37:10.362199 139687630427328 submission_runner.py:469] Time since start: 1459.29s, 	Step: 4878, 	{'train/accuracy': 0.9887829422950745, 'train/loss': 0.03813507780432701, 'train/mean_average_precision': 0.22384947313674747, 'validation/accuracy': 0.9857782125473022, 'validation/loss': 0.04807915911078453, 'validation/mean_average_precision': 0.1835097667892105, 'validation/num_examples': 43793, 'test/accuracy': 0.9849051833152771, 'test/loss': 0.05067272484302521, 'test/mean_average_precision': 0.1875361934730356, 'test/num_examples': 43793, 'score': 970.9223113059998, 'total_duration': 1459.2904365062714, 'accumulated_submission_time': 970.9223113059998, 'accumulated_eval_time': 488.17245721817017, 'accumulated_logging_time': 0.06733155250549316}
I0305 19:37:10.370358 139545811003136 logging_writer.py:48] [4878] accumulated_eval_time=488.172, accumulated_logging_time=0.0673316, accumulated_submission_time=970.922, global_step=4878, preemption_count=0, score=970.922, test/accuracy=0.984905, test/loss=0.0506727, test/mean_average_precision=0.187536, test/num_examples=43793, total_duration=1459.29, train/accuracy=0.988783, train/loss=0.0381351, train/mean_average_precision=0.223849, validation/accuracy=0.985778, validation/loss=0.0480792, validation/mean_average_precision=0.18351, validation/num_examples=43793
I0305 19:37:14.858439 139545802610432 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.05889641493558884, loss=0.04478757083415985
I0305 19:37:34.525356 139545811003136 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.1271372139453888, loss=0.03203832730650902
I0305 19:37:53.792197 139545802610432 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.0672181025147438, loss=0.04385790973901749
I0305 19:38:13.004748 139545811003136 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.025140585377812386, loss=0.03284325450658798
I0305 19:38:32.104186 139545802610432 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.053649257868528366, loss=0.04400040581822395
I0305 19:38:51.306374 139545811003136 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.0424589179456234, loss=0.03593587130308151
I0305 19:39:10.695956 139545802610432 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.04485438019037247, loss=0.036683592945337296
I0305 19:39:30.269545 139545811003136 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.04984334856271744, loss=0.03899938985705376
I0305 19:39:49.869661 139545802610432 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.02924135886132717, loss=0.039810094982385635
I0305 19:40:09.561245 139545811003136 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.0576321966946125, loss=0.040473807603120804
I0305 19:40:29.154815 139545802610432 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.035134557634592056, loss=0.038627397269010544
I0305 19:40:48.479249 139545811003136 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.041047725826501846, loss=0.03725101798772812
I0305 19:41:08.141589 139545802610432 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.036648496985435486, loss=0.03720321133732796
I0305 19:41:10.477542 139687630427328 spec.py:321] Evaluating on the training split.
I0305 19:42:18.630417 139687630427328 spec.py:333] Evaluating on the validation split.
I0305 19:42:20.525831 139687630427328 spec.py:349] Evaluating on the test split.
I0305 19:42:22.369985 139687630427328 submission_runner.py:469] Time since start: 1771.30s, 	Step: 6113, 	{'train/accuracy': 0.9889467358589172, 'train/loss': 0.037651777267456055, 'train/mean_average_precision': 0.246404636580146, 'validation/accuracy': 0.9860400557518005, 'validation/loss': 0.0470418706536293, 'validation/mean_average_precision': 0.20975559101106836, 'validation/num_examples': 43793, 'test/accuracy': 0.9851292371749878, 'test/loss': 0.049701400101184845, 'test/mean_average_precision': 0.20457672642767086, 'test/num_examples': 43793, 'score': 1210.9870240688324, 'total_duration': 1771.2982120513916, 'accumulated_submission_time': 1210.9870240688324, 'accumulated_eval_time': 560.0648708343506, 'accumulated_logging_time': 0.08810544013977051}
I0305 19:42:22.378122 139545811003136 logging_writer.py:48] [6113] accumulated_eval_time=560.065, accumulated_logging_time=0.0881054, accumulated_submission_time=1210.99, global_step=6113, preemption_count=0, score=1210.99, test/accuracy=0.985129, test/loss=0.0497014, test/mean_average_precision=0.204577, test/num_examples=43793, total_duration=1771.3, train/accuracy=0.988947, train/loss=0.0376518, train/mean_average_precision=0.246405, validation/accuracy=0.98604, validation/loss=0.0470419, validation/mean_average_precision=0.209756, validation/num_examples=43793
I0305 19:42:39.659852 139545802610432 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.054142098873853683, loss=0.033967841416597366
I0305 19:42:59.372450 139545811003136 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.04356582835316658, loss=0.03632667288184166
I0305 19:43:18.776913 139545802610432 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.029486441984772682, loss=0.039552632719278336
I0305 19:43:38.279343 139545811003136 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.025635583326220512, loss=0.03767075017094612
I0305 19:43:57.584155 139545802610432 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.026010757312178612, loss=0.03808654844760895
I0305 19:44:17.161800 139545811003136 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.02924662083387375, loss=0.03198026865720749
I0305 19:44:36.702080 139545802610432 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.05179430916905403, loss=0.03838403895497322
I0305 19:44:56.344210 139545811003136 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.042207054793834686, loss=0.0374266617000103
I0305 19:45:15.948957 139545802610432 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.036372315138578415, loss=0.04169346019625664
I0305 19:45:35.525841 139545811003136 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.0492972731590271, loss=0.03697258606553078
I0305 19:45:55.281171 139545802610432 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.0329105444252491, loss=0.038195159286260605
I0305 19:46:14.801057 139545811003136 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.037363551557064056, loss=0.038444921374320984
I0305 19:46:22.403013 139687630427328 spec.py:321] Evaluating on the training split.
I0305 19:47:29.371196 139687630427328 spec.py:333] Evaluating on the validation split.
I0305 19:47:31.231153 139687630427328 spec.py:349] Evaluating on the test split.
I0305 19:47:33.094782 139687630427328 submission_runner.py:469] Time since start: 2082.02s, 	Step: 7340, 	{'train/accuracy': 0.9892685413360596, 'train/loss': 0.03604841232299805, 'train/mean_average_precision': 0.2736420161819601, 'validation/accuracy': 0.9862799644470215, 'validation/loss': 0.046389758586883545, 'validation/mean_average_precision': 0.22707269065121521, 'validation/num_examples': 43793, 'test/accuracy': 0.9854047298431396, 'test/loss': 0.04925492778420448, 'test/mean_average_precision': 0.22034588418347525, 'test/num_examples': 43793, 'score': 1450.9730019569397, 'total_duration': 2082.0228946208954, 'accumulated_submission_time': 1450.9730019569397, 'accumulated_eval_time': 630.7564730644226, 'accumulated_logging_time': 0.10465407371520996}
I0305 19:47:33.103292 139545802610432 logging_writer.py:48] [7340] accumulated_eval_time=630.756, accumulated_logging_time=0.104654, accumulated_submission_time=1450.97, global_step=7340, preemption_count=0, score=1450.97, test/accuracy=0.985405, test/loss=0.0492549, test/mean_average_precision=0.220346, test/num_examples=43793, total_duration=2082.02, train/accuracy=0.989269, train/loss=0.0360484, train/mean_average_precision=0.273642, validation/accuracy=0.98628, validation/loss=0.0463898, validation/mean_average_precision=0.227073, validation/num_examples=43793
I0305 19:47:45.139578 139545811003136 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.022526204586029053, loss=0.033019546419382095
I0305 19:48:04.986495 139545802610432 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.028100959956645966, loss=0.036416538059711456
I0305 19:48:24.538902 139545811003136 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.024821287021040916, loss=0.03264303877949715
I0305 19:48:44.156474 139545802610432 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.02916361205279827, loss=0.033964257687330246
I0305 19:49:03.634656 139545811003136 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.03143886476755142, loss=0.038961008191108704
I0305 19:49:23.269437 139545802610432 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.025905637070536613, loss=0.03741668909788132
I0305 19:49:42.700660 139545811003136 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.02632407657802105, loss=0.032287001609802246
I0305 19:50:02.202909 139545802610432 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.04108119383454323, loss=0.0354362390935421
I0305 19:50:21.608537 139545811003136 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.03045923262834549, loss=0.03535177931189537
I0305 19:50:41.129549 139545802610432 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.03198477625846863, loss=0.030486147850751877
I0305 19:51:00.463044 139545811003136 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.022934051230549812, loss=0.03776536509394646
I0305 19:51:19.911695 139545802610432 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.03001534752547741, loss=0.042536959052085876
I0305 19:51:33.270651 139687630427328 spec.py:321] Evaluating on the training split.
I0305 19:52:40.523560 139687630427328 spec.py:333] Evaluating on the validation split.
I0305 19:52:42.371124 139687630427328 spec.py:349] Evaluating on the test split.
I0305 19:52:44.187723 139687630427328 submission_runner.py:469] Time since start: 2393.12s, 	Step: 8569, 	{'train/accuracy': 0.989592432975769, 'train/loss': 0.03482768312096596, 'train/mean_average_precision': 0.3000889191251173, 'validation/accuracy': 0.9864391088485718, 'validation/loss': 0.04533017799258232, 'validation/mean_average_precision': 0.2371081444502424, 'validation/num_examples': 43793, 'test/accuracy': 0.9855997562408447, 'test/loss': 0.04814217612147331, 'test/mean_average_precision': 0.23688613582142568, 'test/num_examples': 43793, 'score': 1691.1018335819244, 'total_duration': 2393.1159620285034, 'accumulated_submission_time': 1691.1018335819244, 'accumulated_eval_time': 701.6735055446625, 'accumulated_logging_time': 0.12132859230041504}
I0305 19:52:44.196075 139545811003136 logging_writer.py:48] [8569] accumulated_eval_time=701.674, accumulated_logging_time=0.121329, accumulated_submission_time=1691.1, global_step=8569, preemption_count=0, score=1691.1, test/accuracy=0.9856, test/loss=0.0481422, test/mean_average_precision=0.236886, test/num_examples=43793, total_duration=2393.12, train/accuracy=0.989592, train/loss=0.0348277, train/mean_average_precision=0.300089, validation/accuracy=0.986439, validation/loss=0.0453302, validation/mean_average_precision=0.237108, validation/num_examples=43793
I0305 19:52:50.389506 139545802610432 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.034320052713155746, loss=0.03642109036445618
I0305 19:53:10.052820 139545811003136 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.047860730439424515, loss=0.03733765333890915
I0305 19:53:29.518095 139545802610432 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.030403615906834602, loss=0.034364692866802216
I0305 19:53:48.924348 139545811003136 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.033401865512132645, loss=0.04035147279500961
I0305 19:54:08.504027 139545802610432 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.025440450757741928, loss=0.03238317742943764
I0305 19:54:27.822965 139545811003136 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.03302455693483353, loss=0.038140568882226944
I0305 19:54:47.153894 139545802610432 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.04684983938932419, loss=0.03510050103068352
I0305 19:55:06.749056 139545811003136 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.04308990016579628, loss=0.03192063048481941
I0305 19:55:26.289031 139545802610432 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.03655926510691643, loss=0.04080270975828171
I0305 19:55:45.679905 139545811003136 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.04324226453900337, loss=0.03574550151824951
I0305 19:56:05.193428 139545802610432 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.047371286898851395, loss=0.03562121093273163
I0305 19:56:24.987183 139545811003136 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.04031054675579071, loss=0.03067687153816223
I0305 19:56:44.346959 139687630427328 spec.py:321] Evaluating on the training split.
I0305 19:57:53.462932 139687630427328 spec.py:333] Evaluating on the validation split.
I0305 19:57:55.360035 139687630427328 spec.py:349] Evaluating on the test split.
I0305 19:57:57.219670 139687630427328 submission_runner.py:469] Time since start: 2706.15s, 	Step: 9798, 	{'train/accuracy': 0.9899195432662964, 'train/loss': 0.03376801684498787, 'train/mean_average_precision': 0.32037757487739493, 'validation/accuracy': 0.9864931106567383, 'validation/loss': 0.044946055859327316, 'validation/mean_average_precision': 0.24195770067966035, 'validation/num_examples': 43793, 'test/accuracy': 0.9856574535369873, 'test/loss': 0.0476822592318058, 'test/mean_average_precision': 0.23730350403973474, 'test/num_examples': 43793, 'score': 1931.2127656936646, 'total_duration': 2706.147906780243, 'accumulated_submission_time': 1931.2127656936646, 'accumulated_eval_time': 774.5461761951447, 'accumulated_logging_time': 0.13878679275512695}
I0305 19:57:57.230281 139545802610432 logging_writer.py:48] [9798] accumulated_eval_time=774.546, accumulated_logging_time=0.138787, accumulated_submission_time=1931.21, global_step=9798, preemption_count=0, score=1931.21, test/accuracy=0.985657, test/loss=0.0476823, test/mean_average_precision=0.237304, test/num_examples=43793, total_duration=2706.15, train/accuracy=0.98992, train/loss=0.033768, train/mean_average_precision=0.320378, validation/accuracy=0.986493, validation/loss=0.0449461, validation/mean_average_precision=0.241958, validation/num_examples=43793
I0305 19:57:57.852040 139545811003136 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.040674012154340744, loss=0.03210155665874481
I0305 19:58:18.011368 139545802610432 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.037513528019189835, loss=0.03274846449494362
I0305 19:58:38.091036 139545811003136 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.02603105455636978, loss=0.031326763331890106
I0305 19:58:58.034039 139545802610432 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.03617028519511223, loss=0.03669930621981621
I0305 19:59:18.127648 139545811003136 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.03288155794143677, loss=0.03220117837190628
I0305 19:59:38.278725 139545802610432 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.028792930766940117, loss=0.03179255500435829
I0305 19:59:57.841235 139545811003136 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.03720714896917343, loss=0.031960416585206985
I0305 20:00:17.352364 139545802610432 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.044242650270462036, loss=0.03677215799689293
I0305 20:00:36.918684 139545811003136 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.07497910410165787, loss=0.03532694652676582
I0305 20:00:56.565285 139545802610432 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.06902989000082016, loss=0.03814222291111946
I0305 20:01:15.870149 139545811003136 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.03580796718597412, loss=0.030222725123167038
I0305 20:01:35.250384 139545802610432 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.03922359645366669, loss=0.031741268932819366
I0305 20:01:54.606751 139545811003136 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.03394829481840134, loss=0.02954941615462303
I0305 20:01:57.329702 139687630427328 spec.py:321] Evaluating on the training split.
I0305 20:03:05.666970 139687630427328 spec.py:333] Evaluating on the validation split.
I0305 20:03:07.512617 139687630427328 spec.py:349] Evaluating on the test split.
I0305 20:03:09.335683 139687630427328 submission_runner.py:469] Time since start: 3018.26s, 	Step: 11015, 	{'train/accuracy': 0.9901907444000244, 'train/loss': 0.03237757086753845, 'train/mean_average_precision': 0.3564640211945917, 'validation/accuracy': 0.9864581823348999, 'validation/loss': 0.04521080479025841, 'validation/mean_average_precision': 0.2513817432603708, 'validation/num_examples': 43793, 'test/accuracy': 0.9856275320053101, 'test/loss': 0.04784120246767998, 'test/mean_average_precision': 0.2451251461321142, 'test/num_examples': 43793, 'score': 2171.271536588669, 'total_duration': 3018.2631964683533, 'accumulated_submission_time': 2171.271536588669, 'accumulated_eval_time': 846.5513925552368, 'accumulated_logging_time': 0.15779662132263184}
I0305 20:03:09.345328 139545802610432 logging_writer.py:48] [11015] accumulated_eval_time=846.551, accumulated_logging_time=0.157797, accumulated_submission_time=2171.27, global_step=11015, preemption_count=0, score=2171.27, test/accuracy=0.985628, test/loss=0.0478412, test/mean_average_precision=0.245125, test/num_examples=43793, total_duration=3018.26, train/accuracy=0.990191, train/loss=0.0323776, train/mean_average_precision=0.356464, validation/accuracy=0.986458, validation/loss=0.0452108, validation/mean_average_precision=0.251382, validation/num_examples=43793
I0305 20:03:26.458765 139545811003136 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.042099710553884506, loss=0.03620607405900955
I0305 20:03:46.100760 139545802610432 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.049024853855371475, loss=0.03364973142743111
I0305 20:04:05.729575 139545811003136 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.037825413048267365, loss=0.03261512890458107
I0305 20:04:25.217293 139545802610432 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.040141720324754715, loss=0.03386615961790085
I0305 20:04:44.605517 139545811003136 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.03460152819752693, loss=0.03411230817437172
I0305 20:05:04.270944 139545802610432 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.04360116273164749, loss=0.03417745977640152
I0305 20:05:23.763431 139545811003136 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.0418168269097805, loss=0.03486907109618187
I0305 20:05:43.262751 139545802610432 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.06759122759103775, loss=0.032577984035015106
I0305 20:06:02.930572 139545811003136 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.04465046525001526, loss=0.030606530606746674
I0305 20:06:22.587795 139545802610432 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.04391445592045784, loss=0.032598722726106644
I0305 20:06:42.259416 139545811003136 logging_writer.py:48] [12100] global_step=12100, grad_norm=0.05213805288076401, loss=0.039245668798685074
I0305 20:07:01.770543 139545802610432 logging_writer.py:48] [12200] global_step=12200, grad_norm=0.056525953114032745, loss=0.03375992923974991
I0305 20:07:09.418443 139687630427328 spec.py:321] Evaluating on the training split.
I0305 20:08:20.960113 139687630427328 spec.py:333] Evaluating on the validation split.
I0305 20:08:23.046111 139687630427328 spec.py:349] Evaluating on the test split.
I0305 20:08:25.085328 139687630427328 submission_runner.py:469] Time since start: 3334.01s, 	Step: 12240, 	{'train/accuracy': 0.9900262355804443, 'train/loss': 0.032942771911621094, 'train/mean_average_precision': 0.3528261543639909, 'validation/accuracy': 0.9864752292633057, 'validation/loss': 0.04505610093474388, 'validation/mean_average_precision': 0.2541238149062396, 'validation/num_examples': 43793, 'test/accuracy': 0.9855698347091675, 'test/loss': 0.048024117946624756, 'test/mean_average_precision': 0.23444229498192212, 'test/num_examples': 43793, 'score': 2411.300901412964, 'total_duration': 3334.0134625434875, 'accumulated_submission_time': 2411.300901412964, 'accumulated_eval_time': 922.2181470394135, 'accumulated_logging_time': 0.1759498119354248}
I0305 20:08:25.095910 139545811003136 logging_writer.py:48] [12240] accumulated_eval_time=922.218, accumulated_logging_time=0.17595, accumulated_submission_time=2411.3, global_step=12240, preemption_count=0, score=2411.3, test/accuracy=0.98557, test/loss=0.0480241, test/mean_average_precision=0.234442, test/num_examples=43793, total_duration=3334.01, train/accuracy=0.990026, train/loss=0.0329428, train/mean_average_precision=0.352826, validation/accuracy=0.986475, validation/loss=0.0450561, validation/mean_average_precision=0.254124, validation/num_examples=43793
I0305 20:08:37.072505 139545802610432 logging_writer.py:48] [12300] global_step=12300, grad_norm=0.07657784968614578, loss=0.03257875144481659
I0305 20:08:56.918731 139545811003136 logging_writer.py:48] [12400] global_step=12400, grad_norm=0.046028923243284225, loss=0.0357971116900444
I0305 20:09:16.676643 139545802610432 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.0707172378897667, loss=0.031224576756358147
I0305 20:09:36.245493 139545811003136 logging_writer.py:48] [12600] global_step=12600, grad_norm=0.06292159110307693, loss=0.0374329648911953
I0305 20:09:55.782622 139545802610432 logging_writer.py:48] [12700] global_step=12700, grad_norm=0.05101441591978073, loss=0.02811189368367195
I0305 20:10:15.416238 139545811003136 logging_writer.py:48] [12800] global_step=12800, grad_norm=0.06879498064517975, loss=0.031483739614486694
I0305 20:10:35.177408 139545802610432 logging_writer.py:48] [12900] global_step=12900, grad_norm=0.04846988245844841, loss=0.03072168305516243
I0305 20:10:54.739758 139545811003136 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.04684755578637123, loss=0.03558850288391113
I0305 20:11:14.334181 139545802610432 logging_writer.py:48] [13100] global_step=13100, grad_norm=0.04901724308729172, loss=0.03150489926338196
I0305 20:11:34.027743 139545811003136 logging_writer.py:48] [13200] global_step=13200, grad_norm=0.049937114119529724, loss=0.03213164955377579
I0305 20:11:53.704804 139545802610432 logging_writer.py:48] [13300] global_step=13300, grad_norm=0.05503641441464424, loss=0.03536577150225639
I0305 20:12:13.247463 139545811003136 logging_writer.py:48] [13400] global_step=13400, grad_norm=0.03864674270153046, loss=0.02988617680966854
I0305 20:12:25.265661 139687630427328 spec.py:321] Evaluating on the training split.
I0305 20:13:32.758502 139687630427328 spec.py:333] Evaluating on the validation split.
I0305 20:13:34.641498 139687630427328 spec.py:349] Evaluating on the test split.
I0305 20:13:36.730017 139687630427328 submission_runner.py:469] Time since start: 3645.66s, 	Step: 13463, 	{'train/accuracy': 0.9906057119369507, 'train/loss': 0.03098604828119278, 'train/mean_average_precision': 0.3852047351392166, 'validation/accuracy': 0.9867269396781921, 'validation/loss': 0.04412157088518143, 'validation/mean_average_precision': 0.2535070583157811, 'validation/num_examples': 43793, 'test/accuracy': 0.9858874082565308, 'test/loss': 0.04695018753409386, 'test/mean_average_precision': 0.2503247057022947, 'test/num_examples': 43793, 'score': 2651.430403470993, 'total_duration': 3645.6581931114197, 'accumulated_submission_time': 2651.430403470993, 'accumulated_eval_time': 993.6824061870575, 'accumulated_logging_time': 0.19545841217041016}
I0305 20:13:36.742007 139545802610432 logging_writer.py:48] [13463] accumulated_eval_time=993.682, accumulated_logging_time=0.195458, accumulated_submission_time=2651.43, global_step=13463, preemption_count=0, score=2651.43, test/accuracy=0.985887, test/loss=0.0469502, test/mean_average_precision=0.250325, test/num_examples=43793, total_duration=3645.66, train/accuracy=0.990606, train/loss=0.030986, train/mean_average_precision=0.385205, validation/accuracy=0.986727, validation/loss=0.0441216, validation/mean_average_precision=0.253507, validation/num_examples=43793
I0305 20:13:44.056990 139545811003136 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.07509278506040573, loss=0.035473354160785675
I0305 20:14:03.739615 139545802610432 logging_writer.py:48] [13600] global_step=13600, grad_norm=0.06158633530139923, loss=0.032371632754802704
I0305 20:14:23.418297 139545811003136 logging_writer.py:48] [13700] global_step=13700, grad_norm=0.05517720803618431, loss=0.031042959541082382
I0305 20:14:42.956487 139545802610432 logging_writer.py:48] [13800] global_step=13800, grad_norm=0.05394553393125534, loss=0.03222988173365593
I0305 20:15:02.583421 139545811003136 logging_writer.py:48] [13900] global_step=13900, grad_norm=0.04113442450761795, loss=0.02938331849873066
I0305 20:15:22.189682 139545802610432 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.049976594746112823, loss=0.03390239551663399
I0305 20:15:41.826811 139545811003136 logging_writer.py:48] [14100] global_step=14100, grad_norm=0.04656698927283287, loss=0.03250366821885109
I0305 20:16:01.390762 139545802610432 logging_writer.py:48] [14200] global_step=14200, grad_norm=0.08925928175449371, loss=0.035119086503982544
I0305 20:16:20.901455 139545811003136 logging_writer.py:48] [14300] global_step=14300, grad_norm=0.07536447048187256, loss=0.029899464920163155
I0305 20:16:40.505546 139545802610432 logging_writer.py:48] [14400] global_step=14400, grad_norm=0.06795723736286163, loss=0.03091506101191044
I0305 20:17:00.092971 139545811003136 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.06890413910150528, loss=0.03136854246258736
I0305 20:17:19.734816 139545802610432 logging_writer.py:48] [14600] global_step=14600, grad_norm=0.08106653392314911, loss=0.03543918952345848
I0305 20:17:36.905368 139687630427328 spec.py:321] Evaluating on the training split.
I0305 20:18:46.447550 139687630427328 spec.py:333] Evaluating on the validation split.
I0305 20:18:48.540004 139687630427328 spec.py:349] Evaluating on the test split.
I0305 20:18:50.395219 139687630427328 submission_runner.py:469] Time since start: 3959.32s, 	Step: 14688, 	{'train/accuracy': 0.9903689622879028, 'train/loss': 0.03166794031858444, 'train/mean_average_precision': 0.36240173407478415, 'validation/accuracy': 0.9867252707481384, 'validation/loss': 0.04441898316144943, 'validation/mean_average_precision': 0.2583871765001981, 'validation/num_examples': 43793, 'test/accuracy': 0.9858574867248535, 'test/loss': 0.047323040664196014, 'test/mean_average_precision': 0.24998265527526783, 'test/num_examples': 43793, 'score': 2891.553037881851, 'total_duration': 3959.3234524726868, 'accumulated_submission_time': 2891.553037881851, 'accumulated_eval_time': 1067.1722207069397, 'accumulated_logging_time': 0.21614336967468262}
I0305 20:18:50.404990 139545811003136 logging_writer.py:48] [14688] accumulated_eval_time=1067.17, accumulated_logging_time=0.216143, accumulated_submission_time=2891.55, global_step=14688, preemption_count=0, score=2891.55, test/accuracy=0.985857, test/loss=0.047323, test/mean_average_precision=0.249983, test/num_examples=43793, total_duration=3959.32, train/accuracy=0.990369, train/loss=0.0316679, train/mean_average_precision=0.362402, validation/accuracy=0.986725, validation/loss=0.044419, validation/mean_average_precision=0.258387, validation/num_examples=43793
I0305 20:18:52.979873 139545802610432 logging_writer.py:48] [14700] global_step=14700, grad_norm=0.05213577300310135, loss=0.03012648969888687
I0305 20:19:12.531641 139545811003136 logging_writer.py:48] [14800] global_step=14800, grad_norm=0.058521587401628494, loss=0.03097558580338955
I0305 20:19:32.302279 139545802610432 logging_writer.py:48] [14900] global_step=14900, grad_norm=0.06007687747478485, loss=0.02865261770784855
I0305 20:19:52.039413 139545811003136 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.06603145599365234, loss=0.02998003177344799
I0305 20:20:11.721373 139545802610432 logging_writer.py:48] [15100] global_step=15100, grad_norm=0.07515449076890945, loss=0.03210283815860748
I0305 20:20:31.447110 139545811003136 logging_writer.py:48] [15200] global_step=15200, grad_norm=0.08476808667182922, loss=0.029620442539453506
I0305 20:20:50.842991 139545802610432 logging_writer.py:48] [15300] global_step=15300, grad_norm=0.08055529743432999, loss=0.031946104019880295
I0305 20:21:10.494623 139545811003136 logging_writer.py:48] [15400] global_step=15400, grad_norm=0.06995367258787155, loss=0.035691965371370316
I0305 20:21:30.146177 139545802610432 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.07935019582509995, loss=0.031744249165058136
I0305 20:21:49.661036 139545811003136 logging_writer.py:48] [15600] global_step=15600, grad_norm=0.06378637999296188, loss=0.03072621487081051
I0305 20:22:09.163833 139545802610432 logging_writer.py:48] [15700] global_step=15700, grad_norm=0.06750240176916122, loss=0.031013239175081253
I0305 20:22:28.750962 139545811003136 logging_writer.py:48] [15800] global_step=15800, grad_norm=0.06172613799571991, loss=0.03161042183637619
I0305 20:22:48.710186 139545802610432 logging_writer.py:48] [15900] global_step=15900, grad_norm=0.0523567721247673, loss=0.030161337926983833
I0305 20:22:50.493304 139687630427328 spec.py:321] Evaluating on the training split.
I0305 20:24:00.282756 139687630427328 spec.py:333] Evaluating on the validation split.
I0305 20:24:02.156531 139687630427328 spec.py:349] Evaluating on the test split.
I0305 20:24:03.986332 139687630427328 submission_runner.py:469] Time since start: 4272.91s, 	Step: 15910, 	{'train/accuracy': 0.9907258152961731, 'train/loss': 0.0304359532892704, 'train/mean_average_precision': 0.40113146296671615, 'validation/accuracy': 0.9865466952323914, 'validation/loss': 0.04438590258359909, 'validation/mean_average_precision': 0.2648781523433797, 'validation/num_examples': 43793, 'test/accuracy': 0.9857627153396606, 'test/loss': 0.047113560140132904, 'test/mean_average_precision': 0.25518366006370996, 'test/num_examples': 43793, 'score': 3131.6017882823944, 'total_duration': 4272.914475917816, 'accumulated_submission_time': 3131.6017882823944, 'accumulated_eval_time': 1140.665118932724, 'accumulated_logging_time': 0.23467564582824707}
I0305 20:24:03.996132 139545811003136 logging_writer.py:48] [15910] accumulated_eval_time=1140.67, accumulated_logging_time=0.234676, accumulated_submission_time=3131.6, global_step=15910, preemption_count=0, score=3131.6, test/accuracy=0.985763, test/loss=0.0471136, test/mean_average_precision=0.255184, test/num_examples=43793, total_duration=4272.91, train/accuracy=0.990726, train/loss=0.030436, train/mean_average_precision=0.401131, validation/accuracy=0.986547, validation/loss=0.0443859, validation/mean_average_precision=0.264878, validation/num_examples=43793
I0305 20:24:21.740796 139545802610432 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.07907987385988235, loss=0.034183382987976074
I0305 20:24:41.237968 139545811003136 logging_writer.py:48] [16100] global_step=16100, grad_norm=0.06042909622192383, loss=0.028110802173614502
I0305 20:25:00.793700 139545802610432 logging_writer.py:48] [16200] global_step=16200, grad_norm=0.09367247670888901, loss=0.0300743505358696
I0305 20:25:20.341298 139545811003136 logging_writer.py:48] [16300] global_step=16300, grad_norm=0.0790412425994873, loss=0.028642285615205765
I0305 20:25:39.892609 139545802610432 logging_writer.py:48] [16400] global_step=16400, grad_norm=0.0815364271402359, loss=0.032089442014694214
I0305 20:25:59.163049 139545811003136 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.06478365510702133, loss=0.031844280660152435
I0305 20:26:18.717079 139545802610432 logging_writer.py:48] [16600] global_step=16600, grad_norm=0.11406224966049194, loss=0.03042261302471161
I0305 20:26:38.253009 139545811003136 logging_writer.py:48] [16700] global_step=16700, grad_norm=0.06485402584075928, loss=0.02928783744573593
I0305 20:26:57.746108 139545802610432 logging_writer.py:48] [16800] global_step=16800, grad_norm=0.058764588087797165, loss=0.02940710075199604
I0305 20:27:17.296575 139545811003136 logging_writer.py:48] [16900] global_step=16900, grad_norm=0.09524404257535934, loss=0.0341574028134346
I0305 20:27:36.856071 139545802610432 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.08462829887866974, loss=0.03013390488922596
I0305 20:27:56.258834 139545811003136 logging_writer.py:48] [17100] global_step=17100, grad_norm=0.05624370276927948, loss=0.027998939156532288
I0305 20:28:04.064232 139687630427328 spec.py:321] Evaluating on the training split.
I0305 20:29:10.775747 139687630427328 spec.py:333] Evaluating on the validation split.
I0305 20:29:12.674797 139687630427328 spec.py:349] Evaluating on the test split.
I0305 20:29:14.477986 139687630427328 submission_runner.py:469] Time since start: 4583.41s, 	Step: 17141, 	{'train/accuracy': 0.9911357760429382, 'train/loss': 0.02902628481388092, 'train/mean_average_precision': 0.44185861078616373, 'validation/accuracy': 0.9866952300071716, 'validation/loss': 0.044143397361040115, 'validation/mean_average_precision': 0.26362411450218015, 'validation/num_examples': 43793, 'test/accuracy': 0.9859699606895447, 'test/loss': 0.046918243169784546, 'test/mean_average_precision': 0.2599515249035375, 'test/num_examples': 43793, 'score': 3371.6309180259705, 'total_duration': 4583.406119585037, 'accumulated_submission_time': 3371.6309180259705, 'accumulated_eval_time': 1211.0787291526794, 'accumulated_logging_time': 0.253375768661499}
I0305 20:29:14.487422 139545802610432 logging_writer.py:48] [17141] accumulated_eval_time=1211.08, accumulated_logging_time=0.253376, accumulated_submission_time=3371.63, global_step=17141, preemption_count=0, score=3371.63, test/accuracy=0.98597, test/loss=0.0469182, test/mean_average_precision=0.259952, test/num_examples=43793, total_duration=4583.41, train/accuracy=0.991136, train/loss=0.0290263, train/mean_average_precision=0.441859, validation/accuracy=0.986695, validation/loss=0.0441434, validation/mean_average_precision=0.263624, validation/num_examples=43793
I0305 20:29:26.203964 139545811003136 logging_writer.py:48] [17200] global_step=17200, grad_norm=0.13353973627090454, loss=0.03194337710738182
I0305 20:29:45.670754 139545802610432 logging_writer.py:48] [17300] global_step=17300, grad_norm=0.07273609191179276, loss=0.031143052503466606
I0305 20:30:05.214298 139545811003136 logging_writer.py:48] [17400] global_step=17400, grad_norm=0.07932672649621964, loss=0.032863799482584
I0305 20:30:24.616521 139545802610432 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.06740035861730576, loss=0.032343193888664246
I0305 20:30:44.333884 139545811003136 logging_writer.py:48] [17600] global_step=17600, grad_norm=0.06899619847536087, loss=0.028389381244778633
I0305 20:31:03.897374 139545802610432 logging_writer.py:48] [17700] global_step=17700, grad_norm=0.0772283747792244, loss=0.028722696006298065
I0305 20:31:23.573420 139545811003136 logging_writer.py:48] [17800] global_step=17800, grad_norm=0.0695289671421051, loss=0.030137410387396812
I0305 20:31:43.401743 139545802610432 logging_writer.py:48] [17900] global_step=17900, grad_norm=0.09335650503635406, loss=0.0348258800804615
I0305 20:32:02.975756 139545811003136 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.08741939812898636, loss=0.03317440301179886
I0305 20:32:22.387086 139545802610432 logging_writer.py:48] [18100] global_step=18100, grad_norm=0.07909316569566727, loss=0.029176460579037666
I0305 20:32:42.009592 139545811003136 logging_writer.py:48] [18200] global_step=18200, grad_norm=0.09413076937198639, loss=0.027898848056793213
I0305 20:33:01.662188 139545802610432 logging_writer.py:48] [18300] global_step=18300, grad_norm=0.07716436684131622, loss=0.032495565712451935
I0305 20:33:14.500286 139687630427328 spec.py:321] Evaluating on the training split.
I0305 20:34:20.717835 139687630427328 spec.py:333] Evaluating on the validation split.
I0305 20:34:22.602015 139687630427328 spec.py:349] Evaluating on the test split.
I0305 20:34:24.434030 139687630427328 submission_runner.py:469] Time since start: 4893.36s, 	Step: 18366, 	{'train/accuracy': 0.990848958492279, 'train/loss': 0.03000793792307377, 'train/mean_average_precision': 0.414172371098464, 'validation/accuracy': 0.9868409633636475, 'validation/loss': 0.044032733887434006, 'validation/mean_average_precision': 0.2691975763112511, 'validation/num_examples': 43793, 'test/accuracy': 0.9858849048614502, 'test/loss': 0.047124069184064865, 'test/mean_average_precision': 0.25930658453885996, 'test/num_examples': 43793, 'score': 3611.6044023036957, 'total_duration': 4893.362140893936, 'accumulated_submission_time': 3611.6044023036957, 'accumulated_eval_time': 1281.0123057365417, 'accumulated_logging_time': 0.2712233066558838}
I0305 20:34:24.444297 139545811003136 logging_writer.py:48] [18366] accumulated_eval_time=1281.01, accumulated_logging_time=0.271223, accumulated_submission_time=3611.6, global_step=18366, preemption_count=0, score=3611.6, test/accuracy=0.985885, test/loss=0.0471241, test/mean_average_precision=0.259307, test/num_examples=43793, total_duration=4893.36, train/accuracy=0.990849, train/loss=0.0300079, train/mean_average_precision=0.414172, validation/accuracy=0.986841, validation/loss=0.0440327, validation/mean_average_precision=0.269198, validation/num_examples=43793
I0305 20:34:31.304206 139545802610432 logging_writer.py:48] [18400] global_step=18400, grad_norm=0.06963039189577103, loss=0.03029734082520008
I0305 20:34:50.873358 139545811003136 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.10362817347049713, loss=0.028244441375136375
I0305 20:35:10.268799 139545802610432 logging_writer.py:48] [18600] global_step=18600, grad_norm=0.07451421767473221, loss=0.029614804312586784
I0305 20:35:29.757665 139545811003136 logging_writer.py:48] [18700] global_step=18700, grad_norm=0.0802001953125, loss=0.03170723840594292
I0305 20:35:49.308990 139545802610432 logging_writer.py:48] [18800] global_step=18800, grad_norm=0.0756826177239418, loss=0.02791445702314377
I0305 20:36:08.892979 139545811003136 logging_writer.py:48] [18900] global_step=18900, grad_norm=0.08764160424470901, loss=0.027694521471858025
I0305 20:36:28.391454 139545802610432 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.0735078752040863, loss=0.029943043366074562
I0305 20:36:47.951646 139545811003136 logging_writer.py:48] [19100] global_step=19100, grad_norm=0.09082608669996262, loss=0.028867505490779877
I0305 20:37:07.498162 139545802610432 logging_writer.py:48] [19200] global_step=19200, grad_norm=0.09320420026779175, loss=0.028196269646286964
I0305 20:37:27.085925 139545811003136 logging_writer.py:48] [19300] global_step=19300, grad_norm=0.08395032584667206, loss=0.0321047343313694
I0305 20:37:46.785002 139545802610432 logging_writer.py:48] [19400] global_step=19400, grad_norm=0.07922125607728958, loss=0.0296170711517334
I0305 20:38:06.429483 139545811003136 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.11284973472356796, loss=0.026432301849126816
I0305 20:38:24.456865 139687630427328 spec.py:321] Evaluating on the training split.
I0305 20:39:32.384765 139687630427328 spec.py:333] Evaluating on the validation split.
I0305 20:39:34.243802 139687630427328 spec.py:349] Evaluating on the test split.
I0305 20:39:36.077699 139687630427328 submission_runner.py:469] Time since start: 5205.01s, 	Step: 19593, 	{'train/accuracy': 0.9913619756698608, 'train/loss': 0.027951931580901146, 'train/mean_average_precision': 0.46056987141319133, 'validation/accuracy': 0.9867938756942749, 'validation/loss': 0.04424015060067177, 'validation/mean_average_precision': 0.2629588466396587, 'validation/num_examples': 43793, 'test/accuracy': 0.9859674572944641, 'test/loss': 0.046958308666944504, 'test/mean_average_precision': 0.2576620552458194, 'test/num_examples': 43793, 'score': 3851.577026128769, 'total_duration': 5205.005909204483, 'accumulated_submission_time': 3851.577026128769, 'accumulated_eval_time': 1352.633070230484, 'accumulated_logging_time': 0.2909684181213379}
I0305 20:39:36.088221 139545802610432 logging_writer.py:48] [19593] accumulated_eval_time=1352.63, accumulated_logging_time=0.290968, accumulated_submission_time=3851.58, global_step=19593, preemption_count=0, score=3851.58, test/accuracy=0.985967, test/loss=0.0469583, test/mean_average_precision=0.257662, test/num_examples=43793, total_duration=5205.01, train/accuracy=0.991362, train/loss=0.0279519, train/mean_average_precision=0.46057, validation/accuracy=0.986794, validation/loss=0.0442402, validation/mean_average_precision=0.262959, validation/num_examples=43793
I0305 20:39:37.665864 139545811003136 logging_writer.py:48] [19600] global_step=19600, grad_norm=0.07799604535102844, loss=0.02849319577217102
I0305 20:39:57.474287 139545802610432 logging_writer.py:48] [19700] global_step=19700, grad_norm=0.10560780018568039, loss=0.02766331471502781
I0305 20:40:17.166088 139545811003136 logging_writer.py:48] [19800] global_step=19800, grad_norm=0.07867591828107834, loss=0.029586641117930412
I0305 20:40:36.890517 139545802610432 logging_writer.py:48] [19900] global_step=19900, grad_norm=0.096717968583107, loss=0.026243049651384354
I0305 20:40:56.563930 139545811003136 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.09081888943910599, loss=0.026695409789681435
I0305 20:41:16.022850 139545802610432 logging_writer.py:48] [20100] global_step=20100, grad_norm=0.08755453675985336, loss=0.02985517866909504
I0305 20:41:35.632956 139545811003136 logging_writer.py:48] [20200] global_step=20200, grad_norm=0.09308922290802002, loss=0.02906688116490841
I0305 20:41:55.132380 139545802610432 logging_writer.py:48] [20300] global_step=20300, grad_norm=0.08369076997041702, loss=0.030457234010100365
I0305 20:42:14.684168 139545811003136 logging_writer.py:48] [20400] global_step=20400, grad_norm=0.1290566623210907, loss=0.030956316739320755
I0305 20:42:34.321881 139545802610432 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.09374283999204636, loss=0.029043808579444885
I0305 20:42:53.885872 139545811003136 logging_writer.py:48] [20600] global_step=20600, grad_norm=0.09043601900339127, loss=0.025873932987451553
I0305 20:43:13.617666 139545802610432 logging_writer.py:48] [20700] global_step=20700, grad_norm=0.08343301713466644, loss=0.026813454926013947
I0305 20:43:33.238687 139545811003136 logging_writer.py:48] [20800] global_step=20800, grad_norm=0.08944481611251831, loss=0.02764427289366722
I0305 20:43:36.187135 139687630427328 spec.py:321] Evaluating on the training split.
I0305 20:44:42.852128 139687630427328 spec.py:333] Evaluating on the validation split.
I0305 20:44:44.733807 139687630427328 spec.py:349] Evaluating on the test split.
I0305 20:44:46.558209 139687630427328 submission_runner.py:469] Time since start: 5515.49s, 	Step: 20816, 	{'train/accuracy': 0.9910523891448975, 'train/loss': 0.029222829267382622, 'train/mean_average_precision': 0.41778139345087284, 'validation/accuracy': 0.9867525100708008, 'validation/loss': 0.04440690204501152, 'validation/mean_average_precision': 0.27012270181055964, 'validation/num_examples': 43793, 'test/accuracy': 0.9858751893043518, 'test/loss': 0.04727625846862793, 'test/mean_average_precision': 0.2598736546946396, 'test/num_examples': 43793, 'score': 4091.637568950653, 'total_duration': 5515.48633646965, 'accumulated_submission_time': 4091.637568950653, 'accumulated_eval_time': 1423.0040102005005, 'accumulated_logging_time': 0.30994296073913574}
I0305 20:44:46.568775 139545802610432 logging_writer.py:48] [20816] accumulated_eval_time=1423, accumulated_logging_time=0.309943, accumulated_submission_time=4091.64, global_step=20816, preemption_count=0, score=4091.64, test/accuracy=0.985875, test/loss=0.0472763, test/mean_average_precision=0.259874, test/num_examples=43793, total_duration=5515.49, train/accuracy=0.991052, train/loss=0.0292228, train/mean_average_precision=0.417781, validation/accuracy=0.986753, validation/loss=0.0444069, validation/mean_average_precision=0.270123, validation/num_examples=43793
I0305 20:45:03.401184 139545811003136 logging_writer.py:48] [20900] global_step=20900, grad_norm=0.08929674327373505, loss=0.029289310798048973
I0305 20:45:22.983048 139545802610432 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.08103993535041809, loss=0.026322245597839355
I0305 20:45:42.621169 139545811003136 logging_writer.py:48] [21100] global_step=21100, grad_norm=0.09990058094263077, loss=0.02891240455210209
I0305 20:46:02.193660 139545802610432 logging_writer.py:48] [21200] global_step=21200, grad_norm=0.1559574007987976, loss=0.029215266928076744
I0305 20:46:21.941469 139545811003136 logging_writer.py:48] [21300] global_step=21300, grad_norm=0.0876622423529625, loss=0.028761206194758415
I0305 20:46:41.575254 139545802610432 logging_writer.py:48] [21400] global_step=21400, grad_norm=0.08794065564870834, loss=0.02997799776494503
I0305 20:47:01.340963 139545811003136 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.09931972622871399, loss=0.029981762170791626
I0305 20:47:21.181554 139545802610432 logging_writer.py:48] [21600] global_step=21600, grad_norm=0.09671685099601746, loss=0.026748187839984894
I0305 20:47:41.193419 139545811003136 logging_writer.py:48] [21700] global_step=21700, grad_norm=0.10562166571617126, loss=0.0339597687125206
I0305 20:48:01.085053 139545802610432 logging_writer.py:48] [21800] global_step=21800, grad_norm=0.09009762853384018, loss=0.03024006448686123
I0305 20:48:20.827753 139545811003136 logging_writer.py:48] [21900] global_step=21900, grad_norm=0.13879439234733582, loss=0.028144871816039085
I0305 20:48:40.513438 139545802610432 logging_writer.py:48] [22000] global_step=22000, grad_norm=0.11102587729692459, loss=0.031389858573675156
I0305 20:48:46.645616 139687630427328 spec.py:321] Evaluating on the training split.
I0305 20:49:55.504646 139687630427328 spec.py:333] Evaluating on the validation split.
I0305 20:49:57.340880 139687630427328 spec.py:349] Evaluating on the test split.
I0305 20:49:59.419050 139687630427328 submission_runner.py:469] Time since start: 5828.35s, 	Step: 22032, 	{'train/accuracy': 0.9913603663444519, 'train/loss': 0.028005976229906082, 'train/mean_average_precision': 0.44828949518350664, 'validation/accuracy': 0.9868925213813782, 'validation/loss': 0.04425288364291191, 'validation/mean_average_precision': 0.26733153866410436, 'validation/num_examples': 43793, 'test/accuracy': 0.9859998822212219, 'test/loss': 0.04715970158576965, 'test/mean_average_precision': 0.25995132085304334, 'test/num_examples': 43793, 'score': 4331.674557924271, 'total_duration': 5828.3471467494965, 'accumulated_submission_time': 4331.674557924271, 'accumulated_eval_time': 1495.7772662639618, 'accumulated_logging_time': 0.32903504371643066}
I0305 20:49:59.431723 139545811003136 logging_writer.py:48] [22032] accumulated_eval_time=1495.78, accumulated_logging_time=0.329035, accumulated_submission_time=4331.67, global_step=22032, preemption_count=0, score=4331.67, test/accuracy=0.986, test/loss=0.0471597, test/mean_average_precision=0.259951, test/num_examples=43793, total_duration=5828.35, train/accuracy=0.99136, train/loss=0.028006, train/mean_average_precision=0.448289, validation/accuracy=0.986893, validation/loss=0.0442529, validation/mean_average_precision=0.267332, validation/num_examples=43793
I0305 20:50:12.952190 139545802610432 logging_writer.py:48] [22100] global_step=22100, grad_norm=0.08189191669225693, loss=0.0262854415923357
I0305 20:50:32.548585 139545811003136 logging_writer.py:48] [22200] global_step=22200, grad_norm=0.09266279637813568, loss=0.02755938284099102
I0305 20:50:52.271024 139545802610432 logging_writer.py:48] [22300] global_step=22300, grad_norm=0.09121103584766388, loss=0.029160548001527786
I0305 20:51:11.853806 139545811003136 logging_writer.py:48] [22400] global_step=22400, grad_norm=0.10340815782546997, loss=0.029931005090475082
I0305 20:51:31.427126 139545802610432 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.09696879237890244, loss=0.03165776655077934
I0305 20:51:51.102183 139545811003136 logging_writer.py:48] [22600] global_step=22600, grad_norm=0.10556991398334503, loss=0.029742514714598656
I0305 20:52:10.854045 139545802610432 logging_writer.py:48] [22700] global_step=22700, grad_norm=0.11702863872051239, loss=0.027222074568271637
I0305 20:52:30.594350 139545811003136 logging_writer.py:48] [22800] global_step=22800, grad_norm=0.12328238040208817, loss=0.029547203332185745
I0305 20:52:50.355175 139545802610432 logging_writer.py:48] [22900] global_step=22900, grad_norm=0.08768604695796967, loss=0.0275508351624012
I0305 20:53:10.021336 139545811003136 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.09024307876825333, loss=0.029638823121786118
I0305 20:53:29.593404 139545802610432 logging_writer.py:48] [23100] global_step=23100, grad_norm=0.12332088500261307, loss=0.03161834180355072
I0305 20:53:49.334820 139545811003136 logging_writer.py:48] [23200] global_step=23200, grad_norm=0.12013792991638184, loss=0.03172236680984497
I0305 20:53:59.560210 139687630427328 spec.py:321] Evaluating on the training split.
I0305 20:55:07.827527 139687630427328 spec.py:333] Evaluating on the validation split.
I0305 20:55:09.722339 139687630427328 spec.py:349] Evaluating on the test split.
I0305 20:55:11.617520 139687630427328 submission_runner.py:469] Time since start: 6140.55s, 	Step: 23253, 	{'train/accuracy': 0.9916965365409851, 'train/loss': 0.02713916264474392, 'train/mean_average_precision': 0.47032236460966204, 'validation/accuracy': 0.9867314100265503, 'validation/loss': 0.044477157294750214, 'validation/mean_average_precision': 0.27236079983454947, 'validation/num_examples': 43793, 'test/accuracy': 0.9858899712562561, 'test/loss': 0.04728430137038231, 'test/mean_average_precision': 0.259879875471389, 'test/num_examples': 43793, 'score': 4571.760729074478, 'total_duration': 6140.5457010269165, 'accumulated_submission_time': 4571.760729074478, 'accumulated_eval_time': 1567.8344867229462, 'accumulated_logging_time': 0.35194921493530273}
I0305 20:55:11.629039 139545802610432 logging_writer.py:48] [23253] accumulated_eval_time=1567.83, accumulated_logging_time=0.351949, accumulated_submission_time=4571.76, global_step=23253, preemption_count=0, score=4571.76, test/accuracy=0.98589, test/loss=0.0472843, test/mean_average_precision=0.25988, test/num_examples=43793, total_duration=6140.55, train/accuracy=0.991697, train/loss=0.0271392, train/mean_average_precision=0.470322, validation/accuracy=0.986731, validation/loss=0.0444772, validation/mean_average_precision=0.272361, validation/num_examples=43793
I0305 20:55:21.023923 139545811003136 logging_writer.py:48] [23300] global_step=23300, grad_norm=0.11258023977279663, loss=0.03152574598789215
I0305 20:55:40.585626 139545802610432 logging_writer.py:48] [23400] global_step=23400, grad_norm=0.0991082489490509, loss=0.026255536824464798
I0305 20:56:00.181720 139545811003136 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.09283436089754105, loss=0.027345050126314163
I0305 20:56:19.839947 139545802610432 logging_writer.py:48] [23600] global_step=23600, grad_norm=0.11543405801057816, loss=0.026472249999642372
I0305 20:56:39.512778 139545811003136 logging_writer.py:48] [23700] global_step=23700, grad_norm=0.09321039170026779, loss=0.029979435727000237
I0305 20:56:59.202411 139545802610432 logging_writer.py:48] [23800] global_step=23800, grad_norm=0.10617361962795258, loss=0.02827225811779499
I0305 20:57:18.873955 139545811003136 logging_writer.py:48] [23900] global_step=23900, grad_norm=0.10689787566661835, loss=0.025664884597063065
I0305 20:57:38.704922 139545802610432 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.10298920422792435, loss=0.029128126800060272
I0305 20:57:58.529911 139545811003136 logging_writer.py:48] [24100] global_step=24100, grad_norm=0.09943050891160965, loss=0.029354291036725044
I0305 20:58:18.346508 139545802610432 logging_writer.py:48] [24200] global_step=24200, grad_norm=0.11446525901556015, loss=0.03325464576482773
I0305 20:58:37.911669 139545811003136 logging_writer.py:48] [24300] global_step=24300, grad_norm=0.10960496217012405, loss=0.030226340517401695
I0305 20:58:57.426631 139545802610432 logging_writer.py:48] [24400] global_step=24400, grad_norm=0.14023470878601074, loss=0.029298260807991028
I0305 20:59:11.655339 139687630427328 spec.py:321] Evaluating on the training split.
I0305 21:00:20.560172 139687630427328 spec.py:333] Evaluating on the validation split.
I0305 21:00:22.466509 139687630427328 spec.py:349] Evaluating on the test split.
I0305 21:00:24.283107 139687630427328 submission_runner.py:469] Time since start: 6453.21s, 	Step: 24474, 	{'train/accuracy': 0.9913771748542786, 'train/loss': 0.02795359119772911, 'train/mean_average_precision': 0.4639880945840712, 'validation/accuracy': 0.9866157174110413, 'validation/loss': 0.04471296817064285, 'validation/mean_average_precision': 0.2658794889031041, 'validation/num_examples': 43793, 'test/accuracy': 0.9857686161994934, 'test/loss': 0.0475628487765789, 'test/mean_average_precision': 0.26343391387702897, 'test/num_examples': 43793, 'score': 4811.74719452858, 'total_duration': 6453.2113468647, 'accumulated_submission_time': 4811.74719452858, 'accumulated_eval_time': 1640.462219953537, 'accumulated_logging_time': 0.3724641799926758}
I0305 21:00:24.293522 139545811003136 logging_writer.py:48] [24474] accumulated_eval_time=1640.46, accumulated_logging_time=0.372464, accumulated_submission_time=4811.75, global_step=24474, preemption_count=0, score=4811.75, test/accuracy=0.985769, test/loss=0.0475628, test/mean_average_precision=0.263434, test/num_examples=43793, total_duration=6453.21, train/accuracy=0.991377, train/loss=0.0279536, train/mean_average_precision=0.463988, validation/accuracy=0.986616, validation/loss=0.044713, validation/mean_average_precision=0.265879, validation/num_examples=43793
I0305 21:00:29.620587 139545802610432 logging_writer.py:48] [24500] global_step=24500, grad_norm=0.09426835179328918, loss=0.02802710235118866
I0305 21:00:49.348632 139545811003136 logging_writer.py:48] [24600] global_step=24600, grad_norm=0.09748143702745438, loss=0.029373949393630028
I0305 21:01:09.092575 139545802610432 logging_writer.py:48] [24700] global_step=24700, grad_norm=0.12141674757003784, loss=0.028410060331225395
I0305 21:01:28.862201 139545811003136 logging_writer.py:48] [24800] global_step=24800, grad_norm=0.12022411078214645, loss=0.029303977265954018
I0305 21:01:48.438517 139545802610432 logging_writer.py:48] [24900] global_step=24900, grad_norm=0.1096901074051857, loss=0.02755175344645977
I0305 21:02:07.972892 139545811003136 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.09921986609697342, loss=0.027231071144342422
I0305 21:02:27.649868 139545802610432 logging_writer.py:48] [25100] global_step=25100, grad_norm=0.10955845564603806, loss=0.025456732138991356
I0305 21:02:47.268948 139545811003136 logging_writer.py:48] [25200] global_step=25200, grad_norm=0.10874120146036148, loss=0.026235712692141533
I0305 21:03:07.022085 139545802610432 logging_writer.py:48] [25300] global_step=25300, grad_norm=0.12219377607107162, loss=0.026099080219864845
I0305 21:03:26.399325 139545811003136 logging_writer.py:48] [25400] global_step=25400, grad_norm=0.11458947509527206, loss=0.022738635540008545
I0305 21:03:46.036349 139545802610432 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.12650667130947113, loss=0.027658186852931976
I0305 21:04:05.685983 139545811003136 logging_writer.py:48] [25600] global_step=25600, grad_norm=0.14327295124530792, loss=0.029923805966973305
I0305 21:04:24.459486 139687630427328 spec.py:321] Evaluating on the training split.
I0305 21:05:29.447062 139687630427328 spec.py:333] Evaluating on the validation split.
I0305 21:05:31.332261 139687630427328 spec.py:349] Evaluating on the test split.
I0305 21:05:33.211012 139687630427328 submission_runner.py:469] Time since start: 6762.14s, 	Step: 25696, 	{'train/accuracy': 0.9922515749931335, 'train/loss': 0.02519526518881321, 'train/mean_average_precision': 0.5202277770118576, 'validation/accuracy': 0.986694872379303, 'validation/loss': 0.044626206159591675, 'validation/mean_average_precision': 0.27444973542798934, 'validation/num_examples': 43793, 'test/accuracy': 0.9858874082565308, 'test/loss': 0.047508157789707184, 'test/mean_average_precision': 0.2572674522705059, 'test/num_examples': 43793, 'score': 5051.872892379761, 'total_duration': 6762.139156103134, 'accumulated_submission_time': 5051.872892379761, 'accumulated_eval_time': 1709.213612318039, 'accumulated_logging_time': 0.39147210121154785}
I0305 21:05:33.222193 139545802610432 logging_writer.py:48] [25696] accumulated_eval_time=1709.21, accumulated_logging_time=0.391472, accumulated_submission_time=5051.87, global_step=25696, preemption_count=0, score=5051.87, test/accuracy=0.985887, test/loss=0.0475082, test/mean_average_precision=0.257267, test/num_examples=43793, total_duration=6762.14, train/accuracy=0.992252, train/loss=0.0251953, train/mean_average_precision=0.520228, validation/accuracy=0.986695, validation/loss=0.0446262, validation/mean_average_precision=0.27445, validation/num_examples=43793
I0305 21:05:34.221939 139545811003136 logging_writer.py:48] [25700] global_step=25700, grad_norm=0.09986097365617752, loss=0.02444063313305378
I0305 21:05:53.699688 139545802610432 logging_writer.py:48] [25800] global_step=25800, grad_norm=0.17462307214736938, loss=0.032712534070014954
I0305 21:06:12.985818 139545811003136 logging_writer.py:48] [25900] global_step=25900, grad_norm=0.10687378793954849, loss=0.027504058554768562
I0305 21:06:32.587414 139545802610432 logging_writer.py:48] [26000] global_step=26000, grad_norm=0.10996715724468231, loss=0.027988122776150703
I0305 21:06:52.146645 139545811003136 logging_writer.py:48] [26100] global_step=26100, grad_norm=0.12131591141223907, loss=0.028000015765428543
I0305 21:07:11.880687 139545802610432 logging_writer.py:48] [26200] global_step=26200, grad_norm=0.119905024766922, loss=0.028296662494540215
I0305 21:07:31.612363 139545811003136 logging_writer.py:48] [26300] global_step=26300, grad_norm=0.08935844898223877, loss=0.025438107550144196
I0305 21:07:51.484161 139545802610432 logging_writer.py:48] [26400] global_step=26400, grad_norm=0.10208234190940857, loss=0.024804184213280678
I0305 21:08:11.367809 139545811003136 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.15459591150283813, loss=0.031089341267943382
I0305 21:08:31.113812 139545802610432 logging_writer.py:48] [26600] global_step=26600, grad_norm=0.09307816624641418, loss=0.026934994384646416
I0305 21:08:50.840122 139545811003136 logging_writer.py:48] [26700] global_step=26700, grad_norm=0.10699029266834259, loss=0.028357870876789093
I0305 21:09:10.668697 139545802610432 logging_writer.py:48] [26800] global_step=26800, grad_norm=0.11245876550674438, loss=0.02524319663643837
I0305 21:09:30.566896 139545811003136 logging_writer.py:48] [26900] global_step=26900, grad_norm=0.10478956252336502, loss=0.026115920394659042
I0305 21:09:33.356840 139687630427328 spec.py:321] Evaluating on the training split.
I0305 21:10:38.697926 139687630427328 spec.py:333] Evaluating on the validation split.
I0305 21:10:40.583631 139687630427328 spec.py:349] Evaluating on the test split.
I0305 21:10:42.441923 139687630427328 submission_runner.py:469] Time since start: 7071.37s, 	Step: 26915, 	{'train/accuracy': 0.9915521740913391, 'train/loss': 0.026987168937921524, 'train/mean_average_precision': 0.4832061447395173, 'validation/accuracy': 0.9869270324707031, 'validation/loss': 0.04500902071595192, 'validation/mean_average_precision': 0.2708809412685069, 'validation/num_examples': 43793, 'test/accuracy': 0.9860883355140686, 'test/loss': 0.0479976125061512, 'test/mean_average_precision': 0.25808631938728016, 'test/num_examples': 43793, 'score': 5291.967322587967, 'total_duration': 7071.370110988617, 'accumulated_submission_time': 5291.967322587967, 'accumulated_eval_time': 1778.298603773117, 'accumulated_logging_time': 0.4122025966644287}
I0305 21:10:42.453121 139545802610432 logging_writer.py:48] [26915] accumulated_eval_time=1778.3, accumulated_logging_time=0.412203, accumulated_submission_time=5291.97, global_step=26915, preemption_count=0, score=5291.97, test/accuracy=0.986088, test/loss=0.0479976, test/mean_average_precision=0.258086, test/num_examples=43793, total_duration=7071.37, train/accuracy=0.991552, train/loss=0.0269872, train/mean_average_precision=0.483206, validation/accuracy=0.986927, validation/loss=0.045009, validation/mean_average_precision=0.270881, validation/num_examples=43793
I0305 21:10:59.422143 139545811003136 logging_writer.py:48] [27000] global_step=27000, grad_norm=0.12003786116838455, loss=0.027530455961823463
I0305 21:11:19.275435 139545802610432 logging_writer.py:48] [27100] global_step=27100, grad_norm=0.14310450851917267, loss=0.025395242497324944
I0305 21:11:39.010308 139545811003136 logging_writer.py:48] [27200] global_step=27200, grad_norm=0.11291316896677017, loss=0.03192642331123352
I0305 21:11:58.623683 139545802610432 logging_writer.py:48] [27300] global_step=27300, grad_norm=0.13461552560329437, loss=0.03140910714864731
I0305 21:12:18.196114 139545811003136 logging_writer.py:48] [27400] global_step=27400, grad_norm=0.10679203271865845, loss=0.02524307370185852
I0305 21:12:37.992590 139545802610432 logging_writer.py:48] [27500] global_step=27500, grad_norm=0.10582441091537476, loss=0.02519383653998375
I0305 21:12:57.840228 139545811003136 logging_writer.py:48] [27600] global_step=27600, grad_norm=0.10505370795726776, loss=0.02597205713391304
I0305 21:13:17.506921 139545802610432 logging_writer.py:48] [27700] global_step=27700, grad_norm=0.1213395744562149, loss=0.02526187151670456
I0305 21:13:37.144030 139545811003136 logging_writer.py:48] [27800] global_step=27800, grad_norm=0.12475509941577911, loss=0.027439579367637634
I0305 21:13:56.928131 139545802610432 logging_writer.py:48] [27900] global_step=27900, grad_norm=0.13243083655834198, loss=0.0287846140563488
I0305 21:14:16.617055 139545811003136 logging_writer.py:48] [28000] global_step=28000, grad_norm=0.13161005079746246, loss=0.02279484085738659
I0305 21:14:36.153964 139545802610432 logging_writer.py:48] [28100] global_step=28100, grad_norm=0.11963024735450745, loss=0.02649046666920185
I0305 21:14:42.589519 139687630427328 spec.py:321] Evaluating on the training split.
I0305 21:15:50.751104 139687630427328 spec.py:333] Evaluating on the validation split.
I0305 21:15:52.665875 139687630427328 spec.py:349] Evaluating on the test split.
I0305 21:15:54.521296 139687630427328 submission_runner.py:469] Time since start: 7383.45s, 	Step: 28134, 	{'train/accuracy': 0.9922338724136353, 'train/loss': 0.024968398734927177, 'train/mean_average_precision': 0.5279221973231529, 'validation/accuracy': 0.9868133664131165, 'validation/loss': 0.04479990899562836, 'validation/mean_average_precision': 0.27574563305826694, 'validation/num_examples': 43793, 'test/accuracy': 0.9859544038772583, 'test/loss': 0.04763316735625267, 'test/mean_average_precision': 0.26170569238711117, 'test/num_examples': 43793, 'score': 5532.063780069351, 'total_duration': 7383.449518203735, 'accumulated_submission_time': 5532.063780069351, 'accumulated_eval_time': 1850.230328321457, 'accumulated_logging_time': 0.43329620361328125}
I0305 21:15:54.532326 139545811003136 logging_writer.py:48] [28134] accumulated_eval_time=1850.23, accumulated_logging_time=0.433296, accumulated_submission_time=5532.06, global_step=28134, preemption_count=0, score=5532.06, test/accuracy=0.985954, test/loss=0.0476332, test/mean_average_precision=0.261706, test/num_examples=43793, total_duration=7383.45, train/accuracy=0.992234, train/loss=0.0249684, train/mean_average_precision=0.527922, validation/accuracy=0.986813, validation/loss=0.0447999, validation/mean_average_precision=0.275746, validation/num_examples=43793
I0305 21:16:07.803430 139545802610432 logging_writer.py:48] [28200] global_step=28200, grad_norm=0.11848831921815872, loss=0.024203011766076088
I0305 21:16:27.351999 139545811003136 logging_writer.py:48] [28300] global_step=28300, grad_norm=0.13555751740932465, loss=0.026292258873581886
I0305 21:16:46.978370 139545802610432 logging_writer.py:48] [28400] global_step=28400, grad_norm=0.12424246966838837, loss=0.029405731707811356
I0305 21:17:06.665371 139545811003136 logging_writer.py:48] [28500] global_step=28500, grad_norm=0.18793387711048126, loss=0.02577904425561428
I0305 21:17:26.542445 139545802610432 logging_writer.py:48] [28600] global_step=28600, grad_norm=0.10617054998874664, loss=0.025316618382930756
I0305 21:17:46.229894 139545811003136 logging_writer.py:48] [28700] global_step=28700, grad_norm=0.10471779108047485, loss=0.02377026341855526
I0305 21:18:06.038881 139545802610432 logging_writer.py:48] [28800] global_step=28800, grad_norm=0.11762692034244537, loss=0.0228422898799181
I0305 21:18:25.750709 139545811003136 logging_writer.py:48] [28900] global_step=28900, grad_norm=0.1262587010860443, loss=0.02436048351228237
I0305 21:18:45.542085 139545802610432 logging_writer.py:48] [29000] global_step=29000, grad_norm=0.1309971660375595, loss=0.02622152306139469
I0305 21:19:05.240157 139545811003136 logging_writer.py:48] [29100] global_step=29100, grad_norm=0.18282654881477356, loss=0.02677878551185131
I0305 21:19:25.065508 139545802610432 logging_writer.py:48] [29200] global_step=29200, grad_norm=0.12076888978481293, loss=0.027166670188307762
I0305 21:19:44.845139 139545811003136 logging_writer.py:48] [29300] global_step=29300, grad_norm=0.1237875446677208, loss=0.0246384609490633
I0305 21:19:54.526597 139687630427328 spec.py:321] Evaluating on the training split.
I0305 21:21:01.101573 139687630427328 spec.py:333] Evaluating on the validation split.
I0305 21:21:03.006702 139687630427328 spec.py:349] Evaluating on the test split.
I0305 21:21:04.906880 139687630427328 submission_runner.py:469] Time since start: 7693.84s, 	Step: 29350, 	{'train/accuracy': 0.9921098351478577, 'train/loss': 0.02539261430501938, 'train/mean_average_precision': 0.5171862517074713, 'validation/accuracy': 0.9866664409637451, 'validation/loss': 0.04558609053492546, 'validation/mean_average_precision': 0.27234923305241, 'validation/num_examples': 43793, 'test/accuracy': 0.9857168197631836, 'test/loss': 0.04859912768006325, 'test/mean_average_precision': 0.2582175632022682, 'test/num_examples': 43793, 'score': 5771.846256017685, 'total_duration': 7693.835065603256, 'accumulated_submission_time': 5771.846256017685, 'accumulated_eval_time': 1920.6105341911316, 'accumulated_logging_time': 0.6227314472198486}
I0305 21:21:04.917898 139545802610432 logging_writer.py:48] [29350] accumulated_eval_time=1920.61, accumulated_logging_time=0.622731, accumulated_submission_time=5771.85, global_step=29350, preemption_count=0, score=5771.85, test/accuracy=0.985717, test/loss=0.0485991, test/mean_average_precision=0.258218, test/num_examples=43793, total_duration=7693.84, train/accuracy=0.99211, train/loss=0.0253926, train/mean_average_precision=0.517186, validation/accuracy=0.986666, validation/loss=0.0455861, validation/mean_average_precision=0.272349, validation/num_examples=43793
I0305 21:21:15.139119 139545811003136 logging_writer.py:48] [29400] global_step=29400, grad_norm=0.13266967236995697, loss=0.026695474982261658
I0305 21:21:34.783378 139545802610432 logging_writer.py:48] [29500] global_step=29500, grad_norm=0.12340600043535233, loss=0.024549489840865135
I0305 21:21:54.447162 139545811003136 logging_writer.py:48] [29600] global_step=29600, grad_norm=0.11603310704231262, loss=0.02606254816055298
I0305 21:22:14.328226 139545802610432 logging_writer.py:48] [29700] global_step=29700, grad_norm=0.13120286166667938, loss=0.025099383667111397
I0305 21:22:34.003502 139545811003136 logging_writer.py:48] [29800] global_step=29800, grad_norm=0.12775783240795135, loss=0.024053147062659264
I0305 21:22:53.679421 139545802610432 logging_writer.py:48] [29900] global_step=29900, grad_norm=0.1357465535402298, loss=0.025951756164431572
I0305 21:23:13.361667 139545811003136 logging_writer.py:48] [30000] global_step=30000, grad_norm=0.13715463876724243, loss=0.026103487238287926
I0305 21:23:33.049251 139545802610432 logging_writer.py:48] [30100] global_step=30100, grad_norm=0.12730927765369415, loss=0.0270874984562397
I0305 21:23:52.782933 139545811003136 logging_writer.py:48] [30200] global_step=30200, grad_norm=0.13311557471752167, loss=0.02678394503891468
I0305 21:24:12.622323 139545802610432 logging_writer.py:48] [30300] global_step=30300, grad_norm=0.14077965915203094, loss=0.023609697818756104
I0305 21:24:32.393548 139545811003136 logging_writer.py:48] [30400] global_step=30400, grad_norm=0.15033993124961853, loss=0.026960894465446472
I0305 21:24:52.208023 139545802610432 logging_writer.py:48] [30500] global_step=30500, grad_norm=0.16253750026226044, loss=0.025059405714273453
I0305 21:25:05.048119 139687630427328 spec.py:321] Evaluating on the training split.
I0305 21:26:09.606010 139687630427328 spec.py:333] Evaluating on the validation split.
I0305 21:26:11.535063 139687630427328 spec.py:349] Evaluating on the test split.
I0305 21:26:13.380008 139687630427328 submission_runner.py:469] Time since start: 8002.31s, 	Step: 30566, 	{'train/accuracy': 0.9924203753471375, 'train/loss': 0.024254292249679565, 'train/mean_average_precision': 0.5492674977421617, 'validation/accuracy': 0.9867610335350037, 'validation/loss': 0.04547472298145294, 'validation/mean_average_precision': 0.27684071807128124, 'validation/num_examples': 43793, 'test/accuracy': 0.9858629703521729, 'test/loss': 0.04846695810556412, 'test/mean_average_precision': 0.2593116743688929, 'test/num_examples': 43793, 'score': 6011.936239719391, 'total_duration': 8002.30813908577, 'accumulated_submission_time': 6011.936239719391, 'accumulated_eval_time': 1988.9422948360443, 'accumulated_logging_time': 0.6426005363464355}
I0305 21:26:13.392752 139545811003136 logging_writer.py:48] [30566] accumulated_eval_time=1988.94, accumulated_logging_time=0.642601, accumulated_submission_time=6011.94, global_step=30566, preemption_count=0, score=6011.94, test/accuracy=0.985863, test/loss=0.048467, test/mean_average_precision=0.259312, test/num_examples=43793, total_duration=8002.31, train/accuracy=0.99242, train/loss=0.0242543, train/mean_average_precision=0.549267, validation/accuracy=0.986761, validation/loss=0.0454747, validation/mean_average_precision=0.276841, validation/num_examples=43793
I0305 21:26:20.508315 139545802610432 logging_writer.py:48] [30600] global_step=30600, grad_norm=0.14209400117397308, loss=0.025041179731488228
I0305 21:26:40.538729 139545811003136 logging_writer.py:48] [30700] global_step=30700, grad_norm=0.1200714036822319, loss=0.02512582205235958
I0305 21:27:00.521950 139545802610432 logging_writer.py:48] [30800] global_step=30800, grad_norm=0.14570344984531403, loss=0.02583015337586403
I0305 21:27:20.624203 139545811003136 logging_writer.py:48] [30900] global_step=30900, grad_norm=0.13706542551517487, loss=0.025249291211366653
I0305 21:27:40.908327 139545802610432 logging_writer.py:48] [31000] global_step=31000, grad_norm=0.11804532259702682, loss=0.021931037306785583
I0305 21:28:01.231860 139545811003136 logging_writer.py:48] [31100] global_step=31100, grad_norm=0.15150290727615356, loss=0.026484593749046326
I0305 21:28:20.972101 139545802610432 logging_writer.py:48] [31200] global_step=31200, grad_norm=0.14774568378925323, loss=0.024119947105646133
I0305 21:28:40.824989 139545811003136 logging_writer.py:48] [31300] global_step=31300, grad_norm=0.15886551141738892, loss=0.024261314421892166
I0305 21:29:00.857939 139545802610432 logging_writer.py:48] [31400] global_step=31400, grad_norm=0.14636141061782837, loss=0.023271480575203896
I0305 21:29:20.788812 139545811003136 logging_writer.py:48] [31500] global_step=31500, grad_norm=0.15184909105300903, loss=0.02600143477320671
I0305 21:29:40.594738 139545802610432 logging_writer.py:48] [31600] global_step=31600, grad_norm=0.13291864097118378, loss=0.022960621863603592
I0305 21:30:00.305293 139545811003136 logging_writer.py:48] [31700] global_step=31700, grad_norm=0.14776071906089783, loss=0.02540680021047592
I0305 21:30:13.500447 139687630427328 spec.py:321] Evaluating on the training split.
I0305 21:31:22.649851 139687630427328 spec.py:333] Evaluating on the validation split.
I0305 21:31:24.510507 139687630427328 spec.py:349] Evaluating on the test split.
I0305 21:31:26.397042 139687630427328 submission_runner.py:469] Time since start: 8315.33s, 	Step: 31767, 	{'train/accuracy': 0.9928501844406128, 'train/loss': 0.02290358953177929, 'train/mean_average_precision': 0.571842355145451, 'validation/accuracy': 0.9867390990257263, 'validation/loss': 0.046047911047935486, 'validation/mean_average_precision': 0.2789651747140922, 'validation/num_examples': 43793, 'test/accuracy': 0.9858962893486023, 'test/loss': 0.049145907163619995, 'test/mean_average_precision': 0.26299917606479417, 'test/num_examples': 43793, 'score': 6252.002446651459, 'total_duration': 8315.325271844864, 'accumulated_submission_time': 6252.002446651459, 'accumulated_eval_time': 2061.8388397693634, 'accumulated_logging_time': 0.6643259525299072}
I0305 21:31:26.409380 139545802610432 logging_writer.py:48] [31767] accumulated_eval_time=2061.84, accumulated_logging_time=0.664326, accumulated_submission_time=6252, global_step=31767, preemption_count=0, score=6252, test/accuracy=0.985896, test/loss=0.0491459, test/mean_average_precision=0.262999, test/num_examples=43793, total_duration=8315.33, train/accuracy=0.99285, train/loss=0.0229036, train/mean_average_precision=0.571842, validation/accuracy=0.986739, validation/loss=0.0460479, validation/mean_average_precision=0.278965, validation/num_examples=43793
I0305 21:31:33.256301 139545811003136 logging_writer.py:48] [31800] global_step=31800, grad_norm=0.16734842956066132, loss=0.023762742057442665
I0305 21:31:53.138974 139545802610432 logging_writer.py:48] [31900] global_step=31900, grad_norm=0.1432359218597412, loss=0.02630752883851528
I0305 21:32:12.880766 139545811003136 logging_writer.py:48] [32000] global_step=32000, grad_norm=0.12570606172084808, loss=0.022771216928958893
I0305 21:32:32.604546 139545802610432 logging_writer.py:48] [32100] global_step=32100, grad_norm=0.14336274564266205, loss=0.02745169773697853
I0305 21:32:52.422628 139545811003136 logging_writer.py:48] [32200] global_step=32200, grad_norm=0.14153870940208435, loss=0.02199726551771164
I0305 21:33:12.376286 139545802610432 logging_writer.py:48] [32300] global_step=32300, grad_norm=0.1672956496477127, loss=0.025181228294968605
I0305 21:33:32.299912 139545811003136 logging_writer.py:48] [32400] global_step=32400, grad_norm=0.17474325001239777, loss=0.02289416640996933
I0305 21:33:52.069821 139545802610432 logging_writer.py:48] [32500] global_step=32500, grad_norm=0.14070554077625275, loss=0.02080351486802101
I0305 21:34:11.737442 139545811003136 logging_writer.py:48] [32600] global_step=32600, grad_norm=0.14370699226856232, loss=0.02472493425011635
I0305 21:34:31.507815 139545802610432 logging_writer.py:48] [32700] global_step=32700, grad_norm=0.13174471259117126, loss=0.022343266755342484
I0305 21:34:51.410654 139545811003136 logging_writer.py:48] [32800] global_step=32800, grad_norm=0.16373267769813538, loss=0.030260037630796432
I0305 21:35:11.123000 139545802610432 logging_writer.py:48] [32900] global_step=32900, grad_norm=0.12664557993412018, loss=0.022369006648659706
I0305 21:35:26.554361 139687630427328 spec.py:321] Evaluating on the training split.
I0305 21:36:31.853026 139687630427328 spec.py:333] Evaluating on the validation split.
I0305 21:36:33.734618 139687630427328 spec.py:349] Evaluating on the test split.
I0305 21:36:35.571749 139687630427328 submission_runner.py:469] Time since start: 8624.50s, 	Step: 32979, 	{'train/accuracy': 0.9926832914352417, 'train/loss': 0.023266520351171494, 'train/mean_average_precision': 0.5640265364883741, 'validation/accuracy': 0.9866628050804138, 'validation/loss': 0.04620925709605217, 'validation/mean_average_precision': 0.2764671411005253, 'validation/num_examples': 43793, 'test/accuracy': 0.9858827590942383, 'test/loss': 0.0491117425262928, 'test/mean_average_precision': 0.26013421347074905, 'test/num_examples': 43793, 'score': 6492.1075756549835, 'total_duration': 8624.499893426895, 'accumulated_submission_time': 6492.1075756549835, 'accumulated_eval_time': 2130.8560919761658, 'accumulated_logging_time': 0.6854081153869629}
I0305 21:36:35.584031 139545811003136 logging_writer.py:48] [32979] accumulated_eval_time=2130.86, accumulated_logging_time=0.685408, accumulated_submission_time=6492.11, global_step=32979, preemption_count=0, score=6492.11, test/accuracy=0.985883, test/loss=0.0491117, test/mean_average_precision=0.260134, test/num_examples=43793, total_duration=8624.5, train/accuracy=0.992683, train/loss=0.0232665, train/mean_average_precision=0.564027, validation/accuracy=0.986663, validation/loss=0.0462093, validation/mean_average_precision=0.276467, validation/num_examples=43793
I0305 21:36:39.924935 139545802610432 logging_writer.py:48] [33000] global_step=33000, grad_norm=0.15320530533790588, loss=0.024820378050208092
I0305 21:36:59.756440 139545811003136 logging_writer.py:48] [33100] global_step=33100, grad_norm=0.1610969603061676, loss=0.020291689783334732
I0305 21:37:19.607650 139545802610432 logging_writer.py:48] [33200] global_step=33200, grad_norm=0.1626882553100586, loss=0.021935848519206047
I0305 21:37:39.376725 139545811003136 logging_writer.py:48] [33300] global_step=33300, grad_norm=0.16805236041545868, loss=0.02268565632402897
I0305 21:37:58.925513 139545802610432 logging_writer.py:48] [33400] global_step=33400, grad_norm=0.13206489384174347, loss=0.02107979916036129
I0305 21:38:18.856997 139545811003136 logging_writer.py:48] [33500] global_step=33500, grad_norm=0.1611703783273697, loss=0.023055877536535263
I0305 21:38:38.710720 139545802610432 logging_writer.py:48] [33600] global_step=33600, grad_norm=0.1672765463590622, loss=0.022922124713659286
I0305 21:38:58.488384 139545811003136 logging_writer.py:48] [33700] global_step=33700, grad_norm=0.16094595193862915, loss=0.022751007229089737
I0305 21:39:18.295685 139545802610432 logging_writer.py:48] [33800] global_step=33800, grad_norm=0.2053573727607727, loss=0.024653898552060127
I0305 21:39:37.975531 139545811003136 logging_writer.py:48] [33900] global_step=33900, grad_norm=0.14889279007911682, loss=0.022834643721580505
I0305 21:39:57.754564 139545802610432 logging_writer.py:48] [34000] global_step=34000, grad_norm=0.15597744286060333, loss=0.02172945626080036
I0305 21:40:17.507766 139545811003136 logging_writer.py:48] [34100] global_step=34100, grad_norm=0.1739138513803482, loss=0.026853930205106735
I0305 21:40:35.763983 139687630427328 spec.py:321] Evaluating on the training split.
I0305 21:41:42.430725 139687630427328 spec.py:333] Evaluating on the validation split.
I0305 21:41:44.419971 139687630427328 spec.py:349] Evaluating on the test split.
I0305 21:41:46.327128 139687630427328 submission_runner.py:469] Time since start: 8935.26s, 	Step: 34193, 	{'train/accuracy': 0.9934563636779785, 'train/loss': 0.02103402651846409, 'train/mean_average_precision': 0.614450689646107, 'validation/accuracy': 0.9866648316383362, 'validation/loss': 0.0466645322740078, 'validation/mean_average_precision': 0.27216556177244544, 'validation/num_examples': 43793, 'test/accuracy': 0.985775351524353, 'test/loss': 0.04976813867688179, 'test/mean_average_precision': 0.25983604405647626, 'test/num_examples': 43793, 'score': 6732.248635292053, 'total_duration': 8935.255256414413, 'accumulated_submission_time': 6732.248635292053, 'accumulated_eval_time': 2201.4190924167633, 'accumulated_logging_time': 0.7073702812194824}
I0305 21:41:46.340870 139545802610432 logging_writer.py:48] [34193] accumulated_eval_time=2201.42, accumulated_logging_time=0.70737, accumulated_submission_time=6732.25, global_step=34193, preemption_count=0, score=6732.25, test/accuracy=0.985775, test/loss=0.0497681, test/mean_average_precision=0.259836, test/num_examples=43793, total_duration=8935.26, train/accuracy=0.993456, train/loss=0.021034, train/mean_average_precision=0.614451, validation/accuracy=0.986665, validation/loss=0.0466645, validation/mean_average_precision=0.272166, validation/num_examples=43793
I0305 21:41:48.022947 139545811003136 logging_writer.py:48] [34200] global_step=34200, grad_norm=0.17911307513713837, loss=0.02265542931854725
I0305 21:42:08.090847 139545802610432 logging_writer.py:48] [34300] global_step=34300, grad_norm=0.16090336441993713, loss=0.02283541113138199
I0305 21:42:28.068931 139545811003136 logging_writer.py:48] [34400] global_step=34400, grad_norm=0.1750887930393219, loss=0.024415750056505203
I0305 21:42:47.773180 139545802610432 logging_writer.py:48] [34500] global_step=34500, grad_norm=0.15488362312316895, loss=0.021732892841100693
I0305 21:43:07.575467 139545811003136 logging_writer.py:48] [34600] global_step=34600, grad_norm=0.18183226883411407, loss=0.021983323618769646
I0305 21:43:27.718457 139545802610432 logging_writer.py:48] [34700] global_step=34700, grad_norm=0.1586993932723999, loss=0.021093690767884254
I0305 21:43:47.832996 139545811003136 logging_writer.py:48] [34800] global_step=34800, grad_norm=0.1743268072605133, loss=0.02405663952231407
I0305 21:44:07.779553 139545802610432 logging_writer.py:48] [34900] global_step=34900, grad_norm=0.1578405201435089, loss=0.019841333851218224
I0305 21:44:27.693240 139545811003136 logging_writer.py:48] [35000] global_step=35000, grad_norm=0.17186102271080017, loss=0.02159138396382332
I0305 21:44:47.691810 139545802610432 logging_writer.py:48] [35100] global_step=35100, grad_norm=0.16203799843788147, loss=0.02093220129609108
I0305 21:45:07.809170 139545811003136 logging_writer.py:48] [35200] global_step=35200, grad_norm=0.18100380897521973, loss=0.020298965275287628
I0305 21:45:27.885262 139545802610432 logging_writer.py:48] [35300] global_step=35300, grad_norm=0.17313022911548615, loss=0.021192234009504318
I0305 21:45:46.436802 139687630427328 spec.py:321] Evaluating on the training split.
I0305 21:46:53.242313 139687630427328 spec.py:333] Evaluating on the validation split.
I0305 21:46:55.165222 139687630427328 spec.py:349] Evaluating on the test split.
I0305 21:46:56.984074 139687630427328 submission_runner.py:469] Time since start: 9245.91s, 	Step: 35394, 	{'train/accuracy': 0.9929504990577698, 'train/loss': 0.022186126559972763, 'train/mean_average_precision': 0.5776467551911265, 'validation/accuracy': 0.9866635799407959, 'validation/loss': 0.04756147041916847, 'validation/mean_average_precision': 0.2751614352413955, 'validation/num_examples': 43793, 'test/accuracy': 0.9858301281929016, 'test/loss': 0.050821080803871155, 'test/mean_average_precision': 0.25950086093037295, 'test/num_examples': 43793, 'score': 6972.303344249725, 'total_duration': 9245.912311553955, 'accumulated_submission_time': 6972.303344249725, 'accumulated_eval_time': 2271.966322660446, 'accumulated_logging_time': 0.7301015853881836}
I0305 21:46:56.995407 139545811003136 logging_writer.py:48] [35394] accumulated_eval_time=2271.97, accumulated_logging_time=0.730102, accumulated_submission_time=6972.3, global_step=35394, preemption_count=0, score=6972.3, test/accuracy=0.98583, test/loss=0.0508211, test/mean_average_precision=0.259501, test/num_examples=43793, total_duration=9245.91, train/accuracy=0.99295, train/loss=0.0221861, train/mean_average_precision=0.577647, validation/accuracy=0.986664, validation/loss=0.0475615, validation/mean_average_precision=0.275161, validation/num_examples=43793
I0305 21:46:58.444406 139545802610432 logging_writer.py:48] [35400] global_step=35400, grad_norm=0.18705621361732483, loss=0.01899164915084839
I0305 21:47:18.169238 139545811003136 logging_writer.py:48] [35500] global_step=35500, grad_norm=0.1590813398361206, loss=0.021234404295682907
I0305 21:47:37.954144 139545802610432 logging_writer.py:48] [35600] global_step=35600, grad_norm=0.15516257286071777, loss=0.02150890976190567
I0305 21:47:57.685443 139545811003136 logging_writer.py:48] [35700] global_step=35700, grad_norm=0.18080385029315948, loss=0.022669609636068344
I0305 21:48:17.464423 139545802610432 logging_writer.py:48] [35800] global_step=35800, grad_norm=0.18502674996852875, loss=0.024118607863783836
I0305 21:48:37.276567 139545811003136 logging_writer.py:48] [35900] global_step=35900, grad_norm=0.15091665089130402, loss=0.020562341436743736
I0305 21:48:56.974741 139545802610432 logging_writer.py:48] [36000] global_step=36000, grad_norm=0.16916455328464508, loss=0.020037781447172165
I0305 21:49:16.620200 139545811003136 logging_writer.py:48] [36100] global_step=36100, grad_norm=0.18259581923484802, loss=0.02077695168554783
I0305 21:49:36.341422 139545802610432 logging_writer.py:48] [36200] global_step=36200, grad_norm=0.17999126017093658, loss=0.02014225535094738
I0305 21:49:56.106419 139545811003136 logging_writer.py:48] [36300] global_step=36300, grad_norm=0.19689033925533295, loss=0.0213598795235157
I0305 21:50:15.898522 139545802610432 logging_writer.py:48] [36400] global_step=36400, grad_norm=0.1638883799314499, loss=0.019962193444371223
I0305 21:50:35.803378 139545811003136 logging_writer.py:48] [36500] global_step=36500, grad_norm=0.19285961985588074, loss=0.021240554749965668
I0305 21:50:55.642245 139545802610432 logging_writer.py:48] [36600] global_step=36600, grad_norm=0.21910922229290009, loss=0.02292540669441223
I0305 21:50:57.035433 139687630427328 spec.py:321] Evaluating on the training split.
I0305 21:52:02.494202 139687630427328 spec.py:333] Evaluating on the validation split.
I0305 21:52:04.365306 139687630427328 spec.py:349] Evaluating on the test split.
I0305 21:52:06.200961 139687630427328 submission_runner.py:469] Time since start: 9555.13s, 	Step: 36608, 	{'train/accuracy': 0.9939799308776855, 'train/loss': 0.01895810477435589, 'train/mean_average_precision': 0.6490916796336258, 'validation/accuracy': 0.9865085482597351, 'validation/loss': 0.04855643957853317, 'validation/mean_average_precision': 0.2744268083014169, 'validation/num_examples': 43793, 'test/accuracy': 0.985675573348999, 'test/loss': 0.05171124264597893, 'test/mean_average_precision': 0.2612139139727055, 'test/num_examples': 43793, 'score': 7212.3032240867615, 'total_duration': 9555.129102945328, 'accumulated_submission_time': 7212.3032240867615, 'accumulated_eval_time': 2341.1317133903503, 'accumulated_logging_time': 0.7511348724365234}
I0305 21:52:06.213914 139545811003136 logging_writer.py:48] [36608] accumulated_eval_time=2341.13, accumulated_logging_time=0.751135, accumulated_submission_time=7212.3, global_step=36608, preemption_count=0, score=7212.3, test/accuracy=0.985676, test/loss=0.0517112, test/mean_average_precision=0.261214, test/num_examples=43793, total_duration=9555.13, train/accuracy=0.99398, train/loss=0.0189581, train/mean_average_precision=0.649092, validation/accuracy=0.986509, validation/loss=0.0485564, validation/mean_average_precision=0.274427, validation/num_examples=43793
I0305 21:52:24.791793 139545802610432 logging_writer.py:48] [36700] global_step=36700, grad_norm=0.18868625164031982, loss=0.021359754726290703
I0305 21:52:44.667194 139545811003136 logging_writer.py:48] [36800] global_step=36800, grad_norm=0.1865295171737671, loss=0.020111454650759697
I0305 21:53:04.464587 139545802610432 logging_writer.py:48] [36900] global_step=36900, grad_norm=0.16898831725120544, loss=0.019355008378624916
I0305 21:53:24.110356 139545811003136 logging_writer.py:48] [37000] global_step=37000, grad_norm=0.172191321849823, loss=0.022684568539261818
I0305 21:53:43.900192 139545802610432 logging_writer.py:48] [37100] global_step=37100, grad_norm=0.18965932726860046, loss=0.020297382026910782
I0305 21:54:03.554981 139545811003136 logging_writer.py:48] [37200] global_step=37200, grad_norm=0.21389813721179962, loss=0.01859528198838234
I0305 21:54:23.322922 139545802610432 logging_writer.py:48] [37300] global_step=37300, grad_norm=0.1973138451576233, loss=0.019601749256253242
I0305 21:54:43.579900 139545811003136 logging_writer.py:48] [37400] global_step=37400, grad_norm=0.19288386404514313, loss=0.02168717049062252
I0305 21:55:03.830867 139545802610432 logging_writer.py:48] [37500] global_step=37500, grad_norm=0.1956034153699875, loss=0.020258691161870956
I0305 21:55:24.200182 139545811003136 logging_writer.py:48] [37600] global_step=37600, grad_norm=0.2350827008485794, loss=0.020833102986216545
I0305 21:55:44.483934 139545802610432 logging_writer.py:48] [37700] global_step=37700, grad_norm=0.18826332688331604, loss=0.01865868829190731
I0305 21:56:04.802570 139545811003136 logging_writer.py:48] [37800] global_step=37800, grad_norm=0.19620762765407562, loss=0.018373260274529457
I0305 21:56:06.215797 139687630427328 spec.py:321] Evaluating on the training split.
I0305 21:57:10.525811 139687630427328 spec.py:333] Evaluating on the validation split.
I0305 21:57:14.468092 139687630427328 spec.py:349] Evaluating on the test split.
I0305 21:57:16.296407 139687630427328 submission_runner.py:469] Time since start: 9865.22s, 	Step: 37808, 	{'train/accuracy': 0.9934921264648438, 'train/loss': 0.020359596237540245, 'train/mean_average_precision': 0.6289925457005716, 'validation/accuracy': 0.9865511655807495, 'validation/loss': 0.0488634929060936, 'validation/mean_average_precision': 0.28069273184885213, 'validation/num_examples': 43793, 'test/accuracy': 0.9857433438301086, 'test/loss': 0.05212013050913811, 'test/mean_average_precision': 0.2588480247348749, 'test/num_examples': 43793, 'score': 7452.266214132309, 'total_duration': 9865.224529504776, 'accumulated_submission_time': 7452.266214132309, 'accumulated_eval_time': 2411.2121844291687, 'accumulated_logging_time': 0.7727429866790771}
I0305 21:57:16.308615 139545802610432 logging_writer.py:48] [37808] accumulated_eval_time=2411.21, accumulated_logging_time=0.772743, accumulated_submission_time=7452.27, global_step=37808, preemption_count=0, score=7452.27, test/accuracy=0.985743, test/loss=0.0521201, test/mean_average_precision=0.258848, test/num_examples=43793, total_duration=9865.22, train/accuracy=0.993492, train/loss=0.0203596, train/mean_average_precision=0.628993, validation/accuracy=0.986551, validation/loss=0.0488635, validation/mean_average_precision=0.280693, validation/num_examples=43793
I0305 21:57:35.035610 139545811003136 logging_writer.py:48] [37900] global_step=37900, grad_norm=0.21424704790115356, loss=0.020956533029675484
I0305 21:57:55.264623 139545802610432 logging_writer.py:48] [38000] global_step=38000, grad_norm=0.1889992207288742, loss=0.020435117185115814
I0305 21:58:15.270485 139545811003136 logging_writer.py:48] [38100] global_step=38100, grad_norm=0.21836794912815094, loss=0.02220875211060047
I0305 21:58:35.080059 139545802610432 logging_writer.py:48] [38200] global_step=38200, grad_norm=0.1961977481842041, loss=0.018613003194332123
I0305 21:58:55.124851 139545811003136 logging_writer.py:48] [38300] global_step=38300, grad_norm=0.21386964619159698, loss=0.019430819898843765
I0305 21:59:14.849150 139545802610432 logging_writer.py:48] [38400] global_step=38400, grad_norm=0.19488020241260529, loss=0.016927100718021393
I0305 21:59:34.692784 139545811003136 logging_writer.py:48] [38500] global_step=38500, grad_norm=0.19399841129779816, loss=0.019108237698674202
I0305 21:59:54.568519 139545802610432 logging_writer.py:48] [38600] global_step=38600, grad_norm=0.19676175713539124, loss=0.019818956032395363
I0305 22:00:14.296734 139545811003136 logging_writer.py:48] [38700] global_step=38700, grad_norm=0.19772309064865112, loss=0.02008954808115959
I0305 22:00:33.813383 139545802610432 logging_writer.py:48] [38800] global_step=38800, grad_norm=0.20936542749404907, loss=0.018016690388321877
I0305 22:00:53.669909 139545811003136 logging_writer.py:48] [38900] global_step=38900, grad_norm=0.20418007671833038, loss=0.01797957718372345
I0305 22:01:13.607234 139545802610432 logging_writer.py:48] [39000] global_step=39000, grad_norm=0.19901855289936066, loss=0.01878422312438488
I0305 22:01:16.405746 139687630427328 spec.py:321] Evaluating on the training split.
I0305 22:02:21.364514 139687630427328 spec.py:333] Evaluating on the validation split.
I0305 22:02:23.246043 139687630427328 spec.py:349] Evaluating on the test split.
I0305 22:02:25.100178 139687630427328 submission_runner.py:469] Time since start: 10174.03s, 	Step: 39015, 	{'train/accuracy': 0.9942969679832458, 'train/loss': 0.01785445772111416, 'train/mean_average_precision': 0.6890866479986226, 'validation/accuracy': 0.986544668674469, 'validation/loss': 0.049699265509843826, 'validation/mean_average_precision': 0.2775979831311004, 'validation/num_examples': 43793, 'test/accuracy': 0.9856738448143005, 'test/loss': 0.0533912256360054, 'test/mean_average_precision': 0.25678655838797937, 'test/num_examples': 43793, 'score': 7692.324460506439, 'total_duration': 10174.028367757797, 'accumulated_submission_time': 7692.324460506439, 'accumulated_eval_time': 2479.9065437316895, 'accumulated_logging_time': 0.7934949398040771}
I0305 22:02:25.113768 139545811003136 logging_writer.py:48] [39015] accumulated_eval_time=2479.91, accumulated_logging_time=0.793495, accumulated_submission_time=7692.32, global_step=39015, preemption_count=0, score=7692.32, test/accuracy=0.985674, test/loss=0.0533912, test/mean_average_precision=0.256787, test/num_examples=43793, total_duration=10174, train/accuracy=0.994297, train/loss=0.0178545, train/mean_average_precision=0.689087, validation/accuracy=0.986545, validation/loss=0.0496993, validation/mean_average_precision=0.277598, validation/num_examples=43793
I0305 22:02:42.243495 139545802610432 logging_writer.py:48] [39100] global_step=39100, grad_norm=0.21609018743038177, loss=0.017703697085380554
I0305 22:03:02.084046 139545811003136 logging_writer.py:48] [39200] global_step=39200, grad_norm=0.19205015897750854, loss=0.015282941050827503
I0305 22:03:21.810670 139545802610432 logging_writer.py:48] [39300] global_step=39300, grad_norm=0.20354411005973816, loss=0.01845133677124977
I0305 22:03:41.773673 139545811003136 logging_writer.py:48] [39400] global_step=39400, grad_norm=0.21303300559520721, loss=0.018247490748763084
I0305 22:04:01.645635 139545802610432 logging_writer.py:48] [39500] global_step=39500, grad_norm=0.19918355345726013, loss=0.018764115869998932
I0305 22:04:21.538053 139545811003136 logging_writer.py:48] [39600] global_step=39600, grad_norm=0.22133928537368774, loss=0.018680071458220482
I0305 22:04:41.517082 139545802610432 logging_writer.py:48] [39700] global_step=39700, grad_norm=0.21778205037117004, loss=0.01820981502532959
I0305 22:05:01.331882 139545811003136 logging_writer.py:48] [39800] global_step=39800, grad_norm=0.2435736507177353, loss=0.019216042011976242
I0305 22:05:21.241593 139545802610432 logging_writer.py:48] [39900] global_step=39900, grad_norm=0.21400749683380127, loss=0.016548531129956245
I0305 22:05:41.272845 139545811003136 logging_writer.py:48] [40000] global_step=40000, grad_norm=0.21236437559127808, loss=0.016490373760461807
I0305 22:06:01.377928 139545802610432 logging_writer.py:48] [40100] global_step=40100, grad_norm=0.20430439710617065, loss=0.016649311408400536
I0305 22:06:21.108844 139545811003136 logging_writer.py:48] [40200] global_step=40200, grad_norm=0.19777356088161469, loss=0.016595805063843727
I0305 22:06:25.119679 139687630427328 spec.py:321] Evaluating on the training split.
I0305 22:07:31.062601 139687630427328 spec.py:333] Evaluating on the validation split.
I0305 22:07:32.938403 139687630427328 spec.py:349] Evaluating on the test split.
I0305 22:07:34.792718 139687630427328 submission_runner.py:469] Time since start: 10483.72s, 	Step: 40221, 	{'train/accuracy': 0.9941980838775635, 'train/loss': 0.01802048645913601, 'train/mean_average_precision': 0.6603194886587318, 'validation/accuracy': 0.9862543940544128, 'validation/loss': 0.051480360329151154, 'validation/mean_average_precision': 0.2699848347524564, 'validation/num_examples': 43793, 'test/accuracy': 0.985448956489563, 'test/loss': 0.05503067374229431, 'test/mean_average_precision': 0.25255849475290615, 'test/num_examples': 43793, 'score': 7932.28755402565, 'total_duration': 10483.720953464508, 'accumulated_submission_time': 7932.28755402565, 'accumulated_eval_time': 2549.5795407295227, 'accumulated_logging_time': 0.8173904418945312}
I0305 22:07:34.805354 139545802610432 logging_writer.py:48] [40221] accumulated_eval_time=2549.58, accumulated_logging_time=0.81739, accumulated_submission_time=7932.29, global_step=40221, preemption_count=0, score=7932.29, test/accuracy=0.985449, test/loss=0.0550307, test/mean_average_precision=0.252558, test/num_examples=43793, total_duration=10483.7, train/accuracy=0.994198, train/loss=0.0180205, train/mean_average_precision=0.660319, validation/accuracy=0.986254, validation/loss=0.0514804, validation/mean_average_precision=0.269985, validation/num_examples=43793
I0305 22:07:50.673848 139545811003136 logging_writer.py:48] [40300] global_step=40300, grad_norm=0.23479004204273224, loss=0.0175393745303154
I0305 22:08:10.457348 139545802610432 logging_writer.py:48] [40400] global_step=40400, grad_norm=0.2209492176771164, loss=0.01800091378390789
I0305 22:08:30.359987 139545811003136 logging_writer.py:48] [40500] global_step=40500, grad_norm=0.22714227437973022, loss=0.017431259155273438
I0305 22:08:50.193060 139545802610432 logging_writer.py:48] [40600] global_step=40600, grad_norm=0.25125789642333984, loss=0.01509031094610691
I0305 22:09:10.117951 139545811003136 logging_writer.py:48] [40700] global_step=40700, grad_norm=0.2294950783252716, loss=0.012909193523228168
I0305 22:09:29.944958 139545802610432 logging_writer.py:48] [40800] global_step=40800, grad_norm=0.23072846233844757, loss=0.015323247760534286
I0305 22:09:49.671681 139545811003136 logging_writer.py:48] [40900] global_step=40900, grad_norm=0.24120645225048065, loss=0.016030656173825264
I0305 22:10:09.409529 139545802610432 logging_writer.py:48] [41000] global_step=41000, grad_norm=0.2050599306821823, loss=0.015344014391303062
I0305 22:10:29.120842 139545811003136 logging_writer.py:48] [41100] global_step=41100, grad_norm=0.22892336547374725, loss=0.0158226415514946
I0305 22:10:49.032750 139545802610432 logging_writer.py:48] [41200] global_step=41200, grad_norm=0.25657615065574646, loss=0.017620960250496864
I0305 22:11:08.876074 139545811003136 logging_writer.py:48] [41300] global_step=41300, grad_norm=0.215685173869133, loss=0.015552645549178123
I0305 22:11:28.811923 139545802610432 logging_writer.py:48] [41400] global_step=41400, grad_norm=0.24004164338111877, loss=0.01669323444366455
I0305 22:11:34.940955 139687630427328 spec.py:321] Evaluating on the training split.
I0305 22:12:41.390175 139687630427328 spec.py:333] Evaluating on the validation split.
I0305 22:12:43.288943 139687630427328 spec.py:349] Evaluating on the test split.
I0305 22:12:45.136742 139687630427328 submission_runner.py:469] Time since start: 10794.06s, 	Step: 41432, 	{'train/accuracy': 0.9941955804824829, 'train/loss': 0.017906980589032173, 'train/mean_average_precision': 0.6777833181242683, 'validation/accuracy': 0.9864252805709839, 'validation/loss': 0.05272669345140457, 'validation/mean_average_precision': 0.2715637943823335, 'validation/num_examples': 43793, 'test/accuracy': 0.9855592846870422, 'test/loss': 0.05640117824077606, 'test/mean_average_precision': 0.25038811633441804, 'test/num_examples': 43793, 'score': 8172.382656335831, 'total_duration': 10794.064980506897, 'accumulated_submission_time': 8172.382656335831, 'accumulated_eval_time': 2619.7752861976624, 'accumulated_logging_time': 0.8405411243438721}
I0305 22:12:45.149179 139545811003136 logging_writer.py:48] [41432] accumulated_eval_time=2619.78, accumulated_logging_time=0.840541, accumulated_submission_time=8172.38, global_step=41432, preemption_count=0, score=8172.38, test/accuracy=0.985559, test/loss=0.0564012, test/mean_average_precision=0.250388, test/num_examples=43793, total_duration=10794.1, train/accuracy=0.994196, train/loss=0.017907, train/mean_average_precision=0.677783, validation/accuracy=0.986425, validation/loss=0.0527267, validation/mean_average_precision=0.271564, validation/num_examples=43793
I0305 22:12:58.940922 139545802610432 logging_writer.py:48] [41500] global_step=41500, grad_norm=0.2255692332983017, loss=0.015429276041686535
I0305 22:13:18.794747 139545811003136 logging_writer.py:48] [41600] global_step=41600, grad_norm=0.21452371776103973, loss=0.014499165117740631
I0305 22:13:38.822161 139545802610432 logging_writer.py:48] [41700] global_step=41700, grad_norm=0.2536785900592804, loss=0.015041770413517952
I0305 22:13:58.633644 139545811003136 logging_writer.py:48] [41800] global_step=41800, grad_norm=0.2507912814617157, loss=0.015971027314662933
I0305 22:14:18.487323 139545802610432 logging_writer.py:48] [41900] global_step=41900, grad_norm=0.24726919829845428, loss=0.015824362635612488
I0305 22:14:38.379767 139545811003136 logging_writer.py:48] [42000] global_step=42000, grad_norm=0.2272336781024933, loss=0.014575447887182236
I0305 22:14:58.206112 139545802610432 logging_writer.py:48] [42100] global_step=42100, grad_norm=0.2434341013431549, loss=0.014186402782797813
I0305 22:15:18.108322 139545811003136 logging_writer.py:48] [42200] global_step=42200, grad_norm=0.21071463823318481, loss=0.015817204490303993
I0305 22:15:37.735511 139545802610432 logging_writer.py:48] [42300] global_step=42300, grad_norm=0.21762250363826752, loss=0.014648301526904106
I0305 22:15:57.404183 139545811003136 logging_writer.py:48] [42400] global_step=42400, grad_norm=0.2745612859725952, loss=0.01572347804903984
I0305 22:16:17.117161 139545802610432 logging_writer.py:48] [42500] global_step=42500, grad_norm=0.275264710187912, loss=0.016469966620206833
I0305 22:16:36.884960 139545811003136 logging_writer.py:48] [42600] global_step=42600, grad_norm=0.22373241186141968, loss=0.014215818606317043
I0305 22:16:45.165824 139687630427328 spec.py:321] Evaluating on the training split.
I0305 22:17:51.082128 139687630427328 spec.py:333] Evaluating on the validation split.
I0305 22:17:52.995575 139687630427328 spec.py:349] Evaluating on the test split.
I0305 22:17:54.834854 139687630427328 submission_runner.py:469] Time since start: 11103.76s, 	Step: 42643, 	{'train/accuracy': 0.9953868985176086, 'train/loss': 0.014556092210114002, 'train/mean_average_precision': 0.7477537997126755, 'validation/accuracy': 0.9862832427024841, 'validation/loss': 0.0541796013712883, 'validation/mean_average_precision': 0.2707801864670295, 'validation/num_examples': 43793, 'test/accuracy': 0.9854830503463745, 'test/loss': 0.05771397054195404, 'test/mean_average_precision': 0.24811548273051695, 'test/num_examples': 43793, 'score': 8412.358704090118, 'total_duration': 11103.762996196747, 'accumulated_submission_time': 8412.358704090118, 'accumulated_eval_time': 2689.444177389145, 'accumulated_logging_time': 0.8624267578125}
I0305 22:17:54.847918 139545802610432 logging_writer.py:48] [42643] accumulated_eval_time=2689.44, accumulated_logging_time=0.862427, accumulated_submission_time=8412.36, global_step=42643, preemption_count=0, score=8412.36, test/accuracy=0.985483, test/loss=0.057714, test/mean_average_precision=0.248115, test/num_examples=43793, total_duration=11103.8, train/accuracy=0.995387, train/loss=0.0145561, train/mean_average_precision=0.747754, validation/accuracy=0.986283, validation/loss=0.0541796, validation/mean_average_precision=0.27078, validation/num_examples=43793
I0305 22:18:06.317324 139545811003136 logging_writer.py:48] [42700] global_step=42700, grad_norm=0.2310524880886078, loss=0.015353183262050152
I0305 22:18:26.018700 139545802610432 logging_writer.py:48] [42800] global_step=42800, grad_norm=0.26267263293266296, loss=0.014620793983340263
I0305 22:18:45.782283 139545811003136 logging_writer.py:48] [42900] global_step=42900, grad_norm=0.27508482336997986, loss=0.015461473725736141
I0305 22:19:05.837504 139545802610432 logging_writer.py:48] [43000] global_step=43000, grad_norm=0.24560163915157318, loss=0.014157463796436787
I0305 22:19:25.547678 139545811003136 logging_writer.py:48] [43100] global_step=43100, grad_norm=0.2582172155380249, loss=0.01480894535779953
I0305 22:19:45.340093 139545802610432 logging_writer.py:48] [43200] global_step=43200, grad_norm=0.22574129700660706, loss=0.015304558910429478
I0305 22:20:05.059950 139545811003136 logging_writer.py:48] [43300] global_step=43300, grad_norm=0.23892247676849365, loss=0.0127757852897048
I0305 22:20:25.027254 139545802610432 logging_writer.py:48] [43400] global_step=43400, grad_norm=0.2782323658466339, loss=0.01651718281209469
I0305 22:20:44.987316 139545811003136 logging_writer.py:48] [43500] global_step=43500, grad_norm=0.2618209719657898, loss=0.013622988946735859
I0305 22:21:04.918509 139545802610432 logging_writer.py:48] [43600] global_step=43600, grad_norm=0.2599782645702362, loss=0.015025210566818714
I0305 22:21:24.668170 139545811003136 logging_writer.py:48] [43700] global_step=43700, grad_norm=0.24235060811042786, loss=0.013675952330231667
I0305 22:21:44.588543 139545802610432 logging_writer.py:48] [43800] global_step=43800, grad_norm=0.2293124496936798, loss=0.012630940414965153
I0305 22:21:54.940852 139687630427328 spec.py:321] Evaluating on the training split.
I0305 22:23:12.478076 139687630427328 spec.py:333] Evaluating on the validation split.
I0305 22:23:14.595809 139687630427328 spec.py:349] Evaluating on the test split.
I0305 22:23:16.662585 139687630427328 submission_runner.py:469] Time since start: 11425.59s, 	Step: 43853, 	{'train/accuracy': 0.9943060278892517, 'train/loss': 0.017227711156010628, 'train/mean_average_precision': 0.6818539268150086, 'validation/accuracy': 0.9862117767333984, 'validation/loss': 0.05548793822526932, 'validation/mean_average_precision': 0.26483907223112185, 'validation/num_examples': 43793, 'test/accuracy': 0.985284686088562, 'test/loss': 0.059298399835824966, 'test/mean_average_precision': 0.24626088717364797, 'test/num_examples': 43793, 'score': 8652.412561416626, 'total_duration': 11425.59073138237, 'accumulated_submission_time': 8652.412561416626, 'accumulated_eval_time': 2771.1657843589783, 'accumulated_logging_time': 0.8839075565338135}
I0305 22:23:16.676467 139545811003136 logging_writer.py:48] [43853] accumulated_eval_time=2771.17, accumulated_logging_time=0.883908, accumulated_submission_time=8652.41, global_step=43853, preemption_count=0, score=8652.41, test/accuracy=0.985285, test/loss=0.0592984, test/mean_average_precision=0.246261, test/num_examples=43793, total_duration=11425.6, train/accuracy=0.994306, train/loss=0.0172277, train/mean_average_precision=0.681854, validation/accuracy=0.986212, validation/loss=0.0554879, validation/mean_average_precision=0.264839, validation/num_examples=43793
I0305 22:23:26.177287 139545802610432 logging_writer.py:48] [43900] global_step=43900, grad_norm=0.2493869811296463, loss=0.0157565176486969
I0305 22:23:46.018280 139545811003136 logging_writer.py:48] [44000] global_step=44000, grad_norm=0.25970855355262756, loss=0.014572356827557087
I0305 22:24:05.929276 139545802610432 logging_writer.py:48] [44100] global_step=44100, grad_norm=0.273693323135376, loss=0.013781443238258362
I0305 22:24:25.790359 139545811003136 logging_writer.py:48] [44200] global_step=44200, grad_norm=0.2593354880809784, loss=0.01355363242328167
I0305 22:24:45.696995 139545802610432 logging_writer.py:48] [44300] global_step=44300, grad_norm=0.29386234283447266, loss=0.016472864896059036
I0305 22:25:05.576301 139545811003136 logging_writer.py:48] [44400] global_step=44400, grad_norm=0.26879793405532837, loss=0.015183987095952034
I0305 22:25:25.522640 139545802610432 logging_writer.py:48] [44500] global_step=44500, grad_norm=0.22419673204421997, loss=0.011565829627215862
I0305 22:25:45.543145 139545811003136 logging_writer.py:48] [44600] global_step=44600, grad_norm=0.2765117883682251, loss=0.013797031715512276
I0305 22:26:05.486260 139545802610432 logging_writer.py:48] [44700] global_step=44700, grad_norm=0.24644087255001068, loss=0.014821181073784828
I0305 22:26:25.190243 139545811003136 logging_writer.py:48] [44800] global_step=44800, grad_norm=0.2661895155906677, loss=0.013610190711915493
I0305 22:26:45.007027 139545802610432 logging_writer.py:48] [44900] global_step=44900, grad_norm=0.276052325963974, loss=0.013654285110533237
I0305 22:27:04.582988 139545811003136 logging_writer.py:48] [45000] global_step=45000, grad_norm=0.28866493701934814, loss=0.014058251865208149
I0305 22:27:16.762324 139687630427328 spec.py:321] Evaluating on the training split.
I0305 22:28:21.252887 139687630427328 spec.py:333] Evaluating on the validation split.
I0305 22:28:23.202649 139687630427328 spec.py:349] Evaluating on the test split.
I0305 22:28:25.076577 139687630427328 submission_runner.py:469] Time since start: 11734.00s, 	Step: 45064, 	{'train/accuracy': 0.995733916759491, 'train/loss': 0.013313550502061844, 'train/mean_average_precision': 0.7693390163231147, 'validation/accuracy': 0.9860656261444092, 'validation/loss': 0.057048480957746506, 'validation/mean_average_precision': 0.26738656516529385, 'validation/num_examples': 43793, 'test/accuracy': 0.9852514266967773, 'test/loss': 0.060975331813097, 'test/mean_average_precision': 0.2454792240816785, 'test/num_examples': 43793, 'score': 8892.458438634872, 'total_duration': 11734.004797458649, 'accumulated_submission_time': 8892.458438634872, 'accumulated_eval_time': 2839.47998213768, 'accumulated_logging_time': 0.9079875946044922}
I0305 22:28:25.090239 139545802610432 logging_writer.py:48] [45064] accumulated_eval_time=2839.48, accumulated_logging_time=0.907988, accumulated_submission_time=8892.46, global_step=45064, preemption_count=0, score=8892.46, test/accuracy=0.985251, test/loss=0.0609753, test/mean_average_precision=0.245479, test/num_examples=43793, total_duration=11734, train/accuracy=0.995734, train/loss=0.0133136, train/mean_average_precision=0.769339, validation/accuracy=0.986066, validation/loss=0.0570485, validation/mean_average_precision=0.267387, validation/num_examples=43793
I0305 22:28:32.519061 139545811003136 logging_writer.py:48] [45100] global_step=45100, grad_norm=0.3035866916179657, loss=0.013636154122650623
I0305 22:28:52.398740 139545802610432 logging_writer.py:48] [45200] global_step=45200, grad_norm=0.25613588094711304, loss=0.012784554623067379
I0305 22:29:12.384552 139545811003136 logging_writer.py:48] [45300] global_step=45300, grad_norm=0.2620902359485626, loss=0.013843635097146034
I0305 22:29:32.284766 139545802610432 logging_writer.py:48] [45400] global_step=45400, grad_norm=0.28961703181266785, loss=0.013513651676476002
I0305 22:29:52.209666 139545811003136 logging_writer.py:48] [45500] global_step=45500, grad_norm=0.2501107156276703, loss=0.01224101334810257
I0305 22:30:12.096169 139545802610432 logging_writer.py:48] [45600] global_step=45600, grad_norm=0.2597951889038086, loss=0.012013595551252365
I0305 22:30:32.000392 139545811003136 logging_writer.py:48] [45700] global_step=45700, grad_norm=0.30082008242607117, loss=0.013160180300474167
I0305 22:30:51.912314 139545802610432 logging_writer.py:48] [45800] global_step=45800, grad_norm=0.2809511423110962, loss=0.013080689124763012
I0305 22:31:11.903138 139545811003136 logging_writer.py:48] [45900] global_step=45900, grad_norm=0.2707359194755554, loss=0.013380425982177258
I0305 22:31:31.678567 139545802610432 logging_writer.py:48] [46000] global_step=46000, grad_norm=0.33767127990722656, loss=0.013832705095410347
I0305 22:31:51.462749 139545811003136 logging_writer.py:48] [46100] global_step=46100, grad_norm=0.27626630663871765, loss=0.011589223518967628
I0305 22:32:11.432689 139545802610432 logging_writer.py:48] [46200] global_step=46200, grad_norm=0.23272855579853058, loss=0.009979072958230972
I0305 22:32:25.248970 139687630427328 spec.py:321] Evaluating on the training split.
I0305 22:33:34.805594 139687630427328 spec.py:333] Evaluating on the validation split.
I0305 22:33:36.678948 139687630427328 spec.py:349] Evaluating on the test split.
I0305 22:33:38.510724 139687630427328 submission_runner.py:469] Time since start: 12047.44s, 	Step: 46270, 	{'train/accuracy': 0.9945857524871826, 'train/loss': 0.01629352569580078, 'train/mean_average_precision': 0.6994421959246826, 'validation/accuracy': 0.9860063791275024, 'validation/loss': 0.0582558810710907, 'validation/mean_average_precision': 0.26356643551369124, 'validation/num_examples': 43793, 'test/accuracy': 0.9851271510124207, 'test/loss': 0.06225794926285744, 'test/mean_average_precision': 0.24191973525140406, 'test/num_examples': 43793, 'score': 9132.576266050339, 'total_duration': 12047.43895483017, 'accumulated_submission_time': 9132.576266050339, 'accumulated_eval_time': 2912.7416954040527, 'accumulated_logging_time': 0.93117356300354}
I0305 22:33:38.524927 139545811003136 logging_writer.py:48] [46270] accumulated_eval_time=2912.74, accumulated_logging_time=0.931174, accumulated_submission_time=9132.58, global_step=46270, preemption_count=0, score=9132.58, test/accuracy=0.985127, test/loss=0.0622579, test/mean_average_precision=0.24192, test/num_examples=43793, total_duration=12047.4, train/accuracy=0.994586, train/loss=0.0162935, train/mean_average_precision=0.699442, validation/accuracy=0.986006, validation/loss=0.0582559, validation/mean_average_precision=0.263566, validation/num_examples=43793
I0305 22:33:44.738575 139545802610432 logging_writer.py:48] [46300] global_step=46300, grad_norm=0.2859041690826416, loss=0.01101340539753437
I0305 22:34:04.433744 139545811003136 logging_writer.py:48] [46400] global_step=46400, grad_norm=0.2928248941898346, loss=0.01422092691063881
I0305 22:34:24.237166 139545802610432 logging_writer.py:48] [46500] global_step=46500, grad_norm=0.2602859437465668, loss=0.011878252029418945
I0305 22:34:44.287594 139545811003136 logging_writer.py:48] [46600] global_step=46600, grad_norm=0.2755977213382721, loss=0.011701161041855812
I0305 22:35:04.341326 139545802610432 logging_writer.py:48] [46700] global_step=46700, grad_norm=0.28773871064186096, loss=0.011830544099211693
I0305 22:35:24.268004 139545811003136 logging_writer.py:48] [46800] global_step=46800, grad_norm=0.27348074316978455, loss=0.011887148022651672
I0305 22:35:44.081666 139545802610432 logging_writer.py:48] [46900] global_step=46900, grad_norm=0.2438226193189621, loss=0.011524427682161331
I0305 22:36:04.184101 139545811003136 logging_writer.py:48] [47000] global_step=47000, grad_norm=0.27087995409965515, loss=0.011489693075418472
I0305 22:36:23.897953 139545802610432 logging_writer.py:48] [47100] global_step=47100, grad_norm=0.3199322819709778, loss=0.01211217325180769
I0305 22:36:43.750791 139545811003136 logging_writer.py:48] [47200] global_step=47200, grad_norm=0.25176292657852173, loss=0.011470009572803974
I0305 22:37:03.496791 139545802610432 logging_writer.py:48] [47300] global_step=47300, grad_norm=0.2741963863372803, loss=0.012840567156672478
I0305 22:37:23.289933 139545811003136 logging_writer.py:48] [47400] global_step=47400, grad_norm=0.2433660328388214, loss=0.009701079688966274
I0305 22:37:38.692994 139687630427328 spec.py:321] Evaluating on the training split.
I0305 22:38:43.521122 139687630427328 spec.py:333] Evaluating on the validation split.
I0305 22:38:45.448443 139687630427328 spec.py:349] Evaluating on the test split.
I0305 22:38:47.244693 139687630427328 submission_runner.py:469] Time since start: 12356.17s, 	Step: 47479, 	{'train/accuracy': 0.9963548183441162, 'train/loss': 0.011834407225251198, 'train/mean_average_precision': 0.791158664736933, 'validation/accuracy': 0.9858565926551819, 'validation/loss': 0.05903805419802666, 'validation/mean_average_precision': 0.2625702047212066, 'validation/num_examples': 43793, 'test/accuracy': 0.9849759340286255, 'test/loss': 0.06291484832763672, 'test/mean_average_precision': 0.24228873326518235, 'test/num_examples': 43793, 'score': 9372.704038858414, 'total_duration': 12356.172856330872, 'accumulated_submission_time': 9372.704038858414, 'accumulated_eval_time': 2981.29327750206, 'accumulated_logging_time': 0.9538002014160156}
I0305 22:38:47.257861 139545802610432 logging_writer.py:48] [47479] accumulated_eval_time=2981.29, accumulated_logging_time=0.9538, accumulated_submission_time=9372.7, global_step=47479, preemption_count=0, score=9372.7, test/accuracy=0.984976, test/loss=0.0629148, test/mean_average_precision=0.242289, test/num_examples=43793, total_duration=12356.2, train/accuracy=0.996355, train/loss=0.0118344, train/mean_average_precision=0.791159, validation/accuracy=0.985857, validation/loss=0.0590381, validation/mean_average_precision=0.26257, validation/num_examples=43793
I0305 22:38:51.641631 139545811003136 logging_writer.py:48] [47500] global_step=47500, grad_norm=0.2672935128211975, loss=0.012763222679495811
I0305 22:39:11.354918 139545802610432 logging_writer.py:48] [47600] global_step=47600, grad_norm=0.2813377380371094, loss=0.011118813417851925
I0305 22:39:31.152251 139545811003136 logging_writer.py:48] [47700] global_step=47700, grad_norm=0.2682293653488159, loss=0.013431884348392487
I0305 22:39:50.829025 139545802610432 logging_writer.py:48] [47800] global_step=47800, grad_norm=0.2838743329048157, loss=0.012318653985857964
I0305 22:40:10.646252 139545811003136 logging_writer.py:48] [47900] global_step=47900, grad_norm=0.29695817828178406, loss=0.013297184370458126
I0305 22:40:30.391373 139545802610432 logging_writer.py:48] [48000] global_step=48000, grad_norm=0.25375327467918396, loss=0.011656674556434155
I0305 22:40:50.072006 139545811003136 logging_writer.py:48] [48100] global_step=48100, grad_norm=0.269296258687973, loss=0.01169421337544918
I0305 22:41:09.803487 139545802610432 logging_writer.py:48] [48200] global_step=48200, grad_norm=0.28431010246276855, loss=0.01052502728998661
I0305 22:41:29.603134 139545811003136 logging_writer.py:48] [48300] global_step=48300, grad_norm=0.2439807504415512, loss=0.01057653222233057
I0305 22:41:49.465197 139545802610432 logging_writer.py:48] [48400] global_step=48400, grad_norm=0.295474112033844, loss=0.012904043309390545
I0305 22:42:09.530429 139545811003136 logging_writer.py:48] [48500] global_step=48500, grad_norm=0.30219221115112305, loss=0.012760816141963005
I0305 22:42:29.410851 139545802610432 logging_writer.py:48] [48600] global_step=48600, grad_norm=0.2742498219013214, loss=0.011559328995645046
I0305 22:42:47.334939 139687630427328 spec.py:321] Evaluating on the training split.
I0305 22:43:53.177291 139687630427328 spec.py:333] Evaluating on the validation split.
I0305 22:43:55.052754 139687630427328 spec.py:349] Evaluating on the test split.
I0305 22:43:56.870670 139687630427328 submission_runner.py:469] Time since start: 12665.80s, 	Step: 48691, 	{'train/accuracy': 0.9956499338150024, 'train/loss': 0.0133504094555974, 'train/mean_average_precision': 0.7631207125557389, 'validation/accuracy': 0.9858602285385132, 'validation/loss': 0.059691060334444046, 'validation/mean_average_precision': 0.2609555060378384, 'validation/num_examples': 43793, 'test/accuracy': 0.9849544763565063, 'test/loss': 0.06358589231967926, 'test/mean_average_precision': 0.24095917274716158, 'test/num_examples': 43793, 'score': 9612.742330789566, 'total_duration': 12665.798841238022, 'accumulated_submission_time': 9612.742330789566, 'accumulated_eval_time': 3050.8289000988007, 'accumulated_logging_time': 0.9754054546356201}
I0305 22:43:56.884224 139545811003136 logging_writer.py:48] [48691] accumulated_eval_time=3050.83, accumulated_logging_time=0.975405, accumulated_submission_time=9612.74, global_step=48691, preemption_count=0, score=9612.74, test/accuracy=0.984954, test/loss=0.0635859, test/mean_average_precision=0.240959, test/num_examples=43793, total_duration=12665.8, train/accuracy=0.99565, train/loss=0.0133504, train/mean_average_precision=0.763121, validation/accuracy=0.98586, validation/loss=0.0596911, validation/mean_average_precision=0.260956, validation/num_examples=43793
I0305 22:43:58.885173 139545802610432 logging_writer.py:48] [48700] global_step=48700, grad_norm=0.2451157569885254, loss=0.010300240479409695
I0305 22:44:18.987947 139545811003136 logging_writer.py:48] [48800] global_step=48800, grad_norm=0.293729692697525, loss=0.01318570040166378
I0305 22:44:38.900280 139545802610432 logging_writer.py:48] [48900] global_step=48900, grad_norm=0.2782984972000122, loss=0.010704364627599716
I0305 22:44:58.983465 139545811003136 logging_writer.py:48] [49000] global_step=49000, grad_norm=0.296353816986084, loss=0.01184714213013649
I0305 22:45:18.998255 139545802610432 logging_writer.py:48] [49100] global_step=49100, grad_norm=0.25381529331207275, loss=0.011445572599768639
I0305 22:45:38.754779 139545811003136 logging_writer.py:48] [49200] global_step=49200, grad_norm=0.28098854422569275, loss=0.012776478193700314
I0305 22:45:58.536098 139545802610432 logging_writer.py:48] [49300] global_step=49300, grad_norm=0.31721094250679016, loss=0.013061393983662128
I0305 22:46:18.007874 139545811003136 logging_writer.py:48] [49400] global_step=49400, grad_norm=0.25345689058303833, loss=0.010891149751842022
I0305 22:46:37.707741 139545802610432 logging_writer.py:48] [49500] global_step=49500, grad_norm=0.27753594517707825, loss=0.010844082571566105
I0305 22:46:57.340259 139545811003136 logging_writer.py:48] [49600] global_step=49600, grad_norm=0.27753642201423645, loss=0.01139348465949297
I0305 22:47:17.235018 139545802610432 logging_writer.py:48] [49700] global_step=49700, grad_norm=0.2895846664905548, loss=0.011880449950695038
I0305 22:47:37.096533 139545811003136 logging_writer.py:48] [49800] global_step=49800, grad_norm=0.26225802302360535, loss=0.010477390140295029
I0305 22:47:56.887608 139545802610432 logging_writer.py:48] [49900] global_step=49900, grad_norm=0.3203548789024353, loss=0.012761394493281841
I0305 22:47:56.891906 139687630427328 spec.py:321] Evaluating on the training split.
I0305 22:48:59.425987 139687630427328 spec.py:333] Evaluating on the validation split.
I0305 22:49:01.309494 139687630427328 spec.py:349] Evaluating on the test split.
I0305 22:49:03.114434 139687630427328 submission_runner.py:469] Time since start: 12972.04s, 	Step: 49901, 	{'train/accuracy': 0.9964169263839722, 'train/loss': 0.011543062515556812, 'train/mean_average_precision': 0.8007839953095489, 'validation/accuracy': 0.9858164191246033, 'validation/loss': 0.059869881719350815, 'validation/mean_average_precision': 0.2643400466467922, 'validation/num_examples': 43793, 'test/accuracy': 0.9849587082862854, 'test/loss': 0.06374257802963257, 'test/mean_average_precision': 0.2422161897737763, 'test/num_examples': 43793, 'score': 9852.70882844925, 'total_duration': 12972.042619943619, 'accumulated_submission_time': 9852.70882844925, 'accumulated_eval_time': 3117.051324367523, 'accumulated_logging_time': 0.9972708225250244}
I0305 22:49:03.128239 139545811003136 logging_writer.py:48] [49901] accumulated_eval_time=3117.05, accumulated_logging_time=0.997271, accumulated_submission_time=9852.71, global_step=49901, preemption_count=0, score=9852.71, test/accuracy=0.984959, test/loss=0.0637426, test/mean_average_precision=0.242216, test/num_examples=43793, total_duration=12972, train/accuracy=0.996417, train/loss=0.0115431, train/mean_average_precision=0.800784, validation/accuracy=0.985816, validation/loss=0.0598699, validation/mean_average_precision=0.26434, validation/num_examples=43793
I0305 22:49:22.850123 139545802610432 logging_writer.py:48] [50000] global_step=50000, grad_norm=0.2974439859390259, loss=0.012606956996023655
I0305 22:49:42.703485 139545811003136 logging_writer.py:48] [50100] global_step=50100, grad_norm=0.28855836391448975, loss=0.011547483503818512
I0305 22:50:02.494563 139545802610432 logging_writer.py:48] [50200] global_step=50200, grad_norm=0.3079264461994171, loss=0.012072179466485977
I0305 22:50:22.243270 139545811003136 logging_writer.py:48] [50300] global_step=50300, grad_norm=0.26725509762763977, loss=0.011053497903048992
I0305 22:50:41.915569 139545802610432 logging_writer.py:48] [50400] global_step=50400, grad_norm=0.28666478395462036, loss=0.010565048083662987
I0305 22:51:01.735035 139545811003136 logging_writer.py:48] [50500] global_step=50500, grad_norm=0.3273249566555023, loss=0.012252368964254856
I0305 22:51:21.409055 139545802610432 logging_writer.py:48] [50600] global_step=50600, grad_norm=0.28355711698532104, loss=0.011919022537767887
I0305 22:51:41.254471 139545811003136 logging_writer.py:48] [50700] global_step=50700, grad_norm=0.34005117416381836, loss=0.014546044170856476
I0305 22:52:00.934935 139545802610432 logging_writer.py:48] [50800] global_step=50800, grad_norm=0.31044188141822815, loss=0.010501492768526077
I0305 22:52:20.529063 139545811003136 logging_writer.py:48] [50900] global_step=50900, grad_norm=0.2870498597621918, loss=0.011011455208063126
I0305 22:52:40.454902 139545802610432 logging_writer.py:48] [51000] global_step=51000, grad_norm=0.2895866632461548, loss=0.011825638823211193
I0305 22:53:00.615168 139545811003136 logging_writer.py:48] [51100] global_step=51100, grad_norm=0.31891101598739624, loss=0.011883911676704884
I0305 22:53:03.197025 139687630427328 spec.py:321] Evaluating on the training split.
I0305 22:54:10.913180 139687630427328 spec.py:333] Evaluating on the validation split.
I0305 22:54:12.816227 139687630427328 spec.py:349] Evaluating on the test split.
I0305 22:54:14.880774 139687630427328 submission_runner.py:469] Time since start: 13283.81s, 	Step: 51114, 	{'train/accuracy': 0.9961540102958679, 'train/loss': 0.012096261605620384, 'train/mean_average_precision': 0.7782761178691838, 'validation/accuracy': 0.9858553409576416, 'validation/loss': 0.05986309424042702, 'validation/mean_average_precision': 0.2644024641725697, 'validation/num_examples': 43793, 'test/accuracy': 0.9849730134010315, 'test/loss': 0.0638059675693512, 'test/mean_average_precision': 0.24113731373265657, 'test/num_examples': 43793, 'score': 10092.739246845245, 'total_duration': 13283.808974981308, 'accumulated_submission_time': 10092.739246845245, 'accumulated_eval_time': 3188.7349989414215, 'accumulated_logging_time': 1.0195181369781494}
I0305 22:54:14.896247 139545802610432 logging_writer.py:48] [51114] accumulated_eval_time=3188.73, accumulated_logging_time=1.01952, accumulated_submission_time=10092.7, global_step=51114, preemption_count=0, score=10092.7, test/accuracy=0.984973, test/loss=0.063806, test/mean_average_precision=0.241137, test/num_examples=43793, total_duration=13283.8, train/accuracy=0.996154, train/loss=0.0120963, train/mean_average_precision=0.778276, validation/accuracy=0.985855, validation/loss=0.0598631, validation/mean_average_precision=0.264402, validation/num_examples=43793
I0305 22:54:32.062641 139545811003136 logging_writer.py:48] [51200] global_step=51200, grad_norm=0.30390143394470215, loss=0.011939618736505508
I0305 22:54:51.803425 139545802610432 logging_writer.py:48] [51300] global_step=51300, grad_norm=0.28817927837371826, loss=0.011600341647863388
I0305 22:55:11.376643 139545811003136 logging_writer.py:48] [51400] global_step=51400, grad_norm=0.28147977590560913, loss=0.010557612404227257
I0305 22:55:31.024047 139545802610432 logging_writer.py:48] [51500] global_step=51500, grad_norm=0.2659617066383362, loss=0.010597108863294125
I0305 22:55:50.686287 139545811003136 logging_writer.py:48] [51600] global_step=51600, grad_norm=0.25908562541007996, loss=0.009605713188648224
I0305 22:56:10.124935 139545802610432 logging_writer.py:48] [51700] global_step=51700, grad_norm=0.3182009160518646, loss=0.012401961721479893
I0305 22:56:29.803171 139545811003136 logging_writer.py:48] [51800] global_step=51800, grad_norm=0.26393625140190125, loss=0.01045171357691288
I0305 22:56:49.558211 139545802610432 logging_writer.py:48] [51900] global_step=51900, grad_norm=0.28728485107421875, loss=0.011334829032421112
I0305 22:57:09.284922 139545811003136 logging_writer.py:48] [52000] global_step=52000, grad_norm=0.2898003160953522, loss=0.012469231151044369
I0305 22:57:28.933171 139545802610432 logging_writer.py:48] [52100] global_step=52100, grad_norm=0.3287292718887329, loss=0.012748844921588898
I0305 22:57:48.503736 139545811003136 logging_writer.py:48] [52200] global_step=52200, grad_norm=0.2667394280433655, loss=0.010070170275866985
I0305 22:58:08.170700 139545802610432 logging_writer.py:48] [52300] global_step=52300, grad_norm=0.26074981689453125, loss=0.010367906652390957
I0305 22:58:15.029705 139687630427328 spec.py:321] Evaluating on the training split.
I0305 22:59:22.130500 139687630427328 spec.py:333] Evaluating on the validation split.
I0305 22:59:24.351044 139687630427328 spec.py:349] Evaluating on the test split.
I0305 22:59:26.448380 139687630427328 submission_runner.py:469] Time since start: 13595.38s, 	Step: 52336, 	{'train/accuracy': 0.996330976486206, 'train/loss': 0.01173893641680479, 'train/mean_average_precision': 0.7921525859571945, 'validation/accuracy': 0.9858870506286621, 'validation/loss': 0.05986879765987396, 'validation/mean_average_precision': 0.26519501289848596, 'validation/num_examples': 43793, 'test/accuracy': 0.9849919676780701, 'test/loss': 0.0637832060456276, 'test/mean_average_precision': 0.24139097782376312, 'test/num_examples': 43793, 'score': 10332.829066991806, 'total_duration': 13595.376545190811, 'accumulated_submission_time': 10332.829066991806, 'accumulated_eval_time': 3260.153562784195, 'accumulated_logging_time': 1.0475268363952637}
I0305 22:59:26.463464 139545811003136 logging_writer.py:48] [52336] accumulated_eval_time=3260.15, accumulated_logging_time=1.04753, accumulated_submission_time=10332.8, global_step=52336, preemption_count=0, score=10332.8, test/accuracy=0.984992, test/loss=0.0637832, test/mean_average_precision=0.241391, test/num_examples=43793, total_duration=13595.4, train/accuracy=0.996331, train/loss=0.0117389, train/mean_average_precision=0.792153, validation/accuracy=0.985887, validation/loss=0.0598688, validation/mean_average_precision=0.265195, validation/num_examples=43793
I0305 22:59:39.431280 139545802610432 logging_writer.py:48] [52400] global_step=52400, grad_norm=0.27833905816078186, loss=0.012385623529553413
I0305 22:59:59.164942 139545811003136 logging_writer.py:48] [52500] global_step=52500, grad_norm=0.3212692141532898, loss=0.012167328037321568
I0305 23:00:19.147187 139545802610432 logging_writer.py:48] [52600] global_step=52600, grad_norm=0.29642465710639954, loss=0.011654453352093697
I0305 23:00:39.080846 139545811003136 logging_writer.py:48] [52700] global_step=52700, grad_norm=0.2757580578327179, loss=0.011424291878938675
I0305 23:00:59.136174 139545802610432 logging_writer.py:48] [52800] global_step=52800, grad_norm=0.28771066665649414, loss=0.011963150463998318
I0305 23:01:19.336781 139545811003136 logging_writer.py:48] [52900] global_step=52900, grad_norm=0.28095775842666626, loss=0.011494738049805164
I0305 23:01:39.365341 139545802610432 logging_writer.py:48] [53000] global_step=53000, grad_norm=0.27094128727912903, loss=0.011933175846934319
I0305 23:01:59.461819 139545811003136 logging_writer.py:48] [53100] global_step=53100, grad_norm=0.2903103828430176, loss=0.010994859039783478
I0305 23:02:19.457532 139545802610432 logging_writer.py:48] [53200] global_step=53200, grad_norm=0.34324657917022705, loss=0.012762114405632019
I0305 23:02:39.452518 139545811003136 logging_writer.py:48] [53300] global_step=53300, grad_norm=0.28660786151885986, loss=0.011078670620918274
I0305 23:02:59.461444 139545802610432 logging_writer.py:48] [53400] global_step=53400, grad_norm=0.2741711437702179, loss=0.0101352259516716
I0305 23:03:19.555370 139545811003136 logging_writer.py:48] [53500] global_step=53500, grad_norm=0.29467353224754333, loss=0.012020682916045189
I0305 23:03:26.515066 139687630427328 spec.py:321] Evaluating on the training split.
I0305 23:04:33.560822 139687630427328 spec.py:333] Evaluating on the validation split.
I0305 23:04:35.485640 139687630427328 spec.py:349] Evaluating on the test split.
I0305 23:04:37.374618 139687630427328 submission_runner.py:469] Time since start: 13906.30s, 	Step: 53536, 	{'train/accuracy': 0.9963099956512451, 'train/loss': 0.011750664561986923, 'train/mean_average_precision': 0.7882473615423955, 'validation/accuracy': 0.9858870506286621, 'validation/loss': 0.05986879765987396, 'validation/mean_average_precision': 0.26490736160127776, 'validation/num_examples': 43793, 'test/accuracy': 0.9849919676780701, 'test/loss': 0.0637832060456276, 'test/mean_average_precision': 0.24133403014563978, 'test/num_examples': 43793, 'score': 10572.83955025673, 'total_duration': 13906.302777290344, 'accumulated_submission_time': 10572.83955025673, 'accumulated_eval_time': 3331.0129964351654, 'accumulated_logging_time': 1.071716547012329}
I0305 23:04:37.390461 139545802610432 logging_writer.py:48] [53536] accumulated_eval_time=3331.01, accumulated_logging_time=1.07172, accumulated_submission_time=10572.8, global_step=53536, preemption_count=0, score=10572.8, test/accuracy=0.984992, test/loss=0.0637832, test/mean_average_precision=0.241334, test/num_examples=43793, total_duration=13906.3, train/accuracy=0.99631, train/loss=0.0117507, train/mean_average_precision=0.788247, validation/accuracy=0.985887, validation/loss=0.0598688, validation/mean_average_precision=0.264907, validation/num_examples=43793
I0305 23:04:50.399035 139545811003136 logging_writer.py:48] [53600] global_step=53600, grad_norm=0.2806642949581146, loss=0.011892932467162609
I0305 23:05:10.436642 139545802610432 logging_writer.py:48] [53700] global_step=53700, grad_norm=0.2742443084716797, loss=0.01245571207255125
I0305 23:05:29.847048 139545811003136 logging_writer.py:48] [53800] global_step=53800, grad_norm=0.2974313497543335, loss=0.011612475849688053
I0305 23:05:49.507674 139545802610432 logging_writer.py:48] [53900] global_step=53900, grad_norm=0.32876113057136536, loss=0.012218967080116272
I0305 23:06:09.482139 139545811003136 logging_writer.py:48] [54000] global_step=54000, grad_norm=0.2648336589336395, loss=0.011356250382959843
I0305 23:06:29.278363 139545802610432 logging_writer.py:48] [54100] global_step=54100, grad_norm=0.3163987994194031, loss=0.013230147771537304
I0305 23:06:49.127168 139545811003136 logging_writer.py:48] [54200] global_step=54200, grad_norm=0.28861674666404724, loss=0.010975840501487255
I0305 23:07:09.071692 139545802610432 logging_writer.py:48] [54300] global_step=54300, grad_norm=0.28797444701194763, loss=0.01206980925053358
I0305 23:07:29.075481 139545811003136 logging_writer.py:48] [54400] global_step=54400, grad_norm=0.23875856399536133, loss=0.010035661980509758
I0305 23:07:48.947318 139545802610432 logging_writer.py:48] [54500] global_step=54500, grad_norm=0.2873336374759674, loss=0.01213260181248188
I0305 23:08:08.960978 139545811003136 logging_writer.py:48] [54600] global_step=54600, grad_norm=0.3087417781352997, loss=0.011484652757644653
I0305 23:08:28.786835 139545802610432 logging_writer.py:48] [54700] global_step=54700, grad_norm=0.3216104805469513, loss=0.012972094118595123
I0305 23:08:37.396897 139687630427328 spec.py:321] Evaluating on the training split.
I0305 23:09:45.722565 139687630427328 spec.py:333] Evaluating on the validation split.
I0305 23:09:47.821645 139687630427328 spec.py:349] Evaluating on the test split.
I0305 23:09:49.922726 139687630427328 submission_runner.py:469] Time since start: 14218.85s, 	Step: 54744, 	{'train/accuracy': 0.9964504837989807, 'train/loss': 0.011426721699535847, 'train/mean_average_precision': 0.7945969880651383, 'validation/accuracy': 0.9858870506286621, 'validation/loss': 0.05986879765987396, 'validation/mean_average_precision': 0.2650052452796968, 'validation/num_examples': 43793, 'test/accuracy': 0.9849919676780701, 'test/loss': 0.0637832060456276, 'test/mean_average_precision': 0.24141086648712926, 'test/num_examples': 43793, 'score': 10812.80606842041, 'total_duration': 14218.85094833374, 'accumulated_submission_time': 10812.80606842041, 'accumulated_eval_time': 3403.5387678146362, 'accumulated_logging_time': 1.0976173877716064}
I0305 23:09:49.938963 139545811003136 logging_writer.py:48] [54744] accumulated_eval_time=3403.54, accumulated_logging_time=1.09762, accumulated_submission_time=10812.8, global_step=54744, preemption_count=0, score=10812.8, test/accuracy=0.984992, test/loss=0.0637832, test/mean_average_precision=0.241411, test/num_examples=43793, total_duration=14218.9, train/accuracy=0.99645, train/loss=0.0114267, train/mean_average_precision=0.794597, validation/accuracy=0.985887, validation/loss=0.0598688, validation/mean_average_precision=0.265005, validation/num_examples=43793
I0305 23:10:01.304073 139545802610432 logging_writer.py:48] [54800] global_step=54800, grad_norm=0.2906607985496521, loss=0.012187023647129536
I0305 23:10:21.087222 139545811003136 logging_writer.py:48] [54900] global_step=54900, grad_norm=0.2620677351951599, loss=0.011549259535968304
I0305 23:10:40.913511 139545802610432 logging_writer.py:48] [55000] global_step=55000, grad_norm=0.2613459527492523, loss=0.012492259964346886
I0305 23:11:00.656640 139545811003136 logging_writer.py:48] [55100] global_step=55100, grad_norm=0.2733129560947418, loss=0.01103251799941063
I0305 23:11:20.297242 139545802610432 logging_writer.py:48] [55200] global_step=55200, grad_norm=0.29574668407440186, loss=0.01107050571590662
I0305 23:11:39.968715 139545811003136 logging_writer.py:48] [55300] global_step=55300, grad_norm=0.28300371766090393, loss=0.0115709463134408
I0305 23:11:59.941633 139545802610432 logging_writer.py:48] [55400] global_step=55400, grad_norm=0.32792243361473083, loss=0.014382224529981613
I0305 23:12:19.915244 139545811003136 logging_writer.py:48] [55500] global_step=55500, grad_norm=0.2580256760120392, loss=0.012262837961316109
I0305 23:12:39.915569 139545802610432 logging_writer.py:48] [55600] global_step=55600, grad_norm=0.29434746503829956, loss=0.011757668107748032
I0305 23:12:59.864449 139545811003136 logging_writer.py:48] [55700] global_step=55700, grad_norm=0.2700827419757843, loss=0.012334165163338184
I0305 23:13:19.941069 139545802610432 logging_writer.py:48] [55800] global_step=55800, grad_norm=0.2569233477115631, loss=0.011093431152403355
I0305 23:13:39.998373 139545811003136 logging_writer.py:48] [55900] global_step=55900, grad_norm=0.2786756157875061, loss=0.010971813462674618
I0305 23:13:49.947190 139687630427328 spec.py:321] Evaluating on the training split.
I0305 23:14:54.995091 139687630427328 spec.py:333] Evaluating on the validation split.
I0305 23:14:56.889994 139687630427328 spec.py:349] Evaluating on the test split.
I0305 23:14:58.747673 139687630427328 submission_runner.py:469] Time since start: 14527.68s, 	Step: 55951, 	{'train/accuracy': 0.9964334964752197, 'train/loss': 0.011494585312902927, 'train/mean_average_precision': 0.801948454500571, 'validation/accuracy': 0.9858870506286621, 'validation/loss': 0.05986879765987396, 'validation/mean_average_precision': 0.26507867878147123, 'validation/num_examples': 43793, 'test/accuracy': 0.9849919676780701, 'test/loss': 0.0637832060456276, 'test/mean_average_precision': 0.24138186741962345, 'test/num_examples': 43793, 'score': 11052.775285720825, 'total_duration': 14527.675853013992, 'accumulated_submission_time': 11052.775285720825, 'accumulated_eval_time': 3472.3391587734222, 'accumulated_logging_time': 1.122654914855957}
I0305 23:14:58.762037 139545802610432 logging_writer.py:48] [55951] accumulated_eval_time=3472.34, accumulated_logging_time=1.12265, accumulated_submission_time=11052.8, global_step=55951, preemption_count=0, score=11052.8, test/accuracy=0.984992, test/loss=0.0637832, test/mean_average_precision=0.241382, test/num_examples=43793, total_duration=14527.7, train/accuracy=0.996433, train/loss=0.0114946, train/mean_average_precision=0.801948, validation/accuracy=0.985887, validation/loss=0.0598688, validation/mean_average_precision=0.265079, validation/num_examples=43793
I0305 23:15:08.850315 139545811003136 logging_writer.py:48] [56000] global_step=56000, grad_norm=0.31635814905166626, loss=0.012839317321777344
I0305 23:15:28.638268 139545802610432 logging_writer.py:48] [56100] global_step=56100, grad_norm=0.30107471346855164, loss=0.014119447208940983
I0305 23:15:48.256664 139545811003136 logging_writer.py:48] [56200] global_step=56200, grad_norm=0.30681872367858887, loss=0.011194214224815369
I0305 23:16:08.247906 139545802610432 logging_writer.py:48] [56300] global_step=56300, grad_norm=0.28064611554145813, loss=0.012110898271203041
I0305 23:16:28.030415 139545811003136 logging_writer.py:48] [56400] global_step=56400, grad_norm=0.29851558804512024, loss=0.01123104803264141
I0305 23:16:47.905600 139545802610432 logging_writer.py:48] [56500] global_step=56500, grad_norm=0.25728845596313477, loss=0.010462261736392975
I0305 23:17:07.706421 139545811003136 logging_writer.py:48] [56600] global_step=56600, grad_norm=0.25938287377357483, loss=0.010691812261939049
I0305 23:17:27.540038 139545802610432 logging_writer.py:48] [56700] global_step=56700, grad_norm=0.28632843494415283, loss=0.011520723812282085
I0305 23:17:47.489793 139545811003136 logging_writer.py:48] [56800] global_step=56800, grad_norm=0.28331995010375977, loss=0.01174112781882286
I0305 23:18:07.508901 139545802610432 logging_writer.py:48] [56900] global_step=56900, grad_norm=0.28179416060447693, loss=0.012016285210847855
I0305 23:18:27.258335 139545811003136 logging_writer.py:48] [57000] global_step=57000, grad_norm=0.30141720175743103, loss=0.010786919854581356
I0305 23:18:46.973837 139545802610432 logging_writer.py:48] [57100] global_step=57100, grad_norm=0.2824713885784149, loss=0.010587537661194801
I0305 23:18:58.915916 139687630427328 spec.py:321] Evaluating on the training split.
I0305 23:20:01.130733 139687630427328 spec.py:333] Evaluating on the validation split.
I0305 23:20:03.021941 139687630427328 spec.py:349] Evaluating on the test split.
I0305 23:20:04.876244 139687630427328 submission_runner.py:469] Time since start: 14833.80s, 	Step: 57161, 	{'train/accuracy': 0.9963083267211914, 'train/loss': 0.011760367080569267, 'train/mean_average_precision': 0.784136590179392, 'validation/accuracy': 0.9858870506286621, 'validation/loss': 0.05986879765987396, 'validation/mean_average_precision': 0.2650243850408263, 'validation/num_examples': 43793, 'test/accuracy': 0.9849919676780701, 'test/loss': 0.0637832060456276, 'test/mean_average_precision': 0.24133501810269609, 'test/num_examples': 43793, 'score': 11292.889128923416, 'total_duration': 14833.804478883743, 'accumulated_submission_time': 11292.889128923416, 'accumulated_eval_time': 3538.29944396019, 'accumulated_logging_time': 1.1454780101776123}
I0305 23:20:04.890906 139545811003136 logging_writer.py:48] [57161] accumulated_eval_time=3538.3, accumulated_logging_time=1.14548, accumulated_submission_time=11292.9, global_step=57161, preemption_count=0, score=11292.9, test/accuracy=0.984992, test/loss=0.0637832, test/mean_average_precision=0.241335, test/num_examples=43793, total_duration=14833.8, train/accuracy=0.996308, train/loss=0.0117604, train/mean_average_precision=0.784137, validation/accuracy=0.985887, validation/loss=0.0598688, validation/mean_average_precision=0.265024, validation/num_examples=43793
I0305 23:20:12.889836 139545802610432 logging_writer.py:48] [57200] global_step=57200, grad_norm=0.2936377227306366, loss=0.011735048145055771
I0305 23:20:32.754757 139545811003136 logging_writer.py:48] [57300] global_step=57300, grad_norm=0.31736263632774353, loss=0.013316461816430092
I0305 23:20:52.466110 139545802610432 logging_writer.py:48] [57400] global_step=57400, grad_norm=0.31553012132644653, loss=0.013503040187060833
I0305 23:21:12.325059 139545811003136 logging_writer.py:48] [57500] global_step=57500, grad_norm=0.26391950249671936, loss=0.010984281077980995
I0305 23:21:32.248889 139545802610432 logging_writer.py:48] [57600] global_step=57600, grad_norm=0.3101065158843994, loss=0.0125318868085742
I0305 23:21:52.043086 139545811003136 logging_writer.py:48] [57700] global_step=57700, grad_norm=0.3151003420352936, loss=0.012626134790480137
I0305 23:22:11.879882 139545802610432 logging_writer.py:48] [57800] global_step=57800, grad_norm=0.2882959246635437, loss=0.012334691360592842
I0305 23:22:31.735662 139545811003136 logging_writer.py:48] [57900] global_step=57900, grad_norm=0.2560642957687378, loss=0.011660616844892502
I0305 23:22:51.800796 139545802610432 logging_writer.py:48] [58000] global_step=58000, grad_norm=0.2986244559288025, loss=0.012967607006430626
I0305 23:23:11.584319 139545811003136 logging_writer.py:48] [58100] global_step=58100, grad_norm=0.2897859215736389, loss=0.011695466935634613
I0305 23:23:31.563596 139545802610432 logging_writer.py:48] [58200] global_step=58200, grad_norm=0.27701789140701294, loss=0.011129646562039852
I0305 23:23:51.679377 139545811003136 logging_writer.py:48] [58300] global_step=58300, grad_norm=0.26652583479881287, loss=0.0105108842253685
I0305 23:24:04.949053 139687630427328 spec.py:321] Evaluating on the training split.
I0305 23:25:09.565933 139687630427328 spec.py:333] Evaluating on the validation split.
I0305 23:25:11.438665 139687630427328 spec.py:349] Evaluating on the test split.
I0305 23:25:13.268332 139687630427328 submission_runner.py:469] Time since start: 15142.20s, 	Step: 58367, 	{'train/accuracy': 0.9963335990905762, 'train/loss': 0.011706226505339146, 'train/mean_average_precision': 0.7993987531607404, 'validation/accuracy': 0.9858870506286621, 'validation/loss': 0.05986879765987396, 'validation/mean_average_precision': 0.2649750634303935, 'validation/num_examples': 43793, 'test/accuracy': 0.9849919676780701, 'test/loss': 0.0637832060456276, 'test/mean_average_precision': 0.2414040330205289, 'test/num_examples': 43793, 'score': 11532.904079198837, 'total_duration': 15142.196521997452, 'accumulated_submission_time': 11532.904079198837, 'accumulated_eval_time': 3606.6186332702637, 'accumulated_logging_time': 1.1685066223144531}
I0305 23:25:13.283006 139545802610432 logging_writer.py:48] [58367] accumulated_eval_time=3606.62, accumulated_logging_time=1.16851, accumulated_submission_time=11532.9, global_step=58367, preemption_count=0, score=11532.9, test/accuracy=0.984992, test/loss=0.0637832, test/mean_average_precision=0.241404, test/num_examples=43793, total_duration=15142.2, train/accuracy=0.996334, train/loss=0.0117062, train/mean_average_precision=0.799399, validation/accuracy=0.985887, validation/loss=0.0598688, validation/mean_average_precision=0.264975, validation/num_examples=43793
I0305 23:25:20.123177 139545811003136 logging_writer.py:48] [58400] global_step=58400, grad_norm=0.2745824158191681, loss=0.010773258283734322
I0305 23:25:40.254024 139545802610432 logging_writer.py:48] [58500] global_step=58500, grad_norm=0.24213595688343048, loss=0.011244049295783043
I0305 23:26:00.096495 139545811003136 logging_writer.py:48] [58600] global_step=58600, grad_norm=0.2633884847164154, loss=0.012402275577187538
I0305 23:26:19.983822 139545802610432 logging_writer.py:48] [58700] global_step=58700, grad_norm=0.2846774756908417, loss=0.012524181045591831
I0305 23:26:39.904504 139545811003136 logging_writer.py:48] [58800] global_step=58800, grad_norm=0.28679990768432617, loss=0.01152737531810999
I0305 23:26:59.703449 139545802610432 logging_writer.py:48] [58900] global_step=58900, grad_norm=0.29342377185821533, loss=0.012130319140851498
I0305 23:27:19.611284 139545811003136 logging_writer.py:48] [59000] global_step=59000, grad_norm=0.29201415181159973, loss=0.011841810308396816
I0305 23:27:39.289852 139545802610432 logging_writer.py:48] [59100] global_step=59100, grad_norm=0.30460670590400696, loss=0.010692252777516842
I0305 23:27:59.120431 139545811003136 logging_writer.py:48] [59200] global_step=59200, grad_norm=0.30285611748695374, loss=0.011174933053553104
I0305 23:28:18.959510 139545802610432 logging_writer.py:48] [59300] global_step=59300, grad_norm=0.3009248971939087, loss=0.010275570675730705
I0305 23:28:38.844386 139545811003136 logging_writer.py:48] [59400] global_step=59400, grad_norm=0.29764094948768616, loss=0.013123960234224796
I0305 23:28:58.652189 139545802610432 logging_writer.py:48] [59500] global_step=59500, grad_norm=0.2835828363895416, loss=0.010266944766044617
I0305 23:29:13.359829 139687630427328 spec.py:321] Evaluating on the training split.
I0305 23:30:21.446474 139687630427328 spec.py:333] Evaluating on the validation split.
I0305 23:30:23.562644 139687630427328 spec.py:349] Evaluating on the test split.
I0305 23:30:25.619795 139687630427328 submission_runner.py:469] Time since start: 15454.55s, 	Step: 59575, 	{'train/accuracy': 0.9963187575340271, 'train/loss': 0.011736666783690453, 'train/mean_average_precision': 0.7824068885693742, 'validation/accuracy': 0.9858870506286621, 'validation/loss': 0.05986879765987396, 'validation/mean_average_precision': 0.2650658166235001, 'validation/num_examples': 43793, 'test/accuracy': 0.9849919676780701, 'test/loss': 0.0637832060456276, 'test/mean_average_precision': 0.2414113984700311, 'test/num_examples': 43793, 'score': 11772.93961405754, 'total_duration': 15454.548013448715, 'accumulated_submission_time': 11772.93961405754, 'accumulated_eval_time': 3678.8785400390625, 'accumulated_logging_time': 1.1925256252288818}
I0305 23:30:25.636481 139545811003136 logging_writer.py:48] [59575] accumulated_eval_time=3678.88, accumulated_logging_time=1.19253, accumulated_submission_time=11772.9, global_step=59575, preemption_count=0, score=11772.9, test/accuracy=0.984992, test/loss=0.0637832, test/mean_average_precision=0.241411, test/num_examples=43793, total_duration=15454.5, train/accuracy=0.996319, train/loss=0.0117367, train/mean_average_precision=0.782407, validation/accuracy=0.985887, validation/loss=0.0598688, validation/mean_average_precision=0.265066, validation/num_examples=43793
I0305 23:30:30.763828 139545802610432 logging_writer.py:48] [59600] global_step=59600, grad_norm=0.29543548822402954, loss=0.011310269124805927
I0305 23:30:50.252858 139545811003136 logging_writer.py:48] [59700] global_step=59700, grad_norm=0.261776328086853, loss=0.010362863540649414
I0305 23:31:09.726978 139545802610432 logging_writer.py:48] [59800] global_step=59800, grad_norm=0.29159414768218994, loss=0.012857724912464619
I0305 23:31:29.305589 139545811003136 logging_writer.py:48] [59900] global_step=59900, grad_norm=0.28027772903442383, loss=0.011527582071721554
I0305 23:31:49.025916 139545802610432 logging_writer.py:48] [60000] global_step=60000, grad_norm=0.29784414172172546, loss=0.012622721493244171
I0305 23:32:08.980138 139545811003136 logging_writer.py:48] [60100] global_step=60100, grad_norm=0.2814176678657532, loss=0.012327499687671661
I0305 23:32:28.816458 139545802610432 logging_writer.py:48] [60200] global_step=60200, grad_norm=0.3050246834754944, loss=0.013016782701015472
I0305 23:32:48.602619 139545811003136 logging_writer.py:48] [60300] global_step=60300, grad_norm=0.2576202154159546, loss=0.01117358636111021
I0305 23:33:08.333107 139545802610432 logging_writer.py:48] [60400] global_step=60400, grad_norm=0.29445862770080566, loss=0.012236000038683414
I0305 23:33:28.107538 139545811003136 logging_writer.py:48] [60500] global_step=60500, grad_norm=0.2929856479167938, loss=0.012066257186233997
I0305 23:33:47.915324 139545802610432 logging_writer.py:48] [60600] global_step=60600, grad_norm=0.2581470012664795, loss=0.010845138691365719
I0305 23:34:07.613781 139545811003136 logging_writer.py:48] [60700] global_step=60700, grad_norm=0.2987041771411896, loss=0.01168107520788908
I0305 23:34:25.822143 139545802610432 logging_writer.py:48] [60793] global_step=60793, preemption_count=0, score=12013.1
I0305 23:34:25.946562 139687630427328 submission_runner.py:646] Tuning trial 3/5
I0305 23:34:25.946722 139687630427328 submission_runner.py:647] Hyperparameters: Hyperparameters(dropout_rate=0.0, label_smoothing=0.0, learning_rate=0.001308209823469072, one_minus_beta1=0.02686663061, beta2=0.9981232922116359, weight_decay=0.16375311233774334, warmup_factor=0.1)
I0305 23:34:25.949864 139687630427328 submission_runner.py:648] Metrics: {'eval_results': [(1, {'train/accuracy': 0.5324849486351013, 'train/loss': 0.7127916216850281, 'train/mean_average_precision': 0.022772149164080206, 'validation/accuracy': 0.5275315046310425, 'validation/loss': 0.7149298787117004, 'validation/mean_average_precision': 0.026223687348441105, 'validation/num_examples': 43793, 'test/accuracy': 0.5263952016830444, 'test/loss': 0.7148033976554871, 'test/mean_average_precision': 0.028037889592799795, 'test/num_examples': 43793, 'score': 10.67402958869934, 'total_duration': 209.10578536987305, 'accumulated_submission_time': 10.67402958869934, 'accumulated_eval_time': 198.43166184425354, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1204, {'train/accuracy': 0.9871761202812195, 'train/loss': 0.04907609522342682, 'train/mean_average_precision': 0.08047075406191484, 'validation/accuracy': 0.9845871925354004, 'validation/loss': 0.05789482966065407, 'validation/mean_average_precision': 0.08065719290145534, 'validation/num_examples': 43793, 'test/accuracy': 0.983574628829956, 'test/loss': 0.06109181419014931, 'test/mean_average_precision': 0.07969899945991725, 'test/num_examples': 43793, 'score': 250.7315583229065, 'total_duration': 522.3575792312622, 'accumulated_submission_time': 250.7315583229065, 'accumulated_eval_time': 271.5774166584015, 'accumulated_logging_time': 0.017308950424194336, 'global_step': 1204, 'preemption_count': 0}), (2409, {'train/accuracy': 0.9880992770195007, 'train/loss': 0.04199444502592087, 'train/mean_average_precision': 0.16339833442805085, 'validation/accuracy': 0.9852679371833801, 'validation/loss': 0.05172494426369667, 'validation/mean_average_precision': 0.14779307375768755, 'validation/num_examples': 43793, 'test/accuracy': 0.9842839241027832, 'test/loss': 0.054605670273303986, 'test/mean_average_precision': 0.1524877077419345, 'test/num_examples': 43793, 'score': 490.7717094421387, 'total_duration': 835.2607462406158, 'accumulated_submission_time': 490.7717094421387, 'accumulated_eval_time': 344.39047598838806, 'accumulated_logging_time': 0.03360939025878906, 'global_step': 2409, 'preemption_count': 0}), (3645, {'train/accuracy': 0.9883466362953186, 'train/loss': 0.04063054174184799, 'train/mean_average_precision': 0.19435524908020002, 'validation/accuracy': 0.9853662252426147, 'validation/loss': 0.05041654780507088, 'validation/mean_average_precision': 0.17264956464667588, 'validation/num_examples': 43793, 'test/accuracy': 0.9844789505004883, 'test/loss': 0.052998971194028854, 'test/mean_average_precision': 0.17195503516865465, 'test/num_examples': 43793, 'score': 730.9027371406555, 'total_duration': 1147.6779839992523, 'accumulated_submission_time': 730.9027371406555, 'accumulated_eval_time': 416.6280953884125, 'accumulated_logging_time': 0.0506742000579834, 'global_step': 3645, 'preemption_count': 0}), (4878, {'train/accuracy': 0.9887829422950745, 'train/loss': 0.03813507780432701, 'train/mean_average_precision': 0.22384947313674747, 'validation/accuracy': 0.9857782125473022, 'validation/loss': 0.04807915911078453, 'validation/mean_average_precision': 0.1835097667892105, 'validation/num_examples': 43793, 'test/accuracy': 0.9849051833152771, 'test/loss': 0.05067272484302521, 'test/mean_average_precision': 0.1875361934730356, 'test/num_examples': 43793, 'score': 970.9223113059998, 'total_duration': 1459.2904365062714, 'accumulated_submission_time': 970.9223113059998, 'accumulated_eval_time': 488.17245721817017, 'accumulated_logging_time': 0.06733155250549316, 'global_step': 4878, 'preemption_count': 0}), (6113, {'train/accuracy': 0.9889467358589172, 'train/loss': 0.037651777267456055, 'train/mean_average_precision': 0.246404636580146, 'validation/accuracy': 0.9860400557518005, 'validation/loss': 0.0470418706536293, 'validation/mean_average_precision': 0.20975559101106836, 'validation/num_examples': 43793, 'test/accuracy': 0.9851292371749878, 'test/loss': 0.049701400101184845, 'test/mean_average_precision': 0.20457672642767086, 'test/num_examples': 43793, 'score': 1210.9870240688324, 'total_duration': 1771.2982120513916, 'accumulated_submission_time': 1210.9870240688324, 'accumulated_eval_time': 560.0648708343506, 'accumulated_logging_time': 0.08810544013977051, 'global_step': 6113, 'preemption_count': 0}), (7340, {'train/accuracy': 0.9892685413360596, 'train/loss': 0.03604841232299805, 'train/mean_average_precision': 0.2736420161819601, 'validation/accuracy': 0.9862799644470215, 'validation/loss': 0.046389758586883545, 'validation/mean_average_precision': 0.22707269065121521, 'validation/num_examples': 43793, 'test/accuracy': 0.9854047298431396, 'test/loss': 0.04925492778420448, 'test/mean_average_precision': 0.22034588418347525, 'test/num_examples': 43793, 'score': 1450.9730019569397, 'total_duration': 2082.0228946208954, 'accumulated_submission_time': 1450.9730019569397, 'accumulated_eval_time': 630.7564730644226, 'accumulated_logging_time': 0.10465407371520996, 'global_step': 7340, 'preemption_count': 0}), (8569, {'train/accuracy': 0.989592432975769, 'train/loss': 0.03482768312096596, 'train/mean_average_precision': 0.3000889191251173, 'validation/accuracy': 0.9864391088485718, 'validation/loss': 0.04533017799258232, 'validation/mean_average_precision': 0.2371081444502424, 'validation/num_examples': 43793, 'test/accuracy': 0.9855997562408447, 'test/loss': 0.04814217612147331, 'test/mean_average_precision': 0.23688613582142568, 'test/num_examples': 43793, 'score': 1691.1018335819244, 'total_duration': 2393.1159620285034, 'accumulated_submission_time': 1691.1018335819244, 'accumulated_eval_time': 701.6735055446625, 'accumulated_logging_time': 0.12132859230041504, 'global_step': 8569, 'preemption_count': 0}), (9798, {'train/accuracy': 0.9899195432662964, 'train/loss': 0.03376801684498787, 'train/mean_average_precision': 0.32037757487739493, 'validation/accuracy': 0.9864931106567383, 'validation/loss': 0.044946055859327316, 'validation/mean_average_precision': 0.24195770067966035, 'validation/num_examples': 43793, 'test/accuracy': 0.9856574535369873, 'test/loss': 0.0476822592318058, 'test/mean_average_precision': 0.23730350403973474, 'test/num_examples': 43793, 'score': 1931.2127656936646, 'total_duration': 2706.147906780243, 'accumulated_submission_time': 1931.2127656936646, 'accumulated_eval_time': 774.5461761951447, 'accumulated_logging_time': 0.13878679275512695, 'global_step': 9798, 'preemption_count': 0}), (11015, {'train/accuracy': 0.9901907444000244, 'train/loss': 0.03237757086753845, 'train/mean_average_precision': 0.3564640211945917, 'validation/accuracy': 0.9864581823348999, 'validation/loss': 0.04521080479025841, 'validation/mean_average_precision': 0.2513817432603708, 'validation/num_examples': 43793, 'test/accuracy': 0.9856275320053101, 'test/loss': 0.04784120246767998, 'test/mean_average_precision': 0.2451251461321142, 'test/num_examples': 43793, 'score': 2171.271536588669, 'total_duration': 3018.2631964683533, 'accumulated_submission_time': 2171.271536588669, 'accumulated_eval_time': 846.5513925552368, 'accumulated_logging_time': 0.15779662132263184, 'global_step': 11015, 'preemption_count': 0}), (12240, {'train/accuracy': 0.9900262355804443, 'train/loss': 0.032942771911621094, 'train/mean_average_precision': 0.3528261543639909, 'validation/accuracy': 0.9864752292633057, 'validation/loss': 0.04505610093474388, 'validation/mean_average_precision': 0.2541238149062396, 'validation/num_examples': 43793, 'test/accuracy': 0.9855698347091675, 'test/loss': 0.048024117946624756, 'test/mean_average_precision': 0.23444229498192212, 'test/num_examples': 43793, 'score': 2411.300901412964, 'total_duration': 3334.0134625434875, 'accumulated_submission_time': 2411.300901412964, 'accumulated_eval_time': 922.2181470394135, 'accumulated_logging_time': 0.1759498119354248, 'global_step': 12240, 'preemption_count': 0}), (13463, {'train/accuracy': 0.9906057119369507, 'train/loss': 0.03098604828119278, 'train/mean_average_precision': 0.3852047351392166, 'validation/accuracy': 0.9867269396781921, 'validation/loss': 0.04412157088518143, 'validation/mean_average_precision': 0.2535070583157811, 'validation/num_examples': 43793, 'test/accuracy': 0.9858874082565308, 'test/loss': 0.04695018753409386, 'test/mean_average_precision': 0.2503247057022947, 'test/num_examples': 43793, 'score': 2651.430403470993, 'total_duration': 3645.6581931114197, 'accumulated_submission_time': 2651.430403470993, 'accumulated_eval_time': 993.6824061870575, 'accumulated_logging_time': 0.19545841217041016, 'global_step': 13463, 'preemption_count': 0}), (14688, {'train/accuracy': 0.9903689622879028, 'train/loss': 0.03166794031858444, 'train/mean_average_precision': 0.36240173407478415, 'validation/accuracy': 0.9867252707481384, 'validation/loss': 0.04441898316144943, 'validation/mean_average_precision': 0.2583871765001981, 'validation/num_examples': 43793, 'test/accuracy': 0.9858574867248535, 'test/loss': 0.047323040664196014, 'test/mean_average_precision': 0.24998265527526783, 'test/num_examples': 43793, 'score': 2891.553037881851, 'total_duration': 3959.3234524726868, 'accumulated_submission_time': 2891.553037881851, 'accumulated_eval_time': 1067.1722207069397, 'accumulated_logging_time': 0.21614336967468262, 'global_step': 14688, 'preemption_count': 0}), (15910, {'train/accuracy': 0.9907258152961731, 'train/loss': 0.0304359532892704, 'train/mean_average_precision': 0.40113146296671615, 'validation/accuracy': 0.9865466952323914, 'validation/loss': 0.04438590258359909, 'validation/mean_average_precision': 0.2648781523433797, 'validation/num_examples': 43793, 'test/accuracy': 0.9857627153396606, 'test/loss': 0.047113560140132904, 'test/mean_average_precision': 0.25518366006370996, 'test/num_examples': 43793, 'score': 3131.6017882823944, 'total_duration': 4272.914475917816, 'accumulated_submission_time': 3131.6017882823944, 'accumulated_eval_time': 1140.665118932724, 'accumulated_logging_time': 0.23467564582824707, 'global_step': 15910, 'preemption_count': 0}), (17141, {'train/accuracy': 0.9911357760429382, 'train/loss': 0.02902628481388092, 'train/mean_average_precision': 0.44185861078616373, 'validation/accuracy': 0.9866952300071716, 'validation/loss': 0.044143397361040115, 'validation/mean_average_precision': 0.26362411450218015, 'validation/num_examples': 43793, 'test/accuracy': 0.9859699606895447, 'test/loss': 0.046918243169784546, 'test/mean_average_precision': 0.2599515249035375, 'test/num_examples': 43793, 'score': 3371.6309180259705, 'total_duration': 4583.406119585037, 'accumulated_submission_time': 3371.6309180259705, 'accumulated_eval_time': 1211.0787291526794, 'accumulated_logging_time': 0.253375768661499, 'global_step': 17141, 'preemption_count': 0}), (18366, {'train/accuracy': 0.990848958492279, 'train/loss': 0.03000793792307377, 'train/mean_average_precision': 0.414172371098464, 'validation/accuracy': 0.9868409633636475, 'validation/loss': 0.044032733887434006, 'validation/mean_average_precision': 0.2691975763112511, 'validation/num_examples': 43793, 'test/accuracy': 0.9858849048614502, 'test/loss': 0.047124069184064865, 'test/mean_average_precision': 0.25930658453885996, 'test/num_examples': 43793, 'score': 3611.6044023036957, 'total_duration': 4893.362140893936, 'accumulated_submission_time': 3611.6044023036957, 'accumulated_eval_time': 1281.0123057365417, 'accumulated_logging_time': 0.2712233066558838, 'global_step': 18366, 'preemption_count': 0}), (19593, {'train/accuracy': 0.9913619756698608, 'train/loss': 0.027951931580901146, 'train/mean_average_precision': 0.46056987141319133, 'validation/accuracy': 0.9867938756942749, 'validation/loss': 0.04424015060067177, 'validation/mean_average_precision': 0.2629588466396587, 'validation/num_examples': 43793, 'test/accuracy': 0.9859674572944641, 'test/loss': 0.046958308666944504, 'test/mean_average_precision': 0.2576620552458194, 'test/num_examples': 43793, 'score': 3851.577026128769, 'total_duration': 5205.005909204483, 'accumulated_submission_time': 3851.577026128769, 'accumulated_eval_time': 1352.633070230484, 'accumulated_logging_time': 0.2909684181213379, 'global_step': 19593, 'preemption_count': 0}), (20816, {'train/accuracy': 0.9910523891448975, 'train/loss': 0.029222829267382622, 'train/mean_average_precision': 0.41778139345087284, 'validation/accuracy': 0.9867525100708008, 'validation/loss': 0.04440690204501152, 'validation/mean_average_precision': 0.27012270181055964, 'validation/num_examples': 43793, 'test/accuracy': 0.9858751893043518, 'test/loss': 0.04727625846862793, 'test/mean_average_precision': 0.2598736546946396, 'test/num_examples': 43793, 'score': 4091.637568950653, 'total_duration': 5515.48633646965, 'accumulated_submission_time': 4091.637568950653, 'accumulated_eval_time': 1423.0040102005005, 'accumulated_logging_time': 0.30994296073913574, 'global_step': 20816, 'preemption_count': 0}), (22032, {'train/accuracy': 0.9913603663444519, 'train/loss': 0.028005976229906082, 'train/mean_average_precision': 0.44828949518350664, 'validation/accuracy': 0.9868925213813782, 'validation/loss': 0.04425288364291191, 'validation/mean_average_precision': 0.26733153866410436, 'validation/num_examples': 43793, 'test/accuracy': 0.9859998822212219, 'test/loss': 0.04715970158576965, 'test/mean_average_precision': 0.25995132085304334, 'test/num_examples': 43793, 'score': 4331.674557924271, 'total_duration': 5828.3471467494965, 'accumulated_submission_time': 4331.674557924271, 'accumulated_eval_time': 1495.7772662639618, 'accumulated_logging_time': 0.32903504371643066, 'global_step': 22032, 'preemption_count': 0}), (23253, {'train/accuracy': 0.9916965365409851, 'train/loss': 0.02713916264474392, 'train/mean_average_precision': 0.47032236460966204, 'validation/accuracy': 0.9867314100265503, 'validation/loss': 0.044477157294750214, 'validation/mean_average_precision': 0.27236079983454947, 'validation/num_examples': 43793, 'test/accuracy': 0.9858899712562561, 'test/loss': 0.04728430137038231, 'test/mean_average_precision': 0.259879875471389, 'test/num_examples': 43793, 'score': 4571.760729074478, 'total_duration': 6140.5457010269165, 'accumulated_submission_time': 4571.760729074478, 'accumulated_eval_time': 1567.8344867229462, 'accumulated_logging_time': 0.35194921493530273, 'global_step': 23253, 'preemption_count': 0}), (24474, {'train/accuracy': 0.9913771748542786, 'train/loss': 0.02795359119772911, 'train/mean_average_precision': 0.4639880945840712, 'validation/accuracy': 0.9866157174110413, 'validation/loss': 0.04471296817064285, 'validation/mean_average_precision': 0.2658794889031041, 'validation/num_examples': 43793, 'test/accuracy': 0.9857686161994934, 'test/loss': 0.0475628487765789, 'test/mean_average_precision': 0.26343391387702897, 'test/num_examples': 43793, 'score': 4811.74719452858, 'total_duration': 6453.2113468647, 'accumulated_submission_time': 4811.74719452858, 'accumulated_eval_time': 1640.462219953537, 'accumulated_logging_time': 0.3724641799926758, 'global_step': 24474, 'preemption_count': 0}), (25696, {'train/accuracy': 0.9922515749931335, 'train/loss': 0.02519526518881321, 'train/mean_average_precision': 0.5202277770118576, 'validation/accuracy': 0.986694872379303, 'validation/loss': 0.044626206159591675, 'validation/mean_average_precision': 0.27444973542798934, 'validation/num_examples': 43793, 'test/accuracy': 0.9858874082565308, 'test/loss': 0.047508157789707184, 'test/mean_average_precision': 0.2572674522705059, 'test/num_examples': 43793, 'score': 5051.872892379761, 'total_duration': 6762.139156103134, 'accumulated_submission_time': 5051.872892379761, 'accumulated_eval_time': 1709.213612318039, 'accumulated_logging_time': 0.39147210121154785, 'global_step': 25696, 'preemption_count': 0}), (26915, {'train/accuracy': 0.9915521740913391, 'train/loss': 0.026987168937921524, 'train/mean_average_precision': 0.4832061447395173, 'validation/accuracy': 0.9869270324707031, 'validation/loss': 0.04500902071595192, 'validation/mean_average_precision': 0.2708809412685069, 'validation/num_examples': 43793, 'test/accuracy': 0.9860883355140686, 'test/loss': 0.0479976125061512, 'test/mean_average_precision': 0.25808631938728016, 'test/num_examples': 43793, 'score': 5291.967322587967, 'total_duration': 7071.370110988617, 'accumulated_submission_time': 5291.967322587967, 'accumulated_eval_time': 1778.298603773117, 'accumulated_logging_time': 0.4122025966644287, 'global_step': 26915, 'preemption_count': 0}), (28134, {'train/accuracy': 0.9922338724136353, 'train/loss': 0.024968398734927177, 'train/mean_average_precision': 0.5279221973231529, 'validation/accuracy': 0.9868133664131165, 'validation/loss': 0.04479990899562836, 'validation/mean_average_precision': 0.27574563305826694, 'validation/num_examples': 43793, 'test/accuracy': 0.9859544038772583, 'test/loss': 0.04763316735625267, 'test/mean_average_precision': 0.26170569238711117, 'test/num_examples': 43793, 'score': 5532.063780069351, 'total_duration': 7383.449518203735, 'accumulated_submission_time': 5532.063780069351, 'accumulated_eval_time': 1850.230328321457, 'accumulated_logging_time': 0.43329620361328125, 'global_step': 28134, 'preemption_count': 0}), (29350, {'train/accuracy': 0.9921098351478577, 'train/loss': 0.02539261430501938, 'train/mean_average_precision': 0.5171862517074713, 'validation/accuracy': 0.9866664409637451, 'validation/loss': 0.04558609053492546, 'validation/mean_average_precision': 0.27234923305241, 'validation/num_examples': 43793, 'test/accuracy': 0.9857168197631836, 'test/loss': 0.04859912768006325, 'test/mean_average_precision': 0.2582175632022682, 'test/num_examples': 43793, 'score': 5771.846256017685, 'total_duration': 7693.835065603256, 'accumulated_submission_time': 5771.846256017685, 'accumulated_eval_time': 1920.6105341911316, 'accumulated_logging_time': 0.6227314472198486, 'global_step': 29350, 'preemption_count': 0}), (30566, {'train/accuracy': 0.9924203753471375, 'train/loss': 0.024254292249679565, 'train/mean_average_precision': 0.5492674977421617, 'validation/accuracy': 0.9867610335350037, 'validation/loss': 0.04547472298145294, 'validation/mean_average_precision': 0.27684071807128124, 'validation/num_examples': 43793, 'test/accuracy': 0.9858629703521729, 'test/loss': 0.04846695810556412, 'test/mean_average_precision': 0.2593116743688929, 'test/num_examples': 43793, 'score': 6011.936239719391, 'total_duration': 8002.30813908577, 'accumulated_submission_time': 6011.936239719391, 'accumulated_eval_time': 1988.9422948360443, 'accumulated_logging_time': 0.6426005363464355, 'global_step': 30566, 'preemption_count': 0}), (31767, {'train/accuracy': 0.9928501844406128, 'train/loss': 0.02290358953177929, 'train/mean_average_precision': 0.571842355145451, 'validation/accuracy': 0.9867390990257263, 'validation/loss': 0.046047911047935486, 'validation/mean_average_precision': 0.2789651747140922, 'validation/num_examples': 43793, 'test/accuracy': 0.9858962893486023, 'test/loss': 0.049145907163619995, 'test/mean_average_precision': 0.26299917606479417, 'test/num_examples': 43793, 'score': 6252.002446651459, 'total_duration': 8315.325271844864, 'accumulated_submission_time': 6252.002446651459, 'accumulated_eval_time': 2061.8388397693634, 'accumulated_logging_time': 0.6643259525299072, 'global_step': 31767, 'preemption_count': 0}), (32979, {'train/accuracy': 0.9926832914352417, 'train/loss': 0.023266520351171494, 'train/mean_average_precision': 0.5640265364883741, 'validation/accuracy': 0.9866628050804138, 'validation/loss': 0.04620925709605217, 'validation/mean_average_precision': 0.2764671411005253, 'validation/num_examples': 43793, 'test/accuracy': 0.9858827590942383, 'test/loss': 0.0491117425262928, 'test/mean_average_precision': 0.26013421347074905, 'test/num_examples': 43793, 'score': 6492.1075756549835, 'total_duration': 8624.499893426895, 'accumulated_submission_time': 6492.1075756549835, 'accumulated_eval_time': 2130.8560919761658, 'accumulated_logging_time': 0.6854081153869629, 'global_step': 32979, 'preemption_count': 0}), (34193, {'train/accuracy': 0.9934563636779785, 'train/loss': 0.02103402651846409, 'train/mean_average_precision': 0.614450689646107, 'validation/accuracy': 0.9866648316383362, 'validation/loss': 0.0466645322740078, 'validation/mean_average_precision': 0.27216556177244544, 'validation/num_examples': 43793, 'test/accuracy': 0.985775351524353, 'test/loss': 0.04976813867688179, 'test/mean_average_precision': 0.25983604405647626, 'test/num_examples': 43793, 'score': 6732.248635292053, 'total_duration': 8935.255256414413, 'accumulated_submission_time': 6732.248635292053, 'accumulated_eval_time': 2201.4190924167633, 'accumulated_logging_time': 0.7073702812194824, 'global_step': 34193, 'preemption_count': 0}), (35394, {'train/accuracy': 0.9929504990577698, 'train/loss': 0.022186126559972763, 'train/mean_average_precision': 0.5776467551911265, 'validation/accuracy': 0.9866635799407959, 'validation/loss': 0.04756147041916847, 'validation/mean_average_precision': 0.2751614352413955, 'validation/num_examples': 43793, 'test/accuracy': 0.9858301281929016, 'test/loss': 0.050821080803871155, 'test/mean_average_precision': 0.25950086093037295, 'test/num_examples': 43793, 'score': 6972.303344249725, 'total_duration': 9245.912311553955, 'accumulated_submission_time': 6972.303344249725, 'accumulated_eval_time': 2271.966322660446, 'accumulated_logging_time': 0.7301015853881836, 'global_step': 35394, 'preemption_count': 0}), (36608, {'train/accuracy': 0.9939799308776855, 'train/loss': 0.01895810477435589, 'train/mean_average_precision': 0.6490916796336258, 'validation/accuracy': 0.9865085482597351, 'validation/loss': 0.04855643957853317, 'validation/mean_average_precision': 0.2744268083014169, 'validation/num_examples': 43793, 'test/accuracy': 0.985675573348999, 'test/loss': 0.05171124264597893, 'test/mean_average_precision': 0.2612139139727055, 'test/num_examples': 43793, 'score': 7212.3032240867615, 'total_duration': 9555.129102945328, 'accumulated_submission_time': 7212.3032240867615, 'accumulated_eval_time': 2341.1317133903503, 'accumulated_logging_time': 0.7511348724365234, 'global_step': 36608, 'preemption_count': 0}), (37808, {'train/accuracy': 0.9934921264648438, 'train/loss': 0.020359596237540245, 'train/mean_average_precision': 0.6289925457005716, 'validation/accuracy': 0.9865511655807495, 'validation/loss': 0.0488634929060936, 'validation/mean_average_precision': 0.28069273184885213, 'validation/num_examples': 43793, 'test/accuracy': 0.9857433438301086, 'test/loss': 0.05212013050913811, 'test/mean_average_precision': 0.2588480247348749, 'test/num_examples': 43793, 'score': 7452.266214132309, 'total_duration': 9865.224529504776, 'accumulated_submission_time': 7452.266214132309, 'accumulated_eval_time': 2411.2121844291687, 'accumulated_logging_time': 0.7727429866790771, 'global_step': 37808, 'preemption_count': 0}), (39015, {'train/accuracy': 0.9942969679832458, 'train/loss': 0.01785445772111416, 'train/mean_average_precision': 0.6890866479986226, 'validation/accuracy': 0.986544668674469, 'validation/loss': 0.049699265509843826, 'validation/mean_average_precision': 0.2775979831311004, 'validation/num_examples': 43793, 'test/accuracy': 0.9856738448143005, 'test/loss': 0.0533912256360054, 'test/mean_average_precision': 0.25678655838797937, 'test/num_examples': 43793, 'score': 7692.324460506439, 'total_duration': 10174.028367757797, 'accumulated_submission_time': 7692.324460506439, 'accumulated_eval_time': 2479.9065437316895, 'accumulated_logging_time': 0.7934949398040771, 'global_step': 39015, 'preemption_count': 0}), (40221, {'train/accuracy': 0.9941980838775635, 'train/loss': 0.01802048645913601, 'train/mean_average_precision': 0.6603194886587318, 'validation/accuracy': 0.9862543940544128, 'validation/loss': 0.051480360329151154, 'validation/mean_average_precision': 0.2699848347524564, 'validation/num_examples': 43793, 'test/accuracy': 0.985448956489563, 'test/loss': 0.05503067374229431, 'test/mean_average_precision': 0.25255849475290615, 'test/num_examples': 43793, 'score': 7932.28755402565, 'total_duration': 10483.720953464508, 'accumulated_submission_time': 7932.28755402565, 'accumulated_eval_time': 2549.5795407295227, 'accumulated_logging_time': 0.8173904418945312, 'global_step': 40221, 'preemption_count': 0}), (41432, {'train/accuracy': 0.9941955804824829, 'train/loss': 0.017906980589032173, 'train/mean_average_precision': 0.6777833181242683, 'validation/accuracy': 0.9864252805709839, 'validation/loss': 0.05272669345140457, 'validation/mean_average_precision': 0.2715637943823335, 'validation/num_examples': 43793, 'test/accuracy': 0.9855592846870422, 'test/loss': 0.05640117824077606, 'test/mean_average_precision': 0.25038811633441804, 'test/num_examples': 43793, 'score': 8172.382656335831, 'total_duration': 10794.064980506897, 'accumulated_submission_time': 8172.382656335831, 'accumulated_eval_time': 2619.7752861976624, 'accumulated_logging_time': 0.8405411243438721, 'global_step': 41432, 'preemption_count': 0}), (42643, {'train/accuracy': 0.9953868985176086, 'train/loss': 0.014556092210114002, 'train/mean_average_precision': 0.7477537997126755, 'validation/accuracy': 0.9862832427024841, 'validation/loss': 0.0541796013712883, 'validation/mean_average_precision': 0.2707801864670295, 'validation/num_examples': 43793, 'test/accuracy': 0.9854830503463745, 'test/loss': 0.05771397054195404, 'test/mean_average_precision': 0.24811548273051695, 'test/num_examples': 43793, 'score': 8412.358704090118, 'total_duration': 11103.762996196747, 'accumulated_submission_time': 8412.358704090118, 'accumulated_eval_time': 2689.444177389145, 'accumulated_logging_time': 0.8624267578125, 'global_step': 42643, 'preemption_count': 0}), (43853, {'train/accuracy': 0.9943060278892517, 'train/loss': 0.017227711156010628, 'train/mean_average_precision': 0.6818539268150086, 'validation/accuracy': 0.9862117767333984, 'validation/loss': 0.05548793822526932, 'validation/mean_average_precision': 0.26483907223112185, 'validation/num_examples': 43793, 'test/accuracy': 0.985284686088562, 'test/loss': 0.059298399835824966, 'test/mean_average_precision': 0.24626088717364797, 'test/num_examples': 43793, 'score': 8652.412561416626, 'total_duration': 11425.59073138237, 'accumulated_submission_time': 8652.412561416626, 'accumulated_eval_time': 2771.1657843589783, 'accumulated_logging_time': 0.8839075565338135, 'global_step': 43853, 'preemption_count': 0}), (45064, {'train/accuracy': 0.995733916759491, 'train/loss': 0.013313550502061844, 'train/mean_average_precision': 0.7693390163231147, 'validation/accuracy': 0.9860656261444092, 'validation/loss': 0.057048480957746506, 'validation/mean_average_precision': 0.26738656516529385, 'validation/num_examples': 43793, 'test/accuracy': 0.9852514266967773, 'test/loss': 0.060975331813097, 'test/mean_average_precision': 0.2454792240816785, 'test/num_examples': 43793, 'score': 8892.458438634872, 'total_duration': 11734.004797458649, 'accumulated_submission_time': 8892.458438634872, 'accumulated_eval_time': 2839.47998213768, 'accumulated_logging_time': 0.9079875946044922, 'global_step': 45064, 'preemption_count': 0}), (46270, {'train/accuracy': 0.9945857524871826, 'train/loss': 0.01629352569580078, 'train/mean_average_precision': 0.6994421959246826, 'validation/accuracy': 0.9860063791275024, 'validation/loss': 0.0582558810710907, 'validation/mean_average_precision': 0.26356643551369124, 'validation/num_examples': 43793, 'test/accuracy': 0.9851271510124207, 'test/loss': 0.06225794926285744, 'test/mean_average_precision': 0.24191973525140406, 'test/num_examples': 43793, 'score': 9132.576266050339, 'total_duration': 12047.43895483017, 'accumulated_submission_time': 9132.576266050339, 'accumulated_eval_time': 2912.7416954040527, 'accumulated_logging_time': 0.93117356300354, 'global_step': 46270, 'preemption_count': 0}), (47479, {'train/accuracy': 0.9963548183441162, 'train/loss': 0.011834407225251198, 'train/mean_average_precision': 0.791158664736933, 'validation/accuracy': 0.9858565926551819, 'validation/loss': 0.05903805419802666, 'validation/mean_average_precision': 0.2625702047212066, 'validation/num_examples': 43793, 'test/accuracy': 0.9849759340286255, 'test/loss': 0.06291484832763672, 'test/mean_average_precision': 0.24228873326518235, 'test/num_examples': 43793, 'score': 9372.704038858414, 'total_duration': 12356.172856330872, 'accumulated_submission_time': 9372.704038858414, 'accumulated_eval_time': 2981.29327750206, 'accumulated_logging_time': 0.9538002014160156, 'global_step': 47479, 'preemption_count': 0}), (48691, {'train/accuracy': 0.9956499338150024, 'train/loss': 0.0133504094555974, 'train/mean_average_precision': 0.7631207125557389, 'validation/accuracy': 0.9858602285385132, 'validation/loss': 0.059691060334444046, 'validation/mean_average_precision': 0.2609555060378384, 'validation/num_examples': 43793, 'test/accuracy': 0.9849544763565063, 'test/loss': 0.06358589231967926, 'test/mean_average_precision': 0.24095917274716158, 'test/num_examples': 43793, 'score': 9612.742330789566, 'total_duration': 12665.798841238022, 'accumulated_submission_time': 9612.742330789566, 'accumulated_eval_time': 3050.8289000988007, 'accumulated_logging_time': 0.9754054546356201, 'global_step': 48691, 'preemption_count': 0}), (49901, {'train/accuracy': 0.9964169263839722, 'train/loss': 0.011543062515556812, 'train/mean_average_precision': 0.8007839953095489, 'validation/accuracy': 0.9858164191246033, 'validation/loss': 0.059869881719350815, 'validation/mean_average_precision': 0.2643400466467922, 'validation/num_examples': 43793, 'test/accuracy': 0.9849587082862854, 'test/loss': 0.06374257802963257, 'test/mean_average_precision': 0.2422161897737763, 'test/num_examples': 43793, 'score': 9852.70882844925, 'total_duration': 12972.042619943619, 'accumulated_submission_time': 9852.70882844925, 'accumulated_eval_time': 3117.051324367523, 'accumulated_logging_time': 0.9972708225250244, 'global_step': 49901, 'preemption_count': 0}), (51114, {'train/accuracy': 0.9961540102958679, 'train/loss': 0.012096261605620384, 'train/mean_average_precision': 0.7782761178691838, 'validation/accuracy': 0.9858553409576416, 'validation/loss': 0.05986309424042702, 'validation/mean_average_precision': 0.2644024641725697, 'validation/num_examples': 43793, 'test/accuracy': 0.9849730134010315, 'test/loss': 0.0638059675693512, 'test/mean_average_precision': 0.24113731373265657, 'test/num_examples': 43793, 'score': 10092.739246845245, 'total_duration': 13283.808974981308, 'accumulated_submission_time': 10092.739246845245, 'accumulated_eval_time': 3188.7349989414215, 'accumulated_logging_time': 1.0195181369781494, 'global_step': 51114, 'preemption_count': 0}), (52336, {'train/accuracy': 0.996330976486206, 'train/loss': 0.01173893641680479, 'train/mean_average_precision': 0.7921525859571945, 'validation/accuracy': 0.9858870506286621, 'validation/loss': 0.05986879765987396, 'validation/mean_average_precision': 0.26519501289848596, 'validation/num_examples': 43793, 'test/accuracy': 0.9849919676780701, 'test/loss': 0.0637832060456276, 'test/mean_average_precision': 0.24139097782376312, 'test/num_examples': 43793, 'score': 10332.829066991806, 'total_duration': 13595.376545190811, 'accumulated_submission_time': 10332.829066991806, 'accumulated_eval_time': 3260.153562784195, 'accumulated_logging_time': 1.0475268363952637, 'global_step': 52336, 'preemption_count': 0}), (53536, {'train/accuracy': 0.9963099956512451, 'train/loss': 0.011750664561986923, 'train/mean_average_precision': 0.7882473615423955, 'validation/accuracy': 0.9858870506286621, 'validation/loss': 0.05986879765987396, 'validation/mean_average_precision': 0.26490736160127776, 'validation/num_examples': 43793, 'test/accuracy': 0.9849919676780701, 'test/loss': 0.0637832060456276, 'test/mean_average_precision': 0.24133403014563978, 'test/num_examples': 43793, 'score': 10572.83955025673, 'total_duration': 13906.302777290344, 'accumulated_submission_time': 10572.83955025673, 'accumulated_eval_time': 3331.0129964351654, 'accumulated_logging_time': 1.071716547012329, 'global_step': 53536, 'preemption_count': 0}), (54744, {'train/accuracy': 0.9964504837989807, 'train/loss': 0.011426721699535847, 'train/mean_average_precision': 0.7945969880651383, 'validation/accuracy': 0.9858870506286621, 'validation/loss': 0.05986879765987396, 'validation/mean_average_precision': 0.2650052452796968, 'validation/num_examples': 43793, 'test/accuracy': 0.9849919676780701, 'test/loss': 0.0637832060456276, 'test/mean_average_precision': 0.24141086648712926, 'test/num_examples': 43793, 'score': 10812.80606842041, 'total_duration': 14218.85094833374, 'accumulated_submission_time': 10812.80606842041, 'accumulated_eval_time': 3403.5387678146362, 'accumulated_logging_time': 1.0976173877716064, 'global_step': 54744, 'preemption_count': 0}), (55951, {'train/accuracy': 0.9964334964752197, 'train/loss': 0.011494585312902927, 'train/mean_average_precision': 0.801948454500571, 'validation/accuracy': 0.9858870506286621, 'validation/loss': 0.05986879765987396, 'validation/mean_average_precision': 0.26507867878147123, 'validation/num_examples': 43793, 'test/accuracy': 0.9849919676780701, 'test/loss': 0.0637832060456276, 'test/mean_average_precision': 0.24138186741962345, 'test/num_examples': 43793, 'score': 11052.775285720825, 'total_duration': 14527.675853013992, 'accumulated_submission_time': 11052.775285720825, 'accumulated_eval_time': 3472.3391587734222, 'accumulated_logging_time': 1.122654914855957, 'global_step': 55951, 'preemption_count': 0}), (57161, {'train/accuracy': 0.9963083267211914, 'train/loss': 0.011760367080569267, 'train/mean_average_precision': 0.784136590179392, 'validation/accuracy': 0.9858870506286621, 'validation/loss': 0.05986879765987396, 'validation/mean_average_precision': 0.2650243850408263, 'validation/num_examples': 43793, 'test/accuracy': 0.9849919676780701, 'test/loss': 0.0637832060456276, 'test/mean_average_precision': 0.24133501810269609, 'test/num_examples': 43793, 'score': 11292.889128923416, 'total_duration': 14833.804478883743, 'accumulated_submission_time': 11292.889128923416, 'accumulated_eval_time': 3538.29944396019, 'accumulated_logging_time': 1.1454780101776123, 'global_step': 57161, 'preemption_count': 0}), (58367, {'train/accuracy': 0.9963335990905762, 'train/loss': 0.011706226505339146, 'train/mean_average_precision': 0.7993987531607404, 'validation/accuracy': 0.9858870506286621, 'validation/loss': 0.05986879765987396, 'validation/mean_average_precision': 0.2649750634303935, 'validation/num_examples': 43793, 'test/accuracy': 0.9849919676780701, 'test/loss': 0.0637832060456276, 'test/mean_average_precision': 0.2414040330205289, 'test/num_examples': 43793, 'score': 11532.904079198837, 'total_duration': 15142.196521997452, 'accumulated_submission_time': 11532.904079198837, 'accumulated_eval_time': 3606.6186332702637, 'accumulated_logging_time': 1.1685066223144531, 'global_step': 58367, 'preemption_count': 0}), (59575, {'train/accuracy': 0.9963187575340271, 'train/loss': 0.011736666783690453, 'train/mean_average_precision': 0.7824068885693742, 'validation/accuracy': 0.9858870506286621, 'validation/loss': 0.05986879765987396, 'validation/mean_average_precision': 0.2650658166235001, 'validation/num_examples': 43793, 'test/accuracy': 0.9849919676780701, 'test/loss': 0.0637832060456276, 'test/mean_average_precision': 0.2414113984700311, 'test/num_examples': 43793, 'score': 11772.93961405754, 'total_duration': 15454.548013448715, 'accumulated_submission_time': 11772.93961405754, 'accumulated_eval_time': 3678.8785400390625, 'accumulated_logging_time': 1.1925256252288818, 'global_step': 59575, 'preemption_count': 0})], 'global_step': 60793}
I0305 23:34:25.949967 139687630427328 submission_runner.py:649] Timing: 12013.074071407318
I0305 23:34:25.950005 139687630427328 submission_runner.py:651] Total number of evals: 50
I0305 23:34:25.950032 139687630427328 submission_runner.py:652] ====================
I0305 23:34:25.950270 139687630427328 submission_runner.py:750] Final ogbg score: 2
