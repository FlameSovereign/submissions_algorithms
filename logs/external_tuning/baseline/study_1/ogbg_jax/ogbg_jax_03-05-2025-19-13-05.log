python submission_runner.py --framework=jax --workload=ogbg --submission_path=prize_qualification_baselines/external_tuning/jax_nadamw_full_budget.py --data_dir=/data/ogbg --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=submissions/rolling_leaderboard/external_tuning/baseline/study_1 --overwrite=True --save_checkpoints=False --rng_seed=-1237958680 --tuning_ruleset=external --tuning_search_space=prize_qualification_baselines/external_tuning/tuning_search_space.json --num_tuning_trials=5 --hparam_start_index=4 --hparam_end_index=5 2>&1 | tee -a /logs/ogbg_jax_03-05-2025-19-13-05.log
2025-03-05 19:13:06.676815: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1741201986.699592       9 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1741201986.706387       9 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
I0305 19:13:13.045449 140593289979072 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_1/ogbg_jax.
I0305 19:13:14.119328 140593289979072 xla_bridge.py:884] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA
I0305 19:13:14.122602 140593289979072 xla_bridge.py:884] Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory
I0305 19:13:14.124341 140593289979072 submission_runner.py:606] Using RNG seed -1237958680
I0305 19:13:14.748533 140593289979072 submission_runner.py:615] --- Tuning run 5/5 ---
I0305 19:13:14.748723 140593289979072 submission_runner.py:620] Creating tuning directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_1/ogbg_jax/trial_5.
I0305 19:13:14.748909 140593289979072 logger_utils.py:97] Saving hparams to /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_1/ogbg_jax/trial_5/hparams.json.
I0305 19:13:14.984135 140593289979072 submission_runner.py:218] Initializing dataset.
I0305 19:13:15.218346 140593289979072 dataset_info.py:690] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0305 19:13:15.263083 140593289979072 reader.py:261] Creating a tf.data.Dataset reading 8 files located in folders: /data/ogbg/ogbg_molpcba/0.1.3.
WARNING:tensorflow:From /usr/local/lib/python3.11/site-packages/tensorflow_datasets/core/reader.py:101: CounterV2 (from tensorflow.python.data.experimental.ops.counter) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.counter(...)` instead.
W0305 19:13:15.496324 140593289979072 deprecation.py:50] From /usr/local/lib/python3.11/site-packages/tensorflow_datasets/core/reader.py:101: CounterV2 (from tensorflow.python.data.experimental.ops.counter) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.counter(...)` instead.
I0305 19:13:15.546344 140593289979072 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split train, from /data/ogbg/ogbg_molpcba/0.1.3
I0305 19:13:15.574979 140593289979072 submission_runner.py:229] Initializing model.
I0305 19:13:23.280471 140593289979072 submission_runner.py:272] Initializing optimizer.
I0305 19:13:23.682083 140593289979072 submission_runner.py:279] Initializing metrics bundle.
I0305 19:13:23.682310 140593289979072 submission_runner.py:301] Initializing checkpoint and logger.
I0305 19:13:23.683136 140593289979072 checkpoints.py:1101] Found no checkpoint files in /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_1/ogbg_jax/trial_5 with prefix checkpoint_
I0305 19:13:23.683244 140593289979072 submission_runner.py:321] Saving meta data to /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_1/ogbg_jax/trial_5/meta_data_0.json.
I0305 19:13:23.683427 140593289979072 logger_utils.py:262] Unable to record workload.train_mean information. Continuing without it.
I0305 19:13:23.683480 140593289979072 logger_utils.py:262] Unable to record workload.train_stddev information. Continuing without it.
I0305 19:13:23.838006 140593289979072 submission_runner.py:325] Saving flags to /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_1/ogbg_jax/trial_5/flags_0.json.
I0305 19:13:23.870921 140593289979072 submission_runner.py:337] Starting training loop.
I0305 19:13:39.335931 140457138575104 logging_writer.py:48] [0] global_step=0, grad_norm=2.0166611671447754, loss=0.7810970544815063
I0305 19:13:39.388988 140593289979072 spec.py:321] Evaluating on the training split.
I0305 19:13:39.392793 140593289979072 dataset_info.py:690] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0305 19:13:39.396334 140593289979072 reader.py:261] Creating a tf.data.Dataset reading 8 files located in folders: /data/ogbg/ogbg_molpcba/0.1.3.
I0305 19:13:39.457524 140593289979072 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split train, from /data/ogbg/ogbg_molpcba/0.1.3
I0305 19:14:55.231622 140593289979072 spec.py:333] Evaluating on the validation split.
I0305 19:14:55.234351 140593289979072 dataset_info.py:690] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0305 19:14:55.237966 140593289979072 reader.py:261] Creating a tf.data.Dataset reading 1 files located in folders: /data/ogbg/ogbg_molpcba/0.1.3.
I0305 19:14:55.295919 140593289979072 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split validation, from /data/ogbg/ogbg_molpcba/0.1.3
I0305 19:15:56.583943 140593289979072 spec.py:349] Evaluating on the test split.
I0305 19:15:56.586407 140593289979072 dataset_info.py:690] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0305 19:15:56.589921 140593289979072 reader.py:261] Creating a tf.data.Dataset reading 1 files located in folders: /data/ogbg/ogbg_molpcba/0.1.3.
I0305 19:15:56.649159 140593289979072 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split test, from /data/ogbg/ogbg_molpcba/0.1.3
I0305 19:16:59.678519 140593289979072 submission_runner.py:469] Time since start: 215.81s, 	Step: 1, 	{'train/accuracy': 0.470598429441452, 'train/loss': 0.7706692218780518, 'train/mean_average_precision': 0.020994739835503692, 'validation/accuracy': 0.4706586003303528, 'validation/loss': 0.7734909057617188, 'validation/mean_average_precision': 0.024877652610798433, 'validation/num_examples': 43793, 'test/accuracy': 0.4712654650211334, 'test/loss': 0.7741150856018066, 'test/mean_average_precision': 0.026006626473790734, 'test/num_examples': 43793, 'score': 15.517959833145142, 'total_duration': 215.80748796463013, 'accumulated_submission_time': 15.517959833145142, 'accumulated_eval_time': 200.28941679000854, 'accumulated_logging_time': 0}
I0305 19:16:59.685081 140451212883712 logging_writer.py:48] [1] accumulated_eval_time=200.289, accumulated_logging_time=0, accumulated_submission_time=15.518, global_step=1, preemption_count=0, score=15.518, test/accuracy=0.471265, test/loss=0.774115, test/mean_average_precision=0.0260066, test/num_examples=43793, total_duration=215.807, train/accuracy=0.470598, train/loss=0.770669, train/mean_average_precision=0.0209947, validation/accuracy=0.470659, validation/loss=0.773491, validation/mean_average_precision=0.0248777, validation/num_examples=43793
I0305 19:17:20.376325 140451221276416 logging_writer.py:48] [100] global_step=100, grad_norm=0.23755154013633728, loss=0.22092051804065704
I0305 19:17:40.932782 140451212883712 logging_writer.py:48] [200] global_step=200, grad_norm=0.05009322613477707, loss=0.07721350342035294
I0305 19:18:01.768714 140451221276416 logging_writer.py:48] [300] global_step=300, grad_norm=0.017643602564930916, loss=0.06646721810102463
I0305 19:18:22.285415 140451212883712 logging_writer.py:48] [400] global_step=400, grad_norm=0.012151694856584072, loss=0.05350137874484062
I0305 19:18:42.977531 140451221276416 logging_writer.py:48] [500] global_step=500, grad_norm=0.014832111075520515, loss=0.05523774400353432
I0305 19:19:03.565188 140451212883712 logging_writer.py:48] [600] global_step=600, grad_norm=0.01962418667972088, loss=0.06160924583673477
I0305 19:19:24.218131 140451839440640 logging_writer.py:48] [700] global_step=700, grad_norm=0.029470257461071014, loss=0.04513133317232132
I0305 19:19:44.612914 140451831047936 logging_writer.py:48] [800] global_step=800, grad_norm=0.017659636214375496, loss=0.05293205380439758
I0305 19:20:05.144788 140451839440640 logging_writer.py:48] [900] global_step=900, grad_norm=0.01680007018148899, loss=0.05054289475083351
I0305 19:20:25.717577 140451831047936 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.01521760132163763, loss=0.052249711006879807
I0305 19:20:46.354761 140451839440640 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.011797197163105011, loss=0.04990139603614807
I0305 19:20:59.791738 140593289979072 spec.py:321] Evaluating on the training split.
I0305 19:22:13.110382 140593289979072 spec.py:333] Evaluating on the validation split.
I0305 19:22:15.073255 140593289979072 spec.py:349] Evaluating on the test split.
I0305 19:22:16.901640 140593289979072 submission_runner.py:469] Time since start: 533.03s, 	Step: 1166, 	{'train/accuracy': 0.9870123863220215, 'train/loss': 0.04836277291178703, 'train/mean_average_precision': 0.08382391293765193, 'validation/accuracy': 0.9843952059745789, 'validation/loss': 0.05830450356006622, 'validation/mean_average_precision': 0.0830809895285746, 'validation/num_examples': 43793, 'test/accuracy': 0.9833783507347107, 'test/loss': 0.061767686158418655, 'test/mean_average_precision': 0.08145000058663757, 'test/num_examples': 43793, 'score': 255.58223843574524, 'total_duration': 533.0306243896484, 'accumulated_submission_time': 255.58223843574524, 'accumulated_eval_time': 277.39924907684326, 'accumulated_logging_time': 0.016170501708984375}
I0305 19:22:16.910208 140451831047936 logging_writer.py:48] [1166] accumulated_eval_time=277.399, accumulated_logging_time=0.0161705, accumulated_submission_time=255.582, global_step=1166, preemption_count=0, score=255.582, test/accuracy=0.983378, test/loss=0.0617677, test/mean_average_precision=0.08145, test/num_examples=43793, total_duration=533.031, train/accuracy=0.987012, train/loss=0.0483628, train/mean_average_precision=0.0838239, validation/accuracy=0.984395, validation/loss=0.0583045, validation/mean_average_precision=0.083081, validation/num_examples=43793
I0305 19:22:24.116176 140451839440640 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.0105226319283247, loss=0.04337980970740318
I0305 19:22:44.715209 140451831047936 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.011666874401271343, loss=0.048930756747722626
I0305 19:23:05.441845 140451839440640 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.021740907803177834, loss=0.04583987221121788
I0305 19:23:26.287916 140451831047936 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.02657621167600155, loss=0.047184787690639496
I0305 19:23:46.967335 140451839440640 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.021350331604480743, loss=0.04706622660160065
I0305 19:24:07.499111 140451831047936 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.011363073252141476, loss=0.049726277589797974
I0305 19:24:28.140590 140451839440640 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.014649963937699795, loss=0.04994824156165123
I0305 19:24:48.879574 140451831047936 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.010427352972328663, loss=0.03715831786394119
I0305 19:25:09.710750 140451839440640 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.017810123041272163, loss=0.050339438021183014
I0305 19:25:30.275305 140451831047936 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.01868600584566593, loss=0.04610263556241989
I0305 19:25:50.733133 140451839440640 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.009017981588840485, loss=0.04170697182416916
I0305 19:26:11.171127 140451831047936 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.012226327322423458, loss=0.04495329037308693
I0305 19:26:17.097943 140593289979072 spec.py:321] Evaluating on the training split.
I0305 19:27:27.471313 140593289979072 spec.py:333] Evaluating on the validation split.
I0305 19:27:29.367628 140593289979072 spec.py:349] Evaluating on the test split.
I0305 19:27:31.231250 140593289979072 submission_runner.py:469] Time since start: 847.36s, 	Step: 2330, 	{'train/accuracy': 0.9877095818519592, 'train/loss': 0.044246431440114975, 'train/mean_average_precision': 0.14285725297083818, 'validation/accuracy': 0.9850065112113953, 'validation/loss': 0.05392184108495712, 'validation/mean_average_precision': 0.12916142013448384, 'validation/num_examples': 43793, 'test/accuracy': 0.9840050935745239, 'test/loss': 0.057139940559864044, 'test/mean_average_precision': 0.12935430305078038, 'test/num_examples': 43793, 'score': 495.7302179336548, 'total_duration': 847.3602809906006, 'accumulated_submission_time': 495.7302179336548, 'accumulated_eval_time': 351.5325057506561, 'accumulated_logging_time': 0.03456521034240723}
I0305 19:27:31.239604 140451839440640 logging_writer.py:48] [2330] accumulated_eval_time=351.533, accumulated_logging_time=0.0345652, accumulated_submission_time=495.73, global_step=2330, preemption_count=0, score=495.73, test/accuracy=0.984005, test/loss=0.0571399, test/mean_average_precision=0.129354, test/num_examples=43793, total_duration=847.36, train/accuracy=0.98771, train/loss=0.0442464, train/mean_average_precision=0.142857, validation/accuracy=0.985007, validation/loss=0.0539218, validation/mean_average_precision=0.129161, validation/num_examples=43793
I0305 19:27:45.819828 140451831047936 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.013827738352119923, loss=0.05019892007112503
I0305 19:28:06.216749 140451839440640 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.014182576909661293, loss=0.04047618433833122
I0305 19:28:26.668870 140451831047936 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.00940921250730753, loss=0.040552034974098206
I0305 19:28:47.124920 140451839440640 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.01086623128503561, loss=0.0385797880589962
I0305 19:29:07.856005 140451831047936 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.009674391709268093, loss=0.04330261051654816
I0305 19:29:28.503999 140451839440640 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.028105974197387695, loss=0.04494348168373108
I0305 19:29:49.081692 140451831047936 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.01759815774857998, loss=0.04584721103310585
I0305 19:30:09.842804 140451839440640 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.016804611310362816, loss=0.04494142904877663
I0305 19:30:30.357357 140451831047936 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.02091018483042717, loss=0.04341507330536842
I0305 19:30:50.926028 140451839440640 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.013489426113665104, loss=0.0423772893846035
I0305 19:31:11.460505 140451831047936 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.01883355900645256, loss=0.042740415781736374
I0305 19:31:31.349067 140593289979072 spec.py:321] Evaluating on the training split.
I0305 19:32:42.768232 140593289979072 spec.py:333] Evaluating on the validation split.
I0305 19:32:44.693073 140593289979072 spec.py:349] Evaluating on the test split.
I0305 19:32:46.586627 140593289979072 submission_runner.py:469] Time since start: 1162.72s, 	Step: 3498, 	{'train/accuracy': 0.9881824254989624, 'train/loss': 0.04154165834188461, 'train/mean_average_precision': 0.1919950392319109, 'validation/accuracy': 0.9851539134979248, 'validation/loss': 0.050627902150154114, 'validation/mean_average_precision': 0.16761772596410998, 'validation/num_examples': 43793, 'test/accuracy': 0.9843028783798218, 'test/loss': 0.05313577502965927, 'test/mean_average_precision': 0.16660779875964182, 'test/num_examples': 43793, 'score': 735.7991650104523, 'total_duration': 1162.7156586647034, 'accumulated_submission_time': 735.7991650104523, 'accumulated_eval_time': 426.7700593471527, 'accumulated_logging_time': 0.05179929733276367}
I0305 19:32:46.595305 140451839440640 logging_writer.py:48] [3498] accumulated_eval_time=426.77, accumulated_logging_time=0.0517993, accumulated_submission_time=735.799, global_step=3498, preemption_count=0, score=735.799, test/accuracy=0.984303, test/loss=0.0531358, test/mean_average_precision=0.166608, test/num_examples=43793, total_duration=1162.72, train/accuracy=0.988182, train/loss=0.0415417, train/mean_average_precision=0.191995, validation/accuracy=0.985154, validation/loss=0.0506279, validation/mean_average_precision=0.167618, validation/num_examples=43793
I0305 19:32:47.233749 140451831047936 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.023670712485909462, loss=0.0414256826043129
I0305 19:33:07.831105 140451839440640 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.011770467273890972, loss=0.04321124404668808
I0305 19:33:28.308150 140451831047936 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.012830971740186214, loss=0.04175794869661331
I0305 19:33:48.701728 140451839440640 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.014446347951889038, loss=0.04358800873160362
I0305 19:34:09.037552 140451831047936 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.018121743574738503, loss=0.04188161343336105
I0305 19:34:29.399540 140451839440640 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.009769131429493427, loss=0.040322624146938324
I0305 19:34:49.655163 140451831047936 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.013617158867418766, loss=0.040874939411878586
I0305 19:35:09.932307 140451839440640 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.016993839293718338, loss=0.04031766206026077
I0305 19:35:30.400403 140451831047936 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.010643614456057549, loss=0.03777842968702316
I0305 19:35:50.548596 140451839440640 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.013425231911242008, loss=0.040665846318006516
I0305 19:36:10.633825 140451831047936 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.016182033345103264, loss=0.04110174626111984
I0305 19:36:30.458351 140451839440640 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.014442773535847664, loss=0.03781064599752426
I0305 19:36:46.589792 140593289979072 spec.py:321] Evaluating on the training split.
I0305 19:37:58.027767 140593289979072 spec.py:333] Evaluating on the validation split.
I0305 19:37:59.935776 140593289979072 spec.py:349] Evaluating on the test split.
I0305 19:38:01.766437 140593289979072 submission_runner.py:469] Time since start: 1477.90s, 	Step: 4681, 	{'train/accuracy': 0.9890322685241699, 'train/loss': 0.03764324262738228, 'train/mean_average_precision': 0.22697431765050724, 'validation/accuracy': 0.9858667254447937, 'validation/loss': 0.04774397611618042, 'validation/mean_average_precision': 0.19701497697683587, 'validation/num_examples': 43793, 'test/accuracy': 0.9849830865859985, 'test/loss': 0.05034923180937767, 'test/mean_average_precision': 0.20462535439916257, 'test/num_examples': 43793, 'score': 975.7545030117035, 'total_duration': 1477.895470380783, 'accumulated_submission_time': 975.7545030117035, 'accumulated_eval_time': 501.9466595649719, 'accumulated_logging_time': 0.07002854347229004}
I0305 19:38:01.775719 140451831047936 logging_writer.py:48] [4681] accumulated_eval_time=501.947, accumulated_logging_time=0.0700285, accumulated_submission_time=975.755, global_step=4681, preemption_count=0, score=975.755, test/accuracy=0.984983, test/loss=0.0503492, test/mean_average_precision=0.204625, test/num_examples=43793, total_duration=1477.9, train/accuracy=0.989032, train/loss=0.0376432, train/mean_average_precision=0.226974, validation/accuracy=0.985867, validation/loss=0.047744, validation/mean_average_precision=0.197015, validation/num_examples=43793
I0305 19:38:05.861028 140451839440640 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.013688796199858189, loss=0.039088983088731766
I0305 19:38:26.200105 140451831047936 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.011473437771201134, loss=0.039196766912937164
I0305 19:38:46.321553 140451839440640 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.01258957851678133, loss=0.04124367982149124
I0305 19:39:06.531758 140451831047936 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.016017822548747063, loss=0.04028421267867088
I0305 19:39:26.816609 140451839440640 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.017460936680436134, loss=0.046120863407850266
I0305 19:39:47.006552 140451831047936 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.011948302388191223, loss=0.03741714358329773
I0305 19:40:07.280966 140451839440640 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.01767841726541519, loss=0.03843502700328827
I0305 19:40:27.306066 140451831047936 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.012864154763519764, loss=0.0376645028591156
I0305 19:40:47.322044 140451839440640 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.011203416623175144, loss=0.036381736397743225
I0305 19:41:07.691501 140451831047936 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.014101904816925526, loss=0.038879893720149994
I0305 19:41:27.819338 140451839440640 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.017861826345324516, loss=0.04150141030550003
I0305 19:41:47.989892 140451831047936 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.0140844052657485, loss=0.03509679436683655
I0305 19:42:01.953414 140593289979072 spec.py:321] Evaluating on the training split.
I0305 19:43:12.493669 140593289979072 spec.py:333] Evaluating on the validation split.
I0305 19:43:14.400792 140593289979072 spec.py:349] Evaluating on the test split.
I0305 19:43:16.268276 140593289979072 submission_runner.py:469] Time since start: 1792.40s, 	Step: 5870, 	{'train/accuracy': 0.9889388680458069, 'train/loss': 0.03738610818982124, 'train/mean_average_precision': 0.2646019166690799, 'validation/accuracy': 0.9860327839851379, 'validation/loss': 0.04713626205921173, 'validation/mean_average_precision': 0.21782264567112658, 'validation/num_examples': 43793, 'test/accuracy': 0.9851166009902954, 'test/loss': 0.04990771785378456, 'test/mean_average_precision': 0.2109028761753901, 'test/num_examples': 43793, 'score': 1215.8908314704895, 'total_duration': 1792.3973093032837, 'accumulated_submission_time': 1215.8908314704895, 'accumulated_eval_time': 576.261474609375, 'accumulated_logging_time': 0.08832907676696777}
I0305 19:43:16.277427 140451839440640 logging_writer.py:48] [5870] accumulated_eval_time=576.261, accumulated_logging_time=0.0883291, accumulated_submission_time=1215.89, global_step=5870, preemption_count=0, score=1215.89, test/accuracy=0.985117, test/loss=0.0499077, test/mean_average_precision=0.210903, test/num_examples=43793, total_duration=1792.4, train/accuracy=0.988939, train/loss=0.0373861, train/mean_average_precision=0.264602, validation/accuracy=0.986033, validation/loss=0.0471363, validation/mean_average_precision=0.217823, validation/num_examples=43793
I0305 19:43:22.536673 140451831047936 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.015980834141373634, loss=0.03748288378119469
I0305 19:43:42.869944 140451839440640 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.012884978204965591, loss=0.03918490558862686
I0305 19:44:03.214288 140451831047936 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.011570236645638943, loss=0.038479410111904144
I0305 19:44:23.607784 140451839440640 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.01875496096909046, loss=0.034559208899736404
I0305 19:44:43.751319 140451831047936 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.014242446981370449, loss=0.037476230412721634
I0305 19:45:03.962378 140451839440640 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.01873915269970894, loss=0.03783711791038513
I0305 19:45:24.297373 140451831047936 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.019097698852419853, loss=0.03303557634353638
I0305 19:45:44.523425 140451839440640 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.013225480914115906, loss=0.0347687229514122
I0305 19:46:05.011448 140451831047936 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.014648059383034706, loss=0.035251516848802567
I0305 19:46:25.430065 140451839440640 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.013464303687214851, loss=0.03477105498313904
I0305 19:46:45.584310 140451831047936 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.015947634354233742, loss=0.036711178719997406
I0305 19:47:05.949496 140451839440640 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.017031552270054817, loss=0.039502404630184174
I0305 19:47:16.405601 140593289979072 spec.py:321] Evaluating on the training split.
I0305 19:48:26.373708 140593289979072 spec.py:333] Evaluating on the validation split.
I0305 19:48:28.270670 140593289979072 spec.py:349] Evaluating on the test split.
I0305 19:48:30.139589 140593289979072 submission_runner.py:469] Time since start: 2106.27s, 	Step: 7053, 	{'train/accuracy': 0.9894576072692871, 'train/loss': 0.035316210240125656, 'train/mean_average_precision': 0.2938534300615726, 'validation/accuracy': 0.9862694144248962, 'validation/loss': 0.046089302748441696, 'validation/mean_average_precision': 0.22478334648302578, 'validation/num_examples': 43793, 'test/accuracy': 0.9854190349578857, 'test/loss': 0.048637717962265015, 'test/mean_average_precision': 0.22192604560566712, 'test/num_examples': 43793, 'score': 1455.976026058197, 'total_duration': 2106.268545627594, 'accumulated_submission_time': 1455.976026058197, 'accumulated_eval_time': 649.9953377246857, 'accumulated_logging_time': 0.10635232925415039}
I0305 19:48:30.150305 140451831047936 logging_writer.py:48] [7053] accumulated_eval_time=649.995, accumulated_logging_time=0.106352, accumulated_submission_time=1455.98, global_step=7053, preemption_count=0, score=1455.98, test/accuracy=0.985419, test/loss=0.0486377, test/mean_average_precision=0.221926, test/num_examples=43793, total_duration=2106.27, train/accuracy=0.989458, train/loss=0.0353162, train/mean_average_precision=0.293853, validation/accuracy=0.986269, validation/loss=0.0460893, validation/mean_average_precision=0.224783, validation/num_examples=43793
I0305 19:48:39.879943 140451839440640 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.016940046101808548, loss=0.03820043429732323
I0305 19:49:00.109438 140451831047936 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.01866912841796875, loss=0.03559159114956856
I0305 19:49:20.534061 140451839440640 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.018642868846654892, loss=0.041295215487480164
I0305 19:49:40.932960 140451831047936 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.01864657923579216, loss=0.038041334599256516
I0305 19:50:01.235366 140451839440640 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.01988990791141987, loss=0.03653978556394577
I0305 19:50:21.686013 140451831047936 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.02366427518427372, loss=0.03588099032640457
I0305 19:50:41.799467 140451839440640 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.017394546419382095, loss=0.036803994327783585
I0305 19:51:02.089982 140451831047936 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.024250108748674393, loss=0.040781568735837936
I0305 19:51:22.369815 140451839440640 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.020024839788675308, loss=0.03595305234193802
I0305 19:51:42.656139 140451831047936 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.018215559422969818, loss=0.03540188819169998
I0305 19:52:03.102935 140451839440640 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.017789147794246674, loss=0.037111058831214905
I0305 19:52:23.424350 140451831047936 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.020248664543032646, loss=0.042304810136556625
I0305 19:52:30.319257 140593289979072 spec.py:321] Evaluating on the training split.
I0305 19:53:41.798279 140593289979072 spec.py:333] Evaluating on the validation split.
I0305 19:53:43.749485 140593289979072 spec.py:349] Evaluating on the test split.
I0305 19:53:45.629848 140593289979072 submission_runner.py:469] Time since start: 2421.76s, 	Step: 8235, 	{'train/accuracy': 0.9898175001144409, 'train/loss': 0.03407149761915207, 'train/mean_average_precision': 0.3153348933859192, 'validation/accuracy': 0.9865024089813232, 'validation/loss': 0.0450764074921608, 'validation/mean_average_precision': 0.23980969811177497, 'validation/num_examples': 43793, 'test/accuracy': 0.9856077432632446, 'test/loss': 0.04782549664378166, 'test/mean_average_precision': 0.2345121696777673, 'test/num_examples': 43793, 'score': 1696.105364561081, 'total_duration': 2421.758775949478, 'accumulated_submission_time': 1696.105364561081, 'accumulated_eval_time': 725.3057918548584, 'accumulated_logging_time': 0.12739133834838867}
I0305 19:53:45.638981 140451839440640 logging_writer.py:48] [8235] accumulated_eval_time=725.306, accumulated_logging_time=0.127391, accumulated_submission_time=1696.11, global_step=8235, preemption_count=0, score=1696.11, test/accuracy=0.985608, test/loss=0.0478255, test/mean_average_precision=0.234512, test/num_examples=43793, total_duration=2421.76, train/accuracy=0.989818, train/loss=0.0340715, train/mean_average_precision=0.315335, validation/accuracy=0.986502, validation/loss=0.0450764, validation/mean_average_precision=0.23981, validation/num_examples=43793
I0305 19:53:59.096171 140451831047936 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.018332744017243385, loss=0.035081539303064346
I0305 19:54:19.456777 140451839440640 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.018214918673038483, loss=0.031593360006809235
I0305 19:54:39.726436 140451831047936 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.016318317502737045, loss=0.03325807675719261
I0305 19:54:59.987667 140451839440640 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.018117088824510574, loss=0.03182335942983627
I0305 19:55:20.391049 140451831047936 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.016711484640836716, loss=0.033048421144485474
I0305 19:55:40.798335 140451839440640 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.018725546076893806, loss=0.032221440225839615
I0305 19:56:01.182745 140451831047936 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.02553124725818634, loss=0.03865329921245575
I0305 19:56:21.343751 140451839440640 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.01475570723414421, loss=0.031407322734594345
I0305 19:56:41.550719 140451831047936 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.02052539773285389, loss=0.033433787524700165
I0305 19:57:01.874142 140451839440640 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.021122878417372704, loss=0.03751249611377716
I0305 19:57:22.394286 140451831047936 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.02431502752006054, loss=0.03951758146286011
I0305 19:57:42.784266 140451839440640 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.017431937158107758, loss=0.035141244530677795
I0305 19:57:45.829504 140593289979072 spec.py:321] Evaluating on the training split.
I0305 19:58:57.423363 140593289979072 spec.py:333] Evaluating on the validation split.
I0305 19:58:59.328471 140593289979072 spec.py:349] Evaluating on the test split.
I0305 19:59:01.161319 140593289979072 submission_runner.py:469] Time since start: 2737.29s, 	Step: 9416, 	{'train/accuracy': 0.9902396202087402, 'train/loss': 0.032740622758865356, 'train/mean_average_precision': 0.36087893163104556, 'validation/accuracy': 0.9863449335098267, 'validation/loss': 0.04543543979525566, 'validation/mean_average_precision': 0.24490224177942718, 'validation/num_examples': 43793, 'test/accuracy': 0.9855172038078308, 'test/loss': 0.048117585480213165, 'test/mean_average_precision': 0.23549960773149287, 'test/num_examples': 43793, 'score': 1936.2549843788147, 'total_duration': 2737.2902657985687, 'accumulated_submission_time': 1936.2549843788147, 'accumulated_eval_time': 800.6374735832214, 'accumulated_logging_time': 0.14531636238098145}
I0305 19:59:01.170415 140451831047936 logging_writer.py:48] [9416] accumulated_eval_time=800.637, accumulated_logging_time=0.145316, accumulated_submission_time=1936.25, global_step=9416, preemption_count=0, score=1936.25, test/accuracy=0.985517, test/loss=0.0481176, test/mean_average_precision=0.2355, test/num_examples=43793, total_duration=2737.29, train/accuracy=0.99024, train/loss=0.0327406, train/mean_average_precision=0.360879, validation/accuracy=0.986345, validation/loss=0.0454354, validation/mean_average_precision=0.244902, validation/num_examples=43793
I0305 19:59:18.491328 140451839440640 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.021045787259936333, loss=0.035373520106077194
I0305 19:59:38.899184 140451831047936 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.019520150497555733, loss=0.03230893239378929
I0305 19:59:59.431866 140451839440640 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.021266061812639236, loss=0.03998405113816261
I0305 20:00:19.879943 140451831047936 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.01809810660779476, loss=0.03474073112010956
I0305 20:00:40.191941 140451839440640 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.03235447034239769, loss=0.03951152414083481
I0305 20:01:00.528449 140451831047936 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.020807072520256042, loss=0.03181848302483559
I0305 20:01:21.008008 140451839440640 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.026977306231856346, loss=0.0351489819586277
I0305 20:01:41.450283 140451831047936 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.026883762329816818, loss=0.03555235639214516
I0305 20:02:01.905759 140451839440640 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.020164305344223976, loss=0.036622054874897
I0305 20:02:22.529817 140451831047936 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.023806428536772728, loss=0.036726415157318115
I0305 20:02:43.088508 140451839440640 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.020714176818728447, loss=0.03262623772025108
I0305 20:03:01.291136 140593289979072 spec.py:321] Evaluating on the training split.
I0305 20:04:11.550814 140593289979072 spec.py:333] Evaluating on the validation split.
I0305 20:04:13.522116 140593289979072 spec.py:349] Evaluating on the test split.
I0305 20:04:15.389529 140593289979072 submission_runner.py:469] Time since start: 3051.52s, 	Step: 10589, 	{'train/accuracy': 0.9903358221054077, 'train/loss': 0.032137755304574966, 'train/mean_average_precision': 0.3571163184543772, 'validation/accuracy': 0.9867212176322937, 'validation/loss': 0.04450750723481178, 'validation/mean_average_precision': 0.2576649284528416, 'validation/num_examples': 43793, 'test/accuracy': 0.9858031868934631, 'test/loss': 0.047379590570926666, 'test/mean_average_precision': 0.24224174415973707, 'test/num_examples': 43793, 'score': 2176.336788892746, 'total_duration': 3051.5184395313263, 'accumulated_submission_time': 2176.336788892746, 'accumulated_eval_time': 874.7356979846954, 'accumulated_logging_time': 0.1644752025604248}
I0305 20:04:15.400206 140451831047936 logging_writer.py:48] [10589] accumulated_eval_time=874.736, accumulated_logging_time=0.164475, accumulated_submission_time=2176.34, global_step=10589, preemption_count=0, score=2176.34, test/accuracy=0.985803, test/loss=0.0473796, test/mean_average_precision=0.242242, test/num_examples=43793, total_duration=3051.52, train/accuracy=0.990336, train/loss=0.0321378, train/mean_average_precision=0.357116, validation/accuracy=0.986721, validation/loss=0.0445075, validation/mean_average_precision=0.257665, validation/num_examples=43793
I0305 20:04:17.894873 140451839440640 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.024102652445435524, loss=0.036079730838537216
I0305 20:04:38.441524 140451831047936 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.030661195516586304, loss=0.03787244111299515
I0305 20:04:58.792174 140451839440640 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.025407176464796066, loss=0.03147832304239273
I0305 20:05:19.392112 140451831047936 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.025645283982157707, loss=0.03497881442308426
I0305 20:05:40.144461 140451839440640 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.03220413625240326, loss=0.03400358185172081
I0305 20:06:00.852212 140451831047936 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.027879498898983, loss=0.028407754376530647
I0305 20:06:21.599876 140451839440640 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.030671242624521255, loss=0.032653845846652985
I0305 20:06:42.323036 140451831047936 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.026106469333171844, loss=0.032924361526966095
I0305 20:07:02.985419 140451839440640 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.028689060360193253, loss=0.03440719097852707
I0305 20:07:23.537170 140451831047936 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.021979155018925667, loss=0.02988380752503872
I0305 20:07:44.037745 140451839440640 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.023840682581067085, loss=0.03419921174645424
I0305 20:08:04.928495 140451831047936 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.025491220876574516, loss=0.032150473445653915
I0305 20:08:15.534891 140593289979072 spec.py:321] Evaluating on the training split.
I0305 20:09:25.670047 140593289979072 spec.py:333] Evaluating on the validation split.
I0305 20:09:27.609766 140593289979072 spec.py:349] Evaluating on the test split.
I0305 20:09:29.531485 140593289979072 submission_runner.py:469] Time since start: 3365.66s, 	Step: 11752, 	{'train/accuracy': 0.9907875657081604, 'train/loss': 0.030610665678977966, 'train/mean_average_precision': 0.3947379646440441, 'validation/accuracy': 0.986735463142395, 'validation/loss': 0.044351790100336075, 'validation/mean_average_precision': 0.255633316955732, 'validation/num_examples': 43793, 'test/accuracy': 0.9858351945877075, 'test/loss': 0.0471184141933918, 'test/mean_average_precision': 0.25451144156030264, 'test/num_examples': 43793, 'score': 2416.428697347641, 'total_duration': 3365.6604993343353, 'accumulated_submission_time': 2416.428697347641, 'accumulated_eval_time': 948.7322244644165, 'accumulated_logging_time': 0.18390536308288574}
I0305 20:09:29.542017 140451515905792 logging_writer.py:48] [11752] accumulated_eval_time=948.732, accumulated_logging_time=0.183905, accumulated_submission_time=2416.43, global_step=11752, preemption_count=0, score=2416.43, test/accuracy=0.985835, test/loss=0.0471184, test/mean_average_precision=0.254511, test/num_examples=43793, total_duration=3365.66, train/accuracy=0.990788, train/loss=0.0306107, train/mean_average_precision=0.394738, validation/accuracy=0.986735, validation/loss=0.0443518, validation/mean_average_precision=0.255633, validation/num_examples=43793
I0305 20:09:39.561078 140451507513088 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.03220542147755623, loss=0.03604862093925476
I0305 20:09:59.861429 140451515905792 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.037616465240716934, loss=0.03378311172127724
I0305 20:10:20.314093 140451507513088 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.03549501672387123, loss=0.037510864436626434
I0305 20:10:40.794975 140451515905792 logging_writer.py:48] [12100] global_step=12100, grad_norm=0.020947344601154327, loss=0.030009526759386063
I0305 20:11:01.267174 140451507513088 logging_writer.py:48] [12200] global_step=12200, grad_norm=0.026649944484233856, loss=0.03061722032725811
I0305 20:11:21.696917 140451515905792 logging_writer.py:48] [12300] global_step=12300, grad_norm=0.028148498386144638, loss=0.03226149082183838
I0305 20:11:42.112102 140451507513088 logging_writer.py:48] [12400] global_step=12400, grad_norm=0.028455613180994987, loss=0.034273162484169006
I0305 20:12:02.484438 140451515905792 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.023493386805057526, loss=0.030364494770765305
I0305 20:12:22.998459 140451507513088 logging_writer.py:48] [12600] global_step=12600, grad_norm=0.02938428521156311, loss=0.035676728934049606
I0305 20:12:43.428792 140451515905792 logging_writer.py:48] [12700] global_step=12700, grad_norm=0.02802767977118492, loss=0.030427217483520508
I0305 20:13:03.811140 140451507513088 logging_writer.py:48] [12800] global_step=12800, grad_norm=0.028711382299661636, loss=0.03329780325293541
I0305 20:13:24.367601 140451515905792 logging_writer.py:48] [12900] global_step=12900, grad_norm=0.027120474725961685, loss=0.032922763377428055
I0305 20:13:29.676418 140593289979072 spec.py:321] Evaluating on the training split.
I0305 20:14:41.249559 140593289979072 spec.py:333] Evaluating on the validation split.
I0305 20:14:43.179563 140593289979072 spec.py:349] Evaluating on the test split.
I0305 20:14:45.059935 140593289979072 submission_runner.py:469] Time since start: 3681.19s, 	Step: 12927, 	{'train/accuracy': 0.9904239773750305, 'train/loss': 0.03133352845907211, 'train/mean_average_precision': 0.38690767302308204, 'validation/accuracy': 0.98676997423172, 'validation/loss': 0.04438430443406105, 'validation/mean_average_precision': 0.27292013179502816, 'validation/num_examples': 43793, 'test/accuracy': 0.9859042763710022, 'test/loss': 0.04745090380311012, 'test/mean_average_precision': 0.25480685969641675, 'test/num_examples': 43793, 'score': 2656.5188870429993, 'total_duration': 3681.188933610916, 'accumulated_submission_time': 2656.5188870429993, 'accumulated_eval_time': 1024.1156587600708, 'accumulated_logging_time': 0.209000825881958}
I0305 20:14:45.070546 140451507513088 logging_writer.py:48] [12927] accumulated_eval_time=1024.12, accumulated_logging_time=0.209001, accumulated_submission_time=2656.52, global_step=12927, preemption_count=0, score=2656.52, test/accuracy=0.985904, test/loss=0.0474509, test/mean_average_precision=0.254807, test/num_examples=43793, total_duration=3681.19, train/accuracy=0.990424, train/loss=0.0313335, train/mean_average_precision=0.386908, validation/accuracy=0.98677, validation/loss=0.0443843, validation/mean_average_precision=0.27292, validation/num_examples=43793
I0305 20:15:00.253908 140451515905792 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.028980156406760216, loss=0.031646355986595154
I0305 20:15:20.491430 140451507513088 logging_writer.py:48] [13100] global_step=13100, grad_norm=0.04206623509526253, loss=0.032921236008405685
I0305 20:15:40.873463 140451515905792 logging_writer.py:48] [13200] global_step=13200, grad_norm=0.039452455937862396, loss=0.031652744859457016
I0305 20:16:01.311847 140451507513088 logging_writer.py:48] [13300] global_step=13300, grad_norm=0.03330182284116745, loss=0.03218677267432213
I0305 20:16:21.843898 140451515905792 logging_writer.py:48] [13400] global_step=13400, grad_norm=0.03132546693086624, loss=0.03218160197138786
I0305 20:16:42.194740 140451507513088 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.03713083267211914, loss=0.035691771656274796
I0305 20:17:02.624509 140451515905792 logging_writer.py:48] [13600] global_step=13600, grad_norm=0.029529878869652748, loss=0.03443580120801926
I0305 20:17:22.936493 140451507513088 logging_writer.py:48] [13700] global_step=13700, grad_norm=0.029052166268229485, loss=0.030978955328464508
I0305 20:17:43.306402 140451515905792 logging_writer.py:48] [13800] global_step=13800, grad_norm=0.03892232105135918, loss=0.035361893475055695
I0305 20:18:03.723281 140451507513088 logging_writer.py:48] [13900] global_step=13900, grad_norm=0.03704593703150749, loss=0.0336550734937191
I0305 20:18:24.335205 140451515905792 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.034457139670848846, loss=0.03040044754743576
I0305 20:18:44.781342 140451507513088 logging_writer.py:48] [14100] global_step=14100, grad_norm=0.03711491823196411, loss=0.03323346748948097
I0305 20:18:45.200416 140593289979072 spec.py:321] Evaluating on the training split.
I0305 20:19:55.931859 140593289979072 spec.py:333] Evaluating on the validation split.
I0305 20:19:57.853773 140593289979072 spec.py:349] Evaluating on the test split.
I0305 20:19:59.737062 140593289979072 submission_runner.py:469] Time since start: 3995.87s, 	Step: 14103, 	{'train/accuracy': 0.9909906983375549, 'train/loss': 0.02965773642063141, 'train/mean_average_precision': 0.41687582696347114, 'validation/accuracy': 0.9866745471954346, 'validation/loss': 0.04410475492477417, 'validation/mean_average_precision': 0.26449423412637213, 'validation/num_examples': 43793, 'test/accuracy': 0.985874354839325, 'test/loss': 0.04678046703338623, 'test/mean_average_precision': 0.25480653767073136, 'test/num_examples': 43793, 'score': 2896.607532262802, 'total_duration': 3995.866068124771, 'accumulated_submission_time': 2896.607532262802, 'accumulated_eval_time': 1098.6522283554077, 'accumulated_logging_time': 0.22865080833435059}
I0305 20:19:59.748366 140451515905792 logging_writer.py:48] [14103] accumulated_eval_time=1098.65, accumulated_logging_time=0.228651, accumulated_submission_time=2896.61, global_step=14103, preemption_count=0, score=2896.61, test/accuracy=0.985874, test/loss=0.0467805, test/mean_average_precision=0.254807, test/num_examples=43793, total_duration=3995.87, train/accuracy=0.990991, train/loss=0.0296577, train/mean_average_precision=0.416876, validation/accuracy=0.986675, validation/loss=0.0441048, validation/mean_average_precision=0.264494, validation/num_examples=43793
I0305 20:20:19.792595 140451507513088 logging_writer.py:48] [14200] global_step=14200, grad_norm=0.031101753935217857, loss=0.0328608863055706
I0305 20:20:40.192140 140451515905792 logging_writer.py:48] [14300] global_step=14300, grad_norm=0.03487298637628555, loss=0.032199110835790634
I0305 20:21:00.549611 140451507513088 logging_writer.py:48] [14400] global_step=14400, grad_norm=0.03383146971464157, loss=0.03271724656224251
I0305 20:21:20.849286 140451515905792 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.037704210728406906, loss=0.03011631965637207
I0305 20:21:41.283867 140451507513088 logging_writer.py:48] [14600] global_step=14600, grad_norm=0.03850673511624336, loss=0.030697492882609367
I0305 20:22:01.881677 140451515905792 logging_writer.py:48] [14700] global_step=14700, grad_norm=0.03950761631131172, loss=0.034307364374399185
I0305 20:22:22.384772 140451507513088 logging_writer.py:48] [14800] global_step=14800, grad_norm=0.04165256768465042, loss=0.03482695296406746
I0305 20:22:42.702655 140451515905792 logging_writer.py:48] [14900] global_step=14900, grad_norm=0.03779575228691101, loss=0.031109031289815903
I0305 20:23:03.289164 140451507513088 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.03540293872356415, loss=0.029173022136092186
I0305 20:23:23.745187 140451515905792 logging_writer.py:48] [15100] global_step=15100, grad_norm=0.06658221036195755, loss=0.041294172406196594
I0305 20:23:44.240911 140451507513088 logging_writer.py:48] [15200] global_step=15200, grad_norm=0.039453938603401184, loss=0.03411703184247017
I0305 20:23:59.810131 140593289979072 spec.py:321] Evaluating on the training split.
I0305 20:25:10.735605 140593289979072 spec.py:333] Evaluating on the validation split.
I0305 20:25:12.661193 140593289979072 spec.py:349] Evaluating on the test split.
I0305 20:25:14.507231 140593289979072 submission_runner.py:469] Time since start: 4310.64s, 	Step: 15277, 	{'train/accuracy': 0.990873396396637, 'train/loss': 0.030069738626480103, 'train/mean_average_precision': 0.41282036928607224, 'validation/accuracy': 0.9868921637535095, 'validation/loss': 0.04422160983085632, 'validation/mean_average_precision': 0.2792304718559141, 'validation/num_examples': 43793, 'test/accuracy': 0.9860057830810547, 'test/loss': 0.047049377113580704, 'test/mean_average_precision': 0.2636919937000252, 'test/num_examples': 43793, 'score': 3136.6291110515594, 'total_duration': 4310.636175870895, 'accumulated_submission_time': 3136.6291110515594, 'accumulated_eval_time': 1173.3491911888123, 'accumulated_logging_time': 0.2494490146636963}
I0305 20:25:14.517116 140451515905792 logging_writer.py:48] [15277] accumulated_eval_time=1173.35, accumulated_logging_time=0.249449, accumulated_submission_time=3136.63, global_step=15277, preemption_count=0, score=3136.63, test/accuracy=0.986006, test/loss=0.0470494, test/mean_average_precision=0.263692, test/num_examples=43793, total_duration=4310.64, train/accuracy=0.990873, train/loss=0.0300697, train/mean_average_precision=0.41282, validation/accuracy=0.986892, validation/loss=0.0442216, validation/mean_average_precision=0.27923, validation/num_examples=43793
I0305 20:25:19.342363 140451507513088 logging_writer.py:48] [15300] global_step=15300, grad_norm=0.03919385373592377, loss=0.03274136781692505
I0305 20:25:39.550838 140451515905792 logging_writer.py:48] [15400] global_step=15400, grad_norm=0.031859058886766434, loss=0.031000232324004173
I0305 20:26:00.056666 140451507513088 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.03814578056335449, loss=0.0325172133743763
I0305 20:26:20.617304 140451515905792 logging_writer.py:48] [15600] global_step=15600, grad_norm=0.04171043261885643, loss=0.03253775089979172
I0305 20:26:40.968946 140451507513088 logging_writer.py:48] [15700] global_step=15700, grad_norm=0.041003938764333725, loss=0.031262077391147614
I0305 20:27:01.496925 140451515905792 logging_writer.py:48] [15800] global_step=15800, grad_norm=0.03625430911779404, loss=0.030105089768767357
I0305 20:27:22.270533 140451507513088 logging_writer.py:48] [15900] global_step=15900, grad_norm=0.034720923751592636, loss=0.02870846539735794
I0305 20:27:42.862442 140451515905792 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.03721406310796738, loss=0.029298292472958565
I0305 20:28:03.436755 140451507513088 logging_writer.py:48] [16100] global_step=16100, grad_norm=0.04326740279793739, loss=0.03189561516046524
I0305 20:28:23.916460 140451515905792 logging_writer.py:48] [16200] global_step=16200, grad_norm=0.047955919057130814, loss=0.0318450890481472
I0305 20:28:44.407553 140451507513088 logging_writer.py:48] [16300] global_step=16300, grad_norm=0.051877573132514954, loss=0.03354370594024658
I0305 20:29:05.161769 140451515905792 logging_writer.py:48] [16400] global_step=16400, grad_norm=0.03560194745659828, loss=0.028412602841854095
I0305 20:29:14.508924 140593289979072 spec.py:321] Evaluating on the training split.
I0305 20:30:22.613276 140593289979072 spec.py:333] Evaluating on the validation split.
I0305 20:30:24.860620 140593289979072 spec.py:349] Evaluating on the test split.
I0305 20:30:26.860805 140593289979072 submission_runner.py:469] Time since start: 4622.99s, 	Step: 16446, 	{'train/accuracy': 0.9912376999855042, 'train/loss': 0.028628481552004814, 'train/mean_average_precision': 0.4425579729862624, 'validation/accuracy': 0.9867433309555054, 'validation/loss': 0.044156014919281006, 'validation/mean_average_precision': 0.2716093800371838, 'validation/num_examples': 43793, 'test/accuracy': 0.9859076142311096, 'test/loss': 0.046792931854724884, 'test/mean_average_precision': 0.26365983135477, 'test/num_examples': 43793, 'score': 3376.5800795555115, 'total_duration': 4622.9897429943085, 'accumulated_submission_time': 3376.5800795555115, 'accumulated_eval_time': 1245.7009282112122, 'accumulated_logging_time': 0.2685251235961914}
I0305 20:30:26.873566 140451507513088 logging_writer.py:48] [16446] accumulated_eval_time=1245.7, accumulated_logging_time=0.268525, accumulated_submission_time=3376.58, global_step=16446, preemption_count=0, score=3376.58, test/accuracy=0.985908, test/loss=0.0467929, test/mean_average_precision=0.26366, test/num_examples=43793, total_duration=4622.99, train/accuracy=0.991238, train/loss=0.0286285, train/mean_average_precision=0.442558, validation/accuracy=0.986743, validation/loss=0.044156, validation/mean_average_precision=0.271609, validation/num_examples=43793
I0305 20:30:38.212782 140451515905792 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.04288060590624809, loss=0.03107244335114956
I0305 20:30:58.832309 140451507513088 logging_writer.py:48] [16600] global_step=16600, grad_norm=0.04325047880411148, loss=0.033898767083883286
I0305 20:31:19.449433 140451515905792 logging_writer.py:48] [16700] global_step=16700, grad_norm=0.046313509345054626, loss=0.028778066858649254
I0305 20:31:39.699374 140451507513088 logging_writer.py:48] [16800] global_step=16800, grad_norm=0.04178602993488312, loss=0.032434627413749695
I0305 20:32:00.342918 140451515905792 logging_writer.py:48] [16900] global_step=16900, grad_norm=0.03570844605565071, loss=0.02893863618373871
I0305 20:32:21.154551 140451507513088 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.03212983161211014, loss=0.02717568725347519
I0305 20:32:41.840668 140451515905792 logging_writer.py:48] [17100] global_step=17100, grad_norm=0.04146363586187363, loss=0.030941441655158997
I0305 20:33:02.427469 140451507513088 logging_writer.py:48] [17200] global_step=17200, grad_norm=0.04762788861989975, loss=0.03334580361843109
I0305 20:33:23.298219 140451515905792 logging_writer.py:48] [17300] global_step=17300, grad_norm=0.05347330868244171, loss=0.03379169479012489
I0305 20:33:43.875658 140451507513088 logging_writer.py:48] [17400] global_step=17400, grad_norm=0.039722003042697906, loss=0.031004132702946663
I0305 20:34:04.703083 140451515905792 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.04260607808828354, loss=0.026172077283263206
I0305 20:34:25.248393 140451507513088 logging_writer.py:48] [17600] global_step=17600, grad_norm=0.0535835437476635, loss=0.03317190706729889
I0305 20:34:26.887144 140593289979072 spec.py:321] Evaluating on the training split.
I0305 20:35:38.222700 140593289979072 spec.py:333] Evaluating on the validation split.
I0305 20:35:40.284813 140593289979072 spec.py:349] Evaluating on the test split.
I0305 20:35:42.185709 140593289979072 submission_runner.py:469] Time since start: 4938.31s, 	Step: 17609, 	{'train/accuracy': 0.9910468459129333, 'train/loss': 0.029164809733629227, 'train/mean_average_precision': 0.4367664442886274, 'validation/accuracy': 0.98692786693573, 'validation/loss': 0.043970558792352676, 'validation/mean_average_precision': 0.2774141644111889, 'validation/num_examples': 43793, 'test/accuracy': 0.9860685467720032, 'test/loss': 0.046715620905160904, 'test/mean_average_precision': 0.2642154545222455, 'test/num_examples': 43793, 'score': 3616.5529549121857, 'total_duration': 4938.314654350281, 'accumulated_submission_time': 3616.5529549121857, 'accumulated_eval_time': 1320.999355316162, 'accumulated_logging_time': 0.2908921241760254}
I0305 20:35:42.196705 140451025659648 logging_writer.py:48] [17609] accumulated_eval_time=1321, accumulated_logging_time=0.290892, accumulated_submission_time=3616.55, global_step=17609, preemption_count=0, score=3616.55, test/accuracy=0.986069, test/loss=0.0467156, test/mean_average_precision=0.264215, test/num_examples=43793, total_duration=4938.31, train/accuracy=0.991047, train/loss=0.0291648, train/mean_average_precision=0.436766, validation/accuracy=0.986928, validation/loss=0.0439706, validation/mean_average_precision=0.277414, validation/num_examples=43793
I0305 20:36:01.234607 140451017266944 logging_writer.py:48] [17700] global_step=17700, grad_norm=0.0397740937769413, loss=0.03101707436144352
I0305 20:36:21.798441 140451025659648 logging_writer.py:48] [17800] global_step=17800, grad_norm=0.03816557675600052, loss=0.029915181919932365
I0305 20:36:42.437138 140451017266944 logging_writer.py:48] [17900] global_step=17900, grad_norm=0.040170107036828995, loss=0.02916649729013443
I0305 20:37:02.974404 140451025659648 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.04745729640126228, loss=0.031570326536893845
I0305 20:37:23.490822 140451017266944 logging_writer.py:48] [18100] global_step=18100, grad_norm=0.05111360549926758, loss=0.030490277335047722
I0305 20:37:44.089859 140451025659648 logging_writer.py:48] [18200] global_step=18200, grad_norm=0.04817865416407585, loss=0.02866118960082531
I0305 20:38:04.757543 140451017266944 logging_writer.py:48] [18300] global_step=18300, grad_norm=0.041064534336328506, loss=0.030650248751044273
I0305 20:38:25.424792 140451025659648 logging_writer.py:48] [18400] global_step=18400, grad_norm=0.040765874087810516, loss=0.027742750942707062
I0305 20:38:46.026734 140451017266944 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.04839453846216202, loss=0.031442560255527496
I0305 20:39:06.908959 140451025659648 logging_writer.py:48] [18600] global_step=18600, grad_norm=0.04464268684387207, loss=0.030287396162748337
I0305 20:39:27.754925 140451017266944 logging_writer.py:48] [18700] global_step=18700, grad_norm=0.05873871222138405, loss=0.031632885336875916
I0305 20:39:42.316396 140593289979072 spec.py:321] Evaluating on the training split.
I0305 20:40:55.286081 140593289979072 spec.py:333] Evaluating on the validation split.
I0305 20:40:57.282237 140593289979072 spec.py:349] Evaluating on the test split.
I0305 20:40:59.186190 140593289979072 submission_runner.py:469] Time since start: 5255.32s, 	Step: 18772, 	{'train/accuracy': 0.991555392742157, 'train/loss': 0.02768556959927082, 'train/mean_average_precision': 0.4718619472609885, 'validation/accuracy': 0.9867857694625854, 'validation/loss': 0.04401611164212227, 'validation/mean_average_precision': 0.2803565531336377, 'validation/num_examples': 43793, 'test/accuracy': 0.9859662055969238, 'test/loss': 0.0468362420797348, 'test/mean_average_precision': 0.26311354145980315, 'test/num_examples': 43793, 'score': 3856.6339406967163, 'total_duration': 5255.31515455246, 'accumulated_submission_time': 3856.6339406967163, 'accumulated_eval_time': 1397.8690345287323, 'accumulated_logging_time': 0.3110542297363281}
I0305 20:40:59.198318 140451025659648 logging_writer.py:48] [18772] accumulated_eval_time=1397.87, accumulated_logging_time=0.311054, accumulated_submission_time=3856.63, global_step=18772, preemption_count=0, score=3856.63, test/accuracy=0.985966, test/loss=0.0468362, test/mean_average_precision=0.263114, test/num_examples=43793, total_duration=5255.32, train/accuracy=0.991555, train/loss=0.0276856, train/mean_average_precision=0.471862, validation/accuracy=0.986786, validation/loss=0.0440161, validation/mean_average_precision=0.280357, validation/num_examples=43793
I0305 20:41:05.212209 140451017266944 logging_writer.py:48] [18800] global_step=18800, grad_norm=0.0534433051943779, loss=0.03071807697415352
I0305 20:41:25.676727 140451025659648 logging_writer.py:48] [18900] global_step=18900, grad_norm=0.04734576866030693, loss=0.028989234939217567
I0305 20:41:45.993588 140451017266944 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.04937126860022545, loss=0.030703717842698097
I0305 20:42:06.304595 140451025659648 logging_writer.py:48] [19100] global_step=19100, grad_norm=0.04253721982240677, loss=0.03132530674338341
I0305 20:42:26.866021 140451017266944 logging_writer.py:48] [19200] global_step=19200, grad_norm=0.0539950355887413, loss=0.029596099629998207
I0305 20:42:47.417953 140451025659648 logging_writer.py:48] [19300] global_step=19300, grad_norm=0.0721745491027832, loss=0.029685528948903084
I0305 20:43:07.928815 140451017266944 logging_writer.py:48] [19400] global_step=19400, grad_norm=0.048310380429029465, loss=0.030115263536572456
I0305 20:43:28.393595 140451025659648 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.05059139057993889, loss=0.03143256530165672
I0305 20:43:48.745107 140451017266944 logging_writer.py:48] [19600] global_step=19600, grad_norm=0.04766010120511055, loss=0.032279808074235916
I0305 20:44:09.321925 140451025659648 logging_writer.py:48] [19700] global_step=19700, grad_norm=0.04474272578954697, loss=0.029712136834859848
I0305 20:44:29.875464 140451017266944 logging_writer.py:48] [19800] global_step=19800, grad_norm=0.04924144223332405, loss=0.03285091370344162
I0305 20:44:50.339148 140451025659648 logging_writer.py:48] [19900] global_step=19900, grad_norm=0.049943309277296066, loss=0.029338287189602852
I0305 20:44:59.335510 140593289979072 spec.py:321] Evaluating on the training split.
I0305 20:46:08.967295 140593289979072 spec.py:333] Evaluating on the validation split.
I0305 20:46:10.903751 140593289979072 spec.py:349] Evaluating on the test split.
I0305 20:46:12.756088 140593289979072 submission_runner.py:469] Time since start: 5568.89s, 	Step: 19945, 	{'train/accuracy': 0.9912647604942322, 'train/loss': 0.028439663350582123, 'train/mean_average_precision': 0.4436082202918662, 'validation/accuracy': 0.9869644045829773, 'validation/loss': 0.04380766302347183, 'validation/mean_average_precision': 0.28048885282160335, 'validation/num_examples': 43793, 'test/accuracy': 0.9860761165618896, 'test/loss': 0.04660138487815857, 'test/mean_average_precision': 0.2757820448619877, 'test/num_examples': 43793, 'score': 4096.7310655117035, 'total_duration': 5568.8851227760315, 'accumulated_submission_time': 4096.7310655117035, 'accumulated_eval_time': 1471.289566040039, 'accumulated_logging_time': 0.33180952072143555}
I0305 20:46:12.767310 140451017266944 logging_writer.py:48] [19945] accumulated_eval_time=1471.29, accumulated_logging_time=0.33181, accumulated_submission_time=4096.73, global_step=19945, preemption_count=0, score=4096.73, test/accuracy=0.986076, test/loss=0.0466014, test/mean_average_precision=0.275782, test/num_examples=43793, total_duration=5568.89, train/accuracy=0.991265, train/loss=0.0284397, train/mean_average_precision=0.443608, validation/accuracy=0.986964, validation/loss=0.0438077, validation/mean_average_precision=0.280489, validation/num_examples=43793
I0305 20:46:24.177815 140451025659648 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.04791190102696419, loss=0.025348439812660217
I0305 20:46:44.520812 140451017266944 logging_writer.py:48] [20100] global_step=20100, grad_norm=0.054652273654937744, loss=0.033514976501464844
I0305 20:47:04.908570 140451025659648 logging_writer.py:48] [20200] global_step=20200, grad_norm=0.05004575103521347, loss=0.02833600342273712
I0305 20:47:25.504902 140451017266944 logging_writer.py:48] [20300] global_step=20300, grad_norm=0.05580993369221687, loss=0.030490798875689507
I0305 20:47:45.982402 140451025659648 logging_writer.py:48] [20400] global_step=20400, grad_norm=0.05635850876569748, loss=0.03304312378168106
I0305 20:48:06.377814 140451017266944 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.045767225325107574, loss=0.030122719705104828
I0305 20:48:26.761094 140451025659648 logging_writer.py:48] [20600] global_step=20600, grad_norm=0.04795873910188675, loss=0.025900186970829964
I0305 20:48:47.260606 140451017266944 logging_writer.py:48] [20700] global_step=20700, grad_norm=0.0536416731774807, loss=0.029169436544179916
I0305 20:49:07.737845 140451025659648 logging_writer.py:48] [20800] global_step=20800, grad_norm=0.03961877524852753, loss=0.024773912504315376
I0305 20:49:28.132304 140451017266944 logging_writer.py:48] [20900] global_step=20900, grad_norm=0.05245083570480347, loss=0.032893355935811996
I0305 20:49:48.559165 140451025659648 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.05815007537603378, loss=0.02817458100616932
I0305 20:50:08.795918 140451017266944 logging_writer.py:48] [21100] global_step=21100, grad_norm=0.053475383669137955, loss=0.029369022697210312
I0305 20:50:12.834465 140593289979072 spec.py:321] Evaluating on the training split.
I0305 20:51:23.679982 140593289979072 spec.py:333] Evaluating on the validation split.
I0305 20:51:25.609610 140593289979072 spec.py:349] Evaluating on the test split.
I0305 20:51:27.499735 140593289979072 submission_runner.py:469] Time since start: 5883.63s, 	Step: 21121, 	{'train/accuracy': 0.9918326735496521, 'train/loss': 0.026531603187322617, 'train/mean_average_precision': 0.48344356074501593, 'validation/accuracy': 0.9870135188102722, 'validation/loss': 0.04403306543827057, 'validation/mean_average_precision': 0.28106007282085416, 'validation/num_examples': 43793, 'test/accuracy': 0.9861531853675842, 'test/loss': 0.046962324529886246, 'test/mean_average_precision': 0.26929872719248626, 'test/num_examples': 43793, 'score': 4336.757949352264, 'total_duration': 5883.628695726395, 'accumulated_submission_time': 4336.757949352264, 'accumulated_eval_time': 1545.9547145366669, 'accumulated_logging_time': 0.35282444953918457}
I0305 20:51:27.510365 140451025659648 logging_writer.py:48] [21121] accumulated_eval_time=1545.95, accumulated_logging_time=0.352824, accumulated_submission_time=4336.76, global_step=21121, preemption_count=0, score=4336.76, test/accuracy=0.986153, test/loss=0.0469623, test/mean_average_precision=0.269299, test/num_examples=43793, total_duration=5883.63, train/accuracy=0.991833, train/loss=0.0265316, train/mean_average_precision=0.483444, validation/accuracy=0.987014, validation/loss=0.0440331, validation/mean_average_precision=0.28106, validation/num_examples=43793
I0305 20:51:27.524384 140451017266944 logging_writer.py:48] [21121] global_step=21121, preemption_count=0, score=4336.76
I0305 20:51:27.672251 140593289979072 submission_runner.py:646] Tuning trial 5/5
I0305 20:51:27.672451 140593289979072 submission_runner.py:647] Hyperparameters: Hyperparameters(dropout_rate=0.1, label_smoothing=0.0, learning_rate=0.0017486387539278373, one_minus_beta1=0.06733926164, beta2=0.9955159689799007, weight_decay=0.08121616522670176, warmup_factor=0.02)
I0305 20:51:27.673537 140593289979072 submission_runner.py:648] Metrics: {'eval_results': [(1, {'train/accuracy': 0.470598429441452, 'train/loss': 0.7706692218780518, 'train/mean_average_precision': 0.020994739835503692, 'validation/accuracy': 0.4706586003303528, 'validation/loss': 0.7734909057617188, 'validation/mean_average_precision': 0.024877652610798433, 'validation/num_examples': 43793, 'test/accuracy': 0.4712654650211334, 'test/loss': 0.7741150856018066, 'test/mean_average_precision': 0.026006626473790734, 'test/num_examples': 43793, 'score': 15.517959833145142, 'total_duration': 215.80748796463013, 'accumulated_submission_time': 15.517959833145142, 'accumulated_eval_time': 200.28941679000854, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1166, {'train/accuracy': 0.9870123863220215, 'train/loss': 0.04836277291178703, 'train/mean_average_precision': 0.08382391293765193, 'validation/accuracy': 0.9843952059745789, 'validation/loss': 0.05830450356006622, 'validation/mean_average_precision': 0.0830809895285746, 'validation/num_examples': 43793, 'test/accuracy': 0.9833783507347107, 'test/loss': 0.061767686158418655, 'test/mean_average_precision': 0.08145000058663757, 'test/num_examples': 43793, 'score': 255.58223843574524, 'total_duration': 533.0306243896484, 'accumulated_submission_time': 255.58223843574524, 'accumulated_eval_time': 277.39924907684326, 'accumulated_logging_time': 0.016170501708984375, 'global_step': 1166, 'preemption_count': 0}), (2330, {'train/accuracy': 0.9877095818519592, 'train/loss': 0.044246431440114975, 'train/mean_average_precision': 0.14285725297083818, 'validation/accuracy': 0.9850065112113953, 'validation/loss': 0.05392184108495712, 'validation/mean_average_precision': 0.12916142013448384, 'validation/num_examples': 43793, 'test/accuracy': 0.9840050935745239, 'test/loss': 0.057139940559864044, 'test/mean_average_precision': 0.12935430305078038, 'test/num_examples': 43793, 'score': 495.7302179336548, 'total_duration': 847.3602809906006, 'accumulated_submission_time': 495.7302179336548, 'accumulated_eval_time': 351.5325057506561, 'accumulated_logging_time': 0.03456521034240723, 'global_step': 2330, 'preemption_count': 0}), (3498, {'train/accuracy': 0.9881824254989624, 'train/loss': 0.04154165834188461, 'train/mean_average_precision': 0.1919950392319109, 'validation/accuracy': 0.9851539134979248, 'validation/loss': 0.050627902150154114, 'validation/mean_average_precision': 0.16761772596410998, 'validation/num_examples': 43793, 'test/accuracy': 0.9843028783798218, 'test/loss': 0.05313577502965927, 'test/mean_average_precision': 0.16660779875964182, 'test/num_examples': 43793, 'score': 735.7991650104523, 'total_duration': 1162.7156586647034, 'accumulated_submission_time': 735.7991650104523, 'accumulated_eval_time': 426.7700593471527, 'accumulated_logging_time': 0.05179929733276367, 'global_step': 3498, 'preemption_count': 0}), (4681, {'train/accuracy': 0.9890322685241699, 'train/loss': 0.03764324262738228, 'train/mean_average_precision': 0.22697431765050724, 'validation/accuracy': 0.9858667254447937, 'validation/loss': 0.04774397611618042, 'validation/mean_average_precision': 0.19701497697683587, 'validation/num_examples': 43793, 'test/accuracy': 0.9849830865859985, 'test/loss': 0.05034923180937767, 'test/mean_average_precision': 0.20462535439916257, 'test/num_examples': 43793, 'score': 975.7545030117035, 'total_duration': 1477.895470380783, 'accumulated_submission_time': 975.7545030117035, 'accumulated_eval_time': 501.9466595649719, 'accumulated_logging_time': 0.07002854347229004, 'global_step': 4681, 'preemption_count': 0}), (5870, {'train/accuracy': 0.9889388680458069, 'train/loss': 0.03738610818982124, 'train/mean_average_precision': 0.2646019166690799, 'validation/accuracy': 0.9860327839851379, 'validation/loss': 0.04713626205921173, 'validation/mean_average_precision': 0.21782264567112658, 'validation/num_examples': 43793, 'test/accuracy': 0.9851166009902954, 'test/loss': 0.04990771785378456, 'test/mean_average_precision': 0.2109028761753901, 'test/num_examples': 43793, 'score': 1215.8908314704895, 'total_duration': 1792.3973093032837, 'accumulated_submission_time': 1215.8908314704895, 'accumulated_eval_time': 576.261474609375, 'accumulated_logging_time': 0.08832907676696777, 'global_step': 5870, 'preemption_count': 0}), (7053, {'train/accuracy': 0.9894576072692871, 'train/loss': 0.035316210240125656, 'train/mean_average_precision': 0.2938534300615726, 'validation/accuracy': 0.9862694144248962, 'validation/loss': 0.046089302748441696, 'validation/mean_average_precision': 0.22478334648302578, 'validation/num_examples': 43793, 'test/accuracy': 0.9854190349578857, 'test/loss': 0.048637717962265015, 'test/mean_average_precision': 0.22192604560566712, 'test/num_examples': 43793, 'score': 1455.976026058197, 'total_duration': 2106.268545627594, 'accumulated_submission_time': 1455.976026058197, 'accumulated_eval_time': 649.9953377246857, 'accumulated_logging_time': 0.10635232925415039, 'global_step': 7053, 'preemption_count': 0}), (8235, {'train/accuracy': 0.9898175001144409, 'train/loss': 0.03407149761915207, 'train/mean_average_precision': 0.3153348933859192, 'validation/accuracy': 0.9865024089813232, 'validation/loss': 0.0450764074921608, 'validation/mean_average_precision': 0.23980969811177497, 'validation/num_examples': 43793, 'test/accuracy': 0.9856077432632446, 'test/loss': 0.04782549664378166, 'test/mean_average_precision': 0.2345121696777673, 'test/num_examples': 43793, 'score': 1696.105364561081, 'total_duration': 2421.758775949478, 'accumulated_submission_time': 1696.105364561081, 'accumulated_eval_time': 725.3057918548584, 'accumulated_logging_time': 0.12739133834838867, 'global_step': 8235, 'preemption_count': 0}), (9416, {'train/accuracy': 0.9902396202087402, 'train/loss': 0.032740622758865356, 'train/mean_average_precision': 0.36087893163104556, 'validation/accuracy': 0.9863449335098267, 'validation/loss': 0.04543543979525566, 'validation/mean_average_precision': 0.24490224177942718, 'validation/num_examples': 43793, 'test/accuracy': 0.9855172038078308, 'test/loss': 0.048117585480213165, 'test/mean_average_precision': 0.23549960773149287, 'test/num_examples': 43793, 'score': 1936.2549843788147, 'total_duration': 2737.2902657985687, 'accumulated_submission_time': 1936.2549843788147, 'accumulated_eval_time': 800.6374735832214, 'accumulated_logging_time': 0.14531636238098145, 'global_step': 9416, 'preemption_count': 0}), (10589, {'train/accuracy': 0.9903358221054077, 'train/loss': 0.032137755304574966, 'train/mean_average_precision': 0.3571163184543772, 'validation/accuracy': 0.9867212176322937, 'validation/loss': 0.04450750723481178, 'validation/mean_average_precision': 0.2576649284528416, 'validation/num_examples': 43793, 'test/accuracy': 0.9858031868934631, 'test/loss': 0.047379590570926666, 'test/mean_average_precision': 0.24224174415973707, 'test/num_examples': 43793, 'score': 2176.336788892746, 'total_duration': 3051.5184395313263, 'accumulated_submission_time': 2176.336788892746, 'accumulated_eval_time': 874.7356979846954, 'accumulated_logging_time': 0.1644752025604248, 'global_step': 10589, 'preemption_count': 0}), (11752, {'train/accuracy': 0.9907875657081604, 'train/loss': 0.030610665678977966, 'train/mean_average_precision': 0.3947379646440441, 'validation/accuracy': 0.986735463142395, 'validation/loss': 0.044351790100336075, 'validation/mean_average_precision': 0.255633316955732, 'validation/num_examples': 43793, 'test/accuracy': 0.9858351945877075, 'test/loss': 0.0471184141933918, 'test/mean_average_precision': 0.25451144156030264, 'test/num_examples': 43793, 'score': 2416.428697347641, 'total_duration': 3365.6604993343353, 'accumulated_submission_time': 2416.428697347641, 'accumulated_eval_time': 948.7322244644165, 'accumulated_logging_time': 0.18390536308288574, 'global_step': 11752, 'preemption_count': 0}), (12927, {'train/accuracy': 0.9904239773750305, 'train/loss': 0.03133352845907211, 'train/mean_average_precision': 0.38690767302308204, 'validation/accuracy': 0.98676997423172, 'validation/loss': 0.04438430443406105, 'validation/mean_average_precision': 0.27292013179502816, 'validation/num_examples': 43793, 'test/accuracy': 0.9859042763710022, 'test/loss': 0.04745090380311012, 'test/mean_average_precision': 0.25480685969641675, 'test/num_examples': 43793, 'score': 2656.5188870429993, 'total_duration': 3681.188933610916, 'accumulated_submission_time': 2656.5188870429993, 'accumulated_eval_time': 1024.1156587600708, 'accumulated_logging_time': 0.209000825881958, 'global_step': 12927, 'preemption_count': 0}), (14103, {'train/accuracy': 0.9909906983375549, 'train/loss': 0.02965773642063141, 'train/mean_average_precision': 0.41687582696347114, 'validation/accuracy': 0.9866745471954346, 'validation/loss': 0.04410475492477417, 'validation/mean_average_precision': 0.26449423412637213, 'validation/num_examples': 43793, 'test/accuracy': 0.985874354839325, 'test/loss': 0.04678046703338623, 'test/mean_average_precision': 0.25480653767073136, 'test/num_examples': 43793, 'score': 2896.607532262802, 'total_duration': 3995.866068124771, 'accumulated_submission_time': 2896.607532262802, 'accumulated_eval_time': 1098.6522283554077, 'accumulated_logging_time': 0.22865080833435059, 'global_step': 14103, 'preemption_count': 0}), (15277, {'train/accuracy': 0.990873396396637, 'train/loss': 0.030069738626480103, 'train/mean_average_precision': 0.41282036928607224, 'validation/accuracy': 0.9868921637535095, 'validation/loss': 0.04422160983085632, 'validation/mean_average_precision': 0.2792304718559141, 'validation/num_examples': 43793, 'test/accuracy': 0.9860057830810547, 'test/loss': 0.047049377113580704, 'test/mean_average_precision': 0.2636919937000252, 'test/num_examples': 43793, 'score': 3136.6291110515594, 'total_duration': 4310.636175870895, 'accumulated_submission_time': 3136.6291110515594, 'accumulated_eval_time': 1173.3491911888123, 'accumulated_logging_time': 0.2494490146636963, 'global_step': 15277, 'preemption_count': 0}), (16446, {'train/accuracy': 0.9912376999855042, 'train/loss': 0.028628481552004814, 'train/mean_average_precision': 0.4425579729862624, 'validation/accuracy': 0.9867433309555054, 'validation/loss': 0.044156014919281006, 'validation/mean_average_precision': 0.2716093800371838, 'validation/num_examples': 43793, 'test/accuracy': 0.9859076142311096, 'test/loss': 0.046792931854724884, 'test/mean_average_precision': 0.26365983135477, 'test/num_examples': 43793, 'score': 3376.5800795555115, 'total_duration': 4622.9897429943085, 'accumulated_submission_time': 3376.5800795555115, 'accumulated_eval_time': 1245.7009282112122, 'accumulated_logging_time': 0.2685251235961914, 'global_step': 16446, 'preemption_count': 0}), (17609, {'train/accuracy': 0.9910468459129333, 'train/loss': 0.029164809733629227, 'train/mean_average_precision': 0.4367664442886274, 'validation/accuracy': 0.98692786693573, 'validation/loss': 0.043970558792352676, 'validation/mean_average_precision': 0.2774141644111889, 'validation/num_examples': 43793, 'test/accuracy': 0.9860685467720032, 'test/loss': 0.046715620905160904, 'test/mean_average_precision': 0.2642154545222455, 'test/num_examples': 43793, 'score': 3616.5529549121857, 'total_duration': 4938.314654350281, 'accumulated_submission_time': 3616.5529549121857, 'accumulated_eval_time': 1320.999355316162, 'accumulated_logging_time': 0.2908921241760254, 'global_step': 17609, 'preemption_count': 0}), (18772, {'train/accuracy': 0.991555392742157, 'train/loss': 0.02768556959927082, 'train/mean_average_precision': 0.4718619472609885, 'validation/accuracy': 0.9867857694625854, 'validation/loss': 0.04401611164212227, 'validation/mean_average_precision': 0.2803565531336377, 'validation/num_examples': 43793, 'test/accuracy': 0.9859662055969238, 'test/loss': 0.0468362420797348, 'test/mean_average_precision': 0.26311354145980315, 'test/num_examples': 43793, 'score': 3856.6339406967163, 'total_duration': 5255.31515455246, 'accumulated_submission_time': 3856.6339406967163, 'accumulated_eval_time': 1397.8690345287323, 'accumulated_logging_time': 0.3110542297363281, 'global_step': 18772, 'preemption_count': 0}), (19945, {'train/accuracy': 0.9912647604942322, 'train/loss': 0.028439663350582123, 'train/mean_average_precision': 0.4436082202918662, 'validation/accuracy': 0.9869644045829773, 'validation/loss': 0.04380766302347183, 'validation/mean_average_precision': 0.28048885282160335, 'validation/num_examples': 43793, 'test/accuracy': 0.9860761165618896, 'test/loss': 0.04660138487815857, 'test/mean_average_precision': 0.2757820448619877, 'test/num_examples': 43793, 'score': 4096.7310655117035, 'total_duration': 5568.8851227760315, 'accumulated_submission_time': 4096.7310655117035, 'accumulated_eval_time': 1471.289566040039, 'accumulated_logging_time': 0.33180952072143555, 'global_step': 19945, 'preemption_count': 0}), (21121, {'train/accuracy': 0.9918326735496521, 'train/loss': 0.026531603187322617, 'train/mean_average_precision': 0.48344356074501593, 'validation/accuracy': 0.9870135188102722, 'validation/loss': 0.04403306543827057, 'validation/mean_average_precision': 0.28106007282085416, 'validation/num_examples': 43793, 'test/accuracy': 0.9861531853675842, 'test/loss': 0.046962324529886246, 'test/mean_average_precision': 0.26929872719248626, 'test/num_examples': 43793, 'score': 4336.757949352264, 'total_duration': 5883.628695726395, 'accumulated_submission_time': 4336.757949352264, 'accumulated_eval_time': 1545.9547145366669, 'accumulated_logging_time': 0.35282444953918457, 'global_step': 21121, 'preemption_count': 0})], 'global_step': 21121}
I0305 20:51:27.673640 140593289979072 submission_runner.py:649] Timing: 4336.757949352264
I0305 20:51:27.673682 140593289979072 submission_runner.py:651] Total number of evals: 19
I0305 20:51:27.673722 140593289979072 submission_runner.py:652] ====================
I0305 20:51:27.673899 140593289979072 submission_runner.py:750] Final ogbg score: 4
