python submission_runner.py --framework=jax --workload=ogbg --submission_path=prize_qualification_baselines/external_tuning/jax_nadamw_full_budget.py --data_dir=/data/ogbg --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=submissions/rolling_leaderboard/external_tuning/baseline/study_1 --overwrite=True --save_checkpoints=False --rng_seed=796950158 --tuning_ruleset=external --tuning_search_space=prize_qualification_baselines/external_tuning/tuning_search_space.json --num_tuning_trials=5 --hparam_start_index=0 --hparam_end_index=1 2>&1 | tee -a /logs/ogbg_jax_03-05-2025-19-12-36.log
2025-03-05 19:12:37.549348: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1741201957.572364       9 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1741201957.579374       9 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
I0305 19:12:43.927021 140626292712640 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_1/ogbg_jax.
I0305 19:12:44.883042 140626292712640 xla_bridge.py:884] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA
I0305 19:12:44.886000 140626292712640 xla_bridge.py:884] Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory
I0305 19:12:44.887622 140626292712640 submission_runner.py:606] Using RNG seed 796950158
I0305 19:12:45.437872 140626292712640 submission_runner.py:615] --- Tuning run 1/5 ---
I0305 19:12:45.438061 140626292712640 submission_runner.py:620] Creating tuning directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_1/ogbg_jax/trial_1.
I0305 19:12:45.438231 140626292712640 logger_utils.py:97] Saving hparams to /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_1/ogbg_jax/trial_1/hparams.json.
I0305 19:12:45.666589 140626292712640 submission_runner.py:218] Initializing dataset.
I0305 19:12:45.885474 140626292712640 dataset_info.py:690] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0305 19:12:45.893314 140626292712640 reader.py:261] Creating a tf.data.Dataset reading 8 files located in folders: /data/ogbg/ogbg_molpcba/0.1.3.
WARNING:tensorflow:From /usr/local/lib/python3.11/site-packages/tensorflow_datasets/core/reader.py:101: CounterV2 (from tensorflow.python.data.experimental.ops.counter) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.counter(...)` instead.
W0305 19:12:46.128299 140626292712640 deprecation.py:50] From /usr/local/lib/python3.11/site-packages/tensorflow_datasets/core/reader.py:101: CounterV2 (from tensorflow.python.data.experimental.ops.counter) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.counter(...)` instead.
I0305 19:12:46.177386 140626292712640 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split train, from /data/ogbg/ogbg_molpcba/0.1.3
I0305 19:12:46.203870 140626292712640 submission_runner.py:229] Initializing model.
I0305 19:12:53.903376 140626292712640 submission_runner.py:272] Initializing optimizer.
I0305 19:12:54.325892 140626292712640 submission_runner.py:279] Initializing metrics bundle.
I0305 19:12:54.326143 140626292712640 submission_runner.py:301] Initializing checkpoint and logger.
I0305 19:12:54.326956 140626292712640 checkpoints.py:1101] Found no checkpoint files in /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_1/ogbg_jax/trial_1 with prefix checkpoint_
I0305 19:12:54.327075 140626292712640 submission_runner.py:321] Saving meta data to /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_1/ogbg_jax/trial_1/meta_data_0.json.
I0305 19:12:54.327250 140626292712640 logger_utils.py:262] Unable to record workload.train_mean information. Continuing without it.
I0305 19:12:54.327303 140626292712640 logger_utils.py:262] Unable to record workload.train_stddev information. Continuing without it.
I0305 19:12:54.487141 140626292712640 submission_runner.py:325] Saving flags to /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_1/ogbg_jax/trial_1/flags_0.json.
I0305 19:12:54.520287 140626292712640 submission_runner.py:337] Starting training loop.
I0305 19:13:05.319054 140490234087168 logging_writer.py:48] [0] global_step=0, grad_norm=3.0798561573028564, loss=0.7426186800003052
I0305 19:13:05.371371 140626292712640 spec.py:321] Evaluating on the training split.
I0305 19:13:05.375314 140626292712640 dataset_info.py:690] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0305 19:13:05.379130 140626292712640 reader.py:261] Creating a tf.data.Dataset reading 8 files located in folders: /data/ogbg/ogbg_molpcba/0.1.3.
I0305 19:13:05.443500 140626292712640 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split train, from /data/ogbg/ogbg_molpcba/0.1.3
I0305 19:14:21.582242 140626292712640 spec.py:333] Evaluating on the validation split.
I0305 19:14:21.585334 140626292712640 dataset_info.py:690] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0305 19:14:21.589436 140626292712640 reader.py:261] Creating a tf.data.Dataset reading 1 files located in folders: /data/ogbg/ogbg_molpcba/0.1.3.
I0305 19:14:21.649224 140626292712640 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split validation, from /data/ogbg/ogbg_molpcba/0.1.3
I0305 19:15:24.906728 140626292712640 spec.py:349] Evaluating on the test split.
I0305 19:15:24.909392 140626292712640 dataset_info.py:690] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0305 19:15:24.913079 140626292712640 reader.py:261] Creating a tf.data.Dataset reading 1 files located in folders: /data/ogbg/ogbg_molpcba/0.1.3.
I0305 19:15:24.972584 140626292712640 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split test, from /data/ogbg/ogbg_molpcba/0.1.3
I0305 19:16:29.010551 140626292712640 submission_runner.py:469] Time since start: 214.49s, 	Step: 1, 	{'train/accuracy': 0.5431785583496094, 'train/loss': 0.7421693205833435, 'train/mean_average_precision': 0.02182718391744925, 'validation/accuracy': 0.5481993556022644, 'validation/loss': 0.7410886883735657, 'validation/mean_average_precision': 0.025415576816744098, 'validation/num_examples': 43793, 'test/accuracy': 0.5508217811584473, 'test/loss': 0.739730954170227, 'test/mean_average_precision': 0.027589280859152434, 'test/num_examples': 43793, 'score': 10.850950717926025, 'total_duration': 214.49020195007324, 'accumulated_submission_time': 10.850950717926025, 'accumulated_eval_time': 203.6391258239746, 'accumulated_logging_time': 0}
I0305 19:16:29.017386 140484278003456 logging_writer.py:48] [1] accumulated_eval_time=203.639, accumulated_logging_time=0, accumulated_submission_time=10.851, global_step=1, preemption_count=0, score=10.851, test/accuracy=0.550822, test/loss=0.739731, test/mean_average_precision=0.0275893, test/num_examples=43793, total_duration=214.49, train/accuracy=0.543179, train/loss=0.742169, train/mean_average_precision=0.0218272, validation/accuracy=0.548199, validation/loss=0.741089, validation/mean_average_precision=0.0254156, validation/num_examples=43793
I0305 19:16:50.009613 140484286396160 logging_writer.py:48] [100] global_step=100, grad_norm=0.5403426289558411, loss=0.4482215940952301
I0305 19:17:10.866228 140484278003456 logging_writer.py:48] [200] global_step=200, grad_norm=0.3353688418865204, loss=0.29872187972068787
I0305 19:17:31.795931 140484286396160 logging_writer.py:48] [300] global_step=300, grad_norm=0.2077571451663971, loss=0.18501156568527222
I0305 19:17:52.589577 140484278003456 logging_writer.py:48] [400] global_step=400, grad_norm=0.11552367359399796, loss=0.11803935468196869
I0305 19:18:13.595585 140484286396160 logging_writer.py:48] [500] global_step=500, grad_norm=0.06463228911161423, loss=0.08371828496456146
I0305 19:18:34.560795 140484278003456 logging_writer.py:48] [600] global_step=600, grad_norm=0.03679778426885605, loss=0.07203202694654465
I0305 19:18:54.805418 140484840232704 logging_writer.py:48] [700] global_step=700, grad_norm=0.028440825641155243, loss=0.06796864420175552
I0305 19:19:15.460654 140484831840000 logging_writer.py:48] [800] global_step=800, grad_norm=0.054124243557453156, loss=0.0566120408475399
I0305 19:19:36.434370 140484840232704 logging_writer.py:48] [900] global_step=900, grad_norm=0.09719153493642807, loss=0.05951201543211937
I0305 19:19:57.398121 140484831840000 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.03918560594320297, loss=0.05483851954340935
I0305 19:20:18.418868 140484840232704 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.10385861992835999, loss=0.0533580407500267
I0305 19:20:29.232918 140626292712640 spec.py:321] Evaluating on the training split.
I0305 19:21:41.311965 140626292712640 spec.py:333] Evaluating on the validation split.
I0305 19:21:43.248827 140626292712640 spec.py:349] Evaluating on the test split.
I0305 19:21:45.147054 140626292712640 submission_runner.py:469] Time since start: 530.63s, 	Step: 1153, 	{'train/accuracy': 0.9870090484619141, 'train/loss': 0.050545353442430496, 'train/mean_average_precision': 0.06407091537008257, 'validation/accuracy': 0.9843781590461731, 'validation/loss': 0.06010604649782181, 'validation/mean_average_precision': 0.06043822553158016, 'validation/num_examples': 43793, 'test/accuracy': 0.9833905696868896, 'test/loss': 0.06316173076629639, 'test/mean_average_precision': 0.06219830724151265, 'test/num_examples': 43793, 'score': 251.0264482498169, 'total_duration': 530.6267046928406, 'accumulated_submission_time': 251.0264482498169, 'accumulated_eval_time': 279.55322909355164, 'accumulated_logging_time': 0.01755666732788086}
I0305 19:21:45.155570 140484831840000 logging_writer.py:48] [1153] accumulated_eval_time=279.553, accumulated_logging_time=0.0175567, accumulated_submission_time=251.026, global_step=1153, preemption_count=0, score=251.026, test/accuracy=0.983391, test/loss=0.0631617, test/mean_average_precision=0.0621983, test/num_examples=43793, total_duration=530.627, train/accuracy=0.987009, train/loss=0.0505454, train/mean_average_precision=0.0640709, validation/accuracy=0.984378, validation/loss=0.060106, validation/mean_average_precision=0.0604382, validation/num_examples=43793
I0305 19:21:55.218651 140484840232704 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.09396851062774658, loss=0.04757675901055336
I0305 19:22:16.011217 140484831840000 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.08238768577575684, loss=0.055098287761211395
I0305 19:22:37.000082 140484840232704 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.11459746956825256, loss=0.04634130373597145
I0305 19:22:58.001146 140484831840000 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.09080763161182404, loss=0.04996625706553459
I0305 19:23:18.971553 140484840232704 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.10710395872592926, loss=0.04936794564127922
I0305 19:23:40.184844 140484831840000 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.1348455399274826, loss=0.05540718510746956
I0305 19:24:01.514873 140484840232704 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.09779875725507736, loss=0.04874707758426666
I0305 19:24:22.830979 140484831840000 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.08812148869037628, loss=0.047832638025283813
I0305 19:24:43.880061 140484840232704 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.10679327696561813, loss=0.046171411871910095
I0305 19:25:04.736163 140484831840000 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.04608958214521408, loss=0.04806104302406311
I0305 19:25:25.686716 140484840232704 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.10024221241474152, loss=0.04598751291632652
I0305 19:25:45.272755 140626292712640 spec.py:321] Evaluating on the training split.
I0305 19:26:56.686069 140626292712640 spec.py:333] Evaluating on the validation split.
I0305 19:26:58.558186 140626292712640 spec.py:349] Evaluating on the test split.
I0305 19:27:00.399963 140626292712640 submission_runner.py:469] Time since start: 845.88s, 	Step: 2294, 	{'train/accuracy': 0.9879885315895081, 'train/loss': 0.04352015629410744, 'train/mean_average_precision': 0.1451954096378576, 'validation/accuracy': 0.9850970506668091, 'validation/loss': 0.05305362492799759, 'validation/mean_average_precision': 0.13762861196349602, 'validation/num_examples': 43793, 'test/accuracy': 0.984125554561615, 'test/loss': 0.055969685316085815, 'test/mean_average_precision': 0.13806424237419235, 'test/num_examples': 43793, 'score': 491.0974769592285, 'total_duration': 845.8796153068542, 'accumulated_submission_time': 491.0974769592285, 'accumulated_eval_time': 354.68038988113403, 'accumulated_logging_time': 0.036165714263916016}
I0305 19:27:00.408065 140484831840000 logging_writer.py:48] [2294] accumulated_eval_time=354.68, accumulated_logging_time=0.0361657, accumulated_submission_time=491.097, global_step=2294, preemption_count=0, score=491.097, test/accuracy=0.984126, test/loss=0.0559697, test/mean_average_precision=0.138064, test/num_examples=43793, total_duration=845.88, train/accuracy=0.987989, train/loss=0.0435202, train/mean_average_precision=0.145195, validation/accuracy=0.985097, validation/loss=0.0530536, validation/mean_average_precision=0.137629, validation/num_examples=43793
I0305 19:27:01.872338 140484840232704 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.06416652351617813, loss=0.043218765407800674
I0305 19:27:22.587792 140484831840000 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.08968819677829742, loss=0.0473506897687912
I0305 19:27:43.422990 140484840232704 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.05935850366950035, loss=0.046376604586839676
I0305 19:28:04.077224 140484831840000 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.09786629676818848, loss=0.04353891685605049
I0305 19:28:24.634127 140484840232704 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.05423908308148384, loss=0.04514899477362633
I0305 19:28:45.083117 140484831840000 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.16388417780399323, loss=0.04327249899506569
I0305 19:29:05.429484 140484840232704 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.06536243110895157, loss=0.04170340299606323
I0305 19:29:25.555588 140484831840000 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.11832864582538605, loss=0.044327784329652786
I0305 19:29:46.156001 140484840232704 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.07157319039106369, loss=0.043251827359199524
I0305 19:30:07.041118 140484831840000 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.0990796834230423, loss=0.044830866158008575
I0305 19:30:27.843785 140484840232704 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.04292713478207588, loss=0.044122207909822464
I0305 19:30:48.406544 140484831840000 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.0660240426659584, loss=0.04475947096943855
I0305 19:31:00.467300 140626292712640 spec.py:321] Evaluating on the training split.
I0305 19:32:13.361169 140626292712640 spec.py:333] Evaluating on the validation split.
I0305 19:32:15.305796 140626292712640 spec.py:349] Evaluating on the test split.
I0305 19:32:17.190042 140626292712640 submission_runner.py:469] Time since start: 1162.67s, 	Step: 3460, 	{'train/accuracy': 0.9879725575447083, 'train/loss': 0.04192984849214554, 'train/mean_average_precision': 0.18725180596473615, 'validation/accuracy': 0.9851510524749756, 'validation/loss': 0.051309239119291306, 'validation/mean_average_precision': 0.17007330437568605, 'validation/num_examples': 43793, 'test/accuracy': 0.9841432571411133, 'test/loss': 0.05397097021341324, 'test/mean_average_precision': 0.16847132039786678, 'test/num_examples': 43793, 'score': 731.1187317371368, 'total_duration': 1162.6696939468384, 'accumulated_submission_time': 731.1187317371368, 'accumulated_eval_time': 431.4030804634094, 'accumulated_logging_time': 0.053813934326171875}
I0305 19:32:17.199314 140484840232704 logging_writer.py:48] [3460] accumulated_eval_time=431.403, accumulated_logging_time=0.0538139, accumulated_submission_time=731.119, global_step=3460, preemption_count=0, score=731.119, test/accuracy=0.984143, test/loss=0.053971, test/mean_average_precision=0.168471, test/num_examples=43793, total_duration=1162.67, train/accuracy=0.987973, train/loss=0.0419298, train/mean_average_precision=0.187252, validation/accuracy=0.985151, validation/loss=0.0513092, validation/mean_average_precision=0.170073, validation/num_examples=43793
I0305 19:32:25.783641 140484831840000 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.03401922807097435, loss=0.0427091158926487
I0305 19:32:46.868919 140484840232704 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.042135026305913925, loss=0.04365694522857666
I0305 19:33:07.703531 140484831840000 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.06162213906645775, loss=0.04078596085309982
I0305 19:33:28.792893 140484840232704 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.05790123715996742, loss=0.04086053743958473
I0305 19:33:49.982680 140484831840000 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.058819349855184555, loss=0.04466302692890167
I0305 19:34:11.004798 140484840232704 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.024887338280677795, loss=0.04327563941478729
I0305 19:34:32.161730 140484831840000 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.03930302709341049, loss=0.0507754310965538
I0305 19:34:52.504012 140484840232704 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.04044321924448013, loss=0.03611833229660988
I0305 19:35:13.211315 140484831840000 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.03516915813088417, loss=0.04768512025475502
I0305 19:35:34.277059 140484840232704 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.03415423259139061, loss=0.03950498625636101
I0305 19:35:54.966703 140484831840000 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.030556021258234978, loss=0.04177454486489296
I0305 19:36:15.712582 140484840232704 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.036566805094480515, loss=0.043900296092033386
I0305 19:36:17.380331 140626292712640 spec.py:321] Evaluating on the training split.
I0305 19:37:28.477996 140626292712640 spec.py:333] Evaluating on the validation split.
I0305 19:37:30.389832 140626292712640 spec.py:349] Evaluating on the test split.
I0305 19:37:32.312488 140626292712640 submission_runner.py:469] Time since start: 1477.79s, 	Step: 4609, 	{'train/accuracy': 0.9885240197181702, 'train/loss': 0.039675094187259674, 'train/mean_average_precision': 0.20599443917842142, 'validation/accuracy': 0.9857413172721863, 'validation/loss': 0.04858216643333435, 'validation/mean_average_precision': 0.18493339896818412, 'validation/num_examples': 43793, 'test/accuracy': 0.9848841428756714, 'test/loss': 0.051082391291856766, 'test/mean_average_precision': 0.18672561271295926, 'test/num_examples': 43793, 'score': 971.2598721981049, 'total_duration': 1477.7921361923218, 'accumulated_submission_time': 971.2598721981049, 'accumulated_eval_time': 506.3351764678955, 'accumulated_logging_time': 0.07214069366455078}
I0305 19:37:32.321012 140484333688576 logging_writer.py:48] [4609] accumulated_eval_time=506.335, accumulated_logging_time=0.0721407, accumulated_submission_time=971.26, global_step=4609, preemption_count=0, score=971.26, test/accuracy=0.984884, test/loss=0.0510824, test/mean_average_precision=0.186726, test/num_examples=43793, total_duration=1477.79, train/accuracy=0.988524, train/loss=0.0396751, train/mean_average_precision=0.205994, validation/accuracy=0.985741, validation/loss=0.0485822, validation/mean_average_precision=0.184933, validation/num_examples=43793
I0305 19:37:51.157035 140484325295872 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.05026669055223465, loss=0.038340598344802856
I0305 19:38:11.931595 140484333688576 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.04386403411626816, loss=0.04238123446702957
I0305 19:38:32.329062 140484325295872 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.032469481229782104, loss=0.04281042888760567
I0305 19:38:52.678331 140484333688576 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.028909580782055855, loss=0.039883390069007874
I0305 19:39:13.202030 140484325295872 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.033433470875024796, loss=0.03891315311193466
I0305 19:39:33.798298 140484333688576 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.031816862523555756, loss=0.041929665952920914
I0305 19:39:54.581087 140484325295872 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.02712888829410076, loss=0.04089675098657608
I0305 19:40:15.045912 140484333688576 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.02122679352760315, loss=0.03869311884045601
I0305 19:40:35.601595 140484325295872 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.0454256497323513, loss=0.03710106760263443
I0305 19:40:56.378779 140484333688576 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.04250995069742203, loss=0.04018934443593025
I0305 19:41:17.094115 140484325295872 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.029776468873023987, loss=0.041272714734077454
I0305 19:41:32.329331 140626292712640 spec.py:321] Evaluating on the training split.
I0305 19:42:44.009202 140626292712640 spec.py:333] Evaluating on the validation split.
I0305 19:42:45.932967 140626292712640 spec.py:349] Evaluating on the test split.
I0305 19:42:47.808397 140626292712640 submission_runner.py:469] Time since start: 1793.29s, 	Step: 5774, 	{'train/accuracy': 0.9888306856155396, 'train/loss': 0.038068272173404694, 'train/mean_average_precision': 0.25070116099068346, 'validation/accuracy': 0.9857628345489502, 'validation/loss': 0.04819110408425331, 'validation/mean_average_precision': 0.1981952354630591, 'validation/num_examples': 43793, 'test/accuracy': 0.9848681092262268, 'test/loss': 0.05086361616849899, 'test/mean_average_precision': 0.193845023724409, 'test/num_examples': 43793, 'score': 1211.225421667099, 'total_duration': 1793.2880549430847, 'accumulated_submission_time': 1211.225421667099, 'accumulated_eval_time': 581.8141956329346, 'accumulated_logging_time': 0.09082365036010742}
I0305 19:42:47.817184 140484333688576 logging_writer.py:48] [5774] accumulated_eval_time=581.814, accumulated_logging_time=0.0908237, accumulated_submission_time=1211.23, global_step=5774, preemption_count=0, score=1211.23, test/accuracy=0.984868, test/loss=0.0508636, test/mean_average_precision=0.193845, test/num_examples=43793, total_duration=1793.29, train/accuracy=0.988831, train/loss=0.0380683, train/mean_average_precision=0.250701, validation/accuracy=0.985763, validation/loss=0.0481911, validation/mean_average_precision=0.198195, validation/num_examples=43793
I0305 19:42:53.460769 140484325295872 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.027663040906190872, loss=0.040476903319358826
I0305 19:43:14.415476 140484333688576 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.022849244996905327, loss=0.03878131881356239
I0305 19:43:35.353167 140484325295872 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.02516152709722519, loss=0.042191680520772934
I0305 19:43:55.984619 140484333688576 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.026055563241243362, loss=0.039262913167476654
I0305 19:44:16.675717 140484325295872 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.037063758820295334, loss=0.044553499668836594
I0305 19:44:37.311641 140484333688576 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.02499200589954853, loss=0.04293576255440712
I0305 19:44:57.917799 140484325295872 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.025150859728455544, loss=0.039539262652397156
I0305 19:45:18.741565 140484333688576 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.029234524816274643, loss=0.03962946683168411
I0305 19:45:39.553920 140484325295872 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.03917114809155464, loss=0.03854730352759361
I0305 19:46:00.386886 140484333688576 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.02192608267068863, loss=0.039816636592149734
I0305 19:46:21.247147 140484325295872 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.027934959158301353, loss=0.03678829222917557
I0305 19:46:41.933138 140484333688576 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.020847614854574203, loss=0.03498326987028122
I0305 19:46:47.839585 140626292712640 spec.py:321] Evaluating on the training split.
I0305 19:48:00.433264 140626292712640 spec.py:333] Evaluating on the validation split.
I0305 19:48:02.422387 140626292712640 spec.py:349] Evaluating on the test split.
I0305 19:48:04.538833 140626292712640 submission_runner.py:469] Time since start: 2110.02s, 	Step: 6930, 	{'train/accuracy': 0.9890471696853638, 'train/loss': 0.0371469222009182, 'train/mean_average_precision': 0.24498108487007086, 'validation/accuracy': 0.9860413074493408, 'validation/loss': 0.046656422317028046, 'validation/mean_average_precision': 0.21532321223722267, 'validation/num_examples': 43793, 'test/accuracy': 0.9851861000061035, 'test/loss': 0.04937116429209709, 'test/mean_average_precision': 0.20953894637990533, 'test/num_examples': 43793, 'score': 1451.207303762436, 'total_duration': 2110.0183398723602, 'accumulated_submission_time': 1451.207303762436, 'accumulated_eval_time': 658.5132474899292, 'accumulated_logging_time': 0.10972833633422852}
I0305 19:48:04.549469 140484325295872 logging_writer.py:48] [6930] accumulated_eval_time=658.513, accumulated_logging_time=0.109728, accumulated_submission_time=1451.21, global_step=6930, preemption_count=0, score=1451.21, test/accuracy=0.985186, test/loss=0.0493712, test/mean_average_precision=0.209539, test/num_examples=43793, total_duration=2110.02, train/accuracy=0.989047, train/loss=0.0371469, train/mean_average_precision=0.244981, validation/accuracy=0.986041, validation/loss=0.0466564, validation/mean_average_precision=0.215323, validation/num_examples=43793
I0305 19:48:18.964262 140484333688576 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.01891675963997841, loss=0.03608784079551697
I0305 19:48:39.498291 140484325295872 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.024373088032007217, loss=0.041563741862773895
I0305 19:49:00.612572 140484333688576 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.04674054682254791, loss=0.04334833845496178
I0305 19:49:21.441077 140484325295872 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.03054625540971756, loss=0.03621315583586693
I0305 19:49:41.963226 140484333688576 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.025064878165721893, loss=0.0335388220846653
I0305 19:50:02.612786 140484325295872 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.02971886843442917, loss=0.03833300247788429
I0305 19:50:23.526469 140484333688576 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.04741029068827629, loss=0.03938351944088936
I0305 19:50:44.079480 140484325295872 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.03179560974240303, loss=0.039262060075998306
I0305 19:51:04.752254 140484333688576 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.021849723532795906, loss=0.037274304777383804
I0305 19:51:25.397641 140484325295872 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.023966168984770775, loss=0.03738335520029068
I0305 19:51:46.068085 140484333688576 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.020600777119398117, loss=0.039355043321847916
I0305 19:52:04.678908 140626292712640 spec.py:321] Evaluating on the training split.
I0305 19:53:15.870220 140626292712640 spec.py:333] Evaluating on the validation split.
I0305 19:53:17.843312 140626292712640 spec.py:349] Evaluating on the test split.
I0305 19:53:19.769776 140626292712640 submission_runner.py:469] Time since start: 2425.25s, 	Step: 8091, 	{'train/accuracy': 0.9894515872001648, 'train/loss': 0.03556307032704353, 'train/mean_average_precision': 0.299316865914813, 'validation/accuracy': 0.9863169193267822, 'validation/loss': 0.04597592353820801, 'validation/mean_average_precision': 0.23158137446964153, 'validation/num_examples': 43793, 'test/accuracy': 0.985472559928894, 'test/loss': 0.04848050698637962, 'test/mean_average_precision': 0.22282319410826892, 'test/num_examples': 43793, 'score': 1691.295039653778, 'total_duration': 2425.2494192123413, 'accumulated_submission_time': 1691.295039653778, 'accumulated_eval_time': 733.6040587425232, 'accumulated_logging_time': 0.13005828857421875}
I0305 19:53:19.779815 140484325295872 logging_writer.py:48] [8091] accumulated_eval_time=733.604, accumulated_logging_time=0.130058, accumulated_submission_time=1691.3, global_step=8091, preemption_count=0, score=1691.3, test/accuracy=0.985473, test/loss=0.0484805, test/mean_average_precision=0.222823, test/num_examples=43793, total_duration=2425.25, train/accuracy=0.989452, train/loss=0.0355631, train/mean_average_precision=0.299317, validation/accuracy=0.986317, validation/loss=0.0459759, validation/mean_average_precision=0.231581, validation/num_examples=43793
I0305 19:53:21.898936 140484333688576 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.021152591332793236, loss=0.03657876327633858
I0305 19:53:42.393819 140484325295872 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.022085165604948997, loss=0.04135468229651451
I0305 19:54:02.608090 140484333688576 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.020580405369400978, loss=0.04025803878903389
I0305 19:54:22.792075 140484325295872 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.02745722234249115, loss=0.038834888488054276
I0305 19:54:43.421231 140484333688576 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.0292607843875885, loss=0.041855957359075546
I0305 19:55:04.323850 140484325295872 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.030791150406003, loss=0.03923565521836281
I0305 19:55:25.111240 140484333688576 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.029287151992321014, loss=0.0398588590323925
I0305 19:55:46.056315 140484325295872 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.027214407920837402, loss=0.03839531168341637
I0305 19:56:06.909863 140484333688576 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.02122517116367817, loss=0.03329407423734665
I0305 19:56:27.403745 140484325295872 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.03167548030614853, loss=0.03837430849671364
I0305 19:56:47.545812 140484333688576 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.044328510761260986, loss=0.03629567474126816
I0305 19:57:08.253856 140484325295872 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.032371435314416885, loss=0.03834059461951256
I0305 19:57:19.787660 140626292712640 spec.py:321] Evaluating on the training split.
I0305 19:58:31.319359 140626292712640 spec.py:333] Evaluating on the validation split.
I0305 19:58:33.267749 140626292712640 spec.py:349] Evaluating on the test split.
I0305 19:58:35.157994 140626292712640 submission_runner.py:469] Time since start: 2740.64s, 	Step: 9257, 	{'train/accuracy': 0.9897158145904541, 'train/loss': 0.03479438275098801, 'train/mean_average_precision': 0.3059435112338704, 'validation/accuracy': 0.9864736199378967, 'validation/loss': 0.04549968242645264, 'validation/mean_average_precision': 0.24027741120070423, 'validation/num_examples': 43793, 'test/accuracy': 0.9856157302856445, 'test/loss': 0.04819253832101822, 'test/mean_average_precision': 0.237937431740224, 'test/num_examples': 43793, 'score': 1931.2635045051575, 'total_duration': 2740.6375207901, 'accumulated_submission_time': 1931.2635045051575, 'accumulated_eval_time': 808.9742133617401, 'accumulated_logging_time': 0.1501317024230957}
I0305 19:58:35.167420 140484333688576 logging_writer.py:48] [9257] accumulated_eval_time=808.974, accumulated_logging_time=0.150132, accumulated_submission_time=1931.26, global_step=9257, preemption_count=0, score=1931.26, test/accuracy=0.985616, test/loss=0.0481925, test/mean_average_precision=0.237937, test/num_examples=43793, total_duration=2740.64, train/accuracy=0.989716, train/loss=0.0347944, train/mean_average_precision=0.305944, validation/accuracy=0.986474, validation/loss=0.0454997, validation/mean_average_precision=0.240277, validation/num_examples=43793
I0305 19:58:44.194868 140484325295872 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.030701065436005592, loss=0.03966335952281952
I0305 19:59:04.742608 140484333688576 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.04764886945486069, loss=0.03850400075316429
I0305 19:59:25.545885 140484325295872 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.02816646546125412, loss=0.03721912205219269
I0305 19:59:46.266040 140484333688576 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.03441433236002922, loss=0.04131479561328888
I0305 20:00:06.943736 140484325295872 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.030562542378902435, loss=0.03691946715116501
I0305 20:00:27.698560 140484333688576 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.035723380744457245, loss=0.03938140347599983
I0305 20:00:48.620417 140484325295872 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.02772665210068226, loss=0.03723038733005524
I0305 20:01:09.555955 140484333688576 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.04349799454212189, loss=0.03823433071374893
I0305 20:01:30.529096 140484325295872 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.03640933334827423, loss=0.03648797795176506
I0305 20:01:51.406971 140484333688576 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.03239592909812927, loss=0.03393404185771942
I0305 20:02:12.037415 140484325295872 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.06159130856394768, loss=0.04115023836493492
I0305 20:02:32.938860 140484333688576 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.024535812437534332, loss=0.03333211690187454
I0305 20:02:35.317167 140626292712640 spec.py:321] Evaluating on the training split.
I0305 20:03:46.173535 140626292712640 spec.py:333] Evaluating on the validation split.
I0305 20:03:48.100688 140626292712640 spec.py:349] Evaluating on the test split.
I0305 20:03:50.149857 140626292712640 submission_runner.py:469] Time since start: 3055.63s, 	Step: 10412, 	{'train/accuracy': 0.989997386932373, 'train/loss': 0.03330015763640404, 'train/mean_average_precision': 0.34457817370553345, 'validation/accuracy': 0.9865730404853821, 'validation/loss': 0.04517685994505882, 'validation/mean_average_precision': 0.24373563617692223, 'validation/num_examples': 43793, 'test/accuracy': 0.9857088327407837, 'test/loss': 0.047590598464012146, 'test/mean_average_precision': 0.2419470619588008, 'test/num_examples': 43793, 'score': 2171.371392726898, 'total_duration': 3055.6293666362762, 'accumulated_submission_time': 2171.371392726898, 'accumulated_eval_time': 883.8067071437836, 'accumulated_logging_time': 0.16864824295043945}
I0305 20:03:50.160227 140484325295872 logging_writer.py:48] [10412] accumulated_eval_time=883.807, accumulated_logging_time=0.168648, accumulated_submission_time=2171.37, global_step=10412, preemption_count=0, score=2171.37, test/accuracy=0.985709, test/loss=0.0475906, test/mean_average_precision=0.241947, test/num_examples=43793, total_duration=3055.63, train/accuracy=0.989997, train/loss=0.0333002, train/mean_average_precision=0.344578, validation/accuracy=0.986573, validation/loss=0.0451769, validation/mean_average_precision=0.243736, validation/num_examples=43793
I0305 20:04:08.169221 140484333688576 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.02552514709532261, loss=0.03571094200015068
I0305 20:04:28.336784 140484325295872 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.02555699087679386, loss=0.034194838255643845
I0305 20:04:48.833794 140484333688576 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.03632991388440132, loss=0.03840461000800133
I0305 20:05:09.649013 140484325295872 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.05506772920489311, loss=0.037629634141922
I0305 20:05:30.295495 140484333688576 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.027192335575819016, loss=0.03834309056401253
I0305 20:05:50.746553 140484325295872 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.04422318562865257, loss=0.03944473713636398
I0305 20:06:11.097850 140484333688576 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.039372846484184265, loss=0.038463860750198364
I0305 20:06:31.806352 140484325295872 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.024680914357304573, loss=0.033375341445207596
I0305 20:06:52.709208 140484333688576 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.04694798216223717, loss=0.03610049560666084
I0305 20:07:13.287062 140484325295872 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.04012935981154442, loss=0.03815874084830284
I0305 20:07:33.768591 140484333688576 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.04054516926407814, loss=0.04000537469983101
I0305 20:07:50.307403 140626292712640 spec.py:321] Evaluating on the training split.
I0305 20:09:00.240872 140626292712640 spec.py:333] Evaluating on the validation split.
I0305 20:09:02.130206 140626292712640 spec.py:349] Evaluating on the test split.
I0305 20:09:04.008999 140626292712640 submission_runner.py:469] Time since start: 3369.49s, 	Step: 11583, 	{'train/accuracy': 0.990007221698761, 'train/loss': 0.033334046602249146, 'train/mean_average_precision': 0.33691781754249356, 'validation/accuracy': 0.9865267872810364, 'validation/loss': 0.04502268135547638, 'validation/mean_average_precision': 0.2502698204058094, 'validation/num_examples': 43793, 'test/accuracy': 0.9856696724891663, 'test/loss': 0.04767971858382225, 'test/mean_average_precision': 0.23946808405373068, 'test/num_examples': 43793, 'score': 2411.47766828537, 'total_duration': 3369.488542318344, 'accumulated_submission_time': 2411.47766828537, 'accumulated_eval_time': 957.5081422328949, 'accumulated_logging_time': 0.1887226104736328}
I0305 20:09:04.018467 140484325295872 logging_writer.py:48] [11583] accumulated_eval_time=957.508, accumulated_logging_time=0.188723, accumulated_submission_time=2411.48, global_step=11583, preemption_count=0, score=2411.48, test/accuracy=0.98567, test/loss=0.0476797, test/mean_average_precision=0.239468, test/num_examples=43793, total_duration=3369.49, train/accuracy=0.990007, train/loss=0.033334, train/mean_average_precision=0.336918, validation/accuracy=0.986527, validation/loss=0.0450227, validation/mean_average_precision=0.25027, validation/num_examples=43793
I0305 20:09:07.714145 140484333688576 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.03712832182645798, loss=0.03943402320146561
I0305 20:09:28.659205 140484325295872 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.03489333391189575, loss=0.0351119227707386
I0305 20:09:49.452092 140484333688576 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.03387073054909706, loss=0.03376694396138191
I0305 20:10:10.229544 140484325295872 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.047301772981882095, loss=0.033674269914627075
I0305 20:10:30.959512 140484333688576 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.04071982577443123, loss=0.03418761491775513
I0305 20:10:51.763548 140484325295872 logging_writer.py:48] [12100] global_step=12100, grad_norm=0.03516792878508568, loss=0.0360812172293663
I0305 20:11:12.503609 140484333688576 logging_writer.py:48] [12200] global_step=12200, grad_norm=0.04535789042711258, loss=0.03587718680500984
I0305 20:11:33.152877 140484325295872 logging_writer.py:48] [12300] global_step=12300, grad_norm=0.04820787161588669, loss=0.04068363830447197
I0305 20:11:54.032357 140484333688576 logging_writer.py:48] [12400] global_step=12400, grad_norm=0.053423162549734116, loss=0.040036000311374664
I0305 20:12:14.675325 140484325295872 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.03437119722366333, loss=0.035335008054971695
I0305 20:12:35.445304 140484333688576 logging_writer.py:48] [12600] global_step=12600, grad_norm=0.056367792189121246, loss=0.03676119074225426
I0305 20:12:56.005591 140484325295872 logging_writer.py:48] [12700] global_step=12700, grad_norm=0.047927264124155045, loss=0.03653848543763161
I0305 20:13:04.117700 140626292712640 spec.py:321] Evaluating on the training split.
I0305 20:14:15.625792 140626292712640 spec.py:333] Evaluating on the validation split.
I0305 20:14:17.740535 140626292712640 spec.py:349] Evaluating on the test split.
I0305 20:14:19.813985 140626292712640 submission_runner.py:469] Time since start: 3685.29s, 	Step: 12740, 	{'train/accuracy': 0.9903923273086548, 'train/loss': 0.031664155423641205, 'train/mean_average_precision': 0.3782949305561416, 'validation/accuracy': 0.9866716861724854, 'validation/loss': 0.04482003301382065, 'validation/mean_average_precision': 0.2579583473552823, 'validation/num_examples': 43793, 'test/accuracy': 0.9857690334320068, 'test/loss': 0.04754732549190521, 'test/mean_average_precision': 0.24710784611764655, 'test/num_examples': 43793, 'score': 2651.5365748405457, 'total_duration': 3685.2935013771057, 'accumulated_submission_time': 2651.5365748405457, 'accumulated_eval_time': 1033.2042524814606, 'accumulated_logging_time': 0.2071077823638916}
I0305 20:14:19.824838 140484333688576 logging_writer.py:48] [12740] accumulated_eval_time=1033.2, accumulated_logging_time=0.207108, accumulated_submission_time=2651.54, global_step=12740, preemption_count=0, score=2651.54, test/accuracy=0.985769, test/loss=0.0475473, test/mean_average_precision=0.247108, test/num_examples=43793, total_duration=3685.29, train/accuracy=0.990392, train/loss=0.0316642, train/mean_average_precision=0.378295, validation/accuracy=0.986672, validation/loss=0.04482, validation/mean_average_precision=0.257958, validation/num_examples=43793
I0305 20:14:32.249138 140484325295872 logging_writer.py:48] [12800] global_step=12800, grad_norm=0.04974113032221794, loss=0.03806985542178154
I0305 20:14:52.976321 140484333688576 logging_writer.py:48] [12900] global_step=12900, grad_norm=0.04624524340033531, loss=0.036361515522003174
I0305 20:15:13.866070 140484325295872 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.04847904294729233, loss=0.03595663234591484
I0305 20:15:34.660101 140484333688576 logging_writer.py:48] [13100] global_step=13100, grad_norm=0.052149754017591476, loss=0.03673696145415306
I0305 20:15:55.382064 140484325295872 logging_writer.py:48] [13200] global_step=13200, grad_norm=0.046582985669374466, loss=0.03492755815386772
I0305 20:16:16.052905 140484333688576 logging_writer.py:48] [13300] global_step=13300, grad_norm=0.0498475544154644, loss=0.03434482589364052
I0305 20:16:37.032693 140484325295872 logging_writer.py:48] [13400] global_step=13400, grad_norm=0.04397253319621086, loss=0.03537100553512573
I0305 20:16:57.922348 140484333688576 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.0419854074716568, loss=0.03516104817390442
I0305 20:17:18.679121 140484325295872 logging_writer.py:48] [13600] global_step=13600, grad_norm=0.049044061452150345, loss=0.03538411855697632
I0305 20:17:39.769920 140484333688576 logging_writer.py:48] [13700] global_step=13700, grad_norm=0.06939226388931274, loss=0.03268127515912056
I0305 20:18:00.551977 140484325295872 logging_writer.py:48] [13800] global_step=13800, grad_norm=0.03818454593420029, loss=0.033059366047382355
I0305 20:18:19.816233 140626292712640 spec.py:321] Evaluating on the training split.
I0305 20:19:32.443576 140626292712640 spec.py:333] Evaluating on the validation split.
I0305 20:19:34.379835 140626292712640 spec.py:349] Evaluating on the test split.
I0305 20:19:36.264664 140626292712640 submission_runner.py:469] Time since start: 4001.74s, 	Step: 13894, 	{'train/accuracy': 0.9904219508171082, 'train/loss': 0.03176099434494972, 'train/mean_average_precision': 0.3657467819493239, 'validation/accuracy': 0.9867581725120544, 'validation/loss': 0.044302795082330704, 'validation/mean_average_precision': 0.2583058038460325, 'validation/num_examples': 43793, 'test/accuracy': 0.9858949780464172, 'test/loss': 0.04674059525132179, 'test/mean_average_precision': 0.25534803714699794, 'test/num_examples': 43793, 'score': 2891.484347820282, 'total_duration': 4001.7441952228546, 'accumulated_submission_time': 2891.484347820282, 'accumulated_eval_time': 1109.6525075435638, 'accumulated_logging_time': 0.2286546230316162}
I0305 20:19:36.274957 140484333688576 logging_writer.py:48] [13894] accumulated_eval_time=1109.65, accumulated_logging_time=0.228655, accumulated_submission_time=2891.48, global_step=13894, preemption_count=0, score=2891.48, test/accuracy=0.985895, test/loss=0.0467406, test/mean_average_precision=0.255348, test/num_examples=43793, total_duration=4001.74, train/accuracy=0.990422, train/loss=0.031761, train/mean_average_precision=0.365747, validation/accuracy=0.986758, validation/loss=0.0443028, validation/mean_average_precision=0.258306, validation/num_examples=43793
I0305 20:19:37.695207 140484325295872 logging_writer.py:48] [13900] global_step=13900, grad_norm=0.036691341549158096, loss=0.03174455091357231
I0305 20:19:58.437816 140484333688576 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.04087665677070618, loss=0.03686560317873955
I0305 20:20:19.154484 140484325295872 logging_writer.py:48] [14100] global_step=14100, grad_norm=0.048955872654914856, loss=0.035796087235212326
I0305 20:20:39.652203 140484333688576 logging_writer.py:48] [14200] global_step=14200, grad_norm=0.049559932202100754, loss=0.03512895107269287
I0305 20:20:59.939991 140484325295872 logging_writer.py:48] [14300] global_step=14300, grad_norm=0.06293468922376633, loss=0.0360533781349659
I0305 20:21:20.501354 140484333688576 logging_writer.py:48] [14400] global_step=14400, grad_norm=0.05453763157129288, loss=0.03600221499800682
I0305 20:21:41.475679 140484325295872 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.06644237786531448, loss=0.03520786017179489
I0305 20:22:02.790266 140484333688576 logging_writer.py:48] [14600] global_step=14600, grad_norm=0.0548943430185318, loss=0.033679768443107605
I0305 20:22:23.887484 140484325295872 logging_writer.py:48] [14700] global_step=14700, grad_norm=0.055595606565475464, loss=0.0339181087911129
I0305 20:22:45.027738 140484333688576 logging_writer.py:48] [14800] global_step=14800, grad_norm=0.04238288849592209, loss=0.03317329287528992
I0305 20:23:06.272424 140484325295872 logging_writer.py:48] [14900] global_step=14900, grad_norm=0.04816237837076187, loss=0.03331075981259346
I0305 20:23:27.378130 140484333688576 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.056512415409088135, loss=0.03332596272230148
I0305 20:23:36.409443 140626292712640 spec.py:321] Evaluating on the training split.
I0305 20:24:49.530883 140626292712640 spec.py:333] Evaluating on the validation split.
I0305 20:24:51.641297 140626292712640 spec.py:349] Evaluating on the test split.
I0305 20:24:53.546189 140626292712640 submission_runner.py:469] Time since start: 4319.03s, 	Step: 15044, 	{'train/accuracy': 0.9907400608062744, 'train/loss': 0.030720708891749382, 'train/mean_average_precision': 0.4013069447816966, 'validation/accuracy': 0.9867216348648071, 'validation/loss': 0.04430534318089485, 'validation/mean_average_precision': 0.2577423415214058, 'validation/num_examples': 43793, 'test/accuracy': 0.9858533143997192, 'test/loss': 0.04703787714242935, 'test/mean_average_precision': 0.248197106573553, 'test/num_examples': 43793, 'score': 3131.580348968506, 'total_duration': 4319.02574467659, 'accumulated_submission_time': 3131.580348968506, 'accumulated_eval_time': 1186.7891039848328, 'accumulated_logging_time': 0.24853157997131348}
I0305 20:24:53.556608 140484325295872 logging_writer.py:48] [15044] accumulated_eval_time=1186.79, accumulated_logging_time=0.248532, accumulated_submission_time=3131.58, global_step=15044, preemption_count=0, score=3131.58, test/accuracy=0.985853, test/loss=0.0470379, test/mean_average_precision=0.248197, test/num_examples=43793, total_duration=4319.03, train/accuracy=0.99074, train/loss=0.0307207, train/mean_average_precision=0.401307, validation/accuracy=0.986722, validation/loss=0.0443053, validation/mean_average_precision=0.257742, validation/num_examples=43793
I0305 20:25:05.851218 140484333688576 logging_writer.py:48] [15100] global_step=15100, grad_norm=0.04516207054257393, loss=0.03190538287162781
I0305 20:25:26.883505 140484325295872 logging_writer.py:48] [15200] global_step=15200, grad_norm=0.06081295758485794, loss=0.03601916506886482
I0305 20:25:47.785673 140484333688576 logging_writer.py:48] [15300] global_step=15300, grad_norm=0.08636204153299332, loss=0.03741708770394325
I0305 20:26:08.702724 140484325295872 logging_writer.py:48] [15400] global_step=15400, grad_norm=0.05392959341406822, loss=0.035435233265161514
I0305 20:26:29.580980 140484333688576 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.053039271384477615, loss=0.03767969086766243
I0305 20:26:50.296947 140484325295872 logging_writer.py:48] [15600] global_step=15600, grad_norm=0.061173684895038605, loss=0.03521931171417236
I0305 20:27:11.049282 140484333688576 logging_writer.py:48] [15700] global_step=15700, grad_norm=0.04853437840938568, loss=0.0343644954264164
I0305 20:27:32.184795 140484325295872 logging_writer.py:48] [15800] global_step=15800, grad_norm=0.05739567428827286, loss=0.035295646637678146
I0305 20:27:53.086040 140484333688576 logging_writer.py:48] [15900] global_step=15900, grad_norm=0.07852136343717575, loss=0.038009196519851685
I0305 20:28:13.880680 140484325295872 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.050118282437324524, loss=0.03281637653708458
I0305 20:28:34.846430 140484333688576 logging_writer.py:48] [16100] global_step=16100, grad_norm=0.05163274705410004, loss=0.033809956163167953
I0305 20:28:53.729711 140626292712640 spec.py:321] Evaluating on the training split.
I0305 20:30:06.963639 140626292712640 spec.py:333] Evaluating on the validation split.
I0305 20:30:09.066975 140626292712640 spec.py:349] Evaluating on the test split.
I0305 20:30:10.983512 140626292712640 submission_runner.py:469] Time since start: 4636.46s, 	Step: 16193, 	{'train/accuracy': 0.9905843734741211, 'train/loss': 0.031096139922738075, 'train/mean_average_precision': 0.3969842596998022, 'validation/accuracy': 0.9867358207702637, 'validation/loss': 0.04470790922641754, 'validation/mean_average_precision': 0.2621638331407883, 'validation/num_examples': 43793, 'test/accuracy': 0.9859779477119446, 'test/loss': 0.04717910662293434, 'test/mean_average_precision': 0.2546289583238784, 'test/num_examples': 43793, 'score': 3371.7145071029663, 'total_duration': 4636.463050365448, 'accumulated_submission_time': 3371.7145071029663, 'accumulated_eval_time': 1264.0427362918854, 'accumulated_logging_time': 0.2681407928466797}
I0305 20:30:10.994385 140484325295872 logging_writer.py:48] [16193] accumulated_eval_time=1264.04, accumulated_logging_time=0.268141, accumulated_submission_time=3371.71, global_step=16193, preemption_count=0, score=3371.71, test/accuracy=0.985978, test/loss=0.0471791, test/mean_average_precision=0.254629, test/num_examples=43793, total_duration=4636.46, train/accuracy=0.990584, train/loss=0.0310961, train/mean_average_precision=0.396984, validation/accuracy=0.986736, validation/loss=0.0447079, validation/mean_average_precision=0.262164, validation/num_examples=43793
I0305 20:30:12.721323 140484333688576 logging_writer.py:48] [16200] global_step=16200, grad_norm=0.06722810119390488, loss=0.03687118738889694
I0305 20:30:33.722347 140484325295872 logging_writer.py:48] [16300] global_step=16300, grad_norm=0.058505330234766006, loss=0.033047519624233246
I0305 20:30:55.005548 140484333688576 logging_writer.py:48] [16400] global_step=16400, grad_norm=0.06443404406309128, loss=0.037204768508672714
I0305 20:31:16.191453 140484325295872 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.05167686566710472, loss=0.033646803349256516
I0305 20:31:37.464099 140484333688576 logging_writer.py:48] [16600] global_step=16600, grad_norm=0.05247700959444046, loss=0.03454050421714783
I0305 20:31:58.531765 140484325295872 logging_writer.py:48] [16700] global_step=16700, grad_norm=0.07572619616985321, loss=0.034254040569067
I0305 20:32:20.042767 140484333688576 logging_writer.py:48] [16800] global_step=16800, grad_norm=0.0644066259264946, loss=0.03542264550924301
I0305 20:32:41.362996 140484325295872 logging_writer.py:48] [16900] global_step=16900, grad_norm=0.057110484689474106, loss=0.034777168184518814
I0305 20:33:02.325146 140484333688576 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.05790536105632782, loss=0.03658285364508629
I0305 20:33:23.175116 140484325295872 logging_writer.py:48] [17100] global_step=17100, grad_norm=0.0737263560295105, loss=0.036994438618421555
I0305 20:33:44.366368 140484333688576 logging_writer.py:48] [17200] global_step=17200, grad_norm=0.057750191539525986, loss=0.03129415586590767
I0305 20:34:05.497667 140484325295872 logging_writer.py:48] [17300] global_step=17300, grad_norm=0.047537848353385925, loss=0.03248513117432594
I0305 20:34:11.139911 140626292712640 spec.py:321] Evaluating on the training split.
I0305 20:35:23.236258 140626292712640 spec.py:333] Evaluating on the validation split.
I0305 20:35:25.184413 140626292712640 spec.py:349] Evaluating on the test split.
I0305 20:35:27.077858 140626292712640 submission_runner.py:469] Time since start: 4952.56s, 	Step: 17328, 	{'train/accuracy': 0.9911754131317139, 'train/loss': 0.028882037848234177, 'train/mean_average_precision': 0.4522144656967047, 'validation/accuracy': 0.9868113398551941, 'validation/loss': 0.04432009160518646, 'validation/mean_average_precision': 0.262738298096607, 'validation/num_examples': 43793, 'test/accuracy': 0.9859063625335693, 'test/loss': 0.047086577862501144, 'test/mean_average_precision': 0.24627107605311116, 'test/num_examples': 43793, 'score': 3611.812557697296, 'total_duration': 4952.557509183884, 'accumulated_submission_time': 3611.812557697296, 'accumulated_eval_time': 1339.9806487560272, 'accumulated_logging_time': 0.29499173164367676}
I0305 20:35:27.088132 140484333688576 logging_writer.py:48] [17328] accumulated_eval_time=1339.98, accumulated_logging_time=0.294992, accumulated_submission_time=3611.81, global_step=17328, preemption_count=0, score=3611.81, test/accuracy=0.985906, test/loss=0.0470866, test/mean_average_precision=0.246271, test/num_examples=43793, total_duration=4952.56, train/accuracy=0.991175, train/loss=0.028882, train/mean_average_precision=0.452214, validation/accuracy=0.986811, validation/loss=0.0443201, validation/mean_average_precision=0.262738, validation/num_examples=43793
I0305 20:35:42.167504 140484325295872 logging_writer.py:48] [17400] global_step=17400, grad_norm=0.07497691363096237, loss=0.03601006418466568
I0305 20:36:03.018183 140484333688576 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.07306354492902756, loss=0.03403483331203461
I0305 20:36:24.037756 140484325295872 logging_writer.py:48] [17600] global_step=17600, grad_norm=0.11206138879060745, loss=0.035193584859371185
I0305 20:36:44.826665 140484333688576 logging_writer.py:48] [17700] global_step=17700, grad_norm=0.06174561008810997, loss=0.03531962260603905
I0305 20:37:05.642894 140484325295872 logging_writer.py:48] [17800] global_step=17800, grad_norm=0.1039545089006424, loss=0.03132987022399902
I0305 20:37:26.521089 140484333688576 logging_writer.py:48] [17900] global_step=17900, grad_norm=0.06384282559156418, loss=0.03453091159462929
I0305 20:37:47.546365 140484325295872 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.1092543974518776, loss=0.03388815373182297
I0305 20:38:08.274874 140484333688576 logging_writer.py:48] [18100] global_step=18100, grad_norm=0.0617634579539299, loss=0.03790809214115143
I0305 20:38:28.939105 140484325295872 logging_writer.py:48] [18200] global_step=18200, grad_norm=0.051656901836395264, loss=0.030968282371759415
I0305 20:38:49.799519 140484333688576 logging_writer.py:48] [18300] global_step=18300, grad_norm=0.08723177015781403, loss=0.03160923346877098
I0305 20:39:10.547861 140484325295872 logging_writer.py:48] [18400] global_step=18400, grad_norm=0.06169683113694191, loss=0.03210197016596794
I0305 20:39:27.118511 140626292712640 spec.py:321] Evaluating on the training split.
I0305 20:40:37.799222 140626292712640 spec.py:333] Evaluating on the validation split.
I0305 20:40:39.768778 140626292712640 spec.py:349] Evaluating on the test split.
I0305 20:40:41.666756 140626292712640 submission_runner.py:469] Time since start: 5267.15s, 	Step: 18481, 	{'train/accuracy': 0.9908531904220581, 'train/loss': 0.029987307265400887, 'train/mean_average_precision': 0.4154749120014567, 'validation/accuracy': 0.9867143034934998, 'validation/loss': 0.04461009055376053, 'validation/mean_average_precision': 0.2608074419654577, 'validation/num_examples': 43793, 'test/accuracy': 0.9859265685081482, 'test/loss': 0.04705549776554108, 'test/mean_average_precision': 0.2517894353220968, 'test/num_examples': 43793, 'score': 3851.803772687912, 'total_duration': 5267.146325111389, 'accumulated_submission_time': 3851.803772687912, 'accumulated_eval_time': 1414.528757095337, 'accumulated_logging_time': 0.3140878677368164}
I0305 20:40:41.677887 140484333688576 logging_writer.py:48] [18481] accumulated_eval_time=1414.53, accumulated_logging_time=0.314088, accumulated_submission_time=3851.8, global_step=18481, preemption_count=0, score=3851.8, test/accuracy=0.985927, test/loss=0.0470555, test/mean_average_precision=0.251789, test/num_examples=43793, total_duration=5267.15, train/accuracy=0.990853, train/loss=0.0299873, train/mean_average_precision=0.415475, validation/accuracy=0.986714, validation/loss=0.0446101, validation/mean_average_precision=0.260807, validation/num_examples=43793
I0305 20:40:45.926975 140484325295872 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.0872889831662178, loss=0.03912174701690674
I0305 20:41:06.559156 140484333688576 logging_writer.py:48] [18600] global_step=18600, grad_norm=0.06937096267938614, loss=0.033894132822752
I0305 20:41:27.222971 140484325295872 logging_writer.py:48] [18700] global_step=18700, grad_norm=0.07336843758821487, loss=0.03395320847630501
I0305 20:41:48.612531 140484333688576 logging_writer.py:48] [18800] global_step=18800, grad_norm=0.07493820786476135, loss=0.03806277737021446
I0305 20:42:09.338610 140484325295872 logging_writer.py:48] [18900] global_step=18900, grad_norm=0.06628969311714172, loss=0.03494466468691826
I0305 20:42:30.021565 140484333688576 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.060264941304922104, loss=0.033833373337984085
I0305 20:42:50.849751 140484325295872 logging_writer.py:48] [19100] global_step=19100, grad_norm=0.06368356198072433, loss=0.036619994789361954
I0305 20:43:11.979304 140484333688576 logging_writer.py:48] [19200] global_step=19200, grad_norm=0.07359352707862854, loss=0.035751789808273315
I0305 20:43:32.800485 140484325295872 logging_writer.py:48] [19300] global_step=19300, grad_norm=0.09096365422010422, loss=0.03477779030799866
I0305 20:43:53.634325 140484333688576 logging_writer.py:48] [19400] global_step=19400, grad_norm=0.07098237425088882, loss=0.030773336067795753
I0305 20:44:14.498405 140484325295872 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.0895630419254303, loss=0.03690566122531891
I0305 20:44:35.674206 140484333688576 logging_writer.py:48] [19600] global_step=19600, grad_norm=0.06563076376914978, loss=0.03300417587161064
I0305 20:44:41.705260 140626292712640 spec.py:321] Evaluating on the training split.
I0305 20:45:53.655229 140626292712640 spec.py:333] Evaluating on the validation split.
I0305 20:45:55.623803 140626292712640 spec.py:349] Evaluating on the test split.
I0305 20:45:57.528241 140626292712640 submission_runner.py:469] Time since start: 5583.01s, 	Step: 19630, 	{'train/accuracy': 0.9913822412490845, 'train/loss': 0.028417296707630157, 'train/mean_average_precision': 0.45791254130624093, 'validation/accuracy': 0.9867870211601257, 'validation/loss': 0.04411396011710167, 'validation/mean_average_precision': 0.26399632994906136, 'validation/num_examples': 43793, 'test/accuracy': 0.9858823418617249, 'test/loss': 0.046861011534929276, 'test/mean_average_precision': 0.2537314853081531, 'test/num_examples': 43793, 'score': 4091.7909755706787, 'total_duration': 5583.007893562317, 'accumulated_submission_time': 4091.7909755706787, 'accumulated_eval_time': 1490.351684808731, 'accumulated_logging_time': 0.3346543312072754}
I0305 20:45:57.539264 140484325295872 logging_writer.py:48] [19630] accumulated_eval_time=1490.35, accumulated_logging_time=0.334654, accumulated_submission_time=4091.79, global_step=19630, preemption_count=0, score=4091.79, test/accuracy=0.985882, test/loss=0.046861, test/mean_average_precision=0.253731, test/num_examples=43793, total_duration=5583.01, train/accuracy=0.991382, train/loss=0.0284173, train/mean_average_precision=0.457913, validation/accuracy=0.986787, validation/loss=0.044114, validation/mean_average_precision=0.263996, validation/num_examples=43793
I0305 20:46:12.547762 140484333688576 logging_writer.py:48] [19700] global_step=19700, grad_norm=0.07406114786863327, loss=0.035817455500364304
I0305 20:46:33.452699 140484325295872 logging_writer.py:48] [19800] global_step=19800, grad_norm=0.060484930872917175, loss=0.03140830621123314
I0305 20:46:53.958745 140484333688576 logging_writer.py:48] [19900] global_step=19900, grad_norm=0.13535574078559875, loss=0.03401238098740578
I0305 20:47:14.530650 140484325295872 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.07040931284427643, loss=0.03346547856926918
I0305 20:47:35.199918 140484333688576 logging_writer.py:48] [20100] global_step=20100, grad_norm=0.16792821884155273, loss=0.03212033584713936
I0305 20:47:55.785923 140484325295872 logging_writer.py:48] [20200] global_step=20200, grad_norm=0.07191967219114304, loss=0.03639805316925049
I0305 20:48:16.830383 140484333688576 logging_writer.py:48] [20300] global_step=20300, grad_norm=0.07406608015298843, loss=0.034838106483221054
I0305 20:48:37.744121 140484325295872 logging_writer.py:48] [20400] global_step=20400, grad_norm=0.08723261207342148, loss=0.03563280403614044
I0305 20:48:58.541693 140484333688576 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.11713762581348419, loss=0.02917703054845333
I0305 20:49:19.224573 140484325295872 logging_writer.py:48] [20600] global_step=20600, grad_norm=0.06319144368171692, loss=0.033398326486349106
I0305 20:49:39.953106 140484333688576 logging_writer.py:48] [20700] global_step=20700, grad_norm=0.10764104872941971, loss=0.03262964263558388
I0305 20:49:57.654029 140626292712640 spec.py:321] Evaluating on the training split.
I0305 20:51:10.178695 140626292712640 spec.py:333] Evaluating on the validation split.
I0305 20:51:12.285403 140626292712640 spec.py:349] Evaluating on the test split.
I0305 20:51:14.178001 140626292712640 submission_runner.py:469] Time since start: 5899.66s, 	Step: 20786, 	{'train/accuracy': 0.9910698533058167, 'train/loss': 0.02939610742032528, 'train/mean_average_precision': 0.41853526120415635, 'validation/accuracy': 0.9867720007896423, 'validation/loss': 0.04439274221658707, 'validation/mean_average_precision': 0.26043943106572215, 'validation/num_examples': 43793, 'test/accuracy': 0.9859459400177002, 'test/loss': 0.046998031437397, 'test/mean_average_precision': 0.2526544455502668, 'test/num_examples': 43793, 'score': 4331.866239309311, 'total_duration': 5899.657552480698, 'accumulated_submission_time': 4331.866239309311, 'accumulated_eval_time': 1566.8755052089691, 'accumulated_logging_time': 0.35552501678466797}
I0305 20:51:14.189532 140484325295872 logging_writer.py:48] [20786] accumulated_eval_time=1566.88, accumulated_logging_time=0.355525, accumulated_submission_time=4331.87, global_step=20786, preemption_count=0, score=4331.87, test/accuracy=0.985946, test/loss=0.046998, test/mean_average_precision=0.252654, test/num_examples=43793, total_duration=5899.66, train/accuracy=0.99107, train/loss=0.0293961, train/mean_average_precision=0.418535, validation/accuracy=0.986772, validation/loss=0.0443927, validation/mean_average_precision=0.260439, validation/num_examples=43793
I0305 20:51:17.372276 140484333688576 logging_writer.py:48] [20800] global_step=20800, grad_norm=0.08316120505332947, loss=0.03834160789847374
I0305 20:51:38.160557 140484325295872 logging_writer.py:48] [20900] global_step=20900, grad_norm=0.06947442144155502, loss=0.03533010929822922
I0305 20:51:59.511631 140484333688576 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.10232274979352951, loss=0.031860265880823135
I0305 20:52:20.710918 140484325295872 logging_writer.py:48] [21100] global_step=21100, grad_norm=0.07477682828903198, loss=0.03417356684803963
I0305 20:52:42.103674 140484333688576 logging_writer.py:48] [21200] global_step=21200, grad_norm=0.07323788106441498, loss=0.03243695944547653
I0305 20:53:03.308208 140484325295872 logging_writer.py:48] [21300] global_step=21300, grad_norm=0.06483811140060425, loss=0.031552016735076904
I0305 20:53:24.596054 140484333688576 logging_writer.py:48] [21400] global_step=21400, grad_norm=0.08376651257276535, loss=0.030707040801644325
I0305 20:53:45.551943 140484325295872 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.08730568736791611, loss=0.02919600158929825
I0305 20:54:06.421233 140484333688576 logging_writer.py:48] [21600] global_step=21600, grad_norm=0.08680270612239838, loss=0.03052023984491825
I0305 20:54:27.353323 140484325295872 logging_writer.py:48] [21700] global_step=21700, grad_norm=0.06287133693695068, loss=0.030461322516202927
I0305 20:54:48.409227 140484333688576 logging_writer.py:48] [21800] global_step=21800, grad_norm=0.06898578256368637, loss=0.03305478021502495
I0305 20:55:09.288313 140484325295872 logging_writer.py:48] [21900] global_step=21900, grad_norm=0.07370644807815552, loss=0.03059946745634079
I0305 20:55:14.311393 140626292712640 spec.py:321] Evaluating on the training split.
I0305 20:56:25.949989 140626292712640 spec.py:333] Evaluating on the validation split.
I0305 20:56:27.912371 140626292712640 spec.py:349] Evaluating on the test split.
I0305 20:56:29.805098 140626292712640 submission_runner.py:469] Time since start: 6215.28s, 	Step: 21925, 	{'train/accuracy': 0.9915573000907898, 'train/loss': 0.027721866965293884, 'train/mean_average_precision': 0.47724112835895405, 'validation/accuracy': 0.9868429899215698, 'validation/loss': 0.04434831067919731, 'validation/mean_average_precision': 0.26843524902494664, 'validation/num_examples': 43793, 'test/accuracy': 0.986050009727478, 'test/loss': 0.04705188423395157, 'test/mean_average_precision': 0.2641816032352969, 'test/num_examples': 43793, 'score': 4571.949301481247, 'total_duration': 6215.284602880478, 'accumulated_submission_time': 4571.949301481247, 'accumulated_eval_time': 1642.3690094947815, 'accumulated_logging_time': 0.3762385845184326}
I0305 20:56:29.816804 140484333688576 logging_writer.py:48] [21925] accumulated_eval_time=1642.37, accumulated_logging_time=0.376239, accumulated_submission_time=4571.95, global_step=21925, preemption_count=0, score=4571.95, test/accuracy=0.98605, test/loss=0.0470519, test/mean_average_precision=0.264182, test/num_examples=43793, total_duration=6215.28, train/accuracy=0.991557, train/loss=0.0277219, train/mean_average_precision=0.477241, validation/accuracy=0.986843, validation/loss=0.0443483, validation/mean_average_precision=0.268435, validation/num_examples=43793
I0305 20:56:45.666867 140484325295872 logging_writer.py:48] [22000] global_step=22000, grad_norm=0.07760918140411377, loss=0.031008200719952583
I0305 20:57:06.534588 140484333688576 logging_writer.py:48] [22100] global_step=22100, grad_norm=0.09171703457832336, loss=0.03331908583641052
I0305 20:57:27.161718 140484325295872 logging_writer.py:48] [22200] global_step=22200, grad_norm=0.091006800532341, loss=0.033947739750146866
I0305 20:57:48.105843 140484333688576 logging_writer.py:48] [22300] global_step=22300, grad_norm=0.10408715903759003, loss=0.028082218021154404
I0305 20:58:09.349889 140484325295872 logging_writer.py:48] [22400] global_step=22400, grad_norm=0.10170159488916397, loss=0.03400290012359619
I0305 20:58:30.579027 140484333688576 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.07987942546606064, loss=0.030809009447693825
I0305 20:58:51.423537 140484325295872 logging_writer.py:48] [22600] global_step=22600, grad_norm=0.07435590028762817, loss=0.033196330070495605
I0305 20:59:12.220334 140484333688576 logging_writer.py:48] [22700] global_step=22700, grad_norm=0.08247795701026917, loss=0.03428037464618683
I0305 20:59:33.182406 140484325295872 logging_writer.py:48] [22800] global_step=22800, grad_norm=0.07396998256444931, loss=0.03377325460314751
I0305 20:59:53.644837 140484333688576 logging_writer.py:48] [22900] global_step=22900, grad_norm=0.09263540804386139, loss=0.030985118821263313
I0305 21:00:14.351181 140484325295872 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.07981257885694504, loss=0.033523645251989365
I0305 21:00:29.969392 140626292712640 spec.py:321] Evaluating on the training split.
I0305 21:01:41.190386 140626292712640 spec.py:333] Evaluating on the validation split.
I0305 21:01:43.186882 140626292712640 spec.py:349] Evaluating on the test split.
I0305 21:01:45.152772 140626292712640 submission_runner.py:469] Time since start: 6530.63s, 	Step: 23075, 	{'train/accuracy': 0.9913856387138367, 'train/loss': 0.028239788487553596, 'train/mean_average_precision': 0.4596777676613564, 'validation/accuracy': 0.9868271946907043, 'validation/loss': 0.044665999710559845, 'validation/mean_average_precision': 0.2763591089913163, 'validation/num_examples': 43793, 'test/accuracy': 0.9859430193901062, 'test/loss': 0.04744134098291397, 'test/mean_average_precision': 0.25927604024938977, 'test/num_examples': 43793, 'score': 4812.061283826828, 'total_duration': 6530.632303953171, 'accumulated_submission_time': 4812.061283826828, 'accumulated_eval_time': 1717.5522155761719, 'accumulated_logging_time': 0.3970015048980713}
I0305 21:01:45.164189 140484333688576 logging_writer.py:48] [23075] accumulated_eval_time=1717.55, accumulated_logging_time=0.397002, accumulated_submission_time=4812.06, global_step=23075, preemption_count=0, score=4812.06, test/accuracy=0.985943, test/loss=0.0474413, test/mean_average_precision=0.259276, test/num_examples=43793, total_duration=6530.63, train/accuracy=0.991386, train/loss=0.0282398, train/mean_average_precision=0.459678, validation/accuracy=0.986827, validation/loss=0.044666, validation/mean_average_precision=0.276359, validation/num_examples=43793
I0305 21:01:50.655898 140484325295872 logging_writer.py:48] [23100] global_step=23100, grad_norm=0.09061378985643387, loss=0.032406799495220184
I0305 21:02:11.432995 140484333688576 logging_writer.py:48] [23200] global_step=23200, grad_norm=0.08659143000841141, loss=0.03483060747385025
I0305 21:02:32.087261 140484325295872 logging_writer.py:48] [23300] global_step=23300, grad_norm=0.10063698887825012, loss=0.031225651502609253
I0305 21:02:52.956925 140484333688576 logging_writer.py:48] [23400] global_step=23400, grad_norm=0.0967491939663887, loss=0.031058352440595627
I0305 21:03:13.822109 140484325295872 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.09215626120567322, loss=0.035497698932886124
I0305 21:03:35.183420 140484333688576 logging_writer.py:48] [23600] global_step=23600, grad_norm=0.06321858614683151, loss=0.03224574029445648
I0305 21:03:56.224827 140484325295872 logging_writer.py:48] [23700] global_step=23700, grad_norm=0.08468490839004517, loss=0.03522495925426483
I0305 21:04:17.425573 140484333688576 logging_writer.py:48] [23800] global_step=23800, grad_norm=0.10719248652458191, loss=0.03332139551639557
I0305 21:04:38.638751 140484325295872 logging_writer.py:48] [23900] global_step=23900, grad_norm=0.10032238066196442, loss=0.030884569510817528
I0305 21:04:59.724099 140484333688576 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.11544457077980042, loss=0.031691908836364746
I0305 21:05:20.858798 140484325295872 logging_writer.py:48] [24100] global_step=24100, grad_norm=0.08370896428823471, loss=0.03112226352095604
I0305 21:05:41.980612 140484333688576 logging_writer.py:48] [24200] global_step=24200, grad_norm=0.08976063132286072, loss=0.03218437358736992
I0305 21:05:45.180687 140626292712640 spec.py:321] Evaluating on the training split.
I0305 21:06:54.363859 140626292712640 spec.py:333] Evaluating on the validation split.
I0305 21:06:56.481370 140626292712640 spec.py:349] Evaluating on the test split.
I0305 21:06:58.534756 140626292712640 submission_runner.py:469] Time since start: 6844.01s, 	Step: 24216, 	{'train/accuracy': 0.9916201233863831, 'train/loss': 0.027259131893515587, 'train/mean_average_precision': 0.4873494983231855, 'validation/accuracy': 0.9869607090950012, 'validation/loss': 0.04439434036612511, 'validation/mean_average_precision': 0.2673983363692704, 'validation/num_examples': 43793, 'test/accuracy': 0.9860592484474182, 'test/loss': 0.04723324254155159, 'test/mean_average_precision': 0.2590070854199632, 'test/num_examples': 43793, 'score': 5052.038455247879, 'total_duration': 6844.014355659485, 'accumulated_submission_time': 5052.038455247879, 'accumulated_eval_time': 1790.9061782360077, 'accumulated_logging_time': 0.41887831687927246}
I0305 21:06:58.547909 140484325295872 logging_writer.py:48] [24216] accumulated_eval_time=1790.91, accumulated_logging_time=0.418878, accumulated_submission_time=5052.04, global_step=24216, preemption_count=0, score=5052.04, test/accuracy=0.986059, test/loss=0.0472332, test/mean_average_precision=0.259007, test/num_examples=43793, total_duration=6844.01, train/accuracy=0.99162, train/loss=0.0272591, train/mean_average_precision=0.487349, validation/accuracy=0.986961, validation/loss=0.0443943, validation/mean_average_precision=0.267398, validation/num_examples=43793
I0305 21:07:16.102613 140484333688576 logging_writer.py:48] [24300] global_step=24300, grad_norm=0.09949413686990738, loss=0.03456128388643265
I0305 21:07:37.008267 140484325295872 logging_writer.py:48] [24400] global_step=24400, grad_norm=0.10712360590696335, loss=0.03505268320441246
I0305 21:07:58.048639 140484333688576 logging_writer.py:48] [24500] global_step=24500, grad_norm=0.10780741274356842, loss=0.03218701854348183
I0305 21:08:18.936870 140484325295872 logging_writer.py:48] [24600] global_step=24600, grad_norm=0.09887653589248657, loss=0.03155554085969925
I0305 21:08:39.975382 140484333688576 logging_writer.py:48] [24700] global_step=24700, grad_norm=0.1285523921251297, loss=0.03112609311938286
I0305 21:09:01.204553 140484325295872 logging_writer.py:48] [24800] global_step=24800, grad_norm=0.09629786014556885, loss=0.03299427777528763
I0305 21:09:22.101812 140484333688576 logging_writer.py:48] [24900] global_step=24900, grad_norm=0.12145258486270905, loss=0.03110061027109623
I0305 21:09:43.131953 140484325295872 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.12068338692188263, loss=0.03241027146577835
I0305 21:10:04.170389 140484333688576 logging_writer.py:48] [25100] global_step=25100, grad_norm=0.08722858130931854, loss=0.031230483204126358
I0305 21:10:25.037324 140484325295872 logging_writer.py:48] [25200] global_step=25200, grad_norm=0.09900974482297897, loss=0.03254074975848198
I0305 21:10:45.972372 140484333688576 logging_writer.py:48] [25300] global_step=25300, grad_norm=0.09385903179645538, loss=0.03102593496441841
I0305 21:10:58.568250 140626292712640 spec.py:321] Evaluating on the training split.
I0305 21:12:08.480094 140626292712640 spec.py:333] Evaluating on the validation split.
I0305 21:12:10.418307 140626292712640 spec.py:349] Evaluating on the test split.
I0305 21:12:12.325911 140626292712640 submission_runner.py:469] Time since start: 7157.81s, 	Step: 25362, 	{'train/accuracy': 0.9917541146278381, 'train/loss': 0.026984816417098045, 'train/mean_average_precision': 0.4709887389471783, 'validation/accuracy': 0.9869915843009949, 'validation/loss': 0.044340845197439194, 'validation/mean_average_precision': 0.2710634574902589, 'validation/num_examples': 43793, 'test/accuracy': 0.9861177802085876, 'test/loss': 0.047006674110889435, 'test/mean_average_precision': 0.2585957858772502, 'test/num_examples': 43793, 'score': 5292.018476009369, 'total_duration': 7157.8054530620575, 'accumulated_submission_time': 5292.018476009369, 'accumulated_eval_time': 1864.663691997528, 'accumulated_logging_time': 0.4414689540863037}
I0305 21:12:12.337376 140484325295872 logging_writer.py:48] [25362] accumulated_eval_time=1864.66, accumulated_logging_time=0.441469, accumulated_submission_time=5292.02, global_step=25362, preemption_count=0, score=5292.02, test/accuracy=0.986118, test/loss=0.0470067, test/mean_average_precision=0.258596, test/num_examples=43793, total_duration=7157.81, train/accuracy=0.991754, train/loss=0.0269848, train/mean_average_precision=0.470989, validation/accuracy=0.986992, validation/loss=0.0443408, validation/mean_average_precision=0.271063, validation/num_examples=43793
I0305 21:12:20.556739 140484333688576 logging_writer.py:48] [25400] global_step=25400, grad_norm=0.08716965466737747, loss=0.032984670251607895
I0305 21:12:41.379745 140484325295872 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.09662287682294846, loss=0.029329171404242516
I0305 21:13:02.443949 140484333688576 logging_writer.py:48] [25600] global_step=25600, grad_norm=0.11315760016441345, loss=0.03159927949309349
I0305 21:13:23.601804 140484325295872 logging_writer.py:48] [25700] global_step=25700, grad_norm=0.07694714516401291, loss=0.029734410345554352
I0305 21:13:44.646853 140484333688576 logging_writer.py:48] [25800] global_step=25800, grad_norm=0.09932474792003632, loss=0.031236320734024048
I0305 21:14:05.998315 140484325295872 logging_writer.py:48] [25900] global_step=25900, grad_norm=0.12479957193136215, loss=0.030953485518693924
I0305 21:14:26.898360 140484333688576 logging_writer.py:48] [26000] global_step=26000, grad_norm=0.148308664560318, loss=0.03295474126935005
I0305 21:14:47.939534 140484325295872 logging_writer.py:48] [26100] global_step=26100, grad_norm=0.0983450785279274, loss=0.030113141983747482
I0305 21:15:08.741075 140484333688576 logging_writer.py:48] [26200] global_step=26200, grad_norm=0.09393012523651123, loss=0.030721457675099373
I0305 21:15:29.982708 140484325295872 logging_writer.py:48] [26300] global_step=26300, grad_norm=0.11573564261198044, loss=0.03281456604599953
I0305 21:15:51.004218 140484333688576 logging_writer.py:48] [26400] global_step=26400, grad_norm=0.10706844925880432, loss=0.030189499258995056
I0305 21:16:12.249079 140484325295872 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.11445504426956177, loss=0.02859276346862316
I0305 21:16:12.461089 140626292712640 spec.py:321] Evaluating on the training split.
I0305 21:17:21.728315 140626292712640 spec.py:333] Evaluating on the validation split.
I0305 21:17:23.704051 140626292712640 spec.py:349] Evaluating on the test split.
I0305 21:17:25.607987 140626292712640 submission_runner.py:469] Time since start: 7471.09s, 	Step: 26502, 	{'train/accuracy': 0.9918228983879089, 'train/loss': 0.026586053892970085, 'train/mean_average_precision': 0.5006675419963024, 'validation/accuracy': 0.986851155757904, 'validation/loss': 0.04470193758606911, 'validation/mean_average_precision': 0.27152355704414244, 'validation/num_examples': 43793, 'test/accuracy': 0.9859699606895447, 'test/loss': 0.04762403294444084, 'test/mean_average_precision': 0.2543782821620701, 'test/num_examples': 43793, 'score': 5532.096855640411, 'total_duration': 7471.087512969971, 'accumulated_submission_time': 5532.096855640411, 'accumulated_eval_time': 1937.8104083538055, 'accumulated_logging_time': 0.46369171142578125}
I0305 21:17:25.619299 140484333688576 logging_writer.py:48] [26502] accumulated_eval_time=1937.81, accumulated_logging_time=0.463692, accumulated_submission_time=5532.1, global_step=26502, preemption_count=0, score=5532.1, test/accuracy=0.98597, test/loss=0.047624, test/mean_average_precision=0.254378, test/num_examples=43793, total_duration=7471.09, train/accuracy=0.991823, train/loss=0.0265861, train/mean_average_precision=0.500668, validation/accuracy=0.986851, validation/loss=0.0447019, validation/mean_average_precision=0.271524, validation/num_examples=43793
I0305 21:17:46.710075 140484325295872 logging_writer.py:48] [26600] global_step=26600, grad_norm=0.1044059619307518, loss=0.0297255702316761
I0305 21:18:08.178669 140484333688576 logging_writer.py:48] [26700] global_step=26700, grad_norm=0.1094897985458374, loss=0.02599046751856804
I0305 21:18:29.795327 140484325295872 logging_writer.py:48] [26800] global_step=26800, grad_norm=0.07955702394247055, loss=0.029548702761530876
I0305 21:18:51.036987 140484333688576 logging_writer.py:48] [26900] global_step=26900, grad_norm=0.07928577065467834, loss=0.02945021167397499
I0305 21:19:12.009678 140484325295872 logging_writer.py:48] [27000] global_step=27000, grad_norm=0.07729792594909668, loss=0.02932988665997982
I0305 21:19:33.096912 140484333688576 logging_writer.py:48] [27100] global_step=27100, grad_norm=0.0884476751089096, loss=0.02854491025209427
I0305 21:19:54.028033 140484325295872 logging_writer.py:48] [27200] global_step=27200, grad_norm=0.10496444255113602, loss=0.03394408896565437
I0305 21:20:14.761366 140484333688576 logging_writer.py:48] [27300] global_step=27300, grad_norm=0.1070830374956131, loss=0.03578582778573036
I0305 21:20:35.693313 140484325295872 logging_writer.py:48] [27400] global_step=27400, grad_norm=0.10563092678785324, loss=0.030313340947031975
I0305 21:20:56.498336 140484333688576 logging_writer.py:48] [27500] global_step=27500, grad_norm=0.10662953555583954, loss=0.032713644206523895
I0305 21:21:17.641834 140484325295872 logging_writer.py:48] [27600] global_step=27600, grad_norm=0.08484632521867752, loss=0.029339799657464027
I0305 21:21:25.625810 140626292712640 spec.py:321] Evaluating on the training split.
I0305 21:22:37.925046 140626292712640 spec.py:333] Evaluating on the validation split.
I0305 21:22:39.883305 140626292712640 spec.py:349] Evaluating on the test split.
I0305 21:22:41.846296 140626292712640 submission_runner.py:469] Time since start: 7787.33s, 	Step: 27639, 	{'train/accuracy': 0.9920358657836914, 'train/loss': 0.02584187313914299, 'train/mean_average_precision': 0.5136132396142394, 'validation/accuracy': 0.9868263602256775, 'validation/loss': 0.045205455273389816, 'validation/mean_average_precision': 0.2640618503547045, 'validation/num_examples': 43793, 'test/accuracy': 0.9859623908996582, 'test/loss': 0.048152439296245575, 'test/mean_average_precision': 0.2594478785245785, 'test/num_examples': 43793, 'score': 5772.063252925873, 'total_duration': 7787.325950145721, 'accumulated_submission_time': 5772.063252925873, 'accumulated_eval_time': 2014.030842781067, 'accumulated_logging_time': 0.48607516288757324}
I0305 21:22:41.858046 140484333688576 logging_writer.py:48] [27639] accumulated_eval_time=2014.03, accumulated_logging_time=0.486075, accumulated_submission_time=5772.06, global_step=27639, preemption_count=0, score=5772.06, test/accuracy=0.985962, test/loss=0.0481524, test/mean_average_precision=0.259448, test/num_examples=43793, total_duration=7787.33, train/accuracy=0.992036, train/loss=0.0258419, train/mean_average_precision=0.513613, validation/accuracy=0.986826, validation/loss=0.0452055, validation/mean_average_precision=0.264062, validation/num_examples=43793
I0305 21:22:54.998000 140484325295872 logging_writer.py:48] [27700] global_step=27700, grad_norm=0.10156570374965668, loss=0.028993643820285797
I0305 21:23:15.858385 140484333688576 logging_writer.py:48] [27800] global_step=27800, grad_norm=0.12642616033554077, loss=0.030931897461414337
I0305 21:23:37.222479 140484325295872 logging_writer.py:48] [27900] global_step=27900, grad_norm=0.08866409212350845, loss=0.03240501508116722
I0305 21:23:57.901628 140484333688576 logging_writer.py:48] [28000] global_step=28000, grad_norm=0.1050981879234314, loss=0.036144960671663284
I0305 21:24:18.625489 140484325295872 logging_writer.py:48] [28100] global_step=28100, grad_norm=0.10895001888275146, loss=0.03313034772872925
I0305 21:24:39.546629 140484333688576 logging_writer.py:48] [28200] global_step=28200, grad_norm=0.08790375292301178, loss=0.02873154543340206
I0305 21:25:00.728340 140484325295872 logging_writer.py:48] [28300] global_step=28300, grad_norm=0.09356766939163208, loss=0.0296043548732996
I0305 21:25:21.667947 140484333688576 logging_writer.py:48] [28400] global_step=28400, grad_norm=0.10485883802175522, loss=0.028256623074412346
I0305 21:25:42.846920 140484325295872 logging_writer.py:48] [28500] global_step=28500, grad_norm=0.09723430871963501, loss=0.029135484248399734
I0305 21:26:03.820523 140484333688576 logging_writer.py:48] [28600] global_step=28600, grad_norm=0.09843096882104874, loss=0.03347603231668472
I0305 21:26:25.248216 140484325295872 logging_writer.py:48] [28700] global_step=28700, grad_norm=0.09692582488059998, loss=0.03101409412920475
I0305 21:26:41.875820 140626292712640 spec.py:321] Evaluating on the training split.
I0305 21:27:54.042713 140626292712640 spec.py:333] Evaluating on the validation split.
I0305 21:27:56.176101 140626292712640 spec.py:349] Evaluating on the test split.
I0305 21:27:58.244815 140626292712640 submission_runner.py:469] Time since start: 8103.72s, 	Step: 28780, 	{'train/accuracy': 0.9919737577438354, 'train/loss': 0.02603919990360737, 'train/mean_average_precision': 0.5035246140586245, 'validation/accuracy': 0.9868036508560181, 'validation/loss': 0.045159272849559784, 'validation/mean_average_precision': 0.2677179756955803, 'validation/num_examples': 43793, 'test/accuracy': 0.9860049486160278, 'test/loss': 0.04824861139059067, 'test/mean_average_precision': 0.25463802983495476, 'test/num_examples': 43793, 'score': 6011.83998632431, 'total_duration': 8103.724343299866, 'accumulated_submission_time': 6011.83998632431, 'accumulated_eval_time': 2090.3996601104736, 'accumulated_logging_time': 0.7079660892486572}
I0305 21:27:58.257759 140484333688576 logging_writer.py:48] [28780] accumulated_eval_time=2090.4, accumulated_logging_time=0.707966, accumulated_submission_time=6011.84, global_step=28780, preemption_count=0, score=6011.84, test/accuracy=0.986005, test/loss=0.0482486, test/mean_average_precision=0.254638, test/num_examples=43793, total_duration=8103.72, train/accuracy=0.991974, train/loss=0.0260392, train/mean_average_precision=0.503525, validation/accuracy=0.986804, validation/loss=0.0451593, validation/mean_average_precision=0.267718, validation/num_examples=43793
I0305 21:28:02.637741 140484325295872 logging_writer.py:48] [28800] global_step=28800, grad_norm=0.09737928956747055, loss=0.029120678082108498
I0305 21:28:23.552503 140484333688576 logging_writer.py:48] [28900] global_step=28900, grad_norm=0.10033300518989563, loss=0.02982935681939125
I0305 21:28:44.384385 140484325295872 logging_writer.py:48] [29000] global_step=29000, grad_norm=0.11854178458452225, loss=0.030371978878974915
I0305 21:29:05.649044 140484333688576 logging_writer.py:48] [29100] global_step=29100, grad_norm=0.08930889517068863, loss=0.03009050525724888
I0305 21:29:26.790177 140484325295872 logging_writer.py:48] [29200] global_step=29200, grad_norm=0.09790587425231934, loss=0.02935493364930153
I0305 21:29:47.704279 140484333688576 logging_writer.py:48] [29300] global_step=29300, grad_norm=0.11779043078422546, loss=0.029694488272070885
I0305 21:30:08.318328 140484325295872 logging_writer.py:48] [29400] global_step=29400, grad_norm=0.08759316056966782, loss=0.025654425844550133
I0305 21:30:29.351967 140484333688576 logging_writer.py:48] [29500] global_step=29500, grad_norm=0.09056876599788666, loss=0.029121531173586845
I0305 21:30:50.127416 140484325295872 logging_writer.py:48] [29600] global_step=29600, grad_norm=0.10146627575159073, loss=0.03012795001268387
I0305 21:31:11.154985 140484333688576 logging_writer.py:48] [29700] global_step=29700, grad_norm=0.10367785394191742, loss=0.028446175158023834
I0305 21:31:32.009321 140484325295872 logging_writer.py:48] [29800] global_step=29800, grad_norm=0.09278742223978043, loss=0.02802875265479088
I0305 21:31:53.093843 140484333688576 logging_writer.py:48] [29900] global_step=29900, grad_norm=0.11471075564622879, loss=0.02900206297636032
I0305 21:31:58.407895 140626292712640 spec.py:321] Evaluating on the training split.
I0305 21:33:08.086760 140626292712640 spec.py:333] Evaluating on the validation split.
I0305 21:33:10.015180 140626292712640 spec.py:349] Evaluating on the test split.
I0305 21:33:11.940664 140626292712640 submission_runner.py:469] Time since start: 8417.42s, 	Step: 29926, 	{'train/accuracy': 0.9924474954605103, 'train/loss': 0.024343539029359818, 'train/mean_average_precision': 0.5518010756451396, 'validation/accuracy': 0.9869071245193481, 'validation/loss': 0.044991057366132736, 'validation/mean_average_precision': 0.2729831598031316, 'validation/num_examples': 43793, 'test/accuracy': 0.9860466122627258, 'test/loss': 0.048020150512456894, 'test/mean_average_precision': 0.2602955732366308, 'test/num_examples': 43793, 'score': 6251.950050830841, 'total_duration': 8417.420310735703, 'accumulated_submission_time': 6251.950050830841, 'accumulated_eval_time': 2163.932371854782, 'accumulated_logging_time': 0.7319021224975586}
I0305 21:33:11.953227 140484325295872 logging_writer.py:48] [29926] accumulated_eval_time=2163.93, accumulated_logging_time=0.731902, accumulated_submission_time=6251.95, global_step=29926, preemption_count=0, score=6251.95, test/accuracy=0.986047, test/loss=0.0480202, test/mean_average_precision=0.260296, test/num_examples=43793, total_duration=8417.42, train/accuracy=0.992447, train/loss=0.0243435, train/mean_average_precision=0.551801, validation/accuracy=0.986907, validation/loss=0.0449911, validation/mean_average_precision=0.272983, validation/num_examples=43793
I0305 21:33:27.828680 140484333688576 logging_writer.py:48] [30000] global_step=30000, grad_norm=0.11275892704725266, loss=0.028812119737267494
I0305 21:33:49.033572 140484325295872 logging_writer.py:48] [30100] global_step=30100, grad_norm=0.1134527251124382, loss=0.030772335827350616
I0305 21:34:10.044196 140484333688576 logging_writer.py:48] [30200] global_step=30200, grad_norm=0.09676337987184525, loss=0.028023194521665573
I0305 21:34:31.011316 140484325295872 logging_writer.py:48] [30300] global_step=30300, grad_norm=0.11628939211368561, loss=0.029210295528173447
I0305 21:34:52.110526 140484333688576 logging_writer.py:48] [30400] global_step=30400, grad_norm=0.1448577493429184, loss=0.032207269221544266
I0305 21:35:13.321685 140484325295872 logging_writer.py:48] [30500] global_step=30500, grad_norm=0.10492908954620361, loss=0.029817640781402588
I0305 21:35:34.372877 140484333688576 logging_writer.py:48] [30600] global_step=30600, grad_norm=0.10667911171913147, loss=0.029182979837059975
I0305 21:35:55.623210 140484325295872 logging_writer.py:48] [30700] global_step=30700, grad_norm=0.1266752928495407, loss=0.03466617316007614
I0305 21:36:16.855159 140484333688576 logging_writer.py:48] [30800] global_step=30800, grad_norm=0.14629817008972168, loss=0.027827274054288864
I0305 21:36:37.898118 140484325295872 logging_writer.py:48] [30900] global_step=30900, grad_norm=0.10919999331235886, loss=0.029201634228229523
I0305 21:36:59.077291 140484333688576 logging_writer.py:48] [31000] global_step=31000, grad_norm=0.09460589289665222, loss=0.02845413237810135
I0305 21:37:11.981131 140626292712640 spec.py:321] Evaluating on the training split.
I0305 21:38:17.923044 140626292712640 spec.py:333] Evaluating on the validation split.
I0305 21:38:19.843033 140626292712640 spec.py:349] Evaluating on the test split.
I0305 21:38:21.736461 140626292712640 submission_runner.py:469] Time since start: 8727.22s, 	Step: 31062, 	{'train/accuracy': 0.9922090172767639, 'train/loss': 0.025188639760017395, 'train/mean_average_precision': 0.5256656340450574, 'validation/accuracy': 0.9868621230125427, 'validation/loss': 0.04534360021352768, 'validation/mean_average_precision': 0.2696833580037658, 'validation/num_examples': 43793, 'test/accuracy': 0.9859977960586548, 'test/loss': 0.04850677773356438, 'test/mean_average_precision': 0.2620445328750984, 'test/num_examples': 43793, 'score': 6491.935678720474, 'total_duration': 8727.216063976288, 'accumulated_submission_time': 6491.935678720474, 'accumulated_eval_time': 2233.6875994205475, 'accumulated_logging_time': 0.7535703182220459}
I0305 21:38:21.748443 140484325295872 logging_writer.py:48] [31062] accumulated_eval_time=2233.69, accumulated_logging_time=0.75357, accumulated_submission_time=6491.94, global_step=31062, preemption_count=0, score=6491.94, test/accuracy=0.985998, test/loss=0.0485068, test/mean_average_precision=0.262045, test/num_examples=43793, total_duration=8727.22, train/accuracy=0.992209, train/loss=0.0251886, train/mean_average_precision=0.525666, validation/accuracy=0.986862, validation/loss=0.0453436, validation/mean_average_precision=0.269683, validation/num_examples=43793
I0305 21:38:30.103336 140484333688576 logging_writer.py:48] [31100] global_step=31100, grad_norm=0.12310989201068878, loss=0.0313417948782444
I0305 21:38:50.935998 140484325295872 logging_writer.py:48] [31200] global_step=31200, grad_norm=0.10325439274311066, loss=0.027689944952726364
I0305 21:39:11.983055 140484333688576 logging_writer.py:48] [31300] global_step=31300, grad_norm=0.11037412285804749, loss=0.028643213212490082
I0305 21:39:32.847384 140484325295872 logging_writer.py:48] [31400] global_step=31400, grad_norm=0.09997181594371796, loss=0.025365017354488373
I0305 21:39:54.239855 140484333688576 logging_writer.py:48] [31500] global_step=31500, grad_norm=0.13566765189170837, loss=0.02997845970094204
I0305 21:40:15.216130 140484325295872 logging_writer.py:48] [31600] global_step=31600, grad_norm=0.12507127225399017, loss=0.02983173169195652
I0305 21:40:35.984113 140484333688576 logging_writer.py:48] [31700] global_step=31700, grad_norm=0.09062071889638901, loss=0.025555932894349098
I0305 21:40:57.177807 140484325295872 logging_writer.py:48] [31800] global_step=31800, grad_norm=0.14867810904979706, loss=0.029948672279715538
I0305 21:41:18.300218 140484333688576 logging_writer.py:48] [31900] global_step=31900, grad_norm=0.1059199869632721, loss=0.028600305318832397
I0305 21:41:39.503489 140484325295872 logging_writer.py:48] [32000] global_step=32000, grad_norm=0.12583832442760468, loss=0.028959156945347786
I0305 21:42:00.476012 140484333688576 logging_writer.py:48] [32100] global_step=32100, grad_norm=0.13493824005126953, loss=0.026716317981481552
I0305 21:42:21.188854 140484325295872 logging_writer.py:48] [32200] global_step=32200, grad_norm=0.12074070423841476, loss=0.02778848633170128
I0305 21:42:21.821051 140626292712640 spec.py:321] Evaluating on the training split.
I0305 21:43:34.892481 140626292712640 spec.py:333] Evaluating on the validation split.
I0305 21:43:37.147124 140626292712640 spec.py:349] Evaluating on the test split.
I0305 21:43:39.113245 140626292712640 submission_runner.py:469] Time since start: 9044.59s, 	Step: 32204, 	{'train/accuracy': 0.9929726123809814, 'train/loss': 0.022708307951688766, 'train/mean_average_precision': 0.578941784430997, 'validation/accuracy': 0.9868385791778564, 'validation/loss': 0.04554077237844467, 'validation/mean_average_precision': 0.27317436290175606, 'validation/num_examples': 43793, 'test/accuracy': 0.9859535694122314, 'test/loss': 0.04854872077703476, 'test/mean_average_precision': 0.2670333447573229, 'test/num_examples': 43793, 'score': 6731.969523191452, 'total_duration': 9044.592772722244, 'accumulated_submission_time': 6731.969523191452, 'accumulated_eval_time': 2310.9796299934387, 'accumulated_logging_time': 0.7754356861114502}
I0305 21:43:39.125560 140484333688576 logging_writer.py:48] [32204] accumulated_eval_time=2310.98, accumulated_logging_time=0.775436, accumulated_submission_time=6731.97, global_step=32204, preemption_count=0, score=6731.97, test/accuracy=0.985954, test/loss=0.0485487, test/mean_average_precision=0.267033, test/num_examples=43793, total_duration=9044.59, train/accuracy=0.992973, train/loss=0.0227083, train/mean_average_precision=0.578942, validation/accuracy=0.986839, validation/loss=0.0455408, validation/mean_average_precision=0.273174, validation/num_examples=43793
I0305 21:43:59.736762 140484325295872 logging_writer.py:48] [32300] global_step=32300, grad_norm=0.13510848581790924, loss=0.02985946275293827
I0305 21:44:21.002684 140484333688576 logging_writer.py:48] [32400] global_step=32400, grad_norm=0.12063387036323547, loss=0.02988966554403305
I0305 21:44:41.788889 140484325295872 logging_writer.py:48] [32500] global_step=32500, grad_norm=0.1310252696275711, loss=0.028387300670146942
I0305 21:45:03.005347 140484333688576 logging_writer.py:48] [32600] global_step=32600, grad_norm=0.17010334134101868, loss=0.02668541669845581
I0305 21:45:24.313244 140484325295872 logging_writer.py:48] [32700] global_step=32700, grad_norm=0.14244863390922546, loss=0.028813421726226807
I0305 21:45:45.541406 140484333688576 logging_writer.py:48] [32800] global_step=32800, grad_norm=0.10874691605567932, loss=0.028286321088671684
I0305 21:46:06.687062 140484325295872 logging_writer.py:48] [32900] global_step=32900, grad_norm=0.11435513943433762, loss=0.026457084342837334
I0305 21:46:27.868866 140484333688576 logging_writer.py:48] [33000] global_step=33000, grad_norm=0.14027801156044006, loss=0.027120711281895638
I0305 21:46:49.027544 140484325295872 logging_writer.py:48] [33100] global_step=33100, grad_norm=0.1573382318019867, loss=0.026838144287467003
I0305 21:47:10.286524 140484333688576 logging_writer.py:48] [33200] global_step=33200, grad_norm=0.13077686727046967, loss=0.027055084705352783
I0305 21:47:31.166704 140484325295872 logging_writer.py:48] [33300] global_step=33300, grad_norm=0.1125328540802002, loss=0.027234913781285286
I0305 21:47:39.129319 140626292712640 spec.py:321] Evaluating on the training split.
I0305 21:48:48.468985 140626292712640 spec.py:333] Evaluating on the validation split.
I0305 21:48:50.502724 140626292712640 spec.py:349] Evaluating on the test split.
I0305 21:48:52.427936 140626292712640 submission_runner.py:469] Time since start: 9357.91s, 	Step: 33339, 	{'train/accuracy': 0.9925864338874817, 'train/loss': 0.023776967078447342, 'train/mean_average_precision': 0.550771518938405, 'validation/accuracy': 0.9868393540382385, 'validation/loss': 0.04593701288104057, 'validation/mean_average_precision': 0.26922325234352384, 'validation/num_examples': 43793, 'test/accuracy': 0.9860407114028931, 'test/loss': 0.049088094383478165, 'test/mean_average_precision': 0.25755181614528694, 'test/num_examples': 43793, 'score': 6971.9351625442505, 'total_duration': 9357.907524347305, 'accumulated_submission_time': 6971.9351625442505, 'accumulated_eval_time': 2384.278130531311, 'accumulated_logging_time': 0.7969334125518799}
I0305 21:48:52.440248 140484333688576 logging_writer.py:48] [33339] accumulated_eval_time=2384.28, accumulated_logging_time=0.796933, accumulated_submission_time=6971.94, global_step=33339, preemption_count=0, score=6971.94, test/accuracy=0.986041, test/loss=0.0490881, test/mean_average_precision=0.257552, test/num_examples=43793, total_duration=9357.91, train/accuracy=0.992586, train/loss=0.023777, train/mean_average_precision=0.550772, validation/accuracy=0.986839, validation/loss=0.045937, validation/mean_average_precision=0.269223, validation/num_examples=43793
I0305 21:49:05.558772 140484325295872 logging_writer.py:48] [33400] global_step=33400, grad_norm=0.10777223855257034, loss=0.0249505452811718
I0305 21:49:26.500454 140484333688576 logging_writer.py:48] [33500] global_step=33500, grad_norm=0.12656408548355103, loss=0.027631059288978577
I0305 21:49:47.302920 140484325295872 logging_writer.py:48] [33600] global_step=33600, grad_norm=0.10622633248567581, loss=0.02483687549829483
I0305 21:50:08.717463 140484333688576 logging_writer.py:48] [33700] global_step=33700, grad_norm=0.13323114812374115, loss=0.02391432598233223
I0305 21:50:29.993557 140484325295872 logging_writer.py:48] [33800] global_step=33800, grad_norm=0.11270525306463242, loss=0.027294211089611053
I0305 21:50:50.961485 140484333688576 logging_writer.py:48] [33900] global_step=33900, grad_norm=0.12016112357378006, loss=0.028864765539765358
I0305 21:51:11.449476 140484325295872 logging_writer.py:48] [34000] global_step=34000, grad_norm=0.14975784718990326, loss=0.02920209802687168
I0305 21:51:31.950685 140484333688576 logging_writer.py:48] [34100] global_step=34100, grad_norm=0.13171736896038055, loss=0.03443434461951256
I0305 21:51:52.846410 140484325295872 logging_writer.py:48] [34200] global_step=34200, grad_norm=0.13571496307849884, loss=0.02873826213181019
I0305 21:52:13.848984 140484333688576 logging_writer.py:48] [34300] global_step=34300, grad_norm=0.12952852249145508, loss=0.02965119481086731
I0305 21:52:34.776531 140484325295872 logging_writer.py:48] [34400] global_step=34400, grad_norm=0.14328673481941223, loss=0.02789612114429474
I0305 21:52:52.620151 140626292712640 spec.py:321] Evaluating on the training split.
I0305 21:54:04.167075 140626292712640 spec.py:333] Evaluating on the validation split.
I0305 21:54:06.108161 140626292712640 spec.py:349] Evaluating on the test split.
I0305 21:54:08.033018 140626292712640 submission_runner.py:469] Time since start: 9673.51s, 	Step: 34485, 	{'train/accuracy': 0.9936516880989075, 'train/loss': 0.020687054842710495, 'train/mean_average_precision': 0.6256037355055908, 'validation/accuracy': 0.986772358417511, 'validation/loss': 0.045973245054483414, 'validation/mean_average_precision': 0.27006564349832346, 'validation/num_examples': 43793, 'test/accuracy': 0.9859750270843506, 'test/loss': 0.049092233180999756, 'test/mean_average_precision': 0.2605718871488263, 'test/num_examples': 43793, 'score': 7212.077303647995, 'total_duration': 9673.51255273819, 'accumulated_submission_time': 7212.077303647995, 'accumulated_eval_time': 2459.690827846527, 'accumulated_logging_time': 0.8182566165924072}
I0305 21:54:08.045354 140484333688576 logging_writer.py:48] [34485] accumulated_eval_time=2459.69, accumulated_logging_time=0.818257, accumulated_submission_time=7212.08, global_step=34485, preemption_count=0, score=7212.08, test/accuracy=0.985975, test/loss=0.0490922, test/mean_average_precision=0.260572, test/num_examples=43793, total_duration=9673.51, train/accuracy=0.993652, train/loss=0.0206871, train/mean_average_precision=0.625604, validation/accuracy=0.986772, validation/loss=0.0459732, validation/mean_average_precision=0.270066, validation/num_examples=43793
I0305 21:54:11.382495 140484325295872 logging_writer.py:48] [34500] global_step=34500, grad_norm=0.1223040446639061, loss=0.025467146188020706
I0305 21:54:32.602400 140484333688576 logging_writer.py:48] [34600] global_step=34600, grad_norm=0.14210118353366852, loss=0.02743632346391678
I0305 21:54:53.920666 140484325295872 logging_writer.py:48] [34700] global_step=34700, grad_norm=0.15602454543113708, loss=0.025810658931732178
I0305 21:55:14.965558 140484333688576 logging_writer.py:48] [34800] global_step=34800, grad_norm=0.13973583281040192, loss=0.02759096771478653
I0305 21:55:36.359997 140484325295872 logging_writer.py:48] [34900] global_step=34900, grad_norm=0.10985616594552994, loss=0.02681945636868477
I0305 21:55:57.414481 140484333688576 logging_writer.py:48] [35000] global_step=35000, grad_norm=0.15549534559249878, loss=0.027975963428616524
I0305 21:56:18.553086 140484325295872 logging_writer.py:48] [35100] global_step=35100, grad_norm=0.1144077479839325, loss=0.027408063411712646
I0305 21:56:39.221285 140484333688576 logging_writer.py:48] [35200] global_step=35200, grad_norm=0.12859487533569336, loss=0.02590998262166977
I0305 21:57:00.350671 140484325295872 logging_writer.py:48] [35300] global_step=35300, grad_norm=0.14475758373737335, loss=0.025301262736320496
I0305 21:57:21.422143 140484333688576 logging_writer.py:48] [35400] global_step=35400, grad_norm=0.15540289878845215, loss=0.0292196124792099
I0305 21:57:42.631985 140484325295872 logging_writer.py:48] [35500] global_step=35500, grad_norm=0.12754137814044952, loss=0.026330942288041115
I0305 21:58:03.630476 140484333688576 logging_writer.py:48] [35600] global_step=35600, grad_norm=0.1250879019498825, loss=0.02568480372428894
I0305 21:58:08.052262 140626292712640 spec.py:321] Evaluating on the training split.
I0305 21:59:17.169032 140626292712640 spec.py:333] Evaluating on the validation split.
I0305 21:59:19.129974 140626292712640 spec.py:349] Evaluating on the test split.
I0305 21:59:21.036259 140626292712640 submission_runner.py:469] Time since start: 9986.52s, 	Step: 35622, 	{'train/accuracy': 0.993052065372467, 'train/loss': 0.022436054423451424, 'train/mean_average_precision': 0.5773628968619146, 'validation/accuracy': 0.9867541193962097, 'validation/loss': 0.046330202370882034, 'validation/mean_average_precision': 0.2695483021539162, 'validation/num_examples': 43793, 'test/accuracy': 0.9859308004379272, 'test/loss': 0.049470216035842896, 'test/mean_average_precision': 0.26252300646751353, 'test/num_examples': 43793, 'score': 7452.0422904491425, 'total_duration': 9986.51591038704, 'accumulated_submission_time': 7452.0422904491425, 'accumulated_eval_time': 2532.674788713455, 'accumulated_logging_time': 0.839911937713623}
I0305 21:59:21.048700 140484325295872 logging_writer.py:48] [35622] accumulated_eval_time=2532.67, accumulated_logging_time=0.839912, accumulated_submission_time=7452.04, global_step=35622, preemption_count=0, score=7452.04, test/accuracy=0.985931, test/loss=0.0494702, test/mean_average_precision=0.262523, test/num_examples=43793, total_duration=9986.52, train/accuracy=0.993052, train/loss=0.0224361, train/mean_average_precision=0.577363, validation/accuracy=0.986754, validation/loss=0.0463302, validation/mean_average_precision=0.269548, validation/num_examples=43793
I0305 21:59:38.060314 140484333688576 logging_writer.py:48] [35700] global_step=35700, grad_norm=0.1513514369726181, loss=0.029102077707648277
I0305 21:59:59.608888 140484325295872 logging_writer.py:48] [35800] global_step=35800, grad_norm=0.1491873562335968, loss=0.02671242505311966
I0305 22:00:20.929722 140484333688576 logging_writer.py:48] [35900] global_step=35900, grad_norm=0.1318814605474472, loss=0.025599829852581024
I0305 22:00:42.095937 140484325295872 logging_writer.py:48] [36000] global_step=36000, grad_norm=0.12840761244297028, loss=0.026468299329280853
I0305 22:01:02.771877 140484333688576 logging_writer.py:48] [36100] global_step=36100, grad_norm=0.1471991240978241, loss=0.024999620392918587
I0305 22:01:23.546873 140484325295872 logging_writer.py:48] [36200] global_step=36200, grad_norm=0.13222485780715942, loss=0.025043876841664314
I0305 22:01:44.536759 140484333688576 logging_writer.py:48] [36300] global_step=36300, grad_norm=0.1361490786075592, loss=0.025941191241145134
I0305 22:02:05.266507 140484325295872 logging_writer.py:48] [36400] global_step=36400, grad_norm=0.13515788316726685, loss=0.029685229063034058
I0305 22:02:26.078727 140484333688576 logging_writer.py:48] [36500] global_step=36500, grad_norm=0.13695374131202698, loss=0.026477087289094925
I0305 22:02:46.994725 140484325295872 logging_writer.py:48] [36600] global_step=36600, grad_norm=0.12908783555030823, loss=0.025258390232920647
I0305 22:03:08.000418 140484333688576 logging_writer.py:48] [36700] global_step=36700, grad_norm=0.12557002902030945, loss=0.025181004777550697
I0305 22:03:21.104578 140626292712640 spec.py:321] Evaluating on the training split.
I0305 22:04:33.639719 140626292712640 spec.py:333] Evaluating on the validation split.
I0305 22:04:35.578102 140626292712640 spec.py:349] Evaluating on the test split.
I0305 22:04:37.498569 140626292712640 submission_runner.py:469] Time since start: 10302.98s, 	Step: 36764, 	{'train/accuracy': 0.9940528273582458, 'train/loss': 0.019451990723609924, 'train/mean_average_precision': 0.6459853723001685, 'validation/accuracy': 0.9867139458656311, 'validation/loss': 0.046674732118844986, 'validation/mean_average_precision': 0.27310003656797, 'validation/num_examples': 43793, 'test/accuracy': 0.9858596324920654, 'test/loss': 0.05010291934013367, 'test/mean_average_precision': 0.2584128058239561, 'test/num_examples': 43793, 'score': 7692.057778835297, 'total_duration': 10302.97818517685, 'accumulated_submission_time': 7692.057778835297, 'accumulated_eval_time': 2609.0686905384064, 'accumulated_logging_time': 0.861299991607666}
I0305 22:04:37.511538 140484325295872 logging_writer.py:48] [36764] accumulated_eval_time=2609.07, accumulated_logging_time=0.8613, accumulated_submission_time=7692.06, global_step=36764, preemption_count=0, score=7692.06, test/accuracy=0.98586, test/loss=0.0501029, test/mean_average_precision=0.258413, test/num_examples=43793, total_duration=10303, train/accuracy=0.994053, train/loss=0.019452, train/mean_average_precision=0.645985, validation/accuracy=0.986714, validation/loss=0.0466747, validation/mean_average_precision=0.2731, validation/num_examples=43793
I0305 22:04:45.249136 140484333688576 logging_writer.py:48] [36800] global_step=36800, grad_norm=0.12725314497947693, loss=0.024889850988984108
I0305 22:05:06.234261 140484325295872 logging_writer.py:48] [36900] global_step=36900, grad_norm=0.14589032530784607, loss=0.025660527870059013
I0305 22:05:26.963087 140484333688576 logging_writer.py:48] [37000] global_step=37000, grad_norm=0.13143007457256317, loss=0.026438089087605476
I0305 22:05:47.959417 140484325295872 logging_writer.py:48] [37100] global_step=37100, grad_norm=0.167842298746109, loss=0.025678599253296852
I0305 22:06:09.159622 140484333688576 logging_writer.py:48] [37200] global_step=37200, grad_norm=0.17421479523181915, loss=0.024408865720033646
I0305 22:06:30.106133 140484325295872 logging_writer.py:48] [37300] global_step=37300, grad_norm=0.13221485912799835, loss=0.02327856421470642
I0305 22:06:51.056451 140484333688576 logging_writer.py:48] [37400] global_step=37400, grad_norm=0.1446445733308792, loss=0.02744566649198532
I0305 22:07:11.985970 140484325295872 logging_writer.py:48] [37500] global_step=37500, grad_norm=0.14209511876106262, loss=0.02530558966100216
I0305 22:07:33.164750 140484333688576 logging_writer.py:48] [37600] global_step=37600, grad_norm=0.1665312945842743, loss=0.026329582557082176
I0305 22:07:54.237730 140484325295872 logging_writer.py:48] [37700] global_step=37700, grad_norm=0.14247436821460724, loss=0.025425203144550323
I0305 22:08:15.350131 140484333688576 logging_writer.py:48] [37800] global_step=37800, grad_norm=0.1499873548746109, loss=0.02532295510172844
I0305 22:08:36.304843 140484325295872 logging_writer.py:48] [37900] global_step=37900, grad_norm=0.17780856788158417, loss=0.025109514594078064
I0305 22:08:37.597196 140626292712640 spec.py:321] Evaluating on the training split.
I0305 22:09:47.697243 140626292712640 spec.py:333] Evaluating on the validation split.
I0305 22:09:49.606104 140626292712640 spec.py:349] Evaluating on the test split.
I0305 22:09:51.505444 140626292712640 submission_runner.py:469] Time since start: 10616.99s, 	Step: 37907, 	{'train/accuracy': 0.9934616684913635, 'train/loss': 0.020867303013801575, 'train/mean_average_precision': 0.6037326811666275, 'validation/accuracy': 0.9866449236869812, 'validation/loss': 0.04767531901597977, 'validation/mean_average_precision': 0.26764534911264387, 'validation/num_examples': 43793, 'test/accuracy': 0.9857770800590515, 'test/loss': 0.051052525639534, 'test/mean_average_precision': 0.2504274527622097, 'test/num_examples': 43793, 'score': 7932.1015610694885, 'total_duration': 10616.9850628376, 'accumulated_submission_time': 7932.1015610694885, 'accumulated_eval_time': 2682.976849794388, 'accumulated_logging_time': 0.8849091529846191}
I0305 22:09:51.518120 140484333688576 logging_writer.py:48] [37907] accumulated_eval_time=2682.98, accumulated_logging_time=0.884909, accumulated_submission_time=7932.1, global_step=37907, preemption_count=0, score=7932.1, test/accuracy=0.985777, test/loss=0.0510525, test/mean_average_precision=0.250427, test/num_examples=43793, total_duration=10617, train/accuracy=0.993462, train/loss=0.0208673, train/mean_average_precision=0.603733, validation/accuracy=0.986645, validation/loss=0.0476753, validation/mean_average_precision=0.267645, validation/num_examples=43793
I0305 22:10:11.291991 140484325295872 logging_writer.py:48] [38000] global_step=38000, grad_norm=0.18563495576381683, loss=0.024759480729699135
I0305 22:10:32.095137 140484333688576 logging_writer.py:48] [38100] global_step=38100, grad_norm=0.1437331885099411, loss=0.0258076973259449
I0305 22:10:53.101906 140484325295872 logging_writer.py:48] [38200] global_step=38200, grad_norm=0.15751148760318756, loss=0.024707596749067307
I0305 22:11:14.354852 140484333688576 logging_writer.py:48] [38300] global_step=38300, grad_norm=0.1581798642873764, loss=0.02559337019920349
I0305 22:11:35.361470 140484325295872 logging_writer.py:48] [38400] global_step=38400, grad_norm=0.1580405980348587, loss=0.025949668139219284
I0305 22:11:56.245927 140484333688576 logging_writer.py:48] [38500] global_step=38500, grad_norm=0.16419191658496857, loss=0.027222314849495888
I0305 22:12:17.391134 140484325295872 logging_writer.py:48] [38600] global_step=38600, grad_norm=0.1531774252653122, loss=0.025936197489500046
I0305 22:12:38.356751 140484333688576 logging_writer.py:48] [38700] global_step=38700, grad_norm=0.14676383137702942, loss=0.02354654110968113
I0305 22:12:59.444373 140484325295872 logging_writer.py:48] [38800] global_step=38800, grad_norm=0.1341986358165741, loss=0.025077728554606438
I0305 22:13:20.382251 140484333688576 logging_writer.py:48] [38900] global_step=38900, grad_norm=0.16037192940711975, loss=0.025340409949421883
I0305 22:13:41.543761 140484325295872 logging_writer.py:48] [39000] global_step=39000, grad_norm=0.15239430963993073, loss=0.024036383256316185
I0305 22:13:51.506894 140626292712640 spec.py:321] Evaluating on the training split.
I0305 22:15:04.189821 140626292712640 spec.py:333] Evaluating on the validation split.
I0305 22:15:06.144018 140626292712640 spec.py:349] Evaluating on the test split.
I0305 22:15:08.093900 140626292712640 submission_runner.py:469] Time since start: 10933.57s, 	Step: 39048, 	{'train/accuracy': 0.99444180727005, 'train/loss': 0.018256405368447304, 'train/mean_average_precision': 0.6654711556176374, 'validation/accuracy': 0.9867054224014282, 'validation/loss': 0.0479443296790123, 'validation/mean_average_precision': 0.26512632998949415, 'validation/num_examples': 43793, 'test/accuracy': 0.9857627153396606, 'test/loss': 0.051474783569574356, 'test/mean_average_precision': 0.2593348225602123, 'test/num_examples': 43793, 'score': 8172.051128864288, 'total_duration': 10933.573536396027, 'accumulated_submission_time': 8172.051128864288, 'accumulated_eval_time': 2759.5637917518616, 'accumulated_logging_time': 0.9068722724914551}
I0305 22:15:08.107767 140484333688576 logging_writer.py:48] [39048] accumulated_eval_time=2759.56, accumulated_logging_time=0.906872, accumulated_submission_time=8172.05, global_step=39048, preemption_count=0, score=8172.05, test/accuracy=0.985763, test/loss=0.0514748, test/mean_average_precision=0.259335, test/num_examples=43793, total_duration=10933.6, train/accuracy=0.994442, train/loss=0.0182564, train/mean_average_precision=0.665471, validation/accuracy=0.986705, validation/loss=0.0479443, validation/mean_average_precision=0.265126, validation/num_examples=43793
I0305 22:15:19.096847 140484325295872 logging_writer.py:48] [39100] global_step=39100, grad_norm=0.19526205956935883, loss=0.027087518945336342
I0305 22:15:40.069291 140484333688576 logging_writer.py:48] [39200] global_step=39200, grad_norm=0.13858656585216522, loss=0.024545768275856972
I0305 22:16:00.892603 140484325295872 logging_writer.py:48] [39300] global_step=39300, grad_norm=0.17242932319641113, loss=0.022467589005827904
I0305 22:16:22.064213 140484333688576 logging_writer.py:48] [39400] global_step=39400, grad_norm=0.14787974953651428, loss=0.025206709280610085
I0305 22:16:42.664734 140484325295872 logging_writer.py:48] [39500] global_step=39500, grad_norm=0.16546930372714996, loss=0.02479376643896103
I0305 22:17:03.557273 140484333688576 logging_writer.py:48] [39600] global_step=39600, grad_norm=0.16059483587741852, loss=0.022292423993349075
I0305 22:17:24.613787 140484325295872 logging_writer.py:48] [39700] global_step=39700, grad_norm=0.170623779296875, loss=0.02246830239892006
I0305 22:17:45.648302 140484333688576 logging_writer.py:48] [39800] global_step=39800, grad_norm=0.19105412065982819, loss=0.024730078876018524
I0305 22:18:06.632920 140484325295872 logging_writer.py:48] [39900] global_step=39900, grad_norm=0.16043713688850403, loss=0.024228623136878014
I0305 22:18:27.424625 140484333688576 logging_writer.py:48] [40000] global_step=40000, grad_norm=0.1840776652097702, loss=0.02359767258167267
I0305 22:18:48.312746 140484325295872 logging_writer.py:48] [40100] global_step=40100, grad_norm=0.2109862118959427, loss=0.02620980329811573
I0305 22:19:08.280962 140626292712640 spec.py:321] Evaluating on the training split.
I0305 22:20:18.228468 140626292712640 spec.py:333] Evaluating on the validation split.
I0305 22:20:20.233170 140626292712640 spec.py:349] Evaluating on the test split.
I0305 22:20:22.138715 140626292712640 submission_runner.py:469] Time since start: 11247.62s, 	Step: 40196, 	{'train/accuracy': 0.9938923120498657, 'train/loss': 0.019340278580784798, 'train/mean_average_precision': 0.6425472444075877, 'validation/accuracy': 0.9865856766700745, 'validation/loss': 0.04878591001033783, 'validation/mean_average_precision': 0.2681716221674736, 'validation/num_examples': 43793, 'test/accuracy': 0.9857202172279358, 'test/loss': 0.05220598727464676, 'test/mean_average_precision': 0.2573468907324039, 'test/num_examples': 43793, 'score': 8412.181686162949, 'total_duration': 11247.618238687515, 'accumulated_submission_time': 8412.181686162949, 'accumulated_eval_time': 2833.4213631153107, 'accumulated_logging_time': 0.9336273670196533}
I0305 22:20:22.153083 140484333688576 logging_writer.py:48] [40196] accumulated_eval_time=2833.42, accumulated_logging_time=0.933627, accumulated_submission_time=8412.18, global_step=40196, preemption_count=0, score=8412.18, test/accuracy=0.98572, test/loss=0.052206, test/mean_average_precision=0.257347, test/num_examples=43793, total_duration=11247.6, train/accuracy=0.993892, train/loss=0.0193403, train/mean_average_precision=0.642547, validation/accuracy=0.986586, validation/loss=0.0487859, validation/mean_average_precision=0.268172, validation/num_examples=43793
I0305 22:20:23.246482 140484325295872 logging_writer.py:48] [40200] global_step=40200, grad_norm=0.19964589178562164, loss=0.023017311468720436
I0305 22:20:44.444938 140484333688576 logging_writer.py:48] [40300] global_step=40300, grad_norm=0.20335470139980316, loss=0.023531978949904442
I0305 22:21:05.270548 140484325295872 logging_writer.py:48] [40400] global_step=40400, grad_norm=0.1717146784067154, loss=0.022537870332598686
I0305 22:21:26.371637 140484333688576 logging_writer.py:48] [40500] global_step=40500, grad_norm=0.1690327376127243, loss=0.023753954097628593
I0305 22:21:47.582931 140484325295872 logging_writer.py:48] [40600] global_step=40600, grad_norm=0.1545131504535675, loss=0.023819956928491592
I0305 22:22:08.642344 140484333688576 logging_writer.py:48] [40700] global_step=40700, grad_norm=0.1668296605348587, loss=0.024385860189795494
I0305 22:22:29.615463 140484325295872 logging_writer.py:48] [40800] global_step=40800, grad_norm=0.173006072640419, loss=0.023389916867017746
I0305 22:22:50.781765 140484333688576 logging_writer.py:48] [40900] global_step=40900, grad_norm=0.17230939865112305, loss=0.023893676698207855
I0305 22:23:11.718931 140484325295872 logging_writer.py:48] [41000] global_step=41000, grad_norm=0.17628900706768036, loss=0.02408790774643421
I0305 22:23:32.760480 140484333688576 logging_writer.py:48] [41100] global_step=41100, grad_norm=0.16240325570106506, loss=0.02358694188296795
I0305 22:23:53.857162 140484325295872 logging_writer.py:48] [41200] global_step=41200, grad_norm=0.17719565331935883, loss=0.02471928671002388
I0305 22:24:15.084795 140484333688576 logging_writer.py:48] [41300] global_step=41300, grad_norm=0.16253426671028137, loss=0.022839481011033058
I0305 22:24:22.230172 140626292712640 spec.py:321] Evaluating on the training split.
I0305 22:25:35.856988 140626292712640 spec.py:333] Evaluating on the validation split.
I0305 22:25:37.820753 140626292712640 spec.py:349] Evaluating on the test split.
I0305 22:25:39.709977 140626292712640 submission_runner.py:469] Time since start: 11565.19s, 	Step: 41335, 	{'train/accuracy': 0.9947255849838257, 'train/loss': 0.017139816656708717, 'train/mean_average_precision': 0.6878107563981042, 'validation/accuracy': 0.9865812063217163, 'validation/loss': 0.049281805753707886, 'validation/mean_average_precision': 0.26648704314260613, 'validation/num_examples': 43793, 'test/accuracy': 0.9856431484222412, 'test/loss': 0.05294788256287575, 'test/mean_average_precision': 0.25068989395746444, 'test/num_examples': 43793, 'score': 8652.21815085411, 'total_duration': 11565.189492702484, 'accumulated_submission_time': 8652.21815085411, 'accumulated_eval_time': 2910.9009759426117, 'accumulated_logging_time': 0.9580135345458984}
I0305 22:25:39.723195 140484325295872 logging_writer.py:48] [41335] accumulated_eval_time=2910.9, accumulated_logging_time=0.958014, accumulated_submission_time=8652.22, global_step=41335, preemption_count=0, score=8652.22, test/accuracy=0.985643, test/loss=0.0529479, test/mean_average_precision=0.25069, test/num_examples=43793, total_duration=11565.2, train/accuracy=0.994726, train/loss=0.0171398, train/mean_average_precision=0.687811, validation/accuracy=0.986581, validation/loss=0.0492818, validation/mean_average_precision=0.266487, validation/num_examples=43793
I0305 22:25:53.729669 140484333688576 logging_writer.py:48] [41400] global_step=41400, grad_norm=0.1738627701997757, loss=0.022413000464439392
I0305 22:26:14.543759 140484325295872 logging_writer.py:48] [41500] global_step=41500, grad_norm=0.1607692539691925, loss=0.021721789613366127
I0305 22:26:35.721580 140484333688576 logging_writer.py:48] [41600] global_step=41600, grad_norm=0.21556761860847473, loss=0.024094074964523315
I0305 22:26:56.920025 140484325295872 logging_writer.py:48] [41700] global_step=41700, grad_norm=0.1877763569355011, loss=0.023002956062555313
I0305 22:27:18.077815 140484333688576 logging_writer.py:48] [41800] global_step=41800, grad_norm=0.18003173172473907, loss=0.021889159455895424
I0305 22:27:38.778286 140484325295872 logging_writer.py:48] [41900] global_step=41900, grad_norm=0.1940079927444458, loss=0.023279333487153053
I0305 22:27:59.951953 140484333688576 logging_writer.py:48] [42000] global_step=42000, grad_norm=0.18025974929332733, loss=0.02333713322877884
I0305 22:28:20.864349 140484325295872 logging_writer.py:48] [42100] global_step=42100, grad_norm=0.15395280718803406, loss=0.02162046916782856
I0305 22:28:41.980800 140484333688576 logging_writer.py:48] [42200] global_step=42200, grad_norm=0.22004739940166473, loss=0.024234751239418983
I0305 22:29:02.745954 140484325295872 logging_writer.py:48] [42300] global_step=42300, grad_norm=0.1933673918247223, loss=0.024124475196003914
I0305 22:29:23.609600 140484333688576 logging_writer.py:48] [42400] global_step=42400, grad_norm=0.18833839893341064, loss=0.023534130305051804
I0305 22:29:39.792011 140626292712640 spec.py:321] Evaluating on the training split.
I0305 22:30:51.181895 140626292712640 spec.py:333] Evaluating on the validation split.
I0305 22:30:53.317735 140626292712640 spec.py:349] Evaluating on the test split.
I0305 22:30:55.384591 140626292712640 submission_runner.py:469] Time since start: 11880.86s, 	Step: 42478, 	{'train/accuracy': 0.9945451617240906, 'train/loss': 0.017343349754810333, 'train/mean_average_precision': 0.6889113238011881, 'validation/accuracy': 0.9864732027053833, 'validation/loss': 0.05091932788491249, 'validation/mean_average_precision': 0.25889919881570533, 'validation/num_examples': 43793, 'test/accuracy': 0.9856178760528564, 'test/loss': 0.0546247698366642, 'test/mean_average_precision': 0.2515059617449239, 'test/num_examples': 43793, 'score': 8892.246052980423, 'total_duration': 11880.864127397537, 'accumulated_submission_time': 8892.246052980423, 'accumulated_eval_time': 2986.493406534195, 'accumulated_logging_time': 0.9803786277770996}
I0305 22:30:55.399752 140484325295872 logging_writer.py:48] [42478] accumulated_eval_time=2986.49, accumulated_logging_time=0.980379, accumulated_submission_time=8892.25, global_step=42478, preemption_count=0, score=8892.25, test/accuracy=0.985618, test/loss=0.0546248, test/mean_average_precision=0.251506, test/num_examples=43793, total_duration=11880.9, train/accuracy=0.994545, train/loss=0.0173433, train/mean_average_precision=0.688911, validation/accuracy=0.986473, validation/loss=0.0509193, validation/mean_average_precision=0.258899, validation/num_examples=43793
I0305 22:31:00.285782 140484333688576 logging_writer.py:48] [42500] global_step=42500, grad_norm=0.17391924560070038, loss=0.02256469428539276
I0305 22:31:21.022736 140484325295872 logging_writer.py:48] [42600] global_step=42600, grad_norm=0.18248189985752106, loss=0.02217205986380577
I0305 22:31:42.025145 140484333688576 logging_writer.py:48] [42700] global_step=42700, grad_norm=0.16842754185199738, loss=0.02161860279738903
I0305 22:32:02.822511 140484325295872 logging_writer.py:48] [42800] global_step=42800, grad_norm=0.19037912786006927, loss=0.022298529744148254
I0305 22:32:24.051558 140484333688576 logging_writer.py:48] [42900] global_step=42900, grad_norm=0.18982192873954773, loss=0.021417902782559395
I0305 22:32:45.499177 140484325295872 logging_writer.py:48] [43000] global_step=43000, grad_norm=0.18579183518886566, loss=0.02196437306702137
I0305 22:33:06.366255 140484333688576 logging_writer.py:48] [43100] global_step=43100, grad_norm=0.2019583135843277, loss=0.022958382964134216
I0305 22:33:27.115655 140484325295872 logging_writer.py:48] [43200] global_step=43200, grad_norm=0.18642881512641907, loss=0.02358279749751091
I0305 22:33:48.200559 140484333688576 logging_writer.py:48] [43300] global_step=43300, grad_norm=0.16217955946922302, loss=0.019905732944607735
I0305 22:34:09.492368 140484325295872 logging_writer.py:48] [43400] global_step=43400, grad_norm=0.23290535807609558, loss=0.02372117154300213
I0305 22:34:30.537784 140484333688576 logging_writer.py:48] [43500] global_step=43500, grad_norm=0.18326826393604279, loss=0.021430952474474907
I0305 22:34:51.493546 140484325295872 logging_writer.py:48] [43600] global_step=43600, grad_norm=0.27168765664100647, loss=0.024940380826592445
I0305 22:34:55.429448 140626292712640 spec.py:321] Evaluating on the training split.
I0305 22:36:07.024371 140626292712640 spec.py:333] Evaluating on the validation split.
I0305 22:36:08.950262 140626292712640 spec.py:349] Evaluating on the test split.
I0305 22:36:10.884319 140626292712640 submission_runner.py:469] Time since start: 12196.36s, 	Step: 43620, 	{'train/accuracy': 0.9944514036178589, 'train/loss': 0.01761283166706562, 'train/mean_average_precision': 0.6719254061668964, 'validation/accuracy': 0.9863747358322144, 'validation/loss': 0.05124706029891968, 'validation/mean_average_precision': 0.25767455289076446, 'validation/num_examples': 43793, 'test/accuracy': 0.9855567812919617, 'test/loss': 0.05482589825987816, 'test/mean_average_precision': 0.2460800162131067, 'test/num_examples': 43793, 'score': 9132.233981370926, 'total_duration': 12196.363843679428, 'accumulated_submission_time': 9132.233981370926, 'accumulated_eval_time': 3061.9480957984924, 'accumulated_logging_time': 1.0067880153656006}
I0305 22:36:10.898634 140484333688576 logging_writer.py:48] [43620] accumulated_eval_time=3061.95, accumulated_logging_time=1.00679, accumulated_submission_time=9132.23, global_step=43620, preemption_count=0, score=9132.23, test/accuracy=0.985557, test/loss=0.0548259, test/mean_average_precision=0.24608, test/num_examples=43793, total_duration=12196.4, train/accuracy=0.994451, train/loss=0.0176128, train/mean_average_precision=0.671925, validation/accuracy=0.986375, validation/loss=0.0512471, validation/mean_average_precision=0.257675, validation/num_examples=43793
I0305 22:36:27.585126 140484325295872 logging_writer.py:48] [43700] global_step=43700, grad_norm=0.19255173206329346, loss=0.021549012511968613
I0305 22:36:48.844455 140484333688576 logging_writer.py:48] [43800] global_step=43800, grad_norm=0.1798330843448639, loss=0.02066762000322342
I0305 22:37:10.069618 140484325295872 logging_writer.py:48] [43900] global_step=43900, grad_norm=0.19703367352485657, loss=0.02100767008960247
I0305 22:37:31.084834 140484333688576 logging_writer.py:48] [44000] global_step=44000, grad_norm=0.1722460389137268, loss=0.023178977891802788
I0305 22:37:52.329752 140484325295872 logging_writer.py:48] [44100] global_step=44100, grad_norm=0.17651700973510742, loss=0.021406883373856544
I0305 22:38:13.411498 140484333688576 logging_writer.py:48] [44200] global_step=44200, grad_norm=0.1796109527349472, loss=0.021874701604247093
I0305 22:38:34.454415 140484325295872 logging_writer.py:48] [44300] global_step=44300, grad_norm=0.2282010018825531, loss=0.022900480777025223
I0305 22:38:55.628576 140484333688576 logging_writer.py:48] [44400] global_step=44400, grad_norm=0.19296003878116608, loss=0.019732588902115822
I0305 22:39:16.612473 140484325295872 logging_writer.py:48] [44500] global_step=44500, grad_norm=0.18702031672000885, loss=0.020404737442731857
I0305 22:39:37.677952 140484333688576 logging_writer.py:48] [44600] global_step=44600, grad_norm=0.20428898930549622, loss=0.021251874044537544
I0305 22:39:58.837800 140484325295872 logging_writer.py:48] [44700] global_step=44700, grad_norm=0.20041094720363617, loss=0.02296431176364422
I0305 22:40:11.084241 140626292712640 spec.py:321] Evaluating on the training split.
I0305 22:41:21.271660 140626292712640 spec.py:333] Evaluating on the validation split.
I0305 22:41:23.199739 140626292712640 spec.py:349] Evaluating on the test split.
I0305 22:41:25.103994 140626292712640 submission_runner.py:469] Time since start: 12510.58s, 	Step: 44759, 	{'train/accuracy': 0.9954023957252502, 'train/loss': 0.014974188059568405, 'train/mean_average_precision': 0.7171394340911992, 'validation/accuracy': 0.9863436818122864, 'validation/loss': 0.05178181082010269, 'validation/mean_average_precision': 0.25856288286675394, 'validation/num_examples': 43793, 'test/accuracy': 0.9854750633239746, 'test/loss': 0.055704325437545776, 'test/mean_average_precision': 0.24768985358280335, 'test/num_examples': 43793, 'score': 9372.377534627914, 'total_duration': 12510.583525180817, 'accumulated_submission_time': 9372.377534627914, 'accumulated_eval_time': 3135.9676780700684, 'accumulated_logging_time': 1.0303173065185547}
I0305 22:41:25.118459 140484333688576 logging_writer.py:48] [44759] accumulated_eval_time=3135.97, accumulated_logging_time=1.03032, accumulated_submission_time=9372.38, global_step=44759, preemption_count=0, score=9372.38, test/accuracy=0.985475, test/loss=0.0557043, test/mean_average_precision=0.24769, test/num_examples=43793, total_duration=12510.6, train/accuracy=0.995402, train/loss=0.0149742, train/mean_average_precision=0.717139, validation/accuracy=0.986344, validation/loss=0.0517818, validation/mean_average_precision=0.258563, validation/num_examples=43793
I0305 22:41:33.817675 140484325295872 logging_writer.py:48] [44800] global_step=44800, grad_norm=0.1914336234331131, loss=0.020945081487298012
I0305 22:41:54.943992 140484333688576 logging_writer.py:48] [44900] global_step=44900, grad_norm=0.17769481241703033, loss=0.020039763301610947
I0305 22:42:15.895052 140484325295872 logging_writer.py:48] [45000] global_step=45000, grad_norm=0.2064812332391739, loss=0.021894238889217377
I0305 22:42:36.570374 140484333688576 logging_writer.py:48] [45100] global_step=45100, grad_norm=0.20631884038448334, loss=0.021238179877400398
I0305 22:42:57.116211 140484325295872 logging_writer.py:48] [45200] global_step=45200, grad_norm=0.19500136375427246, loss=0.021661214530467987
I0305 22:43:17.580669 140484333688576 logging_writer.py:48] [45300] global_step=45300, grad_norm=0.1885971873998642, loss=0.021379530429840088
I0305 22:43:38.501466 140484325295872 logging_writer.py:48] [45400] global_step=45400, grad_norm=0.18683120608329773, loss=0.022007104009389877
I0305 22:43:59.235173 140484333688576 logging_writer.py:48] [45500] global_step=45500, grad_norm=0.19495625793933868, loss=0.02041318267583847
I0305 22:44:20.005932 140484325295872 logging_writer.py:48] [45600] global_step=45600, grad_norm=0.1594412475824356, loss=0.018871450796723366
I0305 22:44:40.889389 140484333688576 logging_writer.py:48] [45700] global_step=45700, grad_norm=0.21616478264331818, loss=0.02225492335855961
I0305 22:45:01.647707 140484325295872 logging_writer.py:48] [45800] global_step=45800, grad_norm=0.19424468278884888, loss=0.01912740245461464
I0305 22:45:22.395913 140484333688576 logging_writer.py:48] [45900] global_step=45900, grad_norm=0.19344083964824677, loss=0.020649774000048637
I0305 22:45:25.128617 140626292712640 spec.py:321] Evaluating on the training split.
I0305 22:46:39.706288 140626292712640 spec.py:333] Evaluating on the validation split.
I0305 22:46:41.880752 140626292712640 spec.py:349] Evaluating on the test split.
I0305 22:46:43.964958 140626292712640 submission_runner.py:469] Time since start: 12829.44s, 	Step: 45914, 	{'train/accuracy': 0.9951494932174683, 'train/loss': 0.015523905865848064, 'train/mean_average_precision': 0.7159863821791338, 'validation/accuracy': 0.9863254427909851, 'validation/loss': 0.052862610667943954, 'validation/mean_average_precision': 0.26114937366781965, 'validation/num_examples': 43793, 'test/accuracy': 0.9853769540786743, 'test/loss': 0.05687498301267624, 'test/mean_average_precision': 0.24520633297990038, 'test/num_examples': 43793, 'score': 9612.348465204239, 'total_duration': 12829.444539546967, 'accumulated_submission_time': 9612.348465204239, 'accumulated_eval_time': 3214.8039078712463, 'accumulated_logging_time': 1.054661512374878}
I0305 22:46:43.980203 140484325295872 logging_writer.py:48] [45914] accumulated_eval_time=3214.8, accumulated_logging_time=1.05466, accumulated_submission_time=9612.35, global_step=45914, preemption_count=0, score=9612.35, test/accuracy=0.985377, test/loss=0.056875, test/mean_average_precision=0.245206, test/num_examples=43793, total_duration=12829.4, train/accuracy=0.995149, train/loss=0.0155239, train/mean_average_precision=0.715986, validation/accuracy=0.986325, validation/loss=0.0528626, validation/mean_average_precision=0.261149, validation/num_examples=43793
I0305 22:47:01.952761 140484333688576 logging_writer.py:48] [46000] global_step=46000, grad_norm=0.20102189481258392, loss=0.020081063732504845
I0305 22:47:22.648434 140484325295872 logging_writer.py:48] [46100] global_step=46100, grad_norm=0.20146077871322632, loss=0.02214125543832779
I0305 22:47:43.510489 140484333688576 logging_writer.py:48] [46200] global_step=46200, grad_norm=0.17618609964847565, loss=0.020468341186642647
I0305 22:48:04.502270 140484325295872 logging_writer.py:48] [46300] global_step=46300, grad_norm=0.19415386021137238, loss=0.022422941401600838
I0305 22:48:25.468817 140484333688576 logging_writer.py:48] [46400] global_step=46400, grad_norm=0.20333454012870789, loss=0.021254129707813263
I0305 22:48:46.670152 140484325295872 logging_writer.py:48] [46500] global_step=46500, grad_norm=0.19235540926456451, loss=0.021657701581716537
I0305 22:49:07.751155 140484333688576 logging_writer.py:48] [46600] global_step=46600, grad_norm=0.18884536623954773, loss=0.01934589259326458
I0305 22:49:29.010043 140484325295872 logging_writer.py:48] [46700] global_step=46700, grad_norm=0.17216479778289795, loss=0.01824364811182022
I0305 22:49:50.171739 140484333688576 logging_writer.py:48] [46800] global_step=46800, grad_norm=0.18906265497207642, loss=0.019016938284039497
I0305 22:50:11.596369 140484325295872 logging_writer.py:48] [46900] global_step=46900, grad_norm=0.19668231904506683, loss=0.01990131288766861
I0305 22:50:32.555931 140484333688576 logging_writer.py:48] [47000] global_step=47000, grad_norm=0.1941637396812439, loss=0.02139296941459179
I0305 22:50:44.043146 140626292712640 spec.py:321] Evaluating on the training split.
I0305 22:51:53.993144 140626292712640 spec.py:333] Evaluating on the validation split.
I0305 22:51:55.923897 140626292712640 spec.py:349] Evaluating on the test split.
I0305 22:51:57.814985 140626292712640 submission_runner.py:469] Time since start: 13143.29s, 	Step: 47056, 	{'train/accuracy': 0.9960127472877502, 'train/loss': 0.013291527517139912, 'train/mean_average_precision': 0.7616745754592504, 'validation/accuracy': 0.9863388538360596, 'validation/loss': 0.05385197699069977, 'validation/mean_average_precision': 0.2518298577249116, 'validation/num_examples': 43793, 'test/accuracy': 0.9854978322982788, 'test/loss': 0.057752713561058044, 'test/mean_average_precision': 0.24329711282038893, 'test/num_examples': 43793, 'score': 9852.368579387665, 'total_duration': 13143.294612169266, 'accumulated_submission_time': 9852.368579387665, 'accumulated_eval_time': 3288.5756690502167, 'accumulated_logging_time': 1.0794250965118408}
I0305 22:51:57.828834 140484325295872 logging_writer.py:48] [47056] accumulated_eval_time=3288.58, accumulated_logging_time=1.07943, accumulated_submission_time=9852.37, global_step=47056, preemption_count=0, score=9852.37, test/accuracy=0.985498, test/loss=0.0577527, test/mean_average_precision=0.243297, test/num_examples=43793, total_duration=13143.3, train/accuracy=0.996013, train/loss=0.0132915, train/mean_average_precision=0.761675, validation/accuracy=0.986339, validation/loss=0.053852, validation/mean_average_precision=0.25183, validation/num_examples=43793
I0305 22:52:07.266166 140484333688576 logging_writer.py:48] [47100] global_step=47100, grad_norm=0.22572074830532074, loss=0.022281691431999207
I0305 22:52:27.926891 140484325295872 logging_writer.py:48] [47200] global_step=47200, grad_norm=0.1802583783864975, loss=0.019754350185394287
I0305 22:52:48.539774 140484333688576 logging_writer.py:48] [47300] global_step=47300, grad_norm=0.21237528324127197, loss=0.021757308393716812
I0305 22:53:09.012657 140484325295872 logging_writer.py:48] [47400] global_step=47400, grad_norm=0.20896543562412262, loss=0.020608382299542427
I0305 22:53:29.663269 140484333688576 logging_writer.py:48] [47500] global_step=47500, grad_norm=0.1893191933631897, loss=0.01958007737994194
I0305 22:53:50.425800 140484325295872 logging_writer.py:48] [47600] global_step=47600, grad_norm=0.1784294694662094, loss=0.02006673440337181
I0305 22:54:11.435371 140484333688576 logging_writer.py:48] [47700] global_step=47700, grad_norm=0.18779999017715454, loss=0.020143713802099228
I0305 22:54:32.345610 140484325295872 logging_writer.py:48] [47800] global_step=47800, grad_norm=0.2241130918264389, loss=0.02178516611456871
I0305 22:54:52.984941 140484333688576 logging_writer.py:48] [47900] global_step=47900, grad_norm=0.1978265792131424, loss=0.019921183586120605
I0305 22:55:13.920156 140484325295872 logging_writer.py:48] [48000] global_step=48000, grad_norm=0.200799822807312, loss=0.020587246865034103
I0305 22:55:34.726327 140484333688576 logging_writer.py:48] [48100] global_step=48100, grad_norm=0.18522976338863373, loss=0.0184251107275486
I0305 22:55:55.753215 140484325295872 logging_writer.py:48] [48200] global_step=48200, grad_norm=0.18290159106254578, loss=0.01943945698440075
I0305 22:55:57.837792 140626292712640 spec.py:321] Evaluating on the training split.
I0305 22:57:06.247112 140626292712640 spec.py:333] Evaluating on the validation split.
I0305 22:57:08.221485 140626292712640 spec.py:349] Evaluating on the test split.
I0305 22:57:10.122290 140626292712640 submission_runner.py:469] Time since start: 13455.60s, 	Step: 48211, 	{'train/accuracy': 0.9951796531677246, 'train/loss': 0.01532951183617115, 'train/mean_average_precision': 0.7219508465405936, 'validation/accuracy': 0.9862767457962036, 'validation/loss': 0.05362145975232124, 'validation/mean_average_precision': 0.2528317094838333, 'validation/num_examples': 43793, 'test/accuracy': 0.985381543636322, 'test/loss': 0.057770851999521255, 'test/mean_average_precision': 0.2435343842369691, 'test/num_examples': 43793, 'score': 10092.339327335358, 'total_duration': 13455.601885795593, 'accumulated_submission_time': 10092.339327335358, 'accumulated_eval_time': 3360.8600544929504, 'accumulated_logging_time': 1.1023645401000977}
I0305 22:57:10.137222 140484333688576 logging_writer.py:48] [48211] accumulated_eval_time=3360.86, accumulated_logging_time=1.10236, accumulated_submission_time=10092.3, global_step=48211, preemption_count=0, score=10092.3, test/accuracy=0.985382, test/loss=0.0577709, test/mean_average_precision=0.243534, test/num_examples=43793, total_duration=13455.6, train/accuracy=0.99518, train/loss=0.0153295, train/mean_average_precision=0.721951, validation/accuracy=0.986277, validation/loss=0.0536215, validation/mean_average_precision=0.252832, validation/num_examples=43793
I0305 22:57:28.714689 140484325295872 logging_writer.py:48] [48300] global_step=48300, grad_norm=0.1931615024805069, loss=0.018528703600168228
I0305 22:57:49.297847 140484333688576 logging_writer.py:48] [48400] global_step=48400, grad_norm=0.17203211784362793, loss=0.018689369782805443
I0305 22:58:09.902168 140484325295872 logging_writer.py:48] [48500] global_step=48500, grad_norm=0.20140352845191956, loss=0.020974546670913696
I0305 22:58:30.710752 140484333688576 logging_writer.py:48] [48600] global_step=48600, grad_norm=0.19312210381031036, loss=0.018949884921312332
I0305 22:58:51.624862 140484325295872 logging_writer.py:48] [48700] global_step=48700, grad_norm=0.19280284643173218, loss=0.020202677696943283
I0305 22:59:12.445450 140484333688576 logging_writer.py:48] [48800] global_step=48800, grad_norm=0.19859862327575684, loss=0.01907580904662609
I0305 22:59:33.293303 140484325295872 logging_writer.py:48] [48900] global_step=48900, grad_norm=0.19883792102336884, loss=0.019480541348457336
I0305 22:59:54.269658 140484333688576 logging_writer.py:48] [49000] global_step=49000, grad_norm=0.21699212491512299, loss=0.01980053074657917
I0305 23:00:15.128878 140484325295872 logging_writer.py:48] [49100] global_step=49100, grad_norm=0.18252991139888763, loss=0.018777385354042053
I0305 23:00:36.402071 140484333688576 logging_writer.py:48] [49200] global_step=49200, grad_norm=0.24177101254463196, loss=0.022574137896299362
I0305 23:00:57.787958 140484325295872 logging_writer.py:48] [49300] global_step=49300, grad_norm=0.20025530457496643, loss=0.01891685649752617
I0305 23:01:10.233566 140626292712640 spec.py:321] Evaluating on the training split.
I0305 23:02:23.758241 140626292712640 spec.py:333] Evaluating on the validation split.
I0305 23:02:25.691225 140626292712640 spec.py:349] Evaluating on the test split.
I0305 23:02:27.786411 140626292712640 submission_runner.py:469] Time since start: 13773.27s, 	Step: 49360, 	{'train/accuracy': 0.996391236782074, 'train/loss': 0.012640495784580708, 'train/mean_average_precision': 0.7657991704603351, 'validation/accuracy': 0.9862958192825317, 'validation/loss': 0.05396689474582672, 'validation/mean_average_precision': 0.25526750691912903, 'validation/num_examples': 43793, 'test/accuracy': 0.9853689074516296, 'test/loss': 0.05793806537985802, 'test/mean_average_precision': 0.24441569099247903, 'test/num_examples': 43793, 'score': 10332.396535873413, 'total_duration': 13773.266067743301, 'accumulated_submission_time': 10332.396535873413, 'accumulated_eval_time': 3438.4128506183624, 'accumulated_logging_time': 1.12646484375}
I0305 23:02:27.802607 140484333688576 logging_writer.py:48] [49360] accumulated_eval_time=3438.41, accumulated_logging_time=1.12646, accumulated_submission_time=10332.4, global_step=49360, preemption_count=0, score=10332.4, test/accuracy=0.985369, test/loss=0.0579381, test/mean_average_precision=0.244416, test/num_examples=43793, total_duration=13773.3, train/accuracy=0.996391, train/loss=0.0126405, train/mean_average_precision=0.765799, validation/accuracy=0.986296, validation/loss=0.0539669, validation/mean_average_precision=0.255268, validation/num_examples=43793
I0305 23:02:36.276828 140484325295872 logging_writer.py:48] [49400] global_step=49400, grad_norm=0.20660772919654846, loss=0.02224019356071949
I0305 23:02:57.611704 140484333688576 logging_writer.py:48] [49500] global_step=49500, grad_norm=0.22374041378498077, loss=0.019825445488095284
I0305 23:03:18.782642 140484325295872 logging_writer.py:48] [49600] global_step=49600, grad_norm=0.19944243133068085, loss=0.020768605172634125
I0305 23:03:40.140280 140484333688576 logging_writer.py:48] [49700] global_step=49700, grad_norm=0.202598437666893, loss=0.01943596825003624
I0305 23:04:01.459730 140484325295872 logging_writer.py:48] [49800] global_step=49800, grad_norm=0.19226549565792084, loss=0.02231547422707081
I0305 23:04:22.438284 140484333688576 logging_writer.py:48] [49900] global_step=49900, grad_norm=0.2131461203098297, loss=0.02103743888437748
I0305 23:04:43.328221 140484325295872 logging_writer.py:48] [50000] global_step=50000, grad_norm=0.2076352834701538, loss=0.020064692944288254
I0305 23:05:04.371777 140484333688576 logging_writer.py:48] [50100] global_step=50100, grad_norm=0.19864995777606964, loss=0.020756138488650322
I0305 23:05:25.184264 140484325295872 logging_writer.py:48] [50200] global_step=50200, grad_norm=0.1960235834121704, loss=0.01998385228216648
I0305 23:05:46.035321 140484333688576 logging_writer.py:48] [50300] global_step=50300, grad_norm=0.22792774438858032, loss=0.021186022087931633
I0305 23:06:07.071745 140484325295872 logging_writer.py:48] [50400] global_step=50400, grad_norm=0.22017638385295868, loss=0.021060388535261154
I0305 23:06:27.802878 140626292712640 spec.py:321] Evaluating on the training split.
I0305 23:07:40.251693 140626292712640 spec.py:333] Evaluating on the validation split.
I0305 23:07:42.298177 140626292712640 spec.py:349] Evaluating on the test split.
I0305 23:07:44.241183 140626292712640 submission_runner.py:469] Time since start: 14089.72s, 	Step: 50499, 	{'train/accuracy': 0.9957308173179626, 'train/loss': 0.013916796073317528, 'train/mean_average_precision': 0.742811462018423, 'validation/accuracy': 0.9863055348396301, 'validation/loss': 0.05398702248930931, 'validation/mean_average_precision': 0.2534965208770851, 'validation/num_examples': 43793, 'test/accuracy': 0.9853870272636414, 'test/loss': 0.05802949517965317, 'test/mean_average_precision': 0.24411861375354624, 'test/num_examples': 43793, 'score': 10572.357874155045, 'total_duration': 14089.720838785172, 'accumulated_submission_time': 10572.357874155045, 'accumulated_eval_time': 3514.851109266281, 'accumulated_logging_time': 1.1524693965911865}
I0305 23:07:44.257920 140484333688576 logging_writer.py:48] [50499] accumulated_eval_time=3514.85, accumulated_logging_time=1.15247, accumulated_submission_time=10572.4, global_step=50499, preemption_count=0, score=10572.4, test/accuracy=0.985387, test/loss=0.0580295, test/mean_average_precision=0.244119, test/num_examples=43793, total_duration=14089.7, train/accuracy=0.995731, train/loss=0.0139168, train/mean_average_precision=0.742811, validation/accuracy=0.986306, validation/loss=0.053987, validation/mean_average_precision=0.253497, validation/num_examples=43793
I0305 23:07:44.731304 140484325295872 logging_writer.py:48] [50500] global_step=50500, grad_norm=0.21467481553554535, loss=0.020072689279913902
I0305 23:08:06.078435 140484333688576 logging_writer.py:48] [50600] global_step=50600, grad_norm=0.23554109036922455, loss=0.020503034815192223
I0305 23:08:27.119796 140484325295872 logging_writer.py:48] [50700] global_step=50700, grad_norm=0.21438521146774292, loss=0.0189104825258255
I0305 23:08:48.088768 140484333688576 logging_writer.py:48] [50800] global_step=50800, grad_norm=0.22546479105949402, loss=0.0216571893543005
I0305 23:09:09.392853 140484325295872 logging_writer.py:48] [50900] global_step=50900, grad_norm=0.19178402423858643, loss=0.020137649029493332
I0305 23:09:30.599371 140484333688576 logging_writer.py:48] [51000] global_step=51000, grad_norm=0.19917604327201843, loss=0.020083969458937645
I0305 23:09:51.611811 140484325295872 logging_writer.py:48] [51100] global_step=51100, grad_norm=0.1985548436641693, loss=0.019651049748063087
I0305 23:10:12.941704 140484333688576 logging_writer.py:48] [51200] global_step=51200, grad_norm=0.21759918332099915, loss=0.020313546061515808
I0305 23:10:33.931590 140484325295872 logging_writer.py:48] [51300] global_step=51300, grad_norm=0.2565089166164398, loss=0.020160991698503494
I0305 23:10:55.120236 140484333688576 logging_writer.py:48] [51400] global_step=51400, grad_norm=0.2214507758617401, loss=0.019866058602929115
I0305 23:11:16.734821 140484325295872 logging_writer.py:48] [51500] global_step=51500, grad_norm=0.2094135284423828, loss=0.019648414105176926
I0305 23:11:38.442466 140484333688576 logging_writer.py:48] [51600] global_step=51600, grad_norm=0.18532726168632507, loss=0.02021467685699463
I0305 23:11:44.272076 140626292712640 spec.py:321] Evaluating on the training split.
I0305 23:12:55.072384 140626292712640 spec.py:333] Evaluating on the validation split.
I0305 23:12:57.088787 140626292712640 spec.py:349] Evaluating on the test split.
I0305 23:12:59.038469 140626292712640 submission_runner.py:469] Time since start: 14404.52s, 	Step: 51628, 	{'train/accuracy': 0.9962584972381592, 'train/loss': 0.01285596750676632, 'train/mean_average_precision': 0.7718106684367108, 'validation/accuracy': 0.9863160848617554, 'validation/loss': 0.05391433835029602, 'validation/mean_average_precision': 0.25734913504349854, 'validation/num_examples': 43793, 'test/accuracy': 0.9854274988174438, 'test/loss': 0.05794017016887665, 'test/mean_average_precision': 0.24487347660624623, 'test/num_examples': 43793, 'score': 10812.330452680588, 'total_duration': 14404.518035888672, 'accumulated_submission_time': 10812.330452680588, 'accumulated_eval_time': 3589.6173639297485, 'accumulated_logging_time': 1.1785552501678467}
I0305 23:12:59.055653 140484325295872 logging_writer.py:48] [51628] accumulated_eval_time=3589.62, accumulated_logging_time=1.17856, accumulated_submission_time=10812.3, global_step=51628, preemption_count=0, score=10812.3, test/accuracy=0.985427, test/loss=0.0579402, test/mean_average_precision=0.244873, test/num_examples=43793, total_duration=14404.5, train/accuracy=0.996258, train/loss=0.012856, train/mean_average_precision=0.771811, validation/accuracy=0.986316, validation/loss=0.0539143, validation/mean_average_precision=0.257349, validation/num_examples=43793
I0305 23:13:14.842183 140484333688576 logging_writer.py:48] [51700] global_step=51700, grad_norm=0.19955869019031525, loss=0.018887074664235115
I0305 23:13:36.489363 140484325295872 logging_writer.py:48] [51800] global_step=51800, grad_norm=0.20983295142650604, loss=0.01980377547442913
I0305 23:13:58.085722 140484333688576 logging_writer.py:48] [51900] global_step=51900, grad_norm=0.2318776547908783, loss=0.02196039818227291
I0305 23:14:18.882345 140484325295872 logging_writer.py:48] [52000] global_step=52000, grad_norm=0.2032645344734192, loss=0.020316898822784424
I0305 23:14:39.317808 140484333688576 logging_writer.py:48] [52100] global_step=52100, grad_norm=0.20144854485988617, loss=0.019515851512551308
I0305 23:15:00.391040 140484325295872 logging_writer.py:48] [52200] global_step=52200, grad_norm=0.19632656872272491, loss=0.019719142466783524
I0305 23:15:21.408630 140484333688576 logging_writer.py:48] [52300] global_step=52300, grad_norm=0.19594496488571167, loss=0.02010236121714115
I0305 23:15:42.783596 140484325295872 logging_writer.py:48] [52400] global_step=52400, grad_norm=0.2020537257194519, loss=0.020554492250084877
I0305 23:16:04.226741 140484333688576 logging_writer.py:48] [52500] global_step=52500, grad_norm=0.22521480917930603, loss=0.02041606605052948
I0305 23:16:25.050703 140484325295872 logging_writer.py:48] [52600] global_step=52600, grad_norm=0.19049492478370667, loss=0.019536329433321953
I0305 23:16:45.559642 140484333688576 logging_writer.py:48] [52700] global_step=52700, grad_norm=0.19113822281360626, loss=0.018846824765205383
I0305 23:16:59.148795 140626292712640 spec.py:321] Evaluating on the training split.
I0305 23:18:12.304759 140626292712640 spec.py:333] Evaluating on the validation split.
I0305 23:18:14.251424 140626292712640 spec.py:349] Evaluating on the test split.
I0305 23:18:16.198360 140626292712640 submission_runner.py:469] Time since start: 14721.68s, 	Step: 52767, 	{'train/accuracy': 0.9962100386619568, 'train/loss': 0.01294196117669344, 'train/mean_average_precision': 0.7580600817690636, 'validation/accuracy': 0.9863229990005493, 'validation/loss': 0.05392349138855934, 'validation/mean_average_precision': 0.2574962308662534, 'validation/num_examples': 43793, 'test/accuracy': 0.9854253530502319, 'test/loss': 0.057959623634815216, 'test/mean_average_precision': 0.24504519932904764, 'test/num_examples': 43793, 'score': 11052.379261493683, 'total_duration': 14721.67795586586, 'accumulated_submission_time': 11052.379261493683, 'accumulated_eval_time': 3666.666816711426, 'accumulated_logging_time': 1.20849609375}
I0305 23:18:16.213418 140484325295872 logging_writer.py:48] [52767] accumulated_eval_time=3666.67, accumulated_logging_time=1.2085, accumulated_submission_time=11052.4, global_step=52767, preemption_count=0, score=11052.4, test/accuracy=0.985425, test/loss=0.0579596, test/mean_average_precision=0.245045, test/num_examples=43793, total_duration=14721.7, train/accuracy=0.99621, train/loss=0.012942, train/mean_average_precision=0.75806, validation/accuracy=0.986323, validation/loss=0.0539235, validation/mean_average_precision=0.257496, validation/num_examples=43793
I0305 23:18:23.268429 140484333688576 logging_writer.py:48] [52800] global_step=52800, grad_norm=0.2052023559808731, loss=0.020263515412807465
I0305 23:18:44.483863 140484325295872 logging_writer.py:48] [52900] global_step=52900, grad_norm=0.22142533957958221, loss=0.019493769854307175
I0305 23:19:05.465754 140484333688576 logging_writer.py:48] [53000] global_step=53000, grad_norm=0.1760043054819107, loss=0.01999519020318985
I0305 23:19:26.501875 140484325295872 logging_writer.py:48] [53100] global_step=53100, grad_norm=0.18562878668308258, loss=0.018496554344892502
I0305 23:19:47.803443 140484333688576 logging_writer.py:48] [53200] global_step=53200, grad_norm=0.23730449378490448, loss=0.02204829268157482
I0305 23:20:09.220900 140484325295872 logging_writer.py:48] [53300] global_step=53300, grad_norm=0.2067887932062149, loss=0.021088263019919395
I0305 23:20:30.371050 140484333688576 logging_writer.py:48] [53400] global_step=53400, grad_norm=0.20104193687438965, loss=0.01903466321527958
I0305 23:20:51.574594 140484325295872 logging_writer.py:48] [53500] global_step=53500, grad_norm=0.20725975930690765, loss=0.021222108975052834
I0305 23:21:13.160150 140484333688576 logging_writer.py:48] [53600] global_step=53600, grad_norm=0.21972288191318512, loss=0.019710158929228783
I0305 23:21:34.337804 140484325295872 logging_writer.py:48] [53700] global_step=53700, grad_norm=0.20993921160697937, loss=0.018918681889772415
I0305 23:21:55.698994 140484333688576 logging_writer.py:48] [53800] global_step=53800, grad_norm=0.18252773582935333, loss=0.0196318868547678
I0305 23:22:16.276365 140626292712640 spec.py:321] Evaluating on the training split.
I0305 23:23:28.133839 140626292712640 spec.py:333] Evaluating on the validation split.
I0305 23:23:30.118717 140626292712640 spec.py:349] Evaluating on the test split.
I0305 23:23:32.043233 140626292712640 submission_runner.py:469] Time since start: 15037.52s, 	Step: 53897, 	{'train/accuracy': 0.9960426688194275, 'train/loss': 0.01325035560876131, 'train/mean_average_precision': 0.763330768978438, 'validation/accuracy': 0.9863229990005493, 'validation/loss': 0.05392349138855934, 'validation/mean_average_precision': 0.25738789199153567, 'validation/num_examples': 43793, 'test/accuracy': 0.9854253530502319, 'test/loss': 0.057959623634815216, 'test/mean_average_precision': 0.24521343681626645, 'test/num_examples': 43793, 'score': 11292.401823997498, 'total_duration': 15037.522889614105, 'accumulated_submission_time': 11292.401823997498, 'accumulated_eval_time': 3742.43364071846, 'accumulated_logging_time': 1.2331297397613525}
I0305 23:23:32.058429 140484325295872 logging_writer.py:48] [53897] accumulated_eval_time=3742.43, accumulated_logging_time=1.23313, accumulated_submission_time=11292.4, global_step=53897, preemption_count=0, score=11292.4, test/accuracy=0.985425, test/loss=0.0579596, test/mean_average_precision=0.245213, test/num_examples=43793, total_duration=15037.5, train/accuracy=0.996043, train/loss=0.0132504, train/mean_average_precision=0.763331, validation/accuracy=0.986323, validation/loss=0.0539235, validation/mean_average_precision=0.257388, validation/num_examples=43793
I0305 23:23:32.921150 140484333688576 logging_writer.py:48] [53900] global_step=53900, grad_norm=0.22115248441696167, loss=0.020117247477173805
I0305 23:23:53.948573 140484325295872 logging_writer.py:48] [54000] global_step=54000, grad_norm=0.20296712219715118, loss=0.01914970576763153
I0305 23:24:15.359103 140484333688576 logging_writer.py:48] [54100] global_step=54100, grad_norm=0.2047283947467804, loss=0.02007751166820526
I0305 23:24:36.403271 140484325295872 logging_writer.py:48] [54200] global_step=54200, grad_norm=0.18587367236614227, loss=0.02100379765033722
I0305 23:24:57.511349 140484333688576 logging_writer.py:48] [54300] global_step=54300, grad_norm=0.18734677135944366, loss=0.01773366704583168
I0305 23:25:18.592978 140484325295872 logging_writer.py:48] [54400] global_step=54400, grad_norm=0.1844218671321869, loss=0.017483852803707123
I0305 23:25:39.737495 140484333688576 logging_writer.py:48] [54500] global_step=54500, grad_norm=0.20846961438655853, loss=0.020174846053123474
I0305 23:26:00.826637 140484325295872 logging_writer.py:48] [54600] global_step=54600, grad_norm=0.23054298758506775, loss=0.019379494711756706
I0305 23:26:21.720299 140484333688576 logging_writer.py:48] [54700] global_step=54700, grad_norm=0.19092366099357605, loss=0.019166160374879837
I0305 23:26:42.775406 140484325295872 logging_writer.py:48] [54800] global_step=54800, grad_norm=0.1875336617231369, loss=0.018804164603352547
I0305 23:27:03.984457 140484333688576 logging_writer.py:48] [54900] global_step=54900, grad_norm=0.20155496895313263, loss=0.01894403249025345
I0305 23:27:24.883138 140484325295872 logging_writer.py:48] [55000] global_step=55000, grad_norm=0.18144060671329498, loss=0.018856942653656006
I0305 23:27:32.057993 140626292712640 spec.py:321] Evaluating on the training split.
I0305 23:28:45.187679 140626292712640 spec.py:333] Evaluating on the validation split.
I0305 23:28:47.287212 140626292712640 spec.py:349] Evaluating on the test split.
I0305 23:28:49.313289 140626292712640 submission_runner.py:469] Time since start: 15354.79s, 	Step: 55035, 	{'train/accuracy': 0.9960747957229614, 'train/loss': 0.013246160000562668, 'train/mean_average_precision': 0.7594668980995638, 'validation/accuracy': 0.9863229990005493, 'validation/loss': 0.05392349138855934, 'validation/mean_average_precision': 0.2573318493412903, 'validation/num_examples': 43793, 'test/accuracy': 0.9854253530502319, 'test/loss': 0.057959623634815216, 'test/mean_average_precision': 0.24495229886100672, 'test/num_examples': 43793, 'score': 11532.362166404724, 'total_duration': 15354.792936086655, 'accumulated_submission_time': 11532.362166404724, 'accumulated_eval_time': 3819.688876390457, 'accumulated_logging_time': 1.2577683925628662}
I0305 23:28:49.328983 140484333688576 logging_writer.py:48] [55035] accumulated_eval_time=3819.69, accumulated_logging_time=1.25777, accumulated_submission_time=11532.4, global_step=55035, preemption_count=0, score=11532.4, test/accuracy=0.985425, test/loss=0.0579596, test/mean_average_precision=0.244952, test/num_examples=43793, total_duration=15354.8, train/accuracy=0.996075, train/loss=0.0132462, train/mean_average_precision=0.759467, validation/accuracy=0.986323, validation/loss=0.0539235, validation/mean_average_precision=0.257332, validation/num_examples=43793
I0305 23:29:03.044058 140484325295872 logging_writer.py:48] [55100] global_step=55100, grad_norm=0.17335864901542664, loss=0.021138567477464676
I0305 23:29:24.284579 140484333688576 logging_writer.py:48] [55200] global_step=55200, grad_norm=0.18758615851402283, loss=0.017762918025255203
I0305 23:29:45.385017 140484325295872 logging_writer.py:48] [55300] global_step=55300, grad_norm=0.22640077769756317, loss=0.020084457471966743
I0305 23:30:06.313691 140484333688576 logging_writer.py:48] [55400] global_step=55400, grad_norm=0.19067472219467163, loss=0.0193770918995142
I0305 23:30:27.192122 140484325295872 logging_writer.py:48] [55500] global_step=55500, grad_norm=0.1914290487766266, loss=0.021978862583637238
I0305 23:30:48.246619 140484333688576 logging_writer.py:48] [55600] global_step=55600, grad_norm=0.2386026531457901, loss=0.02151891030371189
I0305 23:31:09.184414 140484325295872 logging_writer.py:48] [55700] global_step=55700, grad_norm=0.1765459030866623, loss=0.017571188509464264
I0305 23:31:30.051346 140484333688576 logging_writer.py:48] [55800] global_step=55800, grad_norm=0.21121014654636383, loss=0.018505098298192024
I0305 23:31:51.192013 140484325295872 logging_writer.py:48] [55900] global_step=55900, grad_norm=0.2111169397830963, loss=0.019945230334997177
I0305 23:32:12.524658 140484333688576 logging_writer.py:48] [56000] global_step=56000, grad_norm=0.22173622250556946, loss=0.018270239233970642
I0305 23:32:33.597317 140484325295872 logging_writer.py:48] [56100] global_step=56100, grad_norm=0.2160091996192932, loss=0.01956363581120968
I0305 23:32:49.337661 140626292712640 spec.py:321] Evaluating on the training split.
I0305 23:33:56.515825 140626292712640 spec.py:333] Evaluating on the validation split.
I0305 23:33:58.470138 140626292712640 spec.py:349] Evaluating on the test split.
I0305 23:34:00.359616 140626292712640 submission_runner.py:469] Time since start: 15665.84s, 	Step: 56176, 	{'train/accuracy': 0.9961246252059937, 'train/loss': 0.013065858744084835, 'train/mean_average_precision': 0.7479835836133831, 'validation/accuracy': 0.9863229990005493, 'validation/loss': 0.05392349138855934, 'validation/mean_average_precision': 0.25751096968548215, 'validation/num_examples': 43793, 'test/accuracy': 0.9854253530502319, 'test/loss': 0.057959623634815216, 'test/mean_average_precision': 0.24492626962904432, 'test/num_examples': 43793, 'score': 11772.329397916794, 'total_duration': 15665.839257240295, 'accumulated_submission_time': 11772.329397916794, 'accumulated_eval_time': 3890.710768699646, 'accumulated_logging_time': 1.2844185829162598}
I0305 23:34:00.376314 140484333688576 logging_writer.py:48] [56176] accumulated_eval_time=3890.71, accumulated_logging_time=1.28442, accumulated_submission_time=11772.3, global_step=56176, preemption_count=0, score=11772.3, test/accuracy=0.985425, test/loss=0.0579596, test/mean_average_precision=0.244926, test/num_examples=43793, total_duration=15665.8, train/accuracy=0.996125, train/loss=0.0130659, train/mean_average_precision=0.747984, validation/accuracy=0.986323, validation/loss=0.0539235, validation/mean_average_precision=0.257511, validation/num_examples=43793
I0305 23:34:05.656829 140484325295872 logging_writer.py:48] [56200] global_step=56200, grad_norm=0.19381476938724518, loss=0.018907329067587852
I0305 23:34:26.362436 140484333688576 logging_writer.py:48] [56300] global_step=56300, grad_norm=0.21026764810085297, loss=0.02147301845252514
I0305 23:34:47.407152 140484325295872 logging_writer.py:48] [56400] global_step=56400, grad_norm=0.1908535212278366, loss=0.018790481612086296
I0305 23:35:08.485113 140484333688576 logging_writer.py:48] [56500] global_step=56500, grad_norm=0.22094422578811646, loss=0.02009957656264305
I0305 23:35:29.279986 140484325295872 logging_writer.py:48] [56600] global_step=56600, grad_norm=0.1781948208808899, loss=0.01883053407073021
I0305 23:35:50.252796 140484333688576 logging_writer.py:48] [56700] global_step=56700, grad_norm=0.25402113795280457, loss=0.021779369562864304
I0305 23:36:11.340384 140484325295872 logging_writer.py:48] [56800] global_step=56800, grad_norm=0.20240002870559692, loss=0.018345395103096962
I0305 23:36:32.128583 140484333688576 logging_writer.py:48] [56900] global_step=56900, grad_norm=0.20791791379451752, loss=0.019622018560767174
I0305 23:36:52.816809 140484325295872 logging_writer.py:48] [57000] global_step=57000, grad_norm=0.1819428652524948, loss=0.017331626266241074
I0305 23:37:13.743071 140484333688576 logging_writer.py:48] [57100] global_step=57100, grad_norm=0.1894061267375946, loss=0.01957717537879944
I0305 23:37:35.074442 140484325295872 logging_writer.py:48] [57200] global_step=57200, grad_norm=0.1938486099243164, loss=0.019943222403526306
I0305 23:37:55.868980 140484333688576 logging_writer.py:48] [57300] global_step=57300, grad_norm=0.20834816992282867, loss=0.019970383495092392
I0305 23:38:00.514006 140484325295872 logging_writer.py:48] [57323] global_step=57323, preemption_count=0, score=12012.4
I0305 23:38:00.659888 140626292712640 submission_runner.py:646] Tuning trial 1/5
I0305 23:38:00.660084 140626292712640 submission_runner.py:647] Hyperparameters: Hyperparameters(dropout_rate=0.0, label_smoothing=0.1, learning_rate=0.001308209823469072, one_minus_beta1=0.02686663061, beta2=0.9981232922116359, weight_decay=0.16375311233774334, warmup_factor=0.1)
I0305 23:38:00.662430 140626292712640 submission_runner.py:648] Metrics: {'eval_results': [(1, {'train/accuracy': 0.5431785583496094, 'train/loss': 0.7421693205833435, 'train/mean_average_precision': 0.02182718391744925, 'validation/accuracy': 0.5481993556022644, 'validation/loss': 0.7410886883735657, 'validation/mean_average_precision': 0.025415576816744098, 'validation/num_examples': 43793, 'test/accuracy': 0.5508217811584473, 'test/loss': 0.739730954170227, 'test/mean_average_precision': 0.027589280859152434, 'test/num_examples': 43793, 'score': 10.850950717926025, 'total_duration': 214.49020195007324, 'accumulated_submission_time': 10.850950717926025, 'accumulated_eval_time': 203.6391258239746, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1153, {'train/accuracy': 0.9870090484619141, 'train/loss': 0.050545353442430496, 'train/mean_average_precision': 0.06407091537008257, 'validation/accuracy': 0.9843781590461731, 'validation/loss': 0.06010604649782181, 'validation/mean_average_precision': 0.06043822553158016, 'validation/num_examples': 43793, 'test/accuracy': 0.9833905696868896, 'test/loss': 0.06316173076629639, 'test/mean_average_precision': 0.06219830724151265, 'test/num_examples': 43793, 'score': 251.0264482498169, 'total_duration': 530.6267046928406, 'accumulated_submission_time': 251.0264482498169, 'accumulated_eval_time': 279.55322909355164, 'accumulated_logging_time': 0.01755666732788086, 'global_step': 1153, 'preemption_count': 0}), (2294, {'train/accuracy': 0.9879885315895081, 'train/loss': 0.04352015629410744, 'train/mean_average_precision': 0.1451954096378576, 'validation/accuracy': 0.9850970506668091, 'validation/loss': 0.05305362492799759, 'validation/mean_average_precision': 0.13762861196349602, 'validation/num_examples': 43793, 'test/accuracy': 0.984125554561615, 'test/loss': 0.055969685316085815, 'test/mean_average_precision': 0.13806424237419235, 'test/num_examples': 43793, 'score': 491.0974769592285, 'total_duration': 845.8796153068542, 'accumulated_submission_time': 491.0974769592285, 'accumulated_eval_time': 354.68038988113403, 'accumulated_logging_time': 0.036165714263916016, 'global_step': 2294, 'preemption_count': 0}), (3460, {'train/accuracy': 0.9879725575447083, 'train/loss': 0.04192984849214554, 'train/mean_average_precision': 0.18725180596473615, 'validation/accuracy': 0.9851510524749756, 'validation/loss': 0.051309239119291306, 'validation/mean_average_precision': 0.17007330437568605, 'validation/num_examples': 43793, 'test/accuracy': 0.9841432571411133, 'test/loss': 0.05397097021341324, 'test/mean_average_precision': 0.16847132039786678, 'test/num_examples': 43793, 'score': 731.1187317371368, 'total_duration': 1162.6696939468384, 'accumulated_submission_time': 731.1187317371368, 'accumulated_eval_time': 431.4030804634094, 'accumulated_logging_time': 0.053813934326171875, 'global_step': 3460, 'preemption_count': 0}), (4609, {'train/accuracy': 0.9885240197181702, 'train/loss': 0.039675094187259674, 'train/mean_average_precision': 0.20599443917842142, 'validation/accuracy': 0.9857413172721863, 'validation/loss': 0.04858216643333435, 'validation/mean_average_precision': 0.18493339896818412, 'validation/num_examples': 43793, 'test/accuracy': 0.9848841428756714, 'test/loss': 0.051082391291856766, 'test/mean_average_precision': 0.18672561271295926, 'test/num_examples': 43793, 'score': 971.2598721981049, 'total_duration': 1477.7921361923218, 'accumulated_submission_time': 971.2598721981049, 'accumulated_eval_time': 506.3351764678955, 'accumulated_logging_time': 0.07214069366455078, 'global_step': 4609, 'preemption_count': 0}), (5774, {'train/accuracy': 0.9888306856155396, 'train/loss': 0.038068272173404694, 'train/mean_average_precision': 0.25070116099068346, 'validation/accuracy': 0.9857628345489502, 'validation/loss': 0.04819110408425331, 'validation/mean_average_precision': 0.1981952354630591, 'validation/num_examples': 43793, 'test/accuracy': 0.9848681092262268, 'test/loss': 0.05086361616849899, 'test/mean_average_precision': 0.193845023724409, 'test/num_examples': 43793, 'score': 1211.225421667099, 'total_duration': 1793.2880549430847, 'accumulated_submission_time': 1211.225421667099, 'accumulated_eval_time': 581.8141956329346, 'accumulated_logging_time': 0.09082365036010742, 'global_step': 5774, 'preemption_count': 0}), (6930, {'train/accuracy': 0.9890471696853638, 'train/loss': 0.0371469222009182, 'train/mean_average_precision': 0.24498108487007086, 'validation/accuracy': 0.9860413074493408, 'validation/loss': 0.046656422317028046, 'validation/mean_average_precision': 0.21532321223722267, 'validation/num_examples': 43793, 'test/accuracy': 0.9851861000061035, 'test/loss': 0.04937116429209709, 'test/mean_average_precision': 0.20953894637990533, 'test/num_examples': 43793, 'score': 1451.207303762436, 'total_duration': 2110.0183398723602, 'accumulated_submission_time': 1451.207303762436, 'accumulated_eval_time': 658.5132474899292, 'accumulated_logging_time': 0.10972833633422852, 'global_step': 6930, 'preemption_count': 0}), (8091, {'train/accuracy': 0.9894515872001648, 'train/loss': 0.03556307032704353, 'train/mean_average_precision': 0.299316865914813, 'validation/accuracy': 0.9863169193267822, 'validation/loss': 0.04597592353820801, 'validation/mean_average_precision': 0.23158137446964153, 'validation/num_examples': 43793, 'test/accuracy': 0.985472559928894, 'test/loss': 0.04848050698637962, 'test/mean_average_precision': 0.22282319410826892, 'test/num_examples': 43793, 'score': 1691.295039653778, 'total_duration': 2425.2494192123413, 'accumulated_submission_time': 1691.295039653778, 'accumulated_eval_time': 733.6040587425232, 'accumulated_logging_time': 0.13005828857421875, 'global_step': 8091, 'preemption_count': 0}), (9257, {'train/accuracy': 0.9897158145904541, 'train/loss': 0.03479438275098801, 'train/mean_average_precision': 0.3059435112338704, 'validation/accuracy': 0.9864736199378967, 'validation/loss': 0.04549968242645264, 'validation/mean_average_precision': 0.24027741120070423, 'validation/num_examples': 43793, 'test/accuracy': 0.9856157302856445, 'test/loss': 0.04819253832101822, 'test/mean_average_precision': 0.237937431740224, 'test/num_examples': 43793, 'score': 1931.2635045051575, 'total_duration': 2740.6375207901, 'accumulated_submission_time': 1931.2635045051575, 'accumulated_eval_time': 808.9742133617401, 'accumulated_logging_time': 0.1501317024230957, 'global_step': 9257, 'preemption_count': 0}), (10412, {'train/accuracy': 0.989997386932373, 'train/loss': 0.03330015763640404, 'train/mean_average_precision': 0.34457817370553345, 'validation/accuracy': 0.9865730404853821, 'validation/loss': 0.04517685994505882, 'validation/mean_average_precision': 0.24373563617692223, 'validation/num_examples': 43793, 'test/accuracy': 0.9857088327407837, 'test/loss': 0.047590598464012146, 'test/mean_average_precision': 0.2419470619588008, 'test/num_examples': 43793, 'score': 2171.371392726898, 'total_duration': 3055.6293666362762, 'accumulated_submission_time': 2171.371392726898, 'accumulated_eval_time': 883.8067071437836, 'accumulated_logging_time': 0.16864824295043945, 'global_step': 10412, 'preemption_count': 0}), (11583, {'train/accuracy': 0.990007221698761, 'train/loss': 0.033334046602249146, 'train/mean_average_precision': 0.33691781754249356, 'validation/accuracy': 0.9865267872810364, 'validation/loss': 0.04502268135547638, 'validation/mean_average_precision': 0.2502698204058094, 'validation/num_examples': 43793, 'test/accuracy': 0.9856696724891663, 'test/loss': 0.04767971858382225, 'test/mean_average_precision': 0.23946808405373068, 'test/num_examples': 43793, 'score': 2411.47766828537, 'total_duration': 3369.488542318344, 'accumulated_submission_time': 2411.47766828537, 'accumulated_eval_time': 957.5081422328949, 'accumulated_logging_time': 0.1887226104736328, 'global_step': 11583, 'preemption_count': 0}), (12740, {'train/accuracy': 0.9903923273086548, 'train/loss': 0.031664155423641205, 'train/mean_average_precision': 0.3782949305561416, 'validation/accuracy': 0.9866716861724854, 'validation/loss': 0.04482003301382065, 'validation/mean_average_precision': 0.2579583473552823, 'validation/num_examples': 43793, 'test/accuracy': 0.9857690334320068, 'test/loss': 0.04754732549190521, 'test/mean_average_precision': 0.24710784611764655, 'test/num_examples': 43793, 'score': 2651.5365748405457, 'total_duration': 3685.2935013771057, 'accumulated_submission_time': 2651.5365748405457, 'accumulated_eval_time': 1033.2042524814606, 'accumulated_logging_time': 0.2071077823638916, 'global_step': 12740, 'preemption_count': 0}), (13894, {'train/accuracy': 0.9904219508171082, 'train/loss': 0.03176099434494972, 'train/mean_average_precision': 0.3657467819493239, 'validation/accuracy': 0.9867581725120544, 'validation/loss': 0.044302795082330704, 'validation/mean_average_precision': 0.2583058038460325, 'validation/num_examples': 43793, 'test/accuracy': 0.9858949780464172, 'test/loss': 0.04674059525132179, 'test/mean_average_precision': 0.25534803714699794, 'test/num_examples': 43793, 'score': 2891.484347820282, 'total_duration': 4001.7441952228546, 'accumulated_submission_time': 2891.484347820282, 'accumulated_eval_time': 1109.6525075435638, 'accumulated_logging_time': 0.2286546230316162, 'global_step': 13894, 'preemption_count': 0}), (15044, {'train/accuracy': 0.9907400608062744, 'train/loss': 0.030720708891749382, 'train/mean_average_precision': 0.4013069447816966, 'validation/accuracy': 0.9867216348648071, 'validation/loss': 0.04430534318089485, 'validation/mean_average_precision': 0.2577423415214058, 'validation/num_examples': 43793, 'test/accuracy': 0.9858533143997192, 'test/loss': 0.04703787714242935, 'test/mean_average_precision': 0.248197106573553, 'test/num_examples': 43793, 'score': 3131.580348968506, 'total_duration': 4319.02574467659, 'accumulated_submission_time': 3131.580348968506, 'accumulated_eval_time': 1186.7891039848328, 'accumulated_logging_time': 0.24853157997131348, 'global_step': 15044, 'preemption_count': 0}), (16193, {'train/accuracy': 0.9905843734741211, 'train/loss': 0.031096139922738075, 'train/mean_average_precision': 0.3969842596998022, 'validation/accuracy': 0.9867358207702637, 'validation/loss': 0.04470790922641754, 'validation/mean_average_precision': 0.2621638331407883, 'validation/num_examples': 43793, 'test/accuracy': 0.9859779477119446, 'test/loss': 0.04717910662293434, 'test/mean_average_precision': 0.2546289583238784, 'test/num_examples': 43793, 'score': 3371.7145071029663, 'total_duration': 4636.463050365448, 'accumulated_submission_time': 3371.7145071029663, 'accumulated_eval_time': 1264.0427362918854, 'accumulated_logging_time': 0.2681407928466797, 'global_step': 16193, 'preemption_count': 0}), (17328, {'train/accuracy': 0.9911754131317139, 'train/loss': 0.028882037848234177, 'train/mean_average_precision': 0.4522144656967047, 'validation/accuracy': 0.9868113398551941, 'validation/loss': 0.04432009160518646, 'validation/mean_average_precision': 0.262738298096607, 'validation/num_examples': 43793, 'test/accuracy': 0.9859063625335693, 'test/loss': 0.047086577862501144, 'test/mean_average_precision': 0.24627107605311116, 'test/num_examples': 43793, 'score': 3611.812557697296, 'total_duration': 4952.557509183884, 'accumulated_submission_time': 3611.812557697296, 'accumulated_eval_time': 1339.9806487560272, 'accumulated_logging_time': 0.29499173164367676, 'global_step': 17328, 'preemption_count': 0}), (18481, {'train/accuracy': 0.9908531904220581, 'train/loss': 0.029987307265400887, 'train/mean_average_precision': 0.4154749120014567, 'validation/accuracy': 0.9867143034934998, 'validation/loss': 0.04461009055376053, 'validation/mean_average_precision': 0.2608074419654577, 'validation/num_examples': 43793, 'test/accuracy': 0.9859265685081482, 'test/loss': 0.04705549776554108, 'test/mean_average_precision': 0.2517894353220968, 'test/num_examples': 43793, 'score': 3851.803772687912, 'total_duration': 5267.146325111389, 'accumulated_submission_time': 3851.803772687912, 'accumulated_eval_time': 1414.528757095337, 'accumulated_logging_time': 0.3140878677368164, 'global_step': 18481, 'preemption_count': 0}), (19630, {'train/accuracy': 0.9913822412490845, 'train/loss': 0.028417296707630157, 'train/mean_average_precision': 0.45791254130624093, 'validation/accuracy': 0.9867870211601257, 'validation/loss': 0.04411396011710167, 'validation/mean_average_precision': 0.26399632994906136, 'validation/num_examples': 43793, 'test/accuracy': 0.9858823418617249, 'test/loss': 0.046861011534929276, 'test/mean_average_precision': 0.2537314853081531, 'test/num_examples': 43793, 'score': 4091.7909755706787, 'total_duration': 5583.007893562317, 'accumulated_submission_time': 4091.7909755706787, 'accumulated_eval_time': 1490.351684808731, 'accumulated_logging_time': 0.3346543312072754, 'global_step': 19630, 'preemption_count': 0}), (20786, {'train/accuracy': 0.9910698533058167, 'train/loss': 0.02939610742032528, 'train/mean_average_precision': 0.41853526120415635, 'validation/accuracy': 0.9867720007896423, 'validation/loss': 0.04439274221658707, 'validation/mean_average_precision': 0.26043943106572215, 'validation/num_examples': 43793, 'test/accuracy': 0.9859459400177002, 'test/loss': 0.046998031437397, 'test/mean_average_precision': 0.2526544455502668, 'test/num_examples': 43793, 'score': 4331.866239309311, 'total_duration': 5899.657552480698, 'accumulated_submission_time': 4331.866239309311, 'accumulated_eval_time': 1566.8755052089691, 'accumulated_logging_time': 0.35552501678466797, 'global_step': 20786, 'preemption_count': 0}), (21925, {'train/accuracy': 0.9915573000907898, 'train/loss': 0.027721866965293884, 'train/mean_average_precision': 0.47724112835895405, 'validation/accuracy': 0.9868429899215698, 'validation/loss': 0.04434831067919731, 'validation/mean_average_precision': 0.26843524902494664, 'validation/num_examples': 43793, 'test/accuracy': 0.986050009727478, 'test/loss': 0.04705188423395157, 'test/mean_average_precision': 0.2641816032352969, 'test/num_examples': 43793, 'score': 4571.949301481247, 'total_duration': 6215.284602880478, 'accumulated_submission_time': 4571.949301481247, 'accumulated_eval_time': 1642.3690094947815, 'accumulated_logging_time': 0.3762385845184326, 'global_step': 21925, 'preemption_count': 0}), (23075, {'train/accuracy': 0.9913856387138367, 'train/loss': 0.028239788487553596, 'train/mean_average_precision': 0.4596777676613564, 'validation/accuracy': 0.9868271946907043, 'validation/loss': 0.044665999710559845, 'validation/mean_average_precision': 0.2763591089913163, 'validation/num_examples': 43793, 'test/accuracy': 0.9859430193901062, 'test/loss': 0.04744134098291397, 'test/mean_average_precision': 0.25927604024938977, 'test/num_examples': 43793, 'score': 4812.061283826828, 'total_duration': 6530.632303953171, 'accumulated_submission_time': 4812.061283826828, 'accumulated_eval_time': 1717.5522155761719, 'accumulated_logging_time': 0.3970015048980713, 'global_step': 23075, 'preemption_count': 0}), (24216, {'train/accuracy': 0.9916201233863831, 'train/loss': 0.027259131893515587, 'train/mean_average_precision': 0.4873494983231855, 'validation/accuracy': 0.9869607090950012, 'validation/loss': 0.04439434036612511, 'validation/mean_average_precision': 0.2673983363692704, 'validation/num_examples': 43793, 'test/accuracy': 0.9860592484474182, 'test/loss': 0.04723324254155159, 'test/mean_average_precision': 0.2590070854199632, 'test/num_examples': 43793, 'score': 5052.038455247879, 'total_duration': 6844.014355659485, 'accumulated_submission_time': 5052.038455247879, 'accumulated_eval_time': 1790.9061782360077, 'accumulated_logging_time': 0.41887831687927246, 'global_step': 24216, 'preemption_count': 0}), (25362, {'train/accuracy': 0.9917541146278381, 'train/loss': 0.026984816417098045, 'train/mean_average_precision': 0.4709887389471783, 'validation/accuracy': 0.9869915843009949, 'validation/loss': 0.044340845197439194, 'validation/mean_average_precision': 0.2710634574902589, 'validation/num_examples': 43793, 'test/accuracy': 0.9861177802085876, 'test/loss': 0.047006674110889435, 'test/mean_average_precision': 0.2585957858772502, 'test/num_examples': 43793, 'score': 5292.018476009369, 'total_duration': 7157.8054530620575, 'accumulated_submission_time': 5292.018476009369, 'accumulated_eval_time': 1864.663691997528, 'accumulated_logging_time': 0.4414689540863037, 'global_step': 25362, 'preemption_count': 0}), (26502, {'train/accuracy': 0.9918228983879089, 'train/loss': 0.026586053892970085, 'train/mean_average_precision': 0.5006675419963024, 'validation/accuracy': 0.986851155757904, 'validation/loss': 0.04470193758606911, 'validation/mean_average_precision': 0.27152355704414244, 'validation/num_examples': 43793, 'test/accuracy': 0.9859699606895447, 'test/loss': 0.04762403294444084, 'test/mean_average_precision': 0.2543782821620701, 'test/num_examples': 43793, 'score': 5532.096855640411, 'total_duration': 7471.087512969971, 'accumulated_submission_time': 5532.096855640411, 'accumulated_eval_time': 1937.8104083538055, 'accumulated_logging_time': 0.46369171142578125, 'global_step': 26502, 'preemption_count': 0}), (27639, {'train/accuracy': 0.9920358657836914, 'train/loss': 0.02584187313914299, 'train/mean_average_precision': 0.5136132396142394, 'validation/accuracy': 0.9868263602256775, 'validation/loss': 0.045205455273389816, 'validation/mean_average_precision': 0.2640618503547045, 'validation/num_examples': 43793, 'test/accuracy': 0.9859623908996582, 'test/loss': 0.048152439296245575, 'test/mean_average_precision': 0.2594478785245785, 'test/num_examples': 43793, 'score': 5772.063252925873, 'total_duration': 7787.325950145721, 'accumulated_submission_time': 5772.063252925873, 'accumulated_eval_time': 2014.030842781067, 'accumulated_logging_time': 0.48607516288757324, 'global_step': 27639, 'preemption_count': 0}), (28780, {'train/accuracy': 0.9919737577438354, 'train/loss': 0.02603919990360737, 'train/mean_average_precision': 0.5035246140586245, 'validation/accuracy': 0.9868036508560181, 'validation/loss': 0.045159272849559784, 'validation/mean_average_precision': 0.2677179756955803, 'validation/num_examples': 43793, 'test/accuracy': 0.9860049486160278, 'test/loss': 0.04824861139059067, 'test/mean_average_precision': 0.25463802983495476, 'test/num_examples': 43793, 'score': 6011.83998632431, 'total_duration': 8103.724343299866, 'accumulated_submission_time': 6011.83998632431, 'accumulated_eval_time': 2090.3996601104736, 'accumulated_logging_time': 0.7079660892486572, 'global_step': 28780, 'preemption_count': 0}), (29926, {'train/accuracy': 0.9924474954605103, 'train/loss': 0.024343539029359818, 'train/mean_average_precision': 0.5518010756451396, 'validation/accuracy': 0.9869071245193481, 'validation/loss': 0.044991057366132736, 'validation/mean_average_precision': 0.2729831598031316, 'validation/num_examples': 43793, 'test/accuracy': 0.9860466122627258, 'test/loss': 0.048020150512456894, 'test/mean_average_precision': 0.2602955732366308, 'test/num_examples': 43793, 'score': 6251.950050830841, 'total_duration': 8417.420310735703, 'accumulated_submission_time': 6251.950050830841, 'accumulated_eval_time': 2163.932371854782, 'accumulated_logging_time': 0.7319021224975586, 'global_step': 29926, 'preemption_count': 0}), (31062, {'train/accuracy': 0.9922090172767639, 'train/loss': 0.025188639760017395, 'train/mean_average_precision': 0.5256656340450574, 'validation/accuracy': 0.9868621230125427, 'validation/loss': 0.04534360021352768, 'validation/mean_average_precision': 0.2696833580037658, 'validation/num_examples': 43793, 'test/accuracy': 0.9859977960586548, 'test/loss': 0.04850677773356438, 'test/mean_average_precision': 0.2620445328750984, 'test/num_examples': 43793, 'score': 6491.935678720474, 'total_duration': 8727.216063976288, 'accumulated_submission_time': 6491.935678720474, 'accumulated_eval_time': 2233.6875994205475, 'accumulated_logging_time': 0.7535703182220459, 'global_step': 31062, 'preemption_count': 0}), (32204, {'train/accuracy': 0.9929726123809814, 'train/loss': 0.022708307951688766, 'train/mean_average_precision': 0.578941784430997, 'validation/accuracy': 0.9868385791778564, 'validation/loss': 0.04554077237844467, 'validation/mean_average_precision': 0.27317436290175606, 'validation/num_examples': 43793, 'test/accuracy': 0.9859535694122314, 'test/loss': 0.04854872077703476, 'test/mean_average_precision': 0.2670333447573229, 'test/num_examples': 43793, 'score': 6731.969523191452, 'total_duration': 9044.592772722244, 'accumulated_submission_time': 6731.969523191452, 'accumulated_eval_time': 2310.9796299934387, 'accumulated_logging_time': 0.7754356861114502, 'global_step': 32204, 'preemption_count': 0}), (33339, {'train/accuracy': 0.9925864338874817, 'train/loss': 0.023776967078447342, 'train/mean_average_precision': 0.550771518938405, 'validation/accuracy': 0.9868393540382385, 'validation/loss': 0.04593701288104057, 'validation/mean_average_precision': 0.26922325234352384, 'validation/num_examples': 43793, 'test/accuracy': 0.9860407114028931, 'test/loss': 0.049088094383478165, 'test/mean_average_precision': 0.25755181614528694, 'test/num_examples': 43793, 'score': 6971.9351625442505, 'total_duration': 9357.907524347305, 'accumulated_submission_time': 6971.9351625442505, 'accumulated_eval_time': 2384.278130531311, 'accumulated_logging_time': 0.7969334125518799, 'global_step': 33339, 'preemption_count': 0}), (34485, {'train/accuracy': 0.9936516880989075, 'train/loss': 0.020687054842710495, 'train/mean_average_precision': 0.6256037355055908, 'validation/accuracy': 0.986772358417511, 'validation/loss': 0.045973245054483414, 'validation/mean_average_precision': 0.27006564349832346, 'validation/num_examples': 43793, 'test/accuracy': 0.9859750270843506, 'test/loss': 0.049092233180999756, 'test/mean_average_precision': 0.2605718871488263, 'test/num_examples': 43793, 'score': 7212.077303647995, 'total_duration': 9673.51255273819, 'accumulated_submission_time': 7212.077303647995, 'accumulated_eval_time': 2459.690827846527, 'accumulated_logging_time': 0.8182566165924072, 'global_step': 34485, 'preemption_count': 0}), (35622, {'train/accuracy': 0.993052065372467, 'train/loss': 0.022436054423451424, 'train/mean_average_precision': 0.5773628968619146, 'validation/accuracy': 0.9867541193962097, 'validation/loss': 0.046330202370882034, 'validation/mean_average_precision': 0.2695483021539162, 'validation/num_examples': 43793, 'test/accuracy': 0.9859308004379272, 'test/loss': 0.049470216035842896, 'test/mean_average_precision': 0.26252300646751353, 'test/num_examples': 43793, 'score': 7452.0422904491425, 'total_duration': 9986.51591038704, 'accumulated_submission_time': 7452.0422904491425, 'accumulated_eval_time': 2532.674788713455, 'accumulated_logging_time': 0.839911937713623, 'global_step': 35622, 'preemption_count': 0}), (36764, {'train/accuracy': 0.9940528273582458, 'train/loss': 0.019451990723609924, 'train/mean_average_precision': 0.6459853723001685, 'validation/accuracy': 0.9867139458656311, 'validation/loss': 0.046674732118844986, 'validation/mean_average_precision': 0.27310003656797, 'validation/num_examples': 43793, 'test/accuracy': 0.9858596324920654, 'test/loss': 0.05010291934013367, 'test/mean_average_precision': 0.2584128058239561, 'test/num_examples': 43793, 'score': 7692.057778835297, 'total_duration': 10302.97818517685, 'accumulated_submission_time': 7692.057778835297, 'accumulated_eval_time': 2609.0686905384064, 'accumulated_logging_time': 0.861299991607666, 'global_step': 36764, 'preemption_count': 0}), (37907, {'train/accuracy': 0.9934616684913635, 'train/loss': 0.020867303013801575, 'train/mean_average_precision': 0.6037326811666275, 'validation/accuracy': 0.9866449236869812, 'validation/loss': 0.04767531901597977, 'validation/mean_average_precision': 0.26764534911264387, 'validation/num_examples': 43793, 'test/accuracy': 0.9857770800590515, 'test/loss': 0.051052525639534, 'test/mean_average_precision': 0.2504274527622097, 'test/num_examples': 43793, 'score': 7932.1015610694885, 'total_duration': 10616.9850628376, 'accumulated_submission_time': 7932.1015610694885, 'accumulated_eval_time': 2682.976849794388, 'accumulated_logging_time': 0.8849091529846191, 'global_step': 37907, 'preemption_count': 0}), (39048, {'train/accuracy': 0.99444180727005, 'train/loss': 0.018256405368447304, 'train/mean_average_precision': 0.6654711556176374, 'validation/accuracy': 0.9867054224014282, 'validation/loss': 0.0479443296790123, 'validation/mean_average_precision': 0.26512632998949415, 'validation/num_examples': 43793, 'test/accuracy': 0.9857627153396606, 'test/loss': 0.051474783569574356, 'test/mean_average_precision': 0.2593348225602123, 'test/num_examples': 43793, 'score': 8172.051128864288, 'total_duration': 10933.573536396027, 'accumulated_submission_time': 8172.051128864288, 'accumulated_eval_time': 2759.5637917518616, 'accumulated_logging_time': 0.9068722724914551, 'global_step': 39048, 'preemption_count': 0}), (40196, {'train/accuracy': 0.9938923120498657, 'train/loss': 0.019340278580784798, 'train/mean_average_precision': 0.6425472444075877, 'validation/accuracy': 0.9865856766700745, 'validation/loss': 0.04878591001033783, 'validation/mean_average_precision': 0.2681716221674736, 'validation/num_examples': 43793, 'test/accuracy': 0.9857202172279358, 'test/loss': 0.05220598727464676, 'test/mean_average_precision': 0.2573468907324039, 'test/num_examples': 43793, 'score': 8412.181686162949, 'total_duration': 11247.618238687515, 'accumulated_submission_time': 8412.181686162949, 'accumulated_eval_time': 2833.4213631153107, 'accumulated_logging_time': 0.9336273670196533, 'global_step': 40196, 'preemption_count': 0}), (41335, {'train/accuracy': 0.9947255849838257, 'train/loss': 0.017139816656708717, 'train/mean_average_precision': 0.6878107563981042, 'validation/accuracy': 0.9865812063217163, 'validation/loss': 0.049281805753707886, 'validation/mean_average_precision': 0.26648704314260613, 'validation/num_examples': 43793, 'test/accuracy': 0.9856431484222412, 'test/loss': 0.05294788256287575, 'test/mean_average_precision': 0.25068989395746444, 'test/num_examples': 43793, 'score': 8652.21815085411, 'total_duration': 11565.189492702484, 'accumulated_submission_time': 8652.21815085411, 'accumulated_eval_time': 2910.9009759426117, 'accumulated_logging_time': 0.9580135345458984, 'global_step': 41335, 'preemption_count': 0}), (42478, {'train/accuracy': 0.9945451617240906, 'train/loss': 0.017343349754810333, 'train/mean_average_precision': 0.6889113238011881, 'validation/accuracy': 0.9864732027053833, 'validation/loss': 0.05091932788491249, 'validation/mean_average_precision': 0.25889919881570533, 'validation/num_examples': 43793, 'test/accuracy': 0.9856178760528564, 'test/loss': 0.0546247698366642, 'test/mean_average_precision': 0.2515059617449239, 'test/num_examples': 43793, 'score': 8892.246052980423, 'total_duration': 11880.864127397537, 'accumulated_submission_time': 8892.246052980423, 'accumulated_eval_time': 2986.493406534195, 'accumulated_logging_time': 0.9803786277770996, 'global_step': 42478, 'preemption_count': 0}), (43620, {'train/accuracy': 0.9944514036178589, 'train/loss': 0.01761283166706562, 'train/mean_average_precision': 0.6719254061668964, 'validation/accuracy': 0.9863747358322144, 'validation/loss': 0.05124706029891968, 'validation/mean_average_precision': 0.25767455289076446, 'validation/num_examples': 43793, 'test/accuracy': 0.9855567812919617, 'test/loss': 0.05482589825987816, 'test/mean_average_precision': 0.2460800162131067, 'test/num_examples': 43793, 'score': 9132.233981370926, 'total_duration': 12196.363843679428, 'accumulated_submission_time': 9132.233981370926, 'accumulated_eval_time': 3061.9480957984924, 'accumulated_logging_time': 1.0067880153656006, 'global_step': 43620, 'preemption_count': 0}), (44759, {'train/accuracy': 0.9954023957252502, 'train/loss': 0.014974188059568405, 'train/mean_average_precision': 0.7171394340911992, 'validation/accuracy': 0.9863436818122864, 'validation/loss': 0.05178181082010269, 'validation/mean_average_precision': 0.25856288286675394, 'validation/num_examples': 43793, 'test/accuracy': 0.9854750633239746, 'test/loss': 0.055704325437545776, 'test/mean_average_precision': 0.24768985358280335, 'test/num_examples': 43793, 'score': 9372.377534627914, 'total_duration': 12510.583525180817, 'accumulated_submission_time': 9372.377534627914, 'accumulated_eval_time': 3135.9676780700684, 'accumulated_logging_time': 1.0303173065185547, 'global_step': 44759, 'preemption_count': 0}), (45914, {'train/accuracy': 0.9951494932174683, 'train/loss': 0.015523905865848064, 'train/mean_average_precision': 0.7159863821791338, 'validation/accuracy': 0.9863254427909851, 'validation/loss': 0.052862610667943954, 'validation/mean_average_precision': 0.26114937366781965, 'validation/num_examples': 43793, 'test/accuracy': 0.9853769540786743, 'test/loss': 0.05687498301267624, 'test/mean_average_precision': 0.24520633297990038, 'test/num_examples': 43793, 'score': 9612.348465204239, 'total_duration': 12829.444539546967, 'accumulated_submission_time': 9612.348465204239, 'accumulated_eval_time': 3214.8039078712463, 'accumulated_logging_time': 1.054661512374878, 'global_step': 45914, 'preemption_count': 0}), (47056, {'train/accuracy': 0.9960127472877502, 'train/loss': 0.013291527517139912, 'train/mean_average_precision': 0.7616745754592504, 'validation/accuracy': 0.9863388538360596, 'validation/loss': 0.05385197699069977, 'validation/mean_average_precision': 0.2518298577249116, 'validation/num_examples': 43793, 'test/accuracy': 0.9854978322982788, 'test/loss': 0.057752713561058044, 'test/mean_average_precision': 0.24329711282038893, 'test/num_examples': 43793, 'score': 9852.368579387665, 'total_duration': 13143.294612169266, 'accumulated_submission_time': 9852.368579387665, 'accumulated_eval_time': 3288.5756690502167, 'accumulated_logging_time': 1.0794250965118408, 'global_step': 47056, 'preemption_count': 0}), (48211, {'train/accuracy': 0.9951796531677246, 'train/loss': 0.01532951183617115, 'train/mean_average_precision': 0.7219508465405936, 'validation/accuracy': 0.9862767457962036, 'validation/loss': 0.05362145975232124, 'validation/mean_average_precision': 0.2528317094838333, 'validation/num_examples': 43793, 'test/accuracy': 0.985381543636322, 'test/loss': 0.057770851999521255, 'test/mean_average_precision': 0.2435343842369691, 'test/num_examples': 43793, 'score': 10092.339327335358, 'total_duration': 13455.601885795593, 'accumulated_submission_time': 10092.339327335358, 'accumulated_eval_time': 3360.8600544929504, 'accumulated_logging_time': 1.1023645401000977, 'global_step': 48211, 'preemption_count': 0}), (49360, {'train/accuracy': 0.996391236782074, 'train/loss': 0.012640495784580708, 'train/mean_average_precision': 0.7657991704603351, 'validation/accuracy': 0.9862958192825317, 'validation/loss': 0.05396689474582672, 'validation/mean_average_precision': 0.25526750691912903, 'validation/num_examples': 43793, 'test/accuracy': 0.9853689074516296, 'test/loss': 0.05793806537985802, 'test/mean_average_precision': 0.24441569099247903, 'test/num_examples': 43793, 'score': 10332.396535873413, 'total_duration': 13773.266067743301, 'accumulated_submission_time': 10332.396535873413, 'accumulated_eval_time': 3438.4128506183624, 'accumulated_logging_time': 1.12646484375, 'global_step': 49360, 'preemption_count': 0}), (50499, {'train/accuracy': 0.9957308173179626, 'train/loss': 0.013916796073317528, 'train/mean_average_precision': 0.742811462018423, 'validation/accuracy': 0.9863055348396301, 'validation/loss': 0.05398702248930931, 'validation/mean_average_precision': 0.2534965208770851, 'validation/num_examples': 43793, 'test/accuracy': 0.9853870272636414, 'test/loss': 0.05802949517965317, 'test/mean_average_precision': 0.24411861375354624, 'test/num_examples': 43793, 'score': 10572.357874155045, 'total_duration': 14089.720838785172, 'accumulated_submission_time': 10572.357874155045, 'accumulated_eval_time': 3514.851109266281, 'accumulated_logging_time': 1.1524693965911865, 'global_step': 50499, 'preemption_count': 0}), (51628, {'train/accuracy': 0.9962584972381592, 'train/loss': 0.01285596750676632, 'train/mean_average_precision': 0.7718106684367108, 'validation/accuracy': 0.9863160848617554, 'validation/loss': 0.05391433835029602, 'validation/mean_average_precision': 0.25734913504349854, 'validation/num_examples': 43793, 'test/accuracy': 0.9854274988174438, 'test/loss': 0.05794017016887665, 'test/mean_average_precision': 0.24487347660624623, 'test/num_examples': 43793, 'score': 10812.330452680588, 'total_duration': 14404.518035888672, 'accumulated_submission_time': 10812.330452680588, 'accumulated_eval_time': 3589.6173639297485, 'accumulated_logging_time': 1.1785552501678467, 'global_step': 51628, 'preemption_count': 0}), (52767, {'train/accuracy': 0.9962100386619568, 'train/loss': 0.01294196117669344, 'train/mean_average_precision': 0.7580600817690636, 'validation/accuracy': 0.9863229990005493, 'validation/loss': 0.05392349138855934, 'validation/mean_average_precision': 0.2574962308662534, 'validation/num_examples': 43793, 'test/accuracy': 0.9854253530502319, 'test/loss': 0.057959623634815216, 'test/mean_average_precision': 0.24504519932904764, 'test/num_examples': 43793, 'score': 11052.379261493683, 'total_duration': 14721.67795586586, 'accumulated_submission_time': 11052.379261493683, 'accumulated_eval_time': 3666.666816711426, 'accumulated_logging_time': 1.20849609375, 'global_step': 52767, 'preemption_count': 0}), (53897, {'train/accuracy': 0.9960426688194275, 'train/loss': 0.01325035560876131, 'train/mean_average_precision': 0.763330768978438, 'validation/accuracy': 0.9863229990005493, 'validation/loss': 0.05392349138855934, 'validation/mean_average_precision': 0.25738789199153567, 'validation/num_examples': 43793, 'test/accuracy': 0.9854253530502319, 'test/loss': 0.057959623634815216, 'test/mean_average_precision': 0.24521343681626645, 'test/num_examples': 43793, 'score': 11292.401823997498, 'total_duration': 15037.522889614105, 'accumulated_submission_time': 11292.401823997498, 'accumulated_eval_time': 3742.43364071846, 'accumulated_logging_time': 1.2331297397613525, 'global_step': 53897, 'preemption_count': 0}), (55035, {'train/accuracy': 0.9960747957229614, 'train/loss': 0.013246160000562668, 'train/mean_average_precision': 0.7594668980995638, 'validation/accuracy': 0.9863229990005493, 'validation/loss': 0.05392349138855934, 'validation/mean_average_precision': 0.2573318493412903, 'validation/num_examples': 43793, 'test/accuracy': 0.9854253530502319, 'test/loss': 0.057959623634815216, 'test/mean_average_precision': 0.24495229886100672, 'test/num_examples': 43793, 'score': 11532.362166404724, 'total_duration': 15354.792936086655, 'accumulated_submission_time': 11532.362166404724, 'accumulated_eval_time': 3819.688876390457, 'accumulated_logging_time': 1.2577683925628662, 'global_step': 55035, 'preemption_count': 0}), (56176, {'train/accuracy': 0.9961246252059937, 'train/loss': 0.013065858744084835, 'train/mean_average_precision': 0.7479835836133831, 'validation/accuracy': 0.9863229990005493, 'validation/loss': 0.05392349138855934, 'validation/mean_average_precision': 0.25751096968548215, 'validation/num_examples': 43793, 'test/accuracy': 0.9854253530502319, 'test/loss': 0.057959623634815216, 'test/mean_average_precision': 0.24492626962904432, 'test/num_examples': 43793, 'score': 11772.329397916794, 'total_duration': 15665.839257240295, 'accumulated_submission_time': 11772.329397916794, 'accumulated_eval_time': 3890.710768699646, 'accumulated_logging_time': 1.2844185829162598, 'global_step': 56176, 'preemption_count': 0})], 'global_step': 57323}
I0305 23:38:00.662544 140626292712640 submission_runner.py:649] Timing: 12012.414421796799
I0305 23:38:00.662582 140626292712640 submission_runner.py:651] Total number of evals: 50
I0305 23:38:00.662617 140626292712640 submission_runner.py:652] ====================
I0305 23:38:00.662968 140626292712640 submission_runner.py:750] Final ogbg score: 0
