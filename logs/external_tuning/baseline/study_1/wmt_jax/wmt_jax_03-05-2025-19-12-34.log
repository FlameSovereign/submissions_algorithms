python submission_runner.py --framework=jax --workload=wmt --submission_path=prize_qualification_baselines/external_tuning/jax_nadamw_full_budget.py --data_dir=/data/wmt --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=submissions/rolling_leaderboard/external_tuning/baseline/study_1 --overwrite=True --save_checkpoints=False --rng_seed=-1216057962 --tuning_ruleset=external --tuning_search_space=prize_qualification_baselines/external_tuning/tuning_search_space.json --num_tuning_trials=5 --hparam_start_index=0 --hparam_end_index=1 2>&1 | tee -a /logs/wmt_jax_03-05-2025-19-12-34.log
2025-03-05 19:12:35.897477: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1741201955.921263       9 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1741201955.928276       9 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
I0305 19:12:42.826590 140477253723328 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_1/wmt_jax.
I0305 19:12:43.853135 140477253723328 xla_bridge.py:884] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA
I0305 19:12:43.856186 140477253723328 xla_bridge.py:884] Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory
I0305 19:12:43.857818 140477253723328 submission_runner.py:606] Using RNG seed -1216057962
I0305 19:12:44.452644 140477253723328 submission_runner.py:615] --- Tuning run 1/5 ---
I0305 19:12:44.452816 140477253723328 submission_runner.py:620] Creating tuning directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_1/wmt_jax/trial_1.
I0305 19:12:44.452992 140477253723328 logger_utils.py:97] Saving hparams to /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_1/wmt_jax/trial_1/hparams.json.
I0305 19:12:44.695789 140477253723328 submission_runner.py:218] Initializing dataset.
I0305 19:12:44.848206 140477253723328 dataset_info.py:690] Load dataset info from /data/wmt/wmt17_translate/de-en/1.0.0
I0305 19:12:44.855985 140477253723328 reader.py:261] Creating a tf.data.Dataset reading 16 files located in folders: /data/wmt/wmt17_translate/de-en/1.0.0.
I0305 19:12:44.930851 140477253723328 logging_logger.py:49] Constructing tf.data.Dataset wmt17_translate for split train, from /data/wmt/wmt17_translate/de-en/1.0.0
I0305 19:12:46.333117 140477253723328 submission_runner.py:229] Initializing model.
I0305 19:13:28.697995 140477253723328 submission_runner.py:272] Initializing optimizer.
I0305 19:13:29.553345 140477253723328 submission_runner.py:279] Initializing metrics bundle.
I0305 19:13:29.553625 140477253723328 submission_runner.py:301] Initializing checkpoint and logger.
I0305 19:13:29.554830 140477253723328 checkpoints.py:1101] Found no checkpoint files in /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_1/wmt_jax/trial_1 with prefix checkpoint_
I0305 19:13:29.554936 140477253723328 submission_runner.py:321] Saving meta data to /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_1/wmt_jax/trial_1/meta_data_0.json.
I0305 19:13:29.555164 140477253723328 logger_utils.py:262] Unable to record workload.train_mean information. Continuing without it.
I0305 19:13:29.555213 140477253723328 logger_utils.py:262] Unable to record workload.train_stddev information. Continuing without it.
I0305 19:13:29.735674 140477253723328 submission_runner.py:325] Saving flags to /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_1/wmt_jax/trial_1/flags_0.json.
I0305 19:13:29.774762 140477253723328 submission_runner.py:337] Starting training loop.
I0305 19:13:55.886737 140341076195072 logging_writer.py:48] [0] global_step=0, grad_norm=5.65512228012085, loss=11.027143478393555
I0305 19:13:55.949070 140477253723328 spec.py:321] Evaluating on the training split.
I0305 19:13:55.951283 140477253723328 dataset_info.py:690] Load dataset info from /data/wmt/wmt17_translate/de-en/1.0.0
I0305 19:13:55.954959 140477253723328 reader.py:261] Creating a tf.data.Dataset reading 16 files located in folders: /data/wmt/wmt17_translate/de-en/1.0.0.
I0305 19:13:55.987286 140477253723328 logging_logger.py:49] Constructing tf.data.Dataset wmt17_translate for split train, from /data/wmt/wmt17_translate/de-en/1.0.0
I0305 19:14:02.162399 140477253723328 workload.py:181] Translating evaluation dataset.
I0305 19:19:10.414526 140477253723328 spec.py:333] Evaluating on the validation split.
I0305 19:19:10.424422 140477253723328 dataset_info.py:690] Load dataset info from /data/wmt/wmt14_translate/de-en/1.0.0
I0305 19:19:10.430117 140477253723328 reader.py:261] Creating a tf.data.Dataset reading 1 files located in folders: /data/wmt/wmt14_translate/de-en/1.0.0.
I0305 19:19:10.462971 140477253723328 logging_logger.py:49] Constructing tf.data.Dataset wmt14_translate for split validation, from /data/wmt/wmt14_translate/de-en/1.0.0
I0305 19:19:15.589238 140477253723328 workload.py:181] Translating evaluation dataset.
I0305 19:24:17.423595 140477253723328 spec.py:349] Evaluating on the test split.
I0305 19:24:17.426076 140477253723328 dataset_info.py:690] Load dataset info from /data/wmt/wmt14_translate/de-en/1.0.0
I0305 19:24:17.429686 140477253723328 reader.py:261] Creating a tf.data.Dataset reading 1 files located in folders: /data/wmt/wmt14_translate/de-en/1.0.0.
I0305 19:24:17.462258 140477253723328 logging_logger.py:49] Constructing tf.data.Dataset wmt14_translate for split test, from /data/wmt/wmt14_translate/de-en/1.0.0
I0305 19:24:20.292814 140477253723328 workload.py:181] Translating evaluation dataset.
I0305 19:29:21.961267 140477253723328 submission_runner.py:469] Time since start: 952.19s, 	Step: 1, 	{'train/accuracy': 0.0006580962799489498, 'train/loss': 11.021956443786621, 'train/bleu': 0.0, 'validation/accuracy': 0.00048204089398495853, 'validation/loss': 10.987210273742676, 'validation/bleu': 0.0, 'validation/num_examples': 3000, 'test/accuracy': 0.0007183408597484231, 'test/loss': 10.986270904541016, 'test/bleu': 8.198374373802917e-11, 'test/num_examples': 3003, 'score': 26.174192667007446, 'total_duration': 952.1864182949066, 'accumulated_submission_time': 26.174192667007446, 'accumulated_eval_time': 926.0121121406555, 'accumulated_logging_time': 0}
I0305 19:29:21.968836 140334398945024 logging_writer.py:48] [1] accumulated_eval_time=926.012, accumulated_logging_time=0, accumulated_submission_time=26.1742, global_step=1, preemption_count=0, score=26.1742, test/accuracy=0.000718341, test/bleu=8.19837e-11, test/loss=10.9863, test/num_examples=3003, total_duration=952.186, train/accuracy=0.000658096, train/bleu=0, train/loss=11.022, validation/accuracy=0.000482041, validation/bleu=0, validation/loss=10.9872, validation/num_examples=3000
I0305 19:29:56.914308 140334390552320 logging_writer.py:48] [100] global_step=100, grad_norm=0.3588223159313202, loss=8.867615699768066
I0305 19:30:31.722420 140334398945024 logging_writer.py:48] [200] global_step=200, grad_norm=0.14809994399547577, loss=8.551779747009277
I0305 19:31:06.562772 140334390552320 logging_writer.py:48] [300] global_step=300, grad_norm=0.16012515127658844, loss=8.305411338806152
I0305 19:31:41.434230 140334398945024 logging_writer.py:48] [400] global_step=400, grad_norm=0.252092182636261, loss=7.888286113739014
I0305 19:32:16.339288 140334390552320 logging_writer.py:48] [500] global_step=500, grad_norm=0.4464525282382965, loss=7.560279846191406
I0305 19:32:51.257949 140334398945024 logging_writer.py:48] [600] global_step=600, grad_norm=0.7672581672668457, loss=7.386105060577393
I0305 19:33:26.174455 140334390552320 logging_writer.py:48] [700] global_step=700, grad_norm=0.5252801775932312, loss=7.043208122253418
I0305 19:34:01.059215 140334398945024 logging_writer.py:48] [800] global_step=800, grad_norm=0.8121420741081238, loss=6.896628379821777
I0305 19:34:35.956014 140334390552320 logging_writer.py:48] [900] global_step=900, grad_norm=0.5498031377792358, loss=6.6656293869018555
I0305 19:35:10.863121 140334398945024 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.576653242111206, loss=6.44569730758667
I0305 19:35:45.774564 140334390552320 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.7964802980422974, loss=6.284797191619873
I0305 19:36:20.687066 140334398945024 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.6843084692955017, loss=6.120370864868164
I0305 19:36:55.613595 140334390552320 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.5159923434257507, loss=5.90349006652832
I0305 19:37:30.526938 140334398945024 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.7653636336326599, loss=5.8344035148620605
I0305 19:38:05.488138 140334390552320 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.9471785426139832, loss=5.7336015701293945
I0305 19:38:40.437607 140334398945024 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.9461246728897095, loss=5.566594123840332
I0305 19:39:15.353386 140334390552320 logging_writer.py:48] [1700] global_step=1700, grad_norm=1.003356695175171, loss=5.51697301864624
I0305 19:39:50.307730 140334398945024 logging_writer.py:48] [1800] global_step=1800, grad_norm=1.1801269054412842, loss=5.356611251831055
I0305 19:40:25.258867 140334390552320 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.8094611763954163, loss=5.243789196014404
I0305 19:41:00.186434 140334398945024 logging_writer.py:48] [2000] global_step=2000, grad_norm=1.5364865064620972, loss=5.23308801651001
I0305 19:41:35.127940 140334390552320 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.8185708522796631, loss=5.09251594543457
I0305 19:42:10.081296 140334398945024 logging_writer.py:48] [2200] global_step=2200, grad_norm=1.6077464818954468, loss=4.977807521820068
I0305 19:42:45.023308 140334390552320 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.7065540552139282, loss=4.819005489349365
I0305 19:43:19.983121 140334398945024 logging_writer.py:48] [2400] global_step=2400, grad_norm=1.083670735359192, loss=4.732388973236084
I0305 19:43:22.086105 140477253723328 spec.py:321] Evaluating on the training split.
I0305 19:43:24.739573 140477253723328 workload.py:181] Translating evaluation dataset.
I0305 19:47:14.648641 140477253723328 spec.py:333] Evaluating on the validation split.
I0305 19:47:17.291709 140477253723328 workload.py:181] Translating evaluation dataset.
I0305 19:51:17.550265 140477253723328 spec.py:349] Evaluating on the test split.
I0305 19:51:20.200529 140477253723328 workload.py:181] Translating evaluation dataset.
I0305 19:54:59.927549 140477253723328 submission_runner.py:469] Time since start: 2490.15s, 	Step: 2407, 	{'train/accuracy': 0.4233400225639343, 'train/loss': 3.8669471740722656, 'train/bleu': 14.814418358497678, 'validation/accuracy': 0.40810322761535645, 'validation/loss': 3.9832510948181152, 'validation/bleu': 10.311084591769495, 'validation/num_examples': 3000, 'test/accuracy': 0.39360445737838745, 'test/loss': 4.177699089050293, 'test/bleu': 8.527815174181656, 'test/num_examples': 3003, 'score': 866.1140217781067, 'total_duration': 2490.1526987552643, 'accumulated_submission_time': 866.1140217781067, 'accumulated_eval_time': 1623.8534841537476, 'accumulated_logging_time': 0.016554832458496094}
I0305 19:54:59.937291 140334390552320 logging_writer.py:48] [2407] accumulated_eval_time=1623.85, accumulated_logging_time=0.0165548, accumulated_submission_time=866.114, global_step=2407, preemption_count=0, score=866.114, test/accuracy=0.393604, test/bleu=8.52782, test/loss=4.1777, test/num_examples=3003, total_duration=2490.15, train/accuracy=0.42334, train/bleu=14.8144, train/loss=3.86695, validation/accuracy=0.408103, validation/bleu=10.3111, validation/loss=3.98325, validation/num_examples=3000
I0305 19:55:32.738312 140334398945024 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.6860532760620117, loss=4.737420082092285
I0305 19:56:07.650173 140334390552320 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.7459626793861389, loss=4.6552205085754395
I0305 19:56:42.591575 140334398945024 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.7298853993415833, loss=4.580524921417236
I0305 19:57:17.518094 140334390552320 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.7120287418365479, loss=4.444859504699707
I0305 19:57:52.470622 140334398945024 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.6630151271820068, loss=4.413619041442871
I0305 19:58:27.421850 140334390552320 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.5536877512931824, loss=4.385655403137207
I0305 19:59:02.409933 140334398945024 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.6110745668411255, loss=4.2920637130737305
I0305 19:59:37.395114 140334390552320 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.9643232226371765, loss=4.225897312164307
I0305 20:00:12.390302 140334398945024 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.5997474193572998, loss=4.153549671173096
I0305 20:00:47.370676 140334390552320 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.8981562256813049, loss=4.1883039474487305
I0305 20:01:22.372138 140334398945024 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.5845626592636108, loss=4.034327507019043
I0305 20:01:57.355496 140334390552320 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.5919594764709473, loss=4.067367076873779
I0305 20:02:32.338573 140334398945024 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.5999714136123657, loss=3.954087734222412
I0305 20:03:07.346618 140334390552320 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.5612784028053284, loss=3.8915817737579346
I0305 20:03:42.354285 140334398945024 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.8054870963096619, loss=3.9984259605407715
I0305 20:04:17.353017 140334390552320 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.5521644353866577, loss=3.9512221813201904
I0305 20:04:52.321424 140334398945024 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.6218885779380798, loss=3.883338451385498
I0305 20:05:27.281585 140334390552320 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.5167264938354492, loss=3.922438144683838
I0305 20:06:02.239567 140334398945024 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.5429258942604065, loss=3.823753833770752
I0305 20:06:37.217238 140334390552320 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.5089518427848816, loss=3.8589072227478027
I0305 20:07:12.270416 140334398945024 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.5059260129928589, loss=3.7855403423309326
I0305 20:07:47.244255 140334390552320 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.9591483473777771, loss=3.8015639781951904
I0305 20:08:22.241820 140334398945024 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.468769371509552, loss=3.7815370559692383
I0305 20:08:57.233865 140334390552320 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.5373741984367371, loss=3.705122709274292
I0305 20:09:00.042068 140477253723328 spec.py:321] Evaluating on the training split.
I0305 20:09:02.697703 140477253723328 workload.py:181] Translating evaluation dataset.
I0305 20:11:49.306489 140477253723328 spec.py:333] Evaluating on the validation split.
I0305 20:11:51.950044 140477253723328 workload.py:181] Translating evaluation dataset.
I0305 20:14:35.291140 140477253723328 spec.py:349] Evaluating on the test split.
I0305 20:14:37.944902 140477253723328 workload.py:181] Translating evaluation dataset.
I0305 20:17:05.456110 140477253723328 submission_runner.py:469] Time since start: 3815.68s, 	Step: 4809, 	{'train/accuracy': 0.5489763021469116, 'train/loss': 2.6858534812927246, 'train/bleu': 24.82935956471385, 'validation/accuracy': 0.5512199401855469, 'validation/loss': 2.637345552444458, 'validation/bleu': 20.524052804691756, 'validation/num_examples': 3000, 'test/accuracy': 0.5523346066474915, 'test/loss': 2.6668059825897217, 'test/bleu': 19.39920070901915, 'test/num_examples': 3003, 'score': 1706.0506069660187, 'total_duration': 3815.6812827587128, 'accumulated_submission_time': 1706.0506069660187, 'accumulated_eval_time': 2109.267467737198, 'accumulated_logging_time': 0.03451800346374512}
I0305 20:17:05.465495 140334398945024 logging_writer.py:48] [4809] accumulated_eval_time=2109.27, accumulated_logging_time=0.034518, accumulated_submission_time=1706.05, global_step=4809, preemption_count=0, score=1706.05, test/accuracy=0.552335, test/bleu=19.3992, test/loss=2.66681, test/num_examples=3003, total_duration=3815.68, train/accuracy=0.548976, train/bleu=24.8294, train/loss=2.68585, validation/accuracy=0.55122, validation/bleu=20.5241, validation/loss=2.63735, validation/num_examples=3000
I0305 20:17:37.550005 140334390552320 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.5210719704627991, loss=3.7254083156585693
I0305 20:18:12.416754 140334398945024 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.7369074821472168, loss=3.708401679992676
I0305 20:18:47.329859 140334390552320 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.49025774002075195, loss=3.6850287914276123
I0305 20:19:22.249253 140334398945024 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.7926802039146423, loss=3.698392629623413
I0305 20:19:57.174372 140334390552320 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.5300295948982239, loss=3.690678596496582
I0305 20:20:32.101514 140334398945024 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.5951091051101685, loss=3.624892234802246
I0305 20:21:07.036783 140334390552320 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.4599147439002991, loss=3.6526806354522705
I0305 20:21:41.982585 140334398945024 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.5192123651504517, loss=3.5025274753570557
I0305 20:22:16.929950 140334390552320 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.48911985754966736, loss=3.580867052078247
I0305 20:22:51.857967 140334398945024 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.47753453254699707, loss=3.605792999267578
I0305 20:23:26.797982 140334390552320 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.49703145027160645, loss=3.532428503036499
I0305 20:24:01.721548 140334398945024 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.41917455196380615, loss=3.5202786922454834
I0305 20:24:36.635759 140334390552320 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.4135158956050873, loss=3.6262552738189697
I0305 20:25:11.530004 140334398945024 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.412324994802475, loss=3.529231071472168
I0305 20:25:46.478468 140334138853120 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.482104629278183, loss=3.6022865772247314
I0305 20:26:21.478110 140334130460416 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.4428797960281372, loss=3.5270023345947266
I0305 20:26:56.473310 140334138853120 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.4088972210884094, loss=3.5541269779205322
I0305 20:27:31.464737 140334130460416 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.5046711564064026, loss=3.4771933555603027
I0305 20:28:06.523774 140334138853120 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.42161762714385986, loss=3.5308051109313965
I0305 20:28:41.536952 140334130460416 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.39894765615463257, loss=3.4835028648376465
I0305 20:29:16.562666 140334138853120 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.38933923840522766, loss=3.4975152015686035
I0305 20:29:51.559310 140334130460416 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.40779659152030945, loss=3.497617244720459
I0305 20:30:26.567927 140334138853120 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.4015512764453888, loss=3.5135395526885986
I0305 20:31:01.571181 140334130460416 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.36739346385002136, loss=3.391988515853882
I0305 20:31:05.783879 140477253723328 spec.py:321] Evaluating on the training split.
I0305 20:31:08.426402 140477253723328 workload.py:181] Translating evaluation dataset.
I0305 20:33:50.370157 140477253723328 spec.py:333] Evaluating on the validation split.
I0305 20:33:53.013190 140477253723328 workload.py:181] Translating evaluation dataset.
I0305 20:36:32.769987 140477253723328 spec.py:349] Evaluating on the test split.
I0305 20:36:35.424960 140477253723328 workload.py:181] Translating evaluation dataset.
I0305 20:39:01.860316 140477253723328 submission_runner.py:469] Time since start: 5132.09s, 	Step: 7213, 	{'train/accuracy': 0.5844482183456421, 'train/loss': 2.3212201595306396, 'train/bleu': 27.392793835247907, 'validation/accuracy': 0.5920698046684265, 'validation/loss': 2.263000249862671, 'validation/bleu': 23.550735069709237, 'validation/num_examples': 3000, 'test/accuracy': 0.5949947834014893, 'test/loss': 2.255920648574829, 'test/bleu': 22.05012883883855, 'test/num_examples': 3003, 'score': 2546.204165697098, 'total_duration': 5132.085480451584, 'accumulated_submission_time': 2546.204165697098, 'accumulated_eval_time': 2585.343838453293, 'accumulated_logging_time': 0.0526432991027832}
I0305 20:39:01.869726 140334138853120 logging_writer.py:48] [7213] accumulated_eval_time=2585.34, accumulated_logging_time=0.0526433, accumulated_submission_time=2546.2, global_step=7213, preemption_count=0, score=2546.2, test/accuracy=0.594995, test/bleu=22.0501, test/loss=2.25592, test/num_examples=3003, total_duration=5132.09, train/accuracy=0.584448, train/bleu=27.3928, train/loss=2.32122, validation/accuracy=0.59207, validation/bleu=23.5507, validation/loss=2.263, validation/num_examples=3000
I0305 20:39:32.676430 140334130460416 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.4139254689216614, loss=3.432857036590576
I0305 20:40:07.638159 140334138853120 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.41732266545295715, loss=3.4790265560150146
I0305 20:40:42.662222 140334130460416 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.37056124210357666, loss=3.4579334259033203
I0305 20:41:17.682521 140334138853120 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.36582866311073303, loss=3.4478559494018555
I0305 20:41:52.711235 140334130460416 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.4397061765193939, loss=3.468297004699707
I0305 20:42:27.743140 140334138853120 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.38547149300575256, loss=3.4742207527160645
I0305 20:43:02.771344 140334130460416 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.4145994186401367, loss=3.388444185256958
I0305 20:43:37.791271 140334138853120 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.36782926321029663, loss=3.3883986473083496
I0305 20:44:12.808327 140334130460416 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.36741307377815247, loss=3.4864330291748047
I0305 20:44:47.828150 140334138853120 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.33289557695388794, loss=3.3475191593170166
I0305 20:45:22.839528 140334130460416 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.3241482973098755, loss=3.322094202041626
I0305 20:45:57.863700 140334138853120 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.333330899477005, loss=3.290726661682129
I0305 20:46:32.870530 140334130460416 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.4370853006839752, loss=3.407083749771118
I0305 20:47:07.871566 140334138853120 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.3814682066440582, loss=3.4010305404663086
I0305 20:47:42.855947 140334130460416 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.3219517469406128, loss=3.340522289276123
I0305 20:48:17.879101 140334138853120 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.3319888114929199, loss=3.2760157585144043
I0305 20:48:52.876106 140334130460416 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.3183383643627167, loss=3.2861416339874268
I0305 20:49:27.899089 140334138853120 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.2922018766403198, loss=3.3608198165893555
I0305 20:50:02.885984 140334130460416 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.306648313999176, loss=3.330613136291504
I0305 20:50:37.869686 140334138853120 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.31821420788764954, loss=3.371692657470703
I0305 20:51:12.895123 140334130460416 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.28763291239738464, loss=3.316450834274292
I0305 20:51:47.891624 140334138853120 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.32756951451301575, loss=3.300154209136963
I0305 20:52:22.876426 140334130460416 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.30906611680984497, loss=3.2954087257385254
I0305 20:52:57.889681 140334138853120 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.3186391592025757, loss=3.3470230102539062
I0305 20:53:02.099268 140477253723328 spec.py:321] Evaluating on the training split.
I0305 20:53:04.746049 140477253723328 workload.py:181] Translating evaluation dataset.
I0305 20:55:45.620871 140477253723328 spec.py:333] Evaluating on the validation split.
I0305 20:55:48.266357 140477253723328 workload.py:181] Translating evaluation dataset.
I0305 20:58:25.559977 140477253723328 spec.py:349] Evaluating on the test split.
I0305 20:58:28.211648 140477253723328 workload.py:181] Translating evaluation dataset.
I0305 21:00:51.495350 140477253723328 submission_runner.py:469] Time since start: 6441.72s, 	Step: 9613, 	{'train/accuracy': 0.5961407423019409, 'train/loss': 2.207930088043213, 'train/bleu': 28.20168804346851, 'validation/accuracy': 0.6107705235481262, 'validation/loss': 2.0926673412323, 'validation/bleu': 24.868520616467972, 'validation/num_examples': 3000, 'test/accuracy': 0.6149461269378662, 'test/loss': 2.0640909671783447, 'test/bleu': 23.480367955581055, 'test/num_examples': 3003, 'score': 3386.2726628780365, 'total_duration': 6441.72052192688, 'accumulated_submission_time': 3386.2726628780365, 'accumulated_eval_time': 3054.739854812622, 'accumulated_logging_time': 0.07064485549926758}
I0305 21:00:51.505661 140334130460416 logging_writer.py:48] [9613] accumulated_eval_time=3054.74, accumulated_logging_time=0.0706449, accumulated_submission_time=3386.27, global_step=9613, preemption_count=0, score=3386.27, test/accuracy=0.614946, test/bleu=23.4804, test/loss=2.06409, test/num_examples=3003, total_duration=6441.72, train/accuracy=0.596141, train/bleu=28.2017, train/loss=2.20793, validation/accuracy=0.610771, validation/bleu=24.8685, validation/loss=2.09267, validation/num_examples=3000
I0305 21:01:22.264754 140334138853120 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.2819128632545471, loss=3.2796249389648438
I0305 21:01:57.213640 140334130460416 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.2812151312828064, loss=3.2900428771972656
I0305 21:02:32.203428 140334138853120 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.36414825916290283, loss=3.213620185852051
I0305 21:03:07.193450 140334130460416 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.3034087121486664, loss=3.2963147163391113
I0305 21:03:42.184912 140334138853120 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.28678518533706665, loss=3.261276960372925
I0305 21:04:17.183502 140334130460416 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.28760093450546265, loss=3.3112704753875732
I0305 21:04:52.166943 140334138853120 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.2636220157146454, loss=3.2238333225250244
I0305 21:05:27.145796 140334130460416 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.2814269959926605, loss=3.254938840866089
I0305 21:06:02.129946 140334138853120 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.2590612769126892, loss=3.247633934020996
I0305 21:06:37.103686 140334130460416 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.29377347230911255, loss=3.3007400035858154
I0305 21:07:12.090615 140334138853120 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.3076830804347992, loss=3.2685203552246094
I0305 21:07:47.037506 140334130460416 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.2910318076610565, loss=3.2778358459472656
I0305 21:08:21.995902 140334138853120 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.25997090339660645, loss=3.2716386318206787
I0305 21:08:56.946395 140334130460416 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.24648278951644897, loss=3.291107177734375
I0305 21:09:31.923082 140334138853120 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.33019712567329407, loss=3.2564001083374023
I0305 21:10:06.884429 140334130460416 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.2655673027038574, loss=3.247471570968628
I0305 21:10:41.844095 140334138853120 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.25471991300582886, loss=3.206934928894043
I0305 21:11:16.812860 140334130460416 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.2573937773704529, loss=3.235534429550171
I0305 21:11:51.789334 140334138853120 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.2992742359638214, loss=3.147986650466919
I0305 21:12:26.757685 140334130460416 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.2584546208381653, loss=3.22770094871521
I0305 21:13:01.732309 140334138853120 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.2502467632293701, loss=3.2160515785217285
I0305 21:13:36.702111 140334130460416 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.27687186002731323, loss=3.252760887145996
I0305 21:14:11.671103 140334138853120 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.23886822164058685, loss=3.277256965637207
I0305 21:14:46.635700 140334130460416 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.23934270441532135, loss=3.340268135070801
I0305 21:14:51.533150 140477253723328 spec.py:321] Evaluating on the training split.
I0305 21:14:54.176071 140477253723328 workload.py:181] Translating evaluation dataset.
I0305 21:19:26.770975 140477253723328 spec.py:333] Evaluating on the validation split.
I0305 21:19:29.401139 140477253723328 workload.py:181] Translating evaluation dataset.
I0305 21:24:00.720453 140477253723328 spec.py:349] Evaluating on the test split.
I0305 21:24:03.358983 140477253723328 workload.py:181] Translating evaluation dataset.
I0305 21:27:47.914121 140477253723328 submission_runner.py:469] Time since start: 8058.14s, 	Step: 12015, 	{'train/accuracy': 0.6048036217689514, 'train/loss': 2.1218347549438477, 'train/bleu': 29.275157052533363, 'validation/accuracy': 0.6225372552871704, 'validation/loss': 1.9904459714889526, 'validation/bleu': 25.114439929177962, 'validation/num_examples': 3000, 'test/accuracy': 0.6314563751220703, 'test/loss': 1.9480689764022827, 'test/bleu': 25.064452307439918, 'test/num_examples': 3003, 'score': 4226.142108440399, 'total_duration': 8058.139294862747, 'accumulated_submission_time': 4226.142108440399, 'accumulated_eval_time': 3831.120766878128, 'accumulated_logging_time': 0.09091997146606445}
I0305 21:27:47.924147 140334138853120 logging_writer.py:48] [12015] accumulated_eval_time=3831.12, accumulated_logging_time=0.09092, accumulated_submission_time=4226.14, global_step=12015, preemption_count=0, score=4226.14, test/accuracy=0.631456, test/bleu=25.0645, test/loss=1.94807, test/num_examples=3003, total_duration=8058.14, train/accuracy=0.604804, train/bleu=29.2752, train/loss=2.12183, validation/accuracy=0.622537, validation/bleu=25.1144, validation/loss=1.99045, validation/num_examples=3000
I0305 21:28:17.940437 140334130460416 logging_writer.py:48] [12100] global_step=12100, grad_norm=0.2539370357990265, loss=3.1549718379974365
I0305 21:28:52.909903 140334138853120 logging_writer.py:48] [12200] global_step=12200, grad_norm=0.2475665956735611, loss=3.1963067054748535
I0305 21:29:27.872671 140334130460416 logging_writer.py:48] [12300] global_step=12300, grad_norm=0.24526046216487885, loss=3.155463457107544
I0305 21:30:02.856109 140334138853120 logging_writer.py:48] [12400] global_step=12400, grad_norm=0.24583888053894043, loss=3.2488622665405273
I0305 21:30:37.811454 140334130460416 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.25839763879776, loss=3.1224138736724854
I0305 21:31:12.714723 140334138853120 logging_writer.py:48] [12600] global_step=12600, grad_norm=0.25008848309516907, loss=3.2392466068267822
I0305 21:31:47.578713 140334130460416 logging_writer.py:48] [12700] global_step=12700, grad_norm=0.22046279907226562, loss=3.2484235763549805
I0305 21:32:22.460177 140334138853120 logging_writer.py:48] [12800] global_step=12800, grad_norm=0.2680572271347046, loss=3.086775541305542
I0305 21:32:57.343963 140334130460416 logging_writer.py:48] [12900] global_step=12900, grad_norm=0.24643456935882568, loss=3.1469154357910156
I0305 21:33:32.197865 140334138853120 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.25326111912727356, loss=3.161365509033203
I0305 21:34:07.047282 140334130460416 logging_writer.py:48] [13100] global_step=13100, grad_norm=0.2636479139328003, loss=3.144899368286133
I0305 21:34:41.913654 140334138853120 logging_writer.py:48] [13200] global_step=13200, grad_norm=0.25279662013053894, loss=3.210942506790161
I0305 21:35:16.792739 140334130460416 logging_writer.py:48] [13300] global_step=13300, grad_norm=0.23437851667404175, loss=3.1477627754211426
I0305 21:35:51.676374 140334138853120 logging_writer.py:48] [13400] global_step=13400, grad_norm=0.2331271916627884, loss=3.1635262966156006
I0305 21:36:26.568582 140334130460416 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.2907082736492157, loss=3.210089683532715
I0305 21:37:01.449483 140334138853120 logging_writer.py:48] [13600] global_step=13600, grad_norm=0.3510422706604004, loss=3.1581764221191406
I0305 21:37:36.326381 140334130460416 logging_writer.py:48] [13700] global_step=13700, grad_norm=0.24190233647823334, loss=3.1215829849243164
I0305 21:38:11.189967 140334138853120 logging_writer.py:48] [13800] global_step=13800, grad_norm=0.24327576160430908, loss=3.1791183948516846
I0305 21:38:46.125237 140334130460416 logging_writer.py:48] [13900] global_step=13900, grad_norm=0.3180071711540222, loss=3.18525767326355
I0305 21:39:21.053850 140334138853120 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.35708650946617126, loss=3.1840245723724365
I0305 21:39:55.939349 140334130460416 logging_writer.py:48] [14100] global_step=14100, grad_norm=0.25347083806991577, loss=3.1604557037353516
I0305 21:40:30.816754 140334138853120 logging_writer.py:48] [14200] global_step=14200, grad_norm=0.2618916630744934, loss=3.119391679763794
I0305 21:41:05.702105 140334130460416 logging_writer.py:48] [14300] global_step=14300, grad_norm=0.3792370557785034, loss=3.134141683578491
I0305 21:41:40.556619 140334138853120 logging_writer.py:48] [14400] global_step=14400, grad_norm=0.2838951647281647, loss=3.1379990577697754
I0305 21:41:48.245064 140477253723328 spec.py:321] Evaluating on the training split.
I0305 21:41:50.896373 140477253723328 workload.py:181] Translating evaluation dataset.
I0305 21:44:49.752639 140477253723328 spec.py:333] Evaluating on the validation split.
I0305 21:44:52.396072 140477253723328 workload.py:181] Translating evaluation dataset.
I0305 21:47:25.477711 140477253723328 spec.py:349] Evaluating on the test split.
I0305 21:47:28.119214 140477253723328 workload.py:181] Translating evaluation dataset.
I0305 21:49:44.344469 140477253723328 submission_runner.py:469] Time since start: 9374.57s, 	Step: 14423, 	{'train/accuracy': 0.6181435585021973, 'train/loss': 2.0152649879455566, 'train/bleu': 29.682170901347856, 'validation/accuracy': 0.63437819480896, 'validation/loss': 1.8945846557617188, 'validation/bleu': 26.621742874759278, 'validation/num_examples': 3000, 'test/accuracy': 0.6426138281822205, 'test/loss': 1.8424679040908813, 'test/bleu': 25.795908160612612, 'test/num_examples': 3003, 'score': 5066.309244155884, 'total_duration': 9374.569645881653, 'accumulated_submission_time': 5066.309244155884, 'accumulated_eval_time': 4307.220116853714, 'accumulated_logging_time': 0.10918354988098145}
I0305 21:49:44.354552 140334130460416 logging_writer.py:48] [14423] accumulated_eval_time=4307.22, accumulated_logging_time=0.109184, accumulated_submission_time=5066.31, global_step=14423, preemption_count=0, score=5066.31, test/accuracy=0.642614, test/bleu=25.7959, test/loss=1.84247, test/num_examples=3003, total_duration=9374.57, train/accuracy=0.618144, train/bleu=29.6822, train/loss=2.01526, validation/accuracy=0.634378, validation/bleu=26.6217, validation/loss=1.89458, validation/num_examples=3000
I0305 21:50:11.547860 140334138853120 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.26226890087127686, loss=3.186244010925293
I0305 21:50:46.391892 140334130460416 logging_writer.py:48] [14600] global_step=14600, grad_norm=0.2556331753730774, loss=3.0980544090270996
I0305 21:51:21.264932 140334138853120 logging_writer.py:48] [14700] global_step=14700, grad_norm=0.2475023716688156, loss=3.1372902393341064
I0305 21:51:56.129719 140334130460416 logging_writer.py:48] [14800] global_step=14800, grad_norm=0.3560871183872223, loss=3.1538732051849365
I0305 21:52:31.039799 140334138853120 logging_writer.py:48] [14900] global_step=14900, grad_norm=0.2669275999069214, loss=3.103679656982422
I0305 21:53:05.906682 140334130460416 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.2687557637691498, loss=3.0376925468444824
I0305 21:53:40.784689 140334138853120 logging_writer.py:48] [15100] global_step=15100, grad_norm=0.23718568682670593, loss=3.1004390716552734
I0305 21:54:15.671320 140334130460416 logging_writer.py:48] [15200] global_step=15200, grad_norm=0.25305435061454773, loss=3.1118364334106445
I0305 21:54:50.560876 140334138853120 logging_writer.py:48] [15300] global_step=15300, grad_norm=0.27821797132492065, loss=3.109628915786743
I0305 21:55:25.436436 140334130460416 logging_writer.py:48] [15400] global_step=15400, grad_norm=0.28178948163986206, loss=3.1592931747436523
I0305 21:56:00.331486 140334138853120 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.2606405019760132, loss=3.2008094787597656
I0305 21:56:35.177457 140334130460416 logging_writer.py:48] [15600] global_step=15600, grad_norm=0.24869854748249054, loss=3.085692882537842
I0305 21:57:10.030212 140334138853120 logging_writer.py:48] [15700] global_step=15700, grad_norm=0.28671377897262573, loss=3.075770139694214
I0305 21:57:44.907007 140334130460416 logging_writer.py:48] [15800] global_step=15800, grad_norm=0.2544948160648346, loss=3.035747528076172
I0305 21:58:19.798970 140334138853120 logging_writer.py:48] [15900] global_step=15900, grad_norm=0.4101572334766388, loss=3.1762101650238037
I0305 21:58:54.674563 140334130460416 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.3889883756637573, loss=3.108449935913086
I0305 21:59:29.524918 140334138853120 logging_writer.py:48] [16100] global_step=16100, grad_norm=0.2750053107738495, loss=3.077812433242798
I0305 22:00:04.403693 140334130460416 logging_writer.py:48] [16200] global_step=16200, grad_norm=0.3882359564304352, loss=3.014986515045166
I0305 22:00:39.280654 140334138853120 logging_writer.py:48] [16300] global_step=16300, grad_norm=0.3141680955886841, loss=3.0653278827667236
I0305 22:01:14.146078 140334130460416 logging_writer.py:48] [16400] global_step=16400, grad_norm=0.2907749116420746, loss=3.068932294845581
I0305 22:01:49.013985 140334138853120 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.3282577097415924, loss=3.048581838607788
I0305 22:02:23.896001 140334130460416 logging_writer.py:48] [16600] global_step=16600, grad_norm=0.24132511019706726, loss=3.1210193634033203
I0305 22:02:58.755700 140334138853120 logging_writer.py:48] [16700] global_step=16700, grad_norm=0.26306626200675964, loss=3.0595006942749023
I0305 22:03:33.623385 140334130460416 logging_writer.py:48] [16800] global_step=16800, grad_norm=0.33605122566223145, loss=3.0086231231689453
I0305 22:03:44.428331 140477253723328 spec.py:321] Evaluating on the training split.
I0305 22:03:47.074552 140477253723328 workload.py:181] Translating evaluation dataset.
I0305 22:06:25.791612 140477253723328 spec.py:333] Evaluating on the validation split.
I0305 22:06:28.425589 140477253723328 workload.py:181] Translating evaluation dataset.
I0305 22:08:57.747343 140477253723328 spec.py:349] Evaluating on the test split.
I0305 22:09:00.387588 140477253723328 workload.py:181] Translating evaluation dataset.
I0305 22:11:18.023929 140477253723328 submission_runner.py:469] Time since start: 10668.25s, 	Step: 16832, 	{'train/accuracy': 0.6170329451560974, 'train/loss': 2.0077483654022217, 'train/bleu': 30.015583654939835, 'validation/accuracy': 0.6412874460220337, 'validation/loss': 1.838304877281189, 'validation/bleu': 27.004861617891624, 'validation/num_examples': 3000, 'test/accuracy': 0.6508516073226929, 'test/loss': 1.782792568206787, 'test/bleu': 26.39376827751109, 'test/num_examples': 3003, 'score': 5906.237273693085, 'total_duration': 10668.249090909958, 'accumulated_submission_time': 5906.237273693085, 'accumulated_eval_time': 4760.815642595291, 'accumulated_logging_time': 0.12747764587402344}
I0305 22:11:18.035295 140334138853120 logging_writer.py:48] [16832] accumulated_eval_time=4760.82, accumulated_logging_time=0.127478, accumulated_submission_time=5906.24, global_step=16832, preemption_count=0, score=5906.24, test/accuracy=0.650852, test/bleu=26.3938, test/loss=1.78279, test/num_examples=3003, total_duration=10668.2, train/accuracy=0.617033, train/bleu=30.0156, train/loss=2.00775, validation/accuracy=0.641287, validation/bleu=27.0049, validation/loss=1.8383, validation/num_examples=3000
I0305 22:11:42.084044 140334130460416 logging_writer.py:48] [16900] global_step=16900, grad_norm=0.3020637035369873, loss=3.0817971229553223
I0305 22:12:16.914536 140334138853120 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.2838457226753235, loss=3.0896732807159424
I0305 22:12:51.773735 140334130460416 logging_writer.py:48] [17100] global_step=17100, grad_norm=0.30142727494239807, loss=3.1088602542877197
I0305 22:13:26.625712 140334138853120 logging_writer.py:48] [17200] global_step=17200, grad_norm=0.28969502449035645, loss=3.0777547359466553
I0305 22:14:01.463913 140334130460416 logging_writer.py:48] [17300] global_step=17300, grad_norm=0.26013320684432983, loss=2.984189033508301
I0305 22:14:36.369969 140334138853120 logging_writer.py:48] [17400] global_step=17400, grad_norm=0.3581947684288025, loss=3.1156580448150635
I0305 22:15:11.251033 140334130460416 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.3184242248535156, loss=3.081007957458496
I0305 22:15:46.159631 140334138853120 logging_writer.py:48] [17600] global_step=17600, grad_norm=0.2700096666812897, loss=3.095229148864746
I0305 22:16:21.073642 140334130460416 logging_writer.py:48] [17700] global_step=17700, grad_norm=0.29404324293136597, loss=3.0387699604034424
I0305 22:16:55.983924 140334138853120 logging_writer.py:48] [17800] global_step=17800, grad_norm=0.32819435000419617, loss=3.0409388542175293
I0305 22:17:30.889135 140334130460416 logging_writer.py:48] [17900] global_step=17900, grad_norm=0.31628793478012085, loss=3.061600685119629
I0305 22:18:05.778889 140334138853120 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.32103586196899414, loss=3.102240800857544
I0305 22:18:40.678402 140334130460416 logging_writer.py:48] [18100] global_step=18100, grad_norm=0.33013978600502014, loss=3.0517313480377197
I0305 22:19:15.583103 140334138853120 logging_writer.py:48] [18200] global_step=18200, grad_norm=0.3688124418258667, loss=3.0288147926330566
I0305 22:19:50.466750 140334130460416 logging_writer.py:48] [18300] global_step=18300, grad_norm=0.3157512843608856, loss=2.9909205436706543
I0305 22:20:25.354405 140334138853120 logging_writer.py:48] [18400] global_step=18400, grad_norm=0.3381846845149994, loss=2.980380058288574
I0305 22:21:00.231659 140334130460416 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.331030011177063, loss=3.0502753257751465
I0305 22:21:35.125448 140334138853120 logging_writer.py:48] [18600] global_step=18600, grad_norm=0.33414626121520996, loss=3.0450761318206787
I0305 22:22:10.014133 140334130460416 logging_writer.py:48] [18700] global_step=18700, grad_norm=0.31612566113471985, loss=3.058324098587036
I0305 22:22:44.899088 140334138853120 logging_writer.py:48] [18800] global_step=18800, grad_norm=0.3530019521713257, loss=3.00785493850708
I0305 22:23:20.044434 140334130460416 logging_writer.py:48] [18900] global_step=18900, grad_norm=0.34390443563461304, loss=3.0520145893096924
I0305 22:23:55.122159 140334138853120 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.3051430284976959, loss=3.0092358589172363
I0305 22:24:30.200025 140334130460416 logging_writer.py:48] [19100] global_step=19100, grad_norm=0.32278549671173096, loss=3.052907705307007
I0305 22:25:05.288402 140334138853120 logging_writer.py:48] [19200] global_step=19200, grad_norm=0.39542996883392334, loss=3.0248754024505615
I0305 22:25:18.282167 140477253723328 spec.py:321] Evaluating on the training split.
I0305 22:25:20.951284 140477253723328 workload.py:181] Translating evaluation dataset.
I0305 22:29:27.800533 140477253723328 spec.py:333] Evaluating on the validation split.
I0305 22:29:30.436194 140477253723328 workload.py:181] Translating evaluation dataset.
I0305 22:33:51.849970 140477253723328 spec.py:349] Evaluating on the test split.
I0305 22:33:54.499279 140477253723328 workload.py:181] Translating evaluation dataset.
I0305 22:37:27.663508 140477253723328 submission_runner.py:469] Time since start: 12237.89s, 	Step: 19238, 	{'train/accuracy': 0.6349213719367981, 'train/loss': 1.8775442838668823, 'train/bleu': 30.845953282666983, 'validation/accuracy': 0.6467505693435669, 'validation/loss': 1.793628454208374, 'validation/bleu': 26.28721387767766, 'validation/num_examples': 3000, 'test/accuracy': 0.6559262871742249, 'test/loss': 1.7444177865982056, 'test/bleu': 26.63896667599549, 'test/num_examples': 3003, 'score': 6746.335658311844, 'total_duration': 12237.88868355751, 'accumulated_submission_time': 6746.335658311844, 'accumulated_eval_time': 5490.196926116943, 'accumulated_logging_time': 0.1474461555480957}
I0305 22:37:27.675525 140334130460416 logging_writer.py:48] [19238] accumulated_eval_time=5490.2, accumulated_logging_time=0.147446, accumulated_submission_time=6746.34, global_step=19238, preemption_count=0, score=6746.34, test/accuracy=0.655926, test/bleu=26.639, test/loss=1.74442, test/num_examples=3003, total_duration=12237.9, train/accuracy=0.634921, train/bleu=30.846, train/loss=1.87754, validation/accuracy=0.646751, validation/bleu=26.2872, validation/loss=1.79363, validation/num_examples=3000
I0305 22:37:49.744521 140334138853120 logging_writer.py:48] [19300] global_step=19300, grad_norm=0.2891519367694855, loss=2.9912009239196777
I0305 22:38:24.754078 140334130460416 logging_writer.py:48] [19400] global_step=19400, grad_norm=0.33511239290237427, loss=3.0805888175964355
I0305 22:38:59.794317 140334138853120 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.30865973234176636, loss=3.007016658782959
I0305 22:39:34.861235 140334130460416 logging_writer.py:48] [19600] global_step=19600, grad_norm=0.2863433063030243, loss=3.038364887237549
I0305 22:40:09.961295 140334138853120 logging_writer.py:48] [19700] global_step=19700, grad_norm=0.29165780544281006, loss=3.1151833534240723
I0305 22:40:45.041928 140334130460416 logging_writer.py:48] [19800] global_step=19800, grad_norm=0.3037990927696228, loss=3.0138614177703857
I0305 22:41:20.106441 140334138853120 logging_writer.py:48] [19900] global_step=19900, grad_norm=0.3437528908252716, loss=3.1044788360595703
I0305 22:41:55.154178 140334130460416 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.34126579761505127, loss=2.9868953227996826
I0305 22:42:30.210925 140334138853120 logging_writer.py:48] [20100] global_step=20100, grad_norm=0.3696233928203583, loss=3.1112873554229736
I0305 22:43:05.247195 140334130460416 logging_writer.py:48] [20200] global_step=20200, grad_norm=0.33198171854019165, loss=2.994539260864258
I0305 22:43:40.295689 140334138853120 logging_writer.py:48] [20300] global_step=20300, grad_norm=0.3947902321815491, loss=3.094303607940674
I0305 22:44:15.334238 140334130460416 logging_writer.py:48] [20400] global_step=20400, grad_norm=0.3267526626586914, loss=3.024949073791504
I0305 22:44:50.394102 140334138853120 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.3457389175891876, loss=2.9898879528045654
I0305 22:45:25.450211 140334130460416 logging_writer.py:48] [20600] global_step=20600, grad_norm=0.33136579394340515, loss=3.0779166221618652
I0305 22:46:00.507700 140334138853120 logging_writer.py:48] [20700] global_step=20700, grad_norm=0.30815377831459045, loss=3.0117027759552
I0305 22:46:35.564936 140334130460416 logging_writer.py:48] [20800] global_step=20800, grad_norm=0.29358604550361633, loss=3.0363974571228027
I0305 22:47:10.625525 140334138853120 logging_writer.py:48] [20900] global_step=20900, grad_norm=0.3812101483345032, loss=3.045264482498169
I0305 22:47:45.681674 140334130460416 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.3190740942955017, loss=2.9815807342529297
I0305 22:48:20.742030 140334138853120 logging_writer.py:48] [21100] global_step=21100, grad_norm=0.3561413884162903, loss=3.0066049098968506
I0305 22:48:55.776600 140334130460416 logging_writer.py:48] [21200] global_step=21200, grad_norm=0.3214426636695862, loss=3.0123212337493896
I0305 22:49:30.828251 140334138853120 logging_writer.py:48] [21300] global_step=21300, grad_norm=0.3402310907840729, loss=3.00787615776062
I0305 22:50:05.868337 140334130460416 logging_writer.py:48] [21400] global_step=21400, grad_norm=0.31657537817955017, loss=3.0557210445404053
I0305 22:50:40.925540 140334138853120 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.33422908186912537, loss=3.0812160968780518
I0305 22:51:15.963515 140334130460416 logging_writer.py:48] [21600] global_step=21600, grad_norm=0.32974597811698914, loss=2.9800899028778076
I0305 22:51:27.893182 140477253723328 spec.py:321] Evaluating on the training split.
I0305 22:51:30.544398 140477253723328 workload.py:181] Translating evaluation dataset.
I0305 22:54:09.190994 140477253723328 spec.py:333] Evaluating on the validation split.
I0305 22:54:11.842696 140477253723328 workload.py:181] Translating evaluation dataset.
I0305 22:56:39.439303 140477253723328 spec.py:349] Evaluating on the test split.
I0305 22:56:42.073504 140477253723328 workload.py:181] Translating evaluation dataset.
I0305 22:58:59.095410 140477253723328 submission_runner.py:469] Time since start: 13529.32s, 	Step: 21635, 	{'train/accuracy': 0.6274845600128174, 'train/loss': 1.9272443056106567, 'train/bleu': 30.46928019174078, 'validation/accuracy': 0.6516327857971191, 'validation/loss': 1.7692039012908936, 'validation/bleu': 27.476626637156603, 'validation/num_examples': 3000, 'test/accuracy': 0.6590082049369812, 'test/loss': 1.7109806537628174, 'test/bleu': 26.718279936118936, 'test/num_examples': 3003, 'score': 7586.405999183655, 'total_duration': 13529.320587396622, 'accumulated_submission_time': 7586.405999183655, 'accumulated_eval_time': 5941.399101495743, 'accumulated_logging_time': 0.16770219802856445}
I0305 22:58:59.106544 140334138853120 logging_writer.py:48] [21635] accumulated_eval_time=5941.4, accumulated_logging_time=0.167702, accumulated_submission_time=7586.41, global_step=21635, preemption_count=0, score=7586.41, test/accuracy=0.659008, test/bleu=26.7183, test/loss=1.71098, test/num_examples=3003, total_duration=13529.3, train/accuracy=0.627485, train/bleu=30.4693, train/loss=1.92724, validation/accuracy=0.651633, validation/bleu=27.4766, validation/loss=1.7692, validation/num_examples=3000
I0305 22:59:22.211461 140334130460416 logging_writer.py:48] [21700] global_step=21700, grad_norm=0.45208704471588135, loss=3.063425064086914
I0305 22:59:57.237505 140334138853120 logging_writer.py:48] [21800] global_step=21800, grad_norm=0.37186703085899353, loss=3.053488254547119
I0305 23:00:32.293691 140334130460416 logging_writer.py:48] [21900] global_step=21900, grad_norm=0.32124415040016174, loss=3.0173847675323486
I0305 23:01:07.356325 140334138853120 logging_writer.py:48] [22000] global_step=22000, grad_norm=0.3415842354297638, loss=3.0571579933166504
I0305 23:01:42.423097 140334130460416 logging_writer.py:48] [22100] global_step=22100, grad_norm=0.29383376240730286, loss=2.96966552734375
I0305 23:02:17.472051 140334138853120 logging_writer.py:48] [22200] global_step=22200, grad_norm=0.33850568532943726, loss=2.984645128250122
I0305 23:02:52.543396 140334130460416 logging_writer.py:48] [22300] global_step=22300, grad_norm=0.3644665479660034, loss=3.0486743450164795
I0305 23:03:27.603889 140334138853120 logging_writer.py:48] [22400] global_step=22400, grad_norm=0.29768770933151245, loss=2.9976646900177
I0305 23:04:02.682466 140334130460416 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.34753796458244324, loss=2.94490647315979
I0305 23:04:37.777997 140334138853120 logging_writer.py:48] [22600] global_step=22600, grad_norm=0.3232799172401428, loss=2.940352201461792
I0305 23:05:12.839391 140334130460416 logging_writer.py:48] [22700] global_step=22700, grad_norm=0.33502596616744995, loss=3.051182985305786
I0305 23:05:47.912184 140334138853120 logging_writer.py:48] [22800] global_step=22800, grad_norm=0.40328314900398254, loss=3.0619313716888428
I0305 23:06:22.974409 140334130460416 logging_writer.py:48] [22900] global_step=22900, grad_norm=0.39650803804397583, loss=3.0541319847106934
I0305 23:06:58.029477 140334138853120 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.6962795853614807, loss=3.0772347450256348
I0305 23:07:33.108344 140334130460416 logging_writer.py:48] [23100] global_step=23100, grad_norm=0.30334046483039856, loss=2.9919791221618652
I0305 23:08:08.165017 140334138853120 logging_writer.py:48] [23200] global_step=23200, grad_norm=0.3609580993652344, loss=3.0849721431732178
I0305 23:08:43.242297 140334130460416 logging_writer.py:48] [23300] global_step=23300, grad_norm=0.3554483950138092, loss=3.0348846912384033
I0305 23:09:18.307810 140334138853120 logging_writer.py:48] [23400] global_step=23400, grad_norm=0.33005133271217346, loss=2.9204812049865723
I0305 23:09:53.404050 140334130460416 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.3366977870464325, loss=2.9968321323394775
I0305 23:10:28.498338 140334138853120 logging_writer.py:48] [23600] global_step=23600, grad_norm=0.36850184202194214, loss=2.9058656692504883
I0305 23:11:03.572089 140334130460416 logging_writer.py:48] [23700] global_step=23700, grad_norm=0.3079598844051361, loss=3.0987765789031982
I0305 23:11:38.646241 140334138853120 logging_writer.py:48] [23800] global_step=23800, grad_norm=0.38954102993011475, loss=3.026106119155884
I0305 23:12:13.724378 140334130460416 logging_writer.py:48] [23900] global_step=23900, grad_norm=0.3240242302417755, loss=2.901740789413452
I0305 23:12:48.811401 140334138853120 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.31589311361312866, loss=2.999143362045288
I0305 23:12:59.326859 140477253723328 spec.py:321] Evaluating on the training split.
I0305 23:13:01.980057 140477253723328 workload.py:181] Translating evaluation dataset.
I0305 23:15:44.453922 140477253723328 spec.py:333] Evaluating on the validation split.
I0305 23:15:47.115365 140477253723328 workload.py:181] Translating evaluation dataset.
I0305 23:18:25.392030 140477253723328 spec.py:349] Evaluating on the test split.
I0305 23:18:28.042456 140477253723328 workload.py:181] Translating evaluation dataset.
I0305 23:21:04.095101 140477253723328 submission_runner.py:469] Time since start: 14854.32s, 	Step: 24031, 	{'train/accuracy': 0.6271534562110901, 'train/loss': 1.9377105236053467, 'train/bleu': 30.3128471892418, 'validation/accuracy': 0.6524608731269836, 'validation/loss': 1.7585605382919312, 'validation/bleu': 27.852425004102997, 'validation/num_examples': 3000, 'test/accuracy': 0.6621249318122864, 'test/loss': 1.6978387832641602, 'test/bleu': 27.3038982293258, 'test/num_examples': 3003, 'score': 8426.477151870728, 'total_duration': 14854.320274829865, 'accumulated_submission_time': 8426.477151870728, 'accumulated_eval_time': 6426.167286872864, 'accumulated_logging_time': 0.1870584487915039}
I0305 23:21:04.107680 140334130460416 logging_writer.py:48] [24031] accumulated_eval_time=6426.17, accumulated_logging_time=0.187058, accumulated_submission_time=8426.48, global_step=24031, preemption_count=0, score=8426.48, test/accuracy=0.662125, test/bleu=27.3039, test/loss=1.69784, test/num_examples=3003, total_duration=14854.3, train/accuracy=0.627153, train/bleu=30.3128, train/loss=1.93771, validation/accuracy=0.652461, validation/bleu=27.8524, validation/loss=1.75856, validation/num_examples=3000
I0305 23:21:28.636603 140334138853120 logging_writer.py:48] [24100] global_step=24100, grad_norm=0.36382973194122314, loss=2.956390142440796
I0305 23:22:03.662012 140334130460416 logging_writer.py:48] [24200] global_step=24200, grad_norm=0.3466755449771881, loss=3.0566771030426025
I0305 23:22:38.725973 140334138853120 logging_writer.py:48] [24300] global_step=24300, grad_norm=0.3947989344596863, loss=2.9942967891693115
I0305 23:23:13.804488 140334130460416 logging_writer.py:48] [24400] global_step=24400, grad_norm=0.34658342599868774, loss=3.070521354675293
I0305 23:23:48.875020 140334138853120 logging_writer.py:48] [24500] global_step=24500, grad_norm=0.4262315034866333, loss=3.020376682281494
I0305 23:24:23.949531 140334130460416 logging_writer.py:48] [24600] global_step=24600, grad_norm=0.32506099343299866, loss=2.9936535358428955
I0305 23:24:59.036924 140334138853120 logging_writer.py:48] [24700] global_step=24700, grad_norm=0.37283337116241455, loss=2.9904677867889404
I0305 23:25:34.131304 140334130460416 logging_writer.py:48] [24800] global_step=24800, grad_norm=0.3436085879802704, loss=3.032339334487915
I0305 23:26:09.213685 140334138853120 logging_writer.py:48] [24900] global_step=24900, grad_norm=0.3447084128856659, loss=2.9772775173187256
I0305 23:26:44.284777 140334130460416 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.325003057718277, loss=3.0461249351501465
I0305 23:27:19.388633 140334138853120 logging_writer.py:48] [25100] global_step=25100, grad_norm=0.34565380215644836, loss=3.016124725341797
I0305 23:27:54.491475 140334130460416 logging_writer.py:48] [25200] global_step=25200, grad_norm=0.3572966158390045, loss=2.9646522998809814
I0305 23:28:29.610432 140334138853120 logging_writer.py:48] [25300] global_step=25300, grad_norm=0.3410782516002655, loss=2.9100170135498047
I0305 23:29:04.717314 140334130460416 logging_writer.py:48] [25400] global_step=25400, grad_norm=0.32787224650382996, loss=3.1231894493103027
I0305 23:29:39.820967 140334138853120 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.4249435365200043, loss=2.9835281372070312
I0305 23:30:14.908838 140334130460416 logging_writer.py:48] [25600] global_step=25600, grad_norm=0.4047997295856476, loss=3.0650718212127686
I0305 23:30:49.989421 140334138853120 logging_writer.py:48] [25700] global_step=25700, grad_norm=0.37544554471969604, loss=3.0304157733917236
I0305 23:31:25.057114 140334130460416 logging_writer.py:48] [25800] global_step=25800, grad_norm=0.3440796136856079, loss=3.002032518386841
I0305 23:32:00.140748 140334138853120 logging_writer.py:48] [25900] global_step=25900, grad_norm=0.30978646874427795, loss=2.9741835594177246
I0305 23:32:35.241903 140334130460416 logging_writer.py:48] [26000] global_step=26000, grad_norm=0.35544008016586304, loss=3.0272858142852783
I0305 23:33:10.345151 140334138853120 logging_writer.py:48] [26100] global_step=26100, grad_norm=0.35098761320114136, loss=3.0282552242279053
I0305 23:33:45.448595 140334130460416 logging_writer.py:48] [26200] global_step=26200, grad_norm=0.34366318583488464, loss=3.0018489360809326
I0305 23:34:20.584110 140334138853120 logging_writer.py:48] [26300] global_step=26300, grad_norm=0.3421092927455902, loss=2.960118055343628
I0305 23:34:55.678458 140334130460416 logging_writer.py:48] [26400] global_step=26400, grad_norm=0.3399929702281952, loss=3.042860269546509
I0305 23:35:04.112209 140477253723328 spec.py:321] Evaluating on the training split.
I0305 23:35:06.758092 140477253723328 workload.py:181] Translating evaluation dataset.
I0305 23:38:07.437542 140477253723328 spec.py:333] Evaluating on the validation split.
I0305 23:38:10.078702 140477253723328 workload.py:181] Translating evaluation dataset.
I0305 23:40:42.241336 140477253723328 spec.py:349] Evaluating on the test split.
I0305 23:40:44.884711 140477253723328 workload.py:181] Translating evaluation dataset.
I0305 23:43:13.973902 140477253723328 submission_runner.py:469] Time since start: 16184.20s, 	Step: 26425, 	{'train/accuracy': 0.6360276341438293, 'train/loss': 1.8789554834365845, 'train/bleu': 31.253834213102273, 'validation/accuracy': 0.6545373797416687, 'validation/loss': 1.7400869131088257, 'validation/bleu': 28.025504971725667, 'validation/num_examples': 3000, 'test/accuracy': 0.6632140278816223, 'test/loss': 1.694216012954712, 'test/bleu': 27.09805478876075, 'test/num_examples': 3003, 'score': 9266.333788394928, 'total_duration': 16184.199075937271, 'accumulated_submission_time': 9266.333788394928, 'accumulated_eval_time': 6916.028918266296, 'accumulated_logging_time': 0.20835328102111816}
I0305 23:43:13.986104 140334138853120 logging_writer.py:48] [26425] accumulated_eval_time=6916.03, accumulated_logging_time=0.208353, accumulated_submission_time=9266.33, global_step=26425, preemption_count=0, score=9266.33, test/accuracy=0.663214, test/bleu=27.0981, test/loss=1.69422, test/num_examples=3003, total_duration=16184.2, train/accuracy=0.636028, train/bleu=31.2538, train/loss=1.87896, validation/accuracy=0.654537, validation/bleu=28.0255, validation/loss=1.74009, validation/num_examples=3000
I0305 23:43:40.623607 140334130460416 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.4100094735622406, loss=2.9656829833984375
I0305 23:44:15.643277 140334138853120 logging_writer.py:48] [26600] global_step=26600, grad_norm=0.36402860283851624, loss=2.9709413051605225
I0305 23:44:50.715190 140334130460416 logging_writer.py:48] [26700] global_step=26700, grad_norm=0.3804594576358795, loss=2.9764163494110107
I0305 23:45:25.768312 140334138853120 logging_writer.py:48] [26800] global_step=26800, grad_norm=0.32217150926589966, loss=2.9448113441467285
I0305 23:46:00.870118 140334130460416 logging_writer.py:48] [26900] global_step=26900, grad_norm=0.3512346148490906, loss=3.039133071899414
I0305 23:46:35.940373 140334138853120 logging_writer.py:48] [27000] global_step=27000, grad_norm=0.7981432676315308, loss=3.0412018299102783
I0305 23:47:11.009194 140334130460416 logging_writer.py:48] [27100] global_step=27100, grad_norm=0.3261273503303528, loss=3.0357770919799805
I0305 23:47:46.114535 140334138853120 logging_writer.py:48] [27200] global_step=27200, grad_norm=0.3873481750488281, loss=2.982165813446045
I0305 23:48:21.235413 140334130460416 logging_writer.py:48] [27300] global_step=27300, grad_norm=0.44776540994644165, loss=3.0002284049987793
I0305 23:48:56.322896 140334138853120 logging_writer.py:48] [27400] global_step=27400, grad_norm=0.3351954221725464, loss=2.9644975662231445
I0305 23:49:31.439406 140334130460416 logging_writer.py:48] [27500] global_step=27500, grad_norm=0.32727834582328796, loss=2.996635675430298
I0305 23:50:06.540123 140334138853120 logging_writer.py:48] [27600] global_step=27600, grad_norm=0.35621631145477295, loss=2.979719400405884
I0305 23:50:41.675545 140334130460416 logging_writer.py:48] [27700] global_step=27700, grad_norm=0.37301114201545715, loss=2.967176914215088
I0305 23:51:16.788983 140334138853120 logging_writer.py:48] [27800] global_step=27800, grad_norm=0.32667988538742065, loss=3.037374973297119
I0305 23:51:51.914677 140334130460416 logging_writer.py:48] [27900] global_step=27900, grad_norm=0.34593427181243896, loss=2.981834888458252
I0305 23:52:27.015704 140334138853120 logging_writer.py:48] [28000] global_step=28000, grad_norm=0.32068005204200745, loss=2.9706149101257324
I0305 23:53:02.128549 140334130460416 logging_writer.py:48] [28100] global_step=28100, grad_norm=0.35452452301979065, loss=2.9734554290771484
I0305 23:53:37.243414 140334138853120 logging_writer.py:48] [28200] global_step=28200, grad_norm=0.326212078332901, loss=2.946429967880249
I0305 23:54:12.337770 140334130460416 logging_writer.py:48] [28300] global_step=28300, grad_norm=0.4282130002975464, loss=2.977724313735962
I0305 23:54:47.461314 140334138853120 logging_writer.py:48] [28400] global_step=28400, grad_norm=0.44836947321891785, loss=2.9271600246429443
I0305 23:55:22.551619 140334130460416 logging_writer.py:48] [28500] global_step=28500, grad_norm=0.32957586646080017, loss=2.9525249004364014
I0305 23:55:57.666596 140334138853120 logging_writer.py:48] [28600] global_step=28600, grad_norm=0.32283806800842285, loss=2.9197046756744385
I0305 23:56:32.759263 140334130460416 logging_writer.py:48] [28700] global_step=28700, grad_norm=0.43227604031562805, loss=3.0081725120544434
I0305 23:57:07.907992 140334138853120 logging_writer.py:48] [28800] global_step=28800, grad_norm=0.3963357210159302, loss=2.9253485202789307
I0305 23:57:14.238442 140477253723328 spec.py:321] Evaluating on the training split.
I0305 23:57:16.886669 140477253723328 workload.py:181] Translating evaluation dataset.
I0305 23:59:49.303259 140477253723328 spec.py:333] Evaluating on the validation split.
I0305 23:59:51.950578 140477253723328 workload.py:181] Translating evaluation dataset.
I0306 00:02:34.294039 140477253723328 spec.py:349] Evaluating on the test split.
I0306 00:02:36.939302 140477253723328 workload.py:181] Translating evaluation dataset.
I0306 00:05:07.584262 140477253723328 submission_runner.py:469] Time since start: 17497.81s, 	Step: 28819, 	{'train/accuracy': 0.6348914504051208, 'train/loss': 1.8895816802978516, 'train/bleu': 30.926485427535294, 'validation/accuracy': 0.6562924981117249, 'validation/loss': 1.7297848463058472, 'validation/bleu': 28.026752332261967, 'validation/num_examples': 3000, 'test/accuracy': 0.6664001941680908, 'test/loss': 1.6742825508117676, 'test/bleu': 27.4100268203981, 'test/num_examples': 3003, 'score': 10106.439329862595, 'total_duration': 17497.809415340424, 'accumulated_submission_time': 10106.439329862595, 'accumulated_eval_time': 7389.374656677246, 'accumulated_logging_time': 0.22908878326416016}
I0306 00:05:07.598627 140334130460416 logging_writer.py:48] [28819] accumulated_eval_time=7389.37, accumulated_logging_time=0.229089, accumulated_submission_time=10106.4, global_step=28819, preemption_count=0, score=10106.4, test/accuracy=0.6664, test/bleu=27.41, test/loss=1.67428, test/num_examples=3003, total_duration=17497.8, train/accuracy=0.634891, train/bleu=30.9265, train/loss=1.88958, validation/accuracy=0.656292, validation/bleu=28.0268, validation/loss=1.72978, validation/num_examples=3000
I0306 00:05:36.303429 140334138853120 logging_writer.py:48] [28900] global_step=28900, grad_norm=0.38561010360717773, loss=2.987881660461426
I0306 00:06:11.316207 140334130460416 logging_writer.py:48] [29000] global_step=29000, grad_norm=0.503997266292572, loss=3.063197612762451
I0306 00:06:46.365087 140334138853120 logging_writer.py:48] [29100] global_step=29100, grad_norm=0.3556731045246124, loss=2.9081952571868896
I0306 00:07:21.410075 140334130460416 logging_writer.py:48] [29200] global_step=29200, grad_norm=0.4344307482242584, loss=3.042189121246338
I0306 00:07:56.473491 140334138853120 logging_writer.py:48] [29300] global_step=29300, grad_norm=0.40466269850730896, loss=2.981455087661743
I0306 00:08:31.553006 140334130460416 logging_writer.py:48] [29400] global_step=29400, grad_norm=0.32922232151031494, loss=2.9470250606536865
I0306 00:09:06.669625 140334138853120 logging_writer.py:48] [29500] global_step=29500, grad_norm=0.33994895219802856, loss=2.9055991172790527
I0306 00:09:41.781742 140334130460416 logging_writer.py:48] [29600] global_step=29600, grad_norm=0.4870187044143677, loss=2.9448249340057373
I0306 00:10:16.876569 140334138853120 logging_writer.py:48] [29700] global_step=29700, grad_norm=0.32474738359451294, loss=2.9434516429901123
I0306 00:10:52.001103 140334130460416 logging_writer.py:48] [29800] global_step=29800, grad_norm=0.3525758981704712, loss=3.0191967487335205
I0306 00:11:27.069394 140334138853120 logging_writer.py:48] [29900] global_step=29900, grad_norm=0.3220028281211853, loss=2.9701614379882812
I0306 00:12:02.170224 140334130460416 logging_writer.py:48] [30000] global_step=30000, grad_norm=0.34920966625213623, loss=2.954833984375
I0306 00:12:37.252250 140334138853120 logging_writer.py:48] [30100] global_step=30100, grad_norm=0.3671515882015228, loss=3.059882164001465
I0306 00:13:12.296623 140334130460416 logging_writer.py:48] [30200] global_step=30200, grad_norm=0.3667823374271393, loss=2.880581855773926
I0306 00:13:47.398091 140334138853120 logging_writer.py:48] [30300] global_step=30300, grad_norm=0.3892347514629364, loss=2.9904797077178955
I0306 00:14:22.489524 140334130460416 logging_writer.py:48] [30400] global_step=30400, grad_norm=0.38873496651649475, loss=2.968057870864868
I0306 00:14:57.594258 140334138853120 logging_writer.py:48] [30500] global_step=30500, grad_norm=0.3476942479610443, loss=2.9892473220825195
I0306 00:15:32.732050 140334130460416 logging_writer.py:48] [30600] global_step=30600, grad_norm=0.4233330190181732, loss=2.954075574874878
I0306 00:16:07.851010 140334138853120 logging_writer.py:48] [30700] global_step=30700, grad_norm=0.33959928154945374, loss=2.9627060890197754
I0306 00:16:42.975980 140334130460416 logging_writer.py:48] [30800] global_step=30800, grad_norm=0.35900014638900757, loss=2.982297658920288
I0306 00:17:18.061576 140334138853120 logging_writer.py:48] [30900] global_step=30900, grad_norm=0.310918390750885, loss=3.0230729579925537
I0306 00:17:53.149873 140334130460416 logging_writer.py:48] [31000] global_step=31000, grad_norm=0.34437283873558044, loss=2.9871671199798584
I0306 00:18:28.236279 140334138853120 logging_writer.py:48] [31100] global_step=31100, grad_norm=0.3753942847251892, loss=3.0230495929718018
I0306 00:19:03.318399 140334130460416 logging_writer.py:48] [31200] global_step=31200, grad_norm=0.3504188358783722, loss=2.8796238899230957
I0306 00:19:07.892511 140477253723328 spec.py:321] Evaluating on the training split.
I0306 00:19:10.545439 140477253723328 workload.py:181] Translating evaluation dataset.
I0306 00:22:06.443798 140477253723328 spec.py:333] Evaluating on the validation split.
I0306 00:22:09.084040 140477253723328 workload.py:181] Translating evaluation dataset.
I0306 00:24:38.905833 140477253723328 spec.py:349] Evaluating on the test split.
I0306 00:24:41.552181 140477253723328 workload.py:181] Translating evaluation dataset.
I0306 00:27:10.574314 140477253723328 submission_runner.py:469] Time since start: 18820.80s, 	Step: 31214, 	{'train/accuracy': 0.6358052492141724, 'train/loss': 1.880720615386963, 'train/bleu': 30.77213321212285, 'validation/accuracy': 0.6580599546432495, 'validation/loss': 1.7265146970748901, 'validation/bleu': 28.13197181209696, 'validation/num_examples': 3000, 'test/accuracy': 0.6677325963973999, 'test/loss': 1.6688928604125977, 'test/bleu': 27.72172317419608, 'test/num_examples': 3003, 'score': 10946.58758854866, 'total_duration': 18820.799479722977, 'accumulated_submission_time': 10946.58758854866, 'accumulated_eval_time': 7872.056391716003, 'accumulated_logging_time': 0.2523503303527832}
I0306 00:27:10.587330 140334138853120 logging_writer.py:48] [31214] accumulated_eval_time=7872.06, accumulated_logging_time=0.25235, accumulated_submission_time=10946.6, global_step=31214, preemption_count=0, score=10946.6, test/accuracy=0.667733, test/bleu=27.7217, test/loss=1.66889, test/num_examples=3003, total_duration=18820.8, train/accuracy=0.635805, train/bleu=30.7721, train/loss=1.88072, validation/accuracy=0.65806, validation/bleu=28.132, validation/loss=1.72651, validation/num_examples=3000
I0306 00:27:41.070260 140334130460416 logging_writer.py:48] [31300] global_step=31300, grad_norm=0.36592888832092285, loss=2.909747362136841
I0306 00:28:16.023949 140334138853120 logging_writer.py:48] [31400] global_step=31400, grad_norm=0.345478355884552, loss=2.9142606258392334
I0306 00:28:50.973544 140334130460416 logging_writer.py:48] [31500] global_step=31500, grad_norm=0.4316824972629547, loss=2.8771018981933594
I0306 00:29:25.891209 140334138853120 logging_writer.py:48] [31600] global_step=31600, grad_norm=0.41622576117515564, loss=2.9235074520111084
I0306 00:30:00.817802 140334130460416 logging_writer.py:48] [31700] global_step=31700, grad_norm=0.4094425439834595, loss=2.9524691104888916
I0306 00:30:35.749467 140334138853120 logging_writer.py:48] [31800] global_step=31800, grad_norm=0.3547983467578888, loss=3.0284218788146973
I0306 00:31:10.652889 140334130460416 logging_writer.py:48] [31900] global_step=31900, grad_norm=0.42096683382987976, loss=2.9936435222625732
I0306 00:31:45.555443 140334138853120 logging_writer.py:48] [32000] global_step=32000, grad_norm=0.34174731373786926, loss=2.906572103500366
I0306 00:32:20.460516 140334130460416 logging_writer.py:48] [32100] global_step=32100, grad_norm=0.36078399419784546, loss=2.941197633743286
I0306 00:32:55.373175 140334138853120 logging_writer.py:48] [32200] global_step=32200, grad_norm=0.33234158158302307, loss=2.9816625118255615
I0306 00:33:30.277191 140334130460416 logging_writer.py:48] [32300] global_step=32300, grad_norm=0.3935454189777374, loss=3.0023109912872314
I0306 00:34:05.197713 140334138853120 logging_writer.py:48] [32400] global_step=32400, grad_norm=0.3449939787387848, loss=3.02710223197937
I0306 00:34:40.067775 140334130460416 logging_writer.py:48] [32500] global_step=32500, grad_norm=0.39679139852523804, loss=2.98348331451416
I0306 00:35:14.918555 140334138853120 logging_writer.py:48] [32600] global_step=32600, grad_norm=0.3652418255805969, loss=2.985106945037842
I0306 00:35:49.793865 140334130460416 logging_writer.py:48] [32700] global_step=32700, grad_norm=0.4228980839252472, loss=3.020610809326172
I0306 00:36:24.660547 140334138853120 logging_writer.py:48] [32800] global_step=32800, grad_norm=0.3702445924282074, loss=2.974207878112793
I0306 00:36:59.521981 140334130460416 logging_writer.py:48] [32900] global_step=32900, grad_norm=0.38451719284057617, loss=3.0176169872283936
I0306 00:37:34.396209 140334138853120 logging_writer.py:48] [33000] global_step=33000, grad_norm=0.3703271746635437, loss=2.95089054107666
I0306 00:38:09.263693 140334130460416 logging_writer.py:48] [33100] global_step=33100, grad_norm=0.36286959052085876, loss=2.8999011516571045
I0306 00:38:44.139224 140334138853120 logging_writer.py:48] [33200] global_step=33200, grad_norm=0.3386719822883606, loss=3.012657880783081
I0306 00:39:18.978053 140334130460416 logging_writer.py:48] [33300] global_step=33300, grad_norm=0.3415694534778595, loss=2.96030592918396
I0306 00:39:53.837930 140334138853120 logging_writer.py:48] [33400] global_step=33400, grad_norm=0.34909331798553467, loss=2.9606950283050537
I0306 00:40:28.707339 140334130460416 logging_writer.py:48] [33500] global_step=33500, grad_norm=0.38710522651672363, loss=2.979336738586426
I0306 00:41:03.562654 140334138853120 logging_writer.py:48] [33600] global_step=33600, grad_norm=0.3692614436149597, loss=2.9635438919067383
I0306 00:41:10.894576 140477253723328 spec.py:321] Evaluating on the training split.
I0306 00:41:13.539826 140477253723328 workload.py:181] Translating evaluation dataset.
I0306 00:44:21.716114 140477253723328 spec.py:333] Evaluating on the validation split.
I0306 00:44:24.359891 140477253723328 workload.py:181] Translating evaluation dataset.
I0306 00:46:57.188723 140477253723328 spec.py:349] Evaluating on the test split.
I0306 00:46:59.824459 140477253723328 workload.py:181] Translating evaluation dataset.
I0306 00:49:50.539681 140477253723328 submission_runner.py:469] Time since start: 20180.76s, 	Step: 33622, 	{'train/accuracy': 0.637984573841095, 'train/loss': 1.8654241561889648, 'train/bleu': 31.043798555665944, 'validation/accuracy': 0.6600005030632019, 'validation/loss': 1.7085578441619873, 'validation/bleu': 28.40906355137768, 'validation/num_examples': 3000, 'test/accuracy': 0.6687637567520142, 'test/loss': 1.6518052816390991, 'test/bleu': 27.754809333751638, 'test/num_examples': 3003, 'score': 11786.744064331055, 'total_duration': 20180.76486158371, 'accumulated_submission_time': 11786.744064331055, 'accumulated_eval_time': 8391.701442480087, 'accumulated_logging_time': 0.2747626304626465}
I0306 00:49:50.551474 140334130460416 logging_writer.py:48] [33622] accumulated_eval_time=8391.7, accumulated_logging_time=0.274763, accumulated_submission_time=11786.7, global_step=33622, preemption_count=0, score=11786.7, test/accuracy=0.668764, test/bleu=27.7548, test/loss=1.65181, test/num_examples=3003, total_duration=20180.8, train/accuracy=0.637985, train/bleu=31.0438, train/loss=1.86542, validation/accuracy=0.660001, validation/bleu=28.4091, validation/loss=1.70856, validation/num_examples=3000
I0306 00:50:18.042866 140334138853120 logging_writer.py:48] [33700] global_step=33700, grad_norm=0.32413166761398315, loss=2.9434523582458496
I0306 00:50:52.880736 140334130460416 logging_writer.py:48] [33800] global_step=33800, grad_norm=0.35746046900749207, loss=2.9566454887390137
I0306 00:51:27.709059 140334138853120 logging_writer.py:48] [33900] global_step=33900, grad_norm=0.3481267988681793, loss=2.9232656955718994
I0306 00:52:02.568560 140334130460416 logging_writer.py:48] [34000] global_step=34000, grad_norm=0.3878590762615204, loss=2.9743330478668213
I0306 00:52:37.461301 140334138853120 logging_writer.py:48] [34100] global_step=34100, grad_norm=0.353369802236557, loss=2.937958002090454
I0306 00:53:12.358761 140334130460416 logging_writer.py:48] [34200] global_step=34200, grad_norm=0.3775175213813782, loss=3.0102474689483643
I0306 00:53:47.255501 140334138853120 logging_writer.py:48] [34300] global_step=34300, grad_norm=0.3399284780025482, loss=2.9935569763183594
I0306 00:54:22.139037 140334130460416 logging_writer.py:48] [34400] global_step=34400, grad_norm=0.35218995809555054, loss=2.9146430492401123
I0306 00:54:57.037543 140334138853120 logging_writer.py:48] [34500] global_step=34500, grad_norm=0.4551173150539398, loss=2.978847026824951
I0306 00:55:31.956241 140334130460416 logging_writer.py:48] [34600] global_step=34600, grad_norm=0.4132981300354004, loss=2.9385054111480713
I0306 00:56:06.845832 140334138853120 logging_writer.py:48] [34700] global_step=34700, grad_norm=0.3735402226448059, loss=2.911583662033081
I0306 00:56:41.761147 140334130460416 logging_writer.py:48] [34800] global_step=34800, grad_norm=0.35315731167793274, loss=2.9497854709625244
I0306 00:57:16.652186 140334138853120 logging_writer.py:48] [34900] global_step=34900, grad_norm=0.3690185248851776, loss=2.965571403503418
I0306 00:57:51.554093 140334130460416 logging_writer.py:48] [35000] global_step=35000, grad_norm=0.36014628410339355, loss=2.9503605365753174
I0306 00:58:26.403780 140334138853120 logging_writer.py:48] [35100] global_step=35100, grad_norm=0.3509678244590759, loss=2.93914794921875
I0306 00:59:01.299708 140334130460416 logging_writer.py:48] [35200] global_step=35200, grad_norm=0.37838003039360046, loss=3.0238237380981445
I0306 00:59:36.190518 140334138853120 logging_writer.py:48] [35300] global_step=35300, grad_norm=0.3433114290237427, loss=2.9676177501678467
I0306 01:00:11.092839 140334130460416 logging_writer.py:48] [35400] global_step=35400, grad_norm=0.4853108525276184, loss=3.0180768966674805
I0306 01:00:45.995830 140334138853120 logging_writer.py:48] [35500] global_step=35500, grad_norm=0.3521745800971985, loss=2.9488022327423096
I0306 01:01:20.908542 140334130460416 logging_writer.py:48] [35600] global_step=35600, grad_norm=0.3794212341308594, loss=2.9465794563293457
I0306 01:01:55.832155 140334138853120 logging_writer.py:48] [35700] global_step=35700, grad_norm=0.38374748826026917, loss=2.9639205932617188
I0306 01:02:30.743938 140334130460416 logging_writer.py:48] [35800] global_step=35800, grad_norm=0.4331492483615875, loss=2.9866397380828857
I0306 01:03:05.632152 140334138853120 logging_writer.py:48] [35900] global_step=35900, grad_norm=0.3563676178455353, loss=3.024794578552246
I0306 01:03:40.534089 140334130460416 logging_writer.py:48] [36000] global_step=36000, grad_norm=0.3287332057952881, loss=2.894015073776245
I0306 01:03:50.659953 140477253723328 spec.py:321] Evaluating on the training split.
I0306 01:03:53.308412 140477253723328 workload.py:181] Translating evaluation dataset.
I0306 01:06:33.360352 140477253723328 spec.py:333] Evaluating on the validation split.
I0306 01:06:35.993263 140477253723328 workload.py:181] Translating evaluation dataset.
I0306 01:09:09.003555 140477253723328 spec.py:349] Evaluating on the test split.
I0306 01:09:11.647787 140477253723328 workload.py:181] Translating evaluation dataset.
I0306 01:11:39.017210 140477253723328 submission_runner.py:469] Time since start: 21489.24s, 	Step: 36030, 	{'train/accuracy': 0.6393105983734131, 'train/loss': 1.8636890649795532, 'train/bleu': 31.0393109128669, 'validation/accuracy': 0.6589746475219727, 'validation/loss': 1.7079150676727295, 'validation/bleu': 28.32518057756314, 'validation/num_examples': 3000, 'test/accuracy': 0.6704437732696533, 'test/loss': 1.6441090106964111, 'test/bleu': 27.6287719475644, 'test/num_examples': 3003, 'score': 12626.705699205399, 'total_duration': 21489.242384433746, 'accumulated_submission_time': 12626.705699205399, 'accumulated_eval_time': 8860.058640480042, 'accumulated_logging_time': 0.2945573329925537}
I0306 01:11:39.030195 140334138853120 logging_writer.py:48] [36030] accumulated_eval_time=8860.06, accumulated_logging_time=0.294557, accumulated_submission_time=12626.7, global_step=36030, preemption_count=0, score=12626.7, test/accuracy=0.670444, test/bleu=27.6288, test/loss=1.64411, test/num_examples=3003, total_duration=21489.2, train/accuracy=0.639311, train/bleu=31.0393, train/loss=1.86369, validation/accuracy=0.658975, validation/bleu=28.3252, validation/loss=1.70792, validation/num_examples=3000
I0306 01:12:03.759685 140334130460416 logging_writer.py:48] [36100] global_step=36100, grad_norm=0.3200221657752991, loss=2.9683656692504883
I0306 01:12:38.584765 140334138853120 logging_writer.py:48] [36200] global_step=36200, grad_norm=0.33466532826423645, loss=3.008744478225708
I0306 01:13:13.416928 140334130460416 logging_writer.py:48] [36300] global_step=36300, grad_norm=0.3410879075527191, loss=2.9807140827178955
I0306 01:13:48.270009 140334138853120 logging_writer.py:48] [36400] global_step=36400, grad_norm=0.3485066592693329, loss=2.9129860401153564
I0306 01:14:23.112215 140334130460416 logging_writer.py:48] [36500] global_step=36500, grad_norm=0.31089136004447937, loss=2.928316831588745
I0306 01:14:57.966724 140334138853120 logging_writer.py:48] [36600] global_step=36600, grad_norm=0.35697993636131287, loss=2.9793663024902344
I0306 01:15:32.857614 140334130460416 logging_writer.py:48] [36700] global_step=36700, grad_norm=0.3439193665981293, loss=3.0287299156188965
I0306 01:16:07.743414 140334138853120 logging_writer.py:48] [36800] global_step=36800, grad_norm=0.3792104721069336, loss=2.906412124633789
I0306 01:16:42.631824 140334130460416 logging_writer.py:48] [36900] global_step=36900, grad_norm=0.33890849351882935, loss=2.975069284439087
I0306 01:17:17.512897 140334138853120 logging_writer.py:48] [37000] global_step=37000, grad_norm=0.368426114320755, loss=2.9043266773223877
I0306 01:17:52.383661 140334130460416 logging_writer.py:48] [37100] global_step=37100, grad_norm=0.3293868899345398, loss=2.9189705848693848
I0306 01:18:27.263696 140334138853120 logging_writer.py:48] [37200] global_step=37200, grad_norm=0.34450584650039673, loss=3.0112552642822266
I0306 01:19:02.126690 140334130460416 logging_writer.py:48] [37300] global_step=37300, grad_norm=0.3642600476741791, loss=2.9626829624176025
I0306 01:19:36.963116 140334138853120 logging_writer.py:48] [37400] global_step=37400, grad_norm=0.45628711581230164, loss=2.9894192218780518
I0306 01:20:11.836356 140334130460416 logging_writer.py:48] [37500] global_step=37500, grad_norm=0.40771058201789856, loss=2.9893083572387695
I0306 01:20:46.684601 140334138853120 logging_writer.py:48] [37600] global_step=37600, grad_norm=0.34708666801452637, loss=3.0012683868408203
I0306 01:21:21.598323 140334130460416 logging_writer.py:48] [37700] global_step=37700, grad_norm=0.32813021540641785, loss=2.941455364227295
I0306 01:21:56.552220 140334138853120 logging_writer.py:48] [37800] global_step=37800, grad_norm=0.3342652916908264, loss=2.981053590774536
I0306 01:22:31.481728 140334130460416 logging_writer.py:48] [37900] global_step=37900, grad_norm=0.31884247064590454, loss=2.9371931552886963
I0306 01:23:06.432589 140334138853120 logging_writer.py:48] [38000] global_step=38000, grad_norm=0.34487617015838623, loss=3.022336959838867
I0306 01:23:41.364578 140334130460416 logging_writer.py:48] [38100] global_step=38100, grad_norm=0.41511791944503784, loss=2.9671545028686523
I0306 01:24:16.313930 140334138853120 logging_writer.py:48] [38200] global_step=38200, grad_norm=0.3183566629886627, loss=2.9841954708099365
I0306 01:24:51.288564 140334130460416 logging_writer.py:48] [38300] global_step=38300, grad_norm=0.4006556570529938, loss=3.0043790340423584
I0306 01:25:26.220162 140334138853120 logging_writer.py:48] [38400] global_step=38400, grad_norm=0.36419668793678284, loss=2.939607620239258
I0306 01:25:39.158667 140477253723328 spec.py:321] Evaluating on the training split.
I0306 01:25:41.809214 140477253723328 workload.py:181] Translating evaluation dataset.
I0306 01:28:20.866184 140477253723328 spec.py:333] Evaluating on the validation split.
I0306 01:28:23.493735 140477253723328 workload.py:181] Translating evaluation dataset.
I0306 01:31:03.122885 140477253723328 spec.py:349] Evaluating on the test split.
I0306 01:31:05.765418 140477253723328 workload.py:181] Translating evaluation dataset.
I0306 01:33:36.398811 140477253723328 submission_runner.py:469] Time since start: 22806.62s, 	Step: 38438, 	{'train/accuracy': 0.6462990641593933, 'train/loss': 1.7982689142227173, 'train/bleu': 31.557606206066044, 'validation/accuracy': 0.6599386930465698, 'validation/loss': 1.6970912218093872, 'validation/bleu': 28.303612518607455, 'validation/num_examples': 3000, 'test/accuracy': 0.6717414259910583, 'test/loss': 1.6359753608703613, 'test/bleu': 27.8366306901697, 'test/num_examples': 3003, 'score': 13466.688513994217, 'total_duration': 22806.62398481369, 'accumulated_submission_time': 13466.688513994217, 'accumulated_eval_time': 9337.298724651337, 'accumulated_logging_time': 0.31583213806152344}
I0306 01:33:36.411801 140334130460416 logging_writer.py:48] [38438] accumulated_eval_time=9337.3, accumulated_logging_time=0.315832, accumulated_submission_time=13466.7, global_step=38438, preemption_count=0, score=13466.7, test/accuracy=0.671741, test/bleu=27.8366, test/loss=1.63598, test/num_examples=3003, total_duration=22806.6, train/accuracy=0.646299, train/bleu=31.5576, train/loss=1.79827, validation/accuracy=0.659939, validation/bleu=28.3036, validation/loss=1.69709, validation/num_examples=3000
I0306 01:33:58.407758 140334138853120 logging_writer.py:48] [38500] global_step=38500, grad_norm=0.36055025458335876, loss=2.916080951690674
I0306 01:34:33.339761 140334130460416 logging_writer.py:48] [38600] global_step=38600, grad_norm=3.8544764518737793, loss=2.950517416000366
I0306 01:35:08.275688 140334138853120 logging_writer.py:48] [38700] global_step=38700, grad_norm=0.39071938395500183, loss=2.926501512527466
I0306 01:35:43.199301 140334130460416 logging_writer.py:48] [38800] global_step=38800, grad_norm=0.39363574981689453, loss=2.990460157394409
I0306 01:36:18.115548 140334138853120 logging_writer.py:48] [38900] global_step=38900, grad_norm=0.3675394654273987, loss=2.9255902767181396
I0306 01:36:53.060653 140334130460416 logging_writer.py:48] [39000] global_step=39000, grad_norm=0.3301233649253845, loss=2.992410659790039
I0306 01:37:28.017037 140334138853120 logging_writer.py:48] [39100] global_step=39100, grad_norm=0.34607887268066406, loss=2.8863332271575928
I0306 01:38:02.959546 140334130460416 logging_writer.py:48] [39200] global_step=39200, grad_norm=0.3685626685619354, loss=3.001274347305298
I0306 01:38:37.923489 140334138853120 logging_writer.py:48] [39300] global_step=39300, grad_norm=0.32965272665023804, loss=3.025104284286499
I0306 01:39:12.894714 140334130460416 logging_writer.py:48] [39400] global_step=39400, grad_norm=0.32173195481300354, loss=2.930245876312256
I0306 01:39:47.879465 140334138853120 logging_writer.py:48] [39500] global_step=39500, grad_norm=0.35359814763069153, loss=2.966827392578125
I0306 01:40:22.900972 140334130460416 logging_writer.py:48] [39600] global_step=39600, grad_norm=0.37907904386520386, loss=2.905848741531372
I0306 01:40:57.859575 140334138853120 logging_writer.py:48] [39700] global_step=39700, grad_norm=0.34401288628578186, loss=2.963406562805176
I0306 01:41:32.809239 140334130460416 logging_writer.py:48] [39800] global_step=39800, grad_norm=0.3614397644996643, loss=2.8705928325653076
I0306 01:42:07.740415 140334138853120 logging_writer.py:48] [39900] global_step=39900, grad_norm=0.3325920104980469, loss=2.92647123336792
I0306 01:42:42.665268 140334130460416 logging_writer.py:48] [40000] global_step=40000, grad_norm=0.3552953600883484, loss=2.934208631515503
I0306 01:43:17.601508 140334138853120 logging_writer.py:48] [40100] global_step=40100, grad_norm=0.3132949471473694, loss=2.8941173553466797
I0306 01:43:52.531158 140334130460416 logging_writer.py:48] [40200] global_step=40200, grad_norm=0.40483227372169495, loss=2.85158371925354
I0306 01:44:27.447261 140334138853120 logging_writer.py:48] [40300] global_step=40300, grad_norm=0.6325792074203491, loss=3.032931327819824
I0306 01:45:02.384625 140334130460416 logging_writer.py:48] [40400] global_step=40400, grad_norm=0.36715465784072876, loss=2.913088798522949
I0306 01:45:37.323218 140334138853120 logging_writer.py:48] [40500] global_step=40500, grad_norm=0.31268489360809326, loss=2.8791632652282715
I0306 01:46:12.242592 140334130460416 logging_writer.py:48] [40600] global_step=40600, grad_norm=0.3284357190132141, loss=2.910203456878662
I0306 01:46:47.206957 140334138853120 logging_writer.py:48] [40700] global_step=40700, grad_norm=0.3732755780220032, loss=2.98968243598938
I0306 01:47:22.155207 140334130460416 logging_writer.py:48] [40800] global_step=40800, grad_norm=0.34815144538879395, loss=2.9149234294891357
I0306 01:47:36.487153 140477253723328 spec.py:321] Evaluating on the training split.
I0306 01:47:39.135845 140477253723328 workload.py:181] Translating evaluation dataset.
I0306 01:50:19.145851 140477253723328 spec.py:333] Evaluating on the validation split.
I0306 01:50:21.794957 140477253723328 workload.py:181] Translating evaluation dataset.
I0306 01:52:57.008267 140477253723328 spec.py:349] Evaluating on the test split.
I0306 01:52:59.651290 140477253723328 workload.py:181] Translating evaluation dataset.
I0306 01:55:39.126517 140477253723328 submission_runner.py:469] Time since start: 24129.35s, 	Step: 40842, 	{'train/accuracy': 0.6398757100105286, 'train/loss': 1.8562971353530884, 'train/bleu': 31.365026029571105, 'validation/accuracy': 0.6622623801231384, 'validation/loss': 1.691129207611084, 'validation/bleu': 28.507663354102036, 'validation/num_examples': 3000, 'test/accuracy': 0.6728188991546631, 'test/loss': 1.6301780939102173, 'test/bleu': 27.910512089367185, 'test/num_examples': 3003, 'score': 14306.614209413528, 'total_duration': 24129.351665496826, 'accumulated_submission_time': 14306.614209413528, 'accumulated_eval_time': 9819.938004732132, 'accumulated_logging_time': 0.33740925788879395}
I0306 01:55:39.139997 140334138853120 logging_writer.py:48] [40842] accumulated_eval_time=9819.94, accumulated_logging_time=0.337409, accumulated_submission_time=14306.6, global_step=40842, preemption_count=0, score=14306.6, test/accuracy=0.672819, test/bleu=27.9105, test/loss=1.63018, test/num_examples=3003, total_duration=24129.4, train/accuracy=0.639876, train/bleu=31.365, train/loss=1.8563, validation/accuracy=0.662262, validation/bleu=28.5077, validation/loss=1.69113, validation/num_examples=3000
I0306 01:55:59.724623 140334130460416 logging_writer.py:48] [40900] global_step=40900, grad_norm=0.3222905099391937, loss=2.9153716564178467
I0306 01:56:34.622090 140334138853120 logging_writer.py:48] [41000] global_step=41000, grad_norm=0.33618369698524475, loss=2.929016351699829
I0306 01:57:09.575100 140334130460416 logging_writer.py:48] [41100] global_step=41100, grad_norm=0.35884889960289, loss=2.9407949447631836
I0306 01:57:44.523205 140334138853120 logging_writer.py:48] [41200] global_step=41200, grad_norm=0.37221837043762207, loss=2.9174296855926514
I0306 01:58:19.545356 140334130460416 logging_writer.py:48] [41300] global_step=41300, grad_norm=0.3389841616153717, loss=2.89874005317688
I0306 01:58:54.500097 140334138853120 logging_writer.py:48] [41400] global_step=41400, grad_norm=0.33578479290008545, loss=2.9467737674713135
I0306 01:59:29.493107 140334130460416 logging_writer.py:48] [41500] global_step=41500, grad_norm=0.33756712079048157, loss=2.9608185291290283
I0306 02:00:04.453894 140334138853120 logging_writer.py:48] [41600] global_step=41600, grad_norm=0.3527374863624573, loss=2.951263666152954
I0306 02:00:39.447237 140334130460416 logging_writer.py:48] [41700] global_step=41700, grad_norm=0.3538520336151123, loss=2.9603238105773926
I0306 02:01:14.443913 140334138853120 logging_writer.py:48] [41800] global_step=41800, grad_norm=0.34122714400291443, loss=2.945615768432617
I0306 02:01:49.431204 140334130460416 logging_writer.py:48] [41900] global_step=41900, grad_norm=0.348991334438324, loss=2.925349235534668
I0306 02:02:24.411107 140334138853120 logging_writer.py:48] [42000] global_step=42000, grad_norm=0.3803531527519226, loss=2.946453332901001
I0306 02:02:59.392194 140334130460416 logging_writer.py:48] [42100] global_step=42100, grad_norm=0.384042888879776, loss=2.921280860900879
I0306 02:03:34.386388 140334138853120 logging_writer.py:48] [42200] global_step=42200, grad_norm=0.33870160579681396, loss=2.8930728435516357
I0306 02:04:09.381237 140334130460416 logging_writer.py:48] [42300] global_step=42300, grad_norm=0.31696659326553345, loss=3.004058361053467
I0306 02:04:44.386362 140334138853120 logging_writer.py:48] [42400] global_step=42400, grad_norm=0.3192395567893982, loss=2.912306547164917
I0306 02:05:19.359710 140334130460416 logging_writer.py:48] [42500] global_step=42500, grad_norm=0.35704267024993896, loss=2.923614263534546
I0306 02:05:54.348378 140334138853120 logging_writer.py:48] [42600] global_step=42600, grad_norm=0.5698915719985962, loss=2.939281940460205
I0306 02:06:29.330493 140334130460416 logging_writer.py:48] [42700] global_step=42700, grad_norm=0.3518966734409332, loss=2.89585542678833
I0306 02:07:04.311539 140334138853120 logging_writer.py:48] [42800] global_step=42800, grad_norm=0.364205539226532, loss=2.9190380573272705
I0306 02:07:39.290679 140334130460416 logging_writer.py:48] [42900] global_step=42900, grad_norm=0.38217779994010925, loss=2.974402666091919
I0306 02:08:14.281333 140334138853120 logging_writer.py:48] [43000] global_step=43000, grad_norm=0.3352643549442291, loss=2.965414047241211
I0306 02:08:49.246211 140334130460416 logging_writer.py:48] [43100] global_step=43100, grad_norm=0.3475533723831177, loss=2.952021837234497
I0306 02:09:24.231110 140334138853120 logging_writer.py:48] [43200] global_step=43200, grad_norm=0.42554596066474915, loss=2.9293861389160156
I0306 02:09:39.275401 140477253723328 spec.py:321] Evaluating on the training split.
I0306 02:09:41.923865 140477253723328 workload.py:181] Translating evaluation dataset.
I0306 02:12:28.771073 140477253723328 spec.py:333] Evaluating on the validation split.
I0306 02:12:31.406235 140477253723328 workload.py:181] Translating evaluation dataset.
I0306 02:15:09.752720 140477253723328 spec.py:349] Evaluating on the test split.
I0306 02:15:12.395478 140477253723328 workload.py:181] Translating evaluation dataset.
I0306 02:17:39.927953 140477253723328 submission_runner.py:469] Time since start: 25450.15s, 	Step: 43244, 	{'train/accuracy': 0.6431131362915039, 'train/loss': 1.8359999656677246, 'train/bleu': 31.52353642299674, 'validation/accuracy': 0.6625590324401855, 'validation/loss': 1.677484393119812, 'validation/bleu': 28.36334018882698, 'validation/num_examples': 3000, 'test/accuracy': 0.6744177937507629, 'test/loss': 1.6152924299240112, 'test/bleu': 28.371843724639163, 'test/num_examples': 3003, 'score': 15146.601977586746, 'total_duration': 25450.153121709824, 'accumulated_submission_time': 15146.601977586746, 'accumulated_eval_time': 10300.590491056442, 'accumulated_logging_time': 0.3592197895050049}
I0306 02:17:39.941428 140334130460416 logging_writer.py:48] [43244] accumulated_eval_time=10300.6, accumulated_logging_time=0.35922, accumulated_submission_time=15146.6, global_step=43244, preemption_count=0, score=15146.6, test/accuracy=0.674418, test/bleu=28.3718, test/loss=1.61529, test/num_examples=3003, total_duration=25450.2, train/accuracy=0.643113, train/bleu=31.5235, train/loss=1.836, validation/accuracy=0.662559, validation/bleu=28.3633, validation/loss=1.67748, validation/num_examples=3000
I0306 02:17:59.842495 140334138853120 logging_writer.py:48] [43300] global_step=43300, grad_norm=0.34313198924064636, loss=2.931933641433716
I0306 02:18:34.775086 140334130460416 logging_writer.py:48] [43400] global_step=43400, grad_norm=0.30386045575141907, loss=2.9569408893585205
I0306 02:19:09.686803 140334138853120 logging_writer.py:48] [43500] global_step=43500, grad_norm=0.3660782277584076, loss=2.9229683876037598
I0306 02:19:44.621361 140334130460416 logging_writer.py:48] [43600] global_step=43600, grad_norm=0.32731643319129944, loss=2.9693098068237305
I0306 02:20:19.541991 140334138853120 logging_writer.py:48] [43700] global_step=43700, grad_norm=0.3926583230495453, loss=3.0498528480529785
I0306 02:20:54.487293 140334130460416 logging_writer.py:48] [43800] global_step=43800, grad_norm=0.3241760730743408, loss=2.923708438873291
I0306 02:21:29.448855 140334138853120 logging_writer.py:48] [43900] global_step=43900, grad_norm=0.3411625623703003, loss=3.001359701156616
I0306 02:22:04.351562 140334130460416 logging_writer.py:48] [44000] global_step=44000, grad_norm=0.34111401438713074, loss=2.9670403003692627
I0306 02:22:39.297143 140334138853120 logging_writer.py:48] [44100] global_step=44100, grad_norm=0.36239707469940186, loss=2.939523220062256
I0306 02:23:14.233928 140334130460416 logging_writer.py:48] [44200] global_step=44200, grad_norm=0.34756961464881897, loss=2.97293758392334
I0306 02:23:49.190672 140334138853120 logging_writer.py:48] [44300] global_step=44300, grad_norm=0.3342129588127136, loss=2.900238037109375
I0306 02:24:24.155685 140334130460416 logging_writer.py:48] [44400] global_step=44400, grad_norm=0.3372806906700134, loss=2.9679243564605713
I0306 02:24:59.101700 140334138853120 logging_writer.py:48] [44500] global_step=44500, grad_norm=0.327349454164505, loss=2.9764163494110107
I0306 02:25:34.059468 140334130460416 logging_writer.py:48] [44600] global_step=44600, grad_norm=0.3466271162033081, loss=2.866001605987549
I0306 02:26:09.017847 140334138853120 logging_writer.py:48] [44700] global_step=44700, grad_norm=0.3399732708930969, loss=2.879055976867676
I0306 02:26:43.959400 140334130460416 logging_writer.py:48] [44800] global_step=44800, grad_norm=0.30036768317222595, loss=2.9650325775146484
I0306 02:27:18.918951 140334138853120 logging_writer.py:48] [44900] global_step=44900, grad_norm=0.42927059531211853, loss=2.9553427696228027
I0306 02:27:53.846776 140334130460416 logging_writer.py:48] [45000] global_step=45000, grad_norm=0.3223940134048462, loss=3.028407335281372
I0306 02:28:28.816579 140334138853120 logging_writer.py:48] [45100] global_step=45100, grad_norm=0.3435443341732025, loss=2.8957464694976807
I0306 02:29:03.769050 140334130460416 logging_writer.py:48] [45200] global_step=45200, grad_norm=0.35628435015678406, loss=2.9597225189208984
I0306 02:29:38.748036 140334138853120 logging_writer.py:48] [45300] global_step=45300, grad_norm=0.3768540024757385, loss=2.8589189052581787
I0306 02:30:13.716014 140334130460416 logging_writer.py:48] [45400] global_step=45400, grad_norm=0.3726898729801178, loss=2.945296049118042
I0306 02:30:48.661422 140334138853120 logging_writer.py:48] [45500] global_step=45500, grad_norm=0.32561779022216797, loss=2.9348068237304688
I0306 02:31:23.627110 140334130460416 logging_writer.py:48] [45600] global_step=45600, grad_norm=0.3138885796070099, loss=2.8970961570739746
I0306 02:31:40.066859 140477253723328 spec.py:321] Evaluating on the training split.
I0306 02:31:42.715706 140477253723328 workload.py:181] Translating evaluation dataset.
I0306 02:34:28.853150 140477253723328 spec.py:333] Evaluating on the validation split.
I0306 02:34:31.495452 140477253723328 workload.py:181] Translating evaluation dataset.
I0306 02:37:10.326081 140477253723328 spec.py:349] Evaluating on the test split.
I0306 02:37:12.968730 140477253723328 workload.py:181] Translating evaluation dataset.
I0306 02:39:58.293711 140477253723328 submission_runner.py:469] Time since start: 26788.52s, 	Step: 45648, 	{'train/accuracy': 0.6492317318916321, 'train/loss': 1.7921429872512817, 'train/bleu': 31.503788220257764, 'validation/accuracy': 0.6653524041175842, 'validation/loss': 1.6730129718780518, 'validation/bleu': 28.700134178939177, 'validation/num_examples': 3000, 'test/accuracy': 0.6756691336631775, 'test/loss': 1.6119084358215332, 'test/bleu': 27.8786194682226, 'test/num_examples': 3003, 'score': 15986.579032182693, 'total_duration': 26788.51887845993, 'accumulated_submission_time': 15986.579032182693, 'accumulated_eval_time': 10798.817275047302, 'accumulated_logging_time': 0.3810243606567383}
I0306 02:39:58.307205 140334138853120 logging_writer.py:48] [45648] accumulated_eval_time=10798.8, accumulated_logging_time=0.381024, accumulated_submission_time=15986.6, global_step=45648, preemption_count=0, score=15986.6, test/accuracy=0.675669, test/bleu=27.8786, test/loss=1.61191, test/num_examples=3003, total_duration=26788.5, train/accuracy=0.649232, train/bleu=31.5038, train/loss=1.79214, validation/accuracy=0.665352, validation/bleu=28.7001, validation/loss=1.67301, validation/num_examples=3000
I0306 02:40:16.838556 140334130460416 logging_writer.py:48] [45700] global_step=45700, grad_norm=0.35380637645721436, loss=2.8903839588165283
I0306 02:40:51.785769 140334138853120 logging_writer.py:48] [45800] global_step=45800, grad_norm=0.33536678552627563, loss=3.0011649131774902
I0306 02:41:26.734186 140334130460416 logging_writer.py:48] [45900] global_step=45900, grad_norm=0.3450605571269989, loss=2.9592418670654297
I0306 02:42:01.700241 140334138853120 logging_writer.py:48] [46000] global_step=46000, grad_norm=0.3442073166370392, loss=2.95920467376709
I0306 02:42:36.649279 140334130460416 logging_writer.py:48] [46100] global_step=46100, grad_norm=0.3354169726371765, loss=2.8955307006835938
I0306 02:43:11.596608 140334138853120 logging_writer.py:48] [46200] global_step=46200, grad_norm=0.34169039130210876, loss=2.893179178237915
I0306 02:43:46.522260 140334130460416 logging_writer.py:48] [46300] global_step=46300, grad_norm=0.3076067268848419, loss=2.8835134506225586
I0306 02:44:21.458347 140334138853120 logging_writer.py:48] [46400] global_step=46400, grad_norm=0.3349544107913971, loss=2.976696252822876
I0306 02:44:56.395930 140334130460416 logging_writer.py:48] [46500] global_step=46500, grad_norm=0.358969122171402, loss=2.940943479537964
I0306 02:45:31.336251 140334138853120 logging_writer.py:48] [46600] global_step=46600, grad_norm=0.33922818303108215, loss=2.8583483695983887
I0306 02:46:06.303033 140334130460416 logging_writer.py:48] [46700] global_step=46700, grad_norm=0.3530757427215576, loss=2.9465856552124023
I0306 02:46:41.279356 140334138853120 logging_writer.py:48] [46800] global_step=46800, grad_norm=0.3508918583393097, loss=2.883455753326416
I0306 02:47:16.217052 140334130460416 logging_writer.py:48] [46900] global_step=46900, grad_norm=0.34533461928367615, loss=2.9515230655670166
I0306 02:47:51.194872 140334138853120 logging_writer.py:48] [47000] global_step=47000, grad_norm=0.32252633571624756, loss=2.8088431358337402
I0306 02:48:26.143954 140334130460416 logging_writer.py:48] [47100] global_step=47100, grad_norm=0.33755606412887573, loss=2.9319190979003906
I0306 02:49:01.067444 140334138853120 logging_writer.py:48] [47200] global_step=47200, grad_norm=0.3137831687927246, loss=2.8178744316101074
I0306 02:49:36.016450 140334130460416 logging_writer.py:48] [47300] global_step=47300, grad_norm=0.36942654848098755, loss=2.8650481700897217
I0306 02:50:10.938570 140334138853120 logging_writer.py:48] [47400] global_step=47400, grad_norm=0.31644025444984436, loss=2.9445178508758545
I0306 02:50:45.901977 140334130460416 logging_writer.py:48] [47500] global_step=47500, grad_norm=0.39569568634033203, loss=2.956193447113037
I0306 02:51:20.866860 140334138853120 logging_writer.py:48] [47600] global_step=47600, grad_norm=0.32978346943855286, loss=2.9159629344940186
I0306 02:51:55.833824 140334130460416 logging_writer.py:48] [47700] global_step=47700, grad_norm=0.39514875411987305, loss=3.0014545917510986
I0306 02:52:30.780755 140334138853120 logging_writer.py:48] [47800] global_step=47800, grad_norm=0.3363263010978699, loss=2.9553916454315186
I0306 02:53:05.732020 140334130460416 logging_writer.py:48] [47900] global_step=47900, grad_norm=0.3244412839412689, loss=2.8785059452056885
I0306 02:53:40.675019 140334138853120 logging_writer.py:48] [48000] global_step=48000, grad_norm=0.32281896471977234, loss=2.9042487144470215
I0306 02:53:58.494072 140477253723328 spec.py:321] Evaluating on the training split.
I0306 02:54:01.136629 140477253723328 workload.py:181] Translating evaluation dataset.
I0306 02:56:58.493057 140477253723328 spec.py:333] Evaluating on the validation split.
I0306 02:57:01.129549 140477253723328 workload.py:181] Translating evaluation dataset.
I0306 02:59:38.774352 140477253723328 spec.py:349] Evaluating on the test split.
I0306 02:59:41.420266 140477253723328 workload.py:181] Translating evaluation dataset.
I0306 03:02:15.511812 140477253723328 submission_runner.py:469] Time since start: 28125.74s, 	Step: 48052, 	{'train/accuracy': 0.6454460024833679, 'train/loss': 1.8072640895843506, 'train/bleu': 31.672306378050664, 'validation/accuracy': 0.6650928258895874, 'validation/loss': 1.6679704189300537, 'validation/bleu': 28.72303339205247, 'validation/num_examples': 3000, 'test/accuracy': 0.6774302124977112, 'test/loss': 1.5986354351043701, 'test/bleu': 28.356947926034227, 'test/num_examples': 3003, 'score': 16826.62109518051, 'total_duration': 28125.73698782921, 'accumulated_submission_time': 16826.62109518051, 'accumulated_eval_time': 11295.83495926857, 'accumulated_logging_time': 0.40302348136901855}
I0306 03:02:15.525547 140334130460416 logging_writer.py:48] [48052] accumulated_eval_time=11295.8, accumulated_logging_time=0.403023, accumulated_submission_time=16826.6, global_step=48052, preemption_count=0, score=16826.6, test/accuracy=0.67743, test/bleu=28.3569, test/loss=1.59864, test/num_examples=3003, total_duration=28125.7, train/accuracy=0.645446, train/bleu=31.6723, train/loss=1.80726, validation/accuracy=0.665093, validation/bleu=28.723, validation/loss=1.66797, validation/num_examples=3000
I0306 03:02:32.624390 140334138853120 logging_writer.py:48] [48100] global_step=48100, grad_norm=0.33216774463653564, loss=2.8775901794433594
I0306 03:03:07.541361 140334130460416 logging_writer.py:48] [48200] global_step=48200, grad_norm=0.35045892000198364, loss=2.919506549835205
I0306 03:03:42.481981 140334138853120 logging_writer.py:48] [48300] global_step=48300, grad_norm=0.3392145037651062, loss=2.923764944076538
I0306 03:04:17.412759 140334130460416 logging_writer.py:48] [48400] global_step=48400, grad_norm=0.35408785939216614, loss=2.90136456489563
I0306 03:04:52.395895 140334138853120 logging_writer.py:48] [48500] global_step=48500, grad_norm=0.3201940655708313, loss=2.9003822803497314
I0306 03:05:27.375741 140334130460416 logging_writer.py:48] [48600] global_step=48600, grad_norm=0.344044029712677, loss=2.8734090328216553
I0306 03:06:02.341795 140334138853120 logging_writer.py:48] [48700] global_step=48700, grad_norm=0.3398192524909973, loss=2.9296395778656006
I0306 03:06:37.338759 140334130460416 logging_writer.py:48] [48800] global_step=48800, grad_norm=0.32072991132736206, loss=2.859461545944214
I0306 03:07:12.349233 140334138853120 logging_writer.py:48] [48900] global_step=48900, grad_norm=0.33704066276550293, loss=2.9103121757507324
I0306 03:07:47.324195 140334130460416 logging_writer.py:48] [49000] global_step=49000, grad_norm=0.3489643931388855, loss=2.894777536392212
I0306 03:08:22.288816 140334138853120 logging_writer.py:48] [49100] global_step=49100, grad_norm=0.3767479956150055, loss=2.8818955421447754
I0306 03:08:57.273896 140334130460416 logging_writer.py:48] [49200] global_step=49200, grad_norm=0.3618897497653961, loss=2.872959852218628
I0306 03:09:32.271014 140334138853120 logging_writer.py:48] [49300] global_step=49300, grad_norm=0.3181658983230591, loss=2.8588593006134033
I0306 03:10:07.240111 140334130460416 logging_writer.py:48] [49400] global_step=49400, grad_norm=0.3992513418197632, loss=2.861060857772827
I0306 03:10:42.214064 140334138853120 logging_writer.py:48] [49500] global_step=49500, grad_norm=0.34365254640579224, loss=2.927248001098633
I0306 03:11:17.204007 140334130460416 logging_writer.py:48] [49600] global_step=49600, grad_norm=0.315645307302475, loss=2.8569488525390625
I0306 03:11:52.202741 140334138853120 logging_writer.py:48] [49700] global_step=49700, grad_norm=0.3387746214866638, loss=2.891571283340454
I0306 03:12:27.172948 140334130460416 logging_writer.py:48] [49800] global_step=49800, grad_norm=0.37211793661117554, loss=2.875641345977783
I0306 03:13:02.148400 140334138853120 logging_writer.py:48] [49900] global_step=49900, grad_norm=0.36200574040412903, loss=2.9364707469940186
I0306 03:13:37.115272 140334130460416 logging_writer.py:48] [50000] global_step=50000, grad_norm=0.32640737295150757, loss=2.945866584777832
I0306 03:14:12.100147 140334138853120 logging_writer.py:48] [50100] global_step=50100, grad_norm=0.35925838351249695, loss=2.8808510303497314
I0306 03:14:47.046444 140334130460416 logging_writer.py:48] [50200] global_step=50200, grad_norm=0.3439587354660034, loss=2.8996713161468506
I0306 03:15:22.038146 140334138853120 logging_writer.py:48] [50300] global_step=50300, grad_norm=0.3537115454673767, loss=2.9294188022613525
I0306 03:15:57.027207 140334130460416 logging_writer.py:48] [50400] global_step=50400, grad_norm=0.31904521584510803, loss=2.893791675567627
I0306 03:16:15.579460 140477253723328 spec.py:321] Evaluating on the training split.
I0306 03:16:18.225085 140477253723328 workload.py:181] Translating evaluation dataset.
I0306 03:20:31.889420 140477253723328 spec.py:333] Evaluating on the validation split.
I0306 03:20:34.523934 140477253723328 workload.py:181] Translating evaluation dataset.
I0306 03:25:15.268884 140477253723328 spec.py:349] Evaluating on the test split.
I0306 03:25:17.907675 140477253723328 workload.py:181] Translating evaluation dataset.
I0306 03:29:29.661503 140477253723328 submission_runner.py:469] Time since start: 29759.89s, 	Step: 50454, 	{'train/accuracy': 0.6673710346221924, 'train/loss': 1.6674065589904785, 'train/bleu': 32.67558024004903, 'validation/accuracy': 0.6685536503791809, 'validation/loss': 1.653506875038147, 'validation/bleu': 27.447097335472453, 'validation/num_examples': 3000, 'test/accuracy': 0.6776850819587708, 'test/loss': 1.59489905834198, 'test/bleu': 28.017952719907147, 'test/num_examples': 3003, 'score': 17666.533705711365, 'total_duration': 29759.886680841446, 'accumulated_submission_time': 17666.533705711365, 'accumulated_eval_time': 12089.916955471039, 'accumulated_logging_time': 0.42466115951538086}
I0306 03:29:29.675799 140334138853120 logging_writer.py:48] [50454] accumulated_eval_time=12089.9, accumulated_logging_time=0.424661, accumulated_submission_time=17666.5, global_step=50454, preemption_count=0, score=17666.5, test/accuracy=0.677685, test/bleu=28.018, test/loss=1.5949, test/num_examples=3003, total_duration=29759.9, train/accuracy=0.667371, train/bleu=32.6756, train/loss=1.66741, validation/accuracy=0.668554, validation/bleu=27.4471, validation/loss=1.65351, validation/num_examples=3000
I0306 03:29:46.087873 140334130460416 logging_writer.py:48] [50500] global_step=50500, grad_norm=0.36953631043434143, loss=2.9529800415039062
I0306 03:30:20.968756 140334138853120 logging_writer.py:48] [50600] global_step=50600, grad_norm=0.3140033483505249, loss=2.8675525188446045
I0306 03:30:55.880901 140334130460416 logging_writer.py:48] [50700] global_step=50700, grad_norm=0.33084985613822937, loss=2.8889825344085693
I0306 03:31:30.823724 140334138853120 logging_writer.py:48] [50800] global_step=50800, grad_norm=0.3509794771671295, loss=2.926622152328491
I0306 03:32:05.761973 140334130460416 logging_writer.py:48] [50900] global_step=50900, grad_norm=0.33147844672203064, loss=2.955559015274048
I0306 03:32:40.716769 140334138853120 logging_writer.py:48] [51000] global_step=51000, grad_norm=0.36209964752197266, loss=2.869422674179077
I0306 03:33:15.684623 140334130460416 logging_writer.py:48] [51100] global_step=51100, grad_norm=0.3503711223602295, loss=3.001599073410034
I0306 03:33:50.640793 140334138853120 logging_writer.py:48] [51200] global_step=51200, grad_norm=0.33273276686668396, loss=2.9175148010253906
I0306 03:34:25.583037 140334130460416 logging_writer.py:48] [51300] global_step=51300, grad_norm=0.34258681535720825, loss=2.8964271545410156
I0306 03:35:00.518331 140334138853120 logging_writer.py:48] [51400] global_step=51400, grad_norm=0.34324172139167786, loss=2.9152708053588867
I0306 03:35:35.459471 140334130460416 logging_writer.py:48] [51500] global_step=51500, grad_norm=0.4283309578895569, loss=2.9042584896087646
I0306 03:36:10.408305 140334138853120 logging_writer.py:48] [51600] global_step=51600, grad_norm=0.3715376853942871, loss=2.9732472896575928
I0306 03:36:45.359138 140334130460416 logging_writer.py:48] [51700] global_step=51700, grad_norm=0.4126259684562683, loss=2.9517838954925537
I0306 03:37:20.292527 140334138853120 logging_writer.py:48] [51800] global_step=51800, grad_norm=0.3185247480869293, loss=2.9279391765594482
I0306 03:37:55.252031 140334130460416 logging_writer.py:48] [51900] global_step=51900, grad_norm=0.3337995409965515, loss=2.9011528491973877
I0306 03:38:30.224144 140334138853120 logging_writer.py:48] [52000] global_step=52000, grad_norm=0.33955350518226624, loss=2.9423537254333496
I0306 03:39:05.160489 140334130460416 logging_writer.py:48] [52100] global_step=52100, grad_norm=0.3279874920845032, loss=2.8584651947021484
I0306 03:39:40.091934 140334138853120 logging_writer.py:48] [52200] global_step=52200, grad_norm=0.37818631529808044, loss=2.878661632537842
I0306 03:40:15.038164 140334130460416 logging_writer.py:48] [52300] global_step=52300, grad_norm=0.35302087664604187, loss=2.875643491744995
I0306 03:40:49.978555 140334138853120 logging_writer.py:48] [52400] global_step=52400, grad_norm=0.343318909406662, loss=2.922783136367798
I0306 03:41:24.917473 140334130460416 logging_writer.py:48] [52500] global_step=52500, grad_norm=0.31706973910331726, loss=2.934040069580078
I0306 03:41:59.857782 140334138853120 logging_writer.py:48] [52600] global_step=52600, grad_norm=0.34517157077789307, loss=2.8787496089935303
I0306 03:42:34.814396 140334130460416 logging_writer.py:48] [52700] global_step=52700, grad_norm=0.34797385334968567, loss=2.8737123012542725
I0306 03:43:09.781166 140334138853120 logging_writer.py:48] [52800] global_step=52800, grad_norm=0.3517334461212158, loss=2.84269642829895
I0306 03:43:29.722306 140477253723328 spec.py:321] Evaluating on the training split.
I0306 03:43:32.374346 140477253723328 workload.py:181] Translating evaluation dataset.
I0306 03:46:06.320950 140477253723328 spec.py:333] Evaluating on the validation split.
I0306 03:46:08.958198 140477253723328 workload.py:181] Translating evaluation dataset.
I0306 03:48:45.955619 140477253723328 spec.py:349] Evaluating on the test split.
I0306 03:48:48.601072 140477253723328 workload.py:181] Translating evaluation dataset.
I0306 03:51:17.347014 140477253723328 submission_runner.py:469] Time since start: 31067.57s, 	Step: 52858, 	{'train/accuracy': 0.6457720398902893, 'train/loss': 1.8041551113128662, 'train/bleu': 32.13398876817845, 'validation/accuracy': 0.6687266826629639, 'validation/loss': 1.6477055549621582, 'validation/bleu': 29.055083924925675, 'validation/num_examples': 3000, 'test/accuracy': 0.6800255179405212, 'test/loss': 1.587914228439331, 'test/bleu': 28.627580162824685, 'test/num_examples': 3003, 'score': 18506.433409929276, 'total_duration': 31067.572167396545, 'accumulated_submission_time': 18506.433409929276, 'accumulated_eval_time': 12557.541586637497, 'accumulated_logging_time': 0.44739747047424316}
I0306 03:51:17.361340 140334130460416 logging_writer.py:48] [52858] accumulated_eval_time=12557.5, accumulated_logging_time=0.447397, accumulated_submission_time=18506.4, global_step=52858, preemption_count=0, score=18506.4, test/accuracy=0.680026, test/bleu=28.6276, test/loss=1.58791, test/num_examples=3003, total_duration=31067.6, train/accuracy=0.645772, train/bleu=32.134, train/loss=1.80416, validation/accuracy=0.668727, validation/bleu=29.0551, validation/loss=1.64771, validation/num_examples=3000
I0306 03:51:32.367279 140334138853120 logging_writer.py:48] [52900] global_step=52900, grad_norm=0.32747650146484375, loss=2.9787709712982178
I0306 03:52:07.284321 140334130460416 logging_writer.py:48] [53000] global_step=53000, grad_norm=0.38676726818084717, loss=2.8364808559417725
I0306 03:52:42.220596 140334138853120 logging_writer.py:48] [53100] global_step=53100, grad_norm=0.39083531498908997, loss=2.9308927059173584
I0306 03:53:17.178077 140334130460416 logging_writer.py:48] [53200] global_step=53200, grad_norm=0.33094650506973267, loss=3.0000548362731934
I0306 03:53:52.111333 140334138853120 logging_writer.py:48] [53300] global_step=53300, grad_norm=0.32799363136291504, loss=2.874534845352173
I0306 03:54:27.071643 140334130460416 logging_writer.py:48] [53400] global_step=53400, grad_norm=0.3735782206058502, loss=2.9658029079437256
I0306 03:55:02.025650 140334138853120 logging_writer.py:48] [53500] global_step=53500, grad_norm=0.35558655858039856, loss=2.8787946701049805
I0306 03:55:36.983579 140334130460416 logging_writer.py:48] [53600] global_step=53600, grad_norm=0.32297587394714355, loss=2.8655524253845215
I0306 03:56:11.938555 140334138853120 logging_writer.py:48] [53700] global_step=53700, grad_norm=0.3073308765888214, loss=2.8799946308135986
I0306 03:56:46.879358 140334130460416 logging_writer.py:48] [53800] global_step=53800, grad_norm=0.3343881070613861, loss=2.8851680755615234
I0306 03:57:21.818367 140334138853120 logging_writer.py:48] [53900] global_step=53900, grad_norm=0.3412522077560425, loss=2.973038673400879
I0306 03:57:56.765007 140334130460416 logging_writer.py:48] [54000] global_step=54000, grad_norm=0.3630695044994354, loss=2.846230983734131
I0306 03:58:31.710275 140334138853120 logging_writer.py:48] [54100] global_step=54100, grad_norm=0.3447781801223755, loss=2.914703607559204
I0306 03:59:06.667521 140334130460416 logging_writer.py:48] [54200] global_step=54200, grad_norm=0.4091889262199402, loss=2.8867578506469727
I0306 03:59:41.605514 140334138853120 logging_writer.py:48] [54300] global_step=54300, grad_norm=0.3591643273830414, loss=2.986675262451172
I0306 04:00:16.533520 140334130460416 logging_writer.py:48] [54400] global_step=54400, grad_norm=0.3916068971157074, loss=2.823485851287842
I0306 04:00:51.474420 140334138853120 logging_writer.py:48] [54500] global_step=54500, grad_norm=0.34049561619758606, loss=2.867123603820801
I0306 04:01:26.462694 140334130460416 logging_writer.py:48] [54600] global_step=54600, grad_norm=0.3596794009208679, loss=2.8616788387298584
I0306 04:02:01.395276 140334138853120 logging_writer.py:48] [54700] global_step=54700, grad_norm=0.3549738824367523, loss=2.9307682514190674
I0306 04:02:36.366752 140334130460416 logging_writer.py:48] [54800] global_step=54800, grad_norm=0.33787834644317627, loss=2.9268887042999268
I0306 04:03:11.311732 140334138853120 logging_writer.py:48] [54900] global_step=54900, grad_norm=0.4259530007839203, loss=2.9221179485321045
I0306 04:03:46.257600 140334130460416 logging_writer.py:48] [55000] global_step=55000, grad_norm=0.41005244851112366, loss=2.95719051361084
I0306 04:04:21.213951 140334138853120 logging_writer.py:48] [55100] global_step=55100, grad_norm=0.35294103622436523, loss=2.939873456954956
I0306 04:04:56.157836 140334130460416 logging_writer.py:48] [55200] global_step=55200, grad_norm=0.6161088347434998, loss=2.8930490016937256
I0306 04:05:17.481286 140477253723328 spec.py:321] Evaluating on the training split.
I0306 04:05:20.126173 140477253723328 workload.py:181] Translating evaluation dataset.
I0306 04:08:38.464159 140477253723328 spec.py:333] Evaluating on the validation split.
I0306 04:08:41.105654 140477253723328 workload.py:181] Translating evaluation dataset.
I0306 04:11:17.574137 140477253723328 spec.py:349] Evaluating on the test split.
I0306 04:11:20.218034 140477253723328 workload.py:181] Translating evaluation dataset.
I0306 04:14:02.876171 140477253723328 submission_runner.py:469] Time since start: 32433.10s, 	Step: 55262, 	{'train/accuracy': 0.6467801332473755, 'train/loss': 1.810073733329773, 'train/bleu': 31.82863315701233, 'validation/accuracy': 0.6691840291023254, 'validation/loss': 1.644310474395752, 'validation/bleu': 29.049534546047454, 'validation/num_examples': 3000, 'test/accuracy': 0.6798169612884521, 'test/loss': 1.5782716274261475, 'test/bleu': 28.35553455257382, 'test/num_examples': 3003, 'score': 19346.4083006382, 'total_duration': 32433.10134243965, 'accumulated_submission_time': 19346.4083006382, 'accumulated_eval_time': 13082.936410665512, 'accumulated_logging_time': 0.4701051712036133}
I0306 04:14:02.891852 140334138853120 logging_writer.py:48] [55262] accumulated_eval_time=13082.9, accumulated_logging_time=0.470105, accumulated_submission_time=19346.4, global_step=55262, preemption_count=0, score=19346.4, test/accuracy=0.679817, test/bleu=28.3555, test/loss=1.57827, test/num_examples=3003, total_duration=32433.1, train/accuracy=0.64678, train/bleu=31.8286, train/loss=1.81007, validation/accuracy=0.669184, validation/bleu=29.0495, validation/loss=1.64431, validation/num_examples=3000
I0306 04:14:16.506676 140334130460416 logging_writer.py:48] [55300] global_step=55300, grad_norm=0.3817847967147827, loss=2.8263447284698486
I0306 04:14:51.403233 140334138853120 logging_writer.py:48] [55400] global_step=55400, grad_norm=0.312136173248291, loss=2.8964438438415527
I0306 04:15:26.315545 140334130460416 logging_writer.py:48] [55500] global_step=55500, grad_norm=0.41575074195861816, loss=2.905670404434204
I0306 04:16:01.269385 140334138853120 logging_writer.py:48] [55600] global_step=55600, grad_norm=0.35159042477607727, loss=2.83736515045166
I0306 04:16:36.228443 140334130460416 logging_writer.py:48] [55700] global_step=55700, grad_norm=0.37329262495040894, loss=2.9167444705963135
I0306 04:17:11.182704 140334138853120 logging_writer.py:48] [55800] global_step=55800, grad_norm=0.35601505637168884, loss=2.946535587310791
I0306 04:17:46.123484 140334130460416 logging_writer.py:48] [55900] global_step=55900, grad_norm=0.33828288316726685, loss=2.929832935333252
I0306 04:18:21.085247 140334138853120 logging_writer.py:48] [56000] global_step=56000, grad_norm=0.33528557419776917, loss=2.8394129276275635
I0306 04:18:56.053793 140334130460416 logging_writer.py:48] [56100] global_step=56100, grad_norm=0.37712693214416504, loss=2.926361083984375
I0306 04:19:31.028061 140334138853120 logging_writer.py:48] [56200] global_step=56200, grad_norm=0.3354988992214203, loss=2.8516809940338135
I0306 04:20:05.984794 140334130460416 logging_writer.py:48] [56300] global_step=56300, grad_norm=0.34620505571365356, loss=2.90488338470459
I0306 04:20:40.979527 140334138853120 logging_writer.py:48] [56400] global_step=56400, grad_norm=0.3419066369533539, loss=2.8804614543914795
I0306 04:21:15.901292 140334130460416 logging_writer.py:48] [56500] global_step=56500, grad_norm=0.3406178057193756, loss=2.908597946166992
I0306 04:21:50.790731 140334138853120 logging_writer.py:48] [56600] global_step=56600, grad_norm=0.35107409954071045, loss=2.947629690170288
I0306 04:22:25.709909 140334130460416 logging_writer.py:48] [56700] global_step=56700, grad_norm=0.33495527505874634, loss=2.8967511653900146
I0306 04:23:00.610431 140334138853120 logging_writer.py:48] [56800] global_step=56800, grad_norm=0.3223802149295807, loss=2.9157299995422363
I0306 04:23:35.502373 140334130460416 logging_writer.py:48] [56900] global_step=56900, grad_norm=0.35481876134872437, loss=2.871053695678711
I0306 04:24:10.417476 140334138853120 logging_writer.py:48] [57000] global_step=57000, grad_norm=0.3464476466178894, loss=2.9220798015594482
I0306 04:24:45.312273 140334130460416 logging_writer.py:48] [57100] global_step=57100, grad_norm=0.32021474838256836, loss=2.918114423751831
I0306 04:25:20.202679 140334138853120 logging_writer.py:48] [57200] global_step=57200, grad_norm=0.3679729402065277, loss=2.9720189571380615
I0306 04:25:55.116258 140334130460416 logging_writer.py:48] [57300] global_step=57300, grad_norm=0.3651752471923828, loss=2.927271842956543
I0306 04:26:30.014636 140334138853120 logging_writer.py:48] [57400] global_step=57400, grad_norm=0.3613241910934448, loss=2.909109115600586
I0306 04:27:04.934796 140334130460416 logging_writer.py:48] [57500] global_step=57500, grad_norm=0.3531721532344818, loss=2.9302942752838135
I0306 04:27:39.837514 140334138853120 logging_writer.py:48] [57600] global_step=57600, grad_norm=0.34608203172683716, loss=2.8179409503936768
I0306 04:28:02.890314 140477253723328 spec.py:321] Evaluating on the training split.
I0306 04:28:05.538151 140477253723328 workload.py:181] Translating evaluation dataset.
I0306 04:31:19.285679 140477253723328 spec.py:333] Evaluating on the validation split.
I0306 04:31:21.930100 140477253723328 workload.py:181] Translating evaluation dataset.
I0306 04:34:05.858946 140477253723328 spec.py:349] Evaluating on the test split.
I0306 04:34:08.498353 140477253723328 workload.py:181] Translating evaluation dataset.
I0306 04:36:56.504816 140477253723328 submission_runner.py:469] Time since start: 33806.73s, 	Step: 57667, 	{'train/accuracy': 0.6530262231826782, 'train/loss': 1.759045124053955, 'train/bleu': 32.16294794604371, 'validation/accuracy': 0.6700615882873535, 'validation/loss': 1.6389081478118896, 'validation/bleu': 29.339930589944416, 'validation/num_examples': 3000, 'test/accuracy': 0.6838952898979187, 'test/loss': 1.5656360387802124, 'test/bleu': 29.069460312787662, 'test/num_examples': 3003, 'score': 20186.262072086334, 'total_duration': 33806.72999358177, 'accumulated_submission_time': 20186.262072086334, 'accumulated_eval_time': 13616.550857067108, 'accumulated_logging_time': 0.4946565628051758}
I0306 04:36:56.521362 140334130460416 logging_writer.py:48] [57667] accumulated_eval_time=13616.6, accumulated_logging_time=0.494657, accumulated_submission_time=20186.3, global_step=57667, preemption_count=0, score=20186.3, test/accuracy=0.683895, test/bleu=29.0695, test/loss=1.56564, test/num_examples=3003, total_duration=33806.7, train/accuracy=0.653026, train/bleu=32.1629, train/loss=1.75905, validation/accuracy=0.670062, validation/bleu=29.3399, validation/loss=1.63891, validation/num_examples=3000
I0306 04:37:08.368112 140334138853120 logging_writer.py:48] [57700] global_step=57700, grad_norm=0.34810513257980347, loss=2.857518196105957
I0306 04:37:43.218184 140334130460416 logging_writer.py:48] [57800] global_step=57800, grad_norm=0.37858542799949646, loss=2.970668077468872
I0306 04:38:18.098359 140334138853120 logging_writer.py:48] [57900] global_step=57900, grad_norm=0.3703092336654663, loss=2.8360090255737305
I0306 04:38:53.015751 140334130460416 logging_writer.py:48] [58000] global_step=58000, grad_norm=0.3493022322654724, loss=2.9552652835845947
I0306 04:39:27.956122 140334138853120 logging_writer.py:48] [58100] global_step=58100, grad_norm=0.3583718538284302, loss=2.915694236755371
I0306 04:40:02.854703 140334130460416 logging_writer.py:48] [58200] global_step=58200, grad_norm=0.3815566599369049, loss=2.8450610637664795
I0306 04:40:37.784083 140334138853120 logging_writer.py:48] [58300] global_step=58300, grad_norm=0.3442540764808655, loss=2.9229180812835693
I0306 04:41:12.690416 140334130460416 logging_writer.py:48] [58400] global_step=58400, grad_norm=0.3579939007759094, loss=2.9464197158813477
I0306 04:41:47.604093 140334138853120 logging_writer.py:48] [58500] global_step=58500, grad_norm=0.34493523836135864, loss=2.883519172668457
I0306 04:42:22.536508 140334130460416 logging_writer.py:48] [58600] global_step=58600, grad_norm=0.359003484249115, loss=2.9314146041870117
I0306 04:42:57.500595 140334138853120 logging_writer.py:48] [58700] global_step=58700, grad_norm=0.3559694290161133, loss=2.902033805847168
I0306 04:43:32.430068 140334130460416 logging_writer.py:48] [58800] global_step=58800, grad_norm=0.3684083819389343, loss=2.856509208679199
I0306 04:44:07.355352 140334138853120 logging_writer.py:48] [58900] global_step=58900, grad_norm=0.39487224817276, loss=2.8967032432556152
I0306 04:44:42.314450 140334130460416 logging_writer.py:48] [59000] global_step=59000, grad_norm=0.40664443373680115, loss=2.959545612335205
I0306 04:45:17.283886 140334138853120 logging_writer.py:48] [59100] global_step=59100, grad_norm=0.3617111146450043, loss=2.924795150756836
I0306 04:45:52.226736 140334130460416 logging_writer.py:48] [59200] global_step=59200, grad_norm=0.3524443209171295, loss=2.8794984817504883
I0306 04:46:27.156360 140334138853120 logging_writer.py:48] [59300] global_step=59300, grad_norm=0.42122864723205566, loss=2.9211392402648926
I0306 04:47:02.088985 140334130460416 logging_writer.py:48] [59400] global_step=59400, grad_norm=0.33310574293136597, loss=2.8423378467559814
I0306 04:47:37.030179 140334138853120 logging_writer.py:48] [59500] global_step=59500, grad_norm=0.35046517848968506, loss=2.9420762062072754
I0306 04:48:11.946721 140334130460416 logging_writer.py:48] [59600] global_step=59600, grad_norm=0.3443945348262787, loss=2.8479363918304443
I0306 04:48:46.894283 140334138853120 logging_writer.py:48] [59700] global_step=59700, grad_norm=0.34435752034187317, loss=2.9140536785125732
I0306 04:49:21.829746 140334130460416 logging_writer.py:48] [59800] global_step=59800, grad_norm=0.32856041193008423, loss=2.9219346046447754
I0306 04:49:56.767434 140334138853120 logging_writer.py:48] [59900] global_step=59900, grad_norm=0.3784893751144409, loss=2.876685380935669
I0306 04:50:31.719389 140334130460416 logging_writer.py:48] [60000] global_step=60000, grad_norm=0.3502230644226074, loss=2.881951093673706
I0306 04:50:56.549645 140477253723328 spec.py:321] Evaluating on the training split.
I0306 04:50:59.199639 140477253723328 workload.py:181] Translating evaluation dataset.
I0306 04:53:40.987872 140477253723328 spec.py:333] Evaluating on the validation split.
I0306 04:53:43.632114 140477253723328 workload.py:181] Translating evaluation dataset.
I0306 04:56:16.757897 140477253723328 spec.py:349] Evaluating on the test split.
I0306 04:56:19.407950 140477253723328 workload.py:181] Translating evaluation dataset.
I0306 04:58:54.847821 140477253723328 submission_runner.py:469] Time since start: 35125.07s, 	Step: 60072, 	{'train/accuracy': 0.6512733101844788, 'train/loss': 1.7800978422164917, 'train/bleu': 31.71721463585255, 'validation/accuracy': 0.6721380352973938, 'validation/loss': 1.6330503225326538, 'validation/bleu': 29.271075281230264, 'validation/num_examples': 3000, 'test/accuracy': 0.6858185529708862, 'test/loss': 1.5646239519119263, 'test/bleu': 28.500827428543484, 'test/num_examples': 3003, 'score': 21026.146174907684, 'total_duration': 35125.07299160957, 'accumulated_submission_time': 21026.146174907684, 'accumulated_eval_time': 14094.848971128464, 'accumulated_logging_time': 0.5200779438018799}
I0306 04:58:54.863097 140334138853120 logging_writer.py:48] [60072] accumulated_eval_time=14094.8, accumulated_logging_time=0.520078, accumulated_submission_time=21026.1, global_step=60072, preemption_count=0, score=21026.1, test/accuracy=0.685819, test/bleu=28.5008, test/loss=1.56462, test/num_examples=3003, total_duration=35125.1, train/accuracy=0.651273, train/bleu=31.7172, train/loss=1.7801, validation/accuracy=0.672138, validation/bleu=29.2711, validation/loss=1.63305, validation/num_examples=3000
I0306 04:59:04.985220 140334130460416 logging_writer.py:48] [60100] global_step=60100, grad_norm=0.34725886583328247, loss=2.899550676345825
I0306 04:59:39.841054 140334138853120 logging_writer.py:48] [60200] global_step=60200, grad_norm=0.3339202105998993, loss=2.886944532394409
I0306 05:00:14.734314 140334130460416 logging_writer.py:48] [60300] global_step=60300, grad_norm=0.37550127506256104, loss=2.8625261783599854
I0306 05:00:49.664391 140334138853120 logging_writer.py:48] [60400] global_step=60400, grad_norm=0.32545343041419983, loss=2.9318387508392334
I0306 05:01:24.547301 140334130460416 logging_writer.py:48] [60500] global_step=60500, grad_norm=0.3712249994277954, loss=2.8976550102233887
I0306 05:01:59.441179 140334138853120 logging_writer.py:48] [60600] global_step=60600, grad_norm=0.34371107816696167, loss=2.851177453994751
I0306 05:02:34.329543 140334130460416 logging_writer.py:48] [60700] global_step=60700, grad_norm=0.3589854836463928, loss=2.9328761100769043
I0306 05:03:09.237744 140334138853120 logging_writer.py:48] [60800] global_step=60800, grad_norm=0.34604671597480774, loss=2.8310086727142334
I0306 05:03:44.141208 140334130460416 logging_writer.py:48] [60900] global_step=60900, grad_norm=0.3325732946395874, loss=2.920748472213745
I0306 05:04:19.032093 140334138853120 logging_writer.py:48] [61000] global_step=61000, grad_norm=0.4194439649581909, loss=2.8444125652313232
I0306 05:04:53.941468 140334130460416 logging_writer.py:48] [61100] global_step=61100, grad_norm=0.36458083987236023, loss=2.8742563724517822
I0306 05:05:28.859871 140334138853120 logging_writer.py:48] [61200] global_step=61200, grad_norm=0.3653382658958435, loss=2.8658018112182617
I0306 05:06:03.760552 140334130460416 logging_writer.py:48] [61300] global_step=61300, grad_norm=0.4236879050731659, loss=2.9495177268981934
I0306 05:06:38.680041 140334138853120 logging_writer.py:48] [61400] global_step=61400, grad_norm=2.1159844398498535, loss=2.9010534286499023
I0306 05:07:13.603601 140334130460416 logging_writer.py:48] [61500] global_step=61500, grad_norm=0.3648001551628113, loss=2.8753252029418945
I0306 05:07:48.483031 140334138853120 logging_writer.py:48] [61600] global_step=61600, grad_norm=0.36039695143699646, loss=2.891603469848633
I0306 05:08:23.415730 140334130460416 logging_writer.py:48] [61700] global_step=61700, grad_norm=0.3763884902000427, loss=2.8604345321655273
I0306 05:08:58.298239 140334138853120 logging_writer.py:48] [61800] global_step=61800, grad_norm=0.37422236800193787, loss=2.8184351921081543
I0306 05:09:33.240700 140334130460416 logging_writer.py:48] [61900] global_step=61900, grad_norm=0.3544280230998993, loss=2.857944965362549
I0306 05:10:08.140001 140334138853120 logging_writer.py:48] [62000] global_step=62000, grad_norm=0.3664913773536682, loss=2.905710220336914
I0306 05:10:43.064328 140334130460416 logging_writer.py:48] [62100] global_step=62100, grad_norm=0.34365126490592957, loss=2.912716865539551
I0306 05:11:17.962721 140334138853120 logging_writer.py:48] [62200] global_step=62200, grad_norm=0.35490182042121887, loss=2.9538791179656982
I0306 05:11:52.891796 140334130460416 logging_writer.py:48] [62300] global_step=62300, grad_norm=0.34780028462409973, loss=2.864934206008911
I0306 05:12:27.807522 140334138853120 logging_writer.py:48] [62400] global_step=62400, grad_norm=0.382951557636261, loss=2.9247450828552246
I0306 05:12:55.038348 140477253723328 spec.py:321] Evaluating on the training split.
I0306 05:12:57.682373 140477253723328 workload.py:181] Translating evaluation dataset.
I0306 05:15:50.996233 140477253723328 spec.py:333] Evaluating on the validation split.
I0306 05:15:53.626539 140477253723328 workload.py:181] Translating evaluation dataset.
I0306 05:18:22.315556 140477253723328 spec.py:349] Evaluating on the test split.
I0306 05:18:24.956520 140477253723328 workload.py:181] Translating evaluation dataset.
I0306 05:20:43.183318 140477253723328 submission_runner.py:469] Time since start: 36433.41s, 	Step: 62479, 	{'train/accuracy': 0.6469464898109436, 'train/loss': 1.7981781959533691, 'train/bleu': 31.962317196838143, 'validation/accuracy': 0.6721627712249756, 'validation/loss': 1.6219220161437988, 'validation/bleu': 29.09704793442837, 'validation/num_examples': 3000, 'test/accuracy': 0.6850423216819763, 'test/loss': 1.5514124631881714, 'test/bleu': 28.307149418625894, 'test/num_examples': 3003, 'score': 21866.178804159164, 'total_duration': 36433.408490896225, 'accumulated_submission_time': 21866.178804159164, 'accumulated_eval_time': 14562.993886470795, 'accumulated_logging_time': 0.5434150695800781}
I0306 05:20:43.199173 140334130460416 logging_writer.py:48] [62479] accumulated_eval_time=14563, accumulated_logging_time=0.543415, accumulated_submission_time=21866.2, global_step=62479, preemption_count=0, score=21866.2, test/accuracy=0.685042, test/bleu=28.3071, test/loss=1.55141, test/num_examples=3003, total_duration=36433.4, train/accuracy=0.646946, train/bleu=31.9623, train/loss=1.79818, validation/accuracy=0.672163, validation/bleu=29.097, validation/loss=1.62192, validation/num_examples=3000
I0306 05:20:50.888552 140334138853120 logging_writer.py:48] [62500] global_step=62500, grad_norm=0.3508990705013275, loss=2.8501803874969482
I0306 05:21:25.755579 140334130460416 logging_writer.py:48] [62600] global_step=62600, grad_norm=0.3668909966945648, loss=2.856525421142578
I0306 05:22:00.620414 140334138853120 logging_writer.py:48] [62700] global_step=62700, grad_norm=0.34479832649230957, loss=2.8810086250305176
I0306 05:22:35.561951 140334130460416 logging_writer.py:48] [62800] global_step=62800, grad_norm=0.35326701402664185, loss=2.912581443786621
I0306 05:23:10.509374 140334138853120 logging_writer.py:48] [62900] global_step=62900, grad_norm=0.36599200963974, loss=2.8798906803131104
I0306 05:23:45.457860 140334130460416 logging_writer.py:48] [63000] global_step=63000, grad_norm=0.3599412441253662, loss=2.8197789192199707
I0306 05:24:20.388522 140334138853120 logging_writer.py:48] [63100] global_step=63100, grad_norm=0.33245036005973816, loss=2.8813493251800537
I0306 05:24:55.327637 140334130460416 logging_writer.py:48] [63200] global_step=63200, grad_norm=0.35498544573783875, loss=2.8850088119506836
I0306 05:25:30.282361 140334138853120 logging_writer.py:48] [63300] global_step=63300, grad_norm=0.4017927944660187, loss=2.820707082748413
I0306 05:26:05.228112 140334130460416 logging_writer.py:48] [63400] global_step=63400, grad_norm=0.3892877399921417, loss=2.839785099029541
I0306 05:26:40.171240 140334138853120 logging_writer.py:48] [63500] global_step=63500, grad_norm=0.3296184837818146, loss=2.861400842666626
I0306 05:27:15.128982 140334130460416 logging_writer.py:48] [63600] global_step=63600, grad_norm=0.33607423305511475, loss=2.813594341278076
I0306 05:27:50.074063 140334138853120 logging_writer.py:48] [63700] global_step=63700, grad_norm=0.3706105649471283, loss=2.883639335632324
I0306 05:28:25.031217 140334130460416 logging_writer.py:48] [63800] global_step=63800, grad_norm=0.35307344794273376, loss=2.8599355220794678
I0306 05:28:59.989446 140334138853120 logging_writer.py:48] [63900] global_step=63900, grad_norm=0.3570806086063385, loss=2.8783769607543945
I0306 05:29:34.926992 140334130460416 logging_writer.py:48] [64000] global_step=64000, grad_norm=0.392273485660553, loss=2.852083444595337
I0306 05:30:09.866799 140334138853120 logging_writer.py:48] [64100] global_step=64100, grad_norm=0.3441455662250519, loss=2.837456226348877
I0306 05:30:44.814721 140334130460416 logging_writer.py:48] [64200] global_step=64200, grad_norm=0.35160863399505615, loss=2.8765170574188232
I0306 05:31:19.740840 140334138853120 logging_writer.py:48] [64300] global_step=64300, grad_norm=0.3668099045753479, loss=2.8078176975250244
I0306 05:31:54.664536 140334130460416 logging_writer.py:48] [64400] global_step=64400, grad_norm=0.3471788763999939, loss=2.8796138763427734
I0306 05:32:29.624560 140334138853120 logging_writer.py:48] [64500] global_step=64500, grad_norm=0.33031731843948364, loss=2.831451416015625
I0306 05:33:04.560793 140334130460416 logging_writer.py:48] [64600] global_step=64600, grad_norm=0.3524128496646881, loss=2.87332820892334
I0306 05:33:39.501717 140334138853120 logging_writer.py:48] [64700] global_step=64700, grad_norm=0.3654041588306427, loss=2.811016082763672
I0306 05:34:14.456972 140334130460416 logging_writer.py:48] [64800] global_step=64800, grad_norm=0.33766552805900574, loss=2.8664779663085938
I0306 05:34:43.474257 140477253723328 spec.py:321] Evaluating on the training split.
I0306 05:34:46.124538 140477253723328 workload.py:181] Translating evaluation dataset.
I0306 05:37:41.947062 140477253723328 spec.py:333] Evaluating on the validation split.
I0306 05:37:44.589861 140477253723328 workload.py:181] Translating evaluation dataset.
I0306 05:40:24.530041 140477253723328 spec.py:349] Evaluating on the test split.
I0306 05:40:27.187077 140477253723328 workload.py:181] Translating evaluation dataset.
I0306 05:42:56.277954 140477253723328 submission_runner.py:469] Time since start: 37766.50s, 	Step: 64884, 	{'train/accuracy': 0.6581771373748779, 'train/loss': 1.7254279851913452, 'train/bleu': 32.19322724442931, 'validation/accuracy': 0.6744864583015442, 'validation/loss': 1.614680528640747, 'validation/bleu': 29.43092399054622, 'validation/num_examples': 3000, 'test/accuracy': 0.6856911182403564, 'test/loss': 1.5491634607315063, 'test/bleu': 28.730474497998962, 'test/num_examples': 3003, 'score': 22706.31214284897, 'total_duration': 37766.50312590599, 'accumulated_submission_time': 22706.31214284897, 'accumulated_eval_time': 15055.797522306442, 'accumulated_logging_time': 0.5674965381622314}
I0306 05:42:56.293364 140334138853120 logging_writer.py:48] [64884] accumulated_eval_time=15055.8, accumulated_logging_time=0.567497, accumulated_submission_time=22706.3, global_step=64884, preemption_count=0, score=22706.3, test/accuracy=0.685691, test/bleu=28.7305, test/loss=1.54916, test/num_examples=3003, total_duration=37766.5, train/accuracy=0.658177, train/bleu=32.1932, train/loss=1.72543, validation/accuracy=0.674486, validation/bleu=29.4309, validation/loss=1.61468, validation/num_examples=3000
I0306 05:43:02.235597 140334130460416 logging_writer.py:48] [64900] global_step=64900, grad_norm=0.34484803676605225, loss=2.8855645656585693
I0306 05:43:37.125782 140334138853120 logging_writer.py:48] [65000] global_step=65000, grad_norm=0.3576693832874298, loss=2.866824150085449
I0306 05:44:12.041114 140334130460416 logging_writer.py:48] [65100] global_step=65100, grad_norm=0.3807253837585449, loss=2.8008081912994385
I0306 05:44:47.004981 140334138853120 logging_writer.py:48] [65200] global_step=65200, grad_norm=0.35645925998687744, loss=2.9132308959960938
I0306 05:45:21.931844 140334130460416 logging_writer.py:48] [65300] global_step=65300, grad_norm=0.3848707377910614, loss=2.886387825012207
I0306 05:45:56.889932 140334138853120 logging_writer.py:48] [65400] global_step=65400, grad_norm=0.34977826476097107, loss=2.840580463409424
I0306 05:46:31.861742 140334130460416 logging_writer.py:48] [65500] global_step=65500, grad_norm=0.37345510721206665, loss=2.8870294094085693
I0306 05:47:06.839655 140334138853120 logging_writer.py:48] [65600] global_step=65600, grad_norm=0.3479388356208801, loss=2.902482748031616
I0306 05:47:41.808089 140334130460416 logging_writer.py:48] [65700] global_step=65700, grad_norm=0.36941778659820557, loss=2.901597738265991
I0306 05:48:16.772068 140334138853120 logging_writer.py:48] [65800] global_step=65800, grad_norm=0.35158541798591614, loss=2.859062671661377
I0306 05:48:51.743741 140334130460416 logging_writer.py:48] [65900] global_step=65900, grad_norm=0.3371191918849945, loss=2.895246982574463
I0306 05:49:26.686040 140334138853120 logging_writer.py:48] [66000] global_step=66000, grad_norm=0.3628004491329193, loss=2.783414840698242
I0306 05:50:01.638598 140334130460416 logging_writer.py:48] [66100] global_step=66100, grad_norm=0.3531516492366791, loss=2.897188901901245
I0306 05:50:36.546823 140334138853120 logging_writer.py:48] [66200] global_step=66200, grad_norm=0.3673374652862549, loss=2.8839762210845947
I0306 05:51:11.518530 140334130460416 logging_writer.py:48] [66300] global_step=66300, grad_norm=0.3624570369720459, loss=2.8298158645629883
I0306 05:51:46.471838 140334138853120 logging_writer.py:48] [66400] global_step=66400, grad_norm=0.42515987157821655, loss=2.8495712280273438
I0306 05:52:21.436863 140334130460416 logging_writer.py:48] [66500] global_step=66500, grad_norm=0.36007633805274963, loss=2.885392904281616
I0306 05:52:56.430805 140334138853120 logging_writer.py:48] [66600] global_step=66600, grad_norm=0.3626711070537567, loss=2.8156960010528564
I0306 05:53:31.390768 140334130460416 logging_writer.py:48] [66700] global_step=66700, grad_norm=0.35769787430763245, loss=2.852436065673828
I0306 05:54:06.356764 140334138853120 logging_writer.py:48] [66800] global_step=66800, grad_norm=0.3903377056121826, loss=2.8868930339813232
I0306 05:54:41.302410 140334130460416 logging_writer.py:48] [66900] global_step=66900, grad_norm=0.342930406332016, loss=2.8570661544799805
I0306 05:55:16.274551 140334138853120 logging_writer.py:48] [67000] global_step=67000, grad_norm=0.33800870180130005, loss=2.8349192142486572
I0306 05:55:51.222039 140334130460416 logging_writer.py:48] [67100] global_step=67100, grad_norm=0.3493148386478424, loss=2.8459951877593994
I0306 05:56:26.180471 140334138853120 logging_writer.py:48] [67200] global_step=67200, grad_norm=0.37246865034103394, loss=2.8756871223449707
I0306 05:56:56.577877 140477253723328 spec.py:321] Evaluating on the training split.
I0306 05:56:59.226273 140477253723328 workload.py:181] Translating evaluation dataset.
I0306 05:59:47.096745 140477253723328 spec.py:333] Evaluating on the validation split.
I0306 05:59:49.729433 140477253723328 workload.py:181] Translating evaluation dataset.
I0306 06:02:28.610072 140477253723328 spec.py:349] Evaluating on the test split.
I0306 06:02:31.260870 140477253723328 workload.py:181] Translating evaluation dataset.
I0306 06:05:00.771231 140477253723328 submission_runner.py:469] Time since start: 39091.00s, 	Step: 67288, 	{'train/accuracy': 0.6542203426361084, 'train/loss': 1.75526762008667, 'train/bleu': 32.21505873333365, 'validation/accuracy': 0.6738437414169312, 'validation/loss': 1.6092305183410645, 'validation/bleu': 29.397676926702164, 'validation/num_examples': 3000, 'test/accuracy': 0.6885760426521301, 'test/loss': 1.5352171659469604, 'test/bleu': 29.04536890698128, 'test/num_examples': 3003, 'score': 23546.45552754402, 'total_duration': 39090.996404886246, 'accumulated_submission_time': 23546.45552754402, 'accumulated_eval_time': 15539.990818738937, 'accumulated_logging_time': 0.5909633636474609}
I0306 06:05:00.787024 140334130460416 logging_writer.py:48] [67288] accumulated_eval_time=15540, accumulated_logging_time=0.590963, accumulated_submission_time=23546.5, global_step=67288, preemption_count=0, score=23546.5, test/accuracy=0.688576, test/bleu=29.0454, test/loss=1.53522, test/num_examples=3003, total_duration=39091, train/accuracy=0.65422, train/bleu=32.2151, train/loss=1.75527, validation/accuracy=0.673844, validation/bleu=29.3977, validation/loss=1.60923, validation/num_examples=3000
I0306 06:05:05.335097 140334138853120 logging_writer.py:48] [67300] global_step=67300, grad_norm=0.3219602406024933, loss=2.8775129318237305
I0306 06:05:40.248943 140334130460416 logging_writer.py:48] [67400] global_step=67400, grad_norm=0.3599167764186859, loss=2.8668978214263916
I0306 06:06:15.169494 140334138853120 logging_writer.py:48] [67500] global_step=67500, grad_norm=0.3639849126338959, loss=2.8434760570526123
I0306 06:06:50.115343 140334130460416 logging_writer.py:48] [67600] global_step=67600, grad_norm=0.3885785639286041, loss=2.8215081691741943
I0306 06:07:25.046789 140334138853120 logging_writer.py:48] [67700] global_step=67700, grad_norm=0.3731582462787628, loss=2.8967812061309814
I0306 06:07:59.999655 140334130460416 logging_writer.py:48] [67800] global_step=67800, grad_norm=0.3816830515861511, loss=2.7950737476348877
I0306 06:08:34.944642 140334138853120 logging_writer.py:48] [67900] global_step=67900, grad_norm=0.31834325194358826, loss=2.7774064540863037
I0306 06:09:09.918394 140334130460416 logging_writer.py:48] [68000] global_step=68000, grad_norm=0.351590096950531, loss=2.808398485183716
I0306 06:09:44.827744 140334138853120 logging_writer.py:48] [68100] global_step=68100, grad_norm=0.35849788784980774, loss=2.814671516418457
I0306 06:10:19.795407 140334130460416 logging_writer.py:48] [68200] global_step=68200, grad_norm=0.368385374546051, loss=2.819772481918335
I0306 06:10:54.759923 140334138853120 logging_writer.py:48] [68300] global_step=68300, grad_norm=0.3576083183288574, loss=2.8210623264312744
I0306 06:11:29.698071 140334130460416 logging_writer.py:48] [68400] global_step=68400, grad_norm=0.4167037904262543, loss=2.841412305831909
I0306 06:12:04.683199 140334138853120 logging_writer.py:48] [68500] global_step=68500, grad_norm=0.40761134028434753, loss=2.779635190963745
I0306 06:12:39.622799 140334130460416 logging_writer.py:48] [68600] global_step=68600, grad_norm=0.36303311586380005, loss=2.9172475337982178
I0306 06:13:14.582123 140334138853120 logging_writer.py:48] [68700] global_step=68700, grad_norm=0.37954163551330566, loss=2.8051645755767822
I0306 06:13:49.525775 140334130460416 logging_writer.py:48] [68800] global_step=68800, grad_norm=0.3786255121231079, loss=2.831732988357544
I0306 06:14:24.467665 140334138853120 logging_writer.py:48] [68900] global_step=68900, grad_norm=0.3190617859363556, loss=2.7731614112854004
I0306 06:14:59.405404 140334130460416 logging_writer.py:48] [69000] global_step=69000, grad_norm=0.36694496870040894, loss=2.924084186553955
I0306 06:15:34.335483 140334138853120 logging_writer.py:48] [69100] global_step=69100, grad_norm=0.37494203448295593, loss=2.843299150466919
I0306 06:16:09.270467 140334130460416 logging_writer.py:48] [69200] global_step=69200, grad_norm=0.35801422595977783, loss=2.791597366333008
I0306 06:16:44.247980 140334138853120 logging_writer.py:48] [69300] global_step=69300, grad_norm=0.36534368991851807, loss=2.8159706592559814
I0306 06:17:19.193128 140334130460416 logging_writer.py:48] [69400] global_step=69400, grad_norm=0.35087519884109497, loss=2.775158405303955
I0306 06:17:54.156174 140334138853120 logging_writer.py:48] [69500] global_step=69500, grad_norm=0.33657094836235046, loss=2.729401111602783
I0306 06:18:29.106261 140334130460416 logging_writer.py:48] [69600] global_step=69600, grad_norm=0.3882392346858978, loss=2.814026355743408
I0306 06:19:00.918469 140477253723328 spec.py:321] Evaluating on the training split.
I0306 06:19:03.570449 140477253723328 workload.py:181] Translating evaluation dataset.
I0306 06:22:29.499823 140477253723328 spec.py:333] Evaluating on the validation split.
I0306 06:22:32.139775 140477253723328 workload.py:181] Translating evaluation dataset.
I0306 06:25:12.531463 140477253723328 spec.py:349] Evaluating on the test split.
I0306 06:25:15.172045 140477253723328 workload.py:181] Translating evaluation dataset.
I0306 06:28:14.436587 140477253723328 submission_runner.py:469] Time since start: 40484.66s, 	Step: 69692, 	{'train/accuracy': 0.6704314947128296, 'train/loss': 1.6495107412338257, 'train/bleu': 33.30320291965666, 'validation/accuracy': 0.6758831143379211, 'validation/loss': 1.6042044162750244, 'validation/bleu': 29.661690041680654, 'validation/num_examples': 3000, 'test/accuracy': 0.6888541579246521, 'test/loss': 1.532083511352539, 'test/bleu': 28.95712420225892, 'test/num_examples': 3003, 'score': 24386.444160938263, 'total_duration': 40484.66174077988, 'accumulated_submission_time': 24386.444160938263, 'accumulated_eval_time': 16093.508858680725, 'accumulated_logging_time': 0.6154155731201172}
I0306 06:28:14.453892 140334138853120 logging_writer.py:48] [69692] accumulated_eval_time=16093.5, accumulated_logging_time=0.615416, accumulated_submission_time=24386.4, global_step=69692, preemption_count=0, score=24386.4, test/accuracy=0.688854, test/bleu=28.9571, test/loss=1.53208, test/num_examples=3003, total_duration=40484.7, train/accuracy=0.670431, train/bleu=33.3032, train/loss=1.64951, validation/accuracy=0.675883, validation/bleu=29.6617, validation/loss=1.6042, validation/num_examples=3000
I0306 06:28:17.620960 140334130460416 logging_writer.py:48] [69700] global_step=69700, grad_norm=0.4479185938835144, loss=2.818873167037964
I0306 06:28:52.527756 140334138853120 logging_writer.py:48] [69800] global_step=69800, grad_norm=0.3742491602897644, loss=2.785841464996338
I0306 06:29:27.438393 140334130460416 logging_writer.py:48] [69900] global_step=69900, grad_norm=0.34249281883239746, loss=2.828955888748169
I0306 06:30:02.386136 140334138853120 logging_writer.py:48] [70000] global_step=70000, grad_norm=0.3903414309024811, loss=2.8290693759918213
I0306 06:30:37.312810 140334130460416 logging_writer.py:48] [70100] global_step=70100, grad_norm=0.3440345823764801, loss=2.826915979385376
I0306 06:31:12.251513 140334138853120 logging_writer.py:48] [70200] global_step=70200, grad_norm=0.4194919466972351, loss=2.83103609085083
I0306 06:31:47.207332 140334130460416 logging_writer.py:48] [70300] global_step=70300, grad_norm=0.37133878469467163, loss=2.888397216796875
I0306 06:32:22.157523 140334138853120 logging_writer.py:48] [70400] global_step=70400, grad_norm=0.36834532022476196, loss=2.8286125659942627
I0306 06:32:57.122138 140334130460416 logging_writer.py:48] [70500] global_step=70500, grad_norm=0.3678809106349945, loss=2.905022621154785
I0306 06:33:32.068143 140334138853120 logging_writer.py:48] [70600] global_step=70600, grad_norm=0.364396333694458, loss=2.832866907119751
I0306 06:34:07.041518 140334130460416 logging_writer.py:48] [70700] global_step=70700, grad_norm=0.37452536821365356, loss=2.8641912937164307
I0306 06:34:41.970052 140334138853120 logging_writer.py:48] [70800] global_step=70800, grad_norm=0.351656436920166, loss=2.7950186729431152
I0306 06:35:16.926628 140334130460416 logging_writer.py:48] [70900] global_step=70900, grad_norm=0.390171080827713, loss=2.825998306274414
I0306 06:35:51.896312 140334138853120 logging_writer.py:48] [71000] global_step=71000, grad_norm=0.3837049901485443, loss=2.8066048622131348
I0306 06:36:26.830651 140334130460416 logging_writer.py:48] [71100] global_step=71100, grad_norm=0.34628042578697205, loss=2.791468381881714
I0306 06:37:01.806435 140334138853120 logging_writer.py:48] [71200] global_step=71200, grad_norm=0.37856388092041016, loss=2.8405556678771973
I0306 06:37:36.760515 140334130460416 logging_writer.py:48] [71300] global_step=71300, grad_norm=0.380912721157074, loss=2.8050522804260254
I0306 06:38:11.738604 140334138853120 logging_writer.py:48] [71400] global_step=71400, grad_norm=0.3743717670440674, loss=2.814474105834961
I0306 06:38:46.726622 140334130460416 logging_writer.py:48] [71500] global_step=71500, grad_norm=0.4179112911224365, loss=2.698263168334961
I0306 06:39:21.671898 140334138853120 logging_writer.py:48] [71600] global_step=71600, grad_norm=0.3582853376865387, loss=2.8053808212280273
I0306 06:39:56.624154 140334130460416 logging_writer.py:48] [71700] global_step=71700, grad_norm=0.3600130081176758, loss=2.8193743228912354
I0306 06:40:31.597273 140334138853120 logging_writer.py:48] [71800] global_step=71800, grad_norm=0.3577583134174347, loss=2.818031072616577
I0306 06:41:06.597994 140334130460416 logging_writer.py:48] [71900] global_step=71900, grad_norm=0.3700748383998871, loss=2.8208773136138916
I0306 06:41:41.556982 140334138853120 logging_writer.py:48] [72000] global_step=72000, grad_norm=0.38005322217941284, loss=2.8547110557556152
I0306 06:42:14.765595 140477253723328 spec.py:321] Evaluating on the training split.
I0306 06:42:17.411673 140477253723328 workload.py:181] Translating evaluation dataset.
I0306 06:45:12.068979 140477253723328 spec.py:333] Evaluating on the validation split.
I0306 06:45:14.703896 140477253723328 workload.py:181] Translating evaluation dataset.
I0306 06:48:02.796533 140477253723328 spec.py:349] Evaluating on the test split.
I0306 06:48:05.438883 140477253723328 workload.py:181] Translating evaluation dataset.
I0306 06:50:38.453661 140477253723328 submission_runner.py:469] Time since start: 41828.68s, 	Step: 72096, 	{'train/accuracy': 0.6583915948867798, 'train/loss': 1.7199987173080444, 'train/bleu': 32.7337992172957, 'validation/accuracy': 0.6770697236061096, 'validation/loss': 1.5976604223251343, 'validation/bleu': 29.66415126409923, 'validation/num_examples': 3000, 'test/accuracy': 0.691067099571228, 'test/loss': 1.5227396488189697, 'test/bleu': 29.729226032439612, 'test/num_examples': 3003, 'score': 25226.611978530884, 'total_duration': 41828.67882490158, 'accumulated_submission_time': 25226.611978530884, 'accumulated_eval_time': 16597.19686627388, 'accumulated_logging_time': 0.6409029960632324}
I0306 06:50:38.470381 140334130460416 logging_writer.py:48] [72096] accumulated_eval_time=16597.2, accumulated_logging_time=0.640903, accumulated_submission_time=25226.6, global_step=72096, preemption_count=0, score=25226.6, test/accuracy=0.691067, test/bleu=29.7292, test/loss=1.52274, test/num_examples=3003, total_duration=41828.7, train/accuracy=0.658392, train/bleu=32.7338, train/loss=1.72, validation/accuracy=0.67707, validation/bleu=29.6642, validation/loss=1.59766, validation/num_examples=3000
I0306 06:50:40.310153 140334138853120 logging_writer.py:48] [72100] global_step=72100, grad_norm=0.4120641052722931, loss=2.811877727508545
I0306 06:51:15.342419 140334130460416 logging_writer.py:48] [72200] global_step=72200, grad_norm=0.3758724331855774, loss=2.7770626544952393
I0306 06:51:50.399900 140334138853120 logging_writer.py:48] [72300] global_step=72300, grad_norm=0.3499864637851715, loss=2.7205898761749268
I0306 06:52:25.424178 140334130460416 logging_writer.py:48] [72400] global_step=72400, grad_norm=0.3825523853302002, loss=2.8599274158477783
I0306 06:53:00.503880 140334138853120 logging_writer.py:48] [72500] global_step=72500, grad_norm=0.3736910820007324, loss=2.835341215133667
I0306 06:53:35.563645 140334130460416 logging_writer.py:48] [72600] global_step=72600, grad_norm=0.39273303747177124, loss=2.8326327800750732
I0306 06:54:10.649810 140334138853120 logging_writer.py:48] [72700] global_step=72700, grad_norm=0.37263381481170654, loss=2.8339591026306152
I0306 06:54:45.750975 140334130460416 logging_writer.py:48] [72800] global_step=72800, grad_norm=0.36481723189353943, loss=2.81992769241333
I0306 06:55:20.850320 140334138853120 logging_writer.py:48] [72900] global_step=72900, grad_norm=0.36840710043907166, loss=2.796776294708252
I0306 06:55:55.977539 140334130460416 logging_writer.py:48] [73000] global_step=73000, grad_norm=0.371918261051178, loss=2.830200433731079
I0306 06:56:31.071718 140334138853120 logging_writer.py:48] [73100] global_step=73100, grad_norm=0.3774711489677429, loss=2.772157907485962
I0306 06:57:06.164275 140334130460416 logging_writer.py:48] [73200] global_step=73200, grad_norm=0.3836982250213623, loss=2.8802411556243896
I0306 06:57:41.276364 140334138853120 logging_writer.py:48] [73300] global_step=73300, grad_norm=0.37121811509132385, loss=2.8606953620910645
I0306 06:58:16.351537 140334130460416 logging_writer.py:48] [73400] global_step=73400, grad_norm=0.38151806592941284, loss=2.837153434753418
I0306 06:58:51.439388 140334138853120 logging_writer.py:48] [73500] global_step=73500, grad_norm=0.38019511103630066, loss=2.8269431591033936
I0306 06:59:26.541717 140334130460416 logging_writer.py:48] [73600] global_step=73600, grad_norm=0.5141366720199585, loss=2.8515453338623047
I0306 07:00:01.646136 140334138853120 logging_writer.py:48] [73700] global_step=73700, grad_norm=0.37790820002555847, loss=2.8162591457366943
I0306 07:00:36.759854 140334130460416 logging_writer.py:48] [73800] global_step=73800, grad_norm=0.36538103222846985, loss=2.787398338317871
I0306 07:01:11.866837 140334138853120 logging_writer.py:48] [73900] global_step=73900, grad_norm=0.3895888328552246, loss=2.860316753387451
I0306 07:01:46.957761 140334130460416 logging_writer.py:48] [74000] global_step=74000, grad_norm=0.3881213366985321, loss=2.817723512649536
I0306 07:02:22.038662 140334138853120 logging_writer.py:48] [74100] global_step=74100, grad_norm=0.38758960366249084, loss=2.846895217895508
I0306 07:02:57.118788 140334130460416 logging_writer.py:48] [74200] global_step=74200, grad_norm=0.38165348768234253, loss=2.767025947570801
I0306 07:03:32.212860 140334138853120 logging_writer.py:48] [74300] global_step=74300, grad_norm=0.385102778673172, loss=2.771070957183838
I0306 07:04:07.302433 140334130460416 logging_writer.py:48] [74400] global_step=74400, grad_norm=0.3638642430305481, loss=2.735544204711914
I0306 07:04:38.551891 140477253723328 spec.py:321] Evaluating on the training split.
I0306 07:04:41.212730 140477253723328 workload.py:181] Translating evaluation dataset.
I0306 07:07:44.276774 140477253723328 spec.py:333] Evaluating on the validation split.
I0306 07:07:46.921669 140477253723328 workload.py:181] Translating evaluation dataset.
I0306 07:10:25.974842 140477253723328 spec.py:349] Evaluating on the test split.
I0306 07:10:28.632600 140477253723328 workload.py:181] Translating evaluation dataset.
I0306 07:12:56.418093 140477253723328 submission_runner.py:469] Time since start: 43166.64s, 	Step: 74490, 	{'train/accuracy': 0.6592912673950195, 'train/loss': 1.7244094610214233, 'train/bleu': 32.69329684068829, 'validation/accuracy': 0.6790226101875305, 'validation/loss': 1.584033489227295, 'validation/bleu': 29.671339006770474, 'validation/num_examples': 3000, 'test/accuracy': 0.6929324865341187, 'test/loss': 1.5079373121261597, 'test/bleu': 29.323018711417834, 'test/num_examples': 3003, 'score': 26066.474137544632, 'total_duration': 43166.64326453209, 'accumulated_submission_time': 26066.474137544632, 'accumulated_eval_time': 17095.063010931015, 'accumulated_logging_time': 0.7395486831665039}
I0306 07:12:56.435872 140334138853120 logging_writer.py:48] [74490] accumulated_eval_time=17095.1, accumulated_logging_time=0.739549, accumulated_submission_time=26066.5, global_step=74490, preemption_count=0, score=26066.5, test/accuracy=0.692932, test/bleu=29.323, test/loss=1.50794, test/num_examples=3003, total_duration=43166.6, train/accuracy=0.659291, train/bleu=32.6933, train/loss=1.72441, validation/accuracy=0.679023, validation/bleu=29.6713, validation/loss=1.58403, validation/num_examples=3000
I0306 07:13:00.298650 140334130460416 logging_writer.py:48] [74500] global_step=74500, grad_norm=0.3785467743873596, loss=2.7831099033355713
I0306 07:13:35.310162 140334138853120 logging_writer.py:48] [74600] global_step=74600, grad_norm=0.4239729344844818, loss=2.850205421447754
I0306 07:14:10.353651 140334130460416 logging_writer.py:48] [74700] global_step=74700, grad_norm=0.3665946424007416, loss=2.7695670127868652
I0306 07:14:45.405055 140334138853120 logging_writer.py:48] [74800] global_step=74800, grad_norm=0.36355453729629517, loss=2.7714667320251465
I0306 07:15:20.468088 140334130460416 logging_writer.py:48] [74900] global_step=74900, grad_norm=0.372121125459671, loss=2.8149945735931396
I0306 07:15:55.544510 140334138853120 logging_writer.py:48] [75000] global_step=75000, grad_norm=0.3584408760070801, loss=2.7914257049560547
I0306 07:16:30.614357 140334130460416 logging_writer.py:48] [75100] global_step=75100, grad_norm=0.4087814390659332, loss=2.8050265312194824
I0306 07:17:05.666270 140334138853120 logging_writer.py:48] [75200] global_step=75200, grad_norm=0.3949737548828125, loss=2.7930502891540527
I0306 07:17:40.693231 140334130460416 logging_writer.py:48] [75300] global_step=75300, grad_norm=0.37585321068763733, loss=2.872854471206665
I0306 07:18:15.619767 140334138853120 logging_writer.py:48] [75400] global_step=75400, grad_norm=0.3850357234477997, loss=2.7290055751800537
I0306 07:18:50.548959 140334130460416 logging_writer.py:48] [75500] global_step=75500, grad_norm=0.3837866485118866, loss=2.7841358184814453
I0306 07:19:25.518054 140334138853120 logging_writer.py:48] [75600] global_step=75600, grad_norm=0.40283671021461487, loss=2.8344757556915283
I0306 07:20:00.454353 140334130460416 logging_writer.py:48] [75700] global_step=75700, grad_norm=0.38485291600227356, loss=2.8976192474365234
I0306 07:20:35.425267 140334138853120 logging_writer.py:48] [75800] global_step=75800, grad_norm=0.3767188787460327, loss=2.813328742980957
I0306 07:21:10.401435 140334130460416 logging_writer.py:48] [75900] global_step=75900, grad_norm=0.3992725610733032, loss=2.8213746547698975
I0306 07:21:45.362878 140334138853120 logging_writer.py:48] [76000] global_step=76000, grad_norm=0.37505632638931274, loss=2.7994112968444824
I0306 07:22:20.312498 140334130460416 logging_writer.py:48] [76100] global_step=76100, grad_norm=0.37960293889045715, loss=2.868929386138916
I0306 07:22:55.258707 140334138853120 logging_writer.py:48] [76200] global_step=76200, grad_norm=0.38063737750053406, loss=2.8231632709503174
I0306 07:23:30.185844 140334130460416 logging_writer.py:48] [76300] global_step=76300, grad_norm=0.3943314254283905, loss=2.7627017498016357
I0306 07:24:05.130585 140334138853120 logging_writer.py:48] [76400] global_step=76400, grad_norm=0.40913718938827515, loss=2.857306957244873
I0306 07:24:40.076786 140334130460416 logging_writer.py:48] [76500] global_step=76500, grad_norm=0.38365548849105835, loss=2.8316895961761475
I0306 07:25:15.031666 140334138853120 logging_writer.py:48] [76600] global_step=76600, grad_norm=0.38216719031333923, loss=2.8054723739624023
I0306 07:25:50.001403 140334130460416 logging_writer.py:48] [76700] global_step=76700, grad_norm=0.37620577216148376, loss=2.8457322120666504
I0306 07:26:24.947839 140334138853120 logging_writer.py:48] [76800] global_step=76800, grad_norm=0.3916913568973541, loss=2.860687494277954
I0306 07:26:56.742832 140477253723328 spec.py:321] Evaluating on the training split.
I0306 07:26:59.400799 140477253723328 workload.py:181] Translating evaluation dataset.
I0306 07:30:00.796938 140477253723328 spec.py:333] Evaluating on the validation split.
I0306 07:30:03.448370 140477253723328 workload.py:181] Translating evaluation dataset.
I0306 07:32:35.695578 140477253723328 spec.py:349] Evaluating on the test split.
I0306 07:32:38.345831 140477253723328 workload.py:181] Translating evaluation dataset.
I0306 07:35:07.504756 140477253723328 submission_runner.py:469] Time since start: 44497.73s, 	Step: 76892, 	{'train/accuracy': 0.668889045715332, 'train/loss': 1.66038978099823, 'train/bleu': 33.3143746828012, 'validation/accuracy': 0.6800732016563416, 'validation/loss': 1.579336166381836, 'validation/bleu': 29.733642603124878, 'validation/num_examples': 3000, 'test/accuracy': 0.6951801776885986, 'test/loss': 1.4990752935409546, 'test/bleu': 29.56411755480837, 'test/num_examples': 3003, 'score': 26906.63858628273, 'total_duration': 44497.72993302345, 'accumulated_submission_time': 26906.63858628273, 'accumulated_eval_time': 17585.82488155365, 'accumulated_logging_time': 0.7651686668395996}
I0306 07:35:07.522450 140334130460416 logging_writer.py:48] [76892] accumulated_eval_time=17585.8, accumulated_logging_time=0.765169, accumulated_submission_time=26906.6, global_step=76892, preemption_count=0, score=26906.6, test/accuracy=0.69518, test/bleu=29.5641, test/loss=1.49908, test/num_examples=3003, total_duration=44497.7, train/accuracy=0.668889, train/bleu=33.3144, train/loss=1.66039, validation/accuracy=0.680073, validation/bleu=29.7336, validation/loss=1.57934, validation/num_examples=3000
I0306 07:35:10.667792 140334138853120 logging_writer.py:48] [76900] global_step=76900, grad_norm=0.3990660309791565, loss=2.8659520149230957
I0306 07:35:45.611696 140334130460416 logging_writer.py:48] [77000] global_step=77000, grad_norm=0.37733566761016846, loss=2.7662627696990967
I0306 07:36:20.522622 140334138853120 logging_writer.py:48] [77100] global_step=77100, grad_norm=0.3892543613910675, loss=2.8412206172943115
I0306 07:36:55.439079 140334130460416 logging_writer.py:48] [77200] global_step=77200, grad_norm=0.36335381865501404, loss=2.8257133960723877
I0306 07:37:30.375480 140334138853120 logging_writer.py:48] [77300] global_step=77300, grad_norm=0.397041916847229, loss=2.8231632709503174
I0306 07:38:05.317485 140334130460416 logging_writer.py:48] [77400] global_step=77400, grad_norm=0.37282490730285645, loss=2.809558629989624
I0306 07:38:40.254967 140334138853120 logging_writer.py:48] [77500] global_step=77500, grad_norm=0.37880659103393555, loss=2.768810272216797
I0306 07:39:15.205478 140334130460416 logging_writer.py:48] [77600] global_step=77600, grad_norm=0.42690759897232056, loss=2.803981304168701
I0306 07:39:50.142946 140334138853120 logging_writer.py:48] [77700] global_step=77700, grad_norm=0.39564672112464905, loss=2.7734439373016357
I0306 07:40:25.082805 140334130460416 logging_writer.py:48] [77800] global_step=77800, grad_norm=0.4033174216747284, loss=2.7382280826568604
I0306 07:41:00.011960 140334138853120 logging_writer.py:48] [77900] global_step=77900, grad_norm=0.40805211663246155, loss=2.729104995727539
I0306 07:41:34.954021 140334130460416 logging_writer.py:48] [78000] global_step=78000, grad_norm=0.3447595238685608, loss=2.807878255844116
I0306 07:42:09.914881 140334138853120 logging_writer.py:48] [78100] global_step=78100, grad_norm=0.3856070041656494, loss=2.8158655166625977
I0306 07:42:44.857613 140334130460416 logging_writer.py:48] [78200] global_step=78200, grad_norm=0.39353644847869873, loss=2.876099109649658
I0306 07:43:19.779967 140334138853120 logging_writer.py:48] [78300] global_step=78300, grad_norm=0.3957187831401825, loss=2.8510632514953613
I0306 07:43:54.710102 140334130460416 logging_writer.py:48] [78400] global_step=78400, grad_norm=0.4300156533718109, loss=2.800858497619629
I0306 07:44:29.678766 140334138853120 logging_writer.py:48] [78500] global_step=78500, grad_norm=0.4568440914154053, loss=2.73516845703125
I0306 07:45:04.626595 140334130460416 logging_writer.py:48] [78600] global_step=78600, grad_norm=0.393818736076355, loss=2.7903811931610107
I0306 07:45:39.578142 140334138853120 logging_writer.py:48] [78700] global_step=78700, grad_norm=0.39914584159851074, loss=2.773585796356201
I0306 07:46:14.548901 140334130460416 logging_writer.py:48] [78800] global_step=78800, grad_norm=0.39925289154052734, loss=2.735790967941284
I0306 07:46:49.495265 140334138853120 logging_writer.py:48] [78900] global_step=78900, grad_norm=0.3796353340148926, loss=2.7791247367858887
I0306 07:47:24.471861 140334130460416 logging_writer.py:48] [79000] global_step=79000, grad_norm=0.37100592255592346, loss=2.7943003177642822
I0306 07:47:59.462733 140334138853120 logging_writer.py:48] [79100] global_step=79100, grad_norm=0.38145801424980164, loss=2.8159501552581787
I0306 07:48:34.445995 140334130460416 logging_writer.py:48] [79200] global_step=79200, grad_norm=0.4003576636314392, loss=2.786792755126953
I0306 07:49:07.643749 140477253723328 spec.py:321] Evaluating on the training split.
I0306 07:49:10.296969 140477253723328 workload.py:181] Translating evaluation dataset.
I0306 07:52:10.280082 140477253723328 spec.py:333] Evaluating on the validation split.
I0306 07:52:12.923655 140477253723328 workload.py:181] Translating evaluation dataset.
I0306 07:54:51.469092 140477253723328 spec.py:349] Evaluating on the test split.
I0306 07:54:54.127430 140477253723328 workload.py:181] Translating evaluation dataset.
I0306 07:57:23.026145 140477253723328 submission_runner.py:469] Time since start: 45833.25s, 	Step: 79296, 	{'train/accuracy': 0.6662702560424805, 'train/loss': 1.6860483884811401, 'train/bleu': 32.861731673712285, 'validation/accuracy': 0.6805304884910583, 'validation/loss': 1.574438214302063, 'validation/bleu': 29.779761244354397, 'validation/num_examples': 3000, 'test/accuracy': 0.6960722804069519, 'test/loss': 1.4949110746383667, 'test/bleu': 29.609990715056238, 'test/num_examples': 3003, 'score': 27746.616654872894, 'total_duration': 45833.251322984695, 'accumulated_submission_time': 27746.616654872894, 'accumulated_eval_time': 18081.20722436905, 'accumulated_logging_time': 0.7908661365509033}
I0306 07:57:23.044392 140334138853120 logging_writer.py:48] [79296] accumulated_eval_time=18081.2, accumulated_logging_time=0.790866, accumulated_submission_time=27746.6, global_step=79296, preemption_count=0, score=27746.6, test/accuracy=0.696072, test/bleu=29.61, test/loss=1.49491, test/num_examples=3003, total_duration=45833.3, train/accuracy=0.66627, train/bleu=32.8617, train/loss=1.68605, validation/accuracy=0.68053, validation/bleu=29.7798, validation/loss=1.57444, validation/num_examples=3000
I0306 07:57:24.799664 140334130460416 logging_writer.py:48] [79300] global_step=79300, grad_norm=0.409475713968277, loss=2.77622127532959
I0306 07:57:59.686780 140334138853120 logging_writer.py:48] [79400] global_step=79400, grad_norm=0.4003306031227112, loss=2.778231620788574
I0306 07:58:34.605567 140334130460416 logging_writer.py:48] [79500] global_step=79500, grad_norm=0.39486944675445557, loss=2.7951250076293945
I0306 07:59:09.537069 140334138853120 logging_writer.py:48] [79600] global_step=79600, grad_norm=0.3878572881221771, loss=2.861168384552002
I0306 07:59:44.517676 140334130460416 logging_writer.py:48] [79700] global_step=79700, grad_norm=0.39652687311172485, loss=2.7430102825164795
I0306 08:00:19.479736 140334138853120 logging_writer.py:48] [79800] global_step=79800, grad_norm=0.39523571729660034, loss=2.7754271030426025
I0306 08:00:54.469799 140334130460416 logging_writer.py:48] [79900] global_step=79900, grad_norm=0.40197908878326416, loss=2.7701416015625
I0306 08:01:29.429462 140334138853120 logging_writer.py:48] [80000] global_step=80000, grad_norm=0.3873225152492523, loss=2.7658274173736572
I0306 08:02:04.399650 140334130460416 logging_writer.py:48] [80100] global_step=80100, grad_norm=0.4010103642940521, loss=2.755866765975952
I0306 08:02:39.361273 140334138853120 logging_writer.py:48] [80200] global_step=80200, grad_norm=0.408426433801651, loss=2.882174015045166
I0306 08:03:14.339834 140334130460416 logging_writer.py:48] [80300] global_step=80300, grad_norm=0.4201664924621582, loss=2.7910983562469482
I0306 08:03:49.325455 140334138853120 logging_writer.py:48] [80400] global_step=80400, grad_norm=0.40469539165496826, loss=2.7730801105499268
I0306 08:04:24.286008 140334130460416 logging_writer.py:48] [80500] global_step=80500, grad_norm=0.40429210662841797, loss=2.8457469940185547
I0306 08:04:59.272699 140334138853120 logging_writer.py:48] [80600] global_step=80600, grad_norm=0.3578316569328308, loss=2.765303373336792
I0306 08:05:34.247807 140334130460416 logging_writer.py:48] [80700] global_step=80700, grad_norm=0.39282649755477905, loss=2.7624828815460205
I0306 08:06:09.211483 140334138853120 logging_writer.py:48] [80800] global_step=80800, grad_norm=0.41056591272354126, loss=2.804816961288452
I0306 08:06:44.183338 140334130460416 logging_writer.py:48] [80900] global_step=80900, grad_norm=0.3934912085533142, loss=2.7540740966796875
I0306 08:07:19.170934 140334138853120 logging_writer.py:48] [81000] global_step=81000, grad_norm=0.41350454092025757, loss=2.8092517852783203
I0306 08:07:54.116653 140334130460416 logging_writer.py:48] [81100] global_step=81100, grad_norm=0.42740365862846375, loss=2.7752199172973633
I0306 08:08:29.068987 140334138853120 logging_writer.py:48] [81200] global_step=81200, grad_norm=0.4077424705028534, loss=2.7988440990448
I0306 08:09:04.077537 140334130460416 logging_writer.py:48] [81300] global_step=81300, grad_norm=0.40583470463752747, loss=2.761399984359741
I0306 08:09:39.050195 140334138853120 logging_writer.py:48] [81400] global_step=81400, grad_norm=0.40599262714385986, loss=2.8369457721710205
I0306 08:10:14.032778 140334130460416 logging_writer.py:48] [81500] global_step=81500, grad_norm=0.42240408062934875, loss=2.8223085403442383
I0306 08:10:49.081273 140334138853120 logging_writer.py:48] [81600] global_step=81600, grad_norm=0.42036259174346924, loss=2.8008172512054443
I0306 08:11:23.105806 140477253723328 spec.py:321] Evaluating on the training split.
I0306 08:11:25.767985 140477253723328 workload.py:181] Translating evaluation dataset.
I0306 08:14:25.825450 140477253723328 spec.py:333] Evaluating on the validation split.
I0306 08:14:28.474132 140477253723328 workload.py:181] Translating evaluation dataset.
I0306 08:17:11.063685 140477253723328 spec.py:349] Evaluating on the test split.
I0306 08:17:13.714621 140477253723328 workload.py:181] Translating evaluation dataset.
I0306 08:19:58.123600 140477253723328 submission_runner.py:469] Time since start: 47188.35s, 	Step: 81698, 	{'train/accuracy': 0.6977536678314209, 'train/loss': 1.502886176109314, 'train/bleu': 34.92097169970208, 'validation/accuracy': 0.6824215650558472, 'validation/loss': 1.563807725906372, 'validation/bleu': 29.71707111037867, 'validation/num_examples': 3000, 'test/accuracy': 0.697404682636261, 'test/loss': 1.4829705953598022, 'test/bleu': 30.0111260470247, 'test/num_examples': 3003, 'score': 28586.533041715622, 'total_duration': 47188.34877991676, 'accumulated_submission_time': 28586.533041715622, 'accumulated_eval_time': 18596.224978923798, 'accumulated_logging_time': 0.8183484077453613}
I0306 08:19:58.141992 140334130460416 logging_writer.py:48] [81698] accumulated_eval_time=18596.2, accumulated_logging_time=0.818348, accumulated_submission_time=28586.5, global_step=81698, preemption_count=0, score=28586.5, test/accuracy=0.697405, test/bleu=30.0111, test/loss=1.48297, test/num_examples=3003, total_duration=47188.3, train/accuracy=0.697754, train/bleu=34.921, train/loss=1.50289, validation/accuracy=0.682422, validation/bleu=29.7171, validation/loss=1.56381, validation/num_examples=3000
I0306 08:19:59.199402 140334138853120 logging_writer.py:48] [81700] global_step=81700, grad_norm=0.3910718858242035, loss=2.761820077896118
I0306 08:20:34.175193 140334130460416 logging_writer.py:48] [81800] global_step=81800, grad_norm=0.40065085887908936, loss=2.768526077270508
I0306 08:21:09.206571 140334138853120 logging_writer.py:48] [81900] global_step=81900, grad_norm=0.3960273265838623, loss=2.7628402709960938
I0306 08:21:44.236436 140334130460416 logging_writer.py:48] [82000] global_step=82000, grad_norm=0.4196043312549591, loss=2.802070140838623
I0306 08:22:19.301581 140334138853120 logging_writer.py:48] [82100] global_step=82100, grad_norm=0.39903467893600464, loss=2.7316064834594727
I0306 08:22:54.366994 140334130460416 logging_writer.py:48] [82200] global_step=82200, grad_norm=0.38798201084136963, loss=2.727473497390747
I0306 08:23:29.409006 140334138853120 logging_writer.py:48] [82300] global_step=82300, grad_norm=0.401917427778244, loss=2.8265950679779053
I0306 08:24:04.492761 140334130460416 logging_writer.py:48] [82400] global_step=82400, grad_norm=0.40801161527633667, loss=2.831181287765503
I0306 08:24:39.563760 140334138853120 logging_writer.py:48] [82500] global_step=82500, grad_norm=0.38949957489967346, loss=2.739511489868164
I0306 08:25:14.609875 140334130460416 logging_writer.py:48] [82600] global_step=82600, grad_norm=0.41680094599723816, loss=2.7643449306488037
I0306 08:25:49.651745 140334138853120 logging_writer.py:48] [82700] global_step=82700, grad_norm=0.39444518089294434, loss=2.7282891273498535
I0306 08:26:24.742660 140334130460416 logging_writer.py:48] [82800] global_step=82800, grad_norm=0.45029351115226746, loss=2.790134906768799
I0306 08:26:59.859849 140334138853120 logging_writer.py:48] [82900] global_step=82900, grad_norm=0.41819289326667786, loss=2.767954111099243
I0306 08:27:34.983428 140334130460416 logging_writer.py:48] [83000] global_step=83000, grad_norm=0.417819619178772, loss=2.778618335723877
I0306 08:28:10.073461 140334138853120 logging_writer.py:48] [83100] global_step=83100, grad_norm=0.49383941292762756, loss=2.7607221603393555
I0306 08:28:45.183757 140334130460416 logging_writer.py:48] [83200] global_step=83200, grad_norm=0.4140181541442871, loss=2.7657105922698975
I0306 08:29:20.251047 140334138853120 logging_writer.py:48] [83300] global_step=83300, grad_norm=0.4105740785598755, loss=2.7594079971313477
I0306 08:29:55.316039 140334130460416 logging_writer.py:48] [83400] global_step=83400, grad_norm=0.41776135563850403, loss=2.8395402431488037
I0306 08:30:30.387801 140334138853120 logging_writer.py:48] [83500] global_step=83500, grad_norm=0.4428580403327942, loss=2.8085947036743164
I0306 08:31:05.448540 140334130460416 logging_writer.py:48] [83600] global_step=83600, grad_norm=0.41961610317230225, loss=2.7977044582366943
I0306 08:31:40.500640 140334138853120 logging_writer.py:48] [83700] global_step=83700, grad_norm=0.3994782567024231, loss=2.820423126220703
I0306 08:32:15.602527 140334130460416 logging_writer.py:48] [83800] global_step=83800, grad_norm=0.47664666175842285, loss=2.8109805583953857
I0306 08:32:50.702838 140334138853120 logging_writer.py:48] [83900] global_step=83900, grad_norm=0.413882315158844, loss=2.7646985054016113
I0306 08:33:25.787257 140334130460416 logging_writer.py:48] [84000] global_step=84000, grad_norm=0.40121495723724365, loss=2.6879119873046875
I0306 08:33:58.379054 140477253723328 spec.py:321] Evaluating on the training split.
I0306 08:34:01.035684 140477253723328 workload.py:181] Translating evaluation dataset.
I0306 08:37:13.224577 140477253723328 spec.py:333] Evaluating on the validation split.
I0306 08:37:15.872111 140477253723328 workload.py:181] Translating evaluation dataset.
I0306 08:39:49.956470 140477253723328 spec.py:349] Evaluating on the test split.
I0306 08:39:52.607812 140477253723328 workload.py:181] Translating evaluation dataset.
I0306 08:42:25.384756 140477253723328 submission_runner.py:469] Time since start: 48535.61s, 	Step: 84094, 	{'train/accuracy': 0.67063307762146, 'train/loss': 1.6480712890625, 'train/bleu': 33.50119627012239, 'validation/accuracy': 0.6833485960960388, 'validation/loss': 1.5514763593673706, 'validation/bleu': 30.033434522788504, 'validation/num_examples': 3000, 'test/accuracy': 0.7003591656684875, 'test/loss': 1.466897964477539, 'test/bleu': 29.899622824104352, 'test/num_examples': 3003, 'score': 29426.624901533127, 'total_duration': 48535.609925985336, 'accumulated_submission_time': 29426.624901533127, 'accumulated_eval_time': 19103.230615615845, 'accumulated_logging_time': 0.8450629711151123}
I0306 08:42:25.404347 140334138853120 logging_writer.py:48] [84094] accumulated_eval_time=19103.2, accumulated_logging_time=0.845063, accumulated_submission_time=29426.6, global_step=84094, preemption_count=0, score=29426.6, test/accuracy=0.700359, test/bleu=29.8996, test/loss=1.4669, test/num_examples=3003, total_duration=48535.6, train/accuracy=0.670633, train/bleu=33.5012, train/loss=1.64807, validation/accuracy=0.683349, validation/bleu=30.0334, validation/loss=1.55148, validation/num_examples=3000
I0306 08:42:27.853956 140334130460416 logging_writer.py:48] [84100] global_step=84100, grad_norm=0.43844667077064514, loss=2.715233325958252
I0306 08:43:02.905601 140334138853120 logging_writer.py:48] [84200] global_step=84200, grad_norm=0.4906027615070343, loss=2.7674949169158936
I0306 08:43:37.955625 140334130460416 logging_writer.py:48] [84300] global_step=84300, grad_norm=0.40467020869255066, loss=2.772430896759033
I0306 08:44:13.017386 140334138853120 logging_writer.py:48] [84400] global_step=84400, grad_norm=0.40511003136634827, loss=2.712104558944702
I0306 08:44:48.064930 140334130460416 logging_writer.py:48] [84500] global_step=84500, grad_norm=0.4212462902069092, loss=2.7508301734924316
I0306 08:45:23.137495 140334138853120 logging_writer.py:48] [84600] global_step=84600, grad_norm=0.40633291006088257, loss=2.7051355838775635
I0306 08:45:58.221261 140334130460416 logging_writer.py:48] [84700] global_step=84700, grad_norm=0.4258631467819214, loss=2.7659530639648438
I0306 08:46:33.276257 140334138853120 logging_writer.py:48] [84800] global_step=84800, grad_norm=0.4402061402797699, loss=2.7832231521606445
I0306 08:47:08.339553 140334130460416 logging_writer.py:48] [84900] global_step=84900, grad_norm=0.42478904128074646, loss=2.759700059890747
I0306 08:47:43.439791 140334138853120 logging_writer.py:48] [85000] global_step=85000, grad_norm=0.43976515531539917, loss=2.751957416534424
I0306 08:48:18.518930 140334130460416 logging_writer.py:48] [85100] global_step=85100, grad_norm=0.4379219710826874, loss=2.85932993888855
I0306 08:48:53.632313 140334138853120 logging_writer.py:48] [85200] global_step=85200, grad_norm=0.4373028576374054, loss=2.6975088119506836
I0306 08:49:28.715310 140334130460416 logging_writer.py:48] [85300] global_step=85300, grad_norm=0.4218490719795227, loss=2.7742865085601807
I0306 08:50:03.770441 140334138853120 logging_writer.py:48] [85400] global_step=85400, grad_norm=0.42516475915908813, loss=2.743051528930664
I0306 08:50:38.792056 140334130460416 logging_writer.py:48] [85500] global_step=85500, grad_norm=0.4425342381000519, loss=2.7516238689422607
I0306 08:51:13.830816 140334138853120 logging_writer.py:48] [85600] global_step=85600, grad_norm=0.41307398676872253, loss=2.7639496326446533
I0306 08:51:48.869665 140334130460416 logging_writer.py:48] [85700] global_step=85700, grad_norm=0.4145970940589905, loss=2.7095816135406494
I0306 08:52:23.888817 140334138853120 logging_writer.py:48] [85800] global_step=85800, grad_norm=0.42944663763046265, loss=2.746262311935425
I0306 08:52:58.917320 140334130460416 logging_writer.py:48] [85900] global_step=85900, grad_norm=0.42971566319465637, loss=2.817178249359131
I0306 08:53:33.936420 140334138853120 logging_writer.py:48] [86000] global_step=86000, grad_norm=0.41810351610183716, loss=2.7511703968048096
I0306 08:54:08.955185 140334130460416 logging_writer.py:48] [86100] global_step=86100, grad_norm=0.4184688627719879, loss=2.7722725868225098
I0306 08:54:43.945497 140334138853120 logging_writer.py:48] [86200] global_step=86200, grad_norm=0.4295594394207001, loss=2.7666513919830322
I0306 08:55:18.923079 140334130460416 logging_writer.py:48] [86300] global_step=86300, grad_norm=0.42841899394989014, loss=2.8298580646514893
I0306 08:55:53.893510 140334138853120 logging_writer.py:48] [86400] global_step=86400, grad_norm=0.44479262828826904, loss=2.7483534812927246
I0306 08:56:25.698482 140477253723328 spec.py:321] Evaluating on the training split.
I0306 08:56:28.349075 140477253723328 workload.py:181] Translating evaluation dataset.
I0306 09:00:05.581974 140477253723328 spec.py:333] Evaluating on the validation split.
I0306 09:00:08.222254 140477253723328 workload.py:181] Translating evaluation dataset.
I0306 09:02:47.896588 140477253723328 spec.py:349] Evaluating on the test split.
I0306 09:02:50.548314 140477253723328 workload.py:181] Translating evaluation dataset.
I0306 09:05:21.878724 140477253723328 submission_runner.py:469] Time since start: 49912.10s, 	Step: 86492, 	{'train/accuracy': 0.6687280535697937, 'train/loss': 1.6682251691818237, 'train/bleu': 33.48890700580598, 'validation/accuracy': 0.6860430836677551, 'validation/loss': 1.547324776649475, 'validation/bleu': 30.45131823230902, 'validation/num_examples': 3000, 'test/accuracy': 0.6998725533485413, 'test/loss': 1.4708218574523926, 'test/bleu': 29.852259755973133, 'test/num_examples': 3003, 'score': 30266.771779060364, 'total_duration': 49912.10389328003, 'accumulated_submission_time': 30266.771779060364, 'accumulated_eval_time': 19639.410803318024, 'accumulated_logging_time': 0.8727223873138428}
I0306 09:05:21.897048 140334130460416 logging_writer.py:48] [86492] accumulated_eval_time=19639.4, accumulated_logging_time=0.872722, accumulated_submission_time=30266.8, global_step=86492, preemption_count=0, score=30266.8, test/accuracy=0.699873, test/bleu=29.8523, test/loss=1.47082, test/num_examples=3003, total_duration=49912.1, train/accuracy=0.668728, train/bleu=33.4889, train/loss=1.66823, validation/accuracy=0.686043, validation/bleu=30.4513, validation/loss=1.54732, validation/num_examples=3000
I0306 09:05:25.051200 140334138853120 logging_writer.py:48] [86500] global_step=86500, grad_norm=0.44761109352111816, loss=2.7716708183288574
I0306 09:05:59.961438 140334130460416 logging_writer.py:48] [86600] global_step=86600, grad_norm=0.43736183643341064, loss=2.732329845428467
I0306 09:06:34.874485 140334138853120 logging_writer.py:48] [86700] global_step=86700, grad_norm=0.43232959508895874, loss=2.710920572280884
I0306 09:07:09.826898 140334130460416 logging_writer.py:48] [86800] global_step=86800, grad_norm=0.4238654673099518, loss=2.715843677520752
I0306 09:07:44.769491 140334138853120 logging_writer.py:48] [86900] global_step=86900, grad_norm=0.4056236445903778, loss=2.710747480392456
I0306 09:08:19.730999 140334130460416 logging_writer.py:48] [87000] global_step=87000, grad_norm=0.4385708272457123, loss=2.754155158996582
I0306 09:08:54.716961 140334138853120 logging_writer.py:48] [87100] global_step=87100, grad_norm=0.4426630437374115, loss=2.78562068939209
I0306 09:09:29.689565 140334130460416 logging_writer.py:48] [87200] global_step=87200, grad_norm=0.43590977787971497, loss=2.7975516319274902
I0306 09:10:04.644311 140334138853120 logging_writer.py:48] [87300] global_step=87300, grad_norm=0.4405876100063324, loss=2.7322120666503906
I0306 09:10:39.588879 140334130460416 logging_writer.py:48] [87400] global_step=87400, grad_norm=0.43075472116470337, loss=2.7498950958251953
I0306 09:11:14.569788 140334138853120 logging_writer.py:48] [87500] global_step=87500, grad_norm=0.4414317011833191, loss=2.788156032562256
I0306 09:11:49.534963 140334130460416 logging_writer.py:48] [87600] global_step=87600, grad_norm=0.4521201550960541, loss=2.7760369777679443
I0306 09:12:24.488051 140334138853120 logging_writer.py:48] [87700] global_step=87700, grad_norm=0.4358276426792145, loss=2.748415231704712
I0306 09:12:59.446722 140334130460416 logging_writer.py:48] [87800] global_step=87800, grad_norm=0.43832430243492126, loss=2.698584794998169
I0306 09:13:34.407954 140334138853120 logging_writer.py:48] [87900] global_step=87900, grad_norm=0.49036717414855957, loss=2.7697393894195557
I0306 09:14:09.377672 140334130460416 logging_writer.py:48] [88000] global_step=88000, grad_norm=0.43931788206100464, loss=2.8025708198547363
I0306 09:14:44.343639 140334138853120 logging_writer.py:48] [88100] global_step=88100, grad_norm=0.4385276436805725, loss=2.7464280128479004
I0306 09:15:19.294463 140334130460416 logging_writer.py:48] [88200] global_step=88200, grad_norm=0.45013219118118286, loss=2.688671827316284
I0306 09:15:54.237431 140334138853120 logging_writer.py:48] [88300] global_step=88300, grad_norm=0.5114050507545471, loss=2.741297960281372
I0306 09:16:29.233860 140334130460416 logging_writer.py:48] [88400] global_step=88400, grad_norm=0.4438747763633728, loss=2.7020866870880127
I0306 09:17:04.186248 140334138853120 logging_writer.py:48] [88500] global_step=88500, grad_norm=0.463282972574234, loss=2.7948970794677734
I0306 09:17:39.163080 140334130460416 logging_writer.py:48] [88600] global_step=88600, grad_norm=0.4388326108455658, loss=2.7144556045532227
I0306 09:18:14.130355 140334138853120 logging_writer.py:48] [88700] global_step=88700, grad_norm=0.43611791729927063, loss=2.7789418697357178
I0306 09:18:49.065841 140334130460416 logging_writer.py:48] [88800] global_step=88800, grad_norm=0.4669933319091797, loss=2.7819199562072754
I0306 09:19:21.949945 140477253723328 spec.py:321] Evaluating on the training split.
I0306 09:19:24.598781 140477253723328 workload.py:181] Translating evaluation dataset.
I0306 09:22:19.596599 140477253723328 spec.py:333] Evaluating on the validation split.
I0306 09:22:22.242643 140477253723328 workload.py:181] Translating evaluation dataset.
I0306 09:25:03.307310 140477253723328 spec.py:349] Evaluating on the test split.
I0306 09:25:05.955794 140477253723328 workload.py:181] Translating evaluation dataset.
I0306 09:27:38.733297 140477253723328 submission_runner.py:469] Time since start: 51248.96s, 	Step: 88895, 	{'train/accuracy': 0.6845171451568604, 'train/loss': 1.5698010921478271, 'train/bleu': 34.454155886492245, 'validation/accuracy': 0.686302661895752, 'validation/loss': 1.5376595258712769, 'validation/bleu': 30.13079544998059, 'validation/num_examples': 3000, 'test/accuracy': 0.7016452550888062, 'test/loss': 1.4551117420196533, 'test/bleu': 30.081749734651165, 'test/num_examples': 3003, 'score': 31106.67918395996, 'total_duration': 51248.95845770836, 'accumulated_submission_time': 31106.67918395996, 'accumulated_eval_time': 20136.194087028503, 'accumulated_logging_time': 0.8988285064697266}
I0306 09:27:38.751977 140334138853120 logging_writer.py:48] [88895] accumulated_eval_time=20136.2, accumulated_logging_time=0.898829, accumulated_submission_time=31106.7, global_step=88895, preemption_count=0, score=31106.7, test/accuracy=0.701645, test/bleu=30.0817, test/loss=1.45511, test/num_examples=3003, total_duration=51249, train/accuracy=0.684517, train/bleu=34.4542, train/loss=1.5698, validation/accuracy=0.686303, validation/bleu=30.1308, validation/loss=1.53766, validation/num_examples=3000
I0306 09:27:40.862473 140334130460416 logging_writer.py:48] [88900] global_step=88900, grad_norm=0.4411710500717163, loss=2.731574773788452
I0306 09:28:15.759288 140334138853120 logging_writer.py:48] [89000] global_step=89000, grad_norm=0.4765576720237732, loss=2.783329486846924
I0306 09:28:50.686253 140334130460416 logging_writer.py:48] [89100] global_step=89100, grad_norm=0.4600183665752411, loss=2.795196533203125
I0306 09:29:25.638183 140334138853120 logging_writer.py:48] [89200] global_step=89200, grad_norm=0.47070255875587463, loss=2.7657597064971924
I0306 09:30:00.580816 140334130460416 logging_writer.py:48] [89300] global_step=89300, grad_norm=0.4733272194862366, loss=2.7222273349761963
I0306 09:30:35.525894 140334138853120 logging_writer.py:48] [89400] global_step=89400, grad_norm=0.45737195014953613, loss=2.693230628967285
I0306 09:31:10.456622 140334130460416 logging_writer.py:48] [89500] global_step=89500, grad_norm=0.4706592857837677, loss=2.7157087326049805
I0306 09:31:45.410921 140334138853120 logging_writer.py:48] [89600] global_step=89600, grad_norm=0.4833601713180542, loss=2.7808966636657715
I0306 09:32:20.350159 140334130460416 logging_writer.py:48] [89700] global_step=89700, grad_norm=0.4782998263835907, loss=2.8022072315216064
I0306 09:32:55.288811 140334138853120 logging_writer.py:48] [89800] global_step=89800, grad_norm=0.46523094177246094, loss=2.77956485748291
I0306 09:33:30.245059 140334130460416 logging_writer.py:48] [89900] global_step=89900, grad_norm=0.45629921555519104, loss=2.6707916259765625
I0306 09:34:05.187468 140334138853120 logging_writer.py:48] [90000] global_step=90000, grad_norm=0.4724063575267792, loss=2.7223799228668213
I0306 09:34:40.122809 140334130460416 logging_writer.py:48] [90100] global_step=90100, grad_norm=0.4693230986595154, loss=2.799136161804199
I0306 09:35:15.079972 140334138853120 logging_writer.py:48] [90200] global_step=90200, grad_norm=0.4643343389034271, loss=2.7149503231048584
I0306 09:35:50.013749 140334130460416 logging_writer.py:48] [90300] global_step=90300, grad_norm=0.542564868927002, loss=2.7799103260040283
I0306 09:36:24.963654 140334138853120 logging_writer.py:48] [90400] global_step=90400, grad_norm=0.4765288233757019, loss=2.7695374488830566
I0306 09:36:59.884160 140334130460416 logging_writer.py:48] [90500] global_step=90500, grad_norm=0.44738277792930603, loss=2.720069408416748
I0306 09:37:34.829062 140334138853120 logging_writer.py:48] [90600] global_step=90600, grad_norm=0.4693051874637604, loss=2.68283748626709
I0306 09:38:09.814390 140334130460416 logging_writer.py:48] [90700] global_step=90700, grad_norm=0.4717281460762024, loss=2.737208127975464
I0306 09:38:44.720036 140334138853120 logging_writer.py:48] [90800] global_step=90800, grad_norm=0.4875803291797638, loss=2.7040858268737793
I0306 09:39:19.670619 140334130460416 logging_writer.py:48] [90900] global_step=90900, grad_norm=0.47295230627059937, loss=2.7471847534179688
I0306 09:39:54.594065 140334138853120 logging_writer.py:48] [91000] global_step=91000, grad_norm=0.48027458786964417, loss=2.767303943634033
I0306 09:40:29.533436 140334130460416 logging_writer.py:48] [91100] global_step=91100, grad_norm=0.46590960025787354, loss=2.735989570617676
I0306 09:41:04.474585 140334138853120 logging_writer.py:48] [91200] global_step=91200, grad_norm=0.4808155596256256, loss=2.6880993843078613
I0306 09:41:39.066005 140477253723328 spec.py:321] Evaluating on the training split.
I0306 09:41:41.711863 140477253723328 workload.py:181] Translating evaluation dataset.
I0306 09:44:39.261583 140477253723328 spec.py:333] Evaluating on the validation split.
I0306 09:44:41.902734 140477253723328 workload.py:181] Translating evaluation dataset.
I0306 09:47:25.712064 140477253723328 spec.py:349] Evaluating on the test split.
I0306 09:47:28.357074 140477253723328 workload.py:181] Translating evaluation dataset.
I0306 09:50:02.194203 140477253723328 submission_runner.py:469] Time since start: 52592.42s, 	Step: 91300, 	{'train/accuracy': 0.6783825159072876, 'train/loss': 1.6039488315582275, 'train/bleu': 33.75529584910428, 'validation/accuracy': 0.6886757612228394, 'validation/loss': 1.5309007167816162, 'validation/bleu': 30.388817109752456, 'validation/num_examples': 3000, 'test/accuracy': 0.703649640083313, 'test/loss': 1.4468092918395996, 'test/bleu': 30.331338156440417, 'test/num_examples': 3003, 'score': 31946.84950709343, 'total_duration': 52592.41937613487, 'accumulated_submission_time': 31946.84950709343, 'accumulated_eval_time': 20639.322226524353, 'accumulated_logging_time': 0.926297664642334}
I0306 09:50:02.214195 140334130460416 logging_writer.py:48] [91300] accumulated_eval_time=20639.3, accumulated_logging_time=0.926298, accumulated_submission_time=31946.8, global_step=91300, preemption_count=0, score=31946.8, test/accuracy=0.70365, test/bleu=30.3313, test/loss=1.44681, test/num_examples=3003, total_duration=52592.4, train/accuracy=0.678383, train/bleu=33.7553, train/loss=1.60395, validation/accuracy=0.688676, validation/bleu=30.3888, validation/loss=1.5309, validation/num_examples=3000
I0306 09:50:02.577569 140334138853120 logging_writer.py:48] [91300] global_step=91300, grad_norm=0.5118598937988281, loss=2.7742767333984375
I0306 09:50:37.507044 140334130460416 logging_writer.py:48] [91400] global_step=91400, grad_norm=0.5261700749397278, loss=2.6911635398864746
I0306 09:51:12.449632 140334138853120 logging_writer.py:48] [91500] global_step=91500, grad_norm=0.49009665846824646, loss=2.7108070850372314
I0306 09:51:47.416239 140334130460416 logging_writer.py:48] [91600] global_step=91600, grad_norm=0.48845216631889343, loss=2.7358322143554688
I0306 09:52:22.371191 140334138853120 logging_writer.py:48] [91700] global_step=91700, grad_norm=0.47124096751213074, loss=2.6869256496429443
I0306 09:52:57.305799 140334130460416 logging_writer.py:48] [91800] global_step=91800, grad_norm=0.4779577851295471, loss=2.6936421394348145
I0306 09:53:32.230042 140334138853120 logging_writer.py:48] [91900] global_step=91900, grad_norm=0.4750937521457672, loss=2.68276047706604
I0306 09:54:07.177774 140334130460416 logging_writer.py:48] [92000] global_step=92000, grad_norm=0.5385258793830872, loss=2.7582356929779053
I0306 09:54:42.094250 140334138853120 logging_writer.py:48] [92100] global_step=92100, grad_norm=0.49402597546577454, loss=2.7477939128875732
I0306 09:55:17.040098 140334130460416 logging_writer.py:48] [92200] global_step=92200, grad_norm=0.49212968349456787, loss=2.783082962036133
I0306 09:55:52.007573 140334138853120 logging_writer.py:48] [92300] global_step=92300, grad_norm=0.48872995376586914, loss=2.7080039978027344
I0306 09:56:26.945797 140334130460416 logging_writer.py:48] [92400] global_step=92400, grad_norm=0.47401759028434753, loss=2.723641872406006
I0306 09:57:01.949062 140334138853120 logging_writer.py:48] [92500] global_step=92500, grad_norm=0.5267027020454407, loss=2.7671375274658203
I0306 09:57:36.909488 140334130460416 logging_writer.py:48] [92600] global_step=92600, grad_norm=0.47192293405532837, loss=2.703059434890747
I0306 09:58:11.882812 140334138853120 logging_writer.py:48] [92700] global_step=92700, grad_norm=0.47975727915763855, loss=2.727912425994873
I0306 09:58:46.859592 140334130460416 logging_writer.py:48] [92800] global_step=92800, grad_norm=0.4751369059085846, loss=2.698955535888672
I0306 09:59:21.869232 140334138853120 logging_writer.py:48] [92900] global_step=92900, grad_norm=0.4982124865055084, loss=2.683298349380493
I0306 09:59:56.825497 140334130460416 logging_writer.py:48] [93000] global_step=93000, grad_norm=0.4788653254508972, loss=2.7345526218414307
I0306 10:00:31.798966 140334138853120 logging_writer.py:48] [93100] global_step=93100, grad_norm=0.4920071065425873, loss=2.6918935775756836
I0306 10:01:06.802605 140334130460416 logging_writer.py:48] [93200] global_step=93200, grad_norm=0.48508888483047485, loss=2.721665143966675
I0306 10:01:41.785172 140334138853120 logging_writer.py:48] [93300] global_step=93300, grad_norm=0.48405197262763977, loss=2.7620208263397217
I0306 10:02:16.747477 140334130460416 logging_writer.py:48] [93400] global_step=93400, grad_norm=0.4758511483669281, loss=2.719306707382202
I0306 10:02:51.739485 140334138853120 logging_writer.py:48] [93500] global_step=93500, grad_norm=0.48687034845352173, loss=2.7799320220947266
I0306 10:03:26.729048 140334130460416 logging_writer.py:48] [93600] global_step=93600, grad_norm=0.4886391758918762, loss=2.724818229675293
I0306 10:04:01.729184 140334138853120 logging_writer.py:48] [93700] global_step=93700, grad_norm=0.4840918183326721, loss=2.669175386428833
I0306 10:04:02.434554 140477253723328 spec.py:321] Evaluating on the training split.
I0306 10:04:05.086289 140477253723328 workload.py:181] Translating evaluation dataset.
I0306 10:07:13.933370 140477253723328 spec.py:333] Evaluating on the validation split.
I0306 10:07:16.565292 140477253723328 workload.py:181] Translating evaluation dataset.
I0306 10:09:51.492372 140477253723328 spec.py:349] Evaluating on the test split.
I0306 10:09:54.146802 140477253723328 workload.py:181] Translating evaluation dataset.
I0306 10:12:28.260086 140477253723328 submission_runner.py:469] Time since start: 53938.49s, 	Step: 93703, 	{'train/accuracy': 0.679144024848938, 'train/loss': 1.5999244451522827, 'train/bleu': 34.07129018170136, 'validation/accuracy': 0.6888982653617859, 'validation/loss': 1.527034044265747, 'validation/bleu': 30.415390972821445, 'validation/num_examples': 3000, 'test/accuracy': 0.7054454684257507, 'test/loss': 1.4403575658798218, 'test/bleu': 30.184775646078077, 'test/num_examples': 3003, 'score': 32786.92387294769, 'total_duration': 53938.485255002975, 'accumulated_submission_time': 32786.92387294769, 'accumulated_eval_time': 21145.14769220352, 'accumulated_logging_time': 0.9554259777069092}
I0306 10:12:28.278311 140334130460416 logging_writer.py:48] [93703] accumulated_eval_time=21145.1, accumulated_logging_time=0.955426, accumulated_submission_time=32786.9, global_step=93703, preemption_count=0, score=32786.9, test/accuracy=0.705445, test/bleu=30.1848, test/loss=1.44036, test/num_examples=3003, total_duration=53938.5, train/accuracy=0.679144, train/bleu=34.0713, train/loss=1.59992, validation/accuracy=0.688898, validation/bleu=30.4154, validation/loss=1.52703, validation/num_examples=3000
I0306 10:13:02.503309 140334138853120 logging_writer.py:48] [93800] global_step=93800, grad_norm=0.4854966104030609, loss=2.7741711139678955
I0306 10:13:37.413618 140334130460416 logging_writer.py:48] [93900] global_step=93900, grad_norm=0.5218803286552429, loss=2.771211862564087
I0306 10:14:12.326787 140334138853120 logging_writer.py:48] [94000] global_step=94000, grad_norm=0.5398783087730408, loss=2.6983649730682373
I0306 10:14:47.238440 140334130460416 logging_writer.py:48] [94100] global_step=94100, grad_norm=0.5417254567146301, loss=2.7501320838928223
I0306 10:15:22.094548 140334138853120 logging_writer.py:48] [94200] global_step=94200, grad_norm=0.4936276972293854, loss=2.663774013519287
I0306 10:15:56.953196 140334130460416 logging_writer.py:48] [94300] global_step=94300, grad_norm=0.49227508902549744, loss=2.69296932220459
I0306 10:16:31.827590 140334138853120 logging_writer.py:48] [94400] global_step=94400, grad_norm=0.5190996527671814, loss=2.748899459838867
I0306 10:17:06.711074 140334130460416 logging_writer.py:48] [94500] global_step=94500, grad_norm=0.4954230785369873, loss=2.7126810550689697
I0306 10:17:41.580950 140334138853120 logging_writer.py:48] [94600] global_step=94600, grad_norm=0.5251733660697937, loss=2.6996071338653564
I0306 10:18:16.433928 140334130460416 logging_writer.py:48] [94700] global_step=94700, grad_norm=0.49793967604637146, loss=2.71620512008667
I0306 10:18:51.303617 140334138853120 logging_writer.py:48] [94800] global_step=94800, grad_norm=0.5153474807739258, loss=2.6688997745513916
I0306 10:19:26.172531 140334130460416 logging_writer.py:48] [94900] global_step=94900, grad_norm=0.5146797895431519, loss=2.7393250465393066
I0306 10:20:01.043083 140334138853120 logging_writer.py:48] [95000] global_step=95000, grad_norm=0.4977629780769348, loss=2.6917145252227783
I0306 10:20:35.903154 140334130460416 logging_writer.py:48] [95100] global_step=95100, grad_norm=0.4961296319961548, loss=2.7337331771850586
I0306 10:21:10.769044 140334138853120 logging_writer.py:48] [95200] global_step=95200, grad_norm=0.49876511096954346, loss=2.675323009490967
I0306 10:21:45.667808 140334130460416 logging_writer.py:48] [95300] global_step=95300, grad_norm=0.5001620650291443, loss=2.6823997497558594
I0306 10:22:20.537543 140334138853120 logging_writer.py:48] [95400] global_step=95400, grad_norm=0.5172513723373413, loss=2.714498519897461
I0306 10:22:55.398588 140334130460416 logging_writer.py:48] [95500] global_step=95500, grad_norm=0.5321565270423889, loss=2.6897571086883545
I0306 10:23:30.261036 140334138853120 logging_writer.py:48] [95600] global_step=95600, grad_norm=0.5170044302940369, loss=2.6790425777435303
I0306 10:24:05.123602 140334130460416 logging_writer.py:48] [95700] global_step=95700, grad_norm=0.4976651966571808, loss=2.6501669883728027
I0306 10:24:39.963519 140334138853120 logging_writer.py:48] [95800] global_step=95800, grad_norm=0.5183537006378174, loss=2.6508612632751465
I0306 10:25:14.816273 140334130460416 logging_writer.py:48] [95900] global_step=95900, grad_norm=0.516577422618866, loss=2.634165048599243
I0306 10:25:49.676150 140334138853120 logging_writer.py:48] [96000] global_step=96000, grad_norm=0.5469570159912109, loss=2.7138619422912598
I0306 10:26:24.546135 140334130460416 logging_writer.py:48] [96100] global_step=96100, grad_norm=0.49347662925720215, loss=2.65171480178833
I0306 10:26:28.390092 140477253723328 spec.py:321] Evaluating on the training split.
I0306 10:26:31.040770 140477253723328 workload.py:181] Translating evaluation dataset.
I0306 10:29:17.412439 140477253723328 spec.py:333] Evaluating on the validation split.
I0306 10:29:20.057437 140477253723328 workload.py:181] Translating evaluation dataset.
I0306 10:31:51.216219 140477253723328 spec.py:349] Evaluating on the test split.
I0306 10:31:53.861128 140477253723328 workload.py:181] Translating evaluation dataset.
I0306 10:34:21.979026 140477253723328 submission_runner.py:469] Time since start: 55252.20s, 	Step: 96112, 	{'train/accuracy': 0.6892513036727905, 'train/loss': 1.54386305809021, 'train/bleu': 34.871488065666085, 'validation/accuracy': 0.689800500869751, 'validation/loss': 1.521233320236206, 'validation/bleu': 30.34727694356915, 'validation/num_examples': 3000, 'test/accuracy': 0.7061638236045837, 'test/loss': 1.437849521636963, 'test/bleu': 30.433286774467756, 'test/num_examples': 3003, 'score': 33626.893287181854, 'total_duration': 55252.2042016983, 'accumulated_submission_time': 33626.893287181854, 'accumulated_eval_time': 21618.736565351486, 'accumulated_logging_time': 0.9815571308135986}
I0306 10:34:21.998569 140334138853120 logging_writer.py:48] [96112] accumulated_eval_time=21618.7, accumulated_logging_time=0.981557, accumulated_submission_time=33626.9, global_step=96112, preemption_count=0, score=33626.9, test/accuracy=0.706164, test/bleu=30.4333, test/loss=1.43785, test/num_examples=3003, total_duration=55252.2, train/accuracy=0.689251, train/bleu=34.8715, train/loss=1.54386, validation/accuracy=0.689801, validation/bleu=30.3473, validation/loss=1.52123, validation/num_examples=3000
I0306 10:34:53.040855 140334130460416 logging_writer.py:48] [96200] global_step=96200, grad_norm=0.5343014001846313, loss=2.652305841445923
I0306 10:35:27.920692 140334138853120 logging_writer.py:48] [96300] global_step=96300, grad_norm=0.5271960496902466, loss=2.642923355102539
I0306 10:36:02.780075 140334130460416 logging_writer.py:48] [96400] global_step=96400, grad_norm=0.5054148435592651, loss=2.7042336463928223
I0306 10:36:37.647032 140334138853120 logging_writer.py:48] [96500] global_step=96500, grad_norm=0.5062212944030762, loss=2.6450793743133545
I0306 10:37:12.521704 140334130460416 logging_writer.py:48] [96600] global_step=96600, grad_norm=0.5238618850708008, loss=2.676318645477295
I0306 10:37:47.409151 140334138853120 logging_writer.py:48] [96700] global_step=96700, grad_norm=0.5294166207313538, loss=2.7026240825653076
I0306 10:38:22.262506 140334130460416 logging_writer.py:48] [96800] global_step=96800, grad_norm=0.5310348272323608, loss=2.669114589691162
I0306 10:38:57.178809 140334138853120 logging_writer.py:48] [96900] global_step=96900, grad_norm=0.5149635076522827, loss=2.6647789478302
I0306 10:39:32.060898 140334130460416 logging_writer.py:48] [97000] global_step=97000, grad_norm=0.5658613443374634, loss=2.7076635360717773
I0306 10:40:06.927336 140334138853120 logging_writer.py:48] [97100] global_step=97100, grad_norm=0.5490167737007141, loss=2.7319135665893555
I0306 10:40:41.813975 140334130460416 logging_writer.py:48] [97200] global_step=97200, grad_norm=0.5454697012901306, loss=2.648831844329834
I0306 10:41:16.718110 140334138853120 logging_writer.py:48] [97300] global_step=97300, grad_norm=0.5088852643966675, loss=2.630740165710449
I0306 10:41:51.607122 140334130460416 logging_writer.py:48] [97400] global_step=97400, grad_norm=0.5231444239616394, loss=2.6817331314086914
I0306 10:42:26.501918 140334138853120 logging_writer.py:48] [97500] global_step=97500, grad_norm=0.5232347249984741, loss=2.6466119289398193
I0306 10:43:01.401386 140334130460416 logging_writer.py:48] [97600] global_step=97600, grad_norm=0.5273032784461975, loss=2.654817581176758
I0306 10:43:36.289695 140334138853120 logging_writer.py:48] [97700] global_step=97700, grad_norm=0.5740616917610168, loss=2.691033124923706
I0306 10:44:11.177561 140334130460416 logging_writer.py:48] [97800] global_step=97800, grad_norm=0.5303769707679749, loss=2.7423622608184814
I0306 10:44:46.071712 140334138853120 logging_writer.py:48] [97900] global_step=97900, grad_norm=0.5198832154273987, loss=2.66611909866333
I0306 10:45:20.955557 140334130460416 logging_writer.py:48] [98000] global_step=98000, grad_norm=0.5454797148704529, loss=2.6865241527557373
I0306 10:45:55.828704 140334138853120 logging_writer.py:48] [98100] global_step=98100, grad_norm=0.5572426319122314, loss=2.656125545501709
I0306 10:46:30.720197 140334130460416 logging_writer.py:48] [98200] global_step=98200, grad_norm=0.5437925457954407, loss=2.648643970489502
I0306 10:47:05.599741 140334138853120 logging_writer.py:48] [98300] global_step=98300, grad_norm=0.5603964328765869, loss=2.6991357803344727
I0306 10:47:40.482046 140334130460416 logging_writer.py:48] [98400] global_step=98400, grad_norm=0.5487915277481079, loss=2.672100782394409
I0306 10:48:15.344569 140334138853120 logging_writer.py:48] [98500] global_step=98500, grad_norm=0.5467736721038818, loss=2.6438257694244385
I0306 10:48:22.325377 140477253723328 spec.py:321] Evaluating on the training split.
I0306 10:48:24.978978 140477253723328 workload.py:181] Translating evaluation dataset.
I0306 10:51:46.995779 140477253723328 spec.py:333] Evaluating on the validation split.
I0306 10:51:49.635673 140477253723328 workload.py:181] Translating evaluation dataset.
I0306 10:54:30.961322 140477253723328 spec.py:349] Evaluating on the test split.
I0306 10:54:33.599378 140477253723328 workload.py:181] Translating evaluation dataset.
I0306 10:57:11.879438 140477253723328 submission_runner.py:469] Time since start: 56622.10s, 	Step: 98521, 	{'train/accuracy': 0.6882555484771729, 'train/loss': 1.551037311553955, 'train/bleu': 34.89218112133462, 'validation/accuracy': 0.6907893419265747, 'validation/loss': 1.5171523094177246, 'validation/bleu': 30.46519660724104, 'validation/num_examples': 3000, 'test/accuracy': 0.7061870098114014, 'test/loss': 1.4302724599838257, 'test/bleu': 30.5430962491934, 'test/num_examples': 3003, 'score': 34467.07543492317, 'total_duration': 56622.10461521149, 'accumulated_submission_time': 34467.07543492317, 'accumulated_eval_time': 22148.290571689606, 'accumulated_logging_time': 1.0089812278747559}
I0306 10:57:11.898964 140334130460416 logging_writer.py:48] [98521] accumulated_eval_time=22148.3, accumulated_logging_time=1.00898, accumulated_submission_time=34467.1, global_step=98521, preemption_count=0, score=34467.1, test/accuracy=0.706187, test/bleu=30.5431, test/loss=1.43027, test/num_examples=3003, total_duration=56622.1, train/accuracy=0.688256, train/bleu=34.8922, train/loss=1.55104, validation/accuracy=0.690789, validation/bleu=30.4652, validation/loss=1.51715, validation/num_examples=3000
I0306 10:57:39.789125 140334138853120 logging_writer.py:48] [98600] global_step=98600, grad_norm=0.5405734777450562, loss=2.603750467300415
I0306 10:58:14.622911 140334130460416 logging_writer.py:48] [98700] global_step=98700, grad_norm=0.5590813755989075, loss=2.6898460388183594
I0306 10:58:49.486050 140334138853120 logging_writer.py:48] [98800] global_step=98800, grad_norm=0.5311793684959412, loss=2.6303253173828125
I0306 10:59:24.321916 140334130460416 logging_writer.py:48] [98900] global_step=98900, grad_norm=0.5397084355354309, loss=2.6239233016967773
I0306 10:59:59.185311 140334138853120 logging_writer.py:48] [99000] global_step=99000, grad_norm=0.5659593343734741, loss=2.6981728076934814
I0306 11:00:34.035146 140334130460416 logging_writer.py:48] [99100] global_step=99100, grad_norm=0.5520619750022888, loss=2.688300609588623
I0306 11:01:08.916357 140334138853120 logging_writer.py:48] [99200] global_step=99200, grad_norm=0.5883226990699768, loss=2.6523449420928955
I0306 11:01:43.770298 140334130460416 logging_writer.py:48] [99300] global_step=99300, grad_norm=0.5426780581474304, loss=2.6981899738311768
I0306 11:02:18.623192 140334138853120 logging_writer.py:48] [99400] global_step=99400, grad_norm=0.5664364099502563, loss=2.758913516998291
I0306 11:02:53.511251 140334130460416 logging_writer.py:48] [99500] global_step=99500, grad_norm=0.5451715588569641, loss=2.6811909675598145
I0306 11:03:28.370957 140334138853120 logging_writer.py:48] [99600] global_step=99600, grad_norm=0.5516715049743652, loss=2.696208953857422
I0306 11:04:03.237706 140334130460416 logging_writer.py:48] [99700] global_step=99700, grad_norm=0.5462794899940491, loss=2.6358962059020996
I0306 11:04:38.117618 140334138853120 logging_writer.py:48] [99800] global_step=99800, grad_norm=0.5696051120758057, loss=2.6912004947662354
I0306 11:05:12.984669 140334130460416 logging_writer.py:48] [99900] global_step=99900, grad_norm=0.5765125751495361, loss=2.6854217052459717
I0306 11:05:47.844974 140334138853120 logging_writer.py:48] [100000] global_step=100000, grad_norm=0.5366962552070618, loss=2.6756019592285156
I0306 11:06:22.697344 140334130460416 logging_writer.py:48] [100100] global_step=100100, grad_norm=0.5418983101844788, loss=2.655596971511841
I0306 11:06:57.561238 140334138853120 logging_writer.py:48] [100200] global_step=100200, grad_norm=0.559077799320221, loss=2.7056169509887695
I0306 11:07:32.451686 140334130460416 logging_writer.py:48] [100300] global_step=100300, grad_norm=0.5605959296226501, loss=2.6106653213500977
I0306 11:08:07.368375 140334138853120 logging_writer.py:48] [100400] global_step=100400, grad_norm=0.5616859197616577, loss=2.670821189880371
I0306 11:08:42.360268 140334130460416 logging_writer.py:48] [100500] global_step=100500, grad_norm=0.5699275732040405, loss=2.628265380859375
I0306 11:09:17.333230 140334138853120 logging_writer.py:48] [100600] global_step=100600, grad_norm=0.5689409971237183, loss=2.6577816009521484
I0306 11:09:52.321720 140334130460416 logging_writer.py:48] [100700] global_step=100700, grad_norm=0.5706937909126282, loss=2.702338933944702
I0306 11:10:27.264443 140334138853120 logging_writer.py:48] [100800] global_step=100800, grad_norm=0.5812363624572754, loss=2.681793212890625
I0306 11:11:02.225563 140334130460416 logging_writer.py:48] [100900] global_step=100900, grad_norm=0.5754390954971313, loss=2.6876204013824463
I0306 11:11:12.005863 140477253723328 spec.py:321] Evaluating on the training split.
I0306 11:11:14.654390 140477253723328 workload.py:181] Translating evaluation dataset.
I0306 11:14:25.120494 140477253723328 spec.py:333] Evaluating on the validation split.
I0306 11:14:27.763747 140477253723328 workload.py:181] Translating evaluation dataset.
I0306 11:17:14.205508 140477253723328 spec.py:349] Evaluating on the test split.
I0306 11:17:16.850074 140477253723328 workload.py:181] Translating evaluation dataset.
I0306 11:19:56.453792 140477253723328 submission_runner.py:469] Time since start: 57986.68s, 	Step: 100929, 	{'train/accuracy': 0.7020270228385925, 'train/loss': 1.4744031429290771, 'train/bleu': 36.28851876574889, 'validation/accuracy': 0.6913949847221375, 'validation/loss': 1.5133466720581055, 'validation/bleu': 30.60845847297415, 'validation/num_examples': 3000, 'test/accuracy': 0.7073572278022766, 'test/loss': 1.4281712770462036, 'test/bleu': 30.62010006542814, 'test/num_examples': 3003, 'score': 35307.03855133057, 'total_duration': 57986.67894530296, 'accumulated_submission_time': 35307.03855133057, 'accumulated_eval_time': 22672.73842072487, 'accumulated_logging_time': 1.036452293395996}
I0306 11:19:56.472932 140334138853120 logging_writer.py:48] [100929] accumulated_eval_time=22672.7, accumulated_logging_time=1.03645, accumulated_submission_time=35307, global_step=100929, preemption_count=0, score=35307, test/accuracy=0.707357, test/bleu=30.6201, test/loss=1.42817, test/num_examples=3003, total_duration=57986.7, train/accuracy=0.702027, train/bleu=36.2885, train/loss=1.4744, validation/accuracy=0.691395, validation/bleu=30.6085, validation/loss=1.51335, validation/num_examples=3000
I0306 11:20:21.620535 140334130460416 logging_writer.py:48] [101000] global_step=101000, grad_norm=0.571374773979187, loss=2.669274091720581
I0306 11:20:56.587778 140334138853120 logging_writer.py:48] [101100] global_step=101100, grad_norm=0.5795459151268005, loss=2.6810693740844727
I0306 11:21:31.543080 140334130460416 logging_writer.py:48] [101200] global_step=101200, grad_norm=0.5831876993179321, loss=2.683359384536743
I0306 11:22:06.505228 140334138853120 logging_writer.py:48] [101300] global_step=101300, grad_norm=0.608273983001709, loss=2.6675984859466553
I0306 11:22:41.458515 140334130460416 logging_writer.py:48] [101400] global_step=101400, grad_norm=0.5683120489120483, loss=2.689209222793579
I0306 11:23:16.439733 140334138853120 logging_writer.py:48] [101500] global_step=101500, grad_norm=0.585115373134613, loss=2.601870059967041
I0306 11:23:51.419309 140334130460416 logging_writer.py:48] [101600] global_step=101600, grad_norm=0.598416268825531, loss=2.685267925262451
I0306 11:24:26.387222 140334138853120 logging_writer.py:48] [101700] global_step=101700, grad_norm=0.5863935947418213, loss=2.6339704990386963
I0306 11:25:01.341237 140334130460416 logging_writer.py:48] [101800] global_step=101800, grad_norm=0.5876042246818542, loss=2.6473593711853027
I0306 11:25:36.295351 140334138853120 logging_writer.py:48] [101900] global_step=101900, grad_norm=0.6017887592315674, loss=2.6497371196746826
I0306 11:26:11.257191 140334130460416 logging_writer.py:48] [102000] global_step=102000, grad_norm=0.6064234972000122, loss=2.691340446472168
I0306 11:26:46.240518 140334138853120 logging_writer.py:48] [102100] global_step=102100, grad_norm=0.5998722910881042, loss=2.6699841022491455
I0306 11:27:21.202398 140334130460416 logging_writer.py:48] [102200] global_step=102200, grad_norm=0.579920768737793, loss=2.6739256381988525
I0306 11:27:56.180716 140334138853120 logging_writer.py:48] [102300] global_step=102300, grad_norm=0.6054530739784241, loss=2.703939199447632
I0306 11:28:31.106276 140334130460416 logging_writer.py:48] [102400] global_step=102400, grad_norm=0.5791692733764648, loss=2.622185468673706
I0306 11:29:06.058015 140334138853120 logging_writer.py:48] [102500] global_step=102500, grad_norm=0.5806074142456055, loss=2.6951475143432617
I0306 11:29:41.034382 140334130460416 logging_writer.py:48] [102600] global_step=102600, grad_norm=0.5830259919166565, loss=2.6544415950775146
I0306 11:30:15.998729 140334138853120 logging_writer.py:48] [102700] global_step=102700, grad_norm=0.60638028383255, loss=2.6726858615875244
I0306 11:30:50.950544 140334130460416 logging_writer.py:48] [102800] global_step=102800, grad_norm=0.5932881832122803, loss=2.662320375442505
I0306 11:31:25.903085 140334138853120 logging_writer.py:48] [102900] global_step=102900, grad_norm=0.5884534120559692, loss=2.643967390060425
I0306 11:32:00.863252 140334130460416 logging_writer.py:48] [103000] global_step=103000, grad_norm=0.6125548481941223, loss=2.6796841621398926
I0306 11:32:35.817627 140334138853120 logging_writer.py:48] [103100] global_step=103100, grad_norm=0.5928636789321899, loss=2.6543478965759277
I0306 11:33:10.797718 140334130460416 logging_writer.py:48] [103200] global_step=103200, grad_norm=0.6033793091773987, loss=2.6375198364257812
I0306 11:33:45.776079 140334138853120 logging_writer.py:48] [103300] global_step=103300, grad_norm=0.5892224311828613, loss=2.651323080062866
I0306 11:33:56.625390 140477253723328 spec.py:321] Evaluating on the training split.
I0306 11:33:59.270352 140477253723328 workload.py:181] Translating evaluation dataset.
I0306 11:37:04.155850 140477253723328 spec.py:333] Evaluating on the validation split.
I0306 11:37:06.799345 140477253723328 workload.py:181] Translating evaluation dataset.
I0306 11:39:42.759238 140477253723328 spec.py:349] Evaluating on the test split.
I0306 11:39:45.404964 140477253723328 workload.py:181] Translating evaluation dataset.
I0306 11:42:08.837684 140477253723328 submission_runner.py:469] Time since start: 59319.06s, 	Step: 103332, 	{'train/accuracy': 0.6952316761016846, 'train/loss': 1.5049054622650146, 'train/bleu': 35.260961121863694, 'validation/accuracy': 0.6928781867027283, 'validation/loss': 1.5057975053787231, 'validation/bleu': 30.746985013389033, 'validation/num_examples': 3000, 'test/accuracy': 0.7093616127967834, 'test/loss': 1.417487621307373, 'test/bleu': 30.697278718913463, 'test/num_examples': 3003, 'score': 36147.04745411873, 'total_duration': 59319.06286215782, 'accumulated_submission_time': 36147.04745411873, 'accumulated_eval_time': 23164.950659036636, 'accumulated_logging_time': 1.063760757446289}
I0306 11:42:08.857665 140334130460416 logging_writer.py:48] [103332] accumulated_eval_time=23165, accumulated_logging_time=1.06376, accumulated_submission_time=36147, global_step=103332, preemption_count=0, score=36147, test/accuracy=0.709362, test/bleu=30.6973, test/loss=1.41749, test/num_examples=3003, total_duration=59319.1, train/accuracy=0.695232, train/bleu=35.261, train/loss=1.50491, validation/accuracy=0.692878, validation/bleu=30.747, validation/loss=1.5058, validation/num_examples=3000
I0306 11:42:32.971726 140334138853120 logging_writer.py:48] [103400] global_step=103400, grad_norm=0.5880558490753174, loss=2.6453378200531006
I0306 11:43:07.937861 140334130460416 logging_writer.py:48] [103500] global_step=103500, grad_norm=0.6175846457481384, loss=2.5971386432647705
I0306 11:43:42.916629 140334138853120 logging_writer.py:48] [103600] global_step=103600, grad_norm=0.5906745195388794, loss=2.757079839706421
I0306 11:44:17.895762 140334130460416 logging_writer.py:48] [103700] global_step=103700, grad_norm=0.6075988411903381, loss=2.703967332839966
I0306 11:44:52.859510 140334138853120 logging_writer.py:48] [103800] global_step=103800, grad_norm=0.6184337139129639, loss=2.639188289642334
I0306 11:45:27.821026 140334130460416 logging_writer.py:48] [103900] global_step=103900, grad_norm=0.6172054409980774, loss=2.656104803085327
I0306 11:46:02.783317 140334138853120 logging_writer.py:48] [104000] global_step=104000, grad_norm=0.6134576201438904, loss=2.5846383571624756
I0306 11:46:37.758600 140334130460416 logging_writer.py:48] [104100] global_step=104100, grad_norm=0.5774096250534058, loss=2.6531403064727783
I0306 11:47:12.702420 140334138853120 logging_writer.py:48] [104200] global_step=104200, grad_norm=0.6026825904846191, loss=2.6321866512298584
I0306 11:47:47.667146 140334130460416 logging_writer.py:48] [104300] global_step=104300, grad_norm=0.5946387648582458, loss=2.6511142253875732
I0306 11:48:22.650321 140334138853120 logging_writer.py:48] [104400] global_step=104400, grad_norm=0.6021575331687927, loss=2.6735875606536865
I0306 11:48:57.629529 140334130460416 logging_writer.py:48] [104500] global_step=104500, grad_norm=0.604033887386322, loss=2.6312849521636963
I0306 11:49:32.624103 140334138853120 logging_writer.py:48] [104600] global_step=104600, grad_norm=0.6081487536430359, loss=2.6234984397888184
I0306 11:50:07.588819 140334130460416 logging_writer.py:48] [104700] global_step=104700, grad_norm=0.6188668608665466, loss=2.61735200881958
I0306 11:50:42.561725 140334138853120 logging_writer.py:48] [104800] global_step=104800, grad_norm=0.6225377321243286, loss=2.6574954986572266
I0306 11:51:17.515766 140334130460416 logging_writer.py:48] [104900] global_step=104900, grad_norm=0.5952323079109192, loss=2.5485177040100098
I0306 11:51:52.465186 140334138853120 logging_writer.py:48] [105000] global_step=105000, grad_norm=0.6044524312019348, loss=2.636460304260254
I0306 11:52:27.450252 140334130460416 logging_writer.py:48] [105100] global_step=105100, grad_norm=0.6285136342048645, loss=2.6217682361602783
I0306 11:53:02.406637 140334138853120 logging_writer.py:48] [105200] global_step=105200, grad_norm=0.6336621642112732, loss=2.6261794567108154
I0306 11:53:37.372546 140334130460416 logging_writer.py:48] [105300] global_step=105300, grad_norm=0.6125810146331787, loss=2.6414544582366943
I0306 11:54:12.320237 140334138853120 logging_writer.py:48] [105400] global_step=105400, grad_norm=0.6264697313308716, loss=2.614879608154297
I0306 11:54:47.326637 140334130460416 logging_writer.py:48] [105500] global_step=105500, grad_norm=0.6243754029273987, loss=2.6073625087738037
I0306 11:55:22.284912 140334138853120 logging_writer.py:48] [105600] global_step=105600, grad_norm=0.6174178719520569, loss=2.5968449115753174
I0306 11:55:57.249839 140334130460416 logging_writer.py:48] [105700] global_step=105700, grad_norm=0.6480989456176758, loss=2.667491912841797
I0306 11:56:09.143270 140477253723328 spec.py:321] Evaluating on the training split.
I0306 11:56:11.792846 140477253723328 workload.py:181] Translating evaluation dataset.
I0306 11:59:27.549226 140477253723328 spec.py:333] Evaluating on the validation split.
I0306 11:59:30.199489 140477253723328 workload.py:181] Translating evaluation dataset.
I0306 12:02:02.429023 140477253723328 spec.py:349] Evaluating on the test split.
I0306 12:02:05.072636 140477253723328 workload.py:181] Translating evaluation dataset.
I0306 12:04:39.403203 140477253723328 submission_runner.py:469] Time since start: 60669.63s, 	Step: 105735, 	{'train/accuracy': 0.6962881684303284, 'train/loss': 1.4978196620941162, 'train/bleu': 35.346667011153265, 'validation/accuracy': 0.6933602094650269, 'validation/loss': 1.5046344995498657, 'validation/bleu': 30.820000096291395, 'validation/num_examples': 3000, 'test/accuracy': 0.7091298699378967, 'test/loss': 1.4145933389663696, 'test/bleu': 30.652485975793297, 'test/num_examples': 3003, 'score': 36987.18598008156, 'total_duration': 60669.628366947174, 'accumulated_submission_time': 36987.18598008156, 'accumulated_eval_time': 23675.21053314209, 'accumulated_logging_time': 1.0919380187988281}
I0306 12:04:39.423464 140334138853120 logging_writer.py:48] [105735] accumulated_eval_time=23675.2, accumulated_logging_time=1.09194, accumulated_submission_time=36987.2, global_step=105735, preemption_count=0, score=36987.2, test/accuracy=0.70913, test/bleu=30.6525, test/loss=1.41459, test/num_examples=3003, total_duration=60669.6, train/accuracy=0.696288, train/bleu=35.3467, train/loss=1.49782, validation/accuracy=0.69336, validation/bleu=30.82, validation/loss=1.50463, validation/num_examples=3000
I0306 12:05:02.483016 140334130460416 logging_writer.py:48] [105800] global_step=105800, grad_norm=0.6043734550476074, loss=2.5647635459899902
I0306 12:05:37.441833 140334138853120 logging_writer.py:48] [105900] global_step=105900, grad_norm=0.6215913891792297, loss=2.6570675373077393
I0306 12:06:12.382811 140334130460416 logging_writer.py:48] [106000] global_step=106000, grad_norm=0.6231992840766907, loss=2.6302542686462402
I0306 12:06:47.371455 140334138853120 logging_writer.py:48] [106100] global_step=106100, grad_norm=0.6272889375686646, loss=2.5885281562805176
I0306 12:07:22.373301 140334130460416 logging_writer.py:48] [106200] global_step=106200, grad_norm=0.6198232173919678, loss=2.6237661838531494
I0306 12:07:57.370167 140334138853120 logging_writer.py:48] [106300] global_step=106300, grad_norm=0.6151195168495178, loss=2.696768283843994
I0306 12:08:32.356220 140334130460416 logging_writer.py:48] [106400] global_step=106400, grad_norm=0.6639008522033691, loss=2.6266677379608154
I0306 12:09:07.391981 140334138853120 logging_writer.py:48] [106500] global_step=106500, grad_norm=0.6415098905563354, loss=2.61492657661438
I0306 12:09:42.384472 140334130460416 logging_writer.py:48] [106600] global_step=106600, grad_norm=0.63853919506073, loss=2.618664264678955
I0306 12:10:17.371621 140334138853120 logging_writer.py:48] [106700] global_step=106700, grad_norm=0.6255031228065491, loss=2.622758150100708
I0306 12:10:52.354528 140334130460416 logging_writer.py:48] [106800] global_step=106800, grad_norm=0.6220762133598328, loss=2.5978243350982666
I0306 12:11:27.360683 140334138853120 logging_writer.py:48] [106900] global_step=106900, grad_norm=0.6220479011535645, loss=2.5770182609558105
I0306 12:12:02.324325 140334130460416 logging_writer.py:48] [107000] global_step=107000, grad_norm=0.6594210267066956, loss=2.6765568256378174
I0306 12:12:37.313013 140334138853120 logging_writer.py:48] [107100] global_step=107100, grad_norm=0.6322068572044373, loss=2.56781268119812
I0306 12:13:12.301126 140334130460416 logging_writer.py:48] [107200] global_step=107200, grad_norm=0.6480893492698669, loss=2.6412851810455322
I0306 12:13:47.321890 140334138853120 logging_writer.py:48] [107300] global_step=107300, grad_norm=0.6645144820213318, loss=2.6372122764587402
I0306 12:14:22.304406 140334130460416 logging_writer.py:48] [107400] global_step=107400, grad_norm=0.6573761701583862, loss=2.6798107624053955
I0306 12:14:57.298154 140334138853120 logging_writer.py:48] [107500] global_step=107500, grad_norm=0.6446403861045837, loss=2.654066324234009
I0306 12:15:32.284356 140334130460416 logging_writer.py:48] [107600] global_step=107600, grad_norm=0.6529931426048279, loss=2.583268642425537
I0306 12:16:07.263138 140334138853120 logging_writer.py:48] [107700] global_step=107700, grad_norm=0.6539671421051025, loss=2.662783145904541
I0306 12:16:42.232995 140334130460416 logging_writer.py:48] [107800] global_step=107800, grad_norm=0.6423430442810059, loss=2.6325876712799072
I0306 12:17:17.218560 140334138853120 logging_writer.py:48] [107900] global_step=107900, grad_norm=0.6545047760009766, loss=2.615565299987793
I0306 12:17:52.212197 140334130460416 logging_writer.py:48] [108000] global_step=108000, grad_norm=0.6627076864242554, loss=2.610074043273926
I0306 12:18:27.184995 140334138853120 logging_writer.py:48] [108100] global_step=108100, grad_norm=0.6349170804023743, loss=2.584238052368164
I0306 12:18:39.425163 140477253723328 spec.py:321] Evaluating on the training split.
I0306 12:18:42.076387 140477253723328 workload.py:181] Translating evaluation dataset.
I0306 12:21:53.101968 140477253723328 spec.py:333] Evaluating on the validation split.
I0306 12:21:55.738493 140477253723328 workload.py:181] Translating evaluation dataset.
I0306 12:24:27.519653 140477253723328 spec.py:349] Evaluating on the test split.
I0306 12:24:30.169013 140477253723328 workload.py:181] Translating evaluation dataset.
I0306 12:27:04.968937 140477253723328 submission_runner.py:469] Time since start: 62015.19s, 	Step: 108136, 	{'train/accuracy': 0.7053859233856201, 'train/loss': 1.4680662155151367, 'train/bleu': 36.207341208105355, 'validation/accuracy': 0.6936073899269104, 'validation/loss': 1.499924659729004, 'validation/bleu': 30.687767334353016, 'validation/num_examples': 3000, 'test/accuracy': 0.709894597530365, 'test/loss': 1.4120579957962036, 'test/bleu': 30.817190224813263, 'test/num_examples': 3003, 'score': 37827.04222822189, 'total_duration': 62015.1941010952, 'accumulated_submission_time': 37827.04222822189, 'accumulated_eval_time': 24180.754237651825, 'accumulated_logging_time': 1.1204121112823486}
I0306 12:27:04.988954 140334130460416 logging_writer.py:48] [108136] accumulated_eval_time=24180.8, accumulated_logging_time=1.12041, accumulated_submission_time=37827, global_step=108136, preemption_count=0, score=37827, test/accuracy=0.709895, test/bleu=30.8172, test/loss=1.41206, test/num_examples=3003, total_duration=62015.2, train/accuracy=0.705386, train/bleu=36.2073, train/loss=1.46807, validation/accuracy=0.693607, validation/bleu=30.6878, validation/loss=1.49992, validation/num_examples=3000
I0306 12:27:27.670744 140334138853120 logging_writer.py:48] [108200] global_step=108200, grad_norm=0.6463966369628906, loss=2.569897413253784
I0306 12:28:02.569266 140334130460416 logging_writer.py:48] [108300] global_step=108300, grad_norm=0.6743264198303223, loss=2.708022356033325
I0306 12:28:37.517630 140334138853120 logging_writer.py:48] [108400] global_step=108400, grad_norm=0.6541986465454102, loss=2.6842143535614014
I0306 12:29:12.457427 140334130460416 logging_writer.py:48] [108500] global_step=108500, grad_norm=0.6424441337585449, loss=2.559561014175415
I0306 12:29:47.393917 140334138853120 logging_writer.py:48] [108600] global_step=108600, grad_norm=0.6422008872032166, loss=2.6125648021698
I0306 12:30:22.360040 140334130460416 logging_writer.py:48] [108700] global_step=108700, grad_norm=0.6617267727851868, loss=2.584926128387451
I0306 12:30:57.318122 140334138853120 logging_writer.py:48] [108800] global_step=108800, grad_norm=0.6245286464691162, loss=2.5854134559631348
I0306 12:31:32.291954 140334130460416 logging_writer.py:48] [108900] global_step=108900, grad_norm=0.6481846570968628, loss=2.6354293823242188
I0306 12:32:07.233024 140334138853120 logging_writer.py:48] [109000] global_step=109000, grad_norm=0.6461177468299866, loss=2.565014123916626
I0306 12:32:42.187721 140334130460416 logging_writer.py:48] [109100] global_step=109100, grad_norm=0.6898830533027649, loss=2.6843044757843018
I0306 12:33:17.141344 140334138853120 logging_writer.py:48] [109200] global_step=109200, grad_norm=0.6515229940414429, loss=2.6254379749298096
I0306 12:33:52.106316 140334130460416 logging_writer.py:48] [109300] global_step=109300, grad_norm=0.6633198857307434, loss=2.6474127769470215
I0306 12:34:27.049951 140334138853120 logging_writer.py:48] [109400] global_step=109400, grad_norm=0.6382433772087097, loss=2.640136480331421
I0306 12:35:02.017306 140334130460416 logging_writer.py:48] [109500] global_step=109500, grad_norm=0.6570934057235718, loss=2.5628139972686768
I0306 12:35:36.961933 140334138853120 logging_writer.py:48] [109600] global_step=109600, grad_norm=0.6706383228302002, loss=2.630392074584961
I0306 12:36:11.927983 140334130460416 logging_writer.py:48] [109700] global_step=109700, grad_norm=0.6478485465049744, loss=2.6597676277160645
I0306 12:36:46.873170 140334138853120 logging_writer.py:48] [109800] global_step=109800, grad_norm=0.6515209674835205, loss=2.572929620742798
I0306 12:37:21.838193 140334130460416 logging_writer.py:48] [109900] global_step=109900, grad_norm=0.6497710347175598, loss=2.5878453254699707
I0306 12:37:56.770587 140334138853120 logging_writer.py:48] [110000] global_step=110000, grad_norm=0.6379318237304688, loss=2.5710999965667725
I0306 12:38:31.742021 140334130460416 logging_writer.py:48] [110100] global_step=110100, grad_norm=0.6494815945625305, loss=2.61704421043396
I0306 12:39:06.680324 140334138853120 logging_writer.py:48] [110200] global_step=110200, grad_norm=0.6681999564170837, loss=2.6166980266571045
I0306 12:39:41.642462 140334130460416 logging_writer.py:48] [110300] global_step=110300, grad_norm=0.7049782872200012, loss=2.6242148876190186
I0306 12:40:16.598188 140334138853120 logging_writer.py:48] [110400] global_step=110400, grad_norm=0.6619824171066284, loss=2.5194125175476074
I0306 12:40:51.533511 140334130460416 logging_writer.py:48] [110500] global_step=110500, grad_norm=0.6374627947807312, loss=2.5628583431243896
I0306 12:41:05.159316 140477253723328 spec.py:321] Evaluating on the training split.
I0306 12:41:07.806017 140477253723328 workload.py:181] Translating evaluation dataset.
I0306 12:44:06.421714 140477253723328 spec.py:333] Evaluating on the validation split.
I0306 12:44:09.066913 140477253723328 workload.py:181] Translating evaluation dataset.
I0306 12:46:52.141509 140477253723328 spec.py:349] Evaluating on the test split.
I0306 12:46:54.792311 140477253723328 workload.py:181] Translating evaluation dataset.
I0306 12:49:19.256881 140477253723328 submission_runner.py:469] Time since start: 63349.48s, 	Step: 110540, 	{'train/accuracy': 0.7011465430259705, 'train/loss': 1.4825764894485474, 'train/bleu': 35.770651307542636, 'validation/accuracy': 0.6944108009338379, 'validation/loss': 1.497456669807434, 'validation/bleu': 30.9391220769988, 'validation/num_examples': 3000, 'test/accuracy': 0.7105433940887451, 'test/loss': 1.4062724113464355, 'test/bleu': 30.880562725770037, 'test/num_examples': 3003, 'score': 38667.06465578079, 'total_duration': 63349.48203134537, 'accumulated_submission_time': 38667.06465578079, 'accumulated_eval_time': 24674.85171842575, 'accumulated_logging_time': 1.148672103881836}
I0306 12:49:19.277567 140334138853120 logging_writer.py:48] [110540] accumulated_eval_time=24674.9, accumulated_logging_time=1.14867, accumulated_submission_time=38667.1, global_step=110540, preemption_count=0, score=38667.1, test/accuracy=0.710543, test/bleu=30.8806, test/loss=1.40627, test/num_examples=3003, total_duration=63349.5, train/accuracy=0.701147, train/bleu=35.7707, train/loss=1.48258, validation/accuracy=0.694411, validation/bleu=30.9391, validation/loss=1.49746, validation/num_examples=3000
I0306 12:49:19.298729 140334130460416 logging_writer.py:48] [110540] global_step=110540, preemption_count=0, score=38667.1
I0306 12:49:19.325108 140477253723328 submission_runner.py:646] Tuning trial 1/5
I0306 12:49:19.325244 140477253723328 submission_runner.py:647] Hyperparameters: Hyperparameters(dropout_rate=0.0, label_smoothing=0.1, learning_rate=0.001308209823469072, one_minus_beta1=0.02686663061, beta2=0.9981232922116359, weight_decay=0.16375311233774334, warmup_factor=0.1)
I0306 12:49:19.327232 140477253723328 submission_runner.py:648] Metrics: {'eval_results': [(1, {'train/accuracy': 0.0006580962799489498, 'train/loss': 11.021956443786621, 'train/bleu': 0.0, 'validation/accuracy': 0.00048204089398495853, 'validation/loss': 10.987210273742676, 'validation/bleu': 0.0, 'validation/num_examples': 3000, 'test/accuracy': 0.0007183408597484231, 'test/loss': 10.986270904541016, 'test/bleu': 8.198374373802917e-11, 'test/num_examples': 3003, 'score': 26.174192667007446, 'total_duration': 952.1864182949066, 'accumulated_submission_time': 26.174192667007446, 'accumulated_eval_time': 926.0121121406555, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (2407, {'train/accuracy': 0.4233400225639343, 'train/loss': 3.8669471740722656, 'train/bleu': 14.814418358497678, 'validation/accuracy': 0.40810322761535645, 'validation/loss': 3.9832510948181152, 'validation/bleu': 10.311084591769495, 'validation/num_examples': 3000, 'test/accuracy': 0.39360445737838745, 'test/loss': 4.177699089050293, 'test/bleu': 8.527815174181656, 'test/num_examples': 3003, 'score': 866.1140217781067, 'total_duration': 2490.1526987552643, 'accumulated_submission_time': 866.1140217781067, 'accumulated_eval_time': 1623.8534841537476, 'accumulated_logging_time': 0.016554832458496094, 'global_step': 2407, 'preemption_count': 0}), (4809, {'train/accuracy': 0.5489763021469116, 'train/loss': 2.6858534812927246, 'train/bleu': 24.82935956471385, 'validation/accuracy': 0.5512199401855469, 'validation/loss': 2.637345552444458, 'validation/bleu': 20.524052804691756, 'validation/num_examples': 3000, 'test/accuracy': 0.5523346066474915, 'test/loss': 2.6668059825897217, 'test/bleu': 19.39920070901915, 'test/num_examples': 3003, 'score': 1706.0506069660187, 'total_duration': 3815.6812827587128, 'accumulated_submission_time': 1706.0506069660187, 'accumulated_eval_time': 2109.267467737198, 'accumulated_logging_time': 0.03451800346374512, 'global_step': 4809, 'preemption_count': 0}), (7213, {'train/accuracy': 0.5844482183456421, 'train/loss': 2.3212201595306396, 'train/bleu': 27.392793835247907, 'validation/accuracy': 0.5920698046684265, 'validation/loss': 2.263000249862671, 'validation/bleu': 23.550735069709237, 'validation/num_examples': 3000, 'test/accuracy': 0.5949947834014893, 'test/loss': 2.255920648574829, 'test/bleu': 22.05012883883855, 'test/num_examples': 3003, 'score': 2546.204165697098, 'total_duration': 5132.085480451584, 'accumulated_submission_time': 2546.204165697098, 'accumulated_eval_time': 2585.343838453293, 'accumulated_logging_time': 0.0526432991027832, 'global_step': 7213, 'preemption_count': 0}), (9613, {'train/accuracy': 0.5961407423019409, 'train/loss': 2.207930088043213, 'train/bleu': 28.20168804346851, 'validation/accuracy': 0.6107705235481262, 'validation/loss': 2.0926673412323, 'validation/bleu': 24.868520616467972, 'validation/num_examples': 3000, 'test/accuracy': 0.6149461269378662, 'test/loss': 2.0640909671783447, 'test/bleu': 23.480367955581055, 'test/num_examples': 3003, 'score': 3386.2726628780365, 'total_duration': 6441.72052192688, 'accumulated_submission_time': 3386.2726628780365, 'accumulated_eval_time': 3054.739854812622, 'accumulated_logging_time': 0.07064485549926758, 'global_step': 9613, 'preemption_count': 0}), (12015, {'train/accuracy': 0.6048036217689514, 'train/loss': 2.1218347549438477, 'train/bleu': 29.275157052533363, 'validation/accuracy': 0.6225372552871704, 'validation/loss': 1.9904459714889526, 'validation/bleu': 25.114439929177962, 'validation/num_examples': 3000, 'test/accuracy': 0.6314563751220703, 'test/loss': 1.9480689764022827, 'test/bleu': 25.064452307439918, 'test/num_examples': 3003, 'score': 4226.142108440399, 'total_duration': 8058.139294862747, 'accumulated_submission_time': 4226.142108440399, 'accumulated_eval_time': 3831.120766878128, 'accumulated_logging_time': 0.09091997146606445, 'global_step': 12015, 'preemption_count': 0}), (14423, {'train/accuracy': 0.6181435585021973, 'train/loss': 2.0152649879455566, 'train/bleu': 29.682170901347856, 'validation/accuracy': 0.63437819480896, 'validation/loss': 1.8945846557617188, 'validation/bleu': 26.621742874759278, 'validation/num_examples': 3000, 'test/accuracy': 0.6426138281822205, 'test/loss': 1.8424679040908813, 'test/bleu': 25.795908160612612, 'test/num_examples': 3003, 'score': 5066.309244155884, 'total_duration': 9374.569645881653, 'accumulated_submission_time': 5066.309244155884, 'accumulated_eval_time': 4307.220116853714, 'accumulated_logging_time': 0.10918354988098145, 'global_step': 14423, 'preemption_count': 0}), (16832, {'train/accuracy': 0.6170329451560974, 'train/loss': 2.0077483654022217, 'train/bleu': 30.015583654939835, 'validation/accuracy': 0.6412874460220337, 'validation/loss': 1.838304877281189, 'validation/bleu': 27.004861617891624, 'validation/num_examples': 3000, 'test/accuracy': 0.6508516073226929, 'test/loss': 1.782792568206787, 'test/bleu': 26.39376827751109, 'test/num_examples': 3003, 'score': 5906.237273693085, 'total_duration': 10668.249090909958, 'accumulated_submission_time': 5906.237273693085, 'accumulated_eval_time': 4760.815642595291, 'accumulated_logging_time': 0.12747764587402344, 'global_step': 16832, 'preemption_count': 0}), (19238, {'train/accuracy': 0.6349213719367981, 'train/loss': 1.8775442838668823, 'train/bleu': 30.845953282666983, 'validation/accuracy': 0.6467505693435669, 'validation/loss': 1.793628454208374, 'validation/bleu': 26.28721387767766, 'validation/num_examples': 3000, 'test/accuracy': 0.6559262871742249, 'test/loss': 1.7444177865982056, 'test/bleu': 26.63896667599549, 'test/num_examples': 3003, 'score': 6746.335658311844, 'total_duration': 12237.88868355751, 'accumulated_submission_time': 6746.335658311844, 'accumulated_eval_time': 5490.196926116943, 'accumulated_logging_time': 0.1474461555480957, 'global_step': 19238, 'preemption_count': 0}), (21635, {'train/accuracy': 0.6274845600128174, 'train/loss': 1.9272443056106567, 'train/bleu': 30.46928019174078, 'validation/accuracy': 0.6516327857971191, 'validation/loss': 1.7692039012908936, 'validation/bleu': 27.476626637156603, 'validation/num_examples': 3000, 'test/accuracy': 0.6590082049369812, 'test/loss': 1.7109806537628174, 'test/bleu': 26.718279936118936, 'test/num_examples': 3003, 'score': 7586.405999183655, 'total_duration': 13529.320587396622, 'accumulated_submission_time': 7586.405999183655, 'accumulated_eval_time': 5941.399101495743, 'accumulated_logging_time': 0.16770219802856445, 'global_step': 21635, 'preemption_count': 0}), (24031, {'train/accuracy': 0.6271534562110901, 'train/loss': 1.9377105236053467, 'train/bleu': 30.3128471892418, 'validation/accuracy': 0.6524608731269836, 'validation/loss': 1.7585605382919312, 'validation/bleu': 27.852425004102997, 'validation/num_examples': 3000, 'test/accuracy': 0.6621249318122864, 'test/loss': 1.6978387832641602, 'test/bleu': 27.3038982293258, 'test/num_examples': 3003, 'score': 8426.477151870728, 'total_duration': 14854.320274829865, 'accumulated_submission_time': 8426.477151870728, 'accumulated_eval_time': 6426.167286872864, 'accumulated_logging_time': 0.1870584487915039, 'global_step': 24031, 'preemption_count': 0}), (26425, {'train/accuracy': 0.6360276341438293, 'train/loss': 1.8789554834365845, 'train/bleu': 31.253834213102273, 'validation/accuracy': 0.6545373797416687, 'validation/loss': 1.7400869131088257, 'validation/bleu': 28.025504971725667, 'validation/num_examples': 3000, 'test/accuracy': 0.6632140278816223, 'test/loss': 1.694216012954712, 'test/bleu': 27.09805478876075, 'test/num_examples': 3003, 'score': 9266.333788394928, 'total_duration': 16184.199075937271, 'accumulated_submission_time': 9266.333788394928, 'accumulated_eval_time': 6916.028918266296, 'accumulated_logging_time': 0.20835328102111816, 'global_step': 26425, 'preemption_count': 0}), (28819, {'train/accuracy': 0.6348914504051208, 'train/loss': 1.8895816802978516, 'train/bleu': 30.926485427535294, 'validation/accuracy': 0.6562924981117249, 'validation/loss': 1.7297848463058472, 'validation/bleu': 28.026752332261967, 'validation/num_examples': 3000, 'test/accuracy': 0.6664001941680908, 'test/loss': 1.6742825508117676, 'test/bleu': 27.4100268203981, 'test/num_examples': 3003, 'score': 10106.439329862595, 'total_duration': 17497.809415340424, 'accumulated_submission_time': 10106.439329862595, 'accumulated_eval_time': 7389.374656677246, 'accumulated_logging_time': 0.22908878326416016, 'global_step': 28819, 'preemption_count': 0}), (31214, {'train/accuracy': 0.6358052492141724, 'train/loss': 1.880720615386963, 'train/bleu': 30.77213321212285, 'validation/accuracy': 0.6580599546432495, 'validation/loss': 1.7265146970748901, 'validation/bleu': 28.13197181209696, 'validation/num_examples': 3000, 'test/accuracy': 0.6677325963973999, 'test/loss': 1.6688928604125977, 'test/bleu': 27.72172317419608, 'test/num_examples': 3003, 'score': 10946.58758854866, 'total_duration': 18820.799479722977, 'accumulated_submission_time': 10946.58758854866, 'accumulated_eval_time': 7872.056391716003, 'accumulated_logging_time': 0.2523503303527832, 'global_step': 31214, 'preemption_count': 0}), (33622, {'train/accuracy': 0.637984573841095, 'train/loss': 1.8654241561889648, 'train/bleu': 31.043798555665944, 'validation/accuracy': 0.6600005030632019, 'validation/loss': 1.7085578441619873, 'validation/bleu': 28.40906355137768, 'validation/num_examples': 3000, 'test/accuracy': 0.6687637567520142, 'test/loss': 1.6518052816390991, 'test/bleu': 27.754809333751638, 'test/num_examples': 3003, 'score': 11786.744064331055, 'total_duration': 20180.76486158371, 'accumulated_submission_time': 11786.744064331055, 'accumulated_eval_time': 8391.701442480087, 'accumulated_logging_time': 0.2747626304626465, 'global_step': 33622, 'preemption_count': 0}), (36030, {'train/accuracy': 0.6393105983734131, 'train/loss': 1.8636890649795532, 'train/bleu': 31.0393109128669, 'validation/accuracy': 0.6589746475219727, 'validation/loss': 1.7079150676727295, 'validation/bleu': 28.32518057756314, 'validation/num_examples': 3000, 'test/accuracy': 0.6704437732696533, 'test/loss': 1.6441090106964111, 'test/bleu': 27.6287719475644, 'test/num_examples': 3003, 'score': 12626.705699205399, 'total_duration': 21489.242384433746, 'accumulated_submission_time': 12626.705699205399, 'accumulated_eval_time': 8860.058640480042, 'accumulated_logging_time': 0.2945573329925537, 'global_step': 36030, 'preemption_count': 0}), (38438, {'train/accuracy': 0.6462990641593933, 'train/loss': 1.7982689142227173, 'train/bleu': 31.557606206066044, 'validation/accuracy': 0.6599386930465698, 'validation/loss': 1.6970912218093872, 'validation/bleu': 28.303612518607455, 'validation/num_examples': 3000, 'test/accuracy': 0.6717414259910583, 'test/loss': 1.6359753608703613, 'test/bleu': 27.8366306901697, 'test/num_examples': 3003, 'score': 13466.688513994217, 'total_duration': 22806.62398481369, 'accumulated_submission_time': 13466.688513994217, 'accumulated_eval_time': 9337.298724651337, 'accumulated_logging_time': 0.31583213806152344, 'global_step': 38438, 'preemption_count': 0}), (40842, {'train/accuracy': 0.6398757100105286, 'train/loss': 1.8562971353530884, 'train/bleu': 31.365026029571105, 'validation/accuracy': 0.6622623801231384, 'validation/loss': 1.691129207611084, 'validation/bleu': 28.507663354102036, 'validation/num_examples': 3000, 'test/accuracy': 0.6728188991546631, 'test/loss': 1.6301780939102173, 'test/bleu': 27.910512089367185, 'test/num_examples': 3003, 'score': 14306.614209413528, 'total_duration': 24129.351665496826, 'accumulated_submission_time': 14306.614209413528, 'accumulated_eval_time': 9819.938004732132, 'accumulated_logging_time': 0.33740925788879395, 'global_step': 40842, 'preemption_count': 0}), (43244, {'train/accuracy': 0.6431131362915039, 'train/loss': 1.8359999656677246, 'train/bleu': 31.52353642299674, 'validation/accuracy': 0.6625590324401855, 'validation/loss': 1.677484393119812, 'validation/bleu': 28.36334018882698, 'validation/num_examples': 3000, 'test/accuracy': 0.6744177937507629, 'test/loss': 1.6152924299240112, 'test/bleu': 28.371843724639163, 'test/num_examples': 3003, 'score': 15146.601977586746, 'total_duration': 25450.153121709824, 'accumulated_submission_time': 15146.601977586746, 'accumulated_eval_time': 10300.590491056442, 'accumulated_logging_time': 0.3592197895050049, 'global_step': 43244, 'preemption_count': 0}), (45648, {'train/accuracy': 0.6492317318916321, 'train/loss': 1.7921429872512817, 'train/bleu': 31.503788220257764, 'validation/accuracy': 0.6653524041175842, 'validation/loss': 1.6730129718780518, 'validation/bleu': 28.700134178939177, 'validation/num_examples': 3000, 'test/accuracy': 0.6756691336631775, 'test/loss': 1.6119084358215332, 'test/bleu': 27.8786194682226, 'test/num_examples': 3003, 'score': 15986.579032182693, 'total_duration': 26788.51887845993, 'accumulated_submission_time': 15986.579032182693, 'accumulated_eval_time': 10798.817275047302, 'accumulated_logging_time': 0.3810243606567383, 'global_step': 45648, 'preemption_count': 0}), (48052, {'train/accuracy': 0.6454460024833679, 'train/loss': 1.8072640895843506, 'train/bleu': 31.672306378050664, 'validation/accuracy': 0.6650928258895874, 'validation/loss': 1.6679704189300537, 'validation/bleu': 28.72303339205247, 'validation/num_examples': 3000, 'test/accuracy': 0.6774302124977112, 'test/loss': 1.5986354351043701, 'test/bleu': 28.356947926034227, 'test/num_examples': 3003, 'score': 16826.62109518051, 'total_duration': 28125.73698782921, 'accumulated_submission_time': 16826.62109518051, 'accumulated_eval_time': 11295.83495926857, 'accumulated_logging_time': 0.40302348136901855, 'global_step': 48052, 'preemption_count': 0}), (50454, {'train/accuracy': 0.6673710346221924, 'train/loss': 1.6674065589904785, 'train/bleu': 32.67558024004903, 'validation/accuracy': 0.6685536503791809, 'validation/loss': 1.653506875038147, 'validation/bleu': 27.447097335472453, 'validation/num_examples': 3000, 'test/accuracy': 0.6776850819587708, 'test/loss': 1.59489905834198, 'test/bleu': 28.017952719907147, 'test/num_examples': 3003, 'score': 17666.533705711365, 'total_duration': 29759.886680841446, 'accumulated_submission_time': 17666.533705711365, 'accumulated_eval_time': 12089.916955471039, 'accumulated_logging_time': 0.42466115951538086, 'global_step': 50454, 'preemption_count': 0}), (52858, {'train/accuracy': 0.6457720398902893, 'train/loss': 1.8041551113128662, 'train/bleu': 32.13398876817845, 'validation/accuracy': 0.6687266826629639, 'validation/loss': 1.6477055549621582, 'validation/bleu': 29.055083924925675, 'validation/num_examples': 3000, 'test/accuracy': 0.6800255179405212, 'test/loss': 1.587914228439331, 'test/bleu': 28.627580162824685, 'test/num_examples': 3003, 'score': 18506.433409929276, 'total_duration': 31067.572167396545, 'accumulated_submission_time': 18506.433409929276, 'accumulated_eval_time': 12557.541586637497, 'accumulated_logging_time': 0.44739747047424316, 'global_step': 52858, 'preemption_count': 0}), (55262, {'train/accuracy': 0.6467801332473755, 'train/loss': 1.810073733329773, 'train/bleu': 31.82863315701233, 'validation/accuracy': 0.6691840291023254, 'validation/loss': 1.644310474395752, 'validation/bleu': 29.049534546047454, 'validation/num_examples': 3000, 'test/accuracy': 0.6798169612884521, 'test/loss': 1.5782716274261475, 'test/bleu': 28.35553455257382, 'test/num_examples': 3003, 'score': 19346.4083006382, 'total_duration': 32433.10134243965, 'accumulated_submission_time': 19346.4083006382, 'accumulated_eval_time': 13082.936410665512, 'accumulated_logging_time': 0.4701051712036133, 'global_step': 55262, 'preemption_count': 0}), (57667, {'train/accuracy': 0.6530262231826782, 'train/loss': 1.759045124053955, 'train/bleu': 32.16294794604371, 'validation/accuracy': 0.6700615882873535, 'validation/loss': 1.6389081478118896, 'validation/bleu': 29.339930589944416, 'validation/num_examples': 3000, 'test/accuracy': 0.6838952898979187, 'test/loss': 1.5656360387802124, 'test/bleu': 29.069460312787662, 'test/num_examples': 3003, 'score': 20186.262072086334, 'total_duration': 33806.72999358177, 'accumulated_submission_time': 20186.262072086334, 'accumulated_eval_time': 13616.550857067108, 'accumulated_logging_time': 0.4946565628051758, 'global_step': 57667, 'preemption_count': 0}), (60072, {'train/accuracy': 0.6512733101844788, 'train/loss': 1.7800978422164917, 'train/bleu': 31.71721463585255, 'validation/accuracy': 0.6721380352973938, 'validation/loss': 1.6330503225326538, 'validation/bleu': 29.271075281230264, 'validation/num_examples': 3000, 'test/accuracy': 0.6858185529708862, 'test/loss': 1.5646239519119263, 'test/bleu': 28.500827428543484, 'test/num_examples': 3003, 'score': 21026.146174907684, 'total_duration': 35125.07299160957, 'accumulated_submission_time': 21026.146174907684, 'accumulated_eval_time': 14094.848971128464, 'accumulated_logging_time': 0.5200779438018799, 'global_step': 60072, 'preemption_count': 0}), (62479, {'train/accuracy': 0.6469464898109436, 'train/loss': 1.7981781959533691, 'train/bleu': 31.962317196838143, 'validation/accuracy': 0.6721627712249756, 'validation/loss': 1.6219220161437988, 'validation/bleu': 29.09704793442837, 'validation/num_examples': 3000, 'test/accuracy': 0.6850423216819763, 'test/loss': 1.5514124631881714, 'test/bleu': 28.307149418625894, 'test/num_examples': 3003, 'score': 21866.178804159164, 'total_duration': 36433.408490896225, 'accumulated_submission_time': 21866.178804159164, 'accumulated_eval_time': 14562.993886470795, 'accumulated_logging_time': 0.5434150695800781, 'global_step': 62479, 'preemption_count': 0}), (64884, {'train/accuracy': 0.6581771373748779, 'train/loss': 1.7254279851913452, 'train/bleu': 32.19322724442931, 'validation/accuracy': 0.6744864583015442, 'validation/loss': 1.614680528640747, 'validation/bleu': 29.43092399054622, 'validation/num_examples': 3000, 'test/accuracy': 0.6856911182403564, 'test/loss': 1.5491634607315063, 'test/bleu': 28.730474497998962, 'test/num_examples': 3003, 'score': 22706.31214284897, 'total_duration': 37766.50312590599, 'accumulated_submission_time': 22706.31214284897, 'accumulated_eval_time': 15055.797522306442, 'accumulated_logging_time': 0.5674965381622314, 'global_step': 64884, 'preemption_count': 0}), (67288, {'train/accuracy': 0.6542203426361084, 'train/loss': 1.75526762008667, 'train/bleu': 32.21505873333365, 'validation/accuracy': 0.6738437414169312, 'validation/loss': 1.6092305183410645, 'validation/bleu': 29.397676926702164, 'validation/num_examples': 3000, 'test/accuracy': 0.6885760426521301, 'test/loss': 1.5352171659469604, 'test/bleu': 29.04536890698128, 'test/num_examples': 3003, 'score': 23546.45552754402, 'total_duration': 39090.996404886246, 'accumulated_submission_time': 23546.45552754402, 'accumulated_eval_time': 15539.990818738937, 'accumulated_logging_time': 0.5909633636474609, 'global_step': 67288, 'preemption_count': 0}), (69692, {'train/accuracy': 0.6704314947128296, 'train/loss': 1.6495107412338257, 'train/bleu': 33.30320291965666, 'validation/accuracy': 0.6758831143379211, 'validation/loss': 1.6042044162750244, 'validation/bleu': 29.661690041680654, 'validation/num_examples': 3000, 'test/accuracy': 0.6888541579246521, 'test/loss': 1.532083511352539, 'test/bleu': 28.95712420225892, 'test/num_examples': 3003, 'score': 24386.444160938263, 'total_duration': 40484.66174077988, 'accumulated_submission_time': 24386.444160938263, 'accumulated_eval_time': 16093.508858680725, 'accumulated_logging_time': 0.6154155731201172, 'global_step': 69692, 'preemption_count': 0}), (72096, {'train/accuracy': 0.6583915948867798, 'train/loss': 1.7199987173080444, 'train/bleu': 32.7337992172957, 'validation/accuracy': 0.6770697236061096, 'validation/loss': 1.5976604223251343, 'validation/bleu': 29.66415126409923, 'validation/num_examples': 3000, 'test/accuracy': 0.691067099571228, 'test/loss': 1.5227396488189697, 'test/bleu': 29.729226032439612, 'test/num_examples': 3003, 'score': 25226.611978530884, 'total_duration': 41828.67882490158, 'accumulated_submission_time': 25226.611978530884, 'accumulated_eval_time': 16597.19686627388, 'accumulated_logging_time': 0.6409029960632324, 'global_step': 72096, 'preemption_count': 0}), (74490, {'train/accuracy': 0.6592912673950195, 'train/loss': 1.7244094610214233, 'train/bleu': 32.69329684068829, 'validation/accuracy': 0.6790226101875305, 'validation/loss': 1.584033489227295, 'validation/bleu': 29.671339006770474, 'validation/num_examples': 3000, 'test/accuracy': 0.6929324865341187, 'test/loss': 1.5079373121261597, 'test/bleu': 29.323018711417834, 'test/num_examples': 3003, 'score': 26066.474137544632, 'total_duration': 43166.64326453209, 'accumulated_submission_time': 26066.474137544632, 'accumulated_eval_time': 17095.063010931015, 'accumulated_logging_time': 0.7395486831665039, 'global_step': 74490, 'preemption_count': 0}), (76892, {'train/accuracy': 0.668889045715332, 'train/loss': 1.66038978099823, 'train/bleu': 33.3143746828012, 'validation/accuracy': 0.6800732016563416, 'validation/loss': 1.579336166381836, 'validation/bleu': 29.733642603124878, 'validation/num_examples': 3000, 'test/accuracy': 0.6951801776885986, 'test/loss': 1.4990752935409546, 'test/bleu': 29.56411755480837, 'test/num_examples': 3003, 'score': 26906.63858628273, 'total_duration': 44497.72993302345, 'accumulated_submission_time': 26906.63858628273, 'accumulated_eval_time': 17585.82488155365, 'accumulated_logging_time': 0.7651686668395996, 'global_step': 76892, 'preemption_count': 0}), (79296, {'train/accuracy': 0.6662702560424805, 'train/loss': 1.6860483884811401, 'train/bleu': 32.861731673712285, 'validation/accuracy': 0.6805304884910583, 'validation/loss': 1.574438214302063, 'validation/bleu': 29.779761244354397, 'validation/num_examples': 3000, 'test/accuracy': 0.6960722804069519, 'test/loss': 1.4949110746383667, 'test/bleu': 29.609990715056238, 'test/num_examples': 3003, 'score': 27746.616654872894, 'total_duration': 45833.251322984695, 'accumulated_submission_time': 27746.616654872894, 'accumulated_eval_time': 18081.20722436905, 'accumulated_logging_time': 0.7908661365509033, 'global_step': 79296, 'preemption_count': 0}), (81698, {'train/accuracy': 0.6977536678314209, 'train/loss': 1.502886176109314, 'train/bleu': 34.92097169970208, 'validation/accuracy': 0.6824215650558472, 'validation/loss': 1.563807725906372, 'validation/bleu': 29.71707111037867, 'validation/num_examples': 3000, 'test/accuracy': 0.697404682636261, 'test/loss': 1.4829705953598022, 'test/bleu': 30.0111260470247, 'test/num_examples': 3003, 'score': 28586.533041715622, 'total_duration': 47188.34877991676, 'accumulated_submission_time': 28586.533041715622, 'accumulated_eval_time': 18596.224978923798, 'accumulated_logging_time': 0.8183484077453613, 'global_step': 81698, 'preemption_count': 0}), (84094, {'train/accuracy': 0.67063307762146, 'train/loss': 1.6480712890625, 'train/bleu': 33.50119627012239, 'validation/accuracy': 0.6833485960960388, 'validation/loss': 1.5514763593673706, 'validation/bleu': 30.033434522788504, 'validation/num_examples': 3000, 'test/accuracy': 0.7003591656684875, 'test/loss': 1.466897964477539, 'test/bleu': 29.899622824104352, 'test/num_examples': 3003, 'score': 29426.624901533127, 'total_duration': 48535.609925985336, 'accumulated_submission_time': 29426.624901533127, 'accumulated_eval_time': 19103.230615615845, 'accumulated_logging_time': 0.8450629711151123, 'global_step': 84094, 'preemption_count': 0}), (86492, {'train/accuracy': 0.6687280535697937, 'train/loss': 1.6682251691818237, 'train/bleu': 33.48890700580598, 'validation/accuracy': 0.6860430836677551, 'validation/loss': 1.547324776649475, 'validation/bleu': 30.45131823230902, 'validation/num_examples': 3000, 'test/accuracy': 0.6998725533485413, 'test/loss': 1.4708218574523926, 'test/bleu': 29.852259755973133, 'test/num_examples': 3003, 'score': 30266.771779060364, 'total_duration': 49912.10389328003, 'accumulated_submission_time': 30266.771779060364, 'accumulated_eval_time': 19639.410803318024, 'accumulated_logging_time': 0.8727223873138428, 'global_step': 86492, 'preemption_count': 0}), (88895, {'train/accuracy': 0.6845171451568604, 'train/loss': 1.5698010921478271, 'train/bleu': 34.454155886492245, 'validation/accuracy': 0.686302661895752, 'validation/loss': 1.5376595258712769, 'validation/bleu': 30.13079544998059, 'validation/num_examples': 3000, 'test/accuracy': 0.7016452550888062, 'test/loss': 1.4551117420196533, 'test/bleu': 30.081749734651165, 'test/num_examples': 3003, 'score': 31106.67918395996, 'total_duration': 51248.95845770836, 'accumulated_submission_time': 31106.67918395996, 'accumulated_eval_time': 20136.194087028503, 'accumulated_logging_time': 0.8988285064697266, 'global_step': 88895, 'preemption_count': 0}), (91300, {'train/accuracy': 0.6783825159072876, 'train/loss': 1.6039488315582275, 'train/bleu': 33.75529584910428, 'validation/accuracy': 0.6886757612228394, 'validation/loss': 1.5309007167816162, 'validation/bleu': 30.388817109752456, 'validation/num_examples': 3000, 'test/accuracy': 0.703649640083313, 'test/loss': 1.4468092918395996, 'test/bleu': 30.331338156440417, 'test/num_examples': 3003, 'score': 31946.84950709343, 'total_duration': 52592.41937613487, 'accumulated_submission_time': 31946.84950709343, 'accumulated_eval_time': 20639.322226524353, 'accumulated_logging_time': 0.926297664642334, 'global_step': 91300, 'preemption_count': 0}), (93703, {'train/accuracy': 0.679144024848938, 'train/loss': 1.5999244451522827, 'train/bleu': 34.07129018170136, 'validation/accuracy': 0.6888982653617859, 'validation/loss': 1.527034044265747, 'validation/bleu': 30.415390972821445, 'validation/num_examples': 3000, 'test/accuracy': 0.7054454684257507, 'test/loss': 1.4403575658798218, 'test/bleu': 30.184775646078077, 'test/num_examples': 3003, 'score': 32786.92387294769, 'total_duration': 53938.485255002975, 'accumulated_submission_time': 32786.92387294769, 'accumulated_eval_time': 21145.14769220352, 'accumulated_logging_time': 0.9554259777069092, 'global_step': 93703, 'preemption_count': 0}), (96112, {'train/accuracy': 0.6892513036727905, 'train/loss': 1.54386305809021, 'train/bleu': 34.871488065666085, 'validation/accuracy': 0.689800500869751, 'validation/loss': 1.521233320236206, 'validation/bleu': 30.34727694356915, 'validation/num_examples': 3000, 'test/accuracy': 0.7061638236045837, 'test/loss': 1.437849521636963, 'test/bleu': 30.433286774467756, 'test/num_examples': 3003, 'score': 33626.893287181854, 'total_duration': 55252.2042016983, 'accumulated_submission_time': 33626.893287181854, 'accumulated_eval_time': 21618.736565351486, 'accumulated_logging_time': 0.9815571308135986, 'global_step': 96112, 'preemption_count': 0}), (98521, {'train/accuracy': 0.6882555484771729, 'train/loss': 1.551037311553955, 'train/bleu': 34.89218112133462, 'validation/accuracy': 0.6907893419265747, 'validation/loss': 1.5171523094177246, 'validation/bleu': 30.46519660724104, 'validation/num_examples': 3000, 'test/accuracy': 0.7061870098114014, 'test/loss': 1.4302724599838257, 'test/bleu': 30.5430962491934, 'test/num_examples': 3003, 'score': 34467.07543492317, 'total_duration': 56622.10461521149, 'accumulated_submission_time': 34467.07543492317, 'accumulated_eval_time': 22148.290571689606, 'accumulated_logging_time': 1.0089812278747559, 'global_step': 98521, 'preemption_count': 0}), (100929, {'train/accuracy': 0.7020270228385925, 'train/loss': 1.4744031429290771, 'train/bleu': 36.28851876574889, 'validation/accuracy': 0.6913949847221375, 'validation/loss': 1.5133466720581055, 'validation/bleu': 30.60845847297415, 'validation/num_examples': 3000, 'test/accuracy': 0.7073572278022766, 'test/loss': 1.4281712770462036, 'test/bleu': 30.62010006542814, 'test/num_examples': 3003, 'score': 35307.03855133057, 'total_duration': 57986.67894530296, 'accumulated_submission_time': 35307.03855133057, 'accumulated_eval_time': 22672.73842072487, 'accumulated_logging_time': 1.036452293395996, 'global_step': 100929, 'preemption_count': 0}), (103332, {'train/accuracy': 0.6952316761016846, 'train/loss': 1.5049054622650146, 'train/bleu': 35.260961121863694, 'validation/accuracy': 0.6928781867027283, 'validation/loss': 1.5057975053787231, 'validation/bleu': 30.746985013389033, 'validation/num_examples': 3000, 'test/accuracy': 0.7093616127967834, 'test/loss': 1.417487621307373, 'test/bleu': 30.697278718913463, 'test/num_examples': 3003, 'score': 36147.04745411873, 'total_duration': 59319.06286215782, 'accumulated_submission_time': 36147.04745411873, 'accumulated_eval_time': 23164.950659036636, 'accumulated_logging_time': 1.063760757446289, 'global_step': 103332, 'preemption_count': 0}), (105735, {'train/accuracy': 0.6962881684303284, 'train/loss': 1.4978196620941162, 'train/bleu': 35.346667011153265, 'validation/accuracy': 0.6933602094650269, 'validation/loss': 1.5046344995498657, 'validation/bleu': 30.820000096291395, 'validation/num_examples': 3000, 'test/accuracy': 0.7091298699378967, 'test/loss': 1.4145933389663696, 'test/bleu': 30.652485975793297, 'test/num_examples': 3003, 'score': 36987.18598008156, 'total_duration': 60669.628366947174, 'accumulated_submission_time': 36987.18598008156, 'accumulated_eval_time': 23675.21053314209, 'accumulated_logging_time': 1.0919380187988281, 'global_step': 105735, 'preemption_count': 0}), (108136, {'train/accuracy': 0.7053859233856201, 'train/loss': 1.4680662155151367, 'train/bleu': 36.207341208105355, 'validation/accuracy': 0.6936073899269104, 'validation/loss': 1.499924659729004, 'validation/bleu': 30.687767334353016, 'validation/num_examples': 3000, 'test/accuracy': 0.709894597530365, 'test/loss': 1.4120579957962036, 'test/bleu': 30.817190224813263, 'test/num_examples': 3003, 'score': 37827.04222822189, 'total_duration': 62015.1941010952, 'accumulated_submission_time': 37827.04222822189, 'accumulated_eval_time': 24180.754237651825, 'accumulated_logging_time': 1.1204121112823486, 'global_step': 108136, 'preemption_count': 0}), (110540, {'train/accuracy': 0.7011465430259705, 'train/loss': 1.4825764894485474, 'train/bleu': 35.770651307542636, 'validation/accuracy': 0.6944108009338379, 'validation/loss': 1.497456669807434, 'validation/bleu': 30.9391220769988, 'validation/num_examples': 3000, 'test/accuracy': 0.7105433940887451, 'test/loss': 1.4062724113464355, 'test/bleu': 30.880562725770037, 'test/num_examples': 3003, 'score': 38667.06465578079, 'total_duration': 63349.48203134537, 'accumulated_submission_time': 38667.06465578079, 'accumulated_eval_time': 24674.85171842575, 'accumulated_logging_time': 1.148672103881836, 'global_step': 110540, 'preemption_count': 0})], 'global_step': 110540}
I0306 12:49:19.327336 140477253723328 submission_runner.py:649] Timing: 38667.06465578079
I0306 12:49:19.327370 140477253723328 submission_runner.py:651] Total number of evals: 47
I0306 12:49:19.327396 140477253723328 submission_runner.py:652] ====================
I0306 12:49:19.327519 140477253723328 submission_runner.py:750] Final wmt score: 0
