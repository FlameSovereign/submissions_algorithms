python submission_runner.py --framework=jax --workload=wmt --submission_path=prize_qualification_baselines/external_tuning/jax_nadamw_full_budget.py --data_dir=/data/wmt --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=submissions/rolling_leaderboard/external_tuning/baseline/study_1 --overwrite=True --save_checkpoints=False --rng_seed=4463175 --tuning_ruleset=external --tuning_search_space=prize_qualification_baselines/external_tuning/tuning_search_space.json --num_tuning_trials=5 --hparam_start_index=1 --hparam_end_index=2 2>&1 | tee -a /logs/wmt_jax_03-05-2025-19-12-04.log
2025-03-05 19:12:05.705984: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1741201925.728391       9 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1741201925.735417       9 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
I0305 19:12:12.584302 139902437545152 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_1/wmt_jax.
I0305 19:12:13.625904 139902437545152 xla_bridge.py:884] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA
I0305 19:12:13.629748 139902437545152 xla_bridge.py:884] Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory
I0305 19:12:13.631408 139902437545152 submission_runner.py:606] Using RNG seed 4463175
I0305 19:12:14.239070 139902437545152 submission_runner.py:615] --- Tuning run 2/5 ---
I0305 19:12:14.239275 139902437545152 submission_runner.py:620] Creating tuning directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_1/wmt_jax/trial_2.
I0305 19:12:14.239454 139902437545152 logger_utils.py:97] Saving hparams to /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_1/wmt_jax/trial_2/hparams.json.
I0305 19:12:14.478540 139902437545152 submission_runner.py:218] Initializing dataset.
I0305 19:12:14.652579 139902437545152 dataset_info.py:690] Load dataset info from /data/wmt/wmt17_translate/de-en/1.0.0
I0305 19:12:14.658717 139902437545152 reader.py:261] Creating a tf.data.Dataset reading 16 files located in folders: /data/wmt/wmt17_translate/de-en/1.0.0.
I0305 19:12:14.739428 139902437545152 logging_logger.py:49] Constructing tf.data.Dataset wmt17_translate for split train, from /data/wmt/wmt17_translate/de-en/1.0.0
I0305 19:12:16.084528 139902437545152 submission_runner.py:229] Initializing model.
I0305 19:12:57.755587 139902437545152 submission_runner.py:272] Initializing optimizer.
I0305 19:12:58.616090 139902437545152 submission_runner.py:279] Initializing metrics bundle.
I0305 19:12:58.616339 139902437545152 submission_runner.py:301] Initializing checkpoint and logger.
I0305 19:12:58.617469 139902437545152 checkpoints.py:1101] Found no checkpoint files in /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_1/wmt_jax/trial_2 with prefix checkpoint_
I0305 19:12:58.617575 139902437545152 submission_runner.py:321] Saving meta data to /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_1/wmt_jax/trial_2/meta_data_0.json.
I0305 19:12:58.617752 139902437545152 logger_utils.py:262] Unable to record workload.train_mean information. Continuing without it.
I0305 19:12:58.617799 139902437545152 logger_utils.py:262] Unable to record workload.train_stddev information. Continuing without it.
I0305 19:12:58.801581 139902437545152 submission_runner.py:325] Saving flags to /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_1/wmt_jax/trial_2/flags_0.json.
I0305 19:12:58.838985 139902437545152 submission_runner.py:337] Starting training loop.
I0305 19:13:24.669590 139766272022272 logging_writer.py:48] [0] global_step=0, grad_norm=5.208498001098633, loss=11.106546401977539
I0305 19:13:24.728572 139902437545152 spec.py:321] Evaluating on the training split.
I0305 19:13:24.730989 139902437545152 dataset_info.py:690] Load dataset info from /data/wmt/wmt17_translate/de-en/1.0.0
I0305 19:13:24.734446 139902437545152 reader.py:261] Creating a tf.data.Dataset reading 16 files located in folders: /data/wmt/wmt17_translate/de-en/1.0.0.
I0305 19:13:24.766159 139902437545152 logging_logger.py:49] Constructing tf.data.Dataset wmt17_translate for split train, from /data/wmt/wmt17_translate/de-en/1.0.0
I0305 19:13:30.908128 139902437545152 workload.py:181] Translating evaluation dataset.
I0305 19:18:35.477403 139902437545152 spec.py:333] Evaluating on the validation split.
I0305 19:18:35.542356 139902437545152 dataset_info.py:690] Load dataset info from /data/wmt/wmt14_translate/de-en/1.0.0
I0305 19:18:35.571783 139902437545152 reader.py:261] Creating a tf.data.Dataset reading 1 files located in folders: /data/wmt/wmt14_translate/de-en/1.0.0.
I0305 19:18:35.603768 139902437545152 logging_logger.py:49] Constructing tf.data.Dataset wmt14_translate for split validation, from /data/wmt/wmt14_translate/de-en/1.0.0
I0305 19:18:40.800826 139902437545152 workload.py:181] Translating evaluation dataset.
I0305 19:23:39.504935 139902437545152 spec.py:349] Evaluating on the test split.
I0305 19:23:39.507580 139902437545152 dataset_info.py:690] Load dataset info from /data/wmt/wmt14_translate/de-en/1.0.0
I0305 19:23:39.510964 139902437545152 reader.py:261] Creating a tf.data.Dataset reading 1 files located in folders: /data/wmt/wmt14_translate/de-en/1.0.0.
I0305 19:23:39.544248 139902437545152 logging_logger.py:49] Constructing tf.data.Dataset wmt14_translate for split test, from /data/wmt/wmt14_translate/de-en/1.0.0
I0305 19:23:42.328090 139902437545152 workload.py:181] Translating evaluation dataset.
I0305 19:28:40.515949 139902437545152 submission_runner.py:469] Time since start: 941.68s, 	Step: 1, 	{'train/accuracy': 0.0006909416988492012, 'train/loss': 11.127187728881836, 'train/bleu': 0.0, 'validation/accuracy': 0.00048204089398495853, 'validation/loss': 11.128292083740234, 'validation/bleu': 0.0, 'validation/num_examples': 3000, 'test/accuracy': 0.0007183408597484231, 'test/loss': 11.135275840759277, 'test/bleu': 0.0, 'test/num_examples': 3003, 'score': 25.889487981796265, 'total_duration': 941.6768958568573, 'accumulated_submission_time': 25.889487981796265, 'accumulated_eval_time': 915.7872984409332, 'accumulated_logging_time': 0}
I0305 19:28:40.523214 139759275980544 logging_writer.py:48] [1] accumulated_eval_time=915.787, accumulated_logging_time=0, accumulated_submission_time=25.8895, global_step=1, preemption_count=0, score=25.8895, test/accuracy=0.000718341, test/bleu=0, test/loss=11.1353, test/num_examples=3003, total_duration=941.677, train/accuracy=0.000690942, train/bleu=0, train/loss=11.1272, validation/accuracy=0.000482041, validation/bleu=0, validation/loss=11.1283, validation/num_examples=3000
I0305 19:29:15.109783 139759267587840 logging_writer.py:48] [100] global_step=100, grad_norm=0.25009462237358093, loss=9.013716697692871
I0305 19:29:49.603820 139759275980544 logging_writer.py:48] [200] global_step=200, grad_norm=0.4466525614261627, loss=8.674158096313477
I0305 19:30:24.102874 139759267587840 logging_writer.py:48] [300] global_step=300, grad_norm=0.6844770312309265, loss=8.239631652832031
I0305 19:30:58.607866 139759275980544 logging_writer.py:48] [400] global_step=400, grad_norm=1.067602276802063, loss=8.069991111755371
I0305 19:31:33.153918 139759267587840 logging_writer.py:48] [500] global_step=500, grad_norm=0.48896172642707825, loss=7.75026273727417
I0305 19:32:07.656728 139759275980544 logging_writer.py:48] [600] global_step=600, grad_norm=0.617097795009613, loss=7.571043968200684
I0305 19:32:42.187007 139759267587840 logging_writer.py:48] [700] global_step=700, grad_norm=0.6055870652198792, loss=7.296823501586914
I0305 19:33:16.700828 139759275980544 logging_writer.py:48] [800] global_step=800, grad_norm=0.5185383558273315, loss=7.191039085388184
I0305 19:33:51.242155 139759267587840 logging_writer.py:48] [900] global_step=900, grad_norm=0.5692044496536255, loss=7.036921501159668
I0305 19:34:25.765000 139759275980544 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.5135598182678223, loss=6.885849475860596
I0305 19:35:00.328932 139759267587840 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.6323511600494385, loss=6.773916244506836
I0305 19:35:34.869719 139759275980544 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.6062461137771606, loss=6.617455005645752
I0305 19:36:09.445601 139759267587840 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.545397937297821, loss=6.513472557067871
I0305 19:36:43.987737 139759275980544 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.5492605566978455, loss=6.457320690155029
I0305 19:37:18.508712 139759267587840 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.5996434092521667, loss=6.2808098793029785
I0305 19:37:53.029893 139759275980544 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.6765326261520386, loss=6.229276657104492
I0305 19:38:27.566979 139759267587840 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.9653973579406738, loss=6.152785778045654
I0305 19:39:02.118248 139759275980544 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.7986388206481934, loss=6.109157562255859
I0305 19:39:36.638362 139759267587840 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.6053870916366577, loss=5.916371822357178
I0305 19:40:11.159656 139759275980544 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.5615870356559753, loss=5.815826892852783
I0305 19:40:45.678768 139759267587840 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.9198346734046936, loss=5.816925525665283
I0305 19:41:20.211687 139759275980544 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.7513967156410217, loss=5.7013421058654785
I0305 19:41:54.733123 139759267587840 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.5428608059883118, loss=5.530777454376221
I0305 19:42:29.340419 139759275980544 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.5674049854278564, loss=5.481256484985352
I0305 19:42:40.739844 139902437545152 spec.py:321] Evaluating on the training split.
I0305 19:42:43.355395 139902437545152 workload.py:181] Translating evaluation dataset.
I0305 19:46:40.471724 139902437545152 spec.py:333] Evaluating on the validation split.
I0305 19:46:43.088143 139902437545152 workload.py:181] Translating evaluation dataset.
I0305 19:50:36.289725 139902437545152 spec.py:349] Evaluating on the test split.
I0305 19:50:38.896929 139902437545152 workload.py:181] Translating evaluation dataset.
I0305 19:54:34.873357 139902437545152 submission_runner.py:469] Time since start: 2496.03s, 	Step: 2434, 	{'train/accuracy': 0.4387555420398712, 'train/loss': 3.780032157897949, 'train/bleu': 17.282684711147255, 'validation/accuracy': 0.4347885251045227, 'validation/loss': 3.847177743911743, 'validation/bleu': 12.865042631562853, 'validation/num_examples': 3000, 'test/accuracy': 0.42230331897735596, 'test/loss': 4.007822036743164, 'test/bleu': 10.885582125641006, 'test/num_examples': 3003, 'score': 865.9368352890015, 'total_duration': 2496.0343101024628, 'accumulated_submission_time': 865.9368352890015, 'accumulated_eval_time': 1629.9207653999329, 'accumulated_logging_time': 0.015886306762695312}
I0305 19:54:34.883257 139758915258112 logging_writer.py:48] [2434] accumulated_eval_time=1629.92, accumulated_logging_time=0.0158863, accumulated_submission_time=865.937, global_step=2434, preemption_count=0, score=865.937, test/accuracy=0.422303, test/bleu=10.8856, test/loss=4.00782, test/num_examples=3003, total_duration=2496.03, train/accuracy=0.438756, train/bleu=17.2827, train/loss=3.78003, validation/accuracy=0.434789, validation/bleu=12.865, validation/loss=3.84718, validation/num_examples=3000
I0305 19:54:58.022117 139758906865408 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.6452799439430237, loss=5.467507362365723
I0305 19:55:32.574297 139758915258112 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.6243512034416199, loss=5.39115571975708
I0305 19:56:07.144359 139758906865408 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.6805566549301147, loss=5.2991156578063965
I0305 19:56:41.698005 139758915258112 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.5049130320549011, loss=5.340844631195068
I0305 19:57:16.265115 139758906865408 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.4755181074142456, loss=5.208634376525879
I0305 19:57:50.806760 139758915258112 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.5159488320350647, loss=5.1194963455200195
I0305 19:58:25.358995 139758906865408 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.4510622024536133, loss=5.071686267852783
I0305 19:58:59.907804 139758915258112 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.4885358214378357, loss=5.086427688598633
I0305 19:59:34.460675 139758906865408 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.4310464560985565, loss=4.979575157165527
I0305 20:00:09.015728 139758915258112 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.42181310057640076, loss=4.973171234130859
I0305 20:00:43.606308 139758906865408 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.5211813449859619, loss=4.9539008140563965
I0305 20:01:18.151360 139758915258112 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.46786361932754517, loss=4.9692254066467285
I0305 20:01:52.682358 139758906865408 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.45880305767059326, loss=4.8838114738464355
I0305 20:02:27.199479 139758915258112 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.4109726846218109, loss=4.890844345092773
I0305 20:03:01.701839 139758906865408 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.3754826784133911, loss=4.831906318664551
I0305 20:03:36.243978 139758915258112 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.3812780976295471, loss=4.829996109008789
I0305 20:04:10.778405 139758906865408 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.3497161269187927, loss=4.797786712646484
I0305 20:04:45.328274 139758915258112 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.4161717891693115, loss=4.849655628204346
I0305 20:05:19.874116 139758906865408 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.35712647438049316, loss=4.778602123260498
I0305 20:05:54.406265 139758915258112 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.45380541682243347, loss=4.747279167175293
I0305 20:06:28.932085 139758906865408 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.3566421866416931, loss=4.7446208000183105
I0305 20:07:03.464434 139758915258112 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.4213395118713379, loss=4.7464823722839355
I0305 20:07:37.975540 139758906865408 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.35062268376350403, loss=4.679818630218506
I0305 20:08:12.507318 139758915258112 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.35724931955337524, loss=4.6034255027771
I0305 20:08:34.935359 139902437545152 spec.py:321] Evaluating on the training split.
I0305 20:08:37.555410 139902437545152 workload.py:181] Translating evaluation dataset.
I0305 20:11:24.559325 139902437545152 spec.py:333] Evaluating on the validation split.
I0305 20:11:27.169610 139902437545152 workload.py:181] Translating evaluation dataset.
I0305 20:14:22.615539 139902437545152 spec.py:349] Evaluating on the test split.
I0305 20:14:25.225975 139902437545152 workload.py:181] Translating evaluation dataset.
I0305 20:17:06.867968 139902437545152 submission_runner.py:469] Time since start: 3848.03s, 	Step: 4866, 	{'train/accuracy': 0.5515024065971375, 'train/loss': 2.7679195404052734, 'train/bleu': 24.42808442075665, 'validation/accuracy': 0.5552369356155396, 'validation/loss': 2.7197988033294678, 'validation/bleu': 20.86397315378164, 'validation/num_examples': 3000, 'test/accuracy': 0.5571544170379639, 'test/loss': 2.739936113357544, 'test/bleu': 19.249846800938936, 'test/num_examples': 3003, 'score': 1705.8244485855103, 'total_duration': 3848.028921842575, 'accumulated_submission_time': 1705.8244485855103, 'accumulated_eval_time': 2141.853317260742, 'accumulated_logging_time': 0.03449702262878418}
I0305 20:17:06.878149 139758906865408 logging_writer.py:48] [4866] accumulated_eval_time=2141.85, accumulated_logging_time=0.034497, accumulated_submission_time=1705.82, global_step=4866, preemption_count=0, score=1705.82, test/accuracy=0.557154, test/bleu=19.2498, test/loss=2.73994, test/num_examples=3003, total_duration=3848.03, train/accuracy=0.551502, train/bleu=24.4281, train/loss=2.76792, validation/accuracy=0.555237, validation/bleu=20.864, validation/loss=2.7198, validation/num_examples=3000
I0305 20:17:18.964208 139758915258112 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.33348503708839417, loss=4.649526596069336
I0305 20:17:53.324223 139758906865408 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.02295064367353916, loss=7.917511463165283
I0305 20:18:27.718077 139758915258112 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.3456491529941559, loss=7.490321159362793
I0305 20:19:02.178083 139758906865408 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.2259063422679901, loss=7.091126918792725
I0305 20:19:36.648350 139758915258112 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.8217494487762451, loss=7.171661376953125
I0305 20:20:11.142852 139758906865408 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.392800897359848, loss=6.526299476623535
I0305 20:20:45.610048 139758915258112 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.3797258138656616, loss=6.330913066864014
I0305 20:21:20.063581 139758906865408 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.24607713520526886, loss=6.216553688049316
I0305 20:21:54.507693 139758915258112 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.21468713879585266, loss=6.104518413543701
I0305 20:22:28.963094 139758906865408 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.25218576192855835, loss=6.044640064239502
I0305 20:23:03.434336 139758915258112 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.32758766412734985, loss=6.04319429397583
I0305 20:23:37.922186 139758906865408 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.26084622740745544, loss=5.9813947677612305
I0305 20:24:12.421107 139758915258112 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.25241193175315857, loss=6.009134292602539
I0305 20:24:46.928492 139758906865408 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.29139453172683716, loss=5.965388774871826
I0305 20:25:21.450415 139758764189440 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.2210988849401474, loss=5.908933639526367
I0305 20:25:55.887267 139758755796736 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.3252924084663391, loss=5.883788585662842
I0305 20:26:30.335210 139758764189440 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.23634600639343262, loss=5.810141563415527
I0305 20:27:04.810173 139758755796736 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.2769107222557068, loss=5.805198669433594
I0305 20:27:39.277925 139758764189440 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.3355645537376404, loss=5.7234907150268555
I0305 20:28:13.731698 139758755796736 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.27974680066108704, loss=5.63665246963501
I0305 20:28:48.155900 139758764189440 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.37360939383506775, loss=6.906008243560791
I0305 20:29:22.596150 139758755796736 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.20168879628181458, loss=5.919948101043701
I0305 20:29:57.059671 139758764189440 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.3392881751060486, loss=5.706108570098877
I0305 20:30:31.506419 139758755796736 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.3606613576412201, loss=5.533601760864258
I0305 20:31:05.968890 139758764189440 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.3877875506877899, loss=5.454875946044922
I0305 20:31:07.005147 139902437545152 spec.py:321] Evaluating on the training split.
I0305 20:31:09.607871 139902437545152 workload.py:181] Translating evaluation dataset.
I0305 20:35:38.924241 139902437545152 spec.py:333] Evaluating on the validation split.
I0305 20:35:41.542623 139902437545152 workload.py:181] Translating evaluation dataset.
I0305 20:39:43.886504 139902437545152 spec.py:349] Evaluating on the test split.
I0305 20:39:46.500510 139902437545152 workload.py:181] Translating evaluation dataset.
I0305 20:43:56.064436 139902437545152 submission_runner.py:469] Time since start: 5457.23s, 	Step: 7304, 	{'train/accuracy': 0.4056222438812256, 'train/loss': 3.751762866973877, 'train/bleu': 7.95668004695303, 'validation/accuracy': 0.37379181385040283, 'validation/loss': 4.059120178222656, 'validation/bleu': 4.061260610006268, 'validation/num_examples': 3000, 'test/accuracy': 0.35187116265296936, 'test/loss': 4.292884826660156, 'test/bleu': 3.022149049837015, 'test/num_examples': 3003, 'score': 2545.790619134903, 'total_duration': 5457.225398302078, 'accumulated_submission_time': 2545.790619134903, 'accumulated_eval_time': 2910.9125502109528, 'accumulated_logging_time': 0.05387568473815918}
I0305 20:43:56.074857 139758755796736 logging_writer.py:48] [7304] accumulated_eval_time=2910.91, accumulated_logging_time=0.0538757, accumulated_submission_time=2545.79, global_step=7304, preemption_count=0, score=2545.79, test/accuracy=0.351871, test/bleu=3.02215, test/loss=4.29288, test/num_examples=3003, total_duration=5457.23, train/accuracy=0.405622, train/bleu=7.95668, train/loss=3.75176, validation/accuracy=0.373792, validation/bleu=4.06126, validation/loss=4.05912, validation/num_examples=3000
I0305 20:44:29.486493 139758764189440 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.4829440116882324, loss=5.337201118469238
I0305 20:45:03.960569 139758755796736 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.3593514859676361, loss=5.2068610191345215
I0305 20:45:38.402709 139758764189440 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.25990158319473267, loss=4.868997097015381
I0305 20:46:12.872212 139758755796736 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.3042343854904175, loss=4.763875484466553
I0305 20:46:47.338903 139758764189440 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.24491232633590698, loss=4.649684429168701
I0305 20:47:21.799168 139758755796736 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.24049299955368042, loss=4.622099876403809
I0305 20:47:56.258894 139758764189440 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.252555251121521, loss=4.569007873535156
I0305 20:48:30.690221 139758755796736 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.25226518511772156, loss=4.533107280731201
I0305 20:49:05.150301 139758764189440 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.2120230793952942, loss=4.493828773498535
I0305 20:49:39.603865 139758755796736 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.24355556070804596, loss=4.520725250244141
I0305 20:50:14.074606 139758764189440 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.21830177307128906, loss=4.511871337890625
I0305 20:50:48.551142 139758755796736 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.23986868560314178, loss=4.4713029861450195
I0305 20:51:22.980678 139758764189440 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.2365856170654297, loss=4.438656806945801
I0305 20:51:57.462642 139758755796736 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.20245251059532166, loss=4.475491046905518
I0305 20:52:31.885441 139758764189440 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.2268785685300827, loss=4.435821056365967
I0305 20:53:06.334708 139758755796736 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.21008536219596863, loss=4.379147052764893
I0305 20:53:40.786681 139758764189440 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.21542888879776, loss=4.47047233581543
I0305 20:54:15.226058 139758755796736 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.20232222974300385, loss=4.386449813842773
I0305 20:54:49.671724 139758764189440 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.24301227927207947, loss=4.340649127960205
I0305 20:55:24.120647 139758755796736 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.20539556443691254, loss=4.3857011795043945
I0305 20:55:58.561782 139758764189440 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.19747282564640045, loss=4.310173988342285
I0305 20:56:33.019867 139758755796736 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.20650504529476166, loss=4.353438854217529
I0305 20:57:07.442174 139758764189440 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.178102046251297, loss=4.357490062713623
I0305 20:57:41.862233 139758755796736 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.18864047527313232, loss=4.310585975646973
I0305 20:57:56.341723 139902437545152 spec.py:321] Evaluating on the training split.
I0305 20:57:58.957692 139902437545152 workload.py:181] Translating evaluation dataset.
I0305 21:00:42.512573 139902437545152 spec.py:333] Evaluating on the validation split.
I0305 21:00:45.112269 139902437545152 workload.py:181] Translating evaluation dataset.
I0305 21:03:34.518820 139902437545152 spec.py:349] Evaluating on the test split.
I0305 21:03:37.123641 139902437545152 workload.py:181] Translating evaluation dataset.
I0305 21:06:18.764553 139902437545152 submission_runner.py:469] Time since start: 6799.93s, 	Step: 9743, 	{'train/accuracy': 0.5885036587715149, 'train/loss': 2.3954296112060547, 'train/bleu': 27.392375123596306, 'validation/accuracy': 0.6007589101791382, 'validation/loss': 2.3107426166534424, 'validation/bleu': 23.99182101207609, 'validation/num_examples': 3000, 'test/accuracy': 0.6055034399032593, 'test/loss': 2.296506404876709, 'test/bleu': 22.460690194453218, 'test/num_examples': 3003, 'score': 3385.8995292186737, 'total_duration': 6799.925513744354, 'accumulated_submission_time': 3385.8995292186737, 'accumulated_eval_time': 3413.335329055786, 'accumulated_logging_time': 0.07249164581298828}
I0305 21:06:18.774000 139758764189440 logging_writer.py:48] [9743] accumulated_eval_time=3413.34, accumulated_logging_time=0.0724916, accumulated_submission_time=3385.9, global_step=9743, preemption_count=0, score=3385.9, test/accuracy=0.605503, test/bleu=22.4607, test/loss=2.29651, test/num_examples=3003, total_duration=6799.93, train/accuracy=0.588504, train/bleu=27.3924, train/loss=2.39543, validation/accuracy=0.600759, validation/bleu=23.9918, validation/loss=2.31074, validation/num_examples=3000
I0305 21:06:38.730575 139758755796736 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.1870506852865219, loss=4.251213073730469
I0305 21:07:13.153078 139758764189440 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.22035175561904907, loss=4.287251949310303
I0305 21:07:47.561929 139758755796736 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.22457866370677948, loss=4.358643054962158
I0305 21:08:21.990915 139758764189440 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.20758236944675446, loss=4.271569728851318
I0305 21:08:56.414991 139758755796736 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.18348799645900726, loss=4.262993335723877
I0305 21:09:30.854458 139758764189440 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.18583761155605316, loss=4.309971809387207
I0305 21:10:05.303372 139758755796736 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.18955911695957184, loss=4.316317081451416
I0305 21:10:39.739270 139758764189440 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.22317631542682648, loss=4.309560775756836
I0305 21:11:14.170898 139758755796736 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.17859850823879242, loss=4.260551929473877
I0305 21:11:48.625084 139758764189440 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.18362796306610107, loss=4.2821831703186035
I0305 21:12:23.031190 139758755796736 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.18689599633216858, loss=4.332292079925537
I0305 21:12:57.440392 139758764189440 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.18506476283073425, loss=4.292253494262695
I0305 21:13:31.837247 139758755796736 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.162139430642128, loss=4.280938625335693
I0305 21:14:06.213122 139758764189440 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.16797815263271332, loss=4.285641670227051
I0305 21:14:40.650875 139758755796736 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.19192516803741455, loss=4.2460713386535645
I0305 21:15:15.044335 139758764189440 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.1827794462442398, loss=4.270366668701172
I0305 21:15:49.467571 139758755796736 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.15983431041240692, loss=4.257107257843018
I0305 21:16:23.902234 139758764189440 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.17199809849262238, loss=4.201827526092529
I0305 21:16:58.353762 139758755796736 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.17349138855934143, loss=4.230468273162842
I0305 21:17:32.770856 139758764189440 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.1908564418554306, loss=4.246825218200684
I0305 21:18:07.211748 139758755796736 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.16478006541728973, loss=4.203910827636719
I0305 21:18:41.639293 139758764189440 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.19371995329856873, loss=4.212606906890869
I0305 21:19:16.072293 139758755796736 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.1641302853822708, loss=4.258421897888184
I0305 21:19:50.492138 139758764189440 logging_writer.py:48] [12100] global_step=12100, grad_norm=0.19261281192302704, loss=4.172885417938232
I0305 21:20:19.038975 139902437545152 spec.py:321] Evaluating on the training split.
I0305 21:20:21.648351 139902437545152 workload.py:181] Translating evaluation dataset.
I0305 21:23:37.070092 139902437545152 spec.py:333] Evaluating on the validation split.
I0305 21:23:39.666142 139902437545152 workload.py:181] Translating evaluation dataset.
I0305 21:26:32.256211 139902437545152 spec.py:349] Evaluating on the test split.
I0305 21:26:34.868864 139902437545152 workload.py:181] Translating evaluation dataset.
I0305 21:29:23.591916 139902437545152 submission_runner.py:469] Time since start: 8184.75s, 	Step: 12184, 	{'train/accuracy': 0.6078491806983948, 'train/loss': 2.213330030441284, 'train/bleu': 28.804056562663057, 'validation/accuracy': 0.624218225479126, 'validation/loss': 2.0900421142578125, 'validation/bleu': 25.43472218448631, 'validation/num_examples': 3000, 'test/accuracy': 0.6304831504821777, 'test/loss': 2.0576913356781006, 'test/bleu': 24.261085169981413, 'test/num_examples': 3003, 'score': 4226.010152339935, 'total_duration': 8184.752869844437, 'accumulated_submission_time': 4226.010152339935, 'accumulated_eval_time': 3957.8882093429565, 'accumulated_logging_time': 0.09021759033203125}
I0305 21:29:23.601593 139758755796736 logging_writer.py:48] [12184] accumulated_eval_time=3957.89, accumulated_logging_time=0.0902176, accumulated_submission_time=4226.01, global_step=12184, preemption_count=0, score=4226.01, test/accuracy=0.630483, test/bleu=24.2611, test/loss=2.05769, test/num_examples=3003, total_duration=8184.75, train/accuracy=0.607849, train/bleu=28.8041, train/loss=2.21333, validation/accuracy=0.624218, validation/bleu=25.4347, validation/loss=2.09004, validation/num_examples=3000
I0305 21:29:29.441006 139758764189440 logging_writer.py:48] [12200] global_step=12200, grad_norm=0.1833042949438095, loss=4.21965217590332
I0305 21:30:03.852622 139758755796736 logging_writer.py:48] [12300] global_step=12300, grad_norm=0.22538676857948303, loss=4.205663204193115
I0305 21:30:38.269032 139758764189440 logging_writer.py:48] [12400] global_step=12400, grad_norm=0.17847372591495514, loss=4.238974571228027
I0305 21:31:12.651805 139758755796736 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.21272045373916626, loss=4.201045989990234
I0305 21:31:47.097786 139758764189440 logging_writer.py:48] [12600] global_step=12600, grad_norm=0.17694514989852905, loss=4.16999626159668
I0305 21:32:21.563025 139758755796736 logging_writer.py:48] [12700] global_step=12700, grad_norm=0.17453433573246002, loss=4.242506980895996
I0305 21:32:56.050968 139758764189440 logging_writer.py:48] [12800] global_step=12800, grad_norm=0.15456365048885345, loss=4.1200079917907715
I0305 21:33:30.523069 139758755796736 logging_writer.py:48] [12900] global_step=12900, grad_norm=0.16940656304359436, loss=4.203794479370117
I0305 21:34:04.998758 139758764189440 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.18763741850852966, loss=4.234979629516602
I0305 21:34:39.499076 139758755796736 logging_writer.py:48] [13100] global_step=13100, grad_norm=0.15294939279556274, loss=4.173274993896484
I0305 21:35:13.982682 139758764189440 logging_writer.py:48] [13200] global_step=13200, grad_norm=0.15963296592235565, loss=4.167702674865723
I0305 21:35:48.478494 139758755796736 logging_writer.py:48] [13300] global_step=13300, grad_norm=0.15819667279720306, loss=4.1059184074401855
I0305 21:36:22.950463 139758764189440 logging_writer.py:48] [13400] global_step=13400, grad_norm=0.17917661368846893, loss=4.320493221282959
I0305 21:36:57.448121 139758755796736 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.1564820557832718, loss=4.2410569190979
I0305 21:37:31.936432 139758764189440 logging_writer.py:48] [13600] global_step=13600, grad_norm=0.2721453905105591, loss=4.19910192489624
I0305 21:38:06.439566 139758755796736 logging_writer.py:48] [13700] global_step=13700, grad_norm=0.16713258624076843, loss=4.15000581741333
I0305 21:38:40.940482 139758764189440 logging_writer.py:48] [13800] global_step=13800, grad_norm=0.21573598682880402, loss=4.219178676605225
I0305 21:39:15.434872 139758755796736 logging_writer.py:48] [13900] global_step=13900, grad_norm=0.16191218793392181, loss=4.149008274078369
I0305 21:39:49.881307 139758764189440 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.15470845997333527, loss=4.142563343048096
I0305 21:40:24.351933 139758755796736 logging_writer.py:48] [14100] global_step=14100, grad_norm=0.20248356461524963, loss=4.16248893737793
I0305 21:40:58.831760 139758764189440 logging_writer.py:48] [14200] global_step=14200, grad_norm=0.14673124253749847, loss=4.082272052764893
I0305 21:41:33.293808 139758755796736 logging_writer.py:48] [14300] global_step=14300, grad_norm=0.15907683968544006, loss=4.113374710083008
I0305 21:42:07.757827 139758764189440 logging_writer.py:48] [14400] global_step=14400, grad_norm=0.150954470038414, loss=4.120187282562256
I0305 21:42:42.241802 139758755796736 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.15605351328849792, loss=4.134664535522461
I0305 21:43:16.720182 139758764189440 logging_writer.py:48] [14600] global_step=14600, grad_norm=0.14841178059577942, loss=4.162402629852295
I0305 21:43:23.628582 139902437545152 spec.py:321] Evaluating on the training split.
I0305 21:43:26.239855 139902437545152 workload.py:181] Translating evaluation dataset.
I0305 21:46:21.766571 139902437545152 spec.py:333] Evaluating on the validation split.
I0305 21:46:24.371161 139902437545152 workload.py:181] Translating evaluation dataset.
I0305 21:49:37.873210 139902437545152 spec.py:349] Evaluating on the test split.
I0305 21:49:40.480266 139902437545152 workload.py:181] Translating evaluation dataset.
I0305 21:52:55.454303 139902437545152 submission_runner.py:469] Time since start: 9596.62s, 	Step: 14621, 	{'train/accuracy': 0.6263440847396851, 'train/loss': 2.0837655067443848, 'train/bleu': 30.384154777135624, 'validation/accuracy': 0.6366153359413147, 'validation/loss': 2.00032114982605, 'validation/bleu': 26.74528699509487, 'validation/num_examples': 3000, 'test/accuracy': 0.6456030607223511, 'test/loss': 1.9602564573287964, 'test/bleu': 25.735044288347087, 'test/num_examples': 3003, 'score': 5065.888287782669, 'total_duration': 9596.61524772644, 'accumulated_submission_time': 5065.888287782669, 'accumulated_eval_time': 4529.713856935501, 'accumulated_logging_time': 0.10824131965637207}
I0305 21:52:55.465932 139758755796736 logging_writer.py:48] [14621] accumulated_eval_time=4529.71, accumulated_logging_time=0.108241, accumulated_submission_time=5065.89, global_step=14621, preemption_count=0, score=5065.89, test/accuracy=0.645603, test/bleu=25.735, test/loss=1.96026, test/num_examples=3003, total_duration=9596.62, train/accuracy=0.626344, train/bleu=30.3842, train/loss=2.08377, validation/accuracy=0.636615, validation/bleu=26.7453, validation/loss=2.00032, validation/num_examples=3000
I0305 21:53:23.144492 139758764189440 logging_writer.py:48] [14700] global_step=14700, grad_norm=0.15254192054271698, loss=4.149710178375244
I0305 21:53:57.620584 139758755796736 logging_writer.py:48] [14800] global_step=14800, grad_norm=0.14338091015815735, loss=4.051980018615723
I0305 21:54:32.082021 139758764189440 logging_writer.py:48] [14900] global_step=14900, grad_norm=0.24890345335006714, loss=4.218515396118164
I0305 21:55:06.542630 139758755796736 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.18120354413986206, loss=4.049161911010742
I0305 21:55:40.984640 139758764189440 logging_writer.py:48] [15100] global_step=15100, grad_norm=0.16320355236530304, loss=4.101559638977051
I0305 21:56:15.460082 139758755796736 logging_writer.py:48] [15200] global_step=15200, grad_norm=0.159054234623909, loss=4.066316604614258
I0305 21:56:49.930837 139758764189440 logging_writer.py:48] [15300] global_step=15300, grad_norm=0.15076793730258942, loss=4.057041168212891
I0305 21:57:24.402099 139758755796736 logging_writer.py:48] [15400] global_step=15400, grad_norm=0.15458455681800842, loss=4.1602277755737305
I0305 21:57:58.862880 139758764189440 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.15173135697841644, loss=4.13340425491333
I0305 21:58:33.379573 139758755796736 logging_writer.py:48] [15600] global_step=15600, grad_norm=0.1467353105545044, loss=4.093953609466553
I0305 21:59:07.880299 139758764189440 logging_writer.py:48] [15700] global_step=15700, grad_norm=0.15004298090934753, loss=4.148229122161865
I0305 21:59:42.368368 139758755796736 logging_writer.py:48] [15800] global_step=15800, grad_norm=0.18447791039943695, loss=4.075490474700928
I0305 22:00:16.869282 139758764189440 logging_writer.py:48] [15900] global_step=15900, grad_norm=0.18757598102092743, loss=4.092889785766602
I0305 22:00:51.374742 139758755796736 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.1542961597442627, loss=4.0197553634643555
I0305 22:01:25.868170 139758764189440 logging_writer.py:48] [16100] global_step=16100, grad_norm=0.18201349675655365, loss=4.128023147583008
I0305 22:02:00.343807 139758755796736 logging_writer.py:48] [16200] global_step=16200, grad_norm=0.14438466727733612, loss=4.0761799812316895
I0305 22:02:34.854867 139758764189440 logging_writer.py:48] [16300] global_step=16300, grad_norm=0.19535352289676666, loss=4.066959381103516
I0305 22:03:09.327864 139758755796736 logging_writer.py:48] [16400] global_step=16400, grad_norm=0.14865581691265106, loss=4.083849906921387
I0305 22:03:43.820682 139758764189440 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.17541655898094177, loss=4.09793758392334
I0305 22:04:18.313817 139758755796736 logging_writer.py:48] [16600] global_step=16600, grad_norm=0.1434362232685089, loss=4.113353729248047
I0305 22:04:52.796217 139758764189440 logging_writer.py:48] [16700] global_step=16700, grad_norm=0.1617920845746994, loss=4.182636260986328
I0305 22:05:27.269753 139758755796736 logging_writer.py:48] [16800] global_step=16800, grad_norm=0.19863004982471466, loss=4.143083572387695
I0305 22:06:01.753310 139758764189440 logging_writer.py:48] [16900] global_step=16900, grad_norm=0.1605226844549179, loss=4.117616653442383
I0305 22:06:36.227970 139758755796736 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.17122235894203186, loss=4.125300407409668
I0305 22:06:55.559659 139902437545152 spec.py:321] Evaluating on the training split.
I0305 22:06:58.173433 139902437545152 workload.py:181] Translating evaluation dataset.
I0305 22:09:55.234097 139902437545152 spec.py:333] Evaluating on the validation split.
I0305 22:09:57.851521 139902437545152 workload.py:181] Translating evaluation dataset.
I0305 22:12:37.973310 139902437545152 spec.py:349] Evaluating on the test split.
I0305 22:12:40.590732 139902437545152 workload.py:181] Translating evaluation dataset.
I0305 22:15:03.028056 139902437545152 submission_runner.py:469] Time since start: 10924.19s, 	Step: 17057, 	{'train/accuracy': 0.6272832155227661, 'train/loss': 2.0608181953430176, 'train/bleu': 29.794582781972643, 'validation/accuracy': 0.6458606123924255, 'validation/loss': 1.9317424297332764, 'validation/bleu': 27.330965677977527, 'validation/num_examples': 3000, 'test/accuracy': 0.6537597179412842, 'test/loss': 1.87694251537323, 'test/bleu': 26.329894935044557, 'test/num_examples': 3003, 'score': 5905.828632354736, 'total_duration': 10924.18899345398, 'accumulated_submission_time': 5905.828632354736, 'accumulated_eval_time': 5017.182178735733, 'accumulated_logging_time': 0.12886714935302734}
I0305 22:15:03.038246 139758764189440 logging_writer.py:48] [17057] accumulated_eval_time=5017.18, accumulated_logging_time=0.128867, accumulated_submission_time=5905.83, global_step=17057, preemption_count=0, score=5905.83, test/accuracy=0.65376, test/bleu=26.3299, test/loss=1.87694, test/num_examples=3003, total_duration=10924.2, train/accuracy=0.627283, train/bleu=29.7946, train/loss=2.06082, validation/accuracy=0.645861, validation/bleu=27.331, validation/loss=1.93174, validation/num_examples=3000
I0305 22:15:18.212165 139758755796736 logging_writer.py:48] [17100] global_step=17100, grad_norm=0.16683876514434814, loss=4.141894340515137
I0305 22:15:52.671712 139758764189440 logging_writer.py:48] [17200] global_step=17200, grad_norm=0.15560288727283478, loss=4.044161796569824
I0305 22:16:27.145715 139758755796736 logging_writer.py:48] [17300] global_step=17300, grad_norm=0.2618753910064697, loss=4.109489917755127
I0305 22:17:01.641824 139758764189440 logging_writer.py:48] [17400] global_step=17400, grad_norm=0.15951448678970337, loss=4.03108024597168
I0305 22:17:36.145407 139758755796736 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.16334600746631622, loss=4.093159198760986
I0305 22:18:10.651132 139758764189440 logging_writer.py:48] [17600] global_step=17600, grad_norm=0.15414050221443176, loss=4.094395160675049
I0305 22:18:45.152897 139758755796736 logging_writer.py:48] [17700] global_step=17700, grad_norm=0.25525638461112976, loss=4.099304676055908
I0305 22:19:19.667492 139758764189440 logging_writer.py:48] [17800] global_step=17800, grad_norm=0.16678142547607422, loss=4.058464050292969
I0305 22:19:54.141258 139758755796736 logging_writer.py:48] [17900] global_step=17900, grad_norm=0.16715385019779205, loss=4.0331220626831055
I0305 22:20:28.626821 139758764189440 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.16356261074543, loss=4.1132073402404785
I0305 22:21:03.092307 139758755796736 logging_writer.py:48] [18100] global_step=18100, grad_norm=0.1992349475622177, loss=4.074429035186768
I0305 22:21:37.578372 139758764189440 logging_writer.py:48] [18200] global_step=18200, grad_norm=0.17352671921253204, loss=4.0717854499816895
I0305 22:22:12.071230 139758755796736 logging_writer.py:48] [18300] global_step=18300, grad_norm=0.14989887177944183, loss=4.1240386962890625
I0305 22:22:46.568383 139758764189440 logging_writer.py:48] [18400] global_step=18400, grad_norm=0.17380574345588684, loss=4.042321681976318
I0305 22:23:21.050719 139758755796736 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.18973268568515778, loss=4.0484819412231445
I0305 22:23:55.556267 139758764189440 logging_writer.py:48] [18600] global_step=18600, grad_norm=0.1531003713607788, loss=4.068973064422607
I0305 22:24:30.076842 139758755796736 logging_writer.py:48] [18700] global_step=18700, grad_norm=0.14482355117797852, loss=4.069533348083496
I0305 22:25:04.553798 139758764189440 logging_writer.py:48] [18800] global_step=18800, grad_norm=0.22643065452575684, loss=4.074278354644775
I0305 22:25:39.304384 139758755796736 logging_writer.py:48] [18900] global_step=18900, grad_norm=0.15970540046691895, loss=4.09857702255249
I0305 22:26:14.022029 139758764189440 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.1704731583595276, loss=4.01440954208374
I0305 22:26:48.662668 139758755796736 logging_writer.py:48] [19100] global_step=19100, grad_norm=0.15875688195228577, loss=4.053401470184326
I0305 22:27:23.274924 139758764189440 logging_writer.py:48] [19200] global_step=19200, grad_norm=0.25942549109458923, loss=4.026883125305176
I0305 22:27:57.883259 139758755796736 logging_writer.py:48] [19300] global_step=19300, grad_norm=0.149411141872406, loss=4.017454147338867
I0305 22:28:32.499539 139758764189440 logging_writer.py:48] [19400] global_step=19400, grad_norm=0.16759657859802246, loss=4.0718183517456055
I0305 22:29:03.318816 139902437545152 spec.py:321] Evaluating on the training split.
I0305 22:29:05.942532 139902437545152 workload.py:181] Translating evaluation dataset.
I0305 22:32:07.731056 139902437545152 spec.py:333] Evaluating on the validation split.
I0305 22:32:10.337673 139902437545152 workload.py:181] Translating evaluation dataset.
I0305 22:34:58.985342 139902437545152 spec.py:349] Evaluating on the test split.
I0305 22:35:01.605349 139902437545152 workload.py:181] Translating evaluation dataset.
I0305 22:37:29.978260 139902437545152 submission_runner.py:469] Time since start: 12271.14s, 	Step: 19490, 	{'train/accuracy': 0.6432692408561707, 'train/loss': 1.9442501068115234, 'train/bleu': 31.540684536979015, 'validation/accuracy': 0.6517563462257385, 'validation/loss': 1.8737787008285522, 'validation/bleu': 27.63499580458185, 'validation/num_examples': 3000, 'test/accuracy': 0.6616151332855225, 'test/loss': 1.8183088302612305, 'test/bleu': 26.892767325535203, 'test/num_examples': 3003, 'score': 6745.9588186740875, 'total_duration': 12271.139216423035, 'accumulated_submission_time': 6745.9588186740875, 'accumulated_eval_time': 5523.841565132141, 'accumulated_logging_time': 0.14786076545715332}
I0305 22:37:29.991505 139758755796736 logging_writer.py:48] [19490] accumulated_eval_time=5523.84, accumulated_logging_time=0.147861, accumulated_submission_time=6745.96, global_step=19490, preemption_count=0, score=6745.96, test/accuracy=0.661615, test/bleu=26.8928, test/loss=1.81831, test/num_examples=3003, total_duration=12271.1, train/accuracy=0.643269, train/bleu=31.5407, train/loss=1.94425, validation/accuracy=0.651756, validation/bleu=27.635, validation/loss=1.87378, validation/num_examples=3000
I0305 22:37:33.818983 139758764189440 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.17050480842590332, loss=4.069907188415527
I0305 22:38:08.433815 139758755796736 logging_writer.py:48] [19600] global_step=19600, grad_norm=0.17793762683868408, loss=3.9838037490844727
I0305 22:38:43.066573 139758764189440 logging_writer.py:48] [19700] global_step=19700, grad_norm=0.15922676026821136, loss=4.038419723510742
I0305 22:39:17.685164 139758755796736 logging_writer.py:48] [19800] global_step=19800, grad_norm=0.18010449409484863, loss=4.069863319396973
I0305 22:39:52.309125 139758764189440 logging_writer.py:48] [19900] global_step=19900, grad_norm=0.16794949769973755, loss=4.071599006652832
I0305 22:40:26.926063 139758755796736 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.1654125154018402, loss=4.029238224029541
I0305 22:41:01.580053 139758764189440 logging_writer.py:48] [20100] global_step=20100, grad_norm=0.1977418214082718, loss=3.9369640350341797
I0305 22:41:36.190445 139758755796736 logging_writer.py:48] [20200] global_step=20200, grad_norm=0.18926620483398438, loss=4.068910121917725
I0305 22:42:10.852031 139758764189440 logging_writer.py:48] [20300] global_step=20300, grad_norm=0.23919247090816498, loss=4.000724792480469
I0305 22:42:45.461225 139758755796736 logging_writer.py:48] [20400] global_step=20400, grad_norm=0.21188053488731384, loss=4.019861221313477
I0305 22:43:20.066864 139758764189440 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.22917786240577698, loss=4.046417236328125
I0305 22:43:54.685924 139758755796736 logging_writer.py:48] [20600] global_step=20600, grad_norm=0.16159313917160034, loss=4.007724285125732
I0305 22:44:29.329462 139758764189440 logging_writer.py:48] [20700] global_step=20700, grad_norm=0.1566964089870453, loss=3.950765609741211
I0305 22:45:03.979859 139758755796736 logging_writer.py:48] [20800] global_step=20800, grad_norm=0.23006011545658112, loss=4.093993186950684
I0305 22:45:38.591327 139758764189440 logging_writer.py:48] [20900] global_step=20900, grad_norm=0.1678454875946045, loss=4.008878707885742
I0305 22:46:13.228700 139758755796736 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.16788537800312042, loss=4.065195083618164
I0305 22:46:47.835561 139758764189440 logging_writer.py:48] [21100] global_step=21100, grad_norm=0.16145822405815125, loss=4.050988674163818
I0305 22:47:22.447676 139758755796736 logging_writer.py:48] [21200] global_step=21200, grad_norm=0.25579917430877686, loss=4.032915115356445
I0305 22:47:57.099399 139758764189440 logging_writer.py:48] [21300] global_step=21300, grad_norm=0.1819734275341034, loss=4.047067165374756
I0305 22:48:31.744073 139758755796736 logging_writer.py:48] [21400] global_step=21400, grad_norm=0.17674756050109863, loss=4.028529167175293
I0305 22:49:06.396763 139758764189440 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.1832321137189865, loss=4.010213375091553
I0305 22:49:41.014458 139758755796736 logging_writer.py:48] [21600] global_step=21600, grad_norm=0.18213245272636414, loss=3.9799907207489014
I0305 22:50:15.636709 139758764189440 logging_writer.py:48] [21700] global_step=21700, grad_norm=0.19246011972427368, loss=4.069609642028809
I0305 22:50:50.267730 139758755796736 logging_writer.py:48] [21800] global_step=21800, grad_norm=0.15422923862934113, loss=4.032073020935059
I0305 22:51:24.872593 139758764189440 logging_writer.py:48] [21900] global_step=21900, grad_norm=0.17761334776878357, loss=3.9965012073516846
I0305 22:51:30.072721 139902437545152 spec.py:321] Evaluating on the training split.
I0305 22:51:32.694617 139902437545152 workload.py:181] Translating evaluation dataset.
I0305 22:55:37.394561 139902437545152 spec.py:333] Evaluating on the validation split.
I0305 22:55:40.005646 139902437545152 workload.py:181] Translating evaluation dataset.
I0305 22:58:18.344341 139902437545152 spec.py:349] Evaluating on the test split.
I0305 22:58:20.962967 139902437545152 workload.py:181] Translating evaluation dataset.
I0305 23:00:47.785670 139902437545152 submission_runner.py:469] Time since start: 13668.95s, 	Step: 21916, 	{'train/accuracy': 0.6408578157424927, 'train/loss': 1.9679399728775024, 'train/bleu': 31.287286764335768, 'validation/accuracy': 0.6556127071380615, 'validation/loss': 1.8453642129898071, 'validation/bleu': 27.701844184533844, 'validation/num_examples': 3000, 'test/accuracy': 0.6660525798797607, 'test/loss': 1.7810513973236084, 'test/bleu': 27.568812016774732, 'test/num_examples': 3003, 'score': 7585.8901126384735, 'total_duration': 13668.946621894836, 'accumulated_submission_time': 7585.8901126384735, 'accumulated_eval_time': 6081.554447650909, 'accumulated_logging_time': 0.17025518417358398}
I0305 23:00:47.796840 139758755796736 logging_writer.py:48] [21916] accumulated_eval_time=6081.55, accumulated_logging_time=0.170255, accumulated_submission_time=7585.89, global_step=21916, preemption_count=0, score=7585.89, test/accuracy=0.666053, test/bleu=27.5688, test/loss=1.78105, test/num_examples=3003, total_duration=13668.9, train/accuracy=0.640858, train/bleu=31.2873, train/loss=1.96794, validation/accuracy=0.655613, validation/bleu=27.7018, validation/loss=1.84536, validation/num_examples=3000
I0305 23:01:17.195495 139758764189440 logging_writer.py:48] [22000] global_step=22000, grad_norm=0.2183133363723755, loss=4.008989334106445
I0305 23:01:51.798995 139758755796736 logging_writer.py:48] [22100] global_step=22100, grad_norm=0.16024400293827057, loss=4.029702663421631
I0305 23:02:26.414519 139758764189440 logging_writer.py:48] [22200] global_step=22200, grad_norm=0.1520141214132309, loss=4.020971775054932
I0305 23:03:01.032166 139758755796736 logging_writer.py:48] [22300] global_step=22300, grad_norm=0.1582050770521164, loss=4.036306858062744
I0305 23:03:35.698428 139758764189440 logging_writer.py:48] [22400] global_step=22400, grad_norm=0.16660982370376587, loss=4.088260650634766
I0305 23:04:10.368778 139758755796736 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.18823273479938507, loss=3.998680353164673
I0305 23:04:45.016166 139758764189440 logging_writer.py:48] [22600] global_step=22600, grad_norm=0.19470088183879852, loss=4.019387245178223
I0305 23:05:19.674488 139758755796736 logging_writer.py:48] [22700] global_step=22700, grad_norm=0.19322548806667328, loss=4.0405120849609375
I0305 23:05:54.342243 139758764189440 logging_writer.py:48] [22800] global_step=22800, grad_norm=0.1654420644044876, loss=3.9625158309936523
I0305 23:06:28.986011 139758755796736 logging_writer.py:48] [22900] global_step=22900, grad_norm=0.21151606738567352, loss=3.9447174072265625
I0305 23:07:03.620360 139758764189440 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.16653957962989807, loss=3.969383955001831
I0305 23:07:38.248968 139758755796736 logging_writer.py:48] [23100] global_step=23100, grad_norm=0.21108219027519226, loss=4.0584282875061035
I0305 23:08:12.895225 139758764189440 logging_writer.py:48] [23200] global_step=23200, grad_norm=0.15469874441623688, loss=4.0195841789245605
I0305 23:08:47.518775 139758755796736 logging_writer.py:48] [23300] global_step=23300, grad_norm=0.16505126655101776, loss=4.026438236236572
I0305 23:09:22.185947 139758764189440 logging_writer.py:48] [23400] global_step=23400, grad_norm=0.16487827897071838, loss=3.9954233169555664
I0305 23:09:56.821995 139758755796736 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.1977916657924652, loss=3.9731528759002686
I0305 23:10:31.424040 139758764189440 logging_writer.py:48] [23600] global_step=23600, grad_norm=0.15542122721672058, loss=3.931173801422119
I0305 23:11:06.044668 139758755796736 logging_writer.py:48] [23700] global_step=23700, grad_norm=0.25374677777290344, loss=4.128650665283203
I0305 23:11:40.643261 139758764189440 logging_writer.py:48] [23800] global_step=23800, grad_norm=0.19087478518486023, loss=3.890476703643799
I0305 23:12:15.291605 139758755796736 logging_writer.py:48] [23900] global_step=23900, grad_norm=0.17189635336399078, loss=4.028042316436768
I0305 23:12:49.927464 139758764189440 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.1904304325580597, loss=3.9971234798431396
I0305 23:13:24.563352 139758755796736 logging_writer.py:48] [24100] global_step=24100, grad_norm=0.22611993551254272, loss=3.9769701957702637
I0305 23:13:59.211036 139758764189440 logging_writer.py:48] [24200] global_step=24200, grad_norm=0.22857338190078735, loss=3.9499871730804443
I0305 23:14:33.835167 139758755796736 logging_writer.py:48] [24300] global_step=24300, grad_norm=0.17996925115585327, loss=3.9089784622192383
I0305 23:14:48.040013 139902437545152 spec.py:321] Evaluating on the training split.
I0305 23:14:50.658040 139902437545152 workload.py:181] Translating evaluation dataset.
I0305 23:17:30.288292 139902437545152 spec.py:333] Evaluating on the validation split.
I0305 23:17:32.902312 139902437545152 workload.py:181] Translating evaluation dataset.
I0305 23:20:10.815505 139902437545152 spec.py:349] Evaluating on the test split.
I0305 23:20:13.438140 139902437545152 workload.py:181] Translating evaluation dataset.
I0305 23:22:50.072777 139902437545152 submission_runner.py:469] Time since start: 14991.23s, 	Step: 24342, 	{'train/accuracy': 0.6377574801445007, 'train/loss': 1.9934712648391724, 'train/bleu': 31.492986124887413, 'validation/accuracy': 0.659271240234375, 'validation/loss': 1.8386343717575073, 'validation/bleu': 28.20744508706731, 'validation/num_examples': 3000, 'test/accuracy': 0.6703162789344788, 'test/loss': 1.77511727809906, 'test/bleu': 27.348556720975562, 'test/num_examples': 3003, 'score': 8425.983776807785, 'total_duration': 14991.233741283417, 'accumulated_submission_time': 8425.983776807785, 'accumulated_eval_time': 6563.5871596336365, 'accumulated_logging_time': 0.18932700157165527}
I0305 23:22:50.084295 139758764189440 logging_writer.py:48] [24342] accumulated_eval_time=6563.59, accumulated_logging_time=0.189327, accumulated_submission_time=8425.98, global_step=24342, preemption_count=0, score=8425.98, test/accuracy=0.670316, test/bleu=27.3486, test/loss=1.77512, test/num_examples=3003, total_duration=14991.2, train/accuracy=0.637757, train/bleu=31.493, train/loss=1.99347, validation/accuracy=0.659271, validation/bleu=28.2074, validation/loss=1.83863, validation/num_examples=3000
I0305 23:23:10.510061 139758755796736 logging_writer.py:48] [24400] global_step=24400, grad_norm=0.2147032767534256, loss=4.054497241973877
I0305 23:23:45.137672 139758764189440 logging_writer.py:48] [24500] global_step=24500, grad_norm=0.16012315452098846, loss=3.964470863342285
I0305 23:24:19.771327 139758755796736 logging_writer.py:48] [24600] global_step=24600, grad_norm=0.17357802391052246, loss=3.975238561630249
I0305 23:24:54.403019 139758764189440 logging_writer.py:48] [24700] global_step=24700, grad_norm=0.23626112937927246, loss=3.9903690814971924
I0305 23:25:29.041250 139758755796736 logging_writer.py:48] [24800] global_step=24800, grad_norm=0.20824360847473145, loss=3.982424020767212
I0305 23:26:03.659704 139758764189440 logging_writer.py:48] [24900] global_step=24900, grad_norm=0.2676686942577362, loss=3.9631283283233643
I0305 23:26:38.309094 139758755796736 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.16814284026622772, loss=3.948742628097534
I0305 23:27:12.927410 139758764189440 logging_writer.py:48] [25100] global_step=25100, grad_norm=0.19743120670318604, loss=4.001847267150879
I0305 23:27:47.447822 139758755796736 logging_writer.py:48] [25200] global_step=25200, grad_norm=0.1882522851228714, loss=3.904144525527954
I0305 23:28:21.976487 139758764189440 logging_writer.py:48] [25300] global_step=25300, grad_norm=0.2630675137042999, loss=4.0067219734191895
I0305 23:28:56.496170 139758755796736 logging_writer.py:48] [25400] global_step=25400, grad_norm=0.16903890669345856, loss=4.03469181060791
I0305 23:29:31.039925 139758764189440 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.19825203716754913, loss=3.9938278198242188
I0305 23:30:05.570627 139758755796736 logging_writer.py:48] [25600] global_step=25600, grad_norm=0.16855621337890625, loss=3.9343080520629883
I0305 23:30:40.099828 139758764189440 logging_writer.py:48] [25700] global_step=25700, grad_norm=0.16573311388492584, loss=3.9616971015930176
I0305 23:31:14.612994 139758755796736 logging_writer.py:48] [25800] global_step=25800, grad_norm=0.20692561566829681, loss=4.008993148803711
I0305 23:31:49.139240 139758764189440 logging_writer.py:48] [25900] global_step=25900, grad_norm=0.22101396322250366, loss=3.9350829124450684
I0305 23:32:23.665541 139758755796736 logging_writer.py:48] [26000] global_step=26000, grad_norm=0.18513137102127075, loss=4.002374649047852
I0305 23:32:58.178907 139758764189440 logging_writer.py:48] [26100] global_step=26100, grad_norm=0.17807093262672424, loss=3.9447128772735596
I0305 23:33:32.684254 139758755796736 logging_writer.py:48] [26200] global_step=26200, grad_norm=0.17272990942001343, loss=3.990943431854248
I0305 23:34:07.179108 139758764189440 logging_writer.py:48] [26300] global_step=26300, grad_norm=0.18096405267715454, loss=4.00179386138916
I0305 23:34:41.699599 139758755796736 logging_writer.py:48] [26400] global_step=26400, grad_norm=0.16680480539798737, loss=3.9771852493286133
I0305 23:35:16.213160 139758764189440 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.1965099573135376, loss=3.9831628799438477
I0305 23:35:50.740536 139758755796736 logging_writer.py:48] [26600] global_step=26600, grad_norm=0.2719486951828003, loss=3.94857120513916
I0305 23:36:25.264446 139758764189440 logging_writer.py:48] [26700] global_step=26700, grad_norm=0.16264548897743225, loss=3.931856870651245
I0305 23:36:50.136392 139902437545152 spec.py:321] Evaluating on the training split.
I0305 23:36:52.758539 139902437545152 workload.py:181] Translating evaluation dataset.
I0305 23:40:56.371551 139902437545152 spec.py:333] Evaluating on the validation split.
I0305 23:40:58.987383 139902437545152 workload.py:181] Translating evaluation dataset.
I0305 23:44:00.964063 139902437545152 spec.py:349] Evaluating on the test split.
I0305 23:44:03.569381 139902437545152 workload.py:181] Translating evaluation dataset.
I0305 23:46:55.885822 139902437545152 submission_runner.py:469] Time since start: 16437.05s, 	Step: 26773, 	{'train/accuracy': 0.6529027819633484, 'train/loss': 1.8722681999206543, 'train/bleu': 31.658544632366144, 'validation/accuracy': 0.6639310121536255, 'validation/loss': 1.7937933206558228, 'validation/bleu': 28.43376417782063, 'validation/num_examples': 3000, 'test/accuracy': 0.6753794550895691, 'test/loss': 1.7213813066482544, 'test/bleu': 28.06940124881642, 'test/num_examples': 3003, 'score': 9265.884675264359, 'total_duration': 16437.04678416252, 'accumulated_submission_time': 9265.884675264359, 'accumulated_eval_time': 7169.336541652679, 'accumulated_logging_time': 0.21043825149536133}
I0305 23:46:55.897707 139758755796736 logging_writer.py:48] [26773] accumulated_eval_time=7169.34, accumulated_logging_time=0.210438, accumulated_submission_time=9265.88, global_step=26773, preemption_count=0, score=9265.88, test/accuracy=0.675379, test/bleu=28.0694, test/loss=1.72138, test/num_examples=3003, total_duration=16437, train/accuracy=0.652903, train/bleu=31.6585, train/loss=1.87227, validation/accuracy=0.663931, validation/bleu=28.4338, validation/loss=1.79379, validation/num_examples=3000
I0305 23:47:05.575546 139758764189440 logging_writer.py:48] [26800] global_step=26800, grad_norm=0.18775470554828644, loss=3.9605910778045654
I0305 23:47:40.074428 139758755796736 logging_writer.py:48] [26900] global_step=26900, grad_norm=0.1775960624217987, loss=4.018770217895508
I0305 23:48:14.597096 139758764189440 logging_writer.py:48] [27000] global_step=27000, grad_norm=0.22381125390529633, loss=3.982246160507202
I0305 23:48:49.126191 139758755796736 logging_writer.py:48] [27100] global_step=27100, grad_norm=0.17401756346225739, loss=3.9392595291137695
I0305 23:49:23.631422 139758764189440 logging_writer.py:48] [27200] global_step=27200, grad_norm=0.17222993075847626, loss=3.9907257556915283
I0305 23:49:58.144009 139758755796736 logging_writer.py:48] [27300] global_step=27300, grad_norm=0.21138329803943634, loss=3.9788119792938232
I0305 23:50:32.660315 139758764189440 logging_writer.py:48] [27400] global_step=27400, grad_norm=0.17899532616138458, loss=3.9100871086120605
I0305 23:51:07.179167 139758755796736 logging_writer.py:48] [27500] global_step=27500, grad_norm=0.21535396575927734, loss=3.902862071990967
I0305 23:51:41.708384 139758764189440 logging_writer.py:48] [27600] global_step=27600, grad_norm=0.22301054000854492, loss=3.984342336654663
I0305 23:52:16.210194 139758755796736 logging_writer.py:48] [27700] global_step=27700, grad_norm=0.1785982847213745, loss=3.9235904216766357
I0305 23:52:50.709767 139758764189440 logging_writer.py:48] [27800] global_step=27800, grad_norm=0.3081367313861847, loss=3.9958362579345703
I0305 23:53:25.193233 139758755796736 logging_writer.py:48] [27900] global_step=27900, grad_norm=0.20213718712329865, loss=3.9236299991607666
I0305 23:53:59.698012 139758764189440 logging_writer.py:48] [28000] global_step=28000, grad_norm=0.22515861690044403, loss=3.944035530090332
I0305 23:54:34.191878 139758755796736 logging_writer.py:48] [28100] global_step=28100, grad_norm=0.22808073461055756, loss=3.985011100769043
I0305 23:55:08.672842 139758764189440 logging_writer.py:48] [28200] global_step=28200, grad_norm=0.3408561050891876, loss=3.9610142707824707
I0305 23:55:43.189523 139758755796736 logging_writer.py:48] [28300] global_step=28300, grad_norm=0.44291460514068604, loss=4.004221439361572
I0305 23:56:17.684034 139758764189440 logging_writer.py:48] [28400] global_step=28400, grad_norm=0.23150183260440826, loss=3.927363395690918
I0305 23:56:52.213277 139758755796736 logging_writer.py:48] [28500] global_step=28500, grad_norm=0.18093271553516388, loss=3.938687324523926
I0305 23:57:26.729224 139758764189440 logging_writer.py:48] [28600] global_step=28600, grad_norm=0.18867211043834686, loss=3.8793792724609375
I0305 23:58:01.244313 139758755796736 logging_writer.py:48] [28700] global_step=28700, grad_norm=0.1796375811100006, loss=3.9376208782196045
I0305 23:58:35.760048 139758764189440 logging_writer.py:48] [28800] global_step=28800, grad_norm=0.1947299987077713, loss=3.9912173748016357
I0305 23:59:10.268257 139758755796736 logging_writer.py:48] [28900] global_step=28900, grad_norm=0.23392218351364136, loss=3.933946132659912
I0305 23:59:44.784671 139758764189440 logging_writer.py:48] [29000] global_step=29000, grad_norm=0.18488666415214539, loss=3.9708330631256104
I0306 00:00:19.307632 139758755796736 logging_writer.py:48] [29100] global_step=29100, grad_norm=0.27885153889656067, loss=3.9926180839538574
I0306 00:00:53.826771 139758764189440 logging_writer.py:48] [29200] global_step=29200, grad_norm=0.1965208351612091, loss=3.969965934753418
I0306 00:00:55.908948 139902437545152 spec.py:321] Evaluating on the training split.
I0306 00:00:58.526139 139902437545152 workload.py:181] Translating evaluation dataset.
I0306 00:04:20.592434 139902437545152 spec.py:333] Evaluating on the validation split.
I0306 00:04:23.204763 139902437545152 workload.py:181] Translating evaluation dataset.
I0306 00:07:31.523628 139902437545152 spec.py:349] Evaluating on the test split.
I0306 00:07:34.135162 139902437545152 workload.py:181] Translating evaluation dataset.
I0306 00:10:41.938707 139902437545152 submission_runner.py:469] Time since start: 17863.10s, 	Step: 29207, 	{'train/accuracy': 0.6426064372062683, 'train/loss': 1.9268392324447632, 'train/bleu': 31.19619603300917, 'validation/accuracy': 0.6654512882232666, 'validation/loss': 1.7741438150405884, 'validation/bleu': 28.504088752337683, 'validation/num_examples': 3000, 'test/accuracy': 0.6771173477172852, 'test/loss': 1.702561855316162, 'test/bleu': 28.1172874472845, 'test/num_examples': 3003, 'score': 10105.749613285065, 'total_duration': 17863.09965658188, 'accumulated_submission_time': 10105.749613285065, 'accumulated_eval_time': 7755.366237401962, 'accumulated_logging_time': 0.2306830883026123}
I0306 00:10:41.950672 139758755796736 logging_writer.py:48] [29207] accumulated_eval_time=7755.37, accumulated_logging_time=0.230683, accumulated_submission_time=10105.7, global_step=29207, preemption_count=0, score=10105.7, test/accuracy=0.677117, test/bleu=28.1173, test/loss=1.70256, test/num_examples=3003, total_duration=17863.1, train/accuracy=0.642606, train/bleu=31.1962, train/loss=1.92684, validation/accuracy=0.665451, validation/bleu=28.5041, validation/loss=1.77414, validation/num_examples=3000
I0306 00:11:14.381159 139758764189440 logging_writer.py:48] [29300] global_step=29300, grad_norm=0.27690765261650085, loss=3.963972330093384
I0306 00:11:48.889333 139758755796736 logging_writer.py:48] [29400] global_step=29400, grad_norm=0.19016791880130768, loss=3.9088659286499023
I0306 00:12:23.385540 139758764189440 logging_writer.py:48] [29500] global_step=29500, grad_norm=0.1880706250667572, loss=3.932264566421509
I0306 00:12:57.870213 139758755796736 logging_writer.py:48] [29600] global_step=29600, grad_norm=0.2117784470319748, loss=3.9474940299987793
I0306 00:13:32.357488 139758764189440 logging_writer.py:48] [29700] global_step=29700, grad_norm=0.21247106790542603, loss=3.9306139945983887
I0306 00:14:06.850870 139758755796736 logging_writer.py:48] [29800] global_step=29800, grad_norm=0.2050330638885498, loss=3.884674072265625
I0306 00:14:41.380503 139758764189440 logging_writer.py:48] [29900] global_step=29900, grad_norm=0.1860085278749466, loss=3.93532133102417
I0306 00:15:15.905512 139758755796736 logging_writer.py:48] [30000] global_step=30000, grad_norm=0.28053566813468933, loss=3.9405574798583984
I0306 00:15:50.443483 139758764189440 logging_writer.py:48] [30100] global_step=30100, grad_norm=0.1886286586523056, loss=3.9344239234924316
I0306 00:16:24.960506 139758755796736 logging_writer.py:48] [30200] global_step=30200, grad_norm=0.19367468357086182, loss=3.9064159393310547
I0306 00:16:59.490586 139758764189440 logging_writer.py:48] [30300] global_step=30300, grad_norm=0.19365930557250977, loss=3.9368505477905273
I0306 00:17:34.018813 139758755796736 logging_writer.py:48] [30400] global_step=30400, grad_norm=0.19297096133232117, loss=3.9289543628692627
I0306 00:18:08.552271 139758764189440 logging_writer.py:48] [30500] global_step=30500, grad_norm=0.18937654793262482, loss=3.910243511199951
I0306 00:18:43.060244 139758755796736 logging_writer.py:48] [30600] global_step=30600, grad_norm=0.2090144157409668, loss=3.9861817359924316
I0306 00:19:17.564339 139758764189440 logging_writer.py:48] [30700] global_step=30700, grad_norm=0.21716254949569702, loss=3.998710870742798
I0306 00:19:52.073605 139758755796736 logging_writer.py:48] [30800] global_step=30800, grad_norm=0.22353820502758026, loss=3.9425930976867676
I0306 00:20:26.616909 139758764189440 logging_writer.py:48] [30900] global_step=30900, grad_norm=0.22860006988048553, loss=3.9853944778442383
I0306 00:21:01.116726 139758755796736 logging_writer.py:48] [31000] global_step=31000, grad_norm=0.1899561583995819, loss=3.9144952297210693
I0306 00:21:35.621466 139758764189440 logging_writer.py:48] [31100] global_step=31100, grad_norm=0.20904269814491272, loss=3.9346933364868164
I0306 00:22:10.143559 139758755796736 logging_writer.py:48] [31200] global_step=31200, grad_norm=0.19168415665626526, loss=3.946615219116211
I0306 00:22:44.653586 139758764189440 logging_writer.py:48] [31300] global_step=31300, grad_norm=0.18238665163516998, loss=3.902745246887207
I0306 00:23:19.207541 139758755796736 logging_writer.py:48] [31400] global_step=31400, grad_norm=0.2585318386554718, loss=3.951937437057495
I0306 00:23:53.831722 139758764189440 logging_writer.py:48] [31500] global_step=31500, grad_norm=0.203356072306633, loss=3.9238743782043457
I0306 00:24:28.431252 139758755796736 logging_writer.py:48] [31600] global_step=31600, grad_norm=0.22809886932373047, loss=3.887261390686035
I0306 00:24:42.281023 139902437545152 spec.py:321] Evaluating on the training split.
I0306 00:24:44.901353 139902437545152 workload.py:181] Translating evaluation dataset.
I0306 00:27:35.594437 139902437545152 spec.py:333] Evaluating on the validation split.
I0306 00:27:38.210704 139902437545152 workload.py:181] Translating evaluation dataset.
I0306 00:30:14.497113 139902437545152 spec.py:349] Evaluating on the test split.
I0306 00:30:17.109286 139902437545152 workload.py:181] Translating evaluation dataset.
I0306 00:32:50.956555 139902437545152 submission_runner.py:469] Time since start: 19192.12s, 	Step: 31641, 	{'train/accuracy': 0.6673250198364258, 'train/loss': 1.7735683917999268, 'train/bleu': 33.23419434757288, 'validation/accuracy': 0.6676760911941528, 'validation/loss': 1.754375696182251, 'validation/bleu': 29.049967447668795, 'validation/num_examples': 3000, 'test/accuracy': 0.6797010898590088, 'test/loss': 1.6794365644454956, 'test/bleu': 28.71526544515316, 'test/num_examples': 3003, 'score': 10945.931043863297, 'total_duration': 19192.117519378662, 'accumulated_submission_time': 10945.931043863297, 'accumulated_eval_time': 8244.041717529297, 'accumulated_logging_time': 0.2508728504180908}
I0306 00:32:50.968758 139758764189440 logging_writer.py:48] [31641] accumulated_eval_time=8244.04, accumulated_logging_time=0.250873, accumulated_submission_time=10945.9, global_step=31641, preemption_count=0, score=10945.9, test/accuracy=0.679701, test/bleu=28.7153, test/loss=1.67944, test/num_examples=3003, total_duration=19192.1, train/accuracy=0.667325, train/bleu=33.2342, train/loss=1.77357, validation/accuracy=0.667676, validation/bleu=29.05, validation/loss=1.75438, validation/num_examples=3000
I0306 00:33:11.686226 139758755796736 logging_writer.py:48] [31700] global_step=31700, grad_norm=0.21236488223075867, loss=3.913029909133911
I0306 00:33:46.240638 139758764189440 logging_writer.py:48] [31800] global_step=31800, grad_norm=0.21710605919361115, loss=3.9341020584106445
I0306 00:34:20.778419 139758755796736 logging_writer.py:48] [31900] global_step=31900, grad_norm=0.20289185643196106, loss=3.9525046348571777
I0306 00:34:55.300869 139758764189440 logging_writer.py:48] [32000] global_step=32000, grad_norm=0.21653102338314056, loss=3.930280923843384
I0306 00:35:29.824894 139758755796736 logging_writer.py:48] [32100] global_step=32100, grad_norm=0.20277263224124908, loss=3.912790536880493
I0306 00:36:04.347253 139758764189440 logging_writer.py:48] [32200] global_step=32200, grad_norm=0.21164816617965698, loss=3.987175941467285
I0306 00:36:38.848833 139758755796736 logging_writer.py:48] [32300] global_step=32300, grad_norm=0.2098720818758011, loss=3.895481824874878
I0306 00:37:13.362853 139758764189440 logging_writer.py:48] [32400] global_step=32400, grad_norm=0.20668768882751465, loss=3.9400973320007324
I0306 00:37:47.848360 139758755796736 logging_writer.py:48] [32500] global_step=32500, grad_norm=0.19856031239032745, loss=3.9337353706359863
I0306 00:38:22.348421 139758764189440 logging_writer.py:48] [32600] global_step=32600, grad_norm=0.19925346970558167, loss=3.9377598762512207
I0306 00:38:56.866984 139758755796736 logging_writer.py:48] [32700] global_step=32700, grad_norm=0.18306557834148407, loss=3.902214527130127
I0306 00:39:31.375248 139758764189440 logging_writer.py:48] [32800] global_step=32800, grad_norm=0.26874542236328125, loss=3.9105823040008545
I0306 00:40:05.889578 139758755796736 logging_writer.py:48] [32900] global_step=32900, grad_norm=0.19259338080883026, loss=3.921172618865967
I0306 00:40:40.393009 139758764189440 logging_writer.py:48] [33000] global_step=33000, grad_norm=0.26459312438964844, loss=3.888693332672119
I0306 00:41:14.886218 139758755796736 logging_writer.py:48] [33100] global_step=33100, grad_norm=0.20215316116809845, loss=3.923408031463623
I0306 00:41:49.368539 139758764189440 logging_writer.py:48] [33200] global_step=33200, grad_norm=0.20295438170433044, loss=3.8940908908843994
I0306 00:42:23.915376 139758755796736 logging_writer.py:48] [33300] global_step=33300, grad_norm=0.23378171026706696, loss=3.9937782287597656
I0306 00:42:58.395744 139758764189440 logging_writer.py:48] [33400] global_step=33400, grad_norm=0.2084192931652069, loss=3.9321255683898926
I0306 00:43:32.902109 139758755796736 logging_writer.py:48] [33500] global_step=33500, grad_norm=0.18915604054927826, loss=5.627646446228027
I0306 00:44:07.397315 139758764189440 logging_writer.py:48] [33600] global_step=33600, grad_norm=1.5001662969589233, loss=4.948622226715088
I0306 00:44:41.918112 139758755796736 logging_writer.py:48] [33700] global_step=33700, grad_norm=0.24925079941749573, loss=3.9678666591644287
I0306 00:45:16.440414 139758764189440 logging_writer.py:48] [33800] global_step=33800, grad_norm=0.22628137469291687, loss=3.869755744934082
I0306 00:45:50.961456 139758755796736 logging_writer.py:48] [33900] global_step=33900, grad_norm=0.24963408708572388, loss=3.9238317012786865
I0306 00:46:25.491627 139758764189440 logging_writer.py:48] [34000] global_step=34000, grad_norm=0.2238793671131134, loss=3.9600183963775635
I0306 00:46:51.035539 139902437545152 spec.py:321] Evaluating on the training split.
I0306 00:46:53.646280 139902437545152 workload.py:181] Translating evaluation dataset.
I0306 00:49:43.982405 139902437545152 spec.py:333] Evaluating on the validation split.
I0306 00:49:46.600394 139902437545152 workload.py:181] Translating evaluation dataset.
I0306 00:52:22.935059 139902437545152 spec.py:349] Evaluating on the test split.
I0306 00:52:25.545378 139902437545152 workload.py:181] Translating evaluation dataset.
I0306 00:55:05.405110 139902437545152 submission_runner.py:469] Time since start: 20526.57s, 	Step: 34075, 	{'train/accuracy': 0.6514691114425659, 'train/loss': 1.8714592456817627, 'train/bleu': 31.947627286445925, 'validation/accuracy': 0.6667861342430115, 'validation/loss': 1.7505391836166382, 'validation/bleu': 28.83209301488511, 'validation/num_examples': 3000, 'test/accuracy': 0.6802803874015808, 'test/loss': 1.6794507503509521, 'test/bleu': 28.63501236329486, 'test/num_examples': 3003, 'score': 11785.844411849976, 'total_duration': 20526.566071748734, 'accumulated_submission_time': 11785.844411849976, 'accumulated_eval_time': 8738.411239385605, 'accumulated_logging_time': 0.27150917053222656}
I0306 00:55:05.417911 139758755796736 logging_writer.py:48] [34075] accumulated_eval_time=8738.41, accumulated_logging_time=0.271509, accumulated_submission_time=11785.8, global_step=34075, preemption_count=0, score=11785.8, test/accuracy=0.68028, test/bleu=28.635, test/loss=1.67945, test/num_examples=3003, total_duration=20526.6, train/accuracy=0.651469, train/bleu=31.9476, train/loss=1.87146, validation/accuracy=0.666786, validation/bleu=28.8321, validation/loss=1.75054, validation/num_examples=3000
I0306 00:55:14.382109 139758764189440 logging_writer.py:48] [34100] global_step=34100, grad_norm=0.21038305759429932, loss=3.9635443687438965
I0306 00:55:48.867744 139758755796736 logging_writer.py:48] [34200] global_step=34200, grad_norm=0.19222888350486755, loss=3.8716113567352295
I0306 00:56:23.388841 139758764189440 logging_writer.py:48] [34300] global_step=34300, grad_norm=0.22687207162380219, loss=3.9066295623779297
I0306 00:56:57.902529 139758755796736 logging_writer.py:48] [34400] global_step=34400, grad_norm=0.20729929208755493, loss=3.92846941947937
I0306 00:57:32.375117 139758764189440 logging_writer.py:48] [34500] global_step=34500, grad_norm=0.203024759888649, loss=3.9667768478393555
I0306 00:58:06.869046 139758755796736 logging_writer.py:48] [34600] global_step=34600, grad_norm=0.20007172226905823, loss=3.8924548625946045
I0306 00:58:41.341236 139758764189440 logging_writer.py:48] [34700] global_step=34700, grad_norm=0.2048344612121582, loss=3.895202875137329
I0306 00:59:15.814319 139758755796736 logging_writer.py:48] [34800] global_step=34800, grad_norm=0.19943515956401825, loss=3.9811058044433594
I0306 00:59:50.310288 139758764189440 logging_writer.py:48] [34900] global_step=34900, grad_norm=0.2092459499835968, loss=3.8708927631378174
I0306 01:00:24.827212 139758755796736 logging_writer.py:48] [35000] global_step=35000, grad_norm=0.3553062379360199, loss=3.9461982250213623
I0306 01:00:59.330568 139758764189440 logging_writer.py:48] [35100] global_step=35100, grad_norm=0.2195437252521515, loss=3.861137628555298
I0306 01:01:33.847664 139758755796736 logging_writer.py:48] [35200] global_step=35200, grad_norm=0.27988603711128235, loss=3.8954458236694336
I0306 01:02:08.324150 139758764189440 logging_writer.py:48] [35300] global_step=35300, grad_norm=0.19075734913349152, loss=3.903568744659424
I0306 01:02:42.824082 139758755796736 logging_writer.py:48] [35400] global_step=35400, grad_norm=0.19449853897094727, loss=3.929830551147461
I0306 01:03:17.325986 139758764189440 logging_writer.py:48] [35500] global_step=35500, grad_norm=0.2043006420135498, loss=3.940971612930298
I0306 01:03:51.817966 139758755796736 logging_writer.py:48] [35600] global_step=35600, grad_norm=0.19839318096637726, loss=3.962193012237549
I0306 01:04:26.304673 139758764189440 logging_writer.py:48] [35700] global_step=35700, grad_norm=0.21028734743595123, loss=3.876844882965088
I0306 01:05:00.821262 139758755796736 logging_writer.py:48] [35800] global_step=35800, grad_norm=0.20631030201911926, loss=3.897998571395874
I0306 01:05:35.333904 139758764189440 logging_writer.py:48] [35900] global_step=35900, grad_norm=0.21777726709842682, loss=3.928438663482666
I0306 01:06:09.848951 139758755796736 logging_writer.py:48] [36000] global_step=36000, grad_norm=0.226527139544487, loss=3.912482261657715
I0306 01:06:44.363158 139758764189440 logging_writer.py:48] [36100] global_step=36100, grad_norm=0.19683854281902313, loss=3.8920068740844727
I0306 01:07:18.904309 139758755796736 logging_writer.py:48] [36200] global_step=36200, grad_norm=0.22556263208389282, loss=3.8985047340393066
I0306 01:07:53.399129 139758764189440 logging_writer.py:48] [36300] global_step=36300, grad_norm=0.20490308105945587, loss=3.990386486053467
I0306 01:08:27.911997 139758755796736 logging_writer.py:48] [36400] global_step=36400, grad_norm=0.2137974053621292, loss=3.862948417663574
I0306 01:09:02.417633 139758764189440 logging_writer.py:48] [36500] global_step=36500, grad_norm=0.25109097361564636, loss=3.955665349960327
I0306 01:09:05.516698 139902437545152 spec.py:321] Evaluating on the training split.
I0306 01:09:08.130939 139902437545152 workload.py:181] Translating evaluation dataset.
I0306 01:11:54.511051 139902437545152 spec.py:333] Evaluating on the validation split.
I0306 01:11:57.122174 139902437545152 workload.py:181] Translating evaluation dataset.
I0306 01:14:33.487301 139902437545152 spec.py:349] Evaluating on the test split.
I0306 01:14:36.100362 139902437545152 workload.py:181] Translating evaluation dataset.
I0306 01:16:57.673808 139902437545152 submission_runner.py:469] Time since start: 21838.83s, 	Step: 36510, 	{'train/accuracy': 0.6509538888931274, 'train/loss': 1.875314712524414, 'train/bleu': 32.122094846082, 'validation/accuracy': 0.668232262134552, 'validation/loss': 1.744018793106079, 'validation/bleu': 29.19220812784533, 'validation/num_examples': 3000, 'test/accuracy': 0.6821225881576538, 'test/loss': 1.666090965270996, 'test/bleu': 28.56957321973769, 'test/num_examples': 3003, 'score': 12625.7977809906, 'total_duration': 21838.834758520126, 'accumulated_submission_time': 12625.7977809906, 'accumulated_eval_time': 9210.568281173706, 'accumulated_logging_time': 0.29282093048095703}
I0306 01:16:57.686208 139758755796736 logging_writer.py:48] [36510] accumulated_eval_time=9210.57, accumulated_logging_time=0.292821, accumulated_submission_time=12625.8, global_step=36510, preemption_count=0, score=12625.8, test/accuracy=0.682123, test/bleu=28.5696, test/loss=1.66609, test/num_examples=3003, total_duration=21838.8, train/accuracy=0.650954, train/bleu=32.1221, train/loss=1.87531, validation/accuracy=0.668232, validation/bleu=29.1922, validation/loss=1.74402, validation/num_examples=3000
I0306 01:17:29.044020 139758764189440 logging_writer.py:48] [36600] global_step=36600, grad_norm=0.2217477411031723, loss=3.884906053543091
I0306 01:18:03.535496 139758755796736 logging_writer.py:48] [36700] global_step=36700, grad_norm=0.20043185353279114, loss=3.858781337738037
I0306 01:18:38.008079 139758764189440 logging_writer.py:48] [36800] global_step=36800, grad_norm=0.2034902274608612, loss=3.875399351119995
I0306 01:19:12.477861 139758755796736 logging_writer.py:48] [36900] global_step=36900, grad_norm=0.3190964162349701, loss=3.8753037452697754
I0306 01:19:46.984582 139758764189440 logging_writer.py:48] [37000] global_step=37000, grad_norm=0.22070583701133728, loss=3.8957178592681885
I0306 01:20:21.483258 139758755796736 logging_writer.py:48] [37100] global_step=37100, grad_norm=0.22462835907936096, loss=3.8964407444000244
I0306 01:20:55.995124 139758764189440 logging_writer.py:48] [37200] global_step=37200, grad_norm=0.1977745145559311, loss=3.933612823486328
I0306 01:21:30.507450 139758755796736 logging_writer.py:48] [37300] global_step=37300, grad_norm=0.21329715847969055, loss=3.906714677810669
I0306 01:22:05.023371 139758764189440 logging_writer.py:48] [37400] global_step=37400, grad_norm=0.21995903551578522, loss=3.897498369216919
I0306 01:22:39.576866 139758755796736 logging_writer.py:48] [37500] global_step=37500, grad_norm=0.21579641103744507, loss=3.896272897720337
I0306 01:23:14.105474 139758764189440 logging_writer.py:48] [37600] global_step=37600, grad_norm=0.19945593178272247, loss=3.889012575149536
I0306 01:23:48.647127 139758755796736 logging_writer.py:48] [37700] global_step=37700, grad_norm=0.24654461443424225, loss=3.901123285293579
I0306 01:24:23.169760 139758764189440 logging_writer.py:48] [37800] global_step=37800, grad_norm=0.22462376952171326, loss=3.920198440551758
I0306 01:24:57.665330 139758755796736 logging_writer.py:48] [37900] global_step=37900, grad_norm=0.2726784944534302, loss=3.8716983795166016
I0306 01:25:32.157691 139758764189440 logging_writer.py:48] [38000] global_step=38000, grad_norm=0.2461474984884262, loss=3.980883836746216
I0306 01:26:06.665436 139758755796736 logging_writer.py:48] [38100] global_step=38100, grad_norm=0.2179185450077057, loss=3.943748712539673
I0306 01:26:41.167135 139758764189440 logging_writer.py:48] [38200] global_step=38200, grad_norm=0.24569684267044067, loss=3.8879356384277344
I0306 01:27:15.661079 139758755796736 logging_writer.py:48] [38300] global_step=38300, grad_norm=0.21112653613090515, loss=3.892521619796753
I0306 01:27:50.133104 139758764189440 logging_writer.py:48] [38400] global_step=38400, grad_norm=0.21444171667099, loss=3.8737387657165527
I0306 01:28:24.605681 139758755796736 logging_writer.py:48] [38500] global_step=38500, grad_norm=0.21439556777477264, loss=3.910029172897339
I0306 01:28:59.096609 139758764189440 logging_writer.py:48] [38600] global_step=38600, grad_norm=0.22608311474323273, loss=3.93322491645813
I0306 01:29:33.609064 139758755796736 logging_writer.py:48] [38700] global_step=38700, grad_norm=0.23671476542949677, loss=3.8455464839935303
I0306 01:30:08.093757 139758764189440 logging_writer.py:48] [38800] global_step=38800, grad_norm=0.21564547717571259, loss=3.9194681644439697
I0306 01:30:42.591875 139758755796736 logging_writer.py:48] [38900] global_step=38900, grad_norm=0.24008624255657196, loss=3.953517198562622
I0306 01:30:57.790163 139902437545152 spec.py:321] Evaluating on the training split.
I0306 01:31:00.401401 139902437545152 workload.py:181] Translating evaluation dataset.
I0306 01:34:04.560801 139902437545152 spec.py:333] Evaluating on the validation split.
I0306 01:34:07.159783 139902437545152 workload.py:181] Translating evaluation dataset.
I0306 01:36:47.102272 139902437545152 spec.py:349] Evaluating on the test split.
I0306 01:36:49.720098 139902437545152 workload.py:181] Translating evaluation dataset.
I0306 01:39:20.049844 139902437545152 submission_runner.py:469] Time since start: 23181.21s, 	Step: 38945, 	{'train/accuracy': 0.661736249923706, 'train/loss': 1.8002737760543823, 'train/bleu': 32.833826159582145, 'validation/accuracy': 0.6705683469772339, 'validation/loss': 1.7332216501235962, 'validation/bleu': 29.18730435959975, 'validation/num_examples': 3000, 'test/accuracy': 0.684926450252533, 'test/loss': 1.6572619676589966, 'test/bleu': 28.714115980440354, 'test/num_examples': 3003, 'score': 13465.75188422203, 'total_duration': 23181.210790157318, 'accumulated_submission_time': 13465.75188422203, 'accumulated_eval_time': 9712.82789182663, 'accumulated_logging_time': 0.31394290924072266}
I0306 01:39:20.062358 139758764189440 logging_writer.py:48] [38945] accumulated_eval_time=9712.83, accumulated_logging_time=0.313943, accumulated_submission_time=13465.8, global_step=38945, preemption_count=0, score=13465.8, test/accuracy=0.684926, test/bleu=28.7141, test/loss=1.65726, test/num_examples=3003, total_duration=23181.2, train/accuracy=0.661736, train/bleu=32.8338, train/loss=1.80027, validation/accuracy=0.670568, validation/bleu=29.1873, validation/loss=1.73322, validation/num_examples=3000
I0306 01:39:39.384661 139758755796736 logging_writer.py:48] [39000] global_step=39000, grad_norm=0.24159789085388184, loss=3.9841392040252686
I0306 01:40:13.870642 139758764189440 logging_writer.py:48] [39100] global_step=39100, grad_norm=0.23876599967479706, loss=3.9125847816467285
I0306 01:40:48.388545 139758755796736 logging_writer.py:48] [39200] global_step=39200, grad_norm=0.23217913508415222, loss=3.944847583770752
I0306 01:41:22.886785 139758764189440 logging_writer.py:48] [39300] global_step=39300, grad_norm=0.23801186680793762, loss=3.938321590423584
I0306 01:41:57.345721 139758755796736 logging_writer.py:48] [39400] global_step=39400, grad_norm=0.3125881850719452, loss=3.875337839126587
I0306 01:42:31.847903 139758764189440 logging_writer.py:48] [39500] global_step=39500, grad_norm=0.20730376243591309, loss=3.8931617736816406
I0306 01:43:06.339418 139758755796736 logging_writer.py:48] [39600] global_step=39600, grad_norm=0.21792392432689667, loss=4.001155853271484
I0306 01:43:40.860774 139758764189440 logging_writer.py:48] [39700] global_step=39700, grad_norm=0.23068155348300934, loss=3.8955535888671875
I0306 01:44:15.356251 139758755796736 logging_writer.py:48] [39800] global_step=39800, grad_norm=0.26555347442626953, loss=3.9113409519195557
I0306 01:44:49.866410 139758764189440 logging_writer.py:48] [39900] global_step=39900, grad_norm=0.20831546187400818, loss=3.8683979511260986
I0306 01:45:24.392908 139758755796736 logging_writer.py:48] [40000] global_step=40000, grad_norm=0.23080852627754211, loss=3.992236375808716
I0306 01:45:58.895881 139758764189440 logging_writer.py:48] [40100] global_step=40100, grad_norm=0.2347024381160736, loss=3.885101079940796
I0306 01:46:33.425246 139758755796736 logging_writer.py:48] [40200] global_step=40200, grad_norm=0.23311948776245117, loss=3.8842341899871826
I0306 01:47:07.910779 139758764189440 logging_writer.py:48] [40300] global_step=40300, grad_norm=0.22336453199386597, loss=3.9183509349823
I0306 01:47:42.449835 139758755796736 logging_writer.py:48] [40400] global_step=40400, grad_norm=0.2239191234111786, loss=3.9069156646728516
I0306 01:48:16.972784 139758764189440 logging_writer.py:48] [40500] global_step=40500, grad_norm=0.24893993139266968, loss=3.9263837337493896
I0306 01:48:51.474191 139758755796736 logging_writer.py:48] [40600] global_step=40600, grad_norm=0.22814321517944336, loss=3.8787145614624023
I0306 01:49:25.984673 139758764189440 logging_writer.py:48] [40700] global_step=40700, grad_norm=0.23085986077785492, loss=3.896195650100708
I0306 01:50:00.487640 139758755796736 logging_writer.py:48] [40800] global_step=40800, grad_norm=0.22200725972652435, loss=3.871234655380249
I0306 01:50:34.969536 139758764189440 logging_writer.py:48] [40900] global_step=40900, grad_norm=0.22707843780517578, loss=3.9038784503936768
I0306 01:51:09.464305 139758755796736 logging_writer.py:48] [41000] global_step=41000, grad_norm=0.2138509750366211, loss=3.8508427143096924
I0306 01:51:43.963765 139758764189440 logging_writer.py:48] [41100] global_step=41100, grad_norm=0.23185387253761292, loss=3.8821630477905273
I0306 01:52:18.449255 139758755796736 logging_writer.py:48] [41200] global_step=41200, grad_norm=0.21359653770923615, loss=3.892486095428467
I0306 01:52:52.968612 139758764189440 logging_writer.py:48] [41300] global_step=41300, grad_norm=0.28036725521087646, loss=3.8593709468841553
I0306 01:53:20.207913 139902437545152 spec.py:321] Evaluating on the training split.
I0306 01:53:22.822690 139902437545152 workload.py:181] Translating evaluation dataset.
I0306 01:56:04.157609 139902437545152 spec.py:333] Evaluating on the validation split.
I0306 01:56:06.766207 139902437545152 workload.py:181] Translating evaluation dataset.
I0306 01:58:47.991070 139902437545152 spec.py:349] Evaluating on the test split.
I0306 01:58:50.604912 139902437545152 workload.py:181] Translating evaluation dataset.
I0306 02:01:22.602918 139902437545152 submission_runner.py:469] Time since start: 24503.76s, 	Step: 41380, 	{'train/accuracy': 0.6569118499755859, 'train/loss': 1.818016529083252, 'train/bleu': 32.01436866281932, 'validation/accuracy': 0.6717672348022461, 'validation/loss': 1.7065868377685547, 'validation/bleu': 29.14190404075308, 'validation/num_examples': 3000, 'test/accuracy': 0.6858417391777039, 'test/loss': 1.62874436378479, 'test/bleu': 28.94579133165665, 'test/num_examples': 3003, 'score': 14305.7463722229, 'total_duration': 24503.763870954514, 'accumulated_submission_time': 14305.7463722229, 'accumulated_eval_time': 10195.222837209702, 'accumulated_logging_time': 0.33432865142822266}
I0306 02:01:22.615839 139758755796736 logging_writer.py:48] [41380] accumulated_eval_time=10195.2, accumulated_logging_time=0.334329, accumulated_submission_time=14305.7, global_step=41380, preemption_count=0, score=14305.7, test/accuracy=0.685842, test/bleu=28.9458, test/loss=1.62874, test/num_examples=3003, total_duration=24503.8, train/accuracy=0.656912, train/bleu=32.0144, train/loss=1.81802, validation/accuracy=0.671767, validation/bleu=29.1419, validation/loss=1.70659, validation/num_examples=3000
I0306 02:01:29.858982 139758764189440 logging_writer.py:48] [41400] global_step=41400, grad_norm=0.21503382921218872, loss=3.898380756378174
I0306 02:02:04.345087 139758755796736 logging_writer.py:48] [41500] global_step=41500, grad_norm=0.21053782105445862, loss=3.8915188312530518
I0306 02:02:38.841663 139758764189440 logging_writer.py:48] [41600] global_step=41600, grad_norm=0.23484712839126587, loss=3.932403564453125
I0306 02:03:13.333259 139758755796736 logging_writer.py:48] [41700] global_step=41700, grad_norm=0.2324012666940689, loss=3.8742358684539795
I0306 02:03:47.812400 139758764189440 logging_writer.py:48] [41800] global_step=41800, grad_norm=0.2274499237537384, loss=3.9245851039886475
I0306 02:04:22.321216 139758755796736 logging_writer.py:48] [41900] global_step=41900, grad_norm=0.3152244985103607, loss=3.868034601211548
I0306 02:04:56.808841 139758764189440 logging_writer.py:48] [42000] global_step=42000, grad_norm=0.21946562826633453, loss=3.8984696865081787
I0306 02:05:31.302696 139758755796736 logging_writer.py:48] [42100] global_step=42100, grad_norm=0.23286695778369904, loss=3.8426568508148193
I0306 02:06:05.784068 139758764189440 logging_writer.py:48] [42200] global_step=42200, grad_norm=0.24748651683330536, loss=3.8943731784820557
I0306 02:06:40.244179 139758755796736 logging_writer.py:48] [42300] global_step=42300, grad_norm=0.27757561206817627, loss=3.850426197052002
I0306 02:07:14.740702 139758764189440 logging_writer.py:48] [42400] global_step=42400, grad_norm=0.2553400993347168, loss=3.9304277896881104
I0306 02:07:49.240252 139758755796736 logging_writer.py:48] [42500] global_step=42500, grad_norm=0.21631741523742676, loss=3.822843551635742
I0306 02:08:23.718708 139758764189440 logging_writer.py:48] [42600] global_step=42600, grad_norm=0.23643095791339874, loss=3.9598889350891113
I0306 02:08:58.199017 139758755796736 logging_writer.py:48] [42700] global_step=42700, grad_norm=0.23593094944953918, loss=3.866239070892334
I0306 02:09:32.696570 139758764189440 logging_writer.py:48] [42800] global_step=42800, grad_norm=0.27529528737068176, loss=3.893559455871582
I0306 02:10:07.188098 139758755796736 logging_writer.py:48] [42900] global_step=42900, grad_norm=0.2306792140007019, loss=3.9346420764923096
I0306 02:10:41.669801 139758764189440 logging_writer.py:48] [43000] global_step=43000, grad_norm=0.23611092567443848, loss=3.8638293743133545
I0306 02:11:16.152627 139758755796736 logging_writer.py:48] [43100] global_step=43100, grad_norm=0.2283075600862503, loss=3.8798751831054688
I0306 02:11:50.625685 139758764189440 logging_writer.py:48] [43200] global_step=43200, grad_norm=0.22916191816329956, loss=3.820045232772827
I0306 02:12:25.147181 139758755796736 logging_writer.py:48] [43300] global_step=43300, grad_norm=0.2689947485923767, loss=3.869398355484009
I0306 02:12:59.663891 139758764189440 logging_writer.py:48] [43400] global_step=43400, grad_norm=0.24100841581821442, loss=3.826428174972534
I0306 02:13:34.156008 139758755796736 logging_writer.py:48] [43500] global_step=43500, grad_norm=0.21093422174453735, loss=3.9377601146698
I0306 02:14:08.642118 139758764189440 logging_writer.py:48] [43600] global_step=43600, grad_norm=0.3096733093261719, loss=3.876011610031128
I0306 02:14:43.131357 139758755796736 logging_writer.py:48] [43700] global_step=43700, grad_norm=0.22382225096225739, loss=3.8068599700927734
I0306 02:15:17.619441 139758764189440 logging_writer.py:48] [43800] global_step=43800, grad_norm=0.21743755042552948, loss=3.869065761566162
I0306 02:15:22.798212 139902437545152 spec.py:321] Evaluating on the training split.
I0306 02:15:25.415392 139902437545152 workload.py:181] Translating evaluation dataset.
I0306 02:19:28.799238 139902437545152 spec.py:333] Evaluating on the validation split.
I0306 02:19:31.402520 139902437545152 workload.py:181] Translating evaluation dataset.
I0306 02:23:28.358120 139902437545152 spec.py:349] Evaluating on the test split.
I0306 02:23:30.973258 139902437545152 workload.py:181] Translating evaluation dataset.
I0306 02:27:15.752133 139902437545152 submission_runner.py:469] Time since start: 26056.91s, 	Step: 43816, 	{'train/accuracy': 0.6561476588249207, 'train/loss': 1.8416184186935425, 'train/bleu': 32.46140068314001, 'validation/accuracy': 0.6723605394363403, 'validation/loss': 1.7095853090286255, 'validation/bleu': 29.260692902819514, 'validation/num_examples': 3000, 'test/accuracy': 0.6873131990432739, 'test/loss': 1.6285985708236694, 'test/bleu': 28.64462975518613, 'test/num_examples': 3003, 'score': 15145.782103538513, 'total_duration': 26056.913061380386, 'accumulated_submission_time': 15145.782103538513, 'accumulated_eval_time': 10908.176669120789, 'accumulated_logging_time': 0.35532498359680176}
I0306 02:27:15.766044 139758755796736 logging_writer.py:48] [43816] accumulated_eval_time=10908.2, accumulated_logging_time=0.355325, accumulated_submission_time=15145.8, global_step=43816, preemption_count=0, score=15145.8, test/accuracy=0.687313, test/bleu=28.6446, test/loss=1.6286, test/num_examples=3003, total_duration=26056.9, train/accuracy=0.656148, train/bleu=32.4614, train/loss=1.84162, validation/accuracy=0.672361, validation/bleu=29.2607, validation/loss=1.70959, validation/num_examples=3000
I0306 02:27:45.025865 139758764189440 logging_writer.py:48] [43900] global_step=43900, grad_norm=0.25069665908813477, loss=3.9093592166900635
I0306 02:28:19.470172 139758755796736 logging_writer.py:48] [44000] global_step=44000, grad_norm=0.23102335631847382, loss=3.8875210285186768
I0306 02:28:53.959017 139758764189440 logging_writer.py:48] [44100] global_step=44100, grad_norm=0.20691417157649994, loss=3.8327066898345947
I0306 02:29:28.400933 139758755796736 logging_writer.py:48] [44200] global_step=44200, grad_norm=0.22966371476650238, loss=3.9293220043182373
I0306 02:30:02.862722 139758764189440 logging_writer.py:48] [44300] global_step=44300, grad_norm=0.218446284532547, loss=3.821532726287842
I0306 02:30:37.327399 139758755796736 logging_writer.py:48] [44400] global_step=44400, grad_norm=0.25409966707229614, loss=3.8869142532348633
I0306 02:31:11.811682 139758764189440 logging_writer.py:48] [44500] global_step=44500, grad_norm=0.24635569751262665, loss=3.9257450103759766
I0306 02:31:46.285581 139758755796736 logging_writer.py:48] [44600] global_step=44600, grad_norm=0.23893234133720398, loss=3.919994592666626
I0306 02:32:20.778681 139758764189440 logging_writer.py:48] [44700] global_step=44700, grad_norm=0.21744930744171143, loss=3.815335512161255
I0306 02:32:55.293432 139758755796736 logging_writer.py:48] [44800] global_step=44800, grad_norm=0.22875918447971344, loss=3.8985848426818848
I0306 02:33:29.818712 139758764189440 logging_writer.py:48] [44900] global_step=44900, grad_norm=0.23162458837032318, loss=3.9133012294769287
I0306 02:34:04.301193 139758755796736 logging_writer.py:48] [45000] global_step=45000, grad_norm=0.2419966459274292, loss=3.905740976333618
I0306 02:34:38.803188 139758764189440 logging_writer.py:48] [45100] global_step=45100, grad_norm=0.2322600781917572, loss=3.8935604095458984
I0306 02:35:13.306348 139758755796736 logging_writer.py:48] [45200] global_step=45200, grad_norm=0.227801114320755, loss=3.858466863632202
I0306 02:35:47.813763 139758764189440 logging_writer.py:48] [45300] global_step=45300, grad_norm=0.27679404616355896, loss=3.8754453659057617
I0306 02:36:22.283991 139758755796736 logging_writer.py:48] [45400] global_step=45400, grad_norm=0.23418833315372467, loss=3.852731227874756
I0306 02:36:56.808690 139758764189440 logging_writer.py:48] [45500] global_step=45500, grad_norm=0.24996605515480042, loss=3.893841028213501
I0306 02:37:31.322890 139758755796736 logging_writer.py:48] [45600] global_step=45600, grad_norm=0.23743078112602234, loss=3.845087766647339
I0306 02:38:05.834567 139758764189440 logging_writer.py:48] [45700] global_step=45700, grad_norm=0.2336498349905014, loss=3.9587388038635254
I0306 02:38:40.343843 139758755796736 logging_writer.py:48] [45800] global_step=45800, grad_norm=0.2468843013048172, loss=3.8940608501434326
I0306 02:39:14.851652 139758764189440 logging_writer.py:48] [45900] global_step=45900, grad_norm=0.2426406890153885, loss=3.9006242752075195
I0306 02:39:49.369787 139758755796736 logging_writer.py:48] [46000] global_step=46000, grad_norm=0.2649557590484619, loss=3.9182851314544678
I0306 02:40:23.854853 139758764189440 logging_writer.py:48] [46100] global_step=46100, grad_norm=0.2534348666667938, loss=3.9009792804718018
I0306 02:40:58.305181 139758755796736 logging_writer.py:48] [46200] global_step=46200, grad_norm=0.2327117770910263, loss=3.9463558197021484
I0306 02:41:15.884517 139902437545152 spec.py:321] Evaluating on the training split.
I0306 02:41:18.499398 139902437545152 workload.py:181] Translating evaluation dataset.
I0306 02:44:58.552139 139902437545152 spec.py:333] Evaluating on the validation split.
I0306 02:45:01.168831 139902437545152 workload.py:181] Translating evaluation dataset.
I0306 02:47:45.431999 139902437545152 spec.py:349] Evaluating on the test split.
I0306 02:47:48.050246 139902437545152 workload.py:181] Translating evaluation dataset.
I0306 02:50:25.494494 139902437545152 submission_runner.py:469] Time since start: 27446.66s, 	Step: 46252, 	{'train/accuracy': 0.6615137457847595, 'train/loss': 1.8036335706710815, 'train/bleu': 32.627632649451904, 'validation/accuracy': 0.674770712852478, 'validation/loss': 1.7107053995132446, 'validation/bleu': 29.3731631291291, 'validation/num_examples': 3000, 'test/accuracy': 0.6885992288589478, 'test/loss': 1.630706787109375, 'test/bleu': 29.006023035876307, 'test/num_examples': 3003, 'score': 15985.75174832344, 'total_duration': 27446.655457258224, 'accumulated_submission_time': 15985.75174832344, 'accumulated_eval_time': 11457.786594629288, 'accumulated_logging_time': 0.37760043144226074}
I0306 02:50:25.507632 139758764189440 logging_writer.py:48] [46252] accumulated_eval_time=11457.8, accumulated_logging_time=0.3776, accumulated_submission_time=15985.8, global_step=46252, preemption_count=0, score=15985.8, test/accuracy=0.688599, test/bleu=29.006, test/loss=1.63071, test/num_examples=3003, total_duration=27446.7, train/accuracy=0.661514, train/bleu=32.6276, train/loss=1.80363, validation/accuracy=0.674771, validation/bleu=29.3732, validation/loss=1.71071, validation/num_examples=3000
I0306 02:50:42.425140 139758755796736 logging_writer.py:48] [46300] global_step=46300, grad_norm=0.24728213250637054, loss=3.9297525882720947
I0306 02:51:16.921640 139758764189440 logging_writer.py:48] [46400] global_step=46400, grad_norm=0.29752105474472046, loss=3.85556960105896
I0306 02:51:51.406512 139758755796736 logging_writer.py:48] [46500] global_step=46500, grad_norm=0.22645443677902222, loss=3.8730924129486084
I0306 02:52:25.941884 139758764189440 logging_writer.py:48] [46600] global_step=46600, grad_norm=0.2408546805381775, loss=3.8581671714782715
I0306 02:53:00.431751 139758755796736 logging_writer.py:48] [46700] global_step=46700, grad_norm=0.22649037837982178, loss=3.8576242923736572
I0306 02:53:34.923123 139758764189440 logging_writer.py:48] [46800] global_step=46800, grad_norm=0.21587342023849487, loss=3.858586072921753
I0306 02:54:09.421343 139758755796736 logging_writer.py:48] [46900] global_step=46900, grad_norm=0.2577008605003357, loss=3.908267021179199
I0306 02:54:43.910755 139758764189440 logging_writer.py:48] [47000] global_step=47000, grad_norm=0.23156332969665527, loss=3.913823127746582
I0306 02:55:18.406742 139758755796736 logging_writer.py:48] [47100] global_step=47100, grad_norm=0.23767824470996857, loss=3.88785982131958
I0306 02:55:52.931159 139758764189440 logging_writer.py:48] [47200] global_step=47200, grad_norm=0.2568545639514923, loss=3.924591302871704
I0306 02:56:27.445566 139758755796736 logging_writer.py:48] [47300] global_step=47300, grad_norm=0.26422229409217834, loss=3.917494535446167
I0306 02:57:01.938091 139758764189440 logging_writer.py:48] [47400] global_step=47400, grad_norm=0.2804184854030609, loss=3.8116142749786377
I0306 02:57:36.401188 139758755796736 logging_writer.py:48] [47500] global_step=47500, grad_norm=0.276572585105896, loss=3.8973116874694824
I0306 02:58:10.897151 139758764189440 logging_writer.py:48] [47600] global_step=47600, grad_norm=0.22597987949848175, loss=3.8923044204711914
I0306 02:58:45.403910 139758755796736 logging_writer.py:48] [47700] global_step=47700, grad_norm=0.272295206785202, loss=3.9120852947235107
I0306 02:59:19.931603 139758764189440 logging_writer.py:48] [47800] global_step=47800, grad_norm=0.277077317237854, loss=3.8968372344970703
I0306 02:59:54.438243 139758755796736 logging_writer.py:48] [47900] global_step=47900, grad_norm=0.25099971890449524, loss=3.864719867706299
I0306 03:00:28.934628 139758764189440 logging_writer.py:48] [48000] global_step=48000, grad_norm=0.2498142570257187, loss=3.8633222579956055
I0306 03:01:03.407588 139758755796736 logging_writer.py:48] [48100] global_step=48100, grad_norm=0.2336818128824234, loss=3.8408665657043457
I0306 03:01:37.913314 139758764189440 logging_writer.py:48] [48200] global_step=48200, grad_norm=0.25684601068496704, loss=3.859635591506958
I0306 03:02:12.413777 139758755796736 logging_writer.py:48] [48300] global_step=48300, grad_norm=0.29545414447784424, loss=3.833477258682251
I0306 03:02:46.909045 139758764189440 logging_writer.py:48] [48400] global_step=48400, grad_norm=0.23271456360816956, loss=3.9100635051727295
I0306 03:03:21.400618 139758755796736 logging_writer.py:48] [48500] global_step=48500, grad_norm=0.23535983264446259, loss=3.900602340698242
I0306 03:03:55.913714 139758764189440 logging_writer.py:48] [48600] global_step=48600, grad_norm=0.25232240557670593, loss=3.860826015472412
I0306 03:04:25.587438 139902437545152 spec.py:321] Evaluating on the training split.
I0306 03:04:28.203988 139902437545152 workload.py:181] Translating evaluation dataset.
I0306 03:07:44.278970 139902437545152 spec.py:333] Evaluating on the validation split.
I0306 03:07:46.893939 139902437545152 workload.py:181] Translating evaluation dataset.
I0306 03:10:27.208870 139902437545152 spec.py:349] Evaluating on the test split.
I0306 03:10:29.825801 139902437545152 workload.py:181] Translating evaluation dataset.
I0306 03:13:14.118898 139902437545152 submission_runner.py:469] Time since start: 28815.28s, 	Step: 48687, 	{'train/accuracy': 0.6578896045684814, 'train/loss': 1.8279101848602295, 'train/bleu': 32.591304660575005, 'validation/accuracy': 0.6747089624404907, 'validation/loss': 1.711646556854248, 'validation/bleu': 29.245133285404577, 'validation/num_examples': 3000, 'test/accuracy': 0.6905109286308289, 'test/loss': 1.6266084909439087, 'test/bleu': 29.425035750226083, 'test/num_examples': 3003, 'score': 16825.683406829834, 'total_duration': 28815.279856443405, 'accumulated_submission_time': 16825.683406829834, 'accumulated_eval_time': 11986.31799530983, 'accumulated_logging_time': 0.3990492820739746}
I0306 03:13:14.134986 139758755796736 logging_writer.py:48] [48687] accumulated_eval_time=11986.3, accumulated_logging_time=0.399049, accumulated_submission_time=16825.7, global_step=48687, preemption_count=0, score=16825.7, test/accuracy=0.690511, test/bleu=29.425, test/loss=1.62661, test/num_examples=3003, total_duration=28815.3, train/accuracy=0.65789, train/bleu=32.5913, train/loss=1.82791, validation/accuracy=0.674709, validation/bleu=29.2451, validation/loss=1.71165, validation/num_examples=3000
I0306 03:13:18.973822 139758764189440 logging_writer.py:48] [48700] global_step=48700, grad_norm=0.2645549476146698, loss=3.9141929149627686
I0306 03:13:53.475919 139758755796736 logging_writer.py:48] [48800] global_step=48800, grad_norm=0.24294842779636383, loss=3.8331170082092285
I0306 03:14:28.000924 139758764189440 logging_writer.py:48] [48900] global_step=48900, grad_norm=0.2696305811405182, loss=3.964033842086792
I0306 03:15:02.511521 139758755796736 logging_writer.py:48] [49000] global_step=49000, grad_norm=0.33574748039245605, loss=3.8945367336273193
I0306 03:15:37.006540 139758764189440 logging_writer.py:48] [49100] global_step=49100, grad_norm=0.2360001653432846, loss=3.81205677986145
I0306 03:16:11.520131 139758755796736 logging_writer.py:48] [49200] global_step=49200, grad_norm=0.38344988226890564, loss=3.8709518909454346
I0306 03:16:46.001053 139758764189440 logging_writer.py:48] [49300] global_step=49300, grad_norm=0.2866458296775818, loss=3.814983606338501
I0306 03:17:20.503642 139758755796736 logging_writer.py:48] [49400] global_step=49400, grad_norm=0.24008822441101074, loss=3.848814010620117
I0306 03:17:55.004670 139758764189440 logging_writer.py:48] [49500] global_step=49500, grad_norm=0.33182191848754883, loss=3.8847317695617676
I0306 03:18:29.506172 139758755796736 logging_writer.py:48] [49600] global_step=49600, grad_norm=0.24494852125644684, loss=3.9177169799804688
I0306 03:19:03.997274 139758764189440 logging_writer.py:48] [49700] global_step=49700, grad_norm=0.25707656145095825, loss=3.8247945308685303
I0306 03:19:38.481478 139758755796736 logging_writer.py:48] [49800] global_step=49800, grad_norm=0.24922314286231995, loss=3.8959500789642334
I0306 03:20:12.947811 139758764189440 logging_writer.py:48] [49900] global_step=49900, grad_norm=0.34583768248558044, loss=3.856712818145752
I0306 03:20:47.413372 139758755796736 logging_writer.py:48] [50000] global_step=50000, grad_norm=0.2637074887752533, loss=3.830212354660034
I0306 03:21:21.859588 139758764189440 logging_writer.py:48] [50100] global_step=50100, grad_norm=0.23717951774597168, loss=3.8584601879119873
I0306 03:21:56.335805 139758755796736 logging_writer.py:48] [50200] global_step=50200, grad_norm=0.23018205165863037, loss=3.8370285034179688
I0306 03:22:30.782246 139758764189440 logging_writer.py:48] [50300] global_step=50300, grad_norm=0.24896948039531708, loss=3.823176145553589
I0306 03:23:05.262962 139758755796736 logging_writer.py:48] [50400] global_step=50400, grad_norm=0.26033419370651245, loss=3.8581440448760986
I0306 03:23:39.714780 139758764189440 logging_writer.py:48] [50500] global_step=50500, grad_norm=0.27406641840934753, loss=3.8882806301116943
I0306 03:24:14.215665 139758755796736 logging_writer.py:48] [50600] global_step=50600, grad_norm=0.22869442403316498, loss=3.858299493789673
I0306 03:24:48.693562 139758764189440 logging_writer.py:48] [50700] global_step=50700, grad_norm=0.2812884747982025, loss=3.82979416847229
I0306 03:25:23.180868 139758755796736 logging_writer.py:48] [50800] global_step=50800, grad_norm=0.248983234167099, loss=3.8803870677948
I0306 03:25:57.649745 139758764189440 logging_writer.py:48] [50900] global_step=50900, grad_norm=0.2396334856748581, loss=3.8863518238067627
I0306 03:26:32.080432 139758755796736 logging_writer.py:48] [51000] global_step=51000, grad_norm=0.2571656107902527, loss=3.8428053855895996
I0306 03:27:06.530852 139758764189440 logging_writer.py:48] [51100] global_step=51100, grad_norm=0.24434702098369598, loss=3.867418050765991
I0306 03:27:14.440697 139902437545152 spec.py:321] Evaluating on the training split.
I0306 03:27:17.050656 139902437545152 workload.py:181] Translating evaluation dataset.
I0306 03:30:30.186109 139902437545152 spec.py:333] Evaluating on the validation split.
I0306 03:30:32.798441 139902437545152 workload.py:181] Translating evaluation dataset.
I0306 03:33:07.809494 139902437545152 spec.py:349] Evaluating on the test split.
I0306 03:33:10.426673 139902437545152 workload.py:181] Translating evaluation dataset.
I0306 03:36:14.889692 139902437545152 submission_runner.py:469] Time since start: 30196.05s, 	Step: 51124, 	{'train/accuracy': 0.673754870891571, 'train/loss': 1.7215735912322998, 'train/bleu': 33.36355953415167, 'validation/accuracy': 0.6772056818008423, 'validation/loss': 1.6882215738296509, 'validation/bleu': 29.576554485971034, 'validation/num_examples': 3000, 'test/accuracy': 0.6914842128753662, 'test/loss': 1.605440378189087, 'test/bleu': 29.329503951261966, 'test/num_examples': 3003, 'score': 17665.840656280518, 'total_duration': 30196.05065202713, 'accumulated_submission_time': 17665.840656280518, 'accumulated_eval_time': 12526.766934394836, 'accumulated_logging_time': 0.42407870292663574}
I0306 03:36:14.905645 139758755796736 logging_writer.py:48] [51124] accumulated_eval_time=12526.8, accumulated_logging_time=0.424079, accumulated_submission_time=17665.8, global_step=51124, preemption_count=0, score=17665.8, test/accuracy=0.691484, test/bleu=29.3295, test/loss=1.60544, test/num_examples=3003, total_duration=30196.1, train/accuracy=0.673755, train/bleu=33.3636, train/loss=1.72157, validation/accuracy=0.677206, validation/bleu=29.5766, validation/loss=1.68822, validation/num_examples=3000
I0306 03:36:41.405874 139758764189440 logging_writer.py:48] [51200] global_step=51200, grad_norm=0.25218820571899414, loss=3.8752715587615967
I0306 03:37:15.829037 139758755796736 logging_writer.py:48] [51300] global_step=51300, grad_norm=0.25311967730522156, loss=3.8813605308532715
I0306 03:37:50.260738 139758764189440 logging_writer.py:48] [51400] global_step=51400, grad_norm=0.22890904545783997, loss=3.832148551940918
I0306 03:38:24.692130 139758755796736 logging_writer.py:48] [51500] global_step=51500, grad_norm=0.27210167050361633, loss=3.7866876125335693
I0306 03:38:59.172723 139758764189440 logging_writer.py:48] [51600] global_step=51600, grad_norm=0.3571101427078247, loss=3.908144235610962
I0306 03:39:33.605702 139758755796736 logging_writer.py:48] [51700] global_step=51700, grad_norm=0.2367771863937378, loss=3.794020175933838
I0306 03:40:08.070660 139758764189440 logging_writer.py:48] [51800] global_step=51800, grad_norm=0.2722504138946533, loss=3.8580572605133057
I0306 03:40:42.538629 139758755796736 logging_writer.py:48] [51900] global_step=51900, grad_norm=0.2821910083293915, loss=3.9065635204315186
I0306 03:41:16.973156 139758764189440 logging_writer.py:48] [52000] global_step=52000, grad_norm=0.23863950371742249, loss=3.856670618057251
I0306 03:41:51.434200 139758755796736 logging_writer.py:48] [52100] global_step=52100, grad_norm=0.23816369473934174, loss=3.7900190353393555
I0306 03:42:25.893370 139758764189440 logging_writer.py:48] [52200] global_step=52200, grad_norm=0.35115113854408264, loss=3.8591389656066895
I0306 03:43:00.353850 139758755796736 logging_writer.py:48] [52300] global_step=52300, grad_norm=0.2533799111843109, loss=3.8686954975128174
I0306 03:43:34.799509 139758764189440 logging_writer.py:48] [52400] global_step=52400, grad_norm=0.26848548650741577, loss=3.8444368839263916
I0306 03:44:09.252042 139758755796736 logging_writer.py:48] [52500] global_step=52500, grad_norm=0.2561705410480499, loss=3.8854548931121826
I0306 03:44:43.698682 139758764189440 logging_writer.py:48] [52600] global_step=52600, grad_norm=0.2868211567401886, loss=3.8499529361724854
I0306 03:45:18.120689 139758755796736 logging_writer.py:48] [52700] global_step=52700, grad_norm=0.24917536973953247, loss=3.8552708625793457
I0306 03:45:52.572977 139758764189440 logging_writer.py:48] [52800] global_step=52800, grad_norm=0.23797452449798584, loss=3.849748373031616
I0306 03:46:27.004763 139758755796736 logging_writer.py:48] [52900] global_step=52900, grad_norm=0.2568504214286804, loss=3.8425309658050537
I0306 03:47:01.476408 139758764189440 logging_writer.py:48] [53000] global_step=53000, grad_norm=0.2562941312789917, loss=3.8334505558013916
I0306 03:47:35.937881 139758755796736 logging_writer.py:48] [53100] global_step=53100, grad_norm=0.29261162877082825, loss=3.8933331966400146
I0306 03:48:10.385051 139758764189440 logging_writer.py:48] [53200] global_step=53200, grad_norm=0.27303728461265564, loss=3.8179924488067627
I0306 03:48:44.842546 139758755796736 logging_writer.py:48] [53300] global_step=53300, grad_norm=0.257232129573822, loss=3.862327814102173
I0306 03:49:19.300638 139758764189440 logging_writer.py:48] [53400] global_step=53400, grad_norm=0.25829964876174927, loss=3.823129653930664
I0306 03:49:53.753508 139758755796736 logging_writer.py:48] [53500] global_step=53500, grad_norm=0.25941359996795654, loss=3.8222172260284424
I0306 03:50:15.106959 139902437545152 spec.py:321] Evaluating on the training split.
I0306 03:50:17.717267 139902437545152 workload.py:181] Translating evaluation dataset.
I0306 03:53:04.145396 139902437545152 spec.py:333] Evaluating on the validation split.
I0306 03:53:06.744248 139902437545152 workload.py:181] Translating evaluation dataset.
I0306 03:55:43.102490 139902437545152 spec.py:349] Evaluating on the test split.
I0306 03:55:45.718103 139902437545152 workload.py:181] Translating evaluation dataset.
I0306 03:58:16.316080 139902437545152 submission_runner.py:469] Time since start: 31517.48s, 	Step: 53563, 	{'train/accuracy': 0.666636049747467, 'train/loss': 1.7679489850997925, 'train/bleu': 32.70166232392654, 'validation/accuracy': 0.6782068014144897, 'validation/loss': 1.6796603202819824, 'validation/bleu': 29.515353139881434, 'validation/num_examples': 3000, 'test/accuracy': 0.6935580968856812, 'test/loss': 1.5931921005249023, 'test/bleu': 29.523683682918122, 'test/num_examples': 3003, 'score': 18505.89375281334, 'total_duration': 31517.47703552246, 'accumulated_submission_time': 18505.89375281334, 'accumulated_eval_time': 13007.976008176804, 'accumulated_logging_time': 0.449481725692749}
I0306 03:58:16.330248 139758764189440 logging_writer.py:48] [53563] accumulated_eval_time=13008, accumulated_logging_time=0.449482, accumulated_submission_time=18505.9, global_step=53563, preemption_count=0, score=18505.9, test/accuracy=0.693558, test/bleu=29.5237, test/loss=1.59319, test/num_examples=3003, total_duration=31517.5, train/accuracy=0.666636, train/bleu=32.7017, train/loss=1.76795, validation/accuracy=0.678207, validation/bleu=29.5154, validation/loss=1.67966, validation/num_examples=3000
I0306 03:58:29.420220 139758755796736 logging_writer.py:48] [53600] global_step=53600, grad_norm=0.25613176822662354, loss=3.8399910926818848
I0306 03:59:03.833092 139758764189440 logging_writer.py:48] [53700] global_step=53700, grad_norm=0.24034662544727325, loss=3.8398783206939697
I0306 03:59:38.292071 139758755796736 logging_writer.py:48] [53800] global_step=53800, grad_norm=0.26400837302207947, loss=3.8570032119750977
I0306 04:00:12.723714 139758764189440 logging_writer.py:48] [53900] global_step=53900, grad_norm=0.2507259249687195, loss=3.863931179046631
I0306 04:00:47.179088 139758755796736 logging_writer.py:48] [54000] global_step=54000, grad_norm=0.2640587389469147, loss=3.861621856689453
I0306 04:01:21.636679 139758764189440 logging_writer.py:48] [54100] global_step=54100, grad_norm=0.24329012632369995, loss=3.830435037612915
I0306 04:01:56.077889 139758755796736 logging_writer.py:48] [54200] global_step=54200, grad_norm=0.23960167169570923, loss=3.824390411376953
I0306 04:02:30.517747 139758764189440 logging_writer.py:48] [54300] global_step=54300, grad_norm=0.2930919826030731, loss=3.8774352073669434
I0306 04:03:04.937123 139758755796736 logging_writer.py:48] [54400] global_step=54400, grad_norm=0.25011134147644043, loss=3.845472574234009
I0306 04:03:39.411521 139758764189440 logging_writer.py:48] [54500] global_step=54500, grad_norm=0.24283728003501892, loss=3.8201026916503906
I0306 04:04:13.865415 139758755796736 logging_writer.py:48] [54600] global_step=54600, grad_norm=0.2861509621143341, loss=3.817274332046509
I0306 04:04:48.322728 139758764189440 logging_writer.py:48] [54700] global_step=54700, grad_norm=0.2800196707248688, loss=3.865135669708252
I0306 04:05:22.770225 139758755796736 logging_writer.py:48] [54800] global_step=54800, grad_norm=0.2659949064254761, loss=3.8962693214416504
I0306 04:05:57.206492 139758764189440 logging_writer.py:48] [54900] global_step=54900, grad_norm=0.2667803168296814, loss=3.8887948989868164
I0306 04:06:31.644250 139758755796736 logging_writer.py:48] [55000] global_step=55000, grad_norm=0.24243296682834625, loss=3.8173763751983643
I0306 04:07:06.130781 139758764189440 logging_writer.py:48] [55100] global_step=55100, grad_norm=0.2762404978275299, loss=3.8553154468536377
I0306 04:07:40.590290 139758755796736 logging_writer.py:48] [55200] global_step=55200, grad_norm=0.2676428556442261, loss=3.865126848220825
I0306 04:08:15.070879 139758764189440 logging_writer.py:48] [55300] global_step=55300, grad_norm=0.244534432888031, loss=3.8543365001678467
I0306 04:08:49.522169 139758755796736 logging_writer.py:48] [55400] global_step=55400, grad_norm=0.27975523471832275, loss=3.8476297855377197
I0306 04:09:23.956078 139758764189440 logging_writer.py:48] [55500] global_step=55500, grad_norm=0.2964117228984833, loss=3.8245034217834473
I0306 04:09:58.427914 139758755796736 logging_writer.py:48] [55600] global_step=55600, grad_norm=0.31492486596107483, loss=3.82918119430542
I0306 04:10:32.891176 139758764189440 logging_writer.py:48] [55700] global_step=55700, grad_norm=0.25521811842918396, loss=3.834625720977783
I0306 04:11:07.341567 139758755796736 logging_writer.py:48] [55800] global_step=55800, grad_norm=0.32937225699424744, loss=3.8079216480255127
I0306 04:11:41.790088 139758764189440 logging_writer.py:48] [55900] global_step=55900, grad_norm=0.2822593152523041, loss=3.871687889099121
I0306 04:12:16.243798 139758755796736 logging_writer.py:48] [56000] global_step=56000, grad_norm=0.24072635173797607, loss=3.809950113296509
I0306 04:12:16.596202 139902437545152 spec.py:321] Evaluating on the training split.
I0306 04:12:19.211179 139902437545152 workload.py:181] Translating evaluation dataset.
I0306 04:15:40.208799 139902437545152 spec.py:333] Evaluating on the validation split.
I0306 04:15:42.814857 139902437545152 workload.py:181] Translating evaluation dataset.
I0306 04:18:48.997239 139902437545152 spec.py:349] Evaluating on the test split.
I0306 04:18:51.598313 139902437545152 workload.py:181] Translating evaluation dataset.
I0306 04:22:21.629930 139902437545152 submission_runner.py:469] Time since start: 32962.79s, 	Step: 56002, 	{'train/accuracy': 0.6629206538200378, 'train/loss': 1.7951312065124512, 'train/bleu': 32.96202748910094, 'validation/accuracy': 0.6785158514976501, 'validation/loss': 1.6791293621063232, 'validation/bleu': 29.548141942289305, 'validation/num_examples': 3000, 'test/accuracy': 0.6938709616661072, 'test/loss': 1.5931220054626465, 'test/bleu': 29.432093183896882, 'test/num_examples': 3003, 'score': 19346.01117825508, 'total_duration': 32962.79088115692, 'accumulated_submission_time': 19346.01117825508, 'accumulated_eval_time': 13613.009667396545, 'accumulated_logging_time': 0.4718954563140869}
I0306 04:22:21.645056 139758764189440 logging_writer.py:48] [56002] accumulated_eval_time=13613, accumulated_logging_time=0.471895, accumulated_submission_time=19346, global_step=56002, preemption_count=0, score=19346, test/accuracy=0.693871, test/bleu=29.4321, test/loss=1.59312, test/num_examples=3003, total_duration=32962.8, train/accuracy=0.662921, train/bleu=32.962, train/loss=1.79513, validation/accuracy=0.678516, validation/bleu=29.5481, validation/loss=1.67913, validation/num_examples=3000
I0306 04:22:55.728709 139758755796736 logging_writer.py:48] [56100] global_step=56100, grad_norm=0.2623317837715149, loss=3.8839564323425293
I0306 04:23:30.159007 139758764189440 logging_writer.py:48] [56200] global_step=56200, grad_norm=0.251528263092041, loss=3.813734769821167
I0306 04:24:04.622773 139758755796736 logging_writer.py:48] [56300] global_step=56300, grad_norm=0.24971765279769897, loss=3.7476882934570312
I0306 04:24:39.063501 139758764189440 logging_writer.py:48] [56400] global_step=56400, grad_norm=0.25115981698036194, loss=3.7879996299743652
I0306 04:25:13.526852 139758755796736 logging_writer.py:48] [56500] global_step=56500, grad_norm=0.2860744297504425, loss=3.853616952896118
I0306 04:25:48.043959 139758764189440 logging_writer.py:48] [56600] global_step=56600, grad_norm=0.24004122614860535, loss=3.8190674781799316
I0306 04:26:22.552219 139758755796736 logging_writer.py:48] [56700] global_step=56700, grad_norm=0.3304629921913147, loss=3.773027181625366
I0306 04:26:57.029529 139758764189440 logging_writer.py:48] [56800] global_step=56800, grad_norm=0.26421383023262024, loss=3.766741991043091
I0306 04:27:31.516192 139758755796736 logging_writer.py:48] [56900] global_step=56900, grad_norm=0.2859167158603668, loss=3.7825875282287598
I0306 04:28:06.001594 139758764189440 logging_writer.py:48] [57000] global_step=57000, grad_norm=0.25208455324172974, loss=3.806121587753296
I0306 04:28:40.509632 139758755796736 logging_writer.py:48] [57100] global_step=57100, grad_norm=0.2747310698032379, loss=3.8157455921173096
I0306 04:29:15.014154 139758764189440 logging_writer.py:48] [57200] global_step=57200, grad_norm=0.274037629365921, loss=3.9266762733459473
I0306 04:29:49.518888 139758755796736 logging_writer.py:48] [57300] global_step=57300, grad_norm=0.2626018524169922, loss=3.842884063720703
I0306 04:30:23.996396 139758764189440 logging_writer.py:48] [57400] global_step=57400, grad_norm=0.2369929552078247, loss=3.879789113998413
I0306 04:30:58.480060 139758755796736 logging_writer.py:48] [57500] global_step=57500, grad_norm=0.2909136116504669, loss=3.745863199234009
I0306 04:31:32.971543 139758764189440 logging_writer.py:48] [57600] global_step=57600, grad_norm=0.3052271604537964, loss=3.8784148693084717
I0306 04:32:07.471984 139758755796736 logging_writer.py:48] [57700] global_step=57700, grad_norm=0.2496555745601654, loss=3.8229565620422363
I0306 04:32:41.966107 139758764189440 logging_writer.py:48] [57800] global_step=57800, grad_norm=0.2666938900947571, loss=3.8318488597869873
I0306 04:33:16.463894 139758755796736 logging_writer.py:48] [57900] global_step=57900, grad_norm=0.27013254165649414, loss=3.824920892715454
I0306 04:33:50.971233 139758764189440 logging_writer.py:48] [58000] global_step=58000, grad_norm=0.24894596636295319, loss=3.7935378551483154
I0306 04:34:25.488637 139758755796736 logging_writer.py:48] [58100] global_step=58100, grad_norm=0.3174828290939331, loss=3.88458514213562
I0306 04:34:59.993215 139758764189440 logging_writer.py:48] [58200] global_step=58200, grad_norm=0.25470736622810364, loss=3.811187505722046
I0306 04:35:34.500714 139758755796736 logging_writer.py:48] [58300] global_step=58300, grad_norm=0.2920392155647278, loss=3.8749847412109375
I0306 04:36:09.013764 139758764189440 logging_writer.py:48] [58400] global_step=58400, grad_norm=0.28811511397361755, loss=3.86995792388916
I0306 04:36:21.769541 139902437545152 spec.py:321] Evaluating on the training split.
I0306 04:36:24.382596 139902437545152 workload.py:181] Translating evaluation dataset.
I0306 04:41:11.091781 139902437545152 spec.py:333] Evaluating on the validation split.
I0306 04:41:13.705557 139902437545152 workload.py:181] Translating evaluation dataset.
I0306 04:45:30.773875 139902437545152 spec.py:349] Evaluating on the test split.
I0306 04:45:33.391827 139902437545152 workload.py:181] Translating evaluation dataset.
I0306 04:49:27.856932 139902437545152 submission_runner.py:469] Time since start: 34589.02s, 	Step: 58438, 	{'train/accuracy': 0.6713078618049622, 'train/loss': 1.7255960702896118, 'train/bleu': 33.472461913434955, 'validation/accuracy': 0.680258572101593, 'validation/loss': 1.663615107536316, 'validation/bleu': 29.819428355457234, 'validation/num_examples': 3000, 'test/accuracy': 0.695736289024353, 'test/loss': 1.581506371498108, 'test/bleu': 29.394815717509346, 'test/num_examples': 3003, 'score': 20185.984320640564, 'total_duration': 34589.017896175385, 'accumulated_submission_time': 20185.984320640564, 'accumulated_eval_time': 14399.097005844116, 'accumulated_logging_time': 0.49561452865600586}
I0306 04:49:27.872620 139758755796736 logging_writer.py:48] [58438] accumulated_eval_time=14399.1, accumulated_logging_time=0.495615, accumulated_submission_time=20186, global_step=58438, preemption_count=0, score=20186, test/accuracy=0.695736, test/bleu=29.3948, test/loss=1.58151, test/num_examples=3003, total_duration=34589, train/accuracy=0.671308, train/bleu=33.4725, train/loss=1.7256, validation/accuracy=0.680259, validation/bleu=29.8194, validation/loss=1.66362, validation/num_examples=3000
I0306 04:49:49.561152 139758764189440 logging_writer.py:48] [58500] global_step=58500, grad_norm=0.24760101735591888, loss=3.795966386795044
I0306 04:50:24.021453 139758755796736 logging_writer.py:48] [58600] global_step=58600, grad_norm=0.2720842957496643, loss=3.8692564964294434
I0306 04:50:58.512984 139758764189440 logging_writer.py:48] [58700] global_step=58700, grad_norm=0.2396453619003296, loss=3.7528085708618164
I0306 04:51:33.027781 139758755796736 logging_writer.py:48] [58800] global_step=58800, grad_norm=0.2927807867527008, loss=3.8470187187194824
I0306 04:52:07.538035 139758764189440 logging_writer.py:48] [58900] global_step=58900, grad_norm=0.25520166754722595, loss=3.8731205463409424
I0306 04:52:42.045036 139758755796736 logging_writer.py:48] [59000] global_step=59000, grad_norm=0.2934668958187103, loss=3.9138424396514893
I0306 04:53:16.530831 139758764189440 logging_writer.py:48] [59100] global_step=59100, grad_norm=0.27057328820228577, loss=3.830042600631714
I0306 04:53:51.027778 139758755796736 logging_writer.py:48] [59200] global_step=59200, grad_norm=0.3104173541069031, loss=3.81295108795166
I0306 04:54:25.529711 139758764189440 logging_writer.py:48] [59300] global_step=59300, grad_norm=0.2636587619781494, loss=3.8572723865509033
I0306 04:55:00.029852 139758755796736 logging_writer.py:48] [59400] global_step=59400, grad_norm=0.2523209750652313, loss=3.8158950805664062
I0306 04:55:34.520054 139758764189440 logging_writer.py:48] [59500] global_step=59500, grad_norm=0.29174599051475525, loss=3.848154067993164
I0306 04:56:09.017024 139758755796736 logging_writer.py:48] [59600] global_step=59600, grad_norm=0.26346635818481445, loss=3.8088724613189697
I0306 04:56:43.528637 139758764189440 logging_writer.py:48] [59700] global_step=59700, grad_norm=0.2556956708431244, loss=3.8663644790649414
I0306 04:57:18.024470 139758755796736 logging_writer.py:48] [59800] global_step=59800, grad_norm=0.2447146773338318, loss=3.779569149017334
I0306 04:57:52.549078 139758764189440 logging_writer.py:48] [59900] global_step=59900, grad_norm=0.24382354319095612, loss=3.7614622116088867
I0306 04:58:27.033584 139758755796736 logging_writer.py:48] [60000] global_step=60000, grad_norm=0.2848754823207855, loss=3.7806265354156494
I0306 04:59:01.528417 139758764189440 logging_writer.py:48] [60100] global_step=60100, grad_norm=0.2939011752605438, loss=3.8592967987060547
I0306 04:59:35.996333 139758755796736 logging_writer.py:48] [60200] global_step=60200, grad_norm=0.2845633625984192, loss=3.791663885116577
I0306 05:00:10.459638 139758764189440 logging_writer.py:48] [60300] global_step=60300, grad_norm=0.27149325609207153, loss=3.8015153408050537
I0306 05:00:44.966996 139758755796736 logging_writer.py:48] [60400] global_step=60400, grad_norm=0.2919517457485199, loss=3.8644113540649414
I0306 05:01:19.477593 139758764189440 logging_writer.py:48] [60500] global_step=60500, grad_norm=0.2687993049621582, loss=3.842672348022461
I0306 05:01:54.017336 139758755796736 logging_writer.py:48] [60600] global_step=60600, grad_norm=0.28433579206466675, loss=3.878031015396118
I0306 05:02:28.519433 139758764189440 logging_writer.py:48] [60700] global_step=60700, grad_norm=0.26441994309425354, loss=3.827626943588257
I0306 05:03:03.018449 139758755796736 logging_writer.py:48] [60800] global_step=60800, grad_norm=0.23636358976364136, loss=3.812527894973755
I0306 05:03:27.872618 139902437545152 spec.py:321] Evaluating on the training split.
I0306 05:03:30.488476 139902437545152 workload.py:181] Translating evaluation dataset.
I0306 05:08:21.008581 139902437545152 spec.py:333] Evaluating on the validation split.
I0306 05:08:23.621681 139902437545152 workload.py:181] Translating evaluation dataset.
I0306 05:11:43.327545 139902437545152 spec.py:349] Evaluating on the test split.
I0306 05:11:45.935631 139902437545152 workload.py:181] Translating evaluation dataset.
I0306 05:14:49.276460 139902437545152 submission_runner.py:469] Time since start: 36110.44s, 	Step: 60873, 	{'train/accuracy': 0.6695363521575928, 'train/loss': 1.7522913217544556, 'train/bleu': 33.44945571442318, 'validation/accuracy': 0.6807406544685364, 'validation/loss': 1.6651467084884644, 'validation/bleu': 29.62307318371099, 'validation/num_examples': 3000, 'test/accuracy': 0.6951106786727905, 'test/loss': 1.5814741849899292, 'test/bleu': 29.65481277059179, 'test/num_examples': 3003, 'score': 21025.833126306534, 'total_duration': 36110.43742418289, 'accumulated_submission_time': 21025.833126306534, 'accumulated_eval_time': 15080.500798940659, 'accumulated_logging_time': 0.5203084945678711}
I0306 05:14:49.291181 139758764189440 logging_writer.py:48] [60873] accumulated_eval_time=15080.5, accumulated_logging_time=0.520308, accumulated_submission_time=21025.8, global_step=60873, preemption_count=0, score=21025.8, test/accuracy=0.695111, test/bleu=29.6548, test/loss=1.58147, test/num_examples=3003, total_duration=36110.4, train/accuracy=0.669536, train/bleu=33.4495, train/loss=1.75229, validation/accuracy=0.680741, validation/bleu=29.6231, validation/loss=1.66515, validation/num_examples=3000
I0306 05:14:58.944744 139758755796736 logging_writer.py:48] [60900] global_step=60900, grad_norm=0.2681378424167633, loss=3.8334834575653076
I0306 05:15:33.419216 139758764189440 logging_writer.py:48] [61000] global_step=61000, grad_norm=0.27439209818840027, loss=3.825191020965576
I0306 05:16:07.877480 139758755796736 logging_writer.py:48] [61100] global_step=61100, grad_norm=0.2887323498725891, loss=3.869297504425049
I0306 05:16:42.346586 139758764189440 logging_writer.py:48] [61200] global_step=61200, grad_norm=0.28140372037887573, loss=3.790377140045166
I0306 05:17:16.804419 139758755796736 logging_writer.py:48] [61300] global_step=61300, grad_norm=0.26065853238105774, loss=3.90527081489563
I0306 05:17:51.287954 139758764189440 logging_writer.py:48] [61400] global_step=61400, grad_norm=0.2860315144062042, loss=3.8692870140075684
I0306 05:18:25.775722 139758755796736 logging_writer.py:48] [61500] global_step=61500, grad_norm=0.2813549041748047, loss=3.7376115322113037
I0306 05:19:00.272123 139758764189440 logging_writer.py:48] [61600] global_step=61600, grad_norm=0.2827906906604767, loss=3.8134961128234863
I0306 05:19:34.749488 139758755796736 logging_writer.py:48] [61700] global_step=61700, grad_norm=0.2592918276786804, loss=3.831611156463623
I0306 05:20:09.235297 139758764189440 logging_writer.py:48] [61800] global_step=61800, grad_norm=0.2780665159225464, loss=3.8117611408233643
I0306 05:20:43.740534 139758755796736 logging_writer.py:48] [61900] global_step=61900, grad_norm=0.35299229621887207, loss=3.8166701793670654
I0306 05:21:18.209239 139758764189440 logging_writer.py:48] [62000] global_step=62000, grad_norm=0.26232385635375977, loss=3.826547861099243
I0306 05:21:52.714001 139758755796736 logging_writer.py:48] [62100] global_step=62100, grad_norm=0.29950371384620667, loss=3.823693037033081
I0306 05:22:27.220602 139758764189440 logging_writer.py:48] [62200] global_step=62200, grad_norm=0.30050310492515564, loss=3.8236234188079834
I0306 05:23:01.745652 139758755796736 logging_writer.py:48] [62300] global_step=62300, grad_norm=0.34155580401420593, loss=3.8656294345855713
I0306 05:23:36.230331 139758764189440 logging_writer.py:48] [62400] global_step=62400, grad_norm=0.26164302229881287, loss=3.800448179244995
I0306 05:24:10.749951 139758755796736 logging_writer.py:48] [62500] global_step=62500, grad_norm=0.27692660689353943, loss=3.809999942779541
I0306 05:24:45.254726 139758764189440 logging_writer.py:48] [62600] global_step=62600, grad_norm=0.2826263904571533, loss=3.8222103118896484
I0306 05:25:19.754607 139758755796736 logging_writer.py:48] [62700] global_step=62700, grad_norm=0.35553473234176636, loss=3.843332052230835
I0306 05:25:54.268271 139758764189440 logging_writer.py:48] [62800] global_step=62800, grad_norm=0.2800031304359436, loss=3.8132517337799072
I0306 05:26:28.751356 139758755796736 logging_writer.py:48] [62900] global_step=62900, grad_norm=0.2623315155506134, loss=3.729351758956909
I0306 05:27:03.271775 139758764189440 logging_writer.py:48] [63000] global_step=63000, grad_norm=0.26438212394714355, loss=3.834669351577759
I0306 05:27:37.757004 139758755796736 logging_writer.py:48] [63100] global_step=63100, grad_norm=0.264286071062088, loss=3.799771785736084
I0306 05:28:12.227098 139758764189440 logging_writer.py:48] [63200] global_step=63200, grad_norm=0.2652492821216583, loss=3.828293561935425
I0306 05:28:46.709095 139758755796736 logging_writer.py:48] [63300] global_step=63300, grad_norm=0.2679639458656311, loss=3.8347153663635254
I0306 05:28:49.480165 139902437545152 spec.py:321] Evaluating on the training split.
I0306 05:28:52.094146 139902437545152 workload.py:181] Translating evaluation dataset.
I0306 05:32:16.281469 139902437545152 spec.py:333] Evaluating on the validation split.
I0306 05:32:18.891747 139902437545152 workload.py:181] Translating evaluation dataset.
I0306 05:35:05.088178 139902437545152 spec.py:349] Evaluating on the test split.
I0306 05:35:07.693673 139902437545152 workload.py:181] Translating evaluation dataset.
I0306 05:37:47.732915 139902437545152 submission_runner.py:469] Time since start: 37488.89s, 	Step: 63309, 	{'train/accuracy': 0.6891443133354187, 'train/loss': 1.62844979763031, 'train/bleu': 34.62150147358941, 'validation/accuracy': 0.6832002997398376, 'validation/loss': 1.6543325185775757, 'validation/bleu': 29.94172546870099, 'validation/num_examples': 3000, 'test/accuracy': 0.6966747641563416, 'test/loss': 1.5670961141586304, 'test/bleu': 29.407910582994433, 'test/num_examples': 3003, 'score': 21865.87493443489, 'total_duration': 37488.89387345314, 'accumulated_submission_time': 21865.87493443489, 'accumulated_eval_time': 15618.75349020958, 'accumulated_logging_time': 0.5432400703430176}
I0306 05:37:47.747530 139758764189440 logging_writer.py:48] [63309] accumulated_eval_time=15618.8, accumulated_logging_time=0.54324, accumulated_submission_time=21865.9, global_step=63309, preemption_count=0, score=21865.9, test/accuracy=0.696675, test/bleu=29.4079, test/loss=1.5671, test/num_examples=3003, total_duration=37488.9, train/accuracy=0.689144, train/bleu=34.6215, train/loss=1.62845, validation/accuracy=0.6832, validation/bleu=29.9417, validation/loss=1.65433, validation/num_examples=3000
I0306 05:38:19.451922 139758755796736 logging_writer.py:48] [63400] global_step=63400, grad_norm=0.2759372889995575, loss=3.864072561264038
I0306 05:38:53.915907 139758764189440 logging_writer.py:48] [63500] global_step=63500, grad_norm=0.2655150592327118, loss=3.8043246269226074
I0306 05:39:28.411376 139758755796736 logging_writer.py:48] [63600] global_step=63600, grad_norm=0.2675963342189789, loss=3.7806575298309326
I0306 05:40:02.933561 139758764189440 logging_writer.py:48] [63700] global_step=63700, grad_norm=0.2997530400753021, loss=3.789773941040039
I0306 05:40:37.445964 139758755796736 logging_writer.py:48] [63800] global_step=63800, grad_norm=0.2687208652496338, loss=3.7882652282714844
I0306 05:41:11.929531 139758764189440 logging_writer.py:48] [63900] global_step=63900, grad_norm=0.2683723568916321, loss=3.7894678115844727
I0306 05:41:46.429031 139758755796736 logging_writer.py:48] [64000] global_step=64000, grad_norm=0.28942418098449707, loss=3.8129308223724365
I0306 05:42:20.911789 139758764189440 logging_writer.py:48] [64100] global_step=64100, grad_norm=0.28141680359840393, loss=3.811009407043457
I0306 05:42:55.377012 139758755796736 logging_writer.py:48] [64200] global_step=64200, grad_norm=0.2658994495868683, loss=3.7921204566955566
I0306 05:43:29.955835 139758764189440 logging_writer.py:48] [64300] global_step=64300, grad_norm=0.25482428073883057, loss=3.8042452335357666
I0306 05:44:04.429504 139758755796736 logging_writer.py:48] [64400] global_step=64400, grad_norm=0.26957958936691284, loss=3.78003191947937
I0306 05:44:38.938769 139758764189440 logging_writer.py:48] [64500] global_step=64500, grad_norm=0.2903394103050232, loss=3.8690943717956543
I0306 05:45:13.435867 139758755796736 logging_writer.py:48] [64600] global_step=64600, grad_norm=0.2864011824131012, loss=3.775477886199951
I0306 05:45:47.923328 139758764189440 logging_writer.py:48] [64700] global_step=64700, grad_norm=0.29735779762268066, loss=3.7292251586914062
I0306 05:46:22.424710 139758755796736 logging_writer.py:48] [64800] global_step=64800, grad_norm=0.2696543037891388, loss=3.8167788982391357
I0306 05:46:56.944192 139758764189440 logging_writer.py:48] [64900] global_step=64900, grad_norm=0.30427294969558716, loss=3.7768735885620117
I0306 05:47:31.458613 139758755796736 logging_writer.py:48] [65000] global_step=65000, grad_norm=0.2598266303539276, loss=3.7711997032165527
I0306 05:48:05.949166 139758764189440 logging_writer.py:48] [65100] global_step=65100, grad_norm=0.2822338342666626, loss=3.791853427886963
I0306 05:48:40.451275 139758755796736 logging_writer.py:48] [65200] global_step=65200, grad_norm=0.3174334764480591, loss=3.8420162200927734
I0306 05:49:14.930654 139758764189440 logging_writer.py:48] [65300] global_step=65300, grad_norm=0.28813010454177856, loss=3.763469696044922
I0306 05:49:49.435478 139758755796736 logging_writer.py:48] [65400] global_step=65400, grad_norm=0.2837936282157898, loss=3.7958786487579346
I0306 05:50:23.948418 139758764189440 logging_writer.py:48] [65500] global_step=65500, grad_norm=0.2752104103565216, loss=3.791449785232544
I0306 05:50:58.441352 139758755796736 logging_writer.py:48] [65600] global_step=65600, grad_norm=0.27759262919425964, loss=3.8008928298950195
I0306 05:51:32.932171 139758764189440 logging_writer.py:48] [65700] global_step=65700, grad_norm=0.2841498553752899, loss=3.799124002456665
I0306 05:51:47.773201 139902437545152 spec.py:321] Evaluating on the training split.
I0306 05:51:50.385890 139902437545152 workload.py:181] Translating evaluation dataset.
I0306 05:54:57.002051 139902437545152 spec.py:333] Evaluating on the validation split.
I0306 05:54:59.622347 139902437545152 workload.py:181] Translating evaluation dataset.
I0306 05:57:41.169531 139902437545152 spec.py:349] Evaluating on the test split.
I0306 05:57:43.780719 139902437545152 workload.py:181] Translating evaluation dataset.
I0306 06:00:18.078832 139902437545152 submission_runner.py:469] Time since start: 38839.24s, 	Step: 65744, 	{'train/accuracy': 0.6767931580543518, 'train/loss': 1.699054479598999, 'train/bleu': 33.88097765701787, 'validation/accuracy': 0.6815687417984009, 'validation/loss': 1.6546235084533691, 'validation/bleu': 30.069167612990167, 'validation/num_examples': 3000, 'test/accuracy': 0.6981925368309021, 'test/loss': 1.567050814628601, 'test/bleu': 30.041491673588858, 'test/num_examples': 3003, 'score': 22705.752933502197, 'total_duration': 38839.23977613449, 'accumulated_submission_time': 22705.752933502197, 'accumulated_eval_time': 16129.05905175209, 'accumulated_logging_time': 0.566107988357544}
I0306 06:00:18.095744 139758755796736 logging_writer.py:48] [65744] accumulated_eval_time=16129.1, accumulated_logging_time=0.566108, accumulated_submission_time=22705.8, global_step=65744, preemption_count=0, score=22705.8, test/accuracy=0.698193, test/bleu=30.0415, test/loss=1.56705, test/num_examples=3003, total_duration=38839.2, train/accuracy=0.676793, train/bleu=33.881, train/loss=1.69905, validation/accuracy=0.681569, validation/bleu=30.0692, validation/loss=1.65462, validation/num_examples=3000
I0306 06:00:37.766651 139758764189440 logging_writer.py:48] [65800] global_step=65800, grad_norm=0.26970669627189636, loss=3.8075637817382812
I0306 06:01:12.233977 139758755796736 logging_writer.py:48] [65900] global_step=65900, grad_norm=0.2665844261646271, loss=3.822040557861328
I0306 06:01:46.717944 139758764189440 logging_writer.py:48] [66000] global_step=66000, grad_norm=0.2815581262111664, loss=3.7780771255493164
I0306 06:02:21.167235 139758755796736 logging_writer.py:48] [66100] global_step=66100, grad_norm=0.2674514353275299, loss=3.828493118286133
I0306 06:02:55.662518 139758764189440 logging_writer.py:48] [66200] global_step=66200, grad_norm=0.34838277101516724, loss=3.7952539920806885
I0306 06:03:30.154486 139758755796736 logging_writer.py:48] [66300] global_step=66300, grad_norm=0.2991054952144623, loss=3.7832601070404053
I0306 06:04:04.632346 139758764189440 logging_writer.py:48] [66400] global_step=66400, grad_norm=0.2697415053844452, loss=3.823908567428589
I0306 06:04:39.134042 139758755796736 logging_writer.py:48] [66500] global_step=66500, grad_norm=0.2757361829280853, loss=3.8291220664978027
I0306 06:05:13.631382 139758764189440 logging_writer.py:48] [66600] global_step=66600, grad_norm=0.2637099623680115, loss=3.790261745452881
I0306 06:05:48.106734 139758755796736 logging_writer.py:48] [66700] global_step=66700, grad_norm=0.2705104351043701, loss=3.789809465408325
I0306 06:06:22.579036 139758764189440 logging_writer.py:48] [66800] global_step=66800, grad_norm=0.2989327907562256, loss=3.7532999515533447
I0306 06:06:57.058249 139758755796736 logging_writer.py:48] [66900] global_step=66900, grad_norm=0.28282424807548523, loss=3.8735711574554443
I0306 06:07:31.532287 139758764189440 logging_writer.py:48] [67000] global_step=67000, grad_norm=0.28095743060112, loss=3.819758653640747
I0306 06:08:06.009223 139758755796736 logging_writer.py:48] [67100] global_step=67100, grad_norm=0.27075913548469543, loss=3.779754877090454
I0306 06:08:40.521424 139758764189440 logging_writer.py:48] [67200] global_step=67200, grad_norm=0.28239715099334717, loss=3.831378221511841
I0306 06:09:15.017555 139758755796736 logging_writer.py:48] [67300] global_step=67300, grad_norm=0.30921095609664917, loss=3.796504020690918
I0306 06:09:49.536121 139758764189440 logging_writer.py:48] [67400] global_step=67400, grad_norm=0.306689977645874, loss=3.8209550380706787
I0306 06:10:24.058839 139758755796736 logging_writer.py:48] [67500] global_step=67500, grad_norm=0.30850210785865784, loss=3.8016135692596436
I0306 06:10:58.550841 139758764189440 logging_writer.py:48] [67600] global_step=67600, grad_norm=0.28738585114479065, loss=3.7798879146575928
I0306 06:11:33.024189 139758755796736 logging_writer.py:48] [67700] global_step=67700, grad_norm=0.2848775386810303, loss=3.788937568664551
I0306 06:12:07.525578 139758764189440 logging_writer.py:48] [67800] global_step=67800, grad_norm=0.3330835998058319, loss=3.7734410762786865
I0306 06:12:42.009820 139758755796736 logging_writer.py:48] [67900] global_step=67900, grad_norm=0.2756425738334656, loss=3.8366892337799072
I0306 06:13:16.497213 139758764189440 logging_writer.py:48] [68000] global_step=68000, grad_norm=0.26873481273651123, loss=3.7281603813171387
I0306 06:13:50.997797 139758755796736 logging_writer.py:48] [68100] global_step=68100, grad_norm=0.27019762992858887, loss=3.770249366760254
I0306 06:14:18.236459 139902437545152 spec.py:321] Evaluating on the training split.
I0306 06:14:20.846262 139902437545152 workload.py:181] Translating evaluation dataset.
I0306 06:17:31.724326 139902437545152 spec.py:333] Evaluating on the validation split.
I0306 06:17:34.339574 139902437545152 workload.py:181] Translating evaluation dataset.
I0306 06:20:30.426486 139902437545152 spec.py:349] Evaluating on the test split.
I0306 06:20:33.031890 139902437545152 workload.py:181] Translating evaluation dataset.
I0306 06:23:26.942981 139902437545152 submission_runner.py:469] Time since start: 40228.10s, 	Step: 68180, 	{'train/accuracy': 0.6731387376785278, 'train/loss': 1.7223179340362549, 'train/bleu': 33.382604382188845, 'validation/accuracy': 0.681309163570404, 'validation/loss': 1.6445823907852173, 'validation/bleu': 29.645243752770817, 'validation/num_examples': 3000, 'test/accuracy': 0.6976479887962341, 'test/loss': 1.5597974061965942, 'test/bleu': 29.69259264871251, 'test/num_examples': 3003, 'score': 23545.74523472786, 'total_duration': 40228.10393571854, 'accumulated_submission_time': 23545.74523472786, 'accumulated_eval_time': 16677.7655107975, 'accumulated_logging_time': 0.592252254486084}
I0306 06:23:26.958857 139758764189440 logging_writer.py:48] [68180] accumulated_eval_time=16677.8, accumulated_logging_time=0.592252, accumulated_submission_time=23545.7, global_step=68180, preemption_count=0, score=23545.7, test/accuracy=0.697648, test/bleu=29.6926, test/loss=1.5598, test/num_examples=3003, total_duration=40228.1, train/accuracy=0.673139, train/bleu=33.3826, train/loss=1.72232, validation/accuracy=0.681309, validation/bleu=29.6452, validation/loss=1.64458, validation/num_examples=3000
I0306 06:23:34.203501 139758755796736 logging_writer.py:48] [68200] global_step=68200, grad_norm=0.27427321672439575, loss=3.8074147701263428
I0306 06:24:08.653092 139758764189440 logging_writer.py:48] [68300] global_step=68300, grad_norm=0.26140493154525757, loss=3.7591965198516846
I0306 06:24:43.139386 139758755796736 logging_writer.py:48] [68400] global_step=68400, grad_norm=0.2943078577518463, loss=3.825740098953247
I0306 06:25:17.585251 139758764189440 logging_writer.py:48] [68500] global_step=68500, grad_norm=0.29753291606903076, loss=3.811225652694702
I0306 06:25:52.075929 139758755796736 logging_writer.py:48] [68600] global_step=68600, grad_norm=0.26527804136276245, loss=3.8153553009033203
I0306 06:26:26.609255 139758764189440 logging_writer.py:48] [68700] global_step=68700, grad_norm=0.2843985855579376, loss=3.8209869861602783
I0306 06:27:01.133533 139758755796736 logging_writer.py:48] [68800] global_step=68800, grad_norm=0.27531129121780396, loss=3.8020009994506836
I0306 06:27:35.596610 139758764189440 logging_writer.py:48] [68900] global_step=68900, grad_norm=0.3299119770526886, loss=3.816086530685425
I0306 06:28:10.069492 139758755796736 logging_writer.py:48] [69000] global_step=69000, grad_norm=0.27374446392059326, loss=3.817370891571045
I0306 06:28:44.486366 139758764189440 logging_writer.py:48] [69100] global_step=69100, grad_norm=0.2780640125274658, loss=3.7243499755859375
I0306 06:29:18.898345 139758755796736 logging_writer.py:48] [69200] global_step=69200, grad_norm=0.28404462337493896, loss=3.770728588104248
I0306 06:29:53.301122 139758764189440 logging_writer.py:48] [69300] global_step=69300, grad_norm=0.2809799015522003, loss=3.849283456802368
I0306 06:30:27.711613 139758755796736 logging_writer.py:48] [69400] global_step=69400, grad_norm=0.28607192635536194, loss=3.7977025508880615
I0306 06:31:02.163643 139758764189440 logging_writer.py:48] [69500] global_step=69500, grad_norm=0.28068435192108154, loss=3.797168016433716
I0306 06:31:36.625591 139758755796736 logging_writer.py:48] [69600] global_step=69600, grad_norm=0.2902512848377228, loss=3.819179058074951
I0306 06:32:11.041576 139758764189440 logging_writer.py:48] [69700] global_step=69700, grad_norm=0.26388558745384216, loss=3.851597309112549
I0306 06:32:45.454359 139758755796736 logging_writer.py:48] [69800] global_step=69800, grad_norm=0.3011122941970825, loss=3.7729716300964355
I0306 06:33:19.839726 139758764189440 logging_writer.py:48] [69900] global_step=69900, grad_norm=0.27217987179756165, loss=3.73637318611145
I0306 06:33:54.250093 139758755796736 logging_writer.py:48] [70000] global_step=70000, grad_norm=0.28172311186790466, loss=3.7726545333862305
I0306 06:34:28.632486 139758764189440 logging_writer.py:48] [70100] global_step=70100, grad_norm=0.2748631536960602, loss=3.8157763481140137
I0306 06:35:03.044352 139758755796736 logging_writer.py:48] [70200] global_step=70200, grad_norm=0.32515642046928406, loss=3.7676727771759033
I0306 06:35:37.440113 139758764189440 logging_writer.py:48] [70300] global_step=70300, grad_norm=0.28668496012687683, loss=3.7891478538513184
I0306 06:36:11.820529 139758755796736 logging_writer.py:48] [70400] global_step=70400, grad_norm=0.26839134097099304, loss=3.8483409881591797
I0306 06:36:46.239476 139758764189440 logging_writer.py:48] [70500] global_step=70500, grad_norm=0.28461211919784546, loss=3.775836944580078
I0306 06:37:20.628684 139758755796736 logging_writer.py:48] [70600] global_step=70600, grad_norm=0.3256727457046509, loss=3.820918321609497
I0306 06:37:27.161513 139902437545152 spec.py:321] Evaluating on the training split.
I0306 06:37:29.780014 139902437545152 workload.py:181] Translating evaluation dataset.
I0306 06:40:16.354768 139902437545152 spec.py:333] Evaluating on the validation split.
I0306 06:40:18.975021 139902437545152 workload.py:181] Translating evaluation dataset.
I0306 06:42:52.149973 139902437545152 spec.py:349] Evaluating on the test split.
I0306 06:42:54.761196 139902437545152 workload.py:181] Translating evaluation dataset.
I0306 06:45:21.607769 139902437545152 submission_runner.py:469] Time since start: 41542.77s, 	Step: 70620, 	{'train/accuracy': 0.6845422387123108, 'train/loss': 1.654647707939148, 'train/bleu': 34.26978804583005, 'validation/accuracy': 0.6842014193534851, 'validation/loss': 1.6421239376068115, 'validation/bleu': 30.04992598556355, 'validation/num_examples': 3000, 'test/accuracy': 0.6999536752700806, 'test/loss': 1.5490161180496216, 'test/bleu': 29.88508683276483, 'test/num_examples': 3003, 'score': 24385.801114082336, 'total_duration': 41542.76871919632, 'accumulated_submission_time': 24385.801114082336, 'accumulated_eval_time': 17152.21170282364, 'accumulated_logging_time': 0.6166341304779053}
I0306 06:45:21.623672 139758764189440 logging_writer.py:48] [70620] accumulated_eval_time=17152.2, accumulated_logging_time=0.616634, accumulated_submission_time=24385.8, global_step=70620, preemption_count=0, score=24385.8, test/accuracy=0.699954, test/bleu=29.8851, test/loss=1.54902, test/num_examples=3003, total_duration=41542.8, train/accuracy=0.684542, train/bleu=34.2698, train/loss=1.65465, validation/accuracy=0.684201, validation/bleu=30.0499, validation/loss=1.64212, validation/num_examples=3000
I0306 06:45:49.477128 139758755796736 logging_writer.py:48] [70700] global_step=70700, grad_norm=0.26813551783561707, loss=3.7333617210388184
I0306 06:46:23.872377 139758764189440 logging_writer.py:48] [70800] global_step=70800, grad_norm=0.28913068771362305, loss=3.810762643814087
I0306 06:46:58.272253 139758755796736 logging_writer.py:48] [70900] global_step=70900, grad_norm=0.31332698464393616, loss=3.774928092956543
I0306 06:47:32.674122 139758764189440 logging_writer.py:48] [71000] global_step=71000, grad_norm=0.2856091856956482, loss=3.771678924560547
I0306 06:48:07.095533 139758755796736 logging_writer.py:48] [71100] global_step=71100, grad_norm=0.27767446637153625, loss=3.7511305809020996
I0306 06:48:41.519822 139758764189440 logging_writer.py:48] [71200] global_step=71200, grad_norm=0.28128185868263245, loss=3.7540125846862793
I0306 06:49:15.947153 139758755796736 logging_writer.py:48] [71300] global_step=71300, grad_norm=0.27898910641670227, loss=3.7399277687072754
I0306 06:49:50.372735 139758764189440 logging_writer.py:48] [71400] global_step=71400, grad_norm=0.29337820410728455, loss=3.7602906227111816
I0306 06:50:24.788023 139758755796736 logging_writer.py:48] [71500] global_step=71500, grad_norm=0.27343955636024475, loss=3.73941969871521
I0306 06:50:59.202374 139758764189440 logging_writer.py:48] [71600] global_step=71600, grad_norm=0.27073177695274353, loss=3.751372814178467
I0306 06:51:33.614395 139758755796736 logging_writer.py:48] [71700] global_step=71700, grad_norm=0.31202346086502075, loss=3.7993760108947754
I0306 06:52:08.071668 139758764189440 logging_writer.py:48] [71800] global_step=71800, grad_norm=0.2852863073348999, loss=3.763035535812378
I0306 06:52:42.506406 139758755796736 logging_writer.py:48] [71900] global_step=71900, grad_norm=0.2701016664505005, loss=3.7588846683502197
I0306 06:53:16.975983 139758764189440 logging_writer.py:48] [72000] global_step=72000, grad_norm=0.28593820333480835, loss=3.7926223278045654
I0306 06:53:51.406735 139758755796736 logging_writer.py:48] [72100] global_step=72100, grad_norm=0.27699652314186096, loss=3.790773630142212
I0306 06:54:25.821115 139758764189440 logging_writer.py:48] [72200] global_step=72200, grad_norm=0.2827973961830139, loss=3.787792444229126
I0306 06:55:00.259677 139758755796736 logging_writer.py:48] [72300] global_step=72300, grad_norm=0.28786128759384155, loss=3.733793020248413
I0306 06:55:34.883847 139758764189440 logging_writer.py:48] [72400] global_step=72400, grad_norm=0.29887863993644714, loss=3.7470340728759766
I0306 06:56:09.406422 139758755796736 logging_writer.py:48] [72500] global_step=72500, grad_norm=0.3146142065525055, loss=3.7636680603027344
I0306 06:56:43.888634 139758764189440 logging_writer.py:48] [72600] global_step=72600, grad_norm=0.28179845213890076, loss=3.7552223205566406
I0306 06:57:18.430097 139758755796736 logging_writer.py:48] [72700] global_step=72700, grad_norm=0.26386067271232605, loss=3.7664945125579834
I0306 06:57:52.909059 139758764189440 logging_writer.py:48] [72800] global_step=72800, grad_norm=0.2960827052593231, loss=3.7334136962890625
I0306 06:58:27.400260 139758755796736 logging_writer.py:48] [72900] global_step=72900, grad_norm=0.2734701931476593, loss=3.8169267177581787
I0306 06:59:01.870034 139758764189440 logging_writer.py:48] [73000] global_step=73000, grad_norm=0.28682592511177063, loss=3.786430835723877
I0306 06:59:21.863319 139902437545152 spec.py:321] Evaluating on the training split.
I0306 06:59:24.481208 139902437545152 workload.py:181] Translating evaluation dataset.
I0306 07:02:16.216679 139902437545152 spec.py:333] Evaluating on the validation split.
I0306 07:02:18.841445 139902437545152 workload.py:181] Translating evaluation dataset.
I0306 07:04:52.587510 139902437545152 spec.py:349] Evaluating on the test split.
I0306 07:04:55.212002 139902437545152 workload.py:181] Translating evaluation dataset.
I0306 07:07:24.061684 139902437545152 submission_runner.py:469] Time since start: 42865.22s, 	Step: 73059, 	{'train/accuracy': 0.677196204662323, 'train/loss': 1.700468897819519, 'train/bleu': 33.95536828809262, 'validation/accuracy': 0.6851902604103088, 'validation/loss': 1.6394544839859009, 'validation/bleu': 30.05032410491529, 'validation/num_examples': 3000, 'test/accuracy': 0.7013787627220154, 'test/loss': 1.5434184074401855, 'test/bleu': 29.87052076192903, 'test/num_examples': 3003, 'score': 25225.892251729965, 'total_duration': 42865.22264432907, 'accumulated_submission_time': 25225.892251729965, 'accumulated_eval_time': 17634.41001367569, 'accumulated_logging_time': 0.6408884525299072}
I0306 07:07:24.079602 139758755796736 logging_writer.py:48] [73059] accumulated_eval_time=17634.4, accumulated_logging_time=0.640888, accumulated_submission_time=25225.9, global_step=73059, preemption_count=0, score=25225.9, test/accuracy=0.701379, test/bleu=29.8705, test/loss=1.54342, test/num_examples=3003, total_duration=42865.2, train/accuracy=0.677196, train/bleu=33.9554, train/loss=1.70047, validation/accuracy=0.68519, validation/bleu=30.0503, validation/loss=1.63945, validation/num_examples=3000
I0306 07:07:38.551768 139758764189440 logging_writer.py:48] [73100] global_step=73100, grad_norm=0.28339269757270813, loss=3.7934908866882324
I0306 07:08:13.059540 139758755796736 logging_writer.py:48] [73200] global_step=73200, grad_norm=0.3063129484653473, loss=3.770542860031128
I0306 07:08:47.546494 139758764189440 logging_writer.py:48] [73300] global_step=73300, grad_norm=0.28883832693099976, loss=3.802600860595703
I0306 07:09:22.029311 139758755796736 logging_writer.py:48] [73400] global_step=73400, grad_norm=0.2708817422389984, loss=3.736058235168457
I0306 07:09:56.536625 139758764189440 logging_writer.py:48] [73500] global_step=73500, grad_norm=0.30083948373794556, loss=3.8284802436828613
I0306 07:10:31.041654 139758755796736 logging_writer.py:48] [73600] global_step=73600, grad_norm=0.304683119058609, loss=3.7943520545959473
I0306 07:11:05.544706 139758764189440 logging_writer.py:48] [73700] global_step=73700, grad_norm=0.2865367531776428, loss=3.7420997619628906
I0306 07:11:40.055584 139758755796736 logging_writer.py:48] [73800] global_step=73800, grad_norm=0.2764515280723572, loss=3.696553945541382
I0306 07:12:14.558375 139758764189440 logging_writer.py:48] [73900] global_step=73900, grad_norm=0.3010098934173584, loss=3.792743444442749
I0306 07:12:49.087691 139758755796736 logging_writer.py:48] [74000] global_step=74000, grad_norm=0.2934609353542328, loss=3.7943310737609863
I0306 07:13:23.607826 139758764189440 logging_writer.py:48] [74100] global_step=74100, grad_norm=0.27515023946762085, loss=3.713843584060669
I0306 07:13:58.171706 139758755796736 logging_writer.py:48] [74200] global_step=74200, grad_norm=0.2788391709327698, loss=3.764580011367798
I0306 07:14:32.691465 139758764189440 logging_writer.py:48] [74300] global_step=74300, grad_norm=0.30818891525268555, loss=3.7239184379577637
I0306 07:15:07.237619 139758755796736 logging_writer.py:48] [74400] global_step=74400, grad_norm=0.32726216316223145, loss=3.726592540740967
I0306 07:15:41.734824 139758764189440 logging_writer.py:48] [74500] global_step=74500, grad_norm=0.3059749901294708, loss=3.7414040565490723
I0306 07:16:16.240654 139758755796736 logging_writer.py:48] [74600] global_step=74600, grad_norm=0.2879411280155182, loss=3.78914475440979
I0306 07:16:50.751913 139758764189440 logging_writer.py:48] [74700] global_step=74700, grad_norm=0.30939197540283203, loss=3.761784791946411
I0306 07:17:25.259864 139758755796736 logging_writer.py:48] [74800] global_step=74800, grad_norm=0.28618863224983215, loss=3.7307193279266357
I0306 07:17:59.781879 139758764189440 logging_writer.py:48] [74900] global_step=74900, grad_norm=0.281751424074173, loss=3.7786245346069336
I0306 07:18:34.331480 139758755796736 logging_writer.py:48] [75000] global_step=75000, grad_norm=0.2628803849220276, loss=3.7301785945892334
I0306 07:19:08.834083 139758764189440 logging_writer.py:48] [75100] global_step=75100, grad_norm=0.2966972291469574, loss=3.6828839778900146
I0306 07:19:43.329520 139758755796736 logging_writer.py:48] [75200] global_step=75200, grad_norm=0.3227646052837372, loss=3.801480293273926
I0306 07:20:17.956193 139758764189440 logging_writer.py:48] [75300] global_step=75300, grad_norm=0.28639867901802063, loss=3.791534185409546
I0306 07:20:52.566075 139758755796736 logging_writer.py:48] [75400] global_step=75400, grad_norm=0.3008405864238739, loss=3.7878165245056152
I0306 07:21:24.073337 139902437545152 spec.py:321] Evaluating on the training split.
I0306 07:21:26.693066 139902437545152 workload.py:181] Translating evaluation dataset.
I0306 07:26:00.962091 139902437545152 spec.py:333] Evaluating on the validation split.
I0306 07:26:03.567098 139902437545152 workload.py:181] Translating evaluation dataset.
I0306 07:30:54.747858 139902437545152 spec.py:349] Evaluating on the test split.
I0306 07:30:57.360782 139902437545152 workload.py:181] Translating evaluation dataset.
I0306 07:35:10.145864 139902437545152 submission_runner.py:469] Time since start: 44531.31s, 	Step: 75492, 	{'train/accuracy': 0.7062452435493469, 'train/loss': 1.534682273864746, 'train/bleu': 36.55732137684128, 'validation/accuracy': 0.6857093572616577, 'validation/loss': 1.6275759935379028, 'validation/bleu': 27.918883764843752, 'validation/num_examples': 3000, 'test/accuracy': 0.7009153366088867, 'test/loss': 1.5392820835113525, 'test/bleu': 29.870187072222258, 'test/num_examples': 3003, 'score': 26065.73565506935, 'total_duration': 44531.30682468414, 'accumulated_submission_time': 26065.73565506935, 'accumulated_eval_time': 18460.48248577118, 'accumulated_logging_time': 0.6671042442321777}
I0306 07:35:10.163595 139758764189440 logging_writer.py:48] [75492] accumulated_eval_time=18460.5, accumulated_logging_time=0.667104, accumulated_submission_time=26065.7, global_step=75492, preemption_count=0, score=26065.7, test/accuracy=0.700915, test/bleu=29.8702, test/loss=1.53928, test/num_examples=3003, total_duration=44531.3, train/accuracy=0.706245, train/bleu=36.5573, train/loss=1.53468, validation/accuracy=0.685709, validation/bleu=27.9189, validation/loss=1.62758, validation/num_examples=3000
I0306 07:35:13.292959 139758755796736 logging_writer.py:48] [75500] global_step=75500, grad_norm=0.28365108370780945, loss=3.7810750007629395
I0306 07:35:47.899477 139758764189440 logging_writer.py:48] [75600] global_step=75600, grad_norm=0.30270838737487793, loss=3.7774105072021484
I0306 07:36:22.520440 139758755796736 logging_writer.py:48] [75700] global_step=75700, grad_norm=0.3221089243888855, loss=3.772223711013794
I0306 07:36:57.132035 139758764189440 logging_writer.py:48] [75800] global_step=75800, grad_norm=0.2805611789226532, loss=3.7386293411254883
I0306 07:37:31.777614 139758755796736 logging_writer.py:48] [75900] global_step=75900, grad_norm=0.3066910207271576, loss=3.7797982692718506
I0306 07:38:06.350295 139758764189440 logging_writer.py:48] [76000] global_step=76000, grad_norm=0.3513588309288025, loss=3.7457385063171387
I0306 07:38:40.987918 139758755796736 logging_writer.py:48] [76100] global_step=76100, grad_norm=0.29203498363494873, loss=3.8291993141174316
I0306 07:39:15.603827 139758764189440 logging_writer.py:48] [76200] global_step=76200, grad_norm=0.2850213646888733, loss=3.7828328609466553
I0306 07:39:50.242207 139758755796736 logging_writer.py:48] [76300] global_step=76300, grad_norm=0.284894198179245, loss=3.7413876056671143
I0306 07:40:24.851882 139758764189440 logging_writer.py:48] [76400] global_step=76400, grad_norm=0.3097264766693115, loss=3.7195873260498047
I0306 07:40:59.445232 139758755796736 logging_writer.py:48] [76500] global_step=76500, grad_norm=0.3020888566970825, loss=3.7207131385803223
I0306 07:41:34.058859 139758764189440 logging_writer.py:48] [76600] global_step=76600, grad_norm=0.29939723014831543, loss=3.7691571712493896
I0306 07:42:08.676936 139758755796736 logging_writer.py:48] [76700] global_step=76700, grad_norm=0.27255091071128845, loss=3.7189526557922363
I0306 07:42:43.273603 139758764189440 logging_writer.py:48] [76800] global_step=76800, grad_norm=0.30822834372520447, loss=3.823516368865967
I0306 07:43:17.884809 139758755796736 logging_writer.py:48] [76900] global_step=76900, grad_norm=0.2782229483127594, loss=3.7075412273406982
I0306 07:43:52.525468 139758764189440 logging_writer.py:48] [77000] global_step=77000, grad_norm=0.29593756794929504, loss=3.726029872894287
I0306 07:44:27.153335 139758755796736 logging_writer.py:48] [77100] global_step=77100, grad_norm=0.2891964018344879, loss=3.7574102878570557
I0306 07:45:01.768081 139758764189440 logging_writer.py:48] [77200] global_step=77200, grad_norm=0.2978062629699707, loss=3.758343458175659
I0306 07:45:36.373299 139758755796736 logging_writer.py:48] [77300] global_step=77300, grad_norm=0.2905775010585785, loss=3.7351677417755127
I0306 07:46:10.994408 139758764189440 logging_writer.py:48] [77400] global_step=77400, grad_norm=0.3024021089076996, loss=3.7878479957580566
I0306 07:46:45.618616 139758755796736 logging_writer.py:48] [77500] global_step=77500, grad_norm=0.32152536511421204, loss=3.7566168308258057
I0306 07:47:20.211154 139758764189440 logging_writer.py:48] [77600] global_step=77600, grad_norm=0.2997856140136719, loss=3.810263156890869
I0306 07:47:54.821819 139758755796736 logging_writer.py:48] [77700] global_step=77700, grad_norm=0.31718629598617554, loss=3.721470832824707
I0306 07:48:29.405667 139758764189440 logging_writer.py:48] [77800] global_step=77800, grad_norm=0.29688456654548645, loss=3.7434771060943604
I0306 07:49:04.029403 139758755796736 logging_writer.py:48] [77900] global_step=77900, grad_norm=0.31202057003974915, loss=3.7023396492004395
I0306 07:49:10.248312 139902437545152 spec.py:321] Evaluating on the training split.
I0306 07:49:12.870297 139902437545152 workload.py:181] Translating evaluation dataset.
I0306 07:53:00.312674 139902437545152 spec.py:333] Evaluating on the validation split.
I0306 07:53:02.927292 139902437545152 workload.py:181] Translating evaluation dataset.
I0306 07:55:47.097822 139902437545152 spec.py:349] Evaluating on the test split.
I0306 07:55:49.720578 139902437545152 workload.py:181] Translating evaluation dataset.
I0306 07:58:26.510758 139902437545152 submission_runner.py:469] Time since start: 45927.67s, 	Step: 77919, 	{'train/accuracy': 0.6888896822929382, 'train/loss': 1.628191590309143, 'train/bleu': 34.74474829487272, 'validation/accuracy': 0.6864138841629028, 'validation/loss': 1.6267073154449463, 'validation/bleu': 30.00586328803466, 'validation/num_examples': 3000, 'test/accuracy': 0.7012744545936584, 'test/loss': 1.5380299091339111, 'test/bleu': 29.935526737695604, 'test/num_examples': 3003, 'score': 26905.66841840744, 'total_duration': 45927.671696424484, 'accumulated_submission_time': 26905.66841840744, 'accumulated_eval_time': 19016.744868516922, 'accumulated_logging_time': 0.6931939125061035}
I0306 07:58:26.528388 139758764189440 logging_writer.py:48] [77919] accumulated_eval_time=19016.7, accumulated_logging_time=0.693194, accumulated_submission_time=26905.7, global_step=77919, preemption_count=0, score=26905.7, test/accuracy=0.701274, test/bleu=29.9355, test/loss=1.53803, test/num_examples=3003, total_duration=45927.7, train/accuracy=0.68889, train/bleu=34.7447, train/loss=1.62819, validation/accuracy=0.686414, validation/bleu=30.0059, validation/loss=1.62671, validation/num_examples=3000
I0306 07:58:54.859041 139758755796736 logging_writer.py:48] [78000] global_step=78000, grad_norm=0.3015131950378418, loss=3.7981090545654297
I0306 07:59:29.429684 139758764189440 logging_writer.py:48] [78100] global_step=78100, grad_norm=0.30066850781440735, loss=3.7512505054473877
I0306 08:00:04.024772 139758755796736 logging_writer.py:48] [78200] global_step=78200, grad_norm=0.32726413011550903, loss=3.7771992683410645
I0306 08:00:38.604308 139758764189440 logging_writer.py:48] [78300] global_step=78300, grad_norm=0.30446505546569824, loss=3.7199881076812744
I0306 08:01:13.190895 139758755796736 logging_writer.py:48] [78400] global_step=78400, grad_norm=0.3221342861652374, loss=3.7617292404174805
I0306 08:01:47.765056 139758764189440 logging_writer.py:48] [78500] global_step=78500, grad_norm=0.3167325258255005, loss=3.756829023361206
I0306 08:02:22.427766 139758755796736 logging_writer.py:48] [78600] global_step=78600, grad_norm=0.30858102440834045, loss=3.7996468544006348
I0306 08:02:56.986620 139758764189440 logging_writer.py:48] [78700] global_step=78700, grad_norm=0.302008718252182, loss=3.748914957046509
I0306 08:03:31.541238 139758755796736 logging_writer.py:48] [78800] global_step=78800, grad_norm=0.2977987825870514, loss=3.7026891708374023
I0306 08:04:06.088518 139758764189440 logging_writer.py:48] [78900] global_step=78900, grad_norm=0.3292357921600342, loss=3.726513624191284
I0306 08:04:40.663341 139758755796736 logging_writer.py:48] [79000] global_step=79000, grad_norm=0.3889327049255371, loss=3.7536144256591797
I0306 08:05:15.231680 139758764189440 logging_writer.py:48] [79100] global_step=79100, grad_norm=0.29412248730659485, loss=3.7487525939941406
I0306 08:05:49.792827 139758755796736 logging_writer.py:48] [79200] global_step=79200, grad_norm=0.3015066087245941, loss=3.7184996604919434
I0306 08:06:24.391185 139758764189440 logging_writer.py:48] [79300] global_step=79300, grad_norm=0.29967015981674194, loss=3.750002384185791
I0306 08:06:58.997396 139758755796736 logging_writer.py:48] [79400] global_step=79400, grad_norm=0.3074014484882355, loss=3.6996376514434814
I0306 08:07:33.598747 139758764189440 logging_writer.py:48] [79500] global_step=79500, grad_norm=0.2999049127101898, loss=3.7280352115631104
I0306 08:08:08.173480 139758755796736 logging_writer.py:48] [79600] global_step=79600, grad_norm=0.2813218832015991, loss=3.8232593536376953
I0306 08:08:42.816404 139758764189440 logging_writer.py:48] [79700] global_step=79700, grad_norm=0.31202343106269836, loss=3.7487125396728516
I0306 08:09:17.426946 139758755796736 logging_writer.py:48] [79800] global_step=79800, grad_norm=0.31219255924224854, loss=3.78676438331604
I0306 08:09:52.049032 139758764189440 logging_writer.py:48] [79900] global_step=79900, grad_norm=0.30387553572654724, loss=3.7396695613861084
I0306 08:10:26.652388 139758755796736 logging_writer.py:48] [80000] global_step=80000, grad_norm=0.2932676672935486, loss=3.73191499710083
I0306 08:11:01.279840 139758764189440 logging_writer.py:48] [80100] global_step=80100, grad_norm=0.3173999786376953, loss=3.7578303813934326
I0306 08:11:35.903704 139758755796736 logging_writer.py:48] [80200] global_step=80200, grad_norm=0.28915175795555115, loss=3.7747180461883545
I0306 08:12:10.499568 139758764189440 logging_writer.py:48] [80300] global_step=80300, grad_norm=0.3099099397659302, loss=3.7484092712402344
I0306 08:12:26.752490 139902437545152 spec.py:321] Evaluating on the training split.
I0306 08:12:29.379865 139902437545152 workload.py:181] Translating evaluation dataset.
I0306 08:15:35.110103 139902437545152 spec.py:333] Evaluating on the validation split.
I0306 08:15:37.728266 139902437545152 workload.py:181] Translating evaluation dataset.
I0306 08:18:19.408241 139902437545152 spec.py:349] Evaluating on the test split.
I0306 08:18:22.031283 139902437545152 workload.py:181] Translating evaluation dataset.
I0306 08:20:52.699625 139902437545152 submission_runner.py:469] Time since start: 47273.86s, 	Step: 80348, 	{'train/accuracy': 0.6849319338798523, 'train/loss': 1.652205228805542, 'train/bleu': 34.472612769596346, 'validation/accuracy': 0.6878970861434937, 'validation/loss': 1.6195728778839111, 'validation/bleu': 30.571553424753397, 'validation/num_examples': 3000, 'test/accuracy': 0.7034063339233398, 'test/loss': 1.5280359983444214, 'test/bleu': 30.412107098044967, 'test/num_examples': 3003, 'score': 27745.74901175499, 'total_duration': 47273.86058163643, 'accumulated_submission_time': 27745.74901175499, 'accumulated_eval_time': 19522.691960573196, 'accumulated_logging_time': 0.719153881072998}
I0306 08:20:52.720137 139758755796736 logging_writer.py:48] [80348] accumulated_eval_time=19522.7, accumulated_logging_time=0.719154, accumulated_submission_time=27745.7, global_step=80348, preemption_count=0, score=27745.7, test/accuracy=0.703406, test/bleu=30.4121, test/loss=1.52804, test/num_examples=3003, total_duration=47273.9, train/accuracy=0.684932, train/bleu=34.4726, train/loss=1.65221, validation/accuracy=0.687897, validation/bleu=30.5716, validation/loss=1.61957, validation/num_examples=3000
I0306 08:21:11.054837 139758764189440 logging_writer.py:48] [80400] global_step=80400, grad_norm=0.2909611463546753, loss=3.713287591934204
I0306 08:21:45.636396 139758755796736 logging_writer.py:48] [80500] global_step=80500, grad_norm=0.2954486012458801, loss=3.72322154045105
I0306 08:22:20.236659 139758764189440 logging_writer.py:48] [80600] global_step=80600, grad_norm=0.2907245457172394, loss=3.693108320236206
I0306 08:22:54.821523 139758755796736 logging_writer.py:48] [80700] global_step=80700, grad_norm=0.31586748361587524, loss=3.7426891326904297
I0306 08:23:29.442398 139758764189440 logging_writer.py:48] [80800] global_step=80800, grad_norm=0.2893379032611847, loss=3.701559066772461
I0306 08:24:04.059324 139758755796736 logging_writer.py:48] [80900] global_step=80900, grad_norm=0.3067306876182556, loss=3.707824230194092
I0306 08:24:38.667642 139758764189440 logging_writer.py:48] [81000] global_step=81000, grad_norm=0.3136138617992401, loss=3.7326583862304688
I0306 08:25:13.291407 139758755796736 logging_writer.py:48] [81100] global_step=81100, grad_norm=0.31350308656692505, loss=3.760061740875244
I0306 08:25:47.890680 139758764189440 logging_writer.py:48] [81200] global_step=81200, grad_norm=0.31484177708625793, loss=3.774989604949951
I0306 08:26:22.504283 139758755796736 logging_writer.py:48] [81300] global_step=81300, grad_norm=0.3122710585594177, loss=3.7513670921325684
I0306 08:26:57.132571 139758764189440 logging_writer.py:48] [81400] global_step=81400, grad_norm=0.29828736186027527, loss=3.718139171600342
I0306 08:27:31.736262 139758755796736 logging_writer.py:48] [81500] global_step=81500, grad_norm=0.30516618490219116, loss=3.684769630432129
I0306 08:28:06.373745 139758764189440 logging_writer.py:48] [81600] global_step=81600, grad_norm=0.2957998216152191, loss=3.7179691791534424
I0306 08:28:41.023622 139758755796736 logging_writer.py:48] [81700] global_step=81700, grad_norm=0.3006569743156433, loss=3.7579102516174316
I0306 08:29:15.599378 139758764189440 logging_writer.py:48] [81800] global_step=81800, grad_norm=0.3126820921897888, loss=3.7257444858551025
I0306 08:29:50.226755 139758755796736 logging_writer.py:48] [81900] global_step=81900, grad_norm=0.30179503560066223, loss=3.7140684127807617
I0306 08:30:24.809569 139758764189440 logging_writer.py:48] [82000] global_step=82000, grad_norm=0.3038094937801361, loss=3.741442918777466
I0306 08:30:59.442800 139758755796736 logging_writer.py:48] [82100] global_step=82100, grad_norm=0.28967344760894775, loss=3.6473803520202637
I0306 08:31:34.087018 139758764189440 logging_writer.py:48] [82200] global_step=82200, grad_norm=0.3129199743270874, loss=3.746319532394409
I0306 08:32:08.718311 139758755796736 logging_writer.py:48] [82300] global_step=82300, grad_norm=0.30007708072662354, loss=3.733356237411499
I0306 08:32:43.347288 139758764189440 logging_writer.py:48] [82400] global_step=82400, grad_norm=0.2993156611919403, loss=3.7553598880767822
I0306 08:33:17.973902 139758755796736 logging_writer.py:48] [82500] global_step=82500, grad_norm=0.3182167708873749, loss=3.7187066078186035
I0306 08:33:52.591921 139758764189440 logging_writer.py:48] [82600] global_step=82600, grad_norm=0.309327632188797, loss=3.6867072582244873
I0306 08:34:27.200110 139758755796736 logging_writer.py:48] [82700] global_step=82700, grad_norm=0.31054967641830444, loss=3.722606658935547
I0306 08:34:52.800226 139902437545152 spec.py:321] Evaluating on the training split.
I0306 08:34:55.422698 139902437545152 workload.py:181] Translating evaluation dataset.
I0306 08:37:58.006979 139902437545152 spec.py:333] Evaluating on the validation split.
I0306 08:38:00.627994 139902437545152 workload.py:181] Translating evaluation dataset.
I0306 08:40:35.710307 139902437545152 spec.py:349] Evaluating on the test split.
I0306 08:40:38.324389 139902437545152 workload.py:181] Translating evaluation dataset.
I0306 08:43:14.426823 139902437545152 submission_runner.py:469] Time since start: 48615.59s, 	Step: 82775, 	{'train/accuracy': 0.6970760226249695, 'train/loss': 1.5778858661651611, 'train/bleu': 35.703569937022515, 'validation/accuracy': 0.6877240538597107, 'validation/loss': 1.6214996576309204, 'validation/bleu': 30.265075091693948, 'validation/num_examples': 3000, 'test/accuracy': 0.7047502994537354, 'test/loss': 1.5246622562408447, 'test/bleu': 30.125939590694443, 'test/num_examples': 3003, 'score': 28585.680349588394, 'total_duration': 48615.58778452873, 'accumulated_submission_time': 28585.680349588394, 'accumulated_eval_time': 20024.318504095078, 'accumulated_logging_time': 0.7488436698913574}
I0306 08:43:14.446047 139758764189440 logging_writer.py:48] [82775] accumulated_eval_time=20024.3, accumulated_logging_time=0.748844, accumulated_submission_time=28585.7, global_step=82775, preemption_count=0, score=28585.7, test/accuracy=0.70475, test/bleu=30.1259, test/loss=1.52466, test/num_examples=3003, total_duration=48615.6, train/accuracy=0.697076, train/bleu=35.7036, train/loss=1.57789, validation/accuracy=0.687724, validation/bleu=30.2651, validation/loss=1.6215, validation/num_examples=3000
I0306 08:43:23.451161 139758755796736 logging_writer.py:48] [82800] global_step=82800, grad_norm=0.3295176029205322, loss=3.7407398223876953
I0306 08:43:58.081562 139758764189440 logging_writer.py:48] [82900] global_step=82900, grad_norm=0.30051368474960327, loss=3.7418911457061768
I0306 08:44:32.708307 139758755796736 logging_writer.py:48] [83000] global_step=83000, grad_norm=0.3058845102787018, loss=3.785311222076416
I0306 08:45:07.364590 139758764189440 logging_writer.py:48] [83100] global_step=83100, grad_norm=0.3056909143924713, loss=3.704800605773926
I0306 08:45:41.976353 139758755796736 logging_writer.py:48] [83200] global_step=83200, grad_norm=0.33757153153419495, loss=3.7264630794525146
I0306 08:46:16.634301 139758764189440 logging_writer.py:48] [83300] global_step=83300, grad_norm=0.2947711646556854, loss=3.7083563804626465
I0306 08:46:51.233877 139758755796736 logging_writer.py:48] [83400] global_step=83400, grad_norm=0.32915517687797546, loss=3.7344839572906494
I0306 08:47:25.872783 139758764189440 logging_writer.py:48] [83500] global_step=83500, grad_norm=0.3011397421360016, loss=3.7318084239959717
I0306 08:48:00.477572 139758755796736 logging_writer.py:48] [83600] global_step=83600, grad_norm=0.29913628101348877, loss=3.703878402709961
I0306 08:48:35.089566 139758764189440 logging_writer.py:48] [83700] global_step=83700, grad_norm=0.34439846873283386, loss=3.7569966316223145
I0306 08:49:09.720311 139758755796736 logging_writer.py:48] [83800] global_step=83800, grad_norm=0.3068133592605591, loss=3.718036651611328
I0306 08:49:44.315121 139758764189440 logging_writer.py:48] [83900] global_step=83900, grad_norm=0.31559452414512634, loss=3.746398448944092
I0306 08:50:18.924518 139758755796736 logging_writer.py:48] [84000] global_step=84000, grad_norm=0.31912362575531006, loss=3.7179408073425293
I0306 08:50:53.493392 139758764189440 logging_writer.py:48] [84100] global_step=84100, grad_norm=0.30306705832481384, loss=3.7310876846313477
I0306 08:51:28.082700 139758755796736 logging_writer.py:48] [84200] global_step=84200, grad_norm=0.3042396903038025, loss=3.682500123977661
I0306 08:52:02.626743 139758764189440 logging_writer.py:48] [84300] global_step=84300, grad_norm=0.32493749260902405, loss=3.766944408416748
I0306 08:52:37.155913 139758755796736 logging_writer.py:48] [84400] global_step=84400, grad_norm=0.32053840160369873, loss=3.7394537925720215
I0306 08:53:11.667525 139758764189440 logging_writer.py:48] [84500] global_step=84500, grad_norm=0.31923773884773254, loss=3.730241537094116
I0306 08:53:46.201753 139758755796736 logging_writer.py:48] [84600] global_step=84600, grad_norm=0.324041485786438, loss=3.7040679454803467
I0306 08:54:20.716150 139758764189440 logging_writer.py:48] [84700] global_step=84700, grad_norm=0.32585540413856506, loss=3.6755335330963135
I0306 08:54:55.269952 139758755796736 logging_writer.py:48] [84800] global_step=84800, grad_norm=0.3091648817062378, loss=3.6728503704071045
I0306 08:55:29.800749 139758764189440 logging_writer.py:48] [84900] global_step=84900, grad_norm=0.3065894842147827, loss=3.6824254989624023
I0306 08:56:04.326117 139758755796736 logging_writer.py:48] [85000] global_step=85000, grad_norm=0.3402026295661926, loss=3.729405403137207
I0306 08:56:38.843457 139758764189440 logging_writer.py:48] [85100] global_step=85100, grad_norm=0.31229615211486816, loss=3.7881650924682617
I0306 08:57:13.371230 139758755796736 logging_writer.py:48] [85200] global_step=85200, grad_norm=0.3369094431400299, loss=3.687072277069092
I0306 08:57:14.757620 139902437545152 spec.py:321] Evaluating on the training split.
I0306 08:57:17.373881 139902437545152 workload.py:181] Translating evaluation dataset.
I0306 09:00:34.106107 139902437545152 spec.py:333] Evaluating on the validation split.
I0306 09:00:36.712010 139902437545152 workload.py:181] Translating evaluation dataset.
I0306 09:03:25.579530 139902437545152 spec.py:349] Evaluating on the test split.
I0306 09:03:28.195468 139902437545152 workload.py:181] Translating evaluation dataset.
I0306 09:06:10.808988 139902437545152 submission_runner.py:469] Time since start: 49991.97s, 	Step: 85205, 	{'train/accuracy': 0.691922128200531, 'train/loss': 1.6089333295822144, 'train/bleu': 35.338945808559764, 'validation/accuracy': 0.6878847479820251, 'validation/loss': 1.621588110923767, 'validation/bleu': 30.314236595613956, 'validation/num_examples': 3000, 'test/accuracy': 0.7032093405723572, 'test/loss': 1.5244542360305786, 'test/bleu': 30.02191390350524, 'test/num_examples': 3003, 'score': 29425.845414161682, 'total_duration': 49991.96995782852, 'accumulated_submission_time': 29425.845414161682, 'accumulated_eval_time': 20560.369824409485, 'accumulated_logging_time': 0.7776467800140381}
I0306 09:06:10.826742 139758764189440 logging_writer.py:48] [85205] accumulated_eval_time=20560.4, accumulated_logging_time=0.777647, accumulated_submission_time=29425.8, global_step=85205, preemption_count=0, score=29425.8, test/accuracy=0.703209, test/bleu=30.0219, test/loss=1.52445, test/num_examples=3003, total_duration=49992, train/accuracy=0.691922, train/bleu=35.3389, train/loss=1.60893, validation/accuracy=0.687885, validation/bleu=30.3142, validation/loss=1.62159, validation/num_examples=3000
I0306 09:06:43.939656 139758755796736 logging_writer.py:48] [85300] global_step=85300, grad_norm=0.3105350434780121, loss=3.693631649017334
I0306 09:07:18.460053 139758764189440 logging_writer.py:48] [85400] global_step=85400, grad_norm=0.3345559239387512, loss=3.7488090991973877
I0306 09:07:52.940496 139758755796736 logging_writer.py:48] [85500] global_step=85500, grad_norm=0.3278193473815918, loss=3.70267391204834
I0306 09:08:27.456690 139758764189440 logging_writer.py:48] [85600] global_step=85600, grad_norm=0.30724820494651794, loss=3.7445626258850098
I0306 09:09:01.969663 139758755796736 logging_writer.py:48] [85700] global_step=85700, grad_norm=0.31696534156799316, loss=3.711397886276245
I0306 09:09:36.478902 139758764189440 logging_writer.py:48] [85800] global_step=85800, grad_norm=0.31552425026893616, loss=3.6911466121673584
I0306 09:10:10.986021 139758755796736 logging_writer.py:48] [85900] global_step=85900, grad_norm=0.3561842441558838, loss=3.763613700866699
I0306 09:10:45.513636 139758764189440 logging_writer.py:48] [86000] global_step=86000, grad_norm=0.31804972887039185, loss=3.733402967453003
I0306 09:11:20.012418 139758755796736 logging_writer.py:48] [86100] global_step=86100, grad_norm=0.34143373370170593, loss=3.776371479034424
I0306 09:11:54.509412 139758764189440 logging_writer.py:48] [86200] global_step=86200, grad_norm=0.3161218762397766, loss=3.691222667694092
I0306 09:12:29.006140 139758755796736 logging_writer.py:48] [86300] global_step=86300, grad_norm=0.32264596223831177, loss=3.69783616065979
I0306 09:13:03.514378 139758764189440 logging_writer.py:48] [86400] global_step=86400, grad_norm=0.3105190694332123, loss=3.664532423019409
I0306 09:13:38.066444 139758755796736 logging_writer.py:48] [86500] global_step=86500, grad_norm=0.3565928041934967, loss=3.703315258026123
I0306 09:14:12.572695 139758764189440 logging_writer.py:48] [86600] global_step=86600, grad_norm=0.3304920196533203, loss=3.715362787246704
I0306 09:14:47.060203 139758755796736 logging_writer.py:48] [86700] global_step=86700, grad_norm=0.33560824394226074, loss=3.754345178604126
I0306 09:15:21.578938 139758764189440 logging_writer.py:48] [86800] global_step=86800, grad_norm=0.3198648691177368, loss=3.738049030303955
I0306 09:15:56.106091 139758755796736 logging_writer.py:48] [86900] global_step=86900, grad_norm=0.3172588348388672, loss=3.698387384414673
I0306 09:16:30.594807 139758764189440 logging_writer.py:48] [87000] global_step=87000, grad_norm=0.32202520966529846, loss=3.760040760040283
I0306 09:17:05.127553 139758755796736 logging_writer.py:48] [87100] global_step=87100, grad_norm=0.30330216884613037, loss=3.672050714492798
I0306 09:17:39.601768 139758764189440 logging_writer.py:48] [87200] global_step=87200, grad_norm=0.3270324468612671, loss=3.7265374660491943
I0306 09:18:14.118239 139758755796736 logging_writer.py:48] [87300] global_step=87300, grad_norm=0.31771907210350037, loss=3.719334602355957
I0306 09:18:48.606487 139758764189440 logging_writer.py:48] [87400] global_step=87400, grad_norm=0.32631027698516846, loss=3.6840810775756836
I0306 09:19:23.111933 139758755796736 logging_writer.py:48] [87500] global_step=87500, grad_norm=0.34049078822135925, loss=3.709500312805176
I0306 09:19:57.598783 139758764189440 logging_writer.py:48] [87600] global_step=87600, grad_norm=0.3265359401702881, loss=3.6886394023895264
I0306 09:20:11.061155 139902437545152 spec.py:321] Evaluating on the training split.
I0306 09:20:13.678385 139902437545152 workload.py:181] Translating evaluation dataset.
I0306 09:24:47.794315 139902437545152 spec.py:333] Evaluating on the validation split.
I0306 09:24:50.412251 139902437545152 workload.py:181] Translating evaluation dataset.
I0306 09:29:08.862010 139902437545152 spec.py:349] Evaluating on the test split.
I0306 09:29:11.468581 139902437545152 workload.py:181] Translating evaluation dataset.
I0306 09:33:35.256390 139902437545152 submission_runner.py:469] Time since start: 51636.42s, 	Step: 87640, 	{'train/accuracy': 0.6934027671813965, 'train/loss': 1.6033387184143066, 'train/bleu': 34.89259967237209, 'validation/accuracy': 0.6884903311729431, 'validation/loss': 1.61361825466156, 'validation/bleu': 29.800504956963028, 'validation/num_examples': 3000, 'test/accuracy': 0.7044143080711365, 'test/loss': 1.5201314687728882, 'test/bleu': 30.043788432620754, 'test/num_examples': 3003, 'score': 30265.936924934387, 'total_duration': 51636.417350530624, 'accumulated_submission_time': 30265.936924934387, 'accumulated_eval_time': 21364.565006494522, 'accumulated_logging_time': 0.8037164211273193}
I0306 09:33:35.274208 139758755796736 logging_writer.py:48] [87640] accumulated_eval_time=21364.6, accumulated_logging_time=0.803716, accumulated_submission_time=30265.9, global_step=87640, preemption_count=0, score=30265.9, test/accuracy=0.704414, test/bleu=30.0438, test/loss=1.52013, test/num_examples=3003, total_duration=51636.4, train/accuracy=0.693403, train/bleu=34.8926, train/loss=1.60334, validation/accuracy=0.68849, validation/bleu=29.8005, validation/loss=1.61362, validation/num_examples=3000
I0306 09:33:56.302018 139758764189440 logging_writer.py:48] [87700] global_step=87700, grad_norm=0.32573843002319336, loss=3.754760265350342
I0306 09:34:30.824661 139758755796736 logging_writer.py:48] [87800] global_step=87800, grad_norm=0.3055743873119354, loss=3.6406478881835938
I0306 09:35:05.236802 139758764189440 logging_writer.py:48] [87900] global_step=87900, grad_norm=0.3092600107192993, loss=3.6995065212249756
I0306 09:35:39.666368 139758755796736 logging_writer.py:48] [88000] global_step=88000, grad_norm=0.34041473269462585, loss=3.667227268218994
I0306 09:36:14.078620 139758764189440 logging_writer.py:48] [88100] global_step=88100, grad_norm=0.31114792823791504, loss=3.690734624862671
I0306 09:36:48.526513 139758755796736 logging_writer.py:48] [88200] global_step=88200, grad_norm=0.32768452167510986, loss=3.7056236267089844
I0306 09:37:22.945676 139758764189440 logging_writer.py:48] [88300] global_step=88300, grad_norm=0.3257520794868469, loss=3.6991944313049316
I0306 09:37:57.385862 139758755796736 logging_writer.py:48] [88400] global_step=88400, grad_norm=0.338441401720047, loss=3.7236087322235107
I0306 09:38:31.799954 139758764189440 logging_writer.py:48] [88500] global_step=88500, grad_norm=0.3315010368824005, loss=3.6991360187530518
I0306 09:39:06.211888 139758755796736 logging_writer.py:48] [88600] global_step=88600, grad_norm=0.3403157889842987, loss=3.716296911239624
I0306 09:39:40.622916 139758764189440 logging_writer.py:48] [88700] global_step=88700, grad_norm=0.35955551266670227, loss=3.706709146499634
I0306 09:40:15.003249 139758755796736 logging_writer.py:48] [88800] global_step=88800, grad_norm=0.3430936336517334, loss=3.732334852218628
I0306 09:40:49.426074 139758764189440 logging_writer.py:48] [88900] global_step=88900, grad_norm=0.34497353434562683, loss=3.6781647205352783
I0306 09:41:23.836804 139758755796736 logging_writer.py:48] [89000] global_step=89000, grad_norm=0.34256812930107117, loss=3.7133710384368896
I0306 09:41:58.246424 139758764189440 logging_writer.py:48] [89100] global_step=89100, grad_norm=0.3348190188407898, loss=3.6891698837280273
I0306 09:42:32.675446 139758755796736 logging_writer.py:48] [89200] global_step=89200, grad_norm=0.3345564007759094, loss=3.6844441890716553
I0306 09:43:07.076057 139758764189440 logging_writer.py:48] [89300] global_step=89300, grad_norm=0.33642756938934326, loss=3.713087558746338
I0306 09:43:41.500863 139758755796736 logging_writer.py:48] [89400] global_step=89400, grad_norm=0.333903968334198, loss=3.633047342300415
I0306 09:44:15.896577 139758764189440 logging_writer.py:48] [89500] global_step=89500, grad_norm=0.33502519130706787, loss=3.6608338356018066
I0306 09:44:50.327282 139758755796736 logging_writer.py:48] [89600] global_step=89600, grad_norm=0.33280712366104126, loss=3.7080657482147217
I0306 09:45:24.751138 139758764189440 logging_writer.py:48] [89700] global_step=89700, grad_norm=0.32345789670944214, loss=3.6885452270507812
I0306 09:45:59.187657 139758755796736 logging_writer.py:48] [89800] global_step=89800, grad_norm=0.3372482657432556, loss=3.710092544555664
I0306 09:46:33.611649 139758764189440 logging_writer.py:48] [89900] global_step=89900, grad_norm=0.39207395911216736, loss=3.645880699157715
I0306 09:47:08.045662 139758755796736 logging_writer.py:48] [90000] global_step=90000, grad_norm=0.3269892930984497, loss=3.665992259979248
I0306 09:47:35.590118 139902437545152 spec.py:321] Evaluating on the training split.
I0306 09:47:38.208662 139902437545152 workload.py:181] Translating evaluation dataset.
I0306 09:50:39.544013 139902437545152 spec.py:333] Evaluating on the validation split.
I0306 09:50:42.150606 139902437545152 workload.py:181] Translating evaluation dataset.
I0306 09:53:23.280200 139902437545152 spec.py:349] Evaluating on the test split.
I0306 09:53:25.899086 139902437545152 workload.py:181] Translating evaluation dataset.
I0306 09:55:54.517780 139902437545152 submission_runner.py:469] Time since start: 52975.68s, 	Step: 90081, 	{'train/accuracy': 0.6959425806999207, 'train/loss': 1.586622714996338, 'train/bleu': 35.28927391162964, 'validation/accuracy': 0.6885521411895752, 'validation/loss': 1.612891435623169, 'validation/bleu': 30.207630332932705, 'validation/num_examples': 3000, 'test/accuracy': 0.7068706154823303, 'test/loss': 1.512256383895874, 'test/bleu': 30.15831282076521, 'test/num_examples': 3003, 'score': 31106.10369205475, 'total_duration': 52975.678736925125, 'accumulated_submission_time': 31106.10369205475, 'accumulated_eval_time': 21863.492612361908, 'accumulated_logging_time': 0.8297824859619141}
I0306 09:55:54.536908 139758764189440 logging_writer.py:48] [90081] accumulated_eval_time=21863.5, accumulated_logging_time=0.829782, accumulated_submission_time=31106.1, global_step=90081, preemption_count=0, score=31106.1, test/accuracy=0.706871, test/bleu=30.1583, test/loss=1.51226, test/num_examples=3003, total_duration=52975.7, train/accuracy=0.695943, train/bleu=35.2893, train/loss=1.58662, validation/accuracy=0.688552, validation/bleu=30.2076, validation/loss=1.61289, validation/num_examples=3000
I0306 09:56:01.427594 139758755796736 logging_writer.py:48] [90100] global_step=90100, grad_norm=0.33242103457450867, loss=3.723508596420288
I0306 09:56:35.866660 139758764189440 logging_writer.py:48] [90200] global_step=90200, grad_norm=0.35284659266471863, loss=3.713557720184326
I0306 09:57:10.288202 139758755796736 logging_writer.py:48] [90300] global_step=90300, grad_norm=0.3505472242832184, loss=3.6944167613983154
I0306 09:57:44.713057 139758764189440 logging_writer.py:48] [90400] global_step=90400, grad_norm=0.35030651092529297, loss=3.657613515853882
I0306 09:58:19.136291 139758755796736 logging_writer.py:48] [90500] global_step=90500, grad_norm=0.3151620030403137, loss=3.674466609954834
I0306 09:58:53.555602 139758764189440 logging_writer.py:48] [90600] global_step=90600, grad_norm=0.31616026163101196, loss=3.663058280944824
I0306 09:59:27.969599 139758755796736 logging_writer.py:48] [90700] global_step=90700, grad_norm=0.34393832087516785, loss=3.717052459716797
I0306 10:00:02.384984 139758764189440 logging_writer.py:48] [90800] global_step=90800, grad_norm=0.32926177978515625, loss=3.694056272506714
I0306 10:00:36.814277 139758755796736 logging_writer.py:48] [90900] global_step=90900, grad_norm=0.32692205905914307, loss=3.6943397521972656
I0306 10:01:11.249262 139758764189440 logging_writer.py:48] [91000] global_step=91000, grad_norm=0.3313387930393219, loss=3.6653635501861572
I0306 10:01:45.689494 139758755796736 logging_writer.py:48] [91100] global_step=91100, grad_norm=0.3453752100467682, loss=3.6862876415252686
I0306 10:02:20.104487 139758764189440 logging_writer.py:48] [91200] global_step=91200, grad_norm=0.3235852122306824, loss=3.666487216949463
I0306 10:02:54.525984 139758755796736 logging_writer.py:48] [91300] global_step=91300, grad_norm=0.3415335714817047, loss=3.647800922393799
I0306 10:03:28.970835 139758764189440 logging_writer.py:48] [91400] global_step=91400, grad_norm=0.34864428639411926, loss=3.7415788173675537
I0306 10:04:03.389364 139758755796736 logging_writer.py:48] [91500] global_step=91500, grad_norm=0.34117433428764343, loss=3.716625213623047
I0306 10:04:37.831120 139758764189440 logging_writer.py:48] [91600] global_step=91600, grad_norm=0.336357980966568, loss=3.6723968982696533
I0306 10:05:12.277235 139758755796736 logging_writer.py:48] [91700] global_step=91700, grad_norm=0.34008944034576416, loss=3.7021567821502686
I0306 10:05:46.715341 139758764189440 logging_writer.py:48] [91800] global_step=91800, grad_norm=0.3441321551799774, loss=3.6881163120269775
I0306 10:06:21.142917 139758755796736 logging_writer.py:48] [91900] global_step=91900, grad_norm=0.3482605814933777, loss=3.698763370513916
I0306 10:06:55.569166 139758764189440 logging_writer.py:48] [92000] global_step=92000, grad_norm=0.3340747654438019, loss=3.635610818862915
I0306 10:07:30.022078 139758755796736 logging_writer.py:48] [92100] global_step=92100, grad_norm=0.3569069802761078, loss=3.691875696182251
I0306 10:08:04.433114 139758764189440 logging_writer.py:48] [92200] global_step=92200, grad_norm=0.33752334117889404, loss=3.6486830711364746
I0306 10:08:38.900609 139758755796736 logging_writer.py:48] [92300] global_step=92300, grad_norm=0.3314715623855591, loss=3.655087947845459
I0306 10:09:13.314430 139758764189440 logging_writer.py:48] [92400] global_step=92400, grad_norm=0.3595679700374603, loss=3.648336410522461
I0306 10:09:47.756411 139758755796736 logging_writer.py:48] [92500] global_step=92500, grad_norm=0.32357847690582275, loss=3.698146104812622
I0306 10:09:54.650091 139902437545152 spec.py:321] Evaluating on the training split.
I0306 10:09:57.266099 139902437545152 workload.py:181] Translating evaluation dataset.
I0306 10:12:52.694915 139902437545152 spec.py:333] Evaluating on the validation split.
I0306 10:12:55.305428 139902437545152 workload.py:181] Translating evaluation dataset.
I0306 10:15:30.506119 139902437545152 spec.py:349] Evaluating on the test split.
I0306 10:15:33.126873 139902437545152 workload.py:181] Translating evaluation dataset.
I0306 10:17:52.726877 139902437545152 submission_runner.py:469] Time since start: 54293.89s, 	Step: 92521, 	{'train/accuracy': 0.6956267952919006, 'train/loss': 1.580920934677124, 'train/bleu': 35.035399892299154, 'validation/accuracy': 0.6892690658569336, 'validation/loss': 1.6103904247283936, 'validation/bleu': 30.613603211121163, 'validation/num_examples': 3000, 'test/accuracy': 0.7051790356636047, 'test/loss': 1.5125402212142944, 'test/bleu': 30.253658976005635, 'test/num_examples': 3003, 'score': 31946.06612801552, 'total_duration': 54293.887838840485, 'accumulated_submission_time': 31946.06612801552, 'accumulated_eval_time': 22341.569347143173, 'accumulated_logging_time': 0.8574233055114746}
I0306 10:17:52.745062 139758764189440 logging_writer.py:48] [92521] accumulated_eval_time=22341.6, accumulated_logging_time=0.857423, accumulated_submission_time=31946.1, global_step=92521, preemption_count=0, score=31946.1, test/accuracy=0.705179, test/bleu=30.2537, test/loss=1.51254, test/num_examples=3003, total_duration=54293.9, train/accuracy=0.695627, train/bleu=35.0354, train/loss=1.58092, validation/accuracy=0.689269, validation/bleu=30.6136, validation/loss=1.61039, validation/num_examples=3000
I0306 10:18:20.297529 139758755796736 logging_writer.py:48] [92600] global_step=92600, grad_norm=0.32927972078323364, loss=3.6242008209228516
I0306 10:18:54.737782 139758764189440 logging_writer.py:48] [92700] global_step=92700, grad_norm=0.3362521231174469, loss=3.6845250129699707
I0306 10:19:29.162956 139758755796736 logging_writer.py:48] [92800] global_step=92800, grad_norm=0.3285049796104431, loss=3.671438217163086
I0306 10:20:03.589416 139758764189440 logging_writer.py:48] [92900] global_step=92900, grad_norm=0.3516881763935089, loss=3.656801223754883
I0306 10:20:38.026548 139758755796736 logging_writer.py:48] [93000] global_step=93000, grad_norm=0.3371325135231018, loss=3.6439974308013916
I0306 10:21:12.436884 139758764189440 logging_writer.py:48] [93100] global_step=93100, grad_norm=0.33458569645881653, loss=3.65301251411438
I0306 10:21:46.864602 139758755796736 logging_writer.py:48] [93200] global_step=93200, grad_norm=0.3284538686275482, loss=3.6932153701782227
I0306 10:22:21.262696 139758764189440 logging_writer.py:48] [93300] global_step=93300, grad_norm=0.33147019147872925, loss=3.6815454959869385
I0306 10:22:55.698938 139758755796736 logging_writer.py:48] [93400] global_step=93400, grad_norm=0.3347451090812683, loss=3.665468692779541
I0306 10:23:30.102307 139758764189440 logging_writer.py:48] [93500] global_step=93500, grad_norm=0.3461618423461914, loss=3.624112606048584
I0306 10:24:04.504969 139758755796736 logging_writer.py:48] [93600] global_step=93600, grad_norm=0.33765920996665955, loss=3.640615463256836
I0306 10:24:38.929412 139758764189440 logging_writer.py:48] [93700] global_step=93700, grad_norm=0.33834072947502136, loss=3.658698081970215
I0306 10:25:13.347247 139758755796736 logging_writer.py:48] [93800] global_step=93800, grad_norm=0.32944390177726746, loss=3.663775682449341
I0306 10:25:47.743084 139758764189440 logging_writer.py:48] [93900] global_step=93900, grad_norm=0.33484476804733276, loss=3.67307710647583
I0306 10:26:22.150545 139758755796736 logging_writer.py:48] [94000] global_step=94000, grad_norm=0.3178752660751343, loss=3.6715476512908936
I0306 10:26:56.605070 139758764189440 logging_writer.py:48] [94100] global_step=94100, grad_norm=0.3418128192424774, loss=3.6662721633911133
I0306 10:27:31.085344 139758755796736 logging_writer.py:48] [94200] global_step=94200, grad_norm=0.3407308757305145, loss=3.6668057441711426
I0306 10:28:05.599389 139758764189440 logging_writer.py:48] [94300] global_step=94300, grad_norm=0.3576260507106781, loss=3.6055567264556885
I0306 10:28:40.114870 139758755796736 logging_writer.py:48] [94400] global_step=94400, grad_norm=0.32961368560791016, loss=3.7271876335144043
I0306 10:29:14.597802 139758764189440 logging_writer.py:48] [94500] global_step=94500, grad_norm=0.34064120054244995, loss=3.6834280490875244
I0306 10:29:49.096720 139758755796736 logging_writer.py:48] [94600] global_step=94600, grad_norm=0.3535645008087158, loss=3.715075731277466
I0306 10:30:23.568077 139758764189440 logging_writer.py:48] [94700] global_step=94700, grad_norm=0.3351060152053833, loss=3.6930582523345947
I0306 10:30:58.085590 139758755796736 logging_writer.py:48] [94800] global_step=94800, grad_norm=0.3629232347011566, loss=3.688187599182129
I0306 10:31:32.587044 139758764189440 logging_writer.py:48] [94900] global_step=94900, grad_norm=0.35465100407600403, loss=3.6377453804016113
I0306 10:31:52.957699 139902437545152 spec.py:321] Evaluating on the training split.
I0306 10:31:55.585772 139902437545152 workload.py:181] Translating evaluation dataset.
I0306 10:35:02.658804 139902437545152 spec.py:333] Evaluating on the validation split.
I0306 10:35:05.271229 139902437545152 workload.py:181] Translating evaluation dataset.
I0306 10:37:50.030702 139902437545152 spec.py:349] Evaluating on the test split.
I0306 10:37:52.635854 139902437545152 workload.py:181] Translating evaluation dataset.
I0306 10:40:32.124317 139902437545152 submission_runner.py:469] Time since start: 55653.29s, 	Step: 94960, 	{'train/accuracy': 0.7145625352859497, 'train/loss': 1.4909170866012573, 'train/bleu': 36.784330342369046, 'validation/accuracy': 0.6893555521965027, 'validation/loss': 1.6122838258743286, 'validation/bleu': 30.405478002749792, 'validation/num_examples': 3000, 'test/accuracy': 0.7056772112846375, 'test/loss': 1.5149322748184204, 'test/bleu': 30.1216361182811, 'test/num_examples': 3003, 'score': 32786.13518118858, 'total_duration': 55653.2852768898, 'accumulated_submission_time': 32786.13518118858, 'accumulated_eval_time': 22860.735911130905, 'accumulated_logging_time': 0.8837485313415527}
I0306 10:40:32.142978 139758755796736 logging_writer.py:48] [94960] accumulated_eval_time=22860.7, accumulated_logging_time=0.883749, accumulated_submission_time=32786.1, global_step=94960, preemption_count=0, score=32786.1, test/accuracy=0.705677, test/bleu=30.1216, test/loss=1.51493, test/num_examples=3003, total_duration=55653.3, train/accuracy=0.714563, train/bleu=36.7843, train/loss=1.49092, validation/accuracy=0.689356, validation/bleu=30.4055, validation/loss=1.61228, validation/num_examples=3000
I0306 10:40:46.265532 139758764189440 logging_writer.py:48] [95000] global_step=95000, grad_norm=0.34760549664497375, loss=3.682600259780884
I0306 10:41:20.746282 139758755796736 logging_writer.py:48] [95100] global_step=95100, grad_norm=0.34466487169265747, loss=3.68057918548584
I0306 10:41:55.263712 139758764189440 logging_writer.py:48] [95200] global_step=95200, grad_norm=0.35215702652931213, loss=3.715806245803833
I0306 10:42:29.764845 139758755796736 logging_writer.py:48] [95300] global_step=95300, grad_norm=0.3251902461051941, loss=3.6744720935821533
I0306 10:43:04.245870 139758764189440 logging_writer.py:48] [95400] global_step=95400, grad_norm=0.35254982113838196, loss=3.6749281883239746
I0306 10:43:38.742756 139758755796736 logging_writer.py:48] [95500] global_step=95500, grad_norm=0.3456863462924957, loss=3.6325390338897705
I0306 10:44:13.267773 139758764189440 logging_writer.py:48] [95600] global_step=95600, grad_norm=0.3704715371131897, loss=3.727219343185425
I0306 10:44:47.744793 139758755796736 logging_writer.py:48] [95700] global_step=95700, grad_norm=0.3541386127471924, loss=3.6559789180755615
I0306 10:45:22.254901 139758764189440 logging_writer.py:48] [95800] global_step=95800, grad_norm=0.35360831022262573, loss=3.649780750274658
I0306 10:45:56.741705 139758755796736 logging_writer.py:48] [95900] global_step=95900, grad_norm=0.3573068082332611, loss=3.6816976070404053
I0306 10:46:31.246511 139758764189440 logging_writer.py:48] [96000] global_step=96000, grad_norm=0.3485000431537628, loss=3.6587634086608887
I0306 10:47:05.714053 139758755796736 logging_writer.py:48] [96100] global_step=96100, grad_norm=0.3504175543785095, loss=3.665985584259033
I0306 10:47:40.199239 139758764189440 logging_writer.py:48] [96200] global_step=96200, grad_norm=0.35868358612060547, loss=3.6544814109802246
I0306 10:48:14.687486 139758755796736 logging_writer.py:48] [96300] global_step=96300, grad_norm=0.3364280164241791, loss=3.6341447830200195
I0306 10:48:49.196967 139758764189440 logging_writer.py:48] [96400] global_step=96400, grad_norm=0.34228768944740295, loss=3.701397180557251
I0306 10:49:23.664681 139758755796736 logging_writer.py:48] [96500] global_step=96500, grad_norm=0.34502121806144714, loss=3.676542043685913
I0306 10:49:58.144258 139758764189440 logging_writer.py:48] [96600] global_step=96600, grad_norm=0.3542196750640869, loss=3.645561456680298
I0306 10:50:32.649144 139758755796736 logging_writer.py:48] [96700] global_step=96700, grad_norm=0.34394535422325134, loss=3.6540236473083496
I0306 10:51:07.136282 139758764189440 logging_writer.py:48] [96800] global_step=96800, grad_norm=0.36098721623420715, loss=3.6349291801452637
I0306 10:51:41.631500 139758755796736 logging_writer.py:48] [96900] global_step=96900, grad_norm=0.34893742203712463, loss=3.616330623626709
I0306 10:52:16.121191 139758764189440 logging_writer.py:48] [97000] global_step=97000, grad_norm=0.33629053831100464, loss=3.671684503555298
I0306 10:52:50.627649 139758755796736 logging_writer.py:48] [97100] global_step=97100, grad_norm=0.35673338174819946, loss=3.717082977294922
I0306 10:53:25.121813 139758764189440 logging_writer.py:48] [97200] global_step=97200, grad_norm=0.3792937099933624, loss=3.6793465614318848
I0306 10:53:59.631097 139758755796736 logging_writer.py:48] [97300] global_step=97300, grad_norm=0.3462655544281006, loss=3.6208479404449463
I0306 10:54:32.407830 139902437545152 spec.py:321] Evaluating on the training split.
I0306 10:54:35.024471 139902437545152 workload.py:181] Translating evaluation dataset.
I0306 10:57:43.020378 139902437545152 spec.py:333] Evaluating on the validation split.
I0306 10:57:45.633938 139902437545152 workload.py:181] Translating evaluation dataset.
I0306 11:00:22.064764 139902437545152 spec.py:349] Evaluating on the test split.
I0306 11:00:24.678161 139902437545152 workload.py:181] Translating evaluation dataset.
I0306 11:03:07.448040 139902437545152 submission_runner.py:469] Time since start: 57008.61s, 	Step: 97396, 	{'train/accuracy': 0.7025385499000549, 'train/loss': 1.5454511642456055, 'train/bleu': 35.72073720291342, 'validation/accuracy': 0.6896522045135498, 'validation/loss': 1.6093624830245972, 'validation/bleu': 30.477596377236903, 'validation/num_examples': 3000, 'test/accuracy': 0.7075541615486145, 'test/loss': 1.507065773010254, 'test/bleu': 30.393583267476235, 'test/num_examples': 3003, 'score': 33626.251001119614, 'total_duration': 57008.60900115967, 'accumulated_submission_time': 33626.251001119614, 'accumulated_eval_time': 23375.77606654167, 'accumulated_logging_time': 0.9121899604797363}
I0306 11:03:07.467370 139758764189440 logging_writer.py:48] [97396] accumulated_eval_time=23375.8, accumulated_logging_time=0.91219, accumulated_submission_time=33626.3, global_step=97396, preemption_count=0, score=33626.3, test/accuracy=0.707554, test/bleu=30.3936, test/loss=1.50707, test/num_examples=3003, total_duration=57008.6, train/accuracy=0.702539, train/bleu=35.7207, train/loss=1.54545, validation/accuracy=0.689652, validation/bleu=30.4776, validation/loss=1.60936, validation/num_examples=3000
I0306 11:03:09.199014 139758755796736 logging_writer.py:48] [97400] global_step=97400, grad_norm=0.35657021403312683, loss=3.662773370742798
I0306 11:03:43.695875 139758764189440 logging_writer.py:48] [97500] global_step=97500, grad_norm=0.38185298442840576, loss=3.639616012573242
I0306 11:04:18.189783 139758755796736 logging_writer.py:48] [97600] global_step=97600, grad_norm=0.3663609027862549, loss=3.6815531253814697
I0306 11:04:52.684869 139758764189440 logging_writer.py:48] [97700] global_step=97700, grad_norm=0.37570545077323914, loss=3.672011613845825
I0306 11:05:27.168334 139758755796736 logging_writer.py:48] [97800] global_step=97800, grad_norm=0.3871222138404846, loss=3.7080466747283936
I0306 11:06:01.680785 139758764189440 logging_writer.py:48] [97900] global_step=97900, grad_norm=0.33519700169563293, loss=3.620985984802246
I0306 11:06:36.165628 139758755796736 logging_writer.py:48] [98000] global_step=98000, grad_norm=0.34571632742881775, loss=3.7296180725097656
I0306 11:07:10.675584 139758764189440 logging_writer.py:48] [98100] global_step=98100, grad_norm=0.34537920355796814, loss=3.665501356124878
I0306 11:07:45.191897 139758755796736 logging_writer.py:48] [98200] global_step=98200, grad_norm=0.3580816388130188, loss=3.6244771480560303
I0306 11:08:19.666414 139758764189440 logging_writer.py:48] [98300] global_step=98300, grad_norm=0.3589591681957245, loss=3.6837778091430664
I0306 11:08:54.164700 139758755796736 logging_writer.py:48] [98400] global_step=98400, grad_norm=0.36734825372695923, loss=3.6712000370025635
I0306 11:09:28.641880 139758764189440 logging_writer.py:48] [98500] global_step=98500, grad_norm=0.3629261255264282, loss=3.6848669052124023
I0306 11:10:03.122746 139758755796736 logging_writer.py:48] [98600] global_step=98600, grad_norm=0.3522863984107971, loss=3.6216869354248047
I0306 11:10:37.623090 139758764189440 logging_writer.py:48] [98700] global_step=98700, grad_norm=0.34929072856903076, loss=3.6296451091766357
I0306 11:11:12.087043 139758755796736 logging_writer.py:48] [98800] global_step=98800, grad_norm=0.3540479838848114, loss=3.648022174835205
I0306 11:11:46.582964 139758764189440 logging_writer.py:48] [98900] global_step=98900, grad_norm=0.3439722955226898, loss=3.6737635135650635
I0306 11:12:21.070059 139758755796736 logging_writer.py:48] [99000] global_step=99000, grad_norm=0.3643195927143097, loss=3.687467098236084
I0306 11:12:55.570093 139758764189440 logging_writer.py:48] [99100] global_step=99100, grad_norm=0.3489113450050354, loss=3.6079230308532715
I0306 11:13:30.069519 139758755796736 logging_writer.py:48] [99200] global_step=99200, grad_norm=0.3624764680862427, loss=3.659838914871216
I0306 11:14:04.565310 139758764189440 logging_writer.py:48] [99300] global_step=99300, grad_norm=0.38208240270614624, loss=3.6636507511138916
I0306 11:14:39.064713 139758755796736 logging_writer.py:48] [99400] global_step=99400, grad_norm=0.37508469820022583, loss=3.700105667114258
I0306 11:15:13.549480 139758764189440 logging_writer.py:48] [99500] global_step=99500, grad_norm=0.35593631863594055, loss=3.7028567790985107
I0306 11:15:48.040478 139758755796736 logging_writer.py:48] [99600] global_step=99600, grad_norm=0.34844961762428284, loss=3.6166796684265137
I0306 11:16:22.549608 139758764189440 logging_writer.py:48] [99700] global_step=99700, grad_norm=0.34481972455978394, loss=3.631424903869629
I0306 11:16:57.045974 139758755796736 logging_writer.py:48] [99800] global_step=99800, grad_norm=0.36517900228500366, loss=3.6736974716186523
I0306 11:17:07.739268 139902437545152 spec.py:321] Evaluating on the training split.
I0306 11:17:10.353596 139902437545152 workload.py:181] Translating evaluation dataset.
I0306 11:20:40.024940 139902437545152 spec.py:333] Evaluating on the validation split.
I0306 11:20:42.634900 139902437545152 workload.py:181] Translating evaluation dataset.
I0306 11:23:21.094413 139902437545152 spec.py:349] Evaluating on the test split.
I0306 11:23:23.710050 139902437545152 workload.py:181] Translating evaluation dataset.
I0306 11:25:58.194607 139902437545152 submission_runner.py:469] Time since start: 58379.36s, 	Step: 99832, 	{'train/accuracy': 0.704258382320404, 'train/loss': 1.5403200387954712, 'train/bleu': 36.27556227984201, 'validation/accuracy': 0.6905668377876282, 'validation/loss': 1.606310486793518, 'validation/bleu': 30.433622101092496, 'validation/num_examples': 3000, 'test/accuracy': 0.7080060243606567, 'test/loss': 1.504724383354187, 'test/bleu': 30.26165718079541, 'test/num_examples': 3003, 'score': 34466.37129020691, 'total_duration': 58379.35556912422, 'accumulated_submission_time': 34466.37129020691, 'accumulated_eval_time': 23906.23136138916, 'accumulated_logging_time': 0.9409689903259277}
I0306 11:25:58.214279 139758764189440 logging_writer.py:48] [99832] accumulated_eval_time=23906.2, accumulated_logging_time=0.940969, accumulated_submission_time=34466.4, global_step=99832, preemption_count=0, score=34466.4, test/accuracy=0.708006, test/bleu=30.2617, test/loss=1.50472, test/num_examples=3003, total_duration=58379.4, train/accuracy=0.704258, train/bleu=36.2756, train/loss=1.54032, validation/accuracy=0.690567, validation/bleu=30.4336, validation/loss=1.60631, validation/num_examples=3000
I0306 11:26:22.024664 139758755796736 logging_writer.py:48] [99900] global_step=99900, grad_norm=0.35932981967926025, loss=3.6418120861053467
I0306 11:26:56.531349 139758764189440 logging_writer.py:48] [100000] global_step=100000, grad_norm=0.3462633192539215, loss=3.6560890674591064
I0306 11:27:31.026074 139758755796736 logging_writer.py:48] [100100] global_step=100100, grad_norm=0.3705565929412842, loss=3.6422717571258545
I0306 11:28:05.532600 139758764189440 logging_writer.py:48] [100200] global_step=100200, grad_norm=0.3616606295108795, loss=3.6641862392425537
I0306 11:28:40.026225 139758755796736 logging_writer.py:48] [100300] global_step=100300, grad_norm=0.3404584228992462, loss=3.6028661727905273
I0306 11:29:14.537643 139758764189440 logging_writer.py:48] [100400] global_step=100400, grad_norm=0.38110193610191345, loss=3.6266088485717773
I0306 11:29:49.025225 139758755796736 logging_writer.py:48] [100500] global_step=100500, grad_norm=0.37661609053611755, loss=3.672760486602783
I0306 11:30:23.518827 139758764189440 logging_writer.py:48] [100600] global_step=100600, grad_norm=0.34786751866340637, loss=3.6565020084381104
I0306 11:30:58.041728 139758755796736 logging_writer.py:48] [100700] global_step=100700, grad_norm=0.3802054524421692, loss=3.6326420307159424
I0306 11:31:32.526532 139758764189440 logging_writer.py:48] [100800] global_step=100800, grad_norm=0.35781270265579224, loss=3.655191421508789
I0306 11:32:07.006265 139758755796736 logging_writer.py:48] [100900] global_step=100900, grad_norm=0.363839328289032, loss=3.6299633979797363
I0306 11:32:41.516298 139758764189440 logging_writer.py:48] [101000] global_step=101000, grad_norm=0.36125078797340393, loss=3.6688005924224854
I0306 11:33:16.029335 139758755796736 logging_writer.py:48] [101100] global_step=101100, grad_norm=0.3573077321052551, loss=3.643393039703369
I0306 11:33:50.527534 139758764189440 logging_writer.py:48] [101200] global_step=101200, grad_norm=0.38652658462524414, loss=3.7033817768096924
I0306 11:34:25.018454 139758755796736 logging_writer.py:48] [101300] global_step=101300, grad_norm=0.35065993666648865, loss=3.618062973022461
I0306 11:34:59.555569 139758764189440 logging_writer.py:48] [101400] global_step=101400, grad_norm=0.3702297806739807, loss=3.706754446029663
I0306 11:35:34.071962 139758755796736 logging_writer.py:48] [101500] global_step=101500, grad_norm=0.35438239574432373, loss=3.624481678009033
I0306 11:36:08.601202 139758764189440 logging_writer.py:48] [101600] global_step=101600, grad_norm=0.3739805221557617, loss=3.655921697616577
I0306 11:36:43.118487 139758755796736 logging_writer.py:48] [101700] global_step=101700, grad_norm=0.36076223850250244, loss=3.6270995140075684
I0306 11:37:17.680994 139758764189440 logging_writer.py:48] [101800] global_step=101800, grad_norm=0.36798039078712463, loss=3.665862798690796
I0306 11:37:52.193721 139758755796736 logging_writer.py:48] [101900] global_step=101900, grad_norm=0.34779858589172363, loss=3.6427292823791504
I0306 11:38:26.669030 139758764189440 logging_writer.py:48] [102000] global_step=102000, grad_norm=0.37842145562171936, loss=3.7119202613830566
I0306 11:39:01.173429 139758755796736 logging_writer.py:48] [102100] global_step=102100, grad_norm=0.3459751307964325, loss=3.6060664653778076
I0306 11:39:35.648207 139758764189440 logging_writer.py:48] [102200] global_step=102200, grad_norm=0.3635751008987427, loss=3.6697006225585938
I0306 11:39:58.410450 139902437545152 spec.py:321] Evaluating on the training split.
I0306 11:40:01.027662 139902437545152 workload.py:181] Translating evaluation dataset.
I0306 11:43:20.336555 139902437545152 spec.py:333] Evaluating on the validation split.
I0306 11:43:22.958745 139902437545152 workload.py:181] Translating evaluation dataset.
I0306 11:46:05.735825 139902437545152 spec.py:349] Evaluating on the test split.
I0306 11:46:08.348920 139902437545152 workload.py:181] Translating evaluation dataset.
I0306 11:48:49.834466 139902437545152 submission_runner.py:469] Time since start: 59751.00s, 	Step: 102267, 	{'train/accuracy': 0.7130854725837708, 'train/loss': 1.4925562143325806, 'train/bleu': 36.811206768579, 'validation/accuracy': 0.6894050240516663, 'validation/loss': 1.6044464111328125, 'validation/bleu': 30.361469755951063, 'validation/num_examples': 3000, 'test/accuracy': 0.7086085081100464, 'test/loss': 1.5032068490982056, 'test/bleu': 30.37053236210157, 'test/num_examples': 3003, 'score': 35306.41939783096, 'total_duration': 59750.995429039, 'accumulated_submission_time': 35306.41939783096, 'accumulated_eval_time': 24437.65533065796, 'accumulated_logging_time': 0.9689481258392334}
I0306 11:48:49.854212 139758755796736 logging_writer.py:48] [102267] accumulated_eval_time=24437.7, accumulated_logging_time=0.968948, accumulated_submission_time=35306.4, global_step=102267, preemption_count=0, score=35306.4, test/accuracy=0.708609, test/bleu=30.3705, test/loss=1.50321, test/num_examples=3003, total_duration=59751, train/accuracy=0.713085, train/bleu=36.8112, train/loss=1.49256, validation/accuracy=0.689405, validation/bleu=30.3615, validation/loss=1.60445, validation/num_examples=3000
I0306 11:49:01.560642 139758764189440 logging_writer.py:48] [102300] global_step=102300, grad_norm=0.36621007323265076, loss=3.6518590450286865
I0306 11:49:36.070729 139758755796736 logging_writer.py:48] [102400] global_step=102400, grad_norm=0.3513505756855011, loss=3.631061553955078
I0306 11:50:10.573301 139758764189440 logging_writer.py:48] [102500] global_step=102500, grad_norm=0.4037339687347412, loss=3.6911048889160156
I0306 11:50:45.058323 139758755796736 logging_writer.py:48] [102600] global_step=102600, grad_norm=0.3596396744251251, loss=3.6445651054382324
I0306 11:51:19.577670 139758764189440 logging_writer.py:48] [102700] global_step=102700, grad_norm=0.37073805928230286, loss=3.6229374408721924
I0306 11:51:54.068858 139758755796736 logging_writer.py:48] [102800] global_step=102800, grad_norm=0.3602852523326874, loss=3.601815700531006
I0306 11:52:28.599915 139758764189440 logging_writer.py:48] [102900] global_step=102900, grad_norm=0.3892541527748108, loss=3.671541213989258
I0306 11:53:03.114758 139758755796736 logging_writer.py:48] [103000] global_step=103000, grad_norm=0.36442098021507263, loss=3.635636568069458
I0306 11:53:37.607133 139758764189440 logging_writer.py:48] [103100] global_step=103100, grad_norm=0.39804989099502563, loss=3.67288875579834
I0306 11:54:12.103854 139758755796736 logging_writer.py:48] [103200] global_step=103200, grad_norm=0.3677324652671814, loss=3.648104190826416
I0306 11:54:46.576768 139758764189440 logging_writer.py:48] [103300] global_step=103300, grad_norm=0.34669229388237, loss=3.6011948585510254
I0306 11:55:21.079337 139758755796736 logging_writer.py:48] [103400] global_step=103400, grad_norm=0.39895546436309814, loss=3.71398663520813
I0306 11:55:55.587929 139758764189440 logging_writer.py:48] [103500] global_step=103500, grad_norm=0.36510229110717773, loss=3.6071321964263916
I0306 11:56:30.090011 139758755796736 logging_writer.py:48] [103600] global_step=103600, grad_norm=0.3623944818973541, loss=3.6165266036987305
I0306 11:57:04.589386 139758764189440 logging_writer.py:48] [103700] global_step=103700, grad_norm=0.3900037109851837, loss=3.6765928268432617
I0306 11:57:39.119467 139758755796736 logging_writer.py:48] [103800] global_step=103800, grad_norm=0.35067903995513916, loss=3.614279270172119
I0306 11:58:13.625346 139758764189440 logging_writer.py:48] [103900] global_step=103900, grad_norm=0.38293540477752686, loss=3.619514226913452
I0306 11:58:48.141261 139758755796736 logging_writer.py:48] [104000] global_step=104000, grad_norm=0.3593325912952423, loss=3.5580379962921143
I0306 11:59:22.637150 139758764189440 logging_writer.py:48] [104100] global_step=104100, grad_norm=0.38313427567481995, loss=3.6416099071502686
I0306 11:59:57.141295 139758755796736 logging_writer.py:48] [104200] global_step=104200, grad_norm=0.3716920018196106, loss=3.642530679702759
I0306 12:00:31.606894 139758764189440 logging_writer.py:48] [104300] global_step=104300, grad_norm=0.3759313225746155, loss=3.669132709503174
I0306 12:01:06.131779 139758755796736 logging_writer.py:48] [104400] global_step=104400, grad_norm=0.37980547547340393, loss=3.6670656204223633
I0306 12:01:40.635847 139758764189440 logging_writer.py:48] [104500] global_step=104500, grad_norm=0.3741205632686615, loss=3.6384031772613525
I0306 12:02:15.116446 139758755796736 logging_writer.py:48] [104600] global_step=104600, grad_norm=0.383800208568573, loss=3.7003297805786133
I0306 12:02:49.608299 139758764189440 logging_writer.py:48] [104700] global_step=104700, grad_norm=0.37779510021209717, loss=3.6387405395507812
I0306 12:02:49.960758 139902437545152 spec.py:321] Evaluating on the training split.
I0306 12:02:52.575881 139902437545152 workload.py:181] Translating evaluation dataset.
I0306 12:06:08.788465 139902437545152 spec.py:333] Evaluating on the validation split.
I0306 12:06:11.388642 139902437545152 workload.py:181] Translating evaluation dataset.
I0306 12:08:51.184002 139902437545152 spec.py:349] Evaluating on the test split.
I0306 12:08:53.795460 139902437545152 workload.py:181] Translating evaluation dataset.
I0306 12:11:18.702650 139902437545152 submission_runner.py:469] Time since start: 61099.86s, 	Step: 104702, 	{'train/accuracy': 0.7111444473266602, 'train/loss': 1.5039526224136353, 'train/bleu': 36.730065726943835, 'validation/accuracy': 0.6906904578208923, 'validation/loss': 1.6080873012542725, 'validation/bleu': 30.178659966114846, 'validation/num_examples': 3000, 'test/accuracy': 0.7075194120407104, 'test/loss': 1.5062726736068726, 'test/bleu': 30.42645662688949, 'test/num_examples': 3003, 'score': 36146.3758354187, 'total_duration': 61099.86361145973, 'accumulated_submission_time': 36146.3758354187, 'accumulated_eval_time': 24946.39716911316, 'accumulated_logging_time': 0.9978053569793701}
I0306 12:11:18.723162 139758755796736 logging_writer.py:48] [104702] accumulated_eval_time=24946.4, accumulated_logging_time=0.997805, accumulated_submission_time=36146.4, global_step=104702, preemption_count=0, score=36146.4, test/accuracy=0.707519, test/bleu=30.4265, test/loss=1.50627, test/num_examples=3003, total_duration=61099.9, train/accuracy=0.711144, train/bleu=36.7301, train/loss=1.50395, validation/accuracy=0.69069, validation/bleu=30.1787, validation/loss=1.60809, validation/num_examples=3000
I0306 12:11:52.841324 139758764189440 logging_writer.py:48] [104800] global_step=104800, grad_norm=0.3967758119106293, loss=3.621150493621826
I0306 12:12:27.332948 139758755796736 logging_writer.py:48] [104900] global_step=104900, grad_norm=0.36685603857040405, loss=3.606186628341675
I0306 12:13:01.855770 139758764189440 logging_writer.py:48] [105000] global_step=105000, grad_norm=0.377811461687088, loss=3.629210948944092
I0306 12:13:36.354971 139758755796736 logging_writer.py:48] [105100] global_step=105100, grad_norm=0.3574357032775879, loss=3.610992431640625
I0306 12:14:10.874508 139758764189440 logging_writer.py:48] [105200] global_step=105200, grad_norm=0.3695388734340668, loss=3.646456718444824
I0306 12:14:45.373089 139758755796736 logging_writer.py:48] [105300] global_step=105300, grad_norm=0.3702722191810608, loss=3.5975189208984375
I0306 12:15:19.856643 139758764189440 logging_writer.py:48] [105400] global_step=105400, grad_norm=0.36976367235183716, loss=3.570251941680908
I0306 12:15:54.368239 139758755796736 logging_writer.py:48] [105500] global_step=105500, grad_norm=0.3858480453491211, loss=3.585758686065674
I0306 12:16:28.859158 139758764189440 logging_writer.py:48] [105600] global_step=105600, grad_norm=0.3738985061645508, loss=3.6049246788024902
I0306 12:17:03.346029 139758755796736 logging_writer.py:48] [105700] global_step=105700, grad_norm=0.3856223225593567, loss=3.592731475830078
I0306 12:17:37.842138 139758764189440 logging_writer.py:48] [105800] global_step=105800, grad_norm=0.40678417682647705, loss=3.645162582397461
I0306 12:18:12.342321 139758755796736 logging_writer.py:48] [105900] global_step=105900, grad_norm=0.3580937683582306, loss=3.609880208969116
I0306 12:18:46.843074 139758764189440 logging_writer.py:48] [106000] global_step=106000, grad_norm=0.40209436416625977, loss=3.6280934810638428
I0306 12:19:21.348518 139758755796736 logging_writer.py:48] [106100] global_step=106100, grad_norm=0.367362916469574, loss=3.628897190093994
I0306 12:19:55.843052 139758764189440 logging_writer.py:48] [106200] global_step=106200, grad_norm=0.37513285875320435, loss=3.648653984069824
I0306 12:20:30.327875 139758755796736 logging_writer.py:48] [106300] global_step=106300, grad_norm=0.39966872334480286, loss=3.6299867630004883
I0306 12:21:04.822460 139758764189440 logging_writer.py:48] [106400] global_step=106400, grad_norm=0.37271425127983093, loss=3.6617231369018555
I0306 12:21:39.318155 139758755796736 logging_writer.py:48] [106500] global_step=106500, grad_norm=0.3690106272697449, loss=3.6896395683288574
I0306 12:22:13.827381 139758764189440 logging_writer.py:48] [106600] global_step=106600, grad_norm=0.3901093900203705, loss=3.682481527328491
I0306 12:22:48.351836 139758755796736 logging_writer.py:48] [106700] global_step=106700, grad_norm=0.36524784564971924, loss=3.5922951698303223
I0306 12:23:22.866683 139758764189440 logging_writer.py:48] [106800] global_step=106800, grad_norm=0.38847362995147705, loss=3.700368642807007
I0306 12:23:57.341416 139758755796736 logging_writer.py:48] [106900] global_step=106900, grad_norm=0.3846701383590698, loss=3.6489646434783936
I0306 12:24:31.859630 139758764189440 logging_writer.py:48] [107000] global_step=107000, grad_norm=0.36739808320999146, loss=3.661367893218994
I0306 12:25:06.363360 139758755796736 logging_writer.py:48] [107100] global_step=107100, grad_norm=0.3844683766365051, loss=3.6183876991271973
I0306 12:25:18.777303 139902437545152 spec.py:321] Evaluating on the training split.
I0306 12:25:21.397437 139902437545152 workload.py:181] Translating evaluation dataset.
I0306 12:28:23.861374 139902437545152 spec.py:333] Evaluating on the validation split.
I0306 12:28:26.475527 139902437545152 workload.py:181] Translating evaluation dataset.
I0306 12:31:09.528643 139902437545152 spec.py:349] Evaluating on the test split.
I0306 12:31:12.143517 139902437545152 workload.py:181] Translating evaluation dataset.
I0306 12:33:34.711731 139902437545152 submission_runner.py:469] Time since start: 62435.87s, 	Step: 107137, 	{'train/accuracy': 0.7217720150947571, 'train/loss': 1.4472801685333252, 'train/bleu': 36.94325686690794, 'validation/accuracy': 0.690010666847229, 'validation/loss': 1.6060467958450317, 'validation/bleu': 30.11693402292428, 'validation/num_examples': 3000, 'test/accuracy': 0.7074382901191711, 'test/loss': 1.5042269229888916, 'test/bleu': 30.18926699185561, 'test/num_examples': 3003, 'score': 36986.282564878464, 'total_duration': 62435.872681856155, 'accumulated_submission_time': 36986.282564878464, 'accumulated_eval_time': 25442.331533432007, 'accumulated_logging_time': 1.0274174213409424}
I0306 12:33:34.734983 139758764189440 logging_writer.py:48] [107137] accumulated_eval_time=25442.3, accumulated_logging_time=1.02742, accumulated_submission_time=36986.3, global_step=107137, preemption_count=0, score=36986.3, test/accuracy=0.707438, test/bleu=30.1893, test/loss=1.50423, test/num_examples=3003, total_duration=62435.9, train/accuracy=0.721772, train/bleu=36.9433, train/loss=1.44728, validation/accuracy=0.690011, validation/bleu=30.1169, validation/loss=1.60605, validation/num_examples=3000
I0306 12:33:56.800822 139758755796736 logging_writer.py:48] [107200] global_step=107200, grad_norm=0.3846941292285919, loss=3.585557460784912
I0306 12:34:31.294118 139758764189440 logging_writer.py:48] [107300] global_step=107300, grad_norm=0.3774424195289612, loss=3.678701400756836
I0306 12:35:05.787096 139758755796736 logging_writer.py:48] [107400] global_step=107400, grad_norm=0.38077372312545776, loss=3.6385867595672607
I0306 12:35:40.250226 139758764189440 logging_writer.py:48] [107500] global_step=107500, grad_norm=0.39167454838752747, loss=3.6394686698913574
I0306 12:36:14.719592 139758755796736 logging_writer.py:48] [107600] global_step=107600, grad_norm=0.3666265308856964, loss=3.574481725692749
I0306 12:36:49.234945 139758764189440 logging_writer.py:48] [107700] global_step=107700, grad_norm=0.3716906011104584, loss=3.598874807357788
I0306 12:37:23.735384 139758755796736 logging_writer.py:48] [107800] global_step=107800, grad_norm=0.3991391956806183, loss=3.606489419937134
I0306 12:37:58.263425 139758764189440 logging_writer.py:48] [107900] global_step=107900, grad_norm=0.4010840654373169, loss=3.616513729095459
I0306 12:38:32.794052 139758755796736 logging_writer.py:48] [108000] global_step=108000, grad_norm=0.37684571743011475, loss=3.622723340988159
I0306 12:39:07.317929 139758764189440 logging_writer.py:48] [108100] global_step=108100, grad_norm=0.34702301025390625, loss=3.578322410583496
I0306 12:39:41.815745 139758755796736 logging_writer.py:48] [108200] global_step=108200, grad_norm=0.3856450915336609, loss=3.6172595024108887
I0306 12:40:16.310432 139758764189440 logging_writer.py:48] [108300] global_step=108300, grad_norm=0.39734652638435364, loss=3.5942182540893555
I0306 12:40:50.819305 139758755796736 logging_writer.py:48] [108400] global_step=108400, grad_norm=0.38668957352638245, loss=3.6558170318603516
I0306 12:41:25.316514 139758764189440 logging_writer.py:48] [108500] global_step=108500, grad_norm=0.3807027339935303, loss=3.640165328979492
I0306 12:41:59.819164 139758755796736 logging_writer.py:48] [108600] global_step=108600, grad_norm=0.38753488659858704, loss=3.7390379905700684
I0306 12:42:34.337099 139758764189440 logging_writer.py:48] [108700] global_step=108700, grad_norm=0.3781554102897644, loss=3.645749807357788
I0306 12:43:08.820650 139758755796736 logging_writer.py:48] [108800] global_step=108800, grad_norm=0.3657175302505493, loss=3.6007187366485596
I0306 12:43:43.286471 139758764189440 logging_writer.py:48] [108900] global_step=108900, grad_norm=0.3673448860645294, loss=3.622915744781494
I0306 12:44:17.765803 139758755796736 logging_writer.py:48] [109000] global_step=109000, grad_norm=0.3523675203323364, loss=3.5906519889831543
I0306 12:44:52.287841 139758764189440 logging_writer.py:48] [109100] global_step=109100, grad_norm=0.3706826865673065, loss=3.6303067207336426
I0306 12:45:26.808785 139758755796736 logging_writer.py:48] [109200] global_step=109200, grad_norm=0.37283968925476074, loss=3.574134588241577
I0306 12:46:01.300640 139758764189440 logging_writer.py:48] [109300] global_step=109300, grad_norm=0.3843105137348175, loss=3.605442762374878
I0306 12:46:35.769940 139758755796736 logging_writer.py:48] [109400] global_step=109400, grad_norm=0.41004040837287903, loss=3.632507801055908
I0306 12:47:10.281446 139758764189440 logging_writer.py:48] [109500] global_step=109500, grad_norm=0.3623247742652893, loss=3.5959746837615967
I0306 12:47:34.762981 139902437545152 spec.py:321] Evaluating on the training split.
I0306 12:47:37.376805 139902437545152 workload.py:181] Translating evaluation dataset.
I0306 12:50:39.051813 139902437545152 spec.py:333] Evaluating on the validation split.
I0306 12:50:41.660835 139902437545152 workload.py:181] Translating evaluation dataset.
I0306 12:53:21.820250 139902437545152 spec.py:349] Evaluating on the test split.
I0306 12:53:24.437412 139902437545152 workload.py:181] Translating evaluation dataset.
I0306 12:55:50.924178 139902437545152 submission_runner.py:469] Time since start: 63772.09s, 	Step: 109572, 	{'train/accuracy': 0.7169502377510071, 'train/loss': 1.467590093612671, 'train/bleu': 37.121234463609994, 'validation/accuracy': 0.6901095509529114, 'validation/loss': 1.6037318706512451, 'validation/bleu': 30.221404611523788, 'validation/num_examples': 3000, 'test/accuracy': 0.7078554034233093, 'test/loss': 1.5015900135040283, 'test/bleu': 30.26127301062836, 'test/num_examples': 3003, 'score': 37826.163227796555, 'total_duration': 63772.08512926102, 'accumulated_submission_time': 37826.163227796555, 'accumulated_eval_time': 25938.49269080162, 'accumulated_logging_time': 1.0587437152862549}
I0306 12:55:50.945420 139758755796736 logging_writer.py:48] [109572] accumulated_eval_time=25938.5, accumulated_logging_time=1.05874, accumulated_submission_time=37826.2, global_step=109572, preemption_count=0, score=37826.2, test/accuracy=0.707855, test/bleu=30.2613, test/loss=1.50159, test/num_examples=3003, total_duration=63772.1, train/accuracy=0.71695, train/bleu=37.1212, train/loss=1.46759, validation/accuracy=0.69011, validation/bleu=30.2214, validation/loss=1.60373, validation/num_examples=3000
I0306 12:56:00.958434 139758764189440 logging_writer.py:48] [109600] global_step=109600, grad_norm=0.3636305034160614, loss=3.6434247493743896
I0306 12:56:35.474538 139758755796736 logging_writer.py:48] [109700] global_step=109700, grad_norm=0.3760722279548645, loss=3.5978076457977295
I0306 12:57:09.997681 139758764189440 logging_writer.py:48] [109800] global_step=109800, grad_norm=0.37875163555145264, loss=3.6173787117004395
I0306 12:57:44.509111 139758755796736 logging_writer.py:48] [109900] global_step=109900, grad_norm=0.37093475461006165, loss=3.5764577388763428
I0306 12:58:19.027344 139758764189440 logging_writer.py:48] [110000] global_step=110000, grad_norm=0.38440489768981934, loss=3.6103200912475586
I0306 12:58:53.528982 139758755796736 logging_writer.py:48] [110100] global_step=110100, grad_norm=0.38480523228645325, loss=3.60158371925354
I0306 12:59:28.037301 139758764189440 logging_writer.py:48] [110200] global_step=110200, grad_norm=0.3900821805000305, loss=3.6042017936706543
I0306 13:00:02.534332 139758755796736 logging_writer.py:48] [110300] global_step=110300, grad_norm=0.3735301196575165, loss=3.6335678100585938
I0306 13:00:37.091966 139758764189440 logging_writer.py:48] [110400] global_step=110400, grad_norm=0.4120805263519287, loss=3.5747830867767334
I0306 13:01:11.606126 139758755796736 logging_writer.py:48] [110500] global_step=110500, grad_norm=0.39223670959472656, loss=3.6066665649414062
I0306 13:01:46.167218 139758764189440 logging_writer.py:48] [110600] global_step=110600, grad_norm=0.39246299862861633, loss=3.6203432083129883
I0306 13:02:20.652181 139758755796736 logging_writer.py:48] [110700] global_step=110700, grad_norm=0.38527920842170715, loss=3.615730047225952
I0306 13:02:55.203011 139758764189440 logging_writer.py:48] [110800] global_step=110800, grad_norm=0.38016021251678467, loss=3.603309154510498
I0306 13:03:29.691460 139758755796736 logging_writer.py:48] [110900] global_step=110900, grad_norm=0.3747154772281647, loss=3.6027417182922363
I0306 13:04:04.184746 139758764189440 logging_writer.py:48] [111000] global_step=111000, grad_norm=0.37992334365844727, loss=3.5599515438079834
I0306 13:04:38.694331 139758755796736 logging_writer.py:48] [111100] global_step=111100, grad_norm=0.3953363001346588, loss=3.610574245452881
I0306 13:05:13.187675 139758764189440 logging_writer.py:48] [111200] global_step=111200, grad_norm=0.38337478041648865, loss=3.6955654621124268
I0306 13:05:47.684586 139758755796736 logging_writer.py:48] [111300] global_step=111300, grad_norm=0.36560672521591187, loss=3.6147515773773193
I0306 13:06:22.172449 139758764189440 logging_writer.py:48] [111400] global_step=111400, grad_norm=0.3827073574066162, loss=3.6326122283935547
I0306 13:06:56.681598 139758755796736 logging_writer.py:48] [111500] global_step=111500, grad_norm=0.38610705733299255, loss=3.6483092308044434
I0306 13:07:31.182813 139758764189440 logging_writer.py:48] [111600] global_step=111600, grad_norm=0.3973129093647003, loss=3.6278812885284424
I0306 13:08:05.679513 139758755796736 logging_writer.py:48] [111700] global_step=111700, grad_norm=0.3729384243488312, loss=3.577648401260376
I0306 13:08:40.217299 139758764189440 logging_writer.py:48] [111800] global_step=111800, grad_norm=0.36762529611587524, loss=3.5669474601745605
I0306 13:09:14.700942 139758755796736 logging_writer.py:48] [111900] global_step=111900, grad_norm=0.38671261072158813, loss=3.6489505767822266
I0306 13:09:49.247163 139758764189440 logging_writer.py:48] [112000] global_step=112000, grad_norm=0.413962185382843, loss=3.634596109390259
I0306 13:09:50.980730 139902437545152 spec.py:321] Evaluating on the training split.
I0306 13:09:53.598943 139902437545152 workload.py:181] Translating evaluation dataset.
I0306 13:13:03.807118 139902437545152 spec.py:333] Evaluating on the validation split.
I0306 13:13:06.420357 139902437545152 workload.py:181] Translating evaluation dataset.
I0306 13:15:42.636331 139902437545152 spec.py:349] Evaluating on the test split.
I0306 13:15:45.251730 139902437545152 workload.py:181] Translating evaluation dataset.
I0306 13:18:17.027677 139902437545152 submission_runner.py:469] Time since start: 65118.19s, 	Step: 112006, 	{'train/accuracy': 0.714897871017456, 'train/loss': 1.4854174852371216, 'train/bleu': 37.17137071138833, 'validation/accuracy': 0.6905791759490967, 'validation/loss': 1.6034214496612549, 'validation/bleu': 30.40829550056841, 'validation/num_examples': 3000, 'test/accuracy': 0.7080175876617432, 'test/loss': 1.5024784803390503, 'test/bleu': 30.290080976555757, 'test/num_examples': 3003, 'score': 38666.04920244217, 'total_duration': 65118.1886343956, 'accumulated_submission_time': 38666.04920244217, 'accumulated_eval_time': 26444.539578437805, 'accumulated_logging_time': 1.0884015560150146}
I0306 13:18:17.048428 139758755796736 logging_writer.py:48] [112006] accumulated_eval_time=26444.5, accumulated_logging_time=1.0884, accumulated_submission_time=38666, global_step=112006, preemption_count=0, score=38666, test/accuracy=0.708018, test/bleu=30.2901, test/loss=1.50248, test/num_examples=3003, total_duration=65118.2, train/accuracy=0.714898, train/bleu=37.1714, train/loss=1.48542, validation/accuracy=0.690579, validation/bleu=30.4083, validation/loss=1.60342, validation/num_examples=3000
I0306 13:18:49.816011 139758764189440 logging_writer.py:48] [112100] global_step=112100, grad_norm=0.3934267461299896, loss=3.699530839920044
I0306 13:19:24.310919 139758755796736 logging_writer.py:48] [112200] global_step=112200, grad_norm=0.37714457511901855, loss=3.621427536010742
I0306 13:19:58.794317 139758764189440 logging_writer.py:48] [112300] global_step=112300, grad_norm=0.40663301944732666, loss=3.6499361991882324
I0306 13:20:33.321064 139758755796736 logging_writer.py:48] [112400] global_step=112400, grad_norm=0.4003044068813324, loss=3.5930261611938477
I0306 13:21:07.781600 139758764189440 logging_writer.py:48] [112500] global_step=112500, grad_norm=0.39362919330596924, loss=3.615229845046997
I0306 13:21:42.276641 139758755796736 logging_writer.py:48] [112600] global_step=112600, grad_norm=0.3719417154788971, loss=3.572895050048828
I0306 13:22:16.787788 139758764189440 logging_writer.py:48] [112700] global_step=112700, grad_norm=0.3840978145599365, loss=3.5873525142669678
I0306 13:22:51.293425 139758755796736 logging_writer.py:48] [112800] global_step=112800, grad_norm=0.402584433555603, loss=3.637666940689087
I0306 13:23:25.768228 139758764189440 logging_writer.py:48] [112900] global_step=112900, grad_norm=0.3847317695617676, loss=3.6092514991760254
I0306 13:24:00.299391 139758755796736 logging_writer.py:48] [113000] global_step=113000, grad_norm=0.38403022289276123, loss=3.664109706878662
I0306 13:24:34.735321 139758764189440 logging_writer.py:48] [113100] global_step=113100, grad_norm=0.3877081274986267, loss=3.6407511234283447
I0306 13:25:09.220671 139758755796736 logging_writer.py:48] [113200] global_step=113200, grad_norm=0.3769899606704712, loss=3.6139352321624756
I0306 13:25:43.687774 139758764189440 logging_writer.py:48] [113300] global_step=113300, grad_norm=0.3989252746105194, loss=3.612398624420166
I0306 13:26:18.154464 139758755796736 logging_writer.py:48] [113400] global_step=113400, grad_norm=0.37772002816200256, loss=3.627901792526245
I0306 13:26:52.638243 139758764189440 logging_writer.py:48] [113500] global_step=113500, grad_norm=0.3876916766166687, loss=3.6079399585723877
I0306 13:27:27.093655 139758755796736 logging_writer.py:48] [113600] global_step=113600, grad_norm=0.38773053884506226, loss=3.6227338314056396
I0306 13:28:01.575602 139758764189440 logging_writer.py:48] [113700] global_step=113700, grad_norm=0.40527570247650146, loss=3.6036226749420166
I0306 13:28:36.046550 139758755796736 logging_writer.py:48] [113800] global_step=113800, grad_norm=0.37384024262428284, loss=3.61814546585083
I0306 13:29:10.481202 139758764189440 logging_writer.py:48] [113900] global_step=113900, grad_norm=0.40686339139938354, loss=3.6242239475250244
I0306 13:29:44.945660 139758755796736 logging_writer.py:48] [114000] global_step=114000, grad_norm=0.36498308181762695, loss=3.606515884399414
I0306 13:30:19.439137 139758764189440 logging_writer.py:48] [114100] global_step=114100, grad_norm=0.3776951730251312, loss=3.591026544570923
I0306 13:30:53.915424 139758755796736 logging_writer.py:48] [114200] global_step=114200, grad_norm=0.39818382263183594, loss=3.637566328048706
I0306 13:31:28.381168 139758764189440 logging_writer.py:48] [114300] global_step=114300, grad_norm=0.38022932410240173, loss=3.63175892829895
I0306 13:32:02.849622 139758755796736 logging_writer.py:48] [114400] global_step=114400, grad_norm=0.3645784854888916, loss=3.570080518722534
I0306 13:32:17.319979 139902437545152 spec.py:321] Evaluating on the training split.
I0306 13:32:19.923685 139902437545152 workload.py:181] Translating evaluation dataset.
I0306 13:35:33.429686 139902437545152 spec.py:333] Evaluating on the validation split.
I0306 13:35:36.040200 139902437545152 workload.py:181] Translating evaluation dataset.
I0306 13:38:13.499262 139902437545152 spec.py:349] Evaluating on the test split.
I0306 13:38:16.117546 139902437545152 workload.py:181] Translating evaluation dataset.
I0306 13:40:41.014658 139902437545152 submission_runner.py:469] Time since start: 66462.18s, 	Step: 114443, 	{'train/accuracy': 0.7187286615371704, 'train/loss': 1.4650784730911255, 'train/bleu': 37.19136532165709, 'validation/accuracy': 0.6907646059989929, 'validation/loss': 1.6020084619522095, 'validation/bleu': 30.212214261023306, 'validation/num_examples': 3000, 'test/accuracy': 0.7082840800285339, 'test/loss': 1.5008214712142944, 'test/bleu': 30.303217273861026, 'test/num_examples': 3003, 'score': 39506.177600860596, 'total_duration': 66462.17561984062, 'accumulated_submission_time': 39506.177600860596, 'accumulated_eval_time': 26948.23420405388, 'accumulated_logging_time': 1.1175146102905273}
I0306 13:40:41.035445 139758764189440 logging_writer.py:48] [114443] accumulated_eval_time=26948.2, accumulated_logging_time=1.11751, accumulated_submission_time=39506.2, global_step=114443, preemption_count=0, score=39506.2, test/accuracy=0.708284, test/bleu=30.3032, test/loss=1.50082, test/num_examples=3003, total_duration=66462.2, train/accuracy=0.718729, train/bleu=37.1914, train/loss=1.46508, validation/accuracy=0.690765, validation/bleu=30.2122, validation/loss=1.60201, validation/num_examples=3000
I0306 13:41:01.057000 139758755796736 logging_writer.py:48] [114500] global_step=114500, grad_norm=0.3795225918292999, loss=3.588721990585327
I0306 13:41:35.533629 139758764189440 logging_writer.py:48] [114600] global_step=114600, grad_norm=0.3759652376174927, loss=3.6122164726257324
I0306 13:42:10.023121 139758755796736 logging_writer.py:48] [114700] global_step=114700, grad_norm=0.3809243440628052, loss=3.6027016639709473
I0306 13:42:44.484982 139758764189440 logging_writer.py:48] [114800] global_step=114800, grad_norm=0.3838583528995514, loss=3.5959970951080322
I0306 13:43:18.910832 139758755796736 logging_writer.py:48] [114900] global_step=114900, grad_norm=0.38935261964797974, loss=3.595118761062622
I0306 13:43:53.363725 139758764189440 logging_writer.py:48] [115000] global_step=115000, grad_norm=0.3697579503059387, loss=3.6496505737304688
I0306 13:44:27.832613 139758755796736 logging_writer.py:48] [115100] global_step=115100, grad_norm=0.3717970848083496, loss=3.6410481929779053
I0306 13:45:02.286267 139758764189440 logging_writer.py:48] [115200] global_step=115200, grad_norm=0.37598228454589844, loss=3.614187717437744
I0306 13:45:36.734711 139758755796736 logging_writer.py:48] [115300] global_step=115300, grad_norm=0.35804370045661926, loss=3.5651042461395264
I0306 13:46:11.197257 139758764189440 logging_writer.py:48] [115400] global_step=115400, grad_norm=0.37878286838531494, loss=3.5910754203796387
I0306 13:46:45.663784 139758755796736 logging_writer.py:48] [115500] global_step=115500, grad_norm=0.39946335554122925, loss=3.651109218597412
I0306 13:47:20.123713 139758764189440 logging_writer.py:48] [115600] global_step=115600, grad_norm=0.3802410662174225, loss=3.5950026512145996
I0306 13:47:54.586292 139758755796736 logging_writer.py:48] [115700] global_step=115700, grad_norm=0.3620976507663727, loss=3.5825436115264893
I0306 13:48:29.052711 139758764189440 logging_writer.py:48] [115800] global_step=115800, grad_norm=0.3828941881656647, loss=3.6611626148223877
I0306 13:49:03.528731 139758755796736 logging_writer.py:48] [115900] global_step=115900, grad_norm=0.3765900433063507, loss=3.645158052444458
I0306 13:49:38.001513 139758764189440 logging_writer.py:48] [116000] global_step=116000, grad_norm=0.3643900752067566, loss=3.618149757385254
I0306 13:50:12.465270 139758755796736 logging_writer.py:48] [116100] global_step=116100, grad_norm=0.3744871914386749, loss=3.614591360092163
I0306 13:50:46.906521 139758764189440 logging_writer.py:48] [116200] global_step=116200, grad_norm=0.3834960460662842, loss=3.64495849609375
I0306 13:51:21.384253 139758755796736 logging_writer.py:48] [116300] global_step=116300, grad_norm=0.38655465841293335, loss=3.6681251525878906
I0306 13:51:55.839096 139758764189440 logging_writer.py:48] [116400] global_step=116400, grad_norm=0.37760260701179504, loss=3.5674164295196533
I0306 13:52:30.286454 139758755796736 logging_writer.py:48] [116500] global_step=116500, grad_norm=0.3813181221485138, loss=3.6391687393188477
I0306 13:53:04.772332 139758764189440 logging_writer.py:48] [116600] global_step=116600, grad_norm=0.37996503710746765, loss=3.624317169189453
I0306 13:53:39.234351 139758755796736 logging_writer.py:48] [116700] global_step=116700, grad_norm=0.36181849241256714, loss=3.5718066692352295
I0306 13:54:13.687955 139758764189440 logging_writer.py:48] [116800] global_step=116800, grad_norm=0.36994436383247375, loss=3.600616931915283
I0306 13:54:41.260830 139902437545152 spec.py:321] Evaluating on the training split.
I0306 13:54:43.873943 139902437545152 workload.py:181] Translating evaluation dataset.
I0306 13:57:45.377703 139902437545152 spec.py:333] Evaluating on the validation split.
I0306 13:57:47.989410 139902437545152 workload.py:181] Translating evaluation dataset.
I0306 14:00:28.256413 139902437545152 spec.py:349] Evaluating on the test split.
I0306 14:00:30.865935 139902437545152 workload.py:181] Translating evaluation dataset.
I0306 14:02:58.674431 139902437545152 submission_runner.py:469] Time since start: 67799.84s, 	Step: 116881, 	{'train/accuracy': 0.7183876633644104, 'train/loss': 1.4690229892730713, 'train/bleu': 37.57023253999794, 'validation/accuracy': 0.690678060054779, 'validation/loss': 1.6023274660110474, 'validation/bleu': 30.44762766319476, 'validation/num_examples': 3000, 'test/accuracy': 0.7086548805236816, 'test/loss': 1.5000958442687988, 'test/bleu': 30.450413474283124, 'test/num_examples': 3003, 'score': 40346.2559568882, 'total_duration': 67799.83539175987, 'accumulated_submission_time': 40346.2559568882, 'accumulated_eval_time': 27445.647748947144, 'accumulated_logging_time': 1.1465184688568115}
I0306 14:02:58.695534 139758755796736 logging_writer.py:48] [116881] accumulated_eval_time=27445.6, accumulated_logging_time=1.14652, accumulated_submission_time=40346.3, global_step=116881, preemption_count=0, score=40346.3, test/accuracy=0.708655, test/bleu=30.4504, test/loss=1.5001, test/num_examples=3003, total_duration=67799.8, train/accuracy=0.718388, train/bleu=37.5702, train/loss=1.46902, validation/accuracy=0.690678, validation/bleu=30.4476, validation/loss=1.60233, validation/num_examples=3000
I0306 14:03:05.598655 139758764189440 logging_writer.py:48] [116900] global_step=116900, grad_norm=0.39798420667648315, loss=3.6060242652893066
I0306 14:03:40.057172 139758755796736 logging_writer.py:48] [117000] global_step=117000, grad_norm=0.3646269738674164, loss=3.546060562133789
I0306 14:04:14.504106 139758764189440 logging_writer.py:48] [117100] global_step=117100, grad_norm=0.37067490816116333, loss=3.6404025554656982
I0306 14:04:48.943052 139758755796736 logging_writer.py:48] [117200] global_step=117200, grad_norm=0.38393479585647583, loss=3.5772292613983154
I0306 14:05:23.399701 139758764189440 logging_writer.py:48] [117300] global_step=117300, grad_norm=0.3786632716655731, loss=3.633758068084717
I0306 14:05:57.849054 139758755796736 logging_writer.py:48] [117400] global_step=117400, grad_norm=0.3606659471988678, loss=3.569321870803833
I0306 14:06:32.292597 139758764189440 logging_writer.py:48] [117500] global_step=117500, grad_norm=0.38118675351142883, loss=3.6375019550323486
I0306 14:07:06.756695 139758755796736 logging_writer.py:48] [117600] global_step=117600, grad_norm=0.37706097960472107, loss=3.6157352924346924
I0306 14:07:41.212624 139758764189440 logging_writer.py:48] [117700] global_step=117700, grad_norm=0.38787853717803955, loss=3.6138899326324463
I0306 14:08:15.664965 139758755796736 logging_writer.py:48] [117800] global_step=117800, grad_norm=0.3714959919452667, loss=3.6226935386657715
I0306 14:08:50.114762 139758764189440 logging_writer.py:48] [117900] global_step=117900, grad_norm=0.37692752480506897, loss=3.6167688369750977
I0306 14:09:24.548772 139758755796736 logging_writer.py:48] [118000] global_step=118000, grad_norm=0.3725685775279999, loss=3.6167144775390625
I0306 14:09:58.995574 139758764189440 logging_writer.py:48] [118100] global_step=118100, grad_norm=0.369150310754776, loss=3.6125378608703613
I0306 14:10:33.444808 139758755796736 logging_writer.py:48] [118200] global_step=118200, grad_norm=0.3751845359802246, loss=3.5955262184143066
I0306 14:11:07.938517 139758764189440 logging_writer.py:48] [118300] global_step=118300, grad_norm=0.3626042306423187, loss=3.5917673110961914
I0306 14:11:42.414080 139758755796736 logging_writer.py:48] [118400] global_step=118400, grad_norm=0.3674964904785156, loss=3.576688528060913
I0306 14:12:16.908369 139758764189440 logging_writer.py:48] [118500] global_step=118500, grad_norm=0.36381494998931885, loss=3.6583478450775146
I0306 14:12:51.381838 139758755796736 logging_writer.py:48] [118600] global_step=118600, grad_norm=0.3680545687675476, loss=3.607726812362671
I0306 14:13:25.863405 139758764189440 logging_writer.py:48] [118700] global_step=118700, grad_norm=0.36891594529151917, loss=3.550161600112915
I0306 14:14:00.277550 139758755796736 logging_writer.py:48] [118800] global_step=118800, grad_norm=0.38329392671585083, loss=3.61457896232605
I0306 14:14:34.757332 139758764189440 logging_writer.py:48] [118900] global_step=118900, grad_norm=0.38686835765838623, loss=3.639444589614868
I0306 14:15:09.206865 139758755796736 logging_writer.py:48] [119000] global_step=119000, grad_norm=0.3722127377986908, loss=3.6309168338775635
I0306 14:15:43.658390 139758764189440 logging_writer.py:48] [119100] global_step=119100, grad_norm=0.3807365894317627, loss=3.6000607013702393
I0306 14:16:18.117381 139758755796736 logging_writer.py:48] [119200] global_step=119200, grad_norm=0.3638257086277008, loss=3.61177921295166
I0306 14:16:52.592736 139758764189440 logging_writer.py:48] [119300] global_step=119300, grad_norm=0.3833194375038147, loss=3.612518548965454
I0306 14:16:58.807321 139902437545152 spec.py:321] Evaluating on the training split.
I0306 14:17:01.425781 139902437545152 workload.py:181] Translating evaluation dataset.
I0306 14:20:12.367487 139902437545152 spec.py:333] Evaluating on the validation split.
I0306 14:20:14.978772 139902437545152 workload.py:181] Translating evaluation dataset.
I0306 14:22:53.145552 139902437545152 spec.py:349] Evaluating on the test split.
I0306 14:22:55.749199 139902437545152 workload.py:181] Translating evaluation dataset.
I0306 14:25:20.966202 139902437545152 submission_runner.py:469] Time since start: 69142.13s, 	Step: 119319, 	{'train/accuracy': 0.7192891836166382, 'train/loss': 1.4630018472671509, 'train/bleu': 37.37378842006101, 'validation/accuracy': 0.6908387541770935, 'validation/loss': 1.6021571159362793, 'validation/bleu': 30.358176824890734, 'validation/num_examples': 3000, 'test/accuracy': 0.7085737586021423, 'test/loss': 1.4997133016586304, 'test/bleu': 30.289649958570816, 'test/num_examples': 3003, 'score': 41186.22371196747, 'total_duration': 69142.12714934349, 'accumulated_submission_time': 41186.22371196747, 'accumulated_eval_time': 27947.80655837059, 'accumulated_logging_time': 1.1762158870697021}
I0306 14:25:20.987687 139758755796736 logging_writer.py:48] [119319] accumulated_eval_time=27947.8, accumulated_logging_time=1.17622, accumulated_submission_time=41186.2, global_step=119319, preemption_count=0, score=41186.2, test/accuracy=0.708574, test/bleu=30.2896, test/loss=1.49971, test/num_examples=3003, total_duration=69142.1, train/accuracy=0.719289, train/bleu=37.3738, train/loss=1.463, validation/accuracy=0.690839, validation/bleu=30.3582, validation/loss=1.60216, validation/num_examples=3000
I0306 14:25:49.271786 139758764189440 logging_writer.py:48] [119400] global_step=119400, grad_norm=0.3709472417831421, loss=3.6134846210479736
I0306 14:26:23.777719 139758755796736 logging_writer.py:48] [119500] global_step=119500, grad_norm=0.37369680404663086, loss=3.571261405944824
I0306 14:26:58.278871 139758764189440 logging_writer.py:48] [119600] global_step=119600, grad_norm=0.37375444173812866, loss=3.621316432952881
I0306 14:27:32.755541 139758755796736 logging_writer.py:48] [119700] global_step=119700, grad_norm=0.3665711581707001, loss=3.6117632389068604
I0306 14:28:07.248049 139758764189440 logging_writer.py:48] [119800] global_step=119800, grad_norm=0.36108943819999695, loss=3.605679512023926
I0306 14:28:41.761809 139758755796736 logging_writer.py:48] [119900] global_step=119900, grad_norm=0.39228078722953796, loss=3.6397266387939453
I0306 14:29:16.258476 139758764189440 logging_writer.py:48] [120000] global_step=120000, grad_norm=0.3769613802433014, loss=3.6222500801086426
I0306 14:29:50.763092 139758755796736 logging_writer.py:48] [120100] global_step=120100, grad_norm=0.4094666540622711, loss=3.6557769775390625
I0306 14:30:25.275750 139758764189440 logging_writer.py:48] [120200] global_step=120200, grad_norm=0.4094637930393219, loss=3.596592903137207
I0306 14:30:59.774687 139758755796736 logging_writer.py:48] [120300] global_step=120300, grad_norm=0.3558650314807892, loss=3.550564765930176
I0306 14:31:34.255931 139758764189440 logging_writer.py:48] [120400] global_step=120400, grad_norm=0.38888615369796753, loss=3.6209003925323486
I0306 14:32:08.756763 139758755796736 logging_writer.py:48] [120500] global_step=120500, grad_norm=0.3689846694469452, loss=3.630990982055664
I0306 14:32:43.268952 139758764189440 logging_writer.py:48] [120600] global_step=120600, grad_norm=0.3844146132469177, loss=3.569425582885742
I0306 14:33:17.763854 139758755796736 logging_writer.py:48] [120700] global_step=120700, grad_norm=0.37460729479789734, loss=3.6138410568237305
I0306 14:33:52.241831 139758764189440 logging_writer.py:48] [120800] global_step=120800, grad_norm=0.3825587332248688, loss=3.6011812686920166
I0306 14:34:26.718734 139758755796736 logging_writer.py:48] [120900] global_step=120900, grad_norm=0.375476211309433, loss=3.625070571899414
I0306 14:35:01.243594 139758764189440 logging_writer.py:48] [121000] global_step=121000, grad_norm=0.35780808329582214, loss=3.595581293106079
I0306 14:35:35.746181 139758755796736 logging_writer.py:48] [121100] global_step=121100, grad_norm=0.38221386075019836, loss=3.5607783794403076
I0306 14:36:10.237302 139758764189440 logging_writer.py:48] [121200] global_step=121200, grad_norm=0.37815913558006287, loss=3.610211133956909
I0306 14:36:44.723788 139758755796736 logging_writer.py:48] [121300] global_step=121300, grad_norm=0.3843575716018677, loss=3.6478219032287598
I0306 14:37:19.205132 139758764189440 logging_writer.py:48] [121400] global_step=121400, grad_norm=0.367207795381546, loss=3.659691333770752
I0306 14:37:53.715098 139758755796736 logging_writer.py:48] [121500] global_step=121500, grad_norm=0.3895399868488312, loss=3.6442229747772217
I0306 14:38:28.179448 139758764189440 logging_writer.py:48] [121600] global_step=121600, grad_norm=0.3842434585094452, loss=3.594856023788452
I0306 14:39:02.700211 139758755796736 logging_writer.py:48] [121700] global_step=121700, grad_norm=0.38023489713668823, loss=3.5580828189849854
I0306 14:39:20.980166 139902437545152 spec.py:321] Evaluating on the training split.
I0306 14:39:23.601121 139902437545152 workload.py:181] Translating evaluation dataset.
I0306 14:42:33.244685 139902437545152 spec.py:333] Evaluating on the validation split.
I0306 14:42:35.860090 139902437545152 workload.py:181] Translating evaluation dataset.
I0306 14:45:15.663883 139902437545152 spec.py:349] Evaluating on the test split.
I0306 14:45:18.278089 139902437545152 workload.py:181] Translating evaluation dataset.
I0306 14:47:44.439537 139902437545152 submission_runner.py:469] Time since start: 70485.60s, 	Step: 121754, 	{'train/accuracy': 0.7201446890830994, 'train/loss': 1.4637565612792969, 'train/bleu': 37.29770397519877, 'validation/accuracy': 0.6909253001213074, 'validation/loss': 1.6021336317062378, 'validation/bleu': 30.360267221076306, 'validation/num_examples': 3000, 'test/accuracy': 0.7085621356964111, 'test/loss': 1.499734878540039, 'test/bleu': 30.29667962556078, 'test/num_examples': 3003, 'score': 42026.07177782059, 'total_duration': 70485.60050177574, 'accumulated_submission_time': 42026.07177782059, 'accumulated_eval_time': 28451.265877962112, 'accumulated_logging_time': 1.2070021629333496}
I0306 14:47:44.460722 139758764189440 logging_writer.py:48] [121754] accumulated_eval_time=28451.3, accumulated_logging_time=1.207, accumulated_submission_time=42026.1, global_step=121754, preemption_count=0, score=42026.1, test/accuracy=0.708562, test/bleu=30.2967, test/loss=1.49973, test/num_examples=3003, total_duration=70485.6, train/accuracy=0.720145, train/bleu=37.2977, train/loss=1.46376, validation/accuracy=0.690925, validation/bleu=30.3603, validation/loss=1.60213, validation/num_examples=3000
I0306 14:48:00.668243 139758755796736 logging_writer.py:48] [121800] global_step=121800, grad_norm=0.3604912757873535, loss=3.6115036010742188
I0306 14:48:35.176547 139758764189440 logging_writer.py:48] [121900] global_step=121900, grad_norm=0.387688010931015, loss=3.6423542499542236
I0306 14:49:09.655175 139758755796736 logging_writer.py:48] [122000] global_step=122000, grad_norm=0.37936434149742126, loss=3.604560613632202
I0306 14:49:44.128216 139758764189440 logging_writer.py:48] [122100] global_step=122100, grad_norm=0.37427082657814026, loss=3.665252923965454
I0306 14:50:18.627510 139758755796736 logging_writer.py:48] [122200] global_step=122200, grad_norm=0.3705379366874695, loss=3.6049697399139404
I0306 14:50:53.114938 139758764189440 logging_writer.py:48] [122300] global_step=122300, grad_norm=0.3927339017391205, loss=3.6567423343658447
I0306 14:51:27.646033 139758755796736 logging_writer.py:48] [122400] global_step=122400, grad_norm=0.3863866627216339, loss=3.5659420490264893
I0306 14:52:02.134433 139758764189440 logging_writer.py:48] [122500] global_step=122500, grad_norm=0.3939293622970581, loss=3.62597918510437
I0306 14:52:36.643251 139758755796736 logging_writer.py:48] [122600] global_step=122600, grad_norm=0.3975105285644531, loss=3.6238150596618652
I0306 14:53:11.097396 139758764189440 logging_writer.py:48] [122700] global_step=122700, grad_norm=0.37884628772735596, loss=3.617245674133301
I0306 14:53:45.580523 139758755796736 logging_writer.py:48] [122800] global_step=122800, grad_norm=0.37556353211402893, loss=3.5773091316223145
I0306 14:54:20.074277 139758764189440 logging_writer.py:48] [122900] global_step=122900, grad_norm=0.38243794441223145, loss=3.6308720111846924
I0306 14:54:54.549501 139758755796736 logging_writer.py:48] [123000] global_step=123000, grad_norm=0.36807548999786377, loss=3.5953369140625
I0306 14:55:29.036614 139758764189440 logging_writer.py:48] [123100] global_step=123100, grad_norm=0.3944210112094879, loss=3.673823118209839
I0306 14:56:03.538361 139758755796736 logging_writer.py:48] [123200] global_step=123200, grad_norm=0.3692916929721832, loss=3.63771915435791
I0306 14:56:38.037849 139758764189440 logging_writer.py:48] [123300] global_step=123300, grad_norm=0.37651288509368896, loss=3.5929839611053467
I0306 14:57:12.545260 139758755796736 logging_writer.py:48] [123400] global_step=123400, grad_norm=0.37377798557281494, loss=3.63527774810791
I0306 14:57:47.048019 139758764189440 logging_writer.py:48] [123500] global_step=123500, grad_norm=0.36369749903678894, loss=3.6213390827178955
I0306 14:58:21.504462 139758755796736 logging_writer.py:48] [123600] global_step=123600, grad_norm=0.38441962003707886, loss=3.575822353363037
I0306 14:58:55.987715 139758764189440 logging_writer.py:48] [123700] global_step=123700, grad_norm=0.38086360692977905, loss=3.6223292350769043
I0306 14:59:30.466604 139758755796736 logging_writer.py:48] [123800] global_step=123800, grad_norm=0.3970200717449188, loss=3.6639556884765625
I0306 15:00:04.969738 139758764189440 logging_writer.py:48] [123900] global_step=123900, grad_norm=0.3684364855289459, loss=3.6451234817504883
I0306 15:00:39.437606 139758755796736 logging_writer.py:48] [124000] global_step=124000, grad_norm=0.3852839171886444, loss=3.5613205432891846
I0306 15:01:13.920637 139758764189440 logging_writer.py:48] [124100] global_step=124100, grad_norm=0.3692106008529663, loss=3.6014416217803955
I0306 15:01:44.638759 139902437545152 spec.py:321] Evaluating on the training split.
I0306 15:01:47.256010 139902437545152 workload.py:181] Translating evaluation dataset.
I0306 15:04:51.368663 139902437545152 spec.py:333] Evaluating on the validation split.
I0306 15:04:53.980157 139902437545152 workload.py:181] Translating evaluation dataset.
I0306 15:07:33.775860 139902437545152 spec.py:349] Evaluating on the test split.
I0306 15:07:36.381942 139902437545152 workload.py:181] Translating evaluation dataset.
I0306 15:10:02.869900 139902437545152 submission_runner.py:469] Time since start: 71824.03s, 	Step: 124190, 	{'train/accuracy': 0.7220081090927124, 'train/loss': 1.4429612159729004, 'train/bleu': 36.945465416202964, 'validation/accuracy': 0.6909253001213074, 'validation/loss': 1.6021336317062378, 'validation/bleu': 30.360267221076306, 'validation/num_examples': 3000, 'test/accuracy': 0.7085621356964111, 'test/loss': 1.499734878540039, 'test/bleu': 30.29667962556078, 'test/num_examples': 3003, 'score': 42866.10156774521, 'total_duration': 71824.03085231781, 'accumulated_submission_time': 42866.10156774521, 'accumulated_eval_time': 28949.49695777893, 'accumulated_logging_time': 1.236433744430542}
I0306 15:10:02.896967 139758755796736 logging_writer.py:48] [124190] accumulated_eval_time=28949.5, accumulated_logging_time=1.23643, accumulated_submission_time=42866.1, global_step=124190, preemption_count=0, score=42866.1, test/accuracy=0.708562, test/bleu=30.2967, test/loss=1.49973, test/num_examples=3003, total_duration=71824, train/accuracy=0.722008, train/bleu=36.9455, train/loss=1.44296, validation/accuracy=0.690925, validation/bleu=30.3603, validation/loss=1.60213, validation/num_examples=3000
I0306 15:10:06.701239 139758764189440 logging_writer.py:48] [124200] global_step=124200, grad_norm=0.3661375939846039, loss=3.6335299015045166
I0306 15:10:41.160038 139758755796736 logging_writer.py:48] [124300] global_step=124300, grad_norm=0.37348899245262146, loss=3.6104354858398438
I0306 15:11:15.619753 139758764189440 logging_writer.py:48] [124400] global_step=124400, grad_norm=0.3680666983127594, loss=3.6222333908081055
I0306 15:11:50.103876 139758755796736 logging_writer.py:48] [124500] global_step=124500, grad_norm=0.38773590326309204, loss=3.665405035018921
I0306 15:12:24.592361 139758764189440 logging_writer.py:48] [124600] global_step=124600, grad_norm=0.3939538896083832, loss=3.6292812824249268
I0306 15:12:59.104002 139758755796736 logging_writer.py:48] [124700] global_step=124700, grad_norm=0.40227752923965454, loss=3.6881513595581055
I0306 15:13:33.592152 139758764189440 logging_writer.py:48] [124800] global_step=124800, grad_norm=0.3691062033176422, loss=3.5534510612487793
I0306 15:14:08.098856 139758755796736 logging_writer.py:48] [124900] global_step=124900, grad_norm=0.37282565236091614, loss=3.621286392211914
I0306 15:14:42.601233 139758764189440 logging_writer.py:48] [125000] global_step=125000, grad_norm=0.3866879343986511, loss=3.64518404006958
I0306 15:15:17.112614 139758755796736 logging_writer.py:48] [125100] global_step=125100, grad_norm=0.36928170919418335, loss=3.632272720336914
I0306 15:15:51.597654 139758764189440 logging_writer.py:48] [125200] global_step=125200, grad_norm=0.38647016882896423, loss=3.657763957977295
I0306 15:16:26.095149 139758755796736 logging_writer.py:48] [125300] global_step=125300, grad_norm=0.3718971908092499, loss=3.572219133377075
I0306 15:17:00.578232 139758764189440 logging_writer.py:48] [125400] global_step=125400, grad_norm=0.36490944027900696, loss=3.6081907749176025
I0306 15:17:35.056582 139758755796736 logging_writer.py:48] [125500] global_step=125500, grad_norm=0.39020612835884094, loss=3.621875524520874
I0306 15:18:09.588350 139758764189440 logging_writer.py:48] [125600] global_step=125600, grad_norm=0.3851986825466156, loss=3.6125030517578125
I0306 15:18:44.062357 139758755796736 logging_writer.py:48] [125700] global_step=125700, grad_norm=0.41042569279670715, loss=3.6065752506256104
I0306 15:19:18.548670 139758764189440 logging_writer.py:48] [125800] global_step=125800, grad_norm=0.36794766783714294, loss=3.575453758239746
I0306 15:19:53.241603 139758755796736 logging_writer.py:48] [125900] global_step=125900, grad_norm=0.3774871230125427, loss=3.598191261291504
I0306 15:20:27.844267 139758764189440 logging_writer.py:48] [126000] global_step=126000, grad_norm=0.3726786673069, loss=3.6388638019561768
I0306 15:21:02.493979 139758755796736 logging_writer.py:48] [126100] global_step=126100, grad_norm=0.37975555658340454, loss=3.605916738510132
I0306 15:21:37.087779 139758764189440 logging_writer.py:48] [126200] global_step=126200, grad_norm=0.39679697155952454, loss=3.6687819957733154
I0306 15:22:11.725717 139758755796736 logging_writer.py:48] [126300] global_step=126300, grad_norm=0.3842603266239166, loss=3.6852424144744873
I0306 15:22:46.359147 139758764189440 logging_writer.py:48] [126400] global_step=126400, grad_norm=0.38734957575798035, loss=3.6030731201171875
I0306 15:23:20.980931 139758755796736 logging_writer.py:48] [126500] global_step=126500, grad_norm=0.37140408158302307, loss=3.6309287548065186
I0306 15:23:55.604254 139758764189440 logging_writer.py:48] [126600] global_step=126600, grad_norm=0.3589703440666199, loss=3.6037333011627197
I0306 15:24:02.890730 139758755796736 logging_writer.py:48] [126622] global_step=126622, preemption_count=0, score=43705.9
I0306 15:24:02.918369 139902437545152 submission_runner.py:646] Tuning trial 2/5
I0306 15:24:02.918491 139902437545152 submission_runner.py:647] Hyperparameters: Hyperparameters(dropout_rate=0.0, label_smoothing=0.2, learning_rate=0.0008445074561975979, one_minus_beta1=0.11042418465, beta2=0.9978504782314613, weight_decay=0.08135402759553023, warmup_factor=0.05)
I0306 15:24:02.920930 139902437545152 submission_runner.py:648] Metrics: {'eval_results': [(1, {'train/accuracy': 0.0006909416988492012, 'train/loss': 11.127187728881836, 'train/bleu': 0.0, 'validation/accuracy': 0.00048204089398495853, 'validation/loss': 11.128292083740234, 'validation/bleu': 0.0, 'validation/num_examples': 3000, 'test/accuracy': 0.0007183408597484231, 'test/loss': 11.135275840759277, 'test/bleu': 0.0, 'test/num_examples': 3003, 'score': 25.889487981796265, 'total_duration': 941.6768958568573, 'accumulated_submission_time': 25.889487981796265, 'accumulated_eval_time': 915.7872984409332, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (2434, {'train/accuracy': 0.4387555420398712, 'train/loss': 3.780032157897949, 'train/bleu': 17.282684711147255, 'validation/accuracy': 0.4347885251045227, 'validation/loss': 3.847177743911743, 'validation/bleu': 12.865042631562853, 'validation/num_examples': 3000, 'test/accuracy': 0.42230331897735596, 'test/loss': 4.007822036743164, 'test/bleu': 10.885582125641006, 'test/num_examples': 3003, 'score': 865.9368352890015, 'total_duration': 2496.0343101024628, 'accumulated_submission_time': 865.9368352890015, 'accumulated_eval_time': 1629.9207653999329, 'accumulated_logging_time': 0.015886306762695312, 'global_step': 2434, 'preemption_count': 0}), (4866, {'train/accuracy': 0.5515024065971375, 'train/loss': 2.7679195404052734, 'train/bleu': 24.42808442075665, 'validation/accuracy': 0.5552369356155396, 'validation/loss': 2.7197988033294678, 'validation/bleu': 20.86397315378164, 'validation/num_examples': 3000, 'test/accuracy': 0.5571544170379639, 'test/loss': 2.739936113357544, 'test/bleu': 19.249846800938936, 'test/num_examples': 3003, 'score': 1705.8244485855103, 'total_duration': 3848.028921842575, 'accumulated_submission_time': 1705.8244485855103, 'accumulated_eval_time': 2141.853317260742, 'accumulated_logging_time': 0.03449702262878418, 'global_step': 4866, 'preemption_count': 0}), (7304, {'train/accuracy': 0.4056222438812256, 'train/loss': 3.751762866973877, 'train/bleu': 7.95668004695303, 'validation/accuracy': 0.37379181385040283, 'validation/loss': 4.059120178222656, 'validation/bleu': 4.061260610006268, 'validation/num_examples': 3000, 'test/accuracy': 0.35187116265296936, 'test/loss': 4.292884826660156, 'test/bleu': 3.022149049837015, 'test/num_examples': 3003, 'score': 2545.790619134903, 'total_duration': 5457.225398302078, 'accumulated_submission_time': 2545.790619134903, 'accumulated_eval_time': 2910.9125502109528, 'accumulated_logging_time': 0.05387568473815918, 'global_step': 7304, 'preemption_count': 0}), (9743, {'train/accuracy': 0.5885036587715149, 'train/loss': 2.3954296112060547, 'train/bleu': 27.392375123596306, 'validation/accuracy': 0.6007589101791382, 'validation/loss': 2.3107426166534424, 'validation/bleu': 23.99182101207609, 'validation/num_examples': 3000, 'test/accuracy': 0.6055034399032593, 'test/loss': 2.296506404876709, 'test/bleu': 22.460690194453218, 'test/num_examples': 3003, 'score': 3385.8995292186737, 'total_duration': 6799.925513744354, 'accumulated_submission_time': 3385.8995292186737, 'accumulated_eval_time': 3413.335329055786, 'accumulated_logging_time': 0.07249164581298828, 'global_step': 9743, 'preemption_count': 0}), (12184, {'train/accuracy': 0.6078491806983948, 'train/loss': 2.213330030441284, 'train/bleu': 28.804056562663057, 'validation/accuracy': 0.624218225479126, 'validation/loss': 2.0900421142578125, 'validation/bleu': 25.43472218448631, 'validation/num_examples': 3000, 'test/accuracy': 0.6304831504821777, 'test/loss': 2.0576913356781006, 'test/bleu': 24.261085169981413, 'test/num_examples': 3003, 'score': 4226.010152339935, 'total_duration': 8184.752869844437, 'accumulated_submission_time': 4226.010152339935, 'accumulated_eval_time': 3957.8882093429565, 'accumulated_logging_time': 0.09021759033203125, 'global_step': 12184, 'preemption_count': 0}), (14621, {'train/accuracy': 0.6263440847396851, 'train/loss': 2.0837655067443848, 'train/bleu': 30.384154777135624, 'validation/accuracy': 0.6366153359413147, 'validation/loss': 2.00032114982605, 'validation/bleu': 26.74528699509487, 'validation/num_examples': 3000, 'test/accuracy': 0.6456030607223511, 'test/loss': 1.9602564573287964, 'test/bleu': 25.735044288347087, 'test/num_examples': 3003, 'score': 5065.888287782669, 'total_duration': 9596.61524772644, 'accumulated_submission_time': 5065.888287782669, 'accumulated_eval_time': 4529.713856935501, 'accumulated_logging_time': 0.10824131965637207, 'global_step': 14621, 'preemption_count': 0}), (17057, {'train/accuracy': 0.6272832155227661, 'train/loss': 2.0608181953430176, 'train/bleu': 29.794582781972643, 'validation/accuracy': 0.6458606123924255, 'validation/loss': 1.9317424297332764, 'validation/bleu': 27.330965677977527, 'validation/num_examples': 3000, 'test/accuracy': 0.6537597179412842, 'test/loss': 1.87694251537323, 'test/bleu': 26.329894935044557, 'test/num_examples': 3003, 'score': 5905.828632354736, 'total_duration': 10924.18899345398, 'accumulated_submission_time': 5905.828632354736, 'accumulated_eval_time': 5017.182178735733, 'accumulated_logging_time': 0.12886714935302734, 'global_step': 17057, 'preemption_count': 0}), (19490, {'train/accuracy': 0.6432692408561707, 'train/loss': 1.9442501068115234, 'train/bleu': 31.540684536979015, 'validation/accuracy': 0.6517563462257385, 'validation/loss': 1.8737787008285522, 'validation/bleu': 27.63499580458185, 'validation/num_examples': 3000, 'test/accuracy': 0.6616151332855225, 'test/loss': 1.8183088302612305, 'test/bleu': 26.892767325535203, 'test/num_examples': 3003, 'score': 6745.9588186740875, 'total_duration': 12271.139216423035, 'accumulated_submission_time': 6745.9588186740875, 'accumulated_eval_time': 5523.841565132141, 'accumulated_logging_time': 0.14786076545715332, 'global_step': 19490, 'preemption_count': 0}), (21916, {'train/accuracy': 0.6408578157424927, 'train/loss': 1.9679399728775024, 'train/bleu': 31.287286764335768, 'validation/accuracy': 0.6556127071380615, 'validation/loss': 1.8453642129898071, 'validation/bleu': 27.701844184533844, 'validation/num_examples': 3000, 'test/accuracy': 0.6660525798797607, 'test/loss': 1.7810513973236084, 'test/bleu': 27.568812016774732, 'test/num_examples': 3003, 'score': 7585.8901126384735, 'total_duration': 13668.946621894836, 'accumulated_submission_time': 7585.8901126384735, 'accumulated_eval_time': 6081.554447650909, 'accumulated_logging_time': 0.17025518417358398, 'global_step': 21916, 'preemption_count': 0}), (24342, {'train/accuracy': 0.6377574801445007, 'train/loss': 1.9934712648391724, 'train/bleu': 31.492986124887413, 'validation/accuracy': 0.659271240234375, 'validation/loss': 1.8386343717575073, 'validation/bleu': 28.20744508706731, 'validation/num_examples': 3000, 'test/accuracy': 0.6703162789344788, 'test/loss': 1.77511727809906, 'test/bleu': 27.348556720975562, 'test/num_examples': 3003, 'score': 8425.983776807785, 'total_duration': 14991.233741283417, 'accumulated_submission_time': 8425.983776807785, 'accumulated_eval_time': 6563.5871596336365, 'accumulated_logging_time': 0.18932700157165527, 'global_step': 24342, 'preemption_count': 0}), (26773, {'train/accuracy': 0.6529027819633484, 'train/loss': 1.8722681999206543, 'train/bleu': 31.658544632366144, 'validation/accuracy': 0.6639310121536255, 'validation/loss': 1.7937933206558228, 'validation/bleu': 28.43376417782063, 'validation/num_examples': 3000, 'test/accuracy': 0.6753794550895691, 'test/loss': 1.7213813066482544, 'test/bleu': 28.06940124881642, 'test/num_examples': 3003, 'score': 9265.884675264359, 'total_duration': 16437.04678416252, 'accumulated_submission_time': 9265.884675264359, 'accumulated_eval_time': 7169.336541652679, 'accumulated_logging_time': 0.21043825149536133, 'global_step': 26773, 'preemption_count': 0}), (29207, {'train/accuracy': 0.6426064372062683, 'train/loss': 1.9268392324447632, 'train/bleu': 31.19619603300917, 'validation/accuracy': 0.6654512882232666, 'validation/loss': 1.7741438150405884, 'validation/bleu': 28.504088752337683, 'validation/num_examples': 3000, 'test/accuracy': 0.6771173477172852, 'test/loss': 1.702561855316162, 'test/bleu': 28.1172874472845, 'test/num_examples': 3003, 'score': 10105.749613285065, 'total_duration': 17863.09965658188, 'accumulated_submission_time': 10105.749613285065, 'accumulated_eval_time': 7755.366237401962, 'accumulated_logging_time': 0.2306830883026123, 'global_step': 29207, 'preemption_count': 0}), (31641, {'train/accuracy': 0.6673250198364258, 'train/loss': 1.7735683917999268, 'train/bleu': 33.23419434757288, 'validation/accuracy': 0.6676760911941528, 'validation/loss': 1.754375696182251, 'validation/bleu': 29.049967447668795, 'validation/num_examples': 3000, 'test/accuracy': 0.6797010898590088, 'test/loss': 1.6794365644454956, 'test/bleu': 28.71526544515316, 'test/num_examples': 3003, 'score': 10945.931043863297, 'total_duration': 19192.117519378662, 'accumulated_submission_time': 10945.931043863297, 'accumulated_eval_time': 8244.041717529297, 'accumulated_logging_time': 0.2508728504180908, 'global_step': 31641, 'preemption_count': 0}), (34075, {'train/accuracy': 0.6514691114425659, 'train/loss': 1.8714592456817627, 'train/bleu': 31.947627286445925, 'validation/accuracy': 0.6667861342430115, 'validation/loss': 1.7505391836166382, 'validation/bleu': 28.83209301488511, 'validation/num_examples': 3000, 'test/accuracy': 0.6802803874015808, 'test/loss': 1.6794507503509521, 'test/bleu': 28.63501236329486, 'test/num_examples': 3003, 'score': 11785.844411849976, 'total_duration': 20526.566071748734, 'accumulated_submission_time': 11785.844411849976, 'accumulated_eval_time': 8738.411239385605, 'accumulated_logging_time': 0.27150917053222656, 'global_step': 34075, 'preemption_count': 0}), (36510, {'train/accuracy': 0.6509538888931274, 'train/loss': 1.875314712524414, 'train/bleu': 32.122094846082, 'validation/accuracy': 0.668232262134552, 'validation/loss': 1.744018793106079, 'validation/bleu': 29.19220812784533, 'validation/num_examples': 3000, 'test/accuracy': 0.6821225881576538, 'test/loss': 1.666090965270996, 'test/bleu': 28.56957321973769, 'test/num_examples': 3003, 'score': 12625.7977809906, 'total_duration': 21838.834758520126, 'accumulated_submission_time': 12625.7977809906, 'accumulated_eval_time': 9210.568281173706, 'accumulated_logging_time': 0.29282093048095703, 'global_step': 36510, 'preemption_count': 0}), (38945, {'train/accuracy': 0.661736249923706, 'train/loss': 1.8002737760543823, 'train/bleu': 32.833826159582145, 'validation/accuracy': 0.6705683469772339, 'validation/loss': 1.7332216501235962, 'validation/bleu': 29.18730435959975, 'validation/num_examples': 3000, 'test/accuracy': 0.684926450252533, 'test/loss': 1.6572619676589966, 'test/bleu': 28.714115980440354, 'test/num_examples': 3003, 'score': 13465.75188422203, 'total_duration': 23181.210790157318, 'accumulated_submission_time': 13465.75188422203, 'accumulated_eval_time': 9712.82789182663, 'accumulated_logging_time': 0.31394290924072266, 'global_step': 38945, 'preemption_count': 0}), (41380, {'train/accuracy': 0.6569118499755859, 'train/loss': 1.818016529083252, 'train/bleu': 32.01436866281932, 'validation/accuracy': 0.6717672348022461, 'validation/loss': 1.7065868377685547, 'validation/bleu': 29.14190404075308, 'validation/num_examples': 3000, 'test/accuracy': 0.6858417391777039, 'test/loss': 1.62874436378479, 'test/bleu': 28.94579133165665, 'test/num_examples': 3003, 'score': 14305.7463722229, 'total_duration': 24503.763870954514, 'accumulated_submission_time': 14305.7463722229, 'accumulated_eval_time': 10195.222837209702, 'accumulated_logging_time': 0.33432865142822266, 'global_step': 41380, 'preemption_count': 0}), (43816, {'train/accuracy': 0.6561476588249207, 'train/loss': 1.8416184186935425, 'train/bleu': 32.46140068314001, 'validation/accuracy': 0.6723605394363403, 'validation/loss': 1.7095853090286255, 'validation/bleu': 29.260692902819514, 'validation/num_examples': 3000, 'test/accuracy': 0.6873131990432739, 'test/loss': 1.6285985708236694, 'test/bleu': 28.64462975518613, 'test/num_examples': 3003, 'score': 15145.782103538513, 'total_duration': 26056.913061380386, 'accumulated_submission_time': 15145.782103538513, 'accumulated_eval_time': 10908.176669120789, 'accumulated_logging_time': 0.35532498359680176, 'global_step': 43816, 'preemption_count': 0}), (46252, {'train/accuracy': 0.6615137457847595, 'train/loss': 1.8036335706710815, 'train/bleu': 32.627632649451904, 'validation/accuracy': 0.674770712852478, 'validation/loss': 1.7107053995132446, 'validation/bleu': 29.3731631291291, 'validation/num_examples': 3000, 'test/accuracy': 0.6885992288589478, 'test/loss': 1.630706787109375, 'test/bleu': 29.006023035876307, 'test/num_examples': 3003, 'score': 15985.75174832344, 'total_duration': 27446.655457258224, 'accumulated_submission_time': 15985.75174832344, 'accumulated_eval_time': 11457.786594629288, 'accumulated_logging_time': 0.37760043144226074, 'global_step': 46252, 'preemption_count': 0}), (48687, {'train/accuracy': 0.6578896045684814, 'train/loss': 1.8279101848602295, 'train/bleu': 32.591304660575005, 'validation/accuracy': 0.6747089624404907, 'validation/loss': 1.711646556854248, 'validation/bleu': 29.245133285404577, 'validation/num_examples': 3000, 'test/accuracy': 0.6905109286308289, 'test/loss': 1.6266084909439087, 'test/bleu': 29.425035750226083, 'test/num_examples': 3003, 'score': 16825.683406829834, 'total_duration': 28815.279856443405, 'accumulated_submission_time': 16825.683406829834, 'accumulated_eval_time': 11986.31799530983, 'accumulated_logging_time': 0.3990492820739746, 'global_step': 48687, 'preemption_count': 0}), (51124, {'train/accuracy': 0.673754870891571, 'train/loss': 1.7215735912322998, 'train/bleu': 33.36355953415167, 'validation/accuracy': 0.6772056818008423, 'validation/loss': 1.6882215738296509, 'validation/bleu': 29.576554485971034, 'validation/num_examples': 3000, 'test/accuracy': 0.6914842128753662, 'test/loss': 1.605440378189087, 'test/bleu': 29.329503951261966, 'test/num_examples': 3003, 'score': 17665.840656280518, 'total_duration': 30196.05065202713, 'accumulated_submission_time': 17665.840656280518, 'accumulated_eval_time': 12526.766934394836, 'accumulated_logging_time': 0.42407870292663574, 'global_step': 51124, 'preemption_count': 0}), (53563, {'train/accuracy': 0.666636049747467, 'train/loss': 1.7679489850997925, 'train/bleu': 32.70166232392654, 'validation/accuracy': 0.6782068014144897, 'validation/loss': 1.6796603202819824, 'validation/bleu': 29.515353139881434, 'validation/num_examples': 3000, 'test/accuracy': 0.6935580968856812, 'test/loss': 1.5931921005249023, 'test/bleu': 29.523683682918122, 'test/num_examples': 3003, 'score': 18505.89375281334, 'total_duration': 31517.47703552246, 'accumulated_submission_time': 18505.89375281334, 'accumulated_eval_time': 13007.976008176804, 'accumulated_logging_time': 0.449481725692749, 'global_step': 53563, 'preemption_count': 0}), (56002, {'train/accuracy': 0.6629206538200378, 'train/loss': 1.7951312065124512, 'train/bleu': 32.96202748910094, 'validation/accuracy': 0.6785158514976501, 'validation/loss': 1.6791293621063232, 'validation/bleu': 29.548141942289305, 'validation/num_examples': 3000, 'test/accuracy': 0.6938709616661072, 'test/loss': 1.5931220054626465, 'test/bleu': 29.432093183896882, 'test/num_examples': 3003, 'score': 19346.01117825508, 'total_duration': 32962.79088115692, 'accumulated_submission_time': 19346.01117825508, 'accumulated_eval_time': 13613.009667396545, 'accumulated_logging_time': 0.4718954563140869, 'global_step': 56002, 'preemption_count': 0}), (58438, {'train/accuracy': 0.6713078618049622, 'train/loss': 1.7255960702896118, 'train/bleu': 33.472461913434955, 'validation/accuracy': 0.680258572101593, 'validation/loss': 1.663615107536316, 'validation/bleu': 29.819428355457234, 'validation/num_examples': 3000, 'test/accuracy': 0.695736289024353, 'test/loss': 1.581506371498108, 'test/bleu': 29.394815717509346, 'test/num_examples': 3003, 'score': 20185.984320640564, 'total_duration': 34589.017896175385, 'accumulated_submission_time': 20185.984320640564, 'accumulated_eval_time': 14399.097005844116, 'accumulated_logging_time': 0.49561452865600586, 'global_step': 58438, 'preemption_count': 0}), (60873, {'train/accuracy': 0.6695363521575928, 'train/loss': 1.7522913217544556, 'train/bleu': 33.44945571442318, 'validation/accuracy': 0.6807406544685364, 'validation/loss': 1.6651467084884644, 'validation/bleu': 29.62307318371099, 'validation/num_examples': 3000, 'test/accuracy': 0.6951106786727905, 'test/loss': 1.5814741849899292, 'test/bleu': 29.65481277059179, 'test/num_examples': 3003, 'score': 21025.833126306534, 'total_duration': 36110.43742418289, 'accumulated_submission_time': 21025.833126306534, 'accumulated_eval_time': 15080.500798940659, 'accumulated_logging_time': 0.5203084945678711, 'global_step': 60873, 'preemption_count': 0}), (63309, {'train/accuracy': 0.6891443133354187, 'train/loss': 1.62844979763031, 'train/bleu': 34.62150147358941, 'validation/accuracy': 0.6832002997398376, 'validation/loss': 1.6543325185775757, 'validation/bleu': 29.94172546870099, 'validation/num_examples': 3000, 'test/accuracy': 0.6966747641563416, 'test/loss': 1.5670961141586304, 'test/bleu': 29.407910582994433, 'test/num_examples': 3003, 'score': 21865.87493443489, 'total_duration': 37488.89387345314, 'accumulated_submission_time': 21865.87493443489, 'accumulated_eval_time': 15618.75349020958, 'accumulated_logging_time': 0.5432400703430176, 'global_step': 63309, 'preemption_count': 0}), (65744, {'train/accuracy': 0.6767931580543518, 'train/loss': 1.699054479598999, 'train/bleu': 33.88097765701787, 'validation/accuracy': 0.6815687417984009, 'validation/loss': 1.6546235084533691, 'validation/bleu': 30.069167612990167, 'validation/num_examples': 3000, 'test/accuracy': 0.6981925368309021, 'test/loss': 1.567050814628601, 'test/bleu': 30.041491673588858, 'test/num_examples': 3003, 'score': 22705.752933502197, 'total_duration': 38839.23977613449, 'accumulated_submission_time': 22705.752933502197, 'accumulated_eval_time': 16129.05905175209, 'accumulated_logging_time': 0.566107988357544, 'global_step': 65744, 'preemption_count': 0}), (68180, {'train/accuracy': 0.6731387376785278, 'train/loss': 1.7223179340362549, 'train/bleu': 33.382604382188845, 'validation/accuracy': 0.681309163570404, 'validation/loss': 1.6445823907852173, 'validation/bleu': 29.645243752770817, 'validation/num_examples': 3000, 'test/accuracy': 0.6976479887962341, 'test/loss': 1.5597974061965942, 'test/bleu': 29.69259264871251, 'test/num_examples': 3003, 'score': 23545.74523472786, 'total_duration': 40228.10393571854, 'accumulated_submission_time': 23545.74523472786, 'accumulated_eval_time': 16677.7655107975, 'accumulated_logging_time': 0.592252254486084, 'global_step': 68180, 'preemption_count': 0}), (70620, {'train/accuracy': 0.6845422387123108, 'train/loss': 1.654647707939148, 'train/bleu': 34.26978804583005, 'validation/accuracy': 0.6842014193534851, 'validation/loss': 1.6421239376068115, 'validation/bleu': 30.04992598556355, 'validation/num_examples': 3000, 'test/accuracy': 0.6999536752700806, 'test/loss': 1.5490161180496216, 'test/bleu': 29.88508683276483, 'test/num_examples': 3003, 'score': 24385.801114082336, 'total_duration': 41542.76871919632, 'accumulated_submission_time': 24385.801114082336, 'accumulated_eval_time': 17152.21170282364, 'accumulated_logging_time': 0.6166341304779053, 'global_step': 70620, 'preemption_count': 0}), (73059, {'train/accuracy': 0.677196204662323, 'train/loss': 1.700468897819519, 'train/bleu': 33.95536828809262, 'validation/accuracy': 0.6851902604103088, 'validation/loss': 1.6394544839859009, 'validation/bleu': 30.05032410491529, 'validation/num_examples': 3000, 'test/accuracy': 0.7013787627220154, 'test/loss': 1.5434184074401855, 'test/bleu': 29.87052076192903, 'test/num_examples': 3003, 'score': 25225.892251729965, 'total_duration': 42865.22264432907, 'accumulated_submission_time': 25225.892251729965, 'accumulated_eval_time': 17634.41001367569, 'accumulated_logging_time': 0.6408884525299072, 'global_step': 73059, 'preemption_count': 0}), (75492, {'train/accuracy': 0.7062452435493469, 'train/loss': 1.534682273864746, 'train/bleu': 36.55732137684128, 'validation/accuracy': 0.6857093572616577, 'validation/loss': 1.6275759935379028, 'validation/bleu': 27.918883764843752, 'validation/num_examples': 3000, 'test/accuracy': 0.7009153366088867, 'test/loss': 1.5392820835113525, 'test/bleu': 29.870187072222258, 'test/num_examples': 3003, 'score': 26065.73565506935, 'total_duration': 44531.30682468414, 'accumulated_submission_time': 26065.73565506935, 'accumulated_eval_time': 18460.48248577118, 'accumulated_logging_time': 0.6671042442321777, 'global_step': 75492, 'preemption_count': 0}), (77919, {'train/accuracy': 0.6888896822929382, 'train/loss': 1.628191590309143, 'train/bleu': 34.74474829487272, 'validation/accuracy': 0.6864138841629028, 'validation/loss': 1.6267073154449463, 'validation/bleu': 30.00586328803466, 'validation/num_examples': 3000, 'test/accuracy': 0.7012744545936584, 'test/loss': 1.5380299091339111, 'test/bleu': 29.935526737695604, 'test/num_examples': 3003, 'score': 26905.66841840744, 'total_duration': 45927.671696424484, 'accumulated_submission_time': 26905.66841840744, 'accumulated_eval_time': 19016.744868516922, 'accumulated_logging_time': 0.6931939125061035, 'global_step': 77919, 'preemption_count': 0}), (80348, {'train/accuracy': 0.6849319338798523, 'train/loss': 1.652205228805542, 'train/bleu': 34.472612769596346, 'validation/accuracy': 0.6878970861434937, 'validation/loss': 1.6195728778839111, 'validation/bleu': 30.571553424753397, 'validation/num_examples': 3000, 'test/accuracy': 0.7034063339233398, 'test/loss': 1.5280359983444214, 'test/bleu': 30.412107098044967, 'test/num_examples': 3003, 'score': 27745.74901175499, 'total_duration': 47273.86058163643, 'accumulated_submission_time': 27745.74901175499, 'accumulated_eval_time': 19522.691960573196, 'accumulated_logging_time': 0.719153881072998, 'global_step': 80348, 'preemption_count': 0}), (82775, {'train/accuracy': 0.6970760226249695, 'train/loss': 1.5778858661651611, 'train/bleu': 35.703569937022515, 'validation/accuracy': 0.6877240538597107, 'validation/loss': 1.6214996576309204, 'validation/bleu': 30.265075091693948, 'validation/num_examples': 3000, 'test/accuracy': 0.7047502994537354, 'test/loss': 1.5246622562408447, 'test/bleu': 30.125939590694443, 'test/num_examples': 3003, 'score': 28585.680349588394, 'total_duration': 48615.58778452873, 'accumulated_submission_time': 28585.680349588394, 'accumulated_eval_time': 20024.318504095078, 'accumulated_logging_time': 0.7488436698913574, 'global_step': 82775, 'preemption_count': 0}), (85205, {'train/accuracy': 0.691922128200531, 'train/loss': 1.6089333295822144, 'train/bleu': 35.338945808559764, 'validation/accuracy': 0.6878847479820251, 'validation/loss': 1.621588110923767, 'validation/bleu': 30.314236595613956, 'validation/num_examples': 3000, 'test/accuracy': 0.7032093405723572, 'test/loss': 1.5244542360305786, 'test/bleu': 30.02191390350524, 'test/num_examples': 3003, 'score': 29425.845414161682, 'total_duration': 49991.96995782852, 'accumulated_submission_time': 29425.845414161682, 'accumulated_eval_time': 20560.369824409485, 'accumulated_logging_time': 0.7776467800140381, 'global_step': 85205, 'preemption_count': 0}), (87640, {'train/accuracy': 0.6934027671813965, 'train/loss': 1.6033387184143066, 'train/bleu': 34.89259967237209, 'validation/accuracy': 0.6884903311729431, 'validation/loss': 1.61361825466156, 'validation/bleu': 29.800504956963028, 'validation/num_examples': 3000, 'test/accuracy': 0.7044143080711365, 'test/loss': 1.5201314687728882, 'test/bleu': 30.043788432620754, 'test/num_examples': 3003, 'score': 30265.936924934387, 'total_duration': 51636.417350530624, 'accumulated_submission_time': 30265.936924934387, 'accumulated_eval_time': 21364.565006494522, 'accumulated_logging_time': 0.8037164211273193, 'global_step': 87640, 'preemption_count': 0}), (90081, {'train/accuracy': 0.6959425806999207, 'train/loss': 1.586622714996338, 'train/bleu': 35.28927391162964, 'validation/accuracy': 0.6885521411895752, 'validation/loss': 1.612891435623169, 'validation/bleu': 30.207630332932705, 'validation/num_examples': 3000, 'test/accuracy': 0.7068706154823303, 'test/loss': 1.512256383895874, 'test/bleu': 30.15831282076521, 'test/num_examples': 3003, 'score': 31106.10369205475, 'total_duration': 52975.678736925125, 'accumulated_submission_time': 31106.10369205475, 'accumulated_eval_time': 21863.492612361908, 'accumulated_logging_time': 0.8297824859619141, 'global_step': 90081, 'preemption_count': 0}), (92521, {'train/accuracy': 0.6956267952919006, 'train/loss': 1.580920934677124, 'train/bleu': 35.035399892299154, 'validation/accuracy': 0.6892690658569336, 'validation/loss': 1.6103904247283936, 'validation/bleu': 30.613603211121163, 'validation/num_examples': 3000, 'test/accuracy': 0.7051790356636047, 'test/loss': 1.5125402212142944, 'test/bleu': 30.253658976005635, 'test/num_examples': 3003, 'score': 31946.06612801552, 'total_duration': 54293.887838840485, 'accumulated_submission_time': 31946.06612801552, 'accumulated_eval_time': 22341.569347143173, 'accumulated_logging_time': 0.8574233055114746, 'global_step': 92521, 'preemption_count': 0}), (94960, {'train/accuracy': 0.7145625352859497, 'train/loss': 1.4909170866012573, 'train/bleu': 36.784330342369046, 'validation/accuracy': 0.6893555521965027, 'validation/loss': 1.6122838258743286, 'validation/bleu': 30.405478002749792, 'validation/num_examples': 3000, 'test/accuracy': 0.7056772112846375, 'test/loss': 1.5149322748184204, 'test/bleu': 30.1216361182811, 'test/num_examples': 3003, 'score': 32786.13518118858, 'total_duration': 55653.2852768898, 'accumulated_submission_time': 32786.13518118858, 'accumulated_eval_time': 22860.735911130905, 'accumulated_logging_time': 0.8837485313415527, 'global_step': 94960, 'preemption_count': 0}), (97396, {'train/accuracy': 0.7025385499000549, 'train/loss': 1.5454511642456055, 'train/bleu': 35.72073720291342, 'validation/accuracy': 0.6896522045135498, 'validation/loss': 1.6093624830245972, 'validation/bleu': 30.477596377236903, 'validation/num_examples': 3000, 'test/accuracy': 0.7075541615486145, 'test/loss': 1.507065773010254, 'test/bleu': 30.393583267476235, 'test/num_examples': 3003, 'score': 33626.251001119614, 'total_duration': 57008.60900115967, 'accumulated_submission_time': 33626.251001119614, 'accumulated_eval_time': 23375.77606654167, 'accumulated_logging_time': 0.9121899604797363, 'global_step': 97396, 'preemption_count': 0}), (99832, {'train/accuracy': 0.704258382320404, 'train/loss': 1.5403200387954712, 'train/bleu': 36.27556227984201, 'validation/accuracy': 0.6905668377876282, 'validation/loss': 1.606310486793518, 'validation/bleu': 30.433622101092496, 'validation/num_examples': 3000, 'test/accuracy': 0.7080060243606567, 'test/loss': 1.504724383354187, 'test/bleu': 30.26165718079541, 'test/num_examples': 3003, 'score': 34466.37129020691, 'total_duration': 58379.35556912422, 'accumulated_submission_time': 34466.37129020691, 'accumulated_eval_time': 23906.23136138916, 'accumulated_logging_time': 0.9409689903259277, 'global_step': 99832, 'preemption_count': 0}), (102267, {'train/accuracy': 0.7130854725837708, 'train/loss': 1.4925562143325806, 'train/bleu': 36.811206768579, 'validation/accuracy': 0.6894050240516663, 'validation/loss': 1.6044464111328125, 'validation/bleu': 30.361469755951063, 'validation/num_examples': 3000, 'test/accuracy': 0.7086085081100464, 'test/loss': 1.5032068490982056, 'test/bleu': 30.37053236210157, 'test/num_examples': 3003, 'score': 35306.41939783096, 'total_duration': 59750.995429039, 'accumulated_submission_time': 35306.41939783096, 'accumulated_eval_time': 24437.65533065796, 'accumulated_logging_time': 0.9689481258392334, 'global_step': 102267, 'preemption_count': 0}), (104702, {'train/accuracy': 0.7111444473266602, 'train/loss': 1.5039526224136353, 'train/bleu': 36.730065726943835, 'validation/accuracy': 0.6906904578208923, 'validation/loss': 1.6080873012542725, 'validation/bleu': 30.178659966114846, 'validation/num_examples': 3000, 'test/accuracy': 0.7075194120407104, 'test/loss': 1.5062726736068726, 'test/bleu': 30.42645662688949, 'test/num_examples': 3003, 'score': 36146.3758354187, 'total_duration': 61099.86361145973, 'accumulated_submission_time': 36146.3758354187, 'accumulated_eval_time': 24946.39716911316, 'accumulated_logging_time': 0.9978053569793701, 'global_step': 104702, 'preemption_count': 0}), (107137, {'train/accuracy': 0.7217720150947571, 'train/loss': 1.4472801685333252, 'train/bleu': 36.94325686690794, 'validation/accuracy': 0.690010666847229, 'validation/loss': 1.6060467958450317, 'validation/bleu': 30.11693402292428, 'validation/num_examples': 3000, 'test/accuracy': 0.7074382901191711, 'test/loss': 1.5042269229888916, 'test/bleu': 30.18926699185561, 'test/num_examples': 3003, 'score': 36986.282564878464, 'total_duration': 62435.872681856155, 'accumulated_submission_time': 36986.282564878464, 'accumulated_eval_time': 25442.331533432007, 'accumulated_logging_time': 1.0274174213409424, 'global_step': 107137, 'preemption_count': 0}), (109572, {'train/accuracy': 0.7169502377510071, 'train/loss': 1.467590093612671, 'train/bleu': 37.121234463609994, 'validation/accuracy': 0.6901095509529114, 'validation/loss': 1.6037318706512451, 'validation/bleu': 30.221404611523788, 'validation/num_examples': 3000, 'test/accuracy': 0.7078554034233093, 'test/loss': 1.5015900135040283, 'test/bleu': 30.26127301062836, 'test/num_examples': 3003, 'score': 37826.163227796555, 'total_duration': 63772.08512926102, 'accumulated_submission_time': 37826.163227796555, 'accumulated_eval_time': 25938.49269080162, 'accumulated_logging_time': 1.0587437152862549, 'global_step': 109572, 'preemption_count': 0}), (112006, {'train/accuracy': 0.714897871017456, 'train/loss': 1.4854174852371216, 'train/bleu': 37.17137071138833, 'validation/accuracy': 0.6905791759490967, 'validation/loss': 1.6034214496612549, 'validation/bleu': 30.40829550056841, 'validation/num_examples': 3000, 'test/accuracy': 0.7080175876617432, 'test/loss': 1.5024784803390503, 'test/bleu': 30.290080976555757, 'test/num_examples': 3003, 'score': 38666.04920244217, 'total_duration': 65118.1886343956, 'accumulated_submission_time': 38666.04920244217, 'accumulated_eval_time': 26444.539578437805, 'accumulated_logging_time': 1.0884015560150146, 'global_step': 112006, 'preemption_count': 0}), (114443, {'train/accuracy': 0.7187286615371704, 'train/loss': 1.4650784730911255, 'train/bleu': 37.19136532165709, 'validation/accuracy': 0.6907646059989929, 'validation/loss': 1.6020084619522095, 'validation/bleu': 30.212214261023306, 'validation/num_examples': 3000, 'test/accuracy': 0.7082840800285339, 'test/loss': 1.5008214712142944, 'test/bleu': 30.303217273861026, 'test/num_examples': 3003, 'score': 39506.177600860596, 'total_duration': 66462.17561984062, 'accumulated_submission_time': 39506.177600860596, 'accumulated_eval_time': 26948.23420405388, 'accumulated_logging_time': 1.1175146102905273, 'global_step': 114443, 'preemption_count': 0}), (116881, {'train/accuracy': 0.7183876633644104, 'train/loss': 1.4690229892730713, 'train/bleu': 37.57023253999794, 'validation/accuracy': 0.690678060054779, 'validation/loss': 1.6023274660110474, 'validation/bleu': 30.44762766319476, 'validation/num_examples': 3000, 'test/accuracy': 0.7086548805236816, 'test/loss': 1.5000958442687988, 'test/bleu': 30.450413474283124, 'test/num_examples': 3003, 'score': 40346.2559568882, 'total_duration': 67799.83539175987, 'accumulated_submission_time': 40346.2559568882, 'accumulated_eval_time': 27445.647748947144, 'accumulated_logging_time': 1.1465184688568115, 'global_step': 116881, 'preemption_count': 0}), (119319, {'train/accuracy': 0.7192891836166382, 'train/loss': 1.4630018472671509, 'train/bleu': 37.37378842006101, 'validation/accuracy': 0.6908387541770935, 'validation/loss': 1.6021571159362793, 'validation/bleu': 30.358176824890734, 'validation/num_examples': 3000, 'test/accuracy': 0.7085737586021423, 'test/loss': 1.4997133016586304, 'test/bleu': 30.289649958570816, 'test/num_examples': 3003, 'score': 41186.22371196747, 'total_duration': 69142.12714934349, 'accumulated_submission_time': 41186.22371196747, 'accumulated_eval_time': 27947.80655837059, 'accumulated_logging_time': 1.1762158870697021, 'global_step': 119319, 'preemption_count': 0}), (121754, {'train/accuracy': 0.7201446890830994, 'train/loss': 1.4637565612792969, 'train/bleu': 37.29770397519877, 'validation/accuracy': 0.6909253001213074, 'validation/loss': 1.6021336317062378, 'validation/bleu': 30.360267221076306, 'validation/num_examples': 3000, 'test/accuracy': 0.7085621356964111, 'test/loss': 1.499734878540039, 'test/bleu': 30.29667962556078, 'test/num_examples': 3003, 'score': 42026.07177782059, 'total_duration': 70485.60050177574, 'accumulated_submission_time': 42026.07177782059, 'accumulated_eval_time': 28451.265877962112, 'accumulated_logging_time': 1.2070021629333496, 'global_step': 121754, 'preemption_count': 0}), (124190, {'train/accuracy': 0.7220081090927124, 'train/loss': 1.4429612159729004, 'train/bleu': 36.945465416202964, 'validation/accuracy': 0.6909253001213074, 'validation/loss': 1.6021336317062378, 'validation/bleu': 30.360267221076306, 'validation/num_examples': 3000, 'test/accuracy': 0.7085621356964111, 'test/loss': 1.499734878540039, 'test/bleu': 30.29667962556078, 'test/num_examples': 3003, 'score': 42866.10156774521, 'total_duration': 71824.03085231781, 'accumulated_submission_time': 42866.10156774521, 'accumulated_eval_time': 28949.49695777893, 'accumulated_logging_time': 1.236433744430542, 'global_step': 124190, 'preemption_count': 0})], 'global_step': 126622}
I0306 15:24:02.921098 139902437545152 submission_runner.py:649] Timing: 43705.92584204674
I0306 15:24:02.921135 139902437545152 submission_runner.py:651] Total number of evals: 52
I0306 15:24:02.921164 139902437545152 submission_runner.py:652] ====================
I0306 15:24:02.921293 139902437545152 submission_runner.py:750] Final wmt score: 1
