python submission_runner.py --framework=jax --workload=wmt --submission_path=prize_qualification_baselines/external_tuning/jax_nadamw_full_budget.py --data_dir=/data/wmt --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=submissions/rolling_leaderboard/external_tuning/baseline/study_1 --overwrite=True --save_checkpoints=False --rng_seed=259856216 --tuning_ruleset=external --tuning_search_space=prize_qualification_baselines/external_tuning/tuning_search_space.json --num_tuning_trials=5 --hparam_start_index=4 --hparam_end_index=5 2>&1 | tee -a /logs/wmt_jax_03-05-2025-19-12-01.log
2025-03-05 19:12:02.408559: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1741201922.430152       9 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1741201922.436852       9 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
I0305 19:12:09.154774 139923228669120 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_1/wmt_jax.
I0305 19:12:10.032595 139923228669120 xla_bridge.py:884] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA
I0305 19:12:10.035954 139923228669120 xla_bridge.py:884] Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory
I0305 19:12:10.037690 139923228669120 submission_runner.py:606] Using RNG seed 259856216
I0305 19:12:10.604743 139923228669120 submission_runner.py:615] --- Tuning run 5/5 ---
I0305 19:12:10.604946 139923228669120 submission_runner.py:620] Creating tuning directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_1/wmt_jax/trial_5.
I0305 19:12:10.605135 139923228669120 logger_utils.py:97] Saving hparams to /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_1/wmt_jax/trial_5/hparams.json.
I0305 19:12:10.835948 139923228669120 submission_runner.py:218] Initializing dataset.
I0305 19:12:11.028443 139923228669120 dataset_info.py:690] Load dataset info from /data/wmt/wmt17_translate/de-en/1.0.0
I0305 19:12:11.058604 139923228669120 reader.py:261] Creating a tf.data.Dataset reading 16 files located in folders: /data/wmt/wmt17_translate/de-en/1.0.0.
I0305 19:12:11.132751 139923228669120 logging_logger.py:49] Constructing tf.data.Dataset wmt17_translate for split train, from /data/wmt/wmt17_translate/de-en/1.0.0
I0305 19:12:12.520299 139923228669120 submission_runner.py:229] Initializing model.
I0305 19:12:53.197628 139923228669120 submission_runner.py:272] Initializing optimizer.
I0305 19:12:54.040998 139923228669120 submission_runner.py:279] Initializing metrics bundle.
I0305 19:12:54.041238 139923228669120 submission_runner.py:301] Initializing checkpoint and logger.
I0305 19:12:54.042195 139923228669120 checkpoints.py:1101] Found no checkpoint files in /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_1/wmt_jax/trial_5 with prefix checkpoint_
I0305 19:12:54.042290 139923228669120 submission_runner.py:321] Saving meta data to /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_1/wmt_jax/trial_5/meta_data_0.json.
I0305 19:12:54.042474 139923228669120 logger_utils.py:262] Unable to record workload.train_mean information. Continuing without it.
I0305 19:12:54.042522 139923228669120 logger_utils.py:262] Unable to record workload.train_stddev information. Continuing without it.
I0305 19:12:54.228755 139923228669120 submission_runner.py:325] Saving flags to /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_1/wmt_jax/trial_5/flags_0.json.
I0305 19:12:54.261506 139923228669120 submission_runner.py:337] Starting training loop.
I0305 19:13:30.739471 139787058984704 logging_writer.py:48] [0] global_step=0, grad_norm=5.337521076202393, loss=11.179624557495117
I0305 19:13:30.794666 139923228669120 spec.py:321] Evaluating on the training split.
I0305 19:13:30.796676 139923228669120 dataset_info.py:690] Load dataset info from /data/wmt/wmt17_translate/de-en/1.0.0
I0305 19:13:30.800191 139923228669120 reader.py:261] Creating a tf.data.Dataset reading 16 files located in folders: /data/wmt/wmt17_translate/de-en/1.0.0.
I0305 19:13:30.831600 139923228669120 logging_logger.py:49] Constructing tf.data.Dataset wmt17_translate for split train, from /data/wmt/wmt17_translate/de-en/1.0.0
I0305 19:13:36.894569 139923228669120 workload.py:181] Translating evaluation dataset.
I0305 19:18:41.991486 139923228669120 spec.py:333] Evaluating on the validation split.
I0305 19:18:42.055813 139923228669120 dataset_info.py:690] Load dataset info from /data/wmt/wmt14_translate/de-en/1.0.0
I0305 19:18:42.069016 139923228669120 reader.py:261] Creating a tf.data.Dataset reading 1 files located in folders: /data/wmt/wmt14_translate/de-en/1.0.0.
I0305 19:18:42.100711 139923228669120 logging_logger.py:49] Constructing tf.data.Dataset wmt14_translate for split validation, from /data/wmt/wmt14_translate/de-en/1.0.0
I0305 19:18:47.219420 139923228669120 workload.py:181] Translating evaluation dataset.
I0305 19:23:44.508717 139923228669120 spec.py:349] Evaluating on the test split.
I0305 19:23:44.510902 139923228669120 dataset_info.py:690] Load dataset info from /data/wmt/wmt14_translate/de-en/1.0.0
I0305 19:23:44.514167 139923228669120 reader.py:261] Creating a tf.data.Dataset reading 1 files located in folders: /data/wmt/wmt14_translate/de-en/1.0.0.
I0305 19:23:44.545692 139923228669120 logging_logger.py:49] Constructing tf.data.Dataset wmt14_translate for split test, from /data/wmt/wmt14_translate/de-en/1.0.0
I0305 19:23:47.360622 139923228669120 workload.py:181] Translating evaluation dataset.
I0305 19:28:44.645444 139923228669120 submission_runner.py:469] Time since start: 950.38s, 	Step: 1, 	{'train/accuracy': 0.0005498344544321299, 'train/loss': 11.211599349975586, 'train/bleu': 0.0, 'validation/accuracy': 0.00048204089398495853, 'validation/loss': 11.20705795288086, 'validation/bleu': 0.0, 'validation/num_examples': 3000, 'test/accuracy': 0.0007183408597484231, 'test/loss': 11.203991889953613, 'test/bleu': 0.0, 'test/num_examples': 3003, 'score': 36.53304934501648, 'total_duration': 950.3838818073273, 'accumulated_submission_time': 36.53304934501648, 'accumulated_eval_time': 913.8507368564606, 'accumulated_logging_time': 0}
I0305 19:28:44.652016 139780440385280 logging_writer.py:48] [1] accumulated_eval_time=913.851, accumulated_logging_time=0, accumulated_submission_time=36.533, global_step=1, preemption_count=0, score=36.533, test/accuracy=0.000718341, test/bleu=0, test/loss=11.204, test/num_examples=3003, total_duration=950.384, train/accuracy=0.000549834, train/bleu=0, train/loss=11.2116, validation/accuracy=0.000482041, validation/bleu=0, validation/loss=11.2071, validation/num_examples=3000
I0305 19:29:19.753216 139780431992576 logging_writer.py:48] [100] global_step=100, grad_norm=0.16106319427490234, loss=8.219757080078125
I0305 19:29:54.762723 139780440385280 logging_writer.py:48] [200] global_step=200, grad_norm=0.628487765789032, loss=7.4336161613464355
I0305 19:30:29.838463 139780431992576 logging_writer.py:48] [300] global_step=300, grad_norm=0.48435893654823303, loss=6.761979579925537
I0305 19:31:04.924074 139780440385280 logging_writer.py:48] [400] global_step=400, grad_norm=0.5627576112747192, loss=6.251725196838379
I0305 19:31:39.988617 139780431992576 logging_writer.py:48] [500] global_step=500, grad_norm=0.4311649799346924, loss=5.806906223297119
I0305 19:32:15.035421 139780440385280 logging_writer.py:48] [600] global_step=600, grad_norm=0.46337366104125977, loss=5.527868747711182
I0305 19:32:50.124806 139780431992576 logging_writer.py:48] [700] global_step=700, grad_norm=0.5120657682418823, loss=5.234981060028076
I0305 19:33:25.199299 139780440385280 logging_writer.py:48] [800] global_step=800, grad_norm=0.5487261414527893, loss=4.955779552459717
I0305 19:34:00.265708 139780431992576 logging_writer.py:48] [900] global_step=900, grad_norm=0.5660536289215088, loss=4.687922954559326
I0305 19:34:35.340464 139780440385280 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.5917717218399048, loss=4.462490081787109
I0305 19:35:10.394966 139780431992576 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.5512263774871826, loss=4.19917631149292
I0305 19:35:45.471811 139780440385280 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.4788350462913513, loss=4.014621257781982
I0305 19:36:20.534933 139780431992576 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.5079762935638428, loss=3.8483946323394775
I0305 19:36:55.615386 139780440385280 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.4673517048358917, loss=3.679866313934326
I0305 19:37:30.681387 139780431992576 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.4820534884929657, loss=3.6229569911956787
I0305 19:38:05.737069 139780440385280 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.44851645827293396, loss=3.4765007495880127
I0305 19:38:40.797607 139780431992576 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.604972779750824, loss=3.3074567317962646
I0305 19:39:15.864372 139780440385280 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.5108103156089783, loss=3.2716498374938965
I0305 19:39:50.936433 139780431992576 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.40610912442207336, loss=3.1551225185394287
I0305 19:40:25.986464 139780440385280 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.40607950091362, loss=3.169785976409912
I0305 19:41:01.085834 139780431992576 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.39902353286743164, loss=3.121068239212036
I0305 19:41:36.148339 139780440385280 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.3689393699169159, loss=3.0531256198883057
I0305 19:42:11.184596 139780431992576 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.32515379786491394, loss=2.9422922134399414
I0305 19:42:44.841440 139923228669120 spec.py:321] Evaluating on the training split.
I0305 19:42:47.465354 139923228669120 workload.py:181] Translating evaluation dataset.
I0305 19:45:55.856749 139923228669120 spec.py:333] Evaluating on the validation split.
I0305 19:45:58.495529 139923228669120 workload.py:181] Translating evaluation dataset.
I0305 19:48:56.573173 139923228669120 spec.py:349] Evaluating on the test split.
I0305 19:48:59.198087 139923228669120 workload.py:181] Translating evaluation dataset.
I0305 19:51:46.547339 139923228669120 submission_runner.py:469] Time since start: 2332.29s, 	Step: 2397, 	{'train/accuracy': 0.5150410532951355, 'train/loss': 2.817685604095459, 'train/bleu': 22.989598778734493, 'validation/accuracy': 0.5129780173301697, 'validation/loss': 2.813084125518799, 'validation/bleu': 18.699414743449253, 'validation/num_examples': 3000, 'test/accuracy': 0.514424741268158, 'test/loss': 2.838932514190674, 'test/bleu': 17.118553953908677, 'test/num_examples': 3003, 'score': 876.5725305080414, 'total_duration': 2332.285767555237, 'accumulated_submission_time': 876.5725305080414, 'accumulated_eval_time': 1455.5565826892853, 'accumulated_logging_time': 0.01490926742553711}
I0305 19:51:46.555969 139780440385280 logging_writer.py:48] [2397] accumulated_eval_time=1455.56, accumulated_logging_time=0.0149093, accumulated_submission_time=876.573, global_step=2397, preemption_count=0, score=876.573, test/accuracy=0.514425, test/bleu=17.1186, test/loss=2.83893, test/num_examples=3003, total_duration=2332.29, train/accuracy=0.515041, train/bleu=22.9896, train/loss=2.81769, validation/accuracy=0.512978, validation/bleu=18.6994, validation/loss=2.81308, validation/num_examples=3000
I0305 19:51:47.974200 139780431992576 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.3255973756313324, loss=3.016465425491333
I0305 19:52:23.035091 139780440385280 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.31734248995780945, loss=2.9108290672302246
I0305 19:52:58.103012 139780431992576 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.2586270570755005, loss=2.832430362701416
I0305 19:53:33.180102 139780440385280 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.310763418674469, loss=2.833211660385132
I0305 19:54:08.225915 139780431992576 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.24473229050636292, loss=2.772770404815674
I0305 19:54:43.291813 139780440385280 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.24362842738628387, loss=2.833651065826416
I0305 19:55:18.385371 139780431992576 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.23675209283828735, loss=2.632547378540039
I0305 19:55:53.474124 139780440385280 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.22884833812713623, loss=2.715912103652954
I0305 19:56:28.566339 139780431992576 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.2652457654476166, loss=2.592116355895996
I0305 19:57:03.625807 139780440385280 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.21010999381542206, loss=2.6370649337768555
I0305 19:57:38.699031 139780431992576 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.21028000116348267, loss=2.6362688541412354
I0305 19:58:13.780045 139780440385280 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.2121984213590622, loss=2.5135834217071533
I0305 19:58:48.869881 139780431992576 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.20702190697193146, loss=2.634059429168701
I0305 19:59:23.942294 139780440385280 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.20272694528102875, loss=2.4493136405944824
I0305 19:59:59.059577 139780431992576 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.22383424639701843, loss=2.451223134994507
I0305 20:00:34.143323 139780440385280 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.1844644844532013, loss=2.4374396800994873
I0305 20:01:09.232649 139780431992576 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.1980210244655609, loss=2.5065438747406006
I0305 20:01:44.327644 139780440385280 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.2048131674528122, loss=2.3419291973114014
I0305 20:02:19.375041 139780431992576 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.17353592813014984, loss=2.4129600524902344
I0305 20:02:54.433969 139780440385280 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.17968375980854034, loss=2.444106101989746
I0305 20:03:29.504531 139780431992576 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.1727287471294403, loss=2.277306079864502
I0305 20:04:04.611758 139780440385280 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.15478335320949554, loss=2.347100257873535
I0305 20:04:39.685353 139780431992576 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.15866626799106598, loss=2.3773934841156006
I0305 20:05:14.745006 139780440385280 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.15562015771865845, loss=2.416928291320801
I0305 20:05:46.643133 139923228669120 spec.py:321] Evaluating on the training split.
I0305 20:05:49.274100 139923228669120 workload.py:181] Translating evaluation dataset.
I0305 20:08:22.745805 139923228669120 spec.py:333] Evaluating on the validation split.
I0305 20:08:25.384837 139923228669120 workload.py:181] Translating evaluation dataset.
I0305 20:10:57.045063 139923228669120 spec.py:349] Evaluating on the test split.
I0305 20:10:59.672010 139923228669120 workload.py:181] Translating evaluation dataset.
I0305 20:13:17.169312 139923228669120 submission_runner.py:469] Time since start: 3622.91s, 	Step: 4792, 	{'train/accuracy': 0.5813414454460144, 'train/loss': 2.223891258239746, 'train/bleu': 27.487418173439774, 'validation/accuracy': 0.5921316146850586, 'validation/loss': 2.123965263366699, 'validation/bleu': 23.79964857018194, 'validation/num_examples': 3000, 'test/accuracy': 0.5976943373680115, 'test/loss': 2.0871341228485107, 'test/bleu': 22.45259197259991, 'test/num_examples': 3003, 'score': 1716.5127515792847, 'total_duration': 3622.9077451229095, 'accumulated_submission_time': 1716.5127515792847, 'accumulated_eval_time': 1906.082717180252, 'accumulated_logging_time': 0.03282356262207031}
I0305 20:13:17.177884 139780431992576 logging_writer.py:48] [4792] accumulated_eval_time=1906.08, accumulated_logging_time=0.0328236, accumulated_submission_time=1716.51, global_step=4792, preemption_count=0, score=1716.51, test/accuracy=0.597694, test/bleu=22.4526, test/loss=2.08713, test/num_examples=3003, total_duration=3622.91, train/accuracy=0.581341, train/bleu=27.4874, train/loss=2.22389, validation/accuracy=0.592132, validation/bleu=23.7996, validation/loss=2.12397, validation/num_examples=3000
I0305 20:13:20.344065 139780440385280 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.17224541306495667, loss=2.2538516521453857
I0305 20:13:55.434371 139780431992576 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.1819273680448532, loss=2.314687490463257
I0305 20:14:30.474182 139780440385280 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.2169579714536667, loss=2.3349087238311768
I0305 20:15:05.550408 139780431992576 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.17683492600917816, loss=2.1861207485198975
I0305 20:15:40.644092 139780440385280 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.15374873578548431, loss=2.162659168243408
I0305 20:16:15.713196 139780431992576 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.15533050894737244, loss=2.280993938446045
I0305 20:16:50.804057 139780440385280 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.18444637954235077, loss=2.23034405708313
I0305 20:17:25.913737 139780431992576 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.15742714703083038, loss=2.231142997741699
I0305 20:18:00.984548 139780440385280 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.15339291095733643, loss=2.2338781356811523
I0305 20:18:36.089842 139780431992576 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.16239146888256073, loss=2.247325897216797
I0305 20:19:11.170035 139780440385280 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.14513468742370605, loss=2.1453654766082764
I0305 20:19:46.242358 139780431992576 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.14424704015254974, loss=2.1989095211029053
I0305 20:20:21.312963 139780440385280 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.14427389204502106, loss=2.207267999649048
I0305 20:20:56.410389 139780431992576 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.13921602070331573, loss=2.200873374938965
I0305 20:21:31.489678 139780440385280 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.1424553394317627, loss=2.12475323677063
I0305 20:22:06.559180 139780431992576 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.1522057056427002, loss=2.2116596698760986
I0305 20:22:41.683763 139780440385280 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.1470801830291748, loss=2.1721510887145996
I0305 20:23:16.769217 139780431992576 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.17244431376457214, loss=2.167070150375366
I0305 20:23:51.834543 139780440385280 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.25861334800720215, loss=2.1330018043518066
I0305 20:24:26.903151 139780431992576 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.24689099192619324, loss=2.2393786907196045
I0305 20:25:01.949518 139780440385280 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.1727306991815567, loss=2.1044366359710693
I0305 20:25:37.020916 139780431992576 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.14632175862789154, loss=2.105682373046875
I0305 20:26:12.080594 139780440385280 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.1574963927268982, loss=2.1406726837158203
I0305 20:26:47.120009 139780431992576 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.15666352212429047, loss=2.110417127609253
I0305 20:27:17.289157 139923228669120 spec.py:321] Evaluating on the training split.
I0305 20:27:19.920038 139923228669120 workload.py:181] Translating evaluation dataset.
I0305 20:30:09.128054 139923228669120 spec.py:333] Evaluating on the validation split.
I0305 20:30:11.759964 139923228669120 workload.py:181] Translating evaluation dataset.
I0305 20:32:46.158444 139923228669120 spec.py:349] Evaluating on the test split.
I0305 20:32:48.794127 139923228669120 workload.py:181] Translating evaluation dataset.
I0305 20:35:17.146611 139923228669120 submission_runner.py:469] Time since start: 4942.89s, 	Step: 7187, 	{'train/accuracy': 0.6096985340118408, 'train/loss': 1.977616310119629, 'train/bleu': 29.002300988505382, 'validation/accuracy': 0.6188045740127563, 'validation/loss': 1.9038896560668945, 'validation/bleu': 25.322380848005757, 'validation/num_examples': 3000, 'test/accuracy': 0.6245858073234558, 'test/loss': 1.8640550374984741, 'test/bleu': 24.10552422920636, 'test/num_examples': 3003, 'score': 2556.4744362831116, 'total_duration': 4942.885024309158, 'accumulated_submission_time': 2556.4744362831116, 'accumulated_eval_time': 2385.940097808838, 'accumulated_logging_time': 0.050182342529296875}
I0305 20:35:17.157476 139780440385280 logging_writer.py:48] [7187] accumulated_eval_time=2385.94, accumulated_logging_time=0.0501823, accumulated_submission_time=2556.47, global_step=7187, preemption_count=0, score=2556.47, test/accuracy=0.624586, test/bleu=24.1055, test/loss=1.86406, test/num_examples=3003, total_duration=4942.89, train/accuracy=0.609699, train/bleu=29.0023, train/loss=1.97762, validation/accuracy=0.618805, validation/bleu=25.3224, validation/loss=1.90389, validation/num_examples=3000
I0305 20:35:22.085893 139780431992576 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.1583445817232132, loss=2.0387489795684814
I0305 20:35:57.168940 139780440385280 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.14715637266635895, loss=2.0961709022521973
I0305 20:36:32.199058 139780431992576 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.1487254649400711, loss=2.0721418857574463
I0305 20:37:07.272063 139780440385280 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.17003683745861053, loss=2.1072959899902344
I0305 20:37:42.315596 139780431992576 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.15561817586421967, loss=2.1337201595306396
I0305 20:38:17.394466 139780440385280 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.24029624462127686, loss=2.148620367050171
I0305 20:38:52.524379 139780431992576 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.1635618358850479, loss=2.059553623199463
I0305 20:39:27.593324 139780440385280 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.18937619030475616, loss=2.1652722358703613
I0305 20:40:02.668562 139780431992576 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.16947144269943237, loss=2.157402515411377
I0305 20:40:37.758154 139780440385280 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.20962916314601898, loss=2.193399667739868
I0305 20:41:12.806559 139780431992576 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.156585231423378, loss=2.125715494155884
I0305 20:41:47.874579 139780440385280 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.22286486625671387, loss=1.9789490699768066
I0305 20:42:22.958646 139780431992576 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.151962012052536, loss=2.0608410835266113
I0305 20:42:58.020658 139780440385280 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.26229527592658997, loss=2.0493898391723633
I0305 20:43:33.052996 139780431992576 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.16007143259048462, loss=2.0547332763671875
I0305 20:44:08.123489 139780440385280 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.14269454777240753, loss=1.997115969657898
I0305 20:44:43.184848 139780431992576 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.1668730527162552, loss=1.9884206056594849
I0305 20:45:18.220423 139780440385280 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.15301714837551117, loss=2.000741720199585
I0305 20:45:53.268247 139780431992576 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.15215067565441132, loss=2.1036252975463867
I0305 20:46:28.318449 139780440385280 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.17713353037834167, loss=2.0547895431518555
I0305 20:47:03.392601 139780431992576 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.16966453194618225, loss=2.092763662338257
I0305 20:47:38.453325 139780440385280 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.20678745210170746, loss=2.112936496734619
I0305 20:48:13.512917 139780431992576 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.18318703770637512, loss=2.0557918548583984
I0305 20:48:48.559139 139780440385280 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.2067146897315979, loss=1.9940491914749146
I0305 20:49:17.316047 139923228669120 spec.py:321] Evaluating on the training split.
I0305 20:49:19.948203 139923228669120 workload.py:181] Translating evaluation dataset.
I0305 20:52:05.595721 139923228669120 spec.py:333] Evaluating on the validation split.
I0305 20:52:08.229093 139923228669120 workload.py:181] Translating evaluation dataset.
I0305 20:54:35.738633 139923228669120 spec.py:349] Evaluating on the test split.
I0305 20:54:38.360206 139923228669120 workload.py:181] Translating evaluation dataset.
I0305 20:56:57.693581 139923228669120 submission_runner.py:469] Time since start: 6243.43s, 	Step: 9583, 	{'train/accuracy': 0.6120362877845764, 'train/loss': 1.9493693113327026, 'train/bleu': 30.09364765457319, 'validation/accuracy': 0.6324129104614258, 'validation/loss': 1.801072597503662, 'validation/bleu': 26.110352396083083, 'validation/num_examples': 3000, 'test/accuracy': 0.6381415724754333, 'test/loss': 1.7433513402938843, 'test/bleu': 25.507556888659064, 'test/num_examples': 3003, 'score': 3396.4902799129486, 'total_duration': 6243.432007312775, 'accumulated_submission_time': 3396.4902799129486, 'accumulated_eval_time': 2846.317569732666, 'accumulated_logging_time': 0.06990194320678711}
I0305 20:56:57.703265 139780431992576 logging_writer.py:48] [9583] accumulated_eval_time=2846.32, accumulated_logging_time=0.0699019, accumulated_submission_time=3396.49, global_step=9583, preemption_count=0, score=3396.49, test/accuracy=0.638142, test/bleu=25.5076, test/loss=1.74335, test/num_examples=3003, total_duration=6243.43, train/accuracy=0.612036, train/bleu=30.0936, train/loss=1.94937, validation/accuracy=0.632413, validation/bleu=26.1104, validation/loss=1.80107, validation/num_examples=3000
I0305 20:57:04.031984 139780440385280 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.1729334443807602, loss=2.057537794113159
I0305 20:57:39.117314 139780431992576 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.21921303868293762, loss=1.9422616958618164
I0305 20:58:14.163635 139780440385280 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.26855039596557617, loss=1.9913889169692993
I0305 20:58:49.246440 139780431992576 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.16985616087913513, loss=1.964410662651062
I0305 20:59:24.306386 139780440385280 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.15961076319217682, loss=2.0020878314971924
I0305 20:59:59.382437 139780431992576 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.1419205516576767, loss=2.066429376602173
I0305 21:00:34.441313 139780440385280 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.21806557476520538, loss=1.9585378170013428
I0305 21:01:09.531096 139780431992576 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.18808473646640778, loss=1.9660485982894897
I0305 21:01:44.589357 139780440385280 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.2304835170507431, loss=2.0317299365997314
I0305 21:02:19.654661 139780431992576 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.16053026914596558, loss=2.0307185649871826
I0305 21:02:54.693218 139780440385280 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.15755698084831238, loss=2.0213890075683594
I0305 21:03:29.726818 139780431992576 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.32384032011032104, loss=1.9761579036712646
I0305 21:04:04.777049 139780440385280 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.1649899035692215, loss=2.068944215774536
I0305 21:04:39.809819 139780431992576 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.1535152941942215, loss=1.9797948598861694
I0305 21:05:14.903826 139780440385280 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.16046777367591858, loss=2.01385760307312
I0305 21:05:49.960453 139780431992576 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.16519644856452942, loss=1.9584095478057861
I0305 21:06:24.987592 139780440385280 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.21422910690307617, loss=1.977199673652649
I0305 21:07:00.037891 139780431992576 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.1552124172449112, loss=1.9643081426620483
I0305 21:07:35.060942 139780440385280 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.16076326370239258, loss=1.9434984922409058
I0305 21:08:10.116463 139780431992576 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.15858550369739532, loss=1.8519014120101929
I0305 21:08:45.178405 139780440385280 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.16269609332084656, loss=1.926746129989624
I0305 21:09:20.196599 139780431992576 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.23036952316761017, loss=1.9109165668487549
I0305 21:09:55.236293 139780440385280 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.17730103433132172, loss=1.954175353050232
I0305 21:10:30.263510 139780431992576 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.1676800549030304, loss=1.9992640018463135
I0305 21:10:57.963333 139923228669120 spec.py:321] Evaluating on the training split.
I0305 21:11:00.595659 139923228669120 workload.py:181] Translating evaluation dataset.
I0305 21:13:55.990508 139923228669120 spec.py:333] Evaluating on the validation split.
I0305 21:13:58.626041 139923228669120 workload.py:181] Translating evaluation dataset.
I0305 21:16:21.360244 139923228669120 spec.py:349] Evaluating on the test split.
I0305 21:16:23.983916 139923228669120 workload.py:181] Translating evaluation dataset.
I0305 21:18:34.564957 139923228669120 submission_runner.py:469] Time since start: 7540.30s, 	Step: 11980, 	{'train/accuracy': 0.6228347420692444, 'train/loss': 1.8648462295532227, 'train/bleu': 30.036355668612682, 'validation/accuracy': 0.6401503086090088, 'validation/loss': 1.7346768379211426, 'validation/bleu': 27.031084599709246, 'validation/num_examples': 3000, 'test/accuracy': 0.6497740745544434, 'test/loss': 1.676055669784546, 'test/bleu': 25.676899373263236, 'test/num_examples': 3003, 'score': 4236.61579990387, 'total_duration': 7540.3033809661865, 'accumulated_submission_time': 4236.61579990387, 'accumulated_eval_time': 3302.919132709503, 'accumulated_logging_time': 0.08744144439697266}
I0305 21:18:34.573998 139780440385280 logging_writer.py:48] [11980] accumulated_eval_time=3302.92, accumulated_logging_time=0.0874414, accumulated_submission_time=4236.62, global_step=11980, preemption_count=0, score=4236.62, test/accuracy=0.649774, test/bleu=25.6769, test/loss=1.67606, test/num_examples=3003, total_duration=7540.3, train/accuracy=0.622835, train/bleu=30.0364, train/loss=1.86485, validation/accuracy=0.64015, validation/bleu=27.0311, validation/loss=1.73468, validation/num_examples=3000
I0305 21:18:41.951142 139780431992576 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.17302539944648743, loss=1.9938914775848389
I0305 21:19:17.091145 139780440385280 logging_writer.py:48] [12100] global_step=12100, grad_norm=0.19663311541080475, loss=1.9572755098342896
I0305 21:19:52.108246 139780431992576 logging_writer.py:48] [12200] global_step=12200, grad_norm=0.1715526282787323, loss=1.9783791303634644
I0305 21:20:27.138685 139780440385280 logging_writer.py:48] [12300] global_step=12300, grad_norm=0.2291196584701538, loss=1.9037363529205322
I0305 21:21:02.199509 139780431992576 logging_writer.py:48] [12400] global_step=12400, grad_norm=0.18795035779476166, loss=1.9228821992874146
I0305 21:21:37.260680 139780440385280 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.1609613448381424, loss=1.9681811332702637
I0305 21:22:12.315543 139780339787520 logging_writer.py:48] [12600] global_step=12600, grad_norm=0.16759498417377472, loss=1.953218936920166
I0305 21:22:47.332159 139780331394816 logging_writer.py:48] [12700] global_step=12700, grad_norm=0.16790936887264252, loss=1.9537895917892456
I0305 21:23:22.360125 139780339787520 logging_writer.py:48] [12800] global_step=12800, grad_norm=0.18365478515625, loss=2.0180251598358154
I0305 21:23:57.385833 139780331394816 logging_writer.py:48] [12900] global_step=12900, grad_norm=0.3075979948043823, loss=1.9538259506225586
I0305 21:24:32.443789 139780339787520 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.1733035445213318, loss=2.0697009563446045
I0305 21:25:07.481898 139780331394816 logging_writer.py:48] [13100] global_step=13100, grad_norm=0.2127077877521515, loss=1.8778518438339233
I0305 21:25:42.510777 139780339787520 logging_writer.py:48] [13200] global_step=13200, grad_norm=0.16909624636173248, loss=1.9276914596557617
I0305 21:26:17.540614 139780331394816 logging_writer.py:48] [13300] global_step=13300, grad_norm=0.25465327501296997, loss=1.9026343822479248
I0305 21:26:52.623744 139780339787520 logging_writer.py:48] [13400] global_step=13400, grad_norm=0.17609675228595734, loss=1.9973922967910767
I0305 21:27:27.702063 139780331394816 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.20203235745429993, loss=1.9918713569641113
I0305 21:28:02.754894 139780339787520 logging_writer.py:48] [13600] global_step=13600, grad_norm=0.17226995527744293, loss=1.9712361097335815
I0305 21:28:37.819846 139780331394816 logging_writer.py:48] [13700] global_step=13700, grad_norm=0.1768587827682495, loss=1.902514934539795
I0305 21:29:12.884430 139780339787520 logging_writer.py:48] [13800] global_step=13800, grad_norm=0.17512784898281097, loss=1.937445044517517
I0305 21:29:47.967518 139780331394816 logging_writer.py:48] [13900] global_step=13900, grad_norm=0.1945280283689499, loss=1.9042024612426758
I0305 21:30:23.046878 139780339787520 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.2307642698287964, loss=2.016188383102417
I0305 21:30:58.075611 139780331394816 logging_writer.py:48] [14100] global_step=14100, grad_norm=0.19963964819908142, loss=1.9583479166030884
I0305 21:31:33.117262 139780339787520 logging_writer.py:48] [14200] global_step=14200, grad_norm=0.17260171473026276, loss=1.8808228969573975
I0305 21:32:08.156142 139780331394816 logging_writer.py:48] [14300] global_step=14300, grad_norm=0.18409894406795502, loss=1.9244073629379272
I0305 21:32:34.783308 139923228669120 spec.py:321] Evaluating on the training split.
I0305 21:32:37.400489 139923228669120 workload.py:181] Translating evaluation dataset.
I0305 21:35:36.644781 139923228669120 spec.py:333] Evaluating on the validation split.
I0305 21:35:39.264345 139923228669120 workload.py:181] Translating evaluation dataset.
I0305 21:38:03.082247 139923228669120 spec.py:349] Evaluating on the test split.
I0305 21:38:05.712595 139923228669120 workload.py:181] Translating evaluation dataset.
I0305 21:40:18.951596 139923228669120 submission_runner.py:469] Time since start: 8844.69s, 	Step: 14377, 	{'train/accuracy': 0.6273400783538818, 'train/loss': 1.8148566484451294, 'train/bleu': 30.551528244628237, 'validation/accuracy': 0.6456381678581238, 'validation/loss': 1.6968975067138672, 'validation/bleu': 27.17611908378438, 'validation/num_examples': 3000, 'test/accuracy': 0.6536438465118408, 'test/loss': 1.638832926750183, 'test/bleu': 26.167060470432695, 'test/num_examples': 3003, 'score': 5076.692269563675, 'total_duration': 8844.690023183823, 'accumulated_submission_time': 5076.692269563675, 'accumulated_eval_time': 3767.087361097336, 'accumulated_logging_time': 0.10435199737548828}
I0305 21:40:18.961499 139780339787520 logging_writer.py:48] [14377] accumulated_eval_time=3767.09, accumulated_logging_time=0.104352, accumulated_submission_time=5076.69, global_step=14377, preemption_count=0, score=5076.69, test/accuracy=0.653644, test/bleu=26.1671, test/loss=1.63883, test/num_examples=3003, total_duration=8844.69, train/accuracy=0.62734, train/bleu=30.5515, train/loss=1.81486, validation/accuracy=0.645638, validation/bleu=27.1761, validation/loss=1.6969, validation/num_examples=3000
I0305 21:40:27.398033 139780331394816 logging_writer.py:48] [14400] global_step=14400, grad_norm=0.17264442145824432, loss=1.915008306503296
I0305 21:41:02.490719 139780339787520 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.17235033214092255, loss=1.9375591278076172
I0305 21:41:37.527409 139780331394816 logging_writer.py:48] [14600] global_step=14600, grad_norm=0.17960640788078308, loss=1.8984180688858032
I0305 21:42:12.547894 139780339787520 logging_writer.py:48] [14700] global_step=14700, grad_norm=0.17999641597270966, loss=1.944119930267334
I0305 21:42:47.600846 139780331394816 logging_writer.py:48] [14800] global_step=14800, grad_norm=0.33746013045310974, loss=1.9693193435668945
I0305 21:43:22.646145 139780339787520 logging_writer.py:48] [14900] global_step=14900, grad_norm=0.20870597660541534, loss=1.8785200119018555
I0305 21:43:57.689930 139780331394816 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.16798977553844452, loss=1.8181476593017578
I0305 21:44:32.757110 139780339787520 logging_writer.py:48] [15100] global_step=15100, grad_norm=0.17445187270641327, loss=1.885991096496582
I0305 21:45:07.821346 139780331394816 logging_writer.py:48] [15200] global_step=15200, grad_norm=0.21270494163036346, loss=1.9299677610397339
I0305 21:45:42.855013 139780339787520 logging_writer.py:48] [15300] global_step=15300, grad_norm=0.19289524853229523, loss=1.9415570497512817
I0305 21:46:17.908660 139780331394816 logging_writer.py:48] [15400] global_step=15400, grad_norm=0.23460496962070465, loss=1.9387786388397217
I0305 21:46:52.949162 139780339787520 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.1867392659187317, loss=2.0080325603485107
I0305 21:47:28.026378 139780331394816 logging_writer.py:48] [15600] global_step=15600, grad_norm=0.18325354158878326, loss=1.8796111345291138
I0305 21:48:03.133217 139780339787520 logging_writer.py:48] [15700] global_step=15700, grad_norm=0.1718350499868393, loss=1.8551305532455444
I0305 21:48:38.213609 139780331394816 logging_writer.py:48] [15800] global_step=15800, grad_norm=0.2016240954399109, loss=1.8759827613830566
I0305 21:49:13.253778 139780339787520 logging_writer.py:48] [15900] global_step=15900, grad_norm=0.25461509823799133, loss=1.8978296518325806
I0305 21:49:48.331920 139780331394816 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.24864302575588226, loss=1.8636748790740967
I0305 21:50:23.369247 139780339787520 logging_writer.py:48] [16100] global_step=16100, grad_norm=0.15787281095981598, loss=1.8941079378128052
I0305 21:50:58.405350 139780331394816 logging_writer.py:48] [16200] global_step=16200, grad_norm=0.17544133961200714, loss=1.8232917785644531
I0305 21:51:33.430204 139780339787520 logging_writer.py:48] [16300] global_step=16300, grad_norm=0.21285247802734375, loss=1.8878768682479858
I0305 21:52:08.460224 139780331394816 logging_writer.py:48] [16400] global_step=16400, grad_norm=0.2286905199289322, loss=1.9074677228927612
I0305 21:52:43.500385 139780339787520 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.21468086540699005, loss=1.846871256828308
I0305 21:53:18.544186 139780331394816 logging_writer.py:48] [16600] global_step=16600, grad_norm=0.3602449893951416, loss=1.8764492273330688
I0305 21:53:53.554949 139780339787520 logging_writer.py:48] [16700] global_step=16700, grad_norm=0.4867420792579651, loss=1.8902904987335205
I0305 21:54:19.137973 139923228669120 spec.py:321] Evaluating on the training split.
I0305 21:54:21.760846 139923228669120 workload.py:181] Translating evaluation dataset.
I0305 21:57:18.509037 139923228669120 spec.py:333] Evaluating on the validation split.
I0305 21:57:21.129296 139923228669120 workload.py:181] Translating evaluation dataset.
I0305 21:59:53.138689 139923228669120 spec.py:349] Evaluating on the test split.
I0305 21:59:55.759384 139923228669120 workload.py:181] Translating evaluation dataset.
I0305 22:02:24.092805 139923228669120 submission_runner.py:469] Time since start: 10169.83s, 	Step: 16774, 	{'train/accuracy': 0.6278868317604065, 'train/loss': 1.8263202905654907, 'train/bleu': 30.488985996820794, 'validation/accuracy': 0.6490247845649719, 'validation/loss': 1.670182466506958, 'validation/bleu': 27.335693433445112, 'validation/num_examples': 3000, 'test/accuracy': 0.6586027145385742, 'test/loss': 1.605661153793335, 'test/bleu': 26.50105871706552, 'test/num_examples': 3003, 'score': 5916.735604524612, 'total_duration': 10169.831251859665, 'accumulated_submission_time': 5916.735604524612, 'accumulated_eval_time': 4252.042149305344, 'accumulated_logging_time': 0.12235307693481445}
I0305 22:02:24.102532 139780331394816 logging_writer.py:48] [16774] accumulated_eval_time=4252.04, accumulated_logging_time=0.122353, accumulated_submission_time=5916.74, global_step=16774, preemption_count=0, score=5916.74, test/accuracy=0.658603, test/bleu=26.5011, test/loss=1.60566, test/num_examples=3003, total_duration=10169.8, train/accuracy=0.627887, train/bleu=30.489, train/loss=1.82632, validation/accuracy=0.649025, validation/bleu=27.3357, validation/loss=1.67018, validation/num_examples=3000
I0305 22:02:33.583790 139780339787520 logging_writer.py:48] [16800] global_step=16800, grad_norm=0.22478151321411133, loss=1.927014946937561
I0305 22:03:08.646960 139780331394816 logging_writer.py:48] [16900] global_step=16900, grad_norm=0.19075502455234528, loss=1.926594853401184
I0305 22:03:43.666174 139780339787520 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.17369215190410614, loss=1.8953510522842407
I0305 22:04:18.681134 139780331394816 logging_writer.py:48] [17100] global_step=17100, grad_norm=0.19369229674339294, loss=1.9656288623809814
I0305 22:04:53.728832 139780339787520 logging_writer.py:48] [17200] global_step=17200, grad_norm=0.23327064514160156, loss=1.8876572847366333
I0305 22:05:28.789967 139780331394816 logging_writer.py:48] [17300] global_step=17300, grad_norm=0.22528651356697083, loss=1.8721011877059937
I0305 22:06:03.844774 139780339787520 logging_writer.py:48] [17400] global_step=17400, grad_norm=0.22171005606651306, loss=1.8864713907241821
I0305 22:06:38.882588 139780331394816 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.19585630297660828, loss=1.799921989440918
I0305 22:07:13.919798 139780339787520 logging_writer.py:48] [17600] global_step=17600, grad_norm=0.19115541875362396, loss=1.9299805164337158
I0305 22:07:48.956955 139780331394816 logging_writer.py:48] [17700] global_step=17700, grad_norm=0.2188863307237625, loss=1.9487515687942505
I0305 22:08:24.000671 139780339787520 logging_writer.py:48] [17800] global_step=17800, grad_norm=0.1735471487045288, loss=1.8151720762252808
I0305 22:08:59.055770 139780331394816 logging_writer.py:48] [17900] global_step=17900, grad_norm=0.17237922549247742, loss=1.8992729187011719
I0305 22:09:34.097454 139780339787520 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.16851167380809784, loss=1.8928645849227905
I0305 22:10:09.136905 139780331394816 logging_writer.py:48] [18100] global_step=18100, grad_norm=0.16779491305351257, loss=1.864368200302124
I0305 22:10:44.171779 139780339787520 logging_writer.py:48] [18200] global_step=18200, grad_norm=0.18490570783615112, loss=1.8392432928085327
I0305 22:11:19.234050 139780331394816 logging_writer.py:48] [18300] global_step=18300, grad_norm=0.19748492538928986, loss=1.754054307937622
I0305 22:11:54.260938 139780339787520 logging_writer.py:48] [18400] global_step=18400, grad_norm=0.25515255331993103, loss=1.8962180614471436
I0305 22:12:29.313629 139780331394816 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.2856042683124542, loss=1.8756829500198364
I0305 22:13:04.345085 139780339787520 logging_writer.py:48] [18600] global_step=18600, grad_norm=0.17830123007297516, loss=1.8935837745666504
I0305 22:13:39.375392 139780331394816 logging_writer.py:48] [18700] global_step=18700, grad_norm=0.19725656509399414, loss=1.875534176826477
I0305 22:14:14.408648 139780339787520 logging_writer.py:48] [18800] global_step=18800, grad_norm=0.17755328118801117, loss=1.887741208076477
I0305 22:14:49.718042 139780331394816 logging_writer.py:48] [18900] global_step=18900, grad_norm=0.18989624083042145, loss=1.8241710662841797
I0305 22:15:24.894457 139780339787520 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.27727290987968445, loss=1.953137755393982
I0305 22:16:00.086405 139780331394816 logging_writer.py:48] [19100] global_step=19100, grad_norm=0.19542503356933594, loss=1.8315469026565552
I0305 22:16:24.382360 139923228669120 spec.py:321] Evaluating on the training split.
I0305 22:16:27.011797 139923228669120 workload.py:181] Translating evaluation dataset.
I0305 22:19:42.682893 139923228669120 spec.py:333] Evaluating on the validation split.
I0305 22:19:45.307187 139923228669120 workload.py:181] Translating evaluation dataset.
I0305 22:22:12.439205 139923228669120 spec.py:349] Evaluating on the test split.
I0305 22:22:15.070656 139923228669120 workload.py:181] Translating evaluation dataset.
I0305 22:24:39.078569 139923228669120 submission_runner.py:469] Time since start: 11504.82s, 	Step: 19170, 	{'train/accuracy': 0.6470143795013428, 'train/loss': 1.6777598857879639, 'train/bleu': 31.80378265917511, 'validation/accuracy': 0.6508664488792419, 'validation/loss': 1.6542633771896362, 'validation/bleu': 27.611061873273393, 'validation/num_examples': 3000, 'test/accuracy': 0.6607229709625244, 'test/loss': 1.583768367767334, 'test/bleu': 26.584595224858393, 'test/num_examples': 3003, 'score': 6756.886587381363, 'total_duration': 11504.817004203796, 'accumulated_submission_time': 6756.886587381363, 'accumulated_eval_time': 4746.738303899765, 'accumulated_logging_time': 0.13991928100585938}
I0305 22:24:39.089339 139780339787520 logging_writer.py:48] [19170] accumulated_eval_time=4746.74, accumulated_logging_time=0.139919, accumulated_submission_time=6756.89, global_step=19170, preemption_count=0, score=6756.89, test/accuracy=0.660723, test/bleu=26.5846, test/loss=1.58377, test/num_examples=3003, total_duration=11504.8, train/accuracy=0.647014, train/bleu=31.8038, train/loss=1.67776, validation/accuracy=0.650866, validation/bleu=27.6111, validation/loss=1.65426, validation/num_examples=3000
I0305 22:24:50.010458 139780331394816 logging_writer.py:48] [19200] global_step=19200, grad_norm=0.17345327138900757, loss=1.8192776441574097
I0305 22:25:25.237075 139780339787520 logging_writer.py:48] [19300] global_step=19300, grad_norm=0.1981336623430252, loss=1.8433040380477905
I0305 22:26:00.397794 139780331394816 logging_writer.py:48] [19400] global_step=19400, grad_norm=0.2196664810180664, loss=1.9339007139205933
I0305 22:26:35.564882 139780339787520 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.30181393027305603, loss=1.8768401145935059
I0305 22:27:10.761892 139780331394816 logging_writer.py:48] [19600] global_step=19600, grad_norm=0.18997247517108917, loss=1.8469972610473633
I0305 22:27:45.930468 139780339787520 logging_writer.py:48] [19700] global_step=19700, grad_norm=0.17004531621932983, loss=1.846009373664856
I0305 22:28:21.146780 139780331394816 logging_writer.py:48] [19800] global_step=19800, grad_norm=0.21549421548843384, loss=1.8227572441101074
I0305 22:28:56.322405 139780339787520 logging_writer.py:48] [19900] global_step=19900, grad_norm=0.34795570373535156, loss=1.8851524591445923
I0305 22:29:31.524045 139780331394816 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.18445263803005219, loss=1.8580188751220703
I0305 22:30:06.795727 139780339787520 logging_writer.py:48] [20100] global_step=20100, grad_norm=0.21462178230285645, loss=1.8256585597991943
I0305 22:30:41.990959 139780331394816 logging_writer.py:48] [20200] global_step=20200, grad_norm=0.18132786452770233, loss=1.798534870147705
I0305 22:31:17.189305 139780339787520 logging_writer.py:48] [20300] global_step=20300, grad_norm=0.2527133822441101, loss=1.8342722654342651
I0305 22:31:52.421225 139780331394816 logging_writer.py:48] [20400] global_step=20400, grad_norm=0.24259862303733826, loss=1.8701978921890259
I0305 22:32:27.639199 139780339787520 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.16447846591472626, loss=1.8502042293548584
I0305 22:33:02.834320 139780331394816 logging_writer.py:48] [20600] global_step=20600, grad_norm=0.17493678629398346, loss=1.8985919952392578
I0305 22:33:38.077247 139780339787520 logging_writer.py:48] [20700] global_step=20700, grad_norm=0.1995653212070465, loss=1.7949085235595703
I0305 22:34:13.274724 139780331394816 logging_writer.py:48] [20800] global_step=20800, grad_norm=0.19783496856689453, loss=1.886293649673462
I0305 22:34:48.471712 139780339787520 logging_writer.py:48] [20900] global_step=20900, grad_norm=0.19127562642097473, loss=1.7815015316009521
I0305 22:35:23.684646 139780331394816 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.23142948746681213, loss=1.831082820892334
I0305 22:35:58.899566 139780339787520 logging_writer.py:48] [21100] global_step=21100, grad_norm=0.18028846383094788, loss=1.8275072574615479
I0305 22:36:34.097797 139780331394816 logging_writer.py:48] [21200] global_step=21200, grad_norm=0.17603474855422974, loss=1.8692655563354492
I0305 22:37:09.329448 139780339787520 logging_writer.py:48] [21300] global_step=21300, grad_norm=0.19296087324619293, loss=1.796093225479126
I0305 22:37:44.537151 139780331394816 logging_writer.py:48] [21400] global_step=21400, grad_norm=0.20300808548927307, loss=1.8281539678573608
I0305 22:38:19.718737 139780339787520 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.2369956523180008, loss=1.8601263761520386
I0305 22:38:39.404772 139923228669120 spec.py:321] Evaluating on the training split.
I0305 22:38:42.038899 139923228669120 workload.py:181] Translating evaluation dataset.
I0305 22:43:17.377134 139923228669120 spec.py:333] Evaluating on the validation split.
I0305 22:43:20.001384 139923228669120 workload.py:181] Translating evaluation dataset.
I0305 22:46:21.158255 139923228669120 spec.py:349] Evaluating on the test split.
I0305 22:46:23.801007 139923228669120 workload.py:181] Translating evaluation dataset.
I0305 22:48:45.729779 139923228669120 submission_runner.py:469] Time since start: 12951.47s, 	Step: 21557, 	{'train/accuracy': 0.6366486549377441, 'train/loss': 1.74783194065094, 'train/bleu': 31.014895561529407, 'validation/accuracy': 0.6557239294052124, 'validation/loss': 1.6255961656570435, 'validation/bleu': 27.855042440442286, 'validation/num_examples': 3000, 'test/accuracy': 0.6649634838104248, 'test/loss': 1.5605164766311646, 'test/bleu': 27.311655045351394, 'test/num_examples': 3003, 'score': 7597.073021173477, 'total_duration': 12951.468220949173, 'accumulated_submission_time': 7597.073021173477, 'accumulated_eval_time': 5353.06326174736, 'accumulated_logging_time': 0.15854406356811523}
I0305 22:48:45.740911 139780331394816 logging_writer.py:48] [21557] accumulated_eval_time=5353.06, accumulated_logging_time=0.158544, accumulated_submission_time=7597.07, global_step=21557, preemption_count=0, score=7597.07, test/accuracy=0.664963, test/bleu=27.3117, test/loss=1.56052, test/num_examples=3003, total_duration=12951.5, train/accuracy=0.636649, train/bleu=31.0149, train/loss=1.74783, validation/accuracy=0.655724, validation/bleu=27.855, validation/loss=1.6256, validation/num_examples=3000
I0305 22:49:01.270104 139780339787520 logging_writer.py:48] [21600] global_step=21600, grad_norm=0.26202329993247986, loss=1.8964914083480835
I0305 22:49:36.452107 139780331394816 logging_writer.py:48] [21700] global_step=21700, grad_norm=0.17144562304019928, loss=1.8211565017700195
I0305 22:50:11.640845 139780339787520 logging_writer.py:48] [21800] global_step=21800, grad_norm=0.26101288199424744, loss=1.8246768712997437
I0305 22:50:46.833085 139780331394816 logging_writer.py:48] [21900] global_step=21900, grad_norm=0.2399095892906189, loss=1.7454404830932617
I0305 22:51:22.083372 139780339787520 logging_writer.py:48] [22000] global_step=22000, grad_norm=0.17520809173583984, loss=1.8114326000213623
I0305 22:51:57.312534 139780331394816 logging_writer.py:48] [22100] global_step=22100, grad_norm=0.17348496615886688, loss=1.7834874391555786
I0305 22:52:32.504330 139780339787520 logging_writer.py:48] [22200] global_step=22200, grad_norm=0.18849332630634308, loss=1.789364218711853
I0305 22:53:07.689826 139780331394816 logging_writer.py:48] [22300] global_step=22300, grad_norm=0.1867503821849823, loss=1.873673439025879
I0305 22:53:42.884957 139780339787520 logging_writer.py:48] [22400] global_step=22400, grad_norm=0.22207167744636536, loss=1.8535382747650146
I0305 22:54:18.087450 139780331394816 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.19369909167289734, loss=1.939763069152832
I0305 22:54:53.251323 139780339787520 logging_writer.py:48] [22600] global_step=22600, grad_norm=0.23145082592964172, loss=1.815363883972168
I0305 22:55:28.450703 139780331394816 logging_writer.py:48] [22700] global_step=22700, grad_norm=0.2162095457315445, loss=1.839151382446289
I0305 22:56:03.677232 139780339787520 logging_writer.py:48] [22800] global_step=22800, grad_norm=0.16618365049362183, loss=1.8178056478500366
I0305 22:56:38.912616 139780331394816 logging_writer.py:48] [22900] global_step=22900, grad_norm=0.186518594622612, loss=1.832353115081787
I0305 22:57:14.087092 139780339787520 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.30920082330703735, loss=1.8441587686538696
I0305 22:57:49.301553 139780331394816 logging_writer.py:48] [23100] global_step=23100, grad_norm=0.270268976688385, loss=1.8518933057785034
I0305 22:58:24.526626 139780339787520 logging_writer.py:48] [23200] global_step=23200, grad_norm=0.21671974658966064, loss=1.879978895187378
I0305 22:58:59.739007 139780331394816 logging_writer.py:48] [23300] global_step=23300, grad_norm=0.17621497809886932, loss=1.934059500694275
I0305 22:59:34.931224 139780339787520 logging_writer.py:48] [23400] global_step=23400, grad_norm=0.17690633237361908, loss=1.8118430376052856
I0305 23:00:10.111725 139780331394816 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.2374652922153473, loss=1.8183521032333374
I0305 23:00:45.398284 139780339787520 logging_writer.py:48] [23600] global_step=23600, grad_norm=0.18724095821380615, loss=1.7572423219680786
I0305 23:01:20.574372 139780331394816 logging_writer.py:48] [23700] global_step=23700, grad_norm=0.2816833257675171, loss=1.8680733442306519
I0305 23:01:55.813295 139780339787520 logging_writer.py:48] [23800] global_step=23800, grad_norm=0.24113260209560394, loss=1.719027042388916
I0305 23:02:31.029504 139780331394816 logging_writer.py:48] [23900] global_step=23900, grad_norm=0.19632042944431305, loss=1.759108066558838
I0305 23:02:45.830138 139923228669120 spec.py:321] Evaluating on the training split.
I0305 23:02:48.476311 139923228669120 workload.py:181] Translating evaluation dataset.
I0305 23:05:41.415465 139923228669120 spec.py:333] Evaluating on the validation split.
I0305 23:05:44.049376 139923228669120 workload.py:181] Translating evaluation dataset.
I0305 23:08:14.835063 139923228669120 spec.py:349] Evaluating on the test split.
I0305 23:08:17.463059 139923228669120 workload.py:181] Translating evaluation dataset.
I0305 23:10:32.129795 139923228669120 submission_runner.py:469] Time since start: 14257.87s, 	Step: 23943, 	{'train/accuracy': 0.636714518070221, 'train/loss': 1.7608840465545654, 'train/bleu': 31.038327625725056, 'validation/accuracy': 0.6578251719474792, 'validation/loss': 1.616212010383606, 'validation/bleu': 28.055206430290824, 'validation/num_examples': 3000, 'test/accuracy': 0.6668983697891235, 'test/loss': 1.545904278755188, 'test/bleu': 27.068156211779904, 'test/num_examples': 3003, 'score': 8437.032063961029, 'total_duration': 14257.868232250214, 'accumulated_submission_time': 8437.032063961029, 'accumulated_eval_time': 5819.362867355347, 'accumulated_logging_time': 0.17757081985473633}
I0305 23:10:32.141384 139780339787520 logging_writer.py:48] [23943] accumulated_eval_time=5819.36, accumulated_logging_time=0.177571, accumulated_submission_time=8437.03, global_step=23943, preemption_count=0, score=8437.03, test/accuracy=0.666898, test/bleu=27.0682, test/loss=1.5459, test/num_examples=3003, total_duration=14257.9, train/accuracy=0.636715, train/bleu=31.0383, train/loss=1.76088, validation/accuracy=0.657825, validation/bleu=28.0552, validation/loss=1.61621, validation/num_examples=3000
I0305 23:10:52.634396 139780331394816 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.18089450895786285, loss=1.805267572402954
I0305 23:11:27.872203 139780339787520 logging_writer.py:48] [24100] global_step=24100, grad_norm=0.22756271064281464, loss=1.8300710916519165
I0305 23:12:03.063347 139780331394816 logging_writer.py:48] [24200] global_step=24200, grad_norm=0.20671434700489044, loss=1.9021456241607666
I0305 23:12:38.294041 139780339787520 logging_writer.py:48] [24300] global_step=24300, grad_norm=0.19285552203655243, loss=1.7635531425476074
I0305 23:13:13.546824 139780331394816 logging_writer.py:48] [24400] global_step=24400, grad_norm=0.19827604293823242, loss=1.8267607688903809
I0305 23:13:48.819074 139780339787520 logging_writer.py:48] [24500] global_step=24500, grad_norm=0.1875094622373581, loss=1.811285138130188
I0305 23:14:24.046698 139780331394816 logging_writer.py:48] [24600] global_step=24600, grad_norm=0.19902941584587097, loss=1.8205174207687378
I0305 23:14:59.265376 139780339787520 logging_writer.py:48] [24700] global_step=24700, grad_norm=0.2027663141489029, loss=1.8505558967590332
I0305 23:15:34.507178 139780331394816 logging_writer.py:48] [24800] global_step=24800, grad_norm=0.236586332321167, loss=1.8913644552230835
I0305 23:16:09.756645 139780339787520 logging_writer.py:48] [24900] global_step=24900, grad_norm=0.18310053646564484, loss=1.7907798290252686
I0305 23:16:44.978929 139780331394816 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.17748410999774933, loss=1.8276795148849487
I0305 23:17:20.176689 139780331394816 logging_writer.py:48] [25100] global_step=25100, grad_norm=0.18665240705013275, loss=1.899526596069336
I0305 23:17:55.277915 139780113299200 logging_writer.py:48] [25200] global_step=25200, grad_norm=0.22004765272140503, loss=1.8028537034988403
I0305 23:18:30.383220 139780331394816 logging_writer.py:48] [25300] global_step=25300, grad_norm=0.19532684981822968, loss=1.8041495084762573
I0305 23:19:05.461315 139780113299200 logging_writer.py:48] [25400] global_step=25400, grad_norm=0.28726479411125183, loss=1.9188631772994995
I0305 23:19:40.542686 139780331394816 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.2028687745332718, loss=1.8709304332733154
I0305 23:20:15.639976 139780113299200 logging_writer.py:48] [25600] global_step=25600, grad_norm=0.20130568742752075, loss=1.8577200174331665
I0305 23:20:50.711525 139780331394816 logging_writer.py:48] [25700] global_step=25700, grad_norm=0.1840144395828247, loss=1.7982330322265625
I0305 23:21:25.786105 139780113299200 logging_writer.py:48] [25800] global_step=25800, grad_norm=0.21406865119934082, loss=1.7779080867767334
I0305 23:22:00.856693 139780331394816 logging_writer.py:48] [25900] global_step=25900, grad_norm=0.18190324306488037, loss=1.7561075687408447
I0305 23:22:35.953864 139780113299200 logging_writer.py:48] [26000] global_step=26000, grad_norm=0.1966845542192459, loss=1.8929083347320557
I0305 23:23:11.024084 139780331394816 logging_writer.py:48] [26100] global_step=26100, grad_norm=0.22721512615680695, loss=1.8155460357666016
I0305 23:23:46.105849 139780113299200 logging_writer.py:48] [26200] global_step=26200, grad_norm=0.2938774824142456, loss=1.8398782014846802
I0305 23:24:21.183643 139780331394816 logging_writer.py:48] [26300] global_step=26300, grad_norm=0.20997653901576996, loss=1.8917579650878906
I0305 23:24:32.442168 139923228669120 spec.py:321] Evaluating on the training split.
I0305 23:24:35.069545 139923228669120 workload.py:181] Translating evaluation dataset.
I0305 23:27:42.516033 139923228669120 spec.py:333] Evaluating on the validation split.
I0305 23:27:45.150555 139923228669120 workload.py:181] Translating evaluation dataset.
I0305 23:30:22.972397 139923228669120 spec.py:349] Evaluating on the test split.
I0305 23:30:25.602888 139923228669120 workload.py:181] Translating evaluation dataset.
I0305 23:32:57.978169 139923228669120 submission_runner.py:469] Time since start: 15603.72s, 	Step: 26333, 	{'train/accuracy': 0.6419850587844849, 'train/loss': 1.7114866971969604, 'train/bleu': 31.73044925554107, 'validation/accuracy': 0.6580228805541992, 'validation/loss': 1.6022928953170776, 'validation/bleu': 28.182939345620774, 'validation/num_examples': 3000, 'test/accuracy': 0.6678252816200256, 'test/loss': 1.5391461849212646, 'test/bleu': 27.450689213528424, 'test/num_examples': 3003, 'score': 9277.20161318779, 'total_duration': 15603.71659898758, 'accumulated_submission_time': 9277.20161318779, 'accumulated_eval_time': 6324.898819923401, 'accumulated_logging_time': 0.19710993766784668}
I0305 23:32:57.990001 139780113299200 logging_writer.py:48] [26333] accumulated_eval_time=6324.9, accumulated_logging_time=0.19711, accumulated_submission_time=9277.2, global_step=26333, preemption_count=0, score=9277.2, test/accuracy=0.667825, test/bleu=27.4507, test/loss=1.53915, test/num_examples=3003, total_duration=15603.7, train/accuracy=0.641985, train/bleu=31.7304, train/loss=1.71149, validation/accuracy=0.658023, validation/bleu=28.1829, validation/loss=1.60229, validation/num_examples=3000
I0305 23:33:21.890207 139780331394816 logging_writer.py:48] [26400] global_step=26400, grad_norm=0.18631353974342346, loss=1.8847962617874146
I0305 23:33:56.972143 139780113299200 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.2229933738708496, loss=1.8677253723144531
I0305 23:34:32.068304 139780331394816 logging_writer.py:48] [26600] global_step=26600, grad_norm=0.2127101570367813, loss=1.8132266998291016
I0305 23:35:07.159859 139780113299200 logging_writer.py:48] [26700] global_step=26700, grad_norm=0.19857068359851837, loss=1.8135862350463867
I0305 23:35:42.246551 139780331394816 logging_writer.py:48] [26800] global_step=26800, grad_norm=0.24358905851840973, loss=1.873745083808899
I0305 23:36:17.338197 139780113299200 logging_writer.py:48] [26900] global_step=26900, grad_norm=0.1792106032371521, loss=1.7547917366027832
I0305 23:36:52.409464 139780331394816 logging_writer.py:48] [27000] global_step=27000, grad_norm=0.1958743780851364, loss=1.8801400661468506
I0305 23:37:27.499267 139780113299200 logging_writer.py:48] [27100] global_step=27100, grad_norm=0.1953805387020111, loss=1.7745428085327148
I0305 23:38:02.578274 139780331394816 logging_writer.py:48] [27200] global_step=27200, grad_norm=0.17541389167308807, loss=1.8440765142440796
I0305 23:38:37.678801 139780113299200 logging_writer.py:48] [27300] global_step=27300, grad_norm=0.2162897139787674, loss=1.8322246074676514
I0305 23:39:12.744017 139780331394816 logging_writer.py:48] [27400] global_step=27400, grad_norm=0.20452244579792023, loss=1.7652289867401123
I0305 23:39:47.842701 139780113299200 logging_writer.py:48] [27500] global_step=27500, grad_norm=0.19659283757209778, loss=1.8133835792541504
I0305 23:40:22.921354 139780331394816 logging_writer.py:48] [27600] global_step=27600, grad_norm=0.20120307803153992, loss=1.8523123264312744
I0305 23:40:58.005442 139780113299200 logging_writer.py:48] [27700] global_step=27700, grad_norm=0.23698104918003082, loss=1.8001240491867065
I0305 23:41:33.061589 139780331394816 logging_writer.py:48] [27800] global_step=27800, grad_norm=0.20971927046775818, loss=1.7798254489898682
I0305 23:42:08.168127 139780113299200 logging_writer.py:48] [27900] global_step=27900, grad_norm=0.17448242008686066, loss=1.7847955226898193
I0305 23:42:43.242674 139780331394816 logging_writer.py:48] [28000] global_step=28000, grad_norm=0.25192469358444214, loss=1.7658823728561401
I0305 23:43:18.318928 139780113299200 logging_writer.py:48] [28100] global_step=28100, grad_norm=0.17643718421459198, loss=1.77324640750885
I0305 23:43:53.393771 139780331394816 logging_writer.py:48] [28200] global_step=28200, grad_norm=0.2215227484703064, loss=1.7324333190917969
I0305 23:44:28.476526 139780113299200 logging_writer.py:48] [28300] global_step=28300, grad_norm=0.19688430428504944, loss=1.8669527769088745
I0305 23:45:03.564085 139780331394816 logging_writer.py:48] [28400] global_step=28400, grad_norm=0.18599438667297363, loss=1.756024718284607
I0305 23:45:38.690827 139780113299200 logging_writer.py:48] [28500] global_step=28500, grad_norm=0.18293537199497223, loss=1.7695649862289429
I0305 23:46:13.792924 139780331394816 logging_writer.py:48] [28600] global_step=28600, grad_norm=0.22635510563850403, loss=1.8022907972335815
I0305 23:46:48.887256 139780113299200 logging_writer.py:48] [28700] global_step=28700, grad_norm=0.19559809565544128, loss=1.8558074235916138
I0305 23:46:58.013299 139923228669120 spec.py:321] Evaluating on the training split.
I0305 23:47:00.640664 139923228669120 workload.py:181] Translating evaluation dataset.
I0305 23:49:57.756832 139923228669120 spec.py:333] Evaluating on the validation split.
I0305 23:50:00.393459 139923228669120 workload.py:181] Translating evaluation dataset.
I0305 23:52:41.864557 139923228669120 spec.py:349] Evaluating on the test split.
I0305 23:52:44.495131 139923228669120 workload.py:181] Translating evaluation dataset.
I0305 23:55:00.288997 139923228669120 submission_runner.py:469] Time since start: 16926.03s, 	Step: 28727, 	{'train/accuracy': 0.6393638253211975, 'train/loss': 1.7340829372406006, 'train/bleu': 31.452897669625195, 'validation/accuracy': 0.6586655974388123, 'validation/loss': 1.5983942747116089, 'validation/bleu': 28.204650952944153, 'validation/num_examples': 3000, 'test/accuracy': 0.6711273193359375, 'test/loss': 1.5238035917282104, 'test/bleu': 27.27621764317378, 'test/num_examples': 3003, 'score': 10117.095172166824, 'total_duration': 16926.027439832687, 'accumulated_submission_time': 10117.095172166824, 'accumulated_eval_time': 6807.174488782883, 'accumulated_logging_time': 0.21719098091125488}
I0305 23:55:00.301337 139780331394816 logging_writer.py:48] [28727] accumulated_eval_time=6807.17, accumulated_logging_time=0.217191, accumulated_submission_time=10117.1, global_step=28727, preemption_count=0, score=10117.1, test/accuracy=0.671127, test/bleu=27.2762, test/loss=1.5238, test/num_examples=3003, total_duration=16926, train/accuracy=0.639364, train/bleu=31.4529, train/loss=1.73408, validation/accuracy=0.658666, validation/bleu=28.2047, validation/loss=1.59839, validation/num_examples=3000
I0305 23:55:26.293421 139780113299200 logging_writer.py:48] [28800] global_step=28800, grad_norm=0.18887650966644287, loss=1.8540359735488892
I0305 23:56:01.398831 139780331394816 logging_writer.py:48] [28900] global_step=28900, grad_norm=0.22113844752311707, loss=1.8420219421386719
I0305 23:56:36.467954 139780113299200 logging_writer.py:48] [29000] global_step=29000, grad_norm=0.17933142185211182, loss=1.78229820728302
I0305 23:57:11.546809 139780331394816 logging_writer.py:48] [29100] global_step=29100, grad_norm=0.21381932497024536, loss=1.7623658180236816
I0305 23:57:46.613983 139780113299200 logging_writer.py:48] [29200] global_step=29200, grad_norm=0.19871412217617035, loss=1.8669233322143555
I0305 23:58:21.685366 139780331394816 logging_writer.py:48] [29300] global_step=29300, grad_norm=0.1995674967765808, loss=1.7690504789352417
I0305 23:58:56.757583 139780113299200 logging_writer.py:48] [29400] global_step=29400, grad_norm=0.16885171830654144, loss=1.7724299430847168
I0305 23:59:31.855594 139780331394816 logging_writer.py:48] [29500] global_step=29500, grad_norm=0.16872580349445343, loss=1.7634469270706177
I0306 00:00:06.932240 139780113299200 logging_writer.py:48] [29600] global_step=29600, grad_norm=0.20645445585250854, loss=1.8071469068527222
I0306 00:00:42.035678 139780331394816 logging_writer.py:48] [29700] global_step=29700, grad_norm=0.2006010264158249, loss=1.7513939142227173
I0306 00:01:17.114140 139780113299200 logging_writer.py:48] [29800] global_step=29800, grad_norm=0.17026843130588531, loss=1.8003815412521362
I0306 00:01:52.199098 139780331394816 logging_writer.py:48] [29900] global_step=29900, grad_norm=0.1889098435640335, loss=1.7487647533416748
I0306 00:02:27.286728 139780113299200 logging_writer.py:48] [30000] global_step=30000, grad_norm=0.18720701336860657, loss=1.7803808450698853
I0306 00:03:02.379556 139780331394816 logging_writer.py:48] [30100] global_step=30100, grad_norm=0.17894276976585388, loss=1.789966344833374
I0306 00:03:37.441401 139780113299200 logging_writer.py:48] [30200] global_step=30200, grad_norm=0.18943078815937042, loss=1.7371907234191895
I0306 00:04:12.491223 139780331394816 logging_writer.py:48] [30300] global_step=30300, grad_norm=0.17538435757160187, loss=1.836674451828003
I0306 00:04:47.569554 139780113299200 logging_writer.py:48] [30400] global_step=30400, grad_norm=0.204151451587677, loss=1.7790205478668213
I0306 00:05:22.649956 139780331394816 logging_writer.py:48] [30500] global_step=30500, grad_norm=0.1753619760274887, loss=1.7594854831695557
I0306 00:05:57.711199 139780113299200 logging_writer.py:48] [30600] global_step=30600, grad_norm=0.18981745839118958, loss=1.7396268844604492
I0306 00:06:32.770560 139780331394816 logging_writer.py:48] [30700] global_step=30700, grad_norm=0.4774697422981262, loss=1.8537170886993408
I0306 00:07:07.794213 139780113299200 logging_writer.py:48] [30800] global_step=30800, grad_norm=0.3142260015010834, loss=1.76066255569458
I0306 00:07:42.818842 139780331394816 logging_writer.py:48] [30900] global_step=30900, grad_norm=0.17569763958454132, loss=1.7731399536132812
I0306 00:08:17.880403 139780113299200 logging_writer.py:48] [31000] global_step=31000, grad_norm=0.18725211918354034, loss=1.807038426399231
I0306 00:08:52.963550 139780331394816 logging_writer.py:48] [31100] global_step=31100, grad_norm=0.2135622352361679, loss=1.8065727949142456
I0306 00:09:00.317747 139923228669120 spec.py:321] Evaluating on the training split.
I0306 00:09:02.949081 139923228669120 workload.py:181] Translating evaluation dataset.
I0306 00:13:41.613774 139923228669120 spec.py:333] Evaluating on the validation split.
I0306 00:13:44.243213 139923228669120 workload.py:181] Translating evaluation dataset.
I0306 00:16:35.866254 139923228669120 spec.py:349] Evaluating on the test split.
I0306 00:16:38.495990 139923228669120 workload.py:181] Translating evaluation dataset.
I0306 00:19:03.548365 139923228669120 submission_runner.py:469] Time since start: 18369.29s, 	Step: 31122, 	{'train/accuracy': 0.6390196681022644, 'train/loss': 1.7345538139343262, 'train/bleu': 31.202942473388664, 'validation/accuracy': 0.6607668399810791, 'validation/loss': 1.5840063095092773, 'validation/bleu': 28.23258252822054, 'validation/num_examples': 3000, 'test/accuracy': 0.6736415028572083, 'test/loss': 1.5088086128234863, 'test/bleu': 27.670258463255426, 'test/num_examples': 3003, 'score': 10956.981642723083, 'total_duration': 18369.286808490753, 'accumulated_submission_time': 10956.981642723083, 'accumulated_eval_time': 7410.405072927475, 'accumulated_logging_time': 0.23796510696411133}
I0306 00:19:03.560849 139780113299200 logging_writer.py:48] [31122] accumulated_eval_time=7410.41, accumulated_logging_time=0.237965, accumulated_submission_time=10957, global_step=31122, preemption_count=0, score=10957, test/accuracy=0.673642, test/bleu=27.6703, test/loss=1.50881, test/num_examples=3003, total_duration=18369.3, train/accuracy=0.63902, train/bleu=31.2029, train/loss=1.73455, validation/accuracy=0.660767, validation/bleu=28.2326, validation/loss=1.58401, validation/num_examples=3000
I0306 00:19:31.288911 139780331394816 logging_writer.py:48] [31200] global_step=31200, grad_norm=0.1922435313463211, loss=1.8301286697387695
I0306 00:20:06.319106 139780113299200 logging_writer.py:48] [31300] global_step=31300, grad_norm=0.1931852251291275, loss=1.7820276021957397
I0306 00:20:41.444207 139780331394816 logging_writer.py:48] [31400] global_step=31400, grad_norm=0.20382514595985413, loss=1.844771146774292
I0306 00:21:16.608020 139780113299200 logging_writer.py:48] [31500] global_step=31500, grad_norm=0.16797997057437897, loss=1.6910247802734375
I0306 00:21:51.788302 139780331394816 logging_writer.py:48] [31600] global_step=31600, grad_norm=0.1841379553079605, loss=1.745280385017395
I0306 00:22:26.948295 139780113299200 logging_writer.py:48] [31700] global_step=31700, grad_norm=0.19539467990398407, loss=1.780023455619812
I0306 00:23:02.126440 139780331394816 logging_writer.py:48] [31800] global_step=31800, grad_norm=0.1904016137123108, loss=1.7434091567993164
I0306 00:23:37.327705 139780113299200 logging_writer.py:48] [31900] global_step=31900, grad_norm=0.1782705932855606, loss=1.726887822151184
I0306 00:24:12.530649 139780331394816 logging_writer.py:48] [32000] global_step=32000, grad_norm=0.18297578394412994, loss=1.7431358098983765
I0306 00:24:47.729801 139780113299200 logging_writer.py:48] [32100] global_step=32100, grad_norm=0.19190068542957306, loss=1.7357975244522095
I0306 00:25:22.876978 139780331394816 logging_writer.py:48] [32200] global_step=32200, grad_norm=0.20364585518836975, loss=1.7828329801559448
I0306 00:25:58.048342 139780113299200 logging_writer.py:48] [32300] global_step=32300, grad_norm=0.17863088846206665, loss=1.8664026260375977
I0306 00:26:33.220931 139780331394816 logging_writer.py:48] [32400] global_step=32400, grad_norm=0.2287222445011139, loss=1.78383469581604
I0306 00:27:08.385730 139780113299200 logging_writer.py:48] [32500] global_step=32500, grad_norm=0.18773531913757324, loss=1.8047677278518677
I0306 00:27:43.516091 139780331394816 logging_writer.py:48] [32600] global_step=32600, grad_norm=0.18991316854953766, loss=1.7275348901748657
I0306 00:28:18.630980 139780113299200 logging_writer.py:48] [32700] global_step=32700, grad_norm=0.18016932904720306, loss=1.7163166999816895
I0306 00:28:53.790902 139780331394816 logging_writer.py:48] [32800] global_step=32800, grad_norm=0.20238201320171356, loss=1.8008259534835815
I0306 00:29:28.893329 139780113299200 logging_writer.py:48] [32900] global_step=32900, grad_norm=0.20950987935066223, loss=1.8287335634231567
I0306 00:30:04.015183 139780331394816 logging_writer.py:48] [33000] global_step=33000, grad_norm=0.18269166350364685, loss=1.7370214462280273
I0306 00:30:39.121349 139780113299200 logging_writer.py:48] [33100] global_step=33100, grad_norm=0.18600749969482422, loss=1.7524718046188354
I0306 00:31:14.211244 139780331394816 logging_writer.py:48] [33200] global_step=33200, grad_norm=0.19942526519298553, loss=1.7221941947937012
I0306 00:31:49.274723 139780113299200 logging_writer.py:48] [33300] global_step=33300, grad_norm=0.22844788432121277, loss=1.7225388288497925
I0306 00:32:24.374963 139780331394816 logging_writer.py:48] [33400] global_step=33400, grad_norm=0.17430049180984497, loss=1.7378579378128052
I0306 00:32:59.462199 139780113299200 logging_writer.py:48] [33500] global_step=33500, grad_norm=0.22317761182785034, loss=1.802157998085022
I0306 00:33:03.682733 139923228669120 spec.py:321] Evaluating on the training split.
I0306 00:33:06.303400 139923228669120 workload.py:181] Translating evaluation dataset.
I0306 00:37:27.243653 139923228669120 spec.py:333] Evaluating on the validation split.
I0306 00:37:29.864038 139923228669120 workload.py:181] Translating evaluation dataset.
I0306 00:40:21.492429 139923228669120 spec.py:349] Evaluating on the test split.
I0306 00:40:24.109844 139923228669120 workload.py:181] Translating evaluation dataset.
I0306 00:43:07.836235 139923228669120 submission_runner.py:469] Time since start: 19813.57s, 	Step: 33513, 	{'train/accuracy': 0.6445894837379456, 'train/loss': 1.6965571641921997, 'train/bleu': 31.974314050553552, 'validation/accuracy': 0.6629669070243835, 'validation/loss': 1.5725905895233154, 'validation/bleu': 28.226341844985445, 'validation/num_examples': 3000, 'test/accuracy': 0.6736531257629395, 'test/loss': 1.496616005897522, 'test/bleu': 27.53553209945815, 'test/num_examples': 3003, 'score': 11796.972217082977, 'total_duration': 19813.574679613113, 'accumulated_submission_time': 11796.972217082977, 'accumulated_eval_time': 8014.558529138565, 'accumulated_logging_time': 0.25838708877563477}
I0306 00:43:07.848391 139780331394816 logging_writer.py:48] [33513] accumulated_eval_time=8014.56, accumulated_logging_time=0.258387, accumulated_submission_time=11797, global_step=33513, preemption_count=0, score=11797, test/accuracy=0.673653, test/bleu=27.5355, test/loss=1.49662, test/num_examples=3003, total_duration=19813.6, train/accuracy=0.644589, train/bleu=31.9743, train/loss=1.69656, validation/accuracy=0.662967, validation/bleu=28.2263, validation/loss=1.57259, validation/num_examples=3000
I0306 00:43:38.719136 139780113299200 logging_writer.py:48] [33600] global_step=33600, grad_norm=0.18550460040569305, loss=1.783989667892456
I0306 00:44:13.713747 139780331394816 logging_writer.py:48] [33700] global_step=33700, grad_norm=0.30454936623573303, loss=1.7958126068115234
I0306 00:44:48.744814 139780113299200 logging_writer.py:48] [33800] global_step=33800, grad_norm=0.19635304808616638, loss=1.7533172369003296
I0306 00:45:23.783074 139780331394816 logging_writer.py:48] [33900] global_step=33900, grad_norm=0.18392595648765564, loss=1.804682731628418
I0306 00:45:58.824963 139780113299200 logging_writer.py:48] [34000] global_step=34000, grad_norm=0.23538169264793396, loss=1.875243902206421
I0306 00:46:33.884136 139780331394816 logging_writer.py:48] [34100] global_step=34100, grad_norm=0.4838213324546814, loss=1.7333199977874756
I0306 00:47:08.922507 139780113299200 logging_writer.py:48] [34200] global_step=34200, grad_norm=0.21521839499473572, loss=1.8242907524108887
I0306 00:47:43.958462 139780331394816 logging_writer.py:48] [34300] global_step=34300, grad_norm=0.5298077464103699, loss=1.7459276914596558
I0306 00:48:19.012188 139780113299200 logging_writer.py:48] [34400] global_step=34400, grad_norm=0.20065706968307495, loss=1.8468372821807861
I0306 00:48:54.069982 139780331394816 logging_writer.py:48] [34500] global_step=34500, grad_norm=0.18366406857967377, loss=1.8415939807891846
I0306 00:49:29.129823 139780113299200 logging_writer.py:48] [34600] global_step=34600, grad_norm=0.2142142802476883, loss=1.8381266593933105
I0306 00:50:04.150241 139780331394816 logging_writer.py:48] [34700] global_step=34700, grad_norm=0.21008029580116272, loss=1.7897741794586182
I0306 00:50:39.179946 139780113299200 logging_writer.py:48] [34800] global_step=34800, grad_norm=0.47121742367744446, loss=1.9020870923995972
I0306 00:51:14.221881 139780331394816 logging_writer.py:48] [34900] global_step=34900, grad_norm=0.26335227489471436, loss=1.796302318572998
I0306 00:51:49.327450 139780113299200 logging_writer.py:48] [35000] global_step=35000, grad_norm=0.1902778297662735, loss=1.8035355806350708
I0306 00:52:24.382847 139780331394816 logging_writer.py:48] [35100] global_step=35100, grad_norm=0.30412256717681885, loss=1.693261981010437
I0306 00:52:59.408162 139780113299200 logging_writer.py:48] [35200] global_step=35200, grad_norm=0.193881094455719, loss=1.8302639722824097
I0306 00:53:34.460084 139780331394816 logging_writer.py:48] [35300] global_step=35300, grad_norm=0.22361484169960022, loss=1.8446455001831055
I0306 00:54:09.488036 139780113299200 logging_writer.py:48] [35400] global_step=35400, grad_norm=0.17853447794914246, loss=1.688736915588379
I0306 00:54:44.575227 139780331394816 logging_writer.py:48] [35500] global_step=35500, grad_norm=0.19831906259059906, loss=1.873663067817688
I0306 00:55:19.595378 139780113299200 logging_writer.py:48] [35600] global_step=35600, grad_norm=0.21191079914569855, loss=1.7809890508651733
I0306 00:55:54.631588 139780331394816 logging_writer.py:48] [35700] global_step=35700, grad_norm=0.19099290668964386, loss=1.7572813034057617
I0306 00:56:29.665855 139780113299200 logging_writer.py:48] [35800] global_step=35800, grad_norm=0.19722484052181244, loss=1.7778092622756958
I0306 00:57:04.718976 139780331394816 logging_writer.py:48] [35900] global_step=35900, grad_norm=0.2034589797258377, loss=1.8139177560806274
I0306 00:57:07.872014 139923228669120 spec.py:321] Evaluating on the training split.
I0306 00:57:10.499014 139923228669120 workload.py:181] Translating evaluation dataset.
I0306 01:00:36.443984 139923228669120 spec.py:333] Evaluating on the validation split.
I0306 01:00:39.073473 139923228669120 workload.py:181] Translating evaluation dataset.
I0306 01:03:22.235193 139923228669120 spec.py:349] Evaluating on the test split.
I0306 01:03:24.876713 139923228669120 workload.py:181] Translating evaluation dataset.
I0306 01:06:03.410170 139923228669120 submission_runner.py:469] Time since start: 21189.15s, 	Step: 35910, 	{'train/accuracy': 0.6458752751350403, 'train/loss': 1.6932095289230347, 'train/bleu': 31.200098308062625, 'validation/accuracy': 0.6643759608268738, 'validation/loss': 1.5690315961837769, 'validation/bleu': 28.283795968933816, 'validation/num_examples': 3000, 'test/accuracy': 0.6754489541053772, 'test/loss': 1.4884008169174194, 'test/bleu': 27.657416297099946, 'test/num_examples': 3003, 'score': 12636.863565206528, 'total_duration': 21189.14860677719, 'accumulated_submission_time': 12636.863565206528, 'accumulated_eval_time': 8550.096631288528, 'accumulated_logging_time': 0.2783670425415039}
I0306 01:06:03.421697 139780113299200 logging_writer.py:48] [35910] accumulated_eval_time=8550.1, accumulated_logging_time=0.278367, accumulated_submission_time=12636.9, global_step=35910, preemption_count=0, score=12636.9, test/accuracy=0.675449, test/bleu=27.6574, test/loss=1.4884, test/num_examples=3003, total_duration=21189.1, train/accuracy=0.645875, train/bleu=31.2001, train/loss=1.69321, validation/accuracy=0.664376, validation/bleu=28.2838, validation/loss=1.56903, validation/num_examples=3000
I0306 01:06:35.340282 139780331394816 logging_writer.py:48] [36000] global_step=36000, grad_norm=0.20721346139907837, loss=1.755966305732727
I0306 01:07:10.367556 139780113299200 logging_writer.py:48] [36100] global_step=36100, grad_norm=0.20316457748413086, loss=1.7477331161499023
I0306 01:07:45.379000 139780331394816 logging_writer.py:48] [36200] global_step=36200, grad_norm=0.24967114627361298, loss=1.753942847251892
I0306 01:08:20.400880 139780113299200 logging_writer.py:48] [36300] global_step=36300, grad_norm=0.2037394493818283, loss=1.7768100500106812
I0306 01:08:55.451291 139780331394816 logging_writer.py:48] [36400] global_step=36400, grad_norm=0.18883737921714783, loss=1.7947454452514648
I0306 01:09:30.493884 139780113299200 logging_writer.py:48] [36500] global_step=36500, grad_norm=0.21186043322086334, loss=1.7446335554122925
I0306 01:10:05.540880 139780331394816 logging_writer.py:48] [36600] global_step=36600, grad_norm=0.18443891406059265, loss=1.801732063293457
I0306 01:10:40.568605 139780113299200 logging_writer.py:48] [36700] global_step=36700, grad_norm=0.2344449907541275, loss=1.7258720397949219
I0306 01:11:15.565245 139780331394816 logging_writer.py:48] [36800] global_step=36800, grad_norm=0.2026580274105072, loss=1.6589435338974
I0306 01:11:50.580812 139780113299200 logging_writer.py:48] [36900] global_step=36900, grad_norm=0.20703917741775513, loss=1.717069149017334
I0306 01:12:25.600472 139780331394816 logging_writer.py:48] [37000] global_step=37000, grad_norm=0.2016306221485138, loss=1.7895889282226562
I0306 01:13:00.637518 139780113299200 logging_writer.py:48] [37100] global_step=37100, grad_norm=0.20500865578651428, loss=1.8313368558883667
I0306 01:13:35.693202 139780331394816 logging_writer.py:48] [37200] global_step=37200, grad_norm=0.20637956261634827, loss=1.7325447797775269
I0306 01:14:10.752433 139780113299200 logging_writer.py:48] [37300] global_step=37300, grad_norm=0.19293420016765594, loss=1.7595627307891846
I0306 01:14:45.799201 139780331394816 logging_writer.py:48] [37400] global_step=37400, grad_norm=0.19631531834602356, loss=1.76353120803833
I0306 01:15:20.822821 139780113299200 logging_writer.py:48] [37500] global_step=37500, grad_norm=0.18311914801597595, loss=1.830880045890808
I0306 01:15:55.842536 139780331394816 logging_writer.py:48] [37600] global_step=37600, grad_norm=0.18353235721588135, loss=1.779778242111206
I0306 01:16:30.800450 139780331394816 logging_writer.py:48] [37700] global_step=37700, grad_norm=0.2240665853023529, loss=1.7596439123153687
I0306 01:17:05.775694 139780104906496 logging_writer.py:48] [37800] global_step=37800, grad_norm=0.21921250224113464, loss=1.7765809297561646
I0306 01:17:40.760861 139780331394816 logging_writer.py:48] [37900] global_step=37900, grad_norm=0.1991148293018341, loss=1.7752416133880615
I0306 01:18:15.735337 139780104906496 logging_writer.py:48] [38000] global_step=38000, grad_norm=0.2253514528274536, loss=1.8221465349197388
I0306 01:18:50.689201 139780331394816 logging_writer.py:48] [38100] global_step=38100, grad_norm=0.20800022780895233, loss=1.7388803958892822
I0306 01:19:25.631979 139780104906496 logging_writer.py:48] [38200] global_step=38200, grad_norm=0.18663786351680756, loss=1.7875726222991943
I0306 01:20:00.596760 139780331394816 logging_writer.py:48] [38300] global_step=38300, grad_norm=0.28240880370140076, loss=1.8083596229553223
I0306 01:20:03.754622 139923228669120 spec.py:321] Evaluating on the training split.
I0306 01:20:06.365322 139923228669120 workload.py:181] Translating evaluation dataset.
I0306 01:24:01.912201 139923228669120 spec.py:333] Evaluating on the validation split.
I0306 01:24:04.529162 139923228669120 workload.py:181] Translating evaluation dataset.
I0306 01:26:36.580543 139923228669120 spec.py:349] Evaluating on the test split.
I0306 01:26:39.199947 139923228669120 workload.py:181] Translating evaluation dataset.
I0306 01:28:47.944061 139923228669120 submission_runner.py:469] Time since start: 22553.68s, 	Step: 38310, 	{'train/accuracy': 0.6566354036331177, 'train/loss': 1.6094287633895874, 'train/bleu': 32.52166016042097, 'validation/accuracy': 0.6647467613220215, 'validation/loss': 1.5627777576446533, 'validation/bleu': 28.540662170040317, 'validation/num_examples': 3000, 'test/accuracy': 0.6741397380828857, 'test/loss': 1.4912638664245605, 'test/bleu': 27.74329929828922, 'test/num_examples': 3003, 'score': 13477.06750702858, 'total_duration': 22553.682485103607, 'accumulated_submission_time': 13477.06750702858, 'accumulated_eval_time': 9074.286002159119, 'accumulated_logging_time': 0.2986142635345459}
I0306 01:28:47.956148 139780104906496 logging_writer.py:48] [38310] accumulated_eval_time=9074.29, accumulated_logging_time=0.298614, accumulated_submission_time=13477.1, global_step=38310, preemption_count=0, score=13477.1, test/accuracy=0.67414, test/bleu=27.7433, test/loss=1.49126, test/num_examples=3003, total_duration=22553.7, train/accuracy=0.656635, train/bleu=32.5217, train/loss=1.60943, validation/accuracy=0.664747, validation/bleu=28.5407, validation/loss=1.56278, validation/num_examples=3000
I0306 01:29:19.839759 139780331394816 logging_writer.py:48] [38400] global_step=38400, grad_norm=0.19908416271209717, loss=1.7743690013885498
I0306 01:29:54.802257 139780104906496 logging_writer.py:48] [38500] global_step=38500, grad_norm=0.2276294231414795, loss=1.796043872833252
I0306 01:30:29.750949 139780331394816 logging_writer.py:48] [38600] global_step=38600, grad_norm=0.18633289635181427, loss=1.7541944980621338
I0306 01:31:04.741959 139780104906496 logging_writer.py:48] [38700] global_step=38700, grad_norm=0.20353195071220398, loss=1.712929368019104
I0306 01:31:39.687388 139780331394816 logging_writer.py:48] [38800] global_step=38800, grad_norm=0.18506938219070435, loss=1.7516719102859497
I0306 01:32:14.632347 139780104906496 logging_writer.py:48] [38900] global_step=38900, grad_norm=0.2180945724248886, loss=1.8776177167892456
I0306 01:32:49.598859 139780331394816 logging_writer.py:48] [39000] global_step=39000, grad_norm=0.2323698103427887, loss=1.8306490182876587
I0306 01:33:24.547000 139780104906496 logging_writer.py:48] [39100] global_step=39100, grad_norm=0.20210686326026917, loss=1.7715271711349487
I0306 01:33:59.488660 139780331394816 logging_writer.py:48] [39200] global_step=39200, grad_norm=0.183440163731575, loss=1.787898063659668
I0306 01:34:34.444943 139780104906496 logging_writer.py:48] [39300] global_step=39300, grad_norm=0.2168993055820465, loss=1.7433513402938843
I0306 01:35:09.400279 139780331394816 logging_writer.py:48] [39400] global_step=39400, grad_norm=0.18785341084003448, loss=1.7867809534072876
I0306 01:35:44.333004 139780104906496 logging_writer.py:48] [39500] global_step=39500, grad_norm=0.1930721551179886, loss=1.7389200925827026
I0306 01:36:19.296239 139780331394816 logging_writer.py:48] [39600] global_step=39600, grad_norm=0.24617163836956024, loss=1.8371074199676514
I0306 01:36:54.262843 139780104906496 logging_writer.py:48] [39700] global_step=39700, grad_norm=0.3097594678401947, loss=1.767954707145691
I0306 01:37:29.230237 139780331394816 logging_writer.py:48] [39800] global_step=39800, grad_norm=0.21560586988925934, loss=1.7038049697875977
I0306 01:38:04.174316 139780104906496 logging_writer.py:48] [39900] global_step=39900, grad_norm=0.17141948640346527, loss=1.7507139444351196
I0306 01:38:39.131407 139780331394816 logging_writer.py:48] [40000] global_step=40000, grad_norm=0.21452884376049042, loss=1.8359323740005493
I0306 01:39:14.085722 139780104906496 logging_writer.py:48] [40100] global_step=40100, grad_norm=0.2154598981142044, loss=1.7587518692016602
I0306 01:39:49.032196 139780331394816 logging_writer.py:48] [40200] global_step=40200, grad_norm=0.1833025962114334, loss=1.7812398672103882
I0306 01:40:24.007445 139780104906496 logging_writer.py:48] [40300] global_step=40300, grad_norm=0.2669367790222168, loss=1.6985576152801514
I0306 01:40:58.945168 139780331394816 logging_writer.py:48] [40400] global_step=40400, grad_norm=0.20056435465812683, loss=1.7297269105911255
I0306 01:41:33.886439 139780104906496 logging_writer.py:48] [40500] global_step=40500, grad_norm=0.19008907675743103, loss=1.7516639232635498
I0306 01:42:08.842907 139780331394816 logging_writer.py:48] [40600] global_step=40600, grad_norm=0.2248762547969818, loss=1.767384648323059
I0306 01:42:43.781843 139780104906496 logging_writer.py:48] [40700] global_step=40700, grad_norm=0.19727389514446259, loss=1.7927974462509155
I0306 01:42:47.982249 139923228669120 spec.py:321] Evaluating on the training split.
I0306 01:42:50.610682 139923228669120 workload.py:181] Translating evaluation dataset.
I0306 01:46:51.534197 139923228669120 spec.py:333] Evaluating on the validation split.
I0306 01:46:54.148360 139923228669120 workload.py:181] Translating evaluation dataset.
I0306 01:49:53.099576 139923228669120 spec.py:349] Evaluating on the test split.
I0306 01:49:55.725920 139923228669120 workload.py:181] Translating evaluation dataset.
I0306 01:52:56.306160 139923228669120 submission_runner.py:469] Time since start: 24002.04s, 	Step: 40713, 	{'train/accuracy': 0.647389829158783, 'train/loss': 1.6865519285202026, 'train/bleu': 31.71911689368959, 'validation/accuracy': 0.6666254997253418, 'validation/loss': 1.5542771816253662, 'validation/bleu': 28.60730318712551, 'validation/num_examples': 3000, 'test/accuracy': 0.6779400110244751, 'test/loss': 1.4746785163879395, 'test/bleu': 27.964013584999243, 'test/num_examples': 3003, 'score': 14316.962766170502, 'total_duration': 24002.044580698013, 'accumulated_submission_time': 14316.962766170502, 'accumulated_eval_time': 9682.609852075577, 'accumulated_logging_time': 0.31876611709594727}
I0306 01:52:56.318633 139780331394816 logging_writer.py:48] [40713] accumulated_eval_time=9682.61, accumulated_logging_time=0.318766, accumulated_submission_time=14317, global_step=40713, preemption_count=0, score=14317, test/accuracy=0.67794, test/bleu=27.964, test/loss=1.47468, test/num_examples=3003, total_duration=24002, train/accuracy=0.64739, train/bleu=31.7191, train/loss=1.68655, validation/accuracy=0.666625, validation/bleu=28.6073, validation/loss=1.55428, validation/num_examples=3000
I0306 01:53:27.133498 139780104906496 logging_writer.py:48] [40800] global_step=40800, grad_norm=1.424274206161499, loss=1.794967770576477
I0306 01:54:02.074375 139780331394816 logging_writer.py:48] [40900] global_step=40900, grad_norm=0.18356043100357056, loss=1.7351489067077637
I0306 01:54:37.035497 139780104906496 logging_writer.py:48] [41000] global_step=41000, grad_norm=0.19582000374794006, loss=1.844336748123169
I0306 01:55:11.988466 139780331394816 logging_writer.py:48] [41100] global_step=41100, grad_norm=0.1770162135362625, loss=1.7664495706558228
I0306 01:55:46.936429 139780104906496 logging_writer.py:48] [41200] global_step=41200, grad_norm=0.4525068700313568, loss=1.8085583448410034
I0306 01:56:21.889117 139780331394816 logging_writer.py:48] [41300] global_step=41300, grad_norm=0.31421443819999695, loss=1.808544635772705
I0306 01:56:56.881584 139780104906496 logging_writer.py:48] [41400] global_step=41400, grad_norm=0.18577411770820618, loss=1.7838778495788574
I0306 01:57:31.833068 139780331394816 logging_writer.py:48] [41500] global_step=41500, grad_norm=0.18293245136737823, loss=1.6799513101577759
I0306 01:58:06.785212 139780104906496 logging_writer.py:48] [41600] global_step=41600, grad_norm=0.20288290083408356, loss=1.797268033027649
I0306 01:58:41.770315 139780331394816 logging_writer.py:48] [41700] global_step=41700, grad_norm=0.17505688965320587, loss=1.7582426071166992
I0306 01:59:16.714407 139780104906496 logging_writer.py:48] [41800] global_step=41800, grad_norm=0.1949264258146286, loss=1.7575663328170776
I0306 01:59:51.678744 139780331394816 logging_writer.py:48] [41900] global_step=41900, grad_norm=0.1804339736700058, loss=1.632840633392334
I0306 02:00:26.615260 139780104906496 logging_writer.py:48] [42000] global_step=42000, grad_norm=0.23357029259204865, loss=1.7827163934707642
I0306 02:01:01.559429 139780331394816 logging_writer.py:48] [42100] global_step=42100, grad_norm=0.1847316026687622, loss=1.7669841051101685
I0306 02:01:36.491833 139780104906496 logging_writer.py:48] [42200] global_step=42200, grad_norm=0.23709537088871002, loss=1.75493586063385
I0306 02:02:11.457705 139780331394816 logging_writer.py:48] [42300] global_step=42300, grad_norm=0.20173883438110352, loss=1.699357032775879
I0306 02:02:46.400352 139780104906496 logging_writer.py:48] [42400] global_step=42400, grad_norm=0.20029091835021973, loss=1.7936922311782837
I0306 02:03:21.346254 139780331394816 logging_writer.py:48] [42500] global_step=42500, grad_norm=0.19910959899425507, loss=1.7592097520828247
I0306 02:03:56.299372 139780104906496 logging_writer.py:48] [42600] global_step=42600, grad_norm=0.20953269302845, loss=1.816374659538269
I0306 02:04:31.247048 139780331394816 logging_writer.py:48] [42700] global_step=42700, grad_norm=0.2042718380689621, loss=1.7279421091079712
I0306 02:05:06.168601 139780104906496 logging_writer.py:48] [42800] global_step=42800, grad_norm=0.19928421080112457, loss=1.8060814142227173
I0306 02:05:41.135524 139780331394816 logging_writer.py:48] [42900] global_step=42900, grad_norm=0.19445109367370605, loss=1.7781703472137451
I0306 02:06:16.095875 139780104906496 logging_writer.py:48] [43000] global_step=43000, grad_norm=0.24177566170692444, loss=1.703502893447876
I0306 02:06:51.065237 139780331394816 logging_writer.py:48] [43100] global_step=43100, grad_norm=0.21003183722496033, loss=1.7690297365188599
I0306 02:06:56.315433 139923228669120 spec.py:321] Evaluating on the training split.
I0306 02:06:58.936576 139923228669120 workload.py:181] Translating evaluation dataset.
I0306 02:10:35.893996 139923228669120 spec.py:333] Evaluating on the validation split.
I0306 02:10:38.522637 139923228669120 workload.py:181] Translating evaluation dataset.
I0306 02:13:07.608545 139923228669120 spec.py:349] Evaluating on the test split.
I0306 02:13:10.233544 139923228669120 workload.py:181] Translating evaluation dataset.
I0306 02:15:25.515032 139923228669120 submission_runner.py:469] Time since start: 25351.25s, 	Step: 43116, 	{'train/accuracy': 0.6481766700744629, 'train/loss': 1.6767008304595947, 'train/bleu': 31.880219846211574, 'validation/accuracy': 0.6671445965766907, 'validation/loss': 1.5450067520141602, 'validation/bleu': 28.865265397169356, 'validation/num_examples': 3000, 'test/accuracy': 0.6772795915603638, 'test/loss': 1.470426321029663, 'test/bleu': 27.905053242648385, 'test/num_examples': 3003, 'score': 15156.829148292542, 'total_duration': 25351.253480434418, 'accumulated_submission_time': 15156.829148292542, 'accumulated_eval_time': 10191.80941104889, 'accumulated_logging_time': 0.3389413356781006}
I0306 02:15:25.527511 139780104906496 logging_writer.py:48] [43116] accumulated_eval_time=10191.8, accumulated_logging_time=0.338941, accumulated_submission_time=15156.8, global_step=43116, preemption_count=0, score=15156.8, test/accuracy=0.67728, test/bleu=27.9051, test/loss=1.47043, test/num_examples=3003, total_duration=25351.3, train/accuracy=0.648177, train/bleu=31.8802, train/loss=1.6767, validation/accuracy=0.667145, validation/bleu=28.8653, validation/loss=1.54501, validation/num_examples=3000
I0306 02:15:55.277867 139780331394816 logging_writer.py:48] [43200] global_step=43200, grad_norm=0.1790800243616104, loss=1.6719084978103638
I0306 02:16:30.212556 139780104906496 logging_writer.py:48] [43300] global_step=43300, grad_norm=0.1802586168050766, loss=1.7790029048919678
I0306 02:17:05.140024 139780331394816 logging_writer.py:48] [43400] global_step=43400, grad_norm=0.21726317703723907, loss=1.7377575635910034
I0306 02:17:40.072079 139780104906496 logging_writer.py:48] [43500] global_step=43500, grad_norm=0.22645050287246704, loss=1.7965744733810425
I0306 02:18:15.015393 139780331394816 logging_writer.py:48] [43600] global_step=43600, grad_norm=0.19752809405326843, loss=1.7348942756652832
I0306 02:18:49.966109 139780104906496 logging_writer.py:48] [43700] global_step=43700, grad_norm=0.20865966379642487, loss=1.7661269903182983
I0306 02:19:24.903968 139780331394816 logging_writer.py:48] [43800] global_step=43800, grad_norm=0.4449455440044403, loss=1.719529628753662
I0306 02:19:59.877935 139780020979456 logging_writer.py:48] [43900] global_step=43900, grad_norm=0.19556280970573425, loss=1.7115110158920288
I0306 02:20:34.937646 139780012586752 logging_writer.py:48] [44000] global_step=44000, grad_norm=0.22505347430706024, loss=1.7732067108154297
I0306 02:21:09.957214 139780020979456 logging_writer.py:48] [44100] global_step=44100, grad_norm=0.18072918057441711, loss=1.7037615776062012
I0306 02:21:45.046790 139780012586752 logging_writer.py:48] [44200] global_step=44200, grad_norm=0.190672367811203, loss=1.8525110483169556
I0306 02:22:20.076829 139780020979456 logging_writer.py:48] [44300] global_step=44300, grad_norm=0.1961093693971634, loss=1.6979459524154663
I0306 02:22:55.116667 139780012586752 logging_writer.py:48] [44400] global_step=44400, grad_norm=0.21292932331562042, loss=1.819920301437378
I0306 02:23:30.140137 139780020979456 logging_writer.py:48] [44500] global_step=44500, grad_norm=0.22672514617443085, loss=1.7757123708724976
I0306 02:24:05.243521 139780012586752 logging_writer.py:48] [44600] global_step=44600, grad_norm=0.21610157191753387, loss=1.739508032798767
I0306 02:24:40.278565 139780020979456 logging_writer.py:48] [44700] global_step=44700, grad_norm=0.20206236839294434, loss=1.7446157932281494
I0306 02:25:15.337862 139780012586752 logging_writer.py:48] [44800] global_step=44800, grad_norm=0.20784121751785278, loss=1.743557095527649
I0306 02:25:50.327619 139780020979456 logging_writer.py:48] [44900] global_step=44900, grad_norm=0.21238815784454346, loss=1.870628833770752
I0306 02:26:25.358820 139780012586752 logging_writer.py:48] [45000] global_step=45000, grad_norm=0.19457753002643585, loss=1.784506916999817
I0306 02:27:00.430674 139780020979456 logging_writer.py:48] [45100] global_step=45100, grad_norm=0.20803867280483246, loss=1.6545568704605103
I0306 02:27:35.460478 139780012586752 logging_writer.py:48] [45200] global_step=45200, grad_norm=0.2118823379278183, loss=1.6737350225448608
I0306 02:28:10.492516 139780020979456 logging_writer.py:48] [45300] global_step=45300, grad_norm=0.18799802660942078, loss=1.8468652963638306
I0306 02:28:45.542990 139780012586752 logging_writer.py:48] [45400] global_step=45400, grad_norm=0.18521127104759216, loss=1.732425570487976
I0306 02:29:20.552871 139780020979456 logging_writer.py:48] [45500] global_step=45500, grad_norm=0.19178465008735657, loss=1.667504906654358
I0306 02:29:25.832860 139923228669120 spec.py:321] Evaluating on the training split.
I0306 02:29:28.452405 139923228669120 workload.py:181] Translating evaluation dataset.
I0306 02:33:37.172604 139923228669120 spec.py:333] Evaluating on the validation split.
I0306 02:33:39.787679 139923228669120 workload.py:181] Translating evaluation dataset.
I0306 02:36:38.566842 139923228669120 spec.py:349] Evaluating on the test split.
I0306 02:36:41.186582 139923228669120 workload.py:181] Translating evaluation dataset.
I0306 02:39:27.455698 139923228669120 submission_runner.py:469] Time since start: 26793.19s, 	Step: 45516, 	{'train/accuracy': 0.6550936698913574, 'train/loss': 1.6347965002059937, 'train/bleu': 32.895621831135855, 'validation/accuracy': 0.6682199239730835, 'validation/loss': 1.5399497747421265, 'validation/bleu': 28.751288121452326, 'validation/num_examples': 3000, 'test/accuracy': 0.6820762157440186, 'test/loss': 1.4533430337905884, 'test/bleu': 28.27365313955735, 'test/num_examples': 3003, 'score': 15997.006652116776, 'total_duration': 26793.19411945343, 'accumulated_submission_time': 15997.006652116776, 'accumulated_eval_time': 10793.432191848755, 'accumulated_logging_time': 0.35938525199890137}
I0306 02:39:27.469015 139780012586752 logging_writer.py:48] [45516] accumulated_eval_time=10793.4, accumulated_logging_time=0.359385, accumulated_submission_time=15997, global_step=45516, preemption_count=0, score=15997, test/accuracy=0.682076, test/bleu=28.2737, test/loss=1.45334, test/num_examples=3003, total_duration=26793.2, train/accuracy=0.655094, train/bleu=32.8956, train/loss=1.6348, validation/accuracy=0.66822, validation/bleu=28.7513, validation/loss=1.53995, validation/num_examples=3000
I0306 02:39:57.288969 139780020979456 logging_writer.py:48] [45600] global_step=45600, grad_norm=0.22771281003952026, loss=1.7112929821014404
I0306 02:40:32.332954 139780012586752 logging_writer.py:48] [45700] global_step=45700, grad_norm=0.2791268825531006, loss=1.7306530475616455
I0306 02:41:07.350599 139780020979456 logging_writer.py:48] [45800] global_step=45800, grad_norm=0.18358543515205383, loss=1.759069561958313
I0306 02:41:42.371358 139780012586752 logging_writer.py:48] [45900] global_step=45900, grad_norm=0.192862868309021, loss=1.7078596353530884
I0306 02:42:17.393554 139780020979456 logging_writer.py:48] [46000] global_step=46000, grad_norm=0.24198220670223236, loss=1.7563785314559937
I0306 02:42:52.447784 139780012586752 logging_writer.py:48] [46100] global_step=46100, grad_norm=0.19066490232944489, loss=1.6918294429779053
I0306 02:43:27.468885 139780020979456 logging_writer.py:48] [46200] global_step=46200, grad_norm=0.20308127999305725, loss=1.8286315202713013
I0306 02:44:02.487002 139780012586752 logging_writer.py:48] [46300] global_step=46300, grad_norm=0.2070370614528656, loss=1.7932792901992798
I0306 02:44:37.536004 139780020979456 logging_writer.py:48] [46400] global_step=46400, grad_norm=0.19921651482582092, loss=1.7847610712051392
I0306 02:45:12.570866 139780012586752 logging_writer.py:48] [46500] global_step=46500, grad_norm=0.185006245970726, loss=1.7001609802246094
I0306 02:45:47.601583 139780020979456 logging_writer.py:48] [46600] global_step=46600, grad_norm=0.24115276336669922, loss=1.7387508153915405
I0306 02:46:22.628467 139780012586752 logging_writer.py:48] [46700] global_step=46700, grad_norm=0.19081473350524902, loss=1.6631544828414917
I0306 02:46:57.640200 139780020979456 logging_writer.py:48] [46800] global_step=46800, grad_norm=0.20181754231452942, loss=1.811268925666809
I0306 02:47:32.674884 139780012586752 logging_writer.py:48] [46900] global_step=46900, grad_norm=0.2325466275215149, loss=1.7822712659835815
I0306 02:48:07.711771 139780020979456 logging_writer.py:48] [47000] global_step=47000, grad_norm=0.20872771739959717, loss=1.7208456993103027
I0306 02:48:42.755198 139780012586752 logging_writer.py:48] [47100] global_step=47100, grad_norm=0.21502730250358582, loss=1.7391058206558228
I0306 02:49:17.805264 139780020979456 logging_writer.py:48] [47200] global_step=47200, grad_norm=0.1838502585887909, loss=1.743851661682129
I0306 02:49:52.820665 139780012586752 logging_writer.py:48] [47300] global_step=47300, grad_norm=0.2106025367975235, loss=1.8228522539138794
I0306 02:50:27.844107 139780020979456 logging_writer.py:48] [47400] global_step=47400, grad_norm=0.21575826406478882, loss=1.6771788597106934
I0306 02:51:02.892210 139780012586752 logging_writer.py:48] [47500] global_step=47500, grad_norm=0.24741148948669434, loss=1.7430428266525269
I0306 02:51:37.920279 139780020979456 logging_writer.py:48] [47600] global_step=47600, grad_norm=0.19889308512210846, loss=1.7919094562530518
I0306 02:52:12.937266 139780012586752 logging_writer.py:48] [47700] global_step=47700, grad_norm=0.18656742572784424, loss=1.7675472497940063
I0306 02:52:47.998513 139780020979456 logging_writer.py:48] [47800] global_step=47800, grad_norm=0.4794306457042694, loss=1.808400273323059
I0306 02:53:23.034240 139780012586752 logging_writer.py:48] [47900] global_step=47900, grad_norm=0.20955057442188263, loss=1.7561075687408447
I0306 02:53:27.588877 139923228669120 spec.py:321] Evaluating on the training split.
I0306 02:53:30.216749 139923228669120 workload.py:181] Translating evaluation dataset.
I0306 02:57:37.002272 139923228669120 spec.py:333] Evaluating on the validation split.
I0306 02:57:39.628773 139923228669120 workload.py:181] Translating evaluation dataset.
I0306 03:00:22.742281 139923228669120 spec.py:349] Evaluating on the test split.
I0306 03:00:25.370086 139923228669120 workload.py:181] Translating evaluation dataset.
I0306 03:03:09.404573 139923228669120 submission_runner.py:469] Time since start: 28215.14s, 	Step: 47914, 	{'train/accuracy': 0.6518192291259766, 'train/loss': 1.6568970680236816, 'train/bleu': 31.86256812427604, 'validation/accuracy': 0.6698514223098755, 'validation/loss': 1.534536600112915, 'validation/bleu': 28.766959118771258, 'validation/num_examples': 3000, 'test/accuracy': 0.6813926696777344, 'test/loss': 1.452790379524231, 'test/bleu': 28.21117258492336, 'test/num_examples': 3003, 'score': 16836.99485516548, 'total_duration': 28215.143009662628, 'accumulated_submission_time': 16836.99485516548, 'accumulated_eval_time': 11375.247840166092, 'accumulated_logging_time': 0.3805251121520996}
I0306 03:03:09.417832 139780020979456 logging_writer.py:48] [47914] accumulated_eval_time=11375.2, accumulated_logging_time=0.380525, accumulated_submission_time=16837, global_step=47914, preemption_count=0, score=16837, test/accuracy=0.681393, test/bleu=28.2112, test/loss=1.45279, test/num_examples=3003, total_duration=28215.1, train/accuracy=0.651819, train/bleu=31.8626, train/loss=1.6569, validation/accuracy=0.669851, validation/bleu=28.767, validation/loss=1.53454, validation/num_examples=3000
I0306 03:03:39.955605 139780012586752 logging_writer.py:48] [48000] global_step=48000, grad_norm=0.18440788984298706, loss=1.7538446187973022
I0306 03:04:14.973821 139780020979456 logging_writer.py:48] [48100] global_step=48100, grad_norm=0.19730143249034882, loss=1.7695469856262207
I0306 03:04:50.013971 139780012586752 logging_writer.py:48] [48200] global_step=48200, grad_norm=0.19867946207523346, loss=1.7819470167160034
I0306 03:05:25.027105 139780020979456 logging_writer.py:48] [48300] global_step=48300, grad_norm=0.20513567328453064, loss=1.7378343343734741
I0306 03:06:00.073421 139780012586752 logging_writer.py:48] [48400] global_step=48400, grad_norm=0.18097048997879028, loss=1.8024851083755493
I0306 03:06:35.161402 139780020979456 logging_writer.py:48] [48500] global_step=48500, grad_norm=0.2106582671403885, loss=1.6883559226989746
I0306 03:07:10.211513 139780012586752 logging_writer.py:48] [48600] global_step=48600, grad_norm=0.19154679775238037, loss=1.600355625152588
I0306 03:07:45.258881 139780020979456 logging_writer.py:48] [48700] global_step=48700, grad_norm=0.30692076683044434, loss=1.7581881284713745
I0306 03:08:20.293972 139780012586752 logging_writer.py:48] [48800] global_step=48800, grad_norm=0.18214933574199677, loss=1.7287964820861816
I0306 03:08:55.322492 139780020979456 logging_writer.py:48] [48900] global_step=48900, grad_norm=0.19680893421173096, loss=1.6906810998916626
I0306 03:09:30.327045 139780012586752 logging_writer.py:48] [49000] global_step=49000, grad_norm=0.2116047888994217, loss=1.6966580152511597
I0306 03:10:05.379523 139780020979456 logging_writer.py:48] [49100] global_step=49100, grad_norm=0.22173410654067993, loss=1.7164499759674072
I0306 03:10:40.441779 139780012586752 logging_writer.py:48] [49200] global_step=49200, grad_norm=0.19877247512340546, loss=1.758658528327942
I0306 03:11:15.500325 139780020979456 logging_writer.py:48] [49300] global_step=49300, grad_norm=0.19715218245983124, loss=1.7479419708251953
I0306 03:11:50.517348 139780012586752 logging_writer.py:48] [49400] global_step=49400, grad_norm=0.21253810822963715, loss=1.712382197380066
I0306 03:12:25.560357 139780020979456 logging_writer.py:48] [49500] global_step=49500, grad_norm=0.1857522875070572, loss=1.7986363172531128
I0306 03:13:00.608692 139780012586752 logging_writer.py:48] [49600] global_step=49600, grad_norm=0.2078821063041687, loss=1.7816160917282104
I0306 03:13:35.638610 139780020979456 logging_writer.py:48] [49700] global_step=49700, grad_norm=0.19239582121372223, loss=1.654741883277893
I0306 03:14:10.644688 139780012586752 logging_writer.py:48] [49800] global_step=49800, grad_norm=0.20801562070846558, loss=1.7215216159820557
I0306 03:14:45.650665 139780020979456 logging_writer.py:48] [49900] global_step=49900, grad_norm=0.20225770771503448, loss=1.7053687572479248
I0306 03:15:20.635630 139780012586752 logging_writer.py:48] [50000] global_step=50000, grad_norm=0.21016645431518555, loss=1.6872036457061768
I0306 03:15:55.628243 139780020979456 logging_writer.py:48] [50100] global_step=50100, grad_norm=0.21876192092895508, loss=1.6976268291473389
I0306 03:16:30.645337 139780012586752 logging_writer.py:48] [50200] global_step=50200, grad_norm=0.2049085646867752, loss=1.748741865158081
I0306 03:17:05.684649 139780020979456 logging_writer.py:48] [50300] global_step=50300, grad_norm=0.2199113368988037, loss=1.6876200437545776
I0306 03:17:09.548356 139923228669120 spec.py:321] Evaluating on the training split.
I0306 03:17:12.170835 139923228669120 workload.py:181] Translating evaluation dataset.
I0306 03:20:58.327722 139923228669120 spec.py:333] Evaluating on the validation split.
I0306 03:21:00.941957 139923228669120 workload.py:181] Translating evaluation dataset.
I0306 03:23:33.297868 139923228669120 spec.py:349] Evaluating on the test split.
I0306 03:23:35.923463 139923228669120 workload.py:181] Translating evaluation dataset.
I0306 03:25:59.028823 139923228669120 submission_runner.py:469] Time since start: 29584.77s, 	Step: 50312, 	{'train/accuracy': 0.6775093078613281, 'train/loss': 1.4744209051132202, 'train/bleu': 33.85640050881976, 'validation/accuracy': 0.6707289814949036, 'validation/loss': 1.521946668624878, 'validation/bleu': 29.179102111317444, 'validation/num_examples': 3000, 'test/accuracy': 0.6835940480232239, 'test/loss': 1.4394971132278442, 'test/bleu': 28.46143191418479, 'test/num_examples': 3003, 'score': 17676.996727466583, 'total_duration': 29584.7672662735, 'accumulated_submission_time': 17676.996727466583, 'accumulated_eval_time': 11904.728259086609, 'accumulated_logging_time': 0.40130114555358887}
I0306 03:25:59.041796 139780012586752 logging_writer.py:48] [50312] accumulated_eval_time=11904.7, accumulated_logging_time=0.401301, accumulated_submission_time=17677, global_step=50312, preemption_count=0, score=17677, test/accuracy=0.683594, test/bleu=28.4614, test/loss=1.4395, test/num_examples=3003, total_duration=29584.8, train/accuracy=0.677509, train/bleu=33.8564, train/loss=1.47442, validation/accuracy=0.670729, validation/bleu=29.1791, validation/loss=1.52195, validation/num_examples=3000
I0306 03:26:30.234960 139780020979456 logging_writer.py:48] [50400] global_step=50400, grad_norm=0.18898653984069824, loss=1.6302616596221924
I0306 03:27:05.244422 139780012586752 logging_writer.py:48] [50500] global_step=50500, grad_norm=0.19513045251369476, loss=1.6962010860443115
I0306 03:27:40.267587 139780020979456 logging_writer.py:48] [50600] global_step=50600, grad_norm=0.2528964579105377, loss=1.7572561502456665
I0306 03:28:15.324601 139780012586752 logging_writer.py:48] [50700] global_step=50700, grad_norm=0.19171705842018127, loss=1.6904531717300415
I0306 03:28:50.353785 139780020979456 logging_writer.py:48] [50800] global_step=50800, grad_norm=0.20958739519119263, loss=1.693733811378479
I0306 03:29:25.395141 139780012586752 logging_writer.py:48] [50900] global_step=50900, grad_norm=0.22028078138828278, loss=1.7271850109100342
I0306 03:30:00.409388 139780020979456 logging_writer.py:48] [51000] global_step=51000, grad_norm=0.18190152943134308, loss=1.6510403156280518
I0306 03:30:35.453925 139780012586752 logging_writer.py:48] [51100] global_step=51100, grad_norm=0.21526339650154114, loss=1.7573187351226807
I0306 03:31:10.465074 139780020979456 logging_writer.py:48] [51200] global_step=51200, grad_norm=0.20486140251159668, loss=1.810552954673767
I0306 03:31:45.472250 139780012586752 logging_writer.py:48] [51300] global_step=51300, grad_norm=0.17870064079761505, loss=1.754655361175537
I0306 03:32:20.483676 139780020979456 logging_writer.py:48] [51400] global_step=51400, grad_norm=0.20579706132411957, loss=1.7720423936843872
I0306 03:32:55.471516 139780012586752 logging_writer.py:48] [51500] global_step=51500, grad_norm=0.23453742265701294, loss=1.732347011566162
I0306 03:33:30.476625 139780020979456 logging_writer.py:48] [51600] global_step=51600, grad_norm=0.20894230902194977, loss=1.7510954141616821
I0306 03:34:05.507206 139780012586752 logging_writer.py:48] [51700] global_step=51700, grad_norm=1.154741883277893, loss=1.7480409145355225
I0306 03:34:40.480071 139780020979456 logging_writer.py:48] [51800] global_step=51800, grad_norm=0.20111283659934998, loss=1.6103986501693726
I0306 03:35:15.492072 139780012586752 logging_writer.py:48] [51900] global_step=51900, grad_norm=0.2102397382259369, loss=1.685311198234558
I0306 03:35:50.519320 139780020979456 logging_writer.py:48] [52000] global_step=52000, grad_norm=0.18973208963871002, loss=1.707573652267456
I0306 03:36:25.538197 139780012586752 logging_writer.py:48] [52100] global_step=52100, grad_norm=0.19553759694099426, loss=1.6505018472671509
I0306 03:37:00.588845 139780020979456 logging_writer.py:48] [52200] global_step=52200, grad_norm=0.1883976310491562, loss=1.7193305492401123
I0306 03:37:35.649899 139780012586752 logging_writer.py:48] [52300] global_step=52300, grad_norm=0.2117825299501419, loss=1.6526514291763306
I0306 03:38:10.693129 139780020979456 logging_writer.py:48] [52400] global_step=52400, grad_norm=0.21039070188999176, loss=1.710404872894287
I0306 03:38:45.727166 139780012586752 logging_writer.py:48] [52500] global_step=52500, grad_norm=0.19829949736595154, loss=1.7180577516555786
I0306 03:39:20.714413 139780020979456 logging_writer.py:48] [52600] global_step=52600, grad_norm=0.2165706604719162, loss=1.6610790491104126
I0306 03:39:55.736788 139780012586752 logging_writer.py:48] [52700] global_step=52700, grad_norm=0.18998636305332184, loss=1.751916766166687
I0306 03:39:59.244075 139923228669120 spec.py:321] Evaluating on the training split.
I0306 03:40:01.864339 139923228669120 workload.py:181] Translating evaluation dataset.
I0306 03:43:55.595391 139923228669120 spec.py:333] Evaluating on the validation split.
I0306 03:43:58.211687 139923228669120 workload.py:181] Translating evaluation dataset.
I0306 03:47:48.032824 139923228669120 spec.py:349] Evaluating on the test split.
I0306 03:47:50.648059 139923228669120 workload.py:181] Translating evaluation dataset.
I0306 03:52:03.634741 139923228669120 submission_runner.py:469] Time since start: 31149.37s, 	Step: 52711, 	{'train/accuracy': 0.6563645601272583, 'train/loss': 1.620229721069336, 'train/bleu': 31.95549662113436, 'validation/accuracy': 0.6721504330635071, 'validation/loss': 1.5149569511413574, 'validation/bleu': 28.721798817041176, 'validation/num_examples': 3000, 'test/accuracy': 0.6860734820365906, 'test/loss': 1.4287725687026978, 'test/bleu': 28.548794208699487, 'test/num_examples': 3003, 'score': 18517.072224140167, 'total_duration': 31149.373165607452, 'accumulated_submission_time': 18517.072224140167, 'accumulated_eval_time': 12629.118858575821, 'accumulated_logging_time': 0.4219939708709717}
I0306 03:52:03.648323 139780020979456 logging_writer.py:48] [52711] accumulated_eval_time=12629.1, accumulated_logging_time=0.421994, accumulated_submission_time=18517.1, global_step=52711, preemption_count=0, score=18517.1, test/accuracy=0.686073, test/bleu=28.5488, test/loss=1.42877, test/num_examples=3003, total_duration=31149.4, train/accuracy=0.656365, train/bleu=31.9555, train/loss=1.62023, validation/accuracy=0.67215, validation/bleu=28.7218, validation/loss=1.51496, validation/num_examples=3000
I0306 03:52:35.206070 139780012586752 logging_writer.py:48] [52800] global_step=52800, grad_norm=0.23872758448123932, loss=1.6468768119812012
I0306 03:53:10.258369 139780020979456 logging_writer.py:48] [52900] global_step=52900, grad_norm=0.1978665292263031, loss=1.7459499835968018
I0306 03:53:45.288652 139780012586752 logging_writer.py:48] [53000] global_step=53000, grad_norm=0.18468883633613586, loss=1.707623839378357
I0306 03:54:20.333489 139780020979456 logging_writer.py:48] [53100] global_step=53100, grad_norm=0.19505570828914642, loss=1.6695386171340942
I0306 03:54:55.381071 139780012586752 logging_writer.py:48] [53200] global_step=53200, grad_norm=0.18859311938285828, loss=1.6868952512741089
I0306 03:55:30.423812 139780020979456 logging_writer.py:48] [53300] global_step=53300, grad_norm=0.183559387922287, loss=1.7600889205932617
I0306 03:56:05.477766 139780012586752 logging_writer.py:48] [53400] global_step=53400, grad_norm=0.20173509418964386, loss=1.7160340547561646
I0306 03:56:40.502957 139780020979456 logging_writer.py:48] [53500] global_step=53500, grad_norm=0.1985335499048233, loss=1.6777641773223877
I0306 03:57:15.534947 139780012586752 logging_writer.py:48] [53600] global_step=53600, grad_norm=0.188372403383255, loss=1.689030408859253
I0306 03:57:50.593878 139780020979456 logging_writer.py:48] [53700] global_step=53700, grad_norm=0.1899641454219818, loss=1.7671141624450684
I0306 03:58:25.661784 139780012586752 logging_writer.py:48] [53800] global_step=53800, grad_norm=0.19390136003494263, loss=1.7166141271591187
I0306 03:59:00.658227 139780020979456 logging_writer.py:48] [53900] global_step=53900, grad_norm=0.24366441369056702, loss=1.6673285961151123
I0306 03:59:35.717607 139780012586752 logging_writer.py:48] [54000] global_step=54000, grad_norm=0.21111595630645752, loss=1.7104370594024658
I0306 04:00:10.757737 139780020979456 logging_writer.py:48] [54100] global_step=54100, grad_norm=0.19849899411201477, loss=1.769531488418579
I0306 04:00:45.781163 139780012586752 logging_writer.py:48] [54200] global_step=54200, grad_norm=0.20748858153820038, loss=1.7491716146469116
I0306 04:01:20.843162 139780020979456 logging_writer.py:48] [54300] global_step=54300, grad_norm=0.21290245652198792, loss=1.7650097608566284
I0306 04:01:55.885206 139780012586752 logging_writer.py:48] [54400] global_step=54400, grad_norm=0.2598041594028473, loss=1.7882742881774902
I0306 04:02:30.926627 139780020979456 logging_writer.py:48] [54500] global_step=54500, grad_norm=0.20901605486869812, loss=1.6963810920715332
I0306 04:03:05.951181 139780012586752 logging_writer.py:48] [54600] global_step=54600, grad_norm=0.21972699463367462, loss=1.6509753465652466
I0306 04:03:40.985977 139780020979456 logging_writer.py:48] [54700] global_step=54700, grad_norm=0.18570683896541595, loss=1.7650797367095947
I0306 04:04:16.020495 139780012586752 logging_writer.py:48] [54800] global_step=54800, grad_norm=0.19989706575870514, loss=1.7393109798431396
I0306 04:04:51.043724 139780020979456 logging_writer.py:48] [54900] global_step=54900, grad_norm=0.2570386528968811, loss=1.708116888999939
I0306 04:05:26.086861 139780012586752 logging_writer.py:48] [55000] global_step=55000, grad_norm=0.2974483370780945, loss=1.7620235681533813
I0306 04:06:01.106047 139780020979456 logging_writer.py:48] [55100] global_step=55100, grad_norm=0.24571305513381958, loss=1.708237648010254
I0306 04:06:03.919782 139923228669120 spec.py:321] Evaluating on the training split.
I0306 04:06:06.548085 139923228669120 workload.py:181] Translating evaluation dataset.
I0306 04:09:42.850173 139923228669120 spec.py:333] Evaluating on the validation split.
I0306 04:09:45.474456 139923228669120 workload.py:181] Translating evaluation dataset.
I0306 04:12:26.330583 139923228669120 spec.py:349] Evaluating on the test split.
I0306 04:12:28.957885 139923228669120 workload.py:181] Translating evaluation dataset.
I0306 04:15:31.584827 139923228669120 submission_runner.py:469] Time since start: 32557.32s, 	Step: 55109, 	{'train/accuracy': 0.6550964117050171, 'train/loss': 1.634689211845398, 'train/bleu': 31.88800653363787, 'validation/accuracy': 0.6731762886047363, 'validation/loss': 1.5070106983184814, 'validation/bleu': 28.990632081884506, 'validation/num_examples': 3000, 'test/accuracy': 0.6839763522148132, 'test/loss': 1.4271059036254883, 'test/bleu': 28.36306301564741, 'test/num_examples': 3003, 'score': 19357.213113307953, 'total_duration': 32557.32327413559, 'accumulated_submission_time': 19357.213113307953, 'accumulated_eval_time': 13196.783860206604, 'accumulated_logging_time': 0.4434971809387207}
I0306 04:15:31.598339 139780012586752 logging_writer.py:48] [55109] accumulated_eval_time=13196.8, accumulated_logging_time=0.443497, accumulated_submission_time=19357.2, global_step=55109, preemption_count=0, score=19357.2, test/accuracy=0.683976, test/bleu=28.3631, test/loss=1.42711, test/num_examples=3003, total_duration=32557.3, train/accuracy=0.655096, train/bleu=31.888, train/loss=1.63469, validation/accuracy=0.673176, validation/bleu=28.9906, validation/loss=1.50701, validation/num_examples=3000
I0306 04:16:03.858154 139780020979456 logging_writer.py:48] [55200] global_step=55200, grad_norm=0.19463330507278442, loss=1.7349116802215576
I0306 04:16:38.878212 139780012586752 logging_writer.py:48] [55300] global_step=55300, grad_norm=0.19875283539295197, loss=1.669385313987732
I0306 04:17:13.889632 139780020979456 logging_writer.py:48] [55400] global_step=55400, grad_norm=0.20584926009178162, loss=1.7577682733535767
I0306 04:17:48.885766 139780012586752 logging_writer.py:48] [55500] global_step=55500, grad_norm=0.19730640947818756, loss=1.6964359283447266
I0306 04:18:23.906194 139780020979456 logging_writer.py:48] [55600] global_step=55600, grad_norm=0.2014234960079193, loss=1.6653064489364624
I0306 04:18:58.971975 139780012586752 logging_writer.py:48] [55700] global_step=55700, grad_norm=0.19898656010627747, loss=1.6836342811584473
I0306 04:19:34.022767 139780020979456 logging_writer.py:48] [55800] global_step=55800, grad_norm=0.19665925204753876, loss=1.7220743894577026
I0306 04:20:09.003119 139780012586752 logging_writer.py:48] [55900] global_step=55900, grad_norm=0.22084306180477142, loss=1.7204707860946655
I0306 04:20:44.045341 139780020979456 logging_writer.py:48] [56000] global_step=56000, grad_norm=0.20482493937015533, loss=1.6347805261611938
I0306 04:21:19.074435 139780012586752 logging_writer.py:48] [56100] global_step=56100, grad_norm=0.22409497201442719, loss=1.6938915252685547
I0306 04:21:54.126856 139780020979456 logging_writer.py:48] [56200] global_step=56200, grad_norm=0.18784359097480774, loss=1.6611605882644653
I0306 04:22:29.162342 139780012586752 logging_writer.py:48] [56300] global_step=56300, grad_norm=0.23620599508285522, loss=1.6586376428604126
I0306 04:23:04.196235 139780020979456 logging_writer.py:48] [56400] global_step=56400, grad_norm=0.18954896926879883, loss=1.6481914520263672
I0306 04:23:39.218028 139780012586752 logging_writer.py:48] [56500] global_step=56500, grad_norm=0.20969276130199432, loss=1.673258900642395
I0306 04:24:14.205560 139780020979456 logging_writer.py:48] [56600] global_step=56600, grad_norm=0.18718905746936798, loss=1.7031278610229492
I0306 04:24:49.177477 139780012586752 logging_writer.py:48] [56700] global_step=56700, grad_norm=0.200934037566185, loss=1.6893869638442993
I0306 04:25:24.144340 139780020979456 logging_writer.py:48] [56800] global_step=56800, grad_norm=0.22171460092067719, loss=1.6577131748199463
I0306 04:25:59.116774 139780012586752 logging_writer.py:48] [56900] global_step=56900, grad_norm=0.21006064116954803, loss=1.727414608001709
I0306 04:26:34.071359 139780020979456 logging_writer.py:48] [57000] global_step=57000, grad_norm=0.200197234749794, loss=1.6467292308807373
I0306 04:27:09.035159 139780012586752 logging_writer.py:48] [57100] global_step=57100, grad_norm=0.19999630749225616, loss=1.750643014907837
I0306 04:27:43.974681 139780020979456 logging_writer.py:48] [57200] global_step=57200, grad_norm=0.18548060953617096, loss=1.748374581336975
I0306 04:28:18.936765 139780012586752 logging_writer.py:48] [57300] global_step=57300, grad_norm=0.1928093284368515, loss=1.7386175394058228
I0306 04:28:53.876066 139780020979456 logging_writer.py:48] [57400] global_step=57400, grad_norm=0.19752025604248047, loss=1.6951220035552979
I0306 04:29:28.843330 139780012586752 logging_writer.py:48] [57500] global_step=57500, grad_norm=0.24096590280532837, loss=1.6935499906539917
I0306 04:29:31.635738 139923228669120 spec.py:321] Evaluating on the training split.
I0306 04:29:34.263943 139923228669120 workload.py:181] Translating evaluation dataset.
I0306 04:33:30.146983 139923228669120 spec.py:333] Evaluating on the validation split.
I0306 04:33:32.773056 139923228669120 workload.py:181] Translating evaluation dataset.
I0306 04:37:42.410051 139923228669120 spec.py:349] Evaluating on the test split.
I0306 04:37:45.042387 139923228669120 workload.py:181] Translating evaluation dataset.
I0306 04:41:48.131425 139923228669120 submission_runner.py:469] Time since start: 34133.87s, 	Step: 57509, 	{'train/accuracy': 0.6644532680511475, 'train/loss': 1.5597906112670898, 'train/bleu': 32.7555638423294, 'validation/accuracy': 0.6729537844657898, 'validation/loss': 1.502052903175354, 'validation/bleu': 28.726654201228463, 'validation/num_examples': 3000, 'test/accuracy': 0.6869887709617615, 'test/loss': 1.4207853078842163, 'test/bleu': 28.454174511035983, 'test/num_examples': 3003, 'score': 20197.1210668087, 'total_duration': 34133.86986708641, 'accumulated_submission_time': 20197.1210668087, 'accumulated_eval_time': 13933.279499053955, 'accumulated_logging_time': 0.46510958671569824}
I0306 04:41:48.145554 139780020979456 logging_writer.py:48] [57509] accumulated_eval_time=13933.3, accumulated_logging_time=0.46511, accumulated_submission_time=20197.1, global_step=57509, preemption_count=0, score=20197.1, test/accuracy=0.686989, test/bleu=28.4542, test/loss=1.42079, test/num_examples=3003, total_duration=34133.9, train/accuracy=0.664453, train/bleu=32.7556, train/loss=1.55979, validation/accuracy=0.672954, validation/bleu=28.7267, validation/loss=1.50205, validation/num_examples=3000
I0306 04:42:20.370018 139780012586752 logging_writer.py:48] [57600] global_step=57600, grad_norm=0.22490811347961426, loss=1.7200815677642822
I0306 04:42:55.335169 139780020979456 logging_writer.py:48] [57700] global_step=57700, grad_norm=0.19509364664554596, loss=1.6760607957839966
I0306 04:43:30.283981 139780012586752 logging_writer.py:48] [57800] global_step=57800, grad_norm=0.20719672739505768, loss=1.7550870180130005
I0306 04:44:05.213937 139780020979456 logging_writer.py:48] [57900] global_step=57900, grad_norm=0.2243155986070633, loss=1.653328537940979
I0306 04:44:40.157715 139780012586752 logging_writer.py:48] [58000] global_step=58000, grad_norm=0.197041854262352, loss=1.746080994606018
I0306 04:45:15.094429 139780020979456 logging_writer.py:48] [58100] global_step=58100, grad_norm=0.21382950246334076, loss=1.7109755277633667
I0306 04:45:50.062830 139780012586752 logging_writer.py:48] [58200] global_step=58200, grad_norm=1.381420612335205, loss=1.7176663875579834
I0306 04:46:25.012811 139780020979456 logging_writer.py:48] [58300] global_step=58300, grad_norm=0.22060802578926086, loss=1.7155888080596924
I0306 04:46:59.982038 139780012586752 logging_writer.py:48] [58400] global_step=58400, grad_norm=0.19880050420761108, loss=1.6960233449935913
I0306 04:47:34.915738 139780020979456 logging_writer.py:48] [58500] global_step=58500, grad_norm=0.19641785323619843, loss=1.6452971696853638
I0306 04:48:09.898605 139780012586752 logging_writer.py:48] [58600] global_step=58600, grad_norm=0.20175813138484955, loss=1.7156120538711548
I0306 04:48:44.867352 139780020979456 logging_writer.py:48] [58700] global_step=58700, grad_norm=0.20427878201007843, loss=1.7102278470993042
I0306 04:49:19.837470 139780012586752 logging_writer.py:48] [58800] global_step=58800, grad_norm=0.20557929575443268, loss=1.6977919340133667
I0306 04:49:54.794533 139780020979456 logging_writer.py:48] [58900] global_step=58900, grad_norm=0.2071671187877655, loss=1.6724964380264282
I0306 04:50:29.810024 139780012586752 logging_writer.py:48] [59000] global_step=59000, grad_norm=0.19392366707324982, loss=1.7769851684570312
I0306 04:51:04.833270 139780020979456 logging_writer.py:48] [59100] global_step=59100, grad_norm=0.19585458934307098, loss=1.714237928390503
I0306 04:51:39.781213 139780012586752 logging_writer.py:48] [59200] global_step=59200, grad_norm=0.19092914462089539, loss=1.7279253005981445
I0306 04:52:14.748010 139780020979456 logging_writer.py:48] [59300] global_step=59300, grad_norm=0.22251850366592407, loss=1.6912598609924316
I0306 04:52:49.712808 139780012586752 logging_writer.py:48] [59400] global_step=59400, grad_norm=0.3252007067203522, loss=1.6539057493209839
I0306 04:53:24.670923 139780020979456 logging_writer.py:48] [59500] global_step=59500, grad_norm=0.1884121149778366, loss=1.704525351524353
I0306 04:53:59.635522 139780012586752 logging_writer.py:48] [59600] global_step=59600, grad_norm=0.1910366415977478, loss=1.7173043489456177
I0306 04:54:34.574640 139780020979456 logging_writer.py:48] [59700] global_step=59700, grad_norm=0.1845390498638153, loss=1.6574801206588745
I0306 04:55:09.528845 139780012586752 logging_writer.py:48] [59800] global_step=59800, grad_norm=0.18723751604557037, loss=1.7308467626571655
I0306 04:55:44.496449 139780020979456 logging_writer.py:48] [59900] global_step=59900, grad_norm=0.18270771205425262, loss=1.6763112545013428
I0306 04:55:48.348876 139923228669120 spec.py:321] Evaluating on the training split.
I0306 04:55:50.975859 139923228669120 workload.py:181] Translating evaluation dataset.
I0306 04:59:20.941520 139923228669120 spec.py:333] Evaluating on the validation split.
I0306 04:59:23.567493 139923228669120 workload.py:181] Translating evaluation dataset.
I0306 05:03:24.391216 139923228669120 spec.py:349] Evaluating on the test split.
I0306 05:03:27.010001 139923228669120 workload.py:181] Translating evaluation dataset.
I0306 05:06:44.602097 139923228669120 submission_runner.py:469] Time since start: 35630.34s, 	Step: 59912, 	{'train/accuracy': 0.6640447974205017, 'train/loss': 1.5789340734481812, 'train/bleu': 32.332880526690204, 'validation/accuracy': 0.6756235957145691, 'validation/loss': 1.4878644943237305, 'validation/bleu': 29.040579486490106, 'validation/num_examples': 3000, 'test/accuracy': 0.6891205906867981, 'test/loss': 1.4018596410751343, 'test/bleu': 28.867091049338526, 'test/num_examples': 3003, 'score': 21037.191143751144, 'total_duration': 35630.34053969383, 'accumulated_submission_time': 21037.191143751144, 'accumulated_eval_time': 14589.532675504684, 'accumulated_logging_time': 0.48888468742370605}
I0306 05:06:44.616002 139780012586752 logging_writer.py:48] [59912] accumulated_eval_time=14589.5, accumulated_logging_time=0.488885, accumulated_submission_time=21037.2, global_step=59912, preemption_count=0, score=21037.2, test/accuracy=0.689121, test/bleu=28.8671, test/loss=1.40186, test/num_examples=3003, total_duration=35630.3, train/accuracy=0.664045, train/bleu=32.3329, train/loss=1.57893, validation/accuracy=0.675624, validation/bleu=29.0406, validation/loss=1.48786, validation/num_examples=3000
I0306 05:07:15.813316 139780020979456 logging_writer.py:48] [60000] global_step=60000, grad_norm=0.2124425321817398, loss=1.6634750366210938
I0306 05:07:50.738575 139780012586752 logging_writer.py:48] [60100] global_step=60100, grad_norm=0.19798468053340912, loss=1.7130144834518433
I0306 05:08:25.700230 139780020979456 logging_writer.py:48] [60200] global_step=60200, grad_norm=0.20506255328655243, loss=1.7504236698150635
I0306 05:09:00.677637 139780012586752 logging_writer.py:48] [60300] global_step=60300, grad_norm=0.192231222987175, loss=1.6750370264053345
I0306 05:09:35.660956 139780020979456 logging_writer.py:48] [60400] global_step=60400, grad_norm=0.20647066831588745, loss=1.7150684595108032
I0306 05:10:10.620728 139780012586752 logging_writer.py:48] [60500] global_step=60500, grad_norm=0.19361746311187744, loss=1.6912870407104492
I0306 05:10:45.622616 139780020979456 logging_writer.py:48] [60600] global_step=60600, grad_norm=0.19953426718711853, loss=1.6974976062774658
I0306 05:11:20.573720 139780012586752 logging_writer.py:48] [60700] global_step=60700, grad_norm=0.2191282957792282, loss=1.706172227859497
I0306 05:11:55.526296 139780020979456 logging_writer.py:48] [60800] global_step=60800, grad_norm=0.20861726999282837, loss=1.7299668788909912
I0306 05:12:30.501523 139780012586752 logging_writer.py:48] [60900] global_step=60900, grad_norm=0.20597612857818604, loss=1.70682692527771
I0306 05:13:05.442539 139780020979456 logging_writer.py:48] [61000] global_step=61000, grad_norm=0.20391178131103516, loss=1.7569235563278198
I0306 05:13:40.363778 139780012586752 logging_writer.py:48] [61100] global_step=61100, grad_norm=0.21710516512393951, loss=1.7005480527877808
I0306 05:14:15.289604 139780020979456 logging_writer.py:48] [61200] global_step=61200, grad_norm=0.21594947576522827, loss=1.6542481184005737
I0306 05:14:50.272935 139780012586752 logging_writer.py:48] [61300] global_step=61300, grad_norm=0.2106337696313858, loss=1.6917957067489624
I0306 05:15:25.215581 139780020979456 logging_writer.py:48] [61400] global_step=61400, grad_norm=0.22220821678638458, loss=1.7074322700500488
I0306 05:16:00.159899 139780012586752 logging_writer.py:48] [61500] global_step=61500, grad_norm=0.2106950879096985, loss=1.7297242879867554
I0306 05:16:35.104799 139780020979456 logging_writer.py:48] [61600] global_step=61600, grad_norm=0.20587694644927979, loss=1.6890766620635986
I0306 05:17:10.052287 139780012586752 logging_writer.py:48] [61700] global_step=61700, grad_norm=0.19304221868515015, loss=1.6700215339660645
I0306 05:17:45.005296 139780020979456 logging_writer.py:48] [61800] global_step=61800, grad_norm=0.19791708886623383, loss=1.6082987785339355
I0306 05:18:19.947885 139780012586752 logging_writer.py:48] [61900] global_step=61900, grad_norm=0.1965225338935852, loss=1.6171578168869019
I0306 05:18:54.887202 139780020979456 logging_writer.py:48] [62000] global_step=62000, grad_norm=0.20089320838451385, loss=1.7059332132339478
I0306 05:19:29.812073 139780012586752 logging_writer.py:48] [62100] global_step=62100, grad_norm=0.2139604538679123, loss=1.636865258216858
I0306 05:20:04.782291 139780020979456 logging_writer.py:48] [62200] global_step=62200, grad_norm=0.1850111186504364, loss=1.6602445840835571
I0306 05:20:39.710341 139780012586752 logging_writer.py:48] [62300] global_step=62300, grad_norm=0.23922613263130188, loss=1.785871982574463
I0306 05:20:44.606376 139923228669120 spec.py:321] Evaluating on the training split.
I0306 05:20:47.234083 139923228669120 workload.py:181] Translating evaluation dataset.
I0306 05:24:04.360116 139923228669120 spec.py:333] Evaluating on the validation split.
I0306 05:24:06.979869 139923228669120 workload.py:181] Translating evaluation dataset.
I0306 05:26:40.278762 139923228669120 spec.py:349] Evaluating on the test split.
I0306 05:26:42.906444 139923228669120 workload.py:181] Translating evaluation dataset.
I0306 05:29:15.267316 139923228669120 submission_runner.py:469] Time since start: 36981.01s, 	Step: 62315, 	{'train/accuracy': 0.6565195918083191, 'train/loss': 1.621074914932251, 'train/bleu': 32.54478324472036, 'validation/accuracy': 0.6752527952194214, 'validation/loss': 1.4893181324005127, 'validation/bleu': 29.201330005934174, 'validation/num_examples': 3000, 'test/accuracy': 0.6880083680152893, 'test/loss': 1.4031387567520142, 'test/bleu': 28.933209828443132, 'test/num_examples': 3003, 'score': 21877.0513548851, 'total_duration': 36981.00573134422, 'accumulated_submission_time': 21877.0513548851, 'accumulated_eval_time': 15100.193541765213, 'accumulated_logging_time': 0.5105130672454834}
I0306 05:29:15.283991 139780020979456 logging_writer.py:48] [62315] accumulated_eval_time=15100.2, accumulated_logging_time=0.510513, accumulated_submission_time=21877.1, global_step=62315, preemption_count=0, score=21877.1, test/accuracy=0.688008, test/bleu=28.9332, test/loss=1.40314, test/num_examples=3003, total_duration=36981, train/accuracy=0.65652, train/bleu=32.5448, train/loss=1.62107, validation/accuracy=0.675253, validation/bleu=29.2013, validation/loss=1.48932, validation/num_examples=3000
I0306 05:29:45.351416 139780012586752 logging_writer.py:48] [62400] global_step=62400, grad_norm=0.2165140062570572, loss=1.7105082273483276
I0306 05:30:20.311669 139780020979456 logging_writer.py:48] [62500] global_step=62500, grad_norm=0.20357877016067505, loss=1.6948862075805664
I0306 05:30:55.236101 139780012586752 logging_writer.py:48] [62600] global_step=62600, grad_norm=0.2043738216161728, loss=1.692923903465271
I0306 05:31:30.189369 139780020979456 logging_writer.py:48] [62700] global_step=62700, grad_norm=0.1897559016942978, loss=1.7750672101974487
I0306 05:32:05.175927 139780012586752 logging_writer.py:48] [62800] global_step=62800, grad_norm=0.2003135085105896, loss=1.6729183197021484
I0306 05:32:40.213562 139780020979456 logging_writer.py:48] [62900] global_step=62900, grad_norm=0.20538124442100525, loss=1.617832064628601
I0306 05:33:15.239802 139780012586752 logging_writer.py:48] [63000] global_step=63000, grad_norm=0.20058093965053558, loss=1.6657485961914062
I0306 05:33:50.258670 139780020979456 logging_writer.py:48] [63100] global_step=63100, grad_norm=0.20997685194015503, loss=1.6780073642730713
I0306 05:34:25.307168 139780012586752 logging_writer.py:48] [63200] global_step=63200, grad_norm=0.1959517002105713, loss=1.6539945602416992
I0306 05:35:00.349367 139780020979456 logging_writer.py:48] [63300] global_step=63300, grad_norm=0.19825589656829834, loss=1.6556648015975952
I0306 05:35:35.371912 139780012586752 logging_writer.py:48] [63400] global_step=63400, grad_norm=0.1976274847984314, loss=1.7635786533355713
I0306 05:36:10.388903 139780020979456 logging_writer.py:48] [63500] global_step=63500, grad_norm=0.19622673094272614, loss=1.7255135774612427
I0306 05:36:45.488862 139780012586752 logging_writer.py:48] [63600] global_step=63600, grad_norm=0.20754288136959076, loss=1.6117351055145264
I0306 05:37:20.525692 139780020979456 logging_writer.py:48] [63700] global_step=63700, grad_norm=0.19124121963977814, loss=1.6391466856002808
I0306 05:37:55.577048 139780012586752 logging_writer.py:48] [63800] global_step=63800, grad_norm=0.18597646057605743, loss=1.6348748207092285
I0306 05:38:30.607402 139780020979456 logging_writer.py:48] [63900] global_step=63900, grad_norm=0.2332753688097, loss=1.643090009689331
I0306 05:39:05.603439 139780012586752 logging_writer.py:48] [64000] global_step=64000, grad_norm=0.20530296862125397, loss=1.6236432790756226
I0306 05:39:40.660065 139780020979456 logging_writer.py:48] [64100] global_step=64100, grad_norm=0.19252170622348785, loss=1.6733293533325195
I0306 05:40:15.694882 139780012586752 logging_writer.py:48] [64200] global_step=64200, grad_norm=0.20338407158851624, loss=1.719628095626831
I0306 05:40:50.695745 139780020979456 logging_writer.py:48] [64300] global_step=64300, grad_norm=0.22268342971801758, loss=1.5905539989471436
I0306 05:41:25.733867 139780012586752 logging_writer.py:48] [64400] global_step=64400, grad_norm=0.2010044902563095, loss=1.6263115406036377
I0306 05:42:00.758424 139780020979456 logging_writer.py:48] [64500] global_step=64500, grad_norm=0.21047227084636688, loss=1.6726676225662231
I0306 05:42:35.810968 139780012586752 logging_writer.py:48] [64600] global_step=64600, grad_norm=0.2143888920545578, loss=1.6733036041259766
I0306 05:43:10.914199 139780020979456 logging_writer.py:48] [64700] global_step=64700, grad_norm=0.20301057398319244, loss=1.6035912036895752
I0306 05:43:15.475589 139923228669120 spec.py:321] Evaluating on the training split.
I0306 05:43:18.095382 139923228669120 workload.py:181] Translating evaluation dataset.
I0306 05:47:01.027519 139923228669120 spec.py:333] Evaluating on the validation split.
I0306 05:47:03.651869 139923228669120 workload.py:181] Translating evaluation dataset.
I0306 05:50:02.642046 139923228669120 spec.py:349] Evaluating on the test split.
I0306 05:50:05.258359 139923228669120 workload.py:181] Translating evaluation dataset.
I0306 05:53:41.631722 139923228669120 submission_runner.py:469] Time since start: 38447.37s, 	Step: 64714, 	{'train/accuracy': 0.6664870977401733, 'train/loss': 1.5533335208892822, 'train/bleu': 32.840460124877104, 'validation/accuracy': 0.6770697236061096, 'validation/loss': 1.4772292375564575, 'validation/bleu': 29.686800295739342, 'validation/num_examples': 3000, 'test/accuracy': 0.6918317675590515, 'test/loss': 1.3866792917251587, 'test/bleu': 28.976123742140455, 'test/num_examples': 3003, 'score': 22717.11159682274, 'total_duration': 38447.370161533356, 'accumulated_submission_time': 22717.11159682274, 'accumulated_eval_time': 15726.3496260643, 'accumulated_logging_time': 0.535804271697998}
I0306 05:53:41.646977 139780012586752 logging_writer.py:48] [64714] accumulated_eval_time=15726.3, accumulated_logging_time=0.535804, accumulated_submission_time=22717.1, global_step=64714, preemption_count=0, score=22717.1, test/accuracy=0.691832, test/bleu=28.9761, test/loss=1.38668, test/num_examples=3003, total_duration=38447.4, train/accuracy=0.666487, train/bleu=32.8405, train/loss=1.55333, validation/accuracy=0.67707, validation/bleu=29.6868, validation/loss=1.47723, validation/num_examples=3000
I0306 05:54:12.149702 139780020979456 logging_writer.py:48] [64800] global_step=64800, grad_norm=0.1892162561416626, loss=1.6639201641082764
I0306 05:54:47.166753 139780012586752 logging_writer.py:48] [64900] global_step=64900, grad_norm=0.20004451274871826, loss=1.7030974626541138
I0306 05:55:22.203243 139780020979456 logging_writer.py:48] [65000] global_step=65000, grad_norm=0.19504882395267487, loss=1.6423882246017456
I0306 05:55:57.207848 139780012586752 logging_writer.py:48] [65100] global_step=65100, grad_norm=0.19705355167388916, loss=1.6688038110733032
I0306 05:56:32.219227 139780020979456 logging_writer.py:48] [65200] global_step=65200, grad_norm=0.19381999969482422, loss=1.692788004875183
I0306 05:57:07.254409 139780012586752 logging_writer.py:48] [65300] global_step=65300, grad_norm=0.1877838671207428, loss=1.6372572183609009
I0306 05:57:42.291585 139780020979456 logging_writer.py:48] [65400] global_step=65400, grad_norm=0.2122466117143631, loss=1.6446008682250977
I0306 05:58:17.309933 139780012586752 logging_writer.py:48] [65500] global_step=65500, grad_norm=0.20067580044269562, loss=1.6979154348373413
I0306 05:58:52.343287 139780020979456 logging_writer.py:48] [65600] global_step=65600, grad_norm=0.197182297706604, loss=1.6061458587646484
I0306 05:59:27.367421 139780012586752 logging_writer.py:48] [65700] global_step=65700, grad_norm=0.20085550844669342, loss=1.7245217561721802
I0306 06:00:02.416475 139780020979456 logging_writer.py:48] [65800] global_step=65800, grad_norm=0.2098369002342224, loss=1.6155842542648315
I0306 06:00:37.454259 139780012586752 logging_writer.py:48] [65900] global_step=65900, grad_norm=0.21602264046669006, loss=1.6644514799118042
I0306 06:01:12.474081 139780020979456 logging_writer.py:48] [66000] global_step=66000, grad_norm=0.1999572217464447, loss=1.6167595386505127
I0306 06:01:47.501131 139780012586752 logging_writer.py:48] [66100] global_step=66100, grad_norm=0.18844673037528992, loss=1.7014106512069702
I0306 06:02:22.528581 139780020979456 logging_writer.py:48] [66200] global_step=66200, grad_norm=0.21071462333202362, loss=1.695135235786438
I0306 06:02:57.564702 139780012586752 logging_writer.py:48] [66300] global_step=66300, grad_norm=0.20033127069473267, loss=1.6379519701004028
I0306 06:03:32.602820 139780020979456 logging_writer.py:48] [66400] global_step=66400, grad_norm=0.22329860925674438, loss=1.6774297952651978
I0306 06:04:07.611870 139780012586752 logging_writer.py:48] [66500] global_step=66500, grad_norm=0.19938263297080994, loss=1.6935083866119385
I0306 06:04:42.669923 139780020979456 logging_writer.py:48] [66600] global_step=66600, grad_norm=0.1878724992275238, loss=1.5883153676986694
I0306 06:05:17.715515 139780012586752 logging_writer.py:48] [66700] global_step=66700, grad_norm=0.19576676189899445, loss=1.7194421291351318
I0306 06:05:52.741917 139780020979456 logging_writer.py:48] [66800] global_step=66800, grad_norm=0.19198593497276306, loss=1.6390706300735474
I0306 06:06:27.911138 139780012586752 logging_writer.py:48] [66900] global_step=66900, grad_norm=0.1921701282262802, loss=1.5561823844909668
I0306 06:07:02.986600 139780020979456 logging_writer.py:48] [67000] global_step=67000, grad_norm=0.2060176134109497, loss=1.6991370916366577
I0306 06:07:38.028592 139780012586752 logging_writer.py:48] [67100] global_step=67100, grad_norm=0.20878514647483826, loss=1.6540709733963013
I0306 06:07:41.892371 139923228669120 spec.py:321] Evaluating on the training split.
I0306 06:07:44.518863 139923228669120 workload.py:181] Translating evaluation dataset.
I0306 06:11:33.220433 139923228669120 spec.py:333] Evaluating on the validation split.
I0306 06:11:35.852166 139923228669120 workload.py:181] Translating evaluation dataset.
I0306 06:15:09.415346 139923228669120 spec.py:349] Evaluating on the test split.
I0306 06:15:12.042312 139923228669120 workload.py:181] Translating evaluation dataset.
I0306 06:18:42.809953 139923228669120 submission_runner.py:469] Time since start: 39948.55s, 	Step: 67112, 	{'train/accuracy': 0.6615347862243652, 'train/loss': 1.5865970849990845, 'train/bleu': 32.70543756268278, 'validation/accuracy': 0.6775022745132446, 'validation/loss': 1.4737094640731812, 'validation/bleu': 29.48211865455347, 'validation/num_examples': 3000, 'test/accuracy': 0.6919012665748596, 'test/loss': 1.3798927068710327, 'test/bleu': 28.88451648651455, 'test/num_examples': 3003, 'score': 23557.22575187683, 'total_duration': 39948.54838728905, 'accumulated_submission_time': 23557.22575187683, 'accumulated_eval_time': 16387.26716184616, 'accumulated_logging_time': 0.5584750175476074}
I0306 06:18:42.827094 139780020979456 logging_writer.py:48] [67112] accumulated_eval_time=16387.3, accumulated_logging_time=0.558475, accumulated_submission_time=23557.2, global_step=67112, preemption_count=0, score=23557.2, test/accuracy=0.691901, test/bleu=28.8845, test/loss=1.37989, test/num_examples=3003, total_duration=39948.5, train/accuracy=0.661535, train/bleu=32.7054, train/loss=1.5866, validation/accuracy=0.677502, validation/bleu=29.4821, validation/loss=1.47371, validation/num_examples=3000
I0306 06:19:14.094106 139780012586752 logging_writer.py:48] [67200] global_step=67200, grad_norm=0.1889420747756958, loss=1.6018325090408325
I0306 06:19:49.134542 139780020979456 logging_writer.py:48] [67300] global_step=67300, grad_norm=0.18667994439601898, loss=1.5978996753692627
I0306 06:20:24.205773 139780012586752 logging_writer.py:48] [67400] global_step=67400, grad_norm=0.19363293051719666, loss=1.7342240810394287
I0306 06:20:59.275727 139780020979456 logging_writer.py:48] [67500] global_step=67500, grad_norm=0.1938880980014801, loss=1.6617460250854492
I0306 06:21:34.313751 139780012586752 logging_writer.py:48] [67600] global_step=67600, grad_norm=0.18937011063098907, loss=1.6495331525802612
I0306 06:22:09.401284 139780020979456 logging_writer.py:48] [67700] global_step=67700, grad_norm=0.20931918919086456, loss=1.6673661470413208
I0306 06:22:44.396955 139780012586752 logging_writer.py:48] [67800] global_step=67800, grad_norm=0.23786278069019318, loss=1.6137268543243408
I0306 06:23:19.465812 139780020979456 logging_writer.py:48] [67900] global_step=67900, grad_norm=0.2035517394542694, loss=1.6196691989898682
I0306 06:23:54.483035 139780012586752 logging_writer.py:48] [68000] global_step=68000, grad_norm=0.21289876103401184, loss=1.7014977931976318
I0306 06:24:29.514730 139780020979456 logging_writer.py:48] [68100] global_step=68100, grad_norm=0.23559916019439697, loss=1.7307969331741333
I0306 06:25:04.548957 139780012586752 logging_writer.py:48] [68200] global_step=68200, grad_norm=0.19983553886413574, loss=1.670471429824829
I0306 06:25:39.557350 139780020979456 logging_writer.py:48] [68300] global_step=68300, grad_norm=0.1991698443889618, loss=1.6636316776275635
I0306 06:26:14.591551 139780012586752 logging_writer.py:48] [68400] global_step=68400, grad_norm=0.2038177251815796, loss=1.6954134702682495
I0306 06:26:49.669510 139780020979456 logging_writer.py:48] [68500] global_step=68500, grad_norm=0.21683895587921143, loss=1.653149962425232
I0306 06:27:24.716890 139780012586752 logging_writer.py:48] [68600] global_step=68600, grad_norm=0.21295179426670074, loss=1.7491462230682373
I0306 06:27:59.743072 139780020979456 logging_writer.py:48] [68700] global_step=68700, grad_norm=0.27251192927360535, loss=1.6037949323654175
I0306 06:28:34.787613 139780012586752 logging_writer.py:48] [68800] global_step=68800, grad_norm=0.27657732367515564, loss=1.6583846807479858
I0306 06:29:09.818831 139780020979456 logging_writer.py:48] [68900] global_step=68900, grad_norm=0.1989099383354187, loss=1.7072579860687256
I0306 06:29:44.852818 139780012586752 logging_writer.py:48] [69000] global_step=69000, grad_norm=0.22080183029174805, loss=1.6681126356124878
I0306 06:30:19.900607 139780020979456 logging_writer.py:48] [69100] global_step=69100, grad_norm=0.22711992263793945, loss=1.6305058002471924
I0306 06:30:54.983172 139780012586752 logging_writer.py:48] [69200] global_step=69200, grad_norm=0.21966835856437683, loss=1.696427345275879
I0306 06:31:30.019188 139780020979456 logging_writer.py:48] [69300] global_step=69300, grad_norm=0.18500514328479767, loss=1.5997049808502197
I0306 06:32:05.042776 139780012586752 logging_writer.py:48] [69400] global_step=69400, grad_norm=0.19507284462451935, loss=1.5981727838516235
I0306 06:32:40.064803 139780020979456 logging_writer.py:48] [69500] global_step=69500, grad_norm=0.21247196197509766, loss=1.6559228897094727
I0306 06:32:42.876065 139923228669120 spec.py:321] Evaluating on the training split.
I0306 06:32:45.496703 139923228669120 workload.py:181] Translating evaluation dataset.
I0306 06:36:31.097498 139923228669120 spec.py:333] Evaluating on the validation split.
I0306 06:36:33.712251 139923228669120 workload.py:181] Translating evaluation dataset.
I0306 06:40:11.248860 139923228669120 spec.py:349] Evaluating on the test split.
I0306 06:40:13.870927 139923228669120 workload.py:181] Translating evaluation dataset.
I0306 06:43:20.107923 139923228669120 submission_runner.py:469] Time since start: 41425.85s, 	Step: 69509, 	{'train/accuracy': 0.6801171898841858, 'train/loss': 1.4684436321258545, 'train/bleu': 34.06425304577874, 'validation/accuracy': 0.6802833080291748, 'validation/loss': 1.4586493968963623, 'validation/bleu': 29.7233410746792, 'validation/num_examples': 3000, 'test/accuracy': 0.6941953301429749, 'test/loss': 1.3733872175216675, 'test/bleu': 28.914641427505625, 'test/num_examples': 3003, 'score': 24397.13818502426, 'total_duration': 41425.846347332, 'accumulated_submission_time': 24397.13818502426, 'accumulated_eval_time': 17024.49896645546, 'accumulated_logging_time': 0.5844078063964844}
I0306 06:43:20.123651 139780012586752 logging_writer.py:48] [69509] accumulated_eval_time=17024.5, accumulated_logging_time=0.584408, accumulated_submission_time=24397.1, global_step=69509, preemption_count=0, score=24397.1, test/accuracy=0.694195, test/bleu=28.9146, test/loss=1.37339, test/num_examples=3003, total_duration=41425.8, train/accuracy=0.680117, train/bleu=34.0643, train/loss=1.46844, validation/accuracy=0.680283, validation/bleu=29.7233, validation/loss=1.45865, validation/num_examples=3000
I0306 06:43:52.458528 139780020979456 logging_writer.py:48] [69600] global_step=69600, grad_norm=0.19681775569915771, loss=1.6484228372573853
I0306 06:44:27.470100 139780012586752 logging_writer.py:48] [69700] global_step=69700, grad_norm=0.2132585644721985, loss=1.6939499378204346
I0306 06:45:02.500129 139780020979456 logging_writer.py:48] [69800] global_step=69800, grad_norm=0.1864873766899109, loss=1.6255536079406738
I0306 06:45:37.545159 139780012586752 logging_writer.py:48] [69900] global_step=69900, grad_norm=0.19513706862926483, loss=1.605348825454712
I0306 06:46:12.584325 139780020979456 logging_writer.py:48] [70000] global_step=70000, grad_norm=0.20250050723552704, loss=1.6011286973953247
I0306 06:46:47.632426 139780012586752 logging_writer.py:48] [70100] global_step=70100, grad_norm=0.19712021946907043, loss=1.6538925170898438
I0306 06:47:22.653317 139780020979456 logging_writer.py:48] [70200] global_step=70200, grad_norm=0.1960352063179016, loss=1.5689669847488403
I0306 06:47:57.659562 139780012586752 logging_writer.py:48] [70300] global_step=70300, grad_norm=0.2056160569190979, loss=1.6575738191604614
I0306 06:48:32.688884 139780020979456 logging_writer.py:48] [70400] global_step=70400, grad_norm=0.1864418238401413, loss=1.5997490882873535
I0306 06:49:07.709258 139780012586752 logging_writer.py:48] [70500] global_step=70500, grad_norm=0.2110821157693863, loss=1.6697996854782104
I0306 06:49:42.770542 139780020979456 logging_writer.py:48] [70600] global_step=70600, grad_norm=0.19859972596168518, loss=1.5626590251922607
I0306 06:50:17.820825 139780012586752 logging_writer.py:48] [70700] global_step=70700, grad_norm=0.22942781448364258, loss=1.6220133304595947
I0306 06:50:52.869051 139780020979456 logging_writer.py:48] [70800] global_step=70800, grad_norm=0.21328400075435638, loss=1.607518196105957
I0306 06:51:27.853480 139780012586752 logging_writer.py:48] [70900] global_step=70900, grad_norm=0.21045857667922974, loss=1.6265257596969604
I0306 06:52:02.899284 139780020979456 logging_writer.py:48] [71000] global_step=71000, grad_norm=0.19544914364814758, loss=1.6447312831878662
I0306 06:52:37.938719 139780012586752 logging_writer.py:48] [71100] global_step=71100, grad_norm=0.19883114099502563, loss=1.6101032495498657
I0306 06:53:12.996496 139780020979456 logging_writer.py:48] [71200] global_step=71200, grad_norm=0.22594869136810303, loss=1.6049882173538208
I0306 06:53:48.030078 139780012586752 logging_writer.py:48] [71300] global_step=71300, grad_norm=0.18974924087524414, loss=1.676335334777832
I0306 06:54:23.088879 139780020979456 logging_writer.py:48] [71400] global_step=71400, grad_norm=0.21413873136043549, loss=1.6403087377548218
I0306 06:54:58.157274 139780012586752 logging_writer.py:48] [71500] global_step=71500, grad_norm=0.1887991577386856, loss=1.6486254930496216
I0306 06:55:33.207812 139780020979456 logging_writer.py:48] [71600] global_step=71600, grad_norm=0.1932860016822815, loss=1.6435188055038452
I0306 06:56:08.246602 139780012586752 logging_writer.py:48] [71700] global_step=71700, grad_norm=0.19845139980316162, loss=1.6504013538360596
I0306 06:56:43.273523 139780020979456 logging_writer.py:48] [71800] global_step=71800, grad_norm=0.23154248297214508, loss=1.6290839910507202
I0306 06:57:18.298861 139780012586752 logging_writer.py:48] [71900] global_step=71900, grad_norm=0.213883176445961, loss=1.6702402830123901
I0306 06:57:20.410187 139923228669120 spec.py:321] Evaluating on the training split.
I0306 06:57:23.035344 139923228669120 workload.py:181] Translating evaluation dataset.
I0306 07:01:42.488683 139923228669120 spec.py:333] Evaluating on the validation split.
I0306 07:01:45.114351 139923228669120 workload.py:181] Translating evaluation dataset.
I0306 07:05:16.403021 139923228669120 spec.py:349] Evaluating on the test split.
I0306 07:05:19.017515 139923228669120 workload.py:181] Translating evaluation dataset.
I0306 07:09:23.111306 139923228669120 submission_runner.py:469] Time since start: 42988.85s, 	Step: 71907, 	{'train/accuracy': 0.6680629849433899, 'train/loss': 1.5376898050308228, 'train/bleu': 33.55331250360029, 'validation/accuracy': 0.6821373105049133, 'validation/loss': 1.4566062688827515, 'validation/bleu': 29.792228137262903, 'validation/num_examples': 3000, 'test/accuracy': 0.6962113380432129, 'test/loss': 1.367648720741272, 'test/bleu': 29.21917840567236, 'test/num_examples': 3003, 'score': 25237.290956258774, 'total_duration': 42988.84974718094, 'accumulated_submission_time': 25237.290956258774, 'accumulated_eval_time': 17747.200035572052, 'accumulated_logging_time': 0.6085481643676758}
I0306 07:09:23.127552 139780020979456 logging_writer.py:48] [71907] accumulated_eval_time=17747.2, accumulated_logging_time=0.608548, accumulated_submission_time=25237.3, global_step=71907, preemption_count=0, score=25237.3, test/accuracy=0.696211, test/bleu=29.2192, test/loss=1.36765, test/num_examples=3003, total_duration=42988.8, train/accuracy=0.668063, train/bleu=33.5533, train/loss=1.53769, validation/accuracy=0.682137, validation/bleu=29.7922, validation/loss=1.45661, validation/num_examples=3000
I0306 07:09:56.382002 139780012586752 logging_writer.py:48] [72000] global_step=72000, grad_norm=0.22137020528316498, loss=1.6770542860031128
I0306 07:10:31.573269 139780020979456 logging_writer.py:48] [72100] global_step=72100, grad_norm=0.22431901097297668, loss=1.6815677881240845
I0306 07:11:06.778135 139780012586752 logging_writer.py:48] [72200] global_step=72200, grad_norm=0.22552049160003662, loss=1.706516981124878
I0306 07:11:41.957023 139780020979456 logging_writer.py:48] [72300] global_step=72300, grad_norm=0.2077319324016571, loss=1.5217695236206055
I0306 07:12:17.149664 139780012586752 logging_writer.py:48] [72400] global_step=72400, grad_norm=0.1903630644083023, loss=1.5185540914535522
I0306 07:12:52.356484 139780020979456 logging_writer.py:48] [72500] global_step=72500, grad_norm=0.20607802271842957, loss=1.5799657106399536
I0306 07:13:27.506324 139780012586752 logging_writer.py:48] [72600] global_step=72600, grad_norm=0.21641063690185547, loss=1.5862579345703125
I0306 07:14:02.696579 139780020979456 logging_writer.py:48] [72700] global_step=72700, grad_norm=0.21036143600940704, loss=1.661658525466919
I0306 07:14:37.919617 139780012586752 logging_writer.py:48] [72800] global_step=72800, grad_norm=0.1995488852262497, loss=1.6309219598770142
I0306 07:15:13.122014 139780020979456 logging_writer.py:48] [72900] global_step=72900, grad_norm=0.18770836293697357, loss=1.5784205198287964
I0306 07:15:48.316566 139780012586752 logging_writer.py:48] [73000] global_step=73000, grad_norm=0.20954269170761108, loss=1.651188611984253
I0306 07:16:23.536575 139780020979456 logging_writer.py:48] [73100] global_step=73100, grad_norm=0.2150583565235138, loss=1.5836663246154785
I0306 07:16:58.729469 139780012586752 logging_writer.py:48] [73200] global_step=73200, grad_norm=0.19062325358390808, loss=1.623384714126587
I0306 07:17:33.892553 139780020979456 logging_writer.py:48] [73300] global_step=73300, grad_norm=1.2159075736999512, loss=1.6905437707901
I0306 07:18:09.056564 139780012586752 logging_writer.py:48] [73400] global_step=73400, grad_norm=0.20560277998447418, loss=1.5685715675354004
I0306 07:18:44.229980 139780020979456 logging_writer.py:48] [73500] global_step=73500, grad_norm=0.22006340324878693, loss=1.6609914302825928
I0306 07:19:19.423593 139780012586752 logging_writer.py:48] [73600] global_step=73600, grad_norm=0.2023378610610962, loss=1.675450325012207
I0306 07:19:54.587948 139780020979456 logging_writer.py:48] [73700] global_step=73700, grad_norm=0.2010365128517151, loss=1.5734703540802002
I0306 07:20:29.755941 139780012586752 logging_writer.py:48] [73800] global_step=73800, grad_norm=0.18478114902973175, loss=1.515234351158142
I0306 07:21:04.925732 139780020979456 logging_writer.py:48] [73900] global_step=73900, grad_norm=0.5056021213531494, loss=1.6227794885635376
I0306 07:21:40.113013 139780012586752 logging_writer.py:48] [74000] global_step=74000, grad_norm=0.20442920923233032, loss=1.633201003074646
I0306 07:22:15.305671 139780020979456 logging_writer.py:48] [74100] global_step=74100, grad_norm=0.20232349634170532, loss=1.6880543231964111
I0306 07:22:50.527058 139780012586752 logging_writer.py:48] [74200] global_step=74200, grad_norm=0.20682787895202637, loss=1.6446973085403442
I0306 07:23:23.260889 139923228669120 spec.py:321] Evaluating on the training split.
I0306 07:23:25.904662 139923228669120 workload.py:181] Translating evaluation dataset.
I0306 07:26:46.069350 139923228669120 spec.py:333] Evaluating on the validation split.
I0306 07:26:48.706080 139923228669120 workload.py:181] Translating evaluation dataset.
I0306 07:29:18.429499 139923228669120 spec.py:349] Evaluating on the test split.
I0306 07:29:21.060760 139923228669120 workload.py:181] Translating evaluation dataset.
I0306 07:31:41.803094 139923228669120 submission_runner.py:469] Time since start: 44327.54s, 	Step: 74294, 	{'train/accuracy': 0.6646674275398254, 'train/loss': 1.5587682723999023, 'train/bleu': 33.263302911492694, 'validation/accuracy': 0.6828912496566772, 'validation/loss': 1.446242094039917, 'validation/bleu': 30.071268519250367, 'validation/num_examples': 3000, 'test/accuracy': 0.6959795951843262, 'test/loss': 1.3576279878616333, 'test/bleu': 29.49128885370752, 'test/num_examples': 3003, 'score': 26077.19045162201, 'total_duration': 44327.541499853134, 'accumulated_submission_time': 26077.19045162201, 'accumulated_eval_time': 18245.742166519165, 'accumulated_logging_time': 0.7365462779998779}
I0306 07:31:41.820813 139780020979456 logging_writer.py:48] [74294] accumulated_eval_time=18245.7, accumulated_logging_time=0.736546, accumulated_submission_time=26077.2, global_step=74294, preemption_count=0, score=26077.2, test/accuracy=0.69598, test/bleu=29.4913, test/loss=1.35763, test/num_examples=3003, total_duration=44327.5, train/accuracy=0.664667, train/bleu=33.2633, train/loss=1.55877, validation/accuracy=0.682891, validation/bleu=30.0713, validation/loss=1.44624, validation/num_examples=3000
I0306 07:31:44.286018 139780012586752 logging_writer.py:48] [74300] global_step=74300, grad_norm=0.22565972805023193, loss=1.583559274673462
I0306 07:32:19.552808 139780020979456 logging_writer.py:48] [74400] global_step=74400, grad_norm=0.19938315451145172, loss=1.6329163312911987
I0306 07:32:54.719110 139780012586752 logging_writer.py:48] [74500] global_step=74500, grad_norm=0.205659419298172, loss=1.6297595500946045
I0306 07:33:29.863567 139780020979456 logging_writer.py:48] [74600] global_step=74600, grad_norm=0.21111136674880981, loss=1.685660481452942
I0306 07:34:05.028778 139780012586752 logging_writer.py:48] [74700] global_step=74700, grad_norm=0.2031734138727188, loss=1.579350471496582
I0306 07:34:40.236124 139780020979456 logging_writer.py:48] [74800] global_step=74800, grad_norm=0.20002023875713348, loss=1.5749322175979614
I0306 07:35:15.422442 139780012586752 logging_writer.py:48] [74900] global_step=74900, grad_norm=0.19891802966594696, loss=1.6064459085464478
I0306 07:35:50.638876 139780020979456 logging_writer.py:48] [75000] global_step=75000, grad_norm=0.20940005779266357, loss=1.6117371320724487
I0306 07:36:25.797480 139780012586752 logging_writer.py:48] [75100] global_step=75100, grad_norm=0.20324285328388214, loss=1.5757699012756348
I0306 07:37:01.004737 139780020979456 logging_writer.py:48] [75200] global_step=75200, grad_norm=0.20395323634147644, loss=1.6504230499267578
I0306 07:37:36.157932 139780012586752 logging_writer.py:48] [75300] global_step=75300, grad_norm=0.2167837768793106, loss=1.5995436906814575
I0306 07:38:11.245721 139780020979456 logging_writer.py:48] [75400] global_step=75400, grad_norm=0.23290890455245972, loss=1.6596428155899048
I0306 07:38:46.302240 139780012586752 logging_writer.py:48] [75500] global_step=75500, grad_norm=0.1984807401895523, loss=1.6212549209594727
I0306 07:39:21.359187 139780020979456 logging_writer.py:48] [75600] global_step=75600, grad_norm=0.20307980477809906, loss=1.5959175825119019
I0306 07:39:56.425767 139780012586752 logging_writer.py:48] [75700] global_step=75700, grad_norm=0.20363211631774902, loss=1.6308717727661133
I0306 07:40:31.494414 139780020979456 logging_writer.py:48] [75800] global_step=75800, grad_norm=0.1982126384973526, loss=1.5927397012710571
I0306 07:41:06.593654 139780012586752 logging_writer.py:48] [75900] global_step=75900, grad_norm=0.1980879008769989, loss=1.6341373920440674
I0306 07:41:41.665958 139780020979456 logging_writer.py:48] [76000] global_step=76000, grad_norm=0.20253713428974152, loss=1.5626779794692993
I0306 07:42:16.768691 139780012586752 logging_writer.py:48] [76100] global_step=76100, grad_norm=0.21850107610225677, loss=1.6061054468154907
I0306 07:42:51.900447 139780020979456 logging_writer.py:48] [76200] global_step=76200, grad_norm=0.2107912003993988, loss=1.6430743932724
I0306 07:43:26.990978 139780012586752 logging_writer.py:48] [76300] global_step=76300, grad_norm=0.21124497056007385, loss=1.576810359954834
I0306 07:44:02.046799 139780020979456 logging_writer.py:48] [76400] global_step=76400, grad_norm=0.2073921263217926, loss=1.6623241901397705
I0306 07:44:37.120889 139780012586752 logging_writer.py:48] [76500] global_step=76500, grad_norm=0.1971990317106247, loss=1.5442174673080444
I0306 07:45:12.209505 139780020979456 logging_writer.py:48] [76600] global_step=76600, grad_norm=0.2039290815591812, loss=1.6130149364471436
I0306 07:45:42.001499 139923228669120 spec.py:321] Evaluating on the training split.
I0306 07:45:44.636279 139923228669120 workload.py:181] Translating evaluation dataset.
I0306 07:49:57.195991 139923228669120 spec.py:333] Evaluating on the validation split.
I0306 07:49:59.823438 139923228669120 workload.py:181] Translating evaluation dataset.
I0306 07:53:37.004879 139923228669120 spec.py:349] Evaluating on the test split.
I0306 07:53:39.630326 139923228669120 workload.py:181] Translating evaluation dataset.
I0306 07:57:05.340242 139923228669120 submission_runner.py:469] Time since start: 45851.08s, 	Step: 76686, 	{'train/accuracy': 0.6756840944290161, 'train/loss': 1.4906566143035889, 'train/bleu': 34.0021745665475, 'validation/accuracy': 0.6824833750724792, 'validation/loss': 1.4404916763305664, 'validation/bleu': 29.66196269686816, 'validation/num_examples': 3000, 'test/accuracy': 0.6970686912536621, 'test/loss': 1.3505268096923828, 'test/bleu': 29.542162491351487, 'test/num_examples': 3003, 'score': 26917.24090409279, 'total_duration': 45851.07866358757, 'accumulated_submission_time': 26917.24090409279, 'accumulated_eval_time': 18929.08084154129, 'accumulated_logging_time': 0.7619938850402832}
I0306 07:57:05.360305 139780012586752 logging_writer.py:48] [76686] accumulated_eval_time=18929.1, accumulated_logging_time=0.761994, accumulated_submission_time=26917.2, global_step=76686, preemption_count=0, score=26917.2, test/accuracy=0.697069, test/bleu=29.5422, test/loss=1.35053, test/num_examples=3003, total_duration=45851.1, train/accuracy=0.675684, train/bleu=34.0022, train/loss=1.49066, validation/accuracy=0.682483, validation/bleu=29.662, validation/loss=1.44049, validation/num_examples=3000
I0306 07:57:10.636051 139780020979456 logging_writer.py:48] [76700] global_step=76700, grad_norm=0.23746073246002197, loss=1.6554664373397827
I0306 07:57:45.791380 139780012586752 logging_writer.py:48] [76800] global_step=76800, grad_norm=0.1911405324935913, loss=1.5560704469680786
I0306 07:58:20.838745 139780020979456 logging_writer.py:48] [76900] global_step=76900, grad_norm=0.2114042341709137, loss=1.6022326946258545
I0306 07:58:55.933946 139780012586752 logging_writer.py:48] [77000] global_step=77000, grad_norm=0.21011370420455933, loss=1.653458833694458
I0306 07:59:30.996481 139780020979456 logging_writer.py:48] [77100] global_step=77100, grad_norm=0.21796299517154694, loss=1.5716192722320557
I0306 08:00:06.071428 139780012586752 logging_writer.py:48] [77200] global_step=77200, grad_norm=0.20827218890190125, loss=1.6412183046340942
I0306 08:00:41.103975 139780020979456 logging_writer.py:48] [77300] global_step=77300, grad_norm=0.19037364423274994, loss=1.5303138494491577
I0306 08:01:16.212323 139780012586752 logging_writer.py:48] [77400] global_step=77400, grad_norm=0.20071396231651306, loss=1.6443275213241577
I0306 08:01:51.283564 139780020979456 logging_writer.py:48] [77500] global_step=77500, grad_norm=0.21695223450660706, loss=1.6070438623428345
I0306 08:02:26.364722 139780012586752 logging_writer.py:48] [77600] global_step=77600, grad_norm=0.19882379472255707, loss=1.6366889476776123
I0306 08:03:01.462899 139780020979456 logging_writer.py:48] [77700] global_step=77700, grad_norm=0.1983584314584732, loss=1.6113322973251343
I0306 08:03:36.578696 139780012586752 logging_writer.py:48] [77800] global_step=77800, grad_norm=0.24273672699928284, loss=1.6220823526382446
I0306 08:04:11.646751 139780020979456 logging_writer.py:48] [77900] global_step=77900, grad_norm=0.1979241669178009, loss=1.5242738723754883
I0306 08:04:46.744979 139780012586752 logging_writer.py:48] [78000] global_step=78000, grad_norm=0.18997329473495483, loss=1.5641413927078247
I0306 08:05:21.797588 139780020979456 logging_writer.py:48] [78100] global_step=78100, grad_norm=0.20057402551174164, loss=1.5696207284927368
I0306 08:05:56.869261 139780012586752 logging_writer.py:48] [78200] global_step=78200, grad_norm=0.20652194321155548, loss=1.6223061084747314
I0306 08:06:31.959975 139780020979456 logging_writer.py:48] [78300] global_step=78300, grad_norm=0.22359682619571686, loss=1.5483535528182983
I0306 08:07:07.056424 139780012586752 logging_writer.py:48] [78400] global_step=78400, grad_norm=0.20181399583816528, loss=1.6155047416687012
I0306 08:07:42.128098 139780020979456 logging_writer.py:48] [78500] global_step=78500, grad_norm=0.19825375080108643, loss=1.5725735425949097
I0306 08:08:17.197393 139780012586752 logging_writer.py:48] [78600] global_step=78600, grad_norm=0.2160915583372116, loss=1.59326171875
I0306 08:08:52.278785 139780020979456 logging_writer.py:48] [78700] global_step=78700, grad_norm=0.19521991908550262, loss=1.6586493253707886
I0306 08:09:27.335983 139780012586752 logging_writer.py:48] [78800] global_step=78800, grad_norm=0.1966308206319809, loss=1.4778573513031006
I0306 08:10:02.402789 139780020979456 logging_writer.py:48] [78900] global_step=78900, grad_norm=0.19442082941532135, loss=1.5237282514572144
I0306 08:10:37.491836 139780012586752 logging_writer.py:48] [79000] global_step=79000, grad_norm=0.21074938774108887, loss=1.6079519987106323
I0306 08:11:05.542972 139923228669120 spec.py:321] Evaluating on the training split.
I0306 08:11:08.178689 139923228669120 workload.py:181] Translating evaluation dataset.
I0306 08:15:11.562767 139923228669120 spec.py:333] Evaluating on the validation split.
I0306 08:15:14.190350 139923228669120 workload.py:181] Translating evaluation dataset.
I0306 08:19:02.588532 139923228669120 spec.py:349] Evaluating on the test split.
I0306 08:19:05.224284 139923228669120 workload.py:181] Translating evaluation dataset.
I0306 08:22:58.962751 139923228669120 submission_runner.py:469] Time since start: 47404.70s, 	Step: 79081, 	{'train/accuracy': 0.670806884765625, 'train/loss': 1.5277334451675415, 'train/bleu': 33.65061434912588, 'validation/accuracy': 0.6856722831726074, 'validation/loss': 1.4323469400405884, 'validation/bleu': 29.820535561879318, 'validation/num_examples': 3000, 'test/accuracy': 0.7002780437469482, 'test/loss': 1.332972526550293, 'test/bleu': 29.877358267681313, 'test/num_examples': 3003, 'score': 27757.291744709015, 'total_duration': 47404.70118904114, 'accumulated_submission_time': 27757.291744709015, 'accumulated_eval_time': 19642.50058221817, 'accumulated_logging_time': 0.7903580665588379}
I0306 08:22:58.980401 139780020979456 logging_writer.py:48] [79081] accumulated_eval_time=19642.5, accumulated_logging_time=0.790358, accumulated_submission_time=27757.3, global_step=79081, preemption_count=0, score=27757.3, test/accuracy=0.700278, test/bleu=29.8774, test/loss=1.33297, test/num_examples=3003, total_duration=47404.7, train/accuracy=0.670807, train/bleu=33.6506, train/loss=1.52773, validation/accuracy=0.685672, validation/bleu=29.8205, validation/loss=1.43235, validation/num_examples=3000
I0306 08:23:06.000096 139780012586752 logging_writer.py:48] [79100] global_step=79100, grad_norm=0.20223137736320496, loss=1.607893466949463
I0306 08:23:41.089607 139780020979456 logging_writer.py:48] [79200] global_step=79200, grad_norm=0.2092888355255127, loss=1.6186586618423462
I0306 08:24:16.160144 139780012586752 logging_writer.py:48] [79300] global_step=79300, grad_norm=0.1986614167690277, loss=1.5553189516067505
I0306 08:24:51.254764 139780020979456 logging_writer.py:48] [79400] global_step=79400, grad_norm=0.2001774162054062, loss=1.5583469867706299
I0306 08:25:26.326576 139780012586752 logging_writer.py:48] [79500] global_step=79500, grad_norm=0.21129290759563446, loss=1.612352967262268
I0306 08:26:01.428972 139780020979456 logging_writer.py:48] [79600] global_step=79600, grad_norm=0.19391091167926788, loss=1.540310263633728
I0306 08:26:36.501002 139780012586752 logging_writer.py:48] [79700] global_step=79700, grad_norm=0.1977175623178482, loss=1.5675932168960571
I0306 08:27:11.578440 139780020979456 logging_writer.py:48] [79800] global_step=79800, grad_norm=0.20037224888801575, loss=1.6633687019348145
I0306 08:27:46.688737 139780012586752 logging_writer.py:48] [79900] global_step=79900, grad_norm=0.2074425369501114, loss=1.5841662883758545
I0306 08:28:21.801532 139780020979456 logging_writer.py:48] [80000] global_step=80000, grad_norm=0.21058990061283112, loss=1.5781759023666382
I0306 08:28:56.867099 139780012586752 logging_writer.py:48] [80100] global_step=80100, grad_norm=0.2081996351480484, loss=1.5606966018676758
I0306 08:29:31.945218 139780020979456 logging_writer.py:48] [80200] global_step=80200, grad_norm=0.21663598716259003, loss=1.5871089696884155
I0306 08:30:07.036840 139780012586752 logging_writer.py:48] [80300] global_step=80300, grad_norm=0.2099158763885498, loss=1.5976635217666626
I0306 08:30:42.091044 139780020979456 logging_writer.py:48] [80400] global_step=80400, grad_norm=0.20576463639736176, loss=1.569994330406189
I0306 08:31:17.155283 139780012586752 logging_writer.py:48] [80500] global_step=80500, grad_norm=0.21320387721061707, loss=1.5355181694030762
I0306 08:31:52.212003 139780020979456 logging_writer.py:48] [80600] global_step=80600, grad_norm=0.1983218640089035, loss=1.6134568452835083
I0306 08:32:27.303879 139780012586752 logging_writer.py:48] [80700] global_step=80700, grad_norm=0.20197230577468872, loss=1.5873968601226807
I0306 08:33:02.368037 139780020979456 logging_writer.py:48] [80800] global_step=80800, grad_norm=0.21105368435382843, loss=1.5765576362609863
I0306 08:33:37.435477 139780012586752 logging_writer.py:48] [80900] global_step=80900, grad_norm=0.20514632761478424, loss=1.6671571731567383
I0306 08:34:12.496837 139780020979456 logging_writer.py:48] [81000] global_step=81000, grad_norm=0.24360418319702148, loss=1.5979377031326294
I0306 08:34:47.585990 139780012586752 logging_writer.py:48] [81100] global_step=81100, grad_norm=0.23101168870925903, loss=1.5535389184951782
I0306 08:35:22.667385 139780020979456 logging_writer.py:48] [81200] global_step=81200, grad_norm=0.22008831799030304, loss=1.5957669019699097
I0306 08:35:57.755178 139780012586752 logging_writer.py:48] [81300] global_step=81300, grad_norm=0.20249120891094208, loss=1.573254942893982
I0306 08:36:32.847388 139780020979456 logging_writer.py:48] [81400] global_step=81400, grad_norm=0.21451979875564575, loss=1.6091394424438477
I0306 08:36:59.148678 139923228669120 spec.py:321] Evaluating on the training split.
I0306 08:37:01.782250 139923228669120 workload.py:181] Translating evaluation dataset.
I0306 08:40:41.202208 139923228669120 spec.py:333] Evaluating on the validation split.
I0306 08:40:43.843719 139923228669120 workload.py:181] Translating evaluation dataset.
I0306 08:43:49.013264 139923228669120 spec.py:349] Evaluating on the test split.
I0306 08:43:51.639842 139923228669120 workload.py:181] Translating evaluation dataset.
I0306 08:47:21.386760 139923228669120 submission_runner.py:469] Time since start: 48867.13s, 	Step: 81476, 	{'train/accuracy': 0.6701619625091553, 'train/loss': 1.5271522998809814, 'train/bleu': 33.62950279115951, 'validation/accuracy': 0.6859689354896545, 'validation/loss': 1.4260343313217163, 'validation/bleu': 30.23455462547178, 'validation/num_examples': 3000, 'test/accuracy': 0.7002085447311401, 'test/loss': 1.3314127922058105, 'test/bleu': 29.82068094715, 'test/num_examples': 3003, 'score': 28597.327392578125, 'total_duration': 48867.12519478798, 'accumulated_submission_time': 28597.327392578125, 'accumulated_eval_time': 20264.73861527443, 'accumulated_logging_time': 0.8158400058746338}
I0306 08:47:21.403407 139780012586752 logging_writer.py:48] [81476] accumulated_eval_time=20264.7, accumulated_logging_time=0.81584, accumulated_submission_time=28597.3, global_step=81476, preemption_count=0, score=28597.3, test/accuracy=0.700209, test/bleu=29.8207, test/loss=1.33141, test/num_examples=3003, total_duration=48867.1, train/accuracy=0.670162, train/bleu=33.6295, train/loss=1.52715, validation/accuracy=0.685969, validation/bleu=30.2346, validation/loss=1.42603, validation/num_examples=3000
I0306 08:47:30.174557 139780020979456 logging_writer.py:48] [81500] global_step=81500, grad_norm=0.2027938812971115, loss=1.6032148599624634
I0306 08:48:05.268769 139780012586752 logging_writer.py:48] [81600] global_step=81600, grad_norm=0.20348027348518372, loss=1.6246813535690308
I0306 08:48:40.351457 139780020979456 logging_writer.py:48] [81700] global_step=81700, grad_norm=0.2424626499414444, loss=1.6509851217269897
I0306 08:49:15.424140 139780012586752 logging_writer.py:48] [81800] global_step=81800, grad_norm=0.21343810856342316, loss=1.6491484642028809
I0306 08:49:50.509753 139780020979456 logging_writer.py:48] [81900] global_step=81900, grad_norm=0.2188054919242859, loss=1.6190251111984253
I0306 08:50:25.548986 139780012586752 logging_writer.py:48] [82000] global_step=82000, grad_norm=0.21135808527469635, loss=1.5676019191741943
I0306 08:51:00.598945 139780020979456 logging_writer.py:48] [82100] global_step=82100, grad_norm=0.2022845447063446, loss=1.5511558055877686
I0306 08:51:35.659924 139780012586752 logging_writer.py:48] [82200] global_step=82200, grad_norm=0.21593578159809113, loss=1.6022135019302368
I0306 08:52:10.694337 139780020979456 logging_writer.py:48] [82300] global_step=82300, grad_norm=0.23582041263580322, loss=1.5644919872283936
I0306 08:52:45.757823 139780012586752 logging_writer.py:48] [82400] global_step=82400, grad_norm=0.20833006501197815, loss=1.5845229625701904
I0306 08:53:20.787638 139780020979456 logging_writer.py:48] [82500] global_step=82500, grad_norm=0.2015562653541565, loss=1.5303611755371094
I0306 08:53:55.823895 139780012586752 logging_writer.py:48] [82600] global_step=82600, grad_norm=0.19728931784629822, loss=1.5118958950042725
I0306 08:54:30.859934 139780020979456 logging_writer.py:48] [82700] global_step=82700, grad_norm=0.2127227634191513, loss=1.4723228216171265
I0306 08:55:05.886213 139780012586752 logging_writer.py:48] [82800] global_step=82800, grad_norm=0.20468975603580475, loss=1.6257838010787964
I0306 08:55:40.969454 139780020979456 logging_writer.py:48] [82900] global_step=82900, grad_norm=0.21058370172977448, loss=1.5305871963500977
I0306 08:56:15.968192 139780012586752 logging_writer.py:48] [83000] global_step=83000, grad_norm=0.20810551941394806, loss=1.5728614330291748
I0306 08:56:51.026630 139780020979456 logging_writer.py:48] [83100] global_step=83100, grad_norm=0.21599730849266052, loss=1.5959529876708984
I0306 08:57:26.079027 139780012586752 logging_writer.py:48] [83200] global_step=83200, grad_norm=0.19936853647232056, loss=1.6232147216796875
I0306 08:58:01.119818 139780020979456 logging_writer.py:48] [83300] global_step=83300, grad_norm=0.20756927132606506, loss=1.5227895975112915
I0306 08:58:36.174759 139780012586752 logging_writer.py:48] [83400] global_step=83400, grad_norm=0.20296156406402588, loss=1.6058043241500854
I0306 08:59:11.208116 139780020979456 logging_writer.py:48] [83500] global_step=83500, grad_norm=0.20428897440433502, loss=1.6824166774749756
I0306 08:59:46.245768 139780012586752 logging_writer.py:48] [83600] global_step=83600, grad_norm=0.20462821424007416, loss=1.4853028059005737
I0306 09:00:21.275268 139780020979456 logging_writer.py:48] [83700] global_step=83700, grad_norm=0.21256880462169647, loss=1.6635111570358276
I0306 09:00:56.318820 139780012586752 logging_writer.py:48] [83800] global_step=83800, grad_norm=0.2074076533317566, loss=1.554805040359497
I0306 09:01:21.533277 139923228669120 spec.py:321] Evaluating on the training split.
I0306 09:01:24.153839 139923228669120 workload.py:181] Translating evaluation dataset.
I0306 09:05:35.166970 139923228669120 spec.py:333] Evaluating on the validation split.
I0306 09:05:37.794938 139923228669120 workload.py:181] Translating evaluation dataset.
I0306 09:09:17.932948 139923228669120 spec.py:349] Evaluating on the test split.
I0306 09:09:20.568984 139923228669120 workload.py:181] Translating evaluation dataset.
I0306 09:13:06.755145 139923228669120 submission_runner.py:469] Time since start: 50412.49s, 	Step: 83873, 	{'train/accuracy': 0.6786085367202759, 'train/loss': 1.469584584236145, 'train/bleu': 34.27985782652492, 'validation/accuracy': 0.6878723502159119, 'validation/loss': 1.4156548976898193, 'validation/bleu': 30.107668900345654, 'validation/num_examples': 3000, 'test/accuracy': 0.7028502225875854, 'test/loss': 1.3193279504776, 'test/bleu': 30.000158975087214, 'test/num_examples': 3003, 'score': 29437.32509827614, 'total_duration': 50412.49358844757, 'accumulated_submission_time': 29437.32509827614, 'accumulated_eval_time': 20969.96043562889, 'accumulated_logging_time': 0.8403637409210205}
I0306 09:13:06.772711 139780020979456 logging_writer.py:48] [83873] accumulated_eval_time=20970, accumulated_logging_time=0.840364, accumulated_submission_time=29437.3, global_step=83873, preemption_count=0, score=29437.3, test/accuracy=0.70285, test/bleu=30.0002, test/loss=1.31933, test/num_examples=3003, total_duration=50412.5, train/accuracy=0.678609, train/bleu=34.2799, train/loss=1.46958, validation/accuracy=0.687872, validation/bleu=30.1077, validation/loss=1.41565, validation/num_examples=3000
I0306 09:13:16.630047 139780012586752 logging_writer.py:48] [83900] global_step=83900, grad_norm=0.23109886050224304, loss=1.5763221979141235
I0306 09:13:51.743665 139780020979456 logging_writer.py:48] [84000] global_step=84000, grad_norm=0.20608989894390106, loss=1.5441263914108276
I0306 09:14:26.811732 139780012586752 logging_writer.py:48] [84100] global_step=84100, grad_norm=0.20973965525627136, loss=1.4879785776138306
I0306 09:15:01.885121 139780020979456 logging_writer.py:48] [84200] global_step=84200, grad_norm=0.21238164603710175, loss=1.5722745656967163
I0306 09:15:36.915552 139780012586752 logging_writer.py:48] [84300] global_step=84300, grad_norm=0.2061731517314911, loss=1.5536143779754639
I0306 09:16:11.992156 139780020979456 logging_writer.py:48] [84400] global_step=84400, grad_norm=0.22149936854839325, loss=1.6384893655776978
I0306 09:16:47.038125 139780012586752 logging_writer.py:48] [84500] global_step=84500, grad_norm=0.23052185773849487, loss=1.6515902280807495
I0306 09:17:22.105903 139780020979456 logging_writer.py:48] [84600] global_step=84600, grad_norm=0.23471598327159882, loss=1.5843451023101807
I0306 09:17:57.146184 139780012586752 logging_writer.py:48] [84700] global_step=84700, grad_norm=0.47417083382606506, loss=1.6850438117980957
I0306 09:18:32.214731 139780020979456 logging_writer.py:48] [84800] global_step=84800, grad_norm=0.2086818963289261, loss=1.6215699911117554
I0306 09:19:07.296134 139780012586752 logging_writer.py:48] [84900] global_step=84900, grad_norm=0.21602897346019745, loss=1.5166685581207275
I0306 09:19:42.332719 139780020979456 logging_writer.py:48] [85000] global_step=85000, grad_norm=0.21283118426799774, loss=1.5034795999526978
I0306 09:20:17.402878 139780012586752 logging_writer.py:48] [85100] global_step=85100, grad_norm=0.2206813544034958, loss=1.6007509231567383
I0306 09:20:52.438969 139780020979456 logging_writer.py:48] [85200] global_step=85200, grad_norm=0.2007669359445572, loss=1.4803802967071533
I0306 09:21:27.503615 139780012586752 logging_writer.py:48] [85300] global_step=85300, grad_norm=0.2043987512588501, loss=1.510687232017517
I0306 09:22:02.539365 139780020979456 logging_writer.py:48] [85400] global_step=85400, grad_norm=0.21409128606319427, loss=1.6059889793395996
I0306 09:22:37.603551 139780012586752 logging_writer.py:48] [85500] global_step=85500, grad_norm=0.220473051071167, loss=1.5668855905532837
I0306 09:23:12.641380 139780020979456 logging_writer.py:48] [85600] global_step=85600, grad_norm=0.2125961035490036, loss=1.595668077468872
I0306 09:23:47.679047 139780012586752 logging_writer.py:48] [85700] global_step=85700, grad_norm=0.23132143914699554, loss=1.6383146047592163
I0306 09:24:22.740792 139780020979456 logging_writer.py:48] [85800] global_step=85800, grad_norm=0.22110329568386078, loss=1.683355689048767
I0306 09:24:57.805011 139780012586752 logging_writer.py:48] [85900] global_step=85900, grad_norm=0.2100931704044342, loss=1.5762794017791748
I0306 09:25:32.837685 139780020979456 logging_writer.py:48] [86000] global_step=86000, grad_norm=0.19724833965301514, loss=1.6106902360916138
I0306 09:26:07.876332 139780012586752 logging_writer.py:48] [86100] global_step=86100, grad_norm=0.201051726937294, loss=1.6191661357879639
I0306 09:26:42.945233 139780020979456 logging_writer.py:48] [86200] global_step=86200, grad_norm=0.20888751745224, loss=1.5349246263504028
I0306 09:27:06.766425 139923228669120 spec.py:321] Evaluating on the training split.
I0306 09:27:09.392431 139923228669120 workload.py:181] Translating evaluation dataset.
I0306 09:30:26.706495 139923228669120 spec.py:333] Evaluating on the validation split.
I0306 09:30:29.329508 139923228669120 workload.py:181] Translating evaluation dataset.
I0306 09:33:11.513093 139923228669120 spec.py:349] Evaluating on the test split.
I0306 09:33:14.132509 139923228669120 workload.py:181] Translating evaluation dataset.
I0306 09:35:42.725531 139923228669120 submission_runner.py:469] Time since start: 51768.46s, 	Step: 86269, 	{'train/accuracy': 0.6765577793121338, 'train/loss': 1.4855766296386719, 'train/bleu': 33.85359031410758, 'validation/accuracy': 0.6886263489723206, 'validation/loss': 1.4098918437957764, 'validation/bleu': 30.391831369339755, 'validation/num_examples': 3000, 'test/accuracy': 0.7034874558448792, 'test/loss': 1.310977816581726, 'test/bleu': 30.17826562874015, 'test/num_examples': 3003, 'score': 30277.185242176056, 'total_duration': 51768.4639582634, 'accumulated_submission_time': 30277.185242176056, 'accumulated_eval_time': 21485.91948032379, 'accumulated_logging_time': 0.8669848442077637}
I0306 09:35:42.742492 139780012586752 logging_writer.py:48] [86269] accumulated_eval_time=21485.9, accumulated_logging_time=0.866985, accumulated_submission_time=30277.2, global_step=86269, preemption_count=0, score=30277.2, test/accuracy=0.703487, test/bleu=30.1783, test/loss=1.31098, test/num_examples=3003, total_duration=51768.5, train/accuracy=0.676558, train/bleu=33.8536, train/loss=1.48558, validation/accuracy=0.688626, validation/bleu=30.3918, validation/loss=1.40989, validation/num_examples=3000
I0306 09:35:53.981714 139780020979456 logging_writer.py:48] [86300] global_step=86300, grad_norm=0.22408102452754974, loss=1.562779188156128
I0306 09:36:29.044080 139780012586752 logging_writer.py:48] [86400] global_step=86400, grad_norm=0.20695184171199799, loss=1.4914559125900269
I0306 09:37:04.081134 139780020979456 logging_writer.py:48] [86500] global_step=86500, grad_norm=0.2050374299287796, loss=1.5527633428573608
I0306 09:37:39.134801 139780012586752 logging_writer.py:48] [86600] global_step=86600, grad_norm=0.2133254110813141, loss=1.6301686763763428
I0306 09:38:14.182410 139780020979456 logging_writer.py:48] [86700] global_step=86700, grad_norm=0.2202782928943634, loss=1.5378811359405518
I0306 09:38:49.236584 139780012586752 logging_writer.py:48] [86800] global_step=86800, grad_norm=0.20760589838027954, loss=1.574330449104309
I0306 09:39:24.278016 139780020979456 logging_writer.py:48] [86900] global_step=86900, grad_norm=0.2058032751083374, loss=1.498368263244629
I0306 09:39:59.421017 139780012586752 logging_writer.py:48] [87000] global_step=87000, grad_norm=0.20402702689170837, loss=1.562425971031189
I0306 09:40:34.455428 139780020979456 logging_writer.py:48] [87100] global_step=87100, grad_norm=0.2232728749513626, loss=1.617807149887085
I0306 09:41:09.500829 139780012586752 logging_writer.py:48] [87200] global_step=87200, grad_norm=0.23079973459243774, loss=1.6497745513916016
I0306 09:41:44.544526 139780020979456 logging_writer.py:48] [87300] global_step=87300, grad_norm=0.2454109638929367, loss=1.596693992614746
I0306 09:42:19.607828 139780012586752 logging_writer.py:48] [87400] global_step=87400, grad_norm=0.21639585494995117, loss=1.5715548992156982
I0306 09:42:54.657816 139780020979456 logging_writer.py:48] [87500] global_step=87500, grad_norm=0.21682748198509216, loss=1.5039340257644653
I0306 09:43:29.711919 139780012586752 logging_writer.py:48] [87600] global_step=87600, grad_norm=0.23021158576011658, loss=1.562168836593628
I0306 09:44:04.786494 139780020979456 logging_writer.py:48] [87700] global_step=87700, grad_norm=0.21044723689556122, loss=1.539106845855713
I0306 09:44:39.803743 139780012586752 logging_writer.py:48] [87800] global_step=87800, grad_norm=0.20177246630191803, loss=1.4837485551834106
I0306 09:45:14.814142 139780020979456 logging_writer.py:48] [87900] global_step=87900, grad_norm=0.21702790260314941, loss=1.5920463800430298
I0306 09:45:49.838494 139780012586752 logging_writer.py:48] [88000] global_step=88000, grad_norm=0.2056390345096588, loss=1.5216652154922485
I0306 09:46:24.878470 139780020979456 logging_writer.py:48] [88100] global_step=88100, grad_norm=0.2148144245147705, loss=1.6159111261367798
I0306 09:46:59.938598 139780012586752 logging_writer.py:48] [88200] global_step=88200, grad_norm=0.21851500868797302, loss=1.6246347427368164
I0306 09:47:34.946774 139780020979456 logging_writer.py:48] [88300] global_step=88300, grad_norm=0.21748791635036469, loss=1.54904305934906
I0306 09:48:09.976563 139780012586752 logging_writer.py:48] [88400] global_step=88400, grad_norm=0.20826418697834015, loss=1.5359735488891602
I0306 09:48:44.990366 139780020979456 logging_writer.py:48] [88500] global_step=88500, grad_norm=0.21777397394180298, loss=1.4878939390182495
I0306 09:49:20.004760 139780012586752 logging_writer.py:48] [88600] global_step=88600, grad_norm=0.21666032075881958, loss=1.545703649520874
I0306 09:49:42.786669 139923228669120 spec.py:321] Evaluating on the training split.
I0306 09:49:45.409147 139923228669120 workload.py:181] Translating evaluation dataset.
I0306 09:53:03.402359 139923228669120 spec.py:333] Evaluating on the validation split.
I0306 09:53:06.015702 139923228669120 workload.py:181] Translating evaluation dataset.
I0306 09:55:58.556257 139923228669120 spec.py:349] Evaluating on the test split.
I0306 09:56:01.180794 139923228669120 workload.py:181] Translating evaluation dataset.
I0306 09:58:47.144569 139923228669120 submission_runner.py:469] Time since start: 53152.88s, 	Step: 88666, 	{'train/accuracy': 0.6845095157623291, 'train/loss': 1.4313515424728394, 'train/bleu': 34.49995457551866, 'validation/accuracy': 0.689800500869751, 'validation/loss': 1.399889588356018, 'validation/bleu': 30.33817035510746, 'validation/num_examples': 3000, 'test/accuracy': 0.7045417428016663, 'test/loss': 1.3043636083602905, 'test/bleu': 30.348573875125766, 'test/num_examples': 3003, 'score': 31117.099716424942, 'total_duration': 53152.883013010025, 'accumulated_submission_time': 31117.099716424942, 'accumulated_eval_time': 22030.27733516693, 'accumulated_logging_time': 0.892803430557251}
I0306 09:58:47.161639 139780020979456 logging_writer.py:48] [88666] accumulated_eval_time=22030.3, accumulated_logging_time=0.892803, accumulated_submission_time=31117.1, global_step=88666, preemption_count=0, score=31117.1, test/accuracy=0.704542, test/bleu=30.3486, test/loss=1.30436, test/num_examples=3003, total_duration=53152.9, train/accuracy=0.68451, train/bleu=34.5, train/loss=1.43135, validation/accuracy=0.689801, validation/bleu=30.3382, validation/loss=1.39989, validation/num_examples=3000
I0306 09:58:59.409708 139780012586752 logging_writer.py:48] [88700] global_step=88700, grad_norm=0.2153799682855606, loss=1.483811855316162
I0306 09:59:34.463927 139780020979456 logging_writer.py:48] [88800] global_step=88800, grad_norm=0.21095313131809235, loss=1.5086923837661743
I0306 10:00:09.489240 139780012586752 logging_writer.py:48] [88900] global_step=88900, grad_norm=0.2167579084634781, loss=1.5573031902313232
I0306 10:00:44.583716 139780020979456 logging_writer.py:48] [89000] global_step=89000, grad_norm=0.20737691223621368, loss=1.496224045753479
I0306 10:01:19.643272 139780012586752 logging_writer.py:48] [89100] global_step=89100, grad_norm=0.2058994024991989, loss=1.551766276359558
I0306 10:01:54.677707 139780020979456 logging_writer.py:48] [89200] global_step=89200, grad_norm=0.21477970480918884, loss=1.538191318511963
I0306 10:02:29.722977 139780012586752 logging_writer.py:48] [89300] global_step=89300, grad_norm=0.22006700932979584, loss=1.5676233768463135
I0306 10:03:04.750151 139780020979456 logging_writer.py:48] [89400] global_step=89400, grad_norm=0.20330187678337097, loss=1.5083708763122559
I0306 10:03:39.809288 139780012586752 logging_writer.py:48] [89500] global_step=89500, grad_norm=0.21198920905590057, loss=1.4796234369277954
I0306 10:04:14.874956 139780020979456 logging_writer.py:48] [89600] global_step=89600, grad_norm=0.2209947556257248, loss=1.5572152137756348
I0306 10:04:49.910925 139780012586752 logging_writer.py:48] [89700] global_step=89700, grad_norm=0.20560090243816376, loss=1.5025088787078857
I0306 10:05:24.942956 139780020979456 logging_writer.py:48] [89800] global_step=89800, grad_norm=0.21459154784679413, loss=1.5070077180862427
I0306 10:05:59.977410 139780012586752 logging_writer.py:48] [89900] global_step=89900, grad_norm=0.2137187421321869, loss=1.579158067703247
I0306 10:06:35.034884 139780020979456 logging_writer.py:48] [90000] global_step=90000, grad_norm=0.20633377134799957, loss=1.5423767566680908
I0306 10:07:10.086473 139780012586752 logging_writer.py:48] [90100] global_step=90100, grad_norm=0.2095956951379776, loss=1.6791257858276367
I0306 10:07:45.164285 139780020979456 logging_writer.py:48] [90200] global_step=90200, grad_norm=0.21942424774169922, loss=1.527912974357605
I0306 10:08:20.214079 139780012586752 logging_writer.py:48] [90300] global_step=90300, grad_norm=0.2208024114370346, loss=1.507028579711914
I0306 10:08:55.273340 139780020979456 logging_writer.py:48] [90400] global_step=90400, grad_norm=0.22561293840408325, loss=1.5894196033477783
I0306 10:09:30.321938 139780012586752 logging_writer.py:48] [90500] global_step=90500, grad_norm=0.21558934450149536, loss=1.5823343992233276
I0306 10:10:05.329879 139780020979456 logging_writer.py:48] [90600] global_step=90600, grad_norm=0.22194749116897583, loss=1.5533812046051025
I0306 10:10:40.358011 139780012586752 logging_writer.py:48] [90700] global_step=90700, grad_norm=0.2044011652469635, loss=1.512109637260437
I0306 10:11:15.413716 139780020979456 logging_writer.py:48] [90800] global_step=90800, grad_norm=0.20899581909179688, loss=1.4933491945266724
I0306 10:11:50.463691 139780012586752 logging_writer.py:48] [90900] global_step=90900, grad_norm=0.23500721156597137, loss=1.5229829549789429
I0306 10:12:25.510604 139780020979456 logging_writer.py:48] [91000] global_step=91000, grad_norm=0.2234928011894226, loss=1.5280860662460327
I0306 10:12:47.219914 139923228669120 spec.py:321] Evaluating on the training split.
I0306 10:12:49.840452 139923228669120 workload.py:181] Translating evaluation dataset.
I0306 10:17:02.753102 139923228669120 spec.py:333] Evaluating on the validation split.
I0306 10:17:05.380328 139923228669120 workload.py:181] Translating evaluation dataset.
I0306 10:20:24.605256 139923228669120 spec.py:349] Evaluating on the test split.
I0306 10:20:27.232026 139923228669120 workload.py:181] Translating evaluation dataset.
I0306 10:23:40.061033 139923228669120 submission_runner.py:469] Time since start: 54645.80s, 	Step: 91063, 	{'train/accuracy': 0.6808391213417053, 'train/loss': 1.460278868675232, 'train/bleu': 34.522505300855954, 'validation/accuracy': 0.6902578473091125, 'validation/loss': 1.3906766176223755, 'validation/bleu': 30.562124483984782, 'validation/num_examples': 3000, 'test/accuracy': 0.7068358063697815, 'test/loss': 1.2929617166519165, 'test/bleu': 30.34585374155825, 'test/num_examples': 3003, 'score': 31957.027379989624, 'total_duration': 54645.79945039749, 'accumulated_submission_time': 31957.027379989624, 'accumulated_eval_time': 22683.118382930756, 'accumulated_logging_time': 0.9178125858306885}
I0306 10:23:40.081403 139780012586752 logging_writer.py:48] [91063] accumulated_eval_time=22683.1, accumulated_logging_time=0.917813, accumulated_submission_time=31957, global_step=91063, preemption_count=0, score=31957, test/accuracy=0.706836, test/bleu=30.3459, test/loss=1.29296, test/num_examples=3003, total_duration=54645.8, train/accuracy=0.680839, train/bleu=34.5225, train/loss=1.46028, validation/accuracy=0.690258, validation/bleu=30.5621, validation/loss=1.39068, validation/num_examples=3000
I0306 10:23:53.428736 139780020979456 logging_writer.py:48] [91100] global_step=91100, grad_norm=0.2064167708158493, loss=1.5232471227645874
I0306 10:24:28.494146 139780012586752 logging_writer.py:48] [91200] global_step=91200, grad_norm=0.20324277877807617, loss=1.5675580501556396
I0306 10:25:03.513128 139780020979456 logging_writer.py:48] [91300] global_step=91300, grad_norm=0.21334373950958252, loss=1.5300230979919434
I0306 10:25:38.567233 139780012586752 logging_writer.py:48] [91400] global_step=91400, grad_norm=0.2723512351512909, loss=1.586773157119751
I0306 10:26:13.612067 139780020979456 logging_writer.py:48] [91500] global_step=91500, grad_norm=0.2179519236087799, loss=1.5199679136276245
I0306 10:26:48.700140 139780012586752 logging_writer.py:48] [91600] global_step=91600, grad_norm=0.22239740192890167, loss=1.5972903966903687
I0306 10:27:23.745512 139780020979456 logging_writer.py:48] [91700] global_step=91700, grad_norm=0.207757368683815, loss=1.5569193363189697
I0306 10:27:58.801023 139780012586752 logging_writer.py:48] [91800] global_step=91800, grad_norm=0.20815223455429077, loss=1.536009669303894
I0306 10:28:33.837072 139780020979456 logging_writer.py:48] [91900] global_step=91900, grad_norm=0.21060289442539215, loss=1.5757238864898682
I0306 10:29:08.883123 139780012586752 logging_writer.py:48] [92000] global_step=92000, grad_norm=0.2150980532169342, loss=1.542974591255188
I0306 10:29:43.899168 139780020979456 logging_writer.py:48] [92100] global_step=92100, grad_norm=0.21806073188781738, loss=1.5059256553649902
I0306 10:30:18.940172 139780012586752 logging_writer.py:48] [92200] global_step=92200, grad_norm=0.23266030848026276, loss=1.558147668838501
I0306 10:30:53.974823 139780020979456 logging_writer.py:48] [92300] global_step=92300, grad_norm=0.2217114418745041, loss=1.5795912742614746
I0306 10:31:29.004153 139780012586752 logging_writer.py:48] [92400] global_step=92400, grad_norm=0.2227371782064438, loss=1.4588779211044312
I0306 10:32:04.047162 139780020979456 logging_writer.py:48] [92500] global_step=92500, grad_norm=0.22965435683727264, loss=1.5411816835403442
I0306 10:32:39.076106 139780012586752 logging_writer.py:48] [92600] global_step=92600, grad_norm=0.21714836359024048, loss=1.4826571941375732
I0306 10:33:14.118320 139780020979456 logging_writer.py:48] [92700] global_step=92700, grad_norm=0.21045547723770142, loss=1.5736101865768433
I0306 10:33:49.138876 139780012586752 logging_writer.py:48] [92800] global_step=92800, grad_norm=0.21882279217243195, loss=1.5458461046218872
I0306 10:34:24.188601 139780020979456 logging_writer.py:48] [92900] global_step=92900, grad_norm=0.2169666737318039, loss=1.5547146797180176
I0306 10:34:59.207084 139780012586752 logging_writer.py:48] [93000] global_step=93000, grad_norm=0.21331991255283356, loss=1.4854185581207275
I0306 10:35:34.260120 139780020979456 logging_writer.py:48] [93100] global_step=93100, grad_norm=0.21522657573223114, loss=1.4545446634292603
I0306 10:36:09.292683 139780012586752 logging_writer.py:48] [93200] global_step=93200, grad_norm=0.22330136597156525, loss=1.5111143589019775
I0306 10:36:44.322722 139780020979456 logging_writer.py:48] [93300] global_step=93300, grad_norm=0.230599045753479, loss=1.5502082109451294
I0306 10:37:19.356644 139780012586752 logging_writer.py:48] [93400] global_step=93400, grad_norm=0.2210683524608612, loss=1.5207629203796387
I0306 10:37:40.360815 139923228669120 spec.py:321] Evaluating on the training split.
I0306 10:37:42.979321 139923228669120 workload.py:181] Translating evaluation dataset.
I0306 10:41:26.983155 139923228669120 spec.py:333] Evaluating on the validation split.
I0306 10:41:29.620794 139923228669120 workload.py:181] Translating evaluation dataset.
I0306 10:44:09.928844 139923228669120 spec.py:349] Evaluating on the test split.
I0306 10:44:12.562993 139923228669120 workload.py:181] Translating evaluation dataset.
I0306 10:46:50.414100 139923228669120 submission_runner.py:469] Time since start: 56036.15s, 	Step: 93461, 	{'train/accuracy': 0.6786174774169922, 'train/loss': 1.4746299982070923, 'train/bleu': 34.5016805190125, 'validation/accuracy': 0.6923096179962158, 'validation/loss': 1.3880749940872192, 'validation/bleu': 30.70616514713522, 'validation/num_examples': 3000, 'test/accuracy': 0.7077627182006836, 'test/loss': 1.288251519203186, 'test/bleu': 30.42520310523495, 'test/num_examples': 3003, 'score': 32797.17566847801, 'total_duration': 56036.15252375603, 'accumulated_submission_time': 32797.17566847801, 'accumulated_eval_time': 23233.17160320282, 'accumulated_logging_time': 0.9467217922210693}
I0306 10:46:50.432035 139780020979456 logging_writer.py:48] [93461] accumulated_eval_time=23233.2, accumulated_logging_time=0.946722, accumulated_submission_time=32797.2, global_step=93461, preemption_count=0, score=32797.2, test/accuracy=0.707763, test/bleu=30.4252, test/loss=1.28825, test/num_examples=3003, total_duration=56036.2, train/accuracy=0.678617, train/bleu=34.5017, train/loss=1.47463, validation/accuracy=0.69231, validation/bleu=30.7062, validation/loss=1.38807, validation/num_examples=3000
I0306 10:47:04.472467 139780012586752 logging_writer.py:48] [93500] global_step=93500, grad_norm=0.2236262708902359, loss=1.4824663400650024
I0306 10:47:39.554626 139780020979456 logging_writer.py:48] [93600] global_step=93600, grad_norm=0.2238396406173706, loss=1.5327903032302856
I0306 10:48:14.594388 139780012586752 logging_writer.py:48] [93700] global_step=93700, grad_norm=0.21646153926849365, loss=1.5215269327163696
I0306 10:48:49.605479 139780020979456 logging_writer.py:48] [93800] global_step=93800, grad_norm=0.21012811362743378, loss=1.5438177585601807
I0306 10:49:24.679069 139780012586752 logging_writer.py:48] [93900] global_step=93900, grad_norm=0.21415062248706818, loss=1.497111201286316
I0306 10:49:59.760498 139780020979456 logging_writer.py:48] [94000] global_step=94000, grad_norm=0.23297008872032166, loss=1.5114339590072632
I0306 10:50:34.762526 139780012586752 logging_writer.py:48] [94100] global_step=94100, grad_norm=0.2282159924507141, loss=1.550937533378601
I0306 10:51:09.827027 139780020979456 logging_writer.py:48] [94200] global_step=94200, grad_norm=0.2127993404865265, loss=1.4826480150222778
I0306 10:51:44.860048 139780012586752 logging_writer.py:48] [94300] global_step=94300, grad_norm=0.21190568804740906, loss=1.4958481788635254
I0306 10:52:19.928676 139780020979456 logging_writer.py:48] [94400] global_step=94400, grad_norm=0.2164575159549713, loss=1.5928070545196533
I0306 10:52:54.963576 139780012586752 logging_writer.py:48] [94500] global_step=94500, grad_norm=0.2327321320772171, loss=1.5957728624343872
I0306 10:53:30.007253 139780020979456 logging_writer.py:48] [94600] global_step=94600, grad_norm=0.2321462482213974, loss=1.5121504068374634
I0306 10:54:05.072537 139780012586752 logging_writer.py:48] [94700] global_step=94700, grad_norm=0.20665787160396576, loss=1.4265707731246948
I0306 10:54:40.082748 139780020979456 logging_writer.py:48] [94800] global_step=94800, grad_norm=0.21826857328414917, loss=1.546622395515442
I0306 10:55:15.129020 139780012586752 logging_writer.py:48] [94900] global_step=94900, grad_norm=0.21788321435451508, loss=1.5526233911514282
I0306 10:55:50.159513 139780020979456 logging_writer.py:48] [95000] global_step=95000, grad_norm=0.24168439209461212, loss=1.6018102169036865
I0306 10:56:25.215809 139780012586752 logging_writer.py:48] [95100] global_step=95100, grad_norm=0.22882887721061707, loss=1.5303887128829956
I0306 10:57:00.260342 139780020979456 logging_writer.py:48] [95200] global_step=95200, grad_norm=0.21612948179244995, loss=1.543360948562622
I0306 10:57:35.284552 139780012586752 logging_writer.py:48] [95300] global_step=95300, grad_norm=0.22502706944942474, loss=1.5215004682540894
I0306 10:58:10.298939 139780020979456 logging_writer.py:48] [95400] global_step=95400, grad_norm=0.22249120473861694, loss=1.5686310529708862
I0306 10:58:45.370262 139780012586752 logging_writer.py:48] [95500] global_step=95500, grad_norm=0.21659909188747406, loss=1.542990803718567
I0306 10:59:20.372388 139780020979456 logging_writer.py:48] [95600] global_step=95600, grad_norm=0.2141738086938858, loss=1.5071607828140259
I0306 10:59:55.428882 139780012586752 logging_writer.py:48] [95700] global_step=95700, grad_norm=0.22342267632484436, loss=1.5537843704223633
I0306 11:00:30.471853 139780020979456 logging_writer.py:48] [95800] global_step=95800, grad_norm=0.21866312623023987, loss=1.5567078590393066
I0306 11:00:50.452166 139923228669120 spec.py:321] Evaluating on the training split.
I0306 11:00:53.083319 139923228669120 workload.py:181] Translating evaluation dataset.
I0306 11:04:49.418005 139923228669120 spec.py:333] Evaluating on the validation split.
I0306 11:04:52.033484 139923228669120 workload.py:181] Translating evaluation dataset.
I0306 11:07:37.613529 139923228669120 spec.py:349] Evaluating on the test split.
I0306 11:07:40.236767 139923228669120 workload.py:181] Translating evaluation dataset.
I0306 11:10:29.087337 139923228669120 submission_runner.py:469] Time since start: 57454.83s, 	Step: 95858, 	{'train/accuracy': 0.6888818144798279, 'train/loss': 1.4096142053604126, 'train/bleu': 34.665538855998165, 'validation/accuracy': 0.69310063123703, 'validation/loss': 1.3813670873641968, 'validation/bleu': 30.60912169938231, 'validation/num_examples': 3000, 'test/accuracy': 0.7091183066368103, 'test/loss': 1.2820110321044922, 'test/bleu': 30.694807185614742, 'test/num_examples': 3003, 'score': 33637.064775943756, 'total_duration': 57454.82578086853, 'accumulated_submission_time': 33637.064775943756, 'accumulated_eval_time': 23811.80674099922, 'accumulated_logging_time': 0.9742169380187988}
I0306 11:10:29.105766 139780012586752 logging_writer.py:48] [95858] accumulated_eval_time=23811.8, accumulated_logging_time=0.974217, accumulated_submission_time=33637.1, global_step=95858, preemption_count=0, score=33637.1, test/accuracy=0.709118, test/bleu=30.6948, test/loss=1.28201, test/num_examples=3003, total_duration=57454.8, train/accuracy=0.688882, train/bleu=34.6655, train/loss=1.40961, validation/accuracy=0.693101, validation/bleu=30.6091, validation/loss=1.38137, validation/num_examples=3000
I0306 11:10:44.198632 139780020979456 logging_writer.py:48] [95900] global_step=95900, grad_norm=0.22400100529193878, loss=1.5395234823226929
I0306 11:11:19.255967 139780012586752 logging_writer.py:48] [96000] global_step=96000, grad_norm=0.2204444259405136, loss=1.5503013134002686
I0306 11:11:54.241403 139780020979456 logging_writer.py:48] [96100] global_step=96100, grad_norm=0.22719310224056244, loss=1.5501811504364014
I0306 11:12:29.301271 139780012586752 logging_writer.py:48] [96200] global_step=96200, grad_norm=0.21574904024600983, loss=1.5222666263580322
I0306 11:13:04.343396 139780020979456 logging_writer.py:48] [96300] global_step=96300, grad_norm=0.2229849398136139, loss=1.4914822578430176
I0306 11:13:39.375953 139780012586752 logging_writer.py:48] [96400] global_step=96400, grad_norm=0.23205451667308807, loss=1.487525224685669
I0306 11:14:14.424971 139780020979456 logging_writer.py:48] [96500] global_step=96500, grad_norm=0.22092358767986298, loss=1.4722756147384644
I0306 11:14:49.470826 139780012586752 logging_writer.py:48] [96600] global_step=96600, grad_norm=0.20774412155151367, loss=1.4399932622909546
I0306 11:15:24.528810 139780020979456 logging_writer.py:48] [96700] global_step=96700, grad_norm=0.22661370038986206, loss=1.4244145154953003
I0306 11:15:59.576469 139780012586752 logging_writer.py:48] [96800] global_step=96800, grad_norm=0.2274889349937439, loss=1.5523148775100708
I0306 11:16:34.594182 139780020979456 logging_writer.py:48] [96900] global_step=96900, grad_norm=0.2218572199344635, loss=1.5255047082901
I0306 11:17:09.649753 139780012586752 logging_writer.py:48] [97000] global_step=97000, grad_norm=0.23894281685352325, loss=1.5334439277648926
I0306 11:17:44.715847 139780020979456 logging_writer.py:48] [97100] global_step=97100, grad_norm=0.21683555841445923, loss=1.502199411392212
I0306 11:18:19.736081 139780012586752 logging_writer.py:48] [97200] global_step=97200, grad_norm=0.21916118264198303, loss=1.4687626361846924
I0306 11:18:54.784102 139780020979456 logging_writer.py:48] [97300] global_step=97300, grad_norm=0.22447913885116577, loss=1.48992919921875
I0306 11:19:29.793424 139780012586752 logging_writer.py:48] [97400] global_step=97400, grad_norm=0.225110724568367, loss=1.522789716720581
I0306 11:20:04.828717 139780020979456 logging_writer.py:48] [97500] global_step=97500, grad_norm=0.2288399189710617, loss=1.5160869359970093
I0306 11:20:39.891798 139780012586752 logging_writer.py:48] [97600] global_step=97600, grad_norm=0.20833545923233032, loss=1.4116262197494507
I0306 11:21:14.902373 139780020979456 logging_writer.py:48] [97700] global_step=97700, grad_norm=0.252490371465683, loss=1.475534439086914
I0306 11:21:49.933067 139780012586752 logging_writer.py:48] [97800] global_step=97800, grad_norm=0.23992642760276794, loss=1.5080064535140991
I0306 11:22:24.965332 139780020979456 logging_writer.py:48] [97900] global_step=97900, grad_norm=0.22634254395961761, loss=1.459691047668457
I0306 11:23:00.006130 139780012586752 logging_writer.py:48] [98000] global_step=98000, grad_norm=0.21423238515853882, loss=1.5535489320755005
I0306 11:23:35.044085 139780020979456 logging_writer.py:48] [98100] global_step=98100, grad_norm=0.21895727515220642, loss=1.5058860778808594
I0306 11:24:10.051616 139780012586752 logging_writer.py:48] [98200] global_step=98200, grad_norm=0.2177232950925827, loss=1.5005913972854614
I0306 11:24:29.306461 139923228669120 spec.py:321] Evaluating on the training split.
I0306 11:24:31.931971 139923228669120 workload.py:181] Translating evaluation dataset.
I0306 11:27:53.622864 139923228669120 spec.py:333] Evaluating on the validation split.
I0306 11:27:56.251059 139923228669120 workload.py:181] Translating evaluation dataset.
I0306 11:30:23.729944 139923228669120 spec.py:349] Evaluating on the test split.
I0306 11:30:26.353422 139923228669120 workload.py:181] Translating evaluation dataset.
I0306 11:32:46.268099 139923228669120 submission_runner.py:469] Time since start: 58792.01s, 	Step: 98256, 	{'train/accuracy': 0.6860619187355042, 'train/loss': 1.425062656402588, 'train/bleu': 34.27644709432437, 'validation/accuracy': 0.6928287148475647, 'validation/loss': 1.3815335035324097, 'validation/bleu': 30.582935570183754, 'validation/num_examples': 3000, 'test/accuracy': 0.7091878056526184, 'test/loss': 1.279125452041626, 'test/bleu': 30.700503734111685, 'test/num_examples': 3003, 'score': 34477.13659238815, 'total_duration': 58792.00652337074, 'accumulated_submission_time': 34477.13659238815, 'accumulated_eval_time': 24308.76831483841, 'accumulated_logging_time': 1.00032377243042}
I0306 11:32:46.286342 139780020979456 logging_writer.py:48] [98256] accumulated_eval_time=24308.8, accumulated_logging_time=1.00032, accumulated_submission_time=34477.1, global_step=98256, preemption_count=0, score=34477.1, test/accuracy=0.709188, test/bleu=30.7005, test/loss=1.27913, test/num_examples=3003, total_duration=58792, train/accuracy=0.686062, train/bleu=34.2764, train/loss=1.42506, validation/accuracy=0.692829, validation/bleu=30.5829, validation/loss=1.38153, validation/num_examples=3000
I0306 11:33:02.089055 139780012586752 logging_writer.py:48] [98300] global_step=98300, grad_norm=0.23131707310676575, loss=1.5022155046463013
I0306 11:33:37.144080 139780020979456 logging_writer.py:48] [98400] global_step=98400, grad_norm=0.2179383784532547, loss=1.544796347618103
I0306 11:34:12.159896 139780012586752 logging_writer.py:48] [98500] global_step=98500, grad_norm=0.21140296757221222, loss=1.485024094581604
I0306 11:34:47.223359 139780020979456 logging_writer.py:48] [98600] global_step=98600, grad_norm=0.21419000625610352, loss=1.475515365600586
I0306 11:35:22.244738 139780012586752 logging_writer.py:48] [98700] global_step=98700, grad_norm=0.2208828181028366, loss=1.5144495964050293
I0306 11:35:57.302677 139780020979456 logging_writer.py:48] [98800] global_step=98800, grad_norm=0.2191915363073349, loss=1.5573545694351196
I0306 11:36:32.371395 139780012586752 logging_writer.py:48] [98900] global_step=98900, grad_norm=0.21703511476516724, loss=1.5170283317565918
I0306 11:37:07.396918 139780020979456 logging_writer.py:48] [99000] global_step=99000, grad_norm=0.22988174855709076, loss=1.4912538528442383
I0306 11:37:42.430082 139780012586752 logging_writer.py:48] [99100] global_step=99100, grad_norm=0.23014278709888458, loss=1.5456953048706055
I0306 11:38:17.488653 139780020979456 logging_writer.py:48] [99200] global_step=99200, grad_norm=0.22311720252037048, loss=1.5242242813110352
I0306 11:38:52.544998 139780012586752 logging_writer.py:48] [99300] global_step=99300, grad_norm=0.2263643741607666, loss=1.474637508392334
I0306 11:39:27.588880 139780020979456 logging_writer.py:48] [99400] global_step=99400, grad_norm=0.22733335196971893, loss=1.5024030208587646
I0306 11:40:02.639012 139780012586752 logging_writer.py:48] [99500] global_step=99500, grad_norm=0.221647709608078, loss=1.5315808057785034
I0306 11:40:37.713161 139780020979456 logging_writer.py:48] [99600] global_step=99600, grad_norm=0.23432843387126923, loss=1.5403761863708496
I0306 11:41:12.748093 139780012586752 logging_writer.py:48] [99700] global_step=99700, grad_norm=0.22076892852783203, loss=1.5380877256393433
I0306 11:41:47.789803 139780020979456 logging_writer.py:48] [99800] global_step=99800, grad_norm=0.21595925092697144, loss=1.513695478439331
I0306 11:42:22.840592 139780012586752 logging_writer.py:48] [99900] global_step=99900, grad_norm=0.22531254589557648, loss=1.478713870048523
I0306 11:42:57.880759 139780020979456 logging_writer.py:48] [100000] global_step=100000, grad_norm=0.2156379669904709, loss=1.4691882133483887
I0306 11:43:32.923759 139780012586752 logging_writer.py:48] [100100] global_step=100100, grad_norm=0.24193242192268372, loss=1.520342469215393
I0306 11:44:08.001241 139780020979456 logging_writer.py:48] [100200] global_step=100200, grad_norm=0.32217103242874146, loss=1.5454280376434326
I0306 11:44:43.070509 139780012586752 logging_writer.py:48] [100300] global_step=100300, grad_norm=0.46456295251846313, loss=1.4439150094985962
I0306 11:45:18.071882 139780020979456 logging_writer.py:48] [100400] global_step=100400, grad_norm=0.22281652688980103, loss=1.4678430557250977
I0306 11:45:53.027013 139780012586752 logging_writer.py:48] [100500] global_step=100500, grad_norm=1.6326258182525635, loss=1.5513496398925781
I0306 11:46:27.977812 139780020979456 logging_writer.py:48] [100600] global_step=100600, grad_norm=0.21469396352767944, loss=1.4743355512619019
I0306 11:46:46.530981 139923228669120 spec.py:321] Evaluating on the training split.
I0306 11:46:49.154818 139923228669120 workload.py:181] Translating evaluation dataset.
I0306 11:50:44.568191 139923228669120 spec.py:333] Evaluating on the validation split.
I0306 11:50:47.198454 139923228669120 workload.py:181] Translating evaluation dataset.
I0306 11:53:40.363898 139923228669120 spec.py:349] Evaluating on the test split.
I0306 11:53:42.987064 139923228669120 workload.py:181] Translating evaluation dataset.
I0306 11:56:42.956152 139923228669120 submission_runner.py:469] Time since start: 60228.69s, 	Step: 100654, 	{'train/accuracy': 0.6953095197677612, 'train/loss': 1.3750799894332886, 'train/bleu': 35.248475580077994, 'validation/accuracy': 0.6936321258544922, 'validation/loss': 1.3750355243682861, 'validation/bleu': 30.970914219958566, 'validation/num_examples': 3000, 'test/accuracy': 0.710914134979248, 'test/loss': 1.2728192806243896, 'test/bleu': 30.864503714998758, 'test/num_examples': 3003, 'score': 35317.25174379349, 'total_duration': 60228.694583654404, 'accumulated_submission_time': 35317.25174379349, 'accumulated_eval_time': 24905.193427801132, 'accumulated_logging_time': 1.0264627933502197}
I0306 11:56:42.974961 139780012586752 logging_writer.py:48] [100654] accumulated_eval_time=24905.2, accumulated_logging_time=1.02646, accumulated_submission_time=35317.3, global_step=100654, preemption_count=0, score=35317.3, test/accuracy=0.710914, test/bleu=30.8645, test/loss=1.27282, test/num_examples=3003, total_duration=60228.7, train/accuracy=0.69531, train/bleu=35.2485, train/loss=1.37508, validation/accuracy=0.693632, validation/bleu=30.9709, validation/loss=1.37504, validation/num_examples=3000
I0306 11:56:42.995650 139780020979456 logging_writer.py:48] [100654] global_step=100654, preemption_count=0, score=35317.3
I0306 11:56:43.020036 139923228669120 submission_runner.py:646] Tuning trial 5/5
I0306 11:56:43.020142 139923228669120 submission_runner.py:647] Hyperparameters: Hyperparameters(dropout_rate=0.1, label_smoothing=0.0, learning_rate=0.0017486387539278373, one_minus_beta1=0.06733926164, beta2=0.9955159689799007, weight_decay=0.08121616522670176, warmup_factor=0.02)
I0306 11:56:43.021924 139923228669120 submission_runner.py:648] Metrics: {'eval_results': [(1, {'train/accuracy': 0.0005498344544321299, 'train/loss': 11.211599349975586, 'train/bleu': 0.0, 'validation/accuracy': 0.00048204089398495853, 'validation/loss': 11.20705795288086, 'validation/bleu': 0.0, 'validation/num_examples': 3000, 'test/accuracy': 0.0007183408597484231, 'test/loss': 11.203991889953613, 'test/bleu': 0.0, 'test/num_examples': 3003, 'score': 36.53304934501648, 'total_duration': 950.3838818073273, 'accumulated_submission_time': 36.53304934501648, 'accumulated_eval_time': 913.8507368564606, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (2397, {'train/accuracy': 0.5150410532951355, 'train/loss': 2.817685604095459, 'train/bleu': 22.989598778734493, 'validation/accuracy': 0.5129780173301697, 'validation/loss': 2.813084125518799, 'validation/bleu': 18.699414743449253, 'validation/num_examples': 3000, 'test/accuracy': 0.514424741268158, 'test/loss': 2.838932514190674, 'test/bleu': 17.118553953908677, 'test/num_examples': 3003, 'score': 876.5725305080414, 'total_duration': 2332.285767555237, 'accumulated_submission_time': 876.5725305080414, 'accumulated_eval_time': 1455.5565826892853, 'accumulated_logging_time': 0.01490926742553711, 'global_step': 2397, 'preemption_count': 0}), (4792, {'train/accuracy': 0.5813414454460144, 'train/loss': 2.223891258239746, 'train/bleu': 27.487418173439774, 'validation/accuracy': 0.5921316146850586, 'validation/loss': 2.123965263366699, 'validation/bleu': 23.79964857018194, 'validation/num_examples': 3000, 'test/accuracy': 0.5976943373680115, 'test/loss': 2.0871341228485107, 'test/bleu': 22.45259197259991, 'test/num_examples': 3003, 'score': 1716.5127515792847, 'total_duration': 3622.9077451229095, 'accumulated_submission_time': 1716.5127515792847, 'accumulated_eval_time': 1906.082717180252, 'accumulated_logging_time': 0.03282356262207031, 'global_step': 4792, 'preemption_count': 0}), (7187, {'train/accuracy': 0.6096985340118408, 'train/loss': 1.977616310119629, 'train/bleu': 29.002300988505382, 'validation/accuracy': 0.6188045740127563, 'validation/loss': 1.9038896560668945, 'validation/bleu': 25.322380848005757, 'validation/num_examples': 3000, 'test/accuracy': 0.6245858073234558, 'test/loss': 1.8640550374984741, 'test/bleu': 24.10552422920636, 'test/num_examples': 3003, 'score': 2556.4744362831116, 'total_duration': 4942.885024309158, 'accumulated_submission_time': 2556.4744362831116, 'accumulated_eval_time': 2385.940097808838, 'accumulated_logging_time': 0.050182342529296875, 'global_step': 7187, 'preemption_count': 0}), (9583, {'train/accuracy': 0.6120362877845764, 'train/loss': 1.9493693113327026, 'train/bleu': 30.09364765457319, 'validation/accuracy': 0.6324129104614258, 'validation/loss': 1.801072597503662, 'validation/bleu': 26.110352396083083, 'validation/num_examples': 3000, 'test/accuracy': 0.6381415724754333, 'test/loss': 1.7433513402938843, 'test/bleu': 25.507556888659064, 'test/num_examples': 3003, 'score': 3396.4902799129486, 'total_duration': 6243.432007312775, 'accumulated_submission_time': 3396.4902799129486, 'accumulated_eval_time': 2846.317569732666, 'accumulated_logging_time': 0.06990194320678711, 'global_step': 9583, 'preemption_count': 0}), (11980, {'train/accuracy': 0.6228347420692444, 'train/loss': 1.8648462295532227, 'train/bleu': 30.036355668612682, 'validation/accuracy': 0.6401503086090088, 'validation/loss': 1.7346768379211426, 'validation/bleu': 27.031084599709246, 'validation/num_examples': 3000, 'test/accuracy': 0.6497740745544434, 'test/loss': 1.676055669784546, 'test/bleu': 25.676899373263236, 'test/num_examples': 3003, 'score': 4236.61579990387, 'total_duration': 7540.3033809661865, 'accumulated_submission_time': 4236.61579990387, 'accumulated_eval_time': 3302.919132709503, 'accumulated_logging_time': 0.08744144439697266, 'global_step': 11980, 'preemption_count': 0}), (14377, {'train/accuracy': 0.6273400783538818, 'train/loss': 1.8148566484451294, 'train/bleu': 30.551528244628237, 'validation/accuracy': 0.6456381678581238, 'validation/loss': 1.6968975067138672, 'validation/bleu': 27.17611908378438, 'validation/num_examples': 3000, 'test/accuracy': 0.6536438465118408, 'test/loss': 1.638832926750183, 'test/bleu': 26.167060470432695, 'test/num_examples': 3003, 'score': 5076.692269563675, 'total_duration': 8844.690023183823, 'accumulated_submission_time': 5076.692269563675, 'accumulated_eval_time': 3767.087361097336, 'accumulated_logging_time': 0.10435199737548828, 'global_step': 14377, 'preemption_count': 0}), (16774, {'train/accuracy': 0.6278868317604065, 'train/loss': 1.8263202905654907, 'train/bleu': 30.488985996820794, 'validation/accuracy': 0.6490247845649719, 'validation/loss': 1.670182466506958, 'validation/bleu': 27.335693433445112, 'validation/num_examples': 3000, 'test/accuracy': 0.6586027145385742, 'test/loss': 1.605661153793335, 'test/bleu': 26.50105871706552, 'test/num_examples': 3003, 'score': 5916.735604524612, 'total_duration': 10169.831251859665, 'accumulated_submission_time': 5916.735604524612, 'accumulated_eval_time': 4252.042149305344, 'accumulated_logging_time': 0.12235307693481445, 'global_step': 16774, 'preemption_count': 0}), (19170, {'train/accuracy': 0.6470143795013428, 'train/loss': 1.6777598857879639, 'train/bleu': 31.80378265917511, 'validation/accuracy': 0.6508664488792419, 'validation/loss': 1.6542633771896362, 'validation/bleu': 27.611061873273393, 'validation/num_examples': 3000, 'test/accuracy': 0.6607229709625244, 'test/loss': 1.583768367767334, 'test/bleu': 26.584595224858393, 'test/num_examples': 3003, 'score': 6756.886587381363, 'total_duration': 11504.817004203796, 'accumulated_submission_time': 6756.886587381363, 'accumulated_eval_time': 4746.738303899765, 'accumulated_logging_time': 0.13991928100585938, 'global_step': 19170, 'preemption_count': 0}), (21557, {'train/accuracy': 0.6366486549377441, 'train/loss': 1.74783194065094, 'train/bleu': 31.014895561529407, 'validation/accuracy': 0.6557239294052124, 'validation/loss': 1.6255961656570435, 'validation/bleu': 27.855042440442286, 'validation/num_examples': 3000, 'test/accuracy': 0.6649634838104248, 'test/loss': 1.5605164766311646, 'test/bleu': 27.311655045351394, 'test/num_examples': 3003, 'score': 7597.073021173477, 'total_duration': 12951.468220949173, 'accumulated_submission_time': 7597.073021173477, 'accumulated_eval_time': 5353.06326174736, 'accumulated_logging_time': 0.15854406356811523, 'global_step': 21557, 'preemption_count': 0}), (23943, {'train/accuracy': 0.636714518070221, 'train/loss': 1.7608840465545654, 'train/bleu': 31.038327625725056, 'validation/accuracy': 0.6578251719474792, 'validation/loss': 1.616212010383606, 'validation/bleu': 28.055206430290824, 'validation/num_examples': 3000, 'test/accuracy': 0.6668983697891235, 'test/loss': 1.545904278755188, 'test/bleu': 27.068156211779904, 'test/num_examples': 3003, 'score': 8437.032063961029, 'total_duration': 14257.868232250214, 'accumulated_submission_time': 8437.032063961029, 'accumulated_eval_time': 5819.362867355347, 'accumulated_logging_time': 0.17757081985473633, 'global_step': 23943, 'preemption_count': 0}), (26333, {'train/accuracy': 0.6419850587844849, 'train/loss': 1.7114866971969604, 'train/bleu': 31.73044925554107, 'validation/accuracy': 0.6580228805541992, 'validation/loss': 1.6022928953170776, 'validation/bleu': 28.182939345620774, 'validation/num_examples': 3000, 'test/accuracy': 0.6678252816200256, 'test/loss': 1.5391461849212646, 'test/bleu': 27.450689213528424, 'test/num_examples': 3003, 'score': 9277.20161318779, 'total_duration': 15603.71659898758, 'accumulated_submission_time': 9277.20161318779, 'accumulated_eval_time': 6324.898819923401, 'accumulated_logging_time': 0.19710993766784668, 'global_step': 26333, 'preemption_count': 0}), (28727, {'train/accuracy': 0.6393638253211975, 'train/loss': 1.7340829372406006, 'train/bleu': 31.452897669625195, 'validation/accuracy': 0.6586655974388123, 'validation/loss': 1.5983942747116089, 'validation/bleu': 28.204650952944153, 'validation/num_examples': 3000, 'test/accuracy': 0.6711273193359375, 'test/loss': 1.5238035917282104, 'test/bleu': 27.27621764317378, 'test/num_examples': 3003, 'score': 10117.095172166824, 'total_duration': 16926.027439832687, 'accumulated_submission_time': 10117.095172166824, 'accumulated_eval_time': 6807.174488782883, 'accumulated_logging_time': 0.21719098091125488, 'global_step': 28727, 'preemption_count': 0}), (31122, {'train/accuracy': 0.6390196681022644, 'train/loss': 1.7345538139343262, 'train/bleu': 31.202942473388664, 'validation/accuracy': 0.6607668399810791, 'validation/loss': 1.5840063095092773, 'validation/bleu': 28.23258252822054, 'validation/num_examples': 3000, 'test/accuracy': 0.6736415028572083, 'test/loss': 1.5088086128234863, 'test/bleu': 27.670258463255426, 'test/num_examples': 3003, 'score': 10956.981642723083, 'total_duration': 18369.286808490753, 'accumulated_submission_time': 10956.981642723083, 'accumulated_eval_time': 7410.405072927475, 'accumulated_logging_time': 0.23796510696411133, 'global_step': 31122, 'preemption_count': 0}), (33513, {'train/accuracy': 0.6445894837379456, 'train/loss': 1.6965571641921997, 'train/bleu': 31.974314050553552, 'validation/accuracy': 0.6629669070243835, 'validation/loss': 1.5725905895233154, 'validation/bleu': 28.226341844985445, 'validation/num_examples': 3000, 'test/accuracy': 0.6736531257629395, 'test/loss': 1.496616005897522, 'test/bleu': 27.53553209945815, 'test/num_examples': 3003, 'score': 11796.972217082977, 'total_duration': 19813.574679613113, 'accumulated_submission_time': 11796.972217082977, 'accumulated_eval_time': 8014.558529138565, 'accumulated_logging_time': 0.25838708877563477, 'global_step': 33513, 'preemption_count': 0}), (35910, {'train/accuracy': 0.6458752751350403, 'train/loss': 1.6932095289230347, 'train/bleu': 31.200098308062625, 'validation/accuracy': 0.6643759608268738, 'validation/loss': 1.5690315961837769, 'validation/bleu': 28.283795968933816, 'validation/num_examples': 3000, 'test/accuracy': 0.6754489541053772, 'test/loss': 1.4884008169174194, 'test/bleu': 27.657416297099946, 'test/num_examples': 3003, 'score': 12636.863565206528, 'total_duration': 21189.14860677719, 'accumulated_submission_time': 12636.863565206528, 'accumulated_eval_time': 8550.096631288528, 'accumulated_logging_time': 0.2783670425415039, 'global_step': 35910, 'preemption_count': 0}), (38310, {'train/accuracy': 0.6566354036331177, 'train/loss': 1.6094287633895874, 'train/bleu': 32.52166016042097, 'validation/accuracy': 0.6647467613220215, 'validation/loss': 1.5627777576446533, 'validation/bleu': 28.540662170040317, 'validation/num_examples': 3000, 'test/accuracy': 0.6741397380828857, 'test/loss': 1.4912638664245605, 'test/bleu': 27.74329929828922, 'test/num_examples': 3003, 'score': 13477.06750702858, 'total_duration': 22553.682485103607, 'accumulated_submission_time': 13477.06750702858, 'accumulated_eval_time': 9074.286002159119, 'accumulated_logging_time': 0.2986142635345459, 'global_step': 38310, 'preemption_count': 0}), (40713, {'train/accuracy': 0.647389829158783, 'train/loss': 1.6865519285202026, 'train/bleu': 31.71911689368959, 'validation/accuracy': 0.6666254997253418, 'validation/loss': 1.5542771816253662, 'validation/bleu': 28.60730318712551, 'validation/num_examples': 3000, 'test/accuracy': 0.6779400110244751, 'test/loss': 1.4746785163879395, 'test/bleu': 27.964013584999243, 'test/num_examples': 3003, 'score': 14316.962766170502, 'total_duration': 24002.044580698013, 'accumulated_submission_time': 14316.962766170502, 'accumulated_eval_time': 9682.609852075577, 'accumulated_logging_time': 0.31876611709594727, 'global_step': 40713, 'preemption_count': 0}), (43116, {'train/accuracy': 0.6481766700744629, 'train/loss': 1.6767008304595947, 'train/bleu': 31.880219846211574, 'validation/accuracy': 0.6671445965766907, 'validation/loss': 1.5450067520141602, 'validation/bleu': 28.865265397169356, 'validation/num_examples': 3000, 'test/accuracy': 0.6772795915603638, 'test/loss': 1.470426321029663, 'test/bleu': 27.905053242648385, 'test/num_examples': 3003, 'score': 15156.829148292542, 'total_duration': 25351.253480434418, 'accumulated_submission_time': 15156.829148292542, 'accumulated_eval_time': 10191.80941104889, 'accumulated_logging_time': 0.3389413356781006, 'global_step': 43116, 'preemption_count': 0}), (45516, {'train/accuracy': 0.6550936698913574, 'train/loss': 1.6347965002059937, 'train/bleu': 32.895621831135855, 'validation/accuracy': 0.6682199239730835, 'validation/loss': 1.5399497747421265, 'validation/bleu': 28.751288121452326, 'validation/num_examples': 3000, 'test/accuracy': 0.6820762157440186, 'test/loss': 1.4533430337905884, 'test/bleu': 28.27365313955735, 'test/num_examples': 3003, 'score': 15997.006652116776, 'total_duration': 26793.19411945343, 'accumulated_submission_time': 15997.006652116776, 'accumulated_eval_time': 10793.432191848755, 'accumulated_logging_time': 0.35938525199890137, 'global_step': 45516, 'preemption_count': 0}), (47914, {'train/accuracy': 0.6518192291259766, 'train/loss': 1.6568970680236816, 'train/bleu': 31.86256812427604, 'validation/accuracy': 0.6698514223098755, 'validation/loss': 1.534536600112915, 'validation/bleu': 28.766959118771258, 'validation/num_examples': 3000, 'test/accuracy': 0.6813926696777344, 'test/loss': 1.452790379524231, 'test/bleu': 28.21117258492336, 'test/num_examples': 3003, 'score': 16836.99485516548, 'total_duration': 28215.143009662628, 'accumulated_submission_time': 16836.99485516548, 'accumulated_eval_time': 11375.247840166092, 'accumulated_logging_time': 0.3805251121520996, 'global_step': 47914, 'preemption_count': 0}), (50312, {'train/accuracy': 0.6775093078613281, 'train/loss': 1.4744209051132202, 'train/bleu': 33.85640050881976, 'validation/accuracy': 0.6707289814949036, 'validation/loss': 1.521946668624878, 'validation/bleu': 29.179102111317444, 'validation/num_examples': 3000, 'test/accuracy': 0.6835940480232239, 'test/loss': 1.4394971132278442, 'test/bleu': 28.46143191418479, 'test/num_examples': 3003, 'score': 17676.996727466583, 'total_duration': 29584.7672662735, 'accumulated_submission_time': 17676.996727466583, 'accumulated_eval_time': 11904.728259086609, 'accumulated_logging_time': 0.40130114555358887, 'global_step': 50312, 'preemption_count': 0}), (52711, {'train/accuracy': 0.6563645601272583, 'train/loss': 1.620229721069336, 'train/bleu': 31.95549662113436, 'validation/accuracy': 0.6721504330635071, 'validation/loss': 1.5149569511413574, 'validation/bleu': 28.721798817041176, 'validation/num_examples': 3000, 'test/accuracy': 0.6860734820365906, 'test/loss': 1.4287725687026978, 'test/bleu': 28.548794208699487, 'test/num_examples': 3003, 'score': 18517.072224140167, 'total_duration': 31149.373165607452, 'accumulated_submission_time': 18517.072224140167, 'accumulated_eval_time': 12629.118858575821, 'accumulated_logging_time': 0.4219939708709717, 'global_step': 52711, 'preemption_count': 0}), (55109, {'train/accuracy': 0.6550964117050171, 'train/loss': 1.634689211845398, 'train/bleu': 31.88800653363787, 'validation/accuracy': 0.6731762886047363, 'validation/loss': 1.5070106983184814, 'validation/bleu': 28.990632081884506, 'validation/num_examples': 3000, 'test/accuracy': 0.6839763522148132, 'test/loss': 1.4271059036254883, 'test/bleu': 28.36306301564741, 'test/num_examples': 3003, 'score': 19357.213113307953, 'total_duration': 32557.32327413559, 'accumulated_submission_time': 19357.213113307953, 'accumulated_eval_time': 13196.783860206604, 'accumulated_logging_time': 0.4434971809387207, 'global_step': 55109, 'preemption_count': 0}), (57509, {'train/accuracy': 0.6644532680511475, 'train/loss': 1.5597906112670898, 'train/bleu': 32.7555638423294, 'validation/accuracy': 0.6729537844657898, 'validation/loss': 1.502052903175354, 'validation/bleu': 28.726654201228463, 'validation/num_examples': 3000, 'test/accuracy': 0.6869887709617615, 'test/loss': 1.4207853078842163, 'test/bleu': 28.454174511035983, 'test/num_examples': 3003, 'score': 20197.1210668087, 'total_duration': 34133.86986708641, 'accumulated_submission_time': 20197.1210668087, 'accumulated_eval_time': 13933.279499053955, 'accumulated_logging_time': 0.46510958671569824, 'global_step': 57509, 'preemption_count': 0}), (59912, {'train/accuracy': 0.6640447974205017, 'train/loss': 1.5789340734481812, 'train/bleu': 32.332880526690204, 'validation/accuracy': 0.6756235957145691, 'validation/loss': 1.4878644943237305, 'validation/bleu': 29.040579486490106, 'validation/num_examples': 3000, 'test/accuracy': 0.6891205906867981, 'test/loss': 1.4018596410751343, 'test/bleu': 28.867091049338526, 'test/num_examples': 3003, 'score': 21037.191143751144, 'total_duration': 35630.34053969383, 'accumulated_submission_time': 21037.191143751144, 'accumulated_eval_time': 14589.532675504684, 'accumulated_logging_time': 0.48888468742370605, 'global_step': 59912, 'preemption_count': 0}), (62315, {'train/accuracy': 0.6565195918083191, 'train/loss': 1.621074914932251, 'train/bleu': 32.54478324472036, 'validation/accuracy': 0.6752527952194214, 'validation/loss': 1.4893181324005127, 'validation/bleu': 29.201330005934174, 'validation/num_examples': 3000, 'test/accuracy': 0.6880083680152893, 'test/loss': 1.4031387567520142, 'test/bleu': 28.933209828443132, 'test/num_examples': 3003, 'score': 21877.0513548851, 'total_duration': 36981.00573134422, 'accumulated_submission_time': 21877.0513548851, 'accumulated_eval_time': 15100.193541765213, 'accumulated_logging_time': 0.5105130672454834, 'global_step': 62315, 'preemption_count': 0}), (64714, {'train/accuracy': 0.6664870977401733, 'train/loss': 1.5533335208892822, 'train/bleu': 32.840460124877104, 'validation/accuracy': 0.6770697236061096, 'validation/loss': 1.4772292375564575, 'validation/bleu': 29.686800295739342, 'validation/num_examples': 3000, 'test/accuracy': 0.6918317675590515, 'test/loss': 1.3866792917251587, 'test/bleu': 28.976123742140455, 'test/num_examples': 3003, 'score': 22717.11159682274, 'total_duration': 38447.370161533356, 'accumulated_submission_time': 22717.11159682274, 'accumulated_eval_time': 15726.3496260643, 'accumulated_logging_time': 0.535804271697998, 'global_step': 64714, 'preemption_count': 0}), (67112, {'train/accuracy': 0.6615347862243652, 'train/loss': 1.5865970849990845, 'train/bleu': 32.70543756268278, 'validation/accuracy': 0.6775022745132446, 'validation/loss': 1.4737094640731812, 'validation/bleu': 29.48211865455347, 'validation/num_examples': 3000, 'test/accuracy': 0.6919012665748596, 'test/loss': 1.3798927068710327, 'test/bleu': 28.88451648651455, 'test/num_examples': 3003, 'score': 23557.22575187683, 'total_duration': 39948.54838728905, 'accumulated_submission_time': 23557.22575187683, 'accumulated_eval_time': 16387.26716184616, 'accumulated_logging_time': 0.5584750175476074, 'global_step': 67112, 'preemption_count': 0}), (69509, {'train/accuracy': 0.6801171898841858, 'train/loss': 1.4684436321258545, 'train/bleu': 34.06425304577874, 'validation/accuracy': 0.6802833080291748, 'validation/loss': 1.4586493968963623, 'validation/bleu': 29.7233410746792, 'validation/num_examples': 3000, 'test/accuracy': 0.6941953301429749, 'test/loss': 1.3733872175216675, 'test/bleu': 28.914641427505625, 'test/num_examples': 3003, 'score': 24397.13818502426, 'total_duration': 41425.846347332, 'accumulated_submission_time': 24397.13818502426, 'accumulated_eval_time': 17024.49896645546, 'accumulated_logging_time': 0.5844078063964844, 'global_step': 69509, 'preemption_count': 0}), (71907, {'train/accuracy': 0.6680629849433899, 'train/loss': 1.5376898050308228, 'train/bleu': 33.55331250360029, 'validation/accuracy': 0.6821373105049133, 'validation/loss': 1.4566062688827515, 'validation/bleu': 29.792228137262903, 'validation/num_examples': 3000, 'test/accuracy': 0.6962113380432129, 'test/loss': 1.367648720741272, 'test/bleu': 29.21917840567236, 'test/num_examples': 3003, 'score': 25237.290956258774, 'total_duration': 42988.84974718094, 'accumulated_submission_time': 25237.290956258774, 'accumulated_eval_time': 17747.200035572052, 'accumulated_logging_time': 0.6085481643676758, 'global_step': 71907, 'preemption_count': 0}), (74294, {'train/accuracy': 0.6646674275398254, 'train/loss': 1.5587682723999023, 'train/bleu': 33.263302911492694, 'validation/accuracy': 0.6828912496566772, 'validation/loss': 1.446242094039917, 'validation/bleu': 30.071268519250367, 'validation/num_examples': 3000, 'test/accuracy': 0.6959795951843262, 'test/loss': 1.3576279878616333, 'test/bleu': 29.49128885370752, 'test/num_examples': 3003, 'score': 26077.19045162201, 'total_duration': 44327.541499853134, 'accumulated_submission_time': 26077.19045162201, 'accumulated_eval_time': 18245.742166519165, 'accumulated_logging_time': 0.7365462779998779, 'global_step': 74294, 'preemption_count': 0}), (76686, {'train/accuracy': 0.6756840944290161, 'train/loss': 1.4906566143035889, 'train/bleu': 34.0021745665475, 'validation/accuracy': 0.6824833750724792, 'validation/loss': 1.4404916763305664, 'validation/bleu': 29.66196269686816, 'validation/num_examples': 3000, 'test/accuracy': 0.6970686912536621, 'test/loss': 1.3505268096923828, 'test/bleu': 29.542162491351487, 'test/num_examples': 3003, 'score': 26917.24090409279, 'total_duration': 45851.07866358757, 'accumulated_submission_time': 26917.24090409279, 'accumulated_eval_time': 18929.08084154129, 'accumulated_logging_time': 0.7619938850402832, 'global_step': 76686, 'preemption_count': 0}), (79081, {'train/accuracy': 0.670806884765625, 'train/loss': 1.5277334451675415, 'train/bleu': 33.65061434912588, 'validation/accuracy': 0.6856722831726074, 'validation/loss': 1.4323469400405884, 'validation/bleu': 29.820535561879318, 'validation/num_examples': 3000, 'test/accuracy': 0.7002780437469482, 'test/loss': 1.332972526550293, 'test/bleu': 29.877358267681313, 'test/num_examples': 3003, 'score': 27757.291744709015, 'total_duration': 47404.70118904114, 'accumulated_submission_time': 27757.291744709015, 'accumulated_eval_time': 19642.50058221817, 'accumulated_logging_time': 0.7903580665588379, 'global_step': 79081, 'preemption_count': 0}), (81476, {'train/accuracy': 0.6701619625091553, 'train/loss': 1.5271522998809814, 'train/bleu': 33.62950279115951, 'validation/accuracy': 0.6859689354896545, 'validation/loss': 1.4260343313217163, 'validation/bleu': 30.23455462547178, 'validation/num_examples': 3000, 'test/accuracy': 0.7002085447311401, 'test/loss': 1.3314127922058105, 'test/bleu': 29.82068094715, 'test/num_examples': 3003, 'score': 28597.327392578125, 'total_duration': 48867.12519478798, 'accumulated_submission_time': 28597.327392578125, 'accumulated_eval_time': 20264.73861527443, 'accumulated_logging_time': 0.8158400058746338, 'global_step': 81476, 'preemption_count': 0}), (83873, {'train/accuracy': 0.6786085367202759, 'train/loss': 1.469584584236145, 'train/bleu': 34.27985782652492, 'validation/accuracy': 0.6878723502159119, 'validation/loss': 1.4156548976898193, 'validation/bleu': 30.107668900345654, 'validation/num_examples': 3000, 'test/accuracy': 0.7028502225875854, 'test/loss': 1.3193279504776, 'test/bleu': 30.000158975087214, 'test/num_examples': 3003, 'score': 29437.32509827614, 'total_duration': 50412.49358844757, 'accumulated_submission_time': 29437.32509827614, 'accumulated_eval_time': 20969.96043562889, 'accumulated_logging_time': 0.8403637409210205, 'global_step': 83873, 'preemption_count': 0}), (86269, {'train/accuracy': 0.6765577793121338, 'train/loss': 1.4855766296386719, 'train/bleu': 33.85359031410758, 'validation/accuracy': 0.6886263489723206, 'validation/loss': 1.4098918437957764, 'validation/bleu': 30.391831369339755, 'validation/num_examples': 3000, 'test/accuracy': 0.7034874558448792, 'test/loss': 1.310977816581726, 'test/bleu': 30.17826562874015, 'test/num_examples': 3003, 'score': 30277.185242176056, 'total_duration': 51768.4639582634, 'accumulated_submission_time': 30277.185242176056, 'accumulated_eval_time': 21485.91948032379, 'accumulated_logging_time': 0.8669848442077637, 'global_step': 86269, 'preemption_count': 0}), (88666, {'train/accuracy': 0.6845095157623291, 'train/loss': 1.4313515424728394, 'train/bleu': 34.49995457551866, 'validation/accuracy': 0.689800500869751, 'validation/loss': 1.399889588356018, 'validation/bleu': 30.33817035510746, 'validation/num_examples': 3000, 'test/accuracy': 0.7045417428016663, 'test/loss': 1.3043636083602905, 'test/bleu': 30.348573875125766, 'test/num_examples': 3003, 'score': 31117.099716424942, 'total_duration': 53152.883013010025, 'accumulated_submission_time': 31117.099716424942, 'accumulated_eval_time': 22030.27733516693, 'accumulated_logging_time': 0.892803430557251, 'global_step': 88666, 'preemption_count': 0}), (91063, {'train/accuracy': 0.6808391213417053, 'train/loss': 1.460278868675232, 'train/bleu': 34.522505300855954, 'validation/accuracy': 0.6902578473091125, 'validation/loss': 1.3906766176223755, 'validation/bleu': 30.562124483984782, 'validation/num_examples': 3000, 'test/accuracy': 0.7068358063697815, 'test/loss': 1.2929617166519165, 'test/bleu': 30.34585374155825, 'test/num_examples': 3003, 'score': 31957.027379989624, 'total_duration': 54645.79945039749, 'accumulated_submission_time': 31957.027379989624, 'accumulated_eval_time': 22683.118382930756, 'accumulated_logging_time': 0.9178125858306885, 'global_step': 91063, 'preemption_count': 0}), (93461, {'train/accuracy': 0.6786174774169922, 'train/loss': 1.4746299982070923, 'train/bleu': 34.5016805190125, 'validation/accuracy': 0.6923096179962158, 'validation/loss': 1.3880749940872192, 'validation/bleu': 30.70616514713522, 'validation/num_examples': 3000, 'test/accuracy': 0.7077627182006836, 'test/loss': 1.288251519203186, 'test/bleu': 30.42520310523495, 'test/num_examples': 3003, 'score': 32797.17566847801, 'total_duration': 56036.15252375603, 'accumulated_submission_time': 32797.17566847801, 'accumulated_eval_time': 23233.17160320282, 'accumulated_logging_time': 0.9467217922210693, 'global_step': 93461, 'preemption_count': 0}), (95858, {'train/accuracy': 0.6888818144798279, 'train/loss': 1.4096142053604126, 'train/bleu': 34.665538855998165, 'validation/accuracy': 0.69310063123703, 'validation/loss': 1.3813670873641968, 'validation/bleu': 30.60912169938231, 'validation/num_examples': 3000, 'test/accuracy': 0.7091183066368103, 'test/loss': 1.2820110321044922, 'test/bleu': 30.694807185614742, 'test/num_examples': 3003, 'score': 33637.064775943756, 'total_duration': 57454.82578086853, 'accumulated_submission_time': 33637.064775943756, 'accumulated_eval_time': 23811.80674099922, 'accumulated_logging_time': 0.9742169380187988, 'global_step': 95858, 'preemption_count': 0}), (98256, {'train/accuracy': 0.6860619187355042, 'train/loss': 1.425062656402588, 'train/bleu': 34.27644709432437, 'validation/accuracy': 0.6928287148475647, 'validation/loss': 1.3815335035324097, 'validation/bleu': 30.582935570183754, 'validation/num_examples': 3000, 'test/accuracy': 0.7091878056526184, 'test/loss': 1.279125452041626, 'test/bleu': 30.700503734111685, 'test/num_examples': 3003, 'score': 34477.13659238815, 'total_duration': 58792.00652337074, 'accumulated_submission_time': 34477.13659238815, 'accumulated_eval_time': 24308.76831483841, 'accumulated_logging_time': 1.00032377243042, 'global_step': 98256, 'preemption_count': 0}), (100654, {'train/accuracy': 0.6953095197677612, 'train/loss': 1.3750799894332886, 'train/bleu': 35.248475580077994, 'validation/accuracy': 0.6936321258544922, 'validation/loss': 1.3750355243682861, 'validation/bleu': 30.970914219958566, 'validation/num_examples': 3000, 'test/accuracy': 0.710914134979248, 'test/loss': 1.2728192806243896, 'test/bleu': 30.864503714998758, 'test/num_examples': 3003, 'score': 35317.25174379349, 'total_duration': 60228.694583654404, 'accumulated_submission_time': 35317.25174379349, 'accumulated_eval_time': 24905.193427801132, 'accumulated_logging_time': 1.0264627933502197, 'global_step': 100654, 'preemption_count': 0})], 'global_step': 100654}
I0306 11:56:43.022033 139923228669120 submission_runner.py:649] Timing: 35317.25174379349
I0306 11:56:43.022076 139923228669120 submission_runner.py:651] Total number of evals: 43
I0306 11:56:43.022109 139923228669120 submission_runner.py:652] ====================
I0306 11:56:43.022215 139923228669120 submission_runner.py:750] Final wmt score: 4
