python submission_runner.py --framework=jax --workload=wmt --submission_path=prize_qualification_baselines/external_tuning/jax_nadamw_full_budget.py --data_dir=/data/wmt --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=submissions/rolling_leaderboard/external_tuning/baseline/study_1 --overwrite=True --save_checkpoints=False --rng_seed=-1978129121 --tuning_ruleset=external --tuning_search_space=prize_qualification_baselines/external_tuning/tuning_search_space.json --num_tuning_trials=5 --hparam_start_index=3 --hparam_end_index=4 2>&1 | tee -a /logs/wmt_jax_03-05-2025-19-12-33.log
2025-03-05 19:12:34.693598: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1741201954.718457       9 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1741201954.726262       9 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
I0305 19:12:41.690587 139780644897984 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_1/wmt_jax.
I0305 19:12:42.714577 139780644897984 xla_bridge.py:884] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA
I0305 19:12:42.717487 139780644897984 xla_bridge.py:884] Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory
I0305 19:12:42.719228 139780644897984 submission_runner.py:606] Using RNG seed -1978129121
I0305 19:12:43.330682 139780644897984 submission_runner.py:615] --- Tuning run 4/5 ---
I0305 19:12:43.330859 139780644897984 submission_runner.py:620] Creating tuning directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_1/wmt_jax/trial_4.
I0305 19:12:43.331057 139780644897984 logger_utils.py:97] Saving hparams to /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_1/wmt_jax/trial_4/hparams.json.
I0305 19:12:43.567722 139780644897984 submission_runner.py:218] Initializing dataset.
I0305 19:12:43.771397 139780644897984 dataset_info.py:690] Load dataset info from /data/wmt/wmt17_translate/de-en/1.0.0
I0305 19:12:43.833896 139780644897984 reader.py:261] Creating a tf.data.Dataset reading 16 files located in folders: /data/wmt/wmt17_translate/de-en/1.0.0.
I0305 19:12:43.910987 139780644897984 logging_logger.py:49] Constructing tf.data.Dataset wmt17_translate for split train, from /data/wmt/wmt17_translate/de-en/1.0.0
I0305 19:12:45.313860 139780644897984 submission_runner.py:229] Initializing model.
I0305 19:13:27.650909 139780644897984 submission_runner.py:272] Initializing optimizer.
I0305 19:13:28.508798 139780644897984 submission_runner.py:279] Initializing metrics bundle.
I0305 19:13:28.509054 139780644897984 submission_runner.py:301] Initializing checkpoint and logger.
I0305 19:13:28.509989 139780644897984 checkpoints.py:1101] Found no checkpoint files in /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_1/wmt_jax/trial_4 with prefix checkpoint_
I0305 19:13:28.510091 139780644897984 submission_runner.py:321] Saving meta data to /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_1/wmt_jax/trial_4/meta_data_0.json.
I0305 19:13:28.510289 139780644897984 logger_utils.py:262] Unable to record workload.train_mean information. Continuing without it.
I0305 19:13:28.510347 139780644897984 logger_utils.py:262] Unable to record workload.train_stddev information. Continuing without it.
I0305 19:13:28.695757 139780644897984 submission_runner.py:325] Saving flags to /experiment_runs/submissions/rolling_leaderboard/external_tuning/baseline/study_1/wmt_jax/trial_4/flags_0.json.
I0305 19:13:28.736447 139780644897984 submission_runner.py:337] Starting training loop.
I0305 19:13:55.359460 139644335183616 logging_writer.py:48] [0] global_step=0, grad_norm=6.251831531524658, loss=11.160120010375977
I0305 19:13:55.420395 139780644897984 spec.py:321] Evaluating on the training split.
I0305 19:13:55.422554 139780644897984 dataset_info.py:690] Load dataset info from /data/wmt/wmt17_translate/de-en/1.0.0
I0305 19:13:55.426262 139780644897984 reader.py:261] Creating a tf.data.Dataset reading 16 files located in folders: /data/wmt/wmt17_translate/de-en/1.0.0.
I0305 19:13:55.459153 139780644897984 logging_logger.py:49] Constructing tf.data.Dataset wmt17_translate for split train, from /data/wmt/wmt17_translate/de-en/1.0.0
I0305 19:14:01.728436 139780644897984 workload.py:181] Translating evaluation dataset.
I0305 19:19:09.173536 139780644897984 spec.py:333] Evaluating on the validation split.
I0305 19:19:09.232494 139780644897984 dataset_info.py:690] Load dataset info from /data/wmt/wmt14_translate/de-en/1.0.0
I0305 19:19:09.283739 139780644897984 reader.py:261] Creating a tf.data.Dataset reading 1 files located in folders: /data/wmt/wmt14_translate/de-en/1.0.0.
I0305 19:19:09.316080 139780644897984 logging_logger.py:49] Constructing tf.data.Dataset wmt14_translate for split validation, from /data/wmt/wmt14_translate/de-en/1.0.0
I0305 19:19:14.599580 139780644897984 workload.py:181] Translating evaluation dataset.
I0305 19:24:16.098042 139780644897984 spec.py:349] Evaluating on the test split.
I0305 19:24:16.100775 139780644897984 dataset_info.py:690] Load dataset info from /data/wmt/wmt14_translate/de-en/1.0.0
I0305 19:24:16.104208 139780644897984 reader.py:261] Creating a tf.data.Dataset reading 1 files located in folders: /data/wmt/wmt14_translate/de-en/1.0.0.
I0305 19:24:16.139398 139780644897984 logging_logger.py:49] Constructing tf.data.Dataset wmt14_translate for split test, from /data/wmt/wmt14_translate/de-en/1.0.0
I0305 19:24:18.987118 139780644897984 workload.py:181] Translating evaluation dataset.
I0305 19:29:20.361847 139780644897984 submission_runner.py:469] Time since start: 951.63s, 	Step: 1, 	{'train/accuracy': 0.0004679510893765837, 'train/loss': 11.170289993286133, 'train/bleu': 3.872421061078569e-11, 'validation/accuracy': 0.00048204089398495853, 'validation/loss': 11.143509864807129, 'validation/bleu': 6.893376491104542e-10, 'validation/num_examples': 3000, 'test/accuracy': 0.0007183408597484231, 'test/loss': 11.128133773803711, 'test/bleu': 0.0, 'test/num_examples': 3003, 'score': 26.683835744857788, 'total_duration': 951.6253178119659, 'accumulated_submission_time': 26.683835744857788, 'accumulated_eval_time': 924.9413712024689, 'accumulated_logging_time': 0}
I0305 19:29:20.369913 139637515323136 logging_writer.py:48] [1] accumulated_eval_time=924.941, accumulated_logging_time=0, accumulated_submission_time=26.6838, global_step=1, preemption_count=0, score=26.6838, test/accuracy=0.000718341, test/bleu=0, test/loss=11.1281, test/num_examples=3003, total_duration=951.625, train/accuracy=0.000467951, train/bleu=3.87242e-11, train/loss=11.1703, validation/accuracy=0.000482041, validation/bleu=6.89338e-10, validation/loss=11.1435, validation/num_examples=3000
I0305 19:29:55.702095 139637506930432 logging_writer.py:48] [100] global_step=100, grad_norm=0.6895171403884888, loss=7.556788921356201
I0305 19:30:30.957859 139637515323136 logging_writer.py:48] [200] global_step=200, grad_norm=0.7943550944328308, loss=6.590933799743652
I0305 19:31:06.219818 139637506930432 logging_writer.py:48] [300] global_step=300, grad_norm=0.4948449730873108, loss=5.866210460662842
I0305 19:31:41.479906 139637515323136 logging_writer.py:48] [400] global_step=400, grad_norm=0.5445959568023682, loss=5.370214462280273
I0305 19:32:16.746026 139637506930432 logging_writer.py:48] [500] global_step=500, grad_norm=0.5853019952774048, loss=5.07620096206665
I0305 19:32:52.029650 139637515323136 logging_writer.py:48] [600] global_step=600, grad_norm=0.5639015436172485, loss=4.729410171508789
I0305 19:33:27.319383 139637506930432 logging_writer.py:48] [700] global_step=700, grad_norm=0.5424275398254395, loss=4.457102298736572
I0305 19:34:02.563578 139637515323136 logging_writer.py:48] [800] global_step=800, grad_norm=0.47940900921821594, loss=4.152703762054443
I0305 19:34:37.828977 139637506930432 logging_writer.py:48] [900] global_step=900, grad_norm=0.4844275414943695, loss=3.9921748638153076
I0305 19:35:13.093147 139637515323136 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.4573361575603485, loss=3.8133208751678467
I0305 19:35:48.338401 139637506930432 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.42429736256599426, loss=3.759028196334839
I0305 19:36:23.575866 139637515323136 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.3404204547405243, loss=3.5511105060577393
I0305 19:36:58.844713 139637506930432 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.34366458654403687, loss=3.464589834213257
I0305 19:37:34.068913 139637515323136 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.29427677392959595, loss=3.4906413555145264
I0305 19:38:09.296613 139637506930432 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.34436023235321045, loss=3.263190984725952
I0305 19:38:44.538171 139637515323136 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.21994838118553162, loss=3.2366936206817627
I0305 19:39:19.744111 139637506930432 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.21523965895175934, loss=3.070140838623047
I0305 19:39:54.977151 139637515323136 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.18927133083343506, loss=3.031278371810913
I0305 19:40:30.192432 139637506930432 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.2600073218345642, loss=2.8946070671081543
I0305 19:41:05.399151 139637515323136 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.24872203171253204, loss=2.8719587326049805
I0305 19:41:40.611530 139637506930432 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.28643134236335754, loss=2.7910501956939697
I0305 19:42:15.813550 139637515323136 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.20987705886363983, loss=2.7098755836486816
I0305 19:42:51.020699 139637506930432 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.30267012119293213, loss=2.764869213104248
I0305 19:43:20.576643 139780644897984 spec.py:321] Evaluating on the training split.
I0305 19:43:23.215114 139780644897984 workload.py:181] Translating evaluation dataset.
I0305 19:47:30.770852 139780644897984 spec.py:333] Evaluating on the validation split.
I0305 19:47:33.402333 139780644897984 workload.py:181] Translating evaluation dataset.
I0305 19:50:30.256228 139780644897984 spec.py:349] Evaluating on the test split.
I0305 19:50:32.893629 139780644897984 workload.py:181] Translating evaluation dataset.
I0305 19:53:19.646552 139780644897984 submission_runner.py:469] Time since start: 2390.91s, 	Step: 2385, 	{'train/accuracy': 0.5247663259506226, 'train/loss': 2.6697239875793457, 'train/bleu': 22.93314766070593, 'validation/accuracy': 0.5305787324905396, 'validation/loss': 2.5991995334625244, 'validation/bleu': 18.903940142801527, 'validation/num_examples': 3000, 'test/accuracy': 0.5276561379432678, 'test/loss': 2.636319637298584, 'test/bleu': 17.27146107014147, 'test/num_examples': 3003, 'score': 866.7110114097595, 'total_duration': 2390.9100246429443, 'accumulated_submission_time': 866.7110114097595, 'accumulated_eval_time': 1524.0112206935883, 'accumulated_logging_time': 0.01844644546508789}
I0305 19:53:19.655349 139637515323136 logging_writer.py:48] [2385] accumulated_eval_time=1524.01, accumulated_logging_time=0.0184464, accumulated_submission_time=866.711, global_step=2385, preemption_count=0, score=866.711, test/accuracy=0.527656, test/bleu=17.2715, test/loss=2.63632, test/num_examples=3003, total_duration=2390.91, train/accuracy=0.524766, train/bleu=22.9331, train/loss=2.66972, validation/accuracy=0.530579, validation/bleu=18.9039, validation/loss=2.5992, validation/num_examples=3000
I0305 19:53:25.270011 139637506930432 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.22295373678207397, loss=2.73343825340271
I0305 19:54:00.413249 139637515323136 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.18769538402557373, loss=2.6084258556365967
I0305 19:54:35.599539 139637506930432 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.2512303292751312, loss=2.5539424419403076
I0305 19:55:10.781828 139637515323136 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.37939032912254333, loss=2.5137290954589844
I0305 19:55:45.976676 139637506930432 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.2695010304450989, loss=2.5436978340148926
I0305 19:56:21.172549 139637515323136 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.3012368977069855, loss=2.4515764713287354
I0305 19:56:56.371796 139637506930432 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.28026869893074036, loss=2.4882874488830566
I0305 19:57:31.568775 139637515323136 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.2371319979429245, loss=2.434119701385498
I0305 19:58:06.774207 139637506930432 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.26489970088005066, loss=2.394333600997925
I0305 19:58:41.992191 139637515323136 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.35311824083328247, loss=2.3319742679595947
I0305 19:59:17.201081 139637506930432 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.5589281916618347, loss=2.4966506958007812
I0305 19:59:52.397772 139637515323136 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.29635298252105713, loss=2.315835475921631
I0305 20:00:27.616101 139637506930432 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.3906644284725189, loss=2.449967861175537
I0305 20:01:02.810996 139637515323136 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.2693762183189392, loss=2.241074800491333
I0305 20:01:38.001021 139637506930432 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.28995785117149353, loss=2.2861545085906982
I0305 20:02:13.196962 139637515323136 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.30502256751060486, loss=2.3459291458129883
I0305 20:02:48.409564 139637506930432 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.43045949935913086, loss=2.3391048908233643
I0305 20:03:23.587978 139637515323136 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.31552278995513916, loss=2.260195732116699
I0305 20:03:58.761152 139637506930432 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.31213077902793884, loss=2.2757790088653564
I0305 20:04:33.944018 139637515323136 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.33971157670021057, loss=2.349449396133423
I0305 20:05:09.092240 139637506930432 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.31390124559402466, loss=2.284651756286621
I0305 20:05:44.261349 139637515323136 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.4316365718841553, loss=2.2542736530303955
I0305 20:06:19.484512 139637506930432 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.29728254675865173, loss=2.1583504676818848
I0305 20:06:54.643572 139637515323136 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.32567235827445984, loss=2.25744891166687
I0305 20:07:19.973054 139780644897984 spec.py:321] Evaluating on the training split.
I0305 20:07:22.616397 139780644897984 workload.py:181] Translating evaluation dataset.
I0305 20:10:31.692202 139780644897984 spec.py:333] Evaluating on the validation split.
I0305 20:10:34.329418 139780644897984 workload.py:181] Translating evaluation dataset.
I0305 20:13:25.614169 139780644897984 spec.py:349] Evaluating on the test split.
I0305 20:13:28.238772 139780644897984 workload.py:181] Translating evaluation dataset.
I0305 20:16:02.706842 139780644897984 submission_runner.py:469] Time since start: 3753.97s, 	Step: 4773, 	{'train/accuracy': 0.5778889060020447, 'train/loss': 2.235605478286743, 'train/bleu': 27.103742486123544, 'validation/accuracy': 0.5906607508659363, 'validation/loss': 2.106868028640747, 'validation/bleu': 23.557852701607683, 'validation/num_examples': 3000, 'test/accuracy': 0.5977870225906372, 'test/loss': 2.067180871963501, 'test/bleu': 22.192404404184018, 'test/num_examples': 3003, 'score': 1706.8607261180878, 'total_duration': 3753.970328092575, 'accumulated_submission_time': 1706.8607261180878, 'accumulated_eval_time': 2046.7449560165405, 'accumulated_logging_time': 0.03565263748168945}
I0305 20:16:02.716711 139637506930432 logging_writer.py:48] [4773] accumulated_eval_time=2046.74, accumulated_logging_time=0.0356526, accumulated_submission_time=1706.86, global_step=4773, preemption_count=0, score=1706.86, test/accuracy=0.597787, test/bleu=22.1924, test/loss=2.06718, test/num_examples=3003, total_duration=3753.97, train/accuracy=0.577889, train/bleu=27.1037, train/loss=2.23561, validation/accuracy=0.590661, validation/bleu=23.5579, validation/loss=2.10687, validation/num_examples=3000
I0305 20:16:12.574244 139637515323136 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.4977900981903076, loss=2.2806456089019775
I0305 20:16:47.689953 139637506930432 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.38971272110939026, loss=2.1429104804992676
I0305 20:17:22.845807 139637515323136 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.29889464378356934, loss=2.1801133155822754
I0305 20:17:58.006820 139637506930432 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.35298851132392883, loss=2.2142648696899414
I0305 20:18:33.183021 139637515323136 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.6361904740333557, loss=2.1906216144561768
I0305 20:19:08.347008 139637506930432 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.23498164117336273, loss=2.14048433303833
I0305 20:19:43.502010 139637515323136 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.530161440372467, loss=2.141604423522949
I0305 20:20:18.689333 139637506930432 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.26932594180107117, loss=2.2133264541625977
I0305 20:20:53.875210 139637515323136 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.8064958453178406, loss=2.1731972694396973
I0305 20:21:29.109074 139637506930432 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.3788294196128845, loss=2.2461369037628174
I0305 20:22:04.291033 139637515323136 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.2654323875904083, loss=2.111917495727539
I0305 20:22:39.468955 139637506930432 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.4035252034664154, loss=2.1608200073242188
I0305 20:23:14.641399 139637515323136 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.36284691095352173, loss=2.145894765853882
I0305 20:23:49.838631 139637506930432 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.29798415303230286, loss=2.148153781890869
I0305 20:24:25.030783 139637515323136 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.2532561123371124, loss=2.149282932281494
I0305 20:25:00.213769 139637473359616 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.34312474727630615, loss=2.2328696250915527
I0305 20:25:35.330771 139637464966912 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.4403223991394043, loss=2.198176145553589
I0305 20:26:10.438758 139637473359616 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.3147210478782654, loss=2.1901495456695557
I0305 20:26:45.561597 139637464966912 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.35110029578208923, loss=2.1484344005584717
I0305 20:27:20.682656 139637473359616 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.37594109773635864, loss=2.1481285095214844
I0305 20:27:55.804999 139637464966912 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.3113594949245453, loss=2.0538442134857178
I0305 20:28:30.919646 139637473359616 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.487045019865036, loss=2.2301931381225586
I0305 20:29:06.023777 139637464966912 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.3653896152973175, loss=2.1929776668548584
I0305 20:29:41.138368 139637473359616 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.3422674238681793, loss=2.185823440551758
I0305 20:30:02.918078 139780644897984 spec.py:321] Evaluating on the training split.
I0305 20:30:05.563404 139780644897984 workload.py:181] Translating evaluation dataset.
I0305 20:34:11.691716 139780644897984 spec.py:333] Evaluating on the validation split.
I0305 20:34:14.319841 139780644897984 workload.py:181] Translating evaluation dataset.
I0305 20:37:20.832802 139780644897984 spec.py:349] Evaluating on the test split.
I0305 20:37:23.467516 139780644897984 workload.py:181] Translating evaluation dataset.
I0305 20:40:03.278784 139780644897984 submission_runner.py:469] Time since start: 5194.54s, 	Step: 7163, 	{'train/accuracy': 0.5905581116676331, 'train/loss': 2.1190900802612305, 'train/bleu': 27.445955000969672, 'validation/accuracy': 0.6041455864906311, 'validation/loss': 2.01488995552063, 'validation/bleu': 24.608990774898974, 'validation/num_examples': 3000, 'test/accuracy': 0.6073224544525146, 'test/loss': 1.98634934425354, 'test/bleu': 22.99201854686428, 'test/num_examples': 3003, 'score': 2546.894848585129, 'total_duration': 5194.542275667191, 'accumulated_submission_time': 2546.894848585129, 'accumulated_eval_time': 2647.105609893799, 'accumulated_logging_time': 0.055379629135131836}
I0305 20:40:03.287998 139637464966912 logging_writer.py:48] [7163] accumulated_eval_time=2647.11, accumulated_logging_time=0.0553796, accumulated_submission_time=2546.89, global_step=7163, preemption_count=0, score=2546.89, test/accuracy=0.607322, test/bleu=22.992, test/loss=1.98635, test/num_examples=3003, total_duration=5194.54, train/accuracy=0.590558, train/bleu=27.446, train/loss=2.11909, validation/accuracy=0.604146, validation/bleu=24.609, validation/loss=2.01489, validation/num_examples=3000
I0305 20:40:16.593314 139637473359616 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.38961607217788696, loss=2.1655540466308594
I0305 20:40:51.658257 139637464966912 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.3974144756793976, loss=2.201139450073242
I0305 20:41:26.736917 139637473359616 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.4680265188217163, loss=2.202591896057129
I0305 20:42:01.844343 139637464966912 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.349287748336792, loss=2.1280953884124756
I0305 20:42:36.958928 139637473359616 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.3130766749382019, loss=2.142719030380249
I0305 20:43:12.091348 139637464966912 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.5049367547035217, loss=2.165081262588501
I0305 20:43:47.184846 139637473359616 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.32161691784858704, loss=2.1820507049560547
I0305 20:44:22.301132 139637464966912 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.7495721578598022, loss=2.2130563259124756
I0305 20:44:57.434509 139637473359616 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.41239383816719055, loss=2.1490039825439453
I0305 20:45:32.536052 139637464966912 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.41045355796813965, loss=2.2376928329467773
I0305 20:46:07.638453 139637473359616 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.7479670643806458, loss=2.0445291996002197
I0305 20:46:42.733421 139637464966912 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.39270490407943726, loss=2.035158395767212
I0305 20:47:17.867486 139637473359616 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.5227770805358887, loss=2.1225802898406982
I0305 20:47:52.980174 139637464966912 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.41006746888160706, loss=2.1809661388397217
I0305 20:48:28.103965 139637473359616 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.4217716157436371, loss=2.134053945541382
I0305 20:49:03.223083 139637464966912 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.309461385011673, loss=2.1637015342712402
I0305 20:49:38.316040 139637473359616 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.5653419494628906, loss=2.1549646854400635
I0305 20:50:13.441830 139637464966912 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.6885561943054199, loss=2.1339173316955566
I0305 20:50:48.551272 139637473359616 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.32165446877479553, loss=2.1330933570861816
I0305 20:51:23.666582 139637464966912 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.35815832018852234, loss=2.1432204246520996
I0305 20:51:58.789107 139637473359616 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.33407920598983765, loss=2.194544792175293
I0305 20:52:33.911271 139637464966912 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.5382003784179688, loss=2.119123935699463
I0305 20:53:09.030548 139637473359616 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.3953821659088135, loss=2.099308490753174
I0305 20:53:44.146439 139637464966912 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.45500078797340393, loss=2.0074288845062256
I0305 20:54:03.444831 139780644897984 spec.py:321] Evaluating on the training split.
I0305 20:54:06.086512 139780644897984 workload.py:181] Translating evaluation dataset.
I0305 20:57:18.056716 139780644897984 spec.py:333] Evaluating on the validation split.
I0305 20:57:20.687923 139780644897984 workload.py:181] Translating evaluation dataset.
I0305 20:59:55.981796 139780644897984 spec.py:349] Evaluating on the test split.
I0305 20:59:58.621929 139780644897984 workload.py:181] Translating evaluation dataset.
I0305 21:02:20.425610 139780644897984 submission_runner.py:469] Time since start: 6531.69s, 	Step: 9556, 	{'train/accuracy': 0.5890974998474121, 'train/loss': 2.14538311958313, 'train/bleu': 27.825029318378128, 'validation/accuracy': 0.6090895533561707, 'validation/loss': 1.9785903692245483, 'validation/bleu': 24.078346317569707, 'validation/num_examples': 3000, 'test/accuracy': 0.6140424013137817, 'test/loss': 1.9433308839797974, 'test/bleu': 22.826923148097105, 'test/num_examples': 3003, 'score': 3386.886279821396, 'total_duration': 6531.68909573555, 'accumulated_submission_time': 3386.886279821396, 'accumulated_eval_time': 3144.086328983307, 'accumulated_logging_time': 0.07317018508911133}
I0305 21:02:20.436476 139637473359616 logging_writer.py:48] [9556] accumulated_eval_time=3144.09, accumulated_logging_time=0.0731702, accumulated_submission_time=3386.89, global_step=9556, preemption_count=0, score=3386.89, test/accuracy=0.614042, test/bleu=22.8269, test/loss=1.94333, test/num_examples=3003, total_duration=6531.69, train/accuracy=0.589097, train/bleu=27.825, train/loss=2.14538, validation/accuracy=0.60909, validation/bleu=24.0783, validation/loss=1.97859, validation/num_examples=3000
I0305 21:02:36.203699 139637464966912 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.40984848141670227, loss=2.1267025470733643
I0305 21:03:11.209217 139637473359616 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.40004876255989075, loss=2.029627561569214
I0305 21:03:46.275077 139637464966912 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.3810749650001526, loss=2.041010618209839
I0305 21:04:21.319638 139637473359616 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.3551272749900818, loss=1.9523425102233887
I0305 21:04:56.410649 139637464966912 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.4225446879863739, loss=2.124279737472534
I0305 21:05:31.491119 139637473359616 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.3111376464366913, loss=2.093905210494995
I0305 21:06:06.570443 139637464966912 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.36426258087158203, loss=2.141507148742676
I0305 21:06:41.636965 139637473359616 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.4483067989349365, loss=2.1200690269470215
I0305 21:07:16.717293 139637464966912 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.38111740350723267, loss=2.172942638397217
I0305 21:07:51.801448 139637473359616 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.44848743081092834, loss=2.1306686401367188
I0305 21:08:26.874484 139637464966912 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.4742587208747864, loss=2.1648802757263184
I0305 21:09:01.961225 139637473359616 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.39785149693489075, loss=2.0421249866485596
I0305 21:09:37.048096 139637464966912 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.26056307554244995, loss=2.1624181270599365
I0305 21:10:12.108805 139637473359616 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.30688387155532837, loss=2.0024664402008057
I0305 21:10:47.189971 139637464966912 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.5657975077629089, loss=2.1373698711395264
I0305 21:11:22.260764 139637473359616 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.39070233702659607, loss=2.1193044185638428
I0305 21:11:57.372254 139637464966912 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.27695322036743164, loss=2.112424612045288
I0305 21:12:32.465966 139637473359616 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.295427531003952, loss=2.092803955078125
I0305 21:13:07.595789 139637464966912 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.6245231628417969, loss=2.1353518962860107
I0305 21:13:42.675793 139637473359616 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.4759731888771057, loss=2.030987501144409
I0305 21:14:17.783110 139637464966912 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.2920190691947937, loss=2.0564723014831543
I0305 21:14:52.875876 139637473359616 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.6128429770469666, loss=2.0520875453948975
I0305 21:15:27.968517 139637464966912 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.5115292072296143, loss=2.1161880493164062
I0305 21:16:03.092734 139637473359616 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.3769614100456238, loss=2.137894630432129
I0305 21:16:20.642707 139780644897984 spec.py:321] Evaluating on the training split.
I0305 21:16:23.287133 139780644897984 workload.py:181] Translating evaluation dataset.
I0305 21:20:00.379648 139780644897984 spec.py:333] Evaluating on the validation split.
I0305 21:20:03.013337 139780644897984 workload.py:181] Translating evaluation dataset.
I0305 21:23:00.910197 139780644897984 spec.py:349] Evaluating on the test split.
I0305 21:23:03.547036 139780644897984 workload.py:181] Translating evaluation dataset.
I0305 21:25:47.766542 139780644897984 submission_runner.py:469] Time since start: 7939.03s, 	Step: 11951, 	{'train/accuracy': 0.5928311347961426, 'train/loss': 2.1261472702026367, 'train/bleu': 28.71801485509682, 'validation/accuracy': 0.6152077913284302, 'validation/loss': 1.9476748704910278, 'validation/bleu': 25.019054489361487, 'validation/num_examples': 3000, 'test/accuracy': 0.6206580996513367, 'test/loss': 1.8979413509368896, 'test/bleu': 23.93205123087907, 'test/num_examples': 3003, 'score': 4226.929615020752, 'total_duration': 7939.030020713806, 'accumulated_submission_time': 4226.929615020752, 'accumulated_eval_time': 3711.210105419159, 'accumulated_logging_time': 0.09382152557373047}
I0305 21:25:47.777255 139637464966912 logging_writer.py:48] [11951] accumulated_eval_time=3711.21, accumulated_logging_time=0.0938215, accumulated_submission_time=4226.93, global_step=11951, preemption_count=0, score=4226.93, test/accuracy=0.620658, test/bleu=23.9321, test/loss=1.89794, test/num_examples=3003, total_duration=7939.03, train/accuracy=0.592831, train/bleu=28.718, train/loss=2.12615, validation/accuracy=0.615208, validation/bleu=25.0191, validation/loss=1.94767, validation/num_examples=3000
I0305 21:26:05.290337 139637473359616 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.2835363447666168, loss=2.0475547313690186
I0305 21:26:40.296323 139637464966912 logging_writer.py:48] [12100] global_step=12100, grad_norm=0.4410468637943268, loss=2.026933193206787
I0305 21:27:15.339166 139637473359616 logging_writer.py:48] [12200] global_step=12200, grad_norm=0.3074534833431244, loss=2.085099697113037
I0305 21:27:50.415480 139637464966912 logging_writer.py:48] [12300] global_step=12300, grad_norm=0.5559307932853699, loss=2.075573444366455
I0305 21:28:25.510034 139637473359616 logging_writer.py:48] [12400] global_step=12400, grad_norm=0.2796314060688019, loss=2.1294727325439453
I0305 21:29:00.632081 139637464966912 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.34722763299942017, loss=2.1344783306121826
I0305 21:29:35.769323 139637473359616 logging_writer.py:48] [12600] global_step=12600, grad_norm=0.6999594569206238, loss=2.057497024536133
I0305 21:30:10.942135 139637464966912 logging_writer.py:48] [12700] global_step=12700, grad_norm=0.25116968154907227, loss=2.152766466140747
I0305 21:30:46.135253 139637473359616 logging_writer.py:48] [12800] global_step=12800, grad_norm=0.7747348546981812, loss=2.0685842037200928
I0305 21:31:21.303646 139637464966912 logging_writer.py:48] [12900] global_step=12900, grad_norm=0.5167492628097534, loss=2.0528273582458496
I0305 21:31:56.473609 139637473359616 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.7156830430030823, loss=2.143784761428833
I0305 21:32:31.629598 139637464966912 logging_writer.py:48] [13100] global_step=13100, grad_norm=0.2973765432834625, loss=2.0911552906036377
I0305 21:33:06.792125 139637473359616 logging_writer.py:48] [13200] global_step=13200, grad_norm=0.30329230427742004, loss=1.9844495058059692
I0305 21:33:41.954456 139637464966912 logging_writer.py:48] [13300] global_step=13300, grad_norm=0.3223801851272583, loss=2.0567643642425537
I0305 21:34:17.121034 139637473359616 logging_writer.py:48] [13400] global_step=13400, grad_norm=0.346615195274353, loss=2.2392172813415527
I0305 21:34:52.308742 139637464966912 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.49618175625801086, loss=2.1839802265167236
I0305 21:35:27.462832 139637473359616 logging_writer.py:48] [13600] global_step=13600, grad_norm=0.4818422198295593, loss=2.1350200176239014
I0305 21:36:02.616849 139637464966912 logging_writer.py:48] [13700] global_step=13700, grad_norm=0.5926491022109985, loss=2.0579235553741455
I0305 21:36:37.729036 139637473359616 logging_writer.py:48] [13800] global_step=13800, grad_norm=0.5042878985404968, loss=2.137359857559204
I0305 21:37:12.869787 139637464966912 logging_writer.py:48] [13900] global_step=13900, grad_norm=0.6604783535003662, loss=2.105006456375122
I0305 21:37:48.130618 139637473359616 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.6382244229316711, loss=2.1150777339935303
I0305 21:38:23.324485 139637464966912 logging_writer.py:48] [14100] global_step=14100, grad_norm=0.5687538981437683, loss=2.1555428504943848
I0305 21:38:58.497805 139637473359616 logging_writer.py:48] [14200] global_step=14200, grad_norm=0.2944907546043396, loss=2.1613101959228516
I0305 21:39:33.657777 139637464966912 logging_writer.py:48] [14300] global_step=14300, grad_norm=0.6222896575927734, loss=2.049663782119751
I0305 21:39:48.076272 139780644897984 spec.py:321] Evaluating on the training split.
I0305 21:39:50.724431 139780644897984 workload.py:181] Translating evaluation dataset.
I0305 21:42:41.861356 139780644897984 spec.py:333] Evaluating on the validation split.
I0305 21:42:44.497494 139780644897984 workload.py:181] Translating evaluation dataset.
I0305 21:45:29.916810 139780644897984 spec.py:349] Evaluating on the test split.
I0305 21:45:32.546423 139780644897984 workload.py:181] Translating evaluation dataset.
I0305 21:48:20.704614 139780644897984 submission_runner.py:469] Time since start: 9291.97s, 	Step: 14342, 	{'train/accuracy': 0.5996617674827576, 'train/loss': 2.0683419704437256, 'train/bleu': 27.678183844809066, 'validation/accuracy': 0.6153560876846313, 'validation/loss': 1.9270122051239014, 'validation/bleu': 24.81260005652625, 'validation/num_examples': 3000, 'test/accuracy': 0.6220600008964539, 'test/loss': 1.8860790729522705, 'test/bleu': 23.592547791068764, 'test/num_examples': 3003, 'score': 5067.073660612106, 'total_duration': 9291.968108415604, 'accumulated_submission_time': 5067.073660612106, 'accumulated_eval_time': 4223.838396072388, 'accumulated_logging_time': 0.11299443244934082}
I0305 21:48:20.714599 139637473359616 logging_writer.py:48] [14342] accumulated_eval_time=4223.84, accumulated_logging_time=0.112994, accumulated_submission_time=5067.07, global_step=14342, preemption_count=0, score=5067.07, test/accuracy=0.62206, test/bleu=23.5925, test/loss=1.88608, test/num_examples=3003, total_duration=9291.97, train/accuracy=0.599662, train/bleu=27.6782, train/loss=2.06834, validation/accuracy=0.615356, validation/bleu=24.8126, validation/loss=1.92701, validation/num_examples=3000
I0305 21:48:41.409989 139637464966912 logging_writer.py:48] [14400] global_step=14400, grad_norm=0.5252788662910461, loss=2.0256705284118652
I0305 21:49:16.533823 139637473359616 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.6142153739929199, loss=2.1052932739257812
I0305 21:49:51.640350 139637464966912 logging_writer.py:48] [14600] global_step=14600, grad_norm=0.37961116433143616, loss=2.017163038253784
I0305 21:50:26.781668 139637473359616 logging_writer.py:48] [14700] global_step=14700, grad_norm=0.34495073556900024, loss=2.041341543197632
I0305 21:51:01.941459 139637464966912 logging_writer.py:48] [14800] global_step=14800, grad_norm=0.39199796319007874, loss=2.086315631866455
I0305 21:51:37.074304 139637473359616 logging_writer.py:48] [14900] global_step=14900, grad_norm=0.701056182384491, loss=2.0368382930755615
I0305 21:52:12.221616 139637464966912 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.4924539029598236, loss=2.0332396030426025
I0305 21:52:47.409919 139637473359616 logging_writer.py:48] [15100] global_step=15100, grad_norm=0.45728814601898193, loss=1.9822918176651
I0305 21:53:22.567340 139637464966912 logging_writer.py:48] [15200] global_step=15200, grad_norm=0.41739851236343384, loss=2.056183338165283
I0305 21:53:57.732420 139637473359616 logging_writer.py:48] [15300] global_step=15300, grad_norm=0.4507118761539459, loss=2.0941624641418457
I0305 21:54:32.930968 139637464966912 logging_writer.py:48] [15400] global_step=15400, grad_norm=0.4261448383331299, loss=2.1032426357269287
I0305 21:55:08.116806 139637473359616 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.9505048394203186, loss=2.1164045333862305
I0305 21:55:43.303305 139637464966912 logging_writer.py:48] [15600] global_step=15600, grad_norm=0.7086944580078125, loss=2.1131458282470703
I0305 21:56:18.480112 139637473359616 logging_writer.py:48] [15700] global_step=15700, grad_norm=0.4080231189727783, loss=2.126732349395752
I0305 21:56:53.667467 139637464966912 logging_writer.py:48] [15800] global_step=15800, grad_norm=0.31983789801597595, loss=2.0538811683654785
I0305 21:57:28.840820 139637473359616 logging_writer.py:48] [15900] global_step=15900, grad_norm=0.7169744372367859, loss=2.1071906089782715
I0305 21:58:04.005277 139637464966912 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.43652206659317017, loss=2.0243499279022217
I0305 21:58:39.157925 139637473359616 logging_writer.py:48] [16100] global_step=16100, grad_norm=0.4227633476257324, loss=2.05898118019104
I0305 21:59:14.341143 139637464966912 logging_writer.py:48] [16200] global_step=16200, grad_norm=0.509998619556427, loss=1.9804086685180664
I0305 21:59:49.499017 139637473359616 logging_writer.py:48] [16300] global_step=16300, grad_norm=0.2693011462688446, loss=2.078216552734375
I0305 22:00:24.663699 139637464966912 logging_writer.py:48] [16400] global_step=16400, grad_norm=0.2665097117424011, loss=2.083420753479004
I0305 22:00:59.853488 139637473359616 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.4188559055328369, loss=2.029357671737671
I0305 22:01:35.014042 139637464966912 logging_writer.py:48] [16600] global_step=16600, grad_norm=0.2907355725765228, loss=1.9719618558883667
I0305 22:02:10.209525 139637473359616 logging_writer.py:48] [16700] global_step=16700, grad_norm=0.3569895327091217, loss=2.131071090698242
I0305 22:02:20.772220 139780644897984 spec.py:321] Evaluating on the training split.
I0305 22:02:23.412384 139780644897984 workload.py:181] Translating evaluation dataset.
I0305 22:05:12.390713 139780644897984 spec.py:333] Evaluating on the validation split.
I0305 22:05:15.023417 139780644897984 workload.py:181] Translating evaluation dataset.
I0305 22:08:08.620159 139780644897984 spec.py:349] Evaluating on the test split.
I0305 22:08:11.248408 139780644897984 workload.py:181] Translating evaluation dataset.
I0305 22:10:55.892030 139780644897984 submission_runner.py:469] Time since start: 10647.16s, 	Step: 16731, 	{'train/accuracy': 0.6019520163536072, 'train/loss': 2.053189516067505, 'train/bleu': 27.9792749992159, 'validation/accuracy': 0.617741584777832, 'validation/loss': 1.9281656742095947, 'validation/bleu': 24.760776842539137, 'validation/num_examples': 3000, 'test/accuracy': 0.6216197609901428, 'test/loss': 1.8789961338043213, 'test/bleu': 23.88516098841343, 'test/num_examples': 3003, 'score': 5906.990766048431, 'total_duration': 10647.155527353287, 'accumulated_submission_time': 5906.990766048431, 'accumulated_eval_time': 4738.958157539368, 'accumulated_logging_time': 0.13117384910583496}
I0305 22:10:55.902587 139637464966912 logging_writer.py:48] [16731] accumulated_eval_time=4738.96, accumulated_logging_time=0.131174, accumulated_submission_time=5906.99, global_step=16731, preemption_count=0, score=5906.99, test/accuracy=0.62162, test/bleu=23.8852, test/loss=1.879, test/num_examples=3003, total_duration=10647.2, train/accuracy=0.601952, train/bleu=27.9793, train/loss=2.05319, validation/accuracy=0.617742, validation/bleu=24.7608, validation/loss=1.92817, validation/num_examples=3000
I0305 22:11:20.442882 139637473359616 logging_writer.py:48] [16800] global_step=16800, grad_norm=0.45390942692756653, loss=2.096547842025757
I0305 22:11:55.521652 139637464966912 logging_writer.py:48] [16900] global_step=16900, grad_norm=0.5789089798927307, loss=2.11572003364563
I0305 22:12:30.657852 139637473359616 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.3050100803375244, loss=2.070993423461914
I0305 22:13:05.794632 139637464966912 logging_writer.py:48] [17100] global_step=17100, grad_norm=0.28603681921958923, loss=2.1219029426574707
I0305 22:13:40.944140 139637473359616 logging_writer.py:48] [17200] global_step=17200, grad_norm=0.2766692340373993, loss=2.0699994564056396
I0305 22:14:16.113192 139637464966912 logging_writer.py:48] [17300] global_step=17300, grad_norm=0.36024507880210876, loss=2.0443246364593506
I0305 22:14:51.286069 139637473359616 logging_writer.py:48] [17400] global_step=17400, grad_norm=0.45209255814552307, loss=2.0546042919158936
I0305 22:15:26.471005 139637464966912 logging_writer.py:48] [17500] global_step=17500, grad_norm=1.0308740139007568, loss=2.087048292160034
I0305 22:16:01.647727 139637473359616 logging_writer.py:48] [17600] global_step=17600, grad_norm=0.3629976212978363, loss=2.0574052333831787
I0305 22:16:36.792210 139637464966912 logging_writer.py:48] [17700] global_step=17700, grad_norm=0.4539610743522644, loss=2.0476107597351074
I0305 22:17:11.960167 139637473359616 logging_writer.py:48] [17800] global_step=17800, grad_norm=0.38173261284828186, loss=2.1002726554870605
I0305 22:17:47.103581 139637464966912 logging_writer.py:48] [17900] global_step=17900, grad_norm=0.25621458888053894, loss=2.0289883613586426
I0305 22:18:22.252078 139637473359616 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.44381454586982727, loss=2.1330718994140625
I0305 22:18:57.428754 139637464966912 logging_writer.py:48] [18100] global_step=18100, grad_norm=0.33249154686927795, loss=2.115079164505005
I0305 22:19:32.572721 139637473359616 logging_writer.py:48] [18200] global_step=18200, grad_norm=0.31373995542526245, loss=2.114358425140381
I0305 22:20:07.729717 139637464966912 logging_writer.py:48] [18300] global_step=18300, grad_norm=0.45225855708122253, loss=2.1412956714630127
I0305 22:20:42.844281 139637473359616 logging_writer.py:48] [18400] global_step=18400, grad_norm=0.30960938334465027, loss=2.1435546875
I0305 22:21:17.983673 139637464966912 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.3175438642501831, loss=2.014216184616089
I0305 22:21:53.120518 139637473359616 logging_writer.py:48] [18600] global_step=18600, grad_norm=0.42594996094703674, loss=2.066209077835083
I0305 22:22:28.266158 139637464966912 logging_writer.py:48] [18700] global_step=18700, grad_norm=0.5986203551292419, loss=2.1378910541534424
I0305 22:23:03.407138 139637473359616 logging_writer.py:48] [18800] global_step=18800, grad_norm=0.36583399772644043, loss=2.0034995079040527
I0305 22:23:38.751619 139637464966912 logging_writer.py:48] [18900] global_step=18900, grad_norm=0.26123568415641785, loss=2.107668161392212
I0305 22:24:13.989256 139637473359616 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.6525834202766418, loss=2.0458948612213135
I0305 22:24:49.246853 139637464966912 logging_writer.py:48] [19100] global_step=19100, grad_norm=0.42301541566848755, loss=2.0690293312072754
I0305 22:24:55.957764 139780644897984 spec.py:321] Evaluating on the training split.
I0305 22:24:58.608285 139780644897984 workload.py:181] Translating evaluation dataset.
I0305 22:27:59.559199 139780644897984 spec.py:333] Evaluating on the validation split.
I0305 22:28:02.205800 139780644897984 workload.py:181] Translating evaluation dataset.
I0305 22:30:51.597189 139780644897984 spec.py:349] Evaluating on the test split.
I0305 22:30:54.247470 139780644897984 workload.py:181] Translating evaluation dataset.
I0305 22:33:26.689415 139780644897984 submission_runner.py:469] Time since start: 11997.95s, 	Step: 19120, 	{'train/accuracy': 0.6120039224624634, 'train/loss': 1.9472448825836182, 'train/bleu': 29.03631012554525, 'validation/accuracy': 0.6171359419822693, 'validation/loss': 1.9207147359848022, 'validation/bleu': 25.140309653101635, 'validation/num_examples': 3000, 'test/accuracy': 0.6252577900886536, 'test/loss': 1.8584861755371094, 'test/bleu': 23.881801119375208, 'test/num_examples': 3003, 'score': 6746.903298616409, 'total_duration': 11997.952909231186, 'accumulated_submission_time': 6746.903298616409, 'accumulated_eval_time': 5249.689759731293, 'accumulated_logging_time': 0.14988160133361816}
I0305 22:33:26.701245 139637473359616 logging_writer.py:48] [19120] accumulated_eval_time=5249.69, accumulated_logging_time=0.149882, accumulated_submission_time=6746.9, global_step=19120, preemption_count=0, score=6746.9, test/accuracy=0.625258, test/bleu=23.8818, test/loss=1.85849, test/num_examples=3003, total_duration=11998, train/accuracy=0.612004, train/bleu=29.0363, train/loss=1.94724, validation/accuracy=0.617136, validation/bleu=25.1403, validation/loss=1.92071, validation/num_examples=3000
I0305 22:33:55.185052 139637464966912 logging_writer.py:48] [19200] global_step=19200, grad_norm=0.29299303889274597, loss=2.0568859577178955
I0305 22:34:30.433079 139637473359616 logging_writer.py:48] [19300] global_step=19300, grad_norm=0.26537132263183594, loss=2.09517765045166
I0305 22:35:05.697579 139637464966912 logging_writer.py:48] [19400] global_step=19400, grad_norm=0.2620205581188202, loss=2.020089626312256
I0305 22:35:40.970070 139637473359616 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.33182746171951294, loss=2.045884847640991
I0305 22:36:16.244603 139637464966912 logging_writer.py:48] [19600] global_step=19600, grad_norm=0.5993199944496155, loss=2.175262689590454
I0305 22:36:51.549812 139637473359616 logging_writer.py:48] [19700] global_step=19700, grad_norm=0.40134015679359436, loss=2.020928382873535
I0305 22:37:26.835245 139637464966912 logging_writer.py:48] [19800] global_step=19800, grad_norm=0.31801700592041016, loss=2.0385448932647705
I0305 22:38:02.149415 139637473359616 logging_writer.py:48] [19900] global_step=19900, grad_norm=0.4351758360862732, loss=2.022777795791626
I0305 22:38:37.447227 139637464966912 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.37259364128112793, loss=2.101452112197876
I0305 22:39:12.763237 139637473359616 logging_writer.py:48] [20100] global_step=20100, grad_norm=0.484468936920166, loss=2.0345263481140137
I0305 22:39:48.050625 139637464966912 logging_writer.py:48] [20200] global_step=20200, grad_norm=0.6273060441017151, loss=2.11775279045105
I0305 22:40:23.363961 139637473359616 logging_writer.py:48] [20300] global_step=20300, grad_norm=0.33410829305648804, loss=2.108684778213501
I0305 22:40:58.689038 139637464966912 logging_writer.py:48] [20400] global_step=20400, grad_norm=0.361054927110672, loss=2.011098861694336
I0305 22:41:33.982610 139637473359616 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.544623851776123, loss=2.056549310684204
I0305 22:42:09.260708 139637464966912 logging_writer.py:48] [20600] global_step=20600, grad_norm=0.47097048163414, loss=2.044774055480957
I0305 22:42:44.591997 139637473359616 logging_writer.py:48] [20700] global_step=20700, grad_norm=0.45852506160736084, loss=2.059697151184082
I0305 22:43:19.904955 139637464966912 logging_writer.py:48] [20800] global_step=20800, grad_norm=0.3389829695224762, loss=2.18009877204895
I0305 22:43:55.206422 139637473359616 logging_writer.py:48] [20900] global_step=20900, grad_norm=0.49292343854904175, loss=2.1388285160064697
I0305 22:44:30.526837 139637464966912 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.43535420298576355, loss=2.0860445499420166
I0305 22:45:05.830743 139637473359616 logging_writer.py:48] [21100] global_step=21100, grad_norm=0.2699000835418701, loss=2.1827099323272705
I0305 22:45:41.101191 139637464966912 logging_writer.py:48] [21200] global_step=21200, grad_norm=0.46071240305900574, loss=2.088449716567993
I0305 22:46:16.430331 139637473359616 logging_writer.py:48] [21300] global_step=21300, grad_norm=0.3554166257381439, loss=1.9817848205566406
I0305 22:46:51.715394 139637464966912 logging_writer.py:48] [21400] global_step=21400, grad_norm=0.4573245942592621, loss=2.0705859661102295
I0305 22:47:27.003793 139637473359616 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.5281482338905334, loss=1.973505973815918
I0305 22:47:27.012215 139780644897984 spec.py:321] Evaluating on the training split.
I0305 22:47:29.658884 139780644897984 workload.py:181] Translating evaluation dataset.
I0305 22:51:46.739915 139780644897984 spec.py:333] Evaluating on the validation split.
I0305 22:51:49.380647 139780644897984 workload.py:181] Translating evaluation dataset.
I0305 22:55:01.172316 139780644897984 spec.py:349] Evaluating on the test split.
I0305 22:55:03.810038 139780644897984 workload.py:181] Translating evaluation dataset.
I0305 22:57:44.296278 139780644897984 submission_runner.py:469] Time since start: 13455.56s, 	Step: 21501, 	{'train/accuracy': 0.6029502153396606, 'train/loss': 2.051133871078491, 'train/bleu': 28.826967214554323, 'validation/accuracy': 0.6226856112480164, 'validation/loss': 1.884505033493042, 'validation/bleu': 25.488042749944302, 'validation/num_examples': 3000, 'test/accuracy': 0.6279805302619934, 'test/loss': 1.838364839553833, 'test/bleu': 24.15120927112284, 'test/num_examples': 3003, 'score': 7587.070657968521, 'total_duration': 13455.559773921967, 'accumulated_submission_time': 7587.070657968521, 'accumulated_eval_time': 5866.973753452301, 'accumulated_logging_time': 0.17002391815185547}
I0305 22:57:44.307821 139637464966912 logging_writer.py:48] [21501] accumulated_eval_time=5866.97, accumulated_logging_time=0.170024, accumulated_submission_time=7587.07, global_step=21501, preemption_count=0, score=7587.07, test/accuracy=0.627981, test/bleu=24.1512, test/loss=1.83836, test/num_examples=3003, total_duration=13455.6, train/accuracy=0.60295, train/bleu=28.827, train/loss=2.05113, validation/accuracy=0.622686, validation/bleu=25.488, validation/loss=1.88451, validation/num_examples=3000
I0305 22:58:19.463577 139637473359616 logging_writer.py:48] [21600] global_step=21600, grad_norm=0.3289901614189148, loss=1.9971033334732056
I0305 22:58:54.692398 139637464966912 logging_writer.py:48] [21700] global_step=21700, grad_norm=0.38728925585746765, loss=1.9575022459030151
I0305 22:59:29.957039 139637473359616 logging_writer.py:48] [21800] global_step=21800, grad_norm=0.42544636130332947, loss=2.07161808013916
I0305 23:00:05.210520 139637464966912 logging_writer.py:48] [21900] global_step=21900, grad_norm=0.652475118637085, loss=2.068086862564087
I0305 23:00:40.467917 139637473359616 logging_writer.py:48] [22000] global_step=22000, grad_norm=0.3807642161846161, loss=2.072235584259033
I0305 23:01:15.722747 139637464966912 logging_writer.py:48] [22100] global_step=22100, grad_norm=0.3615970015525818, loss=2.001408100128174
I0305 23:01:51.003114 139637473359616 logging_writer.py:48] [22200] global_step=22200, grad_norm=0.25908565521240234, loss=2.0840821266174316
I0305 23:02:26.287117 139637464966912 logging_writer.py:48] [22300] global_step=22300, grad_norm=0.3268403112888336, loss=1.9781683683395386
I0305 23:03:01.585161 139637473359616 logging_writer.py:48] [22400] global_step=22400, grad_norm=0.34566619992256165, loss=2.073453903198242
I0305 23:03:36.877420 139637464966912 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.5958392024040222, loss=1.9864592552185059
I0305 23:04:12.162728 139637473359616 logging_writer.py:48] [22600] global_step=22600, grad_norm=0.3163633942604065, loss=2.048475742340088
I0305 23:04:47.488430 139637464966912 logging_writer.py:48] [22700] global_step=22700, grad_norm=0.2820732295513153, loss=2.0941333770751953
I0305 23:05:22.815773 139637473359616 logging_writer.py:48] [22800] global_step=22800, grad_norm=0.28493988513946533, loss=2.0224294662475586
I0305 23:05:58.104248 139637464966912 logging_writer.py:48] [22900] global_step=22900, grad_norm=0.46847471594810486, loss=2.1110081672668457
I0305 23:06:33.411031 139637473359616 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.48719286918640137, loss=2.0240185260772705
I0305 23:07:08.726776 139637464966912 logging_writer.py:48] [23100] global_step=23100, grad_norm=0.41084784269332886, loss=2.0057923793792725
I0305 23:07:44.053641 139637473359616 logging_writer.py:48] [23200] global_step=23200, grad_norm=0.3389676511287689, loss=2.012234687805176
I0305 23:08:19.397551 139637464966912 logging_writer.py:48] [23300] global_step=23300, grad_norm=0.3956042230129242, loss=2.0658860206604004
I0305 23:08:54.700414 139637473359616 logging_writer.py:48] [23400] global_step=23400, grad_norm=0.3753185272216797, loss=2.053697109222412
I0305 23:09:30.006662 139637464966912 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.31717804074287415, loss=2.1032845973968506
I0305 23:10:05.321232 139637473359616 logging_writer.py:48] [23600] global_step=23600, grad_norm=0.4029789865016937, loss=2.023529052734375
I0305 23:10:40.620091 139637464966912 logging_writer.py:48] [23700] global_step=23700, grad_norm=0.29309892654418945, loss=1.9676477909088135
I0305 23:11:15.924413 139637473359616 logging_writer.py:48] [23800] global_step=23800, grad_norm=0.2807651162147522, loss=2.129958152770996
I0305 23:11:44.516207 139780644897984 spec.py:321] Evaluating on the training split.
I0305 23:11:47.165600 139780644897984 workload.py:181] Translating evaluation dataset.
I0305 23:14:54.254023 139780644897984 spec.py:333] Evaluating on the validation split.
I0305 23:14:56.896315 139780644897984 workload.py:181] Translating evaluation dataset.
I0305 23:17:41.444715 139780644897984 spec.py:349] Evaluating on the test split.
I0305 23:17:44.079876 139780644897984 workload.py:181] Translating evaluation dataset.
I0305 23:20:18.199602 139780644897984 submission_runner.py:469] Time since start: 14809.46s, 	Step: 23882, 	{'train/accuracy': 0.6027191281318665, 'train/loss': 2.0590901374816895, 'train/bleu': 28.5559862475143, 'validation/accuracy': 0.621795654296875, 'validation/loss': 1.8877594470977783, 'validation/bleu': 25.326515955964172, 'validation/num_examples': 3000, 'test/accuracy': 0.6278530955314636, 'test/loss': 1.838442325592041, 'test/bleu': 24.210287195888444, 'test/num_examples': 3003, 'score': 8427.136903524399, 'total_duration': 14809.463087320328, 'accumulated_submission_time': 8427.136903524399, 'accumulated_eval_time': 6380.6570925712585, 'accumulated_logging_time': 0.1900310516357422}
I0305 23:20:18.212845 139637464966912 logging_writer.py:48] [23882] accumulated_eval_time=6380.66, accumulated_logging_time=0.190031, accumulated_submission_time=8427.14, global_step=23882, preemption_count=0, score=8427.14, test/accuracy=0.627853, test/bleu=24.2103, test/loss=1.83844, test/num_examples=3003, total_duration=14809.5, train/accuracy=0.602719, train/bleu=28.556, train/loss=2.05909, validation/accuracy=0.621796, validation/bleu=25.3265, validation/loss=1.88776, validation/num_examples=3000
I0305 23:20:24.908888 139637473359616 logging_writer.py:48] [23900] global_step=23900, grad_norm=0.38995957374572754, loss=2.0949716567993164
I0305 23:21:00.133441 139637464966912 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.3744640648365021, loss=2.067305088043213
I0305 23:21:35.376792 139637473359616 logging_writer.py:48] [24100] global_step=24100, grad_norm=0.31099629402160645, loss=1.9751874208450317
I0305 23:22:10.611277 139637464966912 logging_writer.py:48] [24200] global_step=24200, grad_norm=0.2800189256668091, loss=2.031541585922241
I0305 23:22:45.928562 139637473359616 logging_writer.py:48] [24300] global_step=24300, grad_norm=0.3614005744457245, loss=2.0423169136047363
I0305 23:23:21.213340 139637464966912 logging_writer.py:48] [24400] global_step=24400, grad_norm=0.34122234582901, loss=2.0318286418914795
I0305 23:23:56.491305 139637473359616 logging_writer.py:48] [24500] global_step=24500, grad_norm=0.39357420802116394, loss=2.029031753540039
I0305 23:24:31.812624 139637464966912 logging_writer.py:48] [24600] global_step=24600, grad_norm=0.3533063530921936, loss=1.9917536973953247
I0305 23:25:07.104399 139637473359616 logging_writer.py:48] [24700] global_step=24700, grad_norm=0.33089590072631836, loss=2.0452847480773926
I0305 23:25:42.424845 139637464966912 logging_writer.py:48] [24800] global_step=24800, grad_norm=0.29689106345176697, loss=1.9994285106658936
I0305 23:26:17.734894 139637473359616 logging_writer.py:48] [24900] global_step=24900, grad_norm=0.38503190875053406, loss=2.029472589492798
I0305 23:26:53.028276 139637464966912 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.2751193046569824, loss=2.0138063430786133
I0305 23:27:28.299365 139637473359616 logging_writer.py:48] [25100] global_step=25100, grad_norm=0.3749070167541504, loss=2.0373036861419678
I0305 23:28:03.485255 139637464966912 logging_writer.py:48] [25200] global_step=25200, grad_norm=0.2527189254760742, loss=2.0302491188049316
I0305 23:28:38.653689 139637473359616 logging_writer.py:48] [25300] global_step=25300, grad_norm=0.27943533658981323, loss=2.0287668704986572
I0305 23:29:13.833164 139637464966912 logging_writer.py:48] [25400] global_step=25400, grad_norm=0.26754653453826904, loss=2.125168800354004
I0305 23:29:48.992312 139637473359616 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.267507404088974, loss=2.098665714263916
I0305 23:30:24.134038 139637464966912 logging_writer.py:48] [25600] global_step=25600, grad_norm=0.37379008531570435, loss=2.1302969455718994
I0305 23:30:59.277609 139637473359616 logging_writer.py:48] [25700] global_step=25700, grad_norm=0.325684130191803, loss=2.027508497238159
I0305 23:31:34.401313 139637464966912 logging_writer.py:48] [25800] global_step=25800, grad_norm=0.4886420965194702, loss=2.0534491539001465
I0305 23:32:09.523702 139637473359616 logging_writer.py:48] [25900] global_step=25900, grad_norm=0.35213375091552734, loss=2.0388388633728027
I0305 23:32:44.709264 139637464966912 logging_writer.py:48] [26000] global_step=26000, grad_norm=0.2763177454471588, loss=2.0427818298339844
I0305 23:33:19.870750 139637473359616 logging_writer.py:48] [26100] global_step=26100, grad_norm=0.3200927674770355, loss=2.158339738845825
I0305 23:33:55.030060 139637464966912 logging_writer.py:48] [26200] global_step=26200, grad_norm=0.36312004923820496, loss=2.0588772296905518
I0305 23:34:18.265502 139780644897984 spec.py:321] Evaluating on the training split.
I0305 23:34:20.913454 139780644897984 workload.py:181] Translating evaluation dataset.
I0305 23:37:13.988481 139780644897984 spec.py:333] Evaluating on the validation split.
I0305 23:37:16.637907 139780644897984 workload.py:181] Translating evaluation dataset.
I0305 23:40:04.410684 139780644897984 spec.py:349] Evaluating on the test split.
I0305 23:40:07.037948 139780644897984 workload.py:181] Translating evaluation dataset.
I0305 23:42:52.624475 139780644897984 submission_runner.py:469] Time since start: 16163.89s, 	Step: 26267, 	{'train/accuracy': 0.6071869134902954, 'train/loss': 2.003427267074585, 'train/bleu': 28.822366695578346, 'validation/accuracy': 0.6217215061187744, 'validation/loss': 1.8891798257827759, 'validation/bleu': 25.516520317739126, 'validation/num_examples': 3000, 'test/accuracy': 0.6298342943191528, 'test/loss': 1.8408453464508057, 'test/bleu': 24.458808111802, 'test/num_examples': 3003, 'score': 9267.046485185623, 'total_duration': 16163.887969017029, 'accumulated_submission_time': 9267.046485185623, 'accumulated_eval_time': 6895.016014814377, 'accumulated_logging_time': 0.21158742904663086}
I0305 23:42:52.637086 139637473359616 logging_writer.py:48] [26267] accumulated_eval_time=6895.02, accumulated_logging_time=0.211587, accumulated_submission_time=9267.05, global_step=26267, preemption_count=0, score=9267.05, test/accuracy=0.629834, test/bleu=24.4588, test/loss=1.84085, test/num_examples=3003, total_duration=16163.9, train/accuracy=0.607187, train/bleu=28.8224, train/loss=2.00343, validation/accuracy=0.621722, validation/bleu=25.5165, validation/loss=1.88918, validation/num_examples=3000
I0305 23:43:04.552265 139637464966912 logging_writer.py:48] [26300] global_step=26300, grad_norm=0.33068448305130005, loss=2.045957326889038
I0305 23:43:39.606623 139637473359616 logging_writer.py:48] [26400] global_step=26400, grad_norm=0.35847988724708557, loss=2.0587689876556396
I0305 23:44:14.684100 139637464966912 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.3506917655467987, loss=1.9916753768920898
I0305 23:44:49.827178 139637473359616 logging_writer.py:48] [26600] global_step=26600, grad_norm=0.4302462339401245, loss=2.108081102371216
I0305 23:45:24.991472 139637464966912 logging_writer.py:48] [26700] global_step=26700, grad_norm=0.3163803219795227, loss=2.0303075313568115
I0305 23:46:00.112672 139637473359616 logging_writer.py:48] [26800] global_step=26800, grad_norm=0.367613822221756, loss=2.047635078430176
I0305 23:46:35.244228 139637464966912 logging_writer.py:48] [26900] global_step=26900, grad_norm=0.3892916142940521, loss=1.9767788648605347
I0305 23:47:10.386278 139637473359616 logging_writer.py:48] [27000] global_step=27000, grad_norm=0.294300377368927, loss=2.0469703674316406
I0305 23:47:45.512644 139637464966912 logging_writer.py:48] [27100] global_step=27100, grad_norm=0.42309004068374634, loss=1.9579378366470337
I0305 23:48:20.680543 139637473359616 logging_writer.py:48] [27200] global_step=27200, grad_norm=0.2893109917640686, loss=1.9846835136413574
I0305 23:48:55.829323 139637464966912 logging_writer.py:48] [27300] global_step=27300, grad_norm=0.30760592222213745, loss=2.0226504802703857
I0305 23:49:30.970242 139637473359616 logging_writer.py:48] [27400] global_step=27400, grad_norm=0.29683274030685425, loss=2.0020337104797363
I0305 23:50:06.120355 139637464966912 logging_writer.py:48] [27500] global_step=27500, grad_norm=0.36641979217529297, loss=2.000035047531128
I0305 23:50:41.302547 139637473359616 logging_writer.py:48] [27600] global_step=27600, grad_norm=0.3605334460735321, loss=2.010373830795288
I0305 23:51:16.482263 139637464966912 logging_writer.py:48] [27700] global_step=27700, grad_norm=0.514961302280426, loss=2.0533437728881836
I0305 23:51:51.686439 139637473359616 logging_writer.py:48] [27800] global_step=27800, grad_norm=0.2670881152153015, loss=1.9004662036895752
I0305 23:52:26.877089 139637464966912 logging_writer.py:48] [27900] global_step=27900, grad_norm=0.31621262431144714, loss=1.9948742389678955
I0305 23:53:02.030524 139637473359616 logging_writer.py:48] [28000] global_step=28000, grad_norm=0.5020815134048462, loss=2.064868211746216
I0305 23:53:37.221284 139637464966912 logging_writer.py:48] [28100] global_step=28100, grad_norm=0.3057800531387329, loss=2.1345717906951904
I0305 23:54:12.376923 139637473359616 logging_writer.py:48] [28200] global_step=28200, grad_norm=0.400174081325531, loss=2.0139963626861572
I0305 23:54:47.526803 139637464966912 logging_writer.py:48] [28300] global_step=28300, grad_norm=0.432500422000885, loss=2.0138535499572754
I0305 23:55:22.663743 139637473359616 logging_writer.py:48] [28400] global_step=28400, grad_norm=0.3165854811668396, loss=2.017681360244751
I0305 23:55:57.805439 139637464966912 logging_writer.py:48] [28500] global_step=28500, grad_norm=0.3235509991645813, loss=1.9970866441726685
I0305 23:56:32.970296 139637473359616 logging_writer.py:48] [28600] global_step=28600, grad_norm=0.4237838089466095, loss=2.019542932510376
I0305 23:56:52.668479 139780644897984 spec.py:321] Evaluating on the training split.
I0305 23:56:55.318669 139780644897984 workload.py:181] Translating evaluation dataset.
I0306 00:00:23.387781 139780644897984 spec.py:333] Evaluating on the validation split.
I0306 00:00:26.024329 139780644897984 workload.py:181] Translating evaluation dataset.
I0306 00:03:16.238309 139780644897984 spec.py:349] Evaluating on the test split.
I0306 00:03:18.882385 139780644897984 workload.py:181] Translating evaluation dataset.
I0306 00:05:54.508424 139780644897984 submission_runner.py:469] Time since start: 17545.77s, 	Step: 28657, 	{'train/accuracy': 0.6039182543754578, 'train/loss': 2.0322377681732178, 'train/bleu': 28.221845513422554, 'validation/accuracy': 0.6241193413734436, 'validation/loss': 1.873083233833313, 'validation/bleu': 25.380839915586105, 'validation/num_examples': 3000, 'test/accuracy': 0.6312941908836365, 'test/loss': 1.8165948390960693, 'test/bleu': 24.48678844159008, 'test/num_examples': 3003, 'score': 10106.934731721878, 'total_duration': 17545.771904230118, 'accumulated_submission_time': 10106.934731721878, 'accumulated_eval_time': 7436.855900287628, 'accumulated_logging_time': 0.23226642608642578}
I0306 00:05:54.521019 139637464966912 logging_writer.py:48] [28657] accumulated_eval_time=7436.86, accumulated_logging_time=0.232266, accumulated_submission_time=10106.9, global_step=28657, preemption_count=0, score=10106.9, test/accuracy=0.631294, test/bleu=24.4868, test/loss=1.81659, test/num_examples=3003, total_duration=17545.8, train/accuracy=0.603918, train/bleu=28.2218, train/loss=2.03224, validation/accuracy=0.624119, validation/bleu=25.3808, validation/loss=1.87308, validation/num_examples=3000
I0306 00:06:09.954256 139637473359616 logging_writer.py:48] [28700] global_step=28700, grad_norm=0.39264965057373047, loss=2.0591742992401123
I0306 00:06:45.025009 139637464966912 logging_writer.py:48] [28800] global_step=28800, grad_norm=0.3903171122074127, loss=2.0247273445129395
I0306 00:07:20.177140 139637473359616 logging_writer.py:48] [28900] global_step=28900, grad_norm=0.3293951749801636, loss=1.9655078649520874
I0306 00:07:55.311285 139637464966912 logging_writer.py:48] [29000] global_step=29000, grad_norm=0.3592641353607178, loss=2.0596513748168945
I0306 00:08:30.490138 139637473359616 logging_writer.py:48] [29100] global_step=29100, grad_norm=0.4116663932800293, loss=2.023210287094116
I0306 00:09:05.676043 139637464966912 logging_writer.py:48] [29200] global_step=29200, grad_norm=0.30598941445350647, loss=1.9886348247528076
I0306 00:09:40.851756 139637473359616 logging_writer.py:48] [29300] global_step=29300, grad_norm=0.37733903527259827, loss=2.051360845565796
I0306 00:10:16.030113 139637464966912 logging_writer.py:48] [29400] global_step=29400, grad_norm=0.2772168815135956, loss=1.9964498281478882
I0306 00:10:51.197883 139637473359616 logging_writer.py:48] [29500] global_step=29500, grad_norm=0.3474356234073639, loss=1.9618360996246338
I0306 00:11:26.396739 139637464966912 logging_writer.py:48] [29600] global_step=29600, grad_norm=0.4041966497898102, loss=2.0205905437469482
I0306 00:12:01.564177 139637473359616 logging_writer.py:48] [29700] global_step=29700, grad_norm=0.35094642639160156, loss=1.9814773797988892
I0306 00:12:36.763703 139637464966912 logging_writer.py:48] [29800] global_step=29800, grad_norm=0.444416880607605, loss=2.019991397857666
I0306 00:13:11.968215 139637473359616 logging_writer.py:48] [29900] global_step=29900, grad_norm=0.3441021144390106, loss=1.9893453121185303
I0306 00:13:47.125100 139637464966912 logging_writer.py:48] [30000] global_step=30000, grad_norm=0.355180025100708, loss=2.0291213989257812
I0306 00:14:22.295565 139637473359616 logging_writer.py:48] [30100] global_step=30100, grad_norm=0.3587372303009033, loss=2.0138986110687256
I0306 00:14:57.450095 139637464966912 logging_writer.py:48] [30200] global_step=30200, grad_norm=0.3370789885520935, loss=1.9455127716064453
I0306 00:15:32.619468 139637473359616 logging_writer.py:48] [30300] global_step=30300, grad_norm=0.5651163458824158, loss=2.0865800380706787
I0306 00:16:07.782580 139637464966912 logging_writer.py:48] [30400] global_step=30400, grad_norm=0.4237966239452362, loss=2.0390312671661377
I0306 00:16:42.969246 139637473359616 logging_writer.py:48] [30500] global_step=30500, grad_norm=0.5222668647766113, loss=2.0333151817321777
I0306 00:17:18.156578 139637464966912 logging_writer.py:48] [30600] global_step=30600, grad_norm=0.31805139780044556, loss=2.0275754928588867
I0306 00:17:53.299306 139637473359616 logging_writer.py:48] [30700] global_step=30700, grad_norm=0.31026896834373474, loss=1.9917851686477661
I0306 00:18:28.460240 139637464966912 logging_writer.py:48] [30800] global_step=30800, grad_norm=0.30184945464134216, loss=2.046393632888794
I0306 00:19:03.631282 139637473359616 logging_writer.py:48] [30900] global_step=30900, grad_norm=0.3954814672470093, loss=1.9885165691375732
I0306 00:19:38.775891 139637464966912 logging_writer.py:48] [31000] global_step=31000, grad_norm=0.46079856157302856, loss=2.0039215087890625
I0306 00:19:54.597807 139780644897984 spec.py:321] Evaluating on the training split.
I0306 00:19:57.249593 139780644897984 workload.py:181] Translating evaluation dataset.
I0306 00:22:57.297287 139780644897984 spec.py:333] Evaluating on the validation split.
I0306 00:22:59.939421 139780644897984 workload.py:181] Translating evaluation dataset.
I0306 00:25:57.334247 139780644897984 spec.py:349] Evaluating on the test split.
I0306 00:25:59.965157 139780644897984 workload.py:181] Translating evaluation dataset.
I0306 00:28:39.270355 139780644897984 submission_runner.py:469] Time since start: 18910.53s, 	Step: 31046, 	{'train/accuracy': 0.6066698431968689, 'train/loss': 2.0188980102539062, 'train/bleu': 28.48382200303516, 'validation/accuracy': 0.6256025433540344, 'validation/loss': 1.8642661571502686, 'validation/bleu': 25.232810435092556, 'validation/num_examples': 3000, 'test/accuracy': 0.6352682113647461, 'test/loss': 1.8046759366989136, 'test/bleu': 24.57316360352151, 'test/num_examples': 3003, 'score': 10946.866646051407, 'total_duration': 18910.53384923935, 'accumulated_submission_time': 10946.866646051407, 'accumulated_eval_time': 7961.52839589119, 'accumulated_logging_time': 0.25343918800354004}
I0306 00:28:39.283992 139637473359616 logging_writer.py:48] [31046] accumulated_eval_time=7961.53, accumulated_logging_time=0.253439, accumulated_submission_time=10946.9, global_step=31046, preemption_count=0, score=10946.9, test/accuracy=0.635268, test/bleu=24.5732, test/loss=1.80468, test/num_examples=3003, total_duration=18910.5, train/accuracy=0.60667, train/bleu=28.4838, train/loss=2.0189, validation/accuracy=0.625603, validation/bleu=25.2328, validation/loss=1.86427, validation/num_examples=3000
I0306 00:28:58.570343 139637464966912 logging_writer.py:48] [31100] global_step=31100, grad_norm=0.41334468126296997, loss=2.0204973220825195
I0306 00:29:33.671009 139637473359616 logging_writer.py:48] [31200] global_step=31200, grad_norm=0.3605113625526428, loss=2.0155529975891113
I0306 00:30:08.747107 139637464966912 logging_writer.py:48] [31300] global_step=31300, grad_norm=0.28165242075920105, loss=1.988093614578247
I0306 00:30:43.961323 139637473359616 logging_writer.py:48] [31400] global_step=31400, grad_norm=0.5206484198570251, loss=1.981506586074829
I0306 00:31:19.191824 139637464966912 logging_writer.py:48] [31500] global_step=31500, grad_norm=0.32899001240730286, loss=2.1125800609588623
I0306 00:31:54.419118 139637473359616 logging_writer.py:48] [31600] global_step=31600, grad_norm=0.3133772909641266, loss=1.9472033977508545
I0306 00:32:29.649085 139637464966912 logging_writer.py:48] [31700] global_step=31700, grad_norm=0.4194791615009308, loss=2.0447068214416504
I0306 00:33:04.924391 139637473359616 logging_writer.py:48] [31800] global_step=31800, grad_norm=0.36296316981315613, loss=1.9522901773452759
I0306 00:33:40.201118 139637464966912 logging_writer.py:48] [31900] global_step=31900, grad_norm=0.2821211814880371, loss=1.941712498664856
I0306 00:34:15.500830 139637473359616 logging_writer.py:48] [32000] global_step=32000, grad_norm=0.2715170681476593, loss=1.9740571975708008
I0306 00:34:50.830214 139637464966912 logging_writer.py:48] [32100] global_step=32100, grad_norm=0.3012540936470032, loss=1.9812700748443604
I0306 00:35:26.131038 139637473359616 logging_writer.py:48] [32200] global_step=32200, grad_norm=0.3851346969604492, loss=2.0179107189178467
I0306 00:36:01.436839 139637464966912 logging_writer.py:48] [32300] global_step=32300, grad_norm=0.4499289393424988, loss=2.0338234901428223
I0306 00:36:36.702710 139637473359616 logging_writer.py:48] [32400] global_step=32400, grad_norm=0.3448030352592468, loss=2.105567693710327
I0306 00:37:11.988599 139637464966912 logging_writer.py:48] [32500] global_step=32500, grad_norm=0.5330579876899719, loss=2.0365240573883057
I0306 00:37:47.249024 139637473359616 logging_writer.py:48] [32600] global_step=32600, grad_norm=0.5471457242965698, loss=2.0588555335998535
I0306 00:38:22.499904 139637464966912 logging_writer.py:48] [32700] global_step=32700, grad_norm=0.28191080689430237, loss=2.0191848278045654
I0306 00:38:57.782490 139637473359616 logging_writer.py:48] [32800] global_step=32800, grad_norm=0.32081514596939087, loss=2.004866361618042
I0306 00:39:33.035881 139637464966912 logging_writer.py:48] [32900] global_step=32900, grad_norm=0.44774436950683594, loss=2.0445973873138428
I0306 00:40:08.286075 139637473359616 logging_writer.py:48] [33000] global_step=33000, grad_norm=0.32577672600746155, loss=1.9979695081710815
I0306 00:40:43.541055 139637464966912 logging_writer.py:48] [33100] global_step=33100, grad_norm=0.4249400794506073, loss=1.9794470071792603
I0306 00:41:18.804900 139637473359616 logging_writer.py:48] [33200] global_step=33200, grad_norm=0.48641759157180786, loss=2.0794589519500732
I0306 00:41:54.055753 139637464966912 logging_writer.py:48] [33300] global_step=33300, grad_norm=0.30318158864974976, loss=1.9829914569854736
I0306 00:42:29.313276 139637473359616 logging_writer.py:48] [33400] global_step=33400, grad_norm=0.4507896900177002, loss=1.9615811109542847
I0306 00:42:39.545479 139780644897984 spec.py:321] Evaluating on the training split.
I0306 00:42:42.196199 139780644897984 workload.py:181] Translating evaluation dataset.
I0306 00:46:47.476407 139780644897984 spec.py:333] Evaluating on the validation split.
I0306 00:46:50.101358 139780644897984 workload.py:181] Translating evaluation dataset.
I0306 00:49:39.446192 139780644897984 spec.py:349] Evaluating on the test split.
I0306 00:49:42.070128 139780644897984 workload.py:181] Translating evaluation dataset.
I0306 00:52:34.531565 139780644897984 submission_runner.py:469] Time since start: 20345.80s, 	Step: 33430, 	{'train/accuracy': 0.6100131273269653, 'train/loss': 1.9983563423156738, 'train/bleu': 28.812939268679422, 'validation/accuracy': 0.6261340379714966, 'validation/loss': 1.845545768737793, 'validation/bleu': 25.829190279812533, 'validation/num_examples': 3000, 'test/accuracy': 0.6353608965873718, 'test/loss': 1.8041815757751465, 'test/bleu': 25.18249660928717, 'test/num_examples': 3003, 'score': 11786.987345457077, 'total_duration': 20345.79505467415, 'accumulated_submission_time': 11786.987345457077, 'accumulated_eval_time': 8556.514425039291, 'accumulated_logging_time': 0.27521753311157227}
I0306 00:52:34.545552 139637464966912 logging_writer.py:48] [33430] accumulated_eval_time=8556.51, accumulated_logging_time=0.275218, accumulated_submission_time=11787, global_step=33430, preemption_count=0, score=11787, test/accuracy=0.635361, test/bleu=25.1825, test/loss=1.80418, test/num_examples=3003, total_duration=20345.8, train/accuracy=0.610013, train/bleu=28.8129, train/loss=1.99836, validation/accuracy=0.626134, validation/bleu=25.8292, validation/loss=1.84555, validation/num_examples=3000
I0306 00:52:59.480239 139637473359616 logging_writer.py:48] [33500] global_step=33500, grad_norm=0.3569517731666565, loss=2.0074124336242676
I0306 00:53:34.614728 139637464966912 logging_writer.py:48] [33600] global_step=33600, grad_norm=0.28206172585487366, loss=2.0633294582366943
I0306 00:54:09.757287 139637473359616 logging_writer.py:48] [33700] global_step=33700, grad_norm=0.3625003695487976, loss=2.0345816612243652
I0306 00:54:44.949918 139637464966912 logging_writer.py:48] [33800] global_step=33800, grad_norm=0.3917059898376465, loss=2.033500909805298
I0306 00:55:20.164438 139637473359616 logging_writer.py:48] [33900] global_step=33900, grad_norm=0.42202475666999817, loss=2.0042130947113037
I0306 00:55:55.332934 139637464966912 logging_writer.py:48] [34000] global_step=34000, grad_norm=0.3919913172721863, loss=1.9668503999710083
I0306 00:56:30.521278 139637473359616 logging_writer.py:48] [34100] global_step=34100, grad_norm=0.5383528470993042, loss=1.9578107595443726
I0306 00:57:05.670230 139637464966912 logging_writer.py:48] [34200] global_step=34200, grad_norm=0.28495335578918457, loss=1.9451366662979126
I0306 00:57:40.841895 139637473359616 logging_writer.py:48] [34300] global_step=34300, grad_norm=0.2887289822101593, loss=1.9072515964508057
I0306 00:58:16.006423 139637464966912 logging_writer.py:48] [34400] global_step=34400, grad_norm=0.42760297656059265, loss=2.026533603668213
I0306 00:58:51.164492 139637473359616 logging_writer.py:48] [34500] global_step=34500, grad_norm=0.4844110310077667, loss=1.9708048105239868
I0306 00:59:26.307747 139637464966912 logging_writer.py:48] [34600] global_step=34600, grad_norm=0.26779457926750183, loss=2.035844564437866
I0306 01:00:01.457849 139637473359616 logging_writer.py:48] [34700] global_step=34700, grad_norm=0.3374493420124054, loss=2.0377819538116455
I0306 01:00:36.612815 139637464966912 logging_writer.py:48] [34800] global_step=34800, grad_norm=0.2980495095252991, loss=1.9310022592544556
I0306 01:01:11.759642 139637473359616 logging_writer.py:48] [34900] global_step=34900, grad_norm=0.43725430965423584, loss=2.001221179962158
I0306 01:01:46.907260 139637464966912 logging_writer.py:48] [35000] global_step=35000, grad_norm=0.35459643602371216, loss=2.0359513759613037
I0306 01:02:22.039142 139637473359616 logging_writer.py:48] [35100] global_step=35100, grad_norm=0.2920898199081421, loss=2.019016981124878
I0306 01:02:57.143759 139637464966912 logging_writer.py:48] [35200] global_step=35200, grad_norm=0.2929246425628662, loss=2.034363269805908
I0306 01:03:32.287363 139637473359616 logging_writer.py:48] [35300] global_step=35300, grad_norm=0.48034146428108215, loss=1.9789605140686035
I0306 01:04:07.393904 139637464966912 logging_writer.py:48] [35400] global_step=35400, grad_norm=0.4889170825481415, loss=2.0252902507781982
I0306 01:04:42.515245 139637473359616 logging_writer.py:48] [35500] global_step=35500, grad_norm=0.35349392890930176, loss=1.9685370922088623
I0306 01:05:17.649849 139637464966912 logging_writer.py:48] [35600] global_step=35600, grad_norm=0.5224875211715698, loss=2.0908985137939453
I0306 01:05:52.788604 139637473359616 logging_writer.py:48] [35700] global_step=35700, grad_norm=0.5382809042930603, loss=1.9760217666625977
I0306 01:06:27.907448 139637464966912 logging_writer.py:48] [35800] global_step=35800, grad_norm=0.4643280804157257, loss=2.057342290878296
I0306 01:06:34.583010 139780644897984 spec.py:321] Evaluating on the training split.
I0306 01:06:37.221657 139780644897984 workload.py:181] Translating evaluation dataset.
I0306 01:10:04.245089 139780644897984 spec.py:333] Evaluating on the validation split.
I0306 01:10:06.876079 139780644897984 workload.py:181] Translating evaluation dataset.
I0306 01:12:56.157350 139780644897984 spec.py:349] Evaluating on the test split.
I0306 01:12:58.781854 139780644897984 workload.py:181] Translating evaluation dataset.
I0306 01:15:44.989516 139780644897984 submission_runner.py:469] Time since start: 21736.25s, 	Step: 35820, 	{'train/accuracy': 0.6073586940765381, 'train/loss': 2.0146327018737793, 'train/bleu': 28.787296842570566, 'validation/accuracy': 0.6300398111343384, 'validation/loss': 1.8302057981491089, 'validation/bleu': 25.98764954646736, 'validation/num_examples': 3000, 'test/accuracy': 0.637481153011322, 'test/loss': 1.7810845375061035, 'test/bleu': 24.675242743362375, 'test/num_examples': 3003, 'score': 12626.878019809723, 'total_duration': 21736.252999782562, 'accumulated_submission_time': 12626.878019809723, 'accumulated_eval_time': 9106.920868635178, 'accumulated_logging_time': 0.2985553741455078}
I0306 01:15:45.002234 139637473359616 logging_writer.py:48] [35820] accumulated_eval_time=9106.92, accumulated_logging_time=0.298555, accumulated_submission_time=12626.9, global_step=35820, preemption_count=0, score=12626.9, test/accuracy=0.637481, test/bleu=24.6752, test/loss=1.78108, test/num_examples=3003, total_duration=21736.3, train/accuracy=0.607359, train/bleu=28.7873, train/loss=2.01463, validation/accuracy=0.63004, validation/bleu=25.9876, validation/loss=1.83021, validation/num_examples=3000
I0306 01:16:13.375628 139637464966912 logging_writer.py:48] [35900] global_step=35900, grad_norm=0.2940298318862915, loss=1.9816349744796753
I0306 01:16:48.487239 139637473359616 logging_writer.py:48] [36000] global_step=36000, grad_norm=0.4327467083930969, loss=2.05326509475708
I0306 01:17:23.627116 139637464966912 logging_writer.py:48] [36100] global_step=36100, grad_norm=0.29653453826904297, loss=2.0324289798736572
I0306 01:17:58.759574 139637473359616 logging_writer.py:48] [36200] global_step=36200, grad_norm=0.3341296315193176, loss=1.9064631462097168
I0306 01:18:33.914782 139637464966912 logging_writer.py:48] [36300] global_step=36300, grad_norm=0.38461917638778687, loss=2.0006027221679688
I0306 01:19:09.093547 139637473359616 logging_writer.py:48] [36400] global_step=36400, grad_norm=0.5602090954780579, loss=2.052847385406494
I0306 01:19:44.248353 139637464966912 logging_writer.py:48] [36500] global_step=36500, grad_norm=0.5894637703895569, loss=1.9878153800964355
I0306 01:20:19.420527 139637473359616 logging_writer.py:48] [36600] global_step=36600, grad_norm=0.30056068301200867, loss=2.002246141433716
I0306 01:20:54.599615 139637464966912 logging_writer.py:48] [36700] global_step=36700, grad_norm=0.24712401628494263, loss=2.0251100063323975
I0306 01:21:29.777644 139637473359616 logging_writer.py:48] [36800] global_step=36800, grad_norm=0.5106268525123596, loss=2.0984301567077637
I0306 01:22:04.919109 139637464966912 logging_writer.py:48] [36900] global_step=36900, grad_norm=0.33862245082855225, loss=2.014282464981079
I0306 01:22:40.098978 139637473359616 logging_writer.py:48] [37000] global_step=37000, grad_norm=0.3720628023147583, loss=2.0158278942108154
I0306 01:23:15.279363 139637464966912 logging_writer.py:48] [37100] global_step=37100, grad_norm=0.3105049133300781, loss=1.9258514642715454
I0306 01:23:50.432042 139637473359616 logging_writer.py:48] [37200] global_step=37200, grad_norm=0.4150126576423645, loss=2.011523723602295
I0306 01:24:25.619333 139637464966912 logging_writer.py:48] [37300] global_step=37300, grad_norm=0.638879120349884, loss=1.9044677019119263
I0306 01:25:00.824302 139637473359616 logging_writer.py:48] [37400] global_step=37400, grad_norm=0.49426934123039246, loss=1.9423141479492188
I0306 01:25:35.997226 139637464966912 logging_writer.py:48] [37500] global_step=37500, grad_norm=0.3378971219062805, loss=2.0648233890533447
I0306 01:26:11.150809 139637473359616 logging_writer.py:48] [37600] global_step=37600, grad_norm=0.5677787065505981, loss=1.9424258470535278
I0306 01:26:46.321280 139637464966912 logging_writer.py:48] [37700] global_step=37700, grad_norm=0.38789188861846924, loss=1.9222551584243774
I0306 01:27:21.496600 139637473359616 logging_writer.py:48] [37800] global_step=37800, grad_norm=0.2894074618816376, loss=1.9809155464172363
I0306 01:27:56.674445 139637464966912 logging_writer.py:48] [37900] global_step=37900, grad_norm=0.39990589022636414, loss=1.9286489486694336
I0306 01:28:31.851233 139637473359616 logging_writer.py:48] [38000] global_step=38000, grad_norm=0.29112711548805237, loss=1.9888782501220703
I0306 01:29:07.002430 139637464966912 logging_writer.py:48] [38100] global_step=38100, grad_norm=0.3166602849960327, loss=1.9947433471679688
I0306 01:29:42.173552 139637473359616 logging_writer.py:48] [38200] global_step=38200, grad_norm=0.4453276991844177, loss=1.9524025917053223
I0306 01:29:45.003756 139780644897984 spec.py:321] Evaluating on the training split.
I0306 01:29:47.642122 139780644897984 workload.py:181] Translating evaluation dataset.
I0306 01:33:00.653703 139780644897984 spec.py:333] Evaluating on the validation split.
I0306 01:33:03.284373 139780644897984 workload.py:181] Translating evaluation dataset.
I0306 01:35:52.265916 139780644897984 spec.py:349] Evaluating on the test split.
I0306 01:35:54.890806 139780644897984 workload.py:181] Translating evaluation dataset.
I0306 01:38:32.707291 139780644897984 submission_runner.py:469] Time since start: 23103.97s, 	Step: 38209, 	{'train/accuracy': 0.6203847527503967, 'train/loss': 1.8942300081253052, 'train/bleu': 29.396835483410868, 'validation/accuracy': 0.6301263570785522, 'validation/loss': 1.8302288055419922, 'validation/bleu': 25.90954802967947, 'validation/num_examples': 3000, 'test/accuracy': 0.6349901556968689, 'test/loss': 1.7912007570266724, 'test/bleu': 24.69379120789673, 'test/num_examples': 3003, 'score': 13466.73539853096, 'total_duration': 23103.97078180313, 'accumulated_submission_time': 13466.73539853096, 'accumulated_eval_time': 9634.624348402023, 'accumulated_logging_time': 0.3203768730163574}
I0306 01:38:32.720334 139637464966912 logging_writer.py:48] [38209] accumulated_eval_time=9634.62, accumulated_logging_time=0.320377, accumulated_submission_time=13466.7, global_step=38209, preemption_count=0, score=13466.7, test/accuracy=0.63499, test/bleu=24.6938, test/loss=1.7912, test/num_examples=3003, total_duration=23104, train/accuracy=0.620385, train/bleu=29.3968, train/loss=1.89423, validation/accuracy=0.630126, validation/bleu=25.9095, validation/loss=1.83023, validation/num_examples=3000
I0306 01:39:04.969245 139637473359616 logging_writer.py:48] [38300] global_step=38300, grad_norm=0.28804415464401245, loss=1.885161280632019
I0306 01:39:40.032510 139637464966912 logging_writer.py:48] [38400] global_step=38400, grad_norm=0.3558163344860077, loss=2.060555934906006
I0306 01:40:15.126218 139637473359616 logging_writer.py:48] [38500] global_step=38500, grad_norm=0.532400369644165, loss=2.067924976348877
I0306 01:40:50.229444 139637464966912 logging_writer.py:48] [38600] global_step=38600, grad_norm=0.2968623638153076, loss=1.925443410873413
I0306 01:41:25.383265 139637473359616 logging_writer.py:48] [38700] global_step=38700, grad_norm=0.2653551399707794, loss=1.9357248544692993
I0306 01:42:00.508972 139637464966912 logging_writer.py:48] [38800] global_step=38800, grad_norm=0.27638766169548035, loss=2.0101399421691895
I0306 01:42:35.640162 139637473359616 logging_writer.py:48] [38900] global_step=38900, grad_norm=0.28461509943008423, loss=1.9860548973083496
I0306 01:43:10.747272 139637464966912 logging_writer.py:48] [39000] global_step=39000, grad_norm=0.3193025290966034, loss=1.9421454668045044
I0306 01:43:45.885035 139637473359616 logging_writer.py:48] [39100] global_step=39100, grad_norm=0.42966970801353455, loss=2.0136122703552246
I0306 01:44:21.035810 139637464966912 logging_writer.py:48] [39200] global_step=39200, grad_norm=0.30151110887527466, loss=2.0222487449645996
I0306 01:44:56.185544 139637473359616 logging_writer.py:48] [39300] global_step=39300, grad_norm=0.2743690609931946, loss=1.9898115396499634
I0306 01:45:31.335863 139637464966912 logging_writer.py:48] [39400] global_step=39400, grad_norm=0.30430299043655396, loss=2.032304525375366
I0306 01:46:06.504326 139637473359616 logging_writer.py:48] [39500] global_step=39500, grad_norm=0.3197232782840729, loss=1.918349027633667
I0306 01:46:41.663448 139637464966912 logging_writer.py:48] [39600] global_step=39600, grad_norm=0.3027195334434509, loss=2.0113608837127686
I0306 01:47:16.833169 139637473359616 logging_writer.py:48] [39700] global_step=39700, grad_norm=0.2556130290031433, loss=1.9290128946304321
I0306 01:47:51.998659 139637464966912 logging_writer.py:48] [39800] global_step=39800, grad_norm=0.29197126626968384, loss=1.985516905784607
I0306 01:48:27.167979 139637473359616 logging_writer.py:48] [39900] global_step=39900, grad_norm=0.38516318798065186, loss=1.9682706594467163
I0306 01:49:02.325123 139637464966912 logging_writer.py:48] [40000] global_step=40000, grad_norm=0.2847130000591278, loss=1.960329294204712
I0306 01:49:37.500218 139637473359616 logging_writer.py:48] [40100] global_step=40100, grad_norm=0.3333788514137268, loss=2.040905237197876
I0306 01:50:12.667503 139637464966912 logging_writer.py:48] [40200] global_step=40200, grad_norm=0.30538904666900635, loss=1.9400125741958618
I0306 01:50:47.827342 139637473359616 logging_writer.py:48] [40300] global_step=40300, grad_norm=0.30455446243286133, loss=2.0183897018432617
I0306 01:51:22.968370 139637464966912 logging_writer.py:48] [40400] global_step=40400, grad_norm=0.4954673945903778, loss=2.0017430782318115
I0306 01:51:58.093220 139637473359616 logging_writer.py:48] [40500] global_step=40500, grad_norm=0.34316471219062805, loss=1.9570540189743042
I0306 01:52:32.883890 139780644897984 spec.py:321] Evaluating on the training split.
I0306 01:52:35.519815 139780644897984 workload.py:181] Translating evaluation dataset.
I0306 01:57:21.273358 139780644897984 spec.py:333] Evaluating on the validation split.
I0306 01:57:23.904432 139780644897984 workload.py:181] Translating evaluation dataset.
I0306 02:00:41.376555 139780644897984 spec.py:349] Evaluating on the test split.
I0306 02:00:43.999497 139780644897984 workload.py:181] Translating evaluation dataset.
I0306 02:03:54.703459 139780644897984 submission_runner.py:469] Time since start: 24625.97s, 	Step: 40600, 	{'train/accuracy': 0.6077876687049866, 'train/loss': 1.993822455406189, 'train/bleu': 28.801807403206897, 'validation/accuracy': 0.6319679617881775, 'validation/loss': 1.8177428245544434, 'validation/bleu': 26.21881731838676, 'validation/num_examples': 3000, 'test/accuracy': 0.6421272158622742, 'test/loss': 1.7599787712097168, 'test/bleu': 25.16565133273084, 'test/num_examples': 3003, 'score': 14306.757709264755, 'total_duration': 24625.96693944931, 'accumulated_submission_time': 14306.757709264755, 'accumulated_eval_time': 10316.443854570389, 'accumulated_logging_time': 0.3419156074523926}
I0306 02:03:54.717220 139637464966912 logging_writer.py:48] [40600] accumulated_eval_time=10316.4, accumulated_logging_time=0.341916, accumulated_submission_time=14306.8, global_step=40600, preemption_count=0, score=14306.8, test/accuracy=0.642127, test/bleu=25.1657, test/loss=1.75998, test/num_examples=3003, total_duration=24626, train/accuracy=0.607788, train/bleu=28.8018, train/loss=1.99382, validation/accuracy=0.631968, validation/bleu=26.2188, validation/loss=1.81774, validation/num_examples=3000
I0306 02:03:55.082793 139637473359616 logging_writer.py:48] [40600] global_step=40600, grad_norm=0.30639296770095825, loss=1.9860600233078003
I0306 02:04:30.154297 139637464966912 logging_writer.py:48] [40700] global_step=40700, grad_norm=0.28685057163238525, loss=1.9990371465682983
I0306 02:05:05.270869 139637473359616 logging_writer.py:48] [40800] global_step=40800, grad_norm=0.32116934657096863, loss=2.0053420066833496
I0306 02:05:40.403466 139637464966912 logging_writer.py:48] [40900] global_step=40900, grad_norm=0.27933990955352783, loss=1.9483357667922974
I0306 02:06:15.560990 139637473359616 logging_writer.py:48] [41000] global_step=41000, grad_norm=0.34168165922164917, loss=1.8941417932510376
I0306 02:06:50.708527 139637464966912 logging_writer.py:48] [41100] global_step=41100, grad_norm=0.4951196610927582, loss=2.01503849029541
I0306 02:07:25.871513 139637473359616 logging_writer.py:48] [41200] global_step=41200, grad_norm=0.3132632374763489, loss=1.9519948959350586
I0306 02:08:01.007733 139637464966912 logging_writer.py:48] [41300] global_step=41300, grad_norm=0.34239083528518677, loss=1.9894654750823975
I0306 02:08:36.151178 139637473359616 logging_writer.py:48] [41400] global_step=41400, grad_norm=0.3223709464073181, loss=1.9284870624542236
I0306 02:09:11.303897 139637464966912 logging_writer.py:48] [41500] global_step=41500, grad_norm=0.6141067743301392, loss=1.9141303300857544
I0306 02:09:46.448613 139637473359616 logging_writer.py:48] [41600] global_step=41600, grad_norm=0.32603609561920166, loss=1.9596195220947266
I0306 02:10:21.585792 139637464966912 logging_writer.py:48] [41700] global_step=41700, grad_norm=0.6412217617034912, loss=1.9343515634536743
I0306 02:10:56.741861 139637473359616 logging_writer.py:48] [41800] global_step=41800, grad_norm=0.642970085144043, loss=2.0091071128845215
I0306 02:11:31.907023 139637464966912 logging_writer.py:48] [41900] global_step=41900, grad_norm=0.35568735003471375, loss=1.91189444065094
I0306 02:12:07.108341 139637473359616 logging_writer.py:48] [42000] global_step=42000, grad_norm=0.554010272026062, loss=1.9052374362945557
I0306 02:12:42.278129 139637464966912 logging_writer.py:48] [42100] global_step=42100, grad_norm=0.5820485949516296, loss=1.9494519233703613
I0306 02:13:17.486953 139637473359616 logging_writer.py:48] [42200] global_step=42200, grad_norm=0.3489956259727478, loss=2.0340752601623535
I0306 02:13:52.646978 139637464966912 logging_writer.py:48] [42300] global_step=42300, grad_norm=0.3036515414714813, loss=2.021700382232666
I0306 02:14:27.840547 139637473359616 logging_writer.py:48] [42400] global_step=42400, grad_norm=0.36058300733566284, loss=2.029416084289551
I0306 02:15:03.008282 139637464966912 logging_writer.py:48] [42500] global_step=42500, grad_norm=0.3576076924800873, loss=2.021291971206665
I0306 02:15:38.210580 139637473359616 logging_writer.py:48] [42600] global_step=42600, grad_norm=0.4758622944355011, loss=1.936934471130371
I0306 02:16:13.499898 139637464966912 logging_writer.py:48] [42700] global_step=42700, grad_norm=0.3727673888206482, loss=1.8879133462905884
I0306 02:16:48.702457 139637473359616 logging_writer.py:48] [42800] global_step=42800, grad_norm=0.41826799511909485, loss=1.9725406169891357
I0306 02:17:23.915571 139637464966912 logging_writer.py:48] [42900] global_step=42900, grad_norm=0.7833629846572876, loss=2.085360288619995
I0306 02:17:54.863969 139780644897984 spec.py:321] Evaluating on the training split.
I0306 02:17:57.501255 139780644897984 workload.py:181] Translating evaluation dataset.
I0306 02:21:35.241225 139780644897984 spec.py:333] Evaluating on the validation split.
I0306 02:21:37.875347 139780644897984 workload.py:181] Translating evaluation dataset.
I0306 02:24:28.670157 139780644897984 spec.py:349] Evaluating on the test split.
I0306 02:24:31.304183 139780644897984 workload.py:181] Translating evaluation dataset.
I0306 02:27:09.576841 139780644897984 submission_runner.py:469] Time since start: 26020.84s, 	Step: 42989, 	{'train/accuracy': 0.6141082644462585, 'train/loss': 1.9725170135498047, 'train/bleu': 28.9625056156541, 'validation/accuracy': 0.6335995197296143, 'validation/loss': 1.8045575618743896, 'validation/bleu': 26.28467386079833, 'validation/num_examples': 3000, 'test/accuracy': 0.6410149335861206, 'test/loss': 1.7512112855911255, 'test/bleu': 25.30449615203908, 'test/num_examples': 3003, 'score': 15146.758295536041, 'total_duration': 26020.840336561203, 'accumulated_submission_time': 15146.758295536041, 'accumulated_eval_time': 10871.156679868698, 'accumulated_logging_time': 0.3637692928314209}
I0306 02:27:09.590117 139637473359616 logging_writer.py:48] [42989] accumulated_eval_time=10871.2, accumulated_logging_time=0.363769, accumulated_submission_time=15146.8, global_step=42989, preemption_count=0, score=15146.8, test/accuracy=0.641015, test/bleu=25.3045, test/loss=1.75121, test/num_examples=3003, total_duration=26020.8, train/accuracy=0.614108, train/bleu=28.9625, train/loss=1.97252, validation/accuracy=0.6336, validation/bleu=26.2847, validation/loss=1.80456, validation/num_examples=3000
I0306 02:27:13.802664 139637464966912 logging_writer.py:48] [43000] global_step=43000, grad_norm=0.3518449366092682, loss=2.0809667110443115
I0306 02:27:48.862770 139637473359616 logging_writer.py:48] [43100] global_step=43100, grad_norm=0.3893463611602783, loss=1.9961379766464233
I0306 02:28:24.000994 139637464966912 logging_writer.py:48] [43200] global_step=43200, grad_norm=0.31998881697654724, loss=2.017622709274292
I0306 02:28:59.168076 139637473359616 logging_writer.py:48] [43300] global_step=43300, grad_norm=0.26032117009162903, loss=1.990806221961975
I0306 02:29:34.335721 139637464966912 logging_writer.py:48] [43400] global_step=43400, grad_norm=0.3469746708869934, loss=1.9439762830734253
I0306 02:30:09.512024 139637473359616 logging_writer.py:48] [43500] global_step=43500, grad_norm=0.4719241261482239, loss=1.96932852268219
I0306 02:30:44.662648 139637464966912 logging_writer.py:48] [43600] global_step=43600, grad_norm=0.40196701884269714, loss=2.0711047649383545
I0306 02:31:19.856261 139637473359616 logging_writer.py:48] [43700] global_step=43700, grad_norm=0.302593857049942, loss=1.9391553401947021
I0306 02:31:55.036778 139637464966912 logging_writer.py:48] [43800] global_step=43800, grad_norm=0.5411770343780518, loss=2.006861448287964
I0306 02:32:30.197062 139637473359616 logging_writer.py:48] [43900] global_step=43900, grad_norm=0.3447795808315277, loss=1.9213109016418457
I0306 02:33:05.298235 139637464966912 logging_writer.py:48] [44000] global_step=44000, grad_norm=0.33621424436569214, loss=1.8962054252624512
I0306 02:33:40.356695 139637473359616 logging_writer.py:48] [44100] global_step=44100, grad_norm=0.5337074995040894, loss=1.9137176275253296
I0306 02:34:15.419914 139637464966912 logging_writer.py:48] [44200] global_step=44200, grad_norm=0.41888296604156494, loss=1.8935754299163818
I0306 02:34:50.498987 139637473359616 logging_writer.py:48] [44300] global_step=44300, grad_norm=0.27869269251823425, loss=1.881792426109314
I0306 02:35:25.554212 139637464966912 logging_writer.py:48] [44400] global_step=44400, grad_norm=0.3011567294597626, loss=1.9538989067077637
I0306 02:36:00.622284 139637473359616 logging_writer.py:48] [44500] global_step=44500, grad_norm=0.7632943987846375, loss=1.981788158416748
I0306 02:36:35.689489 139637464966912 logging_writer.py:48] [44600] global_step=44600, grad_norm=0.2832707166671753, loss=1.943621039390564
I0306 02:37:10.756070 139637473359616 logging_writer.py:48] [44700] global_step=44700, grad_norm=0.6827667355537415, loss=2.058878183364868
I0306 02:37:45.818457 139637464966912 logging_writer.py:48] [44800] global_step=44800, grad_norm=0.31921303272247314, loss=2.05722975730896
I0306 02:38:20.857520 139637473359616 logging_writer.py:48] [44900] global_step=44900, grad_norm=0.4100167453289032, loss=2.0152831077575684
I0306 02:38:55.905230 139637464966912 logging_writer.py:48] [45000] global_step=45000, grad_norm=0.3701196610927582, loss=1.9887514114379883
I0306 02:39:30.961159 139637473359616 logging_writer.py:48] [45100] global_step=45100, grad_norm=0.390004962682724, loss=1.8984991312026978
I0306 02:40:05.982987 139637464966912 logging_writer.py:48] [45200] global_step=45200, grad_norm=0.36493703722953796, loss=2.0322632789611816
I0306 02:40:41.044429 139637473359616 logging_writer.py:48] [45300] global_step=45300, grad_norm=0.5237318873405457, loss=2.0751760005950928
I0306 02:41:09.792023 139780644897984 spec.py:321] Evaluating on the training split.
I0306 02:41:12.430443 139780644897984 workload.py:181] Translating evaluation dataset.
I0306 02:45:03.136716 139780644897984 spec.py:333] Evaluating on the validation split.
I0306 02:45:05.760929 139780644897984 workload.py:181] Translating evaluation dataset.
I0306 02:48:24.338999 139780644897984 spec.py:349] Evaluating on the test split.
I0306 02:48:26.964831 139780644897984 workload.py:181] Translating evaluation dataset.
I0306 02:51:12.975214 139780644897984 submission_runner.py:469] Time since start: 27464.24s, 	Step: 45383, 	{'train/accuracy': 0.6164857745170593, 'train/loss': 1.9382786750793457, 'train/bleu': 29.11654390962891, 'validation/accuracy': 0.6342793107032776, 'validation/loss': 1.7984269857406616, 'validation/bleu': 26.248522204476153, 'validation/num_examples': 3000, 'test/accuracy': 0.6431931257247925, 'test/loss': 1.738349199295044, 'test/bleu': 25.20431710493032, 'test/num_examples': 3003, 'score': 15986.81736421585, 'total_duration': 27464.238711595535, 'accumulated_submission_time': 15986.81736421585, 'accumulated_eval_time': 11474.33982682228, 'accumulated_logging_time': 0.38511180877685547}
I0306 02:51:12.988597 139637464966912 logging_writer.py:48] [45383] accumulated_eval_time=11474.3, accumulated_logging_time=0.385112, accumulated_submission_time=15986.8, global_step=45383, preemption_count=0, score=15986.8, test/accuracy=0.643193, test/bleu=25.2043, test/loss=1.73835, test/num_examples=3003, total_duration=27464.2, train/accuracy=0.616486, train/bleu=29.1165, train/loss=1.93828, validation/accuracy=0.634279, validation/bleu=26.2485, validation/loss=1.79843, validation/num_examples=3000
I0306 02:51:19.272119 139637473359616 logging_writer.py:48] [45400] global_step=45400, grad_norm=0.2857002019882202, loss=1.9334369897842407
I0306 02:51:54.263214 139637464966912 logging_writer.py:48] [45500] global_step=45500, grad_norm=0.33333101868629456, loss=1.9446046352386475
I0306 02:52:29.332852 139637473359616 logging_writer.py:48] [45600] global_step=45600, grad_norm=0.4022943675518036, loss=1.9588857889175415
I0306 02:53:04.373984 139637464966912 logging_writer.py:48] [45700] global_step=45700, grad_norm=0.2951368987560272, loss=1.8153913021087646
I0306 02:53:39.445508 139637473359616 logging_writer.py:48] [45800] global_step=45800, grad_norm=0.29349765181541443, loss=1.9764454364776611
I0306 02:54:14.500322 139637464966912 logging_writer.py:48] [45900] global_step=45900, grad_norm=0.3706439137458801, loss=1.9809396266937256
I0306 02:54:49.590806 139637473359616 logging_writer.py:48] [46000] global_step=46000, grad_norm=0.42931634187698364, loss=1.9391448497772217
I0306 02:55:24.647563 139637464966912 logging_writer.py:48] [46100] global_step=46100, grad_norm=0.2705637812614441, loss=1.872930645942688
I0306 02:55:59.711247 139637473359616 logging_writer.py:48] [46200] global_step=46200, grad_norm=0.27546703815460205, loss=1.9931385517120361
I0306 02:56:34.771654 139637464966912 logging_writer.py:48] [46300] global_step=46300, grad_norm=0.3136020004749298, loss=1.9697974920272827
I0306 02:57:09.830959 139637473359616 logging_writer.py:48] [46400] global_step=46400, grad_norm=0.3483111560344696, loss=1.9258228540420532
I0306 02:57:44.880910 139637464966912 logging_writer.py:48] [46500] global_step=46500, grad_norm=0.35384809970855713, loss=1.975277304649353
I0306 02:58:19.917739 139637473359616 logging_writer.py:48] [46600] global_step=46600, grad_norm=0.48145774006843567, loss=1.9351015090942383
I0306 02:58:54.971588 139637464966912 logging_writer.py:48] [46700] global_step=46700, grad_norm=0.3814278542995453, loss=1.930311679840088
I0306 02:59:30.022887 139637473359616 logging_writer.py:48] [46800] global_step=46800, grad_norm=0.3085651099681854, loss=1.961220622062683
I0306 03:00:05.095156 139637464966912 logging_writer.py:48] [46900] global_step=46900, grad_norm=0.4189409911632538, loss=1.9557275772094727
I0306 03:00:40.170020 139637473359616 logging_writer.py:48] [47000] global_step=47000, grad_norm=0.3740101456642151, loss=1.9761992692947388
I0306 03:01:15.246202 139637464966912 logging_writer.py:48] [47100] global_step=47100, grad_norm=0.2588994801044464, loss=1.9480849504470825
I0306 03:01:50.280019 139637473359616 logging_writer.py:48] [47200] global_step=47200, grad_norm=0.2951192557811737, loss=1.9686429500579834
I0306 03:02:25.315878 139637464966912 logging_writer.py:48] [47300] global_step=47300, grad_norm=0.328916996717453, loss=1.9282793998718262
I0306 03:03:00.368258 139637473359616 logging_writer.py:48] [47400] global_step=47400, grad_norm=0.33724308013916016, loss=1.9434531927108765
I0306 03:03:35.410617 139637464966912 logging_writer.py:48] [47500] global_step=47500, grad_norm=0.2754528224468231, loss=1.9713915586471558
I0306 03:04:10.468914 139637473359616 logging_writer.py:48] [47600] global_step=47600, grad_norm=0.4279667139053345, loss=1.977152705192566
I0306 03:04:45.546844 139637464966912 logging_writer.py:48] [47700] global_step=47700, grad_norm=0.29390186071395874, loss=1.9901455640792847
I0306 03:05:13.280541 139780644897984 spec.py:321] Evaluating on the training split.
I0306 03:05:15.929821 139780644897984 workload.py:181] Translating evaluation dataset.
I0306 03:08:28.329223 139780644897984 spec.py:333] Evaluating on the validation split.
I0306 03:08:30.957602 139780644897984 workload.py:181] Translating evaluation dataset.
I0306 03:11:35.410171 139780644897984 spec.py:349] Evaluating on the test split.
I0306 03:11:38.032066 139780644897984 workload.py:181] Translating evaluation dataset.
I0306 03:14:41.066685 139780644897984 submission_runner.py:469] Time since start: 28872.33s, 	Step: 47780, 	{'train/accuracy': 0.6148956418037415, 'train/loss': 1.9567922353744507, 'train/bleu': 29.69566775485104, 'validation/accuracy': 0.6333893537521362, 'validation/loss': 1.7925602197647095, 'validation/bleu': 26.410435676635764, 'validation/num_examples': 3000, 'test/accuracy': 0.6440505385398865, 'test/loss': 1.7370941638946533, 'test/bleu': 25.275655385631534, 'test/num_examples': 3003, 'score': 16826.967888355255, 'total_duration': 28872.330182552338, 'accumulated_submission_time': 16826.967888355255, 'accumulated_eval_time': 12042.125931501389, 'accumulated_logging_time': 0.4067695140838623}
I0306 03:14:41.081065 139637473359616 logging_writer.py:48] [47780] accumulated_eval_time=12042.1, accumulated_logging_time=0.40677, accumulated_submission_time=16827, global_step=47780, preemption_count=0, score=16827, test/accuracy=0.644051, test/bleu=25.2757, test/loss=1.73709, test/num_examples=3003, total_duration=28872.3, train/accuracy=0.614896, train/bleu=29.6957, train/loss=1.95679, validation/accuracy=0.633389, validation/bleu=26.4104, validation/loss=1.79256, validation/num_examples=3000
I0306 03:14:48.417650 139637464966912 logging_writer.py:48] [47800] global_step=47800, grad_norm=0.44715791940689087, loss=1.8957113027572632
I0306 03:15:23.391807 139637473359616 logging_writer.py:48] [47900] global_step=47900, grad_norm=0.2822575271129608, loss=1.9346466064453125
I0306 03:15:58.381285 139637464966912 logging_writer.py:48] [48000] global_step=48000, grad_norm=0.3921017050743103, loss=1.9162698984146118
I0306 03:16:33.416162 139637473359616 logging_writer.py:48] [48100] global_step=48100, grad_norm=0.29892557859420776, loss=1.9740358591079712
I0306 03:17:08.450033 139637464966912 logging_writer.py:48] [48200] global_step=48200, grad_norm=0.35918888449668884, loss=1.9191615581512451
I0306 03:17:43.491686 139637473359616 logging_writer.py:48] [48300] global_step=48300, grad_norm=0.4649685323238373, loss=1.9719395637512207
I0306 03:18:18.537718 139637464966912 logging_writer.py:48] [48400] global_step=48400, grad_norm=0.2962026596069336, loss=1.9358081817626953
I0306 03:18:53.602104 139637473359616 logging_writer.py:48] [48500] global_step=48500, grad_norm=0.30912017822265625, loss=1.919824242591858
I0306 03:19:28.673928 139637464966912 logging_writer.py:48] [48600] global_step=48600, grad_norm=0.45808786153793335, loss=1.9803402423858643
I0306 03:20:03.734157 139637473359616 logging_writer.py:48] [48700] global_step=48700, grad_norm=0.27579134702682495, loss=1.982857346534729
I0306 03:20:38.802955 139637464966912 logging_writer.py:48] [48800] global_step=48800, grad_norm=0.3610808849334717, loss=1.8567256927490234
I0306 03:21:13.902281 139637473359616 logging_writer.py:48] [48900] global_step=48900, grad_norm=0.4133014380931854, loss=1.9640871286392212
I0306 03:21:49.014077 139637464966912 logging_writer.py:48] [49000] global_step=49000, grad_norm=0.3393237888813019, loss=1.9258383512496948
I0306 03:22:24.085983 139637473359616 logging_writer.py:48] [49100] global_step=49100, grad_norm=0.3259962797164917, loss=2.0670032501220703
I0306 03:22:59.178271 139637464966912 logging_writer.py:48] [49200] global_step=49200, grad_norm=0.3341977894306183, loss=1.9137771129608154
I0306 03:23:34.260513 139637473359616 logging_writer.py:48] [49300] global_step=49300, grad_norm=0.3108489513397217, loss=1.8943299055099487
I0306 03:24:09.347146 139637464966912 logging_writer.py:48] [49400] global_step=49400, grad_norm=0.2752312421798706, loss=1.8827539682388306
I0306 03:24:44.432215 139637473359616 logging_writer.py:48] [49500] global_step=49500, grad_norm=0.2465556263923645, loss=1.899808406829834
I0306 03:25:19.500789 139637464966912 logging_writer.py:48] [49600] global_step=49600, grad_norm=0.2568928599357605, loss=1.9025356769561768
I0306 03:25:54.582481 139637473359616 logging_writer.py:48] [49700] global_step=49700, grad_norm=0.5204377770423889, loss=1.851978063583374
I0306 03:26:29.650202 139637464966912 logging_writer.py:48] [49800] global_step=49800, grad_norm=0.34566015005111694, loss=1.9089057445526123
I0306 03:27:04.722360 139637473359616 logging_writer.py:48] [49900] global_step=49900, grad_norm=0.41277164220809937, loss=1.92756986618042
I0306 03:27:39.822405 139637464966912 logging_writer.py:48] [50000] global_step=50000, grad_norm=0.35490527749061584, loss=1.9537339210510254
I0306 03:28:14.900932 139637473359616 logging_writer.py:48] [50100] global_step=50100, grad_norm=0.34339386224746704, loss=1.9523849487304688
I0306 03:28:41.203018 139780644897984 spec.py:321] Evaluating on the training split.
I0306 03:28:43.849734 139780644897984 workload.py:181] Translating evaluation dataset.
I0306 03:31:52.177322 139780644897984 spec.py:333] Evaluating on the validation split.
I0306 03:31:54.808016 139780644897984 workload.py:181] Translating evaluation dataset.
I0306 03:34:59.660099 139780644897984 spec.py:349] Evaluating on the test split.
I0306 03:35:02.296328 139780644897984 workload.py:181] Translating evaluation dataset.
I0306 03:38:19.270752 139780644897984 submission_runner.py:469] Time since start: 30290.53s, 	Step: 50176, 	{'train/accuracy': 0.6177985072135925, 'train/loss': 1.9343087673187256, 'train/bleu': 29.218852992000112, 'validation/accuracy': 0.63688725233078, 'validation/loss': 1.7758233547210693, 'validation/bleu': 26.168164404893616, 'validation/num_examples': 3000, 'test/accuracy': 0.6449773907661438, 'test/loss': 1.7257072925567627, 'test/bleu': 25.12729817791399, 'test/num_examples': 3003, 'score': 17666.948279619217, 'total_duration': 30290.534225940704, 'accumulated_submission_time': 17666.948279619217, 'accumulated_eval_time': 12620.193596363068, 'accumulated_logging_time': 0.42921018600463867}
I0306 03:38:19.286477 139637464966912 logging_writer.py:48] [50176] accumulated_eval_time=12620.2, accumulated_logging_time=0.42921, accumulated_submission_time=17666.9, global_step=50176, preemption_count=0, score=17666.9, test/accuracy=0.644977, test/bleu=25.1273, test/loss=1.72571, test/num_examples=3003, total_duration=30290.5, train/accuracy=0.617799, train/bleu=29.2189, train/loss=1.93431, validation/accuracy=0.636887, validation/bleu=26.1682, validation/loss=1.77582, validation/num_examples=3000
I0306 03:38:28.036801 139637473359616 logging_writer.py:48] [50200] global_step=50200, grad_norm=0.3645009398460388, loss=1.9288723468780518
I0306 03:39:03.068663 139637464966912 logging_writer.py:48] [50300] global_step=50300, grad_norm=0.4383317232131958, loss=1.95884108543396
I0306 03:39:38.129709 139637473359616 logging_writer.py:48] [50400] global_step=50400, grad_norm=0.3445618152618408, loss=1.9143579006195068
I0306 03:40:13.235812 139637464966912 logging_writer.py:48] [50500] global_step=50500, grad_norm=0.3487699627876282, loss=1.9300826787948608
I0306 03:40:48.368682 139637473359616 logging_writer.py:48] [50600] global_step=50600, grad_norm=0.33950939774513245, loss=1.986135721206665
I0306 03:41:23.533310 139637464966912 logging_writer.py:48] [50700] global_step=50700, grad_norm=0.3816884160041809, loss=1.8534201383590698
I0306 03:41:58.699255 139637473359616 logging_writer.py:48] [50800] global_step=50800, grad_norm=0.522158682346344, loss=1.9460302591323853
I0306 03:42:33.828343 139637464966912 logging_writer.py:48] [50900] global_step=50900, grad_norm=0.2860197126865387, loss=1.988649606704712
I0306 03:43:09.012875 139637473359616 logging_writer.py:48] [51000] global_step=51000, grad_norm=0.33812934160232544, loss=1.9307591915130615
I0306 03:43:44.174056 139637464966912 logging_writer.py:48] [51100] global_step=51100, grad_norm=0.2979496121406555, loss=1.8869447708129883
I0306 03:44:19.339345 139637473359616 logging_writer.py:48] [51200] global_step=51200, grad_norm=0.31036561727523804, loss=1.8915410041809082
I0306 03:44:54.495124 139637464966912 logging_writer.py:48] [51300] global_step=51300, grad_norm=0.2782507538795471, loss=1.885477900505066
I0306 03:45:29.659107 139637473359616 logging_writer.py:48] [51400] global_step=51400, grad_norm=0.3218396008014679, loss=1.8786979913711548
I0306 03:46:04.807400 139637464966912 logging_writer.py:48] [51500] global_step=51500, grad_norm=0.2534526288509369, loss=1.9249169826507568
I0306 03:46:39.992175 139637473359616 logging_writer.py:48] [51600] global_step=51600, grad_norm=0.35440966486930847, loss=1.868416666984558
I0306 03:47:15.164098 139637464966912 logging_writer.py:48] [51700] global_step=51700, grad_norm=0.30500927567481995, loss=1.9408689737319946
I0306 03:47:50.321185 139637473359616 logging_writer.py:48] [51800] global_step=51800, grad_norm=0.3388468027114868, loss=1.9334595203399658
I0306 03:48:25.488015 139637464966912 logging_writer.py:48] [51900] global_step=51900, grad_norm=0.41504403948783875, loss=1.8988062143325806
I0306 03:49:00.674728 139637473359616 logging_writer.py:48] [52000] global_step=52000, grad_norm=0.4637155830860138, loss=1.989715576171875
I0306 03:49:35.814590 139637464966912 logging_writer.py:48] [52100] global_step=52100, grad_norm=0.39534804224967957, loss=1.8881299495697021
I0306 03:50:10.966603 139637473359616 logging_writer.py:48] [52200] global_step=52200, grad_norm=0.30538031458854675, loss=1.9071358442306519
I0306 03:50:46.110056 139637464966912 logging_writer.py:48] [52300] global_step=52300, grad_norm=0.31798553466796875, loss=1.885406732559204
I0306 03:51:21.291392 139637473359616 logging_writer.py:48] [52400] global_step=52400, grad_norm=0.516474187374115, loss=1.9083685874938965
I0306 03:51:56.451405 139637464966912 logging_writer.py:48] [52500] global_step=52500, grad_norm=0.31047961115837097, loss=1.902626872062683
I0306 03:52:19.305515 139780644897984 spec.py:321] Evaluating on the training split.
I0306 03:52:21.941657 139780644897984 workload.py:181] Translating evaluation dataset.
I0306 03:55:57.853324 139780644897984 spec.py:333] Evaluating on the validation split.
I0306 03:56:00.476293 139780644897984 workload.py:181] Translating evaluation dataset.
I0306 03:58:47.035509 139780644897984 spec.py:349] Evaluating on the test split.
I0306 03:58:49.660230 139780644897984 workload.py:181] Translating evaluation dataset.
I0306 04:01:31.204288 139780644897984 submission_runner.py:469] Time since start: 31682.47s, 	Step: 52566, 	{'train/accuracy': 0.6242986917495728, 'train/loss': 1.8886134624481201, 'train/bleu': 29.056832370245576, 'validation/accuracy': 0.6402491927146912, 'validation/loss': 1.755562424659729, 'validation/bleu': 26.479100189923333, 'validation/num_examples': 3000, 'test/accuracy': 0.6487544775009155, 'test/loss': 1.7023996114730835, 'test/bleu': 25.58458299916109, 'test/num_examples': 3003, 'score': 18506.825142383575, 'total_duration': 31682.467784643173, 'accumulated_submission_time': 18506.825142383575, 'accumulated_eval_time': 13172.0923371315, 'accumulated_logging_time': 0.4538750648498535}
I0306 04:01:31.218704 139637473359616 logging_writer.py:48] [52566] accumulated_eval_time=13172.1, accumulated_logging_time=0.453875, accumulated_submission_time=18506.8, global_step=52566, preemption_count=0, score=18506.8, test/accuracy=0.648754, test/bleu=25.5846, test/loss=1.7024, test/num_examples=3003, total_duration=31682.5, train/accuracy=0.624299, train/bleu=29.0568, train/loss=1.88861, validation/accuracy=0.640249, validation/bleu=26.4791, validation/loss=1.75556, validation/num_examples=3000
I0306 04:01:43.466016 139637464966912 logging_writer.py:48] [52600] global_step=52600, grad_norm=0.28153371810913086, loss=1.8937971591949463
I0306 04:02:18.554979 139637473359616 logging_writer.py:48] [52700] global_step=52700, grad_norm=0.47913092374801636, loss=1.8373161554336548
I0306 04:02:53.637103 139637464966912 logging_writer.py:48] [52800] global_step=52800, grad_norm=0.3366987109184265, loss=1.8879355192184448
I0306 04:03:28.759921 139637473359616 logging_writer.py:48] [52900] global_step=52900, grad_norm=0.24419814348220825, loss=1.8726292848587036
I0306 04:04:03.897188 139637464966912 logging_writer.py:48] [53000] global_step=53000, grad_norm=0.28316888213157654, loss=1.9053499698638916
I0306 04:04:39.040500 139637473359616 logging_writer.py:48] [53100] global_step=53100, grad_norm=0.377993106842041, loss=1.983187198638916
I0306 04:05:14.216637 139637464966912 logging_writer.py:48] [53200] global_step=53200, grad_norm=0.37602028250694275, loss=1.957112193107605
I0306 04:05:49.357904 139637473359616 logging_writer.py:48] [53300] global_step=53300, grad_norm=0.283846378326416, loss=1.8771458864212036
I0306 04:06:24.494476 139637464966912 logging_writer.py:48] [53400] global_step=53400, grad_norm=0.35174983739852905, loss=1.8082538843154907
I0306 04:06:59.670469 139637473359616 logging_writer.py:48] [53500] global_step=53500, grad_norm=0.2949157953262329, loss=1.8778932094573975
I0306 04:07:34.839269 139637464966912 logging_writer.py:48] [53600] global_step=53600, grad_norm=0.3350835144519806, loss=1.8935165405273438
I0306 04:08:09.953599 139637473359616 logging_writer.py:48] [53700] global_step=53700, grad_norm=0.29714080691337585, loss=1.9170156717300415
I0306 04:08:45.104251 139637464966912 logging_writer.py:48] [53800] global_step=53800, grad_norm=0.2860580086708069, loss=1.9616514444351196
I0306 04:09:20.238591 139637473359616 logging_writer.py:48] [53900] global_step=53900, grad_norm=0.2935924232006073, loss=1.9545906782150269
I0306 04:09:55.375520 139637464966912 logging_writer.py:48] [54000] global_step=54000, grad_norm=0.2459031194448471, loss=1.907746434211731
I0306 04:10:30.508386 139637473359616 logging_writer.py:48] [54100] global_step=54100, grad_norm=0.38998010754585266, loss=1.9584754705429077
I0306 04:11:05.667368 139637464966912 logging_writer.py:48] [54200] global_step=54200, grad_norm=0.2958393692970276, loss=1.927635908126831
I0306 04:11:40.785616 139637473359616 logging_writer.py:48] [54300] global_step=54300, grad_norm=0.3699069321155548, loss=1.9659390449523926
I0306 04:12:15.914208 139637464966912 logging_writer.py:48] [54400] global_step=54400, grad_norm=0.3121549189090729, loss=1.8822098970413208
I0306 04:12:51.019878 139637473359616 logging_writer.py:48] [54500] global_step=54500, grad_norm=0.35533663630485535, loss=1.8931559324264526
I0306 04:13:26.148382 139637464966912 logging_writer.py:48] [54600] global_step=54600, grad_norm=0.2748519778251648, loss=1.8883275985717773
I0306 04:14:01.273075 139637473359616 logging_writer.py:48] [54700] global_step=54700, grad_norm=0.31021812558174133, loss=1.9935646057128906
I0306 04:14:36.383093 139637464966912 logging_writer.py:48] [54800] global_step=54800, grad_norm=0.35015273094177246, loss=1.8955029249191284
I0306 04:15:11.561539 139637473359616 logging_writer.py:48] [54900] global_step=54900, grad_norm=0.25723621249198914, loss=1.9330356121063232
I0306 04:15:31.236601 139780644897984 spec.py:321] Evaluating on the training split.
I0306 04:15:33.880800 139780644897984 workload.py:181] Translating evaluation dataset.
I0306 04:18:58.984698 139780644897984 spec.py:333] Evaluating on the validation split.
I0306 04:19:01.621886 139780644897984 workload.py:181] Translating evaluation dataset.
I0306 04:22:00.330152 139780644897984 spec.py:349] Evaluating on the test split.
I0306 04:22:02.950198 139780644897984 workload.py:181] Translating evaluation dataset.
I0306 04:24:53.556443 139780644897984 submission_runner.py:469] Time since start: 33084.82s, 	Step: 54957, 	{'train/accuracy': 0.6213428974151611, 'train/loss': 1.9050761461257935, 'train/bleu': 30.01752305387121, 'validation/accuracy': 0.6413492560386658, 'validation/loss': 1.7537524700164795, 'validation/bleu': 26.549013811113188, 'validation/num_examples': 3000, 'test/accuracy': 0.6496466398239136, 'test/loss': 1.6951619386672974, 'test/bleu': 25.780822763996838, 'test/num_examples': 3003, 'score': 19346.699776649475, 'total_duration': 33084.819929122925, 'accumulated_submission_time': 19346.699776649475, 'accumulated_eval_time': 13734.412126541138, 'accumulated_logging_time': 0.4764060974121094}
I0306 04:24:53.570752 139637464966912 logging_writer.py:48] [54957] accumulated_eval_time=13734.4, accumulated_logging_time=0.476406, accumulated_submission_time=19346.7, global_step=54957, preemption_count=0, score=19346.7, test/accuracy=0.649647, test/bleu=25.7808, test/loss=1.69516, test/num_examples=3003, total_duration=33084.8, train/accuracy=0.621343, train/bleu=30.0175, train/loss=1.90508, validation/accuracy=0.641349, validation/bleu=26.549, validation/loss=1.75375, validation/num_examples=3000
I0306 04:25:08.957664 139637473359616 logging_writer.py:48] [55000] global_step=55000, grad_norm=0.287631094455719, loss=1.9067370891571045
I0306 04:25:43.987075 139637464966912 logging_writer.py:48] [55100] global_step=55100, grad_norm=0.32771816849708557, loss=1.9149951934814453
I0306 04:26:19.093873 139637473359616 logging_writer.py:48] [55200] global_step=55200, grad_norm=0.26347726583480835, loss=1.8662930727005005
I0306 04:26:54.205980 139637464966912 logging_writer.py:48] [55300] global_step=55300, grad_norm=0.33406639099121094, loss=1.8525795936584473
I0306 04:27:29.329000 139637473359616 logging_writer.py:48] [55400] global_step=55400, grad_norm=0.465945839881897, loss=1.9939347505569458
I0306 04:28:04.459563 139637464966912 logging_writer.py:48] [55500] global_step=55500, grad_norm=0.2931014895439148, loss=1.9966813325881958
I0306 04:28:39.602924 139637473359616 logging_writer.py:48] [55600] global_step=55600, grad_norm=0.30011823773384094, loss=1.8389791250228882
I0306 04:29:14.746438 139637464966912 logging_writer.py:48] [55700] global_step=55700, grad_norm=0.3259729743003845, loss=1.9158248901367188
I0306 04:29:49.863329 139637473359616 logging_writer.py:48] [55800] global_step=55800, grad_norm=0.3291736841201782, loss=1.8550525903701782
I0306 04:30:24.980221 139637464966912 logging_writer.py:48] [55900] global_step=55900, grad_norm=0.2730727195739746, loss=1.8979874849319458
I0306 04:31:00.114729 139637473359616 logging_writer.py:48] [56000] global_step=56000, grad_norm=0.28513675928115845, loss=1.9150564670562744
I0306 04:31:35.202230 139637464966912 logging_writer.py:48] [56100] global_step=56100, grad_norm=0.30659329891204834, loss=1.9980111122131348
I0306 04:32:10.344679 139637473359616 logging_writer.py:48] [56200] global_step=56200, grad_norm=0.31931954622268677, loss=2.005983591079712
I0306 04:32:45.512038 139637464966912 logging_writer.py:48] [56300] global_step=56300, grad_norm=0.29442140460014343, loss=1.8516093492507935
I0306 04:33:20.701542 139637473359616 logging_writer.py:48] [56400] global_step=56400, grad_norm=0.28162968158721924, loss=1.8977841138839722
I0306 04:33:55.898904 139637464966912 logging_writer.py:48] [56500] global_step=56500, grad_norm=0.26286476850509644, loss=1.9129176139831543
I0306 04:34:31.049184 139637473359616 logging_writer.py:48] [56600] global_step=56600, grad_norm=0.29154762625694275, loss=1.9710532426834106
I0306 04:35:06.226211 139637464966912 logging_writer.py:48] [56700] global_step=56700, grad_norm=0.3196571469306946, loss=1.8246928453445435
I0306 04:35:41.393805 139637473359616 logging_writer.py:48] [56800] global_step=56800, grad_norm=0.2935870885848999, loss=1.8832402229309082
I0306 04:36:16.542446 139637464966912 logging_writer.py:48] [56900] global_step=56900, grad_norm=0.32208701968193054, loss=1.9255568981170654
I0306 04:36:51.727408 139637473359616 logging_writer.py:48] [57000] global_step=57000, grad_norm=0.3034355938434601, loss=1.966895341873169
I0306 04:37:26.884006 139637464966912 logging_writer.py:48] [57100] global_step=57100, grad_norm=0.2895265221595764, loss=1.938270926475525
I0306 04:38:02.046160 139637473359616 logging_writer.py:48] [57200] global_step=57200, grad_norm=0.4005039930343628, loss=1.9521880149841309
I0306 04:38:37.211285 139637464966912 logging_writer.py:48] [57300] global_step=57300, grad_norm=0.36368829011917114, loss=1.9377473592758179
I0306 04:38:53.746218 139780644897984 spec.py:321] Evaluating on the training split.
I0306 04:38:56.383291 139780644897984 workload.py:181] Translating evaluation dataset.
I0306 04:42:30.819727 139780644897984 spec.py:333] Evaluating on the validation split.
I0306 04:42:33.450644 139780644897984 workload.py:181] Translating evaluation dataset.
I0306 04:45:45.976518 139780644897984 spec.py:349] Evaluating on the test split.
I0306 04:45:48.597371 139780644897984 workload.py:181] Translating evaluation dataset.
I0306 04:48:50.635786 139780644897984 submission_runner.py:469] Time since start: 34521.90s, 	Step: 57348, 	{'train/accuracy': 0.6256750226020813, 'train/loss': 1.855641484260559, 'train/bleu': 29.939524256442464, 'validation/accuracy': 0.6423627734184265, 'validation/loss': 1.7347811460494995, 'validation/bleu': 26.631667853075847, 'validation/num_examples': 3000, 'test/accuracy': 0.6516278386116028, 'test/loss': 1.6796623468399048, 'test/bleu': 26.455345305136365, 'test/num_examples': 3003, 'score': 20186.733825206757, 'total_duration': 34521.89927506447, 'accumulated_submission_time': 20186.733825206757, 'accumulated_eval_time': 14331.30164551735, 'accumulated_logging_time': 0.4990816116333008}
I0306 04:48:50.652405 139637473359616 logging_writer.py:48] [57348] accumulated_eval_time=14331.3, accumulated_logging_time=0.499082, accumulated_submission_time=20186.7, global_step=57348, preemption_count=0, score=20186.7, test/accuracy=0.651628, test/bleu=26.4553, test/loss=1.67966, test/num_examples=3003, total_duration=34521.9, train/accuracy=0.625675, train/bleu=29.9395, train/loss=1.85564, validation/accuracy=0.642363, validation/bleu=26.6317, validation/loss=1.73478, validation/num_examples=3000
I0306 04:49:09.195271 139637464966912 logging_writer.py:48] [57400] global_step=57400, grad_norm=0.28781914710998535, loss=1.8900043964385986
I0306 04:49:44.284837 139637473359616 logging_writer.py:48] [57500] global_step=57500, grad_norm=0.29862549901008606, loss=1.8071516752243042
I0306 04:50:19.438133 139637464966912 logging_writer.py:48] [57600] global_step=57600, grad_norm=0.3003341555595398, loss=1.9036906957626343
I0306 04:50:54.569467 139637473359616 logging_writer.py:48] [57700] global_step=57700, grad_norm=0.2774971127510071, loss=1.8734054565429688
I0306 04:51:29.688635 139637464966912 logging_writer.py:48] [57800] global_step=57800, grad_norm=0.2893902361392975, loss=1.92592453956604
I0306 04:52:04.842126 139637473359616 logging_writer.py:48] [57900] global_step=57900, grad_norm=0.30374452471733093, loss=1.9317055940628052
I0306 04:52:40.000373 139637464966912 logging_writer.py:48] [58000] global_step=58000, grad_norm=0.29124823212623596, loss=1.8913280963897705
I0306 04:53:15.150117 139637473359616 logging_writer.py:48] [58100] global_step=58100, grad_norm=0.35523080825805664, loss=1.8424757719039917
I0306 04:53:50.297343 139637464966912 logging_writer.py:48] [58200] global_step=58200, grad_norm=0.32913047075271606, loss=1.851548194885254
I0306 04:54:25.412824 139637473359616 logging_writer.py:48] [58300] global_step=58300, grad_norm=0.31479692459106445, loss=1.9081870317459106
I0306 04:55:00.568789 139637464966912 logging_writer.py:48] [58400] global_step=58400, grad_norm=0.3300918638706207, loss=1.900067687034607
I0306 04:55:35.703958 139637473359616 logging_writer.py:48] [58500] global_step=58500, grad_norm=0.33289074897766113, loss=1.9344521760940552
I0306 04:56:10.822164 139637464966912 logging_writer.py:48] [58600] global_step=58600, grad_norm=0.25297513604164124, loss=1.8731433153152466
I0306 04:56:45.966178 139637473359616 logging_writer.py:48] [58700] global_step=58700, grad_norm=0.3080517649650574, loss=1.8519017696380615
I0306 04:57:21.099139 139637464966912 logging_writer.py:48] [58800] global_step=58800, grad_norm=0.3197392225265503, loss=1.8967993259429932
I0306 04:57:56.212148 139637473359616 logging_writer.py:48] [58900] global_step=58900, grad_norm=0.3254202902317047, loss=2.020617723464966
I0306 04:58:31.346056 139637464966912 logging_writer.py:48] [59000] global_step=59000, grad_norm=0.26656320691108704, loss=1.9262888431549072
I0306 04:59:06.483122 139637473359616 logging_writer.py:48] [59100] global_step=59100, grad_norm=0.29645630717277527, loss=1.9098854064941406
I0306 04:59:41.596962 139637464966912 logging_writer.py:48] [59200] global_step=59200, grad_norm=0.3503496050834656, loss=1.8548941612243652
I0306 05:00:16.743289 139637473359616 logging_writer.py:48] [59300] global_step=59300, grad_norm=0.39521726965904236, loss=1.9424902200698853
I0306 05:00:51.893926 139637464966912 logging_writer.py:48] [59400] global_step=59400, grad_norm=0.3101956248283386, loss=1.854657530784607
I0306 05:01:27.047922 139637473359616 logging_writer.py:48] [59500] global_step=59500, grad_norm=0.26830655336380005, loss=1.8983988761901855
I0306 05:02:02.178251 139637464966912 logging_writer.py:48] [59600] global_step=59600, grad_norm=0.2891821265220642, loss=1.827948808670044
I0306 05:02:37.354527 139637473359616 logging_writer.py:48] [59700] global_step=59700, grad_norm=0.2928987741470337, loss=1.9424208402633667
I0306 05:02:50.717344 139780644897984 spec.py:321] Evaluating on the training split.
I0306 05:02:53.361556 139780644897984 workload.py:181] Translating evaluation dataset.
I0306 05:06:09.949717 139780644897984 spec.py:333] Evaluating on the validation split.
I0306 05:06:12.574650 139780644897984 workload.py:181] Translating evaluation dataset.
I0306 05:09:12.184350 139780644897984 spec.py:349] Evaluating on the test split.
I0306 05:09:14.811723 139780644897984 workload.py:181] Translating evaluation dataset.
I0306 05:12:00.127301 139780644897984 submission_runner.py:469] Time since start: 35911.39s, 	Step: 59739, 	{'train/accuracy': 0.6223832368850708, 'train/loss': 1.903565764427185, 'train/bleu': 29.0736830086834, 'validation/accuracy': 0.6441178917884827, 'validation/loss': 1.7266254425048828, 'validation/bleu': 26.47571224019692, 'validation/num_examples': 3000, 'test/accuracy': 0.6523346304893494, 'test/loss': 1.6676288843154907, 'test/bleu': 25.343149795552087, 'test/num_examples': 3003, 'score': 21026.65973854065, 'total_duration': 35911.3907866478, 'accumulated_submission_time': 21026.65973854065, 'accumulated_eval_time': 14880.711547613144, 'accumulated_logging_time': 0.5244557857513428}
I0306 05:12:00.142344 139637464966912 logging_writer.py:48] [59739] accumulated_eval_time=14880.7, accumulated_logging_time=0.524456, accumulated_submission_time=21026.7, global_step=59739, preemption_count=0, score=21026.7, test/accuracy=0.652335, test/bleu=25.3431, test/loss=1.66763, test/num_examples=3003, total_duration=35911.4, train/accuracy=0.622383, train/bleu=29.0737, train/loss=1.90357, validation/accuracy=0.644118, validation/bleu=26.4757, validation/loss=1.72663, validation/num_examples=3000
I0306 05:12:21.842487 139637473359616 logging_writer.py:48] [59800] global_step=59800, grad_norm=0.2722979187965393, loss=1.8823202848434448
I0306 05:12:56.934185 139637464966912 logging_writer.py:48] [59900] global_step=59900, grad_norm=0.28151559829711914, loss=1.9109307527542114
I0306 05:13:32.041814 139637473359616 logging_writer.py:48] [60000] global_step=60000, grad_norm=0.27828022837638855, loss=1.7999757528305054
I0306 05:14:07.180312 139637464966912 logging_writer.py:48] [60100] global_step=60100, grad_norm=0.28592684864997864, loss=1.8708891868591309
I0306 05:14:42.355127 139637473359616 logging_writer.py:48] [60200] global_step=60200, grad_norm=0.24660509824752808, loss=1.932374119758606
I0306 05:15:17.510055 139637464966912 logging_writer.py:48] [60300] global_step=60300, grad_norm=0.30350634455680847, loss=1.8934438228607178
I0306 05:15:52.678597 139637473359616 logging_writer.py:48] [60400] global_step=60400, grad_norm=0.3194524645805359, loss=1.9139283895492554
I0306 05:16:27.845695 139637464966912 logging_writer.py:48] [60500] global_step=60500, grad_norm=0.34238970279693604, loss=1.908629059791565
I0306 05:17:03.019322 139637473359616 logging_writer.py:48] [60600] global_step=60600, grad_norm=0.2974719703197479, loss=1.952767252922058
I0306 05:17:38.197924 139637464966912 logging_writer.py:48] [60700] global_step=60700, grad_norm=0.30195683240890503, loss=1.9442646503448486
I0306 05:18:13.338331 139637473359616 logging_writer.py:48] [60800] global_step=60800, grad_norm=0.2526926100254059, loss=1.8021951913833618
I0306 05:18:48.516133 139637464966912 logging_writer.py:48] [60900] global_step=60900, grad_norm=0.29806315898895264, loss=1.8717628717422485
I0306 05:19:23.655433 139637473359616 logging_writer.py:48] [61000] global_step=61000, grad_norm=0.3286097049713135, loss=1.8503153324127197
I0306 05:19:58.836290 139637464966912 logging_writer.py:48] [61100] global_step=61100, grad_norm=0.3602513372898102, loss=2.0234861373901367
I0306 05:20:33.981015 139637473359616 logging_writer.py:48] [61200] global_step=61200, grad_norm=0.30377933382987976, loss=1.868046760559082
I0306 05:21:09.123476 139637464966912 logging_writer.py:48] [61300] global_step=61300, grad_norm=0.34007325768470764, loss=1.8823418617248535
I0306 05:21:44.259316 139637473359616 logging_writer.py:48] [61400] global_step=61400, grad_norm=0.29654985666275024, loss=1.8099429607391357
I0306 05:22:19.383806 139637464966912 logging_writer.py:48] [61500] global_step=61500, grad_norm=0.3431464433670044, loss=1.893903136253357
I0306 05:22:54.521123 139637473359616 logging_writer.py:48] [61600] global_step=61600, grad_norm=0.27083346247673035, loss=1.9139741659164429
I0306 05:23:29.637384 139637464966912 logging_writer.py:48] [61700] global_step=61700, grad_norm=0.28554636240005493, loss=1.8735861778259277
I0306 05:24:04.763751 139637473359616 logging_writer.py:48] [61800] global_step=61800, grad_norm=0.27721914649009705, loss=1.872132420539856
I0306 05:24:39.881224 139637464966912 logging_writer.py:48] [61900] global_step=61900, grad_norm=0.305984765291214, loss=1.9284210205078125
I0306 05:25:15.033688 139637473359616 logging_writer.py:48] [62000] global_step=62000, grad_norm=0.30863457918167114, loss=1.9502061605453491
I0306 05:25:50.212879 139637464966912 logging_writer.py:48] [62100] global_step=62100, grad_norm=0.3627638518810272, loss=1.7808904647827148
I0306 05:26:00.409644 139780644897984 spec.py:321] Evaluating on the training split.
I0306 05:26:03.054016 139780644897984 workload.py:181] Translating evaluation dataset.
I0306 05:30:31.157831 139780644897984 spec.py:333] Evaluating on the validation split.
I0306 05:30:33.797091 139780644897984 workload.py:181] Translating evaluation dataset.
I0306 05:34:02.879726 139780644897984 spec.py:349] Evaluating on the test split.
I0306 05:34:05.503273 139780644897984 workload.py:181] Translating evaluation dataset.
I0306 05:37:48.974634 139780644897984 submission_runner.py:469] Time since start: 37460.24s, 	Step: 62130, 	{'train/accuracy': 0.629525899887085, 'train/loss': 1.8580882549285889, 'train/bleu': 30.115997303002217, 'validation/accuracy': 0.6494203209877014, 'validation/loss': 1.6964271068572998, 'validation/bleu': 27.13753839038238, 'validation/num_examples': 3000, 'test/accuracy': 0.6586953997612, 'test/loss': 1.6383004188537598, 'test/bleu': 26.551777613555267, 'test/num_examples': 3003, 'score': 21866.7843477726, 'total_duration': 37460.23810696602, 'accumulated_submission_time': 21866.7843477726, 'accumulated_eval_time': 15589.276465177536, 'accumulated_logging_time': 0.5475189685821533}
I0306 05:37:48.989728 139637473359616 logging_writer.py:48] [62130] accumulated_eval_time=15589.3, accumulated_logging_time=0.547519, accumulated_submission_time=21866.8, global_step=62130, preemption_count=0, score=21866.8, test/accuracy=0.658695, test/bleu=26.5518, test/loss=1.6383, test/num_examples=3003, total_duration=37460.2, train/accuracy=0.629526, train/bleu=30.116, train/loss=1.85809, validation/accuracy=0.64942, validation/bleu=27.1375, validation/loss=1.69643, validation/num_examples=3000
I0306 05:38:13.868421 139637464966912 logging_writer.py:48] [62200] global_step=62200, grad_norm=0.30667978525161743, loss=1.7895097732543945
I0306 05:38:48.943808 139637473359616 logging_writer.py:48] [62300] global_step=62300, grad_norm=0.3029436767101288, loss=1.8753751516342163
I0306 05:39:24.088970 139637464966912 logging_writer.py:48] [62400] global_step=62400, grad_norm=0.34566348791122437, loss=1.8500851392745972
I0306 05:39:59.196669 139637473359616 logging_writer.py:48] [62500] global_step=62500, grad_norm=0.283514142036438, loss=1.8787485361099243
I0306 05:40:34.372305 139637464966912 logging_writer.py:48] [62600] global_step=62600, grad_norm=0.32542699575424194, loss=1.9255056381225586
I0306 05:41:09.532446 139637473359616 logging_writer.py:48] [62700] global_step=62700, grad_norm=0.3272099196910858, loss=1.959852695465088
I0306 05:41:44.617371 139637464966912 logging_writer.py:48] [62800] global_step=62800, grad_norm=0.2730504870414734, loss=1.785066843032837
I0306 05:42:19.687431 139637473359616 logging_writer.py:48] [62900] global_step=62900, grad_norm=0.33130893111228943, loss=1.7937109470367432
I0306 05:42:54.737665 139637464966912 logging_writer.py:48] [63000] global_step=63000, grad_norm=0.3531530201435089, loss=1.8929567337036133
I0306 05:43:29.804306 139637473359616 logging_writer.py:48] [63100] global_step=63100, grad_norm=0.31779253482818604, loss=1.797252893447876
I0306 05:44:04.844216 139637464966912 logging_writer.py:48] [63200] global_step=63200, grad_norm=0.298094779253006, loss=1.8993752002716064
I0306 05:44:39.883966 139637473359616 logging_writer.py:48] [63300] global_step=63300, grad_norm=0.3458528220653534, loss=1.936998963356018
I0306 05:45:14.955611 139637464966912 logging_writer.py:48] [63400] global_step=63400, grad_norm=0.26490476727485657, loss=1.8527216911315918
I0306 05:45:49.990375 139637473359616 logging_writer.py:48] [63500] global_step=63500, grad_norm=0.2987394630908966, loss=1.8595761060714722
I0306 05:46:25.057031 139637464966912 logging_writer.py:48] [63600] global_step=63600, grad_norm=0.31253302097320557, loss=1.8319159746170044
I0306 05:47:00.072966 139637473359616 logging_writer.py:48] [63700] global_step=63700, grad_norm=0.32463517785072327, loss=1.92903470993042
I0306 05:47:35.073169 139637464966912 logging_writer.py:48] [63800] global_step=63800, grad_norm=0.2648031711578369, loss=1.8515050411224365
I0306 05:48:10.103935 139637473359616 logging_writer.py:48] [63900] global_step=63900, grad_norm=0.2830776274204254, loss=1.9010010957717896
I0306 05:48:45.125667 139637464966912 logging_writer.py:48] [64000] global_step=64000, grad_norm=0.3579467833042145, loss=1.921001672744751
I0306 05:49:20.144158 139637473359616 logging_writer.py:48] [64100] global_step=64100, grad_norm=0.30395689606666565, loss=1.7733285427093506
I0306 05:49:55.170068 139637464966912 logging_writer.py:48] [64200] global_step=64200, grad_norm=0.37847834825515747, loss=1.9116160869598389
I0306 05:50:30.224370 139637473359616 logging_writer.py:48] [64300] global_step=64300, grad_norm=0.2817879915237427, loss=1.865735411643982
I0306 05:51:05.298287 139637464966912 logging_writer.py:48] [64400] global_step=64400, grad_norm=0.34445929527282715, loss=1.885257363319397
I0306 05:51:40.367959 139637473359616 logging_writer.py:48] [64500] global_step=64500, grad_norm=0.33907824754714966, loss=1.9379123449325562
I0306 05:51:49.148588 139780644897984 spec.py:321] Evaluating on the training split.
I0306 05:51:51.792697 139780644897984 workload.py:181] Translating evaluation dataset.
I0306 05:55:21.929423 139780644897984 spec.py:333] Evaluating on the validation split.
I0306 05:55:24.558798 139780644897984 workload.py:181] Translating evaluation dataset.
I0306 05:58:26.160181 139780644897984 spec.py:349] Evaluating on the test split.
I0306 05:58:28.800573 139780644897984 workload.py:181] Translating evaluation dataset.
I0306 06:01:19.868308 139780644897984 submission_runner.py:469] Time since start: 38871.13s, 	Step: 64526, 	{'train/accuracy': 0.631919801235199, 'train/loss': 1.8332370519638062, 'train/bleu': 30.10764812044717, 'validation/accuracy': 0.6493709087371826, 'validation/loss': 1.684674620628357, 'validation/bleu': 27.036690597365407, 'validation/num_examples': 3000, 'test/accuracy': 0.6584520936012268, 'test/loss': 1.6274995803833008, 'test/bleu': 26.280884335373482, 'test/num_examples': 3003, 'score': 22706.800669431686, 'total_duration': 38871.131801366806, 'accumulated_submission_time': 22706.800669431686, 'accumulated_eval_time': 16159.996134757996, 'accumulated_logging_time': 0.5718483924865723}
I0306 06:01:19.883886 139637464966912 logging_writer.py:48] [64526] accumulated_eval_time=16160, accumulated_logging_time=0.571848, accumulated_submission_time=22706.8, global_step=64526, preemption_count=0, score=22706.8, test/accuracy=0.658452, test/bleu=26.2809, test/loss=1.6275, test/num_examples=3003, total_duration=38871.1, train/accuracy=0.63192, train/bleu=30.1076, train/loss=1.83324, validation/accuracy=0.649371, validation/bleu=27.0367, validation/loss=1.68467, validation/num_examples=3000
I0306 06:01:46.122678 139637473359616 logging_writer.py:48] [64600] global_step=64600, grad_norm=0.3095558285713196, loss=1.8377106189727783
I0306 06:02:21.132562 139637464966912 logging_writer.py:48] [64700] global_step=64700, grad_norm=0.271718293428421, loss=1.8902685642242432
I0306 06:02:56.128267 139637473359616 logging_writer.py:48] [64800] global_step=64800, grad_norm=0.2649613916873932, loss=1.7990171909332275
I0306 06:03:31.168466 139637464966912 logging_writer.py:48] [64900] global_step=64900, grad_norm=0.3402286767959595, loss=1.8709317445755005
I0306 06:04:06.216927 139637473359616 logging_writer.py:48] [65000] global_step=65000, grad_norm=0.2778589129447937, loss=1.8406832218170166
I0306 06:04:41.275848 139637464966912 logging_writer.py:48] [65100] global_step=65100, grad_norm=0.30597296357154846, loss=1.8302507400512695
I0306 06:05:16.349425 139637473359616 logging_writer.py:48] [65200] global_step=65200, grad_norm=0.3146420121192932, loss=1.7980761528015137
I0306 06:05:51.401735 139637464966912 logging_writer.py:48] [65300] global_step=65300, grad_norm=0.31528836488723755, loss=1.7881357669830322
I0306 06:06:26.469345 139637473359616 logging_writer.py:48] [65400] global_step=65400, grad_norm=0.2971706688404083, loss=1.88515043258667
I0306 06:07:01.530708 139637464966912 logging_writer.py:48] [65500] global_step=65500, grad_norm=0.29807838797569275, loss=1.9055875539779663
I0306 06:07:36.570173 139637473359616 logging_writer.py:48] [65600] global_step=65600, grad_norm=0.31070882081985474, loss=1.8652291297912598
I0306 06:08:11.620242 139637464966912 logging_writer.py:48] [65700] global_step=65700, grad_norm=0.31405404210090637, loss=1.7929273843765259
I0306 06:08:46.692718 139637473359616 logging_writer.py:48] [65800] global_step=65800, grad_norm=0.33745279908180237, loss=1.7588282823562622
I0306 06:09:21.718767 139637464966912 logging_writer.py:48] [65900] global_step=65900, grad_norm=0.31672269105911255, loss=1.8180583715438843
I0306 06:09:56.768051 139637473359616 logging_writer.py:48] [66000] global_step=66000, grad_norm=0.26014265418052673, loss=1.8112431764602661
I0306 06:10:31.837107 139637464966912 logging_writer.py:48] [66100] global_step=66100, grad_norm=0.27970486879348755, loss=1.8216280937194824
I0306 06:11:06.905954 139637473359616 logging_writer.py:48] [66200] global_step=66200, grad_norm=0.2856217324733734, loss=1.8818315267562866
I0306 06:11:42.009146 139637464966912 logging_writer.py:48] [66300] global_step=66300, grad_norm=0.32915937900543213, loss=1.850553035736084
I0306 06:12:17.075106 139637473359616 logging_writer.py:48] [66400] global_step=66400, grad_norm=0.2909289598464966, loss=1.9084850549697876
I0306 06:12:52.178224 139637464966912 logging_writer.py:48] [66500] global_step=66500, grad_norm=0.3489297330379486, loss=1.897446870803833
I0306 06:13:27.241523 139637473359616 logging_writer.py:48] [66600] global_step=66600, grad_norm=0.31511884927749634, loss=1.8250263929367065
I0306 06:14:02.328243 139637464966912 logging_writer.py:48] [66700] global_step=66700, grad_norm=0.32276931405067444, loss=1.7280775308609009
I0306 06:14:37.400368 139637473359616 logging_writer.py:48] [66800] global_step=66800, grad_norm=0.3009750545024872, loss=1.802229881286621
I0306 06:15:12.462842 139637464966912 logging_writer.py:48] [66900] global_step=66900, grad_norm=0.3075295090675354, loss=1.8836601972579956
I0306 06:15:20.172881 139780644897984 spec.py:321] Evaluating on the training split.
I0306 06:15:22.814525 139780644897984 workload.py:181] Translating evaluation dataset.
I0306 06:18:33.932410 139780644897984 spec.py:333] Evaluating on the validation split.
I0306 06:18:36.568418 139780644897984 workload.py:181] Translating evaluation dataset.
I0306 06:21:24.254084 139780644897984 spec.py:349] Evaluating on the test split.
I0306 06:21:26.887059 139780644897984 workload.py:181] Translating evaluation dataset.
I0306 06:24:10.837985 139780644897984 submission_runner.py:469] Time since start: 40242.10s, 	Step: 66923, 	{'train/accuracy': 0.6317277550697327, 'train/loss': 1.8329476118087769, 'train/bleu': 30.41384039937238, 'validation/accuracy': 0.6514968276023865, 'validation/loss': 1.6818063259124756, 'validation/bleu': 27.42363948793524, 'validation/num_examples': 3000, 'test/accuracy': 0.660398542881012, 'test/loss': 1.6205717325210571, 'test/bleu': 26.482613775499154, 'test/num_examples': 3003, 'score': 23546.947669029236, 'total_duration': 40242.10146975517, 'accumulated_submission_time': 23546.947669029236, 'accumulated_eval_time': 16690.66118168831, 'accumulated_logging_time': 0.596975564956665}
I0306 06:24:10.853780 139637473359616 logging_writer.py:48] [66923] accumulated_eval_time=16690.7, accumulated_logging_time=0.596976, accumulated_submission_time=23546.9, global_step=66923, preemption_count=0, score=23546.9, test/accuracy=0.660399, test/bleu=26.4826, test/loss=1.62057, test/num_examples=3003, total_duration=40242.1, train/accuracy=0.631728, train/bleu=30.4138, train/loss=1.83295, validation/accuracy=0.651497, validation/bleu=27.4236, validation/loss=1.68181, validation/num_examples=3000
I0306 06:24:38.159229 139637464966912 logging_writer.py:48] [67000] global_step=67000, grad_norm=0.2968161404132843, loss=1.795347809791565
I0306 06:25:13.206436 139637473359616 logging_writer.py:48] [67100] global_step=67100, grad_norm=0.27280959486961365, loss=1.7958741188049316
I0306 06:25:48.270967 139637464966912 logging_writer.py:48] [67200] global_step=67200, grad_norm=0.2955949306488037, loss=1.7464715242385864
I0306 06:26:23.347059 139637473359616 logging_writer.py:48] [67300] global_step=67300, grad_norm=0.3334618806838989, loss=1.7812387943267822
I0306 06:26:58.421668 139637464966912 logging_writer.py:48] [67400] global_step=67400, grad_norm=0.3462049067020416, loss=1.8260464668273926
I0306 06:27:33.473965 139637473359616 logging_writer.py:48] [67500] global_step=67500, grad_norm=0.3305996060371399, loss=1.8795359134674072
I0306 06:28:08.558297 139637464966912 logging_writer.py:48] [67600] global_step=67600, grad_norm=0.2962113618850708, loss=1.7866021394729614
I0306 06:28:43.644225 139637473359616 logging_writer.py:48] [67700] global_step=67700, grad_norm=0.3163903057575226, loss=1.985893964767456
I0306 06:29:18.717052 139637464966912 logging_writer.py:48] [67800] global_step=67800, grad_norm=0.30581310391426086, loss=1.861306071281433
I0306 06:29:53.801630 139637473359616 logging_writer.py:48] [67900] global_step=67900, grad_norm=0.296871155500412, loss=1.802517294883728
I0306 06:30:28.861738 139637464966912 logging_writer.py:48] [68000] global_step=68000, grad_norm=0.3166019022464752, loss=1.7704812288284302
I0306 06:31:03.937410 139637473359616 logging_writer.py:48] [68100] global_step=68100, grad_norm=0.3825863301753998, loss=1.7718205451965332
I0306 06:31:39.018433 139637464966912 logging_writer.py:48] [68200] global_step=68200, grad_norm=0.2815166115760803, loss=1.8569376468658447
I0306 06:32:14.092504 139637473359616 logging_writer.py:48] [68300] global_step=68300, grad_norm=0.3266507387161255, loss=1.8248389959335327
I0306 06:32:49.151957 139637464966912 logging_writer.py:48] [68400] global_step=68400, grad_norm=0.30573830008506775, loss=1.8446153402328491
I0306 06:33:24.183356 139637473359616 logging_writer.py:48] [68500] global_step=68500, grad_norm=0.2972661852836609, loss=1.872590184211731
I0306 06:33:59.217871 139637464966912 logging_writer.py:48] [68600] global_step=68600, grad_norm=0.3041645586490631, loss=1.8987349271774292
I0306 06:34:34.280309 139637473359616 logging_writer.py:48] [68700] global_step=68700, grad_norm=0.3201364576816559, loss=1.8082938194274902
I0306 06:35:09.347277 139637464966912 logging_writer.py:48] [68800] global_step=68800, grad_norm=0.3048117160797119, loss=1.8625017404556274
I0306 06:35:44.384244 139637473359616 logging_writer.py:48] [68900] global_step=68900, grad_norm=0.3111252784729004, loss=1.8429744243621826
I0306 06:36:19.424624 139637464966912 logging_writer.py:48] [69000] global_step=69000, grad_norm=0.2651377320289612, loss=1.8078727722167969
I0306 06:36:54.583903 139637473359616 logging_writer.py:48] [69100] global_step=69100, grad_norm=0.37429365515708923, loss=1.8378016948699951
I0306 06:37:29.747664 139637464966912 logging_writer.py:48] [69200] global_step=69200, grad_norm=0.31722691655158997, loss=1.7972131967544556
I0306 06:38:04.876096 139637473359616 logging_writer.py:48] [69300] global_step=69300, grad_norm=0.2914119362831116, loss=1.7913450002670288
I0306 06:38:10.849471 139780644897984 spec.py:321] Evaluating on the training split.
I0306 06:38:13.492449 139780644897984 workload.py:181] Translating evaluation dataset.
I0306 06:41:21.768106 139780644897984 spec.py:333] Evaluating on the validation split.
I0306 06:41:24.399123 139780644897984 workload.py:181] Translating evaluation dataset.
I0306 06:44:31.495384 139780644897984 spec.py:349] Evaluating on the test split.
I0306 06:44:34.135551 139780644897984 workload.py:181] Translating evaluation dataset.
I0306 06:48:02.755055 139780644897984 submission_runner.py:469] Time since start: 41674.02s, 	Step: 69318, 	{'train/accuracy': 0.6512231826782227, 'train/loss': 1.675499439239502, 'train/bleu': 31.225384018976147, 'validation/accuracy': 0.6507057547569275, 'validation/loss': 1.6675829887390137, 'validation/bleu': 27.576943731066336, 'validation/num_examples': 3000, 'test/accuracy': 0.662009060382843, 'test/loss': 1.6103739738464355, 'test/bleu': 26.587290454280655, 'test/num_examples': 3003, 'score': 24386.8045296669, 'total_duration': 41674.018525123596, 'accumulated_submission_time': 24386.8045296669, 'accumulated_eval_time': 17282.56668806076, 'accumulated_logging_time': 0.6208243370056152}
I0306 06:48:02.771071 139637464966912 logging_writer.py:48] [69318] accumulated_eval_time=17282.6, accumulated_logging_time=0.620824, accumulated_submission_time=24386.8, global_step=69318, preemption_count=0, score=24386.8, test/accuracy=0.662009, test/bleu=26.5873, test/loss=1.61037, test/num_examples=3003, total_duration=41674, train/accuracy=0.651223, train/bleu=31.2254, train/loss=1.6755, validation/accuracy=0.650706, validation/bleu=27.5769, validation/loss=1.66758, validation/num_examples=3000
I0306 06:48:31.837319 139637473359616 logging_writer.py:48] [69400] global_step=69400, grad_norm=0.30958130955696106, loss=1.7730151414871216
I0306 06:49:06.880726 139637464966912 logging_writer.py:48] [69500] global_step=69500, grad_norm=0.27778151631355286, loss=1.8391221761703491
I0306 06:49:41.966107 139637473359616 logging_writer.py:48] [69600] global_step=69600, grad_norm=0.3703995645046234, loss=1.8495900630950928
I0306 06:50:17.052072 139637464966912 logging_writer.py:48] [69700] global_step=69700, grad_norm=0.34208497405052185, loss=1.798597812652588
I0306 06:50:52.138063 139637473359616 logging_writer.py:48] [69800] global_step=69800, grad_norm=0.33319520950317383, loss=1.7754281759262085
I0306 06:51:27.272466 139637464966912 logging_writer.py:48] [69900] global_step=69900, grad_norm=0.3199179172515869, loss=1.9488561153411865
I0306 06:52:02.401163 139637473359616 logging_writer.py:48] [70000] global_step=70000, grad_norm=0.3383280336856842, loss=1.7761975526809692
I0306 06:52:37.545725 139637464966912 logging_writer.py:48] [70100] global_step=70100, grad_norm=0.337586909532547, loss=1.8950031995773315
I0306 06:53:12.696882 139637473359616 logging_writer.py:48] [70200] global_step=70200, grad_norm=0.29353052377700806, loss=1.7655647993087769
I0306 06:53:47.843755 139637464966912 logging_writer.py:48] [70300] global_step=70300, grad_norm=0.29518985748291016, loss=1.7716723680496216
I0306 06:54:22.996969 139637473359616 logging_writer.py:48] [70400] global_step=70400, grad_norm=0.3039652109146118, loss=1.8427491188049316
I0306 06:54:58.152757 139637464966912 logging_writer.py:48] [70500] global_step=70500, grad_norm=0.3133372962474823, loss=1.784477710723877
I0306 06:55:33.324479 139637473359616 logging_writer.py:48] [70600] global_step=70600, grad_norm=0.2862255871295929, loss=1.8419623374938965
I0306 06:56:08.455710 139637464966912 logging_writer.py:48] [70700] global_step=70700, grad_norm=0.2902258634567261, loss=1.721335530281067
I0306 06:56:43.625857 139637473359616 logging_writer.py:48] [70800] global_step=70800, grad_norm=0.3264124393463135, loss=1.7900283336639404
I0306 06:57:18.784391 139637464966912 logging_writer.py:48] [70900] global_step=70900, grad_norm=0.3054834306240082, loss=1.8710722923278809
I0306 06:57:53.940725 139637473359616 logging_writer.py:48] [71000] global_step=71000, grad_norm=0.3118993043899536, loss=1.7898955345153809
I0306 06:58:29.104868 139637464966912 logging_writer.py:48] [71100] global_step=71100, grad_norm=0.3401266038417816, loss=1.733248233795166
I0306 06:59:04.274315 139637473359616 logging_writer.py:48] [71200] global_step=71200, grad_norm=0.32619866728782654, loss=1.7930575609207153
I0306 06:59:39.436793 139637464966912 logging_writer.py:48] [71300] global_step=71300, grad_norm=0.32527124881744385, loss=1.8716814517974854
I0306 07:00:14.588081 139637473359616 logging_writer.py:48] [71400] global_step=71400, grad_norm=0.3031628429889679, loss=1.8342604637145996
I0306 07:00:49.719323 139637464966912 logging_writer.py:48] [71500] global_step=71500, grad_norm=0.31624096632003784, loss=1.8256796598434448
I0306 07:01:24.854615 139637473359616 logging_writer.py:48] [71600] global_step=71600, grad_norm=0.35751500725746155, loss=1.736656665802002
I0306 07:01:59.988564 139637464966912 logging_writer.py:48] [71700] global_step=71700, grad_norm=0.3237069547176361, loss=1.7678329944610596
I0306 07:02:02.808167 139780644897984 spec.py:321] Evaluating on the training split.
I0306 07:02:05.455657 139780644897984 workload.py:181] Translating evaluation dataset.
I0306 07:05:56.213510 139780644897984 spec.py:333] Evaluating on the validation split.
I0306 07:05:58.842478 139780644897984 workload.py:181] Translating evaluation dataset.
I0306 07:09:07.559777 139780644897984 spec.py:349] Evaluating on the test split.
I0306 07:09:10.194066 139780644897984 workload.py:181] Translating evaluation dataset.
I0306 07:11:33.876738 139780644897984 submission_runner.py:469] Time since start: 43085.14s, 	Step: 71709, 	{'train/accuracy': 0.6380018591880798, 'train/loss': 1.784746527671814, 'train/bleu': 31.115631648645245, 'validation/accuracy': 0.6546115279197693, 'validation/loss': 1.6529085636138916, 'validation/bleu': 27.843570385245886, 'validation/num_examples': 3000, 'test/accuracy': 0.6642103791236877, 'test/loss': 1.5897046327590942, 'test/bleu': 27.33826033335166, 'test/num_examples': 3003, 'score': 25226.702499628067, 'total_duration': 43085.14022421837, 'accumulated_submission_time': 25226.702499628067, 'accumulated_eval_time': 17853.635200738907, 'accumulated_logging_time': 0.6452395915985107}
I0306 07:11:33.893401 139637473359616 logging_writer.py:48] [71709] accumulated_eval_time=17853.6, accumulated_logging_time=0.64524, accumulated_submission_time=25226.7, global_step=71709, preemption_count=0, score=25226.7, test/accuracy=0.66421, test/bleu=27.3383, test/loss=1.5897, test/num_examples=3003, total_duration=43085.1, train/accuracy=0.638002, train/bleu=31.1156, train/loss=1.78475, validation/accuracy=0.654612, validation/bleu=27.8436, validation/loss=1.65291, validation/num_examples=3000
I0306 07:12:06.342352 139637464966912 logging_writer.py:48] [71800] global_step=71800, grad_norm=0.30223017930984497, loss=1.836439847946167
I0306 07:12:41.607819 139637473359616 logging_writer.py:48] [71900] global_step=71900, grad_norm=0.31066519021987915, loss=1.7996084690093994
I0306 07:13:16.892801 139637464966912 logging_writer.py:48] [72000] global_step=72000, grad_norm=0.2849876284599304, loss=1.8041174411773682
I0306 07:13:52.188672 139637473359616 logging_writer.py:48] [72100] global_step=72100, grad_norm=0.3179209530353546, loss=1.751423954963684
I0306 07:14:27.483944 139637464966912 logging_writer.py:48] [72200] global_step=72200, grad_norm=0.2999705672264099, loss=1.80769681930542
I0306 07:15:02.786495 139637473359616 logging_writer.py:48] [72300] global_step=72300, grad_norm=0.29579877853393555, loss=1.7328497171401978
I0306 07:15:38.107434 139637464966912 logging_writer.py:48] [72400] global_step=72400, grad_norm=0.29354482889175415, loss=1.810476541519165
I0306 07:16:13.381893 139637473359616 logging_writer.py:48] [72500] global_step=72500, grad_norm=0.34253445267677307, loss=1.8090285062789917
I0306 07:16:48.678866 139637464966912 logging_writer.py:48] [72600] global_step=72600, grad_norm=0.3041209876537323, loss=1.7970292568206787
I0306 07:17:23.950902 139637473359616 logging_writer.py:48] [72700] global_step=72700, grad_norm=0.2820650339126587, loss=1.8179380893707275
I0306 07:17:59.240378 139637464966912 logging_writer.py:48] [72800] global_step=72800, grad_norm=0.2809125483036041, loss=1.761762261390686
I0306 07:18:34.529150 139637473359616 logging_writer.py:48] [72900] global_step=72900, grad_norm=0.2989426255226135, loss=1.804553508758545
I0306 07:19:09.831374 139637464966912 logging_writer.py:48] [73000] global_step=73000, grad_norm=0.3510802388191223, loss=1.8254947662353516
I0306 07:19:45.111076 139637473359616 logging_writer.py:48] [73100] global_step=73100, grad_norm=0.33087462186813354, loss=1.7367171049118042
I0306 07:20:20.387275 139637464966912 logging_writer.py:48] [73200] global_step=73200, grad_norm=0.31805843114852905, loss=1.7903252840042114
I0306 07:20:55.708397 139637473359616 logging_writer.py:48] [73300] global_step=73300, grad_norm=0.29598844051361084, loss=1.7538875341415405
I0306 07:21:30.968286 139637464966912 logging_writer.py:48] [73400] global_step=73400, grad_norm=0.3441385328769684, loss=1.7838515043258667
I0306 07:22:06.219003 139637473359616 logging_writer.py:48] [73500] global_step=73500, grad_norm=0.326860249042511, loss=1.8777934312820435
I0306 07:22:41.461446 139637464966912 logging_writer.py:48] [73600] global_step=73600, grad_norm=0.3935472369194031, loss=1.8695704936981201
I0306 07:23:16.691787 139637473359616 logging_writer.py:48] [73700] global_step=73700, grad_norm=0.32408007979393005, loss=1.8435510396957397
I0306 07:23:51.951009 139637464966912 logging_writer.py:48] [73800] global_step=73800, grad_norm=0.32580092549324036, loss=1.7610760927200317
I0306 07:24:27.196302 139637473359616 logging_writer.py:48] [73900] global_step=73900, grad_norm=0.273988276720047, loss=1.8308639526367188
I0306 07:25:02.446287 139637464966912 logging_writer.py:48] [74000] global_step=74000, grad_norm=0.29020988941192627, loss=1.8126220703125
I0306 07:25:34.213226 139780644897984 spec.py:321] Evaluating on the training split.
I0306 07:25:36.869521 139780644897984 workload.py:181] Translating evaluation dataset.
I0306 07:30:05.624355 139780644897984 spec.py:333] Evaluating on the validation split.
I0306 07:30:08.264086 139780644897984 workload.py:181] Translating evaluation dataset.
I0306 07:33:04.548408 139780644897984 spec.py:349] Evaluating on the test split.
I0306 07:33:07.186510 139780644897984 workload.py:181] Translating evaluation dataset.
I0306 07:35:54.049258 139780644897984 submission_runner.py:469] Time since start: 44545.31s, 	Step: 74091, 	{'train/accuracy': 0.634438693523407, 'train/loss': 1.814536690711975, 'train/bleu': 30.601603241312475, 'validation/accuracy': 0.6576026678085327, 'validation/loss': 1.6407557725906372, 'validation/bleu': 27.658826333780933, 'validation/num_examples': 3000, 'test/accuracy': 0.6680801510810852, 'test/loss': 1.5713880062103271, 'test/bleu': 26.986297745060345, 'test/num_examples': 3003, 'score': 26066.805859088898, 'total_duration': 44545.31275200844, 'accumulated_submission_time': 26066.805859088898, 'accumulated_eval_time': 18473.471185207367, 'accumulated_logging_time': 0.7442440986633301}
I0306 07:35:54.067723 139637473359616 logging_writer.py:48] [74091] accumulated_eval_time=18473.5, accumulated_logging_time=0.744244, accumulated_submission_time=26066.8, global_step=74091, preemption_count=0, score=26066.8, test/accuracy=0.66808, test/bleu=26.9863, test/loss=1.57139, test/num_examples=3003, total_duration=44545.3, train/accuracy=0.634439, train/bleu=30.6016, train/loss=1.81454, validation/accuracy=0.657603, validation/bleu=27.6588, validation/loss=1.64076, validation/num_examples=3000
I0306 07:35:57.580295 139637464966912 logging_writer.py:48] [74100] global_step=74100, grad_norm=0.29489535093307495, loss=1.8568755388259888
I0306 07:36:32.737171 139637473359616 logging_writer.py:48] [74200] global_step=74200, grad_norm=0.26653197407722473, loss=1.7880240678787231
I0306 07:37:07.944933 139637464966912 logging_writer.py:48] [74300] global_step=74300, grad_norm=0.2955479621887207, loss=1.7713905572891235
I0306 07:37:43.191074 139637473359616 logging_writer.py:48] [74400] global_step=74400, grad_norm=0.3254045844078064, loss=1.7904298305511475
I0306 07:38:18.489314 139637464966912 logging_writer.py:48] [74500] global_step=74500, grad_norm=0.37930920720100403, loss=1.818149447441101
I0306 07:38:53.759645 139637473359616 logging_writer.py:48] [74600] global_step=74600, grad_norm=0.332244336605072, loss=1.839687466621399
I0306 07:39:29.035208 139637464966912 logging_writer.py:48] [74700] global_step=74700, grad_norm=0.3380407989025116, loss=1.9040642976760864
I0306 07:40:04.293767 139637473359616 logging_writer.py:48] [74800] global_step=74800, grad_norm=0.3103598356246948, loss=1.7605724334716797
I0306 07:40:39.567396 139637464966912 logging_writer.py:48] [74900] global_step=74900, grad_norm=0.30317819118499756, loss=1.788588285446167
I0306 07:41:14.844750 139637473359616 logging_writer.py:48] [75000] global_step=75000, grad_norm=0.30440208315849304, loss=1.7587484121322632
I0306 07:41:50.102237 139637464966912 logging_writer.py:48] [75100] global_step=75100, grad_norm=0.28807333111763, loss=1.7235524654388428
I0306 07:42:25.386131 139637473359616 logging_writer.py:48] [75200] global_step=75200, grad_norm=0.270818829536438, loss=1.8105223178863525
I0306 07:43:00.666538 139637464966912 logging_writer.py:48] [75300] global_step=75300, grad_norm=0.266735315322876, loss=1.8461494445800781
I0306 07:43:35.911756 139637473359616 logging_writer.py:48] [75400] global_step=75400, grad_norm=0.3310135304927826, loss=1.7537963390350342
I0306 07:44:11.156056 139637464966912 logging_writer.py:48] [75500] global_step=75500, grad_norm=0.27621710300445557, loss=1.7473139762878418
I0306 07:44:46.416533 139637473359616 logging_writer.py:48] [75600] global_step=75600, grad_norm=0.28353631496429443, loss=1.7979024648666382
I0306 07:45:21.669867 139637464966912 logging_writer.py:48] [75700] global_step=75700, grad_norm=0.33427417278289795, loss=1.8491222858428955
I0306 07:45:56.922614 139637473359616 logging_writer.py:48] [75800] global_step=75800, grad_norm=0.3212162256240845, loss=1.7664806842803955
I0306 07:46:32.216779 139637464966912 logging_writer.py:48] [75900] global_step=75900, grad_norm=0.3394845128059387, loss=1.7216891050338745
I0306 07:47:07.444302 139637473359616 logging_writer.py:48] [76000] global_step=76000, grad_norm=0.36238646507263184, loss=1.7812507152557373
I0306 07:47:42.708048 139637464966912 logging_writer.py:48] [76100] global_step=76100, grad_norm=0.320217490196228, loss=1.685911774635315
I0306 07:48:17.954004 139637473359616 logging_writer.py:48] [76200] global_step=76200, grad_norm=0.35954099893569946, loss=1.79604172706604
I0306 07:48:53.193545 139637464966912 logging_writer.py:48] [76300] global_step=76300, grad_norm=0.29996204376220703, loss=1.7903602123260498
I0306 07:49:28.400132 139637473359616 logging_writer.py:48] [76400] global_step=76400, grad_norm=0.32472357153892517, loss=1.7547868490219116
I0306 07:49:54.139925 139780644897984 spec.py:321] Evaluating on the training split.
I0306 07:49:56.791423 139780644897984 workload.py:181] Translating evaluation dataset.
I0306 07:53:45.533519 139780644897984 spec.py:333] Evaluating on the validation split.
I0306 07:53:48.167926 139780644897984 workload.py:181] Translating evaluation dataset.
I0306 07:56:37.548612 139780644897984 spec.py:349] Evaluating on the test split.
I0306 07:56:40.192147 139780644897984 workload.py:181] Translating evaluation dataset.
I0306 07:59:44.488442 139780644897984 submission_runner.py:469] Time since start: 45975.75s, 	Step: 76474, 	{'train/accuracy': 0.6419686079025269, 'train/loss': 1.749300241470337, 'train/bleu': 31.50837936251988, 'validation/accuracy': 0.6599140167236328, 'validation/loss': 1.621609091758728, 'validation/bleu': 28.044321730944986, 'validation/num_examples': 3000, 'test/accuracy': 0.6724249720573425, 'test/loss': 1.549238920211792, 'test/bleu': 27.67812647667077, 'test/num_examples': 3003, 'score': 26906.741980314255, 'total_duration': 45975.75192594528, 'accumulated_submission_time': 26906.741980314255, 'accumulated_eval_time': 19063.819647789, 'accumulated_logging_time': 0.7711005210876465}
I0306 07:59:44.506518 139637464966912 logging_writer.py:48] [76474] accumulated_eval_time=19063.8, accumulated_logging_time=0.771101, accumulated_submission_time=26906.7, global_step=76474, preemption_count=0, score=26906.7, test/accuracy=0.672425, test/bleu=27.6781, test/loss=1.54924, test/num_examples=3003, total_duration=45975.8, train/accuracy=0.641969, train/bleu=31.5084, train/loss=1.7493, validation/accuracy=0.659914, validation/bleu=28.0443, validation/loss=1.62161, validation/num_examples=3000
I0306 07:59:54.010734 139637473359616 logging_writer.py:48] [76500] global_step=76500, grad_norm=0.28633373975753784, loss=1.7563120126724243
I0306 08:00:29.274378 139637464966912 logging_writer.py:48] [76600] global_step=76600, grad_norm=0.33286407589912415, loss=1.7395908832550049
I0306 08:01:04.542752 139637473359616 logging_writer.py:48] [76700] global_step=76700, grad_norm=0.31295034289360046, loss=1.7867841720581055
I0306 08:01:39.810516 139637464966912 logging_writer.py:48] [76800] global_step=76800, grad_norm=0.3137546479701996, loss=1.7682989835739136
I0306 08:02:15.097842 139637473359616 logging_writer.py:48] [76900] global_step=76900, grad_norm=0.33704912662506104, loss=1.7871241569519043
I0306 08:02:50.345768 139637464966912 logging_writer.py:48] [77000] global_step=77000, grad_norm=0.3160822093486786, loss=1.7324113845825195
I0306 08:03:25.622621 139637473359616 logging_writer.py:48] [77100] global_step=77100, grad_norm=0.3002893030643463, loss=1.6952050924301147
I0306 08:04:00.890808 139637464966912 logging_writer.py:48] [77200] global_step=77200, grad_norm=0.2950291037559509, loss=1.721789836883545
I0306 08:04:36.193125 139637473359616 logging_writer.py:48] [77300] global_step=77300, grad_norm=0.3581960201263428, loss=1.6592605113983154
I0306 08:05:11.485520 139637464966912 logging_writer.py:48] [77400] global_step=77400, grad_norm=0.36895909905433655, loss=1.8066715002059937
I0306 08:05:46.752110 139637473359616 logging_writer.py:48] [77500] global_step=77500, grad_norm=0.31903010606765747, loss=1.8395262956619263
I0306 08:06:22.025261 139637464966912 logging_writer.py:48] [77600] global_step=77600, grad_norm=0.32364872097969055, loss=1.7370513677597046
I0306 08:06:57.320771 139637473359616 logging_writer.py:48] [77700] global_step=77700, grad_norm=0.31439465284347534, loss=1.8182454109191895
I0306 08:07:32.606914 139637464966912 logging_writer.py:48] [77800] global_step=77800, grad_norm=0.37670332193374634, loss=1.6576123237609863
I0306 08:08:07.853217 139637473359616 logging_writer.py:48] [77900] global_step=77900, grad_norm=0.32083022594451904, loss=1.8888248205184937
I0306 08:08:43.106698 139637464966912 logging_writer.py:48] [78000] global_step=78000, grad_norm=0.2832961082458496, loss=1.7380462884902954
I0306 08:09:18.378199 139637473359616 logging_writer.py:48] [78100] global_step=78100, grad_norm=0.3180610239505768, loss=1.8355075120925903
I0306 08:09:53.652277 139637464966912 logging_writer.py:48] [78200] global_step=78200, grad_norm=0.3329731822013855, loss=1.8071050643920898
I0306 08:10:28.945804 139637473359616 logging_writer.py:48] [78300] global_step=78300, grad_norm=0.3154458999633789, loss=1.7639104127883911
I0306 08:11:04.242610 139637464966912 logging_writer.py:48] [78400] global_step=78400, grad_norm=0.2912980318069458, loss=1.7991665601730347
I0306 08:11:39.495879 139637473359616 logging_writer.py:48] [78500] global_step=78500, grad_norm=0.3049665093421936, loss=1.8091695308685303
I0306 08:12:14.775133 139637464966912 logging_writer.py:48] [78600] global_step=78600, grad_norm=0.3122885227203369, loss=1.8296113014221191
I0306 08:12:50.075047 139637473359616 logging_writer.py:48] [78700] global_step=78700, grad_norm=0.31936556100845337, loss=1.8213536739349365
I0306 08:13:25.340979 139637464966912 logging_writer.py:48] [78800] global_step=78800, grad_norm=0.39605656266212463, loss=1.7561756372451782
I0306 08:13:44.734216 139780644897984 spec.py:321] Evaluating on the training split.
I0306 08:13:47.387531 139780644897984 workload.py:181] Translating evaluation dataset.
I0306 08:16:54.200643 139780644897984 spec.py:333] Evaluating on the validation split.
I0306 08:16:56.843702 139780644897984 workload.py:181] Translating evaluation dataset.
I0306 08:19:51.740233 139780644897984 spec.py:349] Evaluating on the test split.
I0306 08:19:54.384791 139780644897984 workload.py:181] Translating evaluation dataset.
I0306 08:22:51.431232 139780644897984 submission_runner.py:469] Time since start: 47362.69s, 	Step: 78856, 	{'train/accuracy': 0.6438496112823486, 'train/loss': 1.74861741065979, 'train/bleu': 31.350223121957583, 'validation/accuracy': 0.662571370601654, 'validation/loss': 1.606427788734436, 'validation/bleu': 28.417334527765732, 'validation/num_examples': 3000, 'test/accuracy': 0.6764453649520874, 'test/loss': 1.5313351154327393, 'test/bleu': 28.10991007755014, 'test/num_examples': 3003, 'score': 27746.82996249199, 'total_duration': 47362.69472885132, 'accumulated_submission_time': 27746.82996249199, 'accumulated_eval_time': 19610.51661515236, 'accumulated_logging_time': 0.7983317375183105}
I0306 08:22:51.450450 139637473359616 logging_writer.py:48] [78856] accumulated_eval_time=19610.5, accumulated_logging_time=0.798332, accumulated_submission_time=27746.8, global_step=78856, preemption_count=0, score=27746.8, test/accuracy=0.676445, test/bleu=28.1099, test/loss=1.53134, test/num_examples=3003, total_duration=47362.7, train/accuracy=0.64385, train/bleu=31.3502, train/loss=1.74862, validation/accuracy=0.662571, validation/bleu=28.4173, validation/loss=1.60643, validation/num_examples=3000
I0306 08:23:07.280282 139637464966912 logging_writer.py:48] [78900] global_step=78900, grad_norm=0.3316231071949005, loss=1.6994941234588623
I0306 08:23:42.473457 139637473359616 logging_writer.py:48] [79000] global_step=79000, grad_norm=0.277116060256958, loss=1.7702480554580688
I0306 08:24:17.707518 139637464966912 logging_writer.py:48] [79100] global_step=79100, grad_norm=0.35217180848121643, loss=1.7300784587860107
I0306 08:24:52.972293 139637473359616 logging_writer.py:48] [79200] global_step=79200, grad_norm=0.29713234305381775, loss=1.7258354425430298
I0306 08:25:28.246649 139637464966912 logging_writer.py:48] [79300] global_step=79300, grad_norm=0.289887398481369, loss=1.7236860990524292
I0306 08:26:03.481520 139637473359616 logging_writer.py:48] [79400] global_step=79400, grad_norm=0.3113066256046295, loss=1.6660298109054565
I0306 08:26:38.735184 139637464966912 logging_writer.py:48] [79500] global_step=79500, grad_norm=0.29655811190605164, loss=1.741868495941162
I0306 08:27:14.012801 139637473359616 logging_writer.py:48] [79600] global_step=79600, grad_norm=0.31361058354377747, loss=1.7709314823150635
I0306 08:27:49.289091 139637464966912 logging_writer.py:48] [79700] global_step=79700, grad_norm=0.3450199067592621, loss=1.7111151218414307
I0306 08:28:24.611772 139637473359616 logging_writer.py:48] [79800] global_step=79800, grad_norm=0.2976779043674469, loss=1.7388885021209717
I0306 08:28:59.864058 139637464966912 logging_writer.py:48] [79900] global_step=79900, grad_norm=0.33428674936294556, loss=1.7539321184158325
I0306 08:29:35.108854 139637473359616 logging_writer.py:48] [80000] global_step=80000, grad_norm=0.2926543653011322, loss=1.7346550226211548
I0306 08:30:10.365219 139637464966912 logging_writer.py:48] [80100] global_step=80100, grad_norm=0.3213232755661011, loss=1.8426570892333984
I0306 08:30:45.623994 139637473359616 logging_writer.py:48] [80200] global_step=80200, grad_norm=0.29085075855255127, loss=1.7798629999160767
I0306 08:31:20.900370 139637464966912 logging_writer.py:48] [80300] global_step=80300, grad_norm=0.2883695960044861, loss=1.7517601251602173
I0306 08:31:56.112263 139637473359616 logging_writer.py:48] [80400] global_step=80400, grad_norm=0.3031536340713501, loss=1.805716872215271
I0306 08:32:31.337805 139637464966912 logging_writer.py:48] [80500] global_step=80500, grad_norm=0.3168661594390869, loss=1.7425614595413208
I0306 08:33:06.580453 139637473359616 logging_writer.py:48] [80600] global_step=80600, grad_norm=0.32040464878082275, loss=1.7056217193603516
I0306 08:33:41.839499 139637464966912 logging_writer.py:48] [80700] global_step=80700, grad_norm=0.30577352643013, loss=1.7428325414657593
I0306 08:34:17.092733 139637473359616 logging_writer.py:48] [80800] global_step=80800, grad_norm=0.3163232207298279, loss=1.7693036794662476
I0306 08:34:52.325015 139637464966912 logging_writer.py:48] [80900] global_step=80900, grad_norm=0.3033849895000458, loss=1.7604961395263672
I0306 08:35:27.540316 139637473359616 logging_writer.py:48] [81000] global_step=81000, grad_norm=0.30874595046043396, loss=1.7560142278671265
I0306 08:36:02.778435 139637464966912 logging_writer.py:48] [81100] global_step=81100, grad_norm=0.316153347492218, loss=1.7637516260147095
I0306 08:36:38.046214 139637473359616 logging_writer.py:48] [81200] global_step=81200, grad_norm=0.3344622254371643, loss=1.7820535898208618
I0306 08:36:51.438014 139780644897984 spec.py:321] Evaluating on the training split.
I0306 08:36:54.089067 139780644897984 workload.py:181] Translating evaluation dataset.
I0306 08:41:04.045042 139780644897984 spec.py:333] Evaluating on the validation split.
I0306 08:41:06.688201 139780644897984 workload.py:181] Translating evaluation dataset.
I0306 08:44:20.250993 139780644897984 spec.py:349] Evaluating on the test split.
I0306 08:44:22.893494 139780644897984 workload.py:181] Translating evaluation dataset.
I0306 08:47:17.762660 139780644897984 submission_runner.py:469] Time since start: 48829.03s, 	Step: 81239, 	{'train/accuracy': 0.64265376329422, 'train/loss': 1.7509026527404785, 'train/bleu': 32.220551993150394, 'validation/accuracy': 0.6659456491470337, 'validation/loss': 1.587671160697937, 'validation/bleu': 28.620837428903414, 'validation/num_examples': 3000, 'test/accuracy': 0.6775808334350586, 'test/loss': 1.5124707221984863, 'test/bleu': 28.293153752275455, 'test/num_examples': 3003, 'score': 28586.68051147461, 'total_duration': 48829.02612519264, 'accumulated_submission_time': 28586.68051147461, 'accumulated_eval_time': 20236.8411860466, 'accumulated_logging_time': 0.8261919021606445}
I0306 08:47:17.782012 139637464966912 logging_writer.py:48] [81239] accumulated_eval_time=20236.8, accumulated_logging_time=0.826192, accumulated_submission_time=28586.7, global_step=81239, preemption_count=0, score=28586.7, test/accuracy=0.677581, test/bleu=28.2932, test/loss=1.51247, test/num_examples=3003, total_duration=48829, train/accuracy=0.642654, train/bleu=32.2206, train/loss=1.7509, validation/accuracy=0.665946, validation/bleu=28.6208, validation/loss=1.58767, validation/num_examples=3000
I0306 08:47:39.560189 139637473359616 logging_writer.py:48] [81300] global_step=81300, grad_norm=0.35053324699401855, loss=1.6974530220031738
I0306 08:48:14.758391 139637464966912 logging_writer.py:48] [81400] global_step=81400, grad_norm=0.35620319843292236, loss=1.7122200727462769
I0306 08:48:50.025972 139637473359616 logging_writer.py:48] [81500] global_step=81500, grad_norm=0.30934450030326843, loss=1.6907708644866943
I0306 08:49:25.228627 139637464966912 logging_writer.py:48] [81600] global_step=81600, grad_norm=0.3024863004684448, loss=1.7663018703460693
I0306 08:50:00.397754 139637473359616 logging_writer.py:48] [81700] global_step=81700, grad_norm=0.29197055101394653, loss=1.7875730991363525
I0306 08:50:35.574276 139637464966912 logging_writer.py:48] [81800] global_step=81800, grad_norm=0.33018139004707336, loss=1.7681198120117188
I0306 08:51:10.745339 139637473359616 logging_writer.py:48] [81900] global_step=81900, grad_norm=0.33000680804252625, loss=1.7292845249176025
I0306 08:51:45.937937 139637464966912 logging_writer.py:48] [82000] global_step=82000, grad_norm=0.2908315360546112, loss=1.7079206705093384
I0306 08:52:21.122280 139637473359616 logging_writer.py:48] [82100] global_step=82100, grad_norm=0.26903825998306274, loss=1.6986713409423828
I0306 08:52:56.282703 139637464966912 logging_writer.py:48] [82200] global_step=82200, grad_norm=0.33042001724243164, loss=1.7453519105911255
I0306 08:53:31.455810 139637473359616 logging_writer.py:48] [82300] global_step=82300, grad_norm=0.32653453946113586, loss=1.6871817111968994
I0306 08:54:06.620176 139637464966912 logging_writer.py:48] [82400] global_step=82400, grad_norm=0.33149877190589905, loss=1.7682641744613647
I0306 08:54:41.766245 139637473359616 logging_writer.py:48] [82500] global_step=82500, grad_norm=0.32604384422302246, loss=1.7984213829040527
I0306 08:55:16.911793 139637464966912 logging_writer.py:48] [82600] global_step=82600, grad_norm=0.27086004614830017, loss=1.6759874820709229
I0306 08:55:52.038082 139637473359616 logging_writer.py:48] [82700] global_step=82700, grad_norm=0.2913667857646942, loss=1.6510567665100098
I0306 08:56:27.124598 139637464966912 logging_writer.py:48] [82800] global_step=82800, grad_norm=0.3208863139152527, loss=1.8608754873275757
I0306 08:57:02.263062 139637473359616 logging_writer.py:48] [82900] global_step=82900, grad_norm=0.2956148684024811, loss=1.7059606313705444
I0306 08:57:37.355643 139637464966912 logging_writer.py:48] [83000] global_step=83000, grad_norm=0.3217012882232666, loss=1.6611683368682861
I0306 08:58:12.490464 139637473359616 logging_writer.py:48] [83100] global_step=83100, grad_norm=0.27686119079589844, loss=1.6866912841796875
I0306 08:58:47.604789 139637464966912 logging_writer.py:48] [83200] global_step=83200, grad_norm=0.3473752737045288, loss=1.8775417804718018
I0306 08:59:22.702124 139637473359616 logging_writer.py:48] [83300] global_step=83300, grad_norm=0.3235141634941101, loss=1.747639536857605
I0306 08:59:57.772511 139637464966912 logging_writer.py:48] [83400] global_step=83400, grad_norm=0.30863869190216064, loss=1.8015844821929932
I0306 09:00:32.859824 139637473359616 logging_writer.py:48] [83500] global_step=83500, grad_norm=0.3006502389907837, loss=1.6843713521957397
I0306 09:01:07.902862 139637464966912 logging_writer.py:48] [83600] global_step=83600, grad_norm=0.3069467842578888, loss=1.7929999828338623
I0306 09:01:18.081270 139780644897984 spec.py:321] Evaluating on the training split.
I0306 09:01:20.734327 139780644897984 workload.py:181] Translating evaluation dataset.
I0306 09:05:38.659565 139780644897984 spec.py:333] Evaluating on the validation split.
I0306 09:05:41.296741 139780644897984 workload.py:181] Translating evaluation dataset.
I0306 09:08:53.326741 139780644897984 spec.py:349] Evaluating on the test split.
I0306 09:08:55.964502 139780644897984 workload.py:181] Translating evaluation dataset.
I0306 09:12:08.025484 139780644897984 submission_runner.py:469] Time since start: 50319.29s, 	Step: 83630, 	{'train/accuracy': 0.6515222787857056, 'train/loss': 1.6892231702804565, 'train/bleu': 32.33494239430436, 'validation/accuracy': 0.6673052906990051, 'validation/loss': 1.5686843395233154, 'validation/bleu': 28.880505117844393, 'validation/num_examples': 3000, 'test/accuracy': 0.6784497499465942, 'test/loss': 1.5015387535095215, 'test/bleu': 28.312973817042256, 'test/num_examples': 3003, 'score': 29426.83834552765, 'total_duration': 50319.288940906525, 'accumulated_submission_time': 29426.83834552765, 'accumulated_eval_time': 20886.785312891006, 'accumulated_logging_time': 0.853816032409668}
I0306 09:12:08.043689 139637473359616 logging_writer.py:48] [83630] accumulated_eval_time=20886.8, accumulated_logging_time=0.853816, accumulated_submission_time=29426.8, global_step=83630, preemption_count=0, score=29426.8, test/accuracy=0.67845, test/bleu=28.313, test/loss=1.50154, test/num_examples=3003, total_duration=50319.3, train/accuracy=0.651522, train/bleu=32.3349, train/loss=1.68922, validation/accuracy=0.667305, validation/bleu=28.8805, validation/loss=1.56868, validation/num_examples=3000
I0306 09:12:32.865355 139637464966912 logging_writer.py:48] [83700] global_step=83700, grad_norm=0.2936660349369049, loss=1.7624295949935913
I0306 09:13:07.831347 139637473359616 logging_writer.py:48] [83800] global_step=83800, grad_norm=0.35790273547172546, loss=1.6981797218322754
I0306 09:13:42.826274 139637464966912 logging_writer.py:48] [83900] global_step=83900, grad_norm=0.2951074540615082, loss=1.5838640928268433
I0306 09:14:17.875172 139637473359616 logging_writer.py:48] [84000] global_step=84000, grad_norm=0.30119842290878296, loss=1.7026230096817017
I0306 09:14:52.927388 139637464966912 logging_writer.py:48] [84100] global_step=84100, grad_norm=0.3395555913448334, loss=1.7999147176742554
I0306 09:15:27.961023 139637473359616 logging_writer.py:48] [84200] global_step=84200, grad_norm=0.31439489126205444, loss=1.721710205078125
I0306 09:16:03.038439 139637464966912 logging_writer.py:48] [84300] global_step=84300, grad_norm=0.3155198097229004, loss=1.7097018957138062
I0306 09:16:38.086549 139637473359616 logging_writer.py:48] [84400] global_step=84400, grad_norm=0.3272983431816101, loss=1.7580350637435913
I0306 09:17:13.155008 139637464966912 logging_writer.py:48] [84500] global_step=84500, grad_norm=0.31388840079307556, loss=1.6913899183273315
I0306 09:17:48.222193 139637473359616 logging_writer.py:48] [84600] global_step=84600, grad_norm=0.3058147430419922, loss=1.6995625495910645
I0306 09:18:23.312247 139637464966912 logging_writer.py:48] [84700] global_step=84700, grad_norm=0.30221280455589294, loss=1.7366819381713867
I0306 09:18:58.368504 139637473359616 logging_writer.py:48] [84800] global_step=84800, grad_norm=0.2810082733631134, loss=1.6246060132980347
I0306 09:19:33.441767 139637464966912 logging_writer.py:48] [84900] global_step=84900, grad_norm=0.31443339586257935, loss=1.620542049407959
I0306 09:20:08.528144 139637473359616 logging_writer.py:48] [85000] global_step=85000, grad_norm=0.3284042179584503, loss=1.6631202697753906
I0306 09:20:43.620104 139637464966912 logging_writer.py:48] [85100] global_step=85100, grad_norm=0.2998075485229492, loss=1.7489615678787231
I0306 09:21:18.689937 139637473359616 logging_writer.py:48] [85200] global_step=85200, grad_norm=0.3347249925136566, loss=1.686046838760376
I0306 09:21:53.803828 139637464966912 logging_writer.py:48] [85300] global_step=85300, grad_norm=0.2876591086387634, loss=1.6509088277816772
I0306 09:22:28.900144 139637473359616 logging_writer.py:48] [85400] global_step=85400, grad_norm=0.30552607774734497, loss=1.7024586200714111
I0306 09:23:03.995993 139637464966912 logging_writer.py:48] [85500] global_step=85500, grad_norm=0.3027753531932831, loss=1.7679861783981323
I0306 09:23:39.056446 139637473359616 logging_writer.py:48] [85600] global_step=85600, grad_norm=0.2896937131881714, loss=1.6169159412384033
I0306 09:24:14.124890 139637464966912 logging_writer.py:48] [85700] global_step=85700, grad_norm=0.3188759684562683, loss=1.7197786569595337
I0306 09:24:49.173590 139637473359616 logging_writer.py:48] [85800] global_step=85800, grad_norm=0.2793242037296295, loss=1.6673650741577148
I0306 09:25:24.223707 139637464966912 logging_writer.py:48] [85900] global_step=85900, grad_norm=0.30510592460632324, loss=1.671641230583191
I0306 09:25:59.296582 139637473359616 logging_writer.py:48] [86000] global_step=86000, grad_norm=0.32562872767448425, loss=1.7310194969177246
I0306 09:26:08.071360 139780644897984 spec.py:321] Evaluating on the training split.
I0306 09:26:10.715940 139780644897984 workload.py:181] Translating evaluation dataset.
I0306 09:30:11.074114 139780644897984 spec.py:333] Evaluating on the validation split.
I0306 09:30:13.715733 139780644897984 workload.py:181] Translating evaluation dataset.
I0306 09:33:07.691803 139780644897984 spec.py:349] Evaluating on the test split.
I0306 09:33:10.329546 139780644897984 workload.py:181] Translating evaluation dataset.
I0306 09:35:54.089468 139780644897984 submission_runner.py:469] Time since start: 51745.35s, 	Step: 86026, 	{'train/accuracy': 0.6517990231513977, 'train/loss': 1.7017759084701538, 'train/bleu': 31.434034272260902, 'validation/accuracy': 0.6699256300926208, 'validation/loss': 1.5585486888885498, 'validation/bleu': 29.043153992479454, 'validation/num_examples': 3000, 'test/accuracy': 0.6815896034240723, 'test/loss': 1.4844871759414673, 'test/bleu': 28.32085368603158, 'test/num_examples': 3003, 'score': 30266.723168373108, 'total_duration': 51745.35295557976, 'accumulated_submission_time': 30266.723168373108, 'accumulated_eval_time': 21472.80336213112, 'accumulated_logging_time': 0.8815252780914307}
I0306 09:35:54.107341 139637464966912 logging_writer.py:48] [86026] accumulated_eval_time=21472.8, accumulated_logging_time=0.881525, accumulated_submission_time=30266.7, global_step=86026, preemption_count=0, score=30266.7, test/accuracy=0.68159, test/bleu=28.3209, test/loss=1.48449, test/num_examples=3003, total_duration=51745.4, train/accuracy=0.651799, train/bleu=31.434, train/loss=1.70178, validation/accuracy=0.669926, validation/bleu=29.0432, validation/loss=1.55855, validation/num_examples=3000
I0306 09:36:20.328866 139637473359616 logging_writer.py:48] [86100] global_step=86100, grad_norm=0.3208920359611511, loss=1.6366033554077148
I0306 09:36:55.328689 139637464966912 logging_writer.py:48] [86200] global_step=86200, grad_norm=0.307937353849411, loss=1.7340177297592163
I0306 09:37:30.346065 139637473359616 logging_writer.py:48] [86300] global_step=86300, grad_norm=0.30352702736854553, loss=1.779203176498413
I0306 09:38:05.368166 139637464966912 logging_writer.py:48] [86400] global_step=86400, grad_norm=0.3035867512226105, loss=1.6881103515625
I0306 09:38:40.404868 139637473359616 logging_writer.py:48] [86500] global_step=86500, grad_norm=0.33171817660331726, loss=1.7429120540618896
I0306 09:39:15.426638 139637464966912 logging_writer.py:48] [86600] global_step=86600, grad_norm=0.30341780185699463, loss=1.7401360273361206
I0306 09:39:50.478612 139637473359616 logging_writer.py:48] [86700] global_step=86700, grad_norm=0.29611936211586, loss=1.7139426469802856
I0306 09:40:25.496384 139637464966912 logging_writer.py:48] [86800] global_step=86800, grad_norm=0.2951197922229767, loss=1.7713600397109985
I0306 09:41:00.526631 139637473359616 logging_writer.py:48] [86900] global_step=86900, grad_norm=0.29055869579315186, loss=1.5893570184707642
I0306 09:41:35.555358 139637464966912 logging_writer.py:48] [87000] global_step=87000, grad_norm=0.3219982385635376, loss=1.7400699853897095
I0306 09:42:10.592132 139637473359616 logging_writer.py:48] [87100] global_step=87100, grad_norm=0.29533788561820984, loss=1.6602298021316528
I0306 09:42:45.629434 139637464966912 logging_writer.py:48] [87200] global_step=87200, grad_norm=0.3143625855445862, loss=1.7693217992782593
I0306 09:43:20.655628 139637473359616 logging_writer.py:48] [87300] global_step=87300, grad_norm=0.3274163007736206, loss=1.6708661317825317
I0306 09:43:55.675597 139637464966912 logging_writer.py:48] [87400] global_step=87400, grad_norm=0.3043937087059021, loss=1.713078260421753
I0306 09:44:30.697541 139637473359616 logging_writer.py:48] [87500] global_step=87500, grad_norm=0.304608553647995, loss=1.684106707572937
I0306 09:45:05.748113 139637464966912 logging_writer.py:48] [87600] global_step=87600, grad_norm=0.32760223746299744, loss=1.6897404193878174
I0306 09:45:40.796177 139637473359616 logging_writer.py:48] [87700] global_step=87700, grad_norm=0.2887668013572693, loss=1.663327932357788
I0306 09:46:15.831058 139637464966912 logging_writer.py:48] [87800] global_step=87800, grad_norm=0.32606032490730286, loss=1.6835112571716309
I0306 09:46:50.941229 139637473359616 logging_writer.py:48] [87900] global_step=87900, grad_norm=0.30792298913002014, loss=1.6604360342025757
I0306 09:47:26.067609 139637464966912 logging_writer.py:48] [88000] global_step=88000, grad_norm=0.3143705129623413, loss=1.6799428462982178
I0306 09:48:01.192099 139637473359616 logging_writer.py:48] [88100] global_step=88100, grad_norm=0.3054571747779846, loss=1.6300771236419678
I0306 09:48:36.330810 139637464966912 logging_writer.py:48] [88200] global_step=88200, grad_norm=0.30567529797554016, loss=1.67123281955719
I0306 09:49:11.455480 139637473359616 logging_writer.py:48] [88300] global_step=88300, grad_norm=0.3243951201438904, loss=1.6978389024734497
I0306 09:49:46.601720 139637464966912 logging_writer.py:48] [88400] global_step=88400, grad_norm=0.3116413652896881, loss=1.7427541017532349
I0306 09:49:54.329066 139780644897984 spec.py:321] Evaluating on the training split.
I0306 09:49:56.970009 139780644897984 workload.py:181] Translating evaluation dataset.
I0306 09:53:27.804224 139780644897984 spec.py:333] Evaluating on the validation split.
I0306 09:53:30.447486 139780644897984 workload.py:181] Translating evaluation dataset.
I0306 09:56:08.491657 139780644897984 spec.py:349] Evaluating on the test split.
I0306 09:56:11.116662 139780644897984 workload.py:181] Translating evaluation dataset.
I0306 09:58:50.579746 139780644897984 submission_runner.py:469] Time since start: 53121.84s, 	Step: 88423, 	{'train/accuracy': 0.6646078824996948, 'train/loss': 1.6003464460372925, 'train/bleu': 32.95042963409473, 'validation/accuracy': 0.6726201176643372, 'validation/loss': 1.5404685735702515, 'validation/bleu': 29.19816262548214, 'validation/num_examples': 3000, 'test/accuracy': 0.6854246258735657, 'test/loss': 1.463981032371521, 'test/bleu': 28.672310872292798, 'test/num_examples': 3003, 'score': 31106.80856180191, 'total_duration': 53121.84323501587, 'accumulated_submission_time': 31106.80856180191, 'accumulated_eval_time': 22009.053982019424, 'accumulated_logging_time': 0.9083998203277588}
I0306 09:58:50.598309 139637473359616 logging_writer.py:48] [88423] accumulated_eval_time=22009.1, accumulated_logging_time=0.9084, accumulated_submission_time=31106.8, global_step=88423, preemption_count=0, score=31106.8, test/accuracy=0.685425, test/bleu=28.6723, test/loss=1.46398, test/num_examples=3003, total_duration=53121.8, train/accuracy=0.664608, train/bleu=32.9504, train/loss=1.60035, validation/accuracy=0.67262, validation/bleu=29.1982, validation/loss=1.54047, validation/num_examples=3000
I0306 09:59:17.978586 139637464966912 logging_writer.py:48] [88500] global_step=88500, grad_norm=0.3144543766975403, loss=1.683784008026123
I0306 09:59:53.090706 139637473359616 logging_writer.py:48] [88600] global_step=88600, grad_norm=0.285495787858963, loss=1.7605400085449219
I0306 10:00:28.192528 139637464966912 logging_writer.py:48] [88700] global_step=88700, grad_norm=0.3151578903198242, loss=1.6812056303024292
I0306 10:01:03.317234 139637473359616 logging_writer.py:48] [88800] global_step=88800, grad_norm=0.3186076879501343, loss=1.762207269668579
I0306 10:01:38.459007 139637464966912 logging_writer.py:48] [88900] global_step=88900, grad_norm=0.413211464881897, loss=1.72674560546875
I0306 10:02:13.631625 139637473359616 logging_writer.py:48] [89000] global_step=89000, grad_norm=0.29545772075653076, loss=1.6416693925857544
I0306 10:02:48.768985 139637464966912 logging_writer.py:48] [89100] global_step=89100, grad_norm=0.36794814467430115, loss=1.7027699947357178
I0306 10:03:23.947308 139637473359616 logging_writer.py:48] [89200] global_step=89200, grad_norm=0.3129308521747589, loss=1.7028578519821167
I0306 10:03:59.119909 139637464966912 logging_writer.py:48] [89300] global_step=89300, grad_norm=0.3228362798690796, loss=1.7015810012817383
I0306 10:04:34.241544 139637473359616 logging_writer.py:48] [89400] global_step=89400, grad_norm=0.31716564297676086, loss=1.6250755786895752
I0306 10:05:09.355916 139637464966912 logging_writer.py:48] [89500] global_step=89500, grad_norm=0.3287433087825775, loss=1.6762129068374634
I0306 10:05:44.475835 139637473359616 logging_writer.py:48] [89600] global_step=89600, grad_norm=0.3225601613521576, loss=1.689025640487671
I0306 10:06:19.661232 139637464966912 logging_writer.py:48] [89700] global_step=89700, grad_norm=0.3219833970069885, loss=1.6134120225906372
I0306 10:06:54.803417 139637473359616 logging_writer.py:48] [89800] global_step=89800, grad_norm=0.3231542706489563, loss=1.6307682991027832
I0306 10:07:29.893320 139637464966912 logging_writer.py:48] [89900] global_step=89900, grad_norm=0.3183753788471222, loss=1.666337251663208
I0306 10:08:05.000921 139637473359616 logging_writer.py:48] [90000] global_step=90000, grad_norm=0.315976619720459, loss=1.7065602540969849
I0306 10:08:40.108811 139637464966912 logging_writer.py:48] [90100] global_step=90100, grad_norm=0.3101012110710144, loss=1.69315767288208
I0306 10:09:15.233218 139637473359616 logging_writer.py:48] [90200] global_step=90200, grad_norm=0.3121047616004944, loss=1.7158159017562866
I0306 10:09:50.348605 139637464966912 logging_writer.py:48] [90300] global_step=90300, grad_norm=0.3293309211730957, loss=1.668148159980774
I0306 10:10:25.452311 139637473359616 logging_writer.py:48] [90400] global_step=90400, grad_norm=0.3164371848106384, loss=1.6887208223342896
I0306 10:11:00.570351 139637464966912 logging_writer.py:48] [90500] global_step=90500, grad_norm=0.326394259929657, loss=1.6603144407272339
I0306 10:11:35.693359 139637473359616 logging_writer.py:48] [90600] global_step=90600, grad_norm=0.30705684423446655, loss=1.6299916505813599
I0306 10:12:10.811336 139637464966912 logging_writer.py:48] [90700] global_step=90700, grad_norm=0.33100229501724243, loss=1.7153499126434326
I0306 10:12:45.960796 139637473359616 logging_writer.py:48] [90800] global_step=90800, grad_norm=0.30267250537872314, loss=1.6639893054962158
I0306 10:12:50.886538 139780644897984 spec.py:321] Evaluating on the training split.
I0306 10:12:53.540443 139780644897984 workload.py:181] Translating evaluation dataset.
I0306 10:17:40.906506 139780644897984 spec.py:333] Evaluating on the validation split.
I0306 10:17:43.540545 139780644897984 workload.py:181] Translating evaluation dataset.
I0306 10:20:43.942877 139780644897984 spec.py:349] Evaluating on the test split.
I0306 10:20:46.581265 139780644897984 workload.py:181] Translating evaluation dataset.
I0306 10:23:52.597340 139780644897984 submission_runner.py:469] Time since start: 54623.86s, 	Step: 90815, 	{'train/accuracy': 0.6605445742607117, 'train/loss': 1.6354273557662964, 'train/bleu': 32.13151031115447, 'validation/accuracy': 0.6734234690666199, 'validation/loss': 1.5262573957443237, 'validation/bleu': 29.212576086568806, 'validation/num_examples': 3000, 'test/accuracy': 0.6882516741752625, 'test/loss': 1.4436302185058594, 'test/bleu': 28.73777890130112, 'test/num_examples': 3003, 'score': 31946.95850801468, 'total_duration': 54623.8608379364, 'accumulated_submission_time': 31946.95850801468, 'accumulated_eval_time': 22670.76473593712, 'accumulated_logging_time': 0.935175895690918}
I0306 10:23:52.616394 139637464966912 logging_writer.py:48] [90815] accumulated_eval_time=22670.8, accumulated_logging_time=0.935176, accumulated_submission_time=31947, global_step=90815, preemption_count=0, score=31947, test/accuracy=0.688252, test/bleu=28.7378, test/loss=1.44363, test/num_examples=3003, total_duration=54623.9, train/accuracy=0.660545, train/bleu=32.1315, train/loss=1.63543, validation/accuracy=0.673423, validation/bleu=29.2126, validation/loss=1.52626, validation/num_examples=3000
I0306 10:24:22.777675 139637473359616 logging_writer.py:48] [90900] global_step=90900, grad_norm=0.33507511019706726, loss=1.682304859161377
I0306 10:24:57.879027 139637464966912 logging_writer.py:48] [91000] global_step=91000, grad_norm=0.31264737248420715, loss=1.6506166458129883
I0306 10:25:33.029705 139637473359616 logging_writer.py:48] [91100] global_step=91100, grad_norm=0.3414159417152405, loss=1.646101713180542
I0306 10:26:08.176980 139637464966912 logging_writer.py:48] [91200] global_step=91200, grad_norm=0.32735005021095276, loss=1.5718878507614136
I0306 10:26:43.304538 139637473359616 logging_writer.py:48] [91300] global_step=91300, grad_norm=0.3094898462295532, loss=1.653983235359192
I0306 10:27:18.475874 139637464966912 logging_writer.py:48] [91400] global_step=91400, grad_norm=0.30929940938949585, loss=1.6437214612960815
I0306 10:27:53.613996 139637473359616 logging_writer.py:48] [91500] global_step=91500, grad_norm=0.3081192076206207, loss=1.68710196018219
I0306 10:28:28.761466 139637464966912 logging_writer.py:48] [91600] global_step=91600, grad_norm=0.32363253831863403, loss=1.6547586917877197
I0306 10:29:03.894526 139637473359616 logging_writer.py:48] [91700] global_step=91700, grad_norm=0.3171171247959137, loss=1.6238127946853638
I0306 10:29:39.038936 139637464966912 logging_writer.py:48] [91800] global_step=91800, grad_norm=0.33568862080574036, loss=1.6606887578964233
I0306 10:30:14.204450 139637473359616 logging_writer.py:48] [91900] global_step=91900, grad_norm=0.320182740688324, loss=1.6758322715759277
I0306 10:30:49.345844 139637464966912 logging_writer.py:48] [92000] global_step=92000, grad_norm=0.33088061213493347, loss=1.6871806383132935
I0306 10:31:24.471572 139637473359616 logging_writer.py:48] [92100] global_step=92100, grad_norm=0.3303625285625458, loss=1.6238867044448853
I0306 10:31:59.640952 139637464966912 logging_writer.py:48] [92200] global_step=92200, grad_norm=0.31043797731399536, loss=1.6732306480407715
I0306 10:32:34.791821 139637473359616 logging_writer.py:48] [92300] global_step=92300, grad_norm=0.3154309391975403, loss=1.65225088596344
I0306 10:33:09.939755 139637464966912 logging_writer.py:48] [92400] global_step=92400, grad_norm=0.3078944683074951, loss=1.6520841121673584
I0306 10:33:45.044543 139637473359616 logging_writer.py:48] [92500] global_step=92500, grad_norm=0.3210426867008209, loss=1.704113483428955
I0306 10:34:20.148261 139637464966912 logging_writer.py:48] [92600] global_step=92600, grad_norm=0.2939372658729553, loss=1.6544753313064575
I0306 10:34:55.223982 139637473359616 logging_writer.py:48] [92700] global_step=92700, grad_norm=0.33012500405311584, loss=1.6578658819198608
I0306 10:35:30.335223 139637464966912 logging_writer.py:48] [92800] global_step=92800, grad_norm=0.3339517116546631, loss=1.5389865636825562
I0306 10:36:05.424948 139637473359616 logging_writer.py:48] [92900] global_step=92900, grad_norm=0.3174414038658142, loss=1.6131234169006348
I0306 10:36:40.543383 139637464966912 logging_writer.py:48] [93000] global_step=93000, grad_norm=0.337164968252182, loss=1.6573888063430786
I0306 10:37:15.666700 139637473359616 logging_writer.py:48] [93100] global_step=93100, grad_norm=0.2973572015762329, loss=1.557560920715332
I0306 10:37:50.799624 139637464966912 logging_writer.py:48] [93200] global_step=93200, grad_norm=0.331730455160141, loss=1.6388514041900635
I0306 10:37:52.916672 139780644897984 spec.py:321] Evaluating on the training split.
I0306 10:37:55.555956 139780644897984 workload.py:181] Translating evaluation dataset.
I0306 10:41:52.453795 139780644897984 spec.py:333] Evaluating on the validation split.
I0306 10:41:55.086289 139780644897984 workload.py:181] Translating evaluation dataset.
I0306 10:44:49.410608 139780644897984 spec.py:349] Evaluating on the test split.
I0306 10:44:52.047162 139780644897984 workload.py:181] Translating evaluation dataset.
I0306 10:47:54.281156 139780644897984 submission_runner.py:469] Time since start: 56065.54s, 	Step: 93207, 	{'train/accuracy': 0.6573771834373474, 'train/loss': 1.6538136005401611, 'train/bleu': 32.298785871225796, 'validation/accuracy': 0.67620450258255, 'validation/loss': 1.511754035949707, 'validation/bleu': 29.471954908641102, 'validation/num_examples': 3000, 'test/accuracy': 0.6913799047470093, 'test/loss': 1.4219073057174683, 'test/bleu': 29.399890740732395, 'test/num_examples': 3003, 'score': 32787.12175369263, 'total_duration': 56065.54461526871, 'accumulated_submission_time': 32787.12175369263, 'accumulated_eval_time': 23272.129132270813, 'accumulated_logging_time': 0.9633364677429199}
I0306 10:47:54.300572 139637473359616 logging_writer.py:48] [93207] accumulated_eval_time=23272.1, accumulated_logging_time=0.963336, accumulated_submission_time=32787.1, global_step=93207, preemption_count=0, score=32787.1, test/accuracy=0.69138, test/bleu=29.3999, test/loss=1.42191, test/num_examples=3003, total_duration=56065.5, train/accuracy=0.657377, train/bleu=32.2988, train/loss=1.65381, validation/accuracy=0.676205, validation/bleu=29.472, validation/loss=1.51175, validation/num_examples=3000
I0306 10:48:27.199938 139637464966912 logging_writer.py:48] [93300] global_step=93300, grad_norm=0.324966162443161, loss=1.585965633392334
I0306 10:49:02.243915 139637473359616 logging_writer.py:48] [93400] global_step=93400, grad_norm=0.3185328543186188, loss=1.6433851718902588
I0306 10:49:37.310791 139637464966912 logging_writer.py:48] [93500] global_step=93500, grad_norm=0.333676815032959, loss=1.644354224205017
I0306 10:50:12.393912 139637473359616 logging_writer.py:48] [93600] global_step=93600, grad_norm=0.3372117877006531, loss=1.6944811344146729
I0306 10:50:47.503826 139637464966912 logging_writer.py:48] [93700] global_step=93700, grad_norm=0.31623774766921997, loss=1.6367969512939453
I0306 10:51:22.614027 139637473359616 logging_writer.py:48] [93800] global_step=93800, grad_norm=0.3127184212207794, loss=1.6659976243972778
I0306 10:51:57.740019 139637464966912 logging_writer.py:48] [93900] global_step=93900, grad_norm=0.3424457609653473, loss=1.6511313915252686
I0306 10:52:32.894684 139637473359616 logging_writer.py:48] [94000] global_step=94000, grad_norm=0.3308573365211487, loss=1.6975449323654175
I0306 10:53:08.009052 139637464966912 logging_writer.py:48] [94100] global_step=94100, grad_norm=0.3111594617366791, loss=1.6457122564315796
I0306 10:53:43.097290 139637473359616 logging_writer.py:48] [94200] global_step=94200, grad_norm=0.3382745385169983, loss=1.627313256263733
I0306 10:54:18.204632 139637464966912 logging_writer.py:48] [94300] global_step=94300, grad_norm=0.34265998005867004, loss=1.68097984790802
I0306 10:54:53.298789 139637473359616 logging_writer.py:48] [94400] global_step=94400, grad_norm=0.3024755120277405, loss=1.6820383071899414
I0306 10:55:28.398187 139637464966912 logging_writer.py:48] [94500] global_step=94500, grad_norm=0.3406921625137329, loss=1.6155810356140137
I0306 10:56:03.529300 139637473359616 logging_writer.py:48] [94600] global_step=94600, grad_norm=0.3056083917617798, loss=1.6542390584945679
I0306 10:56:38.664281 139637464966912 logging_writer.py:48] [94700] global_step=94700, grad_norm=0.32249483466148376, loss=1.601185917854309
I0306 10:57:13.776816 139637473359616 logging_writer.py:48] [94800] global_step=94800, grad_norm=0.3019132912158966, loss=1.567685604095459
I0306 10:57:48.910308 139637464966912 logging_writer.py:48] [94900] global_step=94900, grad_norm=0.33723199367523193, loss=1.6046603918075562
I0306 10:58:24.048449 139637473359616 logging_writer.py:48] [95000] global_step=95000, grad_norm=0.3278540074825287, loss=1.631976842880249
I0306 10:58:59.195571 139637464966912 logging_writer.py:48] [95100] global_step=95100, grad_norm=0.3451336920261383, loss=1.6837018728256226
I0306 10:59:34.316159 139637473359616 logging_writer.py:48] [95200] global_step=95200, grad_norm=0.31772440671920776, loss=1.5538986921310425
I0306 11:00:09.423743 139637464966912 logging_writer.py:48] [95300] global_step=95300, grad_norm=0.3218415677547455, loss=1.5628533363342285
I0306 11:00:44.535059 139637473359616 logging_writer.py:48] [95400] global_step=95400, grad_norm=0.2999279201030731, loss=1.5717800855636597
I0306 11:01:19.671805 139637464966912 logging_writer.py:48] [95500] global_step=95500, grad_norm=0.3304165303707123, loss=1.6624647378921509
I0306 11:01:54.425914 139780644897984 spec.py:321] Evaluating on the training split.
I0306 11:01:57.072396 139780644897984 workload.py:181] Translating evaluation dataset.
I0306 11:05:52.538833 139780644897984 spec.py:333] Evaluating on the validation split.
I0306 11:05:55.184781 139780644897984 workload.py:181] Translating evaluation dataset.
I0306 11:09:14.567161 139780644897984 spec.py:349] Evaluating on the test split.
I0306 11:09:17.212504 139780644897984 workload.py:181] Translating evaluation dataset.
I0306 11:12:33.547430 139780644897984 submission_runner.py:469] Time since start: 57544.81s, 	Step: 95600, 	{'train/accuracy': 0.6688386797904968, 'train/loss': 1.5782347917556763, 'train/bleu': 33.27323988757846, 'validation/accuracy': 0.6803698539733887, 'validation/loss': 1.4902026653289795, 'validation/bleu': 29.742770265849412, 'validation/num_examples': 3000, 'test/accuracy': 0.6924574375152588, 'test/loss': 1.4117414951324463, 'test/bleu': 29.368151843750923, 'test/num_examples': 3003, 'score': 33627.111221790314, 'total_duration': 57544.81092381477, 'accumulated_submission_time': 33627.111221790314, 'accumulated_eval_time': 23911.250597715378, 'accumulated_logging_time': 0.9911746978759766}
I0306 11:12:33.566760 139637473359616 logging_writer.py:48] [95600] accumulated_eval_time=23911.3, accumulated_logging_time=0.991175, accumulated_submission_time=33627.1, global_step=95600, preemption_count=0, score=33627.1, test/accuracy=0.692457, test/bleu=29.3682, test/loss=1.41174, test/num_examples=3003, total_duration=57544.8, train/accuracy=0.668839, train/bleu=33.2732, train/loss=1.57823, validation/accuracy=0.68037, validation/bleu=29.7428, validation/loss=1.4902, validation/num_examples=3000
I0306 11:12:33.924443 139637464966912 logging_writer.py:48] [95600] global_step=95600, grad_norm=0.3246693015098572, loss=1.6035007238388062
I0306 11:13:08.969071 139637473359616 logging_writer.py:48] [95700] global_step=95700, grad_norm=0.33398622274398804, loss=1.533881664276123
I0306 11:13:44.036108 139637464966912 logging_writer.py:48] [95800] global_step=95800, grad_norm=0.34301939606666565, loss=1.4918016195297241
I0306 11:14:19.100440 139637473359616 logging_writer.py:48] [95900] global_step=95900, grad_norm=0.31215015053749084, loss=1.658409595489502
I0306 11:14:54.214127 139637464966912 logging_writer.py:48] [96000] global_step=96000, grad_norm=0.31179773807525635, loss=1.6264567375183105
I0306 11:15:29.346289 139637473359616 logging_writer.py:48] [96100] global_step=96100, grad_norm=0.3298560082912445, loss=1.552639126777649
I0306 11:16:04.479905 139637464966912 logging_writer.py:48] [96200] global_step=96200, grad_norm=0.3410084545612335, loss=1.6070911884307861
I0306 11:16:39.604671 139637473359616 logging_writer.py:48] [96300] global_step=96300, grad_norm=0.3220435678958893, loss=1.540725827217102
I0306 11:17:14.765628 139637464966912 logging_writer.py:48] [96400] global_step=96400, grad_norm=0.3044295608997345, loss=1.6115885972976685
I0306 11:17:49.913970 139637473359616 logging_writer.py:48] [96500] global_step=96500, grad_norm=0.33980852365493774, loss=1.6132642030715942
I0306 11:18:25.027314 139637464966912 logging_writer.py:48] [96600] global_step=96600, grad_norm=0.33856281638145447, loss=1.5823701620101929
I0306 11:19:00.158151 139637473359616 logging_writer.py:48] [96700] global_step=96700, grad_norm=0.34706708788871765, loss=1.5864404439926147
I0306 11:19:35.300308 139637464966912 logging_writer.py:48] [96800] global_step=96800, grad_norm=0.30756425857543945, loss=1.5342167615890503
I0306 11:20:10.478550 139637473359616 logging_writer.py:48] [96900] global_step=96900, grad_norm=0.3392448425292969, loss=1.6025006771087646
I0306 11:20:45.636859 139637464966912 logging_writer.py:48] [97000] global_step=97000, grad_norm=0.3397209346294403, loss=1.6488409042358398
I0306 11:21:20.798533 139637473359616 logging_writer.py:48] [97100] global_step=97100, grad_norm=0.3166595995426178, loss=1.5716800689697266
I0306 11:21:55.932693 139637464966912 logging_writer.py:48] [97200] global_step=97200, grad_norm=0.3466705083847046, loss=1.6888705492019653
I0306 11:22:31.090218 139637473359616 logging_writer.py:48] [97300] global_step=97300, grad_norm=0.32293030619621277, loss=1.6174776554107666
I0306 11:23:06.277975 139637464966912 logging_writer.py:48] [97400] global_step=97400, grad_norm=0.3185570538043976, loss=1.5907654762268066
I0306 11:23:41.403841 139637473359616 logging_writer.py:48] [97500] global_step=97500, grad_norm=0.3311324417591095, loss=1.6183652877807617
I0306 11:24:16.564109 139637464966912 logging_writer.py:48] [97600] global_step=97600, grad_norm=0.306027889251709, loss=1.55094313621521
I0306 11:24:51.703144 139637473359616 logging_writer.py:48] [97700] global_step=97700, grad_norm=0.3360949754714966, loss=1.5365016460418701
I0306 11:25:26.832505 139637464966912 logging_writer.py:48] [97800] global_step=97800, grad_norm=0.34048202633857727, loss=1.5002304315567017
I0306 11:26:01.960295 139637473359616 logging_writer.py:48] [97900] global_step=97900, grad_norm=0.3135666847229004, loss=1.529258370399475
I0306 11:26:33.587733 139780644897984 spec.py:321] Evaluating on the training split.
I0306 11:26:36.236526 139780644897984 workload.py:181] Translating evaluation dataset.
I0306 11:30:46.411295 139780644897984 spec.py:333] Evaluating on the validation split.
I0306 11:30:49.045529 139780644897984 workload.py:181] Translating evaluation dataset.
I0306 11:33:30.206139 139780644897984 spec.py:349] Evaluating on the test split.
I0306 11:33:32.845476 139780644897984 workload.py:181] Translating evaluation dataset.
I0306 11:36:20.552175 139780644897984 submission_runner.py:469] Time since start: 58971.82s, 	Step: 97991, 	{'train/accuracy': 0.6652668118476868, 'train/loss': 1.6025745868682861, 'train/bleu': 32.74790467887688, 'validation/accuracy': 0.6798754334449768, 'validation/loss': 1.4846218824386597, 'validation/bleu': 29.92811404282832, 'validation/num_examples': 3000, 'test/accuracy': 0.696454644203186, 'test/loss': 1.395054817199707, 'test/bleu': 29.669121398385812, 'test/num_examples': 3003, 'score': 34466.99495434761, 'total_duration': 58971.81566500664, 'accumulated_submission_time': 34466.99495434761, 'accumulated_eval_time': 24498.21498775482, 'accumulated_logging_time': 1.0186359882354736}
I0306 11:36:20.571285 139637464966912 logging_writer.py:48] [97991] accumulated_eval_time=24498.2, accumulated_logging_time=1.01864, accumulated_submission_time=34467, global_step=97991, preemption_count=0, score=34467, test/accuracy=0.696455, test/bleu=29.6691, test/loss=1.39505, test/num_examples=3003, total_duration=58971.8, train/accuracy=0.665267, train/bleu=32.7479, train/loss=1.60257, validation/accuracy=0.679875, validation/bleu=29.9281, validation/loss=1.48462, validation/num_examples=3000
I0306 11:36:24.082939 139637473359616 logging_writer.py:48] [98000] global_step=98000, grad_norm=0.33916836977005005, loss=1.5842498540878296
I0306 11:36:59.106184 139637464966912 logging_writer.py:48] [98100] global_step=98100, grad_norm=0.3311736583709717, loss=1.5296400785446167
I0306 11:37:34.140126 139637473359616 logging_writer.py:48] [98200] global_step=98200, grad_norm=0.31877362728118896, loss=1.5670443773269653
I0306 11:38:09.192863 139637464966912 logging_writer.py:48] [98300] global_step=98300, grad_norm=0.310129851102829, loss=1.5837161540985107
I0306 11:38:44.298515 139637473359616 logging_writer.py:48] [98400] global_step=98400, grad_norm=0.3417816758155823, loss=1.6155095100402832
I0306 11:39:19.416440 139637464966912 logging_writer.py:48] [98500] global_step=98500, grad_norm=0.3318498730659485, loss=1.5943372249603271
I0306 11:39:54.524912 139637473359616 logging_writer.py:48] [98600] global_step=98600, grad_norm=0.3434276878833771, loss=1.5099306106567383
I0306 11:40:29.647832 139637464966912 logging_writer.py:48] [98700] global_step=98700, grad_norm=0.30649200081825256, loss=1.5430688858032227
I0306 11:41:04.775429 139637473359616 logging_writer.py:48] [98800] global_step=98800, grad_norm=0.3245234489440918, loss=1.541115641593933
I0306 11:41:39.920784 139637464966912 logging_writer.py:48] [98900] global_step=98900, grad_norm=0.33949434757232666, loss=1.5153980255126953
I0306 11:42:15.044489 139637473359616 logging_writer.py:48] [99000] global_step=99000, grad_norm=0.32728973031044006, loss=1.5989047288894653
I0306 11:42:50.222875 139637464966912 logging_writer.py:48] [99100] global_step=99100, grad_norm=0.3098021149635315, loss=1.4640697240829468
I0306 11:43:25.340997 139637473359616 logging_writer.py:48] [99200] global_step=99200, grad_norm=0.3298141658306122, loss=1.5003470182418823
I0306 11:44:00.490295 139637464966912 logging_writer.py:48] [99300] global_step=99300, grad_norm=0.36089855432510376, loss=1.5771193504333496
I0306 11:44:35.628995 139637473359616 logging_writer.py:48] [99400] global_step=99400, grad_norm=0.3241100013256073, loss=1.578895926475525
I0306 11:45:10.771331 139637464966912 logging_writer.py:48] [99500] global_step=99500, grad_norm=0.3368859589099884, loss=1.5481353998184204
I0306 11:45:45.892652 139637473359616 logging_writer.py:48] [99600] global_step=99600, grad_norm=0.3134450316429138, loss=1.5778255462646484
I0306 11:46:21.042920 139637464966912 logging_writer.py:48] [99700] global_step=99700, grad_norm=0.40731868147850037, loss=1.650758981704712
I0306 11:46:56.198878 139637473359616 logging_writer.py:48] [99800] global_step=99800, grad_norm=0.3368760645389557, loss=1.5282565355300903
I0306 11:47:31.355359 139637464966912 logging_writer.py:48] [99900] global_step=99900, grad_norm=0.3591240346431732, loss=1.560167908668518
I0306 11:48:06.494993 139637473359616 logging_writer.py:48] [100000] global_step=100000, grad_norm=0.32266151905059814, loss=1.5320531129837036
I0306 11:48:41.635426 139637464966912 logging_writer.py:48] [100100] global_step=100100, grad_norm=0.3540569543838501, loss=1.5943650007247925
I0306 11:49:16.792899 139637473359616 logging_writer.py:48] [100200] global_step=100200, grad_norm=0.33602455258369446, loss=1.5367125272750854
I0306 11:49:51.907026 139637464966912 logging_writer.py:48] [100300] global_step=100300, grad_norm=0.3688771426677704, loss=1.5655916929244995
I0306 11:50:20.748745 139780644897984 spec.py:321] Evaluating on the training split.
I0306 11:50:23.402500 139780644897984 workload.py:181] Translating evaluation dataset.
I0306 11:54:02.688287 139780644897984 spec.py:333] Evaluating on the validation split.
I0306 11:54:05.324943 139780644897984 workload.py:181] Translating evaluation dataset.
I0306 11:56:52.100234 139780644897984 spec.py:349] Evaluating on the test split.
I0306 11:56:54.737668 139780644897984 workload.py:181] Translating evaluation dataset.
I0306 11:59:29.299715 139780644897984 submission_runner.py:469] Time since start: 60360.56s, 	Step: 100383, 	{'train/accuracy': 0.7098280191421509, 'train/loss': 1.3569326400756836, 'train/bleu': 36.33429913693139, 'validation/accuracy': 0.6833362579345703, 'validation/loss': 1.4667052030563354, 'validation/bleu': 29.852403005562365, 'validation/num_examples': 3000, 'test/accuracy': 0.6978797316551208, 'test/loss': 1.3814998865127563, 'test/bleu': 30.161131676454566, 'test/num_examples': 3003, 'score': 35307.034719944, 'total_duration': 60360.56320953369, 'accumulated_submission_time': 35307.034719944, 'accumulated_eval_time': 25046.765909910202, 'accumulated_logging_time': 1.0459561347961426}
I0306 11:59:29.319696 139637473359616 logging_writer.py:48] [100383] accumulated_eval_time=25046.8, accumulated_logging_time=1.04596, accumulated_submission_time=35307, global_step=100383, preemption_count=0, score=35307, test/accuracy=0.69788, test/bleu=30.1611, test/loss=1.3815, test/num_examples=3003, total_duration=60360.6, train/accuracy=0.709828, train/bleu=36.3343, train/loss=1.35693, validation/accuracy=0.683336, validation/bleu=29.8524, validation/loss=1.46671, validation/num_examples=3000
I0306 11:59:35.643138 139637464966912 logging_writer.py:48] [100400] global_step=100400, grad_norm=0.3574560582637787, loss=1.5869790315628052
I0306 12:00:10.734967 139637473359616 logging_writer.py:48] [100500] global_step=100500, grad_norm=0.3285481631755829, loss=1.5605732202529907
I0306 12:00:45.868107 139637464966912 logging_writer.py:48] [100600] global_step=100600, grad_norm=0.3457406163215637, loss=1.5560532808303833
I0306 12:01:21.051005 139637473359616 logging_writer.py:48] [100700] global_step=100700, grad_norm=0.32660624384880066, loss=1.457706093788147
I0306 12:01:56.191470 139637464966912 logging_writer.py:48] [100800] global_step=100800, grad_norm=0.34632718563079834, loss=1.568985104560852
I0306 12:02:31.337963 139637473359616 logging_writer.py:48] [100900] global_step=100900, grad_norm=0.33437007665634155, loss=1.6310805082321167
I0306 12:03:06.491270 139637464966912 logging_writer.py:48] [101000] global_step=101000, grad_norm=0.3482254445552826, loss=1.5408954620361328
I0306 12:03:41.694054 139637473359616 logging_writer.py:48] [101100] global_step=101100, grad_norm=0.3329678475856781, loss=1.5110361576080322
I0306 12:04:16.854079 139637464966912 logging_writer.py:48] [101200] global_step=101200, grad_norm=0.37328648567199707, loss=1.5301867723464966
I0306 12:04:52.011739 139637473359616 logging_writer.py:48] [101300] global_step=101300, grad_norm=0.3383672833442688, loss=1.4895522594451904
I0306 12:05:27.180977 139637464966912 logging_writer.py:48] [101400] global_step=101400, grad_norm=0.3491809070110321, loss=1.5936826467514038
I0306 12:06:02.331278 139637473359616 logging_writer.py:48] [101500] global_step=101500, grad_norm=0.35334107279777527, loss=1.4963451623916626
I0306 12:06:37.479402 139637464966912 logging_writer.py:48] [101600] global_step=101600, grad_norm=0.35051020979881287, loss=1.6207361221313477
I0306 12:07:12.635432 139637473359616 logging_writer.py:48] [101700] global_step=101700, grad_norm=0.3286207616329193, loss=1.5358526706695557
I0306 12:07:47.796807 139637464966912 logging_writer.py:48] [101800] global_step=101800, grad_norm=0.34118470549583435, loss=1.5312285423278809
I0306 12:08:22.979154 139637473359616 logging_writer.py:48] [101900] global_step=101900, grad_norm=0.34360644221305847, loss=1.5786902904510498
I0306 12:08:58.154281 139637464966912 logging_writer.py:48] [102000] global_step=102000, grad_norm=0.35071930289268494, loss=1.5896159410476685
I0306 12:09:33.286386 139637473359616 logging_writer.py:48] [102100] global_step=102100, grad_norm=0.3428877592086792, loss=1.5651648044586182
I0306 12:10:08.429235 139637464966912 logging_writer.py:48] [102200] global_step=102200, grad_norm=0.33447524905204773, loss=1.4743444919586182
I0306 12:10:43.583356 139637473359616 logging_writer.py:48] [102300] global_step=102300, grad_norm=0.3469627797603607, loss=1.5339250564575195
I0306 12:11:18.719162 139637464966912 logging_writer.py:48] [102400] global_step=102400, grad_norm=0.33739444613456726, loss=1.555425763130188
I0306 12:11:53.859227 139637473359616 logging_writer.py:48] [102500] global_step=102500, grad_norm=0.3343115746974945, loss=1.4609451293945312
I0306 12:12:29.012051 139637464966912 logging_writer.py:48] [102600] global_step=102600, grad_norm=0.3478221297264099, loss=1.4799346923828125
I0306 12:13:04.166035 139637473359616 logging_writer.py:48] [102700] global_step=102700, grad_norm=0.3373849093914032, loss=1.577579379081726
I0306 12:13:29.484812 139780644897984 spec.py:321] Evaluating on the training split.
I0306 12:13:32.137415 139780644897984 workload.py:181] Translating evaluation dataset.
I0306 12:17:40.328418 139780644897984 spec.py:333] Evaluating on the validation split.
I0306 12:17:42.963504 139780644897984 workload.py:181] Translating evaluation dataset.
I0306 12:20:54.453540 139780644897984 spec.py:349] Evaluating on the test split.
I0306 12:20:57.094807 139780644897984 workload.py:181] Translating evaluation dataset.
I0306 12:24:09.708735 139780644897984 submission_runner.py:469] Time since start: 61840.97s, 	Step: 102773, 	{'train/accuracy': 0.678148627281189, 'train/loss': 1.5241607427597046, 'train/bleu': 34.77623422627334, 'validation/accuracy': 0.6866858005523682, 'validation/loss': 1.4503389596939087, 'validation/bleu': 30.23379173470218, 'validation/num_examples': 3000, 'test/accuracy': 0.7020971179008484, 'test/loss': 1.3652292490005493, 'test/bleu': 30.252482468017, 'test/num_examples': 3003, 'score': 36147.062624931335, 'total_duration': 61840.97221279144, 'accumulated_submission_time': 36147.062624931335, 'accumulated_eval_time': 25686.989778280258, 'accumulated_logging_time': 1.0742733478546143}
I0306 12:24:09.728654 139637464966912 logging_writer.py:48] [102773] accumulated_eval_time=25687, accumulated_logging_time=1.07427, accumulated_submission_time=36147.1, global_step=102773, preemption_count=0, score=36147.1, test/accuracy=0.702097, test/bleu=30.2525, test/loss=1.36523, test/num_examples=3003, total_duration=61841, train/accuracy=0.678149, train/bleu=34.7762, train/loss=1.52416, validation/accuracy=0.686686, validation/bleu=30.2338, validation/loss=1.45034, validation/num_examples=3000
I0306 12:24:19.535559 139637473359616 logging_writer.py:48] [102800] global_step=102800, grad_norm=0.42318642139434814, loss=1.5938512086868286
I0306 12:24:54.554273 139637464966912 logging_writer.py:48] [102900] global_step=102900, grad_norm=0.3382495939731598, loss=1.5311529636383057
I0306 12:25:29.617194 139637473359616 logging_writer.py:48] [103000] global_step=103000, grad_norm=0.34903350472450256, loss=1.5732197761535645
I0306 12:26:04.754226 139637464966912 logging_writer.py:48] [103100] global_step=103100, grad_norm=0.36213985085487366, loss=1.6560142040252686
I0306 12:26:39.861185 139637473359616 logging_writer.py:48] [103200] global_step=103200, grad_norm=0.341711163520813, loss=1.5461262464523315
I0306 12:27:15.009896 139637464966912 logging_writer.py:48] [103300] global_step=103300, grad_norm=0.3950749635696411, loss=1.5674679279327393
I0306 12:27:50.152527 139637473359616 logging_writer.py:48] [103400] global_step=103400, grad_norm=0.3556075692176819, loss=1.5694332122802734
I0306 12:28:25.284934 139637464966912 logging_writer.py:48] [103500] global_step=103500, grad_norm=0.3392789959907532, loss=1.5428358316421509
I0306 12:29:00.404090 139637473359616 logging_writer.py:48] [103600] global_step=103600, grad_norm=0.366265207529068, loss=1.6455377340316772
I0306 12:29:35.557157 139637464966912 logging_writer.py:48] [103700] global_step=103700, grad_norm=0.3465856909751892, loss=1.5484497547149658
I0306 12:30:10.672182 139637473359616 logging_writer.py:48] [103800] global_step=103800, grad_norm=0.3433851897716522, loss=1.5465140342712402
I0306 12:30:45.820748 139637464966912 logging_writer.py:48] [103900] global_step=103900, grad_norm=0.3433729112148285, loss=1.520564317703247
I0306 12:31:20.975432 139637473359616 logging_writer.py:48] [104000] global_step=104000, grad_norm=0.33808013796806335, loss=1.535460352897644
I0306 12:31:56.150835 139637464966912 logging_writer.py:48] [104100] global_step=104100, grad_norm=0.3572674095630646, loss=1.5901410579681396
I0306 12:32:31.291983 139637473359616 logging_writer.py:48] [104200] global_step=104200, grad_norm=0.3646843135356903, loss=1.6158013343811035
I0306 12:33:06.468135 139637464966912 logging_writer.py:48] [104300] global_step=104300, grad_norm=0.3603282868862152, loss=1.524024248123169
I0306 12:33:41.596957 139637473359616 logging_writer.py:48] [104400] global_step=104400, grad_norm=0.35477781295776367, loss=1.468921422958374
I0306 12:34:16.736055 139637464966912 logging_writer.py:48] [104500] global_step=104500, grad_norm=0.331670343875885, loss=1.4732352495193481
I0306 12:34:51.879278 139637473359616 logging_writer.py:48] [104600] global_step=104600, grad_norm=0.34250351786613464, loss=1.547810673713684
I0306 12:35:27.023912 139637464966912 logging_writer.py:48] [104700] global_step=104700, grad_norm=0.32709047198295593, loss=1.4193052053451538
I0306 12:36:02.167384 139637473359616 logging_writer.py:48] [104800] global_step=104800, grad_norm=0.3337084949016571, loss=1.5426702499389648
I0306 12:36:37.323051 139637464966912 logging_writer.py:48] [104900] global_step=104900, grad_norm=0.3568320870399475, loss=1.5117813348770142
I0306 12:37:12.505595 139637473359616 logging_writer.py:48] [105000] global_step=105000, grad_norm=0.3409346640110016, loss=1.5267715454101562
I0306 12:37:47.644268 139637464966912 logging_writer.py:48] [105100] global_step=105100, grad_norm=0.3418673574924469, loss=1.451818585395813
I0306 12:38:09.772757 139780644897984 spec.py:321] Evaluating on the training split.
I0306 12:38:12.426517 139780644897984 workload.py:181] Translating evaluation dataset.
I0306 12:42:37.280869 139780644897984 spec.py:333] Evaluating on the validation split.
I0306 12:42:39.920243 139780644897984 workload.py:181] Translating evaluation dataset.
I0306 12:45:34.088697 139780644897984 spec.py:349] Evaluating on the test split.
I0306 12:45:36.721268 139780644897984 workload.py:181] Translating evaluation dataset.
I0306 12:48:46.057946 139780644897984 submission_runner.py:469] Time since start: 63317.32s, 	Step: 105164, 	{'train/accuracy': 0.677069902420044, 'train/loss': 1.5350580215454102, 'train/bleu': 33.95803755505226, 'validation/accuracy': 0.686772346496582, 'validation/loss': 1.4431196451187134, 'validation/bleu': 30.274995899610502, 'validation/num_examples': 3000, 'test/accuracy': 0.7041130661964417, 'test/loss': 1.3482943773269653, 'test/bleu': 30.349883356951473, 'test/num_examples': 3003, 'score': 36986.969824790955, 'total_duration': 63317.321434259415, 'accumulated_submission_time': 36986.969824790955, 'accumulated_eval_time': 26323.274929523468, 'accumulated_logging_time': 1.1026649475097656}
I0306 12:48:46.078101 139637473359616 logging_writer.py:48] [105164] accumulated_eval_time=26323.3, accumulated_logging_time=1.10266, accumulated_submission_time=36987, global_step=105164, preemption_count=0, score=36987, test/accuracy=0.704113, test/bleu=30.3499, test/loss=1.34829, test/num_examples=3003, total_duration=63317.3, train/accuracy=0.67707, train/bleu=33.958, train/loss=1.53506, validation/accuracy=0.686772, validation/bleu=30.275, validation/loss=1.44312, validation/num_examples=3000
I0306 12:48:59.057150 139637464966912 logging_writer.py:48] [105200] global_step=105200, grad_norm=0.35147038102149963, loss=1.5081290006637573
I0306 12:49:34.121174 139637473359616 logging_writer.py:48] [105300] global_step=105300, grad_norm=0.342780739068985, loss=1.4953689575195312
I0306 12:50:09.266688 139637464966912 logging_writer.py:48] [105400] global_step=105400, grad_norm=0.3778402507305145, loss=1.5037939548492432
I0306 12:50:44.403954 139637473359616 logging_writer.py:48] [105500] global_step=105500, grad_norm=0.3667728304862976, loss=1.5538159608840942
I0306 12:51:19.558748 139637464966912 logging_writer.py:48] [105600] global_step=105600, grad_norm=0.3768698573112488, loss=1.5365408658981323
I0306 12:51:54.686788 139637473359616 logging_writer.py:48] [105700] global_step=105700, grad_norm=0.32586008310317993, loss=1.4002522230148315
I0306 12:52:29.819967 139637464966912 logging_writer.py:48] [105800] global_step=105800, grad_norm=0.3598700165748596, loss=1.5781155824661255
I0306 12:53:04.981279 139637473359616 logging_writer.py:48] [105900] global_step=105900, grad_norm=0.35173556208610535, loss=1.5049033164978027
I0306 12:53:40.112812 139637464966912 logging_writer.py:48] [106000] global_step=106000, grad_norm=0.35819050669670105, loss=1.5800271034240723
I0306 12:54:15.247810 139637473359616 logging_writer.py:48] [106100] global_step=106100, grad_norm=0.3615483045578003, loss=1.5680820941925049
I0306 12:54:50.383435 139637464966912 logging_writer.py:48] [106200] global_step=106200, grad_norm=0.3320732116699219, loss=1.4654663801193237
I0306 12:55:25.551406 139637473359616 logging_writer.py:48] [106300] global_step=106300, grad_norm=0.34626254439353943, loss=1.492380976676941
I0306 12:56:00.710684 139637464966912 logging_writer.py:48] [106400] global_step=106400, grad_norm=0.37245678901672363, loss=1.5942596197128296
I0306 12:56:35.834176 139637473359616 logging_writer.py:48] [106500] global_step=106500, grad_norm=0.3733034133911133, loss=1.6153512001037598
I0306 12:57:10.998764 139637464966912 logging_writer.py:48] [106600] global_step=106600, grad_norm=0.37891024351119995, loss=1.449172854423523
I0306 12:57:46.114552 139637473359616 logging_writer.py:48] [106700] global_step=106700, grad_norm=0.37016722559928894, loss=1.5494320392608643
I0306 12:58:21.193907 139637464966912 logging_writer.py:48] [106800] global_step=106800, grad_norm=0.3426140248775482, loss=1.5520403385162354
I0306 12:58:56.276260 139637473359616 logging_writer.py:48] [106900] global_step=106900, grad_norm=0.365143358707428, loss=1.5157439708709717
I0306 12:59:31.375694 139637464966912 logging_writer.py:48] [107000] global_step=107000, grad_norm=0.3887638747692108, loss=1.5334153175354004
I0306 13:00:06.463457 139637473359616 logging_writer.py:48] [107100] global_step=107100, grad_norm=0.3523778021335602, loss=1.5063806772232056
I0306 13:00:41.560753 139637464966912 logging_writer.py:48] [107200] global_step=107200, grad_norm=0.36131200194358826, loss=1.51479172706604
I0306 13:01:16.661824 139637473359616 logging_writer.py:48] [107300] global_step=107300, grad_norm=0.3681802749633789, loss=1.5187739133834839
I0306 13:01:51.728409 139637464966912 logging_writer.py:48] [107400] global_step=107400, grad_norm=0.3556968569755554, loss=1.5384459495544434
I0306 13:02:26.823782 139637473359616 logging_writer.py:48] [107500] global_step=107500, grad_norm=0.37150874733924866, loss=1.5599138736724854
I0306 13:02:46.103890 139780644897984 spec.py:321] Evaluating on the training split.
I0306 13:02:48.749542 139780644897984 workload.py:181] Translating evaluation dataset.
I0306 13:06:59.558482 139780644897984 spec.py:333] Evaluating on the validation split.
I0306 13:07:02.204536 139780644897984 workload.py:181] Translating evaluation dataset.
I0306 13:09:58.468599 139780644897984 spec.py:349] Evaluating on the test split.
I0306 13:10:01.104445 139780644897984 workload.py:181] Translating evaluation dataset.
I0306 13:12:51.804774 139780644897984 submission_runner.py:469] Time since start: 64763.07s, 	Step: 107556, 	{'train/accuracy': 0.6941144466400146, 'train/loss': 1.4346460103988647, 'train/bleu': 35.112526953808626, 'validation/accuracy': 0.6891207098960876, 'validation/loss': 1.432898759841919, 'validation/bleu': 30.198097813531618, 'validation/num_examples': 3000, 'test/accuracy': 0.7042057514190674, 'test/loss': 1.3423876762390137, 'test/bleu': 30.7095453089601, 'test/num_examples': 3003, 'score': 37826.85638546944, 'total_duration': 64763.06825017929, 'accumulated_submission_time': 37826.85638546944, 'accumulated_eval_time': 26928.975747346878, 'accumulated_logging_time': 1.1313133239746094}
I0306 13:12:51.827668 139637464966912 logging_writer.py:48] [107556] accumulated_eval_time=26929, accumulated_logging_time=1.13131, accumulated_submission_time=37826.9, global_step=107556, preemption_count=0, score=37826.9, test/accuracy=0.704206, test/bleu=30.7095, test/loss=1.34239, test/num_examples=3003, total_duration=64763.1, train/accuracy=0.694114, train/bleu=35.1125, train/loss=1.43465, validation/accuracy=0.689121, validation/bleu=30.1981, validation/loss=1.4329, validation/num_examples=3000
I0306 13:13:07.571598 139637473359616 logging_writer.py:48] [107600] global_step=107600, grad_norm=0.3385801315307617, loss=1.4684274196624756
I0306 13:13:42.552871 139637464966912 logging_writer.py:48] [107700] global_step=107700, grad_norm=0.36239659786224365, loss=1.505117416381836
I0306 13:14:17.601734 139637473359616 logging_writer.py:48] [107800] global_step=107800, grad_norm=0.3520994186401367, loss=1.5174880027770996
I0306 13:14:52.633864 139637464966912 logging_writer.py:48] [107900] global_step=107900, grad_norm=0.3718249797821045, loss=1.476955771446228
I0306 13:15:27.712221 139637473359616 logging_writer.py:48] [108000] global_step=108000, grad_norm=0.35147082805633545, loss=1.4719407558441162
I0306 13:16:02.805988 139637464966912 logging_writer.py:48] [108100] global_step=108100, grad_norm=0.380485475063324, loss=1.5184500217437744
I0306 13:16:37.895494 139637473359616 logging_writer.py:48] [108200] global_step=108200, grad_norm=0.35808998346328735, loss=1.463967204093933
I0306 13:17:12.991337 139637464966912 logging_writer.py:48] [108300] global_step=108300, grad_norm=0.35809072852134705, loss=1.4758902788162231
I0306 13:17:48.068142 139637473359616 logging_writer.py:48] [108400] global_step=108400, grad_norm=0.38482752442359924, loss=1.568849802017212
I0306 13:18:23.121915 139637464966912 logging_writer.py:48] [108500] global_step=108500, grad_norm=0.34817829728126526, loss=1.4503718614578247
I0306 13:18:58.226062 139637473359616 logging_writer.py:48] [108600] global_step=108600, grad_norm=0.3768594563007355, loss=1.5213803052902222
I0306 13:19:33.315587 139637464966912 logging_writer.py:48] [108700] global_step=108700, grad_norm=0.3701859712600708, loss=1.4856520891189575
I0306 13:20:08.398880 139637473359616 logging_writer.py:48] [108800] global_step=108800, grad_norm=0.3721083104610443, loss=1.4841485023498535
I0306 13:20:43.483196 139637464966912 logging_writer.py:48] [108900] global_step=108900, grad_norm=0.33878421783447266, loss=1.4768024682998657
I0306 13:21:18.585817 139637473359616 logging_writer.py:48] [109000] global_step=109000, grad_norm=0.3643994927406311, loss=1.5057017803192139
I0306 13:21:53.678068 139637464966912 logging_writer.py:48] [109100] global_step=109100, grad_norm=0.3489207327365875, loss=1.4924229383468628
I0306 13:22:28.753198 139637473359616 logging_writer.py:48] [109200] global_step=109200, grad_norm=0.36837026476860046, loss=1.4400471448898315
I0306 13:23:03.825726 139637464966912 logging_writer.py:48] [109300] global_step=109300, grad_norm=0.3531765341758728, loss=1.4342542886734009
I0306 13:23:38.920699 139637473359616 logging_writer.py:48] [109400] global_step=109400, grad_norm=0.3723393678665161, loss=1.5239777565002441
I0306 13:24:13.992353 139637464966912 logging_writer.py:48] [109500] global_step=109500, grad_norm=0.3340408504009247, loss=1.4686908721923828
I0306 13:24:49.116800 139637473359616 logging_writer.py:48] [109600] global_step=109600, grad_norm=0.35075023770332336, loss=1.3900229930877686
I0306 13:25:24.209408 139637464966912 logging_writer.py:48] [109700] global_step=109700, grad_norm=0.3499351143836975, loss=1.4371747970581055
I0306 13:25:59.303891 139637473359616 logging_writer.py:48] [109800] global_step=109800, grad_norm=0.35203635692596436, loss=1.5230985879898071
I0306 13:26:34.402626 139637464966912 logging_writer.py:48] [109900] global_step=109900, grad_norm=0.3759458065032959, loss=1.5257773399353027
I0306 13:26:51.979512 139780644897984 spec.py:321] Evaluating on the training split.
I0306 13:26:54.630455 139780644897984 workload.py:181] Translating evaluation dataset.
I0306 13:30:53.184533 139780644897984 spec.py:333] Evaluating on the validation split.
I0306 13:30:55.816550 139780644897984 workload.py:181] Translating evaluation dataset.
I0306 13:33:26.017398 139780644897984 spec.py:349] Evaluating on the test split.
I0306 13:33:28.657734 139780644897984 workload.py:181] Translating evaluation dataset.
I0306 13:36:22.319205 139780644897984 submission_runner.py:469] Time since start: 66173.58s, 	Step: 109951, 	{'train/accuracy': 0.6846319437026978, 'train/loss': 1.4830002784729004, 'train/bleu': 34.77058372290943, 'validation/accuracy': 0.6907522678375244, 'validation/loss': 1.4243083000183105, 'validation/bleu': 30.496464512797576, 'validation/num_examples': 3000, 'test/accuracy': 0.7065577507019043, 'test/loss': 1.3313876390457153, 'test/bleu': 30.943426328952334, 'test/num_examples': 3003, 'score': 38666.86701631546, 'total_duration': 66173.58270144463, 'accumulated_submission_time': 38666.86701631546, 'accumulated_eval_time': 27499.315398693085, 'accumulated_logging_time': 1.1643099784851074}
I0306 13:36:22.340588 139637473359616 logging_writer.py:48] [109951] accumulated_eval_time=27499.3, accumulated_logging_time=1.16431, accumulated_submission_time=38666.9, global_step=109951, preemption_count=0, score=38666.9, test/accuracy=0.706558, test/bleu=30.9434, test/loss=1.33139, test/num_examples=3003, total_duration=66173.6, train/accuracy=0.684632, train/bleu=34.7706, train/loss=1.483, validation/accuracy=0.690752, validation/bleu=30.4965, validation/loss=1.42431, validation/num_examples=3000
I0306 13:36:39.850696 139637464966912 logging_writer.py:48] [110000] global_step=110000, grad_norm=0.3673999309539795, loss=1.5187329053878784
I0306 13:37:14.832145 139637473359616 logging_writer.py:48] [110100] global_step=110100, grad_norm=0.36830130219459534, loss=1.5541094541549683
I0306 13:37:49.829655 139637464966912 logging_writer.py:48] [110200] global_step=110200, grad_norm=0.36670616269111633, loss=1.5077910423278809
I0306 13:38:24.901059 139637473359616 logging_writer.py:48] [110300] global_step=110300, grad_norm=0.3620798885822296, loss=1.4295356273651123
I0306 13:38:59.944020 139637464966912 logging_writer.py:48] [110400] global_step=110400, grad_norm=0.36815083026885986, loss=1.4670403003692627
I0306 13:39:35.023090 139637473359616 logging_writer.py:48] [110500] global_step=110500, grad_norm=0.36438700556755066, loss=1.439713478088379
I0306 13:40:10.080979 139637464966912 logging_writer.py:48] [110600] global_step=110600, grad_norm=0.40113818645477295, loss=1.5565541982650757
I0306 13:40:45.164968 139637473359616 logging_writer.py:48] [110700] global_step=110700, grad_norm=0.37352219223976135, loss=1.4278935194015503
I0306 13:41:20.256368 139637464966912 logging_writer.py:48] [110800] global_step=110800, grad_norm=0.35975968837738037, loss=1.493045449256897
I0306 13:41:55.335283 139637473359616 logging_writer.py:48] [110900] global_step=110900, grad_norm=0.387263685464859, loss=1.51071298122406
I0306 13:42:30.407688 139637464966912 logging_writer.py:48] [111000] global_step=111000, grad_norm=0.3566723167896271, loss=1.4511680603027344
I0306 13:43:05.497882 139637473359616 logging_writer.py:48] [111100] global_step=111100, grad_norm=0.38497403264045715, loss=1.5003780126571655
I0306 13:43:40.518156 139637464966912 logging_writer.py:48] [111200] global_step=111200, grad_norm=0.3687783479690552, loss=1.4730898141860962
I0306 13:44:15.577937 139637473359616 logging_writer.py:48] [111300] global_step=111300, grad_norm=0.3524128794670105, loss=1.4533367156982422
I0306 13:44:50.627079 139637464966912 logging_writer.py:48] [111400] global_step=111400, grad_norm=0.3710005581378937, loss=1.4357571601867676
I0306 13:45:25.667568 139637473359616 logging_writer.py:48] [111500] global_step=111500, grad_norm=0.3679535984992981, loss=1.4340496063232422
I0306 13:46:00.695166 139637464966912 logging_writer.py:48] [111600] global_step=111600, grad_norm=0.3511523902416229, loss=1.3760708570480347
I0306 13:46:35.753850 139637473359616 logging_writer.py:48] [111700] global_step=111700, grad_norm=0.36244723200798035, loss=1.4478883743286133
I0306 13:47:10.813688 139637464966912 logging_writer.py:48] [111800] global_step=111800, grad_norm=0.3556351065635681, loss=1.444191813468933
I0306 13:47:45.845562 139637473359616 logging_writer.py:48] [111900] global_step=111900, grad_norm=0.35944095253944397, loss=1.457626461982727
I0306 13:48:20.882308 139637464966912 logging_writer.py:48] [112000] global_step=112000, grad_norm=0.3734222948551178, loss=1.4798249006271362
I0306 13:48:55.973159 139637473359616 logging_writer.py:48] [112100] global_step=112100, grad_norm=0.35770007967948914, loss=1.3586082458496094
I0306 13:49:31.028493 139637464966912 logging_writer.py:48] [112200] global_step=112200, grad_norm=0.37736430764198303, loss=1.4941576719284058
I0306 13:50:06.090153 139637473359616 logging_writer.py:48] [112300] global_step=112300, grad_norm=0.35403972864151, loss=1.4776932001113892
I0306 13:50:22.571141 139780644897984 spec.py:321] Evaluating on the training split.
I0306 13:50:25.219431 139780644897984 workload.py:181] Translating evaluation dataset.
I0306 13:54:27.980451 139780644897984 spec.py:333] Evaluating on the validation split.
I0306 13:54:30.623152 139780644897984 workload.py:181] Translating evaluation dataset.
I0306 13:57:21.113605 139780644897984 spec.py:349] Evaluating on the test split.
I0306 13:57:23.758864 139780644897984 workload.py:181] Translating evaluation dataset.
I0306 14:00:26.834569 139780644897984 submission_runner.py:469] Time since start: 67618.10s, 	Step: 112348, 	{'train/accuracy': 0.6892876625061035, 'train/loss': 1.4602874517440796, 'train/bleu': 35.21436361010657, 'validation/accuracy': 0.6915556192398071, 'validation/loss': 1.4185775518417358, 'validation/bleu': 30.628741935996597, 'validation/num_examples': 3000, 'test/accuracy': 0.7084115147590637, 'test/loss': 1.324640154838562, 'test/bleu': 30.794080876055325, 'test/num_examples': 3003, 'score': 39506.95842409134, 'total_duration': 67618.09801673889, 'accumulated_submission_time': 39506.95842409134, 'accumulated_eval_time': 28103.578729391098, 'accumulated_logging_time': 1.1939146518707275}
I0306 14:00:26.855500 139637464966912 logging_writer.py:48] [112348] accumulated_eval_time=28103.6, accumulated_logging_time=1.19391, accumulated_submission_time=39507, global_step=112348, preemption_count=0, score=39507, test/accuracy=0.708412, test/bleu=30.7941, test/loss=1.32464, test/num_examples=3003, total_duration=67618.1, train/accuracy=0.689288, train/bleu=35.2144, train/loss=1.46029, validation/accuracy=0.691556, validation/bleu=30.6287, validation/loss=1.41858, validation/num_examples=3000
I0306 14:00:45.372840 139637473359616 logging_writer.py:48] [112400] global_step=112400, grad_norm=0.3835681080818176, loss=1.5011355876922607
I0306 14:01:20.337860 139637464966912 logging_writer.py:48] [112500] global_step=112500, grad_norm=0.38718071579933167, loss=1.4486703872680664
I0306 14:01:55.344257 139637473359616 logging_writer.py:48] [112600] global_step=112600, grad_norm=0.35984551906585693, loss=1.409688949584961
I0306 14:02:30.391879 139637464966912 logging_writer.py:48] [112700] global_step=112700, grad_norm=0.3649309575557709, loss=1.4645144939422607
I0306 14:03:05.422475 139637473359616 logging_writer.py:48] [112800] global_step=112800, grad_norm=0.3695909082889557, loss=1.426176905632019
I0306 14:03:40.485041 139637464966912 logging_writer.py:48] [112900] global_step=112900, grad_norm=0.3717498779296875, loss=1.4069373607635498
I0306 14:04:15.607282 139637473359616 logging_writer.py:48] [113000] global_step=113000, grad_norm=0.35682910680770874, loss=1.4566177129745483
I0306 14:04:50.754066 139637464966912 logging_writer.py:48] [113100] global_step=113100, grad_norm=0.34821516275405884, loss=1.3321397304534912
I0306 14:05:25.905837 139637473359616 logging_writer.py:48] [113200] global_step=113200, grad_norm=0.38136017322540283, loss=1.5160431861877441
I0306 14:06:01.074403 139637464966912 logging_writer.py:48] [113300] global_step=113300, grad_norm=0.3698142468929291, loss=1.4731673002243042
I0306 14:06:36.234854 139637473359616 logging_writer.py:48] [113400] global_step=113400, grad_norm=0.36540281772613525, loss=1.4239217042922974
I0306 14:07:11.397162 139637464966912 logging_writer.py:48] [113500] global_step=113500, grad_norm=0.36496254801750183, loss=1.5024561882019043
I0306 14:07:46.560685 139637473359616 logging_writer.py:48] [113600] global_step=113600, grad_norm=0.3750835359096527, loss=1.435854196548462
I0306 14:08:21.720251 139637464966912 logging_writer.py:48] [113700] global_step=113700, grad_norm=0.3687956631183624, loss=1.4546639919281006
I0306 14:08:56.880927 139637473359616 logging_writer.py:48] [113800] global_step=113800, grad_norm=0.37935537099838257, loss=1.4867634773254395
I0306 14:09:32.046472 139637464966912 logging_writer.py:48] [113900] global_step=113900, grad_norm=0.35623612999916077, loss=1.407326102256775
I0306 14:10:07.183752 139637473359616 logging_writer.py:48] [114000] global_step=114000, grad_norm=0.36307623982429504, loss=1.5114760398864746
I0306 14:10:42.342714 139637464966912 logging_writer.py:48] [114100] global_step=114100, grad_norm=0.37464234232902527, loss=1.416917324066162
I0306 14:11:17.525135 139637473359616 logging_writer.py:48] [114200] global_step=114200, grad_norm=0.3511887192726135, loss=1.3897063732147217
I0306 14:11:52.662898 139637464966912 logging_writer.py:48] [114300] global_step=114300, grad_norm=0.37455055117607117, loss=1.4780616760253906
I0306 14:12:27.800744 139637473359616 logging_writer.py:48] [114400] global_step=114400, grad_norm=0.3630593419075012, loss=1.3992403745651245
I0306 14:13:02.946632 139637464966912 logging_writer.py:48] [114500] global_step=114500, grad_norm=0.3525765836238861, loss=1.4330848455429077
I0306 14:13:38.077767 139637473359616 logging_writer.py:48] [114600] global_step=114600, grad_norm=0.37350064516067505, loss=1.4294923543930054
I0306 14:14:13.217803 139637464966912 logging_writer.py:48] [114700] global_step=114700, grad_norm=0.37038010358810425, loss=1.426094889640808
I0306 14:14:26.915568 139780644897984 spec.py:321] Evaluating on the training split.
I0306 14:14:29.566340 139780644897984 workload.py:181] Translating evaluation dataset.
I0306 14:18:18.015824 139780644897984 spec.py:333] Evaluating on the validation split.
I0306 14:18:20.668414 139780644897984 workload.py:181] Translating evaluation dataset.
I0306 14:20:50.950489 139780644897984 spec.py:349] Evaluating on the test split.
I0306 14:20:53.581534 139780644897984 workload.py:181] Translating evaluation dataset.
I0306 14:23:38.310730 139780644897984 submission_runner.py:469] Time since start: 69009.57s, 	Step: 114740, 	{'train/accuracy': 0.6956056952476501, 'train/loss': 1.4176084995269775, 'train/bleu': 35.21629381808173, 'validation/accuracy': 0.6917533874511719, 'validation/loss': 1.4145179986953735, 'validation/bleu': 30.634616504696258, 'validation/num_examples': 3000, 'test/accuracy': 0.7091066837310791, 'test/loss': 1.3200386762619019, 'test/bleu': 30.869499447208245, 'test/num_examples': 3003, 'score': 40346.873964071274, 'total_duration': 69009.57422375679, 'accumulated_submission_time': 40346.873964071274, 'accumulated_eval_time': 28654.973845481873, 'accumulated_logging_time': 1.224264144897461}
I0306 14:23:38.331537 139637473359616 logging_writer.py:48] [114740] accumulated_eval_time=28655, accumulated_logging_time=1.22426, accumulated_submission_time=40346.9, global_step=114740, preemption_count=0, score=40346.9, test/accuracy=0.709107, test/bleu=30.8695, test/loss=1.32004, test/num_examples=3003, total_duration=69009.6, train/accuracy=0.695606, train/bleu=35.2163, train/loss=1.41761, validation/accuracy=0.691753, validation/bleu=30.6346, validation/loss=1.41452, validation/num_examples=3000
I0306 14:23:59.693518 139637464966912 logging_writer.py:48] [114800] global_step=114800, grad_norm=0.35385990142822266, loss=1.3805053234100342
I0306 14:24:34.760331 139637473359616 logging_writer.py:48] [114900] global_step=114900, grad_norm=0.37368589639663696, loss=1.4434829950332642
I0306 14:25:09.846700 139637464966912 logging_writer.py:48] [115000] global_step=115000, grad_norm=0.38230887055397034, loss=1.4927480220794678
I0306 14:25:44.975131 139637473359616 logging_writer.py:48] [115100] global_step=115100, grad_norm=0.3672633469104767, loss=1.4138721227645874
I0306 14:26:20.100069 139637464966912 logging_writer.py:48] [115200] global_step=115200, grad_norm=0.37950652837753296, loss=1.4576327800750732
I0306 14:26:55.229290 139637473359616 logging_writer.py:48] [115300] global_step=115300, grad_norm=0.3723727762699127, loss=1.4012079238891602
I0306 14:27:30.384426 139637464966912 logging_writer.py:48] [115400] global_step=115400, grad_norm=0.3984426259994507, loss=1.4690476655960083
I0306 14:28:05.497183 139637473359616 logging_writer.py:48] [115500] global_step=115500, grad_norm=0.3699171841144562, loss=1.459542155265808
I0306 14:28:40.677123 139637464966912 logging_writer.py:48] [115600] global_step=115600, grad_norm=0.3809081017971039, loss=1.4362636804580688
I0306 14:29:15.829370 139637473359616 logging_writer.py:48] [115700] global_step=115700, grad_norm=0.3859337866306305, loss=1.539259433746338
I0306 14:29:50.993707 139637464966912 logging_writer.py:48] [115800] global_step=115800, grad_norm=0.3725421726703644, loss=1.4847038984298706
I0306 14:30:26.164502 139637473359616 logging_writer.py:48] [115900] global_step=115900, grad_norm=0.3692394495010376, loss=1.4333021640777588
I0306 14:31:01.326246 139637464966912 logging_writer.py:48] [116000] global_step=116000, grad_norm=0.36893430352211, loss=1.4565014839172363
I0306 14:31:36.472728 139637473359616 logging_writer.py:48] [116100] global_step=116100, grad_norm=0.39350491762161255, loss=1.5118745565414429
I0306 14:32:11.608746 139637464966912 logging_writer.py:48] [116200] global_step=116200, grad_norm=0.37550088763237, loss=1.4432413578033447
I0306 14:32:46.813270 139637473359616 logging_writer.py:48] [116300] global_step=116300, grad_norm=0.3605295419692993, loss=1.4178071022033691
I0306 14:33:21.963420 139637464966912 logging_writer.py:48] [116400] global_step=116400, grad_norm=0.3712571859359741, loss=1.4709261655807495
I0306 14:33:57.092045 139637473359616 logging_writer.py:48] [116500] global_step=116500, grad_norm=0.3650514781475067, loss=1.4308996200561523
I0306 14:34:32.262010 139637464966912 logging_writer.py:48] [116600] global_step=116600, grad_norm=0.3554325997829437, loss=1.367289662361145
I0306 14:35:07.405015 139637473359616 logging_writer.py:48] [116700] global_step=116700, grad_norm=0.36753416061401367, loss=1.4461008310317993
I0306 14:35:42.554099 139637464966912 logging_writer.py:48] [116800] global_step=116800, grad_norm=0.35666346549987793, loss=1.3922123908996582
I0306 14:36:17.711172 139637473359616 logging_writer.py:48] [116900] global_step=116900, grad_norm=0.38786202669143677, loss=1.4436317682266235
I0306 14:36:52.864266 139637464966912 logging_writer.py:48] [117000] global_step=117000, grad_norm=0.37657663226127625, loss=1.4787931442260742
I0306 14:37:27.999069 139637473359616 logging_writer.py:48] [117100] global_step=117100, grad_norm=0.38075217604637146, loss=1.4500609636306763
I0306 14:37:38.542286 139780644897984 spec.py:321] Evaluating on the training split.
I0306 14:37:41.191342 139780644897984 workload.py:181] Translating evaluation dataset.
I0306 14:42:01.808687 139780644897984 spec.py:333] Evaluating on the validation split.
I0306 14:42:04.455355 139780644897984 workload.py:181] Translating evaluation dataset.
I0306 14:44:37.429354 139780644897984 spec.py:349] Evaluating on the test split.
I0306 14:44:40.077370 139780644897984 workload.py:181] Translating evaluation dataset.
I0306 14:47:46.841993 139780644897984 submission_runner.py:469] Time since start: 70458.11s, 	Step: 117131, 	{'train/accuracy': 0.6980080008506775, 'train/loss': 1.4130302667617798, 'train/bleu': 35.43024650972528, 'validation/accuracy': 0.6925073862075806, 'validation/loss': 1.412505030632019, 'validation/bleu': 30.782688578398776, 'validation/num_examples': 3000, 'test/accuracy': 0.7100452184677124, 'test/loss': 1.316109538078308, 'test/bleu': 31.049246057639397, 'test/num_examples': 3003, 'score': 41186.94385623932, 'total_duration': 70458.10548996925, 'accumulated_submission_time': 41186.94385623932, 'accumulated_eval_time': 29263.273505449295, 'accumulated_logging_time': 1.2537436485290527}
I0306 14:47:46.864268 139637464966912 logging_writer.py:48] [117131] accumulated_eval_time=29263.3, accumulated_logging_time=1.25374, accumulated_submission_time=41186.9, global_step=117131, preemption_count=0, score=41186.9, test/accuracy=0.710045, test/bleu=31.0492, test/loss=1.31611, test/num_examples=3003, total_duration=70458.1, train/accuracy=0.698008, train/bleu=35.4302, train/loss=1.41303, validation/accuracy=0.692507, validation/bleu=30.7827, validation/loss=1.41251, validation/num_examples=3000
I0306 14:48:11.373000 139637473359616 logging_writer.py:48] [117200] global_step=117200, grad_norm=0.3635683059692383, loss=1.365289330482483
I0306 14:48:46.399706 139637464966912 logging_writer.py:48] [117300] global_step=117300, grad_norm=0.35655614733695984, loss=1.4135288000106812
I0306 14:49:21.496657 139637473359616 logging_writer.py:48] [117400] global_step=117400, grad_norm=0.38345158100128174, loss=1.4744094610214233
I0306 14:49:56.598578 139637464966912 logging_writer.py:48] [117500] global_step=117500, grad_norm=0.3833893835544586, loss=1.4536986351013184
I0306 14:50:31.714152 139637473359616 logging_writer.py:48] [117600] global_step=117600, grad_norm=0.3675217032432556, loss=1.4076499938964844
I0306 14:51:06.822359 139637464966912 logging_writer.py:48] [117700] global_step=117700, grad_norm=0.3616366684436798, loss=1.405754566192627
I0306 14:51:41.943069 139637473359616 logging_writer.py:48] [117800] global_step=117800, grad_norm=0.3745948076248169, loss=1.4019241333007812
I0306 14:52:17.052081 139637464966912 logging_writer.py:48] [117900] global_step=117900, grad_norm=0.35986679792404175, loss=1.4048352241516113
I0306 14:52:52.204726 139637473359616 logging_writer.py:48] [118000] global_step=118000, grad_norm=0.37270641326904297, loss=1.4610154628753662
I0306 14:53:27.309646 139637464966912 logging_writer.py:48] [118100] global_step=118100, grad_norm=0.35900893807411194, loss=1.3876283168792725
I0306 14:54:02.422601 139637473359616 logging_writer.py:48] [118200] global_step=118200, grad_norm=0.35937219858169556, loss=1.4144994020462036
I0306 14:54:37.553389 139637464966912 logging_writer.py:48] [118300] global_step=118300, grad_norm=0.3615042269229889, loss=1.421834111213684
I0306 14:55:12.687077 139637473359616 logging_writer.py:48] [118400] global_step=118400, grad_norm=0.378378689289093, loss=1.4335227012634277
I0306 14:55:47.862157 139637464966912 logging_writer.py:48] [118500] global_step=118500, grad_norm=0.368964284658432, loss=1.376618504524231
I0306 14:56:22.995744 139637473359616 logging_writer.py:48] [118600] global_step=118600, grad_norm=0.3625063896179199, loss=1.328976035118103
I0306 14:56:58.105772 139637464966912 logging_writer.py:48] [118700] global_step=118700, grad_norm=0.37003064155578613, loss=1.3822959661483765
I0306 14:57:33.217402 139637473359616 logging_writer.py:48] [118800] global_step=118800, grad_norm=0.37218305468559265, loss=1.391776204109192
I0306 14:58:08.325568 139637464966912 logging_writer.py:48] [118900] global_step=118900, grad_norm=0.38905179500579834, loss=1.4140684604644775
I0306 14:58:43.430399 139637473359616 logging_writer.py:48] [119000] global_step=119000, grad_norm=0.35312357544898987, loss=1.4170286655426025
I0306 14:59:18.549240 139637464966912 logging_writer.py:48] [119100] global_step=119100, grad_norm=0.38721373677253723, loss=1.4764269590377808
I0306 14:59:53.691343 139637473359616 logging_writer.py:48] [119200] global_step=119200, grad_norm=0.3817214071750641, loss=1.4960013628005981
I0306 15:00:28.783679 139637464966912 logging_writer.py:48] [119300] global_step=119300, grad_norm=0.35785210132598877, loss=1.4299628734588623
I0306 15:01:03.912700 139637473359616 logging_writer.py:48] [119400] global_step=119400, grad_norm=0.3691585958003998, loss=1.4365417957305908
I0306 15:01:39.027689 139637464966912 logging_writer.py:48] [119500] global_step=119500, grad_norm=0.37959209084510803, loss=1.416243076324463
I0306 15:01:47.103569 139780644897984 spec.py:321] Evaluating on the training split.
I0306 15:01:49.746746 139780644897984 workload.py:181] Translating evaluation dataset.
I0306 15:05:46.339151 139780644897984 spec.py:333] Evaluating on the validation split.
I0306 15:05:48.979567 139780644897984 workload.py:181] Translating evaluation dataset.
I0306 15:08:19.513132 139780644897984 spec.py:349] Evaluating on the test split.
I0306 15:08:22.163471 139780644897984 workload.py:181] Translating evaluation dataset.
I0306 15:11:14.010857 139780644897984 submission_runner.py:469] Time since start: 71865.27s, 	Step: 119524, 	{'train/accuracy': 0.6956333518028259, 'train/loss': 1.4250198602676392, 'train/bleu': 35.59555212175863, 'validation/accuracy': 0.6926433444023132, 'validation/loss': 1.4123377799987793, 'validation/bleu': 30.838573364431188, 'validation/num_examples': 3000, 'test/accuracy': 0.7107172012329102, 'test/loss': 1.3154990673065186, 'test/bleu': 30.978145950859947, 'test/num_examples': 3003, 'score': 42027.04538106918, 'total_duration': 71865.27434062958, 'accumulated_submission_time': 42027.04538106918, 'accumulated_eval_time': 29830.180732011795, 'accumulated_logging_time': 1.284287929534912}
I0306 15:11:14.032940 139637473359616 logging_writer.py:48] [119524] accumulated_eval_time=29830.2, accumulated_logging_time=1.28429, accumulated_submission_time=42027, global_step=119524, preemption_count=0, score=42027, test/accuracy=0.710717, test/bleu=30.9781, test/loss=1.3155, test/num_examples=3003, total_duration=71865.3, train/accuracy=0.695633, train/bleu=35.5956, train/loss=1.42502, validation/accuracy=0.692643, validation/bleu=30.8386, validation/loss=1.41234, validation/num_examples=3000
I0306 15:11:41.017534 139637464966912 logging_writer.py:48] [119600] global_step=119600, grad_norm=0.3733934462070465, loss=1.4074227809906006
I0306 15:12:16.072962 139637473359616 logging_writer.py:48] [119700] global_step=119700, grad_norm=0.3650023341178894, loss=1.4388303756713867
I0306 15:12:51.158665 139637464966912 logging_writer.py:48] [119800] global_step=119800, grad_norm=0.38311320543289185, loss=1.4691897630691528
I0306 15:13:26.274137 139637473359616 logging_writer.py:48] [119900] global_step=119900, grad_norm=0.36675089597702026, loss=1.3742221593856812
I0306 15:14:01.393096 139637464966912 logging_writer.py:48] [120000] global_step=120000, grad_norm=0.3716508746147156, loss=1.5191892385482788
I0306 15:14:36.544873 139637473359616 logging_writer.py:48] [120100] global_step=120100, grad_norm=0.3713836073875427, loss=1.3404594659805298
I0306 15:15:11.711520 139637464966912 logging_writer.py:48] [120200] global_step=120200, grad_norm=0.3716641664505005, loss=1.4403917789459229
I0306 15:15:46.868847 139637473359616 logging_writer.py:48] [120300] global_step=120300, grad_norm=0.37821897864341736, loss=1.4660032987594604
I0306 15:16:22.010877 139637464966912 logging_writer.py:48] [120400] global_step=120400, grad_norm=0.3776293396949768, loss=1.3327621221542358
I0306 15:16:57.138309 139637473359616 logging_writer.py:48] [120500] global_step=120500, grad_norm=0.3729258179664612, loss=1.4518836736679077
I0306 15:17:32.280476 139637464966912 logging_writer.py:48] [120600] global_step=120600, grad_norm=0.3577098250389099, loss=1.3350169658660889
I0306 15:18:07.414305 139637473359616 logging_writer.py:48] [120700] global_step=120700, grad_norm=0.34854552149772644, loss=1.3667051792144775
I0306 15:18:42.594520 139637464966912 logging_writer.py:48] [120800] global_step=120800, grad_norm=0.38131994009017944, loss=1.4100098609924316
I0306 15:19:17.719090 139637473359616 logging_writer.py:48] [120900] global_step=120900, grad_norm=0.38155823945999146, loss=1.499983310699463
I0306 15:19:52.852682 139637464966912 logging_writer.py:48] [121000] global_step=121000, grad_norm=0.3771113455295563, loss=1.4078329801559448
I0306 15:20:28.006173 139637473359616 logging_writer.py:48] [121100] global_step=121100, grad_norm=0.3659559488296509, loss=1.4658676385879517
I0306 15:21:03.144941 139637464966912 logging_writer.py:48] [121200] global_step=121200, grad_norm=0.3811987042427063, loss=1.4338713884353638
I0306 15:21:38.255346 139637473359616 logging_writer.py:48] [121300] global_step=121300, grad_norm=0.37129223346710205, loss=1.4393612146377563
I0306 15:22:13.384278 139637464966912 logging_writer.py:48] [121400] global_step=121400, grad_norm=0.33978140354156494, loss=1.3401635885238647
I0306 15:22:48.513913 139637473359616 logging_writer.py:48] [121500] global_step=121500, grad_norm=0.3845112919807434, loss=1.4504668712615967
I0306 15:23:23.635339 139637464966912 logging_writer.py:48] [121600] global_step=121600, grad_norm=0.3709300458431244, loss=1.444719672203064
I0306 15:23:58.753393 139637473359616 logging_writer.py:48] [121700] global_step=121700, grad_norm=0.34604400396347046, loss=1.3636186122894287
I0306 15:24:33.905145 139637464966912 logging_writer.py:48] [121800] global_step=121800, grad_norm=0.36426639556884766, loss=1.390317440032959
I0306 15:25:09.039994 139637473359616 logging_writer.py:48] [121900] global_step=121900, grad_norm=0.3831237554550171, loss=1.460990309715271
I0306 15:25:14.317685 139780644897984 spec.py:321] Evaluating on the training split.
I0306 15:25:16.970209 139780644897984 workload.py:181] Translating evaluation dataset.
I0306 15:29:34.490657 139780644897984 spec.py:333] Evaluating on the validation split.
I0306 15:29:37.129580 139780644897984 workload.py:181] Translating evaluation dataset.
I0306 15:32:07.447598 139780644897984 spec.py:349] Evaluating on the test split.
I0306 15:32:10.086513 139780644897984 workload.py:181] Translating evaluation dataset.
I0306 15:35:00.908814 139780644897984 submission_runner.py:469] Time since start: 73292.17s, 	Step: 121916, 	{'train/accuracy': 0.694487988948822, 'train/loss': 1.4363741874694824, 'train/bleu': 35.27090909222089, 'validation/accuracy': 0.6925197243690491, 'validation/loss': 1.4123674631118774, 'validation/bleu': 30.830389522012513, 'validation/num_examples': 3000, 'test/accuracy': 0.7107867002487183, 'test/loss': 1.315580129623413, 'test/bleu': 30.981668317630366, 'test/num_examples': 3003, 'score': 42867.189265966415, 'total_duration': 73292.17225456238, 'accumulated_submission_time': 42867.189265966415, 'accumulated_eval_time': 30416.771754026413, 'accumulated_logging_time': 1.31484055519104}
I0306 15:35:00.930314 139637464966912 logging_writer.py:48] [121916] accumulated_eval_time=30416.8, accumulated_logging_time=1.31484, accumulated_submission_time=42867.2, global_step=121916, preemption_count=0, score=42867.2, test/accuracy=0.710787, test/bleu=30.9817, test/loss=1.31558, test/num_examples=3003, total_duration=73292.2, train/accuracy=0.694488, train/bleu=35.2709, train/loss=1.43637, validation/accuracy=0.69252, validation/bleu=30.8304, validation/loss=1.41237, validation/num_examples=3000
I0306 15:35:30.705785 139637473359616 logging_writer.py:48] [122000] global_step=122000, grad_norm=0.3541545569896698, loss=1.3787528276443481
I0306 15:36:05.782721 139637464966912 logging_writer.py:48] [122100] global_step=122100, grad_norm=0.36258694529533386, loss=1.4072448015213013
I0306 15:36:40.864686 139637473359616 logging_writer.py:48] [122200] global_step=122200, grad_norm=0.3590201139450073, loss=1.4617313146591187
I0306 15:37:15.978478 139637464966912 logging_writer.py:48] [122300] global_step=122300, grad_norm=0.3758128583431244, loss=1.466953158378601
I0306 15:37:51.092135 139637473359616 logging_writer.py:48] [122400] global_step=122400, grad_norm=0.3666767179965973, loss=1.44317626953125
I0306 15:38:26.220051 139637464966912 logging_writer.py:48] [122500] global_step=122500, grad_norm=0.3781803250312805, loss=1.4894206523895264
I0306 15:39:01.345283 139637473359616 logging_writer.py:48] [122600] global_step=122600, grad_norm=0.37013471126556396, loss=1.4136203527450562
I0306 15:39:36.448280 139637464966912 logging_writer.py:48] [122700] global_step=122700, grad_norm=0.34781068563461304, loss=1.3967350721359253
I0306 15:40:11.552421 139637473359616 logging_writer.py:48] [122800] global_step=122800, grad_norm=0.359023779630661, loss=1.3183603286743164
I0306 15:40:46.698514 139637464966912 logging_writer.py:48] [122900] global_step=122900, grad_norm=0.3626358211040497, loss=1.4156434535980225
I0306 15:41:21.835918 139637473359616 logging_writer.py:48] [123000] global_step=123000, grad_norm=0.3612171411514282, loss=1.3975523710250854
I0306 15:41:56.917733 139637464966912 logging_writer.py:48] [123100] global_step=123100, grad_norm=0.37007394433021545, loss=1.3147825002670288
I0306 15:42:32.013082 139637473359616 logging_writer.py:48] [123200] global_step=123200, grad_norm=0.3698906898498535, loss=1.467551827430725
I0306 15:43:07.124624 139637464966912 logging_writer.py:48] [123300] global_step=123300, grad_norm=0.35624057054519653, loss=1.4585942029953003
I0306 15:43:42.239253 139637473359616 logging_writer.py:48] [123400] global_step=123400, grad_norm=0.37751394510269165, loss=1.4512808322906494
I0306 15:44:17.366120 139637464966912 logging_writer.py:48] [123500] global_step=123500, grad_norm=0.3706016540527344, loss=1.4830471277236938
I0306 15:44:52.485170 139637473359616 logging_writer.py:48] [123600] global_step=123600, grad_norm=0.36591997742652893, loss=1.4660159349441528
I0306 15:45:27.588002 139637464966912 logging_writer.py:48] [123700] global_step=123700, grad_norm=0.3594145178794861, loss=1.4282556772232056
I0306 15:46:02.696060 139637473359616 logging_writer.py:48] [123800] global_step=123800, grad_norm=0.3549315631389618, loss=1.4270763397216797
I0306 15:46:37.797321 139637464966912 logging_writer.py:48] [123900] global_step=123900, grad_norm=0.3770190477371216, loss=1.4979678392410278
I0306 15:47:12.906100 139637473359616 logging_writer.py:48] [124000] global_step=124000, grad_norm=0.3664807081222534, loss=1.4836061000823975
I0306 15:47:47.995186 139637464966912 logging_writer.py:48] [124100] global_step=124100, grad_norm=0.39618414640426636, loss=1.5212857723236084
I0306 15:48:23.096805 139637473359616 logging_writer.py:48] [124200] global_step=124200, grad_norm=0.37287190556526184, loss=1.4252830743789673
I0306 15:48:58.214473 139637464966912 logging_writer.py:48] [124300] global_step=124300, grad_norm=0.3664446771144867, loss=1.4058961868286133
I0306 15:49:01.055011 139637473359616 logging_writer.py:48] [124309] global_step=124309, preemption_count=0, score=43707.2
I0306 15:49:01.081644 139780644897984 submission_runner.py:646] Tuning trial 4/5
I0306 15:49:01.081802 139780644897984 submission_runner.py:647] Hyperparameters: Hyperparameters(dropout_rate=0.0, label_smoothing=0.0, learning_rate=0.004958460849689891, one_minus_beta1=0.13625575743, beta2=0.6291854735396584, weight_decay=0.1147386261512052, warmup_factor=0.02)
I0306 15:49:01.084261 139780644897984 submission_runner.py:648] Metrics: {'eval_results': [(1, {'train/accuracy': 0.0004679510893765837, 'train/loss': 11.170289993286133, 'train/bleu': 3.872421061078569e-11, 'validation/accuracy': 0.00048204089398495853, 'validation/loss': 11.143509864807129, 'validation/bleu': 6.893376491104542e-10, 'validation/num_examples': 3000, 'test/accuracy': 0.0007183408597484231, 'test/loss': 11.128133773803711, 'test/bleu': 0.0, 'test/num_examples': 3003, 'score': 26.683835744857788, 'total_duration': 951.6253178119659, 'accumulated_submission_time': 26.683835744857788, 'accumulated_eval_time': 924.9413712024689, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (2385, {'train/accuracy': 0.5247663259506226, 'train/loss': 2.6697239875793457, 'train/bleu': 22.93314766070593, 'validation/accuracy': 0.5305787324905396, 'validation/loss': 2.5991995334625244, 'validation/bleu': 18.903940142801527, 'validation/num_examples': 3000, 'test/accuracy': 0.5276561379432678, 'test/loss': 2.636319637298584, 'test/bleu': 17.27146107014147, 'test/num_examples': 3003, 'score': 866.7110114097595, 'total_duration': 2390.9100246429443, 'accumulated_submission_time': 866.7110114097595, 'accumulated_eval_time': 1524.0112206935883, 'accumulated_logging_time': 0.01844644546508789, 'global_step': 2385, 'preemption_count': 0}), (4773, {'train/accuracy': 0.5778889060020447, 'train/loss': 2.235605478286743, 'train/bleu': 27.103742486123544, 'validation/accuracy': 0.5906607508659363, 'validation/loss': 2.106868028640747, 'validation/bleu': 23.557852701607683, 'validation/num_examples': 3000, 'test/accuracy': 0.5977870225906372, 'test/loss': 2.067180871963501, 'test/bleu': 22.192404404184018, 'test/num_examples': 3003, 'score': 1706.8607261180878, 'total_duration': 3753.970328092575, 'accumulated_submission_time': 1706.8607261180878, 'accumulated_eval_time': 2046.7449560165405, 'accumulated_logging_time': 0.03565263748168945, 'global_step': 4773, 'preemption_count': 0}), (7163, {'train/accuracy': 0.5905581116676331, 'train/loss': 2.1190900802612305, 'train/bleu': 27.445955000969672, 'validation/accuracy': 0.6041455864906311, 'validation/loss': 2.01488995552063, 'validation/bleu': 24.608990774898974, 'validation/num_examples': 3000, 'test/accuracy': 0.6073224544525146, 'test/loss': 1.98634934425354, 'test/bleu': 22.99201854686428, 'test/num_examples': 3003, 'score': 2546.894848585129, 'total_duration': 5194.542275667191, 'accumulated_submission_time': 2546.894848585129, 'accumulated_eval_time': 2647.105609893799, 'accumulated_logging_time': 0.055379629135131836, 'global_step': 7163, 'preemption_count': 0}), (9556, {'train/accuracy': 0.5890974998474121, 'train/loss': 2.14538311958313, 'train/bleu': 27.825029318378128, 'validation/accuracy': 0.6090895533561707, 'validation/loss': 1.9785903692245483, 'validation/bleu': 24.078346317569707, 'validation/num_examples': 3000, 'test/accuracy': 0.6140424013137817, 'test/loss': 1.9433308839797974, 'test/bleu': 22.826923148097105, 'test/num_examples': 3003, 'score': 3386.886279821396, 'total_duration': 6531.68909573555, 'accumulated_submission_time': 3386.886279821396, 'accumulated_eval_time': 3144.086328983307, 'accumulated_logging_time': 0.07317018508911133, 'global_step': 9556, 'preemption_count': 0}), (11951, {'train/accuracy': 0.5928311347961426, 'train/loss': 2.1261472702026367, 'train/bleu': 28.71801485509682, 'validation/accuracy': 0.6152077913284302, 'validation/loss': 1.9476748704910278, 'validation/bleu': 25.019054489361487, 'validation/num_examples': 3000, 'test/accuracy': 0.6206580996513367, 'test/loss': 1.8979413509368896, 'test/bleu': 23.93205123087907, 'test/num_examples': 3003, 'score': 4226.929615020752, 'total_duration': 7939.030020713806, 'accumulated_submission_time': 4226.929615020752, 'accumulated_eval_time': 3711.210105419159, 'accumulated_logging_time': 0.09382152557373047, 'global_step': 11951, 'preemption_count': 0}), (14342, {'train/accuracy': 0.5996617674827576, 'train/loss': 2.0683419704437256, 'train/bleu': 27.678183844809066, 'validation/accuracy': 0.6153560876846313, 'validation/loss': 1.9270122051239014, 'validation/bleu': 24.81260005652625, 'validation/num_examples': 3000, 'test/accuracy': 0.6220600008964539, 'test/loss': 1.8860790729522705, 'test/bleu': 23.592547791068764, 'test/num_examples': 3003, 'score': 5067.073660612106, 'total_duration': 9291.968108415604, 'accumulated_submission_time': 5067.073660612106, 'accumulated_eval_time': 4223.838396072388, 'accumulated_logging_time': 0.11299443244934082, 'global_step': 14342, 'preemption_count': 0}), (16731, {'train/accuracy': 0.6019520163536072, 'train/loss': 2.053189516067505, 'train/bleu': 27.9792749992159, 'validation/accuracy': 0.617741584777832, 'validation/loss': 1.9281656742095947, 'validation/bleu': 24.760776842539137, 'validation/num_examples': 3000, 'test/accuracy': 0.6216197609901428, 'test/loss': 1.8789961338043213, 'test/bleu': 23.88516098841343, 'test/num_examples': 3003, 'score': 5906.990766048431, 'total_duration': 10647.155527353287, 'accumulated_submission_time': 5906.990766048431, 'accumulated_eval_time': 4738.958157539368, 'accumulated_logging_time': 0.13117384910583496, 'global_step': 16731, 'preemption_count': 0}), (19120, {'train/accuracy': 0.6120039224624634, 'train/loss': 1.9472448825836182, 'train/bleu': 29.03631012554525, 'validation/accuracy': 0.6171359419822693, 'validation/loss': 1.9207147359848022, 'validation/bleu': 25.140309653101635, 'validation/num_examples': 3000, 'test/accuracy': 0.6252577900886536, 'test/loss': 1.8584861755371094, 'test/bleu': 23.881801119375208, 'test/num_examples': 3003, 'score': 6746.903298616409, 'total_duration': 11997.952909231186, 'accumulated_submission_time': 6746.903298616409, 'accumulated_eval_time': 5249.689759731293, 'accumulated_logging_time': 0.14988160133361816, 'global_step': 19120, 'preemption_count': 0}), (21501, {'train/accuracy': 0.6029502153396606, 'train/loss': 2.051133871078491, 'train/bleu': 28.826967214554323, 'validation/accuracy': 0.6226856112480164, 'validation/loss': 1.884505033493042, 'validation/bleu': 25.488042749944302, 'validation/num_examples': 3000, 'test/accuracy': 0.6279805302619934, 'test/loss': 1.838364839553833, 'test/bleu': 24.15120927112284, 'test/num_examples': 3003, 'score': 7587.070657968521, 'total_duration': 13455.559773921967, 'accumulated_submission_time': 7587.070657968521, 'accumulated_eval_time': 5866.973753452301, 'accumulated_logging_time': 0.17002391815185547, 'global_step': 21501, 'preemption_count': 0}), (23882, {'train/accuracy': 0.6027191281318665, 'train/loss': 2.0590901374816895, 'train/bleu': 28.5559862475143, 'validation/accuracy': 0.621795654296875, 'validation/loss': 1.8877594470977783, 'validation/bleu': 25.326515955964172, 'validation/num_examples': 3000, 'test/accuracy': 0.6278530955314636, 'test/loss': 1.838442325592041, 'test/bleu': 24.210287195888444, 'test/num_examples': 3003, 'score': 8427.136903524399, 'total_duration': 14809.463087320328, 'accumulated_submission_time': 8427.136903524399, 'accumulated_eval_time': 6380.6570925712585, 'accumulated_logging_time': 0.1900310516357422, 'global_step': 23882, 'preemption_count': 0}), (26267, {'train/accuracy': 0.6071869134902954, 'train/loss': 2.003427267074585, 'train/bleu': 28.822366695578346, 'validation/accuracy': 0.6217215061187744, 'validation/loss': 1.8891798257827759, 'validation/bleu': 25.516520317739126, 'validation/num_examples': 3000, 'test/accuracy': 0.6298342943191528, 'test/loss': 1.8408453464508057, 'test/bleu': 24.458808111802, 'test/num_examples': 3003, 'score': 9267.046485185623, 'total_duration': 16163.887969017029, 'accumulated_submission_time': 9267.046485185623, 'accumulated_eval_time': 6895.016014814377, 'accumulated_logging_time': 0.21158742904663086, 'global_step': 26267, 'preemption_count': 0}), (28657, {'train/accuracy': 0.6039182543754578, 'train/loss': 2.0322377681732178, 'train/bleu': 28.221845513422554, 'validation/accuracy': 0.6241193413734436, 'validation/loss': 1.873083233833313, 'validation/bleu': 25.380839915586105, 'validation/num_examples': 3000, 'test/accuracy': 0.6312941908836365, 'test/loss': 1.8165948390960693, 'test/bleu': 24.48678844159008, 'test/num_examples': 3003, 'score': 10106.934731721878, 'total_duration': 17545.771904230118, 'accumulated_submission_time': 10106.934731721878, 'accumulated_eval_time': 7436.855900287628, 'accumulated_logging_time': 0.23226642608642578, 'global_step': 28657, 'preemption_count': 0}), (31046, {'train/accuracy': 0.6066698431968689, 'train/loss': 2.0188980102539062, 'train/bleu': 28.48382200303516, 'validation/accuracy': 0.6256025433540344, 'validation/loss': 1.8642661571502686, 'validation/bleu': 25.232810435092556, 'validation/num_examples': 3000, 'test/accuracy': 0.6352682113647461, 'test/loss': 1.8046759366989136, 'test/bleu': 24.57316360352151, 'test/num_examples': 3003, 'score': 10946.866646051407, 'total_duration': 18910.53384923935, 'accumulated_submission_time': 10946.866646051407, 'accumulated_eval_time': 7961.52839589119, 'accumulated_logging_time': 0.25343918800354004, 'global_step': 31046, 'preemption_count': 0}), (33430, {'train/accuracy': 0.6100131273269653, 'train/loss': 1.9983563423156738, 'train/bleu': 28.812939268679422, 'validation/accuracy': 0.6261340379714966, 'validation/loss': 1.845545768737793, 'validation/bleu': 25.829190279812533, 'validation/num_examples': 3000, 'test/accuracy': 0.6353608965873718, 'test/loss': 1.8041815757751465, 'test/bleu': 25.18249660928717, 'test/num_examples': 3003, 'score': 11786.987345457077, 'total_duration': 20345.79505467415, 'accumulated_submission_time': 11786.987345457077, 'accumulated_eval_time': 8556.514425039291, 'accumulated_logging_time': 0.27521753311157227, 'global_step': 33430, 'preemption_count': 0}), (35820, {'train/accuracy': 0.6073586940765381, 'train/loss': 2.0146327018737793, 'train/bleu': 28.787296842570566, 'validation/accuracy': 0.6300398111343384, 'validation/loss': 1.8302057981491089, 'validation/bleu': 25.98764954646736, 'validation/num_examples': 3000, 'test/accuracy': 0.637481153011322, 'test/loss': 1.7810845375061035, 'test/bleu': 24.675242743362375, 'test/num_examples': 3003, 'score': 12626.878019809723, 'total_duration': 21736.252999782562, 'accumulated_submission_time': 12626.878019809723, 'accumulated_eval_time': 9106.920868635178, 'accumulated_logging_time': 0.2985553741455078, 'global_step': 35820, 'preemption_count': 0}), (38209, {'train/accuracy': 0.6203847527503967, 'train/loss': 1.8942300081253052, 'train/bleu': 29.396835483410868, 'validation/accuracy': 0.6301263570785522, 'validation/loss': 1.8302288055419922, 'validation/bleu': 25.90954802967947, 'validation/num_examples': 3000, 'test/accuracy': 0.6349901556968689, 'test/loss': 1.7912007570266724, 'test/bleu': 24.69379120789673, 'test/num_examples': 3003, 'score': 13466.73539853096, 'total_duration': 23103.97078180313, 'accumulated_submission_time': 13466.73539853096, 'accumulated_eval_time': 9634.624348402023, 'accumulated_logging_time': 0.3203768730163574, 'global_step': 38209, 'preemption_count': 0}), (40600, {'train/accuracy': 0.6077876687049866, 'train/loss': 1.993822455406189, 'train/bleu': 28.801807403206897, 'validation/accuracy': 0.6319679617881775, 'validation/loss': 1.8177428245544434, 'validation/bleu': 26.21881731838676, 'validation/num_examples': 3000, 'test/accuracy': 0.6421272158622742, 'test/loss': 1.7599787712097168, 'test/bleu': 25.16565133273084, 'test/num_examples': 3003, 'score': 14306.757709264755, 'total_duration': 24625.96693944931, 'accumulated_submission_time': 14306.757709264755, 'accumulated_eval_time': 10316.443854570389, 'accumulated_logging_time': 0.3419156074523926, 'global_step': 40600, 'preemption_count': 0}), (42989, {'train/accuracy': 0.6141082644462585, 'train/loss': 1.9725170135498047, 'train/bleu': 28.9625056156541, 'validation/accuracy': 0.6335995197296143, 'validation/loss': 1.8045575618743896, 'validation/bleu': 26.28467386079833, 'validation/num_examples': 3000, 'test/accuracy': 0.6410149335861206, 'test/loss': 1.7512112855911255, 'test/bleu': 25.30449615203908, 'test/num_examples': 3003, 'score': 15146.758295536041, 'total_duration': 26020.840336561203, 'accumulated_submission_time': 15146.758295536041, 'accumulated_eval_time': 10871.156679868698, 'accumulated_logging_time': 0.3637692928314209, 'global_step': 42989, 'preemption_count': 0}), (45383, {'train/accuracy': 0.6164857745170593, 'train/loss': 1.9382786750793457, 'train/bleu': 29.11654390962891, 'validation/accuracy': 0.6342793107032776, 'validation/loss': 1.7984269857406616, 'validation/bleu': 26.248522204476153, 'validation/num_examples': 3000, 'test/accuracy': 0.6431931257247925, 'test/loss': 1.738349199295044, 'test/bleu': 25.20431710493032, 'test/num_examples': 3003, 'score': 15986.81736421585, 'total_duration': 27464.238711595535, 'accumulated_submission_time': 15986.81736421585, 'accumulated_eval_time': 11474.33982682228, 'accumulated_logging_time': 0.38511180877685547, 'global_step': 45383, 'preemption_count': 0}), (47780, {'train/accuracy': 0.6148956418037415, 'train/loss': 1.9567922353744507, 'train/bleu': 29.69566775485104, 'validation/accuracy': 0.6333893537521362, 'validation/loss': 1.7925602197647095, 'validation/bleu': 26.410435676635764, 'validation/num_examples': 3000, 'test/accuracy': 0.6440505385398865, 'test/loss': 1.7370941638946533, 'test/bleu': 25.275655385631534, 'test/num_examples': 3003, 'score': 16826.967888355255, 'total_duration': 28872.330182552338, 'accumulated_submission_time': 16826.967888355255, 'accumulated_eval_time': 12042.125931501389, 'accumulated_logging_time': 0.4067695140838623, 'global_step': 47780, 'preemption_count': 0}), (50176, {'train/accuracy': 0.6177985072135925, 'train/loss': 1.9343087673187256, 'train/bleu': 29.218852992000112, 'validation/accuracy': 0.63688725233078, 'validation/loss': 1.7758233547210693, 'validation/bleu': 26.168164404893616, 'validation/num_examples': 3000, 'test/accuracy': 0.6449773907661438, 'test/loss': 1.7257072925567627, 'test/bleu': 25.12729817791399, 'test/num_examples': 3003, 'score': 17666.948279619217, 'total_duration': 30290.534225940704, 'accumulated_submission_time': 17666.948279619217, 'accumulated_eval_time': 12620.193596363068, 'accumulated_logging_time': 0.42921018600463867, 'global_step': 50176, 'preemption_count': 0}), (52566, {'train/accuracy': 0.6242986917495728, 'train/loss': 1.8886134624481201, 'train/bleu': 29.056832370245576, 'validation/accuracy': 0.6402491927146912, 'validation/loss': 1.755562424659729, 'validation/bleu': 26.479100189923333, 'validation/num_examples': 3000, 'test/accuracy': 0.6487544775009155, 'test/loss': 1.7023996114730835, 'test/bleu': 25.58458299916109, 'test/num_examples': 3003, 'score': 18506.825142383575, 'total_duration': 31682.467784643173, 'accumulated_submission_time': 18506.825142383575, 'accumulated_eval_time': 13172.0923371315, 'accumulated_logging_time': 0.4538750648498535, 'global_step': 52566, 'preemption_count': 0}), (54957, {'train/accuracy': 0.6213428974151611, 'train/loss': 1.9050761461257935, 'train/bleu': 30.01752305387121, 'validation/accuracy': 0.6413492560386658, 'validation/loss': 1.7537524700164795, 'validation/bleu': 26.549013811113188, 'validation/num_examples': 3000, 'test/accuracy': 0.6496466398239136, 'test/loss': 1.6951619386672974, 'test/bleu': 25.780822763996838, 'test/num_examples': 3003, 'score': 19346.699776649475, 'total_duration': 33084.819929122925, 'accumulated_submission_time': 19346.699776649475, 'accumulated_eval_time': 13734.412126541138, 'accumulated_logging_time': 0.4764060974121094, 'global_step': 54957, 'preemption_count': 0}), (57348, {'train/accuracy': 0.6256750226020813, 'train/loss': 1.855641484260559, 'train/bleu': 29.939524256442464, 'validation/accuracy': 0.6423627734184265, 'validation/loss': 1.7347811460494995, 'validation/bleu': 26.631667853075847, 'validation/num_examples': 3000, 'test/accuracy': 0.6516278386116028, 'test/loss': 1.6796623468399048, 'test/bleu': 26.455345305136365, 'test/num_examples': 3003, 'score': 20186.733825206757, 'total_duration': 34521.89927506447, 'accumulated_submission_time': 20186.733825206757, 'accumulated_eval_time': 14331.30164551735, 'accumulated_logging_time': 0.4990816116333008, 'global_step': 57348, 'preemption_count': 0}), (59739, {'train/accuracy': 0.6223832368850708, 'train/loss': 1.903565764427185, 'train/bleu': 29.0736830086834, 'validation/accuracy': 0.6441178917884827, 'validation/loss': 1.7266254425048828, 'validation/bleu': 26.47571224019692, 'validation/num_examples': 3000, 'test/accuracy': 0.6523346304893494, 'test/loss': 1.6676288843154907, 'test/bleu': 25.343149795552087, 'test/num_examples': 3003, 'score': 21026.65973854065, 'total_duration': 35911.3907866478, 'accumulated_submission_time': 21026.65973854065, 'accumulated_eval_time': 14880.711547613144, 'accumulated_logging_time': 0.5244557857513428, 'global_step': 59739, 'preemption_count': 0}), (62130, {'train/accuracy': 0.629525899887085, 'train/loss': 1.8580882549285889, 'train/bleu': 30.115997303002217, 'validation/accuracy': 0.6494203209877014, 'validation/loss': 1.6964271068572998, 'validation/bleu': 27.13753839038238, 'validation/num_examples': 3000, 'test/accuracy': 0.6586953997612, 'test/loss': 1.6383004188537598, 'test/bleu': 26.551777613555267, 'test/num_examples': 3003, 'score': 21866.7843477726, 'total_duration': 37460.23810696602, 'accumulated_submission_time': 21866.7843477726, 'accumulated_eval_time': 15589.276465177536, 'accumulated_logging_time': 0.5475189685821533, 'global_step': 62130, 'preemption_count': 0}), (64526, {'train/accuracy': 0.631919801235199, 'train/loss': 1.8332370519638062, 'train/bleu': 30.10764812044717, 'validation/accuracy': 0.6493709087371826, 'validation/loss': 1.684674620628357, 'validation/bleu': 27.036690597365407, 'validation/num_examples': 3000, 'test/accuracy': 0.6584520936012268, 'test/loss': 1.6274995803833008, 'test/bleu': 26.280884335373482, 'test/num_examples': 3003, 'score': 22706.800669431686, 'total_duration': 38871.131801366806, 'accumulated_submission_time': 22706.800669431686, 'accumulated_eval_time': 16159.996134757996, 'accumulated_logging_time': 0.5718483924865723, 'global_step': 64526, 'preemption_count': 0}), (66923, {'train/accuracy': 0.6317277550697327, 'train/loss': 1.8329476118087769, 'train/bleu': 30.41384039937238, 'validation/accuracy': 0.6514968276023865, 'validation/loss': 1.6818063259124756, 'validation/bleu': 27.42363948793524, 'validation/num_examples': 3000, 'test/accuracy': 0.660398542881012, 'test/loss': 1.6205717325210571, 'test/bleu': 26.482613775499154, 'test/num_examples': 3003, 'score': 23546.947669029236, 'total_duration': 40242.10146975517, 'accumulated_submission_time': 23546.947669029236, 'accumulated_eval_time': 16690.66118168831, 'accumulated_logging_time': 0.596975564956665, 'global_step': 66923, 'preemption_count': 0}), (69318, {'train/accuracy': 0.6512231826782227, 'train/loss': 1.675499439239502, 'train/bleu': 31.225384018976147, 'validation/accuracy': 0.6507057547569275, 'validation/loss': 1.6675829887390137, 'validation/bleu': 27.576943731066336, 'validation/num_examples': 3000, 'test/accuracy': 0.662009060382843, 'test/loss': 1.6103739738464355, 'test/bleu': 26.587290454280655, 'test/num_examples': 3003, 'score': 24386.8045296669, 'total_duration': 41674.018525123596, 'accumulated_submission_time': 24386.8045296669, 'accumulated_eval_time': 17282.56668806076, 'accumulated_logging_time': 0.6208243370056152, 'global_step': 69318, 'preemption_count': 0}), (71709, {'train/accuracy': 0.6380018591880798, 'train/loss': 1.784746527671814, 'train/bleu': 31.115631648645245, 'validation/accuracy': 0.6546115279197693, 'validation/loss': 1.6529085636138916, 'validation/bleu': 27.843570385245886, 'validation/num_examples': 3000, 'test/accuracy': 0.6642103791236877, 'test/loss': 1.5897046327590942, 'test/bleu': 27.33826033335166, 'test/num_examples': 3003, 'score': 25226.702499628067, 'total_duration': 43085.14022421837, 'accumulated_submission_time': 25226.702499628067, 'accumulated_eval_time': 17853.635200738907, 'accumulated_logging_time': 0.6452395915985107, 'global_step': 71709, 'preemption_count': 0}), (74091, {'train/accuracy': 0.634438693523407, 'train/loss': 1.814536690711975, 'train/bleu': 30.601603241312475, 'validation/accuracy': 0.6576026678085327, 'validation/loss': 1.6407557725906372, 'validation/bleu': 27.658826333780933, 'validation/num_examples': 3000, 'test/accuracy': 0.6680801510810852, 'test/loss': 1.5713880062103271, 'test/bleu': 26.986297745060345, 'test/num_examples': 3003, 'score': 26066.805859088898, 'total_duration': 44545.31275200844, 'accumulated_submission_time': 26066.805859088898, 'accumulated_eval_time': 18473.471185207367, 'accumulated_logging_time': 0.7442440986633301, 'global_step': 74091, 'preemption_count': 0}), (76474, {'train/accuracy': 0.6419686079025269, 'train/loss': 1.749300241470337, 'train/bleu': 31.50837936251988, 'validation/accuracy': 0.6599140167236328, 'validation/loss': 1.621609091758728, 'validation/bleu': 28.044321730944986, 'validation/num_examples': 3000, 'test/accuracy': 0.6724249720573425, 'test/loss': 1.549238920211792, 'test/bleu': 27.67812647667077, 'test/num_examples': 3003, 'score': 26906.741980314255, 'total_duration': 45975.75192594528, 'accumulated_submission_time': 26906.741980314255, 'accumulated_eval_time': 19063.819647789, 'accumulated_logging_time': 0.7711005210876465, 'global_step': 76474, 'preemption_count': 0}), (78856, {'train/accuracy': 0.6438496112823486, 'train/loss': 1.74861741065979, 'train/bleu': 31.350223121957583, 'validation/accuracy': 0.662571370601654, 'validation/loss': 1.606427788734436, 'validation/bleu': 28.417334527765732, 'validation/num_examples': 3000, 'test/accuracy': 0.6764453649520874, 'test/loss': 1.5313351154327393, 'test/bleu': 28.10991007755014, 'test/num_examples': 3003, 'score': 27746.82996249199, 'total_duration': 47362.69472885132, 'accumulated_submission_time': 27746.82996249199, 'accumulated_eval_time': 19610.51661515236, 'accumulated_logging_time': 0.7983317375183105, 'global_step': 78856, 'preemption_count': 0}), (81239, {'train/accuracy': 0.64265376329422, 'train/loss': 1.7509026527404785, 'train/bleu': 32.220551993150394, 'validation/accuracy': 0.6659456491470337, 'validation/loss': 1.587671160697937, 'validation/bleu': 28.620837428903414, 'validation/num_examples': 3000, 'test/accuracy': 0.6775808334350586, 'test/loss': 1.5124707221984863, 'test/bleu': 28.293153752275455, 'test/num_examples': 3003, 'score': 28586.68051147461, 'total_duration': 48829.02612519264, 'accumulated_submission_time': 28586.68051147461, 'accumulated_eval_time': 20236.8411860466, 'accumulated_logging_time': 0.8261919021606445, 'global_step': 81239, 'preemption_count': 0}), (83630, {'train/accuracy': 0.6515222787857056, 'train/loss': 1.6892231702804565, 'train/bleu': 32.33494239430436, 'validation/accuracy': 0.6673052906990051, 'validation/loss': 1.5686843395233154, 'validation/bleu': 28.880505117844393, 'validation/num_examples': 3000, 'test/accuracy': 0.6784497499465942, 'test/loss': 1.5015387535095215, 'test/bleu': 28.312973817042256, 'test/num_examples': 3003, 'score': 29426.83834552765, 'total_duration': 50319.288940906525, 'accumulated_submission_time': 29426.83834552765, 'accumulated_eval_time': 20886.785312891006, 'accumulated_logging_time': 0.853816032409668, 'global_step': 83630, 'preemption_count': 0}), (86026, {'train/accuracy': 0.6517990231513977, 'train/loss': 1.7017759084701538, 'train/bleu': 31.434034272260902, 'validation/accuracy': 0.6699256300926208, 'validation/loss': 1.5585486888885498, 'validation/bleu': 29.043153992479454, 'validation/num_examples': 3000, 'test/accuracy': 0.6815896034240723, 'test/loss': 1.4844871759414673, 'test/bleu': 28.32085368603158, 'test/num_examples': 3003, 'score': 30266.723168373108, 'total_duration': 51745.35295557976, 'accumulated_submission_time': 30266.723168373108, 'accumulated_eval_time': 21472.80336213112, 'accumulated_logging_time': 0.8815252780914307, 'global_step': 86026, 'preemption_count': 0}), (88423, {'train/accuracy': 0.6646078824996948, 'train/loss': 1.6003464460372925, 'train/bleu': 32.95042963409473, 'validation/accuracy': 0.6726201176643372, 'validation/loss': 1.5404685735702515, 'validation/bleu': 29.19816262548214, 'validation/num_examples': 3000, 'test/accuracy': 0.6854246258735657, 'test/loss': 1.463981032371521, 'test/bleu': 28.672310872292798, 'test/num_examples': 3003, 'score': 31106.80856180191, 'total_duration': 53121.84323501587, 'accumulated_submission_time': 31106.80856180191, 'accumulated_eval_time': 22009.053982019424, 'accumulated_logging_time': 0.9083998203277588, 'global_step': 88423, 'preemption_count': 0}), (90815, {'train/accuracy': 0.6605445742607117, 'train/loss': 1.6354273557662964, 'train/bleu': 32.13151031115447, 'validation/accuracy': 0.6734234690666199, 'validation/loss': 1.5262573957443237, 'validation/bleu': 29.212576086568806, 'validation/num_examples': 3000, 'test/accuracy': 0.6882516741752625, 'test/loss': 1.4436302185058594, 'test/bleu': 28.73777890130112, 'test/num_examples': 3003, 'score': 31946.95850801468, 'total_duration': 54623.8608379364, 'accumulated_submission_time': 31946.95850801468, 'accumulated_eval_time': 22670.76473593712, 'accumulated_logging_time': 0.935175895690918, 'global_step': 90815, 'preemption_count': 0}), (93207, {'train/accuracy': 0.6573771834373474, 'train/loss': 1.6538136005401611, 'train/bleu': 32.298785871225796, 'validation/accuracy': 0.67620450258255, 'validation/loss': 1.511754035949707, 'validation/bleu': 29.471954908641102, 'validation/num_examples': 3000, 'test/accuracy': 0.6913799047470093, 'test/loss': 1.4219073057174683, 'test/bleu': 29.399890740732395, 'test/num_examples': 3003, 'score': 32787.12175369263, 'total_duration': 56065.54461526871, 'accumulated_submission_time': 32787.12175369263, 'accumulated_eval_time': 23272.129132270813, 'accumulated_logging_time': 0.9633364677429199, 'global_step': 93207, 'preemption_count': 0}), (95600, {'train/accuracy': 0.6688386797904968, 'train/loss': 1.5782347917556763, 'train/bleu': 33.27323988757846, 'validation/accuracy': 0.6803698539733887, 'validation/loss': 1.4902026653289795, 'validation/bleu': 29.742770265849412, 'validation/num_examples': 3000, 'test/accuracy': 0.6924574375152588, 'test/loss': 1.4117414951324463, 'test/bleu': 29.368151843750923, 'test/num_examples': 3003, 'score': 33627.111221790314, 'total_duration': 57544.81092381477, 'accumulated_submission_time': 33627.111221790314, 'accumulated_eval_time': 23911.250597715378, 'accumulated_logging_time': 0.9911746978759766, 'global_step': 95600, 'preemption_count': 0}), (97991, {'train/accuracy': 0.6652668118476868, 'train/loss': 1.6025745868682861, 'train/bleu': 32.74790467887688, 'validation/accuracy': 0.6798754334449768, 'validation/loss': 1.4846218824386597, 'validation/bleu': 29.92811404282832, 'validation/num_examples': 3000, 'test/accuracy': 0.696454644203186, 'test/loss': 1.395054817199707, 'test/bleu': 29.669121398385812, 'test/num_examples': 3003, 'score': 34466.99495434761, 'total_duration': 58971.81566500664, 'accumulated_submission_time': 34466.99495434761, 'accumulated_eval_time': 24498.21498775482, 'accumulated_logging_time': 1.0186359882354736, 'global_step': 97991, 'preemption_count': 0}), (100383, {'train/accuracy': 0.7098280191421509, 'train/loss': 1.3569326400756836, 'train/bleu': 36.33429913693139, 'validation/accuracy': 0.6833362579345703, 'validation/loss': 1.4667052030563354, 'validation/bleu': 29.852403005562365, 'validation/num_examples': 3000, 'test/accuracy': 0.6978797316551208, 'test/loss': 1.3814998865127563, 'test/bleu': 30.161131676454566, 'test/num_examples': 3003, 'score': 35307.034719944, 'total_duration': 60360.56320953369, 'accumulated_submission_time': 35307.034719944, 'accumulated_eval_time': 25046.765909910202, 'accumulated_logging_time': 1.0459561347961426, 'global_step': 100383, 'preemption_count': 0}), (102773, {'train/accuracy': 0.678148627281189, 'train/loss': 1.5241607427597046, 'train/bleu': 34.77623422627334, 'validation/accuracy': 0.6866858005523682, 'validation/loss': 1.4503389596939087, 'validation/bleu': 30.23379173470218, 'validation/num_examples': 3000, 'test/accuracy': 0.7020971179008484, 'test/loss': 1.3652292490005493, 'test/bleu': 30.252482468017, 'test/num_examples': 3003, 'score': 36147.062624931335, 'total_duration': 61840.97221279144, 'accumulated_submission_time': 36147.062624931335, 'accumulated_eval_time': 25686.989778280258, 'accumulated_logging_time': 1.0742733478546143, 'global_step': 102773, 'preemption_count': 0}), (105164, {'train/accuracy': 0.677069902420044, 'train/loss': 1.5350580215454102, 'train/bleu': 33.95803755505226, 'validation/accuracy': 0.686772346496582, 'validation/loss': 1.4431196451187134, 'validation/bleu': 30.274995899610502, 'validation/num_examples': 3000, 'test/accuracy': 0.7041130661964417, 'test/loss': 1.3482943773269653, 'test/bleu': 30.349883356951473, 'test/num_examples': 3003, 'score': 36986.969824790955, 'total_duration': 63317.321434259415, 'accumulated_submission_time': 36986.969824790955, 'accumulated_eval_time': 26323.274929523468, 'accumulated_logging_time': 1.1026649475097656, 'global_step': 105164, 'preemption_count': 0}), (107556, {'train/accuracy': 0.6941144466400146, 'train/loss': 1.4346460103988647, 'train/bleu': 35.112526953808626, 'validation/accuracy': 0.6891207098960876, 'validation/loss': 1.432898759841919, 'validation/bleu': 30.198097813531618, 'validation/num_examples': 3000, 'test/accuracy': 0.7042057514190674, 'test/loss': 1.3423876762390137, 'test/bleu': 30.7095453089601, 'test/num_examples': 3003, 'score': 37826.85638546944, 'total_duration': 64763.06825017929, 'accumulated_submission_time': 37826.85638546944, 'accumulated_eval_time': 26928.975747346878, 'accumulated_logging_time': 1.1313133239746094, 'global_step': 107556, 'preemption_count': 0}), (109951, {'train/accuracy': 0.6846319437026978, 'train/loss': 1.4830002784729004, 'train/bleu': 34.77058372290943, 'validation/accuracy': 0.6907522678375244, 'validation/loss': 1.4243083000183105, 'validation/bleu': 30.496464512797576, 'validation/num_examples': 3000, 'test/accuracy': 0.7065577507019043, 'test/loss': 1.3313876390457153, 'test/bleu': 30.943426328952334, 'test/num_examples': 3003, 'score': 38666.86701631546, 'total_duration': 66173.58270144463, 'accumulated_submission_time': 38666.86701631546, 'accumulated_eval_time': 27499.315398693085, 'accumulated_logging_time': 1.1643099784851074, 'global_step': 109951, 'preemption_count': 0}), (112348, {'train/accuracy': 0.6892876625061035, 'train/loss': 1.4602874517440796, 'train/bleu': 35.21436361010657, 'validation/accuracy': 0.6915556192398071, 'validation/loss': 1.4185775518417358, 'validation/bleu': 30.628741935996597, 'validation/num_examples': 3000, 'test/accuracy': 0.7084115147590637, 'test/loss': 1.324640154838562, 'test/bleu': 30.794080876055325, 'test/num_examples': 3003, 'score': 39506.95842409134, 'total_duration': 67618.09801673889, 'accumulated_submission_time': 39506.95842409134, 'accumulated_eval_time': 28103.578729391098, 'accumulated_logging_time': 1.1939146518707275, 'global_step': 112348, 'preemption_count': 0}), (114740, {'train/accuracy': 0.6956056952476501, 'train/loss': 1.4176084995269775, 'train/bleu': 35.21629381808173, 'validation/accuracy': 0.6917533874511719, 'validation/loss': 1.4145179986953735, 'validation/bleu': 30.634616504696258, 'validation/num_examples': 3000, 'test/accuracy': 0.7091066837310791, 'test/loss': 1.3200386762619019, 'test/bleu': 30.869499447208245, 'test/num_examples': 3003, 'score': 40346.873964071274, 'total_duration': 69009.57422375679, 'accumulated_submission_time': 40346.873964071274, 'accumulated_eval_time': 28654.973845481873, 'accumulated_logging_time': 1.224264144897461, 'global_step': 114740, 'preemption_count': 0}), (117131, {'train/accuracy': 0.6980080008506775, 'train/loss': 1.4130302667617798, 'train/bleu': 35.43024650972528, 'validation/accuracy': 0.6925073862075806, 'validation/loss': 1.412505030632019, 'validation/bleu': 30.782688578398776, 'validation/num_examples': 3000, 'test/accuracy': 0.7100452184677124, 'test/loss': 1.316109538078308, 'test/bleu': 31.049246057639397, 'test/num_examples': 3003, 'score': 41186.94385623932, 'total_duration': 70458.10548996925, 'accumulated_submission_time': 41186.94385623932, 'accumulated_eval_time': 29263.273505449295, 'accumulated_logging_time': 1.2537436485290527, 'global_step': 117131, 'preemption_count': 0}), (119524, {'train/accuracy': 0.6956333518028259, 'train/loss': 1.4250198602676392, 'train/bleu': 35.59555212175863, 'validation/accuracy': 0.6926433444023132, 'validation/loss': 1.4123377799987793, 'validation/bleu': 30.838573364431188, 'validation/num_examples': 3000, 'test/accuracy': 0.7107172012329102, 'test/loss': 1.3154990673065186, 'test/bleu': 30.978145950859947, 'test/num_examples': 3003, 'score': 42027.04538106918, 'total_duration': 71865.27434062958, 'accumulated_submission_time': 42027.04538106918, 'accumulated_eval_time': 29830.180732011795, 'accumulated_logging_time': 1.284287929534912, 'global_step': 119524, 'preemption_count': 0}), (121916, {'train/accuracy': 0.694487988948822, 'train/loss': 1.4363741874694824, 'train/bleu': 35.27090909222089, 'validation/accuracy': 0.6925197243690491, 'validation/loss': 1.4123674631118774, 'validation/bleu': 30.830389522012513, 'validation/num_examples': 3000, 'test/accuracy': 0.7107867002487183, 'test/loss': 1.315580129623413, 'test/bleu': 30.981668317630366, 'test/num_examples': 3003, 'score': 42867.189265966415, 'total_duration': 73292.17225456238, 'accumulated_submission_time': 42867.189265966415, 'accumulated_eval_time': 30416.771754026413, 'accumulated_logging_time': 1.31484055519104, 'global_step': 121916, 'preemption_count': 0})], 'global_step': 124309}
I0306 15:49:01.084383 139780644897984 submission_runner.py:649] Timing: 43707.15691661835
I0306 15:49:01.084418 139780644897984 submission_runner.py:651] Total number of evals: 52
I0306 15:49:01.084446 139780644897984 submission_runner.py:652] ====================
I0306 15:49:01.084597 139780644897984 submission_runner.py:750] Final wmt score: 3
