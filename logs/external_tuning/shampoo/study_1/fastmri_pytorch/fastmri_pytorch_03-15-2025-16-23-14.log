torchrun --redirects 1:0,2:0,3:0,4:0,5:0,6:0,7:0 --standalone --nnodes=1 --nproc_per_node=8 submission_runner.py --framework=pytorch --workload=fastmri --submission_path=submissions_algorithms/leaderboard/external_tuning/shampoo_submission/submission.py --data_dir=/data/fastmri --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=submissions/rolling_leaderboard/external_tuning/shampoo/study_1 --overwrite=True --save_checkpoints=False --rng_seed=2090646378 --torch_compile=true --tuning_ruleset=external --tuning_search_space=submissions_algorithms/leaderboard/external_tuning/shampoo_submission/tuning_search_space.json --num_tuning_trials=5 --hparam_start_index=0 --hparam_end_index=1 2>&1 | tee -a /logs/fastmri_pytorch_03-15-2025-16-23-14.log
W0315 16:23:25.348000 9 site-packages/torch/distributed/run.py:793] 
W0315 16:23:25.348000 9 site-packages/torch/distributed/run.py:793] *****************************************
W0315 16:23:25.348000 9 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0315 16:23:25.348000 9 site-packages/torch/distributed/run.py:793] *****************************************
2025-03-15 16:23:31.781468: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-03-15 16:23:31.781470: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-03-15 16:23:31.781468: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-03-15 16:23:31.781485: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-03-15 16:23:31.781483: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-03-15 16:23:31.781468: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-03-15 16:23:31.781493: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-03-15 16:23:31.781474: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1742055811.804378      45 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1742055811.804376      48 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1742055811.804379      46 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1742055811.804378      50 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1742055811.804376      51 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1742055811.804380      44 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1742055811.804386      49 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1742055811.804378      47 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1742055811.811376      51 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
E0000 00:00:1742055811.811378      46 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
E0000 00:00:1742055811.811384      48 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
E0000 00:00:1742055811.811389      50 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
E0000 00:00:1742055811.811402      47 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
E0000 00:00:1742055811.811413      49 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
E0000 00:00:1742055811.811414      45 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
E0000 00:00:1742055811.811415      44 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
[rank5]:[W315 16:23:48.060882239 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 5]  using GPU 5 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank1]:[W315 16:23:48.060989307 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank2]:[W315 16:23:48.063405104 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank6]:[W315 16:23:48.064425868 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 6]  using GPU 6 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank3]:[W315 16:23:48.066462210 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank4]:[W315 16:23:48.088101408 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 4]  using GPU 4 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank7]:[W315 16:23:48.116963419 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 7]  using GPU 7 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank0]:[W315 16:23:48.145687179 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
I0315 16:23:49.993524 140269333836992 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_1/fastmri_pytorch.
I0315 16:23:49.993524 139933674378432 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_1/fastmri_pytorch.
I0315 16:23:49.993524 140297289000128 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_1/fastmri_pytorch.
I0315 16:23:49.993524 139783847388352 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_1/fastmri_pytorch.
I0315 16:23:49.993524 140273276859584 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_1/fastmri_pytorch.
I0315 16:23:49.993548 140421256889536 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_1/fastmri_pytorch.
I0315 16:23:49.993546 140496964252864 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_1/fastmri_pytorch.
I0315 16:23:49.993683 139701292631232 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_1/fastmri_pytorch.
I0315 16:23:50.457357 139701292631232 submission_runner.py:606] Using RNG seed 2090646378
I0315 16:23:50.458277 140269333836992 logger_utils.py:97] Saving hparams to /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_1/fastmri_pytorch/trial_1/hparams.json.
I0315 16:23:50.458269 139933674378432 logger_utils.py:97] Saving hparams to /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_1/fastmri_pytorch/trial_1/hparams.json.
I0315 16:23:50.458298 140273276859584 logger_utils.py:97] Saving hparams to /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_1/fastmri_pytorch/trial_1/hparams.json.
I0315 16:23:50.458698 139701292631232 submission_runner.py:615] --- Tuning run 1/5 ---
I0315 16:23:50.458840 139701292631232 submission_runner.py:620] Creating tuning directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_1/fastmri_pytorch/trial_1.
I0315 16:23:50.459065 139701292631232 logger_utils.py:97] Saving hparams to /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_1/fastmri_pytorch/trial_1/hparams.json.
I0315 16:23:50.459535 140297289000128 logger_utils.py:91] Loading hparams from /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_1/fastmri_pytorch/trial_1/hparams.json.
I0315 16:23:50.459548 139783847388352 logger_utils.py:91] Loading hparams from /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_1/fastmri_pytorch/trial_1/hparams.json.
I0315 16:23:50.460099 140421256889536 logger_utils.py:91] Loading hparams from /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_1/fastmri_pytorch/trial_1/hparams.json.
I0315 16:23:50.460704 140496964252864 logger_utils.py:91] Loading hparams from /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_1/fastmri_pytorch/trial_1/hparams.json.
I0315 16:23:50.806212 139701292631232 submission_runner.py:218] Initializing dataset.
I0315 16:23:50.806403 139701292631232 submission_runner.py:229] Initializing model.
I0315 16:23:50.987789 139701292631232 submission_runner.py:268] Performing `torch.compile`.
I0315 16:23:52.245546 139701292631232 submission_runner.py:272] Initializing optimizer.
W0315 16:23:52.246571 140273276859584 distributed_shampoo.py:341] start_preconditioning_step set to -1. Setting start_preconditioning_step equal to precondition frequency 100 by default.
W0315 16:23:52.246733 140273276859584 distributed_shampoo.py:359] Nesterov flag is enabled but momentum parameter is zero! Continuing without using momentum or Nesterov acceleration...
W0315 16:23:52.246773 139783847388352 distributed_shampoo.py:341] start_preconditioning_step set to -1. Setting start_preconditioning_step equal to precondition frequency 100 by default.
W0315 16:23:52.246775 140297289000128 distributed_shampoo.py:341] start_preconditioning_step set to -1. Setting start_preconditioning_step equal to precondition frequency 100 by default.
W0315 16:23:52.246817 140496964252864 distributed_shampoo.py:341] start_preconditioning_step set to -1. Setting start_preconditioning_step equal to precondition frequency 100 by default.
W0315 16:23:52.246924 140297289000128 distributed_shampoo.py:359] Nesterov flag is enabled but momentum parameter is zero! Continuing without using momentum or Nesterov acceleration...
W0315 16:23:52.246942 139783847388352 distributed_shampoo.py:359] Nesterov flag is enabled but momentum parameter is zero! Continuing without using momentum or Nesterov acceleration...
W0315 16:23:52.246981 140496964252864 distributed_shampoo.py:359] Nesterov flag is enabled but momentum parameter is zero! Continuing without using momentum or Nesterov acceleration...
W0315 16:23:52.246988 139701292631232 distributed_shampoo.py:341] start_preconditioning_step set to -1. Setting start_preconditioning_step equal to precondition frequency 100 by default.
W0315 16:23:52.247115 139701292631232 distributed_shampoo.py:359] Nesterov flag is enabled but momentum parameter is zero! Continuing without using momentum or Nesterov acceleration...
W0315 16:23:52.247058 140269333836992 distributed_shampoo.py:341] start_preconditioning_step set to -1. Setting start_preconditioning_step equal to precondition frequency 100 by default.
W0315 16:23:52.247236 140269333836992 distributed_shampoo.py:359] Nesterov flag is enabled but momentum parameter is zero! Continuing without using momentum or Nesterov acceleration...
W0315 16:23:52.247160 140421256889536 distributed_shampoo.py:341] start_preconditioning_step set to -1. Setting start_preconditioning_step equal to precondition frequency 100 by default.
W0315 16:23:52.247316 140421256889536 distributed_shampoo.py:359] Nesterov flag is enabled but momentum parameter is zero! Continuing without using momentum or Nesterov acceleration...
W0315 16:23:52.247888 139933674378432 distributed_shampoo.py:341] start_preconditioning_step set to -1. Setting start_preconditioning_step equal to precondition frequency 100 by default.
W0315 16:23:52.248067 139933674378432 distributed_shampoo.py:359] Nesterov flag is enabled but momentum parameter is zero! Continuing without using momentum or Nesterov acceleration...
I0315 16:23:52.249715 140273276859584 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 0.block_0 ([torch.Size([0])]) for Parameter 0 (torch.Size([32, 1, 3, 3])), Block block_0 (torch.Size([288])).
I0315 16:23:52.249906 140297289000128 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 0.block_0 ([torch.Size([0])]) for Parameter 0 (torch.Size([32, 1, 3, 3])), Block block_0 (torch.Size([288])).
I0315 16:23:52.252078 140273276859584 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 1.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 1 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 16:23:52.252249 140273276859584 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 2.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 2 (torch.Size([64, 32, 3, 3])), Block block_0 (torch.Size([64, 288])).
I0315 16:23:52.252258 140297289000128 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 1.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 1 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 16:23:52.252449 140297289000128 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 2.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 2 (torch.Size([64, 32, 3, 3])), Block block_0 (torch.Size([64, 288])).
I0315 16:23:52.250397 139783847388352 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 0.block_0 ([torch.Size([0])]) for Parameter 0 (torch.Size([32, 1, 3, 3])), Block block_0 (torch.Size([288])).
I0315 16:23:52.252543 140273276859584 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 3.block_0 ([torch.Size([64, 64]), torch.Size([576, 576])]) for Parameter 3 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 16:23:52.252612 140297289000128 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 3.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 3 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 16:23:52.250449 140496964252864 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 0.block_0 ([torch.Size([0])]) for Parameter 0 (torch.Size([32, 1, 3, 3])), Block block_0 (torch.Size([288])).
I0315 16:23:52.252702 140273276859584 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 4.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 4 (torch.Size([128, 64, 3, 3])), Block block_0 (torch.Size([128, 576])).
I0315 16:23:52.252779 140297289000128 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 4.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 4 (torch.Size([128, 64, 3, 3])), Block block_0 (torch.Size([128, 576])).
I0315 16:23:52.252793 139783847388352 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 1.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 1 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 16:23:52.252872 140273276859584 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 5.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 5 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 16:23:52.250735 139701292631232 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 0.block_0 ([torch.Size([0])]) for Parameter 0 (torch.Size([32, 1, 3, 3])), Block block_0 (torch.Size([288])).
I0315 16:23:52.252943 140297289000128 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 5.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 5 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 16:23:52.252987 139783847388352 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 2.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 2 (torch.Size([64, 32, 3, 3])), Block block_0 (torch.Size([64, 288])).
I0315 16:23:52.252976 140496964252864 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 1.block_0 ([torch.Size([1024, 1024]), torch.Size([9, 9])]) for Parameter 1 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 16:23:52.253037 140273276859584 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 6.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 6 (torch.Size([256, 128, 3, 3])), Block block_0 (torch.Size([256, 384, 3])).
I0315 16:23:52.250883 140269333836992 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 0.block_0 ([torch.Size([0])]) for Parameter 0 (torch.Size([32, 1, 3, 3])), Block block_0 (torch.Size([288])).
I0315 16:23:52.253108 140297289000128 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 6.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 6 (torch.Size([256, 128, 3, 3])), Block block_0 (torch.Size([256, 384, 3])).
I0315 16:23:52.253126 139701292631232 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 1.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 1 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 16:23:52.253151 139783847388352 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 3.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 3 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 16:23:52.253178 140496964252864 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 2.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 2 (torch.Size([64, 32, 3, 3])), Block block_0 (torch.Size([64, 288])).
I0315 16:23:52.253196 140273276859584 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 7.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 7 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 16:23:52.250980 140421256889536 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 0.block_0 ([torch.Size([0])]) for Parameter 0 (torch.Size([32, 1, 3, 3])), Block block_0 (torch.Size([288])).
I0315 16:23:52.253279 140297289000128 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 7.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 7 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 16:23:52.253294 139701292631232 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 2.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 2 (torch.Size([64, 32, 3, 3])), Block block_0 (torch.Size([64, 288])).
I0315 16:23:52.253309 139783847388352 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 4.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 4 (torch.Size([128, 64, 3, 3])), Block block_0 (torch.Size([128, 576])).
I0315 16:23:52.253346 140273276859584 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 8.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 8 (torch.Size([512, 256, 3, 3])), Block block_0 (torch.Size([512, 768, 3])).
I0315 16:23:52.253314 140269333836992 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 1.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 1 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 16:23:52.253347 140496964252864 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 3.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 3 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 16:23:52.253434 139701292631232 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 3.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 3 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 16:23:52.253457 139783847388352 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 5.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 5 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 16:23:52.253448 140421256889536 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 1.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 1 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 16:23:52.253491 140273276859584 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 9.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 9 (torch.Size([512, 512, 3, 3])), Block block_0 (torch.Size([512, 512, 9])).
I0315 16:23:52.253581 139701292631232 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 4.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 4 (torch.Size([128, 64, 3, 3])), Block block_0 (torch.Size([128, 576])).
I0315 16:23:52.253617 139783847388352 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 6.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 6 (torch.Size([256, 128, 3, 3])), Block block_0 (torch.Size([256, 384, 3])).
I0315 16:23:52.253605 140496964252864 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 4.block_0 ([torch.Size([128, 128]), torch.Size([576, 576])]) for Parameter 4 (torch.Size([128, 64, 3, 3])), Block block_0 (torch.Size([128, 576])).
I0315 16:23:52.253645 140273276859584 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 10.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 10 (torch.Size([256, 512, 3, 3])), Block block_0 (torch.Size([256, 512, 9])).
I0315 16:23:52.253633 140269333836992 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 2.block_0 ([torch.Size([64, 64]), torch.Size([288, 288])]) for Parameter 2 (torch.Size([64, 32, 3, 3])), Block block_0 (torch.Size([64, 288])).
I0315 16:23:52.253663 140421256889536 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 2.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 2 (torch.Size([64, 32, 3, 3])), Block block_0 (torch.Size([64, 288])).
I0315 16:23:52.253764 139701292631232 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 5.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 5 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 16:23:52.251457 139933674378432 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 0.block_0 ([torch.Size([288, 288])]) for Parameter 0 (torch.Size([32, 1, 3, 3])), Block block_0 (torch.Size([288])).
I0315 16:23:52.253793 140496964252864 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 5.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 5 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 16:23:52.253798 139783847388352 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 7.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 7 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 16:23:52.253812 140273276859584 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 11.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 11 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 16:23:52.253829 140421256889536 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 3.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 3 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 16:23:52.253840 140269333836992 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 3.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 3 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 16:23:52.253928 139701292631232 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 6.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 6 (torch.Size([256, 128, 3, 3])), Block block_0 (torch.Size([256, 384, 3])).
I0315 16:23:52.253953 140273276859584 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 12.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 12 (torch.Size([128, 256, 3, 3])), Block block_0 (torch.Size([128, 768, 3])).
I0315 16:23:52.253954 139783847388352 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 8.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 8 (torch.Size([512, 256, 3, 3])), Block block_0 (torch.Size([512, 768, 3])).
I0315 16:23:52.253959 140496964252864 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 6.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 6 (torch.Size([256, 128, 3, 3])), Block block_0 (torch.Size([256, 384, 3])).
I0315 16:23:52.253989 140421256889536 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 4.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 4 (torch.Size([128, 64, 3, 3])), Block block_0 (torch.Size([128, 576])).
I0315 16:23:52.253995 140269333836992 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 4.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 4 (torch.Size([128, 64, 3, 3])), Block block_0 (torch.Size([128, 576])).
I0315 16:23:52.254039 139933674378432 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 1.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 1 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 16:23:52.254052 140297289000128 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 8.block_0 ([torch.Size([512, 512]), torch.Size([768, 768]), torch.Size([3, 3])]) for Parameter 8 (torch.Size([512, 256, 3, 3])), Block block_0 (torch.Size([512, 768, 3])).
I0315 16:23:52.254088 139701292631232 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 7.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 7 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 16:23:52.254096 139783847388352 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 9.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 9 (torch.Size([512, 512, 3, 3])), Block block_0 (torch.Size([512, 512, 9])).
I0315 16:23:52.254096 140273276859584 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 13.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 13 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 16:23:52.254114 140496964252864 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 7.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 7 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 16:23:52.254145 140421256889536 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 5.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 5 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 16:23:52.254148 140269333836992 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 5.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 5 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 16:23:52.254231 139783847388352 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 10.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 10 (torch.Size([256, 512, 3, 3])), Block block_0 (torch.Size([256, 512, 9])).
I0315 16:23:52.254234 139701292631232 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 8.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 8 (torch.Size([512, 256, 3, 3])), Block block_0 (torch.Size([512, 768, 3])).
I0315 16:23:52.254236 140273276859584 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 14.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 14 (torch.Size([64, 128, 3, 3])), Block block_0 (torch.Size([64, 384, 3])).
I0315 16:23:52.254237 140297289000128 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 9.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 9 (torch.Size([512, 512, 3, 3])), Block block_0 (torch.Size([512, 512, 9])).
I0315 16:23:52.254276 140496964252864 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 8.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 8 (torch.Size([512, 256, 3, 3])), Block block_0 (torch.Size([512, 768, 3])).
I0315 16:23:52.254265 139933674378432 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 2.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 2 (torch.Size([64, 32, 3, 3])), Block block_0 (torch.Size([64, 288])).
I0315 16:23:52.254293 140421256889536 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 6.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 6 (torch.Size([256, 128, 3, 3])), Block block_0 (torch.Size([256, 384, 3])).
I0315 16:23:52.254296 140269333836992 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 6.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 6 (torch.Size([256, 128, 3, 3])), Block block_0 (torch.Size([256, 384, 3])).
I0315 16:23:52.254384 140297289000128 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 10.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 10 (torch.Size([256, 512, 3, 3])), Block block_0 (torch.Size([256, 512, 9])).
I0315 16:23:52.254423 140496964252864 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 9.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 9 (torch.Size([512, 512, 3, 3])), Block block_0 (torch.Size([512, 512, 9])).
I0315 16:23:52.254442 140421256889536 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 7.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 7 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 16:23:52.254448 140273276859584 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 15.block_0 ([torch.Size([64, 64]), torch.Size([576, 576])]) for Parameter 15 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 16:23:52.254456 139933674378432 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 3.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 3 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 16:23:52.254528 139783847388352 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 11.block_0 ([torch.Size([256, 256]), torch.Size([768, 768]), torch.Size([3, 3])]) for Parameter 11 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 16:23:52.254548 140297289000128 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 11.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 11 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 16:23:52.254568 140496964252864 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 10.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 10 (torch.Size([256, 512, 3, 3])), Block block_0 (torch.Size([256, 512, 9])).
I0315 16:23:52.254578 140273276859584 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 16.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 16 (torch.Size([32, 64, 3, 3])), Block block_0 (torch.Size([32, 576])).
I0315 16:23:52.254598 140421256889536 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 8.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 8 (torch.Size([512, 256, 3, 3])), Block block_0 (torch.Size([512, 768, 3])).
I0315 16:23:52.254652 139933674378432 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 4.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 4 (torch.Size([128, 64, 3, 3])), Block block_0 (torch.Size([128, 576])).
I0315 16:23:52.254701 140297289000128 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 12.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 12 (torch.Size([128, 256, 3, 3])), Block block_0 (torch.Size([128, 768, 3])).
I0315 16:23:52.254716 139783847388352 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 12.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 12 (torch.Size([128, 256, 3, 3])), Block block_0 (torch.Size([128, 768, 3])).
I0315 16:23:52.254733 140496964252864 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 11.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 11 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 16:23:52.254748 140421256889536 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 9.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 9 (torch.Size([512, 512, 3, 3])), Block block_0 (torch.Size([512, 512, 9])).
I0315 16:23:52.254862 140297289000128 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 13.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 13 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 16:23:52.254883 139783847388352 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 13.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 13 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 16:23:52.255025 140297289000128 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 14.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 14 (torch.Size([64, 128, 3, 3])), Block block_0 (torch.Size([64, 384, 3])).
I0315 16:23:52.255048 139783847388352 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 14.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 14 (torch.Size([64, 128, 3, 3])), Block block_0 (torch.Size([64, 384, 3])).
I0315 16:23:52.255154 140297289000128 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 15.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 15 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 16:23:52.255183 139783847388352 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 15.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 15 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 16:23:52.255279 140297289000128 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 16.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 16 (torch.Size([32, 64, 3, 3])), Block block_0 (torch.Size([32, 576])).
I0315 16:23:52.255340 140269333836992 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 7.block_0 ([torch.Size([256, 256]), torch.Size([768, 768]), torch.Size([3, 3])]) for Parameter 7 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 16:23:52.255403 140297289000128 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 17.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 17 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 16:23:52.255408 139783847388352 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 16.block_0 ([torch.Size([32, 32]), torch.Size([576, 576])]) for Parameter 16 (torch.Size([32, 64, 3, 3])), Block block_0 (torch.Size([32, 576])).
I0315 16:23:52.255502 140297289000128 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 18.block_0 ([torch.Size([0])]) for Parameter 18 (torch.Size([1, 32, 1, 1])), Block block_0 (torch.Size([32])).
I0315 16:23:52.255544 139783847388352 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 17.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 17 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 16:23:52.255549 140269333836992 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 8.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 8 (torch.Size([512, 256, 3, 3])), Block block_0 (torch.Size([512, 768, 3])).
I0315 16:23:52.255593 140297289000128 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 19.block_0 ([torch.Size([0])]) for Parameter 19 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0315 16:23:52.255645 139783847388352 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 18.block_0 ([torch.Size([0])]) for Parameter 18 (torch.Size([1, 32, 1, 1])), Block block_0 (torch.Size([32])).
I0315 16:23:52.255706 140269333836992 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 9.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 9 (torch.Size([512, 512, 3, 3])), Block block_0 (torch.Size([512, 512, 9])).
I0315 16:23:52.255729 140297289000128 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 20.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 20 (torch.Size([512, 256, 2, 2])), Block block_0 (torch.Size([512, 1024])).
I0315 16:23:52.255757 139783847388352 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 19.block_0 ([torch.Size([0])]) for Parameter 19 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0315 16:23:52.255779 140273276859584 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 17.block_0 ([torch.Size([1024, 1024]), torch.Size([9, 9])]) for Parameter 17 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 16:23:52.255868 140297289000128 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 21.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 21 (torch.Size([256, 128, 2, 2])), Block block_0 (torch.Size([256, 512])).
I0315 16:23:52.255870 140269333836992 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 10.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 10 (torch.Size([256, 512, 3, 3])), Block block_0 (torch.Size([256, 512, 9])).
I0315 16:23:52.255883 139783847388352 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 20.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 20 (torch.Size([512, 256, 2, 2])), Block block_0 (torch.Size([512, 1024])).
I0315 16:23:52.255920 140273276859584 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 18.block_0 ([torch.Size([0])]) for Parameter 18 (torch.Size([1, 32, 1, 1])), Block block_0 (torch.Size([32])).
I0315 16:23:52.255992 140297289000128 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 22.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 22 (torch.Size([128, 64, 2, 2])), Block block_0 (torch.Size([128, 256])).
I0315 16:23:52.256012 139783847388352 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 21.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 21 (torch.Size([256, 128, 2, 2])), Block block_0 (torch.Size([256, 512])).
I0315 16:23:52.256018 140273276859584 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 19.block_0 ([torch.Size([0])]) for Parameter 19 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0315 16:23:52.256017 140269333836992 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 11.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 11 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 16:23:52.256123 140297289000128 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 23.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 23 (torch.Size([64, 32, 2, 2])), Block block_0 (torch.Size([64, 128])).
I0315 16:23:52.256139 139783847388352 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 22.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 22 (torch.Size([128, 64, 2, 2])), Block block_0 (torch.Size([128, 256])).
I0315 16:23:52.256161 140269333836992 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 12.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 12 (torch.Size([128, 256, 3, 3])), Block block_0 (torch.Size([128, 768, 3])).
I0315 16:23:52.256231 140297289000128 shampoo_preconditioner_list.py:612] Rank 1: ShampooPreconditionerList Numel Breakdown: (165888, 2097314, 174080, 671744, 696320, 327698, 426002, 1310738, 1703954, 1048738, 655522, 1310738, 1212434, 327698, 303122, 671744, 665600, 2097314, 2048, 2, 2621440, 655360, 163840, 40960)
I0315 16:23:52.256256 139783847388352 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 23.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 23 (torch.Size([64, 32, 2, 2])), Block block_0 (torch.Size([64, 128])).
I0315 16:23:52.256278 140297289000128 shampoo_preconditioner_list.py:615] Rank 1: ShampooPreconditionerList Bytes Breakdown: (663552,)
I0315 16:23:52.256270 140496964252864 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 12.block_0 ([torch.Size([128, 128]), torch.Size([768, 768]), torch.Size([3, 3])]) for Parameter 12 (torch.Size([128, 256, 3, 3])), Block block_0 (torch.Size([128, 768, 3])).
I0315 16:23:52.256317 140297289000128 shampoo_preconditioner_list.py:618] Rank 1: ShampooPreconditionerList Total Elements: 19350298
I0315 16:23:52.256290 139701292631232 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 9.block_0 ([torch.Size([512, 512]), torch.Size([512, 512]), torch.Size([9, 9])]) for Parameter 9 (torch.Size([512, 512, 3, 3])), Block block_0 (torch.Size([512, 512, 9])).
I0315 16:23:52.256286 139933674378432 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 5.block_0 ([torch.Size([128, 128]), torch.Size([384, 384]), torch.Size([3, 3])]) for Parameter 5 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 16:23:52.256312 140269333836992 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 13.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 13 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 16:23:52.256350 140297289000128 shampoo_preconditioner_list.py:621] Rank 1: ShampooPreconditionerList Total Bytes: 663552
I0315 16:23:52.256327 140421256889536 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 10.block_0 ([torch.Size([256, 256]), torch.Size([512, 512]), torch.Size([9, 9])]) for Parameter 10 (torch.Size([256, 512, 3, 3])), Block block_0 (torch.Size([256, 512, 9])).
I0315 16:23:52.256365 139783847388352 shampoo_preconditioner_list.py:612] Rank 4: ShampooPreconditionerList Numel Breakdown: (165888, 2097314, 174080, 671744, 696320, 327698, 426002, 1310738, 1703954, 1048738, 655522, 1310738, 1212434, 327698, 303122, 671744, 665600, 2097314, 2048, 2, 2621440, 655360, 163840, 40960)
I0315 16:23:52.256411 139783847388352 shampoo_preconditioner_list.py:615] Rank 4: ShampooPreconditionerList Bytes Breakdown: (663552, 8389256)
I0315 16:23:52.256448 139783847388352 shampoo_preconditioner_list.py:618] Rank 4: ShampooPreconditionerList Total Elements: 19350298
I0315 16:23:52.256468 140269333836992 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 14.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 14 (torch.Size([64, 128, 3, 3])), Block block_0 (torch.Size([64, 384, 3])).
I0315 16:23:52.256471 139701292631232 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 10.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 10 (torch.Size([256, 512, 3, 3])), Block block_0 (torch.Size([256, 512, 9])).
I0315 16:23:52.256503 139783847388352 shampoo_preconditioner_list.py:621] Rank 4: ShampooPreconditionerList Total Bytes: 9052808
I0315 16:23:52.256529 140421256889536 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 11.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 11 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 16:23:52.256549 140297289000128 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([32, 1, 3, 3])), Block block_0 (torch.Size([288])).
I0315 16:23:52.256623 140269333836992 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 15.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 15 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 16:23:52.256638 140297289000128 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 16:23:52.256622 140273276859584 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 20.block_0 ([torch.Size([512, 512]), torch.Size([1024, 1024])]) for Parameter 20 (torch.Size([512, 256, 2, 2])), Block block_0 (torch.Size([512, 1024])).
I0315 16:23:52.256653 139701292631232 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 11.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 11 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 16:23:52.256680 139783847388352 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([32, 1, 3, 3])), Block block_0 (torch.Size([288])).
I0315 16:23:52.256705 140297289000128 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([64, 32, 3, 3])), Block block_0 (torch.Size([64, 288])).
I0315 16:23:52.256705 140421256889536 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 12.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 12 (torch.Size([128, 256, 3, 3])), Block block_0 (torch.Size([128, 768, 3])).
I0315 16:23:52.256748 140269333836992 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 16.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 16 (torch.Size([32, 64, 3, 3])), Block block_0 (torch.Size([32, 576])).
I0315 16:23:52.256760 140297289000128 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 16:23:52.256757 139783847388352 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 16:23:52.256800 140273276859584 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 21.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 21 (torch.Size([256, 128, 2, 2])), Block block_0 (torch.Size([256, 512])).
I0315 16:23:52.256812 140297289000128 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 4.block_0 (torch.Size([0])) for Parameter 4 (torch.Size([128, 64, 3, 3])), Block block_0 (torch.Size([128, 576])).
I0315 16:23:52.256816 139783847388352 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([64, 32, 3, 3])), Block block_0 (torch.Size([64, 288])).
I0315 16:23:52.256828 139701292631232 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 12.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 12 (torch.Size([128, 256, 3, 3])), Block block_0 (torch.Size([128, 768, 3])).
I0315 16:23:52.256866 140297289000128 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 5.block_0 (torch.Size([0])) for Parameter 5 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 16:23:52.256870 139783847388352 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 16:23:52.256867 140421256889536 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 13.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 13 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 16:23:52.256896 140269333836992 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 17.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 17 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 16:23:52.256923 139783847388352 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 4.block_0 (torch.Size([0])) for Parameter 4 (torch.Size([128, 64, 3, 3])), Block block_0 (torch.Size([128, 576])).
I0315 16:23:52.256927 140297289000128 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 6.block_0 (torch.Size([0])) for Parameter 6 (torch.Size([256, 128, 3, 3])), Block block_0 (torch.Size([256, 384, 3])).
I0315 16:23:52.256946 140273276859584 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 22.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 22 (torch.Size([128, 64, 2, 2])), Block block_0 (torch.Size([128, 256])).
I0315 16:23:52.256986 139783847388352 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 5.block_0 (torch.Size([0])) for Parameter 5 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 16:23:52.256989 140297289000128 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 7.block_0 (torch.Size([0])) for Parameter 7 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 16:23:52.256997 139701292631232 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 13.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 13 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 16:23:52.257005 140269333836992 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 18.block_0 ([torch.Size([0])]) for Parameter 18 (torch.Size([1, 32, 1, 1])), Block block_0 (torch.Size([32])).
I0315 16:23:52.257017 140421256889536 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 14.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 14 (torch.Size([64, 128, 3, 3])), Block block_0 (torch.Size([64, 384, 3])).
I0315 16:23:52.257047 139783847388352 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 6.block_0 (torch.Size([0])) for Parameter 6 (torch.Size([256, 128, 3, 3])), Block block_0 (torch.Size([256, 384, 3])).
I0315 16:23:52.257076 140273276859584 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 23.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 23 (torch.Size([64, 32, 2, 2])), Block block_0 (torch.Size([64, 128])).
I0315 16:23:52.257101 140269333836992 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 19.block_0 ([torch.Size([0])]) for Parameter 19 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0315 16:23:52.257098 140297289000128 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 8.block_0 (torch.Size([512, 768, 3])) for Parameter 8 (torch.Size([512, 256, 3, 3])), Block block_0 (torch.Size([512, 768, 3])).
I0315 16:23:52.257114 139783847388352 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 7.block_0 (torch.Size([0])) for Parameter 7 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 16:23:52.257101 140496964252864 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 13.block_0 ([torch.Size([128, 128]), torch.Size([384, 384]), torch.Size([3, 3])]) for Parameter 13 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 16:23:52.257146 140421256889536 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 15.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 15 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 16:23:52.257158 139701292631232 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 14.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 14 (torch.Size([64, 128, 3, 3])), Block block_0 (torch.Size([64, 384, 3])).
I0315 16:23:52.257168 139783847388352 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 8.block_0 (torch.Size([0])) for Parameter 8 (torch.Size([512, 256, 3, 3])), Block block_0 (torch.Size([512, 768, 3])).
I0315 16:23:52.257170 140297289000128 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 9.block_0 (torch.Size([0])) for Parameter 9 (torch.Size([512, 512, 3, 3])), Block block_0 (torch.Size([512, 512, 9])).
I0315 16:23:52.257197 140273276859584 shampoo_preconditioner_list.py:612] Rank 5: ShampooPreconditionerList Numel Breakdown: (165888, 2097314, 174080, 671744, 696320, 327698, 426002, 1310738, 1703954, 1048738, 655522, 1310738, 1212434, 327698, 303122, 671744, 665600, 2097314, 2048, 2, 2621440, 655360, 163840, 40960)
I0315 16:23:52.257233 139783847388352 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 9.block_0 (torch.Size([0])) for Parameter 9 (torch.Size([512, 512, 3, 3])), Block block_0 (torch.Size([512, 512, 9])).
I0315 16:23:52.257237 140297289000128 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 10.block_0 (torch.Size([0])) for Parameter 10 (torch.Size([256, 512, 3, 3])), Block block_0 (torch.Size([256, 512, 9])).
I0315 16:23:52.257194 139933674378432 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 6.block_0 ([torch.Size([256, 256]), torch.Size([384, 384]), torch.Size([3, 3])]) for Parameter 6 (torch.Size([256, 128, 3, 3])), Block block_0 (torch.Size([256, 384, 3])).
I0315 16:23:52.257245 140269333836992 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 20.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 20 (torch.Size([512, 256, 2, 2])), Block block_0 (torch.Size([512, 1024])).
I0315 16:23:52.257261 140273276859584 shampoo_preconditioner_list.py:615] Rank 5: ShampooPreconditionerList Bytes Breakdown: (663552, 8389256, 696320, 2686976)
I0315 16:23:52.257278 140421256889536 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 16.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 16 (torch.Size([32, 64, 3, 3])), Block block_0 (torch.Size([32, 576])).
I0315 16:23:52.257283 139783847388352 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 10.block_0 (torch.Size([0])) for Parameter 10 (torch.Size([256, 512, 3, 3])), Block block_0 (torch.Size([256, 512, 9])).
I0315 16:23:52.257288 140297289000128 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 11.block_0 (torch.Size([0])) for Parameter 11 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 16:23:52.257295 140273276859584 shampoo_preconditioner_list.py:618] Rank 5: ShampooPreconditionerList Total Elements: 19350298
I0315 16:23:52.257306 139701292631232 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 15.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 15 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 16:23:52.257326 140273276859584 shampoo_preconditioner_list.py:621] Rank 5: ShampooPreconditionerList Total Bytes: 12436104
I0315 16:23:52.257337 140297289000128 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 12.block_0 (torch.Size([0])) for Parameter 12 (torch.Size([128, 256, 3, 3])), Block block_0 (torch.Size([128, 768, 3])).
I0315 16:23:52.257381 139783847388352 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 11.block_0 (torch.Size([256, 768, 3])) for Parameter 11 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 16:23:52.257385 140269333836992 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 21.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 21 (torch.Size([256, 128, 2, 2])), Block block_0 (torch.Size([256, 512])).
I0315 16:23:52.257398 140297289000128 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 13.block_0 (torch.Size([0])) for Parameter 13 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 16:23:52.257390 140496964252864 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 14.block_0 ([torch.Size([64, 64]), torch.Size([384, 384]), torch.Size([3, 3])]) for Parameter 14 (torch.Size([64, 128, 3, 3])), Block block_0 (torch.Size([64, 384, 3])).
I0315 16:23:52.257405 140421256889536 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 17.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 17 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 16:23:52.257434 139701292631232 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 16.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 16 (torch.Size([32, 64, 3, 3])), Block block_0 (torch.Size([32, 576])).
I0315 16:23:52.257448 140297289000128 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 14.block_0 (torch.Size([0])) for Parameter 14 (torch.Size([64, 128, 3, 3])), Block block_0 (torch.Size([64, 384, 3])).
I0315 16:23:52.257447 139933674378432 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 7.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 7 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 16:23:52.257458 139783847388352 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 12.block_0 (torch.Size([0])) for Parameter 12 (torch.Size([128, 256, 3, 3])), Block block_0 (torch.Size([128, 768, 3])).
I0315 16:23:52.257492 140421256889536 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 18.block_0 ([torch.Size([0])]) for Parameter 18 (torch.Size([1, 32, 1, 1])), Block block_0 (torch.Size([32])).
I0315 16:23:52.257495 140297289000128 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 15.block_0 (torch.Size([0])) for Parameter 15 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 16:23:52.257506 140269333836992 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 22.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 22 (torch.Size([128, 64, 2, 2])), Block block_0 (torch.Size([128, 256])).
I0315 16:23:52.257512 139783847388352 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 13.block_0 (torch.Size([0])) for Parameter 13 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 16:23:52.257504 140273276859584 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([32, 1, 3, 3])), Block block_0 (torch.Size([288])).
I0315 16:23:52.257535 140496964252864 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 15.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 15 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 16:23:52.257544 140297289000128 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 16.block_0 (torch.Size([0])) for Parameter 16 (torch.Size([32, 64, 3, 3])), Block block_0 (torch.Size([32, 576])).
I0315 16:23:52.257562 139783847388352 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 14.block_0 (torch.Size([0])) for Parameter 14 (torch.Size([64, 128, 3, 3])), Block block_0 (torch.Size([64, 384, 3])).
I0315 16:23:52.257557 139701292631232 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 17.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 17 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 16:23:52.257570 140273276859584 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 16:23:52.257586 140421256889536 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 19.block_0 ([torch.Size([0])]) for Parameter 19 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0315 16:23:52.257612 140297289000128 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 17.block_0 (torch.Size([0])) for Parameter 17 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 16:23:52.257634 139783847388352 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 15.block_0 (torch.Size([0])) for Parameter 15 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 16:23:52.257653 140269333836992 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 23.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 23 (torch.Size([64, 32, 2, 2])), Block block_0 (torch.Size([64, 128])).
I0315 16:23:52.257661 140273276859584 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([64, 32, 3, 3])), Block block_0 (torch.Size([64, 288])).
I0315 16:23:52.257665 139701292631232 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 18.block_0 ([torch.Size([0])]) for Parameter 18 (torch.Size([1, 32, 1, 1])), Block block_0 (torch.Size([32])).
I0315 16:23:52.257667 140297289000128 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 18.block_0 (torch.Size([0])) for Parameter 18 (torch.Size([1, 32, 1, 1])), Block block_0 (torch.Size([32])).
I0315 16:23:52.257707 140496964252864 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 16.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 16 (torch.Size([32, 64, 3, 3])), Block block_0 (torch.Size([32, 576])).
I0315 16:23:52.257724 140297289000128 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 19.block_0 (torch.Size([0])) for Parameter 19 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0315 16:23:52.257725 140421256889536 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 20.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 20 (torch.Size([512, 256, 2, 2])), Block block_0 (torch.Size([512, 1024])).
I0315 16:23:52.257733 139933674378432 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 8.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 8 (torch.Size([512, 256, 3, 3])), Block block_0 (torch.Size([512, 768, 3])).
I0315 16:23:52.257745 139783847388352 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 16.block_0 (torch.Size([32, 576])) for Parameter 16 (torch.Size([32, 64, 3, 3])), Block block_0 (torch.Size([32, 576])).
I0315 16:23:52.257766 139701292631232 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 19.block_0 ([torch.Size([0])]) for Parameter 19 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0315 16:23:52.257774 140297289000128 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 20.block_0 (torch.Size([0])) for Parameter 20 (torch.Size([512, 256, 2, 2])), Block block_0 (torch.Size([512, 1024])).
I0315 16:23:52.257771 140273276859584 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([64, 576])) for Parameter 3 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 16:23:52.257770 140269333836992 shampoo_preconditioner_list.py:612] Rank 3: ShampooPreconditionerList Numel Breakdown: (165888, 2097314, 174080, 671744, 696320, 327698, 426002, 1310738, 1703954, 1048738, 655522, 1310738, 1212434, 327698, 303122, 671744, 665600, 2097314, 2048, 2, 2621440, 655360, 163840, 40960)
I0315 16:23:52.257811 139783847388352 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 17.block_0 (torch.Size([0])) for Parameter 17 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 16:23:52.257819 140269333836992 shampoo_preconditioner_list.py:615] Rank 3: ShampooPreconditionerList Bytes Breakdown: (663552, 8389256)
I0315 16:23:52.257823 140297289000128 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 21.block_0 (torch.Size([0])) for Parameter 21 (torch.Size([256, 128, 2, 2])), Block block_0 (torch.Size([256, 512])).
I0315 16:23:52.257843 140273276859584 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 4.block_0 (torch.Size([0])) for Parameter 4 (torch.Size([128, 64, 3, 3])), Block block_0 (torch.Size([128, 576])).
I0315 16:23:52.257853 140269333836992 shampoo_preconditioner_list.py:618] Rank 3: ShampooPreconditionerList Total Elements: 19350298
I0315 16:23:52.257846 140496964252864 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 17.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 17 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 16:23:52.257859 140421256889536 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 21.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 21 (torch.Size([256, 128, 2, 2])), Block block_0 (torch.Size([256, 512])).
I0315 16:23:52.257872 140297289000128 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 22.block_0 (torch.Size([0])) for Parameter 22 (torch.Size([128, 64, 2, 2])), Block block_0 (torch.Size([128, 256])).
I0315 16:23:52.257871 139783847388352 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 18.block_0 (torch.Size([0])) for Parameter 18 (torch.Size([1, 32, 1, 1])), Block block_0 (torch.Size([32])).
I0315 16:23:52.257884 140269333836992 shampoo_preconditioner_list.py:621] Rank 3: ShampooPreconditionerList Total Bytes: 9052808
I0315 16:23:52.257892 139701292631232 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 20.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 20 (torch.Size([512, 256, 2, 2])), Block block_0 (torch.Size([512, 1024])).
I0315 16:23:52.257910 140273276859584 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 5.block_0 (torch.Size([0])) for Parameter 5 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 16:23:52.257921 139783847388352 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 19.block_0 (torch.Size([0])) for Parameter 19 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0315 16:23:52.257931 140297289000128 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 23.block_0 (torch.Size([0])) for Parameter 23 (torch.Size([64, 32, 2, 2])), Block block_0 (torch.Size([64, 128])).
I0315 16:23:52.257936 140496964252864 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 18.block_0 ([torch.Size([0])]) for Parameter 18 (torch.Size([1, 32, 1, 1])), Block block_0 (torch.Size([32])).
I0315 16:23:52.257947 139933674378432 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 9.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 9 (torch.Size([512, 512, 3, 3])), Block block_0 (torch.Size([512, 512, 9])).
I0315 16:23:52.257972 139783847388352 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 20.block_0 (torch.Size([0])) for Parameter 20 (torch.Size([512, 256, 2, 2])), Block block_0 (torch.Size([512, 1024])).
I0315 16:23:52.257975 140273276859584 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 6.block_0 (torch.Size([0])) for Parameter 6 (torch.Size([256, 128, 3, 3])), Block block_0 (torch.Size([256, 384, 3])).
I0315 16:23:52.257985 140421256889536 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 22.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 22 (torch.Size([128, 64, 2, 2])), Block block_0 (torch.Size([128, 256])).
I0315 16:23:52.258005 140297289000128 shampoo_preconditioner_list.py:375] Rank 1: AdaGradPreconditionerList Numel Breakdown: (0, 0, 0, 0, 0, 0, 0, 0, 1179648, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)
I0315 16:23:52.258021 139783847388352 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 21.block_0 (torch.Size([0])) for Parameter 21 (torch.Size([256, 128, 2, 2])), Block block_0 (torch.Size([256, 512])).
I0315 16:23:52.258025 140496964252864 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 19.block_0 ([torch.Size([0])]) for Parameter 19 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0315 16:23:52.258034 140273276859584 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 7.block_0 (torch.Size([0])) for Parameter 7 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 16:23:52.258029 139701292631232 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 21.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 21 (torch.Size([256, 128, 2, 2])), Block block_0 (torch.Size([256, 512])).
I0315 16:23:52.258039 140297289000128 shampoo_preconditioner_list.py:378] Rank 1: AdaGradPreconditionerList Bytes Breakdown: (0, 0, 0, 0, 0, 0, 0, 0, 4718592, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)
I0315 16:23:52.258069 140297289000128 shampoo_preconditioner_list.py:381] Rank 1: AdaGradPreconditionerList Total Elements: 1179648
I0315 16:23:52.258077 139783847388352 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 22.block_0 (torch.Size([0])) for Parameter 22 (torch.Size([128, 64, 2, 2])), Block block_0 (torch.Size([128, 256])).
I0315 16:23:52.258064 140269333836992 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([32, 1, 3, 3])), Block block_0 (torch.Size([288])).
I0315 16:23:52.258086 140273276859584 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 8.block_0 (torch.Size([0])) for Parameter 8 (torch.Size([512, 256, 3, 3])), Block block_0 (torch.Size([512, 768, 3])).
I0315 16:23:52.258104 140297289000128 shampoo_preconditioner_list.py:384] Rank 1: AdaGradPreconditionerList Total Bytes: 4718592
I0315 16:23:52.258106 140421256889536 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 23.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 23 (torch.Size([64, 32, 2, 2])), Block block_0 (torch.Size([64, 128])).
I0315 16:23:52.258130 139783847388352 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 23.block_0 (torch.Size([0])) for Parameter 23 (torch.Size([64, 32, 2, 2])), Block block_0 (torch.Size([64, 128])).
I0315 16:23:52.258138 140273276859584 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 9.block_0 (torch.Size([0])) for Parameter 9 (torch.Size([512, 512, 3, 3])), Block block_0 (torch.Size([512, 512, 9])).
I0315 16:23:52.258138 140269333836992 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 16:23:52.258143 140496964252864 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 20.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 20 (torch.Size([512, 256, 2, 2])), Block block_0 (torch.Size([512, 1024])).
I0315 16:23:52.258143 139933674378432 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 10.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 10 (torch.Size([256, 512, 3, 3])), Block block_0 (torch.Size([256, 512, 9])).
I0315 16:23:52.258147 139701292631232 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 22.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 22 (torch.Size([128, 64, 2, 2])), Block block_0 (torch.Size([128, 256])).
I0315 16:23:52.258188 140273276859584 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 10.block_0 (torch.Size([0])) for Parameter 10 (torch.Size([256, 512, 3, 3])), Block block_0 (torch.Size([256, 512, 9])).
I0315 16:23:52.258196 139783847388352 shampoo_preconditioner_list.py:375] Rank 4: AdaGradPreconditionerList Numel Breakdown: (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 589824, 0, 0, 0, 0, 18432, 0, 0, 0, 0, 0, 0, 0)
I0315 16:23:52.258207 140421256889536 shampoo_preconditioner_list.py:612] Rank 2: ShampooPreconditionerList Numel Breakdown: (165888, 2097314, 174080, 671744, 696320, 327698, 426002, 1310738, 1703954, 1048738, 655522, 1310738, 1212434, 327698, 303122, 671744, 665600, 2097314, 2048, 2, 2621440, 655360, 163840, 40960)
I0315 16:23:52.258231 139783847388352 shampoo_preconditioner_list.py:378] Rank 4: AdaGradPreconditionerList Bytes Breakdown: (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2359296, 0, 0, 0, 0, 73728, 0, 0, 0, 0, 0, 0, 0)
I0315 16:23:52.258247 140273276859584 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 11.block_0 (torch.Size([0])) for Parameter 11 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 16:23:52.258255 140421256889536 shampoo_preconditioner_list.py:615] Rank 2: ShampooPreconditionerList Bytes Breakdown: (663552,)
I0315 16:23:52.258247 140269333836992 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([64, 288])) for Parameter 2 (torch.Size([64, 32, 3, 3])), Block block_0 (torch.Size([64, 288])).
I0315 16:23:52.258267 139783847388352 shampoo_preconditioner_list.py:381] Rank 4: AdaGradPreconditionerList Total Elements: 608256
I0315 16:23:52.258266 139701292631232 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 23.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 23 (torch.Size([64, 32, 2, 2])), Block block_0 (torch.Size([64, 128])).
I0315 16:23:52.258270 140496964252864 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 21.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 21 (torch.Size([256, 128, 2, 2])), Block block_0 (torch.Size([256, 512])).
I0315 16:23:52.258288 140421256889536 shampoo_preconditioner_list.py:618] Rank 2: ShampooPreconditionerList Total Elements: 19350298
I0315 16:23:52.258297 139783847388352 shampoo_preconditioner_list.py:384] Rank 4: AdaGradPreconditionerList Total Bytes: 2433024
I0315 16:23:52.258305 140273276859584 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 12.block_0 (torch.Size([0])) for Parameter 12 (torch.Size([128, 256, 3, 3])), Block block_0 (torch.Size([128, 768, 3])).
I0315 16:23:52.258317 140269333836992 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 16:23:52.258323 140421256889536 shampoo_preconditioner_list.py:621] Rank 2: ShampooPreconditionerList Total Bytes: 663552
I0315 16:23:52.258336 139933674378432 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 11.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 11 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 16:23:52.258355 140273276859584 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 13.block_0 (torch.Size([0])) for Parameter 13 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 16:23:52.258367 139701292631232 shampoo_preconditioner_list.py:612] Rank 0: ShampooPreconditionerList Numel Breakdown: (165888, 2097314, 174080, 671744, 696320, 327698, 426002, 1310738, 1703954, 1048738, 655522, 1310738, 1212434, 327698, 303122, 671744, 665600, 2097314, 2048, 2, 2621440, 655360, 163840, 40960)
I0315 16:23:52.258372 140269333836992 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 4.block_0 (torch.Size([0])) for Parameter 4 (torch.Size([128, 64, 3, 3])), Block block_0 (torch.Size([128, 576])).
I0315 16:23:52.258386 140496964252864 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 22.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 22 (torch.Size([128, 64, 2, 2])), Block block_0 (torch.Size([128, 256])).
I0315 16:23:52.258404 140273276859584 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 14.block_0 (torch.Size([0])) for Parameter 14 (torch.Size([64, 128, 3, 3])), Block block_0 (torch.Size([64, 384, 3])).
I0315 16:23:52.258416 139701292631232 shampoo_preconditioner_list.py:615] Rank 0: ShampooPreconditionerList Bytes Breakdown: (663552,)
I0315 16:23:52.258425 140269333836992 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 5.block_0 (torch.Size([0])) for Parameter 5 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 16:23:52.258450 139701292631232 shampoo_preconditioner_list.py:618] Rank 0: ShampooPreconditionerList Total Elements: 19350298
I0315 16:23:52.258486 139701292631232 shampoo_preconditioner_list.py:621] Rank 0: ShampooPreconditionerList Total Bytes: 663552
I0315 16:23:52.258489 140269333836992 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 6.block_0 (torch.Size([0])) for Parameter 6 (torch.Size([256, 128, 3, 3])), Block block_0 (torch.Size([256, 384, 3])).
I0315 16:23:52.258501 140273276859584 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 15.block_0 (torch.Size([64, 576])) for Parameter 15 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 16:23:52.258499 140421256889536 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([32, 1, 3, 3])), Block block_0 (torch.Size([288])).
I0315 16:23:52.258557 139933674378432 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 12.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 12 (torch.Size([128, 256, 3, 3])), Block block_0 (torch.Size([128, 768, 3])).
I0315 16:23:52.258585 140421256889536 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 16:23:52.258589 140273276859584 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 16.block_0 (torch.Size([0])) for Parameter 16 (torch.Size([32, 64, 3, 3])), Block block_0 (torch.Size([32, 576])).
I0315 16:23:52.258603 140269333836992 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 7.block_0 (torch.Size([256, 768, 3])) for Parameter 7 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 16:23:52.258620 140496964252864 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 23.block_0 ([torch.Size([64, 64]), torch.Size([128, 128])]) for Parameter 23 (torch.Size([64, 32, 2, 2])), Block block_0 (torch.Size([64, 128])).
I0315 16:23:52.258650 140421256889536 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([64, 32, 3, 3])), Block block_0 (torch.Size([64, 288])).
I0315 16:23:52.258673 140269333836992 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 8.block_0 (torch.Size([0])) for Parameter 8 (torch.Size([512, 256, 3, 3])), Block block_0 (torch.Size([512, 768, 3])).
I0315 16:23:52.258673 140273276859584 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 17.block_0 (torch.Size([1024, 9])) for Parameter 17 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 16:23:52.258702 140421256889536 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 16:23:52.258689 139701292631232 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([32, 1, 3, 3])), Block block_0 (torch.Size([288])).
I0315 16:23:52.258733 140273276859584 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 18.block_0 (torch.Size([0])) for Parameter 18 (torch.Size([1, 32, 1, 1])), Block block_0 (torch.Size([32])).
I0315 16:23:52.258741 140269333836992 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 9.block_0 (torch.Size([0])) for Parameter 9 (torch.Size([512, 512, 3, 3])), Block block_0 (torch.Size([512, 512, 9])).
I0315 16:23:52.258766 140421256889536 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 4.block_0 (torch.Size([0])) for Parameter 4 (torch.Size([128, 64, 3, 3])), Block block_0 (torch.Size([128, 576])).
I0315 16:23:52.258768 139701292631232 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 16:23:52.258768 140496964252864 shampoo_preconditioner_list.py:612] Rank 7: ShampooPreconditionerList Numel Breakdown: (165888, 2097314, 174080, 671744, 696320, 327698, 426002, 1310738, 1703954, 1048738, 655522, 1310738, 1212434, 327698, 303122, 671744, 665600, 2097314, 2048, 2, 2621440, 655360, 163840, 40960)
I0315 16:23:52.258769 139933674378432 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 13.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 13 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 16:23:52.258800 140273276859584 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 19.block_0 (torch.Size([0])) for Parameter 19 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0315 16:23:52.258803 140269333836992 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 10.block_0 (torch.Size([0])) for Parameter 10 (torch.Size([256, 512, 3, 3])), Block block_0 (torch.Size([256, 512, 9])).
I0315 16:23:52.258816 140496964252864 shampoo_preconditioner_list.py:615] Rank 7: ShampooPreconditionerList Bytes Breakdown: (663552, 8389256, 696320, 2686976, 2785280, 1310792)
I0315 16:23:52.258823 139701292631232 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([64, 32, 3, 3])), Block block_0 (torch.Size([64, 288])).
I0315 16:23:52.258829 140421256889536 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 5.block_0 (torch.Size([0])) for Parameter 5 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 16:23:52.258851 140496964252864 shampoo_preconditioner_list.py:618] Rank 7: ShampooPreconditionerList Total Elements: 19350298
I0315 16:23:52.258856 140269333836992 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 11.block_0 (torch.Size([0])) for Parameter 11 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 16:23:52.258882 140421256889536 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 6.block_0 (torch.Size([0])) for Parameter 6 (torch.Size([256, 128, 3, 3])), Block block_0 (torch.Size([256, 384, 3])).
I0315 16:23:52.258885 139701292631232 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 16:23:52.258891 140496964252864 shampoo_preconditioner_list.py:621] Rank 7: ShampooPreconditionerList Total Bytes: 16532176
I0315 16:23:52.258887 140273276859584 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 20.block_0 (torch.Size([512, 1024])) for Parameter 20 (torch.Size([512, 256, 2, 2])), Block block_0 (torch.Size([512, 1024])).
I0315 16:23:52.258908 140269333836992 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 12.block_0 (torch.Size([0])) for Parameter 12 (torch.Size([128, 256, 3, 3])), Block block_0 (torch.Size([128, 768, 3])).
I0315 16:23:52.258938 139701292631232 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 4.block_0 (torch.Size([0])) for Parameter 4 (torch.Size([128, 64, 3, 3])), Block block_0 (torch.Size([128, 576])).
I0315 16:23:52.258945 140421256889536 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 7.block_0 (torch.Size([0])) for Parameter 7 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 16:23:52.258948 140273276859584 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 21.block_0 (torch.Size([0])) for Parameter 21 (torch.Size([256, 128, 2, 2])), Block block_0 (torch.Size([256, 512])).
I0315 16:23:52.258967 140269333836992 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 13.block_0 (torch.Size([0])) for Parameter 13 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 16:23:52.258965 139933674378432 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 14.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 14 (torch.Size([64, 128, 3, 3])), Block block_0 (torch.Size([64, 384, 3])).
I0315 16:23:52.258997 140421256889536 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 8.block_0 (torch.Size([0])) for Parameter 8 (torch.Size([512, 256, 3, 3])), Block block_0 (torch.Size([512, 768, 3])).
I0315 16:23:52.258998 140273276859584 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 22.block_0 (torch.Size([0])) for Parameter 22 (torch.Size([128, 64, 2, 2])), Block block_0 (torch.Size([128, 256])).
I0315 16:23:52.258999 139701292631232 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 5.block_0 (torch.Size([0])) for Parameter 5 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 16:23:52.259025 140269333836992 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 14.block_0 (torch.Size([0])) for Parameter 14 (torch.Size([64, 128, 3, 3])), Block block_0 (torch.Size([64, 384, 3])).
I0315 16:23:52.259049 140273276859584 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 23.block_0 (torch.Size([0])) for Parameter 23 (torch.Size([64, 32, 2, 2])), Block block_0 (torch.Size([64, 128])).
I0315 16:23:52.259052 139701292631232 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 6.block_0 (torch.Size([0])) for Parameter 6 (torch.Size([256, 128, 3, 3])), Block block_0 (torch.Size([256, 384, 3])).
I0315 16:23:52.259054 140421256889536 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 9.block_0 (torch.Size([0])) for Parameter 9 (torch.Size([512, 512, 3, 3])), Block block_0 (torch.Size([512, 512, 9])).
I0315 16:23:52.259063 140496964252864 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([32, 1, 3, 3])), Block block_0 (torch.Size([288])).
I0315 16:23:52.259082 140269333836992 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 15.block_0 (torch.Size([0])) for Parameter 15 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 16:23:52.259110 139701292631232 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 7.block_0 (torch.Size([0])) for Parameter 7 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 16:23:52.259125 140273276859584 shampoo_preconditioner_list.py:375] Rank 5: AdaGradPreconditionerList Numel Breakdown: (0, 0, 0, 36864, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 36864, 0, 9216, 0, 0, 524288, 0, 0, 0)
I0315 16:23:52.259132 140269333836992 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 16.block_0 (torch.Size([0])) for Parameter 16 (torch.Size([32, 64, 3, 3])), Block block_0 (torch.Size([32, 576])).
I0315 16:23:52.259135 139933674378432 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 15.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 15 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 16:23:52.259161 140273276859584 shampoo_preconditioner_list.py:378] Rank 5: AdaGradPreconditionerList Bytes Breakdown: (0, 0, 0, 147456, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 147456, 0, 36864, 0, 0, 2097152, 0, 0, 0)
I0315 16:23:52.259160 140421256889536 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 10.block_0 (torch.Size([256, 512, 9])) for Parameter 10 (torch.Size([256, 512, 3, 3])), Block block_0 (torch.Size([256, 512, 9])).
I0315 16:23:52.259170 139701292631232 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 8.block_0 (torch.Size([0])) for Parameter 8 (torch.Size([512, 256, 3, 3])), Block block_0 (torch.Size([512, 768, 3])).
I0315 16:23:52.259182 140269333836992 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 17.block_0 (torch.Size([0])) for Parameter 17 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 16:23:52.259191 140273276859584 shampoo_preconditioner_list.py:381] Rank 5: AdaGradPreconditionerList Total Elements: 607232
I0315 16:23:52.259190 140496964252864 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([1024, 9])) for Parameter 1 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 16:23:52.259221 140273276859584 shampoo_preconditioner_list.py:384] Rank 5: AdaGradPreconditionerList Total Bytes: 2428928
I0315 16:23:52.259232 140269333836992 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 18.block_0 (torch.Size([0])) for Parameter 18 (torch.Size([1, 32, 1, 1])), Block block_0 (torch.Size([32])).
I0315 16:23:52.259214 140297289000128 logger_utils.py:262] Unable to record workload.train_mean information. Continuing without it.
I0315 16:23:52.259240 140421256889536 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 11.block_0 (torch.Size([0])) for Parameter 11 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 16:23:52.259273 140496964252864 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([64, 32, 3, 3])), Block block_0 (torch.Size([64, 288])).
I0315 16:23:52.259278 140297289000128 logger_utils.py:262] Unable to record workload.train_stddev information. Continuing without it.
I0315 16:23:52.259274 139701292631232 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 9.block_0 (torch.Size([512, 512, 9])) for Parameter 9 (torch.Size([512, 512, 3, 3])), Block block_0 (torch.Size([512, 512, 9])).
I0315 16:23:52.259287 140269333836992 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 19.block_0 (torch.Size([0])) for Parameter 19 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0315 16:23:52.259292 140421256889536 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 12.block_0 (torch.Size([0])) for Parameter 12 (torch.Size([128, 256, 3, 3])), Block block_0 (torch.Size([128, 768, 3])).
I0315 16:23:52.259298 139933674378432 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 16.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 16 (torch.Size([32, 64, 3, 3])), Block block_0 (torch.Size([32, 576])).
I0315 16:23:52.259335 140496964252864 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 16:23:52.259338 140269333836992 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 20.block_0 (torch.Size([0])) for Parameter 20 (torch.Size([512, 256, 2, 2])), Block block_0 (torch.Size([512, 1024])).
I0315 16:23:52.259342 139701292631232 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 10.block_0 (torch.Size([0])) for Parameter 10 (torch.Size([256, 512, 3, 3])), Block block_0 (torch.Size([256, 512, 9])).
I0315 16:23:52.259346 140421256889536 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 13.block_0 (torch.Size([0])) for Parameter 13 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 16:23:52.259396 140421256889536 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 14.block_0 (torch.Size([0])) for Parameter 14 (torch.Size([64, 128, 3, 3])), Block block_0 (torch.Size([64, 384, 3])).
I0315 16:23:52.259395 140269333836992 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 21.block_0 (torch.Size([0])) for Parameter 21 (torch.Size([256, 128, 2, 2])), Block block_0 (torch.Size([256, 512])).
I0315 16:23:52.259396 139701292631232 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 11.block_0 (torch.Size([0])) for Parameter 11 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 16:23:52.259382 139783847388352 logger_utils.py:262] Unable to record workload.train_mean information. Continuing without it.
I0315 16:23:52.259446 140269333836992 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 22.block_0 (torch.Size([0])) for Parameter 22 (torch.Size([128, 64, 2, 2])), Block block_0 (torch.Size([128, 256])).
I0315 16:23:52.259445 139783847388352 logger_utils.py:262] Unable to record workload.train_stddev information. Continuing without it.
I0315 16:23:52.259447 139701292631232 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 12.block_0 (torch.Size([0])) for Parameter 12 (torch.Size([128, 256, 3, 3])), Block block_0 (torch.Size([128, 768, 3])).
I0315 16:23:52.259454 140421256889536 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 15.block_0 (torch.Size([0])) for Parameter 15 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 16:23:52.259450 139933674378432 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 17.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 17 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 16:23:52.259498 139701292631232 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 13.block_0 (torch.Size([0])) for Parameter 13 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 16:23:52.259504 140421256889536 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 16.block_0 (torch.Size([0])) for Parameter 16 (torch.Size([32, 64, 3, 3])), Block block_0 (torch.Size([32, 576])).
I0315 16:23:52.259504 140269333836992 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 23.block_0 (torch.Size([0])) for Parameter 23 (torch.Size([64, 32, 2, 2])), Block block_0 (torch.Size([64, 128])).
I0315 16:23:52.259552 140421256889536 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 17.block_0 (torch.Size([0])) for Parameter 17 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 16:23:52.259558 139701292631232 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 14.block_0 (torch.Size([0])) for Parameter 14 (torch.Size([64, 128, 3, 3])), Block block_0 (torch.Size([64, 384, 3])).
I0315 16:23:52.259573 140269333836992 shampoo_preconditioner_list.py:375] Rank 3: AdaGradPreconditionerList Numel Breakdown: (0, 0, 18432, 0, 0, 0, 0, 589824, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)
I0315 16:23:52.259608 140269333836992 shampoo_preconditioner_list.py:378] Rank 3: AdaGradPreconditionerList Bytes Breakdown: (0, 0, 73728, 0, 0, 0, 0, 2359296, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)
I0315 16:23:52.259609 140421256889536 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 18.block_0 (torch.Size([0])) for Parameter 18 (torch.Size([1, 32, 1, 1])), Block block_0 (torch.Size([32])).
I0315 16:23:52.259609 139701292631232 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 15.block_0 (torch.Size([0])) for Parameter 15 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 16:23:52.259645 140269333836992 shampoo_preconditioner_list.py:381] Rank 3: AdaGradPreconditionerList Total Elements: 608256
I0315 16:23:52.259656 140421256889536 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 19.block_0 (torch.Size([0])) for Parameter 19 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0315 16:23:52.259659 139701292631232 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 16.block_0 (torch.Size([0])) for Parameter 16 (torch.Size([32, 64, 3, 3])), Block block_0 (torch.Size([32, 576])).
I0315 16:23:52.259681 140269333836992 shampoo_preconditioner_list.py:384] Rank 3: AdaGradPreconditionerList Total Bytes: 2433024
I0315 16:23:52.259660 139933674378432 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 18.block_0 ([torch.Size([32, 32])]) for Parameter 18 (torch.Size([1, 32, 1, 1])), Block block_0 (torch.Size([32])).
I0315 16:23:52.259704 140421256889536 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 20.block_0 (torch.Size([0])) for Parameter 20 (torch.Size([512, 256, 2, 2])), Block block_0 (torch.Size([512, 1024])).
I0315 16:23:52.259716 139701292631232 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 17.block_0 (torch.Size([0])) for Parameter 17 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 16:23:52.259790 139701292631232 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 18.block_0 (torch.Size([0])) for Parameter 18 (torch.Size([1, 32, 1, 1])), Block block_0 (torch.Size([32])).
I0315 16:23:52.259790 140421256889536 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 21.block_0 (torch.Size([0])) for Parameter 21 (torch.Size([256, 128, 2, 2])), Block block_0 (torch.Size([256, 512])).
I0315 16:23:52.259816 140496964252864 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 4.block_0 (torch.Size([128, 576])) for Parameter 4 (torch.Size([128, 64, 3, 3])), Block block_0 (torch.Size([128, 576])).
I0315 16:23:52.259846 140421256889536 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 22.block_0 (torch.Size([0])) for Parameter 22 (torch.Size([128, 64, 2, 2])), Block block_0 (torch.Size([128, 256])).
I0315 16:23:52.259847 139701292631232 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 19.block_0 (torch.Size([0])) for Parameter 19 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0315 16:23:52.259877 139933674378432 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 19.block_0 ([torch.Size([1, 1])]) for Parameter 19 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0315 16:23:52.259898 140421256889536 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 23.block_0 (torch.Size([0])) for Parameter 23 (torch.Size([64, 32, 2, 2])), Block block_0 (torch.Size([64, 128])).
I0315 16:23:52.259901 139701292631232 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 20.block_0 (torch.Size([0])) for Parameter 20 (torch.Size([512, 256, 2, 2])), Block block_0 (torch.Size([512, 1024])).
I0315 16:23:52.259937 140496964252864 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 5.block_0 (torch.Size([0])) for Parameter 5 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 16:23:52.259951 139701292631232 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 21.block_0 (torch.Size([0])) for Parameter 21 (torch.Size([256, 128, 2, 2])), Block block_0 (torch.Size([256, 512])).
I0315 16:23:52.259966 140421256889536 shampoo_preconditioner_list.py:375] Rank 2: AdaGradPreconditionerList Numel Breakdown: (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1179648, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)
I0315 16:23:52.260002 140421256889536 shampoo_preconditioner_list.py:378] Rank 2: AdaGradPreconditionerList Bytes Breakdown: (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4718592, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)
I0315 16:23:52.260003 139701292631232 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 22.block_0 (torch.Size([0])) for Parameter 22 (torch.Size([128, 64, 2, 2])), Block block_0 (torch.Size([128, 256])).
I0315 16:23:52.260008 140496964252864 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 6.block_0 (torch.Size([0])) for Parameter 6 (torch.Size([256, 128, 3, 3])), Block block_0 (torch.Size([256, 384, 3])).
I0315 16:23:52.260040 140421256889536 shampoo_preconditioner_list.py:381] Rank 2: AdaGradPreconditionerList Total Elements: 1179648
I0315 16:23:52.260053 139701292631232 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 23.block_0 (torch.Size([0])) for Parameter 23 (torch.Size([64, 32, 2, 2])), Block block_0 (torch.Size([64, 128])).
I0315 16:23:52.260076 140421256889536 shampoo_preconditioner_list.py:384] Rank 2: AdaGradPreconditionerList Total Bytes: 4718592
I0315 16:23:52.260073 140496964252864 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 7.block_0 (torch.Size([0])) for Parameter 7 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 16:23:52.260070 139933674378432 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 20.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 20 (torch.Size([512, 256, 2, 2])), Block block_0 (torch.Size([512, 1024])).
I0315 16:23:52.260123 139701292631232 shampoo_preconditioner_list.py:375] Rank 0: AdaGradPreconditionerList Numel Breakdown: (0, 0, 0, 0, 0, 0, 0, 0, 0, 2359296, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)
I0315 16:23:52.260129 140496964252864 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 8.block_0 (torch.Size([0])) for Parameter 8 (torch.Size([512, 256, 3, 3])), Block block_0 (torch.Size([512, 768, 3])).
I0315 16:23:52.260169 139701292631232 shampoo_preconditioner_list.py:378] Rank 0: AdaGradPreconditionerList Bytes Breakdown: (0, 0, 0, 0, 0, 0, 0, 0, 0, 9437184, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)
I0315 16:23:52.260184 140496964252864 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 9.block_0 (torch.Size([0])) for Parameter 9 (torch.Size([512, 512, 3, 3])), Block block_0 (torch.Size([512, 512, 9])).
I0315 16:23:52.260207 139701292631232 shampoo_preconditioner_list.py:381] Rank 0: AdaGradPreconditionerList Total Elements: 2359296
I0315 16:23:52.260243 139701292631232 shampoo_preconditioner_list.py:384] Rank 0: AdaGradPreconditionerList Total Bytes: 9437184
I0315 16:23:52.260246 140496964252864 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 10.block_0 (torch.Size([0])) for Parameter 10 (torch.Size([256, 512, 3, 3])), Block block_0 (torch.Size([256, 512, 9])).
I0315 16:23:52.260302 140496964252864 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 11.block_0 (torch.Size([0])) for Parameter 11 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 16:23:52.260405 140496964252864 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 12.block_0 (torch.Size([128, 768, 3])) for Parameter 12 (torch.Size([128, 256, 3, 3])), Block block_0 (torch.Size([128, 768, 3])).
I0315 16:23:52.260401 140273276859584 logger_utils.py:262] Unable to record workload.train_mean information. Continuing without it.
I0315 16:23:52.260466 140273276859584 logger_utils.py:262] Unable to record workload.train_stddev information. Continuing without it.
I0315 16:23:52.260503 140496964252864 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 13.block_0 (torch.Size([128, 384, 3])) for Parameter 13 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 16:23:52.260594 140496964252864 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 14.block_0 (torch.Size([64, 384, 3])) for Parameter 14 (torch.Size([64, 128, 3, 3])), Block block_0 (torch.Size([64, 384, 3])).
I0315 16:23:52.260668 140496964252864 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 15.block_0 (torch.Size([0])) for Parameter 15 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 16:23:52.260738 140496964252864 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 16.block_0 (torch.Size([0])) for Parameter 16 (torch.Size([32, 64, 3, 3])), Block block_0 (torch.Size([32, 576])).
I0315 16:23:52.260752 139933674378432 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 21.block_0 ([torch.Size([256, 256]), torch.Size([512, 512])]) for Parameter 21 (torch.Size([256, 128, 2, 2])), Block block_0 (torch.Size([256, 512])).
I0315 16:23:52.260800 140496964252864 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 17.block_0 (torch.Size([0])) for Parameter 17 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 16:23:52.260855 140496964252864 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 18.block_0 (torch.Size([0])) for Parameter 18 (torch.Size([1, 32, 1, 1])), Block block_0 (torch.Size([32])).
I0315 16:23:52.260857 140269333836992 logger_utils.py:262] Unable to record workload.train_mean information. Continuing without it.
I0315 16:23:52.260924 140496964252864 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 19.block_0 (torch.Size([0])) for Parameter 19 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0315 16:23:52.260947 140269333836992 logger_utils.py:262] Unable to record workload.train_stddev information. Continuing without it.
I0315 16:23:52.260982 140496964252864 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 20.block_0 (torch.Size([0])) for Parameter 20 (torch.Size([512, 256, 2, 2])), Block block_0 (torch.Size([512, 1024])).
I0315 16:23:52.261040 140496964252864 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 21.block_0 (torch.Size([0])) for Parameter 21 (torch.Size([256, 128, 2, 2])), Block block_0 (torch.Size([256, 512])).
I0315 16:23:52.261063 139933674378432 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 22.block_0 ([torch.Size([128, 128]), torch.Size([256, 256])]) for Parameter 22 (torch.Size([128, 64, 2, 2])), Block block_0 (torch.Size([128, 256])).
I0315 16:23:52.261096 140496964252864 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 22.block_0 (torch.Size([0])) for Parameter 22 (torch.Size([128, 64, 2, 2])), Block block_0 (torch.Size([128, 256])).
I0315 16:23:52.261100 139701292631232 submission.py:142] No large parameters detected! Continuing with only Shampoo....
I0315 16:23:52.261186 140496964252864 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 23.block_0 (torch.Size([64, 128])) for Parameter 23 (torch.Size([64, 32, 2, 2])), Block block_0 (torch.Size([64, 128])).
I0315 16:23:52.261175 140421256889536 logger_utils.py:262] Unable to record workload.train_mean information. Continuing without it.
I0315 16:23:52.261239 140421256889536 logger_utils.py:262] Unable to record workload.train_stddev information. Continuing without it.
I0315 16:23:52.261250 139933674378432 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 23.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 23 (torch.Size([64, 32, 2, 2])), Block block_0 (torch.Size([64, 128])).
I0315 16:23:52.261270 140496964252864 shampoo_preconditioner_list.py:375] Rank 7: AdaGradPreconditionerList Numel Breakdown: (0, 9216, 0, 0, 73728, 0, 0, 0, 0, 0, 0, 0, 294912, 147456, 73728, 0, 0, 0, 0, 0, 0, 0, 0, 8192)
I0315 16:23:52.261297 139701292631232 submission_runner.py:279] Initializing metrics bundle.
I0315 16:23:52.261314 140496964252864 shampoo_preconditioner_list.py:378] Rank 7: AdaGradPreconditionerList Bytes Breakdown: (0, 36864, 0, 0, 294912, 0, 0, 0, 0, 0, 0, 0, 1179648, 589824, 294912, 0, 0, 0, 0, 0, 0, 0, 0, 32768)
I0315 16:23:52.261346 140496964252864 shampoo_preconditioner_list.py:381] Rank 7: AdaGradPreconditionerList Total Elements: 607232
I0315 16:23:52.261381 140496964252864 shampoo_preconditioner_list.py:384] Rank 7: AdaGradPreconditionerList Total Bytes: 2428928
I0315 16:23:52.261380 139933674378432 shampoo_preconditioner_list.py:612] Rank 6: ShampooPreconditionerList Numel Breakdown: (165888, 2097314, 174080, 671744, 696320, 327698, 426002, 1310738, 1703954, 1048738, 655522, 1310738, 1212434, 327698, 303122, 671744, 665600, 2097314, 2048, 2, 2621440, 655360, 163840, 40960)
I0315 16:23:52.261426 139933674378432 shampoo_preconditioner_list.py:615] Rank 6: ShampooPreconditionerList Bytes Breakdown: (663552, 8389256, 696320, 2686976, 2785280, 1310792, 1704008)
I0315 16:23:52.261424 139701292631232 submission_runner.py:301] Initializing checkpoint and logger.
I0315 16:23:52.261473 139933674378432 shampoo_preconditioner_list.py:618] Rank 6: ShampooPreconditionerList Total Elements: 19350298
I0315 16:23:52.261515 139933674378432 shampoo_preconditioner_list.py:621] Rank 6: ShampooPreconditionerList Total Bytes: 18236184
I0315 16:23:52.261826 139933674378432 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([288])) for Parameter 0 (torch.Size([32, 1, 3, 3])), Block block_0 (torch.Size([288])).
I0315 16:23:52.261898 139701292631232 submission_runner.py:321] Saving meta data to /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_1/fastmri_pytorch/trial_1/meta_data_0.json.
I0315 16:23:52.261952 139933674378432 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 16:23:52.262032 139933674378432 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([64, 32, 3, 3])), Block block_0 (torch.Size([64, 288])).
I0315 16:23:52.262075 139701292631232 logger_utils.py:262] Unable to record workload.train_mean information. Continuing without it.
I0315 16:23:52.262104 139933674378432 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 16:23:52.262125 139701292631232 logger_utils.py:262] Unable to record workload.train_stddev information. Continuing without it.
I0315 16:23:52.262175 139933674378432 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 4.block_0 (torch.Size([0])) for Parameter 4 (torch.Size([128, 64, 3, 3])), Block block_0 (torch.Size([128, 576])).
I0315 16:23:52.262659 139933674378432 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 5.block_0 (torch.Size([128, 384, 3])) for Parameter 5 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 16:23:52.262821 139933674378432 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 6.block_0 (torch.Size([256, 384, 3])) for Parameter 6 (torch.Size([256, 128, 3, 3])), Block block_0 (torch.Size([256, 384, 3])).
I0315 16:23:52.262939 139933674378432 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 7.block_0 (torch.Size([0])) for Parameter 7 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 16:23:52.263016 139933674378432 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 8.block_0 (torch.Size([0])) for Parameter 8 (torch.Size([512, 256, 3, 3])), Block block_0 (torch.Size([512, 768, 3])).
I0315 16:23:52.263051 140496964252864 logger_utils.py:262] Unable to record workload.train_mean information. Continuing without it.
I0315 16:23:52.263087 139933674378432 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 9.block_0 (torch.Size([0])) for Parameter 9 (torch.Size([512, 512, 3, 3])), Block block_0 (torch.Size([512, 512, 9])).
I0315 16:23:52.263128 140496964252864 logger_utils.py:262] Unable to record workload.train_stddev information. Continuing without it.
I0315 16:23:52.263148 139933674378432 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 10.block_0 (torch.Size([0])) for Parameter 10 (torch.Size([256, 512, 3, 3])), Block block_0 (torch.Size([256, 512, 9])).
I0315 16:23:52.263216 139933674378432 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 11.block_0 (torch.Size([0])) for Parameter 11 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 16:23:52.263307 139933674378432 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 12.block_0 (torch.Size([0])) for Parameter 12 (torch.Size([128, 256, 3, 3])), Block block_0 (torch.Size([128, 768, 3])).
I0315 16:23:52.263390 139933674378432 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 13.block_0 (torch.Size([0])) for Parameter 13 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 16:23:52.263456 139933674378432 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 14.block_0 (torch.Size([0])) for Parameter 14 (torch.Size([64, 128, 3, 3])), Block block_0 (torch.Size([64, 384, 3])).
I0315 16:23:52.263539 139933674378432 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 15.block_0 (torch.Size([0])) for Parameter 15 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 16:23:52.263610 139933674378432 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 16.block_0 (torch.Size([0])) for Parameter 16 (torch.Size([32, 64, 3, 3])), Block block_0 (torch.Size([32, 576])).
I0315 16:23:52.263687 139933674378432 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 17.block_0 (torch.Size([0])) for Parameter 17 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 16:23:52.263799 139933674378432 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 18.block_0 (torch.Size([32])) for Parameter 18 (torch.Size([1, 32, 1, 1])), Block block_0 (torch.Size([32])).
I0315 16:23:52.263902 139933674378432 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 19.block_0 (torch.Size([1])) for Parameter 19 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0315 16:23:52.263986 139933674378432 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 20.block_0 (torch.Size([0])) for Parameter 20 (torch.Size([512, 256, 2, 2])), Block block_0 (torch.Size([512, 1024])).
I0315 16:23:52.264109 139933674378432 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 21.block_0 (torch.Size([256, 512])) for Parameter 21 (torch.Size([256, 128, 2, 2])), Block block_0 (torch.Size([256, 512])).
I0315 16:23:52.264225 139933674378432 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 22.block_0 (torch.Size([128, 256])) for Parameter 22 (torch.Size([128, 64, 2, 2])), Block block_0 (torch.Size([128, 256])).
I0315 16:23:52.264329 139933674378432 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 23.block_0 (torch.Size([0])) for Parameter 23 (torch.Size([64, 32, 2, 2])), Block block_0 (torch.Size([64, 128])).
I0315 16:23:52.264419 139933674378432 shampoo_preconditioner_list.py:375] Rank 6: AdaGradPreconditionerList Numel Breakdown: (288, 0, 0, 0, 0, 147456, 294912, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 32, 1, 0, 131072, 32768, 0)
I0315 16:23:52.264467 139933674378432 shampoo_preconditioner_list.py:378] Rank 6: AdaGradPreconditionerList Bytes Breakdown: (1152, 0, 0, 0, 0, 589824, 1179648, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 128, 4, 0, 524288, 131072, 0)
I0315 16:23:52.264509 139933674378432 shampoo_preconditioner_list.py:381] Rank 6: AdaGradPreconditionerList Total Elements: 606529
I0315 16:23:52.264550 139933674378432 shampoo_preconditioner_list.py:384] Rank 6: AdaGradPreconditionerList Total Bytes: 2426116
I0315 16:23:52.266384 139933674378432 logger_utils.py:262] Unable to record workload.train_mean information. Continuing without it.
I0315 16:23:52.266469 139933674378432 logger_utils.py:262] Unable to record workload.train_stddev information. Continuing without it.
I0315 16:23:52.681055 139701292631232 submission_runner.py:325] Saving flags to /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_1/fastmri_pytorch/trial_1/flags_0.json.
I0315 16:23:52.715113 139701292631232 submission_runner.py:337] Starting training loop.
[rank7]:W0315 16:23:52.749000 51 site-packages/torch/_logging/_internal.py:1081] [0/0] Profiler function <class 'torch.autograd.profiler.record_function'> will be ignored
[rank5]:W0315 16:23:52.749000 49 site-packages/torch/_logging/_internal.py:1081] [0/0] Profiler function <class 'torch.autograd.profiler.record_function'> will be ignored
[rank3]:W0315 16:23:52.749000 47 site-packages/torch/_logging/_internal.py:1081] [0/0] Profiler function <class 'torch.autograd.profiler.record_function'> will be ignored
[rank4]:W0315 16:23:52.749000 48 site-packages/torch/_logging/_internal.py:1081] [0/0] Profiler function <class 'torch.autograd.profiler.record_function'> will be ignored
[rank1]:W0315 16:23:52.749000 45 site-packages/torch/_logging/_internal.py:1081] [0/0] Profiler function <class 'torch.autograd.profiler.record_function'> will be ignored
[rank2]:W0315 16:23:52.749000 46 site-packages/torch/_logging/_internal.py:1081] [0/0] Profiler function <class 'torch.autograd.profiler.record_function'> will be ignored
[rank6]:W0315 16:23:52.750000 50 site-packages/torch/_logging/_internal.py:1081] [0/0] Profiler function <class 'torch.autograd.profiler.record_function'> will be ignored
[rank0]:W0315 16:24:29.130000 44 site-packages/torch/_logging/_internal.py:1081] [0/0] Profiler function <class 'torch.autograd.profiler.record_function'> will be ignored
[rank1]:W0315 16:25:04.766000 45 site-packages/torch/_dynamo/convert_frame.py:844] [2/8] torch._dynamo hit config.cache_size_limit (8)
[rank1]:W0315 16:25:04.766000 45 site-packages/torch/_dynamo/convert_frame.py:844] [2/8]    function: 'forward' (/algorithmic-efficiency/algoperf/workloads/fastmri/fastmri_pytorch/models.py:141)
[rank1]:W0315 16:25:04.766000 45 site-packages/torch/_dynamo/convert_frame.py:844] [2/8]    last reason: 2/0: tensor 'L['x']' requires_grad mismatch. expected requires_grad=0
[rank1]:W0315 16:25:04.766000 45 site-packages/torch/_dynamo/convert_frame.py:844] [2/8] To log all recompilation reasons, use TORCH_LOGS="recompiles".
[rank1]:W0315 16:25:04.766000 45 site-packages/torch/_dynamo/convert_frame.py:844] [2/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.
[rank5]:W0315 16:25:04.782000 49 site-packages/torch/_dynamo/convert_frame.py:844] [2/8] torch._dynamo hit config.cache_size_limit (8)
[rank5]:W0315 16:25:04.782000 49 site-packages/torch/_dynamo/convert_frame.py:844] [2/8]    function: 'forward' (/algorithmic-efficiency/algoperf/workloads/fastmri/fastmri_pytorch/models.py:141)
[rank5]:W0315 16:25:04.782000 49 site-packages/torch/_dynamo/convert_frame.py:844] [2/8]    last reason: 2/0: tensor 'L['x']' requires_grad mismatch. expected requires_grad=0
[rank5]:W0315 16:25:04.782000 49 site-packages/torch/_dynamo/convert_frame.py:844] [2/8] To log all recompilation reasons, use TORCH_LOGS="recompiles".
[rank5]:W0315 16:25:04.782000 49 site-packages/torch/_dynamo/convert_frame.py:844] [2/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.
[rank6]:W0315 16:25:04.865000 50 site-packages/torch/_dynamo/convert_frame.py:844] [2/8] torch._dynamo hit config.cache_size_limit (8)
[rank6]:W0315 16:25:04.865000 50 site-packages/torch/_dynamo/convert_frame.py:844] [2/8]    function: 'forward' (/algorithmic-efficiency/algoperf/workloads/fastmri/fastmri_pytorch/models.py:141)
[rank6]:W0315 16:25:04.865000 50 site-packages/torch/_dynamo/convert_frame.py:844] [2/8]    last reason: 2/0: tensor 'L['x']' requires_grad mismatch. expected requires_grad=0
[rank6]:W0315 16:25:04.865000 50 site-packages/torch/_dynamo/convert_frame.py:844] [2/8] To log all recompilation reasons, use TORCH_LOGS="recompiles".
[rank6]:W0315 16:25:04.865000 50 site-packages/torch/_dynamo/convert_frame.py:844] [2/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.
[rank3]:W0315 16:25:04.957000 47 site-packages/torch/_dynamo/convert_frame.py:844] [2/8] torch._dynamo hit config.cache_size_limit (8)
[rank3]:W0315 16:25:04.957000 47 site-packages/torch/_dynamo/convert_frame.py:844] [2/8]    function: 'forward' (/algorithmic-efficiency/algoperf/workloads/fastmri/fastmri_pytorch/models.py:141)
[rank3]:W0315 16:25:04.957000 47 site-packages/torch/_dynamo/convert_frame.py:844] [2/8]    last reason: 2/0: tensor 'L['x']' requires_grad mismatch. expected requires_grad=0
[rank3]:W0315 16:25:04.957000 47 site-packages/torch/_dynamo/convert_frame.py:844] [2/8] To log all recompilation reasons, use TORCH_LOGS="recompiles".
[rank3]:W0315 16:25:04.957000 47 site-packages/torch/_dynamo/convert_frame.py:844] [2/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.
[rank2]:W0315 16:25:05.019000 46 site-packages/torch/_dynamo/convert_frame.py:844] [2/8] torch._dynamo hit config.cache_size_limit (8)
[rank2]:W0315 16:25:05.019000 46 site-packages/torch/_dynamo/convert_frame.py:844] [2/8]    function: 'forward' (/algorithmic-efficiency/algoperf/workloads/fastmri/fastmri_pytorch/models.py:141)
[rank2]:W0315 16:25:05.019000 46 site-packages/torch/_dynamo/convert_frame.py:844] [2/8]    last reason: 2/0: tensor 'L['x']' requires_grad mismatch. expected requires_grad=0
[rank2]:W0315 16:25:05.019000 46 site-packages/torch/_dynamo/convert_frame.py:844] [2/8] To log all recompilation reasons, use TORCH_LOGS="recompiles".
[rank2]:W0315 16:25:05.019000 46 site-packages/torch/_dynamo/convert_frame.py:844] [2/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.
[rank7]:W0315 16:25:05.217000 51 site-packages/torch/_dynamo/convert_frame.py:844] [2/8] torch._dynamo hit config.cache_size_limit (8)
[rank7]:W0315 16:25:05.217000 51 site-packages/torch/_dynamo/convert_frame.py:844] [2/8]    function: 'forward' (/algorithmic-efficiency/algoperf/workloads/fastmri/fastmri_pytorch/models.py:141)
[rank7]:W0315 16:25:05.217000 51 site-packages/torch/_dynamo/convert_frame.py:844] [2/8]    last reason: 2/0: tensor 'L['x']' requires_grad mismatch. expected requires_grad=0
[rank7]:W0315 16:25:05.217000 51 site-packages/torch/_dynamo/convert_frame.py:844] [2/8] To log all recompilation reasons, use TORCH_LOGS="recompiles".
[rank7]:W0315 16:25:05.217000 51 site-packages/torch/_dynamo/convert_frame.py:844] [2/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.
[rank4]:W0315 16:25:05.239000 48 site-packages/torch/_dynamo/convert_frame.py:844] [2/8] torch._dynamo hit config.cache_size_limit (8)
[rank4]:W0315 16:25:05.239000 48 site-packages/torch/_dynamo/convert_frame.py:844] [2/8]    function: 'forward' (/algorithmic-efficiency/algoperf/workloads/fastmri/fastmri_pytorch/models.py:141)
[rank4]:W0315 16:25:05.239000 48 site-packages/torch/_dynamo/convert_frame.py:844] [2/8]    last reason: 2/0: tensor 'L['x']' requires_grad mismatch. expected requires_grad=0
[rank4]:W0315 16:25:05.239000 48 site-packages/torch/_dynamo/convert_frame.py:844] [2/8] To log all recompilation reasons, use TORCH_LOGS="recompiles".
[rank4]:W0315 16:25:05.239000 48 site-packages/torch/_dynamo/convert_frame.py:844] [2/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.
[rank0]:W0315 16:25:07.614000 44 site-packages/torch/_dynamo/convert_frame.py:844] [2/8] torch._dynamo hit config.cache_size_limit (8)
[rank0]:W0315 16:25:07.614000 44 site-packages/torch/_dynamo/convert_frame.py:844] [2/8]    function: 'forward' (/algorithmic-efficiency/algoperf/workloads/fastmri/fastmri_pytorch/models.py:141)
[rank0]:W0315 16:25:07.614000 44 site-packages/torch/_dynamo/convert_frame.py:844] [2/8]    last reason: 2/0: tensor 'L['x']' requires_grad mismatch. expected requires_grad=0
[rank0]:W0315 16:25:07.614000 44 site-packages/torch/_dynamo/convert_frame.py:844] [2/8] To log all recompilation reasons, use TORCH_LOGS="recompiles".
[rank0]:W0315 16:25:07.614000 44 site-packages/torch/_dynamo/convert_frame.py:844] [2/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.
I0315 16:25:18.622164 139673889961728 logging_writer.py:48] [0] global_step=0, grad_norm=2.98464, loss=0.992029
I0315 16:25:18.821755 139701292631232 submission.py:265] 0) loss = 0.992, grad_norm = 2.985
I0315 16:25:19.597546 139701292631232 spec.py:321] Evaluating on the training split.
I0315 16:26:22.218504 139701292631232 spec.py:333] Evaluating on the validation split.
I0315 16:27:02.447430 139701292631232 spec.py:349] Evaluating on the test split.
I0315 16:27:25.127401 139701292631232 submission_runner.py:469] Time since start: 212.41s, 	Step: 1, 	{'train/ssim': 0.2271113055092948, 'train/loss': 0.946084703717913, 'validation/ssim': 0.2227439214892111, 'validation/loss': 0.9462951076252111, 'validation/num_examples': 3554, 'test/ssim': 0.24524549597703155, 'test/loss': 0.9454325591009843, 'test/num_examples': 3581, 'score': 86.10756826400757, 'total_duration': 212.41248416900635, 'accumulated_submission_time': 86.10756826400757, 'accumulated_eval_time': 125.53008151054382, 'accumulated_logging_time': 0}
I0315 16:27:25.136080 139650249254656 logging_writer.py:48] [1] accumulated_eval_time=125.53, accumulated_logging_time=0, accumulated_submission_time=86.1076, global_step=1, preemption_count=0, score=86.1076, test/loss=0.945433, test/num_examples=3581, test/ssim=0.245245, total_duration=212.412, train/loss=0.946085, train/ssim=0.227111, validation/loss=0.946295, validation/num_examples=3554, validation/ssim=0.222744
I0315 16:27:26.282454 139650240861952 logging_writer.py:48] [1] global_step=1, grad_norm=2.90788, loss=0.93168
I0315 16:27:26.285953 139701292631232 submission.py:265] 1) loss = 0.932, grad_norm = 2.908
I0315 16:27:26.391410 139650249254656 logging_writer.py:48] [2] global_step=2, grad_norm=3.50226, loss=0.95263
I0315 16:27:26.395869 139701292631232 submission.py:265] 2) loss = 0.953, grad_norm = 3.502
I0315 16:27:26.491262 139650240861952 logging_writer.py:48] [3] global_step=3, grad_norm=3.32544, loss=0.97896
I0315 16:27:26.496594 139701292631232 submission.py:265] 3) loss = 0.979, grad_norm = 3.325
I0315 16:27:26.584671 139650249254656 logging_writer.py:48] [4] global_step=4, grad_norm=2.86781, loss=0.929291
I0315 16:27:26.594731 139701292631232 submission.py:265] 4) loss = 0.929, grad_norm = 2.868
I0315 16:27:26.672943 139650240861952 logging_writer.py:48] [5] global_step=5, grad_norm=2.56942, loss=0.958664
I0315 16:27:26.678295 139701292631232 submission.py:265] 5) loss = 0.959, grad_norm = 2.569
I0315 16:27:26.761577 139650249254656 logging_writer.py:48] [6] global_step=6, grad_norm=3.10053, loss=0.958248
I0315 16:27:26.765514 139701292631232 submission.py:265] 6) loss = 0.958, grad_norm = 3.101
I0315 16:27:26.849589 139650240861952 logging_writer.py:48] [7] global_step=7, grad_norm=3.15473, loss=0.920673
I0315 16:27:26.853528 139701292631232 submission.py:265] 7) loss = 0.921, grad_norm = 3.155
I0315 16:27:26.946162 139650249254656 logging_writer.py:48] [8] global_step=8, grad_norm=2.68328, loss=0.842717
I0315 16:27:26.952142 139701292631232 submission.py:265] 8) loss = 0.843, grad_norm = 2.683
I0315 16:27:27.027881 139650240861952 logging_writer.py:48] [9] global_step=9, grad_norm=2.79865, loss=0.934886
I0315 16:27:27.033330 139701292631232 submission.py:265] 9) loss = 0.935, grad_norm = 2.799
I0315 16:27:27.113335 139650249254656 logging_writer.py:48] [10] global_step=10, grad_norm=2.57519, loss=0.834982
I0315 16:27:27.118574 139701292631232 submission.py:265] 10) loss = 0.835, grad_norm = 2.575
I0315 16:27:27.208923 139650240861952 logging_writer.py:48] [11] global_step=11, grad_norm=2.21536, loss=0.8669
I0315 16:27:27.214431 139701292631232 submission.py:265] 11) loss = 0.867, grad_norm = 2.215
I0315 16:27:27.291722 139650249254656 logging_writer.py:48] [12] global_step=12, grad_norm=2.50575, loss=0.84951
I0315 16:27:27.296069 139701292631232 submission.py:265] 12) loss = 0.850, grad_norm = 2.506
I0315 16:27:27.385464 139650240861952 logging_writer.py:48] [13] global_step=13, grad_norm=2.27245, loss=0.767133
I0315 16:27:27.395248 139701292631232 submission.py:265] 13) loss = 0.767, grad_norm = 2.272
I0315 16:27:27.481381 139650249254656 logging_writer.py:48] [14] global_step=14, grad_norm=2.04533, loss=0.758072
I0315 16:27:27.486564 139701292631232 submission.py:265] 14) loss = 0.758, grad_norm = 2.045
I0315 16:27:27.574456 139650240861952 logging_writer.py:48] [15] global_step=15, grad_norm=2.19025, loss=0.745234
I0315 16:27:27.579209 139701292631232 submission.py:265] 15) loss = 0.745, grad_norm = 2.190
I0315 16:27:27.664900 139650249254656 logging_writer.py:48] [16] global_step=16, grad_norm=1.92199, loss=0.756512
I0315 16:27:27.671769 139701292631232 submission.py:265] 16) loss = 0.757, grad_norm = 1.922
I0315 16:27:27.769711 139650240861952 logging_writer.py:48] [17] global_step=17, grad_norm=1.90661, loss=0.72293
I0315 16:27:27.774149 139701292631232 submission.py:265] 17) loss = 0.723, grad_norm = 1.907
I0315 16:27:27.857813 139650249254656 logging_writer.py:48] [18] global_step=18, grad_norm=2.04478, loss=0.682645
I0315 16:27:27.863111 139701292631232 submission.py:265] 18) loss = 0.683, grad_norm = 2.045
I0315 16:27:27.965289 139650240861952 logging_writer.py:48] [19] global_step=19, grad_norm=1.65824, loss=0.752198
I0315 16:27:27.977333 139701292631232 submission.py:265] 19) loss = 0.752, grad_norm = 1.658
I0315 16:27:28.066403 139650249254656 logging_writer.py:48] [20] global_step=20, grad_norm=1.67598, loss=0.680516
I0315 16:27:28.079255 139701292631232 submission.py:265] 20) loss = 0.681, grad_norm = 1.676
I0315 16:27:28.167279 139650240861952 logging_writer.py:48] [21] global_step=21, grad_norm=1.45084, loss=0.587965
I0315 16:27:28.179002 139701292631232 submission.py:265] 21) loss = 0.588, grad_norm = 1.451
I0315 16:27:28.262069 139650249254656 logging_writer.py:48] [22] global_step=22, grad_norm=1.56267, loss=0.656187
I0315 16:27:28.265350 139701292631232 submission.py:265] 22) loss = 0.656, grad_norm = 1.563
I0315 16:27:28.347106 139650240861952 logging_writer.py:48] [23] global_step=23, grad_norm=1.36288, loss=0.597014
I0315 16:27:28.354645 139701292631232 submission.py:265] 23) loss = 0.597, grad_norm = 1.363
I0315 16:27:28.441771 139650249254656 logging_writer.py:48] [24] global_step=24, grad_norm=1.17825, loss=0.655246
I0315 16:27:28.446945 139701292631232 submission.py:265] 24) loss = 0.655, grad_norm = 1.178
I0315 16:27:28.537043 139650240861952 logging_writer.py:48] [25] global_step=25, grad_norm=1.17803, loss=0.583276
I0315 16:27:28.541489 139701292631232 submission.py:265] 25) loss = 0.583, grad_norm = 1.178
I0315 16:27:28.629057 139650249254656 logging_writer.py:48] [26] global_step=26, grad_norm=1.1111, loss=0.551053
I0315 16:27:28.635515 139701292631232 submission.py:265] 26) loss = 0.551, grad_norm = 1.111
I0315 16:27:28.716126 139650240861952 logging_writer.py:48] [27] global_step=27, grad_norm=1.05895, loss=0.571659
I0315 16:27:28.722774 139701292631232 submission.py:265] 27) loss = 0.572, grad_norm = 1.059
I0315 16:27:28.802474 139650249254656 logging_writer.py:48] [28] global_step=28, grad_norm=1.09278, loss=0.563189
I0315 16:27:28.807014 139701292631232 submission.py:265] 28) loss = 0.563, grad_norm = 1.093
I0315 16:27:28.893146 139650240861952 logging_writer.py:48] [29] global_step=29, grad_norm=1.10012, loss=0.589815
I0315 16:27:28.898776 139701292631232 submission.py:265] 29) loss = 0.590, grad_norm = 1.100
I0315 16:27:28.994244 139650249254656 logging_writer.py:48] [30] global_step=30, grad_norm=0.912534, loss=0.559395
I0315 16:27:29.000091 139701292631232 submission.py:265] 30) loss = 0.559, grad_norm = 0.913
I0315 16:27:29.083124 139650240861952 logging_writer.py:48] [31] global_step=31, grad_norm=1.12131, loss=0.54912
I0315 16:27:29.088202 139701292631232 submission.py:265] 31) loss = 0.549, grad_norm = 1.121
I0315 16:27:29.174727 139650249254656 logging_writer.py:48] [32] global_step=32, grad_norm=1.20579, loss=0.578056
I0315 16:27:29.179389 139701292631232 submission.py:265] 32) loss = 0.578, grad_norm = 1.206
I0315 16:27:29.273721 139650240861952 logging_writer.py:48] [33] global_step=33, grad_norm=1.01155, loss=0.434507
I0315 16:27:29.279392 139701292631232 submission.py:265] 33) loss = 0.435, grad_norm = 1.012
I0315 16:27:29.362914 139650249254656 logging_writer.py:48] [34] global_step=34, grad_norm=1.16267, loss=0.530426
I0315 16:27:29.367681 139701292631232 submission.py:265] 34) loss = 0.530, grad_norm = 1.163
I0315 16:27:29.452558 139650240861952 logging_writer.py:48] [35] global_step=35, grad_norm=1.21873, loss=0.551536
I0315 16:27:29.457483 139701292631232 submission.py:265] 35) loss = 0.552, grad_norm = 1.219
I0315 16:27:29.546612 139650249254656 logging_writer.py:48] [36] global_step=36, grad_norm=1.10237, loss=0.471457
I0315 16:27:29.551174 139701292631232 submission.py:265] 36) loss = 0.471, grad_norm = 1.102
I0315 16:27:29.635653 139650240861952 logging_writer.py:48] [37] global_step=37, grad_norm=1.32453, loss=0.527051
I0315 16:27:29.640224 139701292631232 submission.py:265] 37) loss = 0.527, grad_norm = 1.325
I0315 16:27:29.715800 139650249254656 logging_writer.py:48] [38] global_step=38, grad_norm=1.11469, loss=0.491529
I0315 16:27:29.721278 139701292631232 submission.py:265] 38) loss = 0.492, grad_norm = 1.115
I0315 16:27:29.798806 139650240861952 logging_writer.py:48] [39] global_step=39, grad_norm=1.29452, loss=0.496114
I0315 16:27:29.804881 139701292631232 submission.py:265] 39) loss = 0.496, grad_norm = 1.295
I0315 16:27:29.882636 139650249254656 logging_writer.py:48] [40] global_step=40, grad_norm=1.05854, loss=0.46822
I0315 16:27:29.888755 139701292631232 submission.py:265] 40) loss = 0.468, grad_norm = 1.059
I0315 16:27:29.957502 139650240861952 logging_writer.py:48] [41] global_step=41, grad_norm=0.864261, loss=0.438817
I0315 16:27:29.962507 139701292631232 submission.py:265] 41) loss = 0.439, grad_norm = 0.864
I0315 16:27:30.042960 139650249254656 logging_writer.py:48] [42] global_step=42, grad_norm=1.05716, loss=0.406323
I0315 16:27:30.048200 139701292631232 submission.py:265] 42) loss = 0.406, grad_norm = 1.057
I0315 16:27:30.118165 139650240861952 logging_writer.py:48] [43] global_step=43, grad_norm=1.51836, loss=0.436383
I0315 16:27:30.122604 139701292631232 submission.py:265] 43) loss = 0.436, grad_norm = 1.518
I0315 16:27:30.211786 139650249254656 logging_writer.py:48] [44] global_step=44, grad_norm=1.19648, loss=0.405479
I0315 16:27:30.219759 139701292631232 submission.py:265] 44) loss = 0.405, grad_norm = 1.196
I0315 16:27:30.301130 139650240861952 logging_writer.py:48] [45] global_step=45, grad_norm=0.882779, loss=0.45822
I0315 16:27:30.308823 139701292631232 submission.py:265] 45) loss = 0.458, grad_norm = 0.883
I0315 16:27:30.377909 139650249254656 logging_writer.py:48] [46] global_step=46, grad_norm=0.970781, loss=0.477795
I0315 16:27:30.383671 139701292631232 submission.py:265] 46) loss = 0.478, grad_norm = 0.971
I0315 16:27:30.460351 139650240861952 logging_writer.py:48] [47] global_step=47, grad_norm=0.772708, loss=0.528542
I0315 16:27:30.465023 139701292631232 submission.py:265] 47) loss = 0.529, grad_norm = 0.773
I0315 16:27:30.538733 139650249254656 logging_writer.py:48] [48] global_step=48, grad_norm=0.968689, loss=0.39964
I0315 16:27:30.543266 139701292631232 submission.py:265] 48) loss = 0.400, grad_norm = 0.969
I0315 16:27:30.616005 139650240861952 logging_writer.py:48] [49] global_step=49, grad_norm=0.98131, loss=0.426643
I0315 16:27:30.621082 139701292631232 submission.py:265] 49) loss = 0.427, grad_norm = 0.981
I0315 16:27:30.696366 139650249254656 logging_writer.py:48] [50] global_step=50, grad_norm=1.08286, loss=0.376829
I0315 16:27:30.702663 139701292631232 submission.py:265] 50) loss = 0.377, grad_norm = 1.083
I0315 16:27:30.778409 139650240861952 logging_writer.py:48] [51] global_step=51, grad_norm=0.881333, loss=0.493901
I0315 16:27:30.782682 139701292631232 submission.py:265] 51) loss = 0.494, grad_norm = 0.881
I0315 16:27:30.857342 139650249254656 logging_writer.py:48] [52] global_step=52, grad_norm=0.709161, loss=0.479957
I0315 16:27:30.862495 139701292631232 submission.py:265] 52) loss = 0.480, grad_norm = 0.709
I0315 16:27:31.058756 139650240861952 logging_writer.py:48] [53] global_step=53, grad_norm=0.7937, loss=0.371321
I0315 16:27:31.064456 139701292631232 submission.py:265] 53) loss = 0.371, grad_norm = 0.794
I0315 16:27:31.473148 139650249254656 logging_writer.py:48] [54] global_step=54, grad_norm=0.74424, loss=0.493175
I0315 16:27:31.479962 139701292631232 submission.py:265] 54) loss = 0.493, grad_norm = 0.744
I0315 16:27:31.794652 139650240861952 logging_writer.py:48] [55] global_step=55, grad_norm=0.785603, loss=0.380978
I0315 16:27:31.802207 139701292631232 submission.py:265] 55) loss = 0.381, grad_norm = 0.786
I0315 16:27:32.128795 139650249254656 logging_writer.py:48] [56] global_step=56, grad_norm=0.775283, loss=0.332209
I0315 16:27:32.134315 139701292631232 submission.py:265] 56) loss = 0.332, grad_norm = 0.775
I0315 16:27:32.370703 139650240861952 logging_writer.py:48] [57] global_step=57, grad_norm=0.729999, loss=0.415345
I0315 16:27:32.378409 139701292631232 submission.py:265] 57) loss = 0.415, grad_norm = 0.730
I0315 16:27:32.604312 139650249254656 logging_writer.py:48] [58] global_step=58, grad_norm=0.634362, loss=0.345429
I0315 16:27:32.609128 139701292631232 submission.py:265] 58) loss = 0.345, grad_norm = 0.634
I0315 16:27:32.960577 139650240861952 logging_writer.py:48] [59] global_step=59, grad_norm=0.621896, loss=0.365918
I0315 16:27:32.964993 139701292631232 submission.py:265] 59) loss = 0.366, grad_norm = 0.622
I0315 16:27:33.263843 139650249254656 logging_writer.py:48] [60] global_step=60, grad_norm=1.10019, loss=0.330355
I0315 16:27:33.270053 139701292631232 submission.py:265] 60) loss = 0.330, grad_norm = 1.100
I0315 16:27:33.734785 139650240861952 logging_writer.py:48] [61] global_step=61, grad_norm=0.916614, loss=0.394049
I0315 16:27:33.738444 139701292631232 submission.py:265] 61) loss = 0.394, grad_norm = 0.917
I0315 16:27:34.021728 139650249254656 logging_writer.py:48] [62] global_step=62, grad_norm=0.652143, loss=0.342727
I0315 16:27:34.028071 139701292631232 submission.py:265] 62) loss = 0.343, grad_norm = 0.652
I0315 16:27:34.481807 139650240861952 logging_writer.py:48] [63] global_step=63, grad_norm=0.98866, loss=0.376773
I0315 16:27:34.487147 139701292631232 submission.py:265] 63) loss = 0.377, grad_norm = 0.989
I0315 16:27:34.904748 139650249254656 logging_writer.py:48] [64] global_step=64, grad_norm=0.826707, loss=0.336234
I0315 16:27:34.912319 139701292631232 submission.py:265] 64) loss = 0.336, grad_norm = 0.827
I0315 16:27:35.269690 139650240861952 logging_writer.py:48] [65] global_step=65, grad_norm=0.483856, loss=0.352153
I0315 16:27:35.275763 139701292631232 submission.py:265] 65) loss = 0.352, grad_norm = 0.484
I0315 16:27:35.598562 139650249254656 logging_writer.py:48] [66] global_step=66, grad_norm=0.508573, loss=0.347146
I0315 16:27:35.603548 139701292631232 submission.py:265] 66) loss = 0.347, grad_norm = 0.509
I0315 16:27:35.808658 139650240861952 logging_writer.py:48] [67] global_step=67, grad_norm=0.879849, loss=0.343829
I0315 16:27:35.814252 139701292631232 submission.py:265] 67) loss = 0.344, grad_norm = 0.880
I0315 16:27:35.955541 139650249254656 logging_writer.py:48] [68] global_step=68, grad_norm=0.527761, loss=0.418983
I0315 16:27:35.960213 139701292631232 submission.py:265] 68) loss = 0.419, grad_norm = 0.528
I0315 16:27:36.128819 139650240861952 logging_writer.py:48] [69] global_step=69, grad_norm=0.311008, loss=0.40279
I0315 16:27:36.140483 139701292631232 submission.py:265] 69) loss = 0.403, grad_norm = 0.311
I0315 16:27:36.247359 139650249254656 logging_writer.py:48] [70] global_step=70, grad_norm=0.559974, loss=0.299897
I0315 16:27:36.251950 139701292631232 submission.py:265] 70) loss = 0.300, grad_norm = 0.560
I0315 16:27:36.380081 139650240861952 logging_writer.py:48] [71] global_step=71, grad_norm=0.37418, loss=0.37876
I0315 16:27:36.384630 139701292631232 submission.py:265] 71) loss = 0.379, grad_norm = 0.374
I0315 16:27:36.504896 139650249254656 logging_writer.py:48] [72] global_step=72, grad_norm=0.392909, loss=0.320953
I0315 16:27:36.509221 139701292631232 submission.py:265] 72) loss = 0.321, grad_norm = 0.393
I0315 16:27:36.595598 139650240861952 logging_writer.py:48] [73] global_step=73, grad_norm=0.809079, loss=0.348817
I0315 16:27:36.601050 139701292631232 submission.py:265] 73) loss = 0.349, grad_norm = 0.809
I0315 16:27:36.703402 139650249254656 logging_writer.py:48] [74] global_step=74, grad_norm=0.334724, loss=0.437825
I0315 16:27:36.710325 139701292631232 submission.py:265] 74) loss = 0.438, grad_norm = 0.335
I0315 16:27:36.798984 139650240861952 logging_writer.py:48] [75] global_step=75, grad_norm=0.325587, loss=0.442006
I0315 16:27:36.807916 139701292631232 submission.py:265] 75) loss = 0.442, grad_norm = 0.326
I0315 16:27:36.898719 139650249254656 logging_writer.py:48] [76] global_step=76, grad_norm=0.572615, loss=0.297151
I0315 16:27:36.903126 139701292631232 submission.py:265] 76) loss = 0.297, grad_norm = 0.573
I0315 16:27:36.976917 139650240861952 logging_writer.py:48] [77] global_step=77, grad_norm=0.406043, loss=0.327744
I0315 16:27:36.982331 139701292631232 submission.py:265] 77) loss = 0.328, grad_norm = 0.406
I0315 16:27:37.062918 139650249254656 logging_writer.py:48] [78] global_step=78, grad_norm=0.53126, loss=0.354375
I0315 16:27:37.069420 139701292631232 submission.py:265] 78) loss = 0.354, grad_norm = 0.531
I0315 16:27:37.145212 139650240861952 logging_writer.py:48] [79] global_step=79, grad_norm=0.894637, loss=0.376319
I0315 16:27:37.149988 139701292631232 submission.py:265] 79) loss = 0.376, grad_norm = 0.895
I0315 16:27:37.271665 139650249254656 logging_writer.py:48] [80] global_step=80, grad_norm=0.324645, loss=0.331128
I0315 16:27:37.276439 139701292631232 submission.py:265] 80) loss = 0.331, grad_norm = 0.325
I0315 16:27:37.389722 139650240861952 logging_writer.py:48] [81] global_step=81, grad_norm=0.4595, loss=0.406383
I0315 16:27:37.395202 139701292631232 submission.py:265] 81) loss = 0.406, grad_norm = 0.460
I0315 16:27:37.478851 139650249254656 logging_writer.py:48] [82] global_step=82, grad_norm=0.398371, loss=0.290449
I0315 16:27:37.486686 139701292631232 submission.py:265] 82) loss = 0.290, grad_norm = 0.398
I0315 16:27:37.609308 139650240861952 logging_writer.py:48] [83] global_step=83, grad_norm=0.689076, loss=0.334215
I0315 16:27:37.614140 139701292631232 submission.py:265] 83) loss = 0.334, grad_norm = 0.689
I0315 16:27:37.748046 139650249254656 logging_writer.py:48] [84] global_step=84, grad_norm=0.363588, loss=0.303799
I0315 16:27:37.753318 139701292631232 submission.py:265] 84) loss = 0.304, grad_norm = 0.364
I0315 16:27:37.947244 139650240861952 logging_writer.py:48] [85] global_step=85, grad_norm=0.452544, loss=0.42447
I0315 16:27:37.951857 139701292631232 submission.py:265] 85) loss = 0.424, grad_norm = 0.453
I0315 16:27:38.195014 139650249254656 logging_writer.py:48] [86] global_step=86, grad_norm=0.4292, loss=0.369124
I0315 16:27:38.200330 139701292631232 submission.py:265] 86) loss = 0.369, grad_norm = 0.429
I0315 16:27:38.494519 139650240861952 logging_writer.py:48] [87] global_step=87, grad_norm=0.556739, loss=0.363042
I0315 16:27:38.501858 139701292631232 submission.py:265] 87) loss = 0.363, grad_norm = 0.557
I0315 16:27:38.691597 139650249254656 logging_writer.py:48] [88] global_step=88, grad_norm=0.306962, loss=0.487098
I0315 16:27:38.696976 139701292631232 submission.py:265] 88) loss = 0.487, grad_norm = 0.307
I0315 16:27:38.870143 139650240861952 logging_writer.py:48] [89] global_step=89, grad_norm=0.261767, loss=0.452244
I0315 16:27:38.877652 139701292631232 submission.py:265] 89) loss = 0.452, grad_norm = 0.262
I0315 16:27:39.000694 139650249254656 logging_writer.py:48] [90] global_step=90, grad_norm=0.431583, loss=0.456352
I0315 16:27:39.005082 139701292631232 submission.py:265] 90) loss = 0.456, grad_norm = 0.432
I0315 16:27:39.228910 139650240861952 logging_writer.py:48] [91] global_step=91, grad_norm=0.724587, loss=0.425864
I0315 16:27:39.234009 139701292631232 submission.py:265] 91) loss = 0.426, grad_norm = 0.725
I0315 16:27:39.362250 139650249254656 logging_writer.py:48] [92] global_step=92, grad_norm=0.445278, loss=0.463249
I0315 16:27:39.367184 139701292631232 submission.py:265] 92) loss = 0.463, grad_norm = 0.445
I0315 16:27:39.498482 139650240861952 logging_writer.py:48] [93] global_step=93, grad_norm=0.543422, loss=0.3903
I0315 16:27:39.502880 139701292631232 submission.py:265] 93) loss = 0.390, grad_norm = 0.543
I0315 16:27:39.713285 139650249254656 logging_writer.py:48] [94] global_step=94, grad_norm=0.407905, loss=0.357013
I0315 16:27:39.719571 139701292631232 submission.py:265] 94) loss = 0.357, grad_norm = 0.408
I0315 16:27:39.958253 139650240861952 logging_writer.py:48] [95] global_step=95, grad_norm=0.750046, loss=0.528541
I0315 16:27:39.962959 139701292631232 submission.py:265] 95) loss = 0.529, grad_norm = 0.750
I0315 16:27:40.250360 139650249254656 logging_writer.py:48] [96] global_step=96, grad_norm=0.279407, loss=0.400393
I0315 16:27:40.255548 139701292631232 submission.py:265] 96) loss = 0.400, grad_norm = 0.279
I0315 16:27:40.725906 139650240861952 logging_writer.py:48] [97] global_step=97, grad_norm=0.563261, loss=0.371374
I0315 16:27:40.730542 139701292631232 submission.py:265] 97) loss = 0.371, grad_norm = 0.563
I0315 16:27:41.123610 139650249254656 logging_writer.py:48] [98] global_step=98, grad_norm=0.831447, loss=0.397832
I0315 16:27:41.128584 139701292631232 submission.py:265] 98) loss = 0.398, grad_norm = 0.831
I0315 16:27:41.685797 139650240861952 logging_writer.py:48] [99] global_step=99, grad_norm=0.392453, loss=0.395817
I0315 16:27:41.690608 139701292631232 submission.py:265] 99) loss = 0.396, grad_norm = 0.392
I0315 16:27:41.768621 139650249254656 logging_writer.py:48] [100] global_step=100, grad_norm=0.348798, loss=0.342177
I0315 16:27:41.776602 139701292631232 submission.py:265] 100) loss = 0.342, grad_norm = 0.349
/usr/local/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
I0315 16:28:46.096160 139701292631232 spec.py:321] Evaluating on the training split.
I0315 16:28:48.183483 139701292631232 spec.py:333] Evaluating on the validation split.
I0315 16:28:50.561347 139701292631232 spec.py:349] Evaluating on the test split.
I0315 16:28:52.886694 139701292631232 submission_runner.py:469] Time since start: 300.17s, 	Step: 390, 	{'train/ssim': 0.7054547582353864, 'train/loss': 0.3018218108585903, 'validation/ssim': 0.6867688833224888, 'validation/loss': 0.320512195215778, 'validation/num_examples': 3554, 'test/ssim': 0.7042586415282044, 'test/loss': 0.32244377061662244, 'test/num_examples': 3581, 'score': 165.25808787345886, 'total_duration': 300.1717827320099, 'accumulated_submission_time': 165.25808787345886, 'accumulated_eval_time': 132.32103157043457, 'accumulated_logging_time': 0.017559051513671875}
I0315 16:28:52.903494 139650240861952 logging_writer.py:48] [390] accumulated_eval_time=132.321, accumulated_logging_time=0.0175591, accumulated_submission_time=165.258, global_step=390, preemption_count=0, score=165.258, test/loss=0.322444, test/num_examples=3581, test/ssim=0.704259, total_duration=300.172, train/loss=0.301822, train/ssim=0.705455, validation/loss=0.320512, validation/num_examples=3554, validation/ssim=0.686769
I0315 16:29:41.575762 139650249254656 logging_writer.py:48] [500] global_step=500, grad_norm=0.372217, loss=0.296674
I0315 16:29:41.581206 139701292631232 submission.py:265] 500) loss = 0.297, grad_norm = 0.372
I0315 16:30:13.639255 139701292631232 spec.py:321] Evaluating on the training split.
I0315 16:30:15.911060 139701292631232 spec.py:333] Evaluating on the validation split.
I0315 16:30:18.314028 139701292631232 spec.py:349] Evaluating on the test split.
I0315 16:30:20.721606 139701292631232 submission_runner.py:469] Time since start: 388.01s, 	Step: 653, 	{'train/ssim': 0.7174690791538784, 'train/loss': 0.29100806372506277, 'validation/ssim': 0.6967504154649691, 'validation/loss': 0.3104598732831844, 'validation/num_examples': 3554, 'test/ssim': 0.7139228196558224, 'test/loss': 0.31248343307124404, 'test/num_examples': 3581, 'score': 244.31596684455872, 'total_duration': 388.00668573379517, 'accumulated_submission_time': 244.31596684455872, 'accumulated_eval_time': 139.40369176864624, 'accumulated_logging_time': 0.05010700225830078}
I0315 16:30:20.731267 139650240861952 logging_writer.py:48] [653] accumulated_eval_time=139.404, accumulated_logging_time=0.050107, accumulated_submission_time=244.316, global_step=653, preemption_count=0, score=244.316, test/loss=0.312483, test/num_examples=3581, test/ssim=0.713923, total_duration=388.007, train/loss=0.291008, train/ssim=0.717469, validation/loss=0.31046, validation/num_examples=3554, validation/ssim=0.69675
I0315 16:31:22.126645 139650249254656 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.141398, loss=0.295621
I0315 16:31:22.133138 139701292631232 submission.py:265] 1000) loss = 0.296, grad_norm = 0.141
I0315 16:31:41.454358 139701292631232 spec.py:321] Evaluating on the training split.
I0315 16:31:43.432326 139701292631232 spec.py:333] Evaluating on the validation split.
I0315 16:31:46.017327 139701292631232 spec.py:349] Evaluating on the test split.
I0315 16:31:48.604434 139701292631232 submission_runner.py:469] Time since start: 475.89s, 	Step: 1296, 	{'train/ssim': 0.7273641313825335, 'train/loss': 0.28153981481279644, 'validation/ssim': 0.7063798877585116, 'validation/loss': 0.30049060312763787, 'validation/num_examples': 3554, 'test/ssim': 0.7235214118350322, 'test/loss': 0.3025932493105976, 'test/num_examples': 3581, 'score': 323.17994022369385, 'total_duration': 475.8895523548126, 'accumulated_submission_time': 323.17994022369385, 'accumulated_eval_time': 146.55387139320374, 'accumulated_logging_time': 0.06798171997070312}
I0315 16:31:48.639329 139650240861952 logging_writer.py:48] [1296] accumulated_eval_time=146.554, accumulated_logging_time=0.0679817, accumulated_submission_time=323.18, global_step=1296, preemption_count=0, score=323.18, test/loss=0.302593, test/num_examples=3581, test/ssim=0.723521, total_duration=475.89, train/loss=0.28154, train/ssim=0.727364, validation/loss=0.300491, validation/num_examples=3554, validation/ssim=0.70638
I0315 16:32:02.322714 139650249254656 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.325859, loss=0.304147
I0315 16:32:02.326488 139701292631232 submission.py:265] 1500) loss = 0.304, grad_norm = 0.326
I0315 16:32:33.687028 139650240861952 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.288853, loss=0.302193
I0315 16:32:33.690474 139701292631232 submission.py:265] 2000) loss = 0.302, grad_norm = 0.289
I0315 16:33:05.049815 139650249254656 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.0977601, loss=0.373587
I0315 16:33:05.053540 139701292631232 submission.py:265] 2500) loss = 0.374, grad_norm = 0.098
I0315 16:33:09.344247 139701292631232 spec.py:321] Evaluating on the training split.
I0315 16:33:11.325835 139701292631232 spec.py:333] Evaluating on the validation split.
I0315 16:33:13.777037 139701292631232 spec.py:349] Evaluating on the test split.
I0315 16:33:16.058744 139701292631232 submission_runner.py:469] Time since start: 563.34s, 	Step: 2559, 	{'train/ssim': 0.7344277245657784, 'train/loss': 0.2763887814113072, 'validation/ssim': 0.7140416031012592, 'validation/loss': 0.2951390186453644, 'validation/num_examples': 3554, 'test/ssim': 0.7312238747032952, 'test/loss': 0.2968394458710032, 'test/num_examples': 3581, 'score': 402.05144333839417, 'total_duration': 563.3437728881836, 'accumulated_submission_time': 402.05144333839417, 'accumulated_eval_time': 153.26849603652954, 'accumulated_logging_time': 0.1112070083618164}
I0315 16:33:16.068656 139650240861952 logging_writer.py:48] [2559] accumulated_eval_time=153.268, accumulated_logging_time=0.111207, accumulated_submission_time=402.051, global_step=2559, preemption_count=0, score=402.051, test/loss=0.296839, test/num_examples=3581, test/ssim=0.731224, total_duration=563.344, train/loss=0.276389, train/ssim=0.734428, validation/loss=0.295139, validation/num_examples=3554, validation/ssim=0.714042
I0315 16:33:44.679467 139650249254656 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.151578, loss=0.302747
I0315 16:33:44.683545 139701292631232 submission.py:265] 3000) loss = 0.303, grad_norm = 0.152
I0315 16:34:16.093786 139650240861952 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.207446, loss=0.23476
I0315 16:34:16.096997 139701292631232 submission.py:265] 3500) loss = 0.235, grad_norm = 0.207
I0315 16:34:36.810508 139701292631232 spec.py:321] Evaluating on the training split.
I0315 16:34:38.831243 139701292631232 spec.py:333] Evaluating on the validation split.
I0315 16:34:41.081157 139701292631232 spec.py:349] Evaluating on the test split.
I0315 16:34:43.285704 139701292631232 submission_runner.py:469] Time since start: 650.57s, 	Step: 3819, 	{'train/ssim': 0.7371646336146763, 'train/loss': 0.2737900699887957, 'validation/ssim': 0.7157713332468697, 'validation/loss': 0.29285488869275816, 'validation/num_examples': 3554, 'test/ssim': 0.7329933999537489, 'test/loss': 0.2946332831916713, 'test/num_examples': 3581, 'score': 480.8701608181, 'total_duration': 650.570797920227, 'accumulated_submission_time': 480.8701608181, 'accumulated_eval_time': 159.74382209777832, 'accumulated_logging_time': 0.12930870056152344}
I0315 16:34:43.295594 139650249254656 logging_writer.py:48] [3819] accumulated_eval_time=159.744, accumulated_logging_time=0.129309, accumulated_submission_time=480.87, global_step=3819, preemption_count=0, score=480.87, test/loss=0.294633, test/num_examples=3581, test/ssim=0.732993, total_duration=650.571, train/loss=0.27379, train/ssim=0.737165, validation/loss=0.292855, validation/num_examples=3554, validation/ssim=0.715771
I0315 16:34:55.520983 139650240861952 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.60432, loss=0.294504
I0315 16:34:55.524252 139701292631232 submission.py:265] 4000) loss = 0.295, grad_norm = 0.604
I0315 16:35:26.878804 139650249254656 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.0947174, loss=0.26555
I0315 16:35:26.882400 139701292631232 submission.py:265] 4500) loss = 0.266, grad_norm = 0.095
I0315 16:35:58.238131 139650240861952 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.219546, loss=0.27742
I0315 16:35:58.241767 139701292631232 submission.py:265] 5000) loss = 0.277, grad_norm = 0.220
I0315 16:36:04.076344 139701292631232 spec.py:321] Evaluating on the training split.
I0315 16:36:06.087089 139701292631232 spec.py:333] Evaluating on the validation split.
I0315 16:36:08.300230 139701292631232 spec.py:349] Evaluating on the test split.
I0315 16:36:10.577248 139701292631232 submission_runner.py:469] Time since start: 737.86s, 	Step: 5083, 	{'train/ssim': 0.7390016828264508, 'train/loss': 0.2727112088884626, 'validation/ssim': 0.7177918476540518, 'validation/loss': 0.29185404265000703, 'validation/num_examples': 3554, 'test/ssim': 0.7351633949141302, 'test/loss': 0.2934568267200154, 'test/num_examples': 3581, 'score': 559.6900432109833, 'total_duration': 737.862352848053, 'accumulated_submission_time': 559.6900432109833, 'accumulated_eval_time': 166.24487161636353, 'accumulated_logging_time': 0.14738798141479492}
I0315 16:36:10.587514 139650249254656 logging_writer.py:48] [5083] accumulated_eval_time=166.245, accumulated_logging_time=0.147388, accumulated_submission_time=559.69, global_step=5083, preemption_count=0, score=559.69, test/loss=0.293457, test/num_examples=3581, test/ssim=0.735163, total_duration=737.862, train/loss=0.272711, train/ssim=0.739002, validation/loss=0.291854, validation/num_examples=3554, validation/ssim=0.717792
I0315 16:36:37.706413 139650240861952 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.255533, loss=0.280926
I0315 16:36:37.710182 139701292631232 submission.py:265] 5500) loss = 0.281, grad_norm = 0.256
I0315 16:37:09.171375 139650249254656 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.284891, loss=0.237839
I0315 16:37:09.175223 139701292631232 submission.py:265] 6000) loss = 0.238, grad_norm = 0.285
I0315 16:37:31.374501 139701292631232 spec.py:321] Evaluating on the training split.
I0315 16:37:33.360535 139701292631232 spec.py:333] Evaluating on the validation split.
I0315 16:37:35.514296 139701292631232 spec.py:349] Evaluating on the test split.
I0315 16:37:37.632755 139701292631232 submission_runner.py:469] Time since start: 824.92s, 	Step: 6343, 	{'train/ssim': 0.7390495709010533, 'train/loss': 0.27166243961879183, 'validation/ssim': 0.7175115049723902, 'validation/loss': 0.2910257231068866, 'validation/num_examples': 3554, 'test/ssim': 0.7349857265341385, 'test/loss': 0.2926165834547787, 'test/num_examples': 3581, 'score': 638.5154089927673, 'total_duration': 824.9178631305695, 'accumulated_submission_time': 638.5154089927673, 'accumulated_eval_time': 172.50324892997742, 'accumulated_logging_time': 0.16601133346557617}
I0315 16:37:37.644250 139650240861952 logging_writer.py:48] [6343] accumulated_eval_time=172.503, accumulated_logging_time=0.166011, accumulated_submission_time=638.515, global_step=6343, preemption_count=0, score=638.515, test/loss=0.292617, test/num_examples=3581, test/ssim=0.734986, total_duration=824.918, train/loss=0.271662, train/ssim=0.73905, validation/loss=0.291026, validation/num_examples=3554, validation/ssim=0.717512
I0315 16:37:48.436328 139650249254656 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.465764, loss=0.23997
I0315 16:37:51.788470 139701292631232 submission.py:265] 6500) loss = 0.240, grad_norm = 0.466
I0315 16:38:23.799839 139650240861952 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.12925, loss=0.260341
I0315 16:38:23.805719 139701292631232 submission.py:265] 7000) loss = 0.260, grad_norm = 0.129
I0315 16:38:55.543520 139650249254656 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.19245, loss=0.293262
I0315 16:38:55.547453 139701292631232 submission.py:265] 7500) loss = 0.293, grad_norm = 0.192
I0315 16:38:58.360259 139701292631232 spec.py:321] Evaluating on the training split.
I0315 16:39:00.346209 139701292631232 spec.py:333] Evaluating on the validation split.
I0315 16:39:02.455281 139701292631232 spec.py:349] Evaluating on the test split.
I0315 16:39:04.584388 139701292631232 submission_runner.py:469] Time since start: 911.87s, 	Step: 7535, 	{'train/ssim': 0.7389940534319196, 'train/loss': 0.2723602567400251, 'validation/ssim': 0.7176094634786508, 'validation/loss': 0.29172901846950616, 'validation/num_examples': 3554, 'test/ssim': 0.7346233675780159, 'test/loss': 0.29342979467371194, 'test/num_examples': 3581, 'score': 717.288907289505, 'total_duration': 911.8695116043091, 'accumulated_submission_time': 717.288907289505, 'accumulated_eval_time': 178.72747659683228, 'accumulated_logging_time': 0.1859121322631836}
I0315 16:39:04.594535 139650240861952 logging_writer.py:48] [7535] accumulated_eval_time=178.727, accumulated_logging_time=0.185912, accumulated_submission_time=717.289, global_step=7535, preemption_count=0, score=717.289, test/loss=0.29343, test/num_examples=3581, test/ssim=0.734623, total_duration=911.87, train/loss=0.27236, train/ssim=0.738994, validation/loss=0.291729, validation/num_examples=3554, validation/ssim=0.717609
I0315 16:39:34.567183 139650249254656 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.202822, loss=0.36703
I0315 16:39:34.571065 139701292631232 submission.py:265] 8000) loss = 0.367, grad_norm = 0.203
I0315 16:40:05.791610 139650240861952 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.228949, loss=0.286213
I0315 16:40:05.795011 139701292631232 submission.py:265] 8500) loss = 0.286, grad_norm = 0.229
I0315 16:40:25.381184 139701292631232 spec.py:321] Evaluating on the training split.
I0315 16:40:27.389074 139701292631232 spec.py:333] Evaluating on the validation split.
I0315 16:40:29.521576 139701292631232 spec.py:349] Evaluating on the test split.
I0315 16:40:31.702732 139701292631232 submission_runner.py:469] Time since start: 998.99s, 	Step: 8802, 	{'train/ssim': 0.7420942442757743, 'train/loss': 0.2698760713849749, 'validation/ssim': 0.7202103786006964, 'validation/loss': 0.28969988288943793, 'validation/num_examples': 3554, 'test/ssim': 0.7375037633517174, 'test/loss': 0.29126569699455457, 'test/num_examples': 3581, 'score': 796.1220152378082, 'total_duration': 998.9878261089325, 'accumulated_submission_time': 796.1220152378082, 'accumulated_eval_time': 185.0491647720337, 'accumulated_logging_time': 0.20443987846374512}
I0315 16:40:31.712827 139650249254656 logging_writer.py:48] [8802] accumulated_eval_time=185.049, accumulated_logging_time=0.20444, accumulated_submission_time=796.122, global_step=8802, preemption_count=0, score=796.122, test/loss=0.291266, test/num_examples=3581, test/ssim=0.737504, total_duration=998.988, train/loss=0.269876, train/ssim=0.742094, validation/loss=0.2897, validation/num_examples=3554, validation/ssim=0.72021
I0315 16:40:45.024713 139650240861952 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.236105, loss=0.259
I0315 16:40:45.027984 139701292631232 submission.py:265] 9000) loss = 0.259, grad_norm = 0.236
I0315 16:41:16.350645 139650249254656 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.169122, loss=0.272634
I0315 16:41:16.353806 139701292631232 submission.py:265] 9500) loss = 0.273, grad_norm = 0.169
I0315 16:41:47.658011 139650240861952 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.168478, loss=0.247539
I0315 16:41:47.661361 139701292631232 submission.py:265] 10000) loss = 0.248, grad_norm = 0.168
I0315 16:41:52.469765 139701292631232 spec.py:321] Evaluating on the training split.
I0315 16:41:54.451518 139701292631232 spec.py:333] Evaluating on the validation split.
I0315 16:41:56.575021 139701292631232 spec.py:349] Evaluating on the test split.
I0315 16:41:58.717411 139701292631232 submission_runner.py:469] Time since start: 1086.00s, 	Step: 10067, 	{'train/ssim': 0.7427368845258441, 'train/loss': 0.26930616583142963, 'validation/ssim': 0.7210059995119935, 'validation/loss': 0.28903625866101573, 'validation/num_examples': 3554, 'test/ssim': 0.7383137020908964, 'test/loss': 0.2905004480570197, 'test/num_examples': 3581, 'score': 874.9218046665192, 'total_duration': 1086.002522468567, 'accumulated_submission_time': 874.9218046665192, 'accumulated_eval_time': 191.29699087142944, 'accumulated_logging_time': 0.2237098217010498}
I0315 16:41:58.728030 139650249254656 logging_writer.py:48] [10067] accumulated_eval_time=191.297, accumulated_logging_time=0.22371, accumulated_submission_time=874.922, global_step=10067, preemption_count=0, score=874.922, test/loss=0.2905, test/num_examples=3581, test/ssim=0.738314, total_duration=1086, train/loss=0.269306, train/ssim=0.742737, validation/loss=0.289036, validation/num_examples=3554, validation/ssim=0.721006
I0315 16:42:26.793659 139650240861952 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.174599, loss=0.229037
I0315 16:42:26.797129 139701292631232 submission.py:265] 10500) loss = 0.229, grad_norm = 0.175
I0315 16:42:58.148918 139650249254656 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.118032, loss=0.357352
I0315 16:42:58.152173 139701292631232 submission.py:265] 11000) loss = 0.357, grad_norm = 0.118
I0315 16:43:19.448822 139701292631232 spec.py:321] Evaluating on the training split.
I0315 16:43:21.434148 139701292631232 spec.py:333] Evaluating on the validation split.
I0315 16:43:23.544228 139701292631232 spec.py:349] Evaluating on the test split.
I0315 16:43:25.649050 139701292631232 submission_runner.py:469] Time since start: 1172.93s, 	Step: 11329, 	{'train/ssim': 0.7433338165283203, 'train/loss': 0.2687626225607736, 'validation/ssim': 0.7215962922499296, 'validation/loss': 0.2884800384469963, 'validation/num_examples': 3554, 'test/ssim': 0.7389056800431094, 'test/loss': 0.2899639999956367, 'test/num_examples': 3581, 'score': 953.7212605476379, 'total_duration': 1172.9341661930084, 'accumulated_submission_time': 953.7212605476379, 'accumulated_eval_time': 197.49743509292603, 'accumulated_logging_time': 0.24236488342285156}
I0315 16:43:25.659817 139650240861952 logging_writer.py:48] [11329] accumulated_eval_time=197.497, accumulated_logging_time=0.242365, accumulated_submission_time=953.721, global_step=11329, preemption_count=0, score=953.721, test/loss=0.289964, test/num_examples=3581, test/ssim=0.738906, total_duration=1172.93, train/loss=0.268763, train/ssim=0.743334, validation/loss=0.28848, validation/num_examples=3554, validation/ssim=0.721596
I0315 16:43:37.210782 139650249254656 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.124569, loss=0.308874
I0315 16:43:37.214057 139701292631232 submission.py:265] 11500) loss = 0.309, grad_norm = 0.125
I0315 16:44:08.576603 139650240861952 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.302569, loss=0.226451
I0315 16:44:08.580453 139701292631232 submission.py:265] 12000) loss = 0.226, grad_norm = 0.303
I0315 16:44:39.922621 139650249254656 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.328877, loss=0.294617
I0315 16:44:39.925980 139701292631232 submission.py:265] 12500) loss = 0.295, grad_norm = 0.329
I0315 16:44:46.417724 139701292631232 spec.py:321] Evaluating on the training split.
I0315 16:44:48.422554 139701292631232 spec.py:333] Evaluating on the validation split.
I0315 16:44:50.554270 139701292631232 spec.py:349] Evaluating on the test split.
I0315 16:44:52.732100 139701292631232 submission_runner.py:469] Time since start: 1260.02s, 	Step: 12594, 	{'train/ssim': 0.742595945085798, 'train/loss': 0.2687539373125349, 'validation/ssim': 0.7204971785751969, 'validation/loss': 0.2885513434466446, 'validation/num_examples': 3554, 'test/ssim': 0.7379300720163712, 'test/loss': 0.29001762093994343, 'test/num_examples': 3581, 'score': 1032.5482022762299, 'total_duration': 1260.017174243927, 'accumulated_submission_time': 1032.5482022762299, 'accumulated_eval_time': 203.81220960617065, 'accumulated_logging_time': 0.26149439811706543}
I0315 16:44:52.744032 139650240861952 logging_writer.py:48] [12594] accumulated_eval_time=203.812, accumulated_logging_time=0.261494, accumulated_submission_time=1032.55, global_step=12594, preemption_count=0, score=1032.55, test/loss=0.290018, test/num_examples=3581, test/ssim=0.73793, total_duration=1260.02, train/loss=0.268754, train/ssim=0.742596, validation/loss=0.288551, validation/num_examples=3554, validation/ssim=0.720497
I0315 16:45:19.060959 139650249254656 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.111617, loss=0.263932
I0315 16:45:19.064353 139701292631232 submission.py:265] 13000) loss = 0.264, grad_norm = 0.112
I0315 16:45:50.416199 139650240861952 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.17491, loss=0.33871
I0315 16:45:50.419631 139701292631232 submission.py:265] 13500) loss = 0.339, grad_norm = 0.175
I0315 16:46:13.521678 139701292631232 spec.py:321] Evaluating on the training split.
I0315 16:46:15.550006 139701292631232 spec.py:333] Evaluating on the validation split.
I0315 16:46:17.702009 139701292631232 spec.py:349] Evaluating on the test split.
I0315 16:46:19.879415 139701292631232 submission_runner.py:469] Time since start: 1347.16s, 	Step: 13859, 	{'train/ssim': 0.7426636559622628, 'train/loss': 0.26871505805424284, 'validation/ssim': 0.7201917623628307, 'validation/loss': 0.28880843300462505, 'validation/num_examples': 3554, 'test/ssim': 0.7376006423877758, 'test/loss': 0.2903110192029112, 'test/num_examples': 3581, 'score': 1111.347316980362, 'total_duration': 1347.1644864082336, 'accumulated_submission_time': 1111.347316980362, 'accumulated_eval_time': 210.1701259613037, 'accumulated_logging_time': 0.2822451591491699}
I0315 16:46:19.891916 139650249254656 logging_writer.py:48] [13859] accumulated_eval_time=210.17, accumulated_logging_time=0.282245, accumulated_submission_time=1111.35, global_step=13859, preemption_count=0, score=1111.35, test/loss=0.290311, test/num_examples=3581, test/ssim=0.737601, total_duration=1347.16, train/loss=0.268715, train/ssim=0.742664, validation/loss=0.288808, validation/num_examples=3554, validation/ssim=0.720192
I0315 16:46:29.668519 139650240861952 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.165294, loss=0.336467
I0315 16:46:29.671710 139701292631232 submission.py:265] 14000) loss = 0.336, grad_norm = 0.165
I0315 16:47:00.969430 139650249254656 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.119283, loss=0.32731
I0315 16:47:00.972980 139701292631232 submission.py:265] 14500) loss = 0.327, grad_norm = 0.119
I0315 16:47:32.334657 139650240861952 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.21477, loss=0.236772
I0315 16:47:32.337947 139701292631232 submission.py:265] 15000) loss = 0.237, grad_norm = 0.215
I0315 16:47:40.628225 139701292631232 spec.py:321] Evaluating on the training split.
I0315 16:47:42.695274 139701292631232 spec.py:333] Evaluating on the validation split.
I0315 16:47:44.885281 139701292631232 spec.py:349] Evaluating on the test split.
I0315 16:47:47.075566 139701292631232 submission_runner.py:469] Time since start: 1434.36s, 	Step: 15122, 	{'train/ssim': 0.7442771366664341, 'train/loss': 0.26784276962280273, 'validation/ssim': 0.7222817270153349, 'validation/loss': 0.2876339613485685, 'validation/num_examples': 3554, 'test/ssim': 0.7395656983035465, 'test/loss': 0.2891190184524225, 'test/num_examples': 3581, 'score': 1190.1393880844116, 'total_duration': 1434.3606748580933, 'accumulated_submission_time': 1190.1393880844116, 'accumulated_eval_time': 216.6176700592041, 'accumulated_logging_time': 0.30364179611206055}
I0315 16:47:47.086630 139650249254656 logging_writer.py:48] [15122] accumulated_eval_time=216.618, accumulated_logging_time=0.303642, accumulated_submission_time=1190.14, global_step=15122, preemption_count=0, score=1190.14, test/loss=0.289119, test/num_examples=3581, test/ssim=0.739566, total_duration=1434.36, train/loss=0.267843, train/ssim=0.744277, validation/loss=0.287634, validation/num_examples=3554, validation/ssim=0.722282
I0315 16:48:11.770807 139650240861952 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.232675, loss=0.324814
I0315 16:48:11.774050 139701292631232 submission.py:265] 15500) loss = 0.325, grad_norm = 0.233
I0315 16:48:43.261770 139650249254656 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.390372, loss=0.206094
I0315 16:48:43.265421 139701292631232 submission.py:265] 16000) loss = 0.206, grad_norm = 0.390
I0315 16:49:07.827191 139701292631232 spec.py:321] Evaluating on the training split.
I0315 16:49:09.810525 139701292631232 spec.py:333] Evaluating on the validation split.
I0315 16:49:11.908096 139701292631232 spec.py:349] Evaluating on the test split.
I0315 16:49:14.023098 139701292631232 submission_runner.py:469] Time since start: 1521.31s, 	Step: 16381, 	{'train/ssim': 0.7445347649710519, 'train/loss': 0.26749324798583984, 'validation/ssim': 0.7225003132473973, 'validation/loss': 0.2873415627637873, 'validation/num_examples': 3554, 'test/ssim': 0.7398439954359816, 'test/loss': 0.28880387183442124, 'test/num_examples': 3581, 'score': 1268.9842171669006, 'total_duration': 1521.3082089424133, 'accumulated_submission_time': 1268.9842171669006, 'accumulated_eval_time': 222.81372022628784, 'accumulated_logging_time': 0.32393956184387207}
I0315 16:49:14.034214 139650240861952 logging_writer.py:48] [16381] accumulated_eval_time=222.814, accumulated_logging_time=0.32394, accumulated_submission_time=1268.98, global_step=16381, preemption_count=0, score=1268.98, test/loss=0.288804, test/num_examples=3581, test/ssim=0.739844, total_duration=1521.31, train/loss=0.267493, train/ssim=0.744535, validation/loss=0.287342, validation/num_examples=3554, validation/ssim=0.7225
I0315 16:49:22.428295 139650249254656 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.245233, loss=0.245248
I0315 16:49:22.431448 139701292631232 submission.py:265] 16500) loss = 0.245, grad_norm = 0.245
I0315 16:49:53.704205 139650240861952 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.256533, loss=0.265735
I0315 16:49:53.707967 139701292631232 submission.py:265] 17000) loss = 0.266, grad_norm = 0.257
I0315 16:50:25.015356 139650249254656 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.327842, loss=0.281695
I0315 16:50:25.018724 139701292631232 submission.py:265] 17500) loss = 0.282, grad_norm = 0.328
I0315 16:50:34.730502 139701292631232 spec.py:321] Evaluating on the training split.
I0315 16:50:36.734583 139701292631232 spec.py:333] Evaluating on the validation split.
I0315 16:50:38.841411 139701292631232 spec.py:349] Evaluating on the test split.
I0315 16:50:41.000621 139701292631232 submission_runner.py:469] Time since start: 1608.29s, 	Step: 17646, 	{'train/ssim': 0.7444740022931781, 'train/loss': 0.26746344566345215, 'validation/ssim': 0.722374739510059, 'validation/loss': 0.28732967859717923, 'validation/num_examples': 3554, 'test/ssim': 0.7397396851438146, 'test/loss': 0.28878106674113374, 'test/num_examples': 3581, 'score': 1347.7574064731598, 'total_duration': 1608.285701751709, 'accumulated_submission_time': 1347.7574064731598, 'accumulated_eval_time': 229.08402967453003, 'accumulated_logging_time': 0.3439915180206299}
I0315 16:50:41.011950 139650240861952 logging_writer.py:48] [17646] accumulated_eval_time=229.084, accumulated_logging_time=0.343992, accumulated_submission_time=1347.76, global_step=17646, preemption_count=0, score=1347.76, test/loss=0.288781, test/num_examples=3581, test/ssim=0.73974, total_duration=1608.29, train/loss=0.267463, train/ssim=0.744474, validation/loss=0.28733, validation/num_examples=3554, validation/ssim=0.722375
I0315 16:51:03.966027 139650249254656 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.255135, loss=0.332802
I0315 16:51:03.969288 139701292631232 submission.py:265] 18000) loss = 0.333, grad_norm = 0.255
I0315 16:51:35.298538 139650240861952 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.297489, loss=0.2071
I0315 16:51:35.302144 139701292631232 submission.py:265] 18500) loss = 0.207, grad_norm = 0.297
I0315 16:52:01.801221 139701292631232 spec.py:321] Evaluating on the training split.
I0315 16:52:03.798089 139701292631232 spec.py:333] Evaluating on the validation split.
I0315 16:52:05.898168 139701292631232 spec.py:349] Evaluating on the test split.
I0315 16:52:08.036098 139701292631232 submission_runner.py:469] Time since start: 1695.32s, 	Step: 18913, 	{'train/ssim': 0.7444512503487724, 'train/loss': 0.2674852269036429, 'validation/ssim': 0.7223838071978756, 'validation/loss': 0.2873302796749701, 'validation/num_examples': 3554, 'test/ssim': 0.7397379807272759, 'test/loss': 0.288783896072588, 'test/num_examples': 3581, 'score': 1426.626967906952, 'total_duration': 1695.3212127685547, 'accumulated_submission_time': 1426.626967906952, 'accumulated_eval_time': 235.31906628608704, 'accumulated_logging_time': 0.36339688301086426}
I0315 16:52:08.046889 139650249254656 logging_writer.py:48] [18913] accumulated_eval_time=235.319, accumulated_logging_time=0.363397, accumulated_submission_time=1426.63, global_step=18913, preemption_count=0, score=1426.63, test/loss=0.288784, test/num_examples=3581, test/ssim=0.739738, total_duration=1695.32, train/loss=0.267485, train/ssim=0.744451, validation/loss=0.28733, validation/num_examples=3554, validation/ssim=0.722384
I0315 16:52:14.351375 139650240861952 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.33088, loss=0.250403
I0315 16:52:14.354577 139701292631232 submission.py:265] 19000) loss = 0.250, grad_norm = 0.331
I0315 16:52:45.574126 139650249254656 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.166783, loss=0.25397
I0315 16:52:45.577423 139701292631232 submission.py:265] 19500) loss = 0.254, grad_norm = 0.167
I0315 16:53:16.798507 139650240861952 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.145628, loss=0.302927
I0315 16:53:16.802104 139701292631232 submission.py:265] 20000) loss = 0.303, grad_norm = 0.146
I0315 16:53:28.745407 139701292631232 spec.py:321] Evaluating on the training split.
I0315 16:53:30.742615 139701292631232 spec.py:333] Evaluating on the validation split.
I0315 16:53:32.838506 139701292631232 spec.py:349] Evaluating on the test split.
I0315 16:53:34.928803 139701292631232 submission_runner.py:469] Time since start: 1782.21s, 	Step: 20182, 	{'train/ssim': 0.7441560200282505, 'train/loss': 0.26767165320260183, 'validation/ssim': 0.7220897942898846, 'validation/loss': 0.2875203061251407, 'validation/num_examples': 3554, 'test/ssim': 0.7394595472415177, 'test/loss': 0.288968723002042, 'test/num_examples': 3581, 'score': 1505.4410138130188, 'total_duration': 1782.2139139175415, 'accumulated_submission_time': 1505.4410138130188, 'accumulated_eval_time': 241.50254726409912, 'accumulated_logging_time': 0.38210630416870117}
I0315 16:53:34.939754 139650249254656 logging_writer.py:48] [20182] accumulated_eval_time=241.503, accumulated_logging_time=0.382106, accumulated_submission_time=1505.44, global_step=20182, preemption_count=0, score=1505.44, test/loss=0.288969, test/num_examples=3581, test/ssim=0.73946, total_duration=1782.21, train/loss=0.267672, train/ssim=0.744156, validation/loss=0.28752, validation/num_examples=3554, validation/ssim=0.72209
I0315 16:53:55.653912 139650240861952 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.164295, loss=0.344245
I0315 16:53:55.657795 139701292631232 submission.py:265] 20500) loss = 0.344, grad_norm = 0.164
I0315 16:54:26.941778 139650249254656 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.101447, loss=0.298169
I0315 16:54:26.945005 139701292631232 submission.py:265] 21000) loss = 0.298, grad_norm = 0.101
I0315 16:54:55.679593 139701292631232 spec.py:321] Evaluating on the training split.
I0315 16:54:57.674132 139701292631232 spec.py:333] Evaluating on the validation split.
I0315 16:54:59.747859 139701292631232 spec.py:349] Evaluating on the test split.
I0315 16:55:01.880373 139701292631232 submission_runner.py:469] Time since start: 1869.17s, 	Step: 21449, 	{'train/ssim': 0.7440898759024483, 'train/loss': 0.26782682963779997, 'validation/ssim': 0.7219666935583146, 'validation/loss': 0.2876852418709553, 'validation/num_examples': 3554, 'test/ssim': 0.7393716675247836, 'test/loss': 0.28910067893046637, 'test/num_examples': 3581, 'score': 1584.3207559585571, 'total_duration': 1869.1654815673828, 'accumulated_submission_time': 1584.3207559585571, 'accumulated_eval_time': 247.70346307754517, 'accumulated_logging_time': 0.4013783931732178}
I0315 16:55:01.891534 139650240861952 logging_writer.py:48] [21449] accumulated_eval_time=247.703, accumulated_logging_time=0.401378, accumulated_submission_time=1584.32, global_step=21449, preemption_count=0, score=1584.32, test/loss=0.289101, test/num_examples=3581, test/ssim=0.739372, total_duration=1869.17, train/loss=0.267827, train/ssim=0.74409, validation/loss=0.287685, validation/num_examples=3554, validation/ssim=0.721967
I0315 16:55:05.971952 139650249254656 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.391395, loss=0.305083
I0315 16:55:05.975418 139701292631232 submission.py:265] 21500) loss = 0.305, grad_norm = 0.391
I0315 16:55:37.170205 139650240861952 logging_writer.py:48] [22000] global_step=22000, grad_norm=0.192674, loss=0.350888
I0315 16:55:37.173344 139701292631232 submission.py:265] 22000) loss = 0.351, grad_norm = 0.193
I0315 16:56:08.435254 139650249254656 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.212262, loss=0.289613
I0315 16:56:08.438940 139701292631232 submission.py:265] 22500) loss = 0.290, grad_norm = 0.212
I0315 16:56:22.641313 139701292631232 spec.py:321] Evaluating on the training split.
I0315 16:56:24.645077 139701292631232 spec.py:333] Evaluating on the validation split.
I0315 16:56:26.750776 139701292631232 spec.py:349] Evaluating on the test split.
I0315 16:56:28.902110 139701292631232 submission_runner.py:469] Time since start: 1956.19s, 	Step: 22717, 	{'train/ssim': 0.7448106493268695, 'train/loss': 0.26824002606528147, 'validation/ssim': 0.7227862201920371, 'validation/loss': 0.288136015866805, 'validation/num_examples': 3554, 'test/ssim': 0.7399673951933817, 'test/loss': 0.28961152665543843, 'test/num_examples': 3581, 'score': 1663.124354839325, 'total_duration': 1956.1872327327728, 'accumulated_submission_time': 1663.124354839325, 'accumulated_eval_time': 253.96441292762756, 'accumulated_logging_time': 0.4206719398498535}
I0315 16:56:28.913002 139650240861952 logging_writer.py:48] [22717] accumulated_eval_time=253.964, accumulated_logging_time=0.420672, accumulated_submission_time=1663.12, global_step=22717, preemption_count=0, score=1663.12, test/loss=0.289612, test/num_examples=3581, test/ssim=0.739967, total_duration=1956.19, train/loss=0.26824, train/ssim=0.744811, validation/loss=0.288136, validation/num_examples=3554, validation/ssim=0.722786
I0315 16:56:47.502797 139650249254656 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.164806, loss=0.28543
I0315 16:56:47.506049 139701292631232 submission.py:265] 23000) loss = 0.285, grad_norm = 0.165
I0315 16:57:18.799311 139650240861952 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.210513, loss=0.250812
I0315 16:57:18.803026 139701292631232 submission.py:265] 23500) loss = 0.251, grad_norm = 0.211
I0315 16:57:49.597350 139701292631232 spec.py:321] Evaluating on the training split.
I0315 16:57:51.573792 139701292631232 spec.py:333] Evaluating on the validation split.
I0315 16:57:53.670282 139701292631232 spec.py:349] Evaluating on the test split.
I0315 16:57:55.781174 139701292631232 submission_runner.py:469] Time since start: 2043.07s, 	Step: 23985, 	{'train/ssim': 0.7449078559875488, 'train/loss': 0.2678837946483067, 'validation/ssim': 0.7229756112171145, 'validation/loss': 0.28779053352626444, 'validation/num_examples': 3554, 'test/ssim': 0.7402129675282743, 'test/loss': 0.2892608259084404, 'test/num_examples': 3581, 'score': 1741.8666412830353, 'total_duration': 2043.0662777423859, 'accumulated_submission_time': 1741.8666412830353, 'accumulated_eval_time': 260.14845609664917, 'accumulated_logging_time': 0.4394540786743164}
I0315 16:57:55.792577 139650249254656 logging_writer.py:48] [23985] accumulated_eval_time=260.148, accumulated_logging_time=0.439454, accumulated_submission_time=1741.87, global_step=23985, preemption_count=0, score=1741.87, test/loss=0.289261, test/num_examples=3581, test/ssim=0.740213, total_duration=2043.07, train/loss=0.267884, train/ssim=0.744908, validation/loss=0.287791, validation/num_examples=3554, validation/ssim=0.722976
I0315 16:57:57.613202 139650240861952 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.307725, loss=0.334685
I0315 16:57:57.617040 139701292631232 submission.py:265] 24000) loss = 0.335, grad_norm = 0.308
I0315 16:58:28.862821 139650249254656 logging_writer.py:48] [24500] global_step=24500, grad_norm=0.327136, loss=0.269534
I0315 16:58:28.866303 139701292631232 submission.py:265] 24500) loss = 0.270, grad_norm = 0.327
I0315 16:59:00.136324 139650240861952 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.580773, loss=0.25601
I0315 16:59:00.139857 139701292631232 submission.py:265] 25000) loss = 0.256, grad_norm = 0.581
I0315 16:59:16.500035 139701292631232 spec.py:321] Evaluating on the training split.
I0315 16:59:18.492024 139701292631232 spec.py:333] Evaluating on the validation split.
I0315 16:59:20.564279 139701292631232 spec.py:349] Evaluating on the test split.
I0315 16:59:22.650463 139701292631232 submission_runner.py:469] Time since start: 2129.94s, 	Step: 25252, 	{'train/ssim': 0.7441214152744838, 'train/loss': 0.267852851322719, 'validation/ssim': 0.7220747501714617, 'validation/loss': 0.28780593829136186, 'validation/num_examples': 3554, 'test/ssim': 0.7394529341053476, 'test/loss': 0.28921968129319675, 'test/num_examples': 3581, 'score': 1820.6918346881866, 'total_duration': 2129.935579061508, 'accumulated_submission_time': 1820.6918346881866, 'accumulated_eval_time': 266.29911184310913, 'accumulated_logging_time': 0.45902132987976074}
I0315 16:59:22.661793 139650249254656 logging_writer.py:48] [25252] accumulated_eval_time=266.299, accumulated_logging_time=0.459021, accumulated_submission_time=1820.69, global_step=25252, preemption_count=0, score=1820.69, test/loss=0.28922, test/num_examples=3581, test/ssim=0.739453, total_duration=2129.94, train/loss=0.267853, train/ssim=0.744121, validation/loss=0.287806, validation/num_examples=3554, validation/ssim=0.722075
I0315 16:59:39.103001 139650240861952 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.334374, loss=0.301361
I0315 16:59:39.106284 139701292631232 submission.py:265] 25500) loss = 0.301, grad_norm = 0.334
I0315 17:00:10.397761 139650249254656 logging_writer.py:48] [26000] global_step=26000, grad_norm=0.121548, loss=0.24914
I0315 17:00:10.401499 139701292631232 submission.py:265] 26000) loss = 0.249, grad_norm = 0.122
I0315 17:00:41.636140 139650240861952 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.635614, loss=0.279913
I0315 17:00:41.640183 139701292631232 submission.py:265] 26500) loss = 0.280, grad_norm = 0.636
I0315 17:00:43.429694 139701292631232 spec.py:321] Evaluating on the training split.
I0315 17:00:45.420485 139701292631232 spec.py:333] Evaluating on the validation split.
I0315 17:00:47.533421 139701292631232 spec.py:349] Evaluating on the test split.
I0315 17:00:49.679519 139701292631232 submission_runner.py:469] Time since start: 2216.96s, 	Step: 26518, 	{'train/ssim': 0.7416701998029437, 'train/loss': 0.2688691275460379, 'validation/ssim': 0.7195377897263646, 'validation/loss': 0.28861210382447594, 'validation/num_examples': 3554, 'test/ssim': 0.7369149215259355, 'test/loss': 0.2900874679296984, 'test/num_examples': 3581, 'score': 1899.5404329299927, 'total_duration': 2216.9646260738373, 'accumulated_submission_time': 1899.5404329299927, 'accumulated_eval_time': 272.54910707473755, 'accumulated_logging_time': 0.47941064834594727}
I0315 17:00:49.690835 139650249254656 logging_writer.py:48] [26518] accumulated_eval_time=272.549, accumulated_logging_time=0.479411, accumulated_submission_time=1899.54, global_step=26518, preemption_count=0, score=1899.54, test/loss=0.290087, test/num_examples=3581, test/ssim=0.736915, total_duration=2216.96, train/loss=0.268869, train/ssim=0.74167, validation/loss=0.288612, validation/num_examples=3554, validation/ssim=0.719538
I0315 17:01:20.666712 139650240861952 logging_writer.py:48] [27000] global_step=27000, grad_norm=0.22412, loss=0.276198
I0315 17:01:20.670057 139701292631232 submission.py:265] 27000) loss = 0.276, grad_norm = 0.224
I0315 17:01:51.980305 139650249254656 logging_writer.py:48] [27500] global_step=27500, grad_norm=0.311762, loss=0.233766
I0315 17:01:51.983773 139701292631232 submission.py:265] 27500) loss = 0.234, grad_norm = 0.312
I0315 17:02:10.397139 139701292631232 spec.py:321] Evaluating on the training split.
I0315 17:02:12.407454 139701292631232 spec.py:333] Evaluating on the validation split.
I0315 17:02:14.527317 139701292631232 spec.py:349] Evaluating on the test split.
I0315 17:02:16.683691 139701292631232 submission_runner.py:469] Time since start: 2303.97s, 	Step: 27785, 	{'train/ssim': 0.7412637301853725, 'train/loss': 0.26948038169315885, 'validation/ssim': 0.7201796034178038, 'validation/loss': 0.28892583208400746, 'validation/num_examples': 3554, 'test/ssim': 0.7372790530752583, 'test/loss': 0.29048364250994835, 'test/num_examples': 3581, 'score': 1978.3490850925446, 'total_duration': 2303.9687719345093, 'accumulated_submission_time': 1978.3490850925446, 'accumulated_eval_time': 278.8358235359192, 'accumulated_logging_time': 0.4989633560180664}
I0315 17:02:16.695546 139650240861952 logging_writer.py:48] [27785] accumulated_eval_time=278.836, accumulated_logging_time=0.498963, accumulated_submission_time=1978.35, global_step=27785, preemption_count=0, score=1978.35, test/loss=0.290484, test/num_examples=3581, test/ssim=0.737279, total_duration=2303.97, train/loss=0.26948, train/ssim=0.741264, validation/loss=0.288926, validation/num_examples=3554, validation/ssim=0.72018
I0315 17:02:30.994468 139650249254656 logging_writer.py:48] [28000] global_step=28000, grad_norm=0.125194, loss=0.315828
I0315 17:02:30.997510 139701292631232 submission.py:265] 28000) loss = 0.316, grad_norm = 0.125
I0315 17:03:02.198951 139650240861952 logging_writer.py:48] [28500] global_step=28500, grad_norm=0.294202, loss=0.307786
I0315 17:03:02.202437 139701292631232 submission.py:265] 28500) loss = 0.308, grad_norm = 0.294
I0315 17:03:33.464914 139650249254656 logging_writer.py:48] [29000] global_step=29000, grad_norm=0.455244, loss=0.264133
I0315 17:03:33.468592 139701292631232 submission.py:265] 29000) loss = 0.264, grad_norm = 0.455
I0315 17:03:37.408439 139701292631232 spec.py:321] Evaluating on the training split.
I0315 17:03:39.400269 139701292631232 spec.py:333] Evaluating on the validation split.
I0315 17:03:41.498864 139701292631232 spec.py:349] Evaluating on the test split.
I0315 17:03:43.618683 139701292631232 submission_runner.py:469] Time since start: 2390.90s, 	Step: 29054, 	{'train/ssim': 0.7393995012555804, 'train/loss': 0.27100365502493723, 'validation/ssim': 0.716813155621307, 'validation/loss': 0.2909634171004502, 'validation/num_examples': 3554, 'test/ssim': 0.7342815979736805, 'test/loss': 0.29248592288292374, 'test/num_examples': 3581, 'score': 2057.195570707321, 'total_duration': 2390.9037413597107, 'accumulated_submission_time': 2057.195570707321, 'accumulated_eval_time': 285.046101808548, 'accumulated_logging_time': 0.5200028419494629}
I0315 17:03:43.630453 139650240861952 logging_writer.py:48] [29054] accumulated_eval_time=285.046, accumulated_logging_time=0.520003, accumulated_submission_time=2057.2, global_step=29054, preemption_count=0, score=2057.2, test/loss=0.292486, test/num_examples=3581, test/ssim=0.734282, total_duration=2390.9, train/loss=0.271004, train/ssim=0.7394, validation/loss=0.290963, validation/num_examples=3554, validation/ssim=0.716813
I0315 17:04:12.276954 139650249254656 logging_writer.py:48] [29500] global_step=29500, grad_norm=0.150426, loss=0.305983
I0315 17:04:12.280381 139701292631232 submission.py:265] 29500) loss = 0.306, grad_norm = 0.150
I0315 17:04:43.576426 139650240861952 logging_writer.py:48] [30000] global_step=30000, grad_norm=0.163453, loss=0.352499
I0315 17:04:43.580086 139701292631232 submission.py:265] 30000) loss = 0.352, grad_norm = 0.163
I0315 17:05:04.323990 139701292631232 spec.py:321] Evaluating on the training split.
I0315 17:05:06.320203 139701292631232 spec.py:333] Evaluating on the validation split.
I0315 17:05:08.407393 139701292631232 spec.py:349] Evaluating on the test split.
I0315 17:05:10.504087 139701292631232 submission_runner.py:469] Time since start: 2477.79s, 	Step: 30323, 	{'train/ssim': 0.742222513471331, 'train/loss': 0.26952074255262104, 'validation/ssim': 0.7207013389402785, 'validation/loss': 0.2889490852076885, 'validation/num_examples': 3554, 'test/ssim': 0.7380180880864283, 'test/loss': 0.2904815290334404, 'test/num_examples': 3581, 'score': 2136.090759754181, 'total_duration': 2477.7891755104065, 'accumulated_submission_time': 2136.090759754181, 'accumulated_eval_time': 291.226389169693, 'accumulated_logging_time': 0.5408432483673096}
I0315 17:05:10.516204 139650249254656 logging_writer.py:48] [30323] accumulated_eval_time=291.226, accumulated_logging_time=0.540843, accumulated_submission_time=2136.09, global_step=30323, preemption_count=0, score=2136.09, test/loss=0.290482, test/num_examples=3581, test/ssim=0.738018, total_duration=2477.79, train/loss=0.269521, train/ssim=0.742223, validation/loss=0.288949, validation/num_examples=3554, validation/ssim=0.720701
I0315 17:05:22.394175 139650240861952 logging_writer.py:48] [30500] global_step=30500, grad_norm=0.134123, loss=0.252125
I0315 17:05:22.397952 139701292631232 submission.py:265] 30500) loss = 0.252, grad_norm = 0.134
I0315 17:05:53.656450 139650249254656 logging_writer.py:48] [31000] global_step=31000, grad_norm=0.510766, loss=0.399057
I0315 17:05:53.659832 139701292631232 submission.py:265] 31000) loss = 0.399, grad_norm = 0.511
I0315 17:06:24.983516 139650240861952 logging_writer.py:48] [31500] global_step=31500, grad_norm=0.145825, loss=0.267965
I0315 17:06:24.986613 139701292631232 submission.py:265] 31500) loss = 0.268, grad_norm = 0.146
I0315 17:06:31.287522 139701292631232 spec.py:321] Evaluating on the training split.
I0315 17:06:33.291552 139701292631232 spec.py:333] Evaluating on the validation split.
I0315 17:06:35.394130 139701292631232 spec.py:349] Evaluating on the test split.
I0315 17:06:37.543608 139701292631232 submission_runner.py:469] Time since start: 2564.83s, 	Step: 31591, 	{'train/ssim': 0.7405420030866351, 'train/loss': 0.26949373313358854, 'validation/ssim': 0.7190367999745005, 'validation/loss': 0.2893049232598832, 'validation/num_examples': 3554, 'test/ssim': 0.7362351320336498, 'test/loss': 0.2907989254812727, 'test/num_examples': 3581, 'score': 2214.936052799225, 'total_duration': 2564.8287177085876, 'accumulated_submission_time': 2214.936052799225, 'accumulated_eval_time': 297.48258876800537, 'accumulated_logging_time': 0.5610044002532959}
I0315 17:06:37.555113 139650249254656 logging_writer.py:48] [31591] accumulated_eval_time=297.483, accumulated_logging_time=0.561004, accumulated_submission_time=2214.94, global_step=31591, preemption_count=0, score=2214.94, test/loss=0.290799, test/num_examples=3581, test/ssim=0.736235, total_duration=2564.83, train/loss=0.269494, train/ssim=0.740542, validation/loss=0.289305, validation/num_examples=3554, validation/ssim=0.719037
I0315 17:07:04.136186 139650240861952 logging_writer.py:48] [32000] global_step=32000, grad_norm=0.542685, loss=0.245389
I0315 17:07:04.139781 139701292631232 submission.py:265] 32000) loss = 0.245, grad_norm = 0.543
I0315 17:07:35.388152 139650249254656 logging_writer.py:48] [32500] global_step=32500, grad_norm=0.270501, loss=0.302346
I0315 17:07:35.391583 139701292631232 submission.py:265] 32500) loss = 0.302, grad_norm = 0.271
I0315 17:07:58.310630 139701292631232 spec.py:321] Evaluating on the training split.
I0315 17:08:00.286932 139701292631232 spec.py:333] Evaluating on the validation split.
I0315 17:08:02.386600 139701292631232 spec.py:349] Evaluating on the test split.
I0315 17:08:04.515055 139701292631232 submission_runner.py:469] Time since start: 2651.80s, 	Step: 32857, 	{'train/ssim': 0.7419319152832031, 'train/loss': 0.27000543049403597, 'validation/ssim': 0.7201886711056205, 'validation/loss': 0.2896304326441158, 'validation/num_examples': 3554, 'test/ssim': 0.7375638269905403, 'test/loss': 0.2910373733550335, 'test/num_examples': 3581, 'score': 2293.7056753635406, 'total_duration': 2651.800154685974, 'accumulated_submission_time': 2293.7056753635406, 'accumulated_eval_time': 303.6871509552002, 'accumulated_logging_time': 0.5809290409088135}
I0315 17:08:04.526884 139650240861952 logging_writer.py:48] [32857] accumulated_eval_time=303.687, accumulated_logging_time=0.580929, accumulated_submission_time=2293.71, global_step=32857, preemption_count=0, score=2293.71, test/loss=0.291037, test/num_examples=3581, test/ssim=0.737564, total_duration=2651.8, train/loss=0.270005, train/ssim=0.741932, validation/loss=0.28963, validation/num_examples=3554, validation/ssim=0.720189
I0315 17:08:14.310823 139650249254656 logging_writer.py:48] [33000] global_step=33000, grad_norm=0.404892, loss=0.270001
I0315 17:08:14.314021 139701292631232 submission.py:265] 33000) loss = 0.270, grad_norm = 0.405
I0315 17:08:45.613750 139650240861952 logging_writer.py:48] [33500] global_step=33500, grad_norm=0.628388, loss=0.285392
I0315 17:08:45.617089 139701292631232 submission.py:265] 33500) loss = 0.285, grad_norm = 0.628
I0315 17:09:16.889333 139650249254656 logging_writer.py:48] [34000] global_step=34000, grad_norm=0.175449, loss=0.273353
I0315 17:09:16.892987 139701292631232 submission.py:265] 34000) loss = 0.273, grad_norm = 0.175
I0315 17:09:25.291433 139701292631232 spec.py:321] Evaluating on the training split.
I0315 17:09:27.277118 139701292631232 spec.py:333] Evaluating on the validation split.
I0315 17:09:29.367798 139701292631232 spec.py:349] Evaluating on the test split.
I0315 17:09:31.452753 139701292631232 submission_runner.py:469] Time since start: 2738.74s, 	Step: 34124, 	{'train/ssim': 0.7433081354413714, 'train/loss': 0.2693252222878592, 'validation/ssim': 0.7214161063018781, 'validation/loss': 0.28906991901730444, 'validation/num_examples': 3554, 'test/ssim': 0.7385846361438844, 'test/loss': 0.290716090837493, 'test/num_examples': 3581, 'score': 2372.531633615494, 'total_duration': 2738.7378540039062, 'accumulated_submission_time': 2372.531633615494, 'accumulated_eval_time': 309.8486080169678, 'accumulated_logging_time': 0.6009893417358398}
I0315 17:09:31.464596 139650240861952 logging_writer.py:48] [34124] accumulated_eval_time=309.849, accumulated_logging_time=0.600989, accumulated_submission_time=2372.53, global_step=34124, preemption_count=0, score=2372.53, test/loss=0.290716, test/num_examples=3581, test/ssim=0.738585, total_duration=2738.74, train/loss=0.269325, train/ssim=0.743308, validation/loss=0.28907, validation/num_examples=3554, validation/ssim=0.721416
I0315 17:09:55.865268 139650249254656 logging_writer.py:48] [34500] global_step=34500, grad_norm=0.408965, loss=0.304274
I0315 17:09:55.869156 139701292631232 submission.py:265] 34500) loss = 0.304, grad_norm = 0.409
I0315 17:10:27.139393 139650240861952 logging_writer.py:48] [35000] global_step=35000, grad_norm=0.187057, loss=0.262071
I0315 17:10:27.142837 139701292631232 submission.py:265] 35000) loss = 0.262, grad_norm = 0.187
I0315 17:10:52.188878 139701292631232 spec.py:321] Evaluating on the training split.
I0315 17:10:54.182076 139701292631232 spec.py:333] Evaluating on the validation split.
I0315 17:10:56.278892 139701292631232 spec.py:349] Evaluating on the test split.
I0315 17:10:58.419126 139701292631232 submission_runner.py:469] Time since start: 2825.70s, 	Step: 35393, 	{'train/ssim': 0.7400978633335659, 'train/loss': 0.270742552621024, 'validation/ssim': 0.7196672103615644, 'validation/loss': 0.28998623634900816, 'validation/num_examples': 3554, 'test/ssim': 0.7366719399041818, 'test/loss': 0.2916215109911687, 'test/num_examples': 3581, 'score': 2451.3641152381897, 'total_duration': 2825.7041816711426, 'accumulated_submission_time': 2451.3641152381897, 'accumulated_eval_time': 316.0790021419525, 'accumulated_logging_time': 0.6208517551422119}
I0315 17:10:58.431548 139650249254656 logging_writer.py:48] [35393] accumulated_eval_time=316.079, accumulated_logging_time=0.620852, accumulated_submission_time=2451.36, global_step=35393, preemption_count=0, score=2451.36, test/loss=0.291622, test/num_examples=3581, test/ssim=0.736672, total_duration=2825.7, train/loss=0.270743, train/ssim=0.740098, validation/loss=0.289986, validation/num_examples=3554, validation/ssim=0.719667
I0315 17:11:06.029370 139650240861952 logging_writer.py:48] [35500] global_step=35500, grad_norm=0.23387, loss=0.268655
I0315 17:11:06.033125 139701292631232 submission.py:265] 35500) loss = 0.269, grad_norm = 0.234
I0315 17:11:37.190147 139650249254656 logging_writer.py:48] [36000] global_step=36000, grad_norm=0.209578, loss=0.265433
I0315 17:11:37.193771 139701292631232 submission.py:265] 36000) loss = 0.265, grad_norm = 0.210
I0315 17:12:08.447052 139650240861952 logging_writer.py:48] [36500] global_step=36500, grad_norm=0.194783, loss=0.305389
I0315 17:12:08.450371 139701292631232 submission.py:265] 36500) loss = 0.305, grad_norm = 0.195
I0315 17:12:19.123772 139701292631232 spec.py:321] Evaluating on the training split.
I0315 17:12:21.141698 139701292631232 spec.py:333] Evaluating on the validation split.
I0315 17:12:23.267878 139701292631232 spec.py:349] Evaluating on the test split.
I0315 17:12:25.441276 139701292631232 submission_runner.py:469] Time since start: 2912.73s, 	Step: 36662, 	{'train/ssim': 0.744178022657122, 'train/loss': 0.2699739933013916, 'validation/ssim': 0.7222212757632246, 'validation/loss': 0.28969703206334413, 'validation/num_examples': 3554, 'test/ssim': 0.7393649180352905, 'test/loss': 0.2913939713832554, 'test/num_examples': 3581, 'score': 2530.162103652954, 'total_duration': 2912.7263884544373, 'accumulated_submission_time': 2530.162103652954, 'accumulated_eval_time': 322.39671087265015, 'accumulated_logging_time': 0.6413724422454834}
I0315 17:12:25.453204 139650249254656 logging_writer.py:48] [36662] accumulated_eval_time=322.397, accumulated_logging_time=0.641372, accumulated_submission_time=2530.16, global_step=36662, preemption_count=0, score=2530.16, test/loss=0.291394, test/num_examples=3581, test/ssim=0.739365, total_duration=2912.73, train/loss=0.269974, train/ssim=0.744178, validation/loss=0.289697, validation/num_examples=3554, validation/ssim=0.722221
I0315 17:12:47.445669 139650240861952 logging_writer.py:48] [37000] global_step=37000, grad_norm=0.171498, loss=0.259208
I0315 17:12:47.448867 139701292631232 submission.py:265] 37000) loss = 0.259, grad_norm = 0.171
I0315 17:13:18.575939 139650249254656 logging_writer.py:48] [37500] global_step=37500, grad_norm=0.103655, loss=0.322778
I0315 17:13:18.579392 139701292631232 submission.py:265] 37500) loss = 0.323, grad_norm = 0.104
I0315 17:13:46.193519 139701292631232 spec.py:321] Evaluating on the training split.
I0315 17:13:48.185323 139701292631232 spec.py:333] Evaluating on the validation split.
I0315 17:13:50.335620 139701292631232 spec.py:349] Evaluating on the test split.
I0315 17:13:52.504433 139701292631232 submission_runner.py:469] Time since start: 2999.79s, 	Step: 37933, 	{'train/ssim': 0.7406497001647949, 'train/loss': 0.2706942558288574, 'validation/ssim': 0.7189205000087929, 'validation/loss': 0.29033335018640966, 'validation/num_examples': 3554, 'test/ssim': 0.7359679476970469, 'test/loss': 0.2919584400525342, 'test/num_examples': 3581, 'score': 2608.98223733902, 'total_duration': 2999.7895138263702, 'accumulated_submission_time': 2608.98223733902, 'accumulated_eval_time': 328.70778465270996, 'accumulated_logging_time': 0.6614723205566406}
I0315 17:13:52.517765 139650240861952 logging_writer.py:48] [37933] accumulated_eval_time=328.708, accumulated_logging_time=0.661472, accumulated_submission_time=2608.98, global_step=37933, preemption_count=0, score=2608.98, test/loss=0.291958, test/num_examples=3581, test/ssim=0.735968, total_duration=2999.79, train/loss=0.270694, train/ssim=0.74065, validation/loss=0.290333, validation/num_examples=3554, validation/ssim=0.718921
I0315 17:13:57.592214 139650249254656 logging_writer.py:48] [38000] global_step=38000, grad_norm=0.15385, loss=0.254395
I0315 17:13:57.595412 139701292631232 submission.py:265] 38000) loss = 0.254, grad_norm = 0.154
I0315 17:14:28.789689 139650240861952 logging_writer.py:48] [38500] global_step=38500, grad_norm=0.381695, loss=0.345205
I0315 17:14:28.792957 139701292631232 submission.py:265] 38500) loss = 0.345, grad_norm = 0.382
I0315 17:15:00.053335 139650249254656 logging_writer.py:48] [39000] global_step=39000, grad_norm=0.292924, loss=0.264093
I0315 17:15:00.056674 139701292631232 submission.py:265] 39000) loss = 0.264, grad_norm = 0.293
I0315 17:15:13.230693 139701292631232 spec.py:321] Evaluating on the training split.
I0315 17:15:15.299191 139701292631232 spec.py:333] Evaluating on the validation split.
I0315 17:15:17.471378 139701292631232 spec.py:349] Evaluating on the test split.
I0315 17:15:19.632418 139701292631232 submission_runner.py:469] Time since start: 3086.92s, 	Step: 39201, 	{'train/ssim': 0.7454389163425991, 'train/loss': 0.26945492199489046, 'validation/ssim': 0.7236765022685706, 'validation/loss': 0.28923588518218907, 'validation/num_examples': 3554, 'test/ssim': 0.7408163309829656, 'test/loss': 0.29085803464683396, 'test/num_examples': 3581, 'score': 2687.7463178634644, 'total_duration': 3086.91752576828, 'accumulated_submission_time': 2687.7463178634644, 'accumulated_eval_time': 335.1096398830414, 'accumulated_logging_time': 0.6833515167236328}
I0315 17:15:19.644754 139650240861952 logging_writer.py:48] [39201] accumulated_eval_time=335.11, accumulated_logging_time=0.683352, accumulated_submission_time=2687.75, global_step=39201, preemption_count=0, score=2687.75, test/loss=0.290858, test/num_examples=3581, test/ssim=0.740816, total_duration=3086.92, train/loss=0.269455, train/ssim=0.745439, validation/loss=0.289236, validation/num_examples=3554, validation/ssim=0.723677
I0315 17:15:20.357010 139650249254656 logging_writer.py:48] [39201] global_step=39201, preemption_count=0, score=2687.75
I0315 17:15:21.284775 139701292631232 submission_runner.py:646] Tuning trial 1/5
I0315 17:15:21.284966 139701292631232 submission_runner.py:647] Hyperparameters: Hyperparameters(learning_rate=0.0017486387539278373, one_minus_beta1=0.06733926164, one_minus_beta2=0.00448403102, epsilon=1e-08, one_minus_momentum=0.0, use_momentum=False, weight_decay=0.08121616522670176, max_preconditioner_dim=1024, precondition_frequency=100, start_preconditioning_step=-1, inv_root_override=0, exponent_multiplier=1.0, grafting_type='ADAM', grafting_epsilon=1e-08, use_normalized_grafting=False, communication_dtype='FP32', communicate_params=True, use_cosine_decay=True, warmup_factor=0.02, label_smoothing=0.0, dropout_rate=0.1, use_nadam=True, step_hint_factor=1.0)
I0315 17:15:21.285878 139701292631232 submission_runner.py:648] Metrics: {'eval_results': [(1, {'train/ssim': 0.2271113055092948, 'train/loss': 0.946084703717913, 'validation/ssim': 0.2227439214892111, 'validation/loss': 0.9462951076252111, 'validation/num_examples': 3554, 'test/ssim': 0.24524549597703155, 'test/loss': 0.9454325591009843, 'test/num_examples': 3581, 'score': 86.10756826400757, 'total_duration': 212.41248416900635, 'accumulated_submission_time': 86.10756826400757, 'accumulated_eval_time': 125.53008151054382, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (390, {'train/ssim': 0.7054547582353864, 'train/loss': 0.3018218108585903, 'validation/ssim': 0.6867688833224888, 'validation/loss': 0.320512195215778, 'validation/num_examples': 3554, 'test/ssim': 0.7042586415282044, 'test/loss': 0.32244377061662244, 'test/num_examples': 3581, 'score': 165.25808787345886, 'total_duration': 300.1717827320099, 'accumulated_submission_time': 165.25808787345886, 'accumulated_eval_time': 132.32103157043457, 'accumulated_logging_time': 0.017559051513671875, 'global_step': 390, 'preemption_count': 0}), (653, {'train/ssim': 0.7174690791538784, 'train/loss': 0.29100806372506277, 'validation/ssim': 0.6967504154649691, 'validation/loss': 0.3104598732831844, 'validation/num_examples': 3554, 'test/ssim': 0.7139228196558224, 'test/loss': 0.31248343307124404, 'test/num_examples': 3581, 'score': 244.31596684455872, 'total_duration': 388.00668573379517, 'accumulated_submission_time': 244.31596684455872, 'accumulated_eval_time': 139.40369176864624, 'accumulated_logging_time': 0.05010700225830078, 'global_step': 653, 'preemption_count': 0}), (1296, {'train/ssim': 0.7273641313825335, 'train/loss': 0.28153981481279644, 'validation/ssim': 0.7063798877585116, 'validation/loss': 0.30049060312763787, 'validation/num_examples': 3554, 'test/ssim': 0.7235214118350322, 'test/loss': 0.3025932493105976, 'test/num_examples': 3581, 'score': 323.17994022369385, 'total_duration': 475.8895523548126, 'accumulated_submission_time': 323.17994022369385, 'accumulated_eval_time': 146.55387139320374, 'accumulated_logging_time': 0.06798171997070312, 'global_step': 1296, 'preemption_count': 0}), (2559, {'train/ssim': 0.7344277245657784, 'train/loss': 0.2763887814113072, 'validation/ssim': 0.7140416031012592, 'validation/loss': 0.2951390186453644, 'validation/num_examples': 3554, 'test/ssim': 0.7312238747032952, 'test/loss': 0.2968394458710032, 'test/num_examples': 3581, 'score': 402.05144333839417, 'total_duration': 563.3437728881836, 'accumulated_submission_time': 402.05144333839417, 'accumulated_eval_time': 153.26849603652954, 'accumulated_logging_time': 0.1112070083618164, 'global_step': 2559, 'preemption_count': 0}), (3819, {'train/ssim': 0.7371646336146763, 'train/loss': 0.2737900699887957, 'validation/ssim': 0.7157713332468697, 'validation/loss': 0.29285488869275816, 'validation/num_examples': 3554, 'test/ssim': 0.7329933999537489, 'test/loss': 0.2946332831916713, 'test/num_examples': 3581, 'score': 480.8701608181, 'total_duration': 650.570797920227, 'accumulated_submission_time': 480.8701608181, 'accumulated_eval_time': 159.74382209777832, 'accumulated_logging_time': 0.12930870056152344, 'global_step': 3819, 'preemption_count': 0}), (5083, {'train/ssim': 0.7390016828264508, 'train/loss': 0.2727112088884626, 'validation/ssim': 0.7177918476540518, 'validation/loss': 0.29185404265000703, 'validation/num_examples': 3554, 'test/ssim': 0.7351633949141302, 'test/loss': 0.2934568267200154, 'test/num_examples': 3581, 'score': 559.6900432109833, 'total_duration': 737.862352848053, 'accumulated_submission_time': 559.6900432109833, 'accumulated_eval_time': 166.24487161636353, 'accumulated_logging_time': 0.14738798141479492, 'global_step': 5083, 'preemption_count': 0}), (6343, {'train/ssim': 0.7390495709010533, 'train/loss': 0.27166243961879183, 'validation/ssim': 0.7175115049723902, 'validation/loss': 0.2910257231068866, 'validation/num_examples': 3554, 'test/ssim': 0.7349857265341385, 'test/loss': 0.2926165834547787, 'test/num_examples': 3581, 'score': 638.5154089927673, 'total_duration': 824.9178631305695, 'accumulated_submission_time': 638.5154089927673, 'accumulated_eval_time': 172.50324892997742, 'accumulated_logging_time': 0.16601133346557617, 'global_step': 6343, 'preemption_count': 0}), (7535, {'train/ssim': 0.7389940534319196, 'train/loss': 0.2723602567400251, 'validation/ssim': 0.7176094634786508, 'validation/loss': 0.29172901846950616, 'validation/num_examples': 3554, 'test/ssim': 0.7346233675780159, 'test/loss': 0.29342979467371194, 'test/num_examples': 3581, 'score': 717.288907289505, 'total_duration': 911.8695116043091, 'accumulated_submission_time': 717.288907289505, 'accumulated_eval_time': 178.72747659683228, 'accumulated_logging_time': 0.1859121322631836, 'global_step': 7535, 'preemption_count': 0}), (8802, {'train/ssim': 0.7420942442757743, 'train/loss': 0.2698760713849749, 'validation/ssim': 0.7202103786006964, 'validation/loss': 0.28969988288943793, 'validation/num_examples': 3554, 'test/ssim': 0.7375037633517174, 'test/loss': 0.29126569699455457, 'test/num_examples': 3581, 'score': 796.1220152378082, 'total_duration': 998.9878261089325, 'accumulated_submission_time': 796.1220152378082, 'accumulated_eval_time': 185.0491647720337, 'accumulated_logging_time': 0.20443987846374512, 'global_step': 8802, 'preemption_count': 0}), (10067, {'train/ssim': 0.7427368845258441, 'train/loss': 0.26930616583142963, 'validation/ssim': 0.7210059995119935, 'validation/loss': 0.28903625866101573, 'validation/num_examples': 3554, 'test/ssim': 0.7383137020908964, 'test/loss': 0.2905004480570197, 'test/num_examples': 3581, 'score': 874.9218046665192, 'total_duration': 1086.002522468567, 'accumulated_submission_time': 874.9218046665192, 'accumulated_eval_time': 191.29699087142944, 'accumulated_logging_time': 0.2237098217010498, 'global_step': 10067, 'preemption_count': 0}), (11329, {'train/ssim': 0.7433338165283203, 'train/loss': 0.2687626225607736, 'validation/ssim': 0.7215962922499296, 'validation/loss': 0.2884800384469963, 'validation/num_examples': 3554, 'test/ssim': 0.7389056800431094, 'test/loss': 0.2899639999956367, 'test/num_examples': 3581, 'score': 953.7212605476379, 'total_duration': 1172.9341661930084, 'accumulated_submission_time': 953.7212605476379, 'accumulated_eval_time': 197.49743509292603, 'accumulated_logging_time': 0.24236488342285156, 'global_step': 11329, 'preemption_count': 0}), (12594, {'train/ssim': 0.742595945085798, 'train/loss': 0.2687539373125349, 'validation/ssim': 0.7204971785751969, 'validation/loss': 0.2885513434466446, 'validation/num_examples': 3554, 'test/ssim': 0.7379300720163712, 'test/loss': 0.29001762093994343, 'test/num_examples': 3581, 'score': 1032.5482022762299, 'total_duration': 1260.017174243927, 'accumulated_submission_time': 1032.5482022762299, 'accumulated_eval_time': 203.81220960617065, 'accumulated_logging_time': 0.26149439811706543, 'global_step': 12594, 'preemption_count': 0}), (13859, {'train/ssim': 0.7426636559622628, 'train/loss': 0.26871505805424284, 'validation/ssim': 0.7201917623628307, 'validation/loss': 0.28880843300462505, 'validation/num_examples': 3554, 'test/ssim': 0.7376006423877758, 'test/loss': 0.2903110192029112, 'test/num_examples': 3581, 'score': 1111.347316980362, 'total_duration': 1347.1644864082336, 'accumulated_submission_time': 1111.347316980362, 'accumulated_eval_time': 210.1701259613037, 'accumulated_logging_time': 0.2822451591491699, 'global_step': 13859, 'preemption_count': 0}), (15122, {'train/ssim': 0.7442771366664341, 'train/loss': 0.26784276962280273, 'validation/ssim': 0.7222817270153349, 'validation/loss': 0.2876339613485685, 'validation/num_examples': 3554, 'test/ssim': 0.7395656983035465, 'test/loss': 0.2891190184524225, 'test/num_examples': 3581, 'score': 1190.1393880844116, 'total_duration': 1434.3606748580933, 'accumulated_submission_time': 1190.1393880844116, 'accumulated_eval_time': 216.6176700592041, 'accumulated_logging_time': 0.30364179611206055, 'global_step': 15122, 'preemption_count': 0}), (16381, {'train/ssim': 0.7445347649710519, 'train/loss': 0.26749324798583984, 'validation/ssim': 0.7225003132473973, 'validation/loss': 0.2873415627637873, 'validation/num_examples': 3554, 'test/ssim': 0.7398439954359816, 'test/loss': 0.28880387183442124, 'test/num_examples': 3581, 'score': 1268.9842171669006, 'total_duration': 1521.3082089424133, 'accumulated_submission_time': 1268.9842171669006, 'accumulated_eval_time': 222.81372022628784, 'accumulated_logging_time': 0.32393956184387207, 'global_step': 16381, 'preemption_count': 0}), (17646, {'train/ssim': 0.7444740022931781, 'train/loss': 0.26746344566345215, 'validation/ssim': 0.722374739510059, 'validation/loss': 0.28732967859717923, 'validation/num_examples': 3554, 'test/ssim': 0.7397396851438146, 'test/loss': 0.28878106674113374, 'test/num_examples': 3581, 'score': 1347.7574064731598, 'total_duration': 1608.285701751709, 'accumulated_submission_time': 1347.7574064731598, 'accumulated_eval_time': 229.08402967453003, 'accumulated_logging_time': 0.3439915180206299, 'global_step': 17646, 'preemption_count': 0}), (18913, {'train/ssim': 0.7444512503487724, 'train/loss': 0.2674852269036429, 'validation/ssim': 0.7223838071978756, 'validation/loss': 0.2873302796749701, 'validation/num_examples': 3554, 'test/ssim': 0.7397379807272759, 'test/loss': 0.288783896072588, 'test/num_examples': 3581, 'score': 1426.626967906952, 'total_duration': 1695.3212127685547, 'accumulated_submission_time': 1426.626967906952, 'accumulated_eval_time': 235.31906628608704, 'accumulated_logging_time': 0.36339688301086426, 'global_step': 18913, 'preemption_count': 0}), (20182, {'train/ssim': 0.7441560200282505, 'train/loss': 0.26767165320260183, 'validation/ssim': 0.7220897942898846, 'validation/loss': 0.2875203061251407, 'validation/num_examples': 3554, 'test/ssim': 0.7394595472415177, 'test/loss': 0.288968723002042, 'test/num_examples': 3581, 'score': 1505.4410138130188, 'total_duration': 1782.2139139175415, 'accumulated_submission_time': 1505.4410138130188, 'accumulated_eval_time': 241.50254726409912, 'accumulated_logging_time': 0.38210630416870117, 'global_step': 20182, 'preemption_count': 0}), (21449, {'train/ssim': 0.7440898759024483, 'train/loss': 0.26782682963779997, 'validation/ssim': 0.7219666935583146, 'validation/loss': 0.2876852418709553, 'validation/num_examples': 3554, 'test/ssim': 0.7393716675247836, 'test/loss': 0.28910067893046637, 'test/num_examples': 3581, 'score': 1584.3207559585571, 'total_duration': 1869.1654815673828, 'accumulated_submission_time': 1584.3207559585571, 'accumulated_eval_time': 247.70346307754517, 'accumulated_logging_time': 0.4013783931732178, 'global_step': 21449, 'preemption_count': 0}), (22717, {'train/ssim': 0.7448106493268695, 'train/loss': 0.26824002606528147, 'validation/ssim': 0.7227862201920371, 'validation/loss': 0.288136015866805, 'validation/num_examples': 3554, 'test/ssim': 0.7399673951933817, 'test/loss': 0.28961152665543843, 'test/num_examples': 3581, 'score': 1663.124354839325, 'total_duration': 1956.1872327327728, 'accumulated_submission_time': 1663.124354839325, 'accumulated_eval_time': 253.96441292762756, 'accumulated_logging_time': 0.4206719398498535, 'global_step': 22717, 'preemption_count': 0}), (23985, {'train/ssim': 0.7449078559875488, 'train/loss': 0.2678837946483067, 'validation/ssim': 0.7229756112171145, 'validation/loss': 0.28779053352626444, 'validation/num_examples': 3554, 'test/ssim': 0.7402129675282743, 'test/loss': 0.2892608259084404, 'test/num_examples': 3581, 'score': 1741.8666412830353, 'total_duration': 2043.0662777423859, 'accumulated_submission_time': 1741.8666412830353, 'accumulated_eval_time': 260.14845609664917, 'accumulated_logging_time': 0.4394540786743164, 'global_step': 23985, 'preemption_count': 0}), (25252, {'train/ssim': 0.7441214152744838, 'train/loss': 0.267852851322719, 'validation/ssim': 0.7220747501714617, 'validation/loss': 0.28780593829136186, 'validation/num_examples': 3554, 'test/ssim': 0.7394529341053476, 'test/loss': 0.28921968129319675, 'test/num_examples': 3581, 'score': 1820.6918346881866, 'total_duration': 2129.935579061508, 'accumulated_submission_time': 1820.6918346881866, 'accumulated_eval_time': 266.29911184310913, 'accumulated_logging_time': 0.45902132987976074, 'global_step': 25252, 'preemption_count': 0}), (26518, {'train/ssim': 0.7416701998029437, 'train/loss': 0.2688691275460379, 'validation/ssim': 0.7195377897263646, 'validation/loss': 0.28861210382447594, 'validation/num_examples': 3554, 'test/ssim': 0.7369149215259355, 'test/loss': 0.2900874679296984, 'test/num_examples': 3581, 'score': 1899.5404329299927, 'total_duration': 2216.9646260738373, 'accumulated_submission_time': 1899.5404329299927, 'accumulated_eval_time': 272.54910707473755, 'accumulated_logging_time': 0.47941064834594727, 'global_step': 26518, 'preemption_count': 0}), (27785, {'train/ssim': 0.7412637301853725, 'train/loss': 0.26948038169315885, 'validation/ssim': 0.7201796034178038, 'validation/loss': 0.28892583208400746, 'validation/num_examples': 3554, 'test/ssim': 0.7372790530752583, 'test/loss': 0.29048364250994835, 'test/num_examples': 3581, 'score': 1978.3490850925446, 'total_duration': 2303.9687719345093, 'accumulated_submission_time': 1978.3490850925446, 'accumulated_eval_time': 278.8358235359192, 'accumulated_logging_time': 0.4989633560180664, 'global_step': 27785, 'preemption_count': 0}), (29054, {'train/ssim': 0.7393995012555804, 'train/loss': 0.27100365502493723, 'validation/ssim': 0.716813155621307, 'validation/loss': 0.2909634171004502, 'validation/num_examples': 3554, 'test/ssim': 0.7342815979736805, 'test/loss': 0.29248592288292374, 'test/num_examples': 3581, 'score': 2057.195570707321, 'total_duration': 2390.9037413597107, 'accumulated_submission_time': 2057.195570707321, 'accumulated_eval_time': 285.046101808548, 'accumulated_logging_time': 0.5200028419494629, 'global_step': 29054, 'preemption_count': 0}), (30323, {'train/ssim': 0.742222513471331, 'train/loss': 0.26952074255262104, 'validation/ssim': 0.7207013389402785, 'validation/loss': 0.2889490852076885, 'validation/num_examples': 3554, 'test/ssim': 0.7380180880864283, 'test/loss': 0.2904815290334404, 'test/num_examples': 3581, 'score': 2136.090759754181, 'total_duration': 2477.7891755104065, 'accumulated_submission_time': 2136.090759754181, 'accumulated_eval_time': 291.226389169693, 'accumulated_logging_time': 0.5408432483673096, 'global_step': 30323, 'preemption_count': 0}), (31591, {'train/ssim': 0.7405420030866351, 'train/loss': 0.26949373313358854, 'validation/ssim': 0.7190367999745005, 'validation/loss': 0.2893049232598832, 'validation/num_examples': 3554, 'test/ssim': 0.7362351320336498, 'test/loss': 0.2907989254812727, 'test/num_examples': 3581, 'score': 2214.936052799225, 'total_duration': 2564.8287177085876, 'accumulated_submission_time': 2214.936052799225, 'accumulated_eval_time': 297.48258876800537, 'accumulated_logging_time': 0.5610044002532959, 'global_step': 31591, 'preemption_count': 0}), (32857, {'train/ssim': 0.7419319152832031, 'train/loss': 0.27000543049403597, 'validation/ssim': 0.7201886711056205, 'validation/loss': 0.2896304326441158, 'validation/num_examples': 3554, 'test/ssim': 0.7375638269905403, 'test/loss': 0.2910373733550335, 'test/num_examples': 3581, 'score': 2293.7056753635406, 'total_duration': 2651.800154685974, 'accumulated_submission_time': 2293.7056753635406, 'accumulated_eval_time': 303.6871509552002, 'accumulated_logging_time': 0.5809290409088135, 'global_step': 32857, 'preemption_count': 0}), (34124, {'train/ssim': 0.7433081354413714, 'train/loss': 0.2693252222878592, 'validation/ssim': 0.7214161063018781, 'validation/loss': 0.28906991901730444, 'validation/num_examples': 3554, 'test/ssim': 0.7385846361438844, 'test/loss': 0.290716090837493, 'test/num_examples': 3581, 'score': 2372.531633615494, 'total_duration': 2738.7378540039062, 'accumulated_submission_time': 2372.531633615494, 'accumulated_eval_time': 309.8486080169678, 'accumulated_logging_time': 0.6009893417358398, 'global_step': 34124, 'preemption_count': 0}), (35393, {'train/ssim': 0.7400978633335659, 'train/loss': 0.270742552621024, 'validation/ssim': 0.7196672103615644, 'validation/loss': 0.28998623634900816, 'validation/num_examples': 3554, 'test/ssim': 0.7366719399041818, 'test/loss': 0.2916215109911687, 'test/num_examples': 3581, 'score': 2451.3641152381897, 'total_duration': 2825.7041816711426, 'accumulated_submission_time': 2451.3641152381897, 'accumulated_eval_time': 316.0790021419525, 'accumulated_logging_time': 0.6208517551422119, 'global_step': 35393, 'preemption_count': 0}), (36662, {'train/ssim': 0.744178022657122, 'train/loss': 0.2699739933013916, 'validation/ssim': 0.7222212757632246, 'validation/loss': 0.28969703206334413, 'validation/num_examples': 3554, 'test/ssim': 0.7393649180352905, 'test/loss': 0.2913939713832554, 'test/num_examples': 3581, 'score': 2530.162103652954, 'total_duration': 2912.7263884544373, 'accumulated_submission_time': 2530.162103652954, 'accumulated_eval_time': 322.39671087265015, 'accumulated_logging_time': 0.6413724422454834, 'global_step': 36662, 'preemption_count': 0}), (37933, {'train/ssim': 0.7406497001647949, 'train/loss': 0.2706942558288574, 'validation/ssim': 0.7189205000087929, 'validation/loss': 0.29033335018640966, 'validation/num_examples': 3554, 'test/ssim': 0.7359679476970469, 'test/loss': 0.2919584400525342, 'test/num_examples': 3581, 'score': 2608.98223733902, 'total_duration': 2999.7895138263702, 'accumulated_submission_time': 2608.98223733902, 'accumulated_eval_time': 328.70778465270996, 'accumulated_logging_time': 0.6614723205566406, 'global_step': 37933, 'preemption_count': 0}), (39201, {'train/ssim': 0.7454389163425991, 'train/loss': 0.26945492199489046, 'validation/ssim': 0.7236765022685706, 'validation/loss': 0.28923588518218907, 'validation/num_examples': 3554, 'test/ssim': 0.7408163309829656, 'test/loss': 0.29085803464683396, 'test/num_examples': 3581, 'score': 2687.7463178634644, 'total_duration': 3086.91752576828, 'accumulated_submission_time': 2687.7463178634644, 'accumulated_eval_time': 335.1096398830414, 'accumulated_logging_time': 0.6833515167236328, 'global_step': 39201, 'preemption_count': 0})], 'global_step': 39201}
I0315 17:15:21.285968 139701292631232 submission_runner.py:649] Timing: 2687.7463178634644
I0315 17:15:21.286002 139701292631232 submission_runner.py:651] Total number of evals: 34
I0315 17:15:21.286029 139701292631232 submission_runner.py:652] ====================
I0315 17:15:21.286145 139701292631232 submission_runner.py:750] Final fastmri score: 0
