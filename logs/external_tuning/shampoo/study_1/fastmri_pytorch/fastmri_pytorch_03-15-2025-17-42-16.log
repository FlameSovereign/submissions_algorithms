torchrun --redirects 1:0,2:0,3:0,4:0,5:0,6:0,7:0 --standalone --nnodes=1 --nproc_per_node=8 submission_runner.py --framework=pytorch --workload=fastmri --submission_path=submissions_algorithms/leaderboard/external_tuning/shampoo_submission/submission.py --data_dir=/data/fastmri --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=submissions/rolling_leaderboard/external_tuning/shampoo/study_1 --overwrite=True --save_checkpoints=False --rng_seed=-955102106 --torch_compile=true --tuning_ruleset=external --tuning_search_space=submissions_algorithms/leaderboard/external_tuning/shampoo_submission/tuning_search_space.json --num_tuning_trials=5 --hparam_start_index=3 --hparam_end_index=4 2>&1 | tee -a /logs/fastmri_pytorch_03-15-2025-17-42-16.log
W0315 17:42:19.000000 9 site-packages/torch/distributed/run.py:793] 
W0315 17:42:19.000000 9 site-packages/torch/distributed/run.py:793] *****************************************
W0315 17:42:19.000000 9 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0315 17:42:19.000000 9 site-packages/torch/distributed/run.py:793] *****************************************
2025-03-15 17:42:21.115811: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-03-15 17:42:21.115811: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-03-15 17:42:21.115812: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-03-15 17:42:21.115811: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-03-15 17:42:21.115811: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-03-15 17:42:21.115811: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-03-15 17:42:21.115811: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-03-15 17:42:21.115811: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1742060541.138802      44 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1742060541.138802      47 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1742060541.138802      45 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1742060541.138802      49 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1742060541.138802      46 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1742060541.138802      51 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1742060541.138806      50 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1742060541.138805      48 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1742060541.145572      49 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
E0000 00:00:1742060541.145572      50 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
E0000 00:00:1742060541.145572      46 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
E0000 00:00:1742060541.145575      45 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
E0000 00:00:1742060541.145576      48 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
E0000 00:00:1742060541.145575      44 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
E0000 00:00:1742060541.145587      51 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
E0000 00:00:1742060541.145587      47 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
[rank7]:[W315 17:42:34.069488386 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 7]  using GPU 7 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank1]:[W315 17:42:34.091637947 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank4]:[W315 17:42:34.101201755 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 4]  using GPU 4 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank5]:[W315 17:42:34.103123119 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 5]  using GPU 5 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank2]:[W315 17:42:34.103557447 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank6]:[W315 17:42:34.106867703 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 6]  using GPU 6 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank0]:[W315 17:42:34.142807808 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank3]:[W315 17:42:34.150305835 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
I0315 17:42:35.999377 139707012998336 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_1/fastmri_pytorch.
I0315 17:42:35.999377 139655922324672 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_1/fastmri_pytorch.
I0315 17:42:35.999391 140460036412608 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_1/fastmri_pytorch.
I0315 17:42:35.999376 139961872823488 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_1/fastmri_pytorch.
I0315 17:42:35.999376 140613204722880 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_1/fastmri_pytorch.
I0315 17:42:35.999377 139826544436416 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_1/fastmri_pytorch.
I0315 17:42:35.999462 140481403532480 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_1/fastmri_pytorch.
I0315 17:42:35.999419 139998127166656 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_1/fastmri_pytorch.
I0315 17:42:36.242916 140460036412608 submission_runner.py:606] Using RNG seed -955102106
I0315 17:42:36.242957 139961872823488 logger_utils.py:97] Saving hparams to /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_1/fastmri_pytorch/trial_4/hparams.json.
I0315 17:42:36.243000 139655922324672 logger_utils.py:97] Saving hparams to /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_1/fastmri_pytorch/trial_4/hparams.json.
I0315 17:42:36.242983 139707012998336 logger_utils.py:97] Saving hparams to /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_1/fastmri_pytorch/trial_4/hparams.json.
I0315 17:42:36.243637 139826544436416 logger_utils.py:97] Saving hparams to /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_1/fastmri_pytorch/trial_4/hparams.json.
I0315 17:42:36.244138 140460036412608 submission_runner.py:615] --- Tuning run 4/5 ---
I0315 17:42:36.244273 140460036412608 submission_runner.py:620] Creating tuning directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_1/fastmri_pytorch/trial_4.
I0315 17:42:36.243909 140613204722880 logger_utils.py:97] Saving hparams to /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_1/fastmri_pytorch/trial_4/hparams.json.
I0315 17:42:36.244252 140481403532480 logger_utils.py:97] Saving hparams to /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_1/fastmri_pytorch/trial_4/hparams.json.
I0315 17:42:36.244489 140460036412608 logger_utils.py:97] Saving hparams to /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_1/fastmri_pytorch/trial_4/hparams.json.
I0315 17:42:36.244937 139998127166656 logger_utils.py:91] Loading hparams from /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_1/fastmri_pytorch/trial_4/hparams.json.
I0315 17:42:36.590067 140460036412608 submission_runner.py:218] Initializing dataset.
I0315 17:42:36.590250 140460036412608 submission_runner.py:229] Initializing model.
I0315 17:42:36.807292 140460036412608 submission_runner.py:268] Performing `torch.compile`.
W0315 17:42:37.829091 139655922324672 distributed_shampoo.py:341] start_preconditioning_step set to -1. Setting start_preconditioning_step equal to precondition frequency 100 by default.
W0315 17:42:37.829113 139707012998336 distributed_shampoo.py:341] start_preconditioning_step set to -1. Setting start_preconditioning_step equal to precondition frequency 100 by default.
W0315 17:42:37.829226 140613204722880 distributed_shampoo.py:341] start_preconditioning_step set to -1. Setting start_preconditioning_step equal to precondition frequency 100 by default.
I0315 17:42:37.829808 140460036412608 submission_runner.py:272] Initializing optimizer.
W0315 17:42:37.830386 139961872823488 distributed_shampoo.py:341] start_preconditioning_step set to -1. Setting start_preconditioning_step equal to precondition frequency 100 by default.
W0315 17:42:37.831260 140460036412608 distributed_shampoo.py:341] start_preconditioning_step set to -1. Setting start_preconditioning_step equal to precondition frequency 100 by default.
W0315 17:42:37.831266 140481403532480 distributed_shampoo.py:341] start_preconditioning_step set to -1. Setting start_preconditioning_step equal to precondition frequency 100 by default.
I0315 17:42:37.832201 139707012998336 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 0.block_0 ([torch.Size([0])]) for Parameter 0 (torch.Size([32, 1, 3, 3])), Block block_0 (torch.Size([288])).
I0315 17:42:37.832202 139655922324672 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 0.block_0 ([torch.Size([0])]) for Parameter 0 (torch.Size([32, 1, 3, 3])), Block block_0 (torch.Size([288])).
I0315 17:42:37.832243 140613204722880 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 0.block_0 ([torch.Size([0])]) for Parameter 0 (torch.Size([32, 1, 3, 3])), Block block_0 (torch.Size([288])).
I0315 17:42:37.834764 139707012998336 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 1.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 1 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 17:42:37.834796 139655922324672 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 1.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 1 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 17:42:37.834914 140613204722880 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 1.block_0 ([torch.Size([1024, 1024]), torch.Size([9, 9])]) for Parameter 1 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 17:42:37.834999 139655922324672 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 2.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 2 (torch.Size([64, 32, 3, 3])), Block block_0 (torch.Size([64, 288])).
I0315 17:42:37.835014 139707012998336 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 2.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 2 (torch.Size([64, 32, 3, 3])), Block block_0 (torch.Size([64, 288])).
I0315 17:42:37.835180 139707012998336 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 3.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 3 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 17:42:37.835207 140613204722880 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 2.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 2 (torch.Size([64, 32, 3, 3])), Block block_0 (torch.Size([64, 288])).
I0315 17:42:37.835244 139655922324672 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 3.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 3 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 17:42:37.835356 139707012998336 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 4.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 4 (torch.Size([128, 64, 3, 3])), Block block_0 (torch.Size([128, 576])).
I0315 17:42:37.832968 139961872823488 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 0.block_0 ([torch.Size([288, 288])]) for Parameter 0 (torch.Size([32, 1, 3, 3])), Block block_0 (torch.Size([288])).
I0315 17:42:37.835394 140613204722880 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 3.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 3 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 17:42:37.835429 139655922324672 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 4.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 4 (torch.Size([128, 64, 3, 3])), Block block_0 (torch.Size([128, 576])).
I0315 17:42:37.835534 139707012998336 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 5.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 5 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 17:42:37.835611 139655922324672 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 5.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 5 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 17:42:37.835642 140613204722880 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 4.block_0 ([torch.Size([128, 128]), torch.Size([576, 576])]) for Parameter 4 (torch.Size([128, 64, 3, 3])), Block block_0 (torch.Size([128, 576])).
I0315 17:42:37.835629 139961872823488 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 1.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 1 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 17:42:37.835706 139707012998336 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 6.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 6 (torch.Size([256, 128, 3, 3])), Block block_0 (torch.Size([256, 384, 3])).
I0315 17:42:37.835783 139655922324672 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 6.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 6 (torch.Size([256, 128, 3, 3])), Block block_0 (torch.Size([256, 384, 3])).
I0315 17:42:37.835839 139961872823488 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 2.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 2 (torch.Size([64, 32, 3, 3])), Block block_0 (torch.Size([64, 288])).
I0315 17:42:37.835854 140613204722880 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 5.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 5 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 17:42:37.835876 139707012998336 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 7.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 7 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 17:42:37.835952 139655922324672 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 7.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 7 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 17:42:37.836010 140613204722880 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 6.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 6 (torch.Size([256, 128, 3, 3])), Block block_0 (torch.Size([256, 384, 3])).
I0315 17:42:37.836014 139961872823488 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 3.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 3 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 17:42:37.836038 139707012998336 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 8.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 8 (torch.Size([512, 256, 3, 3])), Block block_0 (torch.Size([512, 768, 3])).
I0315 17:42:37.833690 140460036412608 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 0.block_0 ([torch.Size([0])]) for Parameter 0 (torch.Size([32, 1, 3, 3])), Block block_0 (torch.Size([288])).
I0315 17:42:37.836112 139655922324672 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 8.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 8 (torch.Size([512, 256, 3, 3])), Block block_0 (torch.Size([512, 768, 3])).
I0315 17:42:37.836174 139961872823488 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 4.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 4 (torch.Size([128, 64, 3, 3])), Block block_0 (torch.Size([128, 576])).
I0315 17:42:37.836189 140613204722880 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 7.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 7 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 17:42:37.836211 139707012998336 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 9.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 9 (torch.Size([512, 512, 3, 3])), Block block_0 (torch.Size([512, 512, 9])).
I0315 17:42:37.836277 139655922324672 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 9.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 9 (torch.Size([512, 512, 3, 3])), Block block_0 (torch.Size([512, 512, 9])).
I0315 17:42:37.833904 140481403532480 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 0.block_0 ([torch.Size([0])]) for Parameter 0 (torch.Size([32, 1, 3, 3])), Block block_0 (torch.Size([288])).
I0315 17:42:37.836340 140613204722880 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 8.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 8 (torch.Size([512, 256, 3, 3])), Block block_0 (torch.Size([512, 768, 3])).
I0315 17:42:37.836323 140460036412608 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 1.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 1 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 17:42:37.836365 139707012998336 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 10.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 10 (torch.Size([256, 512, 3, 3])), Block block_0 (torch.Size([256, 512, 9])).
I0315 17:42:37.836484 140460036412608 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 2.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 2 (torch.Size([64, 32, 3, 3])), Block block_0 (torch.Size([64, 288])).
I0315 17:42:37.836493 140613204722880 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 9.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 9 (torch.Size([512, 512, 3, 3])), Block block_0 (torch.Size([512, 512, 9])).
I0315 17:42:37.836526 140481403532480 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 1.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 1 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 17:42:37.836631 140613204722880 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 10.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 10 (torch.Size([256, 512, 3, 3])), Block block_0 (torch.Size([256, 512, 9])).
I0315 17:42:37.836627 140460036412608 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 3.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 3 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 17:42:37.836668 139707012998336 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 11.block_0 ([torch.Size([256, 256]), torch.Size([768, 768]), torch.Size([3, 3])]) for Parameter 11 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 17:42:37.836733 140481403532480 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 2.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 2 (torch.Size([64, 32, 3, 3])), Block block_0 (torch.Size([64, 288])).
I0315 17:42:37.836773 140460036412608 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 4.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 4 (torch.Size([128, 64, 3, 3])), Block block_0 (torch.Size([128, 576])).
I0315 17:42:37.836781 140613204722880 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 11.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 11 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 17:42:37.836841 139707012998336 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 12.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 12 (torch.Size([128, 256, 3, 3])), Block block_0 (torch.Size([128, 768, 3])).
I0315 17:42:37.836916 140460036412608 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 5.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 5 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 17:42:37.836904 140481403532480 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 3.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 3 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 17:42:37.837003 139707012998336 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 13.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 13 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 17:42:37.837006 139961872823488 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 5.block_0 ([torch.Size([128, 128]), torch.Size([384, 384]), torch.Size([3, 3])]) for Parameter 5 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 17:42:37.837049 140481403532480 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 4.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 4 (torch.Size([128, 64, 3, 3])), Block block_0 (torch.Size([128, 576])).
I0315 17:42:37.837071 140460036412608 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 6.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 6 (torch.Size([256, 128, 3, 3])), Block block_0 (torch.Size([256, 384, 3])).
I0315 17:42:37.837140 139655922324672 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 10.block_0 ([torch.Size([256, 256]), torch.Size([512, 512]), torch.Size([9, 9])]) for Parameter 10 (torch.Size([256, 512, 3, 3])), Block block_0 (torch.Size([256, 512, 9])).
I0315 17:42:37.837163 139707012998336 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 14.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 14 (torch.Size([64, 128, 3, 3])), Block block_0 (torch.Size([64, 384, 3])).
I0315 17:42:37.837218 140481403532480 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 5.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 5 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 17:42:37.837225 140460036412608 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 7.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 7 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 17:42:37.837290 139707012998336 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 15.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 15 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 17:42:37.837332 139655922324672 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 11.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 11 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 17:42:37.837370 140460036412608 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 8.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 8 (torch.Size([512, 256, 3, 3])), Block block_0 (torch.Size([512, 768, 3])).
I0315 17:42:37.837372 140481403532480 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 6.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 6 (torch.Size([256, 128, 3, 3])), Block block_0 (torch.Size([256, 384, 3])).
I0315 17:42:37.837456 140613204722880 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 12.block_0 ([torch.Size([128, 128]), torch.Size([768, 768]), torch.Size([3, 3])]) for Parameter 12 (torch.Size([128, 256, 3, 3])), Block block_0 (torch.Size([128, 768, 3])).
I0315 17:42:37.837494 139655922324672 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 12.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 12 (torch.Size([128, 256, 3, 3])), Block block_0 (torch.Size([128, 768, 3])).
I0315 17:42:37.837499 139707012998336 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 16.block_0 ([torch.Size([32, 32]), torch.Size([576, 576])]) for Parameter 16 (torch.Size([32, 64, 3, 3])), Block block_0 (torch.Size([32, 576])).
I0315 17:42:37.837526 140481403532480 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 7.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 7 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 17:42:37.837653 139655922324672 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 13.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 13 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 17:42:37.837668 139961872823488 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 6.block_0 ([torch.Size([256, 256]), torch.Size([384, 384]), torch.Size([3, 3])]) for Parameter 6 (torch.Size([256, 128, 3, 3])), Block block_0 (torch.Size([256, 384, 3])).
I0315 17:42:37.837694 139707012998336 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 17.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 17 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 17:42:37.837804 139707012998336 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 18.block_0 ([torch.Size([0])]) for Parameter 18 (torch.Size([1, 32, 1, 1])), Block block_0 (torch.Size([32])).
I0315 17:42:37.837819 139655922324672 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 14.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 14 (torch.Size([64, 128, 3, 3])), Block block_0 (torch.Size([64, 384, 3])).
I0315 17:42:37.837880 139961872823488 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 7.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 7 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 17:42:37.837910 139707012998336 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 19.block_0 ([torch.Size([0])]) for Parameter 19 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0315 17:42:37.837941 139655922324672 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 15.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 15 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 17:42:37.838045 139707012998336 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 20.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 20 (torch.Size([512, 256, 2, 2])), Block block_0 (torch.Size([512, 1024])).
I0315 17:42:37.838047 139961872823488 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 8.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 8 (torch.Size([512, 256, 3, 3])), Block block_0 (torch.Size([512, 768, 3])).
I0315 17:42:37.838073 139655922324672 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 16.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 16 (torch.Size([32, 64, 3, 3])), Block block_0 (torch.Size([32, 576])).
I0315 17:42:37.838203 139961872823488 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 9.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 9 (torch.Size([512, 512, 3, 3])), Block block_0 (torch.Size([512, 512, 9])).
I0315 17:42:37.838206 139655922324672 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 17.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 17 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 17:42:37.838207 139707012998336 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 21.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 21 (torch.Size([256, 128, 2, 2])), Block block_0 (torch.Size([256, 512])).
I0315 17:42:37.838304 139655922324672 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 18.block_0 ([torch.Size([0])]) for Parameter 18 (torch.Size([1, 32, 1, 1])), Block block_0 (torch.Size([32])).
I0315 17:42:37.838349 139707012998336 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 22.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 22 (torch.Size([128, 64, 2, 2])), Block block_0 (torch.Size([128, 256])).
I0315 17:42:37.838379 139961872823488 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 10.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 10 (torch.Size([256, 512, 3, 3])), Block block_0 (torch.Size([256, 512, 9])).
I0315 17:42:37.838364 140613204722880 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 13.block_0 ([torch.Size([128, 128]), torch.Size([384, 384]), torch.Size([3, 3])]) for Parameter 13 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 17:42:37.838423 139655922324672 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 19.block_0 ([torch.Size([0])]) for Parameter 19 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0315 17:42:37.838478 139707012998336 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 23.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 23 (torch.Size([64, 32, 2, 2])), Block block_0 (torch.Size([64, 128])).
I0315 17:42:37.838557 139961872823488 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 11.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 11 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 17:42:37.838568 139655922324672 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 20.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 20 (torch.Size([512, 256, 2, 2])), Block block_0 (torch.Size([512, 1024])).
I0315 17:42:37.838610 139707012998336 shampoo_preconditioner_list.py:612] Rank 4: ShampooPreconditionerList Numel Breakdown: (165888, 2097314, 174080, 671744, 696320, 327698, 426002, 1310738, 1703954, 1048738, 655522, 1310738, 1212434, 327698, 303122, 671744, 665600, 2097314, 2048, 2, 2621440, 655360, 163840, 40960)
I0315 17:42:37.838601 140481403532480 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 8.block_0 ([torch.Size([512, 512]), torch.Size([768, 768]), torch.Size([3, 3])]) for Parameter 8 (torch.Size([512, 256, 3, 3])), Block block_0 (torch.Size([512, 768, 3])).
I0315 17:42:37.838661 139707012998336 shampoo_preconditioner_list.py:615] Rank 4: ShampooPreconditionerList Bytes Breakdown: (663552, 8389256)
I0315 17:42:37.838677 140613204722880 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 14.block_0 ([torch.Size([64, 64]), torch.Size([384, 384]), torch.Size([3, 3])]) for Parameter 14 (torch.Size([64, 128, 3, 3])), Block block_0 (torch.Size([64, 384, 3])).
I0315 17:42:37.838703 139707012998336 shampoo_preconditioner_list.py:618] Rank 4: ShampooPreconditionerList Total Elements: 19350298
I0315 17:42:37.838716 139655922324672 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 21.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 21 (torch.Size([256, 128, 2, 2])), Block block_0 (torch.Size([256, 512])).
I0315 17:42:37.838724 139961872823488 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 12.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 12 (torch.Size([128, 256, 3, 3])), Block block_0 (torch.Size([128, 768, 3])).
I0315 17:42:37.838736 139707012998336 shampoo_preconditioner_list.py:621] Rank 4: ShampooPreconditionerList Total Bytes: 9052808
I0315 17:42:37.838748 140460036412608 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 9.block_0 ([torch.Size([512, 512]), torch.Size([512, 512]), torch.Size([9, 9])]) for Parameter 9 (torch.Size([512, 512, 3, 3])), Block block_0 (torch.Size([512, 512, 9])).
I0315 17:42:37.838798 140481403532480 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 9.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 9 (torch.Size([512, 512, 3, 3])), Block block_0 (torch.Size([512, 512, 9])).
I0315 17:42:37.838838 139655922324672 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 22.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 22 (torch.Size([128, 64, 2, 2])), Block block_0 (torch.Size([128, 256])).
I0315 17:42:37.838865 140613204722880 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 15.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 15 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 17:42:37.838871 139961872823488 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 13.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 13 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 17:42:37.838924 139707012998336 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([32, 1, 3, 3])), Block block_0 (torch.Size([288])).
I0315 17:42:37.838950 140460036412608 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 10.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 10 (torch.Size([256, 512, 3, 3])), Block block_0 (torch.Size([256, 512, 9])).
I0315 17:42:37.838959 139655922324672 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 23.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 23 (torch.Size([64, 32, 2, 2])), Block block_0 (torch.Size([64, 128])).
I0315 17:42:37.838962 140481403532480 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 10.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 10 (torch.Size([256, 512, 3, 3])), Block block_0 (torch.Size([256, 512, 9])).
I0315 17:42:37.839007 140613204722880 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 16.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 16 (torch.Size([32, 64, 3, 3])), Block block_0 (torch.Size([32, 576])).
I0315 17:42:37.839012 139707012998336 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 17:42:37.839031 139961872823488 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 14.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 14 (torch.Size([64, 128, 3, 3])), Block block_0 (torch.Size([64, 384, 3])).
I0315 17:42:37.839064 139655922324672 shampoo_preconditioner_list.py:612] Rank 2: ShampooPreconditionerList Numel Breakdown: (165888, 2097314, 174080, 671744, 696320, 327698, 426002, 1310738, 1703954, 1048738, 655522, 1310738, 1212434, 327698, 303122, 671744, 665600, 2097314, 2048, 2, 2621440, 655360, 163840, 40960)
I0315 17:42:37.839073 139707012998336 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([64, 32, 3, 3])), Block block_0 (torch.Size([64, 288])).
I0315 17:42:37.839105 140481403532480 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 11.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 11 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 17:42:37.839113 139655922324672 shampoo_preconditioner_list.py:615] Rank 2: ShampooPreconditionerList Bytes Breakdown: (663552,)
I0315 17:42:37.839119 140460036412608 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 11.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 11 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 17:42:37.839128 139707012998336 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 17:42:37.839148 139655922324672 shampoo_preconditioner_list.py:618] Rank 2: ShampooPreconditionerList Total Elements: 19350298
I0315 17:42:37.839142 140613204722880 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 17.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 17 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 17:42:37.839167 139961872823488 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 15.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 15 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 17:42:37.839185 139655922324672 shampoo_preconditioner_list.py:621] Rank 2: ShampooPreconditionerList Total Bytes: 663552
I0315 17:42:37.839182 139707012998336 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 4.block_0 (torch.Size([0])) for Parameter 4 (torch.Size([128, 64, 3, 3])), Block block_0 (torch.Size([128, 576])).
I0315 17:42:37.839239 139707012998336 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 5.block_0 (torch.Size([0])) for Parameter 5 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 17:42:37.839239 140613204722880 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 18.block_0 ([torch.Size([0])]) for Parameter 18 (torch.Size([1, 32, 1, 1])), Block block_0 (torch.Size([32])).
I0315 17:42:37.839263 140481403532480 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 12.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 12 (torch.Size([128, 256, 3, 3])), Block block_0 (torch.Size([128, 768, 3])).
I0315 17:42:37.839266 140460036412608 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 12.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 12 (torch.Size([128, 256, 3, 3])), Block block_0 (torch.Size([128, 768, 3])).
I0315 17:42:37.839294 139961872823488 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 16.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 16 (torch.Size([32, 64, 3, 3])), Block block_0 (torch.Size([32, 576])).
I0315 17:42:37.839302 139707012998336 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 6.block_0 (torch.Size([0])) for Parameter 6 (torch.Size([256, 128, 3, 3])), Block block_0 (torch.Size([256, 384, 3])).
I0315 17:42:37.839330 140613204722880 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 19.block_0 ([torch.Size([0])]) for Parameter 19 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0315 17:42:37.839363 139707012998336 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 7.block_0 (torch.Size([0])) for Parameter 7 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 17:42:37.839384 139655922324672 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([32, 1, 3, 3])), Block block_0 (torch.Size([288])).
I0315 17:42:37.839410 140460036412608 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 13.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 13 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 17:42:37.839426 139707012998336 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 8.block_0 (torch.Size([0])) for Parameter 8 (torch.Size([512, 256, 3, 3])), Block block_0 (torch.Size([512, 768, 3])).
I0315 17:42:37.839421 140481403532480 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 13.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 13 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 17:42:37.839434 139961872823488 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 17.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 17 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
W0315 17:42:37.839348 139826544436416 distributed_shampoo.py:341] start_preconditioning_step set to -1. Setting start_preconditioning_step equal to precondition frequency 100 by default.
I0315 17:42:37.839457 140613204722880 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 20.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 20 (torch.Size([512, 256, 2, 2])), Block block_0 (torch.Size([512, 1024])).
I0315 17:42:37.839465 139655922324672 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 17:42:37.839494 139707012998336 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 9.block_0 (torch.Size([0])) for Parameter 9 (torch.Size([512, 512, 3, 3])), Block block_0 (torch.Size([512, 512, 9])).
I0315 17:42:37.839524 139655922324672 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([64, 32, 3, 3])), Block block_0 (torch.Size([64, 288])).
I0315 17:42:37.839555 139707012998336 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 10.block_0 (torch.Size([0])) for Parameter 10 (torch.Size([256, 512, 3, 3])), Block block_0 (torch.Size([256, 512, 9])).
I0315 17:42:37.839561 140460036412608 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 14.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 14 (torch.Size([64, 128, 3, 3])), Block block_0 (torch.Size([64, 384, 3])).
I0315 17:42:37.839567 140481403532480 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 14.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 14 (torch.Size([64, 128, 3, 3])), Block block_0 (torch.Size([64, 384, 3])).
I0315 17:42:37.839586 139655922324672 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 17:42:37.839588 140613204722880 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 21.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 21 (torch.Size([256, 128, 2, 2])), Block block_0 (torch.Size([256, 512])).
I0315 17:42:37.839605 139961872823488 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 18.block_0 ([torch.Size([32, 32])]) for Parameter 18 (torch.Size([1, 32, 1, 1])), Block block_0 (torch.Size([32])).
I0315 17:42:37.839648 139655922324672 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 4.block_0 (torch.Size([0])) for Parameter 4 (torch.Size([128, 64, 3, 3])), Block block_0 (torch.Size([128, 576])).
I0315 17:42:37.839667 139707012998336 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 11.block_0 (torch.Size([256, 768, 3])) for Parameter 11 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 17:42:37.839708 140460036412608 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 15.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 15 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 17:42:37.839720 140481403532480 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 15.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 15 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 17:42:37.839718 140613204722880 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 22.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 22 (torch.Size([128, 64, 2, 2])), Block block_0 (torch.Size([128, 256])).
I0315 17:42:37.839730 139655922324672 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 5.block_0 (torch.Size([0])) for Parameter 5 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 17:42:37.839758 139707012998336 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 12.block_0 (torch.Size([0])) for Parameter 12 (torch.Size([128, 256, 3, 3])), Block block_0 (torch.Size([128, 768, 3])).
I0315 17:42:37.839784 139655922324672 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 6.block_0 (torch.Size([0])) for Parameter 6 (torch.Size([256, 128, 3, 3])), Block block_0 (torch.Size([256, 384, 3])).
I0315 17:42:37.839801 139961872823488 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 19.block_0 ([torch.Size([1, 1])]) for Parameter 19 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0315 17:42:37.839820 139707012998336 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 13.block_0 (torch.Size([0])) for Parameter 13 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 17:42:37.839836 140460036412608 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 16.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 16 (torch.Size([32, 64, 3, 3])), Block block_0 (torch.Size([32, 576])).
I0315 17:42:37.839841 139655922324672 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 7.block_0 (torch.Size([0])) for Parameter 7 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 17:42:37.839842 140481403532480 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 16.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 16 (torch.Size([32, 64, 3, 3])), Block block_0 (torch.Size([32, 576])).
I0315 17:42:37.839872 139707012998336 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 14.block_0 (torch.Size([0])) for Parameter 14 (torch.Size([64, 128, 3, 3])), Block block_0 (torch.Size([64, 384, 3])).
I0315 17:42:37.839893 139655922324672 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 8.block_0 (torch.Size([0])) for Parameter 8 (torch.Size([512, 256, 3, 3])), Block block_0 (torch.Size([512, 768, 3])).
I0315 17:42:37.839929 139707012998336 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 15.block_0 (torch.Size([0])) for Parameter 15 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 17:42:37.839935 140613204722880 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 23.block_0 ([torch.Size([64, 64]), torch.Size([128, 128])]) for Parameter 23 (torch.Size([64, 32, 2, 2])), Block block_0 (torch.Size([64, 128])).
I0315 17:42:37.839944 139961872823488 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 20.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 20 (torch.Size([512, 256, 2, 2])), Block block_0 (torch.Size([512, 1024])).
I0315 17:42:37.839951 139655922324672 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 9.block_0 (torch.Size([0])) for Parameter 9 (torch.Size([512, 512, 3, 3])), Block block_0 (torch.Size([512, 512, 9])).
I0315 17:42:37.839957 140481403532480 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 17.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 17 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 17:42:37.839970 140460036412608 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 17.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 17 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 17:42:37.840028 139707012998336 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 16.block_0 (torch.Size([32, 576])) for Parameter 16 (torch.Size([32, 64, 3, 3])), Block block_0 (torch.Size([32, 576])).
I0315 17:42:37.840057 140481403532480 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 18.block_0 ([torch.Size([0])]) for Parameter 18 (torch.Size([1, 32, 1, 1])), Block block_0 (torch.Size([32])).
I0315 17:42:37.840054 139655922324672 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 10.block_0 (torch.Size([256, 512, 9])) for Parameter 10 (torch.Size([256, 512, 3, 3])), Block block_0 (torch.Size([256, 512, 9])).
I0315 17:42:37.840060 140613204722880 shampoo_preconditioner_list.py:612] Rank 7: ShampooPreconditionerList Numel Breakdown: (165888, 2097314, 174080, 671744, 696320, 327698, 426002, 1310738, 1703954, 1048738, 655522, 1310738, 1212434, 327698, 303122, 671744, 665600, 2097314, 2048, 2, 2621440, 655360, 163840, 40960)
I0315 17:42:37.840061 140460036412608 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 18.block_0 ([torch.Size([0])]) for Parameter 18 (torch.Size([1, 32, 1, 1])), Block block_0 (torch.Size([32])).
I0315 17:42:37.840102 140613204722880 shampoo_preconditioner_list.py:615] Rank 7: ShampooPreconditionerList Bytes Breakdown: (663552, 8389256, 696320, 2686976, 2785280, 1310792)
I0315 17:42:37.840104 139707012998336 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 17.block_0 (torch.Size([0])) for Parameter 17 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 17:42:37.840123 139655922324672 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 11.block_0 (torch.Size([0])) for Parameter 11 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 17:42:37.840147 140613204722880 shampoo_preconditioner_list.py:618] Rank 7: ShampooPreconditionerList Total Elements: 19350298
I0315 17:42:37.840149 140460036412608 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 19.block_0 ([torch.Size([0])]) for Parameter 19 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0315 17:42:37.840152 140481403532480 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 19.block_0 ([torch.Size([0])]) for Parameter 19 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0315 17:42:37.840158 139707012998336 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 18.block_0 (torch.Size([0])) for Parameter 18 (torch.Size([1, 32, 1, 1])), Block block_0 (torch.Size([32])).
I0315 17:42:37.840183 140613204722880 shampoo_preconditioner_list.py:621] Rank 7: ShampooPreconditionerList Total Bytes: 16532176
I0315 17:42:37.840187 139655922324672 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 12.block_0 (torch.Size([0])) for Parameter 12 (torch.Size([128, 256, 3, 3])), Block block_0 (torch.Size([128, 768, 3])).
I0315 17:42:37.840208 139707012998336 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 19.block_0 (torch.Size([0])) for Parameter 19 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0315 17:42:37.840238 139655922324672 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 13.block_0 (torch.Size([0])) for Parameter 13 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 17:42:37.840265 140481403532480 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 20.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 20 (torch.Size([512, 256, 2, 2])), Block block_0 (torch.Size([512, 1024])).
I0315 17:42:37.840267 139707012998336 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 20.block_0 (torch.Size([0])) for Parameter 20 (torch.Size([512, 256, 2, 2])), Block block_0 (torch.Size([512, 1024])).
I0315 17:42:37.840268 140460036412608 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 20.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 20 (torch.Size([512, 256, 2, 2])), Block block_0 (torch.Size([512, 1024])).
I0315 17:42:37.840296 139655922324672 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 14.block_0 (torch.Size([0])) for Parameter 14 (torch.Size([64, 128, 3, 3])), Block block_0 (torch.Size([64, 384, 3])).
I0315 17:42:37.840320 139707012998336 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 21.block_0 (torch.Size([0])) for Parameter 21 (torch.Size([256, 128, 2, 2])), Block block_0 (torch.Size([256, 512])).
I0315 17:42:37.840353 139655922324672 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 15.block_0 (torch.Size([0])) for Parameter 15 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 17:42:37.840378 139707012998336 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 22.block_0 (torch.Size([0])) for Parameter 22 (torch.Size([128, 64, 2, 2])), Block block_0 (torch.Size([128, 256])).
I0315 17:42:37.840375 140613204722880 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([32, 1, 3, 3])), Block block_0 (torch.Size([288])).
I0315 17:42:37.840397 140460036412608 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 21.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 21 (torch.Size([256, 128, 2, 2])), Block block_0 (torch.Size([256, 512])).
I0315 17:42:37.840404 140481403532480 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 21.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 21 (torch.Size([256, 128, 2, 2])), Block block_0 (torch.Size([256, 512])).
I0315 17:42:37.840410 139655922324672 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 16.block_0 (torch.Size([0])) for Parameter 16 (torch.Size([32, 64, 3, 3])), Block block_0 (torch.Size([32, 576])).
I0315 17:42:37.840430 139707012998336 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 23.block_0 (torch.Size([0])) for Parameter 23 (torch.Size([64, 32, 2, 2])), Block block_0 (torch.Size([64, 128])).
I0315 17:42:37.840461 139655922324672 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 17.block_0 (torch.Size([0])) for Parameter 17 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 17:42:37.840506 139707012998336 shampoo_preconditioner_list.py:375] Rank 4: AdaGradPreconditionerList Numel Breakdown: (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 589824, 0, 0, 0, 0, 18432, 0, 0, 0, 0, 0, 0, 0)
I0315 17:42:37.840501 140613204722880 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([1024, 9])) for Parameter 1 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 17:42:37.840514 140460036412608 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 22.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 22 (torch.Size([128, 64, 2, 2])), Block block_0 (torch.Size([128, 256])).
I0315 17:42:37.840518 139655922324672 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 18.block_0 (torch.Size([0])) for Parameter 18 (torch.Size([1, 32, 1, 1])), Block block_0 (torch.Size([32])).
I0315 17:42:37.840519 140481403532480 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 22.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 22 (torch.Size([128, 64, 2, 2])), Block block_0 (torch.Size([128, 256])).
I0315 17:42:37.840522 139961872823488 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 21.block_0 ([torch.Size([256, 256]), torch.Size([512, 512])]) for Parameter 21 (torch.Size([256, 128, 2, 2])), Block block_0 (torch.Size([256, 512])).
I0315 17:42:37.840548 139707012998336 shampoo_preconditioner_list.py:378] Rank 4: AdaGradPreconditionerList Bytes Breakdown: (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2359296, 0, 0, 0, 0, 73728, 0, 0, 0, 0, 0, 0, 0)
I0315 17:42:37.840567 139655922324672 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 19.block_0 (torch.Size([0])) for Parameter 19 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0315 17:42:37.840574 140613204722880 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([64, 32, 3, 3])), Block block_0 (torch.Size([64, 288])).
I0315 17:42:37.840580 139707012998336 shampoo_preconditioner_list.py:381] Rank 4: AdaGradPreconditionerList Total Elements: 608256
I0315 17:42:37.840610 139707012998336 shampoo_preconditioner_list.py:384] Rank 4: AdaGradPreconditionerList Total Bytes: 2433024
I0315 17:42:37.840618 139655922324672 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 20.block_0 (torch.Size([0])) for Parameter 20 (torch.Size([512, 256, 2, 2])), Block block_0 (torch.Size([512, 1024])).
I0315 17:42:37.840626 140481403532480 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 23.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 23 (torch.Size([64, 32, 2, 2])), Block block_0 (torch.Size([64, 128])).
I0315 17:42:37.840631 140460036412608 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 23.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 23 (torch.Size([64, 32, 2, 2])), Block block_0 (torch.Size([64, 128])).
I0315 17:42:37.840640 140613204722880 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 17:42:37.840667 139655922324672 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 21.block_0 (torch.Size([0])) for Parameter 21 (torch.Size([256, 128, 2, 2])), Block block_0 (torch.Size([256, 512])).
I0315 17:42:37.840732 139655922324672 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 22.block_0 (torch.Size([0])) for Parameter 22 (torch.Size([128, 64, 2, 2])), Block block_0 (torch.Size([128, 256])).
I0315 17:42:37.840735 140481403532480 shampoo_preconditioner_list.py:612] Rank 1: ShampooPreconditionerList Numel Breakdown: (165888, 2097314, 174080, 671744, 696320, 327698, 426002, 1310738, 1703954, 1048738, 655522, 1310738, 1212434, 327698, 303122, 671744, 665600, 2097314, 2048, 2, 2621440, 655360, 163840, 40960)
I0315 17:42:37.840744 140460036412608 shampoo_preconditioner_list.py:612] Rank 0: ShampooPreconditionerList Numel Breakdown: (165888, 2097314, 174080, 671744, 696320, 327698, 426002, 1310738, 1703954, 1048738, 655522, 1310738, 1212434, 327698, 303122, 671744, 665600, 2097314, 2048, 2, 2621440, 655360, 163840, 40960)
I0315 17:42:37.840778 140481403532480 shampoo_preconditioner_list.py:615] Rank 1: ShampooPreconditionerList Bytes Breakdown: (663552,)
I0315 17:42:37.840785 140460036412608 shampoo_preconditioner_list.py:615] Rank 0: ShampooPreconditionerList Bytes Breakdown: (663552,)
I0315 17:42:37.840770 139961872823488 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 22.block_0 ([torch.Size([128, 128]), torch.Size([256, 256])]) for Parameter 22 (torch.Size([128, 64, 2, 2])), Block block_0 (torch.Size([128, 256])).
I0315 17:42:37.840790 139655922324672 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 23.block_0 (torch.Size([0])) for Parameter 23 (torch.Size([64, 32, 2, 2])), Block block_0 (torch.Size([64, 128])).
I0315 17:42:37.840818 140460036412608 shampoo_preconditioner_list.py:618] Rank 0: ShampooPreconditionerList Total Elements: 19350298
I0315 17:42:37.840821 140481403532480 shampoo_preconditioner_list.py:618] Rank 1: ShampooPreconditionerList Total Elements: 19350298
I0315 17:42:37.840852 140481403532480 shampoo_preconditioner_list.py:621] Rank 1: ShampooPreconditionerList Total Bytes: 663552
I0315 17:42:37.840857 140460036412608 shampoo_preconditioner_list.py:621] Rank 0: ShampooPreconditionerList Total Bytes: 663552
I0315 17:42:37.840858 139655922324672 shampoo_preconditioner_list.py:375] Rank 2: AdaGradPreconditionerList Numel Breakdown: (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1179648, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)
I0315 17:42:37.840893 139655922324672 shampoo_preconditioner_list.py:378] Rank 2: AdaGradPreconditionerList Bytes Breakdown: (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4718592, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)
I0315 17:42:37.840911 139961872823488 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 23.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 23 (torch.Size([64, 32, 2, 2])), Block block_0 (torch.Size([64, 128])).
I0315 17:42:37.840930 139655922324672 shampoo_preconditioner_list.py:381] Rank 2: AdaGradPreconditionerList Total Elements: 1179648
I0315 17:42:37.840960 139655922324672 shampoo_preconditioner_list.py:384] Rank 2: AdaGradPreconditionerList Total Bytes: 4718592
I0315 17:42:37.841048 139961872823488 shampoo_preconditioner_list.py:612] Rank 6: ShampooPreconditionerList Numel Breakdown: (165888, 2097314, 174080, 671744, 696320, 327698, 426002, 1310738, 1703954, 1048738, 655522, 1310738, 1212434, 327698, 303122, 671744, 665600, 2097314, 2048, 2, 2621440, 655360, 163840, 40960)
I0315 17:42:37.841049 140460036412608 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([32, 1, 3, 3])), Block block_0 (torch.Size([288])).
I0315 17:42:37.841046 140481403532480 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([32, 1, 3, 3])), Block block_0 (torch.Size([288])).
I0315 17:42:37.841100 139961872823488 shampoo_preconditioner_list.py:615] Rank 6: ShampooPreconditionerList Bytes Breakdown: (663552, 8389256, 696320, 2686976, 2785280, 1310792, 1704008)
I0315 17:42:37.841117 140460036412608 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 17:42:37.841118 140481403532480 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 17:42:37.841141 139961872823488 shampoo_preconditioner_list.py:618] Rank 6: ShampooPreconditionerList Total Elements: 19350298
I0315 17:42:37.841174 140481403532480 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([64, 32, 3, 3])), Block block_0 (torch.Size([64, 288])).
I0315 17:42:37.841178 139961872823488 shampoo_preconditioner_list.py:621] Rank 6: ShampooPreconditionerList Total Bytes: 18236184
I0315 17:42:37.841183 140460036412608 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([64, 32, 3, 3])), Block block_0 (torch.Size([64, 288])).
I0315 17:42:37.841197 140613204722880 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 4.block_0 (torch.Size([128, 576])) for Parameter 4 (torch.Size([128, 64, 3, 3])), Block block_0 (torch.Size([128, 576])).
I0315 17:42:37.841235 140481403532480 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 17:42:37.841236 140460036412608 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 17:42:37.841286 140481403532480 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 4.block_0 (torch.Size([0])) for Parameter 4 (torch.Size([128, 64, 3, 3])), Block block_0 (torch.Size([128, 576])).
I0315 17:42:37.841287 140460036412608 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 4.block_0 (torch.Size([0])) for Parameter 4 (torch.Size([128, 64, 3, 3])), Block block_0 (torch.Size([128, 576])).
I0315 17:42:37.841309 140613204722880 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 5.block_0 (torch.Size([0])) for Parameter 5 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 17:42:37.841339 140460036412608 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 5.block_0 (torch.Size([0])) for Parameter 5 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 17:42:37.841345 140481403532480 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 5.block_0 (torch.Size([0])) for Parameter 5 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 17:42:37.841375 140613204722880 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 6.block_0 (torch.Size([0])) for Parameter 6 (torch.Size([256, 128, 3, 3])), Block block_0 (torch.Size([256, 384, 3])).
I0315 17:42:37.841396 140481403532480 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 6.block_0 (torch.Size([0])) for Parameter 6 (torch.Size([256, 128, 3, 3])), Block block_0 (torch.Size([256, 384, 3])).
I0315 17:42:37.841400 140460036412608 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 6.block_0 (torch.Size([0])) for Parameter 6 (torch.Size([256, 128, 3, 3])), Block block_0 (torch.Size([256, 384, 3])).
I0315 17:42:37.841409 139961872823488 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([288])) for Parameter 0 (torch.Size([32, 1, 3, 3])), Block block_0 (torch.Size([288])).
I0315 17:42:37.841443 140613204722880 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 7.block_0 (torch.Size([0])) for Parameter 7 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 17:42:37.841452 140481403532480 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 7.block_0 (torch.Size([0])) for Parameter 7 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 17:42:37.841459 140460036412608 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 7.block_0 (torch.Size([0])) for Parameter 7 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 17:42:37.841498 140613204722880 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 8.block_0 (torch.Size([0])) for Parameter 8 (torch.Size([512, 256, 3, 3])), Block block_0 (torch.Size([512, 768, 3])).
I0315 17:42:37.841506 139961872823488 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 17:42:37.841519 140460036412608 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 8.block_0 (torch.Size([0])) for Parameter 8 (torch.Size([512, 256, 3, 3])), Block block_0 (torch.Size([512, 768, 3])).
I0315 17:42:37.841557 140613204722880 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 9.block_0 (torch.Size([0])) for Parameter 9 (torch.Size([512, 512, 3, 3])), Block block_0 (torch.Size([512, 512, 9])).
I0315 17:42:37.841557 140481403532480 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 8.block_0 (torch.Size([512, 768, 3])) for Parameter 8 (torch.Size([512, 256, 3, 3])), Block block_0 (torch.Size([512, 768, 3])).
I0315 17:42:37.841565 139961872823488 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([64, 32, 3, 3])), Block block_0 (torch.Size([64, 288])).
I0315 17:42:37.841631 140613204722880 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 10.block_0 (torch.Size([0])) for Parameter 10 (torch.Size([256, 512, 3, 3])), Block block_0 (torch.Size([256, 512, 9])).
I0315 17:42:37.841638 139961872823488 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 17:42:37.841647 140481403532480 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 9.block_0 (torch.Size([0])) for Parameter 9 (torch.Size([512, 512, 3, 3])), Block block_0 (torch.Size([512, 512, 9])).
I0315 17:42:37.841642 140460036412608 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 9.block_0 (torch.Size([512, 512, 9])) for Parameter 9 (torch.Size([512, 512, 3, 3])), Block block_0 (torch.Size([512, 512, 9])).
I0315 17:42:37.841695 140613204722880 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 11.block_0 (torch.Size([0])) for Parameter 11 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 17:42:37.841701 139961872823488 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 4.block_0 (torch.Size([0])) for Parameter 4 (torch.Size([128, 64, 3, 3])), Block block_0 (torch.Size([128, 576])).
I0315 17:42:37.841700 140481403532480 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 10.block_0 (torch.Size([0])) for Parameter 10 (torch.Size([256, 512, 3, 3])), Block block_0 (torch.Size([256, 512, 9])).
I0315 17:42:37.841727 140460036412608 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 10.block_0 (torch.Size([0])) for Parameter 10 (torch.Size([256, 512, 3, 3])), Block block_0 (torch.Size([256, 512, 9])).
I0315 17:42:37.841762 140481403532480 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 11.block_0 (torch.Size([0])) for Parameter 11 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 17:42:37.841793 140460036412608 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 11.block_0 (torch.Size([0])) for Parameter 11 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 17:42:37.841801 140613204722880 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 12.block_0 (torch.Size([128, 768, 3])) for Parameter 12 (torch.Size([128, 256, 3, 3])), Block block_0 (torch.Size([128, 768, 3])).
I0315 17:42:37.841821 140481403532480 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 12.block_0 (torch.Size([0])) for Parameter 12 (torch.Size([128, 256, 3, 3])), Block block_0 (torch.Size([128, 768, 3])).
I0315 17:42:37.841853 140460036412608 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 12.block_0 (torch.Size([0])) for Parameter 12 (torch.Size([128, 256, 3, 3])), Block block_0 (torch.Size([128, 768, 3])).
I0315 17:42:37.841877 140481403532480 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 13.block_0 (torch.Size([0])) for Parameter 13 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 17:42:37.841885 140613204722880 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 13.block_0 (torch.Size([128, 384, 3])) for Parameter 13 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 17:42:37.841911 140460036412608 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 13.block_0 (torch.Size([0])) for Parameter 13 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 17:42:37.841935 140481403532480 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 14.block_0 (torch.Size([0])) for Parameter 14 (torch.Size([64, 128, 3, 3])), Block block_0 (torch.Size([64, 384, 3])).
I0315 17:42:37.841960 140613204722880 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 14.block_0 (torch.Size([64, 384, 3])) for Parameter 14 (torch.Size([64, 128, 3, 3])), Block block_0 (torch.Size([64, 384, 3])).
I0315 17:42:37.841969 140460036412608 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 14.block_0 (torch.Size([0])) for Parameter 14 (torch.Size([64, 128, 3, 3])), Block block_0 (torch.Size([64, 384, 3])).
I0315 17:42:37.841992 140481403532480 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 15.block_0 (torch.Size([0])) for Parameter 15 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 17:42:37.842020 140460036412608 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 15.block_0 (torch.Size([0])) for Parameter 15 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 17:42:37.842033 140613204722880 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 15.block_0 (torch.Size([0])) for Parameter 15 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 17:42:37.842048 140481403532480 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 16.block_0 (torch.Size([0])) for Parameter 16 (torch.Size([32, 64, 3, 3])), Block block_0 (torch.Size([32, 576])).
I0315 17:42:37.842076 140460036412608 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 16.block_0 (torch.Size([0])) for Parameter 16 (torch.Size([32, 64, 3, 3])), Block block_0 (torch.Size([32, 576])).
I0315 17:42:37.842086 140613204722880 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 16.block_0 (torch.Size([0])) for Parameter 16 (torch.Size([32, 64, 3, 3])), Block block_0 (torch.Size([32, 576])).
I0315 17:42:37.842103 140481403532480 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 17.block_0 (torch.Size([0])) for Parameter 17 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 17:42:37.842133 140460036412608 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 17.block_0 (torch.Size([0])) for Parameter 17 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 17:42:37.842136 140613204722880 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 17.block_0 (torch.Size([0])) for Parameter 17 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 17:42:37.842153 140481403532480 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 18.block_0 (torch.Size([0])) for Parameter 18 (torch.Size([1, 32, 1, 1])), Block block_0 (torch.Size([32])).
I0315 17:42:37.842149 139961872823488 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 5.block_0 (torch.Size([128, 384, 3])) for Parameter 5 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 17:42:37.842187 140613204722880 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 18.block_0 (torch.Size([0])) for Parameter 18 (torch.Size([1, 32, 1, 1])), Block block_0 (torch.Size([32])).
I0315 17:42:37.842191 140460036412608 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 18.block_0 (torch.Size([0])) for Parameter 18 (torch.Size([1, 32, 1, 1])), Block block_0 (torch.Size([32])).
I0315 17:42:37.842207 140481403532480 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 19.block_0 (torch.Size([0])) for Parameter 19 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0315 17:42:37.842236 140613204722880 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 19.block_0 (torch.Size([0])) for Parameter 19 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0315 17:42:37.842242 140460036412608 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 19.block_0 (torch.Size([0])) for Parameter 19 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0315 17:42:37.842265 140481403532480 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 20.block_0 (torch.Size([0])) for Parameter 20 (torch.Size([512, 256, 2, 2])), Block block_0 (torch.Size([512, 1024])).
I0315 17:42:37.842291 140460036412608 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 20.block_0 (torch.Size([0])) for Parameter 20 (torch.Size([512, 256, 2, 2])), Block block_0 (torch.Size([512, 1024])).
I0315 17:42:37.842294 140613204722880 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 20.block_0 (torch.Size([0])) for Parameter 20 (torch.Size([512, 256, 2, 2])), Block block_0 (torch.Size([512, 1024])).
I0315 17:42:37.842288 139961872823488 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 6.block_0 (torch.Size([256, 384, 3])) for Parameter 6 (torch.Size([256, 128, 3, 3])), Block block_0 (torch.Size([256, 384, 3])).
I0315 17:42:37.842320 140481403532480 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 21.block_0 (torch.Size([0])) for Parameter 21 (torch.Size([256, 128, 2, 2])), Block block_0 (torch.Size([256, 512])).
I0315 17:42:37.842343 140613204722880 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 21.block_0 (torch.Size([0])) for Parameter 21 (torch.Size([256, 128, 2, 2])), Block block_0 (torch.Size([256, 512])).
I0315 17:42:37.842347 140460036412608 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 21.block_0 (torch.Size([0])) for Parameter 21 (torch.Size([256, 128, 2, 2])), Block block_0 (torch.Size([256, 512])).
I0315 17:42:37.842370 139961872823488 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 7.block_0 (torch.Size([0])) for Parameter 7 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 17:42:37.842376 140481403532480 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 22.block_0 (torch.Size([0])) for Parameter 22 (torch.Size([128, 64, 2, 2])), Block block_0 (torch.Size([128, 256])).
I0315 17:42:37.842397 140460036412608 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 22.block_0 (torch.Size([0])) for Parameter 22 (torch.Size([128, 64, 2, 2])), Block block_0 (torch.Size([128, 256])).
I0315 17:42:37.842399 140613204722880 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 22.block_0 (torch.Size([0])) for Parameter 22 (torch.Size([128, 64, 2, 2])), Block block_0 (torch.Size([128, 256])).
I0315 17:42:37.842425 140481403532480 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 23.block_0 (torch.Size([0])) for Parameter 23 (torch.Size([64, 32, 2, 2])), Block block_0 (torch.Size([64, 128])).
I0315 17:42:37.842437 139961872823488 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 8.block_0 (torch.Size([0])) for Parameter 8 (torch.Size([512, 256, 3, 3])), Block block_0 (torch.Size([512, 768, 3])).
I0315 17:42:37.842453 140460036412608 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 23.block_0 (torch.Size([0])) for Parameter 23 (torch.Size([64, 32, 2, 2])), Block block_0 (torch.Size([64, 128])).
I0315 17:42:37.842478 140613204722880 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 23.block_0 (torch.Size([64, 128])) for Parameter 23 (torch.Size([64, 32, 2, 2])), Block block_0 (torch.Size([64, 128])).
I0315 17:42:37.842492 139961872823488 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 9.block_0 (torch.Size([0])) for Parameter 9 (torch.Size([512, 512, 3, 3])), Block block_0 (torch.Size([512, 512, 9])).
I0315 17:42:37.842497 140481403532480 shampoo_preconditioner_list.py:375] Rank 1: AdaGradPreconditionerList Numel Breakdown: (0, 0, 0, 0, 0, 0, 0, 0, 1179648, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)
I0315 17:42:37.842527 140460036412608 shampoo_preconditioner_list.py:375] Rank 0: AdaGradPreconditionerList Numel Breakdown: (0, 0, 0, 0, 0, 0, 0, 0, 0, 2359296, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)
I0315 17:42:37.842533 140481403532480 shampoo_preconditioner_list.py:378] Rank 1: AdaGradPreconditionerList Bytes Breakdown: (0, 0, 0, 0, 0, 0, 0, 0, 4718592, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)
I0315 17:42:37.842544 139961872823488 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 10.block_0 (torch.Size([0])) for Parameter 10 (torch.Size([256, 512, 3, 3])), Block block_0 (torch.Size([256, 512, 9])).
I0315 17:42:37.842527 139707012998336 logger_utils.py:262] Unable to record workload.train_mean information. Continuing without it.
I0315 17:42:37.842564 140481403532480 shampoo_preconditioner_list.py:381] Rank 1: AdaGradPreconditionerList Total Elements: 1179648
I0315 17:42:37.842563 140613204722880 shampoo_preconditioner_list.py:375] Rank 7: AdaGradPreconditionerList Numel Breakdown: (0, 9216, 0, 0, 73728, 0, 0, 0, 0, 0, 0, 0, 294912, 147456, 73728, 0, 0, 0, 0, 0, 0, 0, 0, 8192)
I0315 17:42:37.842570 140460036412608 shampoo_preconditioner_list.py:378] Rank 0: AdaGradPreconditionerList Bytes Breakdown: (0, 0, 0, 0, 0, 0, 0, 0, 0, 9437184, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)
I0315 17:42:37.842593 140481403532480 shampoo_preconditioner_list.py:384] Rank 1: AdaGradPreconditionerList Total Bytes: 4718592
I0315 17:42:37.842595 139961872823488 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 11.block_0 (torch.Size([0])) for Parameter 11 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 17:42:37.842600 140613204722880 shampoo_preconditioner_list.py:378] Rank 7: AdaGradPreconditionerList Bytes Breakdown: (0, 36864, 0, 0, 294912, 0, 0, 0, 0, 0, 0, 0, 1179648, 589824, 294912, 0, 0, 0, 0, 0, 0, 0, 0, 32768)
I0315 17:42:37.842602 140460036412608 shampoo_preconditioner_list.py:381] Rank 0: AdaGradPreconditionerList Total Elements: 2359296
I0315 17:42:37.842605 139707012998336 logger_utils.py:262] Unable to record workload.train_stddev information. Continuing without it.
I0315 17:42:37.842630 140613204722880 shampoo_preconditioner_list.py:381] Rank 7: AdaGradPreconditionerList Total Elements: 607232
I0315 17:42:37.842633 140460036412608 shampoo_preconditioner_list.py:384] Rank 0: AdaGradPreconditionerList Total Bytes: 9437184
I0315 17:42:37.842648 139961872823488 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 12.block_0 (torch.Size([0])) for Parameter 12 (torch.Size([128, 256, 3, 3])), Block block_0 (torch.Size([128, 768, 3])).
I0315 17:42:37.842665 140613204722880 shampoo_preconditioner_list.py:384] Rank 7: AdaGradPreconditionerList Total Bytes: 2428928
I0315 17:42:37.842714 139961872823488 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 13.block_0 (torch.Size([0])) for Parameter 13 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 17:42:37.842773 139961872823488 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 14.block_0 (torch.Size([0])) for Parameter 14 (torch.Size([64, 128, 3, 3])), Block block_0 (torch.Size([64, 384, 3])).
I0315 17:42:37.842780 139655922324672 logger_utils.py:262] Unable to record workload.train_mean information. Continuing without it.
I0315 17:42:37.842830 139961872823488 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 15.block_0 (torch.Size([0])) for Parameter 15 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 17:42:37.842855 139655922324672 logger_utils.py:262] Unable to record workload.train_stddev information. Continuing without it.
I0315 17:42:37.842886 139961872823488 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 16.block_0 (torch.Size([0])) for Parameter 16 (torch.Size([32, 64, 3, 3])), Block block_0 (torch.Size([32, 576])).
I0315 17:42:37.842942 139961872823488 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 17.block_0 (torch.Size([0])) for Parameter 17 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 17:42:37.843036 139961872823488 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 18.block_0 (torch.Size([32])) for Parameter 18 (torch.Size([1, 32, 1, 1])), Block block_0 (torch.Size([32])).
I0315 17:42:37.843127 139961872823488 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 19.block_0 (torch.Size([1])) for Parameter 19 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0315 17:42:37.843199 139961872823488 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 20.block_0 (torch.Size([0])) for Parameter 20 (torch.Size([512, 256, 2, 2])), Block block_0 (torch.Size([512, 1024])).
I0315 17:42:37.843286 139961872823488 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 21.block_0 (torch.Size([256, 512])) for Parameter 21 (torch.Size([256, 128, 2, 2])), Block block_0 (torch.Size([256, 512])).
I0315 17:42:37.843372 139961872823488 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 22.block_0 (torch.Size([128, 256])) for Parameter 22 (torch.Size([128, 64, 2, 2])), Block block_0 (torch.Size([128, 256])).
I0315 17:42:37.843440 139961872823488 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 23.block_0 (torch.Size([0])) for Parameter 23 (torch.Size([64, 32, 2, 2])), Block block_0 (torch.Size([64, 128])).
I0315 17:42:37.843518 139961872823488 shampoo_preconditioner_list.py:375] Rank 6: AdaGradPreconditionerList Numel Breakdown: (288, 0, 0, 0, 0, 147456, 294912, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 32, 1, 0, 131072, 32768, 0)
I0315 17:42:37.843561 139961872823488 shampoo_preconditioner_list.py:378] Rank 6: AdaGradPreconditionerList Bytes Breakdown: (1152, 0, 0, 0, 0, 589824, 1179648, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 128, 4, 0, 524288, 131072, 0)
I0315 17:42:37.843602 139961872823488 shampoo_preconditioner_list.py:381] Rank 6: AdaGradPreconditionerList Total Elements: 606529
I0315 17:42:37.843637 139961872823488 shampoo_preconditioner_list.py:384] Rank 6: AdaGradPreconditionerList Total Bytes: 2426116
I0315 17:42:37.843790 140460036412608 submission.py:142] No large parameters detected! Continuing with only Shampoo....
I0315 17:42:37.843939 140481403532480 logger_utils.py:262] Unable to record workload.train_mean information. Continuing without it.
I0315 17:42:37.843994 140460036412608 submission_runner.py:279] Initializing metrics bundle.
I0315 17:42:37.844016 140481403532480 logger_utils.py:262] Unable to record workload.train_stddev information. Continuing without it.
I0315 17:42:37.844130 140460036412608 submission_runner.py:301] Initializing checkpoint and logger.
I0315 17:42:37.844533 140460036412608 submission_runner.py:321] Saving meta data to /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_1/fastmri_pytorch/trial_4/meta_data_0.json.
I0315 17:42:37.844658 140613204722880 logger_utils.py:262] Unable to record workload.train_mean information. Continuing without it.
I0315 17:42:37.842374 139826544436416 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 0.block_0 ([torch.Size([0])]) for Parameter 0 (torch.Size([32, 1, 3, 3])), Block block_0 (torch.Size([288])).
I0315 17:42:37.844709 140460036412608 logger_utils.py:262] Unable to record workload.train_mean information. Continuing without it.
I0315 17:42:37.844744 140613204722880 logger_utils.py:262] Unable to record workload.train_stddev information. Continuing without it.
I0315 17:42:37.844752 140460036412608 logger_utils.py:262] Unable to record workload.train_stddev information. Continuing without it.
I0315 17:42:37.844939 139826544436416 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 1.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 1 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 17:42:37.845164 139826544436416 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 2.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 2 (torch.Size([64, 32, 3, 3])), Block block_0 (torch.Size([64, 288])).
I0315 17:42:37.845519 139826544436416 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 3.block_0 ([torch.Size([64, 64]), torch.Size([576, 576])]) for Parameter 3 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 17:42:37.845632 139961872823488 logger_utils.py:262] Unable to record workload.train_mean information. Continuing without it.
I0315 17:42:37.845737 139961872823488 logger_utils.py:262] Unable to record workload.train_stddev information. Continuing without it.
I0315 17:42:37.845795 139826544436416 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 4.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 4 (torch.Size([128, 64, 3, 3])), Block block_0 (torch.Size([128, 576])).
I0315 17:42:37.846048 139826544436416 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 5.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 5 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 17:42:37.846264 139826544436416 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 6.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 6 (torch.Size([256, 128, 3, 3])), Block block_0 (torch.Size([256, 384, 3])).
I0315 17:42:37.846479 139826544436416 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 7.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 7 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 17:42:37.846699 139826544436416 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 8.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 8 (torch.Size([512, 256, 3, 3])), Block block_0 (torch.Size([512, 768, 3])).
I0315 17:42:37.846917 139826544436416 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 9.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 9 (torch.Size([512, 512, 3, 3])), Block block_0 (torch.Size([512, 512, 9])).
I0315 17:42:37.847119 139826544436416 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 10.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 10 (torch.Size([256, 512, 3, 3])), Block block_0 (torch.Size([256, 512, 9])).
I0315 17:42:37.847316 139826544436416 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 11.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 11 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 17:42:37.847518 139826544436416 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 12.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 12 (torch.Size([128, 256, 3, 3])), Block block_0 (torch.Size([128, 768, 3])).
I0315 17:42:37.847725 139826544436416 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 13.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 13 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 17:42:37.847939 139826544436416 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 14.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 14 (torch.Size([64, 128, 3, 3])), Block block_0 (torch.Size([64, 384, 3])).
I0315 17:42:37.848228 139826544436416 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 15.block_0 ([torch.Size([64, 64]), torch.Size([576, 576])]) for Parameter 15 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 17:42:37.848448 139826544436416 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 16.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 16 (torch.Size([32, 64, 3, 3])), Block block_0 (torch.Size([32, 576])).
I0315 17:42:37.848733 139826544436416 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 17.block_0 ([torch.Size([1024, 1024]), torch.Size([9, 9])]) for Parameter 17 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 17:42:37.848889 139826544436416 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 18.block_0 ([torch.Size([0])]) for Parameter 18 (torch.Size([1, 32, 1, 1])), Block block_0 (torch.Size([32])).
I0315 17:42:37.849011 139826544436416 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 19.block_0 ([torch.Size([0])]) for Parameter 19 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0315 17:42:37.849845 139826544436416 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 20.block_0 ([torch.Size([512, 512]), torch.Size([1024, 1024])]) for Parameter 20 (torch.Size([512, 256, 2, 2])), Block block_0 (torch.Size([512, 1024])).
I0315 17:42:37.850075 139826544436416 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 21.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 21 (torch.Size([256, 128, 2, 2])), Block block_0 (torch.Size([256, 512])).
I0315 17:42:37.850259 139826544436416 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 22.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 22 (torch.Size([128, 64, 2, 2])), Block block_0 (torch.Size([128, 256])).
I0315 17:42:37.850428 139826544436416 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 23.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 23 (torch.Size([64, 32, 2, 2])), Block block_0 (torch.Size([64, 128])).
I0315 17:42:37.850572 139826544436416 shampoo_preconditioner_list.py:612] Rank 5: ShampooPreconditionerList Numel Breakdown: (165888, 2097314, 174080, 671744, 696320, 327698, 426002, 1310738, 1703954, 1048738, 655522, 1310738, 1212434, 327698, 303122, 671744, 665600, 2097314, 2048, 2, 2621440, 655360, 163840, 40960)
I0315 17:42:37.850627 139826544436416 shampoo_preconditioner_list.py:615] Rank 5: ShampooPreconditionerList Bytes Breakdown: (663552, 8389256, 696320, 2686976)
I0315 17:42:37.850683 139826544436416 shampoo_preconditioner_list.py:618] Rank 5: ShampooPreconditionerList Total Elements: 19350298
I0315 17:42:37.850726 139826544436416 shampoo_preconditioner_list.py:621] Rank 5: ShampooPreconditionerList Total Bytes: 12436104
I0315 17:42:37.850985 139826544436416 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([32, 1, 3, 3])), Block block_0 (torch.Size([288])).
I0315 17:42:37.851073 139826544436416 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 17:42:37.851145 139826544436416 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([64, 32, 3, 3])), Block block_0 (torch.Size([64, 288])).
I0315 17:42:37.851280 139826544436416 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([64, 576])) for Parameter 3 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 17:42:37.851372 139826544436416 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 4.block_0 (torch.Size([0])) for Parameter 4 (torch.Size([128, 64, 3, 3])), Block block_0 (torch.Size([128, 576])).
I0315 17:42:37.851448 139826544436416 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 5.block_0 (torch.Size([0])) for Parameter 5 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 17:42:37.851518 139826544436416 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 6.block_0 (torch.Size([0])) for Parameter 6 (torch.Size([256, 128, 3, 3])), Block block_0 (torch.Size([256, 384, 3])).
I0315 17:42:37.851594 139826544436416 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 7.block_0 (torch.Size([0])) for Parameter 7 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 17:42:37.851673 139826544436416 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 8.block_0 (torch.Size([0])) for Parameter 8 (torch.Size([512, 256, 3, 3])), Block block_0 (torch.Size([512, 768, 3])).
I0315 17:42:37.851754 139826544436416 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 9.block_0 (torch.Size([0])) for Parameter 9 (torch.Size([512, 512, 3, 3])), Block block_0 (torch.Size([512, 512, 9])).
I0315 17:42:37.851824 139826544436416 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 10.block_0 (torch.Size([0])) for Parameter 10 (torch.Size([256, 512, 3, 3])), Block block_0 (torch.Size([256, 512, 9])).
I0315 17:42:37.851893 139826544436416 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 11.block_0 (torch.Size([0])) for Parameter 11 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 17:42:37.851960 139826544436416 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 12.block_0 (torch.Size([0])) for Parameter 12 (torch.Size([128, 256, 3, 3])), Block block_0 (torch.Size([128, 768, 3])).
I0315 17:42:37.852026 139826544436416 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 13.block_0 (torch.Size([0])) for Parameter 13 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 17:42:37.852092 139826544436416 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 14.block_0 (torch.Size([0])) for Parameter 14 (torch.Size([64, 128, 3, 3])), Block block_0 (torch.Size([64, 384, 3])).
I0315 17:42:37.852205 139826544436416 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 15.block_0 (torch.Size([64, 576])) for Parameter 15 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 17:42:37.852292 139826544436416 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 16.block_0 (torch.Size([0])) for Parameter 16 (torch.Size([32, 64, 3, 3])), Block block_0 (torch.Size([32, 576])).
I0315 17:42:37.852400 139826544436416 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 17.block_0 (torch.Size([1024, 9])) for Parameter 17 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 17:42:37.852480 139826544436416 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 18.block_0 (torch.Size([0])) for Parameter 18 (torch.Size([1, 32, 1, 1])), Block block_0 (torch.Size([32])).
I0315 17:42:37.852548 139826544436416 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 19.block_0 (torch.Size([0])) for Parameter 19 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0315 17:42:37.852657 139826544436416 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 20.block_0 (torch.Size([512, 1024])) for Parameter 20 (torch.Size([512, 256, 2, 2])), Block block_0 (torch.Size([512, 1024])).
I0315 17:42:37.852740 139826544436416 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 21.block_0 (torch.Size([0])) for Parameter 21 (torch.Size([256, 128, 2, 2])), Block block_0 (torch.Size([256, 512])).
I0315 17:42:37.852809 139826544436416 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 22.block_0 (torch.Size([0])) for Parameter 22 (torch.Size([128, 64, 2, 2])), Block block_0 (torch.Size([128, 256])).
I0315 17:42:37.852875 139826544436416 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 23.block_0 (torch.Size([0])) for Parameter 23 (torch.Size([64, 32, 2, 2])), Block block_0 (torch.Size([64, 128])).
I0315 17:42:37.852959 139826544436416 shampoo_preconditioner_list.py:375] Rank 5: AdaGradPreconditionerList Numel Breakdown: (0, 0, 0, 36864, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 36864, 0, 9216, 0, 0, 524288, 0, 0, 0)
I0315 17:42:37.853005 139826544436416 shampoo_preconditioner_list.py:378] Rank 5: AdaGradPreconditionerList Bytes Breakdown: (0, 0, 0, 147456, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 147456, 0, 36864, 0, 0, 2097152, 0, 0, 0)
I0315 17:42:37.853046 139826544436416 shampoo_preconditioner_list.py:381] Rank 5: AdaGradPreconditionerList Total Elements: 607232
I0315 17:42:37.853086 139826544436416 shampoo_preconditioner_list.py:384] Rank 5: AdaGradPreconditionerList Total Bytes: 2428928
I0315 17:42:37.855175 139826544436416 logger_utils.py:262] Unable to record workload.train_mean information. Continuing without it.
I0315 17:42:37.855269 139826544436416 logger_utils.py:262] Unable to record workload.train_stddev information. Continuing without it.
W0315 17:42:37.914248 139998127166656 distributed_shampoo.py:341] start_preconditioning_step set to -1. Setting start_preconditioning_step equal to precondition frequency 100 by default.
I0315 17:42:37.916923 139998127166656 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 0.block_0 ([torch.Size([0])]) for Parameter 0 (torch.Size([32, 1, 3, 3])), Block block_0 (torch.Size([288])).
I0315 17:42:37.919213 139998127166656 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 1.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 1 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 17:42:37.919546 139998127166656 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 2.block_0 ([torch.Size([64, 64]), torch.Size([288, 288])]) for Parameter 2 (torch.Size([64, 32, 3, 3])), Block block_0 (torch.Size([64, 288])).
I0315 17:42:37.919724 139998127166656 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 3.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 3 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 17:42:37.919862 139998127166656 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 4.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 4 (torch.Size([128, 64, 3, 3])), Block block_0 (torch.Size([128, 576])).
I0315 17:42:37.920041 139998127166656 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 5.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 5 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 17:42:37.920188 139998127166656 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 6.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 6 (torch.Size([256, 128, 3, 3])), Block block_0 (torch.Size([256, 384, 3])).
I0315 17:42:37.921347 139998127166656 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 7.block_0 ([torch.Size([256, 256]), torch.Size([768, 768]), torch.Size([3, 3])]) for Parameter 7 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 17:42:37.921562 139998127166656 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 8.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 8 (torch.Size([512, 256, 3, 3])), Block block_0 (torch.Size([512, 768, 3])).
I0315 17:42:37.921733 139998127166656 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 9.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 9 (torch.Size([512, 512, 3, 3])), Block block_0 (torch.Size([512, 512, 9])).
I0315 17:42:37.921875 139998127166656 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 10.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 10 (torch.Size([256, 512, 3, 3])), Block block_0 (torch.Size([256, 512, 9])).
I0315 17:42:37.922012 139998127166656 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 11.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 11 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 17:42:37.922162 139998127166656 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 12.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 12 (torch.Size([128, 256, 3, 3])), Block block_0 (torch.Size([128, 768, 3])).
I0315 17:42:37.922309 139998127166656 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 13.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 13 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 17:42:37.922447 139998127166656 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 14.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 14 (torch.Size([64, 128, 3, 3])), Block block_0 (torch.Size([64, 384, 3])).
I0315 17:42:37.922586 139998127166656 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 15.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 15 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 17:42:37.922711 139998127166656 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 16.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 16 (torch.Size([32, 64, 3, 3])), Block block_0 (torch.Size([32, 576])).
I0315 17:42:37.922830 139998127166656 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 17.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 17 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 17:42:37.922919 139998127166656 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 18.block_0 ([torch.Size([0])]) for Parameter 18 (torch.Size([1, 32, 1, 1])), Block block_0 (torch.Size([32])).
I0315 17:42:37.923015 139998127166656 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 19.block_0 ([torch.Size([0])]) for Parameter 19 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0315 17:42:37.923130 139998127166656 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 20.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 20 (torch.Size([512, 256, 2, 2])), Block block_0 (torch.Size([512, 1024])).
I0315 17:42:37.923251 139998127166656 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 21.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 21 (torch.Size([256, 128, 2, 2])), Block block_0 (torch.Size([256, 512])).
I0315 17:42:37.923372 139998127166656 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 22.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 22 (torch.Size([128, 64, 2, 2])), Block block_0 (torch.Size([128, 256])).
I0315 17:42:37.923487 139998127166656 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 23.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 23 (torch.Size([64, 32, 2, 2])), Block block_0 (torch.Size([64, 128])).
I0315 17:42:37.923607 139998127166656 shampoo_preconditioner_list.py:612] Rank 3: ShampooPreconditionerList Numel Breakdown: (165888, 2097314, 174080, 671744, 696320, 327698, 426002, 1310738, 1703954, 1048738, 655522, 1310738, 1212434, 327698, 303122, 671744, 665600, 2097314, 2048, 2, 2621440, 655360, 163840, 40960)
I0315 17:42:37.923653 139998127166656 shampoo_preconditioner_list.py:615] Rank 3: ShampooPreconditionerList Bytes Breakdown: (663552, 8389256)
I0315 17:42:37.923694 139998127166656 shampoo_preconditioner_list.py:618] Rank 3: ShampooPreconditionerList Total Elements: 19350298
I0315 17:42:37.923730 139998127166656 shampoo_preconditioner_list.py:621] Rank 3: ShampooPreconditionerList Total Bytes: 9052808
I0315 17:42:37.923892 139998127166656 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([32, 1, 3, 3])), Block block_0 (torch.Size([288])).
I0315 17:42:37.923987 139998127166656 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 17:42:37.924113 139998127166656 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([64, 288])) for Parameter 2 (torch.Size([64, 32, 3, 3])), Block block_0 (torch.Size([64, 288])).
I0315 17:42:37.924194 139998127166656 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 17:42:37.924253 139998127166656 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 4.block_0 (torch.Size([0])) for Parameter 4 (torch.Size([128, 64, 3, 3])), Block block_0 (torch.Size([128, 576])).
I0315 17:42:37.924311 139998127166656 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 5.block_0 (torch.Size([0])) for Parameter 5 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 17:42:37.924369 139998127166656 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 6.block_0 (torch.Size([0])) for Parameter 6 (torch.Size([256, 128, 3, 3])), Block block_0 (torch.Size([256, 384, 3])).
I0315 17:42:37.924468 139998127166656 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 7.block_0 (torch.Size([256, 768, 3])) for Parameter 7 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 17:42:37.924569 139998127166656 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 8.block_0 (torch.Size([0])) for Parameter 8 (torch.Size([512, 256, 3, 3])), Block block_0 (torch.Size([512, 768, 3])).
I0315 17:42:37.924633 139998127166656 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 9.block_0 (torch.Size([0])) for Parameter 9 (torch.Size([512, 512, 3, 3])), Block block_0 (torch.Size([512, 512, 9])).
I0315 17:42:37.924695 139998127166656 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 10.block_0 (torch.Size([0])) for Parameter 10 (torch.Size([256, 512, 3, 3])), Block block_0 (torch.Size([256, 512, 9])).
I0315 17:42:37.924756 139998127166656 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 11.block_0 (torch.Size([0])) for Parameter 11 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 17:42:37.924815 139998127166656 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 12.block_0 (torch.Size([0])) for Parameter 12 (torch.Size([128, 256, 3, 3])), Block block_0 (torch.Size([128, 768, 3])).
I0315 17:42:37.924884 139998127166656 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 13.block_0 (torch.Size([0])) for Parameter 13 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 17:42:37.924943 139998127166656 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 14.block_0 (torch.Size([0])) for Parameter 14 (torch.Size([64, 128, 3, 3])), Block block_0 (torch.Size([64, 384, 3])).
I0315 17:42:37.925010 139998127166656 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 15.block_0 (torch.Size([0])) for Parameter 15 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 17:42:37.925070 139998127166656 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 16.block_0 (torch.Size([0])) for Parameter 16 (torch.Size([32, 64, 3, 3])), Block block_0 (torch.Size([32, 576])).
I0315 17:42:37.925127 139998127166656 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 17.block_0 (torch.Size([0])) for Parameter 17 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 17:42:37.925186 139998127166656 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 18.block_0 (torch.Size([0])) for Parameter 18 (torch.Size([1, 32, 1, 1])), Block block_0 (torch.Size([32])).
I0315 17:42:37.925243 139998127166656 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 19.block_0 (torch.Size([0])) for Parameter 19 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0315 17:42:37.925301 139998127166656 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 20.block_0 (torch.Size([0])) for Parameter 20 (torch.Size([512, 256, 2, 2])), Block block_0 (torch.Size([512, 1024])).
I0315 17:42:37.925359 139998127166656 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 21.block_0 (torch.Size([0])) for Parameter 21 (torch.Size([256, 128, 2, 2])), Block block_0 (torch.Size([256, 512])).
I0315 17:42:37.925417 139998127166656 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 22.block_0 (torch.Size([0])) for Parameter 22 (torch.Size([128, 64, 2, 2])), Block block_0 (torch.Size([128, 256])).
I0315 17:42:37.925474 139998127166656 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 23.block_0 (torch.Size([0])) for Parameter 23 (torch.Size([64, 32, 2, 2])), Block block_0 (torch.Size([64, 128])).
I0315 17:42:37.925557 139998127166656 shampoo_preconditioner_list.py:375] Rank 3: AdaGradPreconditionerList Numel Breakdown: (0, 0, 18432, 0, 0, 0, 0, 589824, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)
I0315 17:42:37.925617 139998127166656 shampoo_preconditioner_list.py:378] Rank 3: AdaGradPreconditionerList Bytes Breakdown: (0, 0, 73728, 0, 0, 0, 0, 2359296, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)
I0315 17:42:37.925656 139998127166656 shampoo_preconditioner_list.py:381] Rank 3: AdaGradPreconditionerList Total Elements: 608256
I0315 17:42:37.925706 139998127166656 shampoo_preconditioner_list.py:384] Rank 3: AdaGradPreconditionerList Total Bytes: 2433024
I0315 17:42:37.927071 139998127166656 logger_utils.py:262] Unable to record workload.train_mean information. Continuing without it.
I0315 17:42:37.927155 139998127166656 logger_utils.py:262] Unable to record workload.train_stddev information. Continuing without it.
I0315 17:42:38.150865 140460036412608 submission_runner.py:325] Saving flags to /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_1/fastmri_pytorch/trial_4/flags_0.json.
I0315 17:42:38.212153 140460036412608 submission_runner.py:337] Starting training loop.
[rank6]:W0315 17:42:38.245000 50 site-packages/torch/_logging/_internal.py:1081] [0/0] Profiler function <class 'torch.autograd.profiler.record_function'> will be ignored
[rank1]:W0315 17:42:38.245000 45 site-packages/torch/_logging/_internal.py:1081] [0/0] Profiler function <class 'torch.autograd.profiler.record_function'> will be ignored
[rank2]:W0315 17:42:38.245000 46 site-packages/torch/_logging/_internal.py:1081] [0/0] Profiler function <class 'torch.autograd.profiler.record_function'> will be ignored
[rank5]:W0315 17:42:38.245000 49 site-packages/torch/_logging/_internal.py:1081] [0/0] Profiler function <class 'torch.autograd.profiler.record_function'> will be ignored
[rank7]:W0315 17:42:38.246000 51 site-packages/torch/_logging/_internal.py:1081] [0/0] Profiler function <class 'torch.autograd.profiler.record_function'> will be ignored
[rank4]:W0315 17:42:38.247000 48 site-packages/torch/_logging/_internal.py:1081] [0/0] Profiler function <class 'torch.autograd.profiler.record_function'> will be ignored
[rank3]:W0315 17:42:38.247000 47 site-packages/torch/_logging/_internal.py:1081] [0/0] Profiler function <class 'torch.autograd.profiler.record_function'> will be ignored
[rank0]:W0315 17:43:04.669000 44 site-packages/torch/_logging/_internal.py:1081] [0/0] Profiler function <class 'torch.autograd.profiler.record_function'> will be ignored
[rank5]:W0315 17:43:40.070000 49 site-packages/torch/_dynamo/convert_frame.py:844] [2/8] torch._dynamo hit config.cache_size_limit (8)
[rank5]:W0315 17:43:40.070000 49 site-packages/torch/_dynamo/convert_frame.py:844] [2/8]    function: 'forward' (/algorithmic-efficiency/algoperf/workloads/fastmri/fastmri_pytorch/models.py:141)
[rank5]:W0315 17:43:40.070000 49 site-packages/torch/_dynamo/convert_frame.py:844] [2/8]    last reason: 2/0: tensor 'L['x']' requires_grad mismatch. expected requires_grad=0
[rank5]:W0315 17:43:40.070000 49 site-packages/torch/_dynamo/convert_frame.py:844] [2/8] To log all recompilation reasons, use TORCH_LOGS="recompiles".
[rank5]:W0315 17:43:40.070000 49 site-packages/torch/_dynamo/convert_frame.py:844] [2/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.
[rank2]:W0315 17:43:40.194000 46 site-packages/torch/_dynamo/convert_frame.py:844] [2/8] torch._dynamo hit config.cache_size_limit (8)
[rank2]:W0315 17:43:40.194000 46 site-packages/torch/_dynamo/convert_frame.py:844] [2/8]    function: 'forward' (/algorithmic-efficiency/algoperf/workloads/fastmri/fastmri_pytorch/models.py:141)
[rank2]:W0315 17:43:40.194000 46 site-packages/torch/_dynamo/convert_frame.py:844] [2/8]    last reason: 2/0: tensor 'L['x']' requires_grad mismatch. expected requires_grad=0
[rank2]:W0315 17:43:40.194000 46 site-packages/torch/_dynamo/convert_frame.py:844] [2/8] To log all recompilation reasons, use TORCH_LOGS="recompiles".
[rank2]:W0315 17:43:40.194000 46 site-packages/torch/_dynamo/convert_frame.py:844] [2/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.
[rank7]:W0315 17:43:40.233000 51 site-packages/torch/_dynamo/convert_frame.py:844] [2/8] torch._dynamo hit config.cache_size_limit (8)
[rank7]:W0315 17:43:40.233000 51 site-packages/torch/_dynamo/convert_frame.py:844] [2/8]    function: 'forward' (/algorithmic-efficiency/algoperf/workloads/fastmri/fastmri_pytorch/models.py:141)
[rank7]:W0315 17:43:40.233000 51 site-packages/torch/_dynamo/convert_frame.py:844] [2/8]    last reason: 2/0: tensor 'L['x']' requires_grad mismatch. expected requires_grad=0
[rank7]:W0315 17:43:40.233000 51 site-packages/torch/_dynamo/convert_frame.py:844] [2/8] To log all recompilation reasons, use TORCH_LOGS="recompiles".
[rank7]:W0315 17:43:40.233000 51 site-packages/torch/_dynamo/convert_frame.py:844] [2/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.
[rank3]:W0315 17:43:40.315000 47 site-packages/torch/_dynamo/convert_frame.py:844] [2/8] torch._dynamo hit config.cache_size_limit (8)
[rank3]:W0315 17:43:40.315000 47 site-packages/torch/_dynamo/convert_frame.py:844] [2/8]    function: 'forward' (/algorithmic-efficiency/algoperf/workloads/fastmri/fastmri_pytorch/models.py:141)
[rank3]:W0315 17:43:40.315000 47 site-packages/torch/_dynamo/convert_frame.py:844] [2/8]    last reason: 2/0: tensor 'L['x']' requires_grad mismatch. expected requires_grad=0
[rank3]:W0315 17:43:40.315000 47 site-packages/torch/_dynamo/convert_frame.py:844] [2/8] To log all recompilation reasons, use TORCH_LOGS="recompiles".
[rank3]:W0315 17:43:40.315000 47 site-packages/torch/_dynamo/convert_frame.py:844] [2/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.
[rank6]:W0315 17:43:40.706000 50 site-packages/torch/_dynamo/convert_frame.py:844] [2/8] torch._dynamo hit config.cache_size_limit (8)
[rank6]:W0315 17:43:40.706000 50 site-packages/torch/_dynamo/convert_frame.py:844] [2/8]    function: 'forward' (/algorithmic-efficiency/algoperf/workloads/fastmri/fastmri_pytorch/models.py:141)
[rank6]:W0315 17:43:40.706000 50 site-packages/torch/_dynamo/convert_frame.py:844] [2/8]    last reason: 2/0: tensor 'L['x']' requires_grad mismatch. expected requires_grad=0
[rank6]:W0315 17:43:40.706000 50 site-packages/torch/_dynamo/convert_frame.py:844] [2/8] To log all recompilation reasons, use TORCH_LOGS="recompiles".
[rank6]:W0315 17:43:40.706000 50 site-packages/torch/_dynamo/convert_frame.py:844] [2/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.
[rank1]:W0315 17:43:40.869000 45 site-packages/torch/_dynamo/convert_frame.py:844] [2/8] torch._dynamo hit config.cache_size_limit (8)
[rank1]:W0315 17:43:40.869000 45 site-packages/torch/_dynamo/convert_frame.py:844] [2/8]    function: 'forward' (/algorithmic-efficiency/algoperf/workloads/fastmri/fastmri_pytorch/models.py:141)
[rank1]:W0315 17:43:40.869000 45 site-packages/torch/_dynamo/convert_frame.py:844] [2/8]    last reason: 2/0: tensor 'L['x']' requires_grad mismatch. expected requires_grad=0
[rank1]:W0315 17:43:40.869000 45 site-packages/torch/_dynamo/convert_frame.py:844] [2/8] To log all recompilation reasons, use TORCH_LOGS="recompiles".
[rank1]:W0315 17:43:40.869000 45 site-packages/torch/_dynamo/convert_frame.py:844] [2/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.
[rank4]:W0315 17:43:40.933000 48 site-packages/torch/_dynamo/convert_frame.py:844] [2/8] torch._dynamo hit config.cache_size_limit (8)
[rank4]:W0315 17:43:40.933000 48 site-packages/torch/_dynamo/convert_frame.py:844] [2/8]    function: 'forward' (/algorithmic-efficiency/algoperf/workloads/fastmri/fastmri_pytorch/models.py:141)
[rank4]:W0315 17:43:40.933000 48 site-packages/torch/_dynamo/convert_frame.py:844] [2/8]    last reason: 2/0: tensor 'L['x']' requires_grad mismatch. expected requires_grad=0
[rank4]:W0315 17:43:40.933000 48 site-packages/torch/_dynamo/convert_frame.py:844] [2/8] To log all recompilation reasons, use TORCH_LOGS="recompiles".
[rank4]:W0315 17:43:40.933000 48 site-packages/torch/_dynamo/convert_frame.py:844] [2/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.
[rank0]:W0315 17:43:42.560000 44 site-packages/torch/_dynamo/convert_frame.py:844] [2/8] torch._dynamo hit config.cache_size_limit (8)
[rank0]:W0315 17:43:42.560000 44 site-packages/torch/_dynamo/convert_frame.py:844] [2/8]    function: 'forward' (/algorithmic-efficiency/algoperf/workloads/fastmri/fastmri_pytorch/models.py:141)
[rank0]:W0315 17:43:42.560000 44 site-packages/torch/_dynamo/convert_frame.py:844] [2/8]    last reason: 2/0: tensor 'L['x']' requires_grad mismatch. expected requires_grad=0
[rank0]:W0315 17:43:42.560000 44 site-packages/torch/_dynamo/convert_frame.py:844] [2/8] To log all recompilation reasons, use TORCH_LOGS="recompiles".
[rank0]:W0315 17:43:42.560000 44 site-packages/torch/_dynamo/convert_frame.py:844] [2/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.
I0315 17:43:53.688602 140432626222848 logging_writer.py:48] [0] global_step=0, grad_norm=2.55172, loss=0.815781
I0315 17:43:53.706840 140460036412608 submission.py:265] 0) loss = 0.816, grad_norm = 2.552
I0315 17:43:54.437507 140460036412608 spec.py:321] Evaluating on the training split.
I0315 17:45:15.967395 140460036412608 spec.py:333] Evaluating on the validation split.
I0315 17:45:50.007912 140460036412608 spec.py:349] Evaluating on the test split.
I0315 17:46:12.922897 140460036412608 submission_runner.py:469] Time since start: 214.71s, 	Step: 1, 	{'train/ssim': 0.2341423715863909, 'train/loss': 0.8492206164768764, 'validation/ssim': 0.22225685956844402, 'validation/loss': 0.8620637755214196, 'validation/num_examples': 3554, 'test/ssim': 0.24622770009577458, 'test/loss': 0.858143320432491, 'test/num_examples': 3581, 'score': 75.49568057060242, 'total_duration': 214.71094751358032, 'accumulated_submission_time': 75.49568057060242, 'accumulated_eval_time': 138.4857199192047, 'accumulated_logging_time': 0}
I0315 17:46:12.932491 140409929987840 logging_writer.py:48] [1] accumulated_eval_time=138.486, accumulated_logging_time=0, accumulated_submission_time=75.4957, global_step=1, preemption_count=0, score=75.4957, test/loss=0.858143, test/num_examples=3581, test/ssim=0.246228, total_duration=214.711, train/loss=0.849221, train/ssim=0.234142, validation/loss=0.862064, validation/num_examples=3554, validation/ssim=0.222257
I0315 17:46:14.131094 140409921595136 logging_writer.py:48] [1] global_step=1, grad_norm=3.12419, loss=0.875258
I0315 17:46:14.134509 140460036412608 submission.py:265] 1) loss = 0.875, grad_norm = 3.124
I0315 17:46:14.243611 140409929987840 logging_writer.py:48] [2] global_step=2, grad_norm=2.95905, loss=0.832837
I0315 17:46:14.248688 140460036412608 submission.py:265] 2) loss = 0.833, grad_norm = 2.959
I0315 17:46:14.353467 140409921595136 logging_writer.py:48] [3] global_step=3, grad_norm=3.60766, loss=0.918581
I0315 17:46:14.357628 140460036412608 submission.py:265] 3) loss = 0.919, grad_norm = 3.608
I0315 17:46:14.473444 140409929987840 logging_writer.py:48] [4] global_step=4, grad_norm=3.08548, loss=0.848758
I0315 17:46:14.477470 140460036412608 submission.py:265] 4) loss = 0.849, grad_norm = 3.085
I0315 17:46:14.554894 140409921595136 logging_writer.py:48] [5] global_step=5, grad_norm=2.82173, loss=0.84355
I0315 17:46:14.559390 140460036412608 submission.py:265] 5) loss = 0.844, grad_norm = 2.822
I0315 17:46:14.643552 140409929987840 logging_writer.py:48] [6] global_step=6, grad_norm=3.02218, loss=0.879231
I0315 17:46:14.648252 140460036412608 submission.py:265] 6) loss = 0.879, grad_norm = 3.022
I0315 17:46:14.741800 140409921595136 logging_writer.py:48] [7] global_step=7, grad_norm=2.97892, loss=0.77721
I0315 17:46:14.746253 140460036412608 submission.py:265] 7) loss = 0.777, grad_norm = 2.979
I0315 17:46:14.830486 140409929987840 logging_writer.py:48] [8] global_step=8, grad_norm=2.81995, loss=0.760054
I0315 17:46:14.838007 140460036412608 submission.py:265] 8) loss = 0.760, grad_norm = 2.820
I0315 17:46:14.924353 140409921595136 logging_writer.py:48] [9] global_step=9, grad_norm=2.60987, loss=0.840379
I0315 17:46:14.931951 140460036412608 submission.py:265] 9) loss = 0.840, grad_norm = 2.610
I0315 17:46:15.013213 140409929987840 logging_writer.py:48] [10] global_step=10, grad_norm=2.65516, loss=0.857396
I0315 17:46:15.017569 140460036412608 submission.py:265] 10) loss = 0.857, grad_norm = 2.655
I0315 17:46:15.101034 140409921595136 logging_writer.py:48] [11] global_step=11, grad_norm=2.17472, loss=0.770534
I0315 17:46:15.105417 140460036412608 submission.py:265] 11) loss = 0.771, grad_norm = 2.175
I0315 17:46:15.187026 140409929987840 logging_writer.py:48] [12] global_step=12, grad_norm=2.45967, loss=0.73915
I0315 17:46:15.191818 140460036412608 submission.py:265] 12) loss = 0.739, grad_norm = 2.460
I0315 17:46:15.276585 140409921595136 logging_writer.py:48] [13] global_step=13, grad_norm=2.10424, loss=0.720815
I0315 17:46:15.280720 140460036412608 submission.py:265] 13) loss = 0.721, grad_norm = 2.104
I0315 17:46:15.367394 140409929987840 logging_writer.py:48] [14] global_step=14, grad_norm=1.78562, loss=0.717212
I0315 17:46:15.375873 140460036412608 submission.py:265] 14) loss = 0.717, grad_norm = 1.786
I0315 17:46:15.468333 140409921595136 logging_writer.py:48] [15] global_step=15, grad_norm=1.9131, loss=0.818761
I0315 17:46:15.473540 140460036412608 submission.py:265] 15) loss = 0.819, grad_norm = 1.913
I0315 17:46:15.543522 140409929987840 logging_writer.py:48] [16] global_step=16, grad_norm=1.77373, loss=0.733429
I0315 17:46:15.549290 140460036412608 submission.py:265] 16) loss = 0.733, grad_norm = 1.774
I0315 17:46:15.629471 140409921595136 logging_writer.py:48] [17] global_step=17, grad_norm=1.49187, loss=0.644884
I0315 17:46:15.633481 140460036412608 submission.py:265] 17) loss = 0.645, grad_norm = 1.492
I0315 17:46:15.701313 140409929987840 logging_writer.py:48] [18] global_step=18, grad_norm=1.35087, loss=0.644582
I0315 17:46:15.705007 140460036412608 submission.py:265] 18) loss = 0.645, grad_norm = 1.351
I0315 17:46:15.790737 140409921595136 logging_writer.py:48] [19] global_step=19, grad_norm=1.51966, loss=0.66295
I0315 17:46:15.795911 140460036412608 submission.py:265] 19) loss = 0.663, grad_norm = 1.520
I0315 17:46:15.883240 140409929987840 logging_writer.py:48] [20] global_step=20, grad_norm=1.25367, loss=0.567562
I0315 17:46:15.887969 140460036412608 submission.py:265] 20) loss = 0.568, grad_norm = 1.254
I0315 17:46:15.970520 140409921595136 logging_writer.py:48] [21] global_step=21, grad_norm=1.32445, loss=0.682651
I0315 17:46:15.974596 140460036412608 submission.py:265] 21) loss = 0.683, grad_norm = 1.324
I0315 17:46:16.053634 140409929987840 logging_writer.py:48] [22] global_step=22, grad_norm=1.52218, loss=0.683101
I0315 17:46:16.058737 140460036412608 submission.py:265] 22) loss = 0.683, grad_norm = 1.522
I0315 17:46:16.146587 140409921595136 logging_writer.py:48] [23] global_step=23, grad_norm=1.35918, loss=0.645245
I0315 17:46:16.150655 140460036412608 submission.py:265] 23) loss = 0.645, grad_norm = 1.359
I0315 17:46:16.232310 140409929987840 logging_writer.py:48] [24] global_step=24, grad_norm=1.41211, loss=0.659605
I0315 17:46:16.237360 140460036412608 submission.py:265] 24) loss = 0.660, grad_norm = 1.412
I0315 17:46:16.336132 140409921595136 logging_writer.py:48] [25] global_step=25, grad_norm=1.41255, loss=0.643061
I0315 17:46:16.340339 140460036412608 submission.py:265] 25) loss = 0.643, grad_norm = 1.413
I0315 17:46:16.433917 140409929987840 logging_writer.py:48] [26] global_step=26, grad_norm=1.19883, loss=0.75228
I0315 17:46:16.438306 140460036412608 submission.py:265] 26) loss = 0.752, grad_norm = 1.199
I0315 17:46:16.530369 140409921595136 logging_writer.py:48] [27] global_step=27, grad_norm=1.43455, loss=0.589729
I0315 17:46:16.535743 140460036412608 submission.py:265] 27) loss = 0.590, grad_norm = 1.435
I0315 17:46:16.619021 140409929987840 logging_writer.py:48] [28] global_step=28, grad_norm=1.34992, loss=0.615018
I0315 17:46:16.623624 140460036412608 submission.py:265] 28) loss = 0.615, grad_norm = 1.350
I0315 17:46:16.699579 140409921595136 logging_writer.py:48] [29] global_step=29, grad_norm=1.12302, loss=0.589846
I0315 17:46:16.704251 140460036412608 submission.py:265] 29) loss = 0.590, grad_norm = 1.123
I0315 17:46:16.791587 140409929987840 logging_writer.py:48] [30] global_step=30, grad_norm=1.39162, loss=0.562364
I0315 17:46:16.795974 140460036412608 submission.py:265] 30) loss = 0.562, grad_norm = 1.392
I0315 17:46:16.883181 140409921595136 logging_writer.py:48] [31] global_step=31, grad_norm=1.73568, loss=0.556839
I0315 17:46:16.890606 140460036412608 submission.py:265] 31) loss = 0.557, grad_norm = 1.736
I0315 17:46:16.976249 140409929987840 logging_writer.py:48] [32] global_step=32, grad_norm=1.32504, loss=0.597049
I0315 17:46:16.984266 140460036412608 submission.py:265] 32) loss = 0.597, grad_norm = 1.325
I0315 17:46:17.074776 140409921595136 logging_writer.py:48] [33] global_step=33, grad_norm=1.13256, loss=0.57213
I0315 17:46:17.078774 140460036412608 submission.py:265] 33) loss = 0.572, grad_norm = 1.133
I0315 17:46:17.162533 140409929987840 logging_writer.py:48] [34] global_step=34, grad_norm=1.29091, loss=0.56747
I0315 17:46:17.167017 140460036412608 submission.py:265] 34) loss = 0.567, grad_norm = 1.291
I0315 17:46:17.256196 140409921595136 logging_writer.py:48] [35] global_step=35, grad_norm=1.13791, loss=0.556671
I0315 17:46:17.266405 140460036412608 submission.py:265] 35) loss = 0.557, grad_norm = 1.138
I0315 17:46:17.349774 140409929987840 logging_writer.py:48] [36] global_step=36, grad_norm=0.950391, loss=0.532135
I0315 17:46:17.355134 140460036412608 submission.py:265] 36) loss = 0.532, grad_norm = 0.950
I0315 17:46:17.441445 140409921595136 logging_writer.py:48] [37] global_step=37, grad_norm=1.35863, loss=0.51093
I0315 17:46:17.448043 140460036412608 submission.py:265] 37) loss = 0.511, grad_norm = 1.359
I0315 17:46:17.521371 140409929987840 logging_writer.py:48] [38] global_step=38, grad_norm=1.27463, loss=0.49926
I0315 17:46:17.525514 140460036412608 submission.py:265] 38) loss = 0.499, grad_norm = 1.275
I0315 17:46:17.598875 140409921595136 logging_writer.py:48] [39] global_step=39, grad_norm=1.39118, loss=0.4684
I0315 17:46:17.604402 140460036412608 submission.py:265] 39) loss = 0.468, grad_norm = 1.391
I0315 17:46:17.689588 140409929987840 logging_writer.py:48] [40] global_step=40, grad_norm=1.57113, loss=0.530683
I0315 17:46:17.695453 140460036412608 submission.py:265] 40) loss = 0.531, grad_norm = 1.571
I0315 17:46:17.767061 140409921595136 logging_writer.py:48] [41] global_step=41, grad_norm=1.1266, loss=0.518544
I0315 17:46:17.773706 140460036412608 submission.py:265] 41) loss = 0.519, grad_norm = 1.127
I0315 17:46:17.851139 140409929987840 logging_writer.py:48] [42] global_step=42, grad_norm=1.30112, loss=0.519181
I0315 17:46:17.861212 140460036412608 submission.py:265] 42) loss = 0.519, grad_norm = 1.301
I0315 17:46:17.934535 140409921595136 logging_writer.py:48] [43] global_step=43, grad_norm=1.2581, loss=0.524902
I0315 17:46:17.943492 140460036412608 submission.py:265] 43) loss = 0.525, grad_norm = 1.258
I0315 17:46:18.022878 140409929987840 logging_writer.py:48] [44] global_step=44, grad_norm=1.37281, loss=0.479321
I0315 17:46:18.027558 140460036412608 submission.py:265] 44) loss = 0.479, grad_norm = 1.373
I0315 17:46:18.110422 140409921595136 logging_writer.py:48] [45] global_step=45, grad_norm=1.30294, loss=0.453808
I0315 17:46:18.115422 140460036412608 submission.py:265] 45) loss = 0.454, grad_norm = 1.303
I0315 17:46:18.194174 140409929987840 logging_writer.py:48] [46] global_step=46, grad_norm=1.13147, loss=0.446158
I0315 17:46:18.198177 140460036412608 submission.py:265] 46) loss = 0.446, grad_norm = 1.131
I0315 17:46:18.281040 140409921595136 logging_writer.py:48] [47] global_step=47, grad_norm=1.20211, loss=0.473465
I0315 17:46:18.286344 140460036412608 submission.py:265] 47) loss = 0.473, grad_norm = 1.202
I0315 17:46:18.365331 140409929987840 logging_writer.py:48] [48] global_step=48, grad_norm=1.47273, loss=0.548743
I0315 17:46:18.369307 140460036412608 submission.py:265] 48) loss = 0.549, grad_norm = 1.473
I0315 17:46:18.449269 140409921595136 logging_writer.py:48] [49] global_step=49, grad_norm=1.0252, loss=0.423333
I0315 17:46:18.453241 140460036412608 submission.py:265] 49) loss = 0.423, grad_norm = 1.025
I0315 17:46:18.535552 140409929987840 logging_writer.py:48] [50] global_step=50, grad_norm=1.03507, loss=0.385963
I0315 17:46:18.540720 140460036412608 submission.py:265] 50) loss = 0.386, grad_norm = 1.035
I0315 17:46:18.617459 140409921595136 logging_writer.py:48] [51] global_step=51, grad_norm=1.00735, loss=0.404054
I0315 17:46:18.622597 140460036412608 submission.py:265] 51) loss = 0.404, grad_norm = 1.007
I0315 17:46:18.704936 140409929987840 logging_writer.py:48] [52] global_step=52, grad_norm=0.815098, loss=0.487954
I0315 17:46:18.709079 140460036412608 submission.py:265] 52) loss = 0.488, grad_norm = 0.815
I0315 17:46:18.924438 140409921595136 logging_writer.py:48] [53] global_step=53, grad_norm=1.17548, loss=0.470882
I0315 17:46:18.930758 140460036412608 submission.py:265] 53) loss = 0.471, grad_norm = 1.175
I0315 17:46:19.065982 140409929987840 logging_writer.py:48] [54] global_step=54, grad_norm=1.40393, loss=0.431795
I0315 17:46:19.071006 140460036412608 submission.py:265] 54) loss = 0.432, grad_norm = 1.404
I0315 17:46:19.363598 140409921595136 logging_writer.py:48] [55] global_step=55, grad_norm=0.98366, loss=0.414276
I0315 17:46:19.368260 140460036412608 submission.py:265] 55) loss = 0.414, grad_norm = 0.984
I0315 17:46:19.955596 140409929987840 logging_writer.py:48] [56] global_step=56, grad_norm=0.82562, loss=0.431321
I0315 17:46:19.961716 140460036412608 submission.py:265] 56) loss = 0.431, grad_norm = 0.826
I0315 17:46:20.256913 140409921595136 logging_writer.py:48] [57] global_step=57, grad_norm=1.24751, loss=0.357168
I0315 17:46:20.264590 140460036412608 submission.py:265] 57) loss = 0.357, grad_norm = 1.248
I0315 17:46:20.553382 140409929987840 logging_writer.py:48] [58] global_step=58, grad_norm=0.769686, loss=0.357305
I0315 17:46:20.558221 140460036412608 submission.py:265] 58) loss = 0.357, grad_norm = 0.770
I0315 17:46:20.709907 140409921595136 logging_writer.py:48] [59] global_step=59, grad_norm=1.41435, loss=0.390335
I0315 17:46:20.714873 140460036412608 submission.py:265] 59) loss = 0.390, grad_norm = 1.414
I0315 17:46:20.920418 140409929987840 logging_writer.py:48] [60] global_step=60, grad_norm=0.996129, loss=0.345083
I0315 17:46:20.924888 140460036412608 submission.py:265] 60) loss = 0.345, grad_norm = 0.996
I0315 17:46:21.276033 140409921595136 logging_writer.py:48] [61] global_step=61, grad_norm=1.25342, loss=0.36591
I0315 17:46:21.285832 140460036412608 submission.py:265] 61) loss = 0.366, grad_norm = 1.253
I0315 17:46:21.588417 140409929987840 logging_writer.py:48] [62] global_step=62, grad_norm=1.19984, loss=0.381701
I0315 17:46:21.595155 140460036412608 submission.py:265] 62) loss = 0.382, grad_norm = 1.200
I0315 17:46:21.844792 140409921595136 logging_writer.py:48] [63] global_step=63, grad_norm=1.4656, loss=0.356172
I0315 17:46:21.855023 140460036412608 submission.py:265] 63) loss = 0.356, grad_norm = 1.466
I0315 17:46:22.281459 140409929987840 logging_writer.py:48] [64] global_step=64, grad_norm=0.842959, loss=0.360577
I0315 17:46:22.286735 140460036412608 submission.py:265] 64) loss = 0.361, grad_norm = 0.843
I0315 17:46:22.657691 140409921595136 logging_writer.py:48] [65] global_step=65, grad_norm=1.38149, loss=0.535611
I0315 17:46:22.663375 140460036412608 submission.py:265] 65) loss = 0.536, grad_norm = 1.381
I0315 17:46:23.006593 140409929987840 logging_writer.py:48] [66] global_step=66, grad_norm=0.819877, loss=0.423083
I0315 17:46:23.011304 140460036412608 submission.py:265] 66) loss = 0.423, grad_norm = 0.820
I0315 17:46:23.327913 140409921595136 logging_writer.py:48] [67] global_step=67, grad_norm=0.647033, loss=0.300799
I0315 17:46:23.333296 140460036412608 submission.py:265] 67) loss = 0.301, grad_norm = 0.647
I0315 17:46:23.513528 140409929987840 logging_writer.py:48] [68] global_step=68, grad_norm=0.609337, loss=0.357593
I0315 17:46:23.518774 140460036412608 submission.py:265] 68) loss = 0.358, grad_norm = 0.609
I0315 17:46:23.805669 140409921595136 logging_writer.py:48] [69] global_step=69, grad_norm=0.625982, loss=0.467831
I0315 17:46:23.810425 140460036412608 submission.py:265] 69) loss = 0.468, grad_norm = 0.626
I0315 17:46:24.021649 140409929987840 logging_writer.py:48] [70] global_step=70, grad_norm=1.03028, loss=0.373094
I0315 17:46:24.026526 140460036412608 submission.py:265] 70) loss = 0.373, grad_norm = 1.030
I0315 17:46:24.183753 140409921595136 logging_writer.py:48] [71] global_step=71, grad_norm=0.680118, loss=0.453763
I0315 17:46:24.189718 140460036412608 submission.py:265] 71) loss = 0.454, grad_norm = 0.680
I0315 17:46:24.314140 140409929987840 logging_writer.py:48] [72] global_step=72, grad_norm=0.435804, loss=0.372493
I0315 17:46:24.320725 140460036412608 submission.py:265] 72) loss = 0.372, grad_norm = 0.436
I0315 17:46:24.402795 140409921595136 logging_writer.py:48] [73] global_step=73, grad_norm=1.19102, loss=0.381489
I0315 17:46:24.407992 140460036412608 submission.py:265] 73) loss = 0.381, grad_norm = 1.191
I0315 17:46:24.524156 140409929987840 logging_writer.py:48] [74] global_step=74, grad_norm=0.998806, loss=0.374273
I0315 17:46:24.529282 140460036412608 submission.py:265] 74) loss = 0.374, grad_norm = 0.999
I0315 17:46:24.702863 140409921595136 logging_writer.py:48] [75] global_step=75, grad_norm=0.696646, loss=0.437802
I0315 17:46:24.708980 140460036412608 submission.py:265] 75) loss = 0.438, grad_norm = 0.697
I0315 17:46:24.834551 140409929987840 logging_writer.py:48] [76] global_step=76, grad_norm=0.517508, loss=0.413816
I0315 17:46:24.840174 140460036412608 submission.py:265] 76) loss = 0.414, grad_norm = 0.518
I0315 17:46:24.921978 140409921595136 logging_writer.py:48] [77] global_step=77, grad_norm=0.520152, loss=0.336215
I0315 17:46:24.926885 140460036412608 submission.py:265] 77) loss = 0.336, grad_norm = 0.520
I0315 17:46:25.008974 140409929987840 logging_writer.py:48] [78] global_step=78, grad_norm=0.858819, loss=0.343848
I0315 17:46:25.014855 140460036412608 submission.py:265] 78) loss = 0.344, grad_norm = 0.859
I0315 17:46:25.096686 140409921595136 logging_writer.py:48] [79] global_step=79, grad_norm=1.1504, loss=0.304262
I0315 17:46:25.104295 140460036412608 submission.py:265] 79) loss = 0.304, grad_norm = 1.150
I0315 17:46:25.216059 140409929987840 logging_writer.py:48] [80] global_step=80, grad_norm=0.597116, loss=0.374557
I0315 17:46:25.221515 140460036412608 submission.py:265] 80) loss = 0.375, grad_norm = 0.597
I0315 17:46:25.340253 140409921595136 logging_writer.py:48] [81] global_step=81, grad_norm=0.521007, loss=0.417397
I0315 17:46:25.348529 140460036412608 submission.py:265] 81) loss = 0.417, grad_norm = 0.521
I0315 17:46:25.463687 140409929987840 logging_writer.py:48] [82] global_step=82, grad_norm=0.889255, loss=0.491115
I0315 17:46:25.468883 140460036412608 submission.py:265] 82) loss = 0.491, grad_norm = 0.889
I0315 17:46:25.567056 140409921595136 logging_writer.py:48] [83] global_step=83, grad_norm=0.748693, loss=0.444187
I0315 17:46:25.574299 140460036412608 submission.py:265] 83) loss = 0.444, grad_norm = 0.749
I0315 17:46:25.660771 140409929987840 logging_writer.py:48] [84] global_step=84, grad_norm=0.960994, loss=0.364673
I0315 17:46:25.666572 140460036412608 submission.py:265] 84) loss = 0.365, grad_norm = 0.961
I0315 17:46:25.888877 140409921595136 logging_writer.py:48] [85] global_step=85, grad_norm=0.944189, loss=0.376651
I0315 17:46:25.893503 140460036412608 submission.py:265] 85) loss = 0.377, grad_norm = 0.944
I0315 17:46:26.014314 140409929987840 logging_writer.py:48] [86] global_step=86, grad_norm=0.692881, loss=0.354237
I0315 17:46:26.019300 140460036412608 submission.py:265] 86) loss = 0.354, grad_norm = 0.693
I0315 17:46:26.199110 140409921595136 logging_writer.py:48] [87] global_step=87, grad_norm=0.643394, loss=0.405549
I0315 17:46:26.203671 140460036412608 submission.py:265] 87) loss = 0.406, grad_norm = 0.643
I0315 17:46:26.354371 140409929987840 logging_writer.py:48] [88] global_step=88, grad_norm=0.483358, loss=0.491075
I0315 17:46:26.360447 140460036412608 submission.py:265] 88) loss = 0.491, grad_norm = 0.483
I0315 17:46:26.581484 140409921595136 logging_writer.py:48] [89] global_step=89, grad_norm=0.733504, loss=0.361274
I0315 17:46:26.586821 140460036412608 submission.py:265] 89) loss = 0.361, grad_norm = 0.734
I0315 17:46:26.965648 140409929987840 logging_writer.py:48] [90] global_step=90, grad_norm=0.653243, loss=0.341685
I0315 17:46:26.975447 140460036412608 submission.py:265] 90) loss = 0.342, grad_norm = 0.653
I0315 17:46:27.155732 140409921595136 logging_writer.py:48] [91] global_step=91, grad_norm=0.387276, loss=0.363081
I0315 17:46:27.160684 140460036412608 submission.py:265] 91) loss = 0.363, grad_norm = 0.387
I0315 17:46:27.389342 140409929987840 logging_writer.py:48] [92] global_step=92, grad_norm=0.599229, loss=0.418642
I0315 17:46:27.394413 140460036412608 submission.py:265] 92) loss = 0.419, grad_norm = 0.599
I0315 17:46:27.556548 140409921595136 logging_writer.py:48] [93] global_step=93, grad_norm=0.869313, loss=0.411646
I0315 17:46:27.561095 140460036412608 submission.py:265] 93) loss = 0.412, grad_norm = 0.869
I0315 17:46:27.751326 140409929987840 logging_writer.py:48] [94] global_step=94, grad_norm=0.856934, loss=0.34538
I0315 17:46:27.756838 140460036412608 submission.py:265] 94) loss = 0.345, grad_norm = 0.857
I0315 17:46:27.951247 140409921595136 logging_writer.py:48] [95] global_step=95, grad_norm=0.409654, loss=0.467089
I0315 17:46:27.957344 140460036412608 submission.py:265] 95) loss = 0.467, grad_norm = 0.410
I0315 17:46:28.077753 140409929987840 logging_writer.py:48] [96] global_step=96, grad_norm=0.780175, loss=0.360923
I0315 17:46:28.082596 140460036412608 submission.py:265] 96) loss = 0.361, grad_norm = 0.780
I0315 17:46:28.299739 140409921595136 logging_writer.py:48] [97] global_step=97, grad_norm=0.590146, loss=0.381358
I0315 17:46:28.304028 140460036412608 submission.py:265] 97) loss = 0.381, grad_norm = 0.590
I0315 17:46:28.499587 140409929987840 logging_writer.py:48] [98] global_step=98, grad_norm=0.316568, loss=0.466945
I0315 17:46:28.507272 140460036412608 submission.py:265] 98) loss = 0.467, grad_norm = 0.317
I0315 17:46:29.094634 140409921595136 logging_writer.py:48] [99] global_step=99, grad_norm=0.448383, loss=0.326476
I0315 17:46:29.100555 140460036412608 submission.py:265] 99) loss = 0.326, grad_norm = 0.448
I0315 17:46:29.190954 140409929987840 logging_writer.py:48] [100] global_step=100, grad_norm=0.461509, loss=0.394035
I0315 17:46:29.195468 140460036412608 submission.py:265] 100) loss = 0.394, grad_norm = 0.462
/usr/local/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
I0315 17:47:33.892961 140460036412608 spec.py:321] Evaluating on the training split.
I0315 17:47:36.177640 140460036412608 spec.py:333] Evaluating on the validation split.
I0315 17:47:38.598061 140460036412608 spec.py:349] Evaluating on the test split.
I0315 17:47:40.805019 140460036412608 submission_runner.py:469] Time since start: 302.59s, 	Step: 427, 	{'train/ssim': 0.7017958504813058, 'train/loss': 0.3139854839869908, 'validation/ssim': 0.6829683543190771, 'validation/loss': 0.32975069216683667, 'validation/num_examples': 3554, 'test/ssim': 0.701429105544017, 'test/loss': 0.33112787514617076, 'test/num_examples': 3581, 'score': 154.54857659339905, 'total_duration': 302.59312319755554, 'accumulated_submission_time': 154.54857659339905, 'accumulated_eval_time': 145.400132894516, 'accumulated_logging_time': 0.018735408782958984}
I0315 17:47:40.815287 140409921595136 logging_writer.py:48] [427] accumulated_eval_time=145.4, accumulated_logging_time=0.0187354, accumulated_submission_time=154.549, global_step=427, preemption_count=0, score=154.549, test/loss=0.331128, test/num_examples=3581, test/ssim=0.701429, total_duration=302.593, train/loss=0.313985, train/ssim=0.701796, validation/loss=0.329751, validation/num_examples=3554, validation/ssim=0.682968
I0315 17:47:51.748240 140409929987840 logging_writer.py:48] [500] global_step=500, grad_norm=0.331086, loss=0.455382
I0315 17:47:51.754958 140460036412608 submission.py:265] 500) loss = 0.455, grad_norm = 0.331
I0315 17:49:02.865633 140460036412608 spec.py:321] Evaluating on the training split.
I0315 17:49:04.917918 140460036412608 spec.py:333] Evaluating on the validation split.
I0315 17:49:07.048171 140460036412608 spec.py:349] Evaluating on the test split.
I0315 17:49:09.154903 140460036412608 submission_runner.py:469] Time since start: 390.94s, 	Step: 772, 	{'train/ssim': 0.7138230460030692, 'train/loss': 0.2998525755746024, 'validation/ssim': 0.6946022664823087, 'validation/loss': 0.31572198304287424, 'validation/num_examples': 3554, 'test/ssim': 0.712553764115296, 'test/loss': 0.31755744701855276, 'test/num_examples': 3581, 'score': 234.83952689170837, 'total_duration': 390.9429864883423, 'accumulated_submission_time': 234.83952689170837, 'accumulated_eval_time': 151.68952083587646, 'accumulated_logging_time': 0.03840351104736328}
I0315 17:49:09.165443 140409921595136 logging_writer.py:48] [772] accumulated_eval_time=151.69, accumulated_logging_time=0.0384035, accumulated_submission_time=234.84, global_step=772, preemption_count=0, score=234.84, test/loss=0.317557, test/num_examples=3581, test/ssim=0.712554, total_duration=390.943, train/loss=0.299853, train/ssim=0.713823, validation/loss=0.315722, validation/num_examples=3554, validation/ssim=0.694602
I0315 17:49:57.022644 140409929987840 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.240426, loss=0.446422
I0315 17:49:57.029683 140460036412608 submission.py:265] 1000) loss = 0.446, grad_norm = 0.240
I0315 17:50:29.173763 140409921595136 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.307261, loss=0.303007
I0315 17:50:29.177168 140460036412608 submission.py:265] 1500) loss = 0.303, grad_norm = 0.307
I0315 17:50:29.942085 140460036412608 spec.py:321] Evaluating on the training split.
I0315 17:50:31.931325 140460036412608 spec.py:333] Evaluating on the validation split.
I0315 17:50:34.589212 140460036412608 spec.py:349] Evaluating on the test split.
I0315 17:50:37.132592 140460036412608 submission_runner.py:469] Time since start: 478.92s, 	Step: 1501, 	{'train/ssim': 0.7284294537135533, 'train/loss': 0.284986104284014, 'validation/ssim': 0.7101420164779122, 'validation/loss': 0.30002636498927265, 'validation/num_examples': 3554, 'test/ssim': 0.727261651664165, 'test/loss': 0.30217880338505304, 'test/num_examples': 3581, 'score': 313.6739218235016, 'total_duration': 478.920706987381, 'accumulated_submission_time': 313.6739218235016, 'accumulated_eval_time': 158.8801383972168, 'accumulated_logging_time': 0.06329751014709473}
I0315 17:50:37.175595 140409929987840 logging_writer.py:48] [1501] accumulated_eval_time=158.88, accumulated_logging_time=0.0632975, accumulated_submission_time=313.674, global_step=1501, preemption_count=0, score=313.674, test/loss=0.302179, test/num_examples=3581, test/ssim=0.727262, total_duration=478.921, train/loss=0.284986, train/ssim=0.728429, validation/loss=0.300026, validation/num_examples=3554, validation/ssim=0.710142
I0315 17:51:09.629447 140409921595136 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.122921, loss=0.353127
I0315 17:51:09.633313 140460036412608 submission.py:265] 2000) loss = 0.353, grad_norm = 0.123
I0315 17:51:41.271207 140409929987840 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.144891, loss=0.362532
I0315 17:51:41.274879 140460036412608 submission.py:265] 2500) loss = 0.363, grad_norm = 0.145
I0315 17:51:57.900132 140460036412608 spec.py:321] Evaluating on the training split.
I0315 17:51:59.883767 140460036412608 spec.py:333] Evaluating on the validation split.
I0315 17:52:02.466380 140460036412608 spec.py:349] Evaluating on the test split.
I0315 17:52:04.674774 140460036412608 submission_runner.py:469] Time since start: 566.46s, 	Step: 2752, 	{'train/ssim': 0.7313822337559291, 'train/loss': 0.2804743732724871, 'validation/ssim': 0.7126930593169668, 'validation/loss': 0.2956162744113147, 'validation/num_examples': 3554, 'test/ssim': 0.7300859380454133, 'test/loss': 0.29736075871352274, 'test/num_examples': 3581, 'score': 392.37354373931885, 'total_duration': 566.4628839492798, 'accumulated_submission_time': 392.37354373931885, 'accumulated_eval_time': 165.6549310684204, 'accumulated_logging_time': 0.11548566818237305}
I0315 17:52:04.685580 140409921595136 logging_writer.py:48] [2752] accumulated_eval_time=165.655, accumulated_logging_time=0.115486, accumulated_submission_time=392.374, global_step=2752, preemption_count=0, score=392.374, test/loss=0.297361, test/num_examples=3581, test/ssim=0.730086, total_duration=566.463, train/loss=0.280474, train/ssim=0.731382, validation/loss=0.295616, validation/num_examples=3554, validation/ssim=0.712693
I0315 17:52:21.283849 140409929987840 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.387925, loss=0.261125
I0315 17:52:21.287220 140460036412608 submission.py:265] 3000) loss = 0.261, grad_norm = 0.388
I0315 17:52:52.893851 140409921595136 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.248373, loss=0.190443
I0315 17:52:52.897256 140460036412608 submission.py:265] 3500) loss = 0.190, grad_norm = 0.248
I0315 17:53:24.550814 140409929987840 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.514359, loss=0.278175
I0315 17:53:24.554051 140460036412608 submission.py:265] 4000) loss = 0.278, grad_norm = 0.514
I0315 17:53:25.403620 140460036412608 spec.py:321] Evaluating on the training split.
I0315 17:53:27.413186 140460036412608 spec.py:333] Evaluating on the validation split.
I0315 17:53:29.660753 140460036412608 spec.py:349] Evaluating on the test split.
I0315 17:53:31.889913 140460036412608 submission_runner.py:469] Time since start: 653.68s, 	Step: 4003, 	{'train/ssim': 0.7349044936043876, 'train/loss': 0.27780682700020926, 'validation/ssim': 0.7155696458875562, 'validation/loss': 0.2932794900442283, 'validation/num_examples': 3554, 'test/ssim': 0.7328780450424114, 'test/loss': 0.2949240566531695, 'test/num_examples': 3581, 'score': 471.12213373184204, 'total_duration': 653.6779997348785, 'accumulated_submission_time': 471.12213373184204, 'accumulated_eval_time': 172.14147639274597, 'accumulated_logging_time': 0.13443684577941895}
I0315 17:53:31.901871 140409921595136 logging_writer.py:48] [4003] accumulated_eval_time=172.141, accumulated_logging_time=0.134437, accumulated_submission_time=471.122, global_step=4003, preemption_count=0, score=471.122, test/loss=0.294924, test/num_examples=3581, test/ssim=0.732878, total_duration=653.678, train/loss=0.277807, train/ssim=0.734904, validation/loss=0.293279, validation/num_examples=3554, validation/ssim=0.71557
I0315 17:54:04.278488 140409929987840 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.188291, loss=0.303429
I0315 17:54:04.282068 140460036412608 submission.py:265] 4500) loss = 0.303, grad_norm = 0.188
I0315 17:54:35.989216 140409921595136 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.178906, loss=0.368234
I0315 17:54:35.992939 140460036412608 submission.py:265] 5000) loss = 0.368, grad_norm = 0.179
I0315 17:54:52.606245 140460036412608 spec.py:321] Evaluating on the training split.
I0315 17:54:54.693791 140460036412608 spec.py:333] Evaluating on the validation split.
I0315 17:54:57.000929 140460036412608 spec.py:349] Evaluating on the test split.
I0315 17:54:59.215378 140460036412608 submission_runner.py:469] Time since start: 741.00s, 	Step: 5253, 	{'train/ssim': 0.7382275036403111, 'train/loss': 0.2753668853214809, 'validation/ssim': 0.7187532286464196, 'validation/loss': 0.29102926087902714, 'validation/num_examples': 3554, 'test/ssim': 0.7360727352258447, 'test/loss': 0.2924933200507016, 'test/num_examples': 3581, 'score': 549.9265217781067, 'total_duration': 741.0034821033478, 'accumulated_submission_time': 549.9265217781067, 'accumulated_eval_time': 178.75071334838867, 'accumulated_logging_time': 0.155348539352417}
I0315 17:54:59.226278 140409929987840 logging_writer.py:48] [5253] accumulated_eval_time=178.751, accumulated_logging_time=0.155349, accumulated_submission_time=549.927, global_step=5253, preemption_count=0, score=549.927, test/loss=0.292493, test/num_examples=3581, test/ssim=0.736073, total_duration=741.003, train/loss=0.275367, train/ssim=0.738228, validation/loss=0.291029, validation/num_examples=3554, validation/ssim=0.718753
I0315 17:55:15.725835 140409921595136 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.352256, loss=0.262902
I0315 17:55:15.729541 140460036412608 submission.py:265] 5500) loss = 0.263, grad_norm = 0.352
I0315 17:55:47.361542 140409929987840 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.592395, loss=0.374928
I0315 17:55:47.365052 140460036412608 submission.py:265] 6000) loss = 0.375, grad_norm = 0.592
I0315 17:56:18.970994 140409921595136 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.304405, loss=0.251858
I0315 17:56:18.974575 140460036412608 submission.py:265] 6500) loss = 0.252, grad_norm = 0.304
I0315 17:56:19.979899 140460036412608 spec.py:321] Evaluating on the training split.
I0315 17:56:22.036925 140460036412608 spec.py:333] Evaluating on the validation split.
I0315 17:56:24.200312 140460036412608 spec.py:349] Evaluating on the test split.
I0315 17:56:26.324389 140460036412608 submission_runner.py:469] Time since start: 828.11s, 	Step: 6505, 	{'train/ssim': 0.7374308449881417, 'train/loss': 0.2757686376571655, 'validation/ssim': 0.7177999536174029, 'validation/loss': 0.2914030969176456, 'validation/num_examples': 3554, 'test/ssim': 0.735047221882854, 'test/loss': 0.2930007248542656, 'test/num_examples': 3581, 'score': 628.7911894321442, 'total_duration': 828.112498998642, 'accumulated_submission_time': 628.7911894321442, 'accumulated_eval_time': 185.09542083740234, 'accumulated_logging_time': 0.1756441593170166}
I0315 17:56:26.335817 140409929987840 logging_writer.py:48] [6505] accumulated_eval_time=185.095, accumulated_logging_time=0.175644, accumulated_submission_time=628.791, global_step=6505, preemption_count=0, score=628.791, test/loss=0.293001, test/num_examples=3581, test/ssim=0.735047, total_duration=828.112, train/loss=0.275769, train/ssim=0.737431, validation/loss=0.291403, validation/num_examples=3554, validation/ssim=0.7178
I0315 17:57:02.893804 140409921595136 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.264659, loss=0.228887
I0315 17:57:02.897060 140460036412608 submission.py:265] 7000) loss = 0.229, grad_norm = 0.265
I0315 17:57:34.550524 140409929987840 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.256346, loss=0.322274
I0315 17:57:34.553847 140460036412608 submission.py:265] 7500) loss = 0.322, grad_norm = 0.256
I0315 17:57:47.034377 140460036412608 spec.py:321] Evaluating on the training split.
I0315 17:57:49.122557 140460036412608 spec.py:333] Evaluating on the validation split.
I0315 17:57:51.317920 140460036412608 spec.py:349] Evaluating on the test split.
I0315 17:57:53.509933 140460036412608 submission_runner.py:469] Time since start: 915.30s, 	Step: 7688, 	{'train/ssim': 0.7383181708199638, 'train/loss': 0.2742515802383423, 'validation/ssim': 0.7184254179929305, 'validation/loss': 0.29005980827061056, 'validation/num_examples': 3554, 'test/ssim': 0.7357163758159383, 'test/loss': 0.2916327942286547, 'test/num_examples': 3581, 'score': 704.1700584888458, 'total_duration': 915.2980053424835, 'accumulated_submission_time': 704.1700584888458, 'accumulated_eval_time': 191.57111430168152, 'accumulated_logging_time': 3.559833526611328}
I0315 17:57:53.521124 140409921595136 logging_writer.py:48] [7688] accumulated_eval_time=191.571, accumulated_logging_time=3.55983, accumulated_submission_time=704.17, global_step=7688, preemption_count=0, score=704.17, test/loss=0.291633, test/num_examples=3581, test/ssim=0.735716, total_duration=915.298, train/loss=0.274252, train/ssim=0.738318, validation/loss=0.29006, validation/num_examples=3554, validation/ssim=0.718425
I0315 17:58:14.115426 140409929987840 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.242058, loss=0.347306
I0315 17:58:14.118700 140460036412608 submission.py:265] 8000) loss = 0.347, grad_norm = 0.242
I0315 17:58:45.706528 140409921595136 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.216036, loss=0.350611
I0315 17:58:45.709964 140460036412608 submission.py:265] 8500) loss = 0.351, grad_norm = 0.216
I0315 17:59:14.264984 140460036412608 spec.py:321] Evaluating on the training split.
I0315 17:59:16.284774 140460036412608 spec.py:333] Evaluating on the validation split.
I0315 17:59:18.425430 140460036412608 spec.py:349] Evaluating on the test split.
I0315 17:59:20.526058 140460036412608 submission_runner.py:469] Time since start: 1002.31s, 	Step: 8941, 	{'train/ssim': 0.7387643541608538, 'train/loss': 0.27422739778246197, 'validation/ssim': 0.718900441184229, 'validation/loss': 0.2902637625518782, 'validation/num_examples': 3554, 'test/ssim': 0.7360171030700223, 'test/loss': 0.2918384491282114, 'test/num_examples': 3581, 'score': 782.9669303894043, 'total_duration': 1002.314138174057, 'accumulated_submission_time': 782.9669303894043, 'accumulated_eval_time': 197.8323690891266, 'accumulated_logging_time': 3.5794622898101807}
I0315 17:59:20.538095 140409929987840 logging_writer.py:48] [8941] accumulated_eval_time=197.832, accumulated_logging_time=3.57946, accumulated_submission_time=782.967, global_step=8941, preemption_count=0, score=782.967, test/loss=0.291838, test/num_examples=3581, test/ssim=0.736017, total_duration=1002.31, train/loss=0.274227, train/ssim=0.738764, validation/loss=0.290264, validation/num_examples=3554, validation/ssim=0.7189
I0315 17:59:25.232317 140409921595136 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.409535, loss=0.336796
I0315 17:59:25.235544 140460036412608 submission.py:265] 9000) loss = 0.337, grad_norm = 0.410
I0315 17:59:56.873425 140409929987840 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.528888, loss=0.290688
I0315 17:59:56.876813 140460036412608 submission.py:265] 9500) loss = 0.291, grad_norm = 0.529
I0315 18:00:28.517223 140409921595136 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.30175, loss=0.202587
I0315 18:00:28.520811 140460036412608 submission.py:265] 10000) loss = 0.203, grad_norm = 0.302
I0315 18:00:41.271873 140460036412608 spec.py:321] Evaluating on the training split.
I0315 18:00:43.296985 140460036412608 spec.py:333] Evaluating on the validation split.
I0315 18:00:45.449708 140460036412608 spec.py:349] Evaluating on the test split.
I0315 18:00:47.592833 140460036412608 submission_runner.py:469] Time since start: 1089.38s, 	Step: 10193, 	{'train/ssim': 0.739696911403111, 'train/loss': 0.27358199868883404, 'validation/ssim': 0.7197897615362971, 'validation/loss': 0.28964406852314295, 'validation/num_examples': 3554, 'test/ssim': 0.736927193325014, 'test/loss': 0.2911998724278309, 'test/num_examples': 3581, 'score': 861.7214970588684, 'total_duration': 1089.380892753601, 'accumulated_submission_time': 861.7214970588684, 'accumulated_eval_time': 204.15355706214905, 'accumulated_logging_time': 3.60011625289917}
I0315 18:00:47.605233 140409929987840 logging_writer.py:48] [10193] accumulated_eval_time=204.154, accumulated_logging_time=3.60012, accumulated_submission_time=861.721, global_step=10193, preemption_count=0, score=861.721, test/loss=0.2912, test/num_examples=3581, test/ssim=0.736927, total_duration=1089.38, train/loss=0.273582, train/ssim=0.739697, validation/loss=0.289644, validation/num_examples=3554, validation/ssim=0.71979
I0315 18:01:07.934597 140409921595136 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.168514, loss=0.225874
I0315 18:01:07.937814 140460036412608 submission.py:265] 10500) loss = 0.226, grad_norm = 0.169
I0315 18:01:39.520365 140409929987840 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.260612, loss=0.258838
I0315 18:01:39.523818 140460036412608 submission.py:265] 11000) loss = 0.259, grad_norm = 0.261
I0315 18:02:08.304157 140460036412608 spec.py:321] Evaluating on the training split.
I0315 18:02:10.372450 140460036412608 spec.py:333] Evaluating on the validation split.
I0315 18:02:12.570885 140460036412608 spec.py:349] Evaluating on the test split.
I0315 18:02:14.745070 140460036412608 submission_runner.py:469] Time since start: 1176.53s, 	Step: 11445, 	{'train/ssim': 0.7394564492361886, 'train/loss': 0.27380335330963135, 'validation/ssim': 0.7195140900877532, 'validation/loss': 0.2898092103527715, 'validation/num_examples': 3554, 'test/ssim': 0.7366889840695685, 'test/loss': 0.29132688554829306, 'test/num_examples': 3581, 'score': 940.4943060874939, 'total_duration': 1176.5331976413727, 'accumulated_submission_time': 940.4943060874939, 'accumulated_eval_time': 210.59468865394592, 'accumulated_logging_time': 3.6214599609375}
I0315 18:02:14.755574 140409921595136 logging_writer.py:48] [11445] accumulated_eval_time=210.595, accumulated_logging_time=3.62146, accumulated_submission_time=940.494, global_step=11445, preemption_count=0, score=940.494, test/loss=0.291327, test/num_examples=3581, test/ssim=0.736689, total_duration=1176.53, train/loss=0.273803, train/ssim=0.739456, validation/loss=0.289809, validation/num_examples=3554, validation/ssim=0.719514
I0315 18:02:19.150921 140409929987840 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.32943, loss=0.297979
I0315 18:02:19.154586 140460036412608 submission.py:265] 11500) loss = 0.298, grad_norm = 0.329
I0315 18:02:50.766739 140409921595136 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.730935, loss=0.219865
I0315 18:02:50.770262 140460036412608 submission.py:265] 12000) loss = 0.220, grad_norm = 0.731
I0315 18:03:22.380402 140409929987840 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.131246, loss=0.280755
I0315 18:03:22.383679 140460036412608 submission.py:265] 12500) loss = 0.281, grad_norm = 0.131
I0315 18:03:35.457394 140460036412608 spec.py:321] Evaluating on the training split.
I0315 18:03:37.421518 140460036412608 spec.py:333] Evaluating on the validation split.
I0315 18:03:39.498012 140460036412608 spec.py:349] Evaluating on the test split.
I0315 18:03:41.542905 140460036412608 submission_runner.py:469] Time since start: 1263.33s, 	Step: 12698, 	{'train/ssim': 0.7399905068533761, 'train/loss': 0.27337896823883057, 'validation/ssim': 0.7200404968433455, 'validation/loss': 0.2894399425053637, 'validation/num_examples': 3554, 'test/ssim': 0.7373290265681723, 'test/loss': 0.29092979058311225, 'test/num_examples': 3581, 'score': 1019.2969977855682, 'total_duration': 1263.3310191631317, 'accumulated_submission_time': 1019.2969977855682, 'accumulated_eval_time': 216.68041920661926, 'accumulated_logging_time': 3.640679359436035}
I0315 18:03:41.553514 140409921595136 logging_writer.py:48] [12698] accumulated_eval_time=216.68, accumulated_logging_time=3.64068, accumulated_submission_time=1019.3, global_step=12698, preemption_count=0, score=1019.3, test/loss=0.29093, test/num_examples=3581, test/ssim=0.737329, total_duration=1263.33, train/loss=0.273379, train/ssim=0.739991, validation/loss=0.28944, validation/num_examples=3554, validation/ssim=0.72004
I0315 18:04:01.646417 140409929987840 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.176686, loss=0.307533
I0315 18:04:01.649555 140460036412608 submission.py:265] 13000) loss = 0.308, grad_norm = 0.177
I0315 18:04:33.254285 140409921595136 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.296127, loss=0.289978
I0315 18:04:33.258085 140460036412608 submission.py:265] 13500) loss = 0.290, grad_norm = 0.296
I0315 18:05:02.246977 140460036412608 spec.py:321] Evaluating on the training split.
I0315 18:05:04.229940 140460036412608 spec.py:333] Evaluating on the validation split.
I0315 18:05:06.319024 140460036412608 spec.py:349] Evaluating on the test split.
I0315 18:05:08.386006 140460036412608 submission_runner.py:469] Time since start: 1350.17s, 	Step: 13950, 	{'train/ssim': 0.7402082170758929, 'train/loss': 0.2734752893447876, 'validation/ssim': 0.7201659331914744, 'validation/loss': 0.2894821896872362, 'validation/num_examples': 3554, 'test/ssim': 0.7374576759285116, 'test/loss': 0.2909083149347249, 'test/num_examples': 3581, 'score': 1098.1200709342957, 'total_duration': 1350.1741073131561, 'accumulated_submission_time': 1098.1200709342957, 'accumulated_eval_time': 222.8195185661316, 'accumulated_logging_time': 3.659877061843872}
I0315 18:05:08.397370 140409929987840 logging_writer.py:48] [13950] accumulated_eval_time=222.82, accumulated_logging_time=3.65988, accumulated_submission_time=1098.12, global_step=13950, preemption_count=0, score=1098.12, test/loss=0.290908, test/num_examples=3581, test/ssim=0.737458, total_duration=1350.17, train/loss=0.273475, train/ssim=0.740208, validation/loss=0.289482, validation/num_examples=3554, validation/ssim=0.720166
I0315 18:05:12.358796 140409921595136 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.141986, loss=0.322039
I0315 18:05:12.362441 140460036412608 submission.py:265] 14000) loss = 0.322, grad_norm = 0.142
I0315 18:05:43.923207 140409929987840 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.290743, loss=0.277096
I0315 18:05:43.926712 140460036412608 submission.py:265] 14500) loss = 0.277, grad_norm = 0.291
I0315 18:06:15.556678 140409921595136 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.178496, loss=0.240976
I0315 18:06:15.560129 140460036412608 submission.py:265] 15000) loss = 0.241, grad_norm = 0.178
I0315 18:06:29.065608 140460036412608 spec.py:321] Evaluating on the training split.
I0315 18:06:31.051716 140460036412608 spec.py:333] Evaluating on the validation split.
I0315 18:06:33.162180 140460036412608 spec.py:349] Evaluating on the test split.
I0315 18:06:35.257928 140460036412608 submission_runner.py:469] Time since start: 1437.05s, 	Step: 15205, 	{'train/ssim': 0.7405697958809989, 'train/loss': 0.27262565067836214, 'validation/ssim': 0.7205650488446117, 'validation/loss': 0.28865716748514, 'validation/num_examples': 3554, 'test/ssim': 0.7379178683939542, 'test/loss': 0.2900627879782184, 'test/num_examples': 3581, 'score': 1176.9815909862518, 'total_duration': 1437.046033859253, 'accumulated_submission_time': 1176.9815909862518, 'accumulated_eval_time': 229.0120177268982, 'accumulated_logging_time': 3.6799533367156982}
I0315 18:06:35.269137 140409929987840 logging_writer.py:48] [15205] accumulated_eval_time=229.012, accumulated_logging_time=3.67995, accumulated_submission_time=1176.98, global_step=15205, preemption_count=0, score=1176.98, test/loss=0.290063, test/num_examples=3581, test/ssim=0.737918, total_duration=1437.05, train/loss=0.272626, train/ssim=0.74057, validation/loss=0.288657, validation/num_examples=3554, validation/ssim=0.720565
I0315 18:06:54.716460 140409921595136 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.167139, loss=0.264963
I0315 18:06:54.719903 140460036412608 submission.py:265] 15500) loss = 0.265, grad_norm = 0.167
I0315 18:07:26.300796 140409929987840 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.175368, loss=0.21933
I0315 18:07:26.304284 140460036412608 submission.py:265] 16000) loss = 0.219, grad_norm = 0.175
I0315 18:07:56.021975 140460036412608 spec.py:321] Evaluating on the training split.
I0315 18:07:57.985994 140460036412608 spec.py:333] Evaluating on the validation split.
I0315 18:08:00.087473 140460036412608 spec.py:349] Evaluating on the test split.
I0315 18:08:02.168107 140460036412608 submission_runner.py:469] Time since start: 1523.96s, 	Step: 16461, 	{'train/ssim': 0.7412521498543876, 'train/loss': 0.27224484511784147, 'validation/ssim': 0.7211431826375211, 'validation/loss': 0.2884532132038724, 'validation/num_examples': 3554, 'test/ssim': 0.7383984456811994, 'test/loss': 0.28991654903919994, 'test/num_examples': 3581, 'score': 1255.8405680656433, 'total_duration': 1523.956220626831, 'accumulated_submission_time': 1255.8405680656433, 'accumulated_eval_time': 235.15829825401306, 'accumulated_logging_time': 3.6993603706359863}
I0315 18:08:02.178940 140409921595136 logging_writer.py:48] [16461] accumulated_eval_time=235.158, accumulated_logging_time=3.69936, accumulated_submission_time=1255.84, global_step=16461, preemption_count=0, score=1255.84, test/loss=0.289917, test/num_examples=3581, test/ssim=0.738398, total_duration=1523.96, train/loss=0.272245, train/ssim=0.741252, validation/loss=0.288453, validation/num_examples=3554, validation/ssim=0.721143
I0315 18:08:05.536618 140409929987840 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.220984, loss=0.253584
I0315 18:08:05.539911 140460036412608 submission.py:265] 16500) loss = 0.254, grad_norm = 0.221
I0315 18:08:37.169769 140409921595136 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.190404, loss=0.250232
I0315 18:08:37.173285 140460036412608 submission.py:265] 17000) loss = 0.250, grad_norm = 0.190
I0315 18:09:08.757358 140409929987840 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.331274, loss=0.296935
I0315 18:09:08.761024 140460036412608 submission.py:265] 17500) loss = 0.297, grad_norm = 0.331
I0315 18:09:22.859179 140460036412608 spec.py:321] Evaluating on the training split.
I0315 18:09:24.828311 140460036412608 spec.py:333] Evaluating on the validation split.
I0315 18:09:26.897022 140460036412608 spec.py:349] Evaluating on the test split.
I0315 18:09:28.933111 140460036412608 submission_runner.py:469] Time since start: 1610.72s, 	Step: 17714, 	{'train/ssim': 0.7406114169529506, 'train/loss': 0.27271839550563265, 'validation/ssim': 0.7204721737390968, 'validation/loss': 0.2888972207811621, 'validation/num_examples': 3554, 'test/ssim': 0.7377420407838243, 'test/loss': 0.2903613335791329, 'test/num_examples': 3581, 'score': 1334.6319365501404, 'total_duration': 1610.7212381362915, 'accumulated_submission_time': 1334.6319365501404, 'accumulated_eval_time': 241.2323398590088, 'accumulated_logging_time': 3.7195885181427}
I0315 18:09:28.944375 140409921595136 logging_writer.py:48] [17714] accumulated_eval_time=241.232, accumulated_logging_time=3.71959, accumulated_submission_time=1334.63, global_step=17714, preemption_count=0, score=1334.63, test/loss=0.290361, test/num_examples=3581, test/ssim=0.737742, total_duration=1610.72, train/loss=0.272718, train/ssim=0.740611, validation/loss=0.288897, validation/num_examples=3554, validation/ssim=0.720472
I0315 18:09:47.791629 140409929987840 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.14283, loss=0.237781
I0315 18:09:47.794772 140460036412608 submission.py:265] 18000) loss = 0.238, grad_norm = 0.143
I0315 18:10:19.390822 140409921595136 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.419293, loss=0.252481
I0315 18:10:19.394179 140460036412608 submission.py:265] 18500) loss = 0.252, grad_norm = 0.419
I0315 18:10:49.710021 140460036412608 spec.py:321] Evaluating on the training split.
I0315 18:10:51.688579 140460036412608 spec.py:333] Evaluating on the validation split.
I0315 18:10:53.783626 140460036412608 spec.py:349] Evaluating on the test split.
I0315 18:10:55.870890 140460036412608 submission_runner.py:469] Time since start: 1697.66s, 	Step: 18970, 	{'train/ssim': 0.7405034473964146, 'train/loss': 0.272862366267613, 'validation/ssim': 0.7203608884795301, 'validation/loss': 0.28903457564320134, 'validation/num_examples': 3554, 'test/ssim': 0.737623345216071, 'test/loss': 0.2905010616469736, 'test/num_examples': 3581, 'score': 1413.5417540073395, 'total_duration': 1697.6589727401733, 'accumulated_submission_time': 1413.5417540073395, 'accumulated_eval_time': 247.39331459999084, 'accumulated_logging_time': 3.739487886428833}
I0315 18:10:55.882892 140409929987840 logging_writer.py:48] [18970] accumulated_eval_time=247.393, accumulated_logging_time=3.73949, accumulated_submission_time=1413.54, global_step=18970, preemption_count=0, score=1413.54, test/loss=0.290501, test/num_examples=3581, test/ssim=0.737623, total_duration=1697.66, train/loss=0.272862, train/ssim=0.740503, validation/loss=0.289035, validation/num_examples=3554, validation/ssim=0.720361
I0315 18:10:58.683457 140409921595136 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.163241, loss=0.251577
I0315 18:10:58.687027 140460036412608 submission.py:265] 19000) loss = 0.252, grad_norm = 0.163
I0315 18:11:30.257337 140409929987840 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.309981, loss=0.236221
I0315 18:11:30.260635 140460036412608 submission.py:265] 19500) loss = 0.236, grad_norm = 0.310
I0315 18:12:01.854263 140409921595136 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.171615, loss=0.283717
I0315 18:12:01.858009 140460036412608 submission.py:265] 20000) loss = 0.284, grad_norm = 0.172
I0315 18:12:16.665888 140460036412608 spec.py:321] Evaluating on the training split.
I0315 18:12:18.652856 140460036412608 spec.py:333] Evaluating on the validation split.
I0315 18:12:20.776506 140460036412608 spec.py:349] Evaluating on the test split.
I0315 18:12:22.871143 140460036412608 submission_runner.py:469] Time since start: 1784.66s, 	Step: 20223, 	{'train/ssim': 0.7401413917541504, 'train/loss': 0.2728884390422276, 'validation/ssim': 0.7199046876099113, 'validation/loss': 0.28916008068593485, 'validation/num_examples': 3554, 'test/ssim': 0.7371773334962302, 'test/loss': 0.29064522119781483, 'test/num_examples': 3581, 'score': 1492.3515481948853, 'total_duration': 1784.6592514514923, 'accumulated_submission_time': 1492.3515481948853, 'accumulated_eval_time': 253.59871459007263, 'accumulated_logging_time': 3.7598989009857178}
I0315 18:12:22.882588 140409929987840 logging_writer.py:48] [20223] accumulated_eval_time=253.599, accumulated_logging_time=3.7599, accumulated_submission_time=1492.35, global_step=20223, preemption_count=0, score=1492.35, test/loss=0.290645, test/num_examples=3581, test/ssim=0.737177, total_duration=1784.66, train/loss=0.272888, train/ssim=0.740141, validation/loss=0.28916, validation/num_examples=3554, validation/ssim=0.719905
I0315 18:12:41.293303 140409921595136 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.207093, loss=0.336556
I0315 18:12:41.296649 140460036412608 submission.py:265] 20500) loss = 0.337, grad_norm = 0.207
I0315 18:13:12.793199 140409929987840 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.391597, loss=0.230666
I0315 18:13:12.796365 140460036412608 submission.py:265] 21000) loss = 0.231, grad_norm = 0.392
I0315 18:13:43.620883 140460036412608 spec.py:321] Evaluating on the training split.
I0315 18:13:45.605808 140460036412608 spec.py:333] Evaluating on the validation split.
I0315 18:13:47.697868 140460036412608 spec.py:349] Evaluating on the test split.
I0315 18:13:49.772651 140460036412608 submission_runner.py:469] Time since start: 1871.56s, 	Step: 21479, 	{'train/ssim': 0.7411462238856724, 'train/loss': 0.2722951684679304, 'validation/ssim': 0.7210982563660664, 'validation/loss': 0.2884151907401871, 'validation/num_examples': 3554, 'test/ssim': 0.7384469874642209, 'test/loss': 0.2898252264010577, 'test/num_examples': 3581, 'score': 1571.123851299286, 'total_duration': 1871.5607752799988, 'accumulated_submission_time': 1571.123851299286, 'accumulated_eval_time': 259.750780582428, 'accumulated_logging_time': 3.7798540592193604}
I0315 18:13:49.784751 140409921595136 logging_writer.py:48] [21479] accumulated_eval_time=259.751, accumulated_logging_time=3.77985, accumulated_submission_time=1571.12, global_step=21479, preemption_count=0, score=1571.12, test/loss=0.289825, test/num_examples=3581, test/ssim=0.738447, total_duration=1871.56, train/loss=0.272295, train/ssim=0.741146, validation/loss=0.288415, validation/num_examples=3554, validation/ssim=0.721098
I0315 18:13:52.005201 140409929987840 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.333708, loss=0.297276
I0315 18:13:52.009006 140460036412608 submission.py:265] 21500) loss = 0.297, grad_norm = 0.334
I0315 18:14:23.588085 140409921595136 logging_writer.py:48] [22000] global_step=22000, grad_norm=0.478515, loss=0.272338
I0315 18:14:23.591238 140460036412608 submission.py:265] 22000) loss = 0.272, grad_norm = 0.479
I0315 18:14:55.175130 140409929987840 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.191419, loss=0.269597
I0315 18:14:55.178771 140460036412608 submission.py:265] 22500) loss = 0.270, grad_norm = 0.191
I0315 18:15:10.466594 140460036412608 spec.py:321] Evaluating on the training split.
I0315 18:15:12.445774 140460036412608 spec.py:333] Evaluating on the validation split.
I0315 18:15:14.542411 140460036412608 spec.py:349] Evaluating on the test split.
I0315 18:15:16.588346 140460036412608 submission_runner.py:469] Time since start: 1958.38s, 	Step: 22734, 	{'train/ssim': 0.7392054966517857, 'train/loss': 0.27319610118865967, 'validation/ssim': 0.7191182030810355, 'validation/loss': 0.28930756800216306, 'validation/num_examples': 3554, 'test/ssim': 0.7364119822937029, 'test/loss': 0.29080697032733527, 'test/num_examples': 3581, 'score': 1649.9474656581879, 'total_duration': 1958.3764433860779, 'accumulated_submission_time': 1649.9474656581879, 'accumulated_eval_time': 265.87260341644287, 'accumulated_logging_time': 3.80045485496521}
I0315 18:15:16.600025 140409921595136 logging_writer.py:48] [22734] accumulated_eval_time=265.873, accumulated_logging_time=3.80045, accumulated_submission_time=1649.95, global_step=22734, preemption_count=0, score=1649.95, test/loss=0.290807, test/num_examples=3581, test/ssim=0.736412, total_duration=1958.38, train/loss=0.273196, train/ssim=0.739205, validation/loss=0.289308, validation/num_examples=3554, validation/ssim=0.719118
I0315 18:15:34.123590 140409929987840 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.221666, loss=0.222738
I0315 18:15:34.126886 140460036412608 submission.py:265] 23000) loss = 0.223, grad_norm = 0.222
I0315 18:16:05.719626 140409921595136 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.252577, loss=0.228417
I0315 18:16:05.723006 140460036412608 submission.py:265] 23500) loss = 0.228, grad_norm = 0.253
I0315 18:16:37.256481 140460036412608 spec.py:321] Evaluating on the training split.
I0315 18:16:39.263846 140460036412608 spec.py:333] Evaluating on the validation split.
I0315 18:16:41.384831 140460036412608 spec.py:349] Evaluating on the test split.
I0315 18:16:43.485996 140460036412608 submission_runner.py:469] Time since start: 2045.27s, 	Step: 23992, 	{'train/ssim': 0.7399381910051618, 'train/loss': 0.27330190794808523, 'validation/ssim': 0.7197383779720034, 'validation/loss': 0.28958598723489376, 'validation/num_examples': 3554, 'test/ssim': 0.7369161487058433, 'test/loss': 0.2911171059607128, 'test/num_examples': 3581, 'score': 1728.8207409381866, 'total_duration': 2045.2740807533264, 'accumulated_submission_time': 1728.8207409381866, 'accumulated_eval_time': 272.1022140979767, 'accumulated_logging_time': 3.819981575012207}
I0315 18:16:43.498168 140409929987840 logging_writer.py:48] [23992] accumulated_eval_time=272.102, accumulated_logging_time=3.81998, accumulated_submission_time=1728.82, global_step=23992, preemption_count=0, score=1728.82, test/loss=0.291117, test/num_examples=3581, test/ssim=0.736916, total_duration=2045.27, train/loss=0.273302, train/ssim=0.739938, validation/loss=0.289586, validation/num_examples=3554, validation/ssim=0.719738
I0315 18:16:44.825680 140409921595136 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.223218, loss=0.296655
I0315 18:16:44.828761 140460036412608 submission.py:265] 24000) loss = 0.297, grad_norm = 0.223
I0315 18:17:16.423874 140409929987840 logging_writer.py:48] [24500] global_step=24500, grad_norm=0.202454, loss=0.300831
I0315 18:17:16.427248 140460036412608 submission.py:265] 24500) loss = 0.301, grad_norm = 0.202
I0315 18:17:47.944417 140409921595136 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.197794, loss=0.337443
I0315 18:17:47.948199 140460036412608 submission.py:265] 25000) loss = 0.337, grad_norm = 0.198
I0315 18:18:04.219343 140460036412608 spec.py:321] Evaluating on the training split.
I0315 18:18:06.264779 140460036412608 spec.py:333] Evaluating on the validation split.
I0315 18:18:08.468645 140460036412608 spec.py:349] Evaluating on the test split.
I0315 18:18:10.642974 140460036412608 submission_runner.py:469] Time since start: 2132.43s, 	Step: 25248, 	{'train/ssim': 0.7396717071533203, 'train/loss': 0.27495689051491873, 'validation/ssim': 0.7197019698315279, 'validation/loss': 0.29098326984120004, 'validation/num_examples': 3554, 'test/ssim': 0.7369310793947221, 'test/loss': 0.2923903732917656, 'test/num_examples': 3581, 'score': 1807.6880269050598, 'total_duration': 2132.4310467243195, 'accumulated_submission_time': 1807.6880269050598, 'accumulated_eval_time': 278.52607345581055, 'accumulated_logging_time': 3.8400399684906006}
I0315 18:18:10.655579 140409929987840 logging_writer.py:48] [25248] accumulated_eval_time=278.526, accumulated_logging_time=3.84004, accumulated_submission_time=1807.69, global_step=25248, preemption_count=0, score=1807.69, test/loss=0.29239, test/num_examples=3581, test/ssim=0.736931, total_duration=2132.43, train/loss=0.274957, train/ssim=0.739672, validation/loss=0.290983, validation/num_examples=3554, validation/ssim=0.719702
I0315 18:18:27.368844 140409921595136 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.401149, loss=0.357376
I0315 18:18:27.372644 140460036412608 submission.py:265] 25500) loss = 0.357, grad_norm = 0.401
I0315 18:18:58.928913 140409929987840 logging_writer.py:48] [26000] global_step=26000, grad_norm=0.220194, loss=0.265012
I0315 18:18:58.932377 140460036412608 submission.py:265] 26000) loss = 0.265, grad_norm = 0.220
I0315 18:19:30.535841 140409921595136 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.325281, loss=0.275552
I0315 18:19:30.539128 140460036412608 submission.py:265] 26500) loss = 0.276, grad_norm = 0.325
I0315 18:19:31.393898 140460036412608 spec.py:321] Evaluating on the training split.
I0315 18:19:33.442384 140460036412608 spec.py:333] Evaluating on the validation split.
I0315 18:19:35.597359 140460036412608 spec.py:349] Evaluating on the test split.
I0315 18:19:37.728452 140460036412608 submission_runner.py:469] Time since start: 2219.52s, 	Step: 26503, 	{'train/ssim': 0.7414919308253697, 'train/loss': 0.2720154012952532, 'validation/ssim': 0.7213788738261466, 'validation/loss': 0.2884324330859595, 'validation/num_examples': 3554, 'test/ssim': 0.7386473586725076, 'test/loss': 0.28990945866639906, 'test/num_examples': 3581, 'score': 1886.4846031665802, 'total_duration': 2219.5165791511536, 'accumulated_submission_time': 1886.4846031665802, 'accumulated_eval_time': 284.8608076572418, 'accumulated_logging_time': 3.8614301681518555}
I0315 18:19:37.739956 140409929987840 logging_writer.py:48] [26503] accumulated_eval_time=284.861, accumulated_logging_time=3.86143, accumulated_submission_time=1886.48, global_step=26503, preemption_count=0, score=1886.48, test/loss=0.289909, test/num_examples=3581, test/ssim=0.738647, total_duration=2219.52, train/loss=0.272015, train/ssim=0.741492, validation/loss=0.288432, validation/num_examples=3554, validation/ssim=0.721379
I0315 18:20:09.948933 140409921595136 logging_writer.py:48] [27000] global_step=27000, grad_norm=0.309211, loss=0.322371
I0315 18:20:09.952788 140460036412608 submission.py:265] 27000) loss = 0.322, grad_norm = 0.309
I0315 18:20:41.568424 140409929987840 logging_writer.py:48] [27500] global_step=27500, grad_norm=0.171295, loss=0.305884
I0315 18:20:41.571579 140460036412608 submission.py:265] 27500) loss = 0.306, grad_norm = 0.171
I0315 18:20:58.388775 140460036412608 spec.py:321] Evaluating on the training split.
I0315 18:21:00.353789 140460036412608 spec.py:333] Evaluating on the validation split.
I0315 18:21:02.452446 140460036412608 spec.py:349] Evaluating on the test split.
I0315 18:21:04.524601 140460036412608 submission_runner.py:469] Time since start: 2306.31s, 	Step: 27758, 	{'train/ssim': 0.7389108112880162, 'train/loss': 0.2730696371623448, 'validation/ssim': 0.7185216591340743, 'validation/loss': 0.2894633673655564, 'validation/num_examples': 3554, 'test/ssim': 0.7359803558494484, 'test/loss': 0.2908841803965373, 'test/num_examples': 3581, 'score': 1965.2565398216248, 'total_duration': 2306.312670469284, 'accumulated_submission_time': 1965.2565398216248, 'accumulated_eval_time': 290.9966950416565, 'accumulated_logging_time': 3.880974531173706}
I0315 18:21:04.536943 140409921595136 logging_writer.py:48] [27758] accumulated_eval_time=290.997, accumulated_logging_time=3.88097, accumulated_submission_time=1965.26, global_step=27758, preemption_count=0, score=1965.26, test/loss=0.290884, test/num_examples=3581, test/ssim=0.73598, total_duration=2306.31, train/loss=0.27307, train/ssim=0.738911, validation/loss=0.289463, validation/num_examples=3554, validation/ssim=0.718522
I0315 18:21:20.556527 140409929987840 logging_writer.py:48] [28000] global_step=28000, grad_norm=0.262171, loss=0.293115
I0315 18:21:20.560333 140460036412608 submission.py:265] 28000) loss = 0.293, grad_norm = 0.262
I0315 18:21:52.049244 140409921595136 logging_writer.py:48] [28500] global_step=28500, grad_norm=0.0902267, loss=0.245996
I0315 18:21:52.052542 140460036412608 submission.py:265] 28500) loss = 0.246, grad_norm = 0.090
I0315 18:22:23.610643 140409929987840 logging_writer.py:48] [29000] global_step=29000, grad_norm=0.31347, loss=0.430393
I0315 18:22:23.614662 140460036412608 submission.py:265] 29000) loss = 0.430, grad_norm = 0.313
I0315 18:22:25.193523 140460036412608 spec.py:321] Evaluating on the training split.
I0315 18:22:27.201873 140460036412608 spec.py:333] Evaluating on the validation split.
I0315 18:22:29.321555 140460036412608 spec.py:349] Evaluating on the test split.
I0315 18:22:31.408685 140460036412608 submission_runner.py:469] Time since start: 2393.20s, 	Step: 29016, 	{'train/ssim': 0.7407393455505371, 'train/loss': 0.27317612511771067, 'validation/ssim': 0.720663282129291, 'validation/loss': 0.28934950605831455, 'validation/num_examples': 3554, 'test/ssim': 0.737837828993298, 'test/loss': 0.2907548492695825, 'test/num_examples': 3581, 'score': 2044.146336555481, 'total_duration': 2393.1968190670013, 'accumulated_submission_time': 2044.146336555481, 'accumulated_eval_time': 297.21201252937317, 'accumulated_logging_time': 3.901247978210449}
I0315 18:22:31.420258 140409921595136 logging_writer.py:48] [29016] accumulated_eval_time=297.212, accumulated_logging_time=3.90125, accumulated_submission_time=2044.15, global_step=29016, preemption_count=0, score=2044.15, test/loss=0.290755, test/num_examples=3581, test/ssim=0.737838, total_duration=2393.2, train/loss=0.273176, train/ssim=0.740739, validation/loss=0.28935, validation/num_examples=3554, validation/ssim=0.720663
I0315 18:23:02.725314 140409929987840 logging_writer.py:48] [29500] global_step=29500, grad_norm=0.206633, loss=0.261382
I0315 18:23:02.728514 140460036412608 submission.py:265] 29500) loss = 0.261, grad_norm = 0.207
I0315 18:23:34.252317 140409921595136 logging_writer.py:48] [30000] global_step=30000, grad_norm=0.165259, loss=0.305869
I0315 18:23:34.256128 140460036412608 submission.py:265] 30000) loss = 0.306, grad_norm = 0.165
I0315 18:23:52.183362 140460036412608 spec.py:321] Evaluating on the training split.
I0315 18:23:54.150814 140460036412608 spec.py:333] Evaluating on the validation split.
I0315 18:23:56.236591 140460036412608 spec.py:349] Evaluating on the test split.
I0315 18:23:58.317398 140460036412608 submission_runner.py:469] Time since start: 2480.11s, 	Step: 30275, 	{'train/ssim': 0.7402591024126325, 'train/loss': 0.2729674407414028, 'validation/ssim': 0.7200935290781514, 'validation/loss': 0.2892322787154439, 'validation/num_examples': 3554, 'test/ssim': 0.7375441921120148, 'test/loss': 0.2906247000226892, 'test/num_examples': 3581, 'score': 2123.0726025104523, 'total_duration': 2480.105504989624, 'accumulated_submission_time': 2123.0726025104523, 'accumulated_eval_time': 303.34617829322815, 'accumulated_logging_time': 3.9214792251586914}
I0315 18:23:58.329383 140409929987840 logging_writer.py:48] [30275] accumulated_eval_time=303.346, accumulated_logging_time=3.92148, accumulated_submission_time=2123.07, global_step=30275, preemption_count=0, score=2123.07, test/loss=0.290625, test/num_examples=3581, test/ssim=0.737544, total_duration=2480.11, train/loss=0.272967, train/ssim=0.740259, validation/loss=0.289232, validation/num_examples=3554, validation/ssim=0.720094
I0315 18:24:13.429376 140409921595136 logging_writer.py:48] [30500] global_step=30500, grad_norm=0.11188, loss=0.329244
I0315 18:24:13.432676 140460036412608 submission.py:265] 30500) loss = 0.329, grad_norm = 0.112
I0315 18:24:44.929672 140409929987840 logging_writer.py:48] [31000] global_step=31000, grad_norm=0.128255, loss=0.437714
I0315 18:24:44.933079 140460036412608 submission.py:265] 31000) loss = 0.438, grad_norm = 0.128
I0315 18:25:16.508115 140409921595136 logging_writer.py:48] [31500] global_step=31500, grad_norm=0.148239, loss=0.356992
I0315 18:25:16.511734 140460036412608 submission.py:265] 31500) loss = 0.357, grad_norm = 0.148
I0315 18:25:19.075343 140460036412608 spec.py:321] Evaluating on the training split.
I0315 18:25:21.040987 140460036412608 spec.py:333] Evaluating on the validation split.
I0315 18:25:23.119036 140460036412608 spec.py:349] Evaluating on the test split.
I0315 18:25:25.151484 140460036412608 submission_runner.py:469] Time since start: 2566.94s, 	Step: 31531, 	{'train/ssim': 0.7412257875714984, 'train/loss': 0.2727081435067313, 'validation/ssim': 0.7214276469954629, 'validation/loss': 0.28872280517990295, 'validation/num_examples': 3554, 'test/ssim': 0.7387331930893954, 'test/loss': 0.29012820348497276, 'test/num_examples': 3581, 'score': 2201.898449897766, 'total_duration': 2566.939565181732, 'accumulated_submission_time': 2201.898449897766, 'accumulated_eval_time': 309.422420501709, 'accumulated_logging_time': 3.9418528079986572}
I0315 18:25:25.163319 140409929987840 logging_writer.py:48] [31531] accumulated_eval_time=309.422, accumulated_logging_time=3.94185, accumulated_submission_time=2201.9, global_step=31531, preemption_count=0, score=2201.9, test/loss=0.290128, test/num_examples=3581, test/ssim=0.738733, total_duration=2566.94, train/loss=0.272708, train/ssim=0.741226, validation/loss=0.288723, validation/num_examples=3554, validation/ssim=0.721428
I0315 18:25:55.568664 140409921595136 logging_writer.py:48] [32000] global_step=32000, grad_norm=0.132319, loss=0.317565
I0315 18:25:55.572479 140460036412608 submission.py:265] 32000) loss = 0.318, grad_norm = 0.132
I0315 18:26:27.061548 140409929987840 logging_writer.py:48] [32500] global_step=32500, grad_norm=0.386218, loss=0.30435
I0315 18:26:27.064862 140460036412608 submission.py:265] 32500) loss = 0.304, grad_norm = 0.386
I0315 18:26:45.926244 140460036412608 spec.py:321] Evaluating on the training split.
I0315 18:26:47.938932 140460036412608 spec.py:333] Evaluating on the validation split.
I0315 18:26:50.132901 140460036412608 spec.py:349] Evaluating on the test split.
I0315 18:26:52.300734 140460036412608 submission_runner.py:469] Time since start: 2654.09s, 	Step: 32790, 	{'train/ssim': 0.7408412524632045, 'train/loss': 0.27228948048182894, 'validation/ssim': 0.7205401127031162, 'validation/loss': 0.2886202097878271, 'validation/num_examples': 3554, 'test/ssim': 0.737864758774609, 'test/loss': 0.2900451302228777, 'test/num_examples': 3581, 'score': 2280.737871170044, 'total_duration': 2654.0887875556946, 'accumulated_submission_time': 2280.737871170044, 'accumulated_eval_time': 315.79712653160095, 'accumulated_logging_time': 3.9616944789886475}
I0315 18:26:52.314536 140409921595136 logging_writer.py:48] [32790] accumulated_eval_time=315.797, accumulated_logging_time=3.96169, accumulated_submission_time=2280.74, global_step=32790, preemption_count=0, score=2280.74, test/loss=0.290045, test/num_examples=3581, test/ssim=0.737865, total_duration=2654.09, train/loss=0.272289, train/ssim=0.740841, validation/loss=0.28862, validation/num_examples=3554, validation/ssim=0.72054
I0315 18:27:06.443862 140409929987840 logging_writer.py:48] [33000] global_step=33000, grad_norm=0.50691, loss=0.311045
I0315 18:27:06.447263 140460036412608 submission.py:265] 33000) loss = 0.311, grad_norm = 0.507
I0315 18:27:37.900817 140409921595136 logging_writer.py:48] [33500] global_step=33500, grad_norm=0.176831, loss=0.326006
I0315 18:27:37.904737 140460036412608 submission.py:265] 33500) loss = 0.326, grad_norm = 0.177
I0315 18:28:09.430556 140409929987840 logging_writer.py:48] [34000] global_step=34000, grad_norm=0.208801, loss=0.264749
I0315 18:28:09.433730 140460036412608 submission.py:265] 34000) loss = 0.265, grad_norm = 0.209
I0315 18:28:13.017823 140460036412608 spec.py:321] Evaluating on the training split.
I0315 18:28:15.120048 140460036412608 spec.py:333] Evaluating on the validation split.
I0315 18:28:17.347731 140460036412608 spec.py:349] Evaluating on the test split.
I0315 18:28:19.551615 140460036412608 submission_runner.py:469] Time since start: 2741.34s, 	Step: 34048, 	{'train/ssim': 0.7415069852556501, 'train/loss': 0.2721993752888271, 'validation/ssim': 0.7214261357141601, 'validation/loss': 0.288579748665676, 'validation/num_examples': 3554, 'test/ssim': 0.7385859996771154, 'test/loss': 0.29009530824577634, 'test/num_examples': 3581, 'score': 2359.5792620182037, 'total_duration': 2741.3397347927094, 'accumulated_submission_time': 2359.5792620182037, 'accumulated_eval_time': 322.33106756210327, 'accumulated_logging_time': 3.984361410140991}
I0315 18:28:19.563866 140409921595136 logging_writer.py:48] [34048] accumulated_eval_time=322.331, accumulated_logging_time=3.98436, accumulated_submission_time=2359.58, global_step=34048, preemption_count=0, score=2359.58, test/loss=0.290095, test/num_examples=3581, test/ssim=0.738586, total_duration=2741.34, train/loss=0.272199, train/ssim=0.741507, validation/loss=0.28858, validation/num_examples=3554, validation/ssim=0.721426
I0315 18:28:48.935658 140409929987840 logging_writer.py:48] [34500] global_step=34500, grad_norm=0.156679, loss=0.23634
I0315 18:28:48.939102 140460036412608 submission.py:265] 34500) loss = 0.236, grad_norm = 0.157
I0315 18:29:20.455525 140409921595136 logging_writer.py:48] [35000] global_step=35000, grad_norm=0.303248, loss=0.275575
I0315 18:29:20.458910 140460036412608 submission.py:265] 35000) loss = 0.276, grad_norm = 0.303
I0315 18:29:40.295576 140460036412608 spec.py:321] Evaluating on the training split.
I0315 18:29:42.263351 140460036412608 spec.py:333] Evaluating on the validation split.
I0315 18:29:44.398103 140460036412608 spec.py:349] Evaluating on the test split.
I0315 18:29:46.521040 140460036412608 submission_runner.py:469] Time since start: 2828.31s, 	Step: 35303, 	{'train/ssim': 0.7402283804757255, 'train/loss': 0.2740968806403024, 'validation/ssim': 0.7200670129607485, 'validation/loss': 0.2901994644019063, 'validation/num_examples': 3554, 'test/ssim': 0.7373731368681933, 'test/loss': 0.29160838698382086, 'test/num_examples': 3581, 'score': 2438.478771209717, 'total_duration': 2828.309157848358, 'accumulated_submission_time': 2438.478771209717, 'accumulated_eval_time': 328.5567319393158, 'accumulated_logging_time': 4.005563020706177}
I0315 18:29:46.533581 140409929987840 logging_writer.py:48] [35303] accumulated_eval_time=328.557, accumulated_logging_time=4.00556, accumulated_submission_time=2438.48, global_step=35303, preemption_count=0, score=2438.48, test/loss=0.291608, test/num_examples=3581, test/ssim=0.737373, total_duration=2828.31, train/loss=0.274097, train/ssim=0.740228, validation/loss=0.290199, validation/num_examples=3554, validation/ssim=0.720067
I0315 18:29:59.843088 140409921595136 logging_writer.py:48] [35500] global_step=35500, grad_norm=0.230074, loss=0.242038
I0315 18:29:59.846455 140460036412608 submission.py:265] 35500) loss = 0.242, grad_norm = 0.230
I0315 18:30:31.480098 140409929987840 logging_writer.py:48] [36000] global_step=36000, grad_norm=0.320282, loss=0.232713
I0315 18:30:31.483816 140460036412608 submission.py:265] 36000) loss = 0.233, grad_norm = 0.320
I0315 18:31:03.014788 140409921595136 logging_writer.py:48] [36500] global_step=36500, grad_norm=0.321059, loss=0.313744
I0315 18:31:03.018444 140460036412608 submission.py:265] 36500) loss = 0.314, grad_norm = 0.321
I0315 18:31:07.217167 140460036412608 spec.py:321] Evaluating on the training split.
I0315 18:31:09.172828 140460036412608 spec.py:333] Evaluating on the validation split.
I0315 18:31:11.293306 140460036412608 spec.py:349] Evaluating on the test split.
I0315 18:31:13.389679 140460036412608 submission_runner.py:469] Time since start: 2915.18s, 	Step: 36557, 	{'train/ssim': 0.7414048058646066, 'train/loss': 0.2718815803527832, 'validation/ssim': 0.7214876860799452, 'validation/loss': 0.28789263088245637, 'validation/num_examples': 3554, 'test/ssim': 0.7388535930736875, 'test/loss': 0.2893171057425475, 'test/num_examples': 3581, 'score': 2517.2554194927216, 'total_duration': 2915.176987886429, 'accumulated_submission_time': 2517.2554194927216, 'accumulated_eval_time': 334.7286117076874, 'accumulated_logging_time': 4.02627968788147}
I0315 18:31:13.402171 140409929987840 logging_writer.py:48] [36557] accumulated_eval_time=334.729, accumulated_logging_time=4.02628, accumulated_submission_time=2517.26, global_step=36557, preemption_count=0, score=2517.26, test/loss=0.289317, test/num_examples=3581, test/ssim=0.738854, total_duration=2915.18, train/loss=0.271882, train/ssim=0.741405, validation/loss=0.287893, validation/num_examples=3554, validation/ssim=0.721488
I0315 18:31:42.291681 140409921595136 logging_writer.py:48] [37000] global_step=37000, grad_norm=0.427557, loss=0.322801
I0315 18:31:42.294990 140460036412608 submission.py:265] 37000) loss = 0.323, grad_norm = 0.428
I0315 18:32:13.911926 140409929987840 logging_writer.py:48] [37500] global_step=37500, grad_norm=0.201195, loss=0.342673
I0315 18:32:13.915148 140460036412608 submission.py:265] 37500) loss = 0.343, grad_norm = 0.201
I0315 18:32:34.135223 140460036412608 spec.py:321] Evaluating on the training split.
I0315 18:32:36.126419 140460036412608 spec.py:333] Evaluating on the validation split.
I0315 18:32:38.310503 140460036412608 spec.py:349] Evaluating on the test split.
I0315 18:32:40.464594 140460036412608 submission_runner.py:469] Time since start: 3002.25s, 	Step: 37810, 	{'train/ssim': 0.7417947224208287, 'train/loss': 0.272100601877485, 'validation/ssim': 0.7219297358610017, 'validation/loss': 0.2881921737061234, 'validation/num_examples': 3554, 'test/ssim': 0.7391873178319603, 'test/loss': 0.2896132651603079, 'test/num_examples': 3581, 'score': 2596.092001438141, 'total_duration': 3002.2527136802673, 'accumulated_submission_time': 2596.092001438141, 'accumulated_eval_time': 341.05812191963196, 'accumulated_logging_time': 4.046571969985962}
I0315 18:32:40.476877 140409921595136 logging_writer.py:48] [37810] accumulated_eval_time=341.058, accumulated_logging_time=4.04657, accumulated_submission_time=2596.09, global_step=37810, preemption_count=0, score=2596.09, test/loss=0.289613, test/num_examples=3581, test/ssim=0.739187, total_duration=3002.25, train/loss=0.272101, train/ssim=0.741795, validation/loss=0.288192, validation/num_examples=3554, validation/ssim=0.72193
I0315 18:32:53.334539 140409929987840 logging_writer.py:48] [38000] global_step=38000, grad_norm=0.137765, loss=0.240679
I0315 18:32:53.338200 140460036412608 submission.py:265] 38000) loss = 0.241, grad_norm = 0.138
I0315 18:33:24.856980 140409921595136 logging_writer.py:48] [38500] global_step=38500, grad_norm=0.188497, loss=0.314427
I0315 18:33:24.860488 140460036412608 submission.py:265] 38500) loss = 0.314, grad_norm = 0.188
I0315 18:33:56.503235 140409929987840 logging_writer.py:48] [39000] global_step=39000, grad_norm=0.128027, loss=0.298495
I0315 18:33:56.506945 140460036412608 submission.py:265] 39000) loss = 0.298, grad_norm = 0.128
I0315 18:34:01.205087 140460036412608 spec.py:321] Evaluating on the training split.
I0315 18:34:03.180737 140460036412608 spec.py:333] Evaluating on the validation split.
I0315 18:34:05.391334 140460036412608 spec.py:349] Evaluating on the test split.
I0315 18:34:07.541939 140460036412608 submission_runner.py:469] Time since start: 3089.33s, 	Step: 39065, 	{'train/ssim': 0.7402568544660296, 'train/loss': 0.2739033358437674, 'validation/ssim': 0.7202379251371693, 'validation/loss': 0.2901022958835995, 'validation/num_examples': 3554, 'test/ssim': 0.7373097325729545, 'test/loss': 0.2915828548240715, 'test/num_examples': 3581, 'score': 2674.911098718643, 'total_duration': 3089.330013036728, 'accumulated_submission_time': 2674.911098718643, 'accumulated_eval_time': 347.3951394557953, 'accumulated_logging_time': 4.066999912261963}
I0315 18:34:07.554816 140409921595136 logging_writer.py:48] [39065] accumulated_eval_time=347.395, accumulated_logging_time=4.067, accumulated_submission_time=2674.91, global_step=39065, preemption_count=0, score=2674.91, test/loss=0.291583, test/num_examples=3581, test/ssim=0.73731, total_duration=3089.33, train/loss=0.273903, train/ssim=0.740257, validation/loss=0.290102, validation/num_examples=3554, validation/ssim=0.720238
I0315 18:34:35.907901 140409929987840 logging_writer.py:48] [39500] global_step=39500, grad_norm=0.205216, loss=0.310531
I0315 18:34:35.911093 140460036412608 submission.py:265] 39500) loss = 0.311, grad_norm = 0.205
I0315 18:35:07.443858 140409921595136 logging_writer.py:48] [40000] global_step=40000, grad_norm=0.0923924, loss=0.253245
I0315 18:35:07.447568 140460036412608 submission.py:265] 40000) loss = 0.253, grad_norm = 0.092
I0315 18:35:28.217295 140460036412608 spec.py:321] Evaluating on the training split.
I0315 18:35:30.185915 140460036412608 spec.py:333] Evaluating on the validation split.
I0315 18:35:32.311571 140460036412608 spec.py:349] Evaluating on the test split.
I0315 18:35:34.407093 140460036412608 submission_runner.py:469] Time since start: 3176.20s, 	Step: 40321, 	{'train/ssim': 0.7416892051696777, 'train/loss': 0.27255620275224957, 'validation/ssim': 0.7216995402407499, 'validation/loss': 0.28875931636228547, 'validation/num_examples': 3554, 'test/ssim': 0.7390789169401005, 'test/loss': 0.29011654527584824, 'test/num_examples': 3581, 'score': 2753.716668844223, 'total_duration': 3176.195204257965, 'accumulated_submission_time': 2753.716668844223, 'accumulated_eval_time': 353.58499336242676, 'accumulated_logging_time': 4.088775634765625}
I0315 18:35:34.419983 140409929987840 logging_writer.py:48] [40321] accumulated_eval_time=353.585, accumulated_logging_time=4.08878, accumulated_submission_time=2753.72, global_step=40321, preemption_count=0, score=2753.72, test/loss=0.290117, test/num_examples=3581, test/ssim=0.739079, total_duration=3176.2, train/loss=0.272556, train/ssim=0.741689, validation/loss=0.288759, validation/num_examples=3554, validation/ssim=0.7217
I0315 18:35:46.503222 140409921595136 logging_writer.py:48] [40500] global_step=40500, grad_norm=0.323651, loss=0.278098
I0315 18:35:46.506527 140460036412608 submission.py:265] 40500) loss = 0.278, grad_norm = 0.324
I0315 18:36:17.938597 140409929987840 logging_writer.py:48] [41000] global_step=41000, grad_norm=0.236205, loss=0.2357
I0315 18:36:17.941879 140460036412608 submission.py:265] 41000) loss = 0.236, grad_norm = 0.236
I0315 18:36:49.352204 140409921595136 logging_writer.py:48] [41500] global_step=41500, grad_norm=0.324703, loss=0.283749
I0315 18:36:49.355748 140460036412608 submission.py:265] 41500) loss = 0.284, grad_norm = 0.325
I0315 18:36:55.127480 140460036412608 spec.py:321] Evaluating on the training split.
I0315 18:36:57.093424 140460036412608 spec.py:333] Evaluating on the validation split.
I0315 18:36:59.242073 140460036412608 spec.py:349] Evaluating on the test split.
I0315 18:37:01.385085 140460036412608 submission_runner.py:469] Time since start: 3263.17s, 	Step: 41582, 	{'train/ssim': 0.7436620848519462, 'train/loss': 0.27114457743508474, 'validation/ssim': 0.723527641060249, 'validation/loss': 0.2876620059209254, 'validation/num_examples': 3554, 'test/ssim': 0.7407767885192683, 'test/loss': 0.28904681936784415, 'test/num_examples': 3581, 'score': 2832.5903844833374, 'total_duration': 3263.1732048988342, 'accumulated_submission_time': 2832.5903844833374, 'accumulated_eval_time': 359.8427515029907, 'accumulated_logging_time': 4.109808921813965}
I0315 18:37:01.397879 140409929987840 logging_writer.py:48] [41582] accumulated_eval_time=359.843, accumulated_logging_time=4.10981, accumulated_submission_time=2832.59, global_step=41582, preemption_count=0, score=2832.59, test/loss=0.289047, test/num_examples=3581, test/ssim=0.740777, total_duration=3263.17, train/loss=0.271145, train/ssim=0.743662, validation/loss=0.287662, validation/num_examples=3554, validation/ssim=0.723528
I0315 18:37:28.680286 140409921595136 logging_writer.py:48] [42000] global_step=42000, grad_norm=0.486215, loss=0.310002
I0315 18:37:28.684271 140460036412608 submission.py:265] 42000) loss = 0.310, grad_norm = 0.486
I0315 18:38:00.169202 140409929987840 logging_writer.py:48] [42500] global_step=42500, grad_norm=0.321482, loss=0.286345
I0315 18:38:00.172656 140460036412608 submission.py:265] 42500) loss = 0.286, grad_norm = 0.321
I0315 18:38:22.116004 140460036412608 spec.py:321] Evaluating on the training split.
I0315 18:38:24.103693 140460036412608 spec.py:333] Evaluating on the validation split.
I0315 18:38:26.290961 140460036412608 spec.py:349] Evaluating on the test split.
I0315 18:38:28.431084 140460036412608 submission_runner.py:469] Time since start: 3350.22s, 	Step: 42838, 	{'train/ssim': 0.7423267364501953, 'train/loss': 0.27204515252794537, 'validation/ssim': 0.7219940340109735, 'validation/loss': 0.2884114125369302, 'validation/num_examples': 3554, 'test/ssim': 0.7393313751178092, 'test/loss': 0.28978670658728356, 'test/num_examples': 3581, 'score': 2911.372321367264, 'total_duration': 3350.219212770462, 'accumulated_submission_time': 2911.372321367264, 'accumulated_eval_time': 366.1580231189728, 'accumulated_logging_time': 4.131276607513428}
I0315 18:38:28.443448 140409921595136 logging_writer.py:48] [42838] accumulated_eval_time=366.158, accumulated_logging_time=4.13128, accumulated_submission_time=2911.37, global_step=42838, preemption_count=0, score=2911.37, test/loss=0.289787, test/num_examples=3581, test/ssim=0.739331, total_duration=3350.22, train/loss=0.272045, train/ssim=0.742327, validation/loss=0.288411, validation/num_examples=3554, validation/ssim=0.721994
I0315 18:38:39.542382 140409929987840 logging_writer.py:48] [43000] global_step=43000, grad_norm=0.316935, loss=0.305558
I0315 18:38:39.545912 140460036412608 submission.py:265] 43000) loss = 0.306, grad_norm = 0.317
I0315 18:39:11.018596 140409921595136 logging_writer.py:48] [43500] global_step=43500, grad_norm=0.231195, loss=0.224813
I0315 18:39:11.021904 140460036412608 submission.py:265] 43500) loss = 0.225, grad_norm = 0.231
I0315 18:39:42.609532 140409929987840 logging_writer.py:48] [44000] global_step=44000, grad_norm=0.616388, loss=0.287704
I0315 18:39:42.612829 140460036412608 submission.py:265] 44000) loss = 0.288, grad_norm = 0.616
I0315 18:39:49.108132 140460036412608 spec.py:321] Evaluating on the training split.
I0315 18:39:51.074377 140460036412608 spec.py:333] Evaluating on the validation split.
I0315 18:39:53.229018 140460036412608 spec.py:349] Evaluating on the test split.
I0315 18:39:55.364248 140460036412608 submission_runner.py:469] Time since start: 3437.15s, 	Step: 44095, 	{'train/ssim': 0.7428388595581055, 'train/loss': 0.271152138710022, 'validation/ssim': 0.7224198718653277, 'validation/loss': 0.287782908425146, 'validation/num_examples': 3554, 'test/ssim': 0.7397805911407428, 'test/loss': 0.289183070425946, 'test/num_examples': 3581, 'score': 2990.1713683605194, 'total_duration': 3437.152360677719, 'accumulated_submission_time': 2990.1713683605194, 'accumulated_eval_time': 372.4141926765442, 'accumulated_logging_time': 4.151675701141357}
I0315 18:39:55.377266 140409921595136 logging_writer.py:48] [44095] accumulated_eval_time=372.414, accumulated_logging_time=4.15168, accumulated_submission_time=2990.17, global_step=44095, preemption_count=0, score=2990.17, test/loss=0.289183, test/num_examples=3581, test/ssim=0.739781, total_duration=3437.15, train/loss=0.271152, train/ssim=0.742839, validation/loss=0.287783, validation/num_examples=3554, validation/ssim=0.72242
I0315 18:40:21.780371 140409929987840 logging_writer.py:48] [44500] global_step=44500, grad_norm=0.382461, loss=0.26057
I0315 18:40:21.783645 140460036412608 submission.py:265] 44500) loss = 0.261, grad_norm = 0.382
I0315 18:40:53.451731 140409921595136 logging_writer.py:48] [45000] global_step=45000, grad_norm=0.430658, loss=0.238543
I0315 18:40:53.454933 140460036412608 submission.py:265] 45000) loss = 0.239, grad_norm = 0.431
I0315 18:41:16.048372 140460036412608 spec.py:321] Evaluating on the training split.
I0315 18:41:18.032397 140460036412608 spec.py:333] Evaluating on the validation split.
I0315 18:41:20.196146 140460036412608 spec.py:349] Evaluating on the test split.
I0315 18:41:22.290397 140460036412608 submission_runner.py:469] Time since start: 3524.08s, 	Step: 45349, 	{'train/ssim': 0.7423931530543736, 'train/loss': 0.27129437242235455, 'validation/ssim': 0.7220201379607485, 'validation/loss': 0.28767790872190663, 'validation/num_examples': 3554, 'test/ssim': 0.7394305721603602, 'test/loss': 0.28903103647069606, 'test/num_examples': 3581, 'score': 3069.061467409134, 'total_duration': 3524.0785105228424, 'accumulated_submission_time': 3069.061467409134, 'accumulated_eval_time': 378.65626859664917, 'accumulated_logging_time': 4.17361044883728}
I0315 18:41:22.303437 140409929987840 logging_writer.py:48] [45349] accumulated_eval_time=378.656, accumulated_logging_time=4.17361, accumulated_submission_time=3069.06, global_step=45349, preemption_count=0, score=3069.06, test/loss=0.289031, test/num_examples=3581, test/ssim=0.739431, total_duration=3524.08, train/loss=0.271294, train/ssim=0.742393, validation/loss=0.287678, validation/num_examples=3554, validation/ssim=0.72202
I0315 18:41:32.681081 140409921595136 logging_writer.py:48] [45500] global_step=45500, grad_norm=0.210713, loss=0.313663
I0315 18:41:32.684811 140460036412608 submission.py:265] 45500) loss = 0.314, grad_norm = 0.211
I0315 18:42:04.230717 140409929987840 logging_writer.py:48] [46000] global_step=46000, grad_norm=0.39441, loss=0.295082
I0315 18:42:04.234407 140460036412608 submission.py:265] 46000) loss = 0.295, grad_norm = 0.394
I0315 18:42:35.706591 140409921595136 logging_writer.py:48] [46500] global_step=46500, grad_norm=0.26796, loss=0.256237
I0315 18:42:35.709679 140460036412608 submission.py:265] 46500) loss = 0.256, grad_norm = 0.268
I0315 18:42:43.022533 140460036412608 spec.py:321] Evaluating on the training split.
I0315 18:42:44.997444 140460036412608 spec.py:333] Evaluating on the validation split.
I0315 18:42:47.150152 140460036412608 spec.py:349] Evaluating on the test split.
I0315 18:42:49.297323 140460036412608 submission_runner.py:469] Time since start: 3611.09s, 	Step: 46606, 	{'train/ssim': 0.7430729184831891, 'train/loss': 0.27081429958343506, 'validation/ssim': 0.7227237081017868, 'validation/loss': 0.28726101833981077, 'validation/num_examples': 3554, 'test/ssim': 0.7400432758176836, 'test/loss': 0.28865009937430186, 'test/num_examples': 3581, 'score': 3147.928908109665, 'total_duration': 3611.0854363441467, 'accumulated_submission_time': 3147.928908109665, 'accumulated_eval_time': 384.9312267303467, 'accumulated_logging_time': 4.1955389976501465}
I0315 18:42:49.310135 140409929987840 logging_writer.py:48] [46606] accumulated_eval_time=384.931, accumulated_logging_time=4.19554, accumulated_submission_time=3147.93, global_step=46606, preemption_count=0, score=3147.93, test/loss=0.28865, test/num_examples=3581, test/ssim=0.740043, total_duration=3611.09, train/loss=0.270814, train/ssim=0.743073, validation/loss=0.287261, validation/num_examples=3554, validation/ssim=0.722724
I0315 18:43:14.924823 140409921595136 logging_writer.py:48] [47000] global_step=47000, grad_norm=0.181939, loss=0.283982
I0315 18:43:14.928632 140460036412608 submission.py:265] 47000) loss = 0.284, grad_norm = 0.182
I0315 18:43:46.412084 140409929987840 logging_writer.py:48] [47500] global_step=47500, grad_norm=0.199418, loss=0.243629
I0315 18:43:46.415698 140460036412608 submission.py:265] 47500) loss = 0.244, grad_norm = 0.199
I0315 18:44:10.085903 140460036412608 spec.py:321] Evaluating on the training split.
I0315 18:44:12.139242 140460036412608 spec.py:333] Evaluating on the validation split.
I0315 18:44:14.301836 140460036412608 spec.py:349] Evaluating on the test split.
I0315 18:44:16.418374 140460036412608 submission_runner.py:469] Time since start: 3698.21s, 	Step: 47866, 	{'train/ssim': 0.7438257081168038, 'train/loss': 0.27023516382489887, 'validation/ssim': 0.7236622824854038, 'validation/loss': 0.28664601271784435, 'validation/num_examples': 3554, 'test/ssim': 0.7409781142008168, 'test/loss': 0.28801278394216, 'test/num_examples': 3581, 'score': 3226.8005199432373, 'total_duration': 3698.206425666809, 'accumulated_submission_time': 3226.8005199432373, 'accumulated_eval_time': 391.26386308670044, 'accumulated_logging_time': 4.2176713943481445}
I0315 18:44:16.433035 140409921595136 logging_writer.py:48] [47866] accumulated_eval_time=391.264, accumulated_logging_time=4.21767, accumulated_submission_time=3226.8, global_step=47866, preemption_count=0, score=3226.8, test/loss=0.288013, test/num_examples=3581, test/ssim=0.740978, total_duration=3698.21, train/loss=0.270235, train/ssim=0.743826, validation/loss=0.286646, validation/num_examples=3554, validation/ssim=0.723662
I0315 18:44:17.182831 140409929987840 logging_writer.py:48] [47866] global_step=47866, preemption_count=0, score=3226.8
I0315 18:44:17.912503 140460036412608 submission_runner.py:646] Tuning trial 4/5
I0315 18:44:17.912710 140460036412608 submission_runner.py:647] Hyperparameters: Hyperparameters(learning_rate=0.0012, one_minus_beta1=0.016610699316537858, one_minus_beta2=0.005888216674053163, epsilon=1e-08, one_minus_momentum=0.5, use_momentum=True, weight_decay=0.00040349948255455174, max_preconditioner_dim=1024, precondition_frequency=100, start_preconditioning_step=-1, inv_root_override=2, exponent_multiplier=1.0, grafting_type='ADAM', grafting_epsilon=1e-08, use_normalized_grafting=False, communication_dtype='FP32', communicate_params=True, use_cosine_decay=True, warmup_factor=0.02, label_smoothing=0.1, dropout_rate=0.1, use_nadam=True, step_hint_factor=1.0)
I0315 18:44:17.913925 140460036412608 submission_runner.py:648] Metrics: {'eval_results': [(1, {'train/ssim': 0.2341423715863909, 'train/loss': 0.8492206164768764, 'validation/ssim': 0.22225685956844402, 'validation/loss': 0.8620637755214196, 'validation/num_examples': 3554, 'test/ssim': 0.24622770009577458, 'test/loss': 0.858143320432491, 'test/num_examples': 3581, 'score': 75.49568057060242, 'total_duration': 214.71094751358032, 'accumulated_submission_time': 75.49568057060242, 'accumulated_eval_time': 138.4857199192047, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (427, {'train/ssim': 0.7017958504813058, 'train/loss': 0.3139854839869908, 'validation/ssim': 0.6829683543190771, 'validation/loss': 0.32975069216683667, 'validation/num_examples': 3554, 'test/ssim': 0.701429105544017, 'test/loss': 0.33112787514617076, 'test/num_examples': 3581, 'score': 154.54857659339905, 'total_duration': 302.59312319755554, 'accumulated_submission_time': 154.54857659339905, 'accumulated_eval_time': 145.400132894516, 'accumulated_logging_time': 0.018735408782958984, 'global_step': 427, 'preemption_count': 0}), (772, {'train/ssim': 0.7138230460030692, 'train/loss': 0.2998525755746024, 'validation/ssim': 0.6946022664823087, 'validation/loss': 0.31572198304287424, 'validation/num_examples': 3554, 'test/ssim': 0.712553764115296, 'test/loss': 0.31755744701855276, 'test/num_examples': 3581, 'score': 234.83952689170837, 'total_duration': 390.9429864883423, 'accumulated_submission_time': 234.83952689170837, 'accumulated_eval_time': 151.68952083587646, 'accumulated_logging_time': 0.03840351104736328, 'global_step': 772, 'preemption_count': 0}), (1501, {'train/ssim': 0.7284294537135533, 'train/loss': 0.284986104284014, 'validation/ssim': 0.7101420164779122, 'validation/loss': 0.30002636498927265, 'validation/num_examples': 3554, 'test/ssim': 0.727261651664165, 'test/loss': 0.30217880338505304, 'test/num_examples': 3581, 'score': 313.6739218235016, 'total_duration': 478.920706987381, 'accumulated_submission_time': 313.6739218235016, 'accumulated_eval_time': 158.8801383972168, 'accumulated_logging_time': 0.06329751014709473, 'global_step': 1501, 'preemption_count': 0}), (2752, {'train/ssim': 0.7313822337559291, 'train/loss': 0.2804743732724871, 'validation/ssim': 0.7126930593169668, 'validation/loss': 0.2956162744113147, 'validation/num_examples': 3554, 'test/ssim': 0.7300859380454133, 'test/loss': 0.29736075871352274, 'test/num_examples': 3581, 'score': 392.37354373931885, 'total_duration': 566.4628839492798, 'accumulated_submission_time': 392.37354373931885, 'accumulated_eval_time': 165.6549310684204, 'accumulated_logging_time': 0.11548566818237305, 'global_step': 2752, 'preemption_count': 0}), (4003, {'train/ssim': 0.7349044936043876, 'train/loss': 0.27780682700020926, 'validation/ssim': 0.7155696458875562, 'validation/loss': 0.2932794900442283, 'validation/num_examples': 3554, 'test/ssim': 0.7328780450424114, 'test/loss': 0.2949240566531695, 'test/num_examples': 3581, 'score': 471.12213373184204, 'total_duration': 653.6779997348785, 'accumulated_submission_time': 471.12213373184204, 'accumulated_eval_time': 172.14147639274597, 'accumulated_logging_time': 0.13443684577941895, 'global_step': 4003, 'preemption_count': 0}), (5253, {'train/ssim': 0.7382275036403111, 'train/loss': 0.2753668853214809, 'validation/ssim': 0.7187532286464196, 'validation/loss': 0.29102926087902714, 'validation/num_examples': 3554, 'test/ssim': 0.7360727352258447, 'test/loss': 0.2924933200507016, 'test/num_examples': 3581, 'score': 549.9265217781067, 'total_duration': 741.0034821033478, 'accumulated_submission_time': 549.9265217781067, 'accumulated_eval_time': 178.75071334838867, 'accumulated_logging_time': 0.155348539352417, 'global_step': 5253, 'preemption_count': 0}), (6505, {'train/ssim': 0.7374308449881417, 'train/loss': 0.2757686376571655, 'validation/ssim': 0.7177999536174029, 'validation/loss': 0.2914030969176456, 'validation/num_examples': 3554, 'test/ssim': 0.735047221882854, 'test/loss': 0.2930007248542656, 'test/num_examples': 3581, 'score': 628.7911894321442, 'total_duration': 828.112498998642, 'accumulated_submission_time': 628.7911894321442, 'accumulated_eval_time': 185.09542083740234, 'accumulated_logging_time': 0.1756441593170166, 'global_step': 6505, 'preemption_count': 0}), (7688, {'train/ssim': 0.7383181708199638, 'train/loss': 0.2742515802383423, 'validation/ssim': 0.7184254179929305, 'validation/loss': 0.29005980827061056, 'validation/num_examples': 3554, 'test/ssim': 0.7357163758159383, 'test/loss': 0.2916327942286547, 'test/num_examples': 3581, 'score': 704.1700584888458, 'total_duration': 915.2980053424835, 'accumulated_submission_time': 704.1700584888458, 'accumulated_eval_time': 191.57111430168152, 'accumulated_logging_time': 3.559833526611328, 'global_step': 7688, 'preemption_count': 0}), (8941, {'train/ssim': 0.7387643541608538, 'train/loss': 0.27422739778246197, 'validation/ssim': 0.718900441184229, 'validation/loss': 0.2902637625518782, 'validation/num_examples': 3554, 'test/ssim': 0.7360171030700223, 'test/loss': 0.2918384491282114, 'test/num_examples': 3581, 'score': 782.9669303894043, 'total_duration': 1002.314138174057, 'accumulated_submission_time': 782.9669303894043, 'accumulated_eval_time': 197.8323690891266, 'accumulated_logging_time': 3.5794622898101807, 'global_step': 8941, 'preemption_count': 0}), (10193, {'train/ssim': 0.739696911403111, 'train/loss': 0.27358199868883404, 'validation/ssim': 0.7197897615362971, 'validation/loss': 0.28964406852314295, 'validation/num_examples': 3554, 'test/ssim': 0.736927193325014, 'test/loss': 0.2911998724278309, 'test/num_examples': 3581, 'score': 861.7214970588684, 'total_duration': 1089.380892753601, 'accumulated_submission_time': 861.7214970588684, 'accumulated_eval_time': 204.15355706214905, 'accumulated_logging_time': 3.60011625289917, 'global_step': 10193, 'preemption_count': 0}), (11445, {'train/ssim': 0.7394564492361886, 'train/loss': 0.27380335330963135, 'validation/ssim': 0.7195140900877532, 'validation/loss': 0.2898092103527715, 'validation/num_examples': 3554, 'test/ssim': 0.7366889840695685, 'test/loss': 0.29132688554829306, 'test/num_examples': 3581, 'score': 940.4943060874939, 'total_duration': 1176.5331976413727, 'accumulated_submission_time': 940.4943060874939, 'accumulated_eval_time': 210.59468865394592, 'accumulated_logging_time': 3.6214599609375, 'global_step': 11445, 'preemption_count': 0}), (12698, {'train/ssim': 0.7399905068533761, 'train/loss': 0.27337896823883057, 'validation/ssim': 0.7200404968433455, 'validation/loss': 0.2894399425053637, 'validation/num_examples': 3554, 'test/ssim': 0.7373290265681723, 'test/loss': 0.29092979058311225, 'test/num_examples': 3581, 'score': 1019.2969977855682, 'total_duration': 1263.3310191631317, 'accumulated_submission_time': 1019.2969977855682, 'accumulated_eval_time': 216.68041920661926, 'accumulated_logging_time': 3.640679359436035, 'global_step': 12698, 'preemption_count': 0}), (13950, {'train/ssim': 0.7402082170758929, 'train/loss': 0.2734752893447876, 'validation/ssim': 0.7201659331914744, 'validation/loss': 0.2894821896872362, 'validation/num_examples': 3554, 'test/ssim': 0.7374576759285116, 'test/loss': 0.2909083149347249, 'test/num_examples': 3581, 'score': 1098.1200709342957, 'total_duration': 1350.1741073131561, 'accumulated_submission_time': 1098.1200709342957, 'accumulated_eval_time': 222.8195185661316, 'accumulated_logging_time': 3.659877061843872, 'global_step': 13950, 'preemption_count': 0}), (15205, {'train/ssim': 0.7405697958809989, 'train/loss': 0.27262565067836214, 'validation/ssim': 0.7205650488446117, 'validation/loss': 0.28865716748514, 'validation/num_examples': 3554, 'test/ssim': 0.7379178683939542, 'test/loss': 0.2900627879782184, 'test/num_examples': 3581, 'score': 1176.9815909862518, 'total_duration': 1437.046033859253, 'accumulated_submission_time': 1176.9815909862518, 'accumulated_eval_time': 229.0120177268982, 'accumulated_logging_time': 3.6799533367156982, 'global_step': 15205, 'preemption_count': 0}), (16461, {'train/ssim': 0.7412521498543876, 'train/loss': 0.27224484511784147, 'validation/ssim': 0.7211431826375211, 'validation/loss': 0.2884532132038724, 'validation/num_examples': 3554, 'test/ssim': 0.7383984456811994, 'test/loss': 0.28991654903919994, 'test/num_examples': 3581, 'score': 1255.8405680656433, 'total_duration': 1523.956220626831, 'accumulated_submission_time': 1255.8405680656433, 'accumulated_eval_time': 235.15829825401306, 'accumulated_logging_time': 3.6993603706359863, 'global_step': 16461, 'preemption_count': 0}), (17714, {'train/ssim': 0.7406114169529506, 'train/loss': 0.27271839550563265, 'validation/ssim': 0.7204721737390968, 'validation/loss': 0.2888972207811621, 'validation/num_examples': 3554, 'test/ssim': 0.7377420407838243, 'test/loss': 0.2903613335791329, 'test/num_examples': 3581, 'score': 1334.6319365501404, 'total_duration': 1610.7212381362915, 'accumulated_submission_time': 1334.6319365501404, 'accumulated_eval_time': 241.2323398590088, 'accumulated_logging_time': 3.7195885181427, 'global_step': 17714, 'preemption_count': 0}), (18970, {'train/ssim': 0.7405034473964146, 'train/loss': 0.272862366267613, 'validation/ssim': 0.7203608884795301, 'validation/loss': 0.28903457564320134, 'validation/num_examples': 3554, 'test/ssim': 0.737623345216071, 'test/loss': 0.2905010616469736, 'test/num_examples': 3581, 'score': 1413.5417540073395, 'total_duration': 1697.6589727401733, 'accumulated_submission_time': 1413.5417540073395, 'accumulated_eval_time': 247.39331459999084, 'accumulated_logging_time': 3.739487886428833, 'global_step': 18970, 'preemption_count': 0}), (20223, {'train/ssim': 0.7401413917541504, 'train/loss': 0.2728884390422276, 'validation/ssim': 0.7199046876099113, 'validation/loss': 0.28916008068593485, 'validation/num_examples': 3554, 'test/ssim': 0.7371773334962302, 'test/loss': 0.29064522119781483, 'test/num_examples': 3581, 'score': 1492.3515481948853, 'total_duration': 1784.6592514514923, 'accumulated_submission_time': 1492.3515481948853, 'accumulated_eval_time': 253.59871459007263, 'accumulated_logging_time': 3.7598989009857178, 'global_step': 20223, 'preemption_count': 0}), (21479, {'train/ssim': 0.7411462238856724, 'train/loss': 0.2722951684679304, 'validation/ssim': 0.7210982563660664, 'validation/loss': 0.2884151907401871, 'validation/num_examples': 3554, 'test/ssim': 0.7384469874642209, 'test/loss': 0.2898252264010577, 'test/num_examples': 3581, 'score': 1571.123851299286, 'total_duration': 1871.5607752799988, 'accumulated_submission_time': 1571.123851299286, 'accumulated_eval_time': 259.750780582428, 'accumulated_logging_time': 3.7798540592193604, 'global_step': 21479, 'preemption_count': 0}), (22734, {'train/ssim': 0.7392054966517857, 'train/loss': 0.27319610118865967, 'validation/ssim': 0.7191182030810355, 'validation/loss': 0.28930756800216306, 'validation/num_examples': 3554, 'test/ssim': 0.7364119822937029, 'test/loss': 0.29080697032733527, 'test/num_examples': 3581, 'score': 1649.9474656581879, 'total_duration': 1958.3764433860779, 'accumulated_submission_time': 1649.9474656581879, 'accumulated_eval_time': 265.87260341644287, 'accumulated_logging_time': 3.80045485496521, 'global_step': 22734, 'preemption_count': 0}), (23992, {'train/ssim': 0.7399381910051618, 'train/loss': 0.27330190794808523, 'validation/ssim': 0.7197383779720034, 'validation/loss': 0.28958598723489376, 'validation/num_examples': 3554, 'test/ssim': 0.7369161487058433, 'test/loss': 0.2911171059607128, 'test/num_examples': 3581, 'score': 1728.8207409381866, 'total_duration': 2045.2740807533264, 'accumulated_submission_time': 1728.8207409381866, 'accumulated_eval_time': 272.1022140979767, 'accumulated_logging_time': 3.819981575012207, 'global_step': 23992, 'preemption_count': 0}), (25248, {'train/ssim': 0.7396717071533203, 'train/loss': 0.27495689051491873, 'validation/ssim': 0.7197019698315279, 'validation/loss': 0.29098326984120004, 'validation/num_examples': 3554, 'test/ssim': 0.7369310793947221, 'test/loss': 0.2923903732917656, 'test/num_examples': 3581, 'score': 1807.6880269050598, 'total_duration': 2132.4310467243195, 'accumulated_submission_time': 1807.6880269050598, 'accumulated_eval_time': 278.52607345581055, 'accumulated_logging_time': 3.8400399684906006, 'global_step': 25248, 'preemption_count': 0}), (26503, {'train/ssim': 0.7414919308253697, 'train/loss': 0.2720154012952532, 'validation/ssim': 0.7213788738261466, 'validation/loss': 0.2884324330859595, 'validation/num_examples': 3554, 'test/ssim': 0.7386473586725076, 'test/loss': 0.28990945866639906, 'test/num_examples': 3581, 'score': 1886.4846031665802, 'total_duration': 2219.5165791511536, 'accumulated_submission_time': 1886.4846031665802, 'accumulated_eval_time': 284.8608076572418, 'accumulated_logging_time': 3.8614301681518555, 'global_step': 26503, 'preemption_count': 0}), (27758, {'train/ssim': 0.7389108112880162, 'train/loss': 0.2730696371623448, 'validation/ssim': 0.7185216591340743, 'validation/loss': 0.2894633673655564, 'validation/num_examples': 3554, 'test/ssim': 0.7359803558494484, 'test/loss': 0.2908841803965373, 'test/num_examples': 3581, 'score': 1965.2565398216248, 'total_duration': 2306.312670469284, 'accumulated_submission_time': 1965.2565398216248, 'accumulated_eval_time': 290.9966950416565, 'accumulated_logging_time': 3.880974531173706, 'global_step': 27758, 'preemption_count': 0}), (29016, {'train/ssim': 0.7407393455505371, 'train/loss': 0.27317612511771067, 'validation/ssim': 0.720663282129291, 'validation/loss': 0.28934950605831455, 'validation/num_examples': 3554, 'test/ssim': 0.737837828993298, 'test/loss': 0.2907548492695825, 'test/num_examples': 3581, 'score': 2044.146336555481, 'total_duration': 2393.1968190670013, 'accumulated_submission_time': 2044.146336555481, 'accumulated_eval_time': 297.21201252937317, 'accumulated_logging_time': 3.901247978210449, 'global_step': 29016, 'preemption_count': 0}), (30275, {'train/ssim': 0.7402591024126325, 'train/loss': 0.2729674407414028, 'validation/ssim': 0.7200935290781514, 'validation/loss': 0.2892322787154439, 'validation/num_examples': 3554, 'test/ssim': 0.7375441921120148, 'test/loss': 0.2906247000226892, 'test/num_examples': 3581, 'score': 2123.0726025104523, 'total_duration': 2480.105504989624, 'accumulated_submission_time': 2123.0726025104523, 'accumulated_eval_time': 303.34617829322815, 'accumulated_logging_time': 3.9214792251586914, 'global_step': 30275, 'preemption_count': 0}), (31531, {'train/ssim': 0.7412257875714984, 'train/loss': 0.2727081435067313, 'validation/ssim': 0.7214276469954629, 'validation/loss': 0.28872280517990295, 'validation/num_examples': 3554, 'test/ssim': 0.7387331930893954, 'test/loss': 0.29012820348497276, 'test/num_examples': 3581, 'score': 2201.898449897766, 'total_duration': 2566.939565181732, 'accumulated_submission_time': 2201.898449897766, 'accumulated_eval_time': 309.422420501709, 'accumulated_logging_time': 3.9418528079986572, 'global_step': 31531, 'preemption_count': 0}), (32790, {'train/ssim': 0.7408412524632045, 'train/loss': 0.27228948048182894, 'validation/ssim': 0.7205401127031162, 'validation/loss': 0.2886202097878271, 'validation/num_examples': 3554, 'test/ssim': 0.737864758774609, 'test/loss': 0.2900451302228777, 'test/num_examples': 3581, 'score': 2280.737871170044, 'total_duration': 2654.0887875556946, 'accumulated_submission_time': 2280.737871170044, 'accumulated_eval_time': 315.79712653160095, 'accumulated_logging_time': 3.9616944789886475, 'global_step': 32790, 'preemption_count': 0}), (34048, {'train/ssim': 0.7415069852556501, 'train/loss': 0.2721993752888271, 'validation/ssim': 0.7214261357141601, 'validation/loss': 0.288579748665676, 'validation/num_examples': 3554, 'test/ssim': 0.7385859996771154, 'test/loss': 0.29009530824577634, 'test/num_examples': 3581, 'score': 2359.5792620182037, 'total_duration': 2741.3397347927094, 'accumulated_submission_time': 2359.5792620182037, 'accumulated_eval_time': 322.33106756210327, 'accumulated_logging_time': 3.984361410140991, 'global_step': 34048, 'preemption_count': 0}), (35303, {'train/ssim': 0.7402283804757255, 'train/loss': 0.2740968806403024, 'validation/ssim': 0.7200670129607485, 'validation/loss': 0.2901994644019063, 'validation/num_examples': 3554, 'test/ssim': 0.7373731368681933, 'test/loss': 0.29160838698382086, 'test/num_examples': 3581, 'score': 2438.478771209717, 'total_duration': 2828.309157848358, 'accumulated_submission_time': 2438.478771209717, 'accumulated_eval_time': 328.5567319393158, 'accumulated_logging_time': 4.005563020706177, 'global_step': 35303, 'preemption_count': 0}), (36557, {'train/ssim': 0.7414048058646066, 'train/loss': 0.2718815803527832, 'validation/ssim': 0.7214876860799452, 'validation/loss': 0.28789263088245637, 'validation/num_examples': 3554, 'test/ssim': 0.7388535930736875, 'test/loss': 0.2893171057425475, 'test/num_examples': 3581, 'score': 2517.2554194927216, 'total_duration': 2915.176987886429, 'accumulated_submission_time': 2517.2554194927216, 'accumulated_eval_time': 334.7286117076874, 'accumulated_logging_time': 4.02627968788147, 'global_step': 36557, 'preemption_count': 0}), (37810, {'train/ssim': 0.7417947224208287, 'train/loss': 0.272100601877485, 'validation/ssim': 0.7219297358610017, 'validation/loss': 0.2881921737061234, 'validation/num_examples': 3554, 'test/ssim': 0.7391873178319603, 'test/loss': 0.2896132651603079, 'test/num_examples': 3581, 'score': 2596.092001438141, 'total_duration': 3002.2527136802673, 'accumulated_submission_time': 2596.092001438141, 'accumulated_eval_time': 341.05812191963196, 'accumulated_logging_time': 4.046571969985962, 'global_step': 37810, 'preemption_count': 0}), (39065, {'train/ssim': 0.7402568544660296, 'train/loss': 0.2739033358437674, 'validation/ssim': 0.7202379251371693, 'validation/loss': 0.2901022958835995, 'validation/num_examples': 3554, 'test/ssim': 0.7373097325729545, 'test/loss': 0.2915828548240715, 'test/num_examples': 3581, 'score': 2674.911098718643, 'total_duration': 3089.330013036728, 'accumulated_submission_time': 2674.911098718643, 'accumulated_eval_time': 347.3951394557953, 'accumulated_logging_time': 4.066999912261963, 'global_step': 39065, 'preemption_count': 0}), (40321, {'train/ssim': 0.7416892051696777, 'train/loss': 0.27255620275224957, 'validation/ssim': 0.7216995402407499, 'validation/loss': 0.28875931636228547, 'validation/num_examples': 3554, 'test/ssim': 0.7390789169401005, 'test/loss': 0.29011654527584824, 'test/num_examples': 3581, 'score': 2753.716668844223, 'total_duration': 3176.195204257965, 'accumulated_submission_time': 2753.716668844223, 'accumulated_eval_time': 353.58499336242676, 'accumulated_logging_time': 4.088775634765625, 'global_step': 40321, 'preemption_count': 0}), (41582, {'train/ssim': 0.7436620848519462, 'train/loss': 0.27114457743508474, 'validation/ssim': 0.723527641060249, 'validation/loss': 0.2876620059209254, 'validation/num_examples': 3554, 'test/ssim': 0.7407767885192683, 'test/loss': 0.28904681936784415, 'test/num_examples': 3581, 'score': 2832.5903844833374, 'total_duration': 3263.1732048988342, 'accumulated_submission_time': 2832.5903844833374, 'accumulated_eval_time': 359.8427515029907, 'accumulated_logging_time': 4.109808921813965, 'global_step': 41582, 'preemption_count': 0}), (42838, {'train/ssim': 0.7423267364501953, 'train/loss': 0.27204515252794537, 'validation/ssim': 0.7219940340109735, 'validation/loss': 0.2884114125369302, 'validation/num_examples': 3554, 'test/ssim': 0.7393313751178092, 'test/loss': 0.28978670658728356, 'test/num_examples': 3581, 'score': 2911.372321367264, 'total_duration': 3350.219212770462, 'accumulated_submission_time': 2911.372321367264, 'accumulated_eval_time': 366.1580231189728, 'accumulated_logging_time': 4.131276607513428, 'global_step': 42838, 'preemption_count': 0}), (44095, {'train/ssim': 0.7428388595581055, 'train/loss': 0.271152138710022, 'validation/ssim': 0.7224198718653277, 'validation/loss': 0.287782908425146, 'validation/num_examples': 3554, 'test/ssim': 0.7397805911407428, 'test/loss': 0.289183070425946, 'test/num_examples': 3581, 'score': 2990.1713683605194, 'total_duration': 3437.152360677719, 'accumulated_submission_time': 2990.1713683605194, 'accumulated_eval_time': 372.4141926765442, 'accumulated_logging_time': 4.151675701141357, 'global_step': 44095, 'preemption_count': 0}), (45349, {'train/ssim': 0.7423931530543736, 'train/loss': 0.27129437242235455, 'validation/ssim': 0.7220201379607485, 'validation/loss': 0.28767790872190663, 'validation/num_examples': 3554, 'test/ssim': 0.7394305721603602, 'test/loss': 0.28903103647069606, 'test/num_examples': 3581, 'score': 3069.061467409134, 'total_duration': 3524.0785105228424, 'accumulated_submission_time': 3069.061467409134, 'accumulated_eval_time': 378.65626859664917, 'accumulated_logging_time': 4.17361044883728, 'global_step': 45349, 'preemption_count': 0}), (46606, {'train/ssim': 0.7430729184831891, 'train/loss': 0.27081429958343506, 'validation/ssim': 0.7227237081017868, 'validation/loss': 0.28726101833981077, 'validation/num_examples': 3554, 'test/ssim': 0.7400432758176836, 'test/loss': 0.28865009937430186, 'test/num_examples': 3581, 'score': 3147.928908109665, 'total_duration': 3611.0854363441467, 'accumulated_submission_time': 3147.928908109665, 'accumulated_eval_time': 384.9312267303467, 'accumulated_logging_time': 4.1955389976501465, 'global_step': 46606, 'preemption_count': 0}), (47866, {'train/ssim': 0.7438257081168038, 'train/loss': 0.27023516382489887, 'validation/ssim': 0.7236622824854038, 'validation/loss': 0.28664601271784435, 'validation/num_examples': 3554, 'test/ssim': 0.7409781142008168, 'test/loss': 0.28801278394216, 'test/num_examples': 3581, 'score': 3226.8005199432373, 'total_duration': 3698.206425666809, 'accumulated_submission_time': 3226.8005199432373, 'accumulated_eval_time': 391.26386308670044, 'accumulated_logging_time': 4.2176713943481445, 'global_step': 47866, 'preemption_count': 0})], 'global_step': 47866}
I0315 18:44:17.914021 140460036412608 submission_runner.py:649] Timing: 3226.8005199432373
I0315 18:44:17.914058 140460036412608 submission_runner.py:651] Total number of evals: 41
I0315 18:44:17.914091 140460036412608 submission_runner.py:652] ====================
I0315 18:44:17.914241 140460036412608 submission_runner.py:750] Final fastmri score: 3
