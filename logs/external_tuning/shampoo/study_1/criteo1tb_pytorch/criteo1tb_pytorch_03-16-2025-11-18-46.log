torchrun --redirects 1:0,2:0,3:0,4:0,5:0,6:0,7:0 --standalone --nnodes=1 --nproc_per_node=8 submission_runner.py --framework=pytorch --workload=criteo1tb --submission_path=submissions_algorithms/leaderboard/external_tuning/shampoo_submission/submission.py --data_dir=/data/criteo1tb --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=submissions/rolling_leaderboard/external_tuning/shampoo/study_1 --overwrite=True --save_checkpoints=False --rng_seed=984569915 --torch_compile=true --tuning_ruleset=external --tuning_search_space=submissions_algorithms/leaderboard/external_tuning/shampoo_submission/tuning_search_space.json --num_tuning_trials=5 --hparam_start_index=1 --hparam_end_index=2 2>&1 | tee -a /logs/criteo1tb_pytorch_03-16-2025-11-18-46.log
W0316 11:19:05.684000 9 site-packages/torch/distributed/run.py:793] 
W0316 11:19:05.684000 9 site-packages/torch/distributed/run.py:793] *****************************************
W0316 11:19:05.684000 9 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0316 11:19:05.684000 9 site-packages/torch/distributed/run.py:793] *****************************************
2025-03-16 11:19:17.945186: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-03-16 11:19:17.945185: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-03-16 11:19:17.945185: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-03-16 11:19:17.945185: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-03-16 11:19:17.945186: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-03-16 11:19:17.945185: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-03-16 11:19:17.945186: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-03-16 11:19:17.945185: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1742123958.368690      50 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1742123958.368661      44 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1742123958.368701      51 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1742123958.368707      47 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1742123958.368691      49 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1742123958.368689      48 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1742123958.368673      46 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1742123958.368691      45 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1742123958.483401      48 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
E0000 00:00:1742123958.483425      47 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
E0000 00:00:1742123958.483438      46 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
E0000 00:00:1742123958.483436      50 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
E0000 00:00:1742123958.483440      45 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
E0000 00:00:1742123958.483446      49 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
E0000 00:00:1742123958.483454      44 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
E0000 00:00:1742123958.483445      51 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
[rank0]:[W316 11:19:49.313675320 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank3]:[W316 11:19:49.313689772 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank6]:[W316 11:19:49.313751968 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 6]  using GPU 6 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank1]:[W316 11:19:49.313909067 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank7]:[W316 11:19:49.313917767 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 7]  using GPU 7 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank2]:[W316 11:19:49.313918881 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank5]:[W316 11:19:49.313919853 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 5]  using GPU 5 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank4]:[W316 11:19:49.313998948 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 4]  using GPU 4 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
I0316 11:19:52.128668 140240332842176 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_1/criteo1tb_pytorch.
I0316 11:19:52.128668 139784754988224 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_1/criteo1tb_pytorch.
I0316 11:19:52.128668 140009507529920 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_1/criteo1tb_pytorch.
I0316 11:19:52.128669 139898951148736 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_1/criteo1tb_pytorch.
I0316 11:19:52.128674 140523242546368 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_1/criteo1tb_pytorch.
I0316 11:19:52.128669 139806749086912 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_1/criteo1tb_pytorch.
I0316 11:19:52.128674 140425680327872 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_1/criteo1tb_pytorch.
I0316 11:19:52.128850 139831750530240 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_1/criteo1tb_pytorch.
I0316 11:20:02.104055 140240332842176 submission_runner.py:606] Using RNG seed 984569915
I0316 11:20:02.104827 140425680327872 logger_utils.py:97] Saving hparams to /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_1/criteo1tb_pytorch/trial_2/hparams.json.
I0316 11:20:02.104835 139806749086912 logger_utils.py:97] Saving hparams to /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_1/criteo1tb_pytorch/trial_2/hparams.json.
I0316 11:20:02.104830 139784754988224 logger_utils.py:97] Saving hparams to /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_1/criteo1tb_pytorch/trial_2/hparams.json.
I0316 11:20:02.104845 140523242546368 logger_utils.py:97] Saving hparams to /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_1/criteo1tb_pytorch/trial_2/hparams.json.
I0316 11:20:02.104841 139898951148736 logger_utils.py:97] Saving hparams to /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_1/criteo1tb_pytorch/trial_2/hparams.json.
I0316 11:20:02.104848 140009507529920 logger_utils.py:97] Saving hparams to /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_1/criteo1tb_pytorch/trial_2/hparams.json.
I0316 11:20:02.105187 140240332842176 submission_runner.py:615] --- Tuning run 2/5 ---
I0316 11:20:02.105316 140240332842176 submission_runner.py:620] Creating tuning directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_1/criteo1tb_pytorch/trial_2.
I0316 11:20:02.105554 140240332842176 logger_utils.py:97] Saving hparams to /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_1/criteo1tb_pytorch/trial_2/hparams.json.
I0316 11:20:02.105323 139831750530240 logger_utils.py:97] Saving hparams to /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_1/criteo1tb_pytorch/trial_2/hparams.json.
I0316 11:20:02.408956 140240332842176 submission_runner.py:218] Initializing dataset.
I0316 11:20:02.409129 140240332842176 submission_runner.py:229] Initializing model.
W0316 11:20:08.929834 140009507529920 submission_runner.py:254] These workloads cannot be fully compiled under current PyTorch version. Proceeding without `torch.compile`.
W0316 11:20:08.929846 140523242546368 submission_runner.py:254] These workloads cannot be fully compiled under current PyTorch version. Proceeding without `torch.compile`.
W0316 11:20:08.929882 139806749086912 submission_runner.py:254] These workloads cannot be fully compiled under current PyTorch version. Proceeding without `torch.compile`.
W0316 11:20:08.929944 139784754988224 submission_runner.py:254] These workloads cannot be fully compiled under current PyTorch version. Proceeding without `torch.compile`.
W0316 11:20:08.929953 140240332842176 submission_runner.py:254] These workloads cannot be fully compiled under current PyTorch version. Proceeding without `torch.compile`.
W0316 11:20:08.929965 139898951148736 submission_runner.py:254] These workloads cannot be fully compiled under current PyTorch version. Proceeding without `torch.compile`.
W0316 11:20:08.929993 140425680327872 submission_runner.py:254] These workloads cannot be fully compiled under current PyTorch version. Proceeding without `torch.compile`.
I0316 11:20:08.930085 140240332842176 submission_runner.py:272] Initializing optimizer.
W0316 11:20:08.930110 139831750530240 submission_runner.py:254] These workloads cannot be fully compiled under current PyTorch version. Proceeding without `torch.compile`.
W0316 11:20:08.972476 140240332842176 distributed_shampoo.py:341] start_preconditioning_step set to -1. Setting start_preconditioning_step equal to precondition frequency 100 by default.
W0316 11:20:08.972473 139806749086912 distributed_shampoo.py:341] start_preconditioning_step set to -1. Setting start_preconditioning_step equal to precondition frequency 100 by default.
W0316 11:20:08.972478 140425680327872 distributed_shampoo.py:341] start_preconditioning_step set to -1. Setting start_preconditioning_step equal to precondition frequency 100 by default.
W0316 11:20:08.972491 139898951148736 distributed_shampoo.py:341] start_preconditioning_step set to -1. Setting start_preconditioning_step equal to precondition frequency 100 by default.
W0316 11:20:08.972563 140240332842176 distributed_shampoo.py:359] Nesterov flag is enabled but momentum parameter is zero! Continuing without using momentum or Nesterov acceleration...
W0316 11:20:08.972510 139784754988224 distributed_shampoo.py:341] start_preconditioning_step set to -1. Setting start_preconditioning_step equal to precondition frequency 100 by default.
W0316 11:20:08.972512 140523242546368 distributed_shampoo.py:341] start_preconditioning_step set to -1. Setting start_preconditioning_step equal to precondition frequency 100 by default.
W0316 11:20:08.972582 139806749086912 distributed_shampoo.py:359] Nesterov flag is enabled but momentum parameter is zero! Continuing without using momentum or Nesterov acceleration...
W0316 11:20:08.972592 140425680327872 distributed_shampoo.py:359] Nesterov flag is enabled but momentum parameter is zero! Continuing without using momentum or Nesterov acceleration...
W0316 11:20:08.972545 140009507529920 distributed_shampoo.py:341] start_preconditioning_step set to -1. Setting start_preconditioning_step equal to precondition frequency 100 by default.
W0316 11:20:08.972615 139898951148736 distributed_shampoo.py:359] Nesterov flag is enabled but momentum parameter is zero! Continuing without using momentum or Nesterov acceleration...
W0316 11:20:08.972618 139784754988224 distributed_shampoo.py:359] Nesterov flag is enabled but momentum parameter is zero! Continuing without using momentum or Nesterov acceleration...
W0316 11:20:08.972621 140523242546368 distributed_shampoo.py:359] Nesterov flag is enabled but momentum parameter is zero! Continuing without using momentum or Nesterov acceleration...
W0316 11:20:08.972645 140009507529920 distributed_shampoo.py:359] Nesterov flag is enabled but momentum parameter is zero! Continuing without using momentum or Nesterov acceleration...
W0316 11:20:08.972640 139831750530240 distributed_shampoo.py:341] start_preconditioning_step set to -1. Setting start_preconditioning_step equal to precondition frequency 100 by default.
W0316 11:20:08.972778 139831750530240 distributed_shampoo.py:359] Nesterov flag is enabled but momentum parameter is zero! Continuing without using momentum or Nesterov acceleration...
I0316 11:20:09.034925 139806749086912 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 0.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 0 (torch.Size([512, 13])), Block block_0 (torch.Size([512, 13])).
I0316 11:20:09.034977 140523242546368 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 0.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 0 (torch.Size([512, 13])), Block block_0 (torch.Size([512, 13])).
I0316 11:20:09.035017 140240332842176 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 0.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 0 (torch.Size([512, 13])), Block block_0 (torch.Size([512, 13])).
I0316 11:20:09.035030 139784754988224 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 0.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 0 (torch.Size([512, 13])), Block block_0 (torch.Size([512, 13])).
I0316 11:20:09.035093 139806749086912 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 1.block_0 ([torch.Size([0])]) for Parameter 1 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 11:20:09.035100 140425680327872 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 0.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 0 (torch.Size([512, 13])), Block block_0 (torch.Size([512, 13])).
I0316 11:20:09.035141 140523242546368 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 1.block_0 ([torch.Size([0])]) for Parameter 1 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 11:20:09.035152 140240332842176 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 1.block_0 ([torch.Size([0])]) for Parameter 1 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 11:20:09.035128 140009507529920 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 0.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 0 (torch.Size([512, 13])), Block block_0 (torch.Size([512, 13])).
I0316 11:20:09.035140 139898951148736 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 0.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 0 (torch.Size([512, 13])), Block block_0 (torch.Size([512, 13])).
I0316 11:20:09.035185 139784754988224 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 1.block_0 ([torch.Size([0])]) for Parameter 1 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 11:20:09.035276 140523242546368 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 2.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 2 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 11:20:09.035287 140240332842176 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 2.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 2 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 11:20:09.035284 140009507529920 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 1.block_0 ([torch.Size([0])]) for Parameter 1 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 11:20:09.035317 139784754988224 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 2.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 2 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 11:20:09.035317 139898951148736 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 1.block_0 ([torch.Size([0])]) for Parameter 1 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 11:20:09.035375 140240332842176 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 3.block_0 ([torch.Size([0])]) for Parameter 3 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 11:20:09.035394 140523242546368 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 3.block_0 ([torch.Size([0])]) for Parameter 3 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 11:20:09.035416 140009507529920 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 2.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 2 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 11:20:09.035431 139784754988224 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 3.block_0 ([torch.Size([0])]) for Parameter 3 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 11:20:09.035461 139898951148736 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 2.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 2 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 11:20:09.035497 140240332842176 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 4.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 4 (torch.Size([128, 256])), Block block_0 (torch.Size([128, 256])).
I0316 11:20:09.035504 140523242546368 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 4.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 4 (torch.Size([128, 256])), Block block_0 (torch.Size([128, 256])).
I0316 11:20:09.035531 140009507529920 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 3.block_0 ([torch.Size([0])]) for Parameter 3 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 11:20:09.035547 139784754988224 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 4.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 4 (torch.Size([128, 256])), Block block_0 (torch.Size([128, 256])).
I0316 11:20:09.035572 139898951148736 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 3.block_0 ([torch.Size([0])]) for Parameter 3 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 11:20:09.035596 140523242546368 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 5.block_0 ([torch.Size([0])]) for Parameter 5 (torch.Size([128])), Block block_0 (torch.Size([128])).
I0316 11:20:09.035602 140240332842176 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 5.block_0 ([torch.Size([0])]) for Parameter 5 (torch.Size([128])), Block block_0 (torch.Size([128])).
I0316 11:20:09.035646 139784754988224 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 5.block_0 ([torch.Size([0])]) for Parameter 5 (torch.Size([128])), Block block_0 (torch.Size([128])).
I0316 11:20:09.035641 139831750530240 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 0.block_0 ([torch.Size([512, 512]), torch.Size([13, 13])]) for Parameter 0 (torch.Size([512, 13])), Block block_0 (torch.Size([512, 13])).
I0316 11:20:09.035721 140523242546368 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 6.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 6 (torch.Size([1024, 506])), Block block_0 (torch.Size([1024, 506])).
I0316 11:20:09.035730 140240332842176 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 6.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 6 (torch.Size([1024, 506])), Block block_0 (torch.Size([1024, 506])).
I0316 11:20:09.035727 139898951148736 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 4.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 4 (torch.Size([128, 256])), Block block_0 (torch.Size([128, 256])).
I0316 11:20:09.035795 140009507529920 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 4.block_0 ([torch.Size([128, 128]), torch.Size([256, 256])]) for Parameter 4 (torch.Size([128, 256])), Block block_0 (torch.Size([128, 256])).
I0316 11:20:09.035823 140523242546368 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 7.block_0 ([torch.Size([0])]) for Parameter 7 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 11:20:09.035827 139898951148736 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 5.block_0 ([torch.Size([0])]) for Parameter 5 (torch.Size([128])), Block block_0 (torch.Size([128])).
I0316 11:20:09.035835 140240332842176 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 7.block_0 ([torch.Size([0])]) for Parameter 7 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 11:20:09.035854 139831750530240 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 1.block_0 ([torch.Size([0])]) for Parameter 1 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 11:20:09.035926 140009507529920 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 5.block_0 ([torch.Size([0])]) for Parameter 5 (torch.Size([128])), Block block_0 (torch.Size([128])).
I0316 11:20:09.035948 139898951148736 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 6.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 6 (torch.Size([1024, 506])), Block block_0 (torch.Size([1024, 506])).
I0316 11:20:09.035954 140523242546368 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 8.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 8 (torch.Size([1024, 1024])), Block block_0 (torch.Size([1024, 1024])).
I0316 11:20:09.035923 139806749086912 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 2.block_0 ([torch.Size([256, 256]), torch.Size([512, 512])]) for Parameter 2 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 11:20:09.035964 140425680327872 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 1.block_0 ([torch.Size([512, 512])]) for Parameter 1 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 11:20:09.036019 139831750530240 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 2.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 2 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 11:20:09.036036 140523242546368 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 9.block_0 ([torch.Size([0])]) for Parameter 9 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 11:20:09.036048 139898951148736 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 7.block_0 ([torch.Size([0])]) for Parameter 7 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 11:20:09.036050 140009507529920 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 6.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 6 (torch.Size([1024, 506])), Block block_0 (torch.Size([1024, 506])).
I0316 11:20:09.036069 139806749086912 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 3.block_0 ([torch.Size([0])]) for Parameter 3 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 11:20:09.036142 139831750530240 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 3.block_0 ([torch.Size([0])]) for Parameter 3 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 11:20:09.036146 140009507529920 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 7.block_0 ([torch.Size([0])]) for Parameter 7 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 11:20:09.036159 139898951148736 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 8.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 8 (torch.Size([1024, 1024])), Block block_0 (torch.Size([1024, 1024])).
I0316 11:20:09.036160 140425680327872 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 2.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 2 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 11:20:09.036239 139806749086912 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 4.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 4 (torch.Size([128, 256])), Block block_0 (torch.Size([128, 256])).
I0316 11:20:09.036266 139898951148736 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 9.block_0 ([torch.Size([0])]) for Parameter 9 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 11:20:09.036285 140009507529920 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 8.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 8 (torch.Size([1024, 1024])), Block block_0 (torch.Size([1024, 1024])).
I0316 11:20:09.036291 139831750530240 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 4.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 4 (torch.Size([128, 256])), Block block_0 (torch.Size([128, 256])).
I0316 11:20:09.036342 139806749086912 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 5.block_0 ([torch.Size([0])]) for Parameter 5 (torch.Size([128])), Block block_0 (torch.Size([128])).
I0316 11:20:09.036330 139784754988224 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 6.block_0 ([torch.Size([1024, 1024]), torch.Size([506, 506])]) for Parameter 6 (torch.Size([1024, 506])), Block block_0 (torch.Size([1024, 506])).
I0316 11:20:09.036375 139898951148736 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 10.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 10 (torch.Size([512, 1024])), Block block_0 (torch.Size([512, 1024])).
I0316 11:20:09.036370 140425680327872 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 3.block_0 ([torch.Size([256, 256])]) for Parameter 3 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 11:20:09.036394 140009507529920 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 9.block_0 ([torch.Size([0])]) for Parameter 9 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 11:20:09.036406 139831750530240 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 5.block_0 ([torch.Size([0])]) for Parameter 5 (torch.Size([128])), Block block_0 (torch.Size([128])).
I0316 11:20:09.036460 139898951148736 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 11.block_0 ([torch.Size([0])]) for Parameter 11 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 11:20:09.036468 139806749086912 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 6.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 6 (torch.Size([1024, 506])), Block block_0 (torch.Size([1024, 506])).
I0316 11:20:09.036475 139784754988224 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 7.block_0 ([torch.Size([0])]) for Parameter 7 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 11:20:09.036503 140009507529920 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 10.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 10 (torch.Size([512, 1024])), Block block_0 (torch.Size([512, 1024])).
I0316 11:20:09.036528 139831750530240 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 6.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 6 (torch.Size([1024, 506])), Block block_0 (torch.Size([1024, 506])).
I0316 11:20:09.036525 140425680327872 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 4.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 4 (torch.Size([128, 256])), Block block_0 (torch.Size([128, 256])).
I0316 11:20:09.036539 140240332842176 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 8.block_0 ([torch.Size([1024, 1024]), torch.Size([1024, 1024])]) for Parameter 8 (torch.Size([1024, 1024])), Block block_0 (torch.Size([1024, 1024])).
I0316 11:20:09.036586 139806749086912 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 7.block_0 ([torch.Size([0])]) for Parameter 7 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 11:20:09.036607 140009507529920 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 11.block_0 ([torch.Size([0])]) for Parameter 11 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 11:20:09.036617 139784754988224 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 8.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 8 (torch.Size([1024, 1024])), Block block_0 (torch.Size([1024, 1024])).
I0316 11:20:09.036649 139831750530240 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 7.block_0 ([torch.Size([0])]) for Parameter 7 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 11:20:09.036680 140425680327872 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 5.block_0 ([torch.Size([128, 128])]) for Parameter 5 (torch.Size([128])), Block block_0 (torch.Size([128])).
I0316 11:20:09.036692 139806749086912 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 8.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 8 (torch.Size([1024, 1024])), Block block_0 (torch.Size([1024, 1024])).
I0316 11:20:09.036696 140240332842176 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 9.block_0 ([torch.Size([0])]) for Parameter 9 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 11:20:09.036712 139784754988224 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 9.block_0 ([torch.Size([0])]) for Parameter 9 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 11:20:09.036714 140009507529920 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 12.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 12 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 11:20:09.036774 139831750530240 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 8.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 8 (torch.Size([1024, 1024])), Block block_0 (torch.Size([1024, 1024])).
I0316 11:20:09.036789 139806749086912 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 9.block_0 ([torch.Size([0])]) for Parameter 9 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 11:20:09.036787 140523242546368 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 10.block_0 ([torch.Size([512, 512]), torch.Size([1024, 1024])]) for Parameter 10 (torch.Size([512, 1024])), Block block_0 (torch.Size([512, 1024])).
I0316 11:20:09.036814 140009507529920 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 13.block_0 ([torch.Size([0])]) for Parameter 13 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 11:20:09.036839 139784754988224 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 10.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 10 (torch.Size([512, 1024])), Block block_0 (torch.Size([512, 1024])).
I0316 11:20:09.036839 140240332842176 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 10.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 10 (torch.Size([512, 1024])), Block block_0 (torch.Size([512, 1024])).
I0316 11:20:09.036843 140425680327872 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 6.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 6 (torch.Size([1024, 506])), Block block_0 (torch.Size([1024, 506])).
I0316 11:20:09.036883 139831750530240 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 9.block_0 ([torch.Size([0])]) for Parameter 9 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 11:20:09.036895 140009507529920 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 14.block_0 ([torch.Size([0])]) for Parameter 14 (torch.Size([1, 256])), Block block_0 (torch.Size([256])).
I0316 11:20:09.036901 139806749086912 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 10.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 10 (torch.Size([512, 1024])), Block block_0 (torch.Size([512, 1024])).
I0316 11:20:09.036927 140240332842176 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 11.block_0 ([torch.Size([0])]) for Parameter 11 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 11:20:09.036926 140523242546368 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 11.block_0 ([torch.Size([0])]) for Parameter 11 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 11:20:09.036941 139784754988224 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 11.block_0 ([torch.Size([0])]) for Parameter 11 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 11:20:09.036975 140009507529920 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 15.block_0 ([torch.Size([0])]) for Parameter 15 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0316 11:20:09.036981 139806749086912 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 11.block_0 ([torch.Size([0])]) for Parameter 11 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 11:20:09.037007 139831750530240 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 10.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 10 (torch.Size([512, 1024])), Block block_0 (torch.Size([512, 1024])).
I0316 11:20:09.037006 140425680327872 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 7.block_0 ([torch.Size([1024, 1024])]) for Parameter 7 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 11:20:09.037043 140240332842176 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 12.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 12 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 11:20:09.037045 140523242546368 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 12.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 12 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 11:20:09.037050 139784754988224 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 12.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 12 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 11:20:09.037071 140009507529920 shampoo_preconditioner_list.py:612] Rank 5: ShampooPreconditionerList Numel Breakdown: (524626, 524288, 655360, 131072, 163840, 32768, 2609224, 2097152, 4194304, 2097152, 2621440, 524288, 655360, 131072, 131072, 2)
I0316 11:20:09.037087 139806749086912 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 12.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 12 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 11:20:09.037100 139831750530240 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 11.block_0 ([torch.Size([0])]) for Parameter 11 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 11:20:09.037114 140009507529920 shampoo_preconditioner_list.py:615] Rank 5: ShampooPreconditionerList Bytes Breakdown: (2098504,)
I0316 11:20:09.037094 139898951148736 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 12.block_0 ([torch.Size([256, 256]), torch.Size([512, 512])]) for Parameter 12 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 11:20:09.037125 140523242546368 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 13.block_0 ([torch.Size([0])]) for Parameter 13 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 11:20:09.037148 140240332842176 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 13.block_0 ([torch.Size([0])]) for Parameter 13 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 11:20:09.037152 139784754988224 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 13.block_0 ([torch.Size([0])]) for Parameter 13 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 11:20:09.037153 140425680327872 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 8.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 8 (torch.Size([1024, 1024])), Block block_0 (torch.Size([1024, 1024])).
I0316 11:20:09.037163 140009507529920 shampoo_preconditioner_list.py:618] Rank 5: ShampooPreconditionerList Total Elements: 17093020
I0316 11:20:09.037194 140009507529920 shampoo_preconditioner_list.py:621] Rank 5: ShampooPreconditionerList Total Bytes: 2098504
I0316 11:20:09.037193 139806749086912 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 13.block_0 ([torch.Size([0])]) for Parameter 13 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 11:20:09.037217 139831750530240 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 12.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 12 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 11:20:09.037231 140523242546368 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 14.block_0 ([torch.Size([0])]) for Parameter 14 (torch.Size([1, 256])), Block block_0 (torch.Size([256])).
I0316 11:20:09.037238 140240332842176 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 14.block_0 ([torch.Size([0])]) for Parameter 14 (torch.Size([1, 256])), Block block_0 (torch.Size([256])).
I0316 11:20:09.037239 139784754988224 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 14.block_0 ([torch.Size([0])]) for Parameter 14 (torch.Size([1, 256])), Block block_0 (torch.Size([256])).
I0316 11:20:09.037235 139898951148736 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 13.block_0 ([torch.Size([0])]) for Parameter 13 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 11:20:09.037281 139806749086912 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 14.block_0 ([torch.Size([0])]) for Parameter 14 (torch.Size([1, 256])), Block block_0 (torch.Size([256])).
I0316 11:20:09.037305 139831750530240 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 13.block_0 ([torch.Size([0])]) for Parameter 13 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 11:20:09.037320 140523242546368 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 15.block_0 ([torch.Size([0])]) for Parameter 15 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0316 11:20:09.037328 140240332842176 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 15.block_0 ([torch.Size([0])]) for Parameter 15 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0316 11:20:09.037337 139784754988224 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 15.block_0 ([torch.Size([0])]) for Parameter 15 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0316 11:20:09.037336 140009507529920 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([512, 13])), Block block_0 (torch.Size([512, 13])).
I0316 11:20:09.037340 139898951148736 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 14.block_0 ([torch.Size([0])]) for Parameter 14 (torch.Size([1, 256])), Block block_0 (torch.Size([256])).
I0316 11:20:09.037377 139806749086912 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 15.block_0 ([torch.Size([0])]) for Parameter 15 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0316 11:20:09.037397 140009507529920 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 11:20:09.037408 139831750530240 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 14.block_0 ([torch.Size([0])]) for Parameter 14 (torch.Size([1, 256])), Block block_0 (torch.Size([256])).
I0316 11:20:09.037415 140523242546368 shampoo_preconditioner_list.py:612] Rank 1: ShampooPreconditionerList Numel Breakdown: (524626, 524288, 655360, 131072, 163840, 32768, 2609224, 2097152, 4194304, 2097152, 2621440, 524288, 655360, 131072, 131072, 2)
I0316 11:20:09.037421 140240332842176 shampoo_preconditioner_list.py:612] Rank 0: ShampooPreconditionerList Numel Breakdown: (524626, 524288, 655360, 131072, 163840, 32768, 2609224, 2097152, 4194304, 2097152, 2621440, 524288, 655360, 131072, 131072, 2)
I0316 11:20:09.037433 139898951148736 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 15.block_0 ([torch.Size([0])]) for Parameter 15 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0316 11:20:09.037440 139784754988224 shampoo_preconditioner_list.py:612] Rank 2: ShampooPreconditionerList Numel Breakdown: (524626, 524288, 655360, 131072, 163840, 32768, 2609224, 2097152, 4194304, 2097152, 2621440, 524288, 655360, 131072, 131072, 2)
I0316 11:20:09.037448 140009507529920 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 11:20:09.037453 140523242546368 shampoo_preconditioner_list.py:615] Rank 1: ShampooPreconditionerList Bytes Breakdown: (2098504,)
I0316 11:20:09.037462 140240332842176 shampoo_preconditioner_list.py:615] Rank 0: ShampooPreconditionerList Bytes Breakdown: (2098504,)
I0316 11:20:09.037485 140523242546368 shampoo_preconditioner_list.py:618] Rank 1: ShampooPreconditionerList Total Elements: 17093020
I0316 11:20:09.037482 139806749086912 shampoo_preconditioner_list.py:612] Rank 3: ShampooPreconditionerList Numel Breakdown: (524626, 524288, 655360, 131072, 163840, 32768, 2609224, 2097152, 4194304, 2097152, 2621440, 524288, 655360, 131072, 131072, 2)
I0316 11:20:09.037480 139784754988224 shampoo_preconditioner_list.py:615] Rank 2: ShampooPreconditionerList Bytes Breakdown: (2098504,)
I0316 11:20:09.037498 140009507529920 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 11:20:09.037504 140240332842176 shampoo_preconditioner_list.py:618] Rank 0: ShampooPreconditionerList Total Elements: 17093020
I0316 11:20:09.037508 139831750530240 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 15.block_0 ([torch.Size([0])]) for Parameter 15 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0316 11:20:09.037515 140523242546368 shampoo_preconditioner_list.py:621] Rank 1: ShampooPreconditionerList Total Bytes: 2098504
I0316 11:20:09.037524 139784754988224 shampoo_preconditioner_list.py:618] Rank 2: ShampooPreconditionerList Total Elements: 17093020
I0316 11:20:09.037526 139806749086912 shampoo_preconditioner_list.py:615] Rank 3: ShampooPreconditionerList Bytes Breakdown: (2098504,)
I0316 11:20:09.037536 140240332842176 shampoo_preconditioner_list.py:621] Rank 0: ShampooPreconditionerList Total Bytes: 2098504
I0316 11:20:09.037532 139898951148736 shampoo_preconditioner_list.py:612] Rank 4: ShampooPreconditionerList Numel Breakdown: (524626, 524288, 655360, 131072, 163840, 32768, 2609224, 2097152, 4194304, 2097152, 2621440, 524288, 655360, 131072, 131072, 2)
I0316 11:20:09.037555 139784754988224 shampoo_preconditioner_list.py:621] Rank 2: ShampooPreconditionerList Total Bytes: 2098504
I0316 11:20:09.037560 139806749086912 shampoo_preconditioner_list.py:618] Rank 3: ShampooPreconditionerList Total Elements: 17093020
I0316 11:20:09.037571 139898951148736 shampoo_preconditioner_list.py:615] Rank 4: ShampooPreconditionerList Bytes Breakdown: (2098504,)
I0316 11:20:09.037590 139806749086912 shampoo_preconditioner_list.py:621] Rank 3: ShampooPreconditionerList Total Bytes: 2098504
I0316 11:20:09.037603 140009507529920 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 4.block_0 (torch.Size([128, 256])) for Parameter 4 (torch.Size([128, 256])), Block block_0 (torch.Size([128, 256])).
I0316 11:20:09.037611 139831750530240 shampoo_preconditioner_list.py:612] Rank 6: ShampooPreconditionerList Numel Breakdown: (524626, 524288, 655360, 131072, 163840, 32768, 2609224, 2097152, 4194304, 2097152, 2621440, 524288, 655360, 131072, 131072, 2)
I0316 11:20:09.037629 139898951148736 shampoo_preconditioner_list.py:618] Rank 4: ShampooPreconditionerList Total Elements: 17093020
I0316 11:20:09.037648 139831750530240 shampoo_preconditioner_list.py:615] Rank 6: ShampooPreconditionerList Bytes Breakdown: (2098504,)
I0316 11:20:09.037647 140523242546368 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([512, 13])), Block block_0 (torch.Size([512, 13])).
I0316 11:20:09.037662 139898951148736 shampoo_preconditioner_list.py:621] Rank 4: ShampooPreconditionerList Total Bytes: 2098504
I0316 11:20:09.037681 140009507529920 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 5.block_0 (torch.Size([0])) for Parameter 5 (torch.Size([128])), Block block_0 (torch.Size([128])).
I0316 11:20:09.037687 139831750530240 shampoo_preconditioner_list.py:618] Rank 6: ShampooPreconditionerList Total Elements: 17093020
I0316 11:20:09.037677 140240332842176 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([512, 13])), Block block_0 (torch.Size([512, 13])).
I0316 11:20:09.037672 140425680327872 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 9.block_0 ([torch.Size([1024, 1024])]) for Parameter 9 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 11:20:09.037707 140523242546368 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 11:20:09.037700 139784754988224 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([512, 13])), Block block_0 (torch.Size([512, 13])).
I0316 11:20:09.037716 139831750530240 shampoo_preconditioner_list.py:621] Rank 6: ShampooPreconditionerList Total Bytes: 2098504
I0316 11:20:09.037729 139806749086912 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([512, 13])), Block block_0 (torch.Size([512, 13])).
I0316 11:20:09.037740 140240332842176 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 11:20:09.037742 140009507529920 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 6.block_0 (torch.Size([0])) for Parameter 6 (torch.Size([1024, 506])), Block block_0 (torch.Size([1024, 506])).
I0316 11:20:09.037763 139784754988224 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 11:20:09.037771 140523242546368 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 11:20:09.037800 140009507529920 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 7.block_0 (torch.Size([0])) for Parameter 7 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 11:20:09.037801 139806749086912 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 11:20:09.037794 140240332842176 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 11:20:09.037805 139898951148736 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([512, 13])), Block block_0 (torch.Size([512, 13])).
I0316 11:20:09.037824 140523242546368 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 11:20:09.037824 139784754988224 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 11:20:09.037823 140425680327872 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 10.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 10 (torch.Size([512, 1024])), Block block_0 (torch.Size([512, 1024])).
I0316 11:20:09.037857 140009507529920 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 8.block_0 (torch.Size([0])) for Parameter 8 (torch.Size([1024, 1024])), Block block_0 (torch.Size([1024, 1024])).
I0316 11:20:09.037858 140240332842176 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 11:20:09.037868 139898951148736 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 11:20:09.037874 140523242546368 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 4.block_0 (torch.Size([0])) for Parameter 4 (torch.Size([128, 256])), Block block_0 (torch.Size([128, 256])).
I0316 11:20:09.037877 139784754988224 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 11:20:09.037910 140240332842176 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 4.block_0 (torch.Size([0])) for Parameter 4 (torch.Size([128, 256])), Block block_0 (torch.Size([128, 256])).
I0316 11:20:09.037912 140009507529920 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 9.block_0 (torch.Size([0])) for Parameter 9 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 11:20:09.037923 140523242546368 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 5.block_0 (torch.Size([0])) for Parameter 5 (torch.Size([128])), Block block_0 (torch.Size([128])).
I0316 11:20:09.037921 139898951148736 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 11:20:09.037943 139784754988224 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 4.block_0 (torch.Size([0])) for Parameter 4 (torch.Size([128, 256])), Block block_0 (torch.Size([128, 256])).
I0316 11:20:09.037909 139831750530240 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([512, 13])) for Parameter 0 (torch.Size([512, 13])), Block block_0 (torch.Size([512, 13])).
I0316 11:20:09.037979 140009507529920 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 10.block_0 (torch.Size([0])) for Parameter 10 (torch.Size([512, 1024])), Block block_0 (torch.Size([512, 1024])).
I0316 11:20:09.037988 140240332842176 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 5.block_0 (torch.Size([0])) for Parameter 5 (torch.Size([128])), Block block_0 (torch.Size([128])).
I0316 11:20:09.037995 139898951148736 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 11:20:09.037995 140523242546368 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 6.block_0 (torch.Size([0])) for Parameter 6 (torch.Size([1024, 506])), Block block_0 (torch.Size([1024, 506])).
I0316 11:20:09.037998 139784754988224 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 5.block_0 (torch.Size([0])) for Parameter 5 (torch.Size([128])), Block block_0 (torch.Size([128])).
I0316 11:20:09.038034 139831750530240 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 11:20:09.038040 140009507529920 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 11.block_0 (torch.Size([0])) for Parameter 11 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 11:20:09.038052 140240332842176 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 6.block_0 (torch.Size([0])) for Parameter 6 (torch.Size([1024, 506])), Block block_0 (torch.Size([1024, 506])).
I0316 11:20:09.038062 139898951148736 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 4.block_0 (torch.Size([0])) for Parameter 4 (torch.Size([128, 256])), Block block_0 (torch.Size([128, 256])).
I0316 11:20:09.038063 140523242546368 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 7.block_0 (torch.Size([0])) for Parameter 7 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 11:20:09.038093 140009507529920 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 12.block_0 (torch.Size([0])) for Parameter 12 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 11:20:09.038097 139831750530240 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 11:20:09.038106 140240332842176 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 7.block_0 (torch.Size([0])) for Parameter 7 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 11:20:09.038110 139898951148736 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 5.block_0 (torch.Size([0])) for Parameter 5 (torch.Size([128])), Block block_0 (torch.Size([128])).
I0316 11:20:09.038105 139784754988224 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 6.block_0 (torch.Size([1024, 506])) for Parameter 6 (torch.Size([1024, 506])), Block block_0 (torch.Size([1024, 506])).
I0316 11:20:09.038112 140523242546368 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 8.block_0 (torch.Size([0])) for Parameter 8 (torch.Size([1024, 1024])), Block block_0 (torch.Size([1024, 1024])).
I0316 11:20:09.038146 140009507529920 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 13.block_0 (torch.Size([0])) for Parameter 13 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 11:20:09.038147 139831750530240 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 11:20:09.038157 139898951148736 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 6.block_0 (torch.Size([0])) for Parameter 6 (torch.Size([1024, 506])), Block block_0 (torch.Size([1024, 506])).
I0316 11:20:09.038158 140523242546368 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 9.block_0 (torch.Size([0])) for Parameter 9 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 11:20:09.038171 139784754988224 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 7.block_0 (torch.Size([0])) for Parameter 7 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 11:20:09.038196 139831750530240 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 4.block_0 (torch.Size([0])) for Parameter 4 (torch.Size([128, 256])), Block block_0 (torch.Size([128, 256])).
I0316 11:20:09.038197 140009507529920 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 14.block_0 (torch.Size([0])) for Parameter 14 (torch.Size([1, 256])), Block block_0 (torch.Size([256])).
I0316 11:20:09.038209 139898951148736 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 7.block_0 (torch.Size([0])) for Parameter 7 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 11:20:09.038203 140240332842176 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 8.block_0 (torch.Size([1024, 1024])) for Parameter 8 (torch.Size([1024, 1024])), Block block_0 (torch.Size([1024, 1024])).
I0316 11:20:09.038220 139784754988224 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 8.block_0 (torch.Size([0])) for Parameter 8 (torch.Size([1024, 1024])), Block block_0 (torch.Size([1024, 1024])).
I0316 11:20:09.038242 140009507529920 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 15.block_0 (torch.Size([0])) for Parameter 15 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0316 11:20:09.038251 139831750530240 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 5.block_0 (torch.Size([0])) for Parameter 5 (torch.Size([128])), Block block_0 (torch.Size([128])).
I0316 11:20:09.038249 140523242546368 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 10.block_0 (torch.Size([512, 1024])) for Parameter 10 (torch.Size([512, 1024])), Block block_0 (torch.Size([512, 1024])).
I0316 11:20:09.038256 139898951148736 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 8.block_0 (torch.Size([0])) for Parameter 8 (torch.Size([1024, 1024])), Block block_0 (torch.Size([1024, 1024])).
I0316 11:20:09.038266 139784754988224 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 9.block_0 (torch.Size([0])) for Parameter 9 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 11:20:09.038266 140240332842176 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 9.block_0 (torch.Size([0])) for Parameter 9 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 11:20:09.038302 139898951148736 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 9.block_0 (torch.Size([0])) for Parameter 9 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 11:20:09.038301 139831750530240 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 6.block_0 (torch.Size([0])) for Parameter 6 (torch.Size([1024, 506])), Block block_0 (torch.Size([1024, 506])).
I0316 11:20:09.038304 140009507529920 shampoo_preconditioner_list.py:375] Rank 5: AdaGradPreconditionerList Numel Breakdown: (0, 0, 0, 0, 32768, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)
I0316 11:20:09.038312 139784754988224 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 10.block_0 (torch.Size([0])) for Parameter 10 (torch.Size([512, 1024])), Block block_0 (torch.Size([512, 1024])).
I0316 11:20:09.038311 140523242546368 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 11.block_0 (torch.Size([0])) for Parameter 11 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 11:20:09.038315 140240332842176 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 10.block_0 (torch.Size([0])) for Parameter 10 (torch.Size([512, 1024])), Block block_0 (torch.Size([512, 1024])).
I0316 11:20:09.038336 140009507529920 shampoo_preconditioner_list.py:378] Rank 5: AdaGradPreconditionerList Bytes Breakdown: (0, 0, 0, 0, 131072, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)
I0316 11:20:09.038356 139898951148736 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 10.block_0 (torch.Size([0])) for Parameter 10 (torch.Size([512, 1024])), Block block_0 (torch.Size([512, 1024])).
I0316 11:20:09.038357 139784754988224 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 11.block_0 (torch.Size([0])) for Parameter 11 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 11:20:09.038359 140523242546368 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 12.block_0 (torch.Size([0])) for Parameter 12 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 11:20:09.038362 140240332842176 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 11.block_0 (torch.Size([0])) for Parameter 11 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 11:20:09.038354 139831750530240 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 7.block_0 (torch.Size([0])) for Parameter 7 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 11:20:09.038368 140009507529920 shampoo_preconditioner_list.py:381] Rank 5: AdaGradPreconditionerList Total Elements: 32768
I0316 11:20:09.038395 140009507529920 shampoo_preconditioner_list.py:384] Rank 5: AdaGradPreconditionerList Total Bytes: 131072
I0316 11:20:09.038371 139806749086912 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([256, 512])) for Parameter 2 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 11:20:09.038402 139784754988224 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 12.block_0 (torch.Size([0])) for Parameter 12 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 11:20:09.038404 140523242546368 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 13.block_0 (torch.Size([0])) for Parameter 13 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 11:20:09.038408 140240332842176 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 12.block_0 (torch.Size([0])) for Parameter 12 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 11:20:09.038409 139898951148736 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 11.block_0 (torch.Size([0])) for Parameter 11 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 11:20:09.038413 139831750530240 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 8.block_0 (torch.Size([0])) for Parameter 8 (torch.Size([1024, 1024])), Block block_0 (torch.Size([1024, 1024])).
I0316 11:20:09.038446 139784754988224 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 13.block_0 (torch.Size([0])) for Parameter 13 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 11:20:09.038450 140523242546368 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 14.block_0 (torch.Size([0])) for Parameter 14 (torch.Size([1, 256])), Block block_0 (torch.Size([256])).
I0316 11:20:09.038453 140240332842176 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 13.block_0 (torch.Size([0])) for Parameter 13 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 11:20:09.038462 139831750530240 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 9.block_0 (torch.Size([0])) for Parameter 9 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 11:20:09.038466 139806749086912 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 11:20:09.038494 140523242546368 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 15.block_0 (torch.Size([0])) for Parameter 15 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0316 11:20:09.038499 140240332842176 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 14.block_0 (torch.Size([0])) for Parameter 14 (torch.Size([1, 256])), Block block_0 (torch.Size([256])).
I0316 11:20:09.038509 139831750530240 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 10.block_0 (torch.Size([0])) for Parameter 10 (torch.Size([512, 1024])), Block block_0 (torch.Size([512, 1024])).
I0316 11:20:09.038510 139784754988224 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 14.block_0 (torch.Size([0])) for Parameter 14 (torch.Size([1, 256])), Block block_0 (torch.Size([256])).
I0316 11:20:09.038506 140425680327872 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 11.block_0 ([torch.Size([512, 512])]) for Parameter 11 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 11:20:09.038529 139806749086912 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 4.block_0 (torch.Size([0])) for Parameter 4 (torch.Size([128, 256])), Block block_0 (torch.Size([128, 256])).
I0316 11:20:09.038544 140240332842176 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 15.block_0 (torch.Size([0])) for Parameter 15 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0316 11:20:09.038555 139831750530240 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 11.block_0 (torch.Size([0])) for Parameter 11 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 11:20:09.038558 139784754988224 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 15.block_0 (torch.Size([0])) for Parameter 15 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0316 11:20:09.038562 140523242546368 shampoo_preconditioner_list.py:375] Rank 1: AdaGradPreconditionerList Numel Breakdown: (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 524288, 0, 0, 0, 0, 0)
I0316 11:20:09.038581 139806749086912 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 5.block_0 (torch.Size([0])) for Parameter 5 (torch.Size([128])), Block block_0 (torch.Size([128])).
I0316 11:20:09.038594 140523242546368 shampoo_preconditioner_list.py:378] Rank 1: AdaGradPreconditionerList Bytes Breakdown: (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2097152, 0, 0, 0, 0, 0)
I0316 11:20:09.038599 140240332842176 shampoo_preconditioner_list.py:375] Rank 0: AdaGradPreconditionerList Numel Breakdown: (0, 0, 0, 0, 0, 0, 0, 0, 1048576, 0, 0, 0, 0, 0, 0, 0)
I0316 11:20:09.038601 139831750530240 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 12.block_0 (torch.Size([0])) for Parameter 12 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 11:20:09.038615 139784754988224 shampoo_preconditioner_list.py:375] Rank 2: AdaGradPreconditionerList Numel Breakdown: (0, 0, 0, 0, 0, 0, 518144, 0, 0, 0, 0, 0, 0, 0, 0, 0)
I0316 11:20:09.038622 140523242546368 shampoo_preconditioner_list.py:381] Rank 1: AdaGradPreconditionerList Total Elements: 524288
I0316 11:20:09.038632 140240332842176 shampoo_preconditioner_list.py:378] Rank 0: AdaGradPreconditionerList Bytes Breakdown: (0, 0, 0, 0, 0, 0, 0, 0, 4194304, 0, 0, 0, 0, 0, 0, 0)
I0316 11:20:09.038640 139806749086912 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 6.block_0 (torch.Size([0])) for Parameter 6 (torch.Size([1024, 506])), Block block_0 (torch.Size([1024, 506])).
I0316 11:20:09.038647 139784754988224 shampoo_preconditioner_list.py:378] Rank 2: AdaGradPreconditionerList Bytes Breakdown: (0, 0, 0, 0, 0, 0, 2072576, 0, 0, 0, 0, 0, 0, 0, 0, 0)
I0316 11:20:09.038645 139831750530240 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 13.block_0 (torch.Size([0])) for Parameter 13 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 11:20:09.038649 140523242546368 shampoo_preconditioner_list.py:384] Rank 1: AdaGradPreconditionerList Total Bytes: 2097152
I0316 11:20:09.038661 140240332842176 shampoo_preconditioner_list.py:381] Rank 0: AdaGradPreconditionerList Total Elements: 1048576
I0316 11:20:09.038659 140425680327872 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 12.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 12 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 11:20:09.038674 139784754988224 shampoo_preconditioner_list.py:381] Rank 2: AdaGradPreconditionerList Total Elements: 518144
I0316 11:20:09.038689 140240332842176 shampoo_preconditioner_list.py:384] Rank 0: AdaGradPreconditionerList Total Bytes: 4194304
I0316 11:20:09.038691 139831750530240 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 14.block_0 (torch.Size([0])) for Parameter 14 (torch.Size([1, 256])), Block block_0 (torch.Size([256])).
I0316 11:20:09.038696 139806749086912 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 7.block_0 (torch.Size([0])) for Parameter 7 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 11:20:09.038701 139784754988224 shampoo_preconditioner_list.py:384] Rank 2: AdaGradPreconditionerList Total Bytes: 2072576
I0316 11:20:09.038736 139831750530240 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 15.block_0 (torch.Size([0])) for Parameter 15 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0316 11:20:09.038747 139806749086912 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 8.block_0 (torch.Size([0])) for Parameter 8 (torch.Size([1024, 1024])), Block block_0 (torch.Size([1024, 1024])).
I0316 11:20:09.038795 139806749086912 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 9.block_0 (torch.Size([0])) for Parameter 9 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 11:20:09.038803 139831750530240 shampoo_preconditioner_list.py:375] Rank 6: AdaGradPreconditionerList Numel Breakdown: (6656, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)
I0316 11:20:09.038849 139806749086912 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 10.block_0 (torch.Size([0])) for Parameter 10 (torch.Size([512, 1024])), Block block_0 (torch.Size([512, 1024])).
I0316 11:20:09.038854 139831750530240 shampoo_preconditioner_list.py:378] Rank 6: AdaGradPreconditionerList Bytes Breakdown: (26624, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)
I0316 11:20:09.038885 139831750530240 shampoo_preconditioner_list.py:381] Rank 6: AdaGradPreconditionerList Total Elements: 6656
I0316 11:20:09.038902 139806749086912 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 11.block_0 (torch.Size([0])) for Parameter 11 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 11:20:09.038888 139898951148736 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 12.block_0 (torch.Size([256, 512])) for Parameter 12 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 11:20:09.038920 139831750530240 shampoo_preconditioner_list.py:384] Rank 6: AdaGradPreconditionerList Total Bytes: 26624
I0316 11:20:09.038949 139806749086912 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 12.block_0 (torch.Size([0])) for Parameter 12 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 11:20:09.038981 139898951148736 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 13.block_0 (torch.Size([0])) for Parameter 13 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 11:20:09.039000 139806749086912 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 13.block_0 (torch.Size([0])) for Parameter 13 (torch.Size([256])), Block block_0 (torch.Size([256])).
W0316 11:20:09.039001 140523242546368 distributed_shampoo.py:359] Nesterov flag is enabled but momentum parameter is zero! Continuing without using momentum or Nesterov acceleration...
I0316 11:20:09.039044 139898951148736 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 14.block_0 (torch.Size([0])) for Parameter 14 (torch.Size([1, 256])), Block block_0 (torch.Size([256])).
I0316 11:20:09.039055 139806749086912 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 14.block_0 (torch.Size([0])) for Parameter 14 (torch.Size([1, 256])), Block block_0 (torch.Size([256])).
I0316 11:20:09.039051 140240332842176 submission.py:105] Large parameters (embedding tables) detected (dim >= 1_000_000). Instantiating Row-Wise AdaGrad...
W0316 11:20:09.039049 139784754988224 distributed_shampoo.py:359] Nesterov flag is enabled but momentum parameter is zero! Continuing without using momentum or Nesterov acceleration...
I0316 11:20:09.039092 139898951148736 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 15.block_0 (torch.Size([0])) for Parameter 15 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0316 11:20:09.039106 139806749086912 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 15.block_0 (torch.Size([0])) for Parameter 15 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0316 11:20:09.039151 139898951148736 shampoo_preconditioner_list.py:375] Rank 4: AdaGradPreconditionerList Numel Breakdown: (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 131072, 0, 0, 0)
W0316 11:20:09.039149 140240332842176 distributed_shampoo.py:359] Nesterov flag is enabled but momentum parameter is zero! Continuing without using momentum or Nesterov acceleration...
I0316 11:20:09.039170 139806749086912 shampoo_preconditioner_list.py:375] Rank 3: AdaGradPreconditionerList Numel Breakdown: (0, 0, 131072, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)
I0316 11:20:09.039188 139898951148736 shampoo_preconditioner_list.py:378] Rank 4: AdaGradPreconditionerList Bytes Breakdown: (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 524288, 0, 0, 0)
I0316 11:20:09.039203 139806749086912 shampoo_preconditioner_list.py:378] Rank 3: AdaGradPreconditionerList Bytes Breakdown: (0, 0, 524288, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)
I0316 11:20:09.039222 139898951148736 shampoo_preconditioner_list.py:381] Rank 4: AdaGradPreconditionerList Total Elements: 131072
I0316 11:20:09.039232 139806749086912 shampoo_preconditioner_list.py:381] Rank 3: AdaGradPreconditionerList Total Elements: 131072
I0316 11:20:09.039254 139898951148736 shampoo_preconditioner_list.py:384] Rank 4: AdaGradPreconditionerList Total Bytes: 524288
I0316 11:20:09.039259 139806749086912 shampoo_preconditioner_list.py:384] Rank 3: AdaGradPreconditionerList Total Bytes: 524288
W0316 11:20:09.039292 140009507529920 distributed_shampoo.py:359] Nesterov flag is enabled but momentum parameter is zero! Continuing without using momentum or Nesterov acceleration...
I0316 11:20:09.039292 140425680327872 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 13.block_0 ([torch.Size([256, 256])]) for Parameter 13 (torch.Size([256])), Block block_0 (torch.Size([256])).
W0316 11:20:09.039327 139831750530240 distributed_shampoo.py:359] Nesterov flag is enabled but momentum parameter is zero! Continuing without using momentum or Nesterov acceleration...
I0316 11:20:09.039464 140425680327872 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 14.block_0 ([torch.Size([256, 256])]) for Parameter 14 (torch.Size([1, 256])), Block block_0 (torch.Size([256])).
W0316 11:20:09.039623 139898951148736 distributed_shampoo.py:359] Nesterov flag is enabled but momentum parameter is zero! Continuing without using momentum or Nesterov acceleration...
W0316 11:20:09.039628 139806749086912 distributed_shampoo.py:359] Nesterov flag is enabled but momentum parameter is zero! Continuing without using momentum or Nesterov acceleration...
I0316 11:20:09.039636 140425680327872 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 15.block_0 ([torch.Size([1, 1])]) for Parameter 15 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0316 11:20:09.039766 140425680327872 shampoo_preconditioner_list.py:612] Rank 7: ShampooPreconditionerList Numel Breakdown: (524626, 524288, 655360, 131072, 163840, 32768, 2609224, 2097152, 4194304, 2097152, 2621440, 524288, 655360, 131072, 131072, 2)
I0316 11:20:09.039818 140425680327872 shampoo_preconditioner_list.py:615] Rank 7: ShampooPreconditionerList Bytes Breakdown: (2098504, 2097152, 2621440, 524288, 655360, 131072, 10436896, 8388608, 16777216)
I0316 11:20:09.039854 140425680327872 shampoo_preconditioner_list.py:618] Rank 7: ShampooPreconditionerList Total Elements: 17093020
I0316 11:20:09.039887 140425680327872 shampoo_preconditioner_list.py:621] Rank 7: ShampooPreconditionerList Total Bytes: 43730536
I0316 11:20:09.040023 140425680327872 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([512, 13])), Block block_0 (torch.Size([512, 13])).
I0316 11:20:09.040130 140425680327872 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([512])) for Parameter 1 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 11:20:09.040201 140425680327872 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 11:20:09.040310 140425680327872 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([256])) for Parameter 3 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 11:20:09.040384 140425680327872 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 4.block_0 (torch.Size([0])) for Parameter 4 (torch.Size([128, 256])), Block block_0 (torch.Size([128, 256])).
I0316 11:20:09.040470 140425680327872 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 5.block_0 (torch.Size([128])) for Parameter 5 (torch.Size([128])), Block block_0 (torch.Size([128])).
I0316 11:20:09.040555 140425680327872 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 6.block_0 (torch.Size([0])) for Parameter 6 (torch.Size([1024, 506])), Block block_0 (torch.Size([1024, 506])).
I0316 11:20:09.040652 140425680327872 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 7.block_0 (torch.Size([1024])) for Parameter 7 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 11:20:09.040729 140425680327872 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 8.block_0 (torch.Size([0])) for Parameter 8 (torch.Size([1024, 1024])), Block block_0 (torch.Size([1024, 1024])).
I0316 11:20:09.040818 140425680327872 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 9.block_0 (torch.Size([1024])) for Parameter 9 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 11:20:09.040888 140425680327872 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 10.block_0 (torch.Size([0])) for Parameter 10 (torch.Size([512, 1024])), Block block_0 (torch.Size([512, 1024])).
I0316 11:20:09.040971 140425680327872 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 11.block_0 (torch.Size([512])) for Parameter 11 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 11:20:09.041047 140425680327872 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 12.block_0 (torch.Size([0])) for Parameter 12 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 11:20:09.041133 140425680327872 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 13.block_0 (torch.Size([256])) for Parameter 13 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 11:20:09.041215 140425680327872 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 14.block_0 (torch.Size([256])) for Parameter 14 (torch.Size([1, 256])), Block block_0 (torch.Size([256])).
I0316 11:20:09.041297 140425680327872 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 15.block_0 (torch.Size([1])) for Parameter 15 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0316 11:20:09.041380 140425680327872 shampoo_preconditioner_list.py:375] Rank 7: AdaGradPreconditionerList Numel Breakdown: (0, 512, 0, 256, 0, 128, 0, 1024, 0, 1024, 0, 512, 0, 256, 256, 1)
I0316 11:20:09.041429 140425680327872 shampoo_preconditioner_list.py:378] Rank 7: AdaGradPreconditionerList Bytes Breakdown: (0, 2048, 0, 1024, 0, 512, 0, 4096, 0, 4096, 0, 2048, 0, 1024, 1024, 4)
I0316 11:20:09.041423 139784754988224 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 11:20:09.041482 140523242546368 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 11:20:09.041529 139784754988224 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_1 (torch.Size([0])) for Parameter 0 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 11:20:09.041479 140425680327872 shampoo_preconditioner_list.py:381] Rank 7: AdaGradPreconditionerList Total Elements: 3969
I0316 11:20:09.041654 140425680327872 shampoo_preconditioner_list.py:384] Rank 7: AdaGradPreconditionerList Total Bytes: 15876
I0316 11:20:09.041768 140009507529920 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 11:20:09.041876 140009507529920 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_1 (torch.Size([0])) for Parameter 0 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 11:20:09.041960 140009507529920 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 11:20:09.041945 139806749086912 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 11:20:09.042026 140009507529920 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_1 (torch.Size([0])) for Parameter 1 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 11:20:09.042039 139806749086912 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_1 (torch.Size([0])) for Parameter 0 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 11:20:09.042019 139898951148736 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 11:20:09.042022 140240332842176 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([524288, 128])) for Parameter 0 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 11:20:09.042085 140009507529920 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 11:20:09.042100 139806749086912 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 11:20:09.042110 139898951148736 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_1 (torch.Size([0])) for Parameter 0 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 11:20:09.042083 139831750530240 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 11:20:09.042138 140240332842176 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_1 (torch.Size([0])) for Parameter 0 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 11:20:09.042183 139898951148736 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 11:20:09.042185 139831750530240 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_1 (torch.Size([0])) for Parameter 0 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 11:20:09.042217 140240332842176 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 11:20:09.042245 139898951148736 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_1 (torch.Size([0])) for Parameter 1 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 11:20:09.042248 139831750530240 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 11:20:09.042282 140240332842176 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_1 (torch.Size([0])) for Parameter 1 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 11:20:09.042270 140523242546368 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_1 (torch.Size([524288, 128])) for Parameter 0 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 11:20:09.042303 139831750530240 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_1 (torch.Size([0])) for Parameter 1 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
W0316 11:20:09.042278 140425680327872 distributed_shampoo.py:359] Nesterov flag is enabled but momentum parameter is zero! Continuing without using momentum or Nesterov acceleration...
I0316 11:20:09.042354 140240332842176 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 11:20:09.042321 139784754988224 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([524288, 128])) for Parameter 1 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 11:20:09.042385 139831750530240 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 11:20:09.042394 140523242546368 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 11:20:09.042426 140240332842176 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_1 (torch.Size([0])) for Parameter 2 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 11:20:09.042438 139784754988224 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_1 (torch.Size([0])) for Parameter 1 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 11:20:09.042456 139831750530240 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_1 (torch.Size([0])) for Parameter 2 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 11:20:09.042469 140523242546368 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_1 (torch.Size([0])) for Parameter 1 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 11:20:09.042486 140240332842176 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 11:20:09.042501 139784754988224 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 11:20:09.042526 140523242546368 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 11:20:09.042537 140240332842176 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_1 (torch.Size([0])) for Parameter 3 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 11:20:09.042578 140523242546368 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_1 (torch.Size([0])) for Parameter 2 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 11:20:09.042567 139784754988224 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_1 (torch.Size([0])) for Parameter 2 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 11:20:09.042592 140240332842176 shampoo_preconditioner_list.py:375] Rank 0: AdaGradPreconditionerList Numel Breakdown: (67108864, 0, 0, 0, 0, 0, 0, 0)
I0316 11:20:09.042626 140240332842176 shampoo_preconditioner_list.py:378] Rank 0: AdaGradPreconditionerList Bytes Breakdown: (268435456, 0, 0, 0, 0, 0, 0, 0)
I0316 11:20:09.042633 139784754988224 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 11:20:09.042641 140523242546368 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 11:20:09.042662 140240332842176 shampoo_preconditioner_list.py:381] Rank 0: AdaGradPreconditionerList Total Elements: 67108864
I0316 11:20:09.042685 139784754988224 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_1 (torch.Size([0])) for Parameter 3 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 11:20:09.042697 140240332842176 shampoo_preconditioner_list.py:384] Rank 0: AdaGradPreconditionerList Total Bytes: 268435456
I0316 11:20:09.042700 140523242546368 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_1 (torch.Size([0])) for Parameter 3 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 11:20:09.042740 139784754988224 shampoo_preconditioner_list.py:375] Rank 2: AdaGradPreconditionerList Numel Breakdown: (0, 0, 67108864, 0, 0, 0, 0, 0)
I0316 11:20:09.042749 140523242546368 shampoo_preconditioner_list.py:375] Rank 1: AdaGradPreconditionerList Numel Breakdown: (0, 67108864, 0, 0, 0, 0, 0, 0)
I0316 11:20:09.042779 139784754988224 shampoo_preconditioner_list.py:378] Rank 2: AdaGradPreconditionerList Bytes Breakdown: (0, 0, 268435456, 0, 0, 0, 0, 0)
I0316 11:20:09.042783 140523242546368 shampoo_preconditioner_list.py:378] Rank 1: AdaGradPreconditionerList Bytes Breakdown: (0, 268435456, 0, 0, 0, 0, 0, 0)
I0316 11:20:09.042814 139784754988224 shampoo_preconditioner_list.py:381] Rank 2: AdaGradPreconditionerList Total Elements: 67108864
I0316 11:20:09.042823 140523242546368 shampoo_preconditioner_list.py:381] Rank 1: AdaGradPreconditionerList Total Elements: 67108864
I0316 11:20:09.042848 139784754988224 shampoo_preconditioner_list.py:384] Rank 2: AdaGradPreconditionerList Total Bytes: 268435456
I0316 11:20:09.042853 140523242546368 shampoo_preconditioner_list.py:384] Rank 1: AdaGradPreconditionerList Total Bytes: 268435456
I0316 11:20:09.042927 140009507529920 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_1 (torch.Size([524288, 128])) for Parameter 2 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 11:20:09.042994 139806749086912 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_1 (torch.Size([524288, 128])) for Parameter 1 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 11:20:09.043064 140009507529920 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 11:20:09.043116 139806749086912 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 11:20:09.043131 140009507529920 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_1 (torch.Size([0])) for Parameter 3 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 11:20:09.043182 140009507529920 shampoo_preconditioner_list.py:375] Rank 5: AdaGradPreconditionerList Numel Breakdown: (0, 0, 0, 0, 0, 67108864, 0, 0)
I0316 11:20:09.043186 139806749086912 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_1 (torch.Size([0])) for Parameter 2 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 11:20:09.043223 140009507529920 shampoo_preconditioner_list.py:378] Rank 5: AdaGradPreconditionerList Bytes Breakdown: (0, 0, 0, 0, 0, 268435456, 0, 0)
I0316 11:20:09.043242 139806749086912 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 11:20:09.043259 140009507529920 shampoo_preconditioner_list.py:381] Rank 5: AdaGradPreconditionerList Total Elements: 67108864
I0316 11:20:09.043289 140009507529920 shampoo_preconditioner_list.py:384] Rank 5: AdaGradPreconditionerList Total Bytes: 268435456
I0316 11:20:09.043302 139806749086912 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_1 (torch.Size([0])) for Parameter 3 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 11:20:09.043301 139898951148736 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([524288, 128])) for Parameter 2 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 11:20:09.043351 139806749086912 shampoo_preconditioner_list.py:375] Rank 3: AdaGradPreconditionerList Numel Breakdown: (0, 0, 0, 67108864, 0, 0, 0, 0)
I0316 11:20:09.043392 139806749086912 shampoo_preconditioner_list.py:378] Rank 3: AdaGradPreconditionerList Bytes Breakdown: (0, 0, 0, 268435456, 0, 0, 0, 0)
I0316 11:20:09.043427 139806749086912 shampoo_preconditioner_list.py:381] Rank 3: AdaGradPreconditionerList Total Elements: 67108864
I0316 11:20:09.043416 139898951148736 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_1 (torch.Size([0])) for Parameter 2 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 11:20:09.043462 139806749086912 shampoo_preconditioner_list.py:384] Rank 3: AdaGradPreconditionerList Total Bytes: 268435456
I0316 11:20:09.043493 139898951148736 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 11:20:09.043556 139898951148736 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_1 (torch.Size([0])) for Parameter 3 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 11:20:09.043619 139898951148736 shampoo_preconditioner_list.py:375] Rank 4: AdaGradPreconditionerList Numel Breakdown: (0, 0, 0, 0, 67108864, 0, 0, 0)
I0316 11:20:09.043662 139898951148736 shampoo_preconditioner_list.py:378] Rank 4: AdaGradPreconditionerList Bytes Breakdown: (0, 0, 0, 0, 268435456, 0, 0, 0)
I0316 11:20:09.043699 139898951148736 shampoo_preconditioner_list.py:381] Rank 4: AdaGradPreconditionerList Total Elements: 67108864
I0316 11:20:09.043734 139898951148736 shampoo_preconditioner_list.py:384] Rank 4: AdaGradPreconditionerList Total Bytes: 268435456
I0316 11:20:09.043825 139831750530240 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([524288, 128])) for Parameter 3 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 11:20:09.043945 139831750530240 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_1 (torch.Size([0])) for Parameter 3 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 11:20:09.044013 139831750530240 shampoo_preconditioner_list.py:375] Rank 6: AdaGradPreconditionerList Numel Breakdown: (0, 0, 0, 0, 0, 0, 67108864, 0)
I0316 11:20:09.044054 139831750530240 shampoo_preconditioner_list.py:378] Rank 6: AdaGradPreconditionerList Bytes Breakdown: (0, 0, 0, 0, 0, 0, 268435456, 0)
I0316 11:20:09.044092 139831750530240 shampoo_preconditioner_list.py:381] Rank 6: AdaGradPreconditionerList Total Elements: 67108864
I0316 11:20:09.044129 139831750530240 shampoo_preconditioner_list.py:384] Rank 6: AdaGradPreconditionerList Total Bytes: 268435456
I0316 11:20:09.044676 140240332842176 submission_runner.py:279] Initializing metrics bundle.
I0316 11:20:09.044740 140425680327872 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 11:20:09.044839 140240332842176 submission_runner.py:301] Initializing checkpoint and logger.
I0316 11:20:09.044844 140425680327872 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_1 (torch.Size([0])) for Parameter 0 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 11:20:09.044904 140425680327872 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 11:20:09.044963 140425680327872 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_1 (torch.Size([0])) for Parameter 1 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 11:20:09.045019 140425680327872 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 11:20:09.045084 140425680327872 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_1 (torch.Size([0])) for Parameter 2 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 11:20:09.045138 140425680327872 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 11:20:09.045315 140240332842176 submission_runner.py:321] Saving meta data to /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_1/criteo1tb_pytorch/trial_2/meta_data_0.json.
I0316 11:20:09.045403 140523242546368 logger_utils.py:262] Unable to record workload.train_mean information. Continuing without it.
I0316 11:20:09.045405 139784754988224 logger_utils.py:262] Unable to record workload.train_mean information. Continuing without it.
I0316 11:20:09.045464 139784754988224 logger_utils.py:262] Unable to record workload.train_stddev information. Continuing without it.
I0316 11:20:09.045473 140523242546368 logger_utils.py:262] Unable to record workload.train_stddev information. Continuing without it.
I0316 11:20:09.045473 140240332842176 logger_utils.py:262] Unable to record workload.train_mean information. Continuing without it.
I0316 11:20:09.045512 140240332842176 logger_utils.py:262] Unable to record workload.train_stddev information. Continuing without it.
I0316 11:20:09.045624 140009507529920 logger_utils.py:262] Unable to record workload.train_mean information. Continuing without it.
I0316 11:20:09.045629 140425680327872 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_1 (torch.Size([524288, 128])) for Parameter 3 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 11:20:09.045695 140009507529920 logger_utils.py:262] Unable to record workload.train_stddev information. Continuing without it.
I0316 11:20:09.045706 140425680327872 shampoo_preconditioner_list.py:375] Rank 7: AdaGradPreconditionerList Numel Breakdown: (0, 0, 0, 0, 0, 0, 0, 67108864)
I0316 11:20:09.045747 140425680327872 shampoo_preconditioner_list.py:378] Rank 7: AdaGradPreconditionerList Bytes Breakdown: (0, 0, 0, 0, 0, 0, 0, 268435456)
I0316 11:20:09.045784 140425680327872 shampoo_preconditioner_list.py:381] Rank 7: AdaGradPreconditionerList Total Elements: 67108864
I0316 11:20:09.045817 140425680327872 shampoo_preconditioner_list.py:384] Rank 7: AdaGradPreconditionerList Total Bytes: 268435456
I0316 11:20:09.045823 139806749086912 logger_utils.py:262] Unable to record workload.train_mean information. Continuing without it.
I0316 11:20:09.045893 139806749086912 logger_utils.py:262] Unable to record workload.train_stddev information. Continuing without it.
I0316 11:20:09.045897 139898951148736 logger_utils.py:262] Unable to record workload.train_mean information. Continuing without it.
I0316 11:20:09.045981 139898951148736 logger_utils.py:262] Unable to record workload.train_stddev information. Continuing without it.
I0316 11:20:09.046227 139831750530240 logger_utils.py:262] Unable to record workload.train_mean information. Continuing without it.
I0316 11:20:09.046302 139831750530240 logger_utils.py:262] Unable to record workload.train_stddev information. Continuing without it.
I0316 11:20:09.047094 140425680327872 logger_utils.py:262] Unable to record workload.train_mean information. Continuing without it.
I0316 11:20:09.047167 140425680327872 logger_utils.py:262] Unable to record workload.train_stddev information. Continuing without it.
I0316 11:20:09.892633 140240332842176 submission_runner.py:325] Saving flags to /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_1/criteo1tb_pytorch/trial_2/flags_0.json.
I0316 11:20:09.989438 140240332842176 submission_runner.py:337] Starting training loop.
I0316 11:20:19.438957 140212320593664 logging_writer.py:48] [0] global_step=0, grad_norm=17.4205, loss=1.82449
I0316 11:20:19.734056 140240332842176 submission.py:265] 0) loss = 1.824, grad_norm = 17.420
I0316 11:20:20.194418 140240332842176 spec.py:321] Evaluating on the training split.
I0316 11:25:27.224606 140240332842176 spec.py:333] Evaluating on the validation split.
I0316 11:30:14.382571 140240332842176 spec.py:349] Evaluating on the test split.
I0316 11:35:46.874561 140240332842176 submission_runner.py:469] Time since start: 936.89s, 	Step: 1, 	{'train/loss': 1.8249944020388822, 'validation/loss': 1.8164274510810579, 'validation/num_examples': 83274637, 'test/loss': 1.8184465324655634, 'test/num_examples': 95000000, 'score': 9.74538540840149, 'total_duration': 936.8851957321167, 'accumulated_submission_time': 9.74538540840149, 'accumulated_eval_time': 926.6803433895111, 'accumulated_logging_time': 0}
I0316 11:35:46.883176 140198016849664 logging_writer.py:48] [1] accumulated_eval_time=926.68, accumulated_logging_time=0, accumulated_submission_time=9.74539, global_step=1, preemption_count=0, score=9.74539, test/loss=1.81845, test/num_examples=95000000, total_duration=936.885, train/loss=1.82499, validation/loss=1.81643, validation/num_examples=83274637
I0316 11:35:47.545438 140198008456960 logging_writer.py:48] [1] global_step=1, grad_norm=17.4128, loss=1.82469
I0316 11:35:47.548557 140240332842176 submission.py:265] 1) loss = 1.825, grad_norm = 17.413
I0316 11:35:47.739868 140198016849664 logging_writer.py:48] [2] global_step=2, grad_norm=17.136, loss=1.77874
I0316 11:35:47.742868 140240332842176 submission.py:265] 2) loss = 1.779, grad_norm = 17.136
I0316 11:35:47.934007 140198008456960 logging_writer.py:48] [3] global_step=3, grad_norm=16.7984, loss=1.70026
I0316 11:35:47.936754 140240332842176 submission.py:265] 3) loss = 1.700, grad_norm = 16.798
I0316 11:35:48.129013 140198016849664 logging_writer.py:48] [4] global_step=4, grad_norm=16.3218, loss=1.59252
I0316 11:35:48.132612 140240332842176 submission.py:265] 4) loss = 1.593, grad_norm = 16.322
I0316 11:35:48.326171 140198008456960 logging_writer.py:48] [5] global_step=5, grad_norm=15.4771, loss=1.46119
I0316 11:35:48.329399 140240332842176 submission.py:265] 5) loss = 1.461, grad_norm = 15.477
I0316 11:35:48.520961 140198016849664 logging_writer.py:48] [6] global_step=6, grad_norm=13.7163, loss=1.3208
I0316 11:35:48.524021 140240332842176 submission.py:265] 6) loss = 1.321, grad_norm = 13.716
I0316 11:35:48.713585 140198008456960 logging_writer.py:48] [7] global_step=7, grad_norm=12.6104, loss=1.17471
I0316 11:35:48.716471 140240332842176 submission.py:265] 7) loss = 1.175, grad_norm = 12.610
I0316 11:35:48.906807 140198016849664 logging_writer.py:48] [8] global_step=8, grad_norm=11.5362, loss=1.02374
I0316 11:35:48.909770 140240332842176 submission.py:265] 8) loss = 1.024, grad_norm = 11.536
I0316 11:35:49.102971 140198008456960 logging_writer.py:48] [9] global_step=9, grad_norm=10.27, loss=0.875341
I0316 11:35:49.106839 140240332842176 submission.py:265] 9) loss = 0.875, grad_norm = 10.270
I0316 11:35:49.301004 140198016849664 logging_writer.py:48] [10] global_step=10, grad_norm=9.04513, loss=0.731788
I0316 11:35:49.304211 140240332842176 submission.py:265] 10) loss = 0.732, grad_norm = 9.045
I0316 11:35:49.494092 140198008456960 logging_writer.py:48] [11] global_step=11, grad_norm=7.71318, loss=0.602072
I0316 11:35:49.496717 140240332842176 submission.py:265] 11) loss = 0.602, grad_norm = 7.713
I0316 11:35:49.685949 140198016849664 logging_writer.py:48] [12] global_step=12, grad_norm=6.15553, loss=0.488687
I0316 11:35:49.688816 140240332842176 submission.py:265] 12) loss = 0.489, grad_norm = 6.156
I0316 11:35:49.879521 140198008456960 logging_writer.py:48] [13] global_step=13, grad_norm=4.65452, loss=0.396527
I0316 11:35:49.882232 140240332842176 submission.py:265] 13) loss = 0.397, grad_norm = 4.655
I0316 11:35:50.072720 140198016849664 logging_writer.py:48] [14] global_step=14, grad_norm=3.34874, loss=0.32906
I0316 11:35:50.076230 140240332842176 submission.py:265] 14) loss = 0.329, grad_norm = 3.349
I0316 11:35:50.267148 140198008456960 logging_writer.py:48] [15] global_step=15, grad_norm=2.29716, loss=0.279231
I0316 11:35:50.270230 140240332842176 submission.py:265] 15) loss = 0.279, grad_norm = 2.297
I0316 11:35:50.461079 140198016849664 logging_writer.py:48] [16] global_step=16, grad_norm=1.41907, loss=0.245773
I0316 11:35:50.464610 140240332842176 submission.py:265] 16) loss = 0.246, grad_norm = 1.419
I0316 11:35:50.656328 140198008456960 logging_writer.py:48] [17] global_step=17, grad_norm=0.764382, loss=0.225908
I0316 11:35:50.659352 140240332842176 submission.py:265] 17) loss = 0.226, grad_norm = 0.764
I0316 11:35:50.851186 140198016849664 logging_writer.py:48] [18] global_step=18, grad_norm=0.345269, loss=0.21421
I0316 11:35:50.853760 140240332842176 submission.py:265] 18) loss = 0.214, grad_norm = 0.345
I0316 11:35:51.042925 140198008456960 logging_writer.py:48] [19] global_step=19, grad_norm=0.454222, loss=0.207073
I0316 11:35:51.045655 140240332842176 submission.py:265] 19) loss = 0.207, grad_norm = 0.454
I0316 11:35:51.236188 140198016849664 logging_writer.py:48] [20] global_step=20, grad_norm=0.752065, loss=0.210453
I0316 11:35:51.238992 140240332842176 submission.py:265] 20) loss = 0.210, grad_norm = 0.752
I0316 11:35:51.446069 140198008456960 logging_writer.py:48] [21] global_step=21, grad_norm=1.05756, loss=0.225154
I0316 11:35:51.449636 140240332842176 submission.py:265] 21) loss = 0.225, grad_norm = 1.058
I0316 11:35:51.639945 140198016849664 logging_writer.py:48] [22] global_step=22, grad_norm=1.27467, loss=0.237811
I0316 11:35:51.643213 140240332842176 submission.py:265] 22) loss = 0.238, grad_norm = 1.275
I0316 11:35:51.835257 140198008456960 logging_writer.py:48] [23] global_step=23, grad_norm=1.44742, loss=0.25194
I0316 11:35:51.838186 140240332842176 submission.py:265] 23) loss = 0.252, grad_norm = 1.447
I0316 11:35:52.032848 140198016849664 logging_writer.py:48] [24] global_step=24, grad_norm=1.66836, loss=0.273824
I0316 11:35:52.035927 140240332842176 submission.py:265] 24) loss = 0.274, grad_norm = 1.668
I0316 11:35:52.227797 140198008456960 logging_writer.py:48] [25] global_step=25, grad_norm=1.82235, loss=0.291635
I0316 11:35:52.231342 140240332842176 submission.py:265] 25) loss = 0.292, grad_norm = 1.822
I0316 11:35:52.421371 140198016849664 logging_writer.py:48] [26] global_step=26, grad_norm=1.98064, loss=0.31069
I0316 11:35:52.424494 140240332842176 submission.py:265] 26) loss = 0.311, grad_norm = 1.981
I0316 11:35:52.613837 140198008456960 logging_writer.py:48] [27] global_step=27, grad_norm=2.1238, loss=0.330252
I0316 11:35:52.616882 140240332842176 submission.py:265] 27) loss = 0.330, grad_norm = 2.124
I0316 11:35:52.807178 140198016849664 logging_writer.py:48] [28] global_step=28, grad_norm=2.17799, loss=0.339354
I0316 11:35:52.811075 140240332842176 submission.py:265] 28) loss = 0.339, grad_norm = 2.178
I0316 11:35:53.005156 140198008456960 logging_writer.py:48] [29] global_step=29, grad_norm=2.30942, loss=0.359305
I0316 11:35:53.008497 140240332842176 submission.py:265] 29) loss = 0.359, grad_norm = 2.309
I0316 11:35:53.200124 140198016849664 logging_writer.py:48] [30] global_step=30, grad_norm=2.36758, loss=0.368587
I0316 11:35:53.203126 140240332842176 submission.py:265] 30) loss = 0.369, grad_norm = 2.368
I0316 11:35:54.497242 140198008456960 logging_writer.py:48] [31] global_step=31, grad_norm=2.40591, loss=0.374431
I0316 11:35:54.500155 140240332842176 submission.py:265] 31) loss = 0.374, grad_norm = 2.406
I0316 11:35:54.931387 140198016849664 logging_writer.py:48] [32] global_step=32, grad_norm=2.39768, loss=0.37301
I0316 11:35:54.944997 140240332842176 submission.py:265] 32) loss = 0.373, grad_norm = 2.398
I0316 11:35:56.900112 140198008456960 logging_writer.py:48] [33] global_step=33, grad_norm=2.44756, loss=0.380386
I0316 11:35:56.903608 140240332842176 submission.py:265] 33) loss = 0.380, grad_norm = 2.448
I0316 11:35:58.006207 140198016849664 logging_writer.py:48] [34] global_step=34, grad_norm=2.43537, loss=0.377832
I0316 11:35:58.009435 140240332842176 submission.py:265] 34) loss = 0.378, grad_norm = 2.435
I0316 11:35:59.365957 140198008456960 logging_writer.py:48] [35] global_step=35, grad_norm=2.38996, loss=0.369752
I0316 11:35:59.369259 140240332842176 submission.py:265] 35) loss = 0.370, grad_norm = 2.390
I0316 11:36:00.060169 140198016849664 logging_writer.py:48] [36] global_step=36, grad_norm=2.38916, loss=0.367774
I0316 11:36:00.063625 140240332842176 submission.py:265] 36) loss = 0.368, grad_norm = 2.389
I0316 11:36:01.615645 140198008456960 logging_writer.py:48] [37] global_step=37, grad_norm=2.30512, loss=0.353629
I0316 11:36:01.619075 140240332842176 submission.py:265] 37) loss = 0.354, grad_norm = 2.305
I0316 11:36:02.585491 140198016849664 logging_writer.py:48] [38] global_step=38, grad_norm=2.11935, loss=0.330582
I0316 11:36:02.588857 140240332842176 submission.py:265] 38) loss = 0.331, grad_norm = 2.119
I0316 11:36:03.734064 140198008456960 logging_writer.py:48] [39] global_step=39, grad_norm=2.01197, loss=0.314125
I0316 11:36:03.736943 140240332842176 submission.py:265] 39) loss = 0.314, grad_norm = 2.012
I0316 11:36:04.874918 140198016849664 logging_writer.py:48] [40] global_step=40, grad_norm=1.90205, loss=0.296751
I0316 11:36:04.877863 140240332842176 submission.py:265] 40) loss = 0.297, grad_norm = 1.902
I0316 11:36:05.864387 140198008456960 logging_writer.py:48] [41] global_step=41, grad_norm=1.82791, loss=0.283958
I0316 11:36:05.867457 140240332842176 submission.py:265] 41) loss = 0.284, grad_norm = 1.828
I0316 11:36:07.464383 140198016849664 logging_writer.py:48] [42] global_step=42, grad_norm=1.76404, loss=0.273272
I0316 11:36:07.467395 140240332842176 submission.py:265] 42) loss = 0.273, grad_norm = 1.764
I0316 11:36:08.566630 140198008456960 logging_writer.py:48] [43] global_step=43, grad_norm=1.66095, loss=0.259493
I0316 11:36:08.569557 140240332842176 submission.py:265] 43) loss = 0.259, grad_norm = 1.661
I0316 11:36:09.825186 140198016849664 logging_writer.py:48] [44] global_step=44, grad_norm=1.51683, loss=0.241503
I0316 11:36:09.828137 140240332842176 submission.py:265] 44) loss = 0.242, grad_norm = 1.517
I0316 11:36:10.931435 140198008456960 logging_writer.py:48] [45] global_step=45, grad_norm=1.33987, loss=0.221059
I0316 11:36:10.934416 140240332842176 submission.py:265] 45) loss = 0.221, grad_norm = 1.340
I0316 11:36:12.180143 140198016849664 logging_writer.py:48] [46] global_step=46, grad_norm=1.2202, loss=0.21102
I0316 11:36:12.183260 140240332842176 submission.py:265] 46) loss = 0.211, grad_norm = 1.220
I0316 11:36:12.713370 140198008456960 logging_writer.py:48] [47] global_step=47, grad_norm=1.01208, loss=0.193593
I0316 11:36:12.716959 140240332842176 submission.py:265] 47) loss = 0.194, grad_norm = 1.012
I0316 11:36:14.683794 140198016849664 logging_writer.py:48] [48] global_step=48, grad_norm=0.830597, loss=0.184541
I0316 11:36:14.687638 140240332842176 submission.py:265] 48) loss = 0.185, grad_norm = 0.831
I0316 11:36:15.810334 140198008456960 logging_writer.py:48] [49] global_step=49, grad_norm=0.579089, loss=0.17246
I0316 11:36:15.813879 140240332842176 submission.py:265] 49) loss = 0.172, grad_norm = 0.579
I0316 11:36:17.072179 140198016849664 logging_writer.py:48] [50] global_step=50, grad_norm=0.314503, loss=0.163783
I0316 11:36:17.075510 140240332842176 submission.py:265] 50) loss = 0.164, grad_norm = 0.315
I0316 11:36:18.299845 140198008456960 logging_writer.py:48] [51] global_step=51, grad_norm=0.184828, loss=0.161606
I0316 11:36:18.303460 140240332842176 submission.py:265] 51) loss = 0.162, grad_norm = 0.185
I0316 11:36:19.518434 140198016849664 logging_writer.py:48] [52] global_step=52, grad_norm=0.326144, loss=0.160945
I0316 11:36:19.521768 140240332842176 submission.py:265] 52) loss = 0.161, grad_norm = 0.326
I0316 11:36:20.532570 140198008456960 logging_writer.py:48] [53] global_step=53, grad_norm=0.535946, loss=0.161673
I0316 11:36:20.536058 140240332842176 submission.py:265] 53) loss = 0.162, grad_norm = 0.536
I0316 11:36:21.930567 140198016849664 logging_writer.py:48] [54] global_step=54, grad_norm=0.731378, loss=0.161448
I0316 11:36:21.934476 140240332842176 submission.py:265] 54) loss = 0.161, grad_norm = 0.731
I0316 11:36:22.792032 140198008456960 logging_writer.py:48] [55] global_step=55, grad_norm=0.810834, loss=0.164658
I0316 11:36:22.795630 140240332842176 submission.py:265] 55) loss = 0.165, grad_norm = 0.811
I0316 11:36:24.505341 140198016849664 logging_writer.py:48] [56] global_step=56, grad_norm=0.860544, loss=0.162938
I0316 11:36:24.508365 140240332842176 submission.py:265] 56) loss = 0.163, grad_norm = 0.861
I0316 11:36:25.702497 140198008456960 logging_writer.py:48] [57] global_step=57, grad_norm=0.825283, loss=0.161399
I0316 11:36:25.705468 140240332842176 submission.py:265] 57) loss = 0.161, grad_norm = 0.825
I0316 11:36:27.134982 140198016849664 logging_writer.py:48] [58] global_step=58, grad_norm=0.748799, loss=0.157897
I0316 11:36:27.138198 140240332842176 submission.py:265] 58) loss = 0.158, grad_norm = 0.749
I0316 11:36:27.824205 140198008456960 logging_writer.py:48] [59] global_step=59, grad_norm=0.616433, loss=0.154849
I0316 11:36:27.828007 140240332842176 submission.py:265] 59) loss = 0.155, grad_norm = 0.616
I0316 11:36:29.640830 140198016849664 logging_writer.py:48] [60] global_step=60, grad_norm=0.455684, loss=0.150745
I0316 11:36:29.643953 140240332842176 submission.py:265] 60) loss = 0.151, grad_norm = 0.456
I0316 11:36:30.205123 140198008456960 logging_writer.py:48] [61] global_step=61, grad_norm=0.298011, loss=0.145934
I0316 11:36:30.208142 140240332842176 submission.py:265] 61) loss = 0.146, grad_norm = 0.298
I0316 11:36:32.082104 140198016849664 logging_writer.py:48] [62] global_step=62, grad_norm=0.132169, loss=0.142896
I0316 11:36:32.085085 140240332842176 submission.py:265] 62) loss = 0.143, grad_norm = 0.132
I0316 11:36:33.126888 140198008456960 logging_writer.py:48] [63] global_step=63, grad_norm=0.0909406, loss=0.145539
I0316 11:36:33.129837 140240332842176 submission.py:265] 63) loss = 0.146, grad_norm = 0.091
I0316 11:36:34.620490 140198016849664 logging_writer.py:48] [64] global_step=64, grad_norm=0.187484, loss=0.14157
I0316 11:36:34.623541 140240332842176 submission.py:265] 64) loss = 0.142, grad_norm = 0.187
I0316 11:36:35.660837 140198008456960 logging_writer.py:48] [65] global_step=65, grad_norm=0.323574, loss=0.145658
I0316 11:36:35.663865 140240332842176 submission.py:265] 65) loss = 0.146, grad_norm = 0.324
I0316 11:36:36.858191 140198016849664 logging_writer.py:48] [66] global_step=66, grad_norm=0.401626, loss=0.146738
I0316 11:36:36.861217 140240332842176 submission.py:265] 66) loss = 0.147, grad_norm = 0.402
I0316 11:36:38.184945 140198008456960 logging_writer.py:48] [67] global_step=67, grad_norm=0.409066, loss=0.142768
I0316 11:36:38.188085 140240332842176 submission.py:265] 67) loss = 0.143, grad_norm = 0.409
I0316 11:36:39.239394 140198016849664 logging_writer.py:48] [68] global_step=68, grad_norm=0.42804, loss=0.144461
I0316 11:36:39.242337 140240332842176 submission.py:265] 68) loss = 0.144, grad_norm = 0.428
I0316 11:36:40.538908 140198008456960 logging_writer.py:48] [69] global_step=69, grad_norm=0.400733, loss=0.143169
I0316 11:36:40.541833 140240332842176 submission.py:265] 69) loss = 0.143, grad_norm = 0.401
I0316 11:36:41.328472 140198016849664 logging_writer.py:48] [70] global_step=70, grad_norm=0.374702, loss=0.143822
I0316 11:36:41.331889 140240332842176 submission.py:265] 70) loss = 0.144, grad_norm = 0.375
I0316 11:36:43.047333 140198008456960 logging_writer.py:48] [71] global_step=71, grad_norm=0.317037, loss=0.144009
I0316 11:36:43.050655 140240332842176 submission.py:265] 71) loss = 0.144, grad_norm = 0.317
I0316 11:36:44.034584 140198016849664 logging_writer.py:48] [72] global_step=72, grad_norm=0.211406, loss=0.139903
I0316 11:36:44.038276 140240332842176 submission.py:265] 72) loss = 0.140, grad_norm = 0.211
I0316 11:36:44.997380 140198008456960 logging_writer.py:48] [73] global_step=73, grad_norm=0.127785, loss=0.139649
I0316 11:36:45.000785 140240332842176 submission.py:265] 73) loss = 0.140, grad_norm = 0.128
I0316 11:36:45.665016 140198016849664 logging_writer.py:48] [74] global_step=74, grad_norm=0.0526587, loss=0.136618
I0316 11:36:45.668432 140240332842176 submission.py:265] 74) loss = 0.137, grad_norm = 0.053
I0316 11:36:47.325639 140198008456960 logging_writer.py:48] [75] global_step=75, grad_norm=0.0860661, loss=0.138209
I0316 11:36:47.329202 140240332842176 submission.py:265] 75) loss = 0.138, grad_norm = 0.086
I0316 11:36:48.294803 140198016849664 logging_writer.py:48] [76] global_step=76, grad_norm=0.13831, loss=0.141625
I0316 11:36:48.298197 140240332842176 submission.py:265] 76) loss = 0.142, grad_norm = 0.138
I0316 11:36:49.437627 140198008456960 logging_writer.py:48] [77] global_step=77, grad_norm=0.187182, loss=0.140185
I0316 11:36:49.441055 140240332842176 submission.py:265] 77) loss = 0.140, grad_norm = 0.187
I0316 11:36:50.599687 140198016849664 logging_writer.py:48] [78] global_step=78, grad_norm=0.204921, loss=0.141154
I0316 11:36:50.603101 140240332842176 submission.py:265] 78) loss = 0.141, grad_norm = 0.205
I0316 11:36:51.721078 140198008456960 logging_writer.py:48] [79] global_step=79, grad_norm=0.189556, loss=0.141659
I0316 11:36:51.724509 140240332842176 submission.py:265] 79) loss = 0.142, grad_norm = 0.190
I0316 11:36:52.566974 140198016849664 logging_writer.py:48] [80] global_step=80, grad_norm=0.154306, loss=0.141097
I0316 11:36:52.570257 140240332842176 submission.py:265] 80) loss = 0.141, grad_norm = 0.154
I0316 11:36:53.919344 140198008456960 logging_writer.py:48] [81] global_step=81, grad_norm=0.135166, loss=0.137647
I0316 11:36:53.922745 140240332842176 submission.py:265] 81) loss = 0.138, grad_norm = 0.135
I0316 11:36:55.144025 140198016849664 logging_writer.py:48] [82] global_step=82, grad_norm=0.0672254, loss=0.140768
I0316 11:36:55.147700 140240332842176 submission.py:265] 82) loss = 0.141, grad_norm = 0.067
I0316 11:36:56.249646 140198008456960 logging_writer.py:48] [83] global_step=83, grad_norm=0.0531388, loss=0.13943
I0316 11:36:56.253292 140240332842176 submission.py:265] 83) loss = 0.139, grad_norm = 0.053
I0316 11:36:57.766331 140198016849664 logging_writer.py:48] [84] global_step=84, grad_norm=0.0774755, loss=0.14041
I0316 11:36:57.769655 140240332842176 submission.py:265] 84) loss = 0.140, grad_norm = 0.077
I0316 11:36:58.664775 140198008456960 logging_writer.py:48] [85] global_step=85, grad_norm=0.0846632, loss=0.137103
I0316 11:36:58.668090 140240332842176 submission.py:265] 85) loss = 0.137, grad_norm = 0.085
I0316 11:37:00.279516 140198016849664 logging_writer.py:48] [86] global_step=86, grad_norm=0.118656, loss=0.140449
I0316 11:37:00.283071 140240332842176 submission.py:265] 86) loss = 0.140, grad_norm = 0.119
I0316 11:37:01.211305 140198008456960 logging_writer.py:48] [87] global_step=87, grad_norm=0.113513, loss=0.1394
I0316 11:37:01.214677 140240332842176 submission.py:265] 87) loss = 0.139, grad_norm = 0.114
I0316 11:37:02.340750 140198016849664 logging_writer.py:48] [88] global_step=88, grad_norm=0.0911482, loss=0.137563
I0316 11:37:02.344251 140240332842176 submission.py:265] 88) loss = 0.138, grad_norm = 0.091
I0316 11:37:03.317298 140198008456960 logging_writer.py:48] [89] global_step=89, grad_norm=0.0458824, loss=0.132778
I0316 11:37:03.320682 140240332842176 submission.py:265] 89) loss = 0.133, grad_norm = 0.046
I0316 11:37:04.465669 140198016849664 logging_writer.py:48] [90] global_step=90, grad_norm=0.0571399, loss=0.137202
I0316 11:37:04.469012 140240332842176 submission.py:265] 90) loss = 0.137, grad_norm = 0.057
I0316 11:37:05.470296 140198008456960 logging_writer.py:48] [91] global_step=91, grad_norm=0.0373303, loss=0.136378
I0316 11:37:05.473524 140240332842176 submission.py:265] 91) loss = 0.136, grad_norm = 0.037
I0316 11:37:06.441869 140198016849664 logging_writer.py:48] [92] global_step=92, grad_norm=0.034858, loss=0.136026
I0316 11:37:06.445171 140240332842176 submission.py:265] 92) loss = 0.136, grad_norm = 0.035
I0316 11:37:07.627194 140198008456960 logging_writer.py:48] [93] global_step=93, grad_norm=0.0441798, loss=0.135628
I0316 11:37:07.630557 140240332842176 submission.py:265] 93) loss = 0.136, grad_norm = 0.044
I0316 11:37:08.254428 140198016849664 logging_writer.py:48] [94] global_step=94, grad_norm=0.0582506, loss=0.133475
I0316 11:37:08.257854 140240332842176 submission.py:265] 94) loss = 0.133, grad_norm = 0.058
I0316 11:37:09.850944 140198008456960 logging_writer.py:48] [95] global_step=95, grad_norm=0.0321581, loss=0.143576
I0316 11:37:09.854340 140240332842176 submission.py:265] 95) loss = 0.144, grad_norm = 0.032
I0316 11:37:10.823759 140198016849664 logging_writer.py:48] [96] global_step=96, grad_norm=0.0319842, loss=0.148533
I0316 11:37:10.827194 140240332842176 submission.py:265] 96) loss = 0.149, grad_norm = 0.032
I0316 11:37:11.633968 140198008456960 logging_writer.py:48] [97] global_step=97, grad_norm=0.0302589, loss=0.147833
I0316 11:37:11.637516 140240332842176 submission.py:265] 97) loss = 0.148, grad_norm = 0.030
I0316 11:37:13.501588 140198016849664 logging_writer.py:48] [98] global_step=98, grad_norm=0.0319386, loss=0.148226
I0316 11:37:13.505199 140240332842176 submission.py:265] 98) loss = 0.148, grad_norm = 0.032
I0316 11:37:16.808039 140198008456960 logging_writer.py:48] [99] global_step=99, grad_norm=0.0263243, loss=0.146786
I0316 11:37:16.811364 140240332842176 submission.py:265] 99) loss = 0.147, grad_norm = 0.026
I0316 11:37:17.004403 140198016849664 logging_writer.py:48] [100] global_step=100, grad_norm=0.0241878, loss=0.146486
I0316 11:37:17.007894 140240332842176 submission.py:265] 100) loss = 0.146, grad_norm = 0.024
I0316 11:37:47.472883 140240332842176 spec.py:321] Evaluating on the training split.
I0316 11:42:58.631222 140240332842176 spec.py:333] Evaluating on the validation split.
I0316 11:47:18.458857 140240332842176 spec.py:349] Evaluating on the test split.
I0316 11:52:17.946077 140240332842176 submission_runner.py:469] Time since start: 1927.96s, 	Step: 127, 	{'train/loss': 0.1295790826712524, 'validation/loss': 0.13249961636749855, 'validation/num_examples': 83274637, 'test/loss': 0.13507422640111824, 'test/num_examples': 95000000, 'score': 129.3834900856018, 'total_duration': 1927.9567475318909, 'accumulated_submission_time': 129.3834900856018, 'accumulated_eval_time': 1797.1536302566528, 'accumulated_logging_time': 0.015717029571533203}
I0316 11:52:17.955473 140198008456960 logging_writer.py:48] [127] accumulated_eval_time=1797.15, accumulated_logging_time=0.015717, accumulated_submission_time=129.383, global_step=127, preemption_count=0, score=129.383, test/loss=0.135074, test/num_examples=95000000, total_duration=1927.96, train/loss=0.129579, validation/loss=0.1325, validation/num_examples=83274637
I0316 11:54:19.331945 140240332842176 spec.py:321] Evaluating on the training split.
I0316 11:59:33.656291 140240332842176 spec.py:333] Evaluating on the validation split.
I0316 12:04:04.419950 140240332842176 spec.py:349] Evaluating on the test split.
I0316 12:09:02.416997 140240332842176 submission_runner.py:469] Time since start: 2932.43s, 	Step: 254, 	{'train/loss': 0.12927126015172608, 'validation/loss': 0.12865946201861944, 'validation/num_examples': 83274637, 'test/loss': 0.13105879971606607, 'test/num_examples': 95000000, 'score': 249.89333415031433, 'total_duration': 2932.4276793003082, 'accumulated_submission_time': 249.89333415031433, 'accumulated_eval_time': 2680.2387075424194, 'accumulated_logging_time': 0.032091379165649414}
I0316 12:09:02.425760 140198016849664 logging_writer.py:48] [254] accumulated_eval_time=2680.24, accumulated_logging_time=0.0320914, accumulated_submission_time=249.893, global_step=254, preemption_count=0, score=249.893, test/loss=0.131059, test/num_examples=95000000, total_duration=2932.43, train/loss=0.129271, validation/loss=0.128659, validation/num_examples=83274637
/usr/local/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
I0316 12:11:03.195846 140240332842176 spec.py:321] Evaluating on the training split.
I0316 12:16:09.641345 140240332842176 spec.py:333] Evaluating on the validation split.
I0316 12:20:32.025277 140240332842176 spec.py:349] Evaluating on the test split.
I0316 12:25:25.844827 140240332842176 submission_runner.py:469] Time since start: 3915.86s, 	Step: 378, 	{'train/loss': 0.12744881773360012, 'validation/loss': 0.12773404004344757, 'validation/num_examples': 83274637, 'test/loss': 0.13001984895071733, 'test/num_examples': 95000000, 'score': 369.7481427192688, 'total_duration': 3915.855469942093, 'accumulated_submission_time': 369.7481427192688, 'accumulated_eval_time': 3542.8878247737885, 'accumulated_logging_time': 0.04726409912109375}
I0316 12:25:25.853795 140198008456960 logging_writer.py:48] [378] accumulated_eval_time=3542.89, accumulated_logging_time=0.0472641, accumulated_submission_time=369.748, global_step=378, preemption_count=0, score=369.748, test/loss=0.13002, test/num_examples=95000000, total_duration=3915.86, train/loss=0.127449, validation/loss=0.127734, validation/num_examples=83274637
I0316 12:27:15.850188 140198016849664 logging_writer.py:48] [500] global_step=500, grad_norm=0.047943, loss=0.130165
I0316 12:27:15.853539 140240332842176 submission.py:265] 500) loss = 0.130, grad_norm = 0.048
I0316 12:27:27.173625 140240332842176 spec.py:321] Evaluating on the training split.
I0316 12:32:42.093860 140240332842176 spec.py:333] Evaluating on the validation split.
I0316 12:36:49.687326 140240332842176 spec.py:349] Evaluating on the test split.
I0316 12:41:27.358332 140240332842176 submission_runner.py:469] Time since start: 4877.37s, 	Step: 510, 	{'train/loss': 0.12482675294412515, 'validation/loss': 0.12677944946267697, 'validation/num_examples': 83274637, 'test/loss': 0.12915272029595626, 'test/num_examples': 95000000, 'score': 490.18702387809753, 'total_duration': 4877.36904501915, 'accumulated_submission_time': 490.18702387809753, 'accumulated_eval_time': 4383.0726153850555, 'accumulated_logging_time': 0.10628533363342285}
I0316 12:41:27.367309 140198008456960 logging_writer.py:48] [510] accumulated_eval_time=4383.07, accumulated_logging_time=0.106285, accumulated_submission_time=490.187, global_step=510, preemption_count=0, score=490.187, test/loss=0.129153, test/num_examples=95000000, total_duration=4877.37, train/loss=0.124827, validation/loss=0.126779, validation/num_examples=83274637
I0316 12:43:28.038835 140240332842176 spec.py:321] Evaluating on the training split.
I0316 12:48:36.893140 140240332842176 spec.py:333] Evaluating on the validation split.
I0316 12:52:42.810478 140240332842176 spec.py:349] Evaluating on the test split.
I0316 12:57:19.467286 140240332842176 submission_runner.py:469] Time since start: 5829.48s, 	Step: 632, 	{'train/loss': 0.12597920408096078, 'validation/loss': 0.12661762605333285, 'validation/num_examples': 83274637, 'test/loss': 0.12893914054685893, 'test/num_examples': 95000000, 'score': 609.9253125190735, 'total_duration': 5829.477991342545, 'accumulated_submission_time': 609.9253125190735, 'accumulated_eval_time': 5214.501209974289, 'accumulated_logging_time': 0.12182736396789551}
I0316 12:57:19.476279 140198016849664 logging_writer.py:48] [632] accumulated_eval_time=5214.5, accumulated_logging_time=0.121827, accumulated_submission_time=609.925, global_step=632, preemption_count=0, score=609.925, test/loss=0.128939, test/num_examples=95000000, total_duration=5829.48, train/loss=0.125979, validation/loss=0.126618, validation/num_examples=83274637
I0316 12:59:20.592970 140240332842176 spec.py:321] Evaluating on the training split.
I0316 13:04:31.793813 140240332842176 spec.py:333] Evaluating on the validation split.
I0316 13:08:32.195061 140240332842176 spec.py:349] Evaluating on the test split.
I0316 13:13:06.604588 140240332842176 submission_runner.py:469] Time since start: 6776.62s, 	Step: 756, 	{'train/loss': 0.124095681240515, 'validation/loss': 0.12653849013641197, 'validation/num_examples': 83274637, 'test/loss': 0.12897276007329037, 'test/num_examples': 95000000, 'score': 730.1396870613098, 'total_duration': 6776.615259408951, 'accumulated_submission_time': 730.1396870613098, 'accumulated_eval_time': 6040.513000011444, 'accumulated_logging_time': 0.13712263107299805}
I0316 13:13:06.644476 140198008456960 logging_writer.py:48] [756] accumulated_eval_time=6040.51, accumulated_logging_time=0.137123, accumulated_submission_time=730.14, global_step=756, preemption_count=0, score=730.14, test/loss=0.128973, test/num_examples=95000000, total_duration=6776.62, train/loss=0.124096, validation/loss=0.126538, validation/num_examples=83274637
I0316 13:15:07.026290 140240332842176 spec.py:321] Evaluating on the training split.
I0316 13:20:09.370636 140240332842176 spec.py:333] Evaluating on the validation split.
I0316 13:24:12.379769 140240332842176 spec.py:349] Evaluating on the test split.
I0316 13:28:47.342391 140240332842176 submission_runner.py:469] Time since start: 7717.35s, 	Step: 880, 	{'train/loss': 0.12512304012454972, 'validation/loss': 0.1266598414533779, 'validation/num_examples': 83274637, 'test/loss': 0.12913931590616326, 'test/num_examples': 95000000, 'score': 849.6865541934967, 'total_duration': 7717.353037595749, 'accumulated_submission_time': 849.6865541934967, 'accumulated_eval_time': 6860.82909655571, 'accumulated_logging_time': 0.18421077728271484}
I0316 13:28:47.353343 140198016849664 logging_writer.py:48] [880] accumulated_eval_time=6860.83, accumulated_logging_time=0.184211, accumulated_submission_time=849.687, global_step=880, preemption_count=0, score=849.687, test/loss=0.129139, test/num_examples=95000000, total_duration=7717.35, train/loss=0.125123, validation/loss=0.12666, validation/num_examples=83274637
I0316 13:30:39.698565 140198008456960 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.0636564, loss=0.127579
I0316 13:30:39.701631 140240332842176 submission.py:265] 1000) loss = 0.128, grad_norm = 0.064
I0316 13:30:48.343251 140240332842176 spec.py:321] Evaluating on the training split.
I0316 13:35:28.300407 140240332842176 spec.py:333] Evaluating on the validation split.
I0316 13:39:23.086467 140240332842176 spec.py:349] Evaluating on the test split.
I0316 13:43:44.739547 140240332842176 submission_runner.py:469] Time since start: 8614.75s, 	Step: 1007, 	{'train/loss': 0.12717237796760128, 'validation/loss': 0.12620122136588827, 'validation/num_examples': 83274637, 'test/loss': 0.12862537911923058, 'test/num_examples': 95000000, 'score': 969.7576699256897, 'total_duration': 8614.750085830688, 'accumulated_submission_time': 969.7576699256897, 'accumulated_eval_time': 7637.225449085236, 'accumulated_logging_time': 0.20209026336669922}
I0316 13:43:44.749071 140198016849664 logging_writer.py:48] [1007] accumulated_eval_time=7637.23, accumulated_logging_time=0.20209, accumulated_submission_time=969.758, global_step=1007, preemption_count=0, score=969.758, test/loss=0.128625, test/num_examples=95000000, total_duration=8614.75, train/loss=0.127172, validation/loss=0.126201, validation/num_examples=83274637
I0316 13:45:46.031794 140240332842176 spec.py:321] Evaluating on the training split.
I0316 13:50:20.025376 140240332842176 spec.py:333] Evaluating on the validation split.
I0316 13:54:09.431042 140240332842176 spec.py:349] Evaluating on the test split.
I0316 13:58:28.028085 140240332842176 submission_runner.py:469] Time since start: 9498.04s, 	Step: 1132, 	{'train/loss': 0.1252772134907647, 'validation/loss': 0.1262649497025433, 'validation/num_examples': 83274637, 'test/loss': 0.12860496375439293, 'test/num_examples': 95000000, 'score': 1090.1197173595428, 'total_duration': 9498.038784503937, 'accumulated_submission_time': 1090.1197173595428, 'accumulated_eval_time': 8399.222056865692, 'accumulated_logging_time': 0.21722841262817383}
I0316 13:58:28.037159 140198008456960 logging_writer.py:48] [1132] accumulated_eval_time=8399.22, accumulated_logging_time=0.217228, accumulated_submission_time=1090.12, global_step=1132, preemption_count=0, score=1090.12, test/loss=0.128605, test/num_examples=95000000, total_duration=9498.04, train/loss=0.125277, validation/loss=0.126265, validation/num_examples=83274637
I0316 14:00:29.429643 140240332842176 spec.py:321] Evaluating on the training split.
I0316 14:04:22.353756 140240332842176 spec.py:333] Evaluating on the validation split.
I0316 14:07:46.451171 140240332842176 spec.py:349] Evaluating on the test split.
I0316 14:11:50.221280 140240332842176 submission_runner.py:469] Time since start: 10300.23s, 	Step: 1259, 	{'train/loss': 0.12383838672366432, 'validation/loss': 0.12584647884617245, 'validation/num_examples': 83274637, 'test/loss': 0.12822483381488198, 'test/num_examples': 95000000, 'score': 1210.5336847305298, 'total_duration': 10300.231985569, 'accumulated_submission_time': 1210.5336847305298, 'accumulated_eval_time': 9080.013966798782, 'accumulated_logging_time': 0.29767870903015137}
I0316 14:11:50.230418 140198016849664 logging_writer.py:48] [1259] accumulated_eval_time=9080.01, accumulated_logging_time=0.297679, accumulated_submission_time=1210.53, global_step=1259, preemption_count=0, score=1210.53, test/loss=0.128225, test/num_examples=95000000, total_duration=10300.2, train/loss=0.123838, validation/loss=0.125846, validation/num_examples=83274637
I0316 14:13:50.674003 140240332842176 spec.py:321] Evaluating on the training split.
I0316 14:16:46.606415 140240332842176 spec.py:333] Evaluating on the validation split.
I0316 14:19:27.941200 140240332842176 spec.py:349] Evaluating on the test split.
I0316 14:22:53.752426 140240332842176 submission_runner.py:469] Time since start: 10963.76s, 	Step: 1383, 	{'train/loss': 0.12502639434846735, 'validation/loss': 0.12575809654793133, 'validation/num_examples': 83274637, 'test/loss': 0.12800088762472053, 'test/num_examples': 95000000, 'score': 1330.0491933822632, 'total_duration': 10963.763128042221, 'accumulated_submission_time': 1330.0491933822632, 'accumulated_eval_time': 9623.092582702637, 'accumulated_logging_time': 0.31249403953552246}
I0316 14:22:53.762026 140198008456960 logging_writer.py:48] [1383] accumulated_eval_time=9623.09, accumulated_logging_time=0.312494, accumulated_submission_time=1330.05, global_step=1383, preemption_count=0, score=1330.05, test/loss=0.128001, test/num_examples=95000000, total_duration=10963.8, train/loss=0.125026, validation/loss=0.125758, validation/num_examples=83274637
I0316 14:24:49.738131 140198016849664 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.00923219, loss=0.126296
I0316 14:24:49.741408 140240332842176 submission.py:265] 1500) loss = 0.126, grad_norm = 0.009
I0316 14:24:55.079234 140240332842176 spec.py:321] Evaluating on the training split.
I0316 14:26:59.784665 140240332842176 spec.py:333] Evaluating on the validation split.
I0316 14:29:01.747453 140240332842176 spec.py:349] Evaluating on the test split.
I0316 14:31:41.371591 140240332842176 submission_runner.py:469] Time since start: 11491.38s, 	Step: 1505, 	{'train/loss': 0.12469699673244508, 'validation/loss': 0.12559443346755975, 'validation/num_examples': 83274637, 'test/loss': 0.12780763844761095, 'test/num_examples': 95000000, 'score': 1450.422436952591, 'total_duration': 11491.382278680801, 'accumulated_submission_time': 1450.422436952591, 'accumulated_eval_time': 10029.38510298729, 'accumulated_logging_time': 0.3281874656677246}
I0316 14:31:41.381942 140198008456960 logging_writer.py:48] [1505] accumulated_eval_time=10029.4, accumulated_logging_time=0.328187, accumulated_submission_time=1450.42, global_step=1505, preemption_count=0, score=1450.42, test/loss=0.127808, test/num_examples=95000000, total_duration=11491.4, train/loss=0.124697, validation/loss=0.125594, validation/num_examples=83274637
I0316 14:33:42.268519 140240332842176 spec.py:321] Evaluating on the training split.
I0316 14:35:45.250114 140240332842176 spec.py:333] Evaluating on the validation split.
I0316 14:37:48.048770 140240332842176 spec.py:349] Evaluating on the test split.
I0316 14:40:10.067620 140240332842176 submission_runner.py:469] Time since start: 12000.08s, 	Step: 1626, 	{'train/loss': 0.12469317086654587, 'validation/loss': 0.12586131330276923, 'validation/num_examples': 83274637, 'test/loss': 0.1283752979091042, 'test/num_examples': 95000000, 'score': 1570.3747045993805, 'total_duration': 12000.078302383423, 'accumulated_submission_time': 1570.3747045993805, 'accumulated_eval_time': 10417.184427976608, 'accumulated_logging_time': 0.3449862003326416}
I0316 14:40:10.078502 140198016849664 logging_writer.py:48] [1626] accumulated_eval_time=10417.2, accumulated_logging_time=0.344986, accumulated_submission_time=1570.37, global_step=1626, preemption_count=0, score=1570.37, test/loss=0.128375, test/num_examples=95000000, total_duration=12000.1, train/loss=0.124693, validation/loss=0.125861, validation/num_examples=83274637
I0316 14:42:10.671094 140240332842176 spec.py:321] Evaluating on the training split.
I0316 14:44:13.861841 140240332842176 spec.py:333] Evaluating on the validation split.
I0316 14:46:17.711545 140240332842176 spec.py:349] Evaluating on the test split.
I0316 14:48:39.656108 140240332842176 submission_runner.py:469] Time since start: 12509.67s, 	Step: 1748, 	{'train/loss': 0.12378551211840591, 'validation/loss': 0.12566993102058396, 'validation/num_examples': 83274637, 'test/loss': 0.1280179622587505, 'test/num_examples': 95000000, 'score': 1690.0266008377075, 'total_duration': 12509.666815519333, 'accumulated_submission_time': 1690.0266008377075, 'accumulated_eval_time': 10806.169630527496, 'accumulated_logging_time': 0.3702843189239502}
I0316 14:48:39.666639 140198008456960 logging_writer.py:48] [1748] accumulated_eval_time=10806.2, accumulated_logging_time=0.370284, accumulated_submission_time=1690.03, global_step=1748, preemption_count=0, score=1690.03, test/loss=0.128018, test/num_examples=95000000, total_duration=12509.7, train/loss=0.123786, validation/loss=0.12567, validation/num_examples=83274637
I0316 14:50:40.447001 140240332842176 spec.py:321] Evaluating on the training split.
I0316 14:52:43.814348 140240332842176 spec.py:333] Evaluating on the validation split.
I0316 14:54:46.893293 140240332842176 spec.py:349] Evaluating on the test split.
I0316 14:57:08.168645 140240332842176 submission_runner.py:469] Time since start: 13018.18s, 	Step: 1872, 	{'train/loss': 0.12503364133014072, 'validation/loss': 0.12556650811700393, 'validation/num_examples': 83274637, 'test/loss': 0.1277942565799111, 'test/num_examples': 95000000, 'score': 1809.858027458191, 'total_duration': 13018.179370641708, 'accumulated_submission_time': 1809.858027458191, 'accumulated_eval_time': 11193.891434431076, 'accumulated_logging_time': 0.38739466667175293}
I0316 14:57:08.178736 140198016849664 logging_writer.py:48] [1872] accumulated_eval_time=11193.9, accumulated_logging_time=0.387395, accumulated_submission_time=1809.86, global_step=1872, preemption_count=0, score=1809.86, test/loss=0.127794, test/num_examples=95000000, total_duration=13018.2, train/loss=0.125034, validation/loss=0.125567, validation/num_examples=83274637
I0316 14:59:08.828277 140240332842176 spec.py:321] Evaluating on the training split.
I0316 15:01:10.755645 140240332842176 spec.py:333] Evaluating on the validation split.
I0316 15:03:13.383024 140240332842176 spec.py:349] Evaluating on the test split.
I0316 15:05:34.545979 140240332842176 submission_runner.py:469] Time since start: 13524.56s, 	Step: 1997, 	{'train/loss': 0.12305802635886266, 'validation/loss': 0.12532471890807986, 'validation/num_examples': 83274637, 'test/loss': 0.1276955578422948, 'test/num_examples': 95000000, 'score': 1929.5862274169922, 'total_duration': 13524.556694507599, 'accumulated_submission_time': 1929.5862274169922, 'accumulated_eval_time': 11579.609279632568, 'accumulated_logging_time': 0.4042544364929199}
I0316 15:05:34.555191 140198008456960 logging_writer.py:48] [1997] accumulated_eval_time=11579.6, accumulated_logging_time=0.404254, accumulated_submission_time=1929.59, global_step=1997, preemption_count=0, score=1929.59, test/loss=0.127696, test/num_examples=95000000, total_duration=13524.6, train/loss=0.123058, validation/loss=0.125325, validation/num_examples=83274637
I0316 15:05:35.818608 140198016849664 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.0148306, loss=0.123172
I0316 15:05:35.821880 140240332842176 submission.py:265] 2000) loss = 0.123, grad_norm = 0.015
I0316 15:07:35.646444 140240332842176 spec.py:321] Evaluating on the training split.
I0316 15:09:37.809839 140240332842176 spec.py:333] Evaluating on the validation split.
I0316 15:11:39.724298 140240332842176 spec.py:349] Evaluating on the test split.
I0316 15:14:00.814375 140240332842176 submission_runner.py:469] Time since start: 14030.83s, 	Step: 2119, 	{'train/loss': 0.12334051020174101, 'validation/loss': 0.12568768069468575, 'validation/num_examples': 83274637, 'test/loss': 0.12800148854105095, 'test/num_examples': 95000000, 'score': 2049.77064371109, 'total_duration': 14030.825075149536, 'accumulated_submission_time': 2049.77064371109, 'accumulated_eval_time': 11964.77750134468, 'accumulated_logging_time': 0.4209280014038086}
I0316 15:14:00.824441 140198008456960 logging_writer.py:48] [2119] accumulated_eval_time=11964.8, accumulated_logging_time=0.420928, accumulated_submission_time=2049.77, global_step=2119, preemption_count=0, score=2049.77, test/loss=0.128001, test/num_examples=95000000, total_duration=14030.8, train/loss=0.123341, validation/loss=0.125688, validation/num_examples=83274637
I0316 15:16:02.070892 140240332842176 spec.py:321] Evaluating on the training split.
I0316 15:18:04.986497 140240332842176 spec.py:333] Evaluating on the validation split.
I0316 15:20:08.669285 140240332842176 spec.py:349] Evaluating on the test split.
I0316 15:22:31.072963 140240332842176 submission_runner.py:469] Time since start: 14541.08s, 	Step: 2242, 	{'train/loss': 0.12435508368751384, 'validation/loss': 0.12567469508568496, 'validation/num_examples': 83274637, 'test/loss': 0.12798783229008726, 'test/num_examples': 95000000, 'score': 2170.058862686157, 'total_duration': 14541.083662509918, 'accumulated_submission_time': 2170.058862686157, 'accumulated_eval_time': 12353.779723644257, 'accumulated_logging_time': 0.450808048248291}
I0316 15:22:31.083606 140198016849664 logging_writer.py:48] [2242] accumulated_eval_time=12353.8, accumulated_logging_time=0.450808, accumulated_submission_time=2170.06, global_step=2242, preemption_count=0, score=2170.06, test/loss=0.127988, test/num_examples=95000000, total_duration=14541.1, train/loss=0.124355, validation/loss=0.125675, validation/num_examples=83274637
I0316 15:24:32.219060 140240332842176 spec.py:321] Evaluating on the training split.
I0316 15:26:33.725992 140240332842176 spec.py:333] Evaluating on the validation split.
I0316 15:28:35.751428 140240332842176 spec.py:349] Evaluating on the test split.
I0316 15:30:56.359834 140240332842176 submission_runner.py:469] Time since start: 15046.37s, 	Step: 2364, 	{'train/loss': 0.12404983314266309, 'validation/loss': 0.12497066234662052, 'validation/num_examples': 83274637, 'test/loss': 0.12731698940823202, 'test/num_examples': 95000000, 'score': 2290.342180490494, 'total_duration': 15046.370571136475, 'accumulated_submission_time': 2290.342180490494, 'accumulated_eval_time': 12737.920649766922, 'accumulated_logging_time': 0.4676778316497803}
I0316 15:30:56.369880 140198008456960 logging_writer.py:48] [2364] accumulated_eval_time=12737.9, accumulated_logging_time=0.467678, accumulated_submission_time=2290.34, global_step=2364, preemption_count=0, score=2290.34, test/loss=0.127317, test/num_examples=95000000, total_duration=15046.4, train/loss=0.12405, validation/loss=0.124971, validation/num_examples=83274637
I0316 15:32:57.890014 140240332842176 spec.py:321] Evaluating on the training split.
I0316 15:34:59.998571 140240332842176 spec.py:333] Evaluating on the validation split.
I0316 15:37:02.577552 140240332842176 spec.py:349] Evaluating on the test split.
I0316 15:39:23.455096 140240332842176 submission_runner.py:469] Time since start: 15553.47s, 	Step: 2483, 	{'train/loss': 0.12148589458099995, 'validation/loss': 0.1251575257380122, 'validation/num_examples': 83274637, 'test/loss': 0.12754113665787548, 'test/num_examples': 95000000, 'score': 2410.952310323715, 'total_duration': 15553.465792179108, 'accumulated_submission_time': 2410.952310323715, 'accumulated_eval_time': 13123.485873699188, 'accumulated_logging_time': 0.4843122959136963}
I0316 15:39:23.465335 140198016849664 logging_writer.py:48] [2483] accumulated_eval_time=13123.5, accumulated_logging_time=0.484312, accumulated_submission_time=2410.95, global_step=2483, preemption_count=0, score=2410.95, test/loss=0.127541, test/num_examples=95000000, total_duration=15553.5, train/loss=0.121486, validation/loss=0.125158, validation/num_examples=83274637
I0316 15:39:27.275016 140198008456960 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.046245, loss=0.126474
I0316 15:39:27.278566 140240332842176 submission.py:265] 2500) loss = 0.126, grad_norm = 0.046
I0316 15:41:24.544767 140240332842176 spec.py:321] Evaluating on the training split.
I0316 15:43:27.605598 140240332842176 spec.py:333] Evaluating on the validation split.
I0316 15:45:30.643894 140240332842176 spec.py:349] Evaluating on the test split.
I0316 15:47:52.795830 140240332842176 submission_runner.py:469] Time since start: 16062.81s, 	Step: 2602, 	{'train/loss': 0.12345951200368885, 'validation/loss': 0.12497655737716233, 'validation/num_examples': 83274637, 'test/loss': 0.1273533337187516, 'test/num_examples': 95000000, 'score': 2531.2009110450745, 'total_duration': 16062.806546926498, 'accumulated_submission_time': 2531.2009110450745, 'accumulated_eval_time': 13511.737191200256, 'accumulated_logging_time': 0.501079797744751}
I0316 15:47:52.806329 140198016849664 logging_writer.py:48] [2602] accumulated_eval_time=13511.7, accumulated_logging_time=0.50108, accumulated_submission_time=2531.2, global_step=2602, preemption_count=0, score=2531.2, test/loss=0.127353, test/num_examples=95000000, total_duration=16062.8, train/loss=0.12346, validation/loss=0.124977, validation/num_examples=83274637
I0316 15:49:54.068412 140240332842176 spec.py:321] Evaluating on the training split.
I0316 15:51:56.976095 140240332842176 spec.py:333] Evaluating on the validation split.
I0316 15:54:00.316882 140240332842176 spec.py:349] Evaluating on the test split.
I0316 15:56:22.417635 140240332842176 submission_runner.py:469] Time since start: 16572.43s, 	Step: 2724, 	{'train/loss': 0.12409650539155201, 'validation/loss': 0.1253635300489712, 'validation/num_examples': 83274637, 'test/loss': 0.12787726730965063, 'test/num_examples': 95000000, 'score': 2651.5256958007812, 'total_duration': 16572.428359746933, 'accumulated_submission_time': 2651.5256958007812, 'accumulated_eval_time': 13900.086666107178, 'accumulated_logging_time': 0.5264492034912109}
I0316 15:56:22.427603 140198008456960 logging_writer.py:48] [2724] accumulated_eval_time=13900.1, accumulated_logging_time=0.526449, accumulated_submission_time=2651.53, global_step=2724, preemption_count=0, score=2651.53, test/loss=0.127877, test/num_examples=95000000, total_duration=16572.4, train/loss=0.124097, validation/loss=0.125364, validation/num_examples=83274637
I0316 15:58:23.307929 140240332842176 spec.py:321] Evaluating on the training split.
I0316 16:00:24.841898 140240332842176 spec.py:333] Evaluating on the validation split.
I0316 16:02:26.001652 140240332842176 spec.py:349] Evaluating on the test split.
I0316 16:04:45.372675 140240332842176 submission_runner.py:469] Time since start: 17075.38s, 	Step: 2844, 	{'train/loss': 0.1245414673719671, 'validation/loss': 0.12486033263301224, 'validation/num_examples': 83274637, 'test/loss': 0.12726854740825452, 'test/num_examples': 95000000, 'score': 2771.4885416030884, 'total_duration': 17075.383397340775, 'accumulated_submission_time': 2771.4885416030884, 'accumulated_eval_time': 14282.151737451553, 'accumulated_logging_time': 0.5429379940032959}
I0316 16:04:45.383529 140198016849664 logging_writer.py:48] [2844] accumulated_eval_time=14282.2, accumulated_logging_time=0.542938, accumulated_submission_time=2771.49, global_step=2844, preemption_count=0, score=2771.49, test/loss=0.127269, test/num_examples=95000000, total_duration=17075.4, train/loss=0.124541, validation/loss=0.12486, validation/num_examples=83274637
I0316 16:06:46.817970 140240332842176 spec.py:321] Evaluating on the training split.
I0316 16:08:48.572472 140240332842176 spec.py:333] Evaluating on the validation split.
I0316 16:10:50.191952 140240332842176 spec.py:349] Evaluating on the test split.
I0316 16:13:08.788634 140240332842176 submission_runner.py:469] Time since start: 17578.80s, 	Step: 2965, 	{'train/loss': 0.12336719852974615, 'validation/loss': 0.12501690166776105, 'validation/num_examples': 83274637, 'test/loss': 0.127361217958631, 'test/num_examples': 95000000, 'score': 2892.0546729564667, 'total_duration': 17578.799298763275, 'accumulated_submission_time': 2892.0546729564667, 'accumulated_eval_time': 14664.122420310974, 'accumulated_logging_time': 0.5616066455841064}
I0316 16:13:08.799878 140198008456960 logging_writer.py:48] [2965] accumulated_eval_time=14664.1, accumulated_logging_time=0.561607, accumulated_submission_time=2892.05, global_step=2965, preemption_count=0, score=2892.05, test/loss=0.127361, test/num_examples=95000000, total_duration=17578.8, train/loss=0.123367, validation/loss=0.125017, validation/num_examples=83274637
I0316 16:13:22.272208 140198016849664 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.012517, loss=0.120648
I0316 16:13:22.275597 140240332842176 submission.py:265] 3000) loss = 0.121, grad_norm = 0.013
I0316 16:15:10.662061 140240332842176 spec.py:321] Evaluating on the training split.
I0316 16:17:12.455666 140240332842176 spec.py:333] Evaluating on the validation split.
I0316 16:19:14.376609 140240332842176 spec.py:349] Evaluating on the test split.
I0316 16:21:35.183600 140240332842176 submission_runner.py:469] Time since start: 18085.19s, 	Step: 3085, 	{'train/loss': 0.12312469988672556, 'validation/loss': 0.12484348505668026, 'validation/num_examples': 83274637, 'test/loss': 0.12721263430007132, 'test/num_examples': 95000000, 'score': 3012.972417116165, 'total_duration': 18085.194281578064, 'accumulated_submission_time': 3012.972417116165, 'accumulated_eval_time': 15048.644133329391, 'accumulated_logging_time': 0.5799634456634521}
I0316 16:21:35.194837 140198008456960 logging_writer.py:48] [3085] accumulated_eval_time=15048.6, accumulated_logging_time=0.579963, accumulated_submission_time=3012.97, global_step=3085, preemption_count=0, score=3012.97, test/loss=0.127213, test/num_examples=95000000, total_duration=18085.2, train/loss=0.123125, validation/loss=0.124843, validation/num_examples=83274637
I0316 16:23:36.107541 140240332842176 spec.py:321] Evaluating on the training split.
I0316 16:25:38.031416 140240332842176 spec.py:333] Evaluating on the validation split.
I0316 16:27:40.410461 140240332842176 spec.py:349] Evaluating on the test split.
I0316 16:30:00.205045 140240332842176 submission_runner.py:469] Time since start: 18590.22s, 	Step: 3207, 	{'train/loss': 0.12431106850339609, 'validation/loss': 0.12470756652449695, 'validation/num_examples': 83274637, 'test/loss': 0.12698579815701935, 'test/num_examples': 95000000, 'score': 3132.870416164398, 'total_duration': 18590.215764522552, 'accumulated_submission_time': 3132.870416164398, 'accumulated_eval_time': 15432.74186706543, 'accumulated_logging_time': 0.669574499130249}
I0316 16:30:00.215193 140198016849664 logging_writer.py:48] [3207] accumulated_eval_time=15432.7, accumulated_logging_time=0.669574, accumulated_submission_time=3132.87, global_step=3207, preemption_count=0, score=3132.87, test/loss=0.126986, test/num_examples=95000000, total_duration=18590.2, train/loss=0.124311, validation/loss=0.124708, validation/num_examples=83274637
I0316 16:32:01.164299 140240332842176 spec.py:321] Evaluating on the training split.
I0316 16:34:03.311922 140240332842176 spec.py:333] Evaluating on the validation split.
I0316 16:36:05.362257 140240332842176 spec.py:349] Evaluating on the test split.
I0316 16:38:24.765406 140240332842176 submission_runner.py:469] Time since start: 19094.78s, 	Step: 3327, 	{'train/loss': 0.12430036228429361, 'validation/loss': 0.12519562843799914, 'validation/num_examples': 83274637, 'test/loss': 0.1275745328320152, 'test/num_examples': 95000000, 'score': 3252.907784461975, 'total_duration': 19094.776104211807, 'accumulated_submission_time': 3252.907784461975, 'accumulated_eval_time': 15816.343222856522, 'accumulated_logging_time': 0.687305212020874}
I0316 16:38:24.776638 140198008456960 logging_writer.py:48] [3327] accumulated_eval_time=15816.3, accumulated_logging_time=0.687305, accumulated_submission_time=3252.91, global_step=3327, preemption_count=0, score=3252.91, test/loss=0.127575, test/num_examples=95000000, total_duration=19094.8, train/loss=0.1243, validation/loss=0.125196, validation/num_examples=83274637
I0316 16:40:25.840967 140240332842176 spec.py:321] Evaluating on the training split.
I0316 16:42:27.427048 140240332842176 spec.py:333] Evaluating on the validation split.
I0316 16:44:29.397394 140240332842176 spec.py:349] Evaluating on the test split.
I0316 16:46:48.901748 140240332842176 submission_runner.py:469] Time since start: 19598.91s, 	Step: 3451, 	{'train/loss': 0.12487519648413099, 'validation/loss': 0.12470368655052513, 'validation/num_examples': 83274637, 'test/loss': 0.1271353808726662, 'test/num_examples': 95000000, 'score': 3373.019310951233, 'total_duration': 19598.912457704544, 'accumulated_submission_time': 3373.019310951233, 'accumulated_eval_time': 16199.404266119003, 'accumulated_logging_time': 0.705085039138794}
I0316 16:46:48.912180 140198016849664 logging_writer.py:48] [3451] accumulated_eval_time=16199.4, accumulated_logging_time=0.705085, accumulated_submission_time=3373.02, global_step=3451, preemption_count=0, score=3373.02, test/loss=0.127135, test/num_examples=95000000, total_duration=19598.9, train/loss=0.124875, validation/loss=0.124704, validation/num_examples=83274637
I0316 16:47:19.266305 140198008456960 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.0133024, loss=0.130267
I0316 16:47:19.269328 140240332842176 submission.py:265] 3500) loss = 0.130, grad_norm = 0.013
I0316 16:48:50.853336 140240332842176 spec.py:321] Evaluating on the training split.
I0316 16:50:51.897633 140240332842176 spec.py:333] Evaluating on the validation split.
I0316 16:52:53.492048 140240332842176 spec.py:349] Evaluating on the test split.
I0316 16:55:13.344422 140240332842176 submission_runner.py:469] Time since start: 20103.36s, 	Step: 3574, 	{'train/loss': 0.12354109774833058, 'validation/loss': 0.1246863197626124, 'validation/num_examples': 83274637, 'test/loss': 0.12699308340088694, 'test/num_examples': 95000000, 'score': 3494.046045064926, 'total_duration': 20103.35507631302, 'accumulated_submission_time': 3494.046045064926, 'accumulated_eval_time': 16581.895528316498, 'accumulated_logging_time': 0.7218673229217529}
I0316 16:55:13.355442 140198016849664 logging_writer.py:48] [3574] accumulated_eval_time=16581.9, accumulated_logging_time=0.721867, accumulated_submission_time=3494.05, global_step=3574, preemption_count=0, score=3494.05, test/loss=0.126993, test/num_examples=95000000, total_duration=20103.4, train/loss=0.123541, validation/loss=0.124686, validation/num_examples=83274637
I0316 16:57:14.746754 140240332842176 spec.py:321] Evaluating on the training split.
I0316 16:59:16.181899 140240332842176 spec.py:333] Evaluating on the validation split.
I0316 17:01:18.157097 140240332842176 spec.py:349] Evaluating on the test split.
I0316 17:03:38.472216 140240332842176 submission_runner.py:469] Time since start: 20608.48s, 	Step: 3697, 	{'train/loss': 0.12234205728503593, 'validation/loss': 0.12455602125640246, 'validation/num_examples': 83274637, 'test/loss': 0.12690470672009116, 'test/num_examples': 95000000, 'score': 3614.52832531929, 'total_duration': 20608.48293185234, 'accumulated_submission_time': 3614.52832531929, 'accumulated_eval_time': 16965.62119626999, 'accumulated_logging_time': 0.7573072910308838}
I0316 17:03:38.482740 140198008456960 logging_writer.py:48] [3697] accumulated_eval_time=16965.6, accumulated_logging_time=0.757307, accumulated_submission_time=3614.53, global_step=3697, preemption_count=0, score=3614.53, test/loss=0.126905, test/num_examples=95000000, total_duration=20608.5, train/loss=0.122342, validation/loss=0.124556, validation/num_examples=83274637
I0316 17:05:39.056740 140240332842176 spec.py:321] Evaluating on the training split.
I0316 17:07:41.183180 140240332842176 spec.py:333] Evaluating on the validation split.
I0316 17:09:43.603871 140240332842176 spec.py:349] Evaluating on the test split.
I0316 17:12:03.984179 140240332842176 submission_runner.py:469] Time since start: 21113.99s, 	Step: 3819, 	{'train/loss': 0.12299460763446642, 'validation/loss': 0.12461414795494417, 'validation/num_examples': 83274637, 'test/loss': 0.1269171231848064, 'test/num_examples': 95000000, 'score': 3734.186513900757, 'total_duration': 21113.99488067627, 'accumulated_submission_time': 3734.186513900757, 'accumulated_eval_time': 17350.54878807068, 'accumulated_logging_time': 0.7742409706115723}
I0316 17:12:03.996514 140198016849664 logging_writer.py:48] [3819] accumulated_eval_time=17350.5, accumulated_logging_time=0.774241, accumulated_submission_time=3734.19, global_step=3819, preemption_count=0, score=3734.19, test/loss=0.126917, test/num_examples=95000000, total_duration=21114, train/loss=0.122995, validation/loss=0.124614, validation/num_examples=83274637
I0316 17:14:04.576260 140240332842176 spec.py:321] Evaluating on the training split.
I0316 17:16:06.665815 140240332842176 spec.py:333] Evaluating on the validation split.
I0316 17:18:09.287639 140240332842176 spec.py:349] Evaluating on the test split.
I0316 17:20:28.927686 140240332842176 submission_runner.py:469] Time since start: 21618.94s, 	Step: 3939, 	{'train/loss': 0.12422622337915551, 'validation/loss': 0.12451240127664018, 'validation/num_examples': 83274637, 'test/loss': 0.12689381959156237, 'test/num_examples': 95000000, 'score': 3853.833815097809, 'total_duration': 21618.938402175903, 'accumulated_submission_time': 3853.833815097809, 'accumulated_eval_time': 17734.90039372444, 'accumulated_logging_time': 0.7944328784942627}
I0316 17:20:28.938974 140198008456960 logging_writer.py:48] [3939] accumulated_eval_time=17734.9, accumulated_logging_time=0.794433, accumulated_submission_time=3853.83, global_step=3939, preemption_count=0, score=3853.83, test/loss=0.126894, test/num_examples=95000000, total_duration=21618.9, train/loss=0.124226, validation/loss=0.124512, validation/num_examples=83274637
I0316 17:21:13.733659 140198016849664 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.0173293, loss=0.126502
I0316 17:21:13.736619 140240332842176 submission.py:265] 4000) loss = 0.127, grad_norm = 0.017
I0316 17:22:30.266638 140240332842176 spec.py:321] Evaluating on the training split.
I0316 17:24:32.857482 140240332842176 spec.py:333] Evaluating on the validation split.
I0316 17:26:35.197493 140240332842176 spec.py:349] Evaluating on the test split.
I0316 17:28:56.786819 140240332842176 submission_runner.py:469] Time since start: 22126.80s, 	Step: 4058, 	{'train/loss': 0.12164704625757887, 'validation/loss': 0.12457594582099821, 'validation/num_examples': 83274637, 'test/loss': 0.12690206928004213, 'test/num_examples': 95000000, 'score': 3974.256808280945, 'total_duration': 22126.797513723373, 'accumulated_submission_time': 3974.256808280945, 'accumulated_eval_time': 18121.420876264572, 'accumulated_logging_time': 0.812131404876709}
I0316 17:28:56.797394 140198008456960 logging_writer.py:48] [4058] accumulated_eval_time=18121.4, accumulated_logging_time=0.812131, accumulated_submission_time=3974.26, global_step=4058, preemption_count=0, score=3974.26, test/loss=0.126902, test/num_examples=95000000, total_duration=22126.8, train/loss=0.121647, validation/loss=0.124576, validation/num_examples=83274637
I0316 17:30:57.603506 140240332842176 spec.py:321] Evaluating on the training split.
I0316 17:32:59.904893 140240332842176 spec.py:333] Evaluating on the validation split.
I0316 17:35:02.523453 140240332842176 spec.py:349] Evaluating on the test split.
I0316 17:37:22.188893 140240332842176 submission_runner.py:469] Time since start: 22632.20s, 	Step: 4179, 	{'train/loss': 0.12319872930177625, 'validation/loss': 0.1248843614322766, 'validation/num_examples': 83274637, 'test/loss': 0.12726214679521258, 'test/num_examples': 95000000, 'score': 4094.111385345459, 'total_duration': 22632.199587106705, 'accumulated_submission_time': 4094.111385345459, 'accumulated_eval_time': 18506.006427526474, 'accumulated_logging_time': 0.8819785118103027}
I0316 17:37:22.199407 140198016849664 logging_writer.py:48] [4179] accumulated_eval_time=18506, accumulated_logging_time=0.881979, accumulated_submission_time=4094.11, global_step=4179, preemption_count=0, score=4094.11, test/loss=0.127262, test/num_examples=95000000, total_duration=22632.2, train/loss=0.123199, validation/loss=0.124884, validation/num_examples=83274637
I0316 17:39:22.958426 140240332842176 spec.py:321] Evaluating on the training split.
I0316 17:41:24.411438 140240332842176 spec.py:333] Evaluating on the validation split.
I0316 17:43:26.379929 140240332842176 spec.py:349] Evaluating on the test split.
I0316 17:45:46.335155 140240332842176 submission_runner.py:469] Time since start: 23136.35s, 	Step: 4302, 	{'train/loss': 0.1221004590057181, 'validation/loss': 0.12456987971138345, 'validation/num_examples': 83274637, 'test/loss': 0.1269060338334736, 'test/num_examples': 95000000, 'score': 4214.001971721649, 'total_duration': 23136.345856666565, 'accumulated_submission_time': 4214.001971721649, 'accumulated_eval_time': 18889.383198976517, 'accumulated_logging_time': 0.8992903232574463}
I0316 17:45:46.346541 140198008456960 logging_writer.py:48] [4302] accumulated_eval_time=18889.4, accumulated_logging_time=0.89929, accumulated_submission_time=4214, global_step=4302, preemption_count=0, score=4214, test/loss=0.126906, test/num_examples=95000000, total_duration=23136.3, train/loss=0.1221, validation/loss=0.12457, validation/num_examples=83274637
I0316 17:47:47.170323 140240332842176 spec.py:321] Evaluating on the training split.
I0316 17:49:48.287964 140240332842176 spec.py:333] Evaluating on the validation split.
I0316 17:51:50.063616 140240332842176 spec.py:349] Evaluating on the test split.
I0316 17:54:09.711512 140240332842176 submission_runner.py:469] Time since start: 23639.72s, 	Step: 4424, 	{'train/loss': 0.12360468920974928, 'validation/loss': 0.12443367299695393, 'validation/num_examples': 83274637, 'test/loss': 0.12670118580490916, 'test/num_examples': 95000000, 'score': 4333.88579082489, 'total_duration': 23639.722157239914, 'accumulated_submission_time': 4333.88579082489, 'accumulated_eval_time': 19271.92449951172, 'accumulated_logging_time': 0.9178085327148438}
I0316 17:54:09.723465 140198016849664 logging_writer.py:48] [4424] accumulated_eval_time=19271.9, accumulated_logging_time=0.917809, accumulated_submission_time=4333.89, global_step=4424, preemption_count=0, score=4333.89, test/loss=0.126701, test/num_examples=95000000, total_duration=23639.7, train/loss=0.123605, validation/loss=0.124434, validation/num_examples=83274637
I0316 17:55:11.925382 140198008456960 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.0132833, loss=0.131839
I0316 17:55:11.928962 140240332842176 submission.py:265] 4500) loss = 0.132, grad_norm = 0.013
I0316 17:56:11.388014 140240332842176 spec.py:321] Evaluating on the training split.
I0316 17:58:14.183751 140240332842176 spec.py:333] Evaluating on the validation split.
I0316 18:00:17.248640 140240332842176 spec.py:349] Evaluating on the test split.
I0316 18:02:39.176028 140240332842176 submission_runner.py:469] Time since start: 24149.19s, 	Step: 4550, 	{'train/loss': 0.12492116034608433, 'validation/loss': 0.12443287295719907, 'validation/num_examples': 83274637, 'test/loss': 0.12669169909667968, 'test/num_examples': 95000000, 'score': 4454.601715564728, 'total_duration': 24149.186757087708, 'accumulated_submission_time': 4454.601715564728, 'accumulated_eval_time': 19659.712699651718, 'accumulated_logging_time': 0.9364697933197021}
I0316 18:02:39.186458 140198016849664 logging_writer.py:48] [4550] accumulated_eval_time=19659.7, accumulated_logging_time=0.93647, accumulated_submission_time=4454.6, global_step=4550, preemption_count=0, score=4454.6, test/loss=0.126692, test/num_examples=95000000, total_duration=24149.2, train/loss=0.124921, validation/loss=0.124433, validation/num_examples=83274637
I0316 18:04:41.013069 140240332842176 spec.py:321] Evaluating on the training split.
I0316 18:06:43.673693 140240332842176 spec.py:333] Evaluating on the validation split.
I0316 18:08:46.415932 140240332842176 spec.py:349] Evaluating on the test split.
I0316 18:11:08.293312 140240332842176 submission_runner.py:469] Time since start: 24658.30s, 	Step: 4670, 	{'train/loss': 0.12187220609110906, 'validation/loss': 0.12437263675760461, 'validation/num_examples': 83274637, 'test/loss': 0.12664845055847168, 'test/num_examples': 95000000, 'score': 4575.511076927185, 'total_duration': 24658.304027557373, 'accumulated_submission_time': 4575.511076927185, 'accumulated_eval_time': 20046.993185043335, 'accumulated_logging_time': 0.9539883136749268}
I0316 18:11:08.304274 140198008456960 logging_writer.py:48] [4670] accumulated_eval_time=20047, accumulated_logging_time=0.953988, accumulated_submission_time=4575.51, global_step=4670, preemption_count=0, score=4575.51, test/loss=0.126648, test/num_examples=95000000, total_duration=24658.3, train/loss=0.121872, validation/loss=0.124373, validation/num_examples=83274637
I0316 18:13:09.925912 140240332842176 spec.py:321] Evaluating on the training split.
I0316 18:15:12.915245 140240332842176 spec.py:333] Evaluating on the validation split.
I0316 18:17:15.467796 140240332842176 spec.py:349] Evaluating on the test split.
I0316 18:19:36.277163 140240332842176 submission_runner.py:469] Time since start: 25166.29s, 	Step: 4792, 	{'train/loss': 0.12254099113312987, 'validation/loss': 0.12432570039558764, 'validation/num_examples': 83274637, 'test/loss': 0.12659345702546773, 'test/num_examples': 95000000, 'score': 4696.194514751434, 'total_duration': 25166.28787255287, 'accumulated_submission_time': 4696.194514751434, 'accumulated_eval_time': 20433.344702243805, 'accumulated_logging_time': 0.9713873863220215}
I0316 18:19:36.288713 140198016849664 logging_writer.py:48] [4792] accumulated_eval_time=20433.3, accumulated_logging_time=0.971387, accumulated_submission_time=4696.19, global_step=4792, preemption_count=0, score=4696.19, test/loss=0.126593, test/num_examples=95000000, total_duration=25166.3, train/loss=0.122541, validation/loss=0.124326, validation/num_examples=83274637
I0316 18:21:37.773072 140240332842176 spec.py:321] Evaluating on the training split.
I0316 18:23:40.658624 140240332842176 spec.py:333] Evaluating on the validation split.
I0316 18:25:43.850542 140240332842176 spec.py:349] Evaluating on the test split.
I0316 18:28:04.294194 140240332842176 submission_runner.py:469] Time since start: 25674.30s, 	Step: 4915, 	{'train/loss': 0.12281449713316214, 'validation/loss': 0.12427417390192434, 'validation/num_examples': 83274637, 'test/loss': 0.1265910074301067, 'test/num_examples': 95000000, 'score': 4816.73704123497, 'total_duration': 25674.30486559868, 'accumulated_submission_time': 4816.73704123497, 'accumulated_eval_time': 20819.865918636322, 'accumulated_logging_time': 1.0013039112091064}
I0316 18:28:04.305854 140198008456960 logging_writer.py:48] [4915] accumulated_eval_time=20819.9, accumulated_logging_time=1.0013, accumulated_submission_time=4816.74, global_step=4915, preemption_count=0, score=4816.74, test/loss=0.126591, test/num_examples=95000000, total_duration=25674.3, train/loss=0.122814, validation/loss=0.124274, validation/num_examples=83274637
I0316 18:29:20.305021 140198016849664 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.00725653, loss=0.125206
I0316 18:29:20.308396 140240332842176 submission.py:265] 5000) loss = 0.125, grad_norm = 0.007
I0316 18:30:04.895305 140240332842176 spec.py:321] Evaluating on the training split.
I0316 18:32:07.974679 140240332842176 spec.py:333] Evaluating on the validation split.
I0316 18:34:10.317869 140240332842176 spec.py:349] Evaluating on the test split.
I0316 18:36:30.647112 140240332842176 submission_runner.py:469] Time since start: 26180.66s, 	Step: 5036, 	{'train/loss': 0.12259955243791079, 'validation/loss': 0.12435981578235637, 'validation/num_examples': 83274637, 'test/loss': 0.12665465016487523, 'test/num_examples': 95000000, 'score': 4936.459621429443, 'total_duration': 26180.657777071, 'accumulated_submission_time': 4936.459621429443, 'accumulated_eval_time': 21205.617896318436, 'accumulated_logging_time': 1.025479793548584}
I0316 18:36:30.669717 140198008456960 logging_writer.py:48] [5036] accumulated_eval_time=21205.6, accumulated_logging_time=1.02548, accumulated_submission_time=4936.46, global_step=5036, preemption_count=0, score=4936.46, test/loss=0.126655, test/num_examples=95000000, total_duration=26180.7, train/loss=0.1226, validation/loss=0.12436, validation/num_examples=83274637
I0316 18:38:31.544304 140240332842176 spec.py:321] Evaluating on the training split.
I0316 18:40:34.403059 140240332842176 spec.py:333] Evaluating on the validation split.
I0316 18:42:37.698781 140240332842176 spec.py:349] Evaluating on the test split.
I0316 18:44:59.519276 140240332842176 submission_runner.py:469] Time since start: 26689.53s, 	Step: 5158, 	{'train/loss': 0.12131350391145708, 'validation/loss': 0.12424445860304664, 'validation/num_examples': 83274637, 'test/loss': 0.12653710406678853, 'test/num_examples': 95000000, 'score': 5056.480705976486, 'total_duration': 26689.529962539673, 'accumulated_submission_time': 5056.480705976486, 'accumulated_eval_time': 21593.59312582016, 'accumulated_logging_time': 1.0556573867797852}
I0316 18:44:59.530411 140198016849664 logging_writer.py:48] [5158] accumulated_eval_time=21593.6, accumulated_logging_time=1.05566, accumulated_submission_time=5056.48, global_step=5158, preemption_count=0, score=5056.48, test/loss=0.126537, test/num_examples=95000000, total_duration=26689.5, train/loss=0.121314, validation/loss=0.124244, validation/num_examples=83274637
I0316 18:47:00.457486 140240332842176 spec.py:321] Evaluating on the training split.
I0316 18:49:03.275249 140240332842176 spec.py:333] Evaluating on the validation split.
I0316 18:51:06.553288 140240332842176 spec.py:349] Evaluating on the test split.
I0316 18:53:27.892140 140240332842176 submission_runner.py:469] Time since start: 27197.90s, 	Step: 5282, 	{'train/loss': 0.12388544183311653, 'validation/loss': 0.12422364962601995, 'validation/num_examples': 83274637, 'test/loss': 0.12653069809000117, 'test/num_examples': 95000000, 'score': 5176.468922615051, 'total_duration': 27197.902804136276, 'accumulated_submission_time': 5176.468922615051, 'accumulated_eval_time': 21981.02797150612, 'accumulated_logging_time': 1.0877408981323242}
I0316 18:53:27.903383 140198008456960 logging_writer.py:48] [5282] accumulated_eval_time=21981, accumulated_logging_time=1.08774, accumulated_submission_time=5176.47, global_step=5282, preemption_count=0, score=5176.47, test/loss=0.126531, test/num_examples=95000000, total_duration=27197.9, train/loss=0.123885, validation/loss=0.124224, validation/num_examples=83274637
I0316 18:55:28.883277 140240332842176 spec.py:321] Evaluating on the training split.
I0316 18:57:31.863559 140240332842176 spec.py:333] Evaluating on the validation split.
I0316 18:59:35.144296 140240332842176 spec.py:349] Evaluating on the test split.
I0316 19:01:56.471683 140240332842176 submission_runner.py:469] Time since start: 27706.48s, 	Step: 5406, 	{'train/loss': 0.1226023253193226, 'validation/loss': 0.12415736881237424, 'validation/num_examples': 83274637, 'test/loss': 0.12649470411670083, 'test/num_examples': 95000000, 'score': 5296.515435934067, 'total_duration': 27706.48237633705, 'accumulated_submission_time': 5296.515435934067, 'accumulated_eval_time': 22368.616581201553, 'accumulated_logging_time': 1.1066944599151611}
I0316 19:01:56.484101 140198016849664 logging_writer.py:48] [5406] accumulated_eval_time=22368.6, accumulated_logging_time=1.10669, accumulated_submission_time=5296.52, global_step=5406, preemption_count=0, score=5296.52, test/loss=0.126495, test/num_examples=95000000, total_duration=27706.5, train/loss=0.122602, validation/loss=0.124157, validation/num_examples=83274637
I0316 19:03:25.993577 140198008456960 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.00862341, loss=0.124469
I0316 19:03:25.996747 140240332842176 submission.py:265] 5500) loss = 0.124, grad_norm = 0.009
I0316 19:03:56.911923 140240332842176 spec.py:321] Evaluating on the training split.
I0316 19:05:59.984372 140240332842176 spec.py:333] Evaluating on the validation split.
I0316 19:08:03.103980 140240332842176 spec.py:349] Evaluating on the test split.
I0316 19:10:24.131089 140240332842176 submission_runner.py:469] Time since start: 28214.14s, 	Step: 5526, 	{'train/loss': 0.1230290738730423, 'validation/loss': 0.12419912630034761, 'validation/num_examples': 83274637, 'test/loss': 0.1265295892748381, 'test/num_examples': 95000000, 'score': 5416.104229450226, 'total_duration': 28214.141768693924, 'accumulated_submission_time': 5416.104229450226, 'accumulated_eval_time': 22755.835795879364, 'accumulated_logging_time': 1.1258301734924316}
I0316 19:10:24.142588 140198016849664 logging_writer.py:48] [5526] accumulated_eval_time=22755.8, accumulated_logging_time=1.12583, accumulated_submission_time=5416.1, global_step=5526, preemption_count=0, score=5416.1, test/loss=0.12653, test/num_examples=95000000, total_duration=28214.1, train/loss=0.123029, validation/loss=0.124199, validation/num_examples=83274637
I0316 19:12:25.651708 140240332842176 spec.py:321] Evaluating on the training split.
I0316 19:14:28.757931 140240332842176 spec.py:333] Evaluating on the validation split.
I0316 19:16:31.172512 140240332842176 spec.py:349] Evaluating on the test split.
I0316 19:18:52.353720 140240332842176 submission_runner.py:469] Time since start: 28722.36s, 	Step: 5648, 	{'train/loss': 0.12579171802237896, 'validation/loss': 0.12414156719962541, 'validation/num_examples': 83274637, 'test/loss': 0.1263909406642713, 'test/num_examples': 95000000, 'score': 5536.68524312973, 'total_duration': 28722.36442923546, 'accumulated_submission_time': 5536.68524312973, 'accumulated_eval_time': 23142.537957906723, 'accumulated_logging_time': 1.1443028450012207}
I0316 19:18:52.364706 140198008456960 logging_writer.py:48] [5648] accumulated_eval_time=23142.5, accumulated_logging_time=1.1443, accumulated_submission_time=5536.69, global_step=5648, preemption_count=0, score=5536.69, test/loss=0.126391, test/num_examples=95000000, total_duration=28722.4, train/loss=0.125792, validation/loss=0.124142, validation/num_examples=83274637
I0316 19:20:54.480265 140240332842176 spec.py:321] Evaluating on the training split.
I0316 19:22:56.494778 140240332842176 spec.py:333] Evaluating on the validation split.
I0316 19:24:58.889313 140240332842176 spec.py:349] Evaluating on the test split.
I0316 19:27:17.827698 140240332842176 submission_runner.py:469] Time since start: 29227.84s, 	Step: 5772, 	{'train/loss': 0.12293437079444217, 'validation/loss': 0.12414787773004139, 'validation/num_examples': 83274637, 'test/loss': 0.12645030699045282, 'test/num_examples': 95000000, 'score': 5657.807493925095, 'total_duration': 29227.83837413788, 'accumulated_submission_time': 5657.807493925095, 'accumulated_eval_time': 23525.885630607605, 'accumulated_logging_time': 1.2218048572540283}
I0316 19:27:17.839934 140198016849664 logging_writer.py:48] [5772] accumulated_eval_time=23525.9, accumulated_logging_time=1.2218, accumulated_submission_time=5657.81, global_step=5772, preemption_count=0, score=5657.81, test/loss=0.12645, test/num_examples=95000000, total_duration=29227.8, train/loss=0.122934, validation/loss=0.124148, validation/num_examples=83274637
I0316 19:29:18.564489 140240332842176 spec.py:321] Evaluating on the training split.
I0316 19:31:20.577729 140240332842176 spec.py:333] Evaluating on the validation split.
I0316 19:33:22.932930 140240332842176 spec.py:349] Evaluating on the test split.
I0316 19:35:43.720333 140240332842176 submission_runner.py:469] Time since start: 29733.73s, 	Step: 5893, 	{'train/loss': 0.12303705147324456, 'validation/loss': 0.12412840512766307, 'validation/num_examples': 83274637, 'test/loss': 0.1263979064597682, 'test/num_examples': 95000000, 'score': 5777.647403478622, 'total_duration': 29733.73103928566, 'accumulated_submission_time': 5777.647403478622, 'accumulated_eval_time': 23911.04171681404, 'accumulated_logging_time': 1.241218090057373}
I0316 19:35:43.731261 140198008456960 logging_writer.py:48] [5893] accumulated_eval_time=23911, accumulated_logging_time=1.24122, accumulated_submission_time=5777.65, global_step=5893, preemption_count=0, score=5777.65, test/loss=0.126398, test/num_examples=95000000, total_duration=29733.7, train/loss=0.123037, validation/loss=0.124128, validation/num_examples=83274637
I0316 19:37:25.666429 140198016849664 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.0052876, loss=0.129442
I0316 19:37:25.669960 140240332842176 submission.py:265] 6000) loss = 0.129, grad_norm = 0.005
I0316 19:37:44.679714 140240332842176 spec.py:321] Evaluating on the training split.
I0316 19:39:46.715987 140240332842176 spec.py:333] Evaluating on the validation split.
I0316 19:41:49.085090 140240332842176 spec.py:349] Evaluating on the test split.
I0316 19:44:09.036850 140240332842176 submission_runner.py:469] Time since start: 30239.05s, 	Step: 6017, 	{'train/loss': 0.12371341989997876, 'validation/loss': 0.12413623025707674, 'validation/num_examples': 83274637, 'test/loss': 0.12646853294111554, 'test/num_examples': 95000000, 'score': 5897.746931552887, 'total_duration': 30239.04755306244, 'accumulated_submission_time': 5897.746931552887, 'accumulated_eval_time': 24295.398953914642, 'accumulated_logging_time': 1.2583611011505127}
I0316 19:44:09.048244 140198008456960 logging_writer.py:48] [6017] accumulated_eval_time=24295.4, accumulated_logging_time=1.25836, accumulated_submission_time=5897.75, global_step=6017, preemption_count=0, score=5897.75, test/loss=0.126469, test/num_examples=95000000, total_duration=30239, train/loss=0.123713, validation/loss=0.124136, validation/num_examples=83274637
I0316 19:46:10.289451 140240332842176 spec.py:321] Evaluating on the training split.
I0316 19:48:12.202465 140240332842176 spec.py:333] Evaluating on the validation split.
I0316 19:50:14.529520 140240332842176 spec.py:349] Evaluating on the test split.
I0316 19:52:35.280557 140240332842176 submission_runner.py:469] Time since start: 30745.29s, 	Step: 6141, 	{'train/loss': 0.12471738191997218, 'validation/loss': 0.1241154553631387, 'validation/num_examples': 83274637, 'test/loss': 0.12641517131066574, 'test/num_examples': 95000000, 'score': 6018.082354784012, 'total_duration': 30745.29123854637, 'accumulated_submission_time': 6018.082354784012, 'accumulated_eval_time': 24680.390204191208, 'accumulated_logging_time': 1.2768852710723877}
I0316 19:52:35.291834 140198016849664 logging_writer.py:48] [6141] accumulated_eval_time=24680.4, accumulated_logging_time=1.27689, accumulated_submission_time=6018.08, global_step=6141, preemption_count=0, score=6018.08, test/loss=0.126415, test/num_examples=95000000, total_duration=30745.3, train/loss=0.124717, validation/loss=0.124115, validation/num_examples=83274637
I0316 19:54:36.372398 140240332842176 spec.py:321] Evaluating on the training split.
I0316 19:56:39.246382 140240332842176 spec.py:333] Evaluating on the validation split.
I0316 19:58:42.667052 140240332842176 spec.py:349] Evaluating on the test split.
I0316 20:01:02.905747 140240332842176 submission_runner.py:469] Time since start: 31252.92s, 	Step: 6264, 	{'train/loss': 0.12234638543547954, 'validation/loss': 0.1241123813632927, 'validation/num_examples': 83274637, 'test/loss': 0.12641858644168252, 'test/num_examples': 95000000, 'score': 6138.246690988541, 'total_duration': 31252.916482925415, 'accumulated_submission_time': 6138.246690988541, 'accumulated_eval_time': 25066.92372751236, 'accumulated_logging_time': 1.2950282096862793}
I0316 20:01:02.916987 140198008456960 logging_writer.py:48] [6264] accumulated_eval_time=25066.9, accumulated_logging_time=1.29503, accumulated_submission_time=6138.25, global_step=6264, preemption_count=0, score=6138.25, test/loss=0.126419, test/num_examples=95000000, total_duration=31252.9, train/loss=0.122346, validation/loss=0.124112, validation/num_examples=83274637
I0316 20:03:03.501135 140240332842176 spec.py:321] Evaluating on the training split.
I0316 20:05:05.482259 140240332842176 spec.py:333] Evaluating on the validation split.
I0316 20:07:08.102327 140240332842176 spec.py:349] Evaluating on the test split.
I0316 20:09:27.515813 140240332842176 submission_runner.py:469] Time since start: 31757.53s, 	Step: 6383, 	{'train/loss': 0.12344784308410887, 'validation/loss': 0.12410938012015292, 'validation/num_examples': 83274637, 'test/loss': 0.12641478384760807, 'test/num_examples': 95000000, 'score': 6257.925441503525, 'total_duration': 31757.526522397995, 'accumulated_submission_time': 6257.925441503525, 'accumulated_eval_time': 25450.93856024742, 'accumulated_logging_time': 1.335385799407959}
I0316 20:09:27.527259 140198016849664 logging_writer.py:48] [6383] accumulated_eval_time=25450.9, accumulated_logging_time=1.33539, accumulated_submission_time=6257.93, global_step=6383, preemption_count=0, score=6257.93, test/loss=0.126415, test/num_examples=95000000, total_duration=31757.5, train/loss=0.123448, validation/loss=0.124109, validation/num_examples=83274637
I0316 20:11:24.258519 140198008456960 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.00549825, loss=0.124011
I0316 20:11:24.262022 140240332842176 submission.py:265] 6500) loss = 0.124, grad_norm = 0.005
I0316 20:11:29.032067 140240332842176 spec.py:321] Evaluating on the training split.
I0316 20:13:30.401125 140240332842176 spec.py:333] Evaluating on the validation split.
I0316 20:15:32.015898 140240332842176 spec.py:349] Evaluating on the test split.
I0316 20:17:51.810384 140240332842176 submission_runner.py:469] Time since start: 32261.82s, 	Step: 6505, 	{'train/loss': 0.12339705700884215, 'validation/loss': 0.12411616950410288, 'validation/num_examples': 83274637, 'test/loss': 0.12642720975088823, 'test/num_examples': 95000000, 'score': 6378.5062346458435, 'total_duration': 32261.82107067108, 'accumulated_submission_time': 6378.5062346458435, 'accumulated_eval_time': 25833.71697998047, 'accumulated_logging_time': 1.3533670902252197}
I0316 20:17:51.821945 140198016849664 logging_writer.py:48] [6505] accumulated_eval_time=25833.7, accumulated_logging_time=1.35337, accumulated_submission_time=6378.51, global_step=6505, preemption_count=0, score=6378.51, test/loss=0.126427, test/num_examples=95000000, total_duration=32261.8, train/loss=0.123397, validation/loss=0.124116, validation/num_examples=83274637
I0316 20:19:52.883581 140240332842176 spec.py:321] Evaluating on the training split.
I0316 20:21:56.151069 140240332842176 spec.py:333] Evaluating on the validation split.
I0316 20:23:59.567045 140240332842176 spec.py:349] Evaluating on the test split.
I0316 20:26:21.103965 140240332842176 submission_runner.py:469] Time since start: 32771.11s, 	Step: 6628, 	{'train/loss': 0.12141636051632429, 'validation/loss': 0.12414851706976847, 'validation/num_examples': 83274637, 'test/loss': 0.12638846308987267, 'test/num_examples': 95000000, 'score': 6498.619480609894, 'total_duration': 32771.114662885666, 'accumulated_submission_time': 6498.619480609894, 'accumulated_eval_time': 26221.937557458878, 'accumulated_logging_time': 1.372077465057373}
I0316 20:26:21.133351 140198008456960 logging_writer.py:48] [6628] accumulated_eval_time=26221.9, accumulated_logging_time=1.37208, accumulated_submission_time=6498.62, global_step=6628, preemption_count=0, score=6498.62, test/loss=0.126388, test/num_examples=95000000, total_duration=32771.1, train/loss=0.121416, validation/loss=0.124149, validation/num_examples=83274637
I0316 20:28:22.118136 140240332842176 spec.py:321] Evaluating on the training split.
I0316 20:30:25.290898 140240332842176 spec.py:333] Evaluating on the validation split.
I0316 20:32:28.721829 140240332842176 spec.py:349] Evaluating on the test split.
I0316 20:34:50.165691 140240332842176 submission_runner.py:469] Time since start: 33280.18s, 	Step: 6750, 	{'train/loss': 0.12335731951664959, 'validation/loss': 0.12428114409634998, 'validation/num_examples': 83274637, 'test/loss': 0.1266328523771587, 'test/num_examples': 95000000, 'score': 6618.658196687698, 'total_duration': 33280.17640519142, 'accumulated_submission_time': 6618.658196687698, 'accumulated_eval_time': 26609.985483169556, 'accumulated_logging_time': 1.4080731868743896}
I0316 20:34:50.177062 140198016849664 logging_writer.py:48] [6750] accumulated_eval_time=26610, accumulated_logging_time=1.40807, accumulated_submission_time=6618.66, global_step=6750, preemption_count=0, score=6618.66, test/loss=0.126633, test/num_examples=95000000, total_duration=33280.2, train/loss=0.123357, validation/loss=0.124281, validation/num_examples=83274637
I0316 20:36:50.995642 140240332842176 spec.py:321] Evaluating on the training split.
I0316 20:38:53.973498 140240332842176 spec.py:333] Evaluating on the validation split.
I0316 20:40:57.325181 140240332842176 spec.py:349] Evaluating on the test split.
I0316 20:43:18.599645 140240332842176 submission_runner.py:469] Time since start: 33788.61s, 	Step: 6873, 	{'train/loss': 0.12117758705206516, 'validation/loss': 0.12415711056987168, 'validation/num_examples': 83274637, 'test/loss': 0.12647257706539755, 'test/num_examples': 95000000, 'score': 6738.605942726135, 'total_duration': 33788.61037325859, 'accumulated_submission_time': 6738.605942726135, 'accumulated_eval_time': 26997.589603424072, 'accumulated_logging_time': 1.4933905601501465}
I0316 20:43:18.610864 140198008456960 logging_writer.py:48] [6873] accumulated_eval_time=26997.6, accumulated_logging_time=1.49339, accumulated_submission_time=6738.61, global_step=6873, preemption_count=0, score=6738.61, test/loss=0.126473, test/num_examples=95000000, total_duration=33788.6, train/loss=0.121178, validation/loss=0.124157, validation/num_examples=83274637
I0316 20:45:19.377532 140240332842176 spec.py:321] Evaluating on the training split.
I0316 20:47:21.495481 140240332842176 spec.py:333] Evaluating on the validation split.
I0316 20:49:24.139526 140240332842176 spec.py:349] Evaluating on the test split.
I0316 20:51:43.201992 140240332842176 submission_runner.py:469] Time since start: 34293.21s, 	Step: 6995, 	{'train/loss': 0.12356405452713913, 'validation/loss': 0.12414560180847042, 'validation/num_examples': 83274637, 'test/loss': 0.12642818600592362, 'test/num_examples': 95000000, 'score': 6858.523713350296, 'total_duration': 34293.212681531906, 'accumulated_submission_time': 6858.523713350296, 'accumulated_eval_time': 27381.414110422134, 'accumulated_logging_time': 1.510969638824463}
I0316 20:51:43.213061 140198016849664 logging_writer.py:48] [6995] accumulated_eval_time=27381.4, accumulated_logging_time=1.51097, accumulated_submission_time=6858.52, global_step=6995, preemption_count=0, score=6858.52, test/loss=0.126428, test/num_examples=95000000, total_duration=34293.2, train/loss=0.123564, validation/loss=0.124146, validation/num_examples=83274637
I0316 20:51:44.835827 140198008456960 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.0109227, loss=0.119867
I0316 20:51:44.839227 140240332842176 submission.py:265] 7000) loss = 0.120, grad_norm = 0.011
I0316 20:53:44.269728 140240332842176 spec.py:321] Evaluating on the training split.
I0316 20:55:47.547732 140240332842176 spec.py:333] Evaluating on the validation split.
I0316 20:57:50.945343 140240332842176 spec.py:349] Evaluating on the test split.
I0316 21:00:12.294645 140240332842176 submission_runner.py:469] Time since start: 34802.31s, 	Step: 7119, 	{'train/loss': 0.12121486856385746, 'validation/loss': 0.12426899859330406, 'validation/num_examples': 83274637, 'test/loss': 0.12659892240207068, 'test/num_examples': 95000000, 'score': 6978.711630105972, 'total_duration': 34802.30529284477, 'accumulated_submission_time': 6978.711630105972, 'accumulated_eval_time': 27769.43922162056, 'accumulated_logging_time': 1.528385877609253}
I0316 21:00:12.306798 140198016849664 logging_writer.py:48] [7119] accumulated_eval_time=27769.4, accumulated_logging_time=1.52839, accumulated_submission_time=6978.71, global_step=7119, preemption_count=0, score=6978.71, test/loss=0.126599, test/num_examples=95000000, total_duration=34802.3, train/loss=0.121215, validation/loss=0.124269, validation/num_examples=83274637
I0316 21:02:13.563150 140240332842176 spec.py:321] Evaluating on the training split.
I0316 21:04:15.620039 140240332842176 spec.py:333] Evaluating on the validation split.
I0316 21:06:17.121770 140240332842176 spec.py:349] Evaluating on the test split.
I0316 21:08:37.657254 140240332842176 submission_runner.py:469] Time since start: 35307.67s, 	Step: 7244, 	{'train/loss': 0.12373447171603877, 'validation/loss': 0.12410787430028809, 'validation/num_examples': 83274637, 'test/loss': 0.12641039360564382, 'test/num_examples': 95000000, 'score': 7099.074926614761, 'total_duration': 35307.66798949242, 'accumulated_submission_time': 7099.074926614761, 'accumulated_eval_time': 28153.533460378647, 'accumulated_logging_time': 1.5471041202545166}
I0316 21:08:37.668837 140198008456960 logging_writer.py:48] [7244] accumulated_eval_time=28153.5, accumulated_logging_time=1.5471, accumulated_submission_time=7099.07, global_step=7244, preemption_count=0, score=7099.07, test/loss=0.12641, test/num_examples=95000000, total_duration=35307.7, train/loss=0.123734, validation/loss=0.124108, validation/num_examples=83274637
I0316 21:10:38.792670 140240332842176 spec.py:321] Evaluating on the training split.
I0316 21:12:40.596179 140240332842176 spec.py:333] Evaluating on the validation split.
I0316 21:14:42.835994 140240332842176 spec.py:349] Evaluating on the test split.
I0316 21:17:03.008028 140240332842176 submission_runner.py:469] Time since start: 35813.02s, 	Step: 7364, 	{'train/loss': 0.12361573105983578, 'validation/loss': 0.12412007668345523, 'validation/num_examples': 83274637, 'test/loss': 0.12642354989102011, 'test/num_examples': 95000000, 'score': 7219.336774110794, 'total_duration': 35813.01874756813, 'accumulated_submission_time': 7219.336774110794, 'accumulated_eval_time': 28537.74915289879, 'accumulated_logging_time': 1.603484869003296}
I0316 21:17:03.021185 140198016849664 logging_writer.py:48] [7364] accumulated_eval_time=28537.7, accumulated_logging_time=1.60348, accumulated_submission_time=7219.34, global_step=7364, preemption_count=0, score=7219.34, test/loss=0.126424, test/num_examples=95000000, total_duration=35813, train/loss=0.123616, validation/loss=0.12412, validation/num_examples=83274637
I0316 21:19:03.658494 140240332842176 spec.py:321] Evaluating on the training split.
I0316 21:21:05.781741 140240332842176 spec.py:333] Evaluating on the validation split.
I0316 21:23:08.197924 140240332842176 spec.py:349] Evaluating on the test split.
I0316 21:25:28.022176 140240332842176 submission_runner.py:469] Time since start: 36318.03s, 	Step: 7486, 	{'train/loss': 0.1228199237512823, 'validation/loss': 0.12418980471128449, 'validation/num_examples': 83274637, 'test/loss': 0.12658848860417415, 'test/num_examples': 95000000, 'score': 7339.081394910812, 'total_duration': 36318.03284955025, 'accumulated_submission_time': 7339.081394910812, 'accumulated_eval_time': 28922.113027572632, 'accumulated_logging_time': 1.6234807968139648}
I0316 21:25:28.034417 140198008456960 logging_writer.py:48] [7486] accumulated_eval_time=28922.1, accumulated_logging_time=1.62348, accumulated_submission_time=7339.08, global_step=7486, preemption_count=0, score=7339.08, test/loss=0.126588, test/num_examples=95000000, total_duration=36318, train/loss=0.12282, validation/loss=0.12419, validation/num_examples=83274637
I0316 21:25:31.379582 140198016849664 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.0056805, loss=0.119061
I0316 21:25:31.383077 140240332842176 submission.py:265] 7500) loss = 0.119, grad_norm = 0.006
I0316 21:27:30.186509 140240332842176 spec.py:321] Evaluating on the training split.
I0316 21:29:32.142867 140240332842176 spec.py:333] Evaluating on the validation split.
I0316 21:31:33.710390 140240332842176 spec.py:349] Evaluating on the test split.
I0316 21:33:54.192883 140240332842176 submission_runner.py:469] Time since start: 36824.20s, 	Step: 7610, 	{'train/loss': 0.12366760458163997, 'validation/loss': 0.12415426744122884, 'validation/num_examples': 83274637, 'test/loss': 0.12648038619300442, 'test/num_examples': 95000000, 'score': 7460.318806886673, 'total_duration': 36824.20361185074, 'accumulated_submission_time': 7460.318806886673, 'accumulated_eval_time': 29306.119697093964, 'accumulated_logging_time': 1.6423754692077637}
I0316 21:33:54.204864 140198008456960 logging_writer.py:48] [7610] accumulated_eval_time=29306.1, accumulated_logging_time=1.64238, accumulated_submission_time=7460.32, global_step=7610, preemption_count=0, score=7460.32, test/loss=0.12648, test/num_examples=95000000, total_duration=36824.2, train/loss=0.123668, validation/loss=0.124154, validation/num_examples=83274637
I0316 21:35:55.650960 140240332842176 spec.py:321] Evaluating on the training split.
I0316 21:37:57.910628 140240332842176 spec.py:333] Evaluating on the validation split.
I0316 21:40:00.534363 140240332842176 spec.py:349] Evaluating on the test split.
I0316 21:42:20.760567 140240332842176 submission_runner.py:469] Time since start: 37330.77s, 	Step: 7736, 	{'train/loss': 0.12243718026135723, 'validation/loss': 0.12422623002941903, 'validation/num_examples': 83274637, 'test/loss': 0.12649880678710937, 'test/num_examples': 95000000, 'score': 7580.87970662117, 'total_duration': 37330.77128005028, 'accumulated_submission_time': 7580.87970662117, 'accumulated_eval_time': 29691.229472398758, 'accumulated_logging_time': 1.6608710289001465}
I0316 21:42:20.772361 140198016849664 logging_writer.py:48] [7736] accumulated_eval_time=29691.2, accumulated_logging_time=1.66087, accumulated_submission_time=7580.88, global_step=7736, preemption_count=0, score=7580.88, test/loss=0.126499, test/num_examples=95000000, total_duration=37330.8, train/loss=0.122437, validation/loss=0.124226, validation/num_examples=83274637
I0316 21:44:21.484367 140240332842176 spec.py:321] Evaluating on the training split.
I0316 21:46:23.489269 140240332842176 spec.py:333] Evaluating on the validation split.
I0316 21:48:25.298649 140240332842176 spec.py:349] Evaluating on the test split.
I0316 21:50:46.024604 140240332842176 submission_runner.py:469] Time since start: 37836.04s, 	Step: 7860, 	{'train/loss': 0.12293940034589433, 'validation/loss': 0.12424809260564083, 'validation/num_examples': 83274637, 'test/loss': 0.12657637659526624, 'test/num_examples': 95000000, 'score': 7700.698132991791, 'total_duration': 37836.03528261185, 'accumulated_submission_time': 7700.698132991791, 'accumulated_eval_time': 30075.76992058754, 'accumulated_logging_time': 1.679187536239624}
I0316 21:50:46.036341 140198008456960 logging_writer.py:48] [7860] accumulated_eval_time=30075.8, accumulated_logging_time=1.67919, accumulated_submission_time=7700.7, global_step=7860, preemption_count=0, score=7700.7, test/loss=0.126576, test/num_examples=95000000, total_duration=37836, train/loss=0.122939, validation/loss=0.124248, validation/num_examples=83274637
I0316 21:52:47.621246 140198016849664 logging_writer.py:48] [7984] global_step=7984, preemption_count=0, score=7821.78
I0316 21:52:49.350531 140240332842176 submission_runner.py:646] Tuning trial 2/5
I0316 21:52:49.350711 140240332842176 submission_runner.py:647] Hyperparameters: Hyperparameters(learning_rate=0.0014381744028656841, one_minus_beta1=0.025337537053408913, one_minus_beta2=0.02508024059481679, epsilon=1e-08, one_minus_momentum=0.0, use_momentum=False, weight_decay=0.00019716633625688372, max_preconditioner_dim=1024, precondition_frequency=100, start_preconditioning_step=-1, inv_root_override=0, exponent_multiplier=1.0, grafting_type='ADAM', grafting_epsilon=1e-08, use_normalized_grafting=False, communication_dtype='FP32', communicate_params=True, use_cosine_decay=True, warmup_factor=0.05, label_smoothing=0.2, dropout_rate=0.0, use_nadam=True, step_hint_factor=0.6)
I0316 21:52:49.351786 140240332842176 submission_runner.py:648] Metrics: {'eval_results': [(1, {'train/loss': 1.8249944020388822, 'validation/loss': 1.8164274510810579, 'validation/num_examples': 83274637, 'test/loss': 1.8184465324655634, 'test/num_examples': 95000000, 'score': 9.74538540840149, 'total_duration': 936.8851957321167, 'accumulated_submission_time': 9.74538540840149, 'accumulated_eval_time': 926.6803433895111, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (127, {'train/loss': 0.1295790826712524, 'validation/loss': 0.13249961636749855, 'validation/num_examples': 83274637, 'test/loss': 0.13507422640111824, 'test/num_examples': 95000000, 'score': 129.3834900856018, 'total_duration': 1927.9567475318909, 'accumulated_submission_time': 129.3834900856018, 'accumulated_eval_time': 1797.1536302566528, 'accumulated_logging_time': 0.015717029571533203, 'global_step': 127, 'preemption_count': 0}), (254, {'train/loss': 0.12927126015172608, 'validation/loss': 0.12865946201861944, 'validation/num_examples': 83274637, 'test/loss': 0.13105879971606607, 'test/num_examples': 95000000, 'score': 249.89333415031433, 'total_duration': 2932.4276793003082, 'accumulated_submission_time': 249.89333415031433, 'accumulated_eval_time': 2680.2387075424194, 'accumulated_logging_time': 0.032091379165649414, 'global_step': 254, 'preemption_count': 0}), (378, {'train/loss': 0.12744881773360012, 'validation/loss': 0.12773404004344757, 'validation/num_examples': 83274637, 'test/loss': 0.13001984895071733, 'test/num_examples': 95000000, 'score': 369.7481427192688, 'total_duration': 3915.855469942093, 'accumulated_submission_time': 369.7481427192688, 'accumulated_eval_time': 3542.8878247737885, 'accumulated_logging_time': 0.04726409912109375, 'global_step': 378, 'preemption_count': 0}), (510, {'train/loss': 0.12482675294412515, 'validation/loss': 0.12677944946267697, 'validation/num_examples': 83274637, 'test/loss': 0.12915272029595626, 'test/num_examples': 95000000, 'score': 490.18702387809753, 'total_duration': 4877.36904501915, 'accumulated_submission_time': 490.18702387809753, 'accumulated_eval_time': 4383.0726153850555, 'accumulated_logging_time': 0.10628533363342285, 'global_step': 510, 'preemption_count': 0}), (632, {'train/loss': 0.12597920408096078, 'validation/loss': 0.12661762605333285, 'validation/num_examples': 83274637, 'test/loss': 0.12893914054685893, 'test/num_examples': 95000000, 'score': 609.9253125190735, 'total_duration': 5829.477991342545, 'accumulated_submission_time': 609.9253125190735, 'accumulated_eval_time': 5214.501209974289, 'accumulated_logging_time': 0.12182736396789551, 'global_step': 632, 'preemption_count': 0}), (756, {'train/loss': 0.124095681240515, 'validation/loss': 0.12653849013641197, 'validation/num_examples': 83274637, 'test/loss': 0.12897276007329037, 'test/num_examples': 95000000, 'score': 730.1396870613098, 'total_duration': 6776.615259408951, 'accumulated_submission_time': 730.1396870613098, 'accumulated_eval_time': 6040.513000011444, 'accumulated_logging_time': 0.13712263107299805, 'global_step': 756, 'preemption_count': 0}), (880, {'train/loss': 0.12512304012454972, 'validation/loss': 0.1266598414533779, 'validation/num_examples': 83274637, 'test/loss': 0.12913931590616326, 'test/num_examples': 95000000, 'score': 849.6865541934967, 'total_duration': 7717.353037595749, 'accumulated_submission_time': 849.6865541934967, 'accumulated_eval_time': 6860.82909655571, 'accumulated_logging_time': 0.18421077728271484, 'global_step': 880, 'preemption_count': 0}), (1007, {'train/loss': 0.12717237796760128, 'validation/loss': 0.12620122136588827, 'validation/num_examples': 83274637, 'test/loss': 0.12862537911923058, 'test/num_examples': 95000000, 'score': 969.7576699256897, 'total_duration': 8614.750085830688, 'accumulated_submission_time': 969.7576699256897, 'accumulated_eval_time': 7637.225449085236, 'accumulated_logging_time': 0.20209026336669922, 'global_step': 1007, 'preemption_count': 0}), (1132, {'train/loss': 0.1252772134907647, 'validation/loss': 0.1262649497025433, 'validation/num_examples': 83274637, 'test/loss': 0.12860496375439293, 'test/num_examples': 95000000, 'score': 1090.1197173595428, 'total_duration': 9498.038784503937, 'accumulated_submission_time': 1090.1197173595428, 'accumulated_eval_time': 8399.222056865692, 'accumulated_logging_time': 0.21722841262817383, 'global_step': 1132, 'preemption_count': 0}), (1259, {'train/loss': 0.12383838672366432, 'validation/loss': 0.12584647884617245, 'validation/num_examples': 83274637, 'test/loss': 0.12822483381488198, 'test/num_examples': 95000000, 'score': 1210.5336847305298, 'total_duration': 10300.231985569, 'accumulated_submission_time': 1210.5336847305298, 'accumulated_eval_time': 9080.013966798782, 'accumulated_logging_time': 0.29767870903015137, 'global_step': 1259, 'preemption_count': 0}), (1383, {'train/loss': 0.12502639434846735, 'validation/loss': 0.12575809654793133, 'validation/num_examples': 83274637, 'test/loss': 0.12800088762472053, 'test/num_examples': 95000000, 'score': 1330.0491933822632, 'total_duration': 10963.763128042221, 'accumulated_submission_time': 1330.0491933822632, 'accumulated_eval_time': 9623.092582702637, 'accumulated_logging_time': 0.31249403953552246, 'global_step': 1383, 'preemption_count': 0}), (1505, {'train/loss': 0.12469699673244508, 'validation/loss': 0.12559443346755975, 'validation/num_examples': 83274637, 'test/loss': 0.12780763844761095, 'test/num_examples': 95000000, 'score': 1450.422436952591, 'total_duration': 11491.382278680801, 'accumulated_submission_time': 1450.422436952591, 'accumulated_eval_time': 10029.38510298729, 'accumulated_logging_time': 0.3281874656677246, 'global_step': 1505, 'preemption_count': 0}), (1626, {'train/loss': 0.12469317086654587, 'validation/loss': 0.12586131330276923, 'validation/num_examples': 83274637, 'test/loss': 0.1283752979091042, 'test/num_examples': 95000000, 'score': 1570.3747045993805, 'total_duration': 12000.078302383423, 'accumulated_submission_time': 1570.3747045993805, 'accumulated_eval_time': 10417.184427976608, 'accumulated_logging_time': 0.3449862003326416, 'global_step': 1626, 'preemption_count': 0}), (1748, {'train/loss': 0.12378551211840591, 'validation/loss': 0.12566993102058396, 'validation/num_examples': 83274637, 'test/loss': 0.1280179622587505, 'test/num_examples': 95000000, 'score': 1690.0266008377075, 'total_duration': 12509.666815519333, 'accumulated_submission_time': 1690.0266008377075, 'accumulated_eval_time': 10806.169630527496, 'accumulated_logging_time': 0.3702843189239502, 'global_step': 1748, 'preemption_count': 0}), (1872, {'train/loss': 0.12503364133014072, 'validation/loss': 0.12556650811700393, 'validation/num_examples': 83274637, 'test/loss': 0.1277942565799111, 'test/num_examples': 95000000, 'score': 1809.858027458191, 'total_duration': 13018.179370641708, 'accumulated_submission_time': 1809.858027458191, 'accumulated_eval_time': 11193.891434431076, 'accumulated_logging_time': 0.38739466667175293, 'global_step': 1872, 'preemption_count': 0}), (1997, {'train/loss': 0.12305802635886266, 'validation/loss': 0.12532471890807986, 'validation/num_examples': 83274637, 'test/loss': 0.1276955578422948, 'test/num_examples': 95000000, 'score': 1929.5862274169922, 'total_duration': 13524.556694507599, 'accumulated_submission_time': 1929.5862274169922, 'accumulated_eval_time': 11579.609279632568, 'accumulated_logging_time': 0.4042544364929199, 'global_step': 1997, 'preemption_count': 0}), (2119, {'train/loss': 0.12334051020174101, 'validation/loss': 0.12568768069468575, 'validation/num_examples': 83274637, 'test/loss': 0.12800148854105095, 'test/num_examples': 95000000, 'score': 2049.77064371109, 'total_duration': 14030.825075149536, 'accumulated_submission_time': 2049.77064371109, 'accumulated_eval_time': 11964.77750134468, 'accumulated_logging_time': 0.4209280014038086, 'global_step': 2119, 'preemption_count': 0}), (2242, {'train/loss': 0.12435508368751384, 'validation/loss': 0.12567469508568496, 'validation/num_examples': 83274637, 'test/loss': 0.12798783229008726, 'test/num_examples': 95000000, 'score': 2170.058862686157, 'total_duration': 14541.083662509918, 'accumulated_submission_time': 2170.058862686157, 'accumulated_eval_time': 12353.779723644257, 'accumulated_logging_time': 0.450808048248291, 'global_step': 2242, 'preemption_count': 0}), (2364, {'train/loss': 0.12404983314266309, 'validation/loss': 0.12497066234662052, 'validation/num_examples': 83274637, 'test/loss': 0.12731698940823202, 'test/num_examples': 95000000, 'score': 2290.342180490494, 'total_duration': 15046.370571136475, 'accumulated_submission_time': 2290.342180490494, 'accumulated_eval_time': 12737.920649766922, 'accumulated_logging_time': 0.4676778316497803, 'global_step': 2364, 'preemption_count': 0}), (2483, {'train/loss': 0.12148589458099995, 'validation/loss': 0.1251575257380122, 'validation/num_examples': 83274637, 'test/loss': 0.12754113665787548, 'test/num_examples': 95000000, 'score': 2410.952310323715, 'total_duration': 15553.465792179108, 'accumulated_submission_time': 2410.952310323715, 'accumulated_eval_time': 13123.485873699188, 'accumulated_logging_time': 0.4843122959136963, 'global_step': 2483, 'preemption_count': 0}), (2602, {'train/loss': 0.12345951200368885, 'validation/loss': 0.12497655737716233, 'validation/num_examples': 83274637, 'test/loss': 0.1273533337187516, 'test/num_examples': 95000000, 'score': 2531.2009110450745, 'total_duration': 16062.806546926498, 'accumulated_submission_time': 2531.2009110450745, 'accumulated_eval_time': 13511.737191200256, 'accumulated_logging_time': 0.501079797744751, 'global_step': 2602, 'preemption_count': 0}), (2724, {'train/loss': 0.12409650539155201, 'validation/loss': 0.1253635300489712, 'validation/num_examples': 83274637, 'test/loss': 0.12787726730965063, 'test/num_examples': 95000000, 'score': 2651.5256958007812, 'total_duration': 16572.428359746933, 'accumulated_submission_time': 2651.5256958007812, 'accumulated_eval_time': 13900.086666107178, 'accumulated_logging_time': 0.5264492034912109, 'global_step': 2724, 'preemption_count': 0}), (2844, {'train/loss': 0.1245414673719671, 'validation/loss': 0.12486033263301224, 'validation/num_examples': 83274637, 'test/loss': 0.12726854740825452, 'test/num_examples': 95000000, 'score': 2771.4885416030884, 'total_duration': 17075.383397340775, 'accumulated_submission_time': 2771.4885416030884, 'accumulated_eval_time': 14282.151737451553, 'accumulated_logging_time': 0.5429379940032959, 'global_step': 2844, 'preemption_count': 0}), (2965, {'train/loss': 0.12336719852974615, 'validation/loss': 0.12501690166776105, 'validation/num_examples': 83274637, 'test/loss': 0.127361217958631, 'test/num_examples': 95000000, 'score': 2892.0546729564667, 'total_duration': 17578.799298763275, 'accumulated_submission_time': 2892.0546729564667, 'accumulated_eval_time': 14664.122420310974, 'accumulated_logging_time': 0.5616066455841064, 'global_step': 2965, 'preemption_count': 0}), (3085, {'train/loss': 0.12312469988672556, 'validation/loss': 0.12484348505668026, 'validation/num_examples': 83274637, 'test/loss': 0.12721263430007132, 'test/num_examples': 95000000, 'score': 3012.972417116165, 'total_duration': 18085.194281578064, 'accumulated_submission_time': 3012.972417116165, 'accumulated_eval_time': 15048.644133329391, 'accumulated_logging_time': 0.5799634456634521, 'global_step': 3085, 'preemption_count': 0}), (3207, {'train/loss': 0.12431106850339609, 'validation/loss': 0.12470756652449695, 'validation/num_examples': 83274637, 'test/loss': 0.12698579815701935, 'test/num_examples': 95000000, 'score': 3132.870416164398, 'total_duration': 18590.215764522552, 'accumulated_submission_time': 3132.870416164398, 'accumulated_eval_time': 15432.74186706543, 'accumulated_logging_time': 0.669574499130249, 'global_step': 3207, 'preemption_count': 0}), (3327, {'train/loss': 0.12430036228429361, 'validation/loss': 0.12519562843799914, 'validation/num_examples': 83274637, 'test/loss': 0.1275745328320152, 'test/num_examples': 95000000, 'score': 3252.907784461975, 'total_duration': 19094.776104211807, 'accumulated_submission_time': 3252.907784461975, 'accumulated_eval_time': 15816.343222856522, 'accumulated_logging_time': 0.687305212020874, 'global_step': 3327, 'preemption_count': 0}), (3451, {'train/loss': 0.12487519648413099, 'validation/loss': 0.12470368655052513, 'validation/num_examples': 83274637, 'test/loss': 0.1271353808726662, 'test/num_examples': 95000000, 'score': 3373.019310951233, 'total_duration': 19598.912457704544, 'accumulated_submission_time': 3373.019310951233, 'accumulated_eval_time': 16199.404266119003, 'accumulated_logging_time': 0.705085039138794, 'global_step': 3451, 'preemption_count': 0}), (3574, {'train/loss': 0.12354109774833058, 'validation/loss': 0.1246863197626124, 'validation/num_examples': 83274637, 'test/loss': 0.12699308340088694, 'test/num_examples': 95000000, 'score': 3494.046045064926, 'total_duration': 20103.35507631302, 'accumulated_submission_time': 3494.046045064926, 'accumulated_eval_time': 16581.895528316498, 'accumulated_logging_time': 0.7218673229217529, 'global_step': 3574, 'preemption_count': 0}), (3697, {'train/loss': 0.12234205728503593, 'validation/loss': 0.12455602125640246, 'validation/num_examples': 83274637, 'test/loss': 0.12690470672009116, 'test/num_examples': 95000000, 'score': 3614.52832531929, 'total_duration': 20608.48293185234, 'accumulated_submission_time': 3614.52832531929, 'accumulated_eval_time': 16965.62119626999, 'accumulated_logging_time': 0.7573072910308838, 'global_step': 3697, 'preemption_count': 0}), (3819, {'train/loss': 0.12299460763446642, 'validation/loss': 0.12461414795494417, 'validation/num_examples': 83274637, 'test/loss': 0.1269171231848064, 'test/num_examples': 95000000, 'score': 3734.186513900757, 'total_duration': 21113.99488067627, 'accumulated_submission_time': 3734.186513900757, 'accumulated_eval_time': 17350.54878807068, 'accumulated_logging_time': 0.7742409706115723, 'global_step': 3819, 'preemption_count': 0}), (3939, {'train/loss': 0.12422622337915551, 'validation/loss': 0.12451240127664018, 'validation/num_examples': 83274637, 'test/loss': 0.12689381959156237, 'test/num_examples': 95000000, 'score': 3853.833815097809, 'total_duration': 21618.938402175903, 'accumulated_submission_time': 3853.833815097809, 'accumulated_eval_time': 17734.90039372444, 'accumulated_logging_time': 0.7944328784942627, 'global_step': 3939, 'preemption_count': 0}), (4058, {'train/loss': 0.12164704625757887, 'validation/loss': 0.12457594582099821, 'validation/num_examples': 83274637, 'test/loss': 0.12690206928004213, 'test/num_examples': 95000000, 'score': 3974.256808280945, 'total_duration': 22126.797513723373, 'accumulated_submission_time': 3974.256808280945, 'accumulated_eval_time': 18121.420876264572, 'accumulated_logging_time': 0.812131404876709, 'global_step': 4058, 'preemption_count': 0}), (4179, {'train/loss': 0.12319872930177625, 'validation/loss': 0.1248843614322766, 'validation/num_examples': 83274637, 'test/loss': 0.12726214679521258, 'test/num_examples': 95000000, 'score': 4094.111385345459, 'total_duration': 22632.199587106705, 'accumulated_submission_time': 4094.111385345459, 'accumulated_eval_time': 18506.006427526474, 'accumulated_logging_time': 0.8819785118103027, 'global_step': 4179, 'preemption_count': 0}), (4302, {'train/loss': 0.1221004590057181, 'validation/loss': 0.12456987971138345, 'validation/num_examples': 83274637, 'test/loss': 0.1269060338334736, 'test/num_examples': 95000000, 'score': 4214.001971721649, 'total_duration': 23136.345856666565, 'accumulated_submission_time': 4214.001971721649, 'accumulated_eval_time': 18889.383198976517, 'accumulated_logging_time': 0.8992903232574463, 'global_step': 4302, 'preemption_count': 0}), (4424, {'train/loss': 0.12360468920974928, 'validation/loss': 0.12443367299695393, 'validation/num_examples': 83274637, 'test/loss': 0.12670118580490916, 'test/num_examples': 95000000, 'score': 4333.88579082489, 'total_duration': 23639.722157239914, 'accumulated_submission_time': 4333.88579082489, 'accumulated_eval_time': 19271.92449951172, 'accumulated_logging_time': 0.9178085327148438, 'global_step': 4424, 'preemption_count': 0}), (4550, {'train/loss': 0.12492116034608433, 'validation/loss': 0.12443287295719907, 'validation/num_examples': 83274637, 'test/loss': 0.12669169909667968, 'test/num_examples': 95000000, 'score': 4454.601715564728, 'total_duration': 24149.186757087708, 'accumulated_submission_time': 4454.601715564728, 'accumulated_eval_time': 19659.712699651718, 'accumulated_logging_time': 0.9364697933197021, 'global_step': 4550, 'preemption_count': 0}), (4670, {'train/loss': 0.12187220609110906, 'validation/loss': 0.12437263675760461, 'validation/num_examples': 83274637, 'test/loss': 0.12664845055847168, 'test/num_examples': 95000000, 'score': 4575.511076927185, 'total_duration': 24658.304027557373, 'accumulated_submission_time': 4575.511076927185, 'accumulated_eval_time': 20046.993185043335, 'accumulated_logging_time': 0.9539883136749268, 'global_step': 4670, 'preemption_count': 0}), (4792, {'train/loss': 0.12254099113312987, 'validation/loss': 0.12432570039558764, 'validation/num_examples': 83274637, 'test/loss': 0.12659345702546773, 'test/num_examples': 95000000, 'score': 4696.194514751434, 'total_duration': 25166.28787255287, 'accumulated_submission_time': 4696.194514751434, 'accumulated_eval_time': 20433.344702243805, 'accumulated_logging_time': 0.9713873863220215, 'global_step': 4792, 'preemption_count': 0}), (4915, {'train/loss': 0.12281449713316214, 'validation/loss': 0.12427417390192434, 'validation/num_examples': 83274637, 'test/loss': 0.1265910074301067, 'test/num_examples': 95000000, 'score': 4816.73704123497, 'total_duration': 25674.30486559868, 'accumulated_submission_time': 4816.73704123497, 'accumulated_eval_time': 20819.865918636322, 'accumulated_logging_time': 1.0013039112091064, 'global_step': 4915, 'preemption_count': 0}), (5036, {'train/loss': 0.12259955243791079, 'validation/loss': 0.12435981578235637, 'validation/num_examples': 83274637, 'test/loss': 0.12665465016487523, 'test/num_examples': 95000000, 'score': 4936.459621429443, 'total_duration': 26180.657777071, 'accumulated_submission_time': 4936.459621429443, 'accumulated_eval_time': 21205.617896318436, 'accumulated_logging_time': 1.025479793548584, 'global_step': 5036, 'preemption_count': 0}), (5158, {'train/loss': 0.12131350391145708, 'validation/loss': 0.12424445860304664, 'validation/num_examples': 83274637, 'test/loss': 0.12653710406678853, 'test/num_examples': 95000000, 'score': 5056.480705976486, 'total_duration': 26689.529962539673, 'accumulated_submission_time': 5056.480705976486, 'accumulated_eval_time': 21593.59312582016, 'accumulated_logging_time': 1.0556573867797852, 'global_step': 5158, 'preemption_count': 0}), (5282, {'train/loss': 0.12388544183311653, 'validation/loss': 0.12422364962601995, 'validation/num_examples': 83274637, 'test/loss': 0.12653069809000117, 'test/num_examples': 95000000, 'score': 5176.468922615051, 'total_duration': 27197.902804136276, 'accumulated_submission_time': 5176.468922615051, 'accumulated_eval_time': 21981.02797150612, 'accumulated_logging_time': 1.0877408981323242, 'global_step': 5282, 'preemption_count': 0}), (5406, {'train/loss': 0.1226023253193226, 'validation/loss': 0.12415736881237424, 'validation/num_examples': 83274637, 'test/loss': 0.12649470411670083, 'test/num_examples': 95000000, 'score': 5296.515435934067, 'total_duration': 27706.48237633705, 'accumulated_submission_time': 5296.515435934067, 'accumulated_eval_time': 22368.616581201553, 'accumulated_logging_time': 1.1066944599151611, 'global_step': 5406, 'preemption_count': 0}), (5526, {'train/loss': 0.1230290738730423, 'validation/loss': 0.12419912630034761, 'validation/num_examples': 83274637, 'test/loss': 0.1265295892748381, 'test/num_examples': 95000000, 'score': 5416.104229450226, 'total_duration': 28214.141768693924, 'accumulated_submission_time': 5416.104229450226, 'accumulated_eval_time': 22755.835795879364, 'accumulated_logging_time': 1.1258301734924316, 'global_step': 5526, 'preemption_count': 0}), (5648, {'train/loss': 0.12579171802237896, 'validation/loss': 0.12414156719962541, 'validation/num_examples': 83274637, 'test/loss': 0.1263909406642713, 'test/num_examples': 95000000, 'score': 5536.68524312973, 'total_duration': 28722.36442923546, 'accumulated_submission_time': 5536.68524312973, 'accumulated_eval_time': 23142.537957906723, 'accumulated_logging_time': 1.1443028450012207, 'global_step': 5648, 'preemption_count': 0}), (5772, {'train/loss': 0.12293437079444217, 'validation/loss': 0.12414787773004139, 'validation/num_examples': 83274637, 'test/loss': 0.12645030699045282, 'test/num_examples': 95000000, 'score': 5657.807493925095, 'total_duration': 29227.83837413788, 'accumulated_submission_time': 5657.807493925095, 'accumulated_eval_time': 23525.885630607605, 'accumulated_logging_time': 1.2218048572540283, 'global_step': 5772, 'preemption_count': 0}), (5893, {'train/loss': 0.12303705147324456, 'validation/loss': 0.12412840512766307, 'validation/num_examples': 83274637, 'test/loss': 0.1263979064597682, 'test/num_examples': 95000000, 'score': 5777.647403478622, 'total_duration': 29733.73103928566, 'accumulated_submission_time': 5777.647403478622, 'accumulated_eval_time': 23911.04171681404, 'accumulated_logging_time': 1.241218090057373, 'global_step': 5893, 'preemption_count': 0}), (6017, {'train/loss': 0.12371341989997876, 'validation/loss': 0.12413623025707674, 'validation/num_examples': 83274637, 'test/loss': 0.12646853294111554, 'test/num_examples': 95000000, 'score': 5897.746931552887, 'total_duration': 30239.04755306244, 'accumulated_submission_time': 5897.746931552887, 'accumulated_eval_time': 24295.398953914642, 'accumulated_logging_time': 1.2583611011505127, 'global_step': 6017, 'preemption_count': 0}), (6141, {'train/loss': 0.12471738191997218, 'validation/loss': 0.1241154553631387, 'validation/num_examples': 83274637, 'test/loss': 0.12641517131066574, 'test/num_examples': 95000000, 'score': 6018.082354784012, 'total_duration': 30745.29123854637, 'accumulated_submission_time': 6018.082354784012, 'accumulated_eval_time': 24680.390204191208, 'accumulated_logging_time': 1.2768852710723877, 'global_step': 6141, 'preemption_count': 0}), (6264, {'train/loss': 0.12234638543547954, 'validation/loss': 0.1241123813632927, 'validation/num_examples': 83274637, 'test/loss': 0.12641858644168252, 'test/num_examples': 95000000, 'score': 6138.246690988541, 'total_duration': 31252.916482925415, 'accumulated_submission_time': 6138.246690988541, 'accumulated_eval_time': 25066.92372751236, 'accumulated_logging_time': 1.2950282096862793, 'global_step': 6264, 'preemption_count': 0}), (6383, {'train/loss': 0.12344784308410887, 'validation/loss': 0.12410938012015292, 'validation/num_examples': 83274637, 'test/loss': 0.12641478384760807, 'test/num_examples': 95000000, 'score': 6257.925441503525, 'total_duration': 31757.526522397995, 'accumulated_submission_time': 6257.925441503525, 'accumulated_eval_time': 25450.93856024742, 'accumulated_logging_time': 1.335385799407959, 'global_step': 6383, 'preemption_count': 0}), (6505, {'train/loss': 0.12339705700884215, 'validation/loss': 0.12411616950410288, 'validation/num_examples': 83274637, 'test/loss': 0.12642720975088823, 'test/num_examples': 95000000, 'score': 6378.5062346458435, 'total_duration': 32261.82107067108, 'accumulated_submission_time': 6378.5062346458435, 'accumulated_eval_time': 25833.71697998047, 'accumulated_logging_time': 1.3533670902252197, 'global_step': 6505, 'preemption_count': 0}), (6628, {'train/loss': 0.12141636051632429, 'validation/loss': 0.12414851706976847, 'validation/num_examples': 83274637, 'test/loss': 0.12638846308987267, 'test/num_examples': 95000000, 'score': 6498.619480609894, 'total_duration': 32771.114662885666, 'accumulated_submission_time': 6498.619480609894, 'accumulated_eval_time': 26221.937557458878, 'accumulated_logging_time': 1.372077465057373, 'global_step': 6628, 'preemption_count': 0}), (6750, {'train/loss': 0.12335731951664959, 'validation/loss': 0.12428114409634998, 'validation/num_examples': 83274637, 'test/loss': 0.1266328523771587, 'test/num_examples': 95000000, 'score': 6618.658196687698, 'total_duration': 33280.17640519142, 'accumulated_submission_time': 6618.658196687698, 'accumulated_eval_time': 26609.985483169556, 'accumulated_logging_time': 1.4080731868743896, 'global_step': 6750, 'preemption_count': 0}), (6873, {'train/loss': 0.12117758705206516, 'validation/loss': 0.12415711056987168, 'validation/num_examples': 83274637, 'test/loss': 0.12647257706539755, 'test/num_examples': 95000000, 'score': 6738.605942726135, 'total_duration': 33788.61037325859, 'accumulated_submission_time': 6738.605942726135, 'accumulated_eval_time': 26997.589603424072, 'accumulated_logging_time': 1.4933905601501465, 'global_step': 6873, 'preemption_count': 0}), (6995, {'train/loss': 0.12356405452713913, 'validation/loss': 0.12414560180847042, 'validation/num_examples': 83274637, 'test/loss': 0.12642818600592362, 'test/num_examples': 95000000, 'score': 6858.523713350296, 'total_duration': 34293.212681531906, 'accumulated_submission_time': 6858.523713350296, 'accumulated_eval_time': 27381.414110422134, 'accumulated_logging_time': 1.510969638824463, 'global_step': 6995, 'preemption_count': 0}), (7119, {'train/loss': 0.12121486856385746, 'validation/loss': 0.12426899859330406, 'validation/num_examples': 83274637, 'test/loss': 0.12659892240207068, 'test/num_examples': 95000000, 'score': 6978.711630105972, 'total_duration': 34802.30529284477, 'accumulated_submission_time': 6978.711630105972, 'accumulated_eval_time': 27769.43922162056, 'accumulated_logging_time': 1.528385877609253, 'global_step': 7119, 'preemption_count': 0}), (7244, {'train/loss': 0.12373447171603877, 'validation/loss': 0.12410787430028809, 'validation/num_examples': 83274637, 'test/loss': 0.12641039360564382, 'test/num_examples': 95000000, 'score': 7099.074926614761, 'total_duration': 35307.66798949242, 'accumulated_submission_time': 7099.074926614761, 'accumulated_eval_time': 28153.533460378647, 'accumulated_logging_time': 1.5471041202545166, 'global_step': 7244, 'preemption_count': 0}), (7364, {'train/loss': 0.12361573105983578, 'validation/loss': 0.12412007668345523, 'validation/num_examples': 83274637, 'test/loss': 0.12642354989102011, 'test/num_examples': 95000000, 'score': 7219.336774110794, 'total_duration': 35813.01874756813, 'accumulated_submission_time': 7219.336774110794, 'accumulated_eval_time': 28537.74915289879, 'accumulated_logging_time': 1.603484869003296, 'global_step': 7364, 'preemption_count': 0}), (7486, {'train/loss': 0.1228199237512823, 'validation/loss': 0.12418980471128449, 'validation/num_examples': 83274637, 'test/loss': 0.12658848860417415, 'test/num_examples': 95000000, 'score': 7339.081394910812, 'total_duration': 36318.03284955025, 'accumulated_submission_time': 7339.081394910812, 'accumulated_eval_time': 28922.113027572632, 'accumulated_logging_time': 1.6234807968139648, 'global_step': 7486, 'preemption_count': 0}), (7610, {'train/loss': 0.12366760458163997, 'validation/loss': 0.12415426744122884, 'validation/num_examples': 83274637, 'test/loss': 0.12648038619300442, 'test/num_examples': 95000000, 'score': 7460.318806886673, 'total_duration': 36824.20361185074, 'accumulated_submission_time': 7460.318806886673, 'accumulated_eval_time': 29306.119697093964, 'accumulated_logging_time': 1.6423754692077637, 'global_step': 7610, 'preemption_count': 0}), (7736, {'train/loss': 0.12243718026135723, 'validation/loss': 0.12422623002941903, 'validation/num_examples': 83274637, 'test/loss': 0.12649880678710937, 'test/num_examples': 95000000, 'score': 7580.87970662117, 'total_duration': 37330.77128005028, 'accumulated_submission_time': 7580.87970662117, 'accumulated_eval_time': 29691.229472398758, 'accumulated_logging_time': 1.6608710289001465, 'global_step': 7736, 'preemption_count': 0}), (7860, {'train/loss': 0.12293940034589433, 'validation/loss': 0.12424809260564083, 'validation/num_examples': 83274637, 'test/loss': 0.12657637659526624, 'test/num_examples': 95000000, 'score': 7700.698132991791, 'total_duration': 37836.03528261185, 'accumulated_submission_time': 7700.698132991791, 'accumulated_eval_time': 30075.76992058754, 'accumulated_logging_time': 1.679187536239624, 'global_step': 7860, 'preemption_count': 0})], 'global_step': 7984}
I0316 21:52:49.351879 140240332842176 submission_runner.py:649] Timing: 7821.78270149231
I0316 21:52:49.351913 140240332842176 submission_runner.py:651] Total number of evals: 65
I0316 21:52:49.351941 140240332842176 submission_runner.py:652] ====================
I0316 21:52:49.352033 140240332842176 submission_runner.py:750] Final criteo1tb score: 1
