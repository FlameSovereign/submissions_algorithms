torchrun --redirects 1:0,2:0,3:0,4:0,5:0,6:0,7:0 --standalone --nnodes=1 --nproc_per_node=8 submission_runner.py --framework=pytorch --workload=criteo1tb --submission_path=submissions_algorithms/leaderboard/external_tuning/shampoo_submission/submission.py --data_dir=/data/criteo1tb --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=submissions/rolling_leaderboard/external_tuning/shampoo/study_1 --overwrite=True --save_checkpoints=False --rng_seed=2075554435 --torch_compile=true --tuning_ruleset=external --tuning_search_space=submissions_algorithms/leaderboard/external_tuning/shampoo_submission/tuning_search_space.json --num_tuning_trials=5 --hparam_start_index=3 --hparam_end_index=4 2>&1 | tee -a /logs/criteo1tb_pytorch_03-16-2025-12-02-14.log
W0316 12:02:16.279000 9 site-packages/torch/distributed/run.py:793] 
W0316 12:02:16.279000 9 site-packages/torch/distributed/run.py:793] *****************************************
W0316 12:02:16.279000 9 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0316 12:02:16.279000 9 site-packages/torch/distributed/run.py:793] *****************************************
2025-03-16 12:02:17.478145: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-03-16 12:02:17.478145: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-03-16 12:02:17.478169: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-03-16 12:02:17.478148: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-03-16 12:02:17.478145: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-03-16 12:02:17.478149: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-03-16 12:02:17.478282: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-03-16 12:02:17.478395: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1742126537.499938      48 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1742126537.499937      47 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1742126537.499941      46 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1742126537.499939      45 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1742126537.499944      51 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1742126537.499938      50 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1742126537.500585      49 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1742126537.500599      44 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1742126537.506468      48 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
E0000 00:00:1742126537.506468      47 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
E0000 00:00:1742126537.506470      50 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
E0000 00:00:1742126537.506471      46 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
E0000 00:00:1742126537.506471      45 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
E0000 00:00:1742126537.506476      51 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
E0000 00:00:1742126537.507265      49 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
E0000 00:00:1742126537.507284      44 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
[rank6]:[W316 12:02:24.554330265 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 6]  using GPU 6 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank1]:[W316 12:02:24.575698047 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank0]:[W316 12:02:24.607790525 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank7]:[W316 12:02:24.687884617 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 7]  using GPU 7 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank4]:[W316 12:02:24.687907537 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 4]  using GPU 4 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank5]:[W316 12:02:24.689052768 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 5]  using GPU 5 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank3]:[W316 12:02:24.701057741 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank2]:[W316 12:02:24.711741728 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
I0316 12:02:26.469126 140157479363776 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_1/criteo1tb_pytorch.
I0316 12:02:26.469126 140662572377280 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_1/criteo1tb_pytorch.
I0316 12:02:26.469127 139759074923712 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_1/criteo1tb_pytorch.
I0316 12:02:26.469127 140538584081600 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_1/criteo1tb_pytorch.
I0316 12:02:26.469127 140628633658560 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_1/criteo1tb_pytorch.
I0316 12:02:26.469142 140107298358464 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_1/criteo1tb_pytorch.
I0316 12:02:26.469136 140452064818368 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_1/criteo1tb_pytorch.
I0316 12:02:26.469224 140344393745600 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_1/criteo1tb_pytorch.
I0316 12:02:27.653558 140157479363776 logger_utils.py:97] Saving hparams to /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_1/criteo1tb_pytorch/trial_4/hparams.json.
I0316 12:02:27.653583 140662572377280 logger_utils.py:97] Saving hparams to /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_1/criteo1tb_pytorch/trial_4/hparams.json.
I0316 12:02:27.653665 140628633658560 logger_utils.py:97] Saving hparams to /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_1/criteo1tb_pytorch/trial_4/hparams.json.
I0316 12:02:27.653923 140538584081600 logger_utils.py:97] Saving hparams to /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_1/criteo1tb_pytorch/trial_4/hparams.json.
I0316 12:02:27.654218 140107298358464 logger_utils.py:97] Saving hparams to /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_1/criteo1tb_pytorch/trial_4/hparams.json.
I0316 12:02:27.654220 140452064818368 submission_runner.py:606] Using RNG seed 2075554435
I0316 12:02:27.654363 139759074923712 logger_utils.py:97] Saving hparams to /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_1/criteo1tb_pytorch/trial_4/hparams.json.
I0316 12:02:27.655392 140452064818368 submission_runner.py:615] --- Tuning run 4/5 ---
I0316 12:02:27.655530 140452064818368 submission_runner.py:620] Creating tuning directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_1/criteo1tb_pytorch/trial_4.
I0316 12:02:27.655763 140452064818368 logger_utils.py:97] Saving hparams to /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_1/criteo1tb_pytorch/trial_4/hparams.json.
I0316 12:02:27.655841 140344393745600 logger_utils.py:97] Saving hparams to /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_1/criteo1tb_pytorch/trial_4/hparams.json.
I0316 12:02:27.986036 140452064818368 submission_runner.py:218] Initializing dataset.
I0316 12:02:27.986209 140452064818368 submission_runner.py:229] Initializing model.
W0316 12:02:34.399398 140452064818368 submission_runner.py:254] These workloads cannot be fully compiled under current PyTorch version. Proceeding without `torch.compile`.
I0316 12:02:34.399572 140452064818368 submission_runner.py:272] Initializing optimizer.
W0316 12:02:34.400613 140452064818368 distributed_shampoo.py:341] start_preconditioning_step set to -1. Setting start_preconditioning_step equal to precondition frequency 100 by default.
W0316 12:02:34.401100 140628633658560 submission_runner.py:254] These workloads cannot be fully compiled under current PyTorch version. Proceeding without `torch.compile`.
W0316 12:02:34.401386 140157479363776 submission_runner.py:254] These workloads cannot be fully compiled under current PyTorch version. Proceeding without `torch.compile`.
W0316 12:02:34.401567 139759074923712 submission_runner.py:254] These workloads cannot be fully compiled under current PyTorch version. Proceeding without `torch.compile`.
W0316 12:02:34.401767 140538584081600 submission_runner.py:254] These workloads cannot be fully compiled under current PyTorch version. Proceeding without `torch.compile`.
W0316 12:02:34.401839 140107298358464 submission_runner.py:254] These workloads cannot be fully compiled under current PyTorch version. Proceeding without `torch.compile`.
W0316 12:02:34.401902 140662572377280 submission_runner.py:254] These workloads cannot be fully compiled under current PyTorch version. Proceeding without `torch.compile`.
W0316 12:02:34.401965 140344393745600 submission_runner.py:254] These workloads cannot be fully compiled under current PyTorch version. Proceeding without `torch.compile`.
W0316 12:02:34.402194 140628633658560 distributed_shampoo.py:341] start_preconditioning_step set to -1. Setting start_preconditioning_step equal to precondition frequency 100 by default.
W0316 12:02:34.402623 140157479363776 distributed_shampoo.py:341] start_preconditioning_step set to -1. Setting start_preconditioning_step equal to precondition frequency 100 by default.
W0316 12:02:34.402788 139759074923712 distributed_shampoo.py:341] start_preconditioning_step set to -1. Setting start_preconditioning_step equal to precondition frequency 100 by default.
I0316 12:02:34.402969 140452064818368 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 0.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 0 (torch.Size([512, 13])), Block block_0 (torch.Size([512, 13])).
I0316 12:02:34.403160 140452064818368 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 1.block_0 ([torch.Size([0])]) for Parameter 1 (torch.Size([512])), Block block_0 (torch.Size([512])).
W0316 12:02:34.403109 140538584081600 distributed_shampoo.py:341] start_preconditioning_step set to -1. Setting start_preconditioning_step equal to precondition frequency 100 by default.
W0316 12:02:34.403202 140107298358464 distributed_shampoo.py:341] start_preconditioning_step set to -1. Setting start_preconditioning_step equal to precondition frequency 100 by default.
W0316 12:02:34.403240 140662572377280 distributed_shampoo.py:341] start_preconditioning_step set to -1. Setting start_preconditioning_step equal to precondition frequency 100 by default.
I0316 12:02:34.403326 140452064818368 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 2.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 2 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 12:02:34.403434 140452064818368 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 3.block_0 ([torch.Size([0])]) for Parameter 3 (torch.Size([256])), Block block_0 (torch.Size([256])).
W0316 12:02:34.403483 140344393745600 distributed_shampoo.py:341] start_preconditioning_step set to -1. Setting start_preconditioning_step equal to precondition frequency 100 by default.
I0316 12:02:34.403594 140452064818368 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 4.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 4 (torch.Size([128, 256])), Block block_0 (torch.Size([128, 256])).
I0316 12:02:34.403692 140452064818368 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 5.block_0 ([torch.Size([0])]) for Parameter 5 (torch.Size([128])), Block block_0 (torch.Size([128])).
I0316 12:02:34.403815 140452064818368 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 6.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 6 (torch.Size([1024, 506])), Block block_0 (torch.Size([1024, 506])).
I0316 12:02:34.403927 140452064818368 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 7.block_0 ([torch.Size([0])]) for Parameter 7 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 12:02:34.404453 140628633658560 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 0.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 0 (torch.Size([512, 13])), Block block_0 (torch.Size([512, 13])).
I0316 12:02:34.404616 140628633658560 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 1.block_0 ([torch.Size([0])]) for Parameter 1 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 12:02:34.404933 140157479363776 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 0.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 0 (torch.Size([512, 13])), Block block_0 (torch.Size([512, 13])).
I0316 12:02:34.405060 140452064818368 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 8.block_0 ([torch.Size([1024, 1024]), torch.Size([1024, 1024])]) for Parameter 8 (torch.Size([1024, 1024])), Block block_0 (torch.Size([1024, 1024])).
I0316 12:02:34.405102 140157479363776 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 1.block_0 ([torch.Size([0])]) for Parameter 1 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 12:02:34.405089 139759074923712 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 0.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 0 (torch.Size([512, 13])), Block block_0 (torch.Size([512, 13])).
I0316 12:02:34.405191 140452064818368 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 9.block_0 ([torch.Size([0])]) for Parameter 9 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 12:02:34.405245 139759074923712 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 1.block_0 ([torch.Size([0])]) for Parameter 1 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 12:02:34.405281 140157479363776 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 2.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 2 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 12:02:34.405334 140452064818368 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 10.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 10 (torch.Size([512, 1024])), Block block_0 (torch.Size([512, 1024])).
I0316 12:02:34.405393 140157479363776 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 3.block_0 ([torch.Size([0])]) for Parameter 3 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 12:02:34.405436 140452064818368 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 11.block_0 ([torch.Size([0])]) for Parameter 11 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 12:02:34.405429 139759074923712 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 2.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 2 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 12:02:34.405541 140157479363776 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 4.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 4 (torch.Size([128, 256])), Block block_0 (torch.Size([128, 256])).
I0316 12:02:34.405550 140452064818368 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 12.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 12 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 12:02:34.405540 139759074923712 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 3.block_0 ([torch.Size([0])]) for Parameter 3 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 12:02:34.405642 140452064818368 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 13.block_0 ([torch.Size([0])]) for Parameter 13 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 12:02:34.405649 140157479363776 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 5.block_0 ([torch.Size([0])]) for Parameter 5 (torch.Size([128])), Block block_0 (torch.Size([128])).
I0316 12:02:34.405691 139759074923712 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 4.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 4 (torch.Size([128, 256])), Block block_0 (torch.Size([128, 256])).
I0316 12:02:34.405732 140452064818368 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 14.block_0 ([torch.Size([0])]) for Parameter 14 (torch.Size([1, 256])), Block block_0 (torch.Size([256])).
I0316 12:02:34.405770 140157479363776 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 6.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 6 (torch.Size([1024, 506])), Block block_0 (torch.Size([1024, 506])).
I0316 12:02:34.405794 139759074923712 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 5.block_0 ([torch.Size([0])]) for Parameter 5 (torch.Size([128])), Block block_0 (torch.Size([128])).
I0316 12:02:34.405772 140628633658560 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 2.block_0 ([torch.Size([256, 256]), torch.Size([512, 512])]) for Parameter 2 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 12:02:34.405825 140452064818368 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 15.block_0 ([torch.Size([0])]) for Parameter 15 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0316 12:02:34.405882 140157479363776 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 7.block_0 ([torch.Size([0])]) for Parameter 7 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 12:02:34.405912 140452064818368 shampoo_preconditioner_list.py:612] Rank 0: ShampooPreconditionerList Numel Breakdown: (524626, 524288, 655360, 131072, 163840, 32768, 2609224, 2097152, 4194304, 2097152, 2621440, 524288, 655360, 131072, 131072, 2)
I0316 12:02:34.405910 139759074923712 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 6.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 6 (torch.Size([1024, 506])), Block block_0 (torch.Size([1024, 506])).
I0316 12:02:34.405934 140628633658560 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 3.block_0 ([torch.Size([0])]) for Parameter 3 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 12:02:34.405955 140452064818368 shampoo_preconditioner_list.py:615] Rank 0: ShampooPreconditionerList Bytes Breakdown: (2098504,)
I0316 12:02:34.405986 140452064818368 shampoo_preconditioner_list.py:618] Rank 0: ShampooPreconditionerList Total Elements: 17093020
I0316 12:02:34.406001 140157479363776 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 8.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 8 (torch.Size([1024, 1024])), Block block_0 (torch.Size([1024, 1024])).
I0316 12:02:34.406010 139759074923712 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 7.block_0 ([torch.Size([0])]) for Parameter 7 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 12:02:34.406020 140452064818368 shampoo_preconditioner_list.py:621] Rank 0: ShampooPreconditionerList Total Bytes: 2098504
I0316 12:02:34.405965 140538584081600 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 0.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 0 (torch.Size([512, 13])), Block block_0 (torch.Size([512, 13])).
I0316 12:02:34.406093 140157479363776 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 9.block_0 ([torch.Size([0])]) for Parameter 9 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 12:02:34.406088 140628633658560 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 4.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 4 (torch.Size([128, 256])), Block block_0 (torch.Size([128, 256])).
I0316 12:02:34.406123 139759074923712 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 8.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 8 (torch.Size([1024, 1024])), Block block_0 (torch.Size([1024, 1024])).
I0316 12:02:34.406148 140452064818368 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([512, 13])), Block block_0 (torch.Size([512, 13])).
I0316 12:02:34.406185 140628633658560 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 5.block_0 ([torch.Size([0])]) for Parameter 5 (torch.Size([128])), Block block_0 (torch.Size([128])).
I0316 12:02:34.406210 140157479363776 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 10.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 10 (torch.Size([512, 1024])), Block block_0 (torch.Size([512, 1024])).
I0316 12:02:34.406213 140452064818368 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 12:02:34.406219 139759074923712 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 9.block_0 ([torch.Size([0])]) for Parameter 9 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 12:02:34.406275 140452064818368 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 12:02:34.406309 140157479363776 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 11.block_0 ([torch.Size([0])]) for Parameter 11 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 12:02:34.406326 140452064818368 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 12:02:34.406322 140628633658560 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 6.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 6 (torch.Size([1024, 506])), Block block_0 (torch.Size([1024, 506])).
I0316 12:02:34.406372 140452064818368 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 4.block_0 (torch.Size([0])) for Parameter 4 (torch.Size([128, 256])), Block block_0 (torch.Size([128, 256])).
I0316 12:02:34.406351 140107298358464 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 0.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 0 (torch.Size([512, 13])), Block block_0 (torch.Size([512, 13])).
I0316 12:02:34.406409 140628633658560 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 7.block_0 ([torch.Size([0])]) for Parameter 7 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 12:02:34.406421 140452064818368 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 5.block_0 (torch.Size([0])) for Parameter 5 (torch.Size([128])), Block block_0 (torch.Size([128])).
I0316 12:02:34.406466 140452064818368 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 6.block_0 (torch.Size([0])) for Parameter 6 (torch.Size([1024, 506])), Block block_0 (torch.Size([1024, 506])).
I0316 12:02:34.406538 140452064818368 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 7.block_0 (torch.Size([0])) for Parameter 7 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 12:02:34.406535 140628633658560 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 8.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 8 (torch.Size([1024, 1024])), Block block_0 (torch.Size([1024, 1024])).
I0316 12:02:34.406558 140107298358464 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 1.block_0 ([torch.Size([0])]) for Parameter 1 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 12:02:34.406628 140628633658560 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 9.block_0 ([torch.Size([0])]) for Parameter 9 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 12:02:34.406648 140452064818368 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 8.block_0 (torch.Size([1024, 1024])) for Parameter 8 (torch.Size([1024, 1024])), Block block_0 (torch.Size([1024, 1024])).
I0316 12:02:34.406737 140452064818368 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 9.block_0 (torch.Size([0])) for Parameter 9 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 12:02:34.406751 140628633658560 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 10.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 10 (torch.Size([512, 1024])), Block block_0 (torch.Size([512, 1024])).
I0316 12:02:34.406752 140107298358464 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 2.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 2 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 12:02:34.406724 140344393745600 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 0.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 0 (torch.Size([512, 13])), Block block_0 (torch.Size([512, 13])).
I0316 12:02:34.406728 140538584081600 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 1.block_0 ([torch.Size([512, 512])]) for Parameter 1 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 12:02:34.406804 140452064818368 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 10.block_0 (torch.Size([0])) for Parameter 10 (torch.Size([512, 1024])), Block block_0 (torch.Size([512, 1024])).
I0316 12:02:34.406848 140452064818368 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 11.block_0 (torch.Size([0])) for Parameter 11 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 12:02:34.406843 140628633658560 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 11.block_0 ([torch.Size([0])]) for Parameter 11 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 12:02:34.406880 140107298358464 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 3.block_0 ([torch.Size([0])]) for Parameter 3 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 12:02:34.406898 140452064818368 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 12.block_0 (torch.Size([0])) for Parameter 12 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 12:02:34.406902 140344393745600 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 1.block_0 ([torch.Size([0])]) for Parameter 1 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 12:02:34.406941 140452064818368 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 13.block_0 (torch.Size([0])) for Parameter 13 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 12:02:34.406950 140628633658560 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 12.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 12 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 12:02:34.406983 140452064818368 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 14.block_0 (torch.Size([0])) for Parameter 14 (torch.Size([1, 256])), Block block_0 (torch.Size([256])).
I0316 12:02:34.406965 140538584081600 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 2.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 2 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 12:02:34.407030 140452064818368 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 15.block_0 (torch.Size([0])) for Parameter 15 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0316 12:02:34.407026 140107298358464 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 4.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 4 (torch.Size([128, 256])), Block block_0 (torch.Size([128, 256])).
I0316 12:02:34.407040 140628633658560 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 13.block_0 ([torch.Size([0])]) for Parameter 13 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 12:02:34.407091 140452064818368 shampoo_preconditioner_list.py:375] Rank 0: AdaGradPreconditionerList Numel Breakdown: (0, 0, 0, 0, 0, 0, 0, 0, 1048576, 0, 0, 0, 0, 0, 0, 0)
I0316 12:02:34.407090 140344393745600 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 2.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 2 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 12:02:34.407115 140628633658560 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 14.block_0 ([torch.Size([0])]) for Parameter 14 (torch.Size([1, 256])), Block block_0 (torch.Size([256])).
I0316 12:02:34.407118 140107298358464 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 5.block_0 ([torch.Size([0])]) for Parameter 5 (torch.Size([128])), Block block_0 (torch.Size([128])).
I0316 12:02:34.407127 140452064818368 shampoo_preconditioner_list.py:378] Rank 0: AdaGradPreconditionerList Bytes Breakdown: (0, 0, 0, 0, 0, 0, 0, 0, 4194304, 0, 0, 0, 0, 0, 0, 0)
I0316 12:02:34.407155 140452064818368 shampoo_preconditioner_list.py:381] Rank 0: AdaGradPreconditionerList Total Elements: 1048576
I0316 12:02:34.407143 140538584081600 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 3.block_0 ([torch.Size([256, 256])]) for Parameter 3 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 12:02:34.407186 140452064818368 shampoo_preconditioner_list.py:384] Rank 0: AdaGradPreconditionerList Total Bytes: 4194304
I0316 12:02:34.407190 140628633658560 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 15.block_0 ([torch.Size([0])]) for Parameter 15 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0316 12:02:34.407219 140344393745600 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 3.block_0 ([torch.Size([0])]) for Parameter 3 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 12:02:34.407276 140628633658560 shampoo_preconditioner_list.py:612] Rank 3: ShampooPreconditionerList Numel Breakdown: (524626, 524288, 655360, 131072, 163840, 32768, 2609224, 2097152, 4194304, 2097152, 2621440, 524288, 655360, 131072, 131072, 2)
I0316 12:02:34.407266 139759074923712 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 10.block_0 ([torch.Size([512, 512]), torch.Size([1024, 1024])]) for Parameter 10 (torch.Size([512, 1024])), Block block_0 (torch.Size([512, 1024])).
I0316 12:02:34.407317 140628633658560 shampoo_preconditioner_list.py:615] Rank 3: ShampooPreconditionerList Bytes Breakdown: (2098504,)
I0316 12:02:34.407262 140662572377280 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 0.block_0 ([torch.Size([512, 512]), torch.Size([13, 13])]) for Parameter 0 (torch.Size([512, 13])), Block block_0 (torch.Size([512, 13])).
I0316 12:02:34.407320 140538584081600 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 4.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 4 (torch.Size([128, 256])), Block block_0 (torch.Size([128, 256])).
I0316 12:02:34.407346 140628633658560 shampoo_preconditioner_list.py:618] Rank 3: ShampooPreconditionerList Total Elements: 17093020
I0316 12:02:34.407337 140157479363776 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 12.block_0 ([torch.Size([256, 256]), torch.Size([512, 512])]) for Parameter 12 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 12:02:34.407372 140628633658560 shampoo_preconditioner_list.py:621] Rank 3: ShampooPreconditionerList Total Bytes: 2098504
I0316 12:02:34.407398 139759074923712 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 11.block_0 ([torch.Size([0])]) for Parameter 11 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 12:02:34.407444 140662572377280 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 1.block_0 ([torch.Size([0])]) for Parameter 1 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 12:02:34.407485 140157479363776 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 13.block_0 ([torch.Size([0])]) for Parameter 13 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 12:02:34.407509 140628633658560 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([512, 13])), Block block_0 (torch.Size([512, 13])).
I0316 12:02:34.407504 140538584081600 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 5.block_0 ([torch.Size([128, 128])]) for Parameter 5 (torch.Size([128])), Block block_0 (torch.Size([128])).
I0316 12:02:34.407511 140344393745600 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 4.block_0 ([torch.Size([128, 128]), torch.Size([256, 256])]) for Parameter 4 (torch.Size([128, 256])), Block block_0 (torch.Size([128, 256])).
I0316 12:02:34.407540 139759074923712 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 12.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 12 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 12:02:34.407570 140157479363776 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 14.block_0 ([torch.Size([0])]) for Parameter 14 (torch.Size([1, 256])), Block block_0 (torch.Size([256])).
I0316 12:02:34.407577 140628633658560 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 12:02:34.407623 140662572377280 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 2.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 2 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 12:02:34.407640 139759074923712 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 13.block_0 ([torch.Size([0])]) for Parameter 13 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 12:02:34.407653 140538584081600 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 6.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 6 (torch.Size([1024, 506])), Block block_0 (torch.Size([1024, 506])).
I0316 12:02:34.407673 140157479363776 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 15.block_0 ([torch.Size([0])]) for Parameter 15 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0316 12:02:34.407667 140344393745600 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 5.block_0 ([torch.Size([0])]) for Parameter 5 (torch.Size([128])), Block block_0 (torch.Size([128])).
I0316 12:02:34.407736 139759074923712 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 14.block_0 ([torch.Size([0])]) for Parameter 14 (torch.Size([1, 256])), Block block_0 (torch.Size([256])).
I0316 12:02:34.407740 140452064818368 submission.py:105] Large parameters (embedding tables) detected (dim >= 1_000_000). Instantiating Row-Wise AdaGrad...
I0316 12:02:34.407773 140157479363776 shampoo_preconditioner_list.py:612] Rank 4: ShampooPreconditionerList Numel Breakdown: (524626, 524288, 655360, 131072, 163840, 32768, 2609224, 2097152, 4194304, 2097152, 2621440, 524288, 655360, 131072, 131072, 2)
I0316 12:02:34.407768 140662572377280 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 3.block_0 ([torch.Size([0])]) for Parameter 3 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 12:02:34.407808 140157479363776 shampoo_preconditioner_list.py:615] Rank 4: ShampooPreconditionerList Bytes Breakdown: (2098504,)
I0316 12:02:34.407824 139759074923712 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 15.block_0 ([torch.Size([0])]) for Parameter 15 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0316 12:02:34.407828 140344393745600 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 6.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 6 (torch.Size([1024, 506])), Block block_0 (torch.Size([1024, 506])).
I0316 12:02:34.407843 140157479363776 shampoo_preconditioner_list.py:618] Rank 4: ShampooPreconditionerList Total Elements: 17093020
I0316 12:02:34.407828 140107298358464 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 6.block_0 ([torch.Size([1024, 1024]), torch.Size([506, 506])]) for Parameter 6 (torch.Size([1024, 506])), Block block_0 (torch.Size([1024, 506])).
I0316 12:02:34.407870 140157479363776 shampoo_preconditioner_list.py:621] Rank 4: ShampooPreconditionerList Total Bytes: 2098504
I0316 12:02:34.407859 140538584081600 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 7.block_0 ([torch.Size([1024, 1024])]) for Parameter 7 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 12:02:34.407917 139759074923712 shampoo_preconditioner_list.py:612] Rank 1: ShampooPreconditionerList Numel Breakdown: (524626, 524288, 655360, 131072, 163840, 32768, 2609224, 2097152, 4194304, 2097152, 2621440, 524288, 655360, 131072, 131072, 2)
I0316 12:02:34.407916 140662572377280 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 4.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 4 (torch.Size([128, 256])), Block block_0 (torch.Size([128, 256])).
I0316 12:02:34.407934 140344393745600 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 7.block_0 ([torch.Size([0])]) for Parameter 7 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 12:02:34.407957 139759074923712 shampoo_preconditioner_list.py:615] Rank 1: ShampooPreconditionerList Bytes Breakdown: (2098504,)
I0316 12:02:34.407990 139759074923712 shampoo_preconditioner_list.py:618] Rank 1: ShampooPreconditionerList Total Elements: 17093020
I0316 12:02:34.407997 140157479363776 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([512, 13])), Block block_0 (torch.Size([512, 13])).
I0316 12:02:34.407994 140107298358464 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 7.block_0 ([torch.Size([0])]) for Parameter 7 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 12:02:34.408003 140538584081600 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 8.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 8 (torch.Size([1024, 1024])), Block block_0 (torch.Size([1024, 1024])).
I0316 12:02:34.408022 139759074923712 shampoo_preconditioner_list.py:621] Rank 1: ShampooPreconditionerList Total Bytes: 2098504
I0316 12:02:34.408023 140662572377280 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 5.block_0 ([torch.Size([0])]) for Parameter 5 (torch.Size([128])), Block block_0 (torch.Size([128])).
I0316 12:02:34.408026 140628633658560 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([256, 512])) for Parameter 2 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 12:02:34.408050 140157479363776 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 12:02:34.408073 140344393745600 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 8.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 8 (torch.Size([1024, 1024])), Block block_0 (torch.Size([1024, 1024])).
I0316 12:02:34.408095 140157479363776 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 12:02:34.408118 140628633658560 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 12:02:34.408138 140107298358464 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 8.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 8 (torch.Size([1024, 1024])), Block block_0 (torch.Size([1024, 1024])).
I0316 12:02:34.408139 140157479363776 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 12:02:34.408158 140662572377280 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 6.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 6 (torch.Size([1024, 506])), Block block_0 (torch.Size([1024, 506])).
I0316 12:02:34.408149 139759074923712 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([512, 13])), Block block_0 (torch.Size([512, 13])).
I0316 12:02:34.408199 140628633658560 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 4.block_0 (torch.Size([0])) for Parameter 4 (torch.Size([128, 256])), Block block_0 (torch.Size([128, 256])).
I0316 12:02:34.408202 140157479363776 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 4.block_0 (torch.Size([0])) for Parameter 4 (torch.Size([128, 256])), Block block_0 (torch.Size([128, 256])).
I0316 12:02:34.408209 140344393745600 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 9.block_0 ([torch.Size([0])]) for Parameter 9 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 12:02:34.408226 139759074923712 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 12:02:34.408249 140157479363776 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 5.block_0 (torch.Size([0])) for Parameter 5 (torch.Size([128])), Block block_0 (torch.Size([128])).
I0316 12:02:34.408246 140107298358464 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 9.block_0 ([torch.Size([0])]) for Parameter 9 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 12:02:34.408255 140628633658560 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 5.block_0 (torch.Size([0])) for Parameter 5 (torch.Size([128])), Block block_0 (torch.Size([128])).
I0316 12:02:34.408273 140662572377280 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 7.block_0 ([torch.Size([0])]) for Parameter 7 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 12:02:34.408285 139759074923712 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 12:02:34.408304 140628633658560 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 6.block_0 (torch.Size([0])) for Parameter 6 (torch.Size([1024, 506])), Block block_0 (torch.Size([1024, 506])).
I0316 12:02:34.408305 140157479363776 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 6.block_0 (torch.Size([0])) for Parameter 6 (torch.Size([1024, 506])), Block block_0 (torch.Size([1024, 506])).
I0316 12:02:34.408333 139759074923712 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 12:02:34.408357 140157479363776 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 7.block_0 (torch.Size([0])) for Parameter 7 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 12:02:34.408357 140628633658560 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 7.block_0 (torch.Size([0])) for Parameter 7 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 12:02:34.408364 140344393745600 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 10.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 10 (torch.Size([512, 1024])), Block block_0 (torch.Size([512, 1024])).
I0316 12:02:34.408389 139759074923712 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 4.block_0 (torch.Size([0])) for Parameter 4 (torch.Size([128, 256])), Block block_0 (torch.Size([128, 256])).
I0316 12:02:34.408384 140107298358464 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 10.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 10 (torch.Size([512, 1024])), Block block_0 (torch.Size([512, 1024])).
I0316 12:02:34.408406 140628633658560 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 8.block_0 (torch.Size([0])) for Parameter 8 (torch.Size([1024, 1024])), Block block_0 (torch.Size([1024, 1024])).
I0316 12:02:34.408412 140157479363776 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 8.block_0 (torch.Size([0])) for Parameter 8 (torch.Size([1024, 1024])), Block block_0 (torch.Size([1024, 1024])).
I0316 12:02:34.408406 140662572377280 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 8.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 8 (torch.Size([1024, 1024])), Block block_0 (torch.Size([1024, 1024])).
I0316 12:02:34.408444 139759074923712 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 5.block_0 (torch.Size([0])) for Parameter 5 (torch.Size([128])), Block block_0 (torch.Size([128])).
I0316 12:02:34.408452 140628633658560 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 9.block_0 (torch.Size([0])) for Parameter 9 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 12:02:34.408458 140157479363776 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 9.block_0 (torch.Size([0])) for Parameter 9 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 12:02:34.408467 140344393745600 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 11.block_0 ([torch.Size([0])]) for Parameter 11 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 12:02:34.408485 140107298358464 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 11.block_0 ([torch.Size([0])]) for Parameter 11 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 12:02:34.408498 140628633658560 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 10.block_0 (torch.Size([0])) for Parameter 10 (torch.Size([512, 1024])), Block block_0 (torch.Size([512, 1024])).
I0316 12:02:34.408500 139759074923712 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 6.block_0 (torch.Size([0])) for Parameter 6 (torch.Size([1024, 506])), Block block_0 (torch.Size([1024, 506])).
I0316 12:02:34.408503 140157479363776 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 10.block_0 (torch.Size([0])) for Parameter 10 (torch.Size([512, 1024])), Block block_0 (torch.Size([512, 1024])).
I0316 12:02:34.408503 140662572377280 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 9.block_0 ([torch.Size([0])]) for Parameter 9 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 12:02:34.408544 140628633658560 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 11.block_0 (torch.Size([0])) for Parameter 11 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 12:02:34.408547 140157479363776 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 11.block_0 (torch.Size([0])) for Parameter 11 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 12:02:34.408553 139759074923712 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 7.block_0 (torch.Size([0])) for Parameter 7 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 12:02:34.408597 140628633658560 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 12.block_0 (torch.Size([0])) for Parameter 12 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 12:02:34.408603 139759074923712 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 8.block_0 (torch.Size([0])) for Parameter 8 (torch.Size([1024, 1024])), Block block_0 (torch.Size([1024, 1024])).
I0316 12:02:34.408624 140107298358464 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 12.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 12 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 12:02:34.408628 140662572377280 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 10.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 10 (torch.Size([512, 1024])), Block block_0 (torch.Size([512, 1024])).
I0316 12:02:34.408629 140344393745600 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 12.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 12 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 12:02:34.408668 140628633658560 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 13.block_0 (torch.Size([0])) for Parameter 13 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 12:02:34.408670 139759074923712 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 9.block_0 (torch.Size([0])) for Parameter 9 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 12:02:34.408667 140538584081600 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 9.block_0 ([torch.Size([1024, 1024])]) for Parameter 9 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 12:02:34.408717 140628633658560 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 14.block_0 (torch.Size([0])) for Parameter 14 (torch.Size([1, 256])), Block block_0 (torch.Size([256])).
I0316 12:02:34.408723 140107298358464 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 13.block_0 ([torch.Size([0])]) for Parameter 13 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 12:02:34.408724 140662572377280 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 11.block_0 ([torch.Size([0])]) for Parameter 11 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 12:02:34.408736 140344393745600 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 13.block_0 ([torch.Size([0])]) for Parameter 13 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 12:02:34.408759 139759074923712 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 10.block_0 (torch.Size([512, 1024])) for Parameter 10 (torch.Size([512, 1024])), Block block_0 (torch.Size([512, 1024])).
I0316 12:02:34.408760 140628633658560 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 15.block_0 (torch.Size([0])) for Parameter 15 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0316 12:02:34.408823 140628633658560 shampoo_preconditioner_list.py:375] Rank 3: AdaGradPreconditionerList Numel Breakdown: (0, 0, 131072, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)
I0316 12:02:34.408828 139759074923712 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 11.block_0 (torch.Size([0])) for Parameter 11 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 12:02:34.408826 140107298358464 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 14.block_0 ([torch.Size([0])]) for Parameter 14 (torch.Size([1, 256])), Block block_0 (torch.Size([256])).
I0316 12:02:34.408831 140344393745600 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 14.block_0 ([torch.Size([0])]) for Parameter 14 (torch.Size([1, 256])), Block block_0 (torch.Size([256])).
I0316 12:02:34.408838 140662572377280 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 12.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 12 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 12:02:34.408843 140538584081600 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 10.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 10 (torch.Size([512, 1024])), Block block_0 (torch.Size([512, 1024])).
I0316 12:02:34.408858 140628633658560 shampoo_preconditioner_list.py:378] Rank 3: AdaGradPreconditionerList Bytes Breakdown: (0, 0, 524288, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)
I0316 12:02:34.408875 139759074923712 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 12.block_0 (torch.Size([0])) for Parameter 12 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 12:02:34.408900 140628633658560 shampoo_preconditioner_list.py:381] Rank 3: AdaGradPreconditionerList Total Elements: 131072
I0316 12:02:34.408928 140107298358464 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 15.block_0 ([torch.Size([0])]) for Parameter 15 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0316 12:02:34.408928 140628633658560 shampoo_preconditioner_list.py:384] Rank 3: AdaGradPreconditionerList Total Bytes: 524288
I0316 12:02:34.408947 140662572377280 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 13.block_0 ([torch.Size([0])]) for Parameter 13 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 12:02:34.408959 139759074923712 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 13.block_0 (torch.Size([0])) for Parameter 13 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 12:02:34.408961 140344393745600 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 15.block_0 ([torch.Size([0])]) for Parameter 15 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0316 12:02:34.409007 139759074923712 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 14.block_0 (torch.Size([0])) for Parameter 14 (torch.Size([1, 256])), Block block_0 (torch.Size([256])).
I0316 12:02:34.409034 140107298358464 shampoo_preconditioner_list.py:612] Rank 2: ShampooPreconditionerList Numel Breakdown: (524626, 524288, 655360, 131072, 163840, 32768, 2609224, 2097152, 4194304, 2097152, 2621440, 524288, 655360, 131072, 131072, 2)
I0316 12:02:34.409034 140662572377280 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 14.block_0 ([torch.Size([0])]) for Parameter 14 (torch.Size([1, 256])), Block block_0 (torch.Size([256])).
I0316 12:02:34.409059 139759074923712 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 15.block_0 (torch.Size([0])) for Parameter 15 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0316 12:02:34.409069 140344393745600 shampoo_preconditioner_list.py:612] Rank 5: ShampooPreconditionerList Numel Breakdown: (524626, 524288, 655360, 131072, 163840, 32768, 2609224, 2097152, 4194304, 2097152, 2621440, 524288, 655360, 131072, 131072, 2)
I0316 12:02:34.409055 140157479363776 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 12.block_0 (torch.Size([256, 512])) for Parameter 12 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 12:02:34.409080 140107298358464 shampoo_preconditioner_list.py:615] Rank 2: ShampooPreconditionerList Bytes Breakdown: (2098504,)
I0316 12:02:34.409115 140107298358464 shampoo_preconditioner_list.py:618] Rank 2: ShampooPreconditionerList Total Elements: 17093020
I0316 12:02:34.409123 139759074923712 shampoo_preconditioner_list.py:375] Rank 1: AdaGradPreconditionerList Numel Breakdown: (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 524288, 0, 0, 0, 0, 0)
I0316 12:02:34.409116 140344393745600 shampoo_preconditioner_list.py:615] Rank 5: ShampooPreconditionerList Bytes Breakdown: (2098504,)
I0316 12:02:34.409128 140662572377280 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 15.block_0 ([torch.Size([0])]) for Parameter 15 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0316 12:02:34.409152 140107298358464 shampoo_preconditioner_list.py:621] Rank 2: ShampooPreconditionerList Total Bytes: 2098504
I0316 12:02:34.409150 140157479363776 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 13.block_0 (torch.Size([0])) for Parameter 13 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 12:02:34.409156 139759074923712 shampoo_preconditioner_list.py:378] Rank 1: AdaGradPreconditionerList Bytes Breakdown: (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2097152, 0, 0, 0, 0, 0)
I0316 12:02:34.409158 140344393745600 shampoo_preconditioner_list.py:618] Rank 5: ShampooPreconditionerList Total Elements: 17093020
I0316 12:02:34.409184 139759074923712 shampoo_preconditioner_list.py:381] Rank 1: AdaGradPreconditionerList Total Elements: 524288
I0316 12:02:34.409192 140344393745600 shampoo_preconditioner_list.py:621] Rank 5: ShampooPreconditionerList Total Bytes: 2098504
I0316 12:02:34.409200 140157479363776 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 14.block_0 (torch.Size([0])) for Parameter 14 (torch.Size([1, 256])), Block block_0 (torch.Size([256])).
I0316 12:02:34.409216 139759074923712 shampoo_preconditioner_list.py:384] Rank 1: AdaGradPreconditionerList Total Bytes: 2097152
I0316 12:02:34.409221 140662572377280 shampoo_preconditioner_list.py:612] Rank 6: ShampooPreconditionerList Numel Breakdown: (524626, 524288, 655360, 131072, 163840, 32768, 2609224, 2097152, 4194304, 2097152, 2621440, 524288, 655360, 131072, 131072, 2)
I0316 12:02:34.409245 140157479363776 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 15.block_0 (torch.Size([0])) for Parameter 15 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0316 12:02:34.409260 140662572377280 shampoo_preconditioner_list.py:615] Rank 6: ShampooPreconditionerList Bytes Breakdown: (2098504,)
I0316 12:02:34.409293 140662572377280 shampoo_preconditioner_list.py:618] Rank 6: ShampooPreconditionerList Total Elements: 17093020
I0316 12:02:34.409298 140107298358464 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([512, 13])), Block block_0 (torch.Size([512, 13])).
I0316 12:02:34.409314 140157479363776 shampoo_preconditioner_list.py:375] Rank 4: AdaGradPreconditionerList Numel Breakdown: (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 131072, 0, 0, 0)
I0316 12:02:34.409335 140662572377280 shampoo_preconditioner_list.py:621] Rank 6: ShampooPreconditionerList Total Bytes: 2098504
I0316 12:02:34.409347 140157479363776 shampoo_preconditioner_list.py:378] Rank 4: AdaGradPreconditionerList Bytes Breakdown: (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 524288, 0, 0, 0)
I0316 12:02:34.409337 140344393745600 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([512, 13])), Block block_0 (torch.Size([512, 13])).
I0316 12:02:34.409360 140107298358464 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 12:02:34.409375 140157479363776 shampoo_preconditioner_list.py:381] Rank 4: AdaGradPreconditionerList Total Elements: 131072
I0316 12:02:34.409402 140157479363776 shampoo_preconditioner_list.py:384] Rank 4: AdaGradPreconditionerList Total Bytes: 524288
I0316 12:02:34.409403 140344393745600 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 12:02:34.409423 140107298358464 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 12:02:34.409400 140538584081600 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 11.block_0 ([torch.Size([512, 512])]) for Parameter 11 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 12:02:34.409460 140344393745600 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 12:02:34.409492 140107298358464 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 12:02:34.409526 140344393745600 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 12:02:34.409554 140107298358464 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 4.block_0 (torch.Size([0])) for Parameter 4 (torch.Size([128, 256])), Block block_0 (torch.Size([128, 256])).
I0316 12:02:34.409548 140662572377280 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([512, 13])) for Parameter 0 (torch.Size([512, 13])), Block block_0 (torch.Size([512, 13])).
I0316 12:02:34.409572 140538584081600 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 12.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 12 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 12:02:34.409614 140107298358464 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 5.block_0 (torch.Size([0])) for Parameter 5 (torch.Size([128])), Block block_0 (torch.Size([128])).
I0316 12:02:34.409636 140662572377280 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 12:02:34.409652 140344393745600 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 4.block_0 (torch.Size([128, 256])) for Parameter 4 (torch.Size([128, 256])), Block block_0 (torch.Size([128, 256])).
I0316 12:02:34.409703 140662572377280 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 12:02:34.409721 140107298358464 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 6.block_0 (torch.Size([1024, 506])) for Parameter 6 (torch.Size([1024, 506])), Block block_0 (torch.Size([1024, 506])).
I0316 12:02:34.409739 140344393745600 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 5.block_0 (torch.Size([0])) for Parameter 5 (torch.Size([128])), Block block_0 (torch.Size([128])).
I0316 12:02:34.409724 140452064818368 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([524288, 128])) for Parameter 0 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 12:02:34.409765 140662572377280 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 12:02:34.409799 140344393745600 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 6.block_0 (torch.Size([0])) for Parameter 6 (torch.Size([1024, 506])), Block block_0 (torch.Size([1024, 506])).
I0316 12:02:34.409811 140107298358464 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 7.block_0 (torch.Size([0])) for Parameter 7 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 12:02:34.409825 140662572377280 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 4.block_0 (torch.Size([0])) for Parameter 4 (torch.Size([128, 256])), Block block_0 (torch.Size([128, 256])).
I0316 12:02:34.409832 140452064818368 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_1 (torch.Size([0])) for Parameter 0 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 12:02:34.409866 140107298358464 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 8.block_0 (torch.Size([0])) for Parameter 8 (torch.Size([1024, 1024])), Block block_0 (torch.Size([1024, 1024])).
I0316 12:02:34.409869 140344393745600 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 7.block_0 (torch.Size([0])) for Parameter 7 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 12:02:34.409884 140662572377280 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 5.block_0 (torch.Size([0])) for Parameter 5 (torch.Size([128])), Block block_0 (torch.Size([128])).
I0316 12:02:34.409892 140452064818368 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 12:02:34.409927 140107298358464 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 9.block_0 (torch.Size([0])) for Parameter 9 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 12:02:34.409927 140344393745600 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 8.block_0 (torch.Size([0])) for Parameter 8 (torch.Size([1024, 1024])), Block block_0 (torch.Size([1024, 1024])).
I0316 12:02:34.409935 140662572377280 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 6.block_0 (torch.Size([0])) for Parameter 6 (torch.Size([1024, 506])), Block block_0 (torch.Size([1024, 506])).
I0316 12:02:34.409944 140452064818368 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_1 (torch.Size([0])) for Parameter 1 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 12:02:34.409982 140344393745600 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 9.block_0 (torch.Size([0])) for Parameter 9 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 12:02:34.409985 140107298358464 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 10.block_0 (torch.Size([0])) for Parameter 10 (torch.Size([512, 1024])), Block block_0 (torch.Size([512, 1024])).
I0316 12:02:34.409993 140452064818368 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 12:02:34.409992 140662572377280 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 7.block_0 (torch.Size([0])) for Parameter 7 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 12:02:34.410037 140107298358464 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 11.block_0 (torch.Size([0])) for Parameter 11 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 12:02:34.410042 140452064818368 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_1 (torch.Size([0])) for Parameter 2 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 12:02:34.410044 140662572377280 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 8.block_0 (torch.Size([0])) for Parameter 8 (torch.Size([1024, 1024])), Block block_0 (torch.Size([1024, 1024])).
I0316 12:02:34.410045 140344393745600 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 10.block_0 (torch.Size([0])) for Parameter 10 (torch.Size([512, 1024])), Block block_0 (torch.Size([512, 1024])).
I0316 12:02:34.410089 140452064818368 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 12:02:34.410093 140662572377280 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 9.block_0 (torch.Size([0])) for Parameter 9 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 12:02:34.410096 140107298358464 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 12.block_0 (torch.Size([0])) for Parameter 12 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 12:02:34.410101 140344393745600 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 11.block_0 (torch.Size([0])) for Parameter 11 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 12:02:34.410144 140662572377280 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 10.block_0 (torch.Size([0])) for Parameter 10 (torch.Size([512, 1024])), Block block_0 (torch.Size([512, 1024])).
I0316 12:02:34.410145 140107298358464 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 13.block_0 (torch.Size([0])) for Parameter 13 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 12:02:34.410150 140452064818368 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_1 (torch.Size([0])) for Parameter 3 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 12:02:34.410154 140344393745600 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 12.block_0 (torch.Size([0])) for Parameter 12 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 12:02:34.410193 140107298358464 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 14.block_0 (torch.Size([0])) for Parameter 14 (torch.Size([1, 256])), Block block_0 (torch.Size([256])).
I0316 12:02:34.410197 140452064818368 shampoo_preconditioner_list.py:375] Rank 0: AdaGradPreconditionerList Numel Breakdown: (67108864, 0, 0, 0, 0, 0, 0, 0)
I0316 12:02:34.410202 140662572377280 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 11.block_0 (torch.Size([0])) for Parameter 11 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 12:02:34.410207 140344393745600 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 13.block_0 (torch.Size([0])) for Parameter 13 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 12:02:34.410228 140452064818368 shampoo_preconditioner_list.py:378] Rank 0: AdaGradPreconditionerList Bytes Breakdown: (268435456, 0, 0, 0, 0, 0, 0, 0)
I0316 12:02:34.410250 140107298358464 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 15.block_0 (torch.Size([0])) for Parameter 15 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0316 12:02:34.410255 140452064818368 shampoo_preconditioner_list.py:381] Rank 0: AdaGradPreconditionerList Total Elements: 67108864
I0316 12:02:34.410252 140662572377280 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 12.block_0 (torch.Size([0])) for Parameter 12 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 12:02:34.410260 140344393745600 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 14.block_0 (torch.Size([0])) for Parameter 14 (torch.Size([1, 256])), Block block_0 (torch.Size([256])).
I0316 12:02:34.410282 140452064818368 shampoo_preconditioner_list.py:384] Rank 0: AdaGradPreconditionerList Total Bytes: 268435456
I0316 12:02:34.410299 140662572377280 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 13.block_0 (torch.Size([0])) for Parameter 13 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 12:02:34.410312 140344393745600 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 15.block_0 (torch.Size([0])) for Parameter 15 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0316 12:02:34.410319 140107298358464 shampoo_preconditioner_list.py:375] Rank 2: AdaGradPreconditionerList Numel Breakdown: (0, 0, 0, 0, 0, 0, 518144, 0, 0, 0, 0, 0, 0, 0, 0, 0)
I0316 12:02:34.410315 140538584081600 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 13.block_0 ([torch.Size([256, 256])]) for Parameter 13 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 12:02:34.410353 140107298358464 shampoo_preconditioner_list.py:378] Rank 2: AdaGradPreconditionerList Bytes Breakdown: (0, 0, 0, 0, 0, 0, 2072576, 0, 0, 0, 0, 0, 0, 0, 0, 0)
I0316 12:02:34.410356 140662572377280 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 14.block_0 (torch.Size([0])) for Parameter 14 (torch.Size([1, 256])), Block block_0 (torch.Size([256])).
I0316 12:02:34.410375 140344393745600 shampoo_preconditioner_list.py:375] Rank 5: AdaGradPreconditionerList Numel Breakdown: (0, 0, 0, 0, 32768, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)
I0316 12:02:34.410384 140107298358464 shampoo_preconditioner_list.py:381] Rank 2: AdaGradPreconditionerList Total Elements: 518144
I0316 12:02:34.410404 140662572377280 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 15.block_0 (torch.Size([0])) for Parameter 15 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0316 12:02:34.410413 140107298358464 shampoo_preconditioner_list.py:384] Rank 2: AdaGradPreconditionerList Total Bytes: 2072576
I0316 12:02:34.410420 140344393745600 shampoo_preconditioner_list.py:378] Rank 5: AdaGradPreconditionerList Bytes Breakdown: (0, 0, 0, 0, 131072, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)
I0316 12:02:34.410453 140344393745600 shampoo_preconditioner_list.py:381] Rank 5: AdaGradPreconditionerList Total Elements: 32768
I0316 12:02:34.410474 140662572377280 shampoo_preconditioner_list.py:375] Rank 6: AdaGradPreconditionerList Numel Breakdown: (6656, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)
I0316 12:02:34.410489 140344393745600 shampoo_preconditioner_list.py:384] Rank 5: AdaGradPreconditionerList Total Bytes: 131072
I0316 12:02:34.410522 140662572377280 shampoo_preconditioner_list.py:378] Rank 6: AdaGradPreconditionerList Bytes Breakdown: (26624, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)
I0316 12:02:34.410517 140538584081600 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 14.block_0 ([torch.Size([256, 256])]) for Parameter 14 (torch.Size([1, 256])), Block block_0 (torch.Size([256])).
I0316 12:02:34.410558 140662572377280 shampoo_preconditioner_list.py:381] Rank 6: AdaGradPreconditionerList Total Elements: 6656
I0316 12:02:34.410604 140662572377280 shampoo_preconditioner_list.py:384] Rank 6: AdaGradPreconditionerList Total Bytes: 26624
I0316 12:02:34.410717 140538584081600 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 15.block_0 ([torch.Size([1, 1])]) for Parameter 15 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0316 12:02:34.410858 140538584081600 shampoo_preconditioner_list.py:612] Rank 7: ShampooPreconditionerList Numel Breakdown: (524626, 524288, 655360, 131072, 163840, 32768, 2609224, 2097152, 4194304, 2097152, 2621440, 524288, 655360, 131072, 131072, 2)
I0316 12:02:34.410909 140538584081600 shampoo_preconditioner_list.py:615] Rank 7: ShampooPreconditionerList Bytes Breakdown: (2098504, 2097152, 2621440, 524288, 655360, 131072, 10436896, 8388608, 16777216)
I0316 12:02:34.410951 140538584081600 shampoo_preconditioner_list.py:618] Rank 7: ShampooPreconditionerList Total Elements: 17093020
I0316 12:02:34.410989 140538584081600 shampoo_preconditioner_list.py:621] Rank 7: ShampooPreconditionerList Total Bytes: 43730536
I0316 12:02:34.411004 140628633658560 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 12:02:34.411121 140628633658560 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_1 (torch.Size([0])) for Parameter 0 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 12:02:34.411143 140538584081600 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([512, 13])), Block block_0 (torch.Size([512, 13])).
I0316 12:02:34.411188 140628633658560 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 12:02:34.411258 140538584081600 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([512])) for Parameter 1 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 12:02:34.411338 140538584081600 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 12:02:34.411426 140538584081600 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([256])) for Parameter 3 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 12:02:34.411510 140538584081600 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 4.block_0 (torch.Size([0])) for Parameter 4 (torch.Size([128, 256])), Block block_0 (torch.Size([128, 256])).
I0316 12:02:34.411599 140538584081600 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 5.block_0 (torch.Size([128])) for Parameter 5 (torch.Size([128])), Block block_0 (torch.Size([128])).
I0316 12:02:34.411673 140538584081600 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 6.block_0 (torch.Size([0])) for Parameter 6 (torch.Size([1024, 506])), Block block_0 (torch.Size([1024, 506])).
I0316 12:02:34.411729 140157479363776 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 12:02:34.411765 140538584081600 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 7.block_0 (torch.Size([1024])) for Parameter 7 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 12:02:34.411831 140538584081600 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 8.block_0 (torch.Size([0])) for Parameter 8 (torch.Size([1024, 1024])), Block block_0 (torch.Size([1024, 1024])).
I0316 12:02:34.411835 140157479363776 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_1 (torch.Size([0])) for Parameter 0 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 12:02:34.411895 140157479363776 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 12:02:34.411916 140538584081600 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 9.block_0 (torch.Size([1024])) for Parameter 9 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 12:02:34.411958 140157479363776 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_1 (torch.Size([0])) for Parameter 1 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 12:02:34.411986 140538584081600 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 10.block_0 (torch.Size([0])) for Parameter 10 (torch.Size([512, 1024])), Block block_0 (torch.Size([512, 1024])).
I0316 12:02:34.412070 140538584081600 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 11.block_0 (torch.Size([512])) for Parameter 11 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 12:02:34.412139 140538584081600 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 12.block_0 (torch.Size([0])) for Parameter 12 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 12:02:34.412220 140538584081600 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 13.block_0 (torch.Size([256])) for Parameter 13 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 12:02:34.412302 140538584081600 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 14.block_0 (torch.Size([256])) for Parameter 14 (torch.Size([1, 256])), Block block_0 (torch.Size([256])).
I0316 12:02:34.412353 139759074923712 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 12:02:34.412412 140538584081600 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 15.block_0 (torch.Size([1])) for Parameter 15 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0316 12:02:34.412483 140628633658560 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_1 (torch.Size([524288, 128])) for Parameter 1 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 12:02:34.412515 140538584081600 shampoo_preconditioner_list.py:375] Rank 7: AdaGradPreconditionerList Numel Breakdown: (0, 512, 0, 256, 0, 128, 0, 1024, 0, 1024, 0, 512, 0, 256, 256, 1)
I0316 12:02:34.412559 140538584081600 shampoo_preconditioner_list.py:378] Rank 7: AdaGradPreconditionerList Bytes Breakdown: (0, 2048, 0, 1024, 0, 512, 0, 4096, 0, 4096, 0, 2048, 0, 1024, 1024, 4)
I0316 12:02:34.412598 140538584081600 shampoo_preconditioner_list.py:381] Rank 7: AdaGradPreconditionerList Total Elements: 3969
I0316 12:02:34.412602 140628633658560 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 12:02:34.412634 140538584081600 shampoo_preconditioner_list.py:384] Rank 7: AdaGradPreconditionerList Total Bytes: 15876
I0316 12:02:34.412673 140628633658560 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_1 (torch.Size([0])) for Parameter 2 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 12:02:34.412739 140628633658560 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 12:02:34.412726 140452064818368 submission_runner.py:279] Initializing metrics bundle.
I0316 12:02:34.412800 140628633658560 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_1 (torch.Size([0])) for Parameter 3 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 12:02:34.412856 140628633658560 shampoo_preconditioner_list.py:375] Rank 3: AdaGradPreconditionerList Numel Breakdown: (0, 0, 0, 67108864, 0, 0, 0, 0)
I0316 12:02:34.412912 140628633658560 shampoo_preconditioner_list.py:378] Rank 3: AdaGradPreconditionerList Bytes Breakdown: (0, 0, 0, 268435456, 0, 0, 0, 0)
I0316 12:02:34.412903 140452064818368 submission_runner.py:301] Initializing checkpoint and logger.
I0316 12:02:34.412950 140628633658560 shampoo_preconditioner_list.py:381] Rank 3: AdaGradPreconditionerList Total Elements: 67108864
I0316 12:02:34.412984 140628633658560 shampoo_preconditioner_list.py:384] Rank 3: AdaGradPreconditionerList Total Bytes: 268435456
I0316 12:02:34.412982 140157479363776 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([524288, 128])) for Parameter 2 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 12:02:34.413098 140157479363776 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_1 (torch.Size([0])) for Parameter 2 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 12:02:34.413166 140157479363776 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 12:02:34.413227 140157479363776 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_1 (torch.Size([0])) for Parameter 3 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 12:02:34.413252 140662572377280 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 12:02:34.413282 140157479363776 shampoo_preconditioner_list.py:375] Rank 4: AdaGradPreconditionerList Numel Breakdown: (0, 0, 0, 0, 67108864, 0, 0, 0)
I0316 12:02:34.413318 140157479363776 shampoo_preconditioner_list.py:378] Rank 4: AdaGradPreconditionerList Bytes Breakdown: (0, 0, 0, 0, 268435456, 0, 0, 0)
I0316 12:02:34.413354 140157479363776 shampoo_preconditioner_list.py:381] Rank 4: AdaGradPreconditionerList Total Elements: 67108864
I0316 12:02:34.413364 140662572377280 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_1 (torch.Size([0])) for Parameter 0 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 12:02:34.413360 140452064818368 submission_runner.py:321] Saving meta data to /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_1/criteo1tb_pytorch/trial_4/meta_data_0.json.
I0316 12:02:34.413388 140157479363776 shampoo_preconditioner_list.py:384] Rank 4: AdaGradPreconditionerList Total Bytes: 268435456
I0316 12:02:34.413373 139759074923712 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_1 (torch.Size([524288, 128])) for Parameter 0 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 12:02:34.413449 140662572377280 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 12:02:34.413501 139759074923712 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 12:02:34.413519 140662572377280 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_1 (torch.Size([0])) for Parameter 1 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 12:02:34.413533 140452064818368 logger_utils.py:262] Unable to record workload.train_mean information. Continuing without it.
I0316 12:02:34.413574 140452064818368 logger_utils.py:262] Unable to record workload.train_stddev information. Continuing without it.
I0316 12:02:34.413591 140662572377280 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 12:02:34.413595 139759074923712 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_1 (torch.Size([0])) for Parameter 1 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 12:02:34.413643 140662572377280 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_1 (torch.Size([0])) for Parameter 2 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 12:02:34.413659 139759074923712 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 12:02:34.413710 139759074923712 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_1 (torch.Size([0])) for Parameter 2 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 12:02:34.413777 139759074923712 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 12:02:34.413861 139759074923712 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_1 (torch.Size([0])) for Parameter 3 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 12:02:34.413913 139759074923712 shampoo_preconditioner_list.py:375] Rank 1: AdaGradPreconditionerList Numel Breakdown: (0, 67108864, 0, 0, 0, 0, 0, 0)
I0316 12:02:34.413950 139759074923712 shampoo_preconditioner_list.py:378] Rank 1: AdaGradPreconditionerList Bytes Breakdown: (0, 268435456, 0, 0, 0, 0, 0, 0)
I0316 12:02:34.413983 139759074923712 shampoo_preconditioner_list.py:381] Rank 1: AdaGradPreconditionerList Total Elements: 67108864
I0316 12:02:34.414015 139759074923712 shampoo_preconditioner_list.py:384] Rank 1: AdaGradPreconditionerList Total Bytes: 268435456
I0316 12:02:34.414186 140107298358464 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 12:02:34.414303 140107298358464 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_1 (torch.Size([0])) for Parameter 0 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 12:02:34.414339 140344393745600 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 12:02:34.414453 140344393745600 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_1 (torch.Size([0])) for Parameter 0 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 12:02:34.414526 140344393745600 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 12:02:34.414589 140344393745600 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_1 (torch.Size([0])) for Parameter 1 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 12:02:34.414661 140344393745600 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 12:02:34.414715 140662572377280 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([524288, 128])) for Parameter 3 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 12:02:34.414831 140662572377280 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_1 (torch.Size([0])) for Parameter 3 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 12:02:34.414893 140662572377280 shampoo_preconditioner_list.py:375] Rank 6: AdaGradPreconditionerList Numel Breakdown: (0, 0, 0, 0, 0, 0, 67108864, 0)
I0316 12:02:34.414935 140662572377280 shampoo_preconditioner_list.py:378] Rank 6: AdaGradPreconditionerList Bytes Breakdown: (0, 0, 0, 0, 0, 0, 268435456, 0)
I0316 12:02:34.414972 140662572377280 shampoo_preconditioner_list.py:381] Rank 6: AdaGradPreconditionerList Total Elements: 67108864
I0316 12:02:34.415009 140662572377280 shampoo_preconditioner_list.py:384] Rank 6: AdaGradPreconditionerList Total Bytes: 268435456
I0316 12:02:34.415659 140538584081600 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 12:02:34.415768 140538584081600 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_1 (torch.Size([0])) for Parameter 0 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 12:02:34.415782 140107298358464 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([524288, 128])) for Parameter 1 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 12:02:34.415839 140538584081600 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 12:02:34.415900 140538584081600 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_1 (torch.Size([0])) for Parameter 1 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 12:02:34.415903 140107298358464 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_1 (torch.Size([0])) for Parameter 1 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 12:02:34.415959 140538584081600 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 12:02:34.415963 140107298358464 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 12:02:34.416014 140107298358464 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_1 (torch.Size([0])) for Parameter 2 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 12:02:34.416016 140538584081600 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_1 (torch.Size([0])) for Parameter 2 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 12:02:34.416087 140538584081600 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 12:02:34.416097 140107298358464 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 12:02:34.416091 140344393745600 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_1 (torch.Size([524288, 128])) for Parameter 2 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 12:02:34.416150 140107298358464 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_1 (torch.Size([0])) for Parameter 3 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 12:02:34.416200 140344393745600 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 12:02:34.416220 140107298358464 shampoo_preconditioner_list.py:375] Rank 2: AdaGradPreconditionerList Numel Breakdown: (0, 0, 67108864, 0, 0, 0, 0, 0)
I0316 12:02:34.416255 140107298358464 shampoo_preconditioner_list.py:378] Rank 2: AdaGradPreconditionerList Bytes Breakdown: (0, 0, 268435456, 0, 0, 0, 0, 0)
I0316 12:02:34.416275 140344393745600 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_1 (torch.Size([0])) for Parameter 3 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 12:02:34.416292 140107298358464 shampoo_preconditioner_list.py:381] Rank 2: AdaGradPreconditionerList Total Elements: 67108864
I0316 12:02:34.416322 140107298358464 shampoo_preconditioner_list.py:384] Rank 2: AdaGradPreconditionerList Total Bytes: 268435456
I0316 12:02:34.416335 140344393745600 shampoo_preconditioner_list.py:375] Rank 5: AdaGradPreconditionerList Numel Breakdown: (0, 0, 0, 0, 0, 67108864, 0, 0)
I0316 12:02:34.416370 140344393745600 shampoo_preconditioner_list.py:378] Rank 5: AdaGradPreconditionerList Bytes Breakdown: (0, 0, 0, 0, 0, 268435456, 0, 0)
I0316 12:02:34.416407 140344393745600 shampoo_preconditioner_list.py:381] Rank 5: AdaGradPreconditionerList Total Elements: 67108864
I0316 12:02:34.416444 140344393745600 shampoo_preconditioner_list.py:384] Rank 5: AdaGradPreconditionerList Total Bytes: 268435456
I0316 12:02:34.416460 140628633658560 logger_utils.py:262] Unable to record workload.train_mean information. Continuing without it.
I0316 12:02:34.416535 140628633658560 logger_utils.py:262] Unable to record workload.train_stddev information. Continuing without it.
I0316 12:02:34.416875 140538584081600 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_1 (torch.Size([524288, 128])) for Parameter 3 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 12:02:34.416878 140157479363776 logger_utils.py:262] Unable to record workload.train_mean information. Continuing without it.
I0316 12:02:34.416957 140157479363776 logger_utils.py:262] Unable to record workload.train_stddev information. Continuing without it.
I0316 12:02:34.416987 140538584081600 shampoo_preconditioner_list.py:375] Rank 7: AdaGradPreconditionerList Numel Breakdown: (0, 0, 0, 0, 0, 0, 0, 67108864)
I0316 12:02:34.417032 140538584081600 shampoo_preconditioner_list.py:378] Rank 7: AdaGradPreconditionerList Bytes Breakdown: (0, 0, 0, 0, 0, 0, 0, 268435456)
I0316 12:02:34.417071 140538584081600 shampoo_preconditioner_list.py:381] Rank 7: AdaGradPreconditionerList Total Elements: 67108864
I0316 12:02:34.417108 140538584081600 shampoo_preconditioner_list.py:384] Rank 7: AdaGradPreconditionerList Total Bytes: 268435456
I0316 12:02:34.417335 139759074923712 logger_utils.py:262] Unable to record workload.train_mean information. Continuing without it.
I0316 12:02:34.417415 139759074923712 logger_utils.py:262] Unable to record workload.train_stddev information. Continuing without it.
I0316 12:02:34.418294 140662572377280 logger_utils.py:262] Unable to record workload.train_mean information. Continuing without it.
I0316 12:02:34.418377 140662572377280 logger_utils.py:262] Unable to record workload.train_stddev information. Continuing without it.
I0316 12:02:34.418885 140107298358464 logger_utils.py:262] Unable to record workload.train_mean information. Continuing without it.
I0316 12:02:34.418961 140107298358464 logger_utils.py:262] Unable to record workload.train_stddev information. Continuing without it.
I0316 12:02:34.419083 140344393745600 logger_utils.py:262] Unable to record workload.train_mean information. Continuing without it.
I0316 12:02:34.419158 140344393745600 logger_utils.py:262] Unable to record workload.train_stddev information. Continuing without it.
I0316 12:02:34.419296 140538584081600 logger_utils.py:262] Unable to record workload.train_mean information. Continuing without it.
I0316 12:02:34.419373 140538584081600 logger_utils.py:262] Unable to record workload.train_stddev information. Continuing without it.
I0316 12:02:34.683350 140452064818368 submission_runner.py:325] Saving flags to /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_1/criteo1tb_pytorch/trial_4/flags_0.json.
I0316 12:02:34.743579 140452064818368 submission_runner.py:337] Starting training loop.
I0316 12:02:38.931032 140424115119872 logging_writer.py:48] [0] global_step=0, grad_norm=7.39664, loss=1.55302
I0316 12:02:38.952098 140452064818368 submission.py:265] 0) loss = 1.553, grad_norm = 7.397
I0316 12:02:39.372640 140452064818368 spec.py:321] Evaluating on the training split.
I0316 12:07:56.311740 140452064818368 spec.py:333] Evaluating on the validation split.
I0316 12:12:19.401174 140452064818368 spec.py:349] Evaluating on the test split.
I0316 12:17:16.410415 140452064818368 submission_runner.py:469] Time since start: 881.67s, 	Step: 1, 	{'train/loss': 1.5559687839643987, 'validation/loss': 1.5543951495709674, 'validation/num_examples': 83274637, 'test/loss': 1.5525968260523746, 'test/num_examples': 95000000, 'score': 4.20938777923584, 'total_duration': 881.6670415401459, 'accumulated_submission_time': 4.20938777923584, 'accumulated_eval_time': 877.0379030704498, 'accumulated_logging_time': 0}
I0316 12:17:16.451565 140409862878976 logging_writer.py:48] [1] accumulated_eval_time=877.038, accumulated_logging_time=0, accumulated_submission_time=4.20939, global_step=1, preemption_count=0, score=4.20939, test/loss=1.5526, test/num_examples=95000000, total_duration=881.667, train/loss=1.55597, validation/loss=1.5544, validation/num_examples=83274637
I0316 12:17:17.146512 140409854486272 logging_writer.py:48] [1] global_step=1, grad_norm=7.41284, loss=1.55454
I0316 12:17:17.149898 140452064818368 submission.py:265] 1) loss = 1.555, grad_norm = 7.413
I0316 12:17:17.348293 140409862878976 logging_writer.py:48] [2] global_step=2, grad_norm=7.33147, loss=1.53058
I0316 12:17:17.351569 140452064818368 submission.py:265] 2) loss = 1.531, grad_norm = 7.331
I0316 12:17:17.547692 140409854486272 logging_writer.py:48] [3] global_step=3, grad_norm=7.19402, loss=1.48916
I0316 12:17:17.550623 140452064818368 submission.py:265] 3) loss = 1.489, grad_norm = 7.194
I0316 12:17:17.745764 140409862878976 logging_writer.py:48] [4] global_step=4, grad_norm=6.95675, loss=1.42992
I0316 12:17:17.748475 140452064818368 submission.py:265] 4) loss = 1.430, grad_norm = 6.957
I0316 12:17:17.943749 140409854486272 logging_writer.py:48] [5] global_step=5, grad_norm=6.45561, loss=1.36064
I0316 12:17:17.946386 140452064818368 submission.py:265] 5) loss = 1.361, grad_norm = 6.456
I0316 12:17:18.148594 140409862878976 logging_writer.py:48] [6] global_step=6, grad_norm=6.13595, loss=1.28449
I0316 12:17:18.151922 140452064818368 submission.py:265] 6) loss = 1.284, grad_norm = 6.136
I0316 12:17:18.350773 140409854486272 logging_writer.py:48] [7] global_step=7, grad_norm=6.04942, loss=1.20056
I0316 12:17:18.353868 140452064818368 submission.py:265] 7) loss = 1.201, grad_norm = 6.049
I0316 12:17:18.552823 140409862878976 logging_writer.py:48] [8] global_step=8, grad_norm=5.94977, loss=1.11249
I0316 12:17:18.555808 140452064818368 submission.py:265] 8) loss = 1.112, grad_norm = 5.950
I0316 12:17:18.751729 140409854486272 logging_writer.py:48] [9] global_step=9, grad_norm=5.84784, loss=1.01833
I0316 12:17:18.754688 140452064818368 submission.py:265] 9) loss = 1.018, grad_norm = 5.848
I0316 12:17:18.951841 140409862878976 logging_writer.py:48] [10] global_step=10, grad_norm=5.7659, loss=0.920521
I0316 12:17:18.954825 140452064818368 submission.py:265] 10) loss = 0.921, grad_norm = 5.766
I0316 12:17:19.157438 140409854486272 logging_writer.py:48] [11] global_step=11, grad_norm=5.64756, loss=0.819391
I0316 12:17:19.160353 140452064818368 submission.py:265] 11) loss = 0.819, grad_norm = 5.648
I0316 12:17:19.357463 140409862878976 logging_writer.py:48] [12] global_step=12, grad_norm=5.4511, loss=0.713447
I0316 12:17:19.360370 140452064818368 submission.py:265] 12) loss = 0.713, grad_norm = 5.451
I0316 12:17:19.555136 140409854486272 logging_writer.py:48] [13] global_step=13, grad_norm=5.06203, loss=0.610155
I0316 12:17:19.559842 140452064818368 submission.py:265] 13) loss = 0.610, grad_norm = 5.062
I0316 12:17:19.758704 140409862878976 logging_writer.py:48] [14] global_step=14, grad_norm=4.57243, loss=0.51388
I0316 12:17:19.762455 140452064818368 submission.py:265] 14) loss = 0.514, grad_norm = 4.572
I0316 12:17:19.961305 140409854486272 logging_writer.py:48] [15] global_step=15, grad_norm=4.01196, loss=0.425747
I0316 12:17:19.965132 140452064818368 submission.py:265] 15) loss = 0.426, grad_norm = 4.012
I0316 12:17:20.162771 140409862878976 logging_writer.py:48] [16] global_step=16, grad_norm=3.27409, loss=0.347502
I0316 12:17:20.166171 140452064818368 submission.py:265] 16) loss = 0.348, grad_norm = 3.274
I0316 12:17:20.369242 140409854486272 logging_writer.py:48] [17] global_step=17, grad_norm=2.44014, loss=0.284207
I0316 12:17:20.372359 140452064818368 submission.py:265] 17) loss = 0.284, grad_norm = 2.440
I0316 12:17:20.568108 140409862878976 logging_writer.py:48] [18] global_step=18, grad_norm=1.57593, loss=0.237888
I0316 12:17:20.571013 140452064818368 submission.py:265] 18) loss = 0.238, grad_norm = 1.576
I0316 12:17:20.766375 140409854486272 logging_writer.py:48] [19] global_step=19, grad_norm=0.918626, loss=0.20547
I0316 12:17:20.769320 140452064818368 submission.py:265] 19) loss = 0.205, grad_norm = 0.919
I0316 12:17:20.966129 140409862878976 logging_writer.py:48] [20] global_step=20, grad_norm=0.31903, loss=0.187542
I0316 12:17:20.969778 140452064818368 submission.py:265] 20) loss = 0.188, grad_norm = 0.319
I0316 12:17:21.163681 140409854486272 logging_writer.py:48] [21] global_step=21, grad_norm=0.341039, loss=0.1867
I0316 12:17:21.166879 140452064818368 submission.py:265] 21) loss = 0.187, grad_norm = 0.341
I0316 12:17:21.360720 140409862878976 logging_writer.py:48] [22] global_step=22, grad_norm=0.745835, loss=0.198674
I0316 12:17:21.363772 140452064818368 submission.py:265] 22) loss = 0.199, grad_norm = 0.746
I0316 12:17:21.559427 140409854486272 logging_writer.py:48] [23] global_step=23, grad_norm=1.07384, loss=0.216412
I0316 12:17:21.562509 140452064818368 submission.py:265] 23) loss = 0.216, grad_norm = 1.074
I0316 12:17:21.756421 140409862878976 logging_writer.py:48] [24] global_step=24, grad_norm=1.38536, loss=0.243566
I0316 12:17:21.759728 140452064818368 submission.py:265] 24) loss = 0.244, grad_norm = 1.385
I0316 12:17:21.956808 140409854486272 logging_writer.py:48] [25] global_step=25, grad_norm=1.63372, loss=0.271248
I0316 12:17:21.960058 140452064818368 submission.py:265] 25) loss = 0.271, grad_norm = 1.634
I0316 12:17:22.155140 140409862878976 logging_writer.py:48] [26] global_step=26, grad_norm=1.83785, loss=0.298168
I0316 12:17:22.158217 140452064818368 submission.py:265] 26) loss = 0.298, grad_norm = 1.838
I0316 12:17:22.350944 140409854486272 logging_writer.py:48] [27] global_step=27, grad_norm=2.0493, loss=0.329183
I0316 12:17:22.354047 140452064818368 submission.py:265] 27) loss = 0.329, grad_norm = 2.049
I0316 12:17:22.549198 140409862878976 logging_writer.py:48] [28] global_step=28, grad_norm=2.2423, loss=0.360441
I0316 12:17:22.552627 140452064818368 submission.py:265] 28) loss = 0.360, grad_norm = 2.242
I0316 12:17:22.748292 140409854486272 logging_writer.py:48] [29] global_step=29, grad_norm=2.46625, loss=0.398028
I0316 12:17:22.751432 140452064818368 submission.py:265] 29) loss = 0.398, grad_norm = 2.466
I0316 12:17:22.945598 140409862878976 logging_writer.py:48] [30] global_step=30, grad_norm=2.59494, loss=0.421646
I0316 12:17:22.948664 140452064818368 submission.py:265] 30) loss = 0.422, grad_norm = 2.595
I0316 12:17:24.355772 140409854486272 logging_writer.py:48] [31] global_step=31, grad_norm=2.73899, loss=0.448267
I0316 12:17:24.359596 140452064818368 submission.py:265] 31) loss = 0.448, grad_norm = 2.739
I0316 12:17:24.916874 140409862878976 logging_writer.py:48] [32] global_step=32, grad_norm=2.75129, loss=0.453453
I0316 12:17:24.920196 140452064818368 submission.py:265] 32) loss = 0.453, grad_norm = 2.751
I0316 12:17:26.459868 140409854486272 logging_writer.py:48] [33] global_step=33, grad_norm=2.89574, loss=0.480167
I0316 12:17:26.462908 140452064818368 submission.py:265] 33) loss = 0.480, grad_norm = 2.896
I0316 12:17:27.571259 140409862878976 logging_writer.py:48] [34] global_step=34, grad_norm=2.95585, loss=0.49031
I0316 12:17:27.574298 140452064818368 submission.py:265] 34) loss = 0.490, grad_norm = 2.956
I0316 12:17:28.719093 140409854486272 logging_writer.py:48] [35] global_step=35, grad_norm=2.92559, loss=0.48769
I0316 12:17:28.722075 140452064818368 submission.py:265] 35) loss = 0.488, grad_norm = 2.926
I0316 12:17:29.945934 140409862878976 logging_writer.py:48] [36] global_step=36, grad_norm=2.97724, loss=0.496268
I0316 12:17:29.948994 140452064818368 submission.py:265] 36) loss = 0.496, grad_norm = 2.977
I0316 12:17:31.243458 140409854486272 logging_writer.py:48] [37] global_step=37, grad_norm=2.94789, loss=0.49074
I0316 12:17:31.247005 140452064818368 submission.py:265] 37) loss = 0.491, grad_norm = 2.948
I0316 12:17:33.022630 140409862878976 logging_writer.py:48] [38] global_step=38, grad_norm=3.10217, loss=0.513487
I0316 12:17:33.025725 140452064818368 submission.py:265] 38) loss = 0.513, grad_norm = 3.102
I0316 12:17:33.743270 140409854486272 logging_writer.py:48] [39] global_step=39, grad_norm=3.10611, loss=0.511568
I0316 12:17:33.746449 140452064818368 submission.py:265] 39) loss = 0.512, grad_norm = 3.106
I0316 12:17:35.542297 140409862878976 logging_writer.py:48] [40] global_step=40, grad_norm=3.04409, loss=0.499647
I0316 12:17:35.545417 140452064818368 submission.py:265] 40) loss = 0.500, grad_norm = 3.044
I0316 12:17:36.070590 140409854486272 logging_writer.py:48] [41] global_step=41, grad_norm=2.89929, loss=0.47292
I0316 12:17:36.074186 140452064818368 submission.py:265] 41) loss = 0.473, grad_norm = 2.899
I0316 12:17:38.042561 140409862878976 logging_writer.py:48] [42] global_step=42, grad_norm=2.80095, loss=0.453904
I0316 12:17:38.046074 140452064818368 submission.py:265] 42) loss = 0.454, grad_norm = 2.801
I0316 12:17:39.142736 140409854486272 logging_writer.py:48] [43] global_step=43, grad_norm=2.60126, loss=0.417299
I0316 12:17:39.146369 140452064818368 submission.py:265] 43) loss = 0.417, grad_norm = 2.601
I0316 12:17:40.880017 140409862878976 logging_writer.py:48] [44] global_step=44, grad_norm=2.46912, loss=0.393734
I0316 12:17:40.883540 140452064818368 submission.py:265] 44) loss = 0.394, grad_norm = 2.469
I0316 12:17:42.029363 140409854486272 logging_writer.py:48] [45] global_step=45, grad_norm=2.32274, loss=0.367033
I0316 12:17:42.033198 140452064818368 submission.py:265] 45) loss = 0.367, grad_norm = 2.323
I0316 12:17:43.815991 140409862878976 logging_writer.py:48] [46] global_step=46, grad_norm=2.17763, loss=0.342553
I0316 12:17:43.819650 140452064818368 submission.py:265] 46) loss = 0.343, grad_norm = 2.178
I0316 12:17:44.417832 140409854486272 logging_writer.py:48] [47] global_step=47, grad_norm=1.94648, loss=0.305465
I0316 12:17:44.421314 140452064818368 submission.py:265] 47) loss = 0.305, grad_norm = 1.946
I0316 12:17:46.288614 140409862878976 logging_writer.py:48] [48] global_step=48, grad_norm=1.79372, loss=0.28208
I0316 12:17:46.291838 140452064818368 submission.py:265] 48) loss = 0.282, grad_norm = 1.794
I0316 12:17:47.256412 140409854486272 logging_writer.py:48] [49] global_step=49, grad_norm=1.60144, loss=0.25552
I0316 12:17:47.259951 140452064818368 submission.py:265] 49) loss = 0.256, grad_norm = 1.601
I0316 12:17:48.992868 140409862878976 logging_writer.py:48] [50] global_step=50, grad_norm=1.41849, loss=0.23381
I0316 12:17:48.996357 140452064818368 submission.py:265] 50) loss = 0.234, grad_norm = 1.418
I0316 12:17:49.758544 140409854486272 logging_writer.py:48] [51] global_step=51, grad_norm=1.16099, loss=0.207526
I0316 12:17:49.762137 140452064818368 submission.py:265] 51) loss = 0.208, grad_norm = 1.161
I0316 12:17:51.502364 140409862878976 logging_writer.py:48] [52] global_step=52, grad_norm=0.930972, loss=0.189137
I0316 12:17:51.505893 140452064818368 submission.py:265] 52) loss = 0.189, grad_norm = 0.931
I0316 12:17:52.083526 140409854486272 logging_writer.py:48] [53] global_step=53, grad_norm=0.675559, loss=0.17544
I0316 12:17:52.087032 140452064818368 submission.py:265] 53) loss = 0.175, grad_norm = 0.676
I0316 12:17:53.684527 140409862878976 logging_writer.py:48] [54] global_step=54, grad_norm=0.345091, loss=0.161546
I0316 12:17:53.688006 140452064818368 submission.py:265] 54) loss = 0.162, grad_norm = 0.345
I0316 12:17:54.552309 140409854486272 logging_writer.py:48] [55] global_step=55, grad_norm=0.151705, loss=0.16109
I0316 12:17:54.555482 140452064818368 submission.py:265] 55) loss = 0.161, grad_norm = 0.152
I0316 12:17:56.055129 140409862878976 logging_writer.py:48] [56] global_step=56, grad_norm=0.392493, loss=0.163801
I0316 12:17:56.058303 140452064818368 submission.py:265] 56) loss = 0.164, grad_norm = 0.392
I0316 12:17:56.872992 140409854486272 logging_writer.py:48] [57] global_step=57, grad_norm=0.805408, loss=0.172484
I0316 12:17:56.876270 140452064818368 submission.py:265] 57) loss = 0.172, grad_norm = 0.805
I0316 12:17:58.719953 140409862878976 logging_writer.py:48] [58] global_step=58, grad_norm=1.13669, loss=0.180915
I0316 12:17:58.723201 140452064818368 submission.py:265] 58) loss = 0.181, grad_norm = 1.137
I0316 12:17:59.561211 140409854486272 logging_writer.py:48] [59] global_step=59, grad_norm=1.35325, loss=0.190079
I0316 12:17:59.564633 140452064818368 submission.py:265] 59) loss = 0.190, grad_norm = 1.353
I0316 12:18:01.649284 140409862878976 logging_writer.py:48] [60] global_step=60, grad_norm=1.49183, loss=0.196446
I0316 12:18:01.652560 140452064818368 submission.py:265] 60) loss = 0.196, grad_norm = 1.492
I0316 12:18:02.276195 140409854486272 logging_writer.py:48] [61] global_step=61, grad_norm=1.54112, loss=0.199392
I0316 12:18:02.279886 140452064818368 submission.py:265] 61) loss = 0.199, grad_norm = 1.541
I0316 12:18:03.948770 140409862878976 logging_writer.py:48] [62] global_step=62, grad_norm=1.49708, loss=0.196731
I0316 12:18:03.952314 140452064818368 submission.py:265] 62) loss = 0.197, grad_norm = 1.497
I0316 12:18:04.862421 140409854486272 logging_writer.py:48] [63] global_step=63, grad_norm=1.39323, loss=0.188493
I0316 12:18:04.866222 140452064818368 submission.py:265] 63) loss = 0.188, grad_norm = 1.393
I0316 12:18:06.117672 140409862878976 logging_writer.py:48] [64] global_step=64, grad_norm=1.22023, loss=0.178401
I0316 12:18:06.121423 140452064818368 submission.py:265] 64) loss = 0.178, grad_norm = 1.220
I0316 12:18:06.922438 140409854486272 logging_writer.py:48] [65] global_step=65, grad_norm=1.00161, loss=0.167464
I0316 12:18:06.925954 140452064818368 submission.py:265] 65) loss = 0.167, grad_norm = 1.002
I0316 12:18:08.290965 140409862878976 logging_writer.py:48] [66] global_step=66, grad_norm=0.760813, loss=0.156023
I0316 12:18:08.294758 140452064818368 submission.py:265] 66) loss = 0.156, grad_norm = 0.761
I0316 12:18:09.231012 140409854486272 logging_writer.py:48] [67] global_step=67, grad_norm=0.496307, loss=0.148231
I0316 12:18:09.234889 140452064818368 submission.py:265] 67) loss = 0.148, grad_norm = 0.496
I0316 12:18:10.296707 140409862878976 logging_writer.py:48] [68] global_step=68, grad_norm=0.244778, loss=0.142968
I0316 12:18:10.300312 140452064818368 submission.py:265] 68) loss = 0.143, grad_norm = 0.245
I0316 12:18:11.368407 140409854486272 logging_writer.py:48] [69] global_step=69, grad_norm=0.0509217, loss=0.139767
I0316 12:18:11.372167 140452064818368 submission.py:265] 69) loss = 0.140, grad_norm = 0.051
I0316 12:18:12.773381 140409862878976 logging_writer.py:48] [70] global_step=70, grad_norm=0.202692, loss=0.140986
I0316 12:18:12.777002 140452064818368 submission.py:265] 70) loss = 0.141, grad_norm = 0.203
I0316 12:18:13.810883 140409854486272 logging_writer.py:48] [71] global_step=71, grad_norm=0.367514, loss=0.145422
I0316 12:18:13.814878 140452064818368 submission.py:265] 71) loss = 0.145, grad_norm = 0.368
I0316 12:18:15.337065 140409862878976 logging_writer.py:48] [72] global_step=72, grad_norm=0.496995, loss=0.150698
I0316 12:18:15.340575 140452064818368 submission.py:265] 72) loss = 0.151, grad_norm = 0.497
I0316 12:18:16.085238 140409854486272 logging_writer.py:48] [73] global_step=73, grad_norm=0.598691, loss=0.157336
I0316 12:18:16.088821 140452064818368 submission.py:265] 73) loss = 0.157, grad_norm = 0.599
I0316 12:18:17.369223 140409862878976 logging_writer.py:48] [74] global_step=74, grad_norm=0.66273, loss=0.162685
I0316 12:18:17.372827 140452064818368 submission.py:265] 74) loss = 0.163, grad_norm = 0.663
I0316 12:18:18.366125 140409854486272 logging_writer.py:48] [75] global_step=75, grad_norm=0.704933, loss=0.166259
I0316 12:18:18.369642 140452064818368 submission.py:265] 75) loss = 0.166, grad_norm = 0.705
I0316 12:18:19.653728 140409862878976 logging_writer.py:48] [76] global_step=76, grad_norm=0.761106, loss=0.172211
I0316 12:18:19.657318 140452064818368 submission.py:265] 76) loss = 0.172, grad_norm = 0.761
I0316 12:18:20.684879 140409854486272 logging_writer.py:48] [77] global_step=77, grad_norm=0.778701, loss=0.175146
I0316 12:18:20.688440 140452064818368 submission.py:265] 77) loss = 0.175, grad_norm = 0.779
I0316 12:18:22.374752 140409862878976 logging_writer.py:48] [78] global_step=78, grad_norm=0.723284, loss=0.167838
I0316 12:18:22.378429 140452064818368 submission.py:265] 78) loss = 0.168, grad_norm = 0.723
I0316 12:18:23.363199 140409854486272 logging_writer.py:48] [79] global_step=79, grad_norm=0.696748, loss=0.16573
I0316 12:18:23.366887 140452064818368 submission.py:265] 79) loss = 0.166, grad_norm = 0.697
I0316 12:18:24.950775 140409862878976 logging_writer.py:48] [80] global_step=80, grad_norm=0.657119, loss=0.164047
I0316 12:18:24.954460 140452064818368 submission.py:265] 80) loss = 0.164, grad_norm = 0.657
I0316 12:18:25.981287 140409854486272 logging_writer.py:48] [81] global_step=81, grad_norm=0.582933, loss=0.156837
I0316 12:18:25.984395 140452064818368 submission.py:265] 81) loss = 0.157, grad_norm = 0.583
I0316 12:18:27.649503 140409862878976 logging_writer.py:48] [82] global_step=82, grad_norm=0.525445, loss=0.15609
I0316 12:18:27.652450 140452064818368 submission.py:265] 82) loss = 0.156, grad_norm = 0.525
I0316 12:18:28.551177 140409854486272 logging_writer.py:48] [83] global_step=83, grad_norm=0.411782, loss=0.148045
I0316 12:18:28.554141 140452064818368 submission.py:265] 83) loss = 0.148, grad_norm = 0.412
I0316 12:18:30.072551 140409862878976 logging_writer.py:48] [84] global_step=84, grad_norm=0.324076, loss=0.147272
I0316 12:18:30.075919 140452064818368 submission.py:265] 84) loss = 0.147, grad_norm = 0.324
I0316 12:18:31.017941 140409854486272 logging_writer.py:48] [85] global_step=85, grad_norm=0.190204, loss=0.142646
I0316 12:18:31.021200 140452064818368 submission.py:265] 85) loss = 0.143, grad_norm = 0.190
I0316 12:18:33.125105 140409862878976 logging_writer.py:48] [86] global_step=86, grad_norm=0.0668284, loss=0.141698
I0316 12:18:33.128661 140452064818368 submission.py:265] 86) loss = 0.142, grad_norm = 0.067
I0316 12:18:34.162411 140409854486272 logging_writer.py:48] [87] global_step=87, grad_norm=0.105419, loss=0.140708
I0316 12:18:34.165597 140452064818368 submission.py:265] 87) loss = 0.141, grad_norm = 0.105
I0316 12:18:36.019844 140409862878976 logging_writer.py:48] [88] global_step=88, grad_norm=0.235126, loss=0.14329
I0316 12:18:36.022943 140452064818368 submission.py:265] 88) loss = 0.143, grad_norm = 0.235
I0316 12:18:37.019361 140409854486272 logging_writer.py:48] [89] global_step=89, grad_norm=0.366264, loss=0.145994
I0316 12:18:37.022581 140452064818368 submission.py:265] 89) loss = 0.146, grad_norm = 0.366
I0316 12:18:38.640823 140409862878976 logging_writer.py:48] [90] global_step=90, grad_norm=0.48016, loss=0.148725
I0316 12:18:38.643788 140452064818368 submission.py:265] 90) loss = 0.149, grad_norm = 0.480
I0316 12:18:39.330406 140409854486272 logging_writer.py:48] [91] global_step=91, grad_norm=0.559458, loss=0.151907
I0316 12:18:39.333854 140452064818368 submission.py:265] 91) loss = 0.152, grad_norm = 0.559
I0316 12:18:41.258371 140409862878976 logging_writer.py:48] [92] global_step=92, grad_norm=0.599805, loss=0.155114
I0316 12:18:41.261530 140452064818368 submission.py:265] 92) loss = 0.155, grad_norm = 0.600
I0316 12:18:42.289357 140409854486272 logging_writer.py:48] [93] global_step=93, grad_norm=0.627522, loss=0.154832
I0316 12:18:42.292472 140452064818368 submission.py:265] 93) loss = 0.155, grad_norm = 0.628
I0316 12:18:44.231510 140409862878976 logging_writer.py:48] [94] global_step=94, grad_norm=0.604675, loss=0.155452
I0316 12:18:44.234807 140452064818368 submission.py:265] 94) loss = 0.155, grad_norm = 0.605
I0316 12:18:44.658652 140409854486272 logging_writer.py:48] [95] global_step=95, grad_norm=0.545145, loss=0.150089
I0316 12:18:44.662006 140452064818368 submission.py:265] 95) loss = 0.150, grad_norm = 0.545
I0316 12:18:46.302499 140409862878976 logging_writer.py:48] [96] global_step=96, grad_norm=0.483901, loss=0.144073
I0316 12:18:46.305599 140452064818368 submission.py:265] 96) loss = 0.144, grad_norm = 0.484
I0316 12:18:47.417294 140409854486272 logging_writer.py:48] [97] global_step=97, grad_norm=0.395833, loss=0.142289
I0316 12:18:47.420356 140452064818368 submission.py:265] 97) loss = 0.142, grad_norm = 0.396
I0316 12:18:48.594884 140409862878976 logging_writer.py:48] [98] global_step=98, grad_norm=0.293543, loss=0.141479
I0316 12:18:48.598101 140452064818368 submission.py:265] 98) loss = 0.141, grad_norm = 0.294
I0316 12:18:50.052628 140409854486272 logging_writer.py:48] [99] global_step=99, grad_norm=0.207806, loss=0.137913
I0316 12:18:50.055861 140452064818368 submission.py:265] 99) loss = 0.138, grad_norm = 0.208
I0316 12:18:50.929301 140409862878976 logging_writer.py:48] [100] global_step=100, grad_norm=0.168528, loss=0.138004
I0316 12:18:50.932371 140452064818368 submission.py:265] 100) loss = 0.138, grad_norm = 0.169
I0316 12:19:17.523601 140452064818368 spec.py:321] Evaluating on the training split.
I0316 12:24:40.426568 140452064818368 spec.py:333] Evaluating on the validation split.
I0316 12:29:07.139626 140452064818368 spec.py:349] Evaluating on the test split.
I0316 12:34:08.707339 140452064818368 submission_runner.py:469] Time since start: 1893.96s, 	Step: 123, 	{'train/loss': 0.13559364615543795, 'validation/loss': 0.1373852469463777, 'validation/num_examples': 83274637, 'test/loss': 0.14072986464502435, 'test/num_examples': 95000000, 'score': 124.32689070701599, 'total_duration': 1893.9639415740967, 'accumulated_submission_time': 124.32689070701599, 'accumulated_eval_time': 1768.2217803001404, 'accumulated_logging_time': 0.0699472427368164}
I0316 12:34:08.717220 140409854486272 logging_writer.py:48] [123] accumulated_eval_time=1768.22, accumulated_logging_time=0.0699472, accumulated_submission_time=124.327, global_step=123, preemption_count=0, score=124.327, test/loss=0.14073, test/num_examples=95000000, total_duration=1893.96, train/loss=0.135594, validation/loss=0.137385, validation/num_examples=83274637
/usr/local/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
I0316 12:36:09.372128 140452064818368 spec.py:321] Evaluating on the training split.
I0316 12:41:33.769750 140452064818368 spec.py:333] Evaluating on the validation split.
I0316 12:46:14.201630 140452064818368 spec.py:349] Evaluating on the test split.
I0316 12:51:23.062955 140452064818368 submission_runner.py:469] Time since start: 2928.32s, 	Step: 243, 	{'train/loss': 0.12789749141411313, 'validation/loss': 0.12938619681209904, 'validation/num_examples': 83274637, 'test/loss': 0.13233890309673108, 'test/num_examples': 95000000, 'score': 244.0833420753479, 'total_duration': 2928.3195593357086, 'accumulated_submission_time': 244.0833420753479, 'accumulated_eval_time': 2681.912724018097, 'accumulated_logging_time': 0.08669519424438477}
I0316 12:51:23.073576 140409862878976 logging_writer.py:48] [243] accumulated_eval_time=2681.91, accumulated_logging_time=0.0866952, accumulated_submission_time=244.083, global_step=243, preemption_count=0, score=244.083, test/loss=0.132339, test/num_examples=95000000, total_duration=2928.32, train/loss=0.127897, validation/loss=0.129386, validation/num_examples=83274637
I0316 12:53:24.247354 140452064818368 spec.py:321] Evaluating on the training split.
I0316 12:58:49.696994 140452064818368 spec.py:333] Evaluating on the validation split.
I0316 13:03:17.389711 140452064818368 spec.py:349] Evaluating on the test split.
I0316 13:08:19.328903 140452064818368 submission_runner.py:469] Time since start: 3944.59s, 	Step: 363, 	{'train/loss': 0.12774561294508405, 'validation/loss': 0.12791970920740173, 'validation/num_examples': 83274637, 'test/loss': 0.13051705193533647, 'test/num_examples': 95000000, 'score': 364.39417147636414, 'total_duration': 3944.585338830948, 'accumulated_submission_time': 364.39417147636414, 'accumulated_eval_time': 3576.994199991226, 'accumulated_logging_time': 0.10437130928039551}
I0316 13:08:19.338926 140409854486272 logging_writer.py:48] [363] accumulated_eval_time=3576.99, accumulated_logging_time=0.104371, accumulated_submission_time=364.394, global_step=363, preemption_count=0, score=364.394, test/loss=0.130517, test/num_examples=95000000, total_duration=3944.59, train/loss=0.127746, validation/loss=0.12792, validation/num_examples=83274637
I0316 13:10:21.023686 140452064818368 spec.py:321] Evaluating on the training split.
I0316 13:15:38.634148 140452064818368 spec.py:333] Evaluating on the validation split.
I0316 13:20:05.214817 140452064818368 spec.py:349] Evaluating on the test split.
I0316 13:25:05.611500 140452064818368 submission_runner.py:469] Time since start: 4950.87s, 	Step: 487, 	{'train/loss': 0.125664055511244, 'validation/loss': 0.12727701761173288, 'validation/num_examples': 83274637, 'test/loss': 0.1297727832484195, 'test/num_examples': 95000000, 'score': 485.18765592575073, 'total_duration': 4950.868125915527, 'accumulated_submission_time': 485.18765592575073, 'accumulated_eval_time': 4461.582173585892, 'accumulated_logging_time': 0.12080049514770508}
I0316 13:25:05.621746 140409862878976 logging_writer.py:48] [487] accumulated_eval_time=4461.58, accumulated_logging_time=0.1208, accumulated_submission_time=485.188, global_step=487, preemption_count=0, score=485.188, test/loss=0.129773, test/num_examples=95000000, total_duration=4950.87, train/loss=0.125664, validation/loss=0.127277, validation/num_examples=83274637
I0316 13:25:08.826264 140409854486272 logging_writer.py:48] [500] global_step=500, grad_norm=0.0299552, loss=0.123884
I0316 13:25:08.830232 140452064818368 submission.py:265] 500) loss = 0.124, grad_norm = 0.030
I0316 13:27:06.849543 140452064818368 spec.py:321] Evaluating on the training split.
I0316 13:32:12.302341 140452064818368 spec.py:333] Evaluating on the validation split.
I0316 13:36:39.640475 140452064818368 spec.py:349] Evaluating on the test split.
I0316 13:41:32.875214 140452064818368 submission_runner.py:469] Time since start: 5938.13s, 	Step: 611, 	{'train/loss': 0.12389275970780453, 'validation/loss': 0.12656235711494032, 'validation/num_examples': 83274637, 'test/loss': 0.12906700013198852, 'test/num_examples': 95000000, 'score': 605.5009610652924, 'total_duration': 5938.131875276566, 'accumulated_submission_time': 605.5009610652924, 'accumulated_eval_time': 5327.607988834381, 'accumulated_logging_time': 0.17924237251281738}
I0316 13:41:32.885177 140409862878976 logging_writer.py:48] [611] accumulated_eval_time=5327.61, accumulated_logging_time=0.179242, accumulated_submission_time=605.501, global_step=611, preemption_count=0, score=605.501, test/loss=0.129067, test/num_examples=95000000, total_duration=5938.13, train/loss=0.123893, validation/loss=0.126562, validation/num_examples=83274637
I0316 13:43:33.947397 140452064818368 spec.py:321] Evaluating on the training split.
I0316 13:48:56.178468 140452064818368 spec.py:333] Evaluating on the validation split.
I0316 13:53:23.192816 140452064818368 spec.py:349] Evaluating on the test split.
I0316 13:58:24.702462 140452064818368 submission_runner.py:469] Time since start: 6949.96s, 	Step: 736, 	{'train/loss': 0.1250022627300957, 'validation/loss': 0.12647404857221686, 'validation/num_examples': 83274637, 'test/loss': 0.12886777501521862, 'test/num_examples': 95000000, 'score': 725.7015452384949, 'total_duration': 6949.959118127823, 'accumulated_submission_time': 725.7015452384949, 'accumulated_eval_time': 6218.363099813461, 'accumulated_logging_time': 0.19576168060302734}
I0316 13:58:24.712081 140409854486272 logging_writer.py:48] [736] accumulated_eval_time=6218.36, accumulated_logging_time=0.195762, accumulated_submission_time=725.702, global_step=736, preemption_count=0, score=725.702, test/loss=0.128868, test/num_examples=95000000, total_duration=6949.96, train/loss=0.125002, validation/loss=0.126474, validation/num_examples=83274637
I0316 14:00:25.835958 140452064818368 spec.py:321] Evaluating on the training split.
I0316 14:05:39.748027 140452064818368 spec.py:333] Evaluating on the validation split.
I0316 14:10:04.401587 140452064818368 spec.py:349] Evaluating on the test split.
I0316 14:14:57.445381 140452064818368 submission_runner.py:469] Time since start: 7942.70s, 	Step: 858, 	{'train/loss': 0.1242563677986964, 'validation/loss': 0.12606761503658345, 'validation/num_examples': 83274637, 'test/loss': 0.12851110228110865, 'test/num_examples': 95000000, 'score': 845.9896948337555, 'total_duration': 7942.702048063278, 'accumulated_submission_time': 845.9896948337555, 'accumulated_eval_time': 7089.972631692886, 'accumulated_logging_time': 0.21303224563598633}
I0316 14:14:57.454846 140409862878976 logging_writer.py:48] [858] accumulated_eval_time=7089.97, accumulated_logging_time=0.213032, accumulated_submission_time=845.99, global_step=858, preemption_count=0, score=845.99, test/loss=0.128511, test/num_examples=95000000, total_duration=7942.7, train/loss=0.124256, validation/loss=0.126068, validation/num_examples=83274637
I0316 14:16:58.175240 140452064818368 spec.py:321] Evaluating on the training split.
I0316 14:21:58.688875 140452064818368 spec.py:333] Evaluating on the validation split.
I0316 14:26:09.654335 140452064818368 spec.py:349] Evaluating on the test split.
I0316 14:30:55.594778 140452064818368 submission_runner.py:469] Time since start: 8900.85s, 	Step: 980, 	{'train/loss': 0.12546846859210953, 'validation/loss': 0.1259920610072411, 'validation/num_examples': 83274637, 'test/loss': 0.12834299912908453, 'test/num_examples': 95000000, 'score': 965.8628640174866, 'total_duration': 8900.851387739182, 'accumulated_submission_time': 965.8628640174866, 'accumulated_eval_time': 7927.3923354148865, 'accumulated_logging_time': 0.22918033599853516}
I0316 14:30:55.604302 140409854486272 logging_writer.py:48] [980] accumulated_eval_time=7927.39, accumulated_logging_time=0.22918, accumulated_submission_time=965.863, global_step=980, preemption_count=0, score=965.863, test/loss=0.128343, test/num_examples=95000000, total_duration=8900.85, train/loss=0.125468, validation/loss=0.125992, validation/num_examples=83274637
I0316 14:31:00.178955 140409862878976 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.0140794, loss=0.132189
I0316 14:31:00.182673 140452064818368 submission.py:265] 1000) loss = 0.132, grad_norm = 0.014
I0316 14:32:56.751232 140452064818368 spec.py:321] Evaluating on the training split.
I0316 14:37:41.927340 140452064818368 spec.py:333] Evaluating on the validation split.
I0316 14:42:16.483525 140452064818368 spec.py:349] Evaluating on the test split.
I0316 14:47:19.420486 140452064818368 submission_runner.py:469] Time since start: 9884.68s, 	Step: 1101, 	{'train/loss': 0.12481992780792518, 'validation/loss': 0.12591176061229148, 'validation/num_examples': 83274637, 'test/loss': 0.12831861611436543, 'test/num_examples': 95000000, 'score': 1086.090036869049, 'total_duration': 9884.677122831345, 'accumulated_submission_time': 1086.090036869049, 'accumulated_eval_time': 8790.061691522598, 'accumulated_logging_time': 0.24562978744506836}
I0316 14:47:19.430668 140409854486272 logging_writer.py:48] [1101] accumulated_eval_time=8790.06, accumulated_logging_time=0.24563, accumulated_submission_time=1086.09, global_step=1101, preemption_count=0, score=1086.09, test/loss=0.128319, test/num_examples=95000000, total_duration=9884.68, train/loss=0.12482, validation/loss=0.125912, validation/num_examples=83274637
I0316 14:49:20.529972 140452064818368 spec.py:321] Evaluating on the training split.
I0316 14:53:43.282106 140452064818368 spec.py:333] Evaluating on the validation split.
I0316 14:58:14.966895 140452064818368 spec.py:349] Evaluating on the test split.
I0316 15:03:04.405304 140452064818368 submission_runner.py:469] Time since start: 10829.66s, 	Step: 1223, 	{'train/loss': 0.12416131637041196, 'validation/loss': 0.1258127702817028, 'validation/num_examples': 83274637, 'test/loss': 0.12815021689762315, 'test/num_examples': 95000000, 'score': 1206.2974948883057, 'total_duration': 10829.661939620972, 'accumulated_submission_time': 1206.2974948883057, 'accumulated_eval_time': 9613.93716931343, 'accumulated_logging_time': 0.2991206645965576}
I0316 15:03:04.416370 140409862878976 logging_writer.py:48] [1223] accumulated_eval_time=9613.94, accumulated_logging_time=0.299121, accumulated_submission_time=1206.3, global_step=1223, preemption_count=0, score=1206.3, test/loss=0.12815, test/num_examples=95000000, total_duration=10829.7, train/loss=0.124161, validation/loss=0.125813, validation/num_examples=83274637
I0316 15:05:05.108863 140452064818368 spec.py:321] Evaluating on the training split.
I0316 15:08:22.600848 140452064818368 spec.py:333] Evaluating on the validation split.
I0316 15:12:47.429118 140452064818368 spec.py:349] Evaluating on the test split.
I0316 15:17:40.672414 140452064818368 submission_runner.py:469] Time since start: 11705.93s, 	Step: 1350, 	{'train/loss': 0.12446779496196424, 'validation/loss': 0.12565363775852995, 'validation/num_examples': 83274637, 'test/loss': 0.12800013631788554, 'test/num_examples': 95000000, 'score': 1326.115199804306, 'total_duration': 11705.92903470993, 'accumulated_submission_time': 1326.115199804306, 'accumulated_eval_time': 10369.500856637955, 'accumulated_logging_time': 0.31755805015563965}
I0316 15:17:40.683423 140409854486272 logging_writer.py:48] [1350] accumulated_eval_time=10369.5, accumulated_logging_time=0.317558, accumulated_submission_time=1326.12, global_step=1350, preemption_count=0, score=1326.12, test/loss=0.128, test/num_examples=95000000, total_duration=11705.9, train/loss=0.124468, validation/loss=0.125654, validation/num_examples=83274637
I0316 15:19:41.722099 140452064818368 spec.py:321] Evaluating on the training split.
I0316 15:21:51.908477 140452064818368 spec.py:333] Evaluating on the validation split.
I0316 15:26:32.000881 140452064818368 spec.py:349] Evaluating on the test split.
I0316 15:31:27.613343 140452064818368 submission_runner.py:469] Time since start: 12532.87s, 	Step: 1471, 	{'train/loss': 0.12350990747859415, 'validation/loss': 0.12566981729135396, 'validation/num_examples': 83274637, 'test/loss': 0.1281688987270556, 'test/num_examples': 95000000, 'score': 1446.286691904068, 'total_duration': 12532.8699426651, 'accumulated_submission_time': 1446.286691904068, 'accumulated_eval_time': 11075.392145633698, 'accumulated_logging_time': 0.3357665538787842}
I0316 15:31:27.623324 140409862878976 logging_writer.py:48] [1471] accumulated_eval_time=11075.4, accumulated_logging_time=0.335767, accumulated_submission_time=1446.29, global_step=1471, preemption_count=0, score=1446.29, test/loss=0.128169, test/num_examples=95000000, total_duration=12532.9, train/loss=0.12351, validation/loss=0.12567, validation/num_examples=83274637
I0316 15:31:34.088391 140409854486272 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.0172765, loss=0.136317
I0316 15:31:34.091601 140452064818368 submission.py:265] 1500) loss = 0.136, grad_norm = 0.017
I0316 15:33:29.303440 140452064818368 spec.py:321] Evaluating on the training split.
I0316 15:35:38.212474 140452064818368 spec.py:333] Evaluating on the validation split.
I0316 15:40:10.240371 140452064818368 spec.py:349] Evaluating on the test split.
I0316 15:45:03.754584 140452064818368 submission_runner.py:469] Time since start: 13349.01s, 	Step: 1598, 	{'train/loss': 0.12554739951582072, 'validation/loss': 0.12540266545437773, 'validation/num_examples': 83274637, 'test/loss': 0.12770052270989668, 'test/num_examples': 95000000, 'score': 1567.140037059784, 'total_duration': 13349.011248588562, 'accumulated_submission_time': 1567.140037059784, 'accumulated_eval_time': 11769.843456029892, 'accumulated_logging_time': 0.35332560539245605}
I0316 15:45:03.765100 140409862878976 logging_writer.py:48] [1598] accumulated_eval_time=11769.8, accumulated_logging_time=0.353326, accumulated_submission_time=1567.14, global_step=1598, preemption_count=0, score=1567.14, test/loss=0.127701, test/num_examples=95000000, total_duration=13349, train/loss=0.125547, validation/loss=0.125403, validation/num_examples=83274637
I0316 15:47:05.339333 140452064818368 spec.py:321] Evaluating on the training split.
I0316 15:49:08.335452 140452064818368 spec.py:333] Evaluating on the validation split.
I0316 15:53:48.546337 140452064818368 spec.py:349] Evaluating on the test split.
I0316 15:58:39.810778 140452064818368 submission_runner.py:469] Time since start: 14165.07s, 	Step: 1719, 	{'train/loss': 0.12291021959072142, 'validation/loss': 0.12523784068114227, 'validation/num_examples': 83274637, 'test/loss': 0.12746143530413978, 'test/num_examples': 95000000, 'score': 1687.8334410190582, 'total_duration': 14165.067366600037, 'accumulated_submission_time': 1687.8334410190582, 'accumulated_eval_time': 12464.314994096756, 'accumulated_logging_time': 0.3713796138763428}
I0316 15:58:39.821270 140409854486272 logging_writer.py:48] [1719] accumulated_eval_time=12464.3, accumulated_logging_time=0.37138, accumulated_submission_time=1687.83, global_step=1719, preemption_count=0, score=1687.83, test/loss=0.127461, test/num_examples=95000000, total_duration=14165.1, train/loss=0.12291, validation/loss=0.125238, validation/num_examples=83274637
I0316 16:00:41.685603 140452064818368 spec.py:321] Evaluating on the training split.
I0316 16:02:52.887105 140452064818368 spec.py:333] Evaluating on the validation split.
I0316 16:07:38.241674 140452064818368 spec.py:349] Evaluating on the test split.
I0316 16:12:34.723513 140452064818368 submission_runner.py:469] Time since start: 14999.98s, 	Step: 1837, 	{'train/loss': 0.12418532343900127, 'validation/loss': 0.1251253183451596, 'validation/num_examples': 83274637, 'test/loss': 0.12757429894690261, 'test/num_examples': 95000000, 'score': 1808.9027123451233, 'total_duration': 14999.980171203613, 'accumulated_submission_time': 1808.9027123451233, 'accumulated_eval_time': 13177.352951288223, 'accumulated_logging_time': 0.38906407356262207}
I0316 16:12:34.733802 140409862878976 logging_writer.py:48] [1837] accumulated_eval_time=13177.4, accumulated_logging_time=0.389064, accumulated_submission_time=1808.9, global_step=1837, preemption_count=0, score=1808.9, test/loss=0.127574, test/num_examples=95000000, total_duration=15000, train/loss=0.124185, validation/loss=0.125125, validation/num_examples=83274637
I0316 16:14:35.556104 140452064818368 spec.py:321] Evaluating on the training split.
I0316 16:16:48.096074 140452064818368 spec.py:333] Evaluating on the validation split.
I0316 16:21:26.353483 140452064818368 spec.py:349] Evaluating on the test split.
I0316 16:26:25.817241 140452064818368 submission_runner.py:469] Time since start: 15831.07s, 	Step: 1960, 	{'train/loss': 0.1272784827857698, 'validation/loss': 0.12508060406532362, 'validation/num_examples': 83274637, 'test/loss': 0.1274504007788407, 'test/num_examples': 95000000, 'score': 1928.925883769989, 'total_duration': 15831.073868989944, 'accumulated_submission_time': 1928.925883769989, 'accumulated_eval_time': 13887.61410689354, 'accumulated_logging_time': 0.40613317489624023}
I0316 16:26:25.827251 140409854486272 logging_writer.py:48] [1960] accumulated_eval_time=13887.6, accumulated_logging_time=0.406133, accumulated_submission_time=1928.93, global_step=1960, preemption_count=0, score=1928.93, test/loss=0.12745, test/num_examples=95000000, total_duration=15831.1, train/loss=0.127278, validation/loss=0.125081, validation/num_examples=83274637
I0316 16:26:45.478261 140409862878976 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.0104333, loss=0.124467
I0316 16:26:45.481943 140452064818368 submission.py:265] 2000) loss = 0.124, grad_norm = 0.010
I0316 16:28:27.079805 140452064818368 spec.py:321] Evaluating on the training split.
I0316 16:30:38.871046 140452064818368 spec.py:333] Evaluating on the validation split.
I0316 16:35:05.279481 140452064818368 spec.py:349] Evaluating on the test split.
I0316 16:39:52.660300 140452064818368 submission_runner.py:469] Time since start: 16637.92s, 	Step: 2082, 	{'train/loss': 0.12435758199888869, 'validation/loss': 0.12518324496061767, 'validation/num_examples': 83274637, 'test/loss': 0.1274853324144062, 'test/num_examples': 95000000, 'score': 2049.305502653122, 'total_duration': 16637.916900634766, 'accumulated_submission_time': 2049.305502653122, 'accumulated_eval_time': 14573.1947016716, 'accumulated_logging_time': 0.42369914054870605}
I0316 16:39:52.670422 140409854486272 logging_writer.py:48] [2082] accumulated_eval_time=14573.2, accumulated_logging_time=0.423699, accumulated_submission_time=2049.31, global_step=2082, preemption_count=0, score=2049.31, test/loss=0.127485, test/num_examples=95000000, total_duration=16637.9, train/loss=0.124358, validation/loss=0.125183, validation/num_examples=83274637
I0316 16:41:53.936284 140452064818368 spec.py:321] Evaluating on the training split.
I0316 16:44:05.327114 140452064818368 spec.py:333] Evaluating on the validation split.
I0316 16:48:37.888044 140452064818368 spec.py:349] Evaluating on the test split.
I0316 16:53:25.431342 140452064818368 submission_runner.py:469] Time since start: 17450.69s, 	Step: 2209, 	{'train/loss': 0.1233498434523425, 'validation/loss': 0.12511047738694356, 'validation/num_examples': 83274637, 'test/loss': 0.12729697007944207, 'test/num_examples': 95000000, 'score': 2169.647258043289, 'total_duration': 17450.68799304962, 'accumulated_submission_time': 2169.647258043289, 'accumulated_eval_time': 15264.68991446495, 'accumulated_logging_time': 0.44046449661254883}
I0316 16:53:25.442631 140409862878976 logging_writer.py:48] [2209] accumulated_eval_time=15264.7, accumulated_logging_time=0.440464, accumulated_submission_time=2169.65, global_step=2209, preemption_count=0, score=2169.65, test/loss=0.127297, test/num_examples=95000000, total_duration=17450.7, train/loss=0.12335, validation/loss=0.12511, validation/num_examples=83274637
I0316 16:55:26.628499 140452064818368 spec.py:321] Evaluating on the training split.
I0316 16:57:40.312867 140452064818368 spec.py:333] Evaluating on the validation split.
I0316 17:02:26.403281 140452064818368 spec.py:349] Evaluating on the test split.
I0316 17:07:32.117594 140452064818368 submission_runner.py:469] Time since start: 18297.37s, 	Step: 2331, 	{'train/loss': 0.12498596727282993, 'validation/loss': 0.12525216967035144, 'validation/num_examples': 83274637, 'test/loss': 0.12758739206695557, 'test/num_examples': 95000000, 'score': 2289.9520835876465, 'total_duration': 18297.37419295311, 'accumulated_submission_time': 2289.9520835876465, 'accumulated_eval_time': 15990.179099321365, 'accumulated_logging_time': 0.4589364528656006}
I0316 17:07:32.128252 140409854486272 logging_writer.py:48] [2331] accumulated_eval_time=15990.2, accumulated_logging_time=0.458936, accumulated_submission_time=2289.95, global_step=2331, preemption_count=0, score=2289.95, test/loss=0.127587, test/num_examples=95000000, total_duration=18297.4, train/loss=0.124986, validation/loss=0.125252, validation/num_examples=83274637
I0316 17:09:34.386073 140452064818368 spec.py:321] Evaluating on the training split.
I0316 17:11:45.025640 140452064818368 spec.py:333] Evaluating on the validation split.
I0316 17:16:26.435164 140452064818368 spec.py:349] Evaluating on the test split.
I0316 17:21:26.224095 140452064818368 submission_runner.py:469] Time since start: 19131.48s, 	Step: 2455, 	{'train/loss': 0.12205388384070318, 'validation/loss': 0.12508738634270553, 'validation/num_examples': 83274637, 'test/loss': 0.12728376804954128, 'test/num_examples': 95000000, 'score': 2411.405754804611, 'total_duration': 19131.480714559555, 'accumulated_submission_time': 2411.405754804611, 'accumulated_eval_time': 16702.017133951187, 'accumulated_logging_time': 0.4765923023223877}
I0316 17:21:26.235317 140409862878976 logging_writer.py:48] [2455] accumulated_eval_time=16702, accumulated_logging_time=0.476592, accumulated_submission_time=2411.41, global_step=2455, preemption_count=0, score=2411.41, test/loss=0.127284, test/num_examples=95000000, total_duration=19131.5, train/loss=0.122054, validation/loss=0.125087, validation/num_examples=83274637
I0316 17:21:52.267728 140409854486272 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.00708992, loss=0.117579
I0316 17:21:52.271605 140452064818368 submission.py:265] 2500) loss = 0.118, grad_norm = 0.007
I0316 17:23:28.054394 140452064818368 spec.py:321] Evaluating on the training split.
I0316 17:25:44.502155 140452064818368 spec.py:333] Evaluating on the validation split.
I0316 17:30:22.931596 140452064818368 spec.py:349] Evaluating on the test split.
I0316 17:35:15.720432 140452064818368 submission_runner.py:469] Time since start: 19960.98s, 	Step: 2576, 	{'train/loss': 0.12277646065547992, 'validation/loss': 0.12477494465732591, 'validation/num_examples': 83274637, 'test/loss': 0.12702831757860686, 'test/num_examples': 95000000, 'score': 2532.3585402965546, 'total_duration': 19960.977091550827, 'accumulated_submission_time': 2532.3585402965546, 'accumulated_eval_time': 17409.683349847794, 'accumulated_logging_time': 0.494856595993042}
I0316 17:35:15.730818 140409862878976 logging_writer.py:48] [2576] accumulated_eval_time=17409.7, accumulated_logging_time=0.494857, accumulated_submission_time=2532.36, global_step=2576, preemption_count=0, score=2532.36, test/loss=0.127028, test/num_examples=95000000, total_duration=19961, train/loss=0.122776, validation/loss=0.124775, validation/num_examples=83274637
I0316 17:37:16.673256 140452064818368 spec.py:321] Evaluating on the training split.
I0316 17:39:29.109814 140452064818368 spec.py:333] Evaluating on the validation split.
I0316 17:44:04.330995 140452064818368 spec.py:349] Evaluating on the test split.
I0316 17:48:54.695101 140452064818368 submission_runner.py:469] Time since start: 20779.95s, 	Step: 2696, 	{'train/loss': 0.12532034974364584, 'validation/loss': 0.12490441414679739, 'validation/num_examples': 83274637, 'test/loss': 0.12728110285451788, 'test/num_examples': 95000000, 'score': 2652.4453897476196, 'total_duration': 20779.95171737671, 'accumulated_submission_time': 2652.4453897476196, 'accumulated_eval_time': 18107.705209493637, 'accumulated_logging_time': 0.5119156837463379}
I0316 17:48:54.706272 140409854486272 logging_writer.py:48] [2696] accumulated_eval_time=18107.7, accumulated_logging_time=0.511916, accumulated_submission_time=2652.45, global_step=2696, preemption_count=0, score=2652.45, test/loss=0.127281, test/num_examples=95000000, total_duration=20780, train/loss=0.12532, validation/loss=0.124904, validation/num_examples=83274637
I0316 17:50:55.917075 140452064818368 spec.py:321] Evaluating on the training split.
I0316 17:53:01.950079 140452064818368 spec.py:333] Evaluating on the validation split.
I0316 17:57:40.611455 140452064818368 spec.py:349] Evaluating on the test split.
I0316 18:02:38.772799 140452064818368 submission_runner.py:469] Time since start: 21604.03s, 	Step: 2819, 	{'train/loss': 0.12393260272857393, 'validation/loss': 0.12474691330412799, 'validation/num_examples': 83274637, 'test/loss': 0.12710381994753386, 'test/num_examples': 95000000, 'score': 2772.788913488388, 'total_duration': 21604.029458761215, 'accumulated_submission_time': 2772.788913488388, 'accumulated_eval_time': 18810.561002492905, 'accumulated_logging_time': 0.5638537406921387}
I0316 18:02:38.784061 140409862878976 logging_writer.py:48] [2819] accumulated_eval_time=18810.6, accumulated_logging_time=0.563854, accumulated_submission_time=2772.79, global_step=2819, preemption_count=0, score=2772.79, test/loss=0.127104, test/num_examples=95000000, total_duration=21604, train/loss=0.123933, validation/loss=0.124747, validation/num_examples=83274637
I0316 18:04:39.406280 140452064818368 spec.py:321] Evaluating on the training split.
I0316 18:06:41.706326 140452064818368 spec.py:333] Evaluating on the validation split.
I0316 18:11:11.486259 140452064818368 spec.py:349] Evaluating on the test split.
I0316 18:16:05.942596 140452064818368 submission_runner.py:469] Time since start: 22411.20s, 	Step: 2940, 	{'train/loss': 0.12354012859895269, 'validation/loss': 0.124748586791193, 'validation/num_examples': 83274637, 'test/loss': 0.12714746886203163, 'test/num_examples': 95000000, 'score': 2892.5249168872833, 'total_duration': 22411.199244976044, 'accumulated_submission_time': 2892.5249168872833, 'accumulated_eval_time': 19497.097414016724, 'accumulated_logging_time': 0.5820839405059814}
I0316 18:16:05.952735 140409854486272 logging_writer.py:48] [2940] accumulated_eval_time=19497.1, accumulated_logging_time=0.582084, accumulated_submission_time=2892.52, global_step=2940, preemption_count=0, score=2892.52, test/loss=0.127147, test/num_examples=95000000, total_duration=22411.2, train/loss=0.12354, validation/loss=0.124749, validation/num_examples=83274637
I0316 18:16:51.313822 140409862878976 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.0253408, loss=0.129464
I0316 18:16:51.317116 140452064818368 submission.py:265] 3000) loss = 0.129, grad_norm = 0.025
I0316 18:18:06.821081 140452064818368 spec.py:321] Evaluating on the training split.
I0316 18:20:12.164567 140452064818368 spec.py:333] Evaluating on the validation split.
I0316 18:24:43.985043 140452064818368 spec.py:349] Evaluating on the test split.
I0316 18:29:33.469110 140452064818368 submission_runner.py:469] Time since start: 23218.73s, 	Step: 3064, 	{'train/loss': 0.12173183175253116, 'validation/loss': 0.12473685723273148, 'validation/num_examples': 83274637, 'test/loss': 0.1271002791008397, 'test/num_examples': 95000000, 'score': 3012.522522211075, 'total_duration': 23218.72576212883, 'accumulated_submission_time': 3012.522522211075, 'accumulated_eval_time': 20183.74552321434, 'accumulated_logging_time': 0.5997507572174072}
I0316 18:29:33.540448 140409854486272 logging_writer.py:48] [3064] accumulated_eval_time=20183.7, accumulated_logging_time=0.599751, accumulated_submission_time=3012.52, global_step=3064, preemption_count=0, score=3012.52, test/loss=0.1271, test/num_examples=95000000, total_duration=23218.7, train/loss=0.121732, validation/loss=0.124737, validation/num_examples=83274637
I0316 18:31:34.904433 140452064818368 spec.py:321] Evaluating on the training split.
I0316 18:33:42.127677 140452064818368 spec.py:333] Evaluating on the validation split.
I0316 18:38:20.403007 140452064818368 spec.py:349] Evaluating on the test split.
I0316 18:43:09.639238 140452064818368 submission_runner.py:469] Time since start: 24034.90s, 	Step: 3188, 	{'train/loss': 0.12475704005351405, 'validation/loss': 0.12467825475592674, 'validation/num_examples': 83274637, 'test/loss': 0.12697045888334577, 'test/num_examples': 95000000, 'score': 3132.9987287521362, 'total_duration': 24034.895886659622, 'accumulated_submission_time': 3132.9987287521362, 'accumulated_eval_time': 20878.48043036461, 'accumulated_logging_time': 0.6780641078948975}
I0316 18:43:09.651610 140409862878976 logging_writer.py:48] [3188] accumulated_eval_time=20878.5, accumulated_logging_time=0.678064, accumulated_submission_time=3133, global_step=3188, preemption_count=0, score=3133, test/loss=0.12697, test/num_examples=95000000, total_duration=24034.9, train/loss=0.124757, validation/loss=0.124678, validation/num_examples=83274637
I0316 18:45:10.843778 140452064818368 spec.py:321] Evaluating on the training split.
I0316 18:47:23.495618 140452064818368 spec.py:333] Evaluating on the validation split.
I0316 18:52:01.277614 140452064818368 spec.py:349] Evaluating on the test split.
I0316 18:56:58.579760 140452064818368 submission_runner.py:469] Time since start: 24863.84s, 	Step: 3313, 	{'train/loss': 0.1226004052599597, 'validation/loss': 0.12472303841509007, 'validation/num_examples': 83274637, 'test/loss': 0.12707351653482538, 'test/num_examples': 95000000, 'score': 3253.3166873455048, 'total_duration': 24863.836413621902, 'accumulated_submission_time': 3253.3166873455048, 'accumulated_eval_time': 21586.216495513916, 'accumulated_logging_time': 0.6978273391723633}
I0316 18:56:58.590501 140409854486272 logging_writer.py:48] [3313] accumulated_eval_time=21586.2, accumulated_logging_time=0.697827, accumulated_submission_time=3253.32, global_step=3313, preemption_count=0, score=3253.32, test/loss=0.127074, test/num_examples=95000000, total_duration=24863.8, train/loss=0.1226, validation/loss=0.124723, validation/num_examples=83274637
I0316 18:58:59.622758 140452064818368 spec.py:321] Evaluating on the training split.
I0316 19:01:09.897268 140452064818368 spec.py:333] Evaluating on the validation split.
I0316 19:05:44.328414 140452064818368 spec.py:349] Evaluating on the test split.
I0316 19:10:29.441092 140452064818368 submission_runner.py:469] Time since start: 25674.70s, 	Step: 3436, 	{'train/loss': 0.1237160834589522, 'validation/loss': 0.12472562965026103, 'validation/num_examples': 83274637, 'test/loss': 0.12713047632639032, 'test/num_examples': 95000000, 'score': 3373.5131628513336, 'total_duration': 25674.697743415833, 'accumulated_submission_time': 3373.5131628513336, 'accumulated_eval_time': 22276.034877300262, 'accumulated_logging_time': 0.7151679992675781}
I0316 19:10:29.451799 140409862878976 logging_writer.py:48] [3436] accumulated_eval_time=22276, accumulated_logging_time=0.715168, accumulated_submission_time=3373.51, global_step=3436, preemption_count=0, score=3373.51, test/loss=0.12713, test/num_examples=95000000, total_duration=25674.7, train/loss=0.123716, validation/loss=0.124726, validation/num_examples=83274637
I0316 19:11:17.271198 140409854486272 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.0108329, loss=0.117995
I0316 19:11:17.274571 140452064818368 submission.py:265] 3500) loss = 0.118, grad_norm = 0.011
I0316 19:12:30.395161 140452064818368 spec.py:321] Evaluating on the training split.
I0316 19:14:37.994742 140452064818368 spec.py:333] Evaluating on the validation split.
I0316 19:19:23.630041 140452064818368 spec.py:349] Evaluating on the test split.
I0316 19:24:22.138638 140452064818368 submission_runner.py:469] Time since start: 26507.40s, 	Step: 3559, 	{'train/loss': 0.1260238999328458, 'validation/loss': 0.12473637068531697, 'validation/num_examples': 83274637, 'test/loss': 0.12709235642443206, 'test/num_examples': 95000000, 'score': 3493.560968399048, 'total_duration': 26507.39527463913, 'accumulated_submission_time': 3493.560968399048, 'accumulated_eval_time': 22987.778450489044, 'accumulated_logging_time': 0.7329566478729248}
I0316 19:24:22.149389 140409862878976 logging_writer.py:48] [3559] accumulated_eval_time=22987.8, accumulated_logging_time=0.732957, accumulated_submission_time=3493.56, global_step=3559, preemption_count=0, score=3493.56, test/loss=0.127092, test/num_examples=95000000, total_duration=26507.4, train/loss=0.126024, validation/loss=0.124736, validation/num_examples=83274637
I0316 19:26:23.315096 140452064818368 spec.py:321] Evaluating on the training split.
I0316 19:28:31.431260 140452064818368 spec.py:333] Evaluating on the validation split.
I0316 19:33:07.245507 140452064818368 spec.py:349] Evaluating on the test split.
I0316 19:37:58.518162 140452064818368 submission_runner.py:469] Time since start: 27323.77s, 	Step: 3689, 	{'train/loss': 0.12401712227610169, 'validation/loss': 0.1248227517640353, 'validation/num_examples': 83274637, 'test/loss': 0.12747303791319195, 'test/num_examples': 95000000, 'score': 3613.8832693099976, 'total_duration': 27323.774816036224, 'accumulated_submission_time': 3613.8832693099976, 'accumulated_eval_time': 23682.981556653976, 'accumulated_logging_time': 0.7501275539398193}
I0316 19:37:58.530040 140409854486272 logging_writer.py:48] [3689] accumulated_eval_time=23683, accumulated_logging_time=0.750128, accumulated_submission_time=3613.88, global_step=3689, preemption_count=0, score=3613.88, test/loss=0.127473, test/num_examples=95000000, total_duration=27323.8, train/loss=0.124017, validation/loss=0.124823, validation/num_examples=83274637
I0316 19:39:59.001619 140452064818368 spec.py:321] Evaluating on the training split.
I0316 19:42:03.109917 140452064818368 spec.py:333] Evaluating on the validation split.
I0316 19:46:51.784772 140452064818368 spec.py:349] Evaluating on the test split.
I0316 19:51:48.828738 140452064818368 submission_runner.py:469] Time since start: 28154.09s, 	Step: 3813, 	{'train/loss': 0.12498849236151523, 'validation/loss': 0.12459954110834236, 'validation/num_examples': 83274637, 'test/loss': 0.1269656788077505, 'test/num_examples': 95000000, 'score': 3733.454436302185, 'total_duration': 28154.08536338806, 'accumulated_submission_time': 3733.454436302185, 'accumulated_eval_time': 24392.808722019196, 'accumulated_logging_time': 0.7835752964019775}
I0316 19:51:48.840062 140409862878976 logging_writer.py:48] [3813] accumulated_eval_time=24392.8, accumulated_logging_time=0.783575, accumulated_submission_time=3733.45, global_step=3813, preemption_count=0, score=3733.45, test/loss=0.126966, test/num_examples=95000000, total_duration=28154.1, train/loss=0.124988, validation/loss=0.1246, validation/num_examples=83274637
I0316 19:53:49.587317 140452064818368 spec.py:321] Evaluating on the training split.
I0316 19:55:55.893550 140452064818368 spec.py:333] Evaluating on the validation split.
I0316 20:00:30.471419 140452064818368 spec.py:349] Evaluating on the test split.
I0316 20:05:20.687362 140452064818368 submission_runner.py:469] Time since start: 28965.94s, 	Step: 3935, 	{'train/loss': 0.12096645933277825, 'validation/loss': 0.12456880227978906, 'validation/num_examples': 83274637, 'test/loss': 0.12686842103038587, 'test/num_examples': 95000000, 'score': 3853.357095003128, 'total_duration': 28965.94380044937, 'accumulated_submission_time': 3853.357095003128, 'accumulated_eval_time': 25083.908751249313, 'accumulated_logging_time': 0.8017215728759766}
I0316 20:05:20.697360 140409854486272 logging_writer.py:48] [3935] accumulated_eval_time=25083.9, accumulated_logging_time=0.801722, accumulated_submission_time=3853.36, global_step=3935, preemption_count=0, score=3853.36, test/loss=0.126868, test/num_examples=95000000, total_duration=28965.9, train/loss=0.120966, validation/loss=0.124569, validation/num_examples=83274637
I0316 20:06:14.657576 140409862878976 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.0073657, loss=0.123535
I0316 20:06:14.661095 140452064818368 submission.py:265] 4000) loss = 0.124, grad_norm = 0.007
I0316 20:07:21.868489 140452064818368 spec.py:321] Evaluating on the training split.
I0316 20:09:31.988046 140452064818368 spec.py:333] Evaluating on the validation split.
I0316 20:14:13.035447 140452064818368 spec.py:349] Evaluating on the test split.
I0316 20:19:16.203077 140452064818368 submission_runner.py:469] Time since start: 29801.46s, 	Step: 4054, 	{'train/loss': 0.12358228968403802, 'validation/loss': 0.12458906232134165, 'validation/num_examples': 83274637, 'test/loss': 0.12680416487446836, 'test/num_examples': 95000000, 'score': 3973.705192089081, 'total_duration': 29801.459738492966, 'accumulated_submission_time': 3973.705192089081, 'accumulated_eval_time': 25798.243510961533, 'accumulated_logging_time': 0.8195385932922363}
I0316 20:19:16.214430 140409854486272 logging_writer.py:48] [4054] accumulated_eval_time=25798.2, accumulated_logging_time=0.819539, accumulated_submission_time=3973.71, global_step=4054, preemption_count=0, score=3973.71, test/loss=0.126804, test/num_examples=95000000, total_duration=29801.5, train/loss=0.123582, validation/loss=0.124589, validation/num_examples=83274637
I0316 20:21:16.818551 140452064818368 spec.py:321] Evaluating on the training split.
I0316 20:23:27.066554 140452064818368 spec.py:333] Evaluating on the validation split.
I0316 20:28:16.781344 140452064818368 spec.py:349] Evaluating on the test split.
I0316 20:33:12.204133 140452064818368 submission_runner.py:469] Time since start: 30637.46s, 	Step: 4175, 	{'train/loss': 0.12271017263085163, 'validation/loss': 0.12455393548087747, 'validation/num_examples': 83274637, 'test/loss': 0.12687363356628417, 'test/num_examples': 95000000, 'score': 4093.462232351303, 'total_duration': 30637.46075129509, 'accumulated_submission_time': 4093.462232351303, 'accumulated_eval_time': 26513.629149198532, 'accumulated_logging_time': 0.8384106159210205}
I0316 20:33:12.216665 140409862878976 logging_writer.py:48] [4175] accumulated_eval_time=26513.6, accumulated_logging_time=0.838411, accumulated_submission_time=4093.46, global_step=4175, preemption_count=0, score=4093.46, test/loss=0.126874, test/num_examples=95000000, total_duration=30637.5, train/loss=0.12271, validation/loss=0.124554, validation/num_examples=83274637
I0316 20:35:12.872858 140452064818368 spec.py:321] Evaluating on the training split.
I0316 20:37:21.655492 140452064818368 spec.py:333] Evaluating on the validation split.
I0316 20:42:03.730770 140452064818368 spec.py:349] Evaluating on the test split.
I0316 20:46:52.692084 140452064818368 submission_runner.py:469] Time since start: 31457.95s, 	Step: 4299, 	{'train/loss': 0.12358242661676445, 'validation/loss': 0.12456168799737823, 'validation/num_examples': 83274637, 'test/loss': 0.1269886673636587, 'test/num_examples': 95000000, 'score': 4213.238229036331, 'total_duration': 31457.948748111725, 'accumulated_submission_time': 4213.238229036331, 'accumulated_eval_time': 27213.448467731476, 'accumulated_logging_time': 0.8781960010528564}
I0316 20:46:52.703766 140409854486272 logging_writer.py:48] [4299] accumulated_eval_time=27213.4, accumulated_logging_time=0.878196, accumulated_submission_time=4213.24, global_step=4299, preemption_count=0, score=4213.24, test/loss=0.126989, test/num_examples=95000000, total_duration=31457.9, train/loss=0.123582, validation/loss=0.124562, validation/num_examples=83274637
I0316 20:48:53.414585 140452064818368 spec.py:321] Evaluating on the training split.
I0316 20:50:58.321815 140452064818368 spec.py:333] Evaluating on the validation split.
I0316 20:55:39.456612 140452064818368 spec.py:349] Evaluating on the test split.
I0316 21:00:32.898390 140452064818368 submission_runner.py:469] Time since start: 32278.16s, 	Step: 4420, 	{'train/loss': 0.1226098140477295, 'validation/loss': 0.12458285201746563, 'validation/num_examples': 83274637, 'test/loss': 0.1269085418898733, 'test/num_examples': 95000000, 'score': 4333.048588752747, 'total_duration': 32278.155025959015, 'accumulated_submission_time': 4333.048588752747, 'accumulated_eval_time': 27912.932423830032, 'accumulated_logging_time': 0.8970916271209717}
I0316 21:00:32.909079 140409862878976 logging_writer.py:48] [4420] accumulated_eval_time=27912.9, accumulated_logging_time=0.897092, accumulated_submission_time=4333.05, global_step=4420, preemption_count=0, score=4333.05, test/loss=0.126909, test/num_examples=95000000, total_duration=32278.2, train/loss=0.12261, validation/loss=0.124583, validation/num_examples=83274637
I0316 21:01:41.880182 140409854486272 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.0161385, loss=0.124752
I0316 21:01:41.883792 140452064818368 submission.py:265] 4500) loss = 0.125, grad_norm = 0.016
I0316 21:02:33.391987 140452064818368 spec.py:321] Evaluating on the training split.
I0316 21:04:40.737239 140452064818368 spec.py:333] Evaluating on the validation split.
I0316 21:09:22.004916 140452064818368 spec.py:349] Evaluating on the test split.
I0316 21:14:12.555654 140452064818368 submission_runner.py:469] Time since start: 33097.81s, 	Step: 4544, 	{'train/loss': 0.1207330059746587, 'validation/loss': 0.12442978587903422, 'validation/num_examples': 83274637, 'test/loss': 0.12673042956237793, 'test/num_examples': 95000000, 'score': 4452.692674160004, 'total_duration': 33097.81225705147, 'accumulated_submission_time': 4452.692674160004, 'accumulated_eval_time': 28612.096150636673, 'accumulated_logging_time': 0.9151108264923096}
I0316 21:14:12.566608 140409862878976 logging_writer.py:48] [4544] accumulated_eval_time=28612.1, accumulated_logging_time=0.915111, accumulated_submission_time=4452.69, global_step=4544, preemption_count=0, score=4452.69, test/loss=0.12673, test/num_examples=95000000, total_duration=33097.8, train/loss=0.120733, validation/loss=0.12443, validation/num_examples=83274637
I0316 21:16:13.366453 140452064818368 spec.py:321] Evaluating on the training split.
I0316 21:18:22.579284 140452064818368 spec.py:333] Evaluating on the validation split.
I0316 21:23:09.559138 140452064818368 spec.py:349] Evaluating on the test split.
I0316 21:28:07.030651 140452064818368 submission_runner.py:469] Time since start: 33932.29s, 	Step: 4663, 	{'train/loss': 0.12151488173893556, 'validation/loss': 0.12448371707163437, 'validation/num_examples': 83274637, 'test/loss': 0.12680591815229716, 'test/num_examples': 95000000, 'score': 4572.632852554321, 'total_duration': 33932.28729176521, 'accumulated_submission_time': 4572.632852554321, 'accumulated_eval_time': 29325.760493040085, 'accumulated_logging_time': 0.933140754699707}
I0316 21:28:07.042623 140409854486272 logging_writer.py:48] [4663] accumulated_eval_time=29325.8, accumulated_logging_time=0.933141, accumulated_submission_time=4572.63, global_step=4663, preemption_count=0, score=4572.63, test/loss=0.126806, test/num_examples=95000000, total_duration=33932.3, train/loss=0.121515, validation/loss=0.124484, validation/num_examples=83274637
I0316 21:30:07.961562 140452064818368 spec.py:321] Evaluating on the training split.
I0316 21:32:17.032239 140452064818368 spec.py:333] Evaluating on the validation split.
I0316 21:36:43.784465 140452064818368 spec.py:349] Evaluating on the test split.
I0316 21:41:37.751812 140452064818368 submission_runner.py:469] Time since start: 34743.01s, 	Step: 4777, 	{'train/loss': 0.12454489025776719, 'validation/loss': 0.12450191213574323, 'validation/num_examples': 83274637, 'test/loss': 0.12682452589484766, 'test/num_examples': 95000000, 'score': 4692.687784671783, 'total_duration': 34743.00843644142, 'accumulated_submission_time': 4692.687784671783, 'accumulated_eval_time': 30015.550897598267, 'accumulated_logging_time': 0.952477216720581}
I0316 21:41:37.799136 140409862878976 logging_writer.py:48] [4777] accumulated_eval_time=30015.6, accumulated_logging_time=0.952477, accumulated_submission_time=4692.69, global_step=4777, preemption_count=0, score=4692.69, test/loss=0.126825, test/num_examples=95000000, total_duration=34743, train/loss=0.124545, validation/loss=0.124502, validation/num_examples=83274637
I0316 21:43:39.792675 140452064818368 spec.py:321] Evaluating on the training split.
I0316 21:45:46.478151 140452064818368 spec.py:333] Evaluating on the validation split.
I0316 21:50:28.367004 140452064818368 spec.py:349] Evaluating on the test split.
I0316 21:55:23.046744 140452064818368 submission_runner.py:469] Time since start: 35568.30s, 	Step: 4899, 	{'train/loss': 0.12467550781067917, 'validation/loss': 0.12439532669714809, 'validation/num_examples': 83274637, 'test/loss': 0.12677003348890606, 'test/num_examples': 95000000, 'score': 4813.844833612442, 'total_duration': 35568.30336332321, 'accumulated_submission_time': 4813.844833612442, 'accumulated_eval_time': 30718.804966926575, 'accumulated_logging_time': 1.0238473415374756}
I0316 21:55:23.058811 140409854486272 logging_writer.py:48] [4899] accumulated_eval_time=30718.8, accumulated_logging_time=1.02385, accumulated_submission_time=4813.84, global_step=4899, preemption_count=0, score=4813.84, test/loss=0.12677, test/num_examples=95000000, total_duration=35568.3, train/loss=0.124676, validation/loss=0.124395, validation/num_examples=83274637
I0316 21:56:56.653403 140409862878976 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.00925964, loss=0.128523
I0316 21:56:56.657018 140452064818368 submission.py:265] 5000) loss = 0.129, grad_norm = 0.009
I0316 21:57:24.338585 140452064818368 spec.py:321] Evaluating on the training split.
I0316 21:59:35.858188 140452064818368 spec.py:333] Evaluating on the validation split.
I0316 22:04:21.899695 140452064818368 spec.py:349] Evaluating on the test split.
I0316 22:09:14.507183 140452064818368 submission_runner.py:469] Time since start: 36399.76s, 	Step: 5023, 	{'train/loss': 0.12479341067553311, 'validation/loss': 0.12436998962142615, 'validation/num_examples': 83274637, 'test/loss': 0.12670422618440327, 'test/num_examples': 95000000, 'score': 4934.32159113884, 'total_duration': 36399.763825416565, 'accumulated_submission_time': 4934.32159113884, 'accumulated_eval_time': 31428.97360754013, 'accumulated_logging_time': 1.0433995723724365}
I0316 22:09:14.519177 140409854486272 logging_writer.py:48] [5023] accumulated_eval_time=31429, accumulated_logging_time=1.0434, accumulated_submission_time=4934.32, global_step=5023, preemption_count=0, score=4934.32, test/loss=0.126704, test/num_examples=95000000, total_duration=36399.8, train/loss=0.124793, validation/loss=0.12437, validation/num_examples=83274637
I0316 22:11:14.920937 140452064818368 spec.py:321] Evaluating on the training split.
I0316 22:13:18.853336 140452064818368 spec.py:333] Evaluating on the validation split.
I0316 22:17:58.702164 140452064818368 spec.py:349] Evaluating on the test split.
I0316 22:22:50.172298 140452064818368 submission_runner.py:469] Time since start: 37215.43s, 	Step: 5144, 	{'train/loss': 0.1205214189890715, 'validation/loss': 0.1244104373564381, 'validation/num_examples': 83274637, 'test/loss': 0.12677206951342132, 'test/num_examples': 95000000, 'score': 5053.872928380966, 'total_duration': 37215.428938150406, 'accumulated_submission_time': 5053.872928380966, 'accumulated_eval_time': 32124.225046634674, 'accumulated_logging_time': 1.0621488094329834}
I0316 22:22:50.184428 140409862878976 logging_writer.py:48] [5144] accumulated_eval_time=32124.2, accumulated_logging_time=1.06215, accumulated_submission_time=5053.87, global_step=5144, preemption_count=0, score=5053.87, test/loss=0.126772, test/num_examples=95000000, total_duration=37215.4, train/loss=0.120521, validation/loss=0.12441, validation/num_examples=83274637
I0316 22:24:51.019189 140452064818368 spec.py:321] Evaluating on the training split.
I0316 22:26:54.029447 140452064818368 spec.py:333] Evaluating on the validation split.
I0316 22:31:40.520081 140452064818368 spec.py:349] Evaluating on the test split.
I0316 22:36:39.624569 140452064818368 submission_runner.py:469] Time since start: 38044.88s, 	Step: 5267, 	{'train/loss': 0.12360398883570572, 'validation/loss': 0.12418982825156212, 'validation/num_examples': 83274637, 'test/loss': 0.12649690432634855, 'test/num_examples': 95000000, 'score': 5173.824204921722, 'total_duration': 38044.88124847412, 'accumulated_submission_time': 5173.824204921722, 'accumulated_eval_time': 32832.83056497574, 'accumulated_logging_time': 1.0818254947662354}
I0316 22:36:39.635800 140409854486272 logging_writer.py:48] [5267] accumulated_eval_time=32832.8, accumulated_logging_time=1.08183, accumulated_submission_time=5173.82, global_step=5267, preemption_count=0, score=5173.82, test/loss=0.126497, test/num_examples=95000000, total_duration=38044.9, train/loss=0.123604, validation/loss=0.12419, validation/num_examples=83274637
I0316 22:38:41.464653 140452064818368 spec.py:321] Evaluating on the training split.
I0316 22:40:44.371864 140452064818368 spec.py:333] Evaluating on the validation split.
I0316 22:45:22.975110 140452064818368 spec.py:349] Evaluating on the test split.
I0316 22:50:21.777990 140452064818368 submission_runner.py:469] Time since start: 38867.03s, 	Step: 5398, 	{'train/loss': 0.12261379256125554, 'validation/loss': 0.12412845759045783, 'validation/num_examples': 83274637, 'test/loss': 0.12638324961290862, 'test/num_examples': 95000000, 'score': 5294.796817541122, 'total_duration': 38867.03460764885, 'accumulated_submission_time': 5294.796817541122, 'accumulated_eval_time': 33533.1440243721, 'accumulated_logging_time': 1.11871337890625}
I0316 22:50:21.790613 140409862878976 logging_writer.py:48] [5398] accumulated_eval_time=33533.1, accumulated_logging_time=1.11871, accumulated_submission_time=5294.8, global_step=5398, preemption_count=0, score=5294.8, test/loss=0.126383, test/num_examples=95000000, total_duration=38867, train/loss=0.122614, validation/loss=0.124128, validation/num_examples=83274637
I0316 22:51:58.839254 140409854486272 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.0110535, loss=0.134089
I0316 22:51:58.843048 140452064818368 submission.py:265] 5500) loss = 0.134, grad_norm = 0.011
I0316 22:52:22.487286 140452064818368 spec.py:321] Evaluating on the training split.
I0316 22:54:26.153131 140452064818368 spec.py:333] Evaluating on the validation split.
I0316 22:59:15.271456 140452064818368 spec.py:349] Evaluating on the test split.
I0316 23:04:09.437028 140452064818368 submission_runner.py:469] Time since start: 39694.69s, 	Step: 5519, 	{'train/loss': 0.12183506858871142, 'validation/loss': 0.12422058120575705, 'validation/num_examples': 83274637, 'test/loss': 0.12656770680927476, 'test/num_examples': 95000000, 'score': 5414.650695800781, 'total_duration': 39694.6936750412, 'accumulated_submission_time': 5414.650695800781, 'accumulated_eval_time': 34240.09390592575, 'accumulated_logging_time': 1.1386075019836426}
I0316 23:04:09.448276 140409862878976 logging_writer.py:48] [5519] accumulated_eval_time=34240.1, accumulated_logging_time=1.13861, accumulated_submission_time=5414.65, global_step=5519, preemption_count=0, score=5414.65, test/loss=0.126568, test/num_examples=95000000, total_duration=39694.7, train/loss=0.121835, validation/loss=0.124221, validation/num_examples=83274637
I0316 23:06:10.588368 140452064818368 spec.py:321] Evaluating on the training split.
I0316 23:08:16.443066 140452064818368 spec.py:333] Evaluating on the validation split.
I0316 23:12:51.129508 140452064818368 spec.py:349] Evaluating on the test split.
I0316 23:17:45.352388 140452064818368 submission_runner.py:469] Time since start: 40510.61s, 	Step: 5642, 	{'train/loss': 0.12353052542233056, 'validation/loss': 0.12394913650254544, 'validation/num_examples': 83274637, 'test/loss': 0.1262620313856426, 'test/num_examples': 95000000, 'score': 5534.940377950668, 'total_duration': 40510.60904979706, 'accumulated_submission_time': 5534.940377950668, 'accumulated_eval_time': 34934.85810256004, 'accumulated_logging_time': 1.1564440727233887}
I0316 23:17:45.403346 140409854486272 logging_writer.py:48] [5642] accumulated_eval_time=34934.9, accumulated_logging_time=1.15644, accumulated_submission_time=5534.94, global_step=5642, preemption_count=0, score=5534.94, test/loss=0.126262, test/num_examples=95000000, total_duration=40510.6, train/loss=0.123531, validation/loss=0.123949, validation/num_examples=83274637
I0316 23:19:45.949696 140452064818368 spec.py:321] Evaluating on the training split.
I0316 23:21:53.250556 140452064818368 spec.py:333] Evaluating on the validation split.
I0316 23:26:42.990449 140452064818368 spec.py:349] Evaluating on the test split.
I0316 23:31:35.943698 140452064818368 submission_runner.py:469] Time since start: 41341.20s, 	Step: 5764, 	{'train/loss': 0.12335673787634026, 'validation/loss': 0.12401565829250386, 'validation/num_examples': 83274637, 'test/loss': 0.1263882255181162, 'test/num_examples': 95000000, 'score': 5654.635269165039, 'total_duration': 41341.200328826904, 'accumulated_submission_time': 5654.635269165039, 'accumulated_eval_time': 35644.85218954086, 'accumulated_logging_time': 1.2147305011749268}
I0316 23:31:35.954676 140409862878976 logging_writer.py:48] [5764] accumulated_eval_time=35644.9, accumulated_logging_time=1.21473, accumulated_submission_time=5654.64, global_step=5764, preemption_count=0, score=5654.64, test/loss=0.126388, test/num_examples=95000000, total_duration=41341.2, train/loss=0.123357, validation/loss=0.124016, validation/num_examples=83274637
I0316 23:33:36.387703 140452064818368 spec.py:321] Evaluating on the training split.
I0316 23:35:38.894694 140452064818368 spec.py:333] Evaluating on the validation split.
I0316 23:40:29.614014 140452064818368 spec.py:349] Evaluating on the test split.
I0316 23:45:23.758267 140452064818368 submission_runner.py:469] Time since start: 42169.01s, 	Step: 5891, 	{'train/loss': 0.12267947374366131, 'validation/loss': 0.12411798511927655, 'validation/num_examples': 83274637, 'test/loss': 0.1265132602184095, 'test/num_examples': 95000000, 'score': 5774.206214666367, 'total_duration': 42169.01492643356, 'accumulated_submission_time': 5774.206214666367, 'accumulated_eval_time': 36352.22287893295, 'accumulated_logging_time': 1.2445228099822998}
I0316 23:45:23.770153 140409854486272 logging_writer.py:48] [5891] accumulated_eval_time=36352.2, accumulated_logging_time=1.24452, accumulated_submission_time=5774.21, global_step=5891, preemption_count=0, score=5774.21, test/loss=0.126513, test/num_examples=95000000, total_duration=42169, train/loss=0.122679, validation/loss=0.124118, validation/num_examples=83274637
I0316 23:47:02.295053 140409862878976 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.0169426, loss=0.129593
I0316 23:47:02.298628 140452064818368 submission.py:265] 6000) loss = 0.130, grad_norm = 0.017
I0316 23:47:24.709705 140452064818368 spec.py:321] Evaluating on the training split.
I0316 23:49:27.037760 140452064818368 spec.py:333] Evaluating on the validation split.
I0316 23:54:11.440110 140452064818368 spec.py:349] Evaluating on the test split.
I0316 23:59:17.074173 140452064818368 submission_runner.py:469] Time since start: 43002.33s, 	Step: 6020, 	{'train/loss': 0.12151943449482114, 'validation/loss': 0.1240022231342554, 'validation/num_examples': 83274637, 'test/loss': 0.12627774915988318, 'test/num_examples': 95000000, 'score': 5894.2629272937775, 'total_duration': 43002.33083629608, 'accumulated_submission_time': 5894.2629272937775, 'accumulated_eval_time': 37064.58747816086, 'accumulated_logging_time': 1.2636346817016602}
I0316 23:59:17.085747 140409854486272 logging_writer.py:48] [6020] accumulated_eval_time=37064.6, accumulated_logging_time=1.26363, accumulated_submission_time=5894.26, global_step=6020, preemption_count=0, score=5894.26, test/loss=0.126278, test/num_examples=95000000, total_duration=43002.3, train/loss=0.121519, validation/loss=0.124002, validation/num_examples=83274637
I0317 00:01:18.468114 140452064818368 spec.py:321] Evaluating on the training split.
I0317 00:03:20.457661 140452064818368 spec.py:333] Evaluating on the validation split.
I0317 00:07:55.204197 140452064818368 spec.py:349] Evaluating on the test split.
I0317 00:12:48.619932 140452064818368 submission_runner.py:469] Time since start: 43813.88s, 	Step: 6140, 	{'train/loss': 0.12356780287678666, 'validation/loss': 0.1239510134900639, 'validation/num_examples': 83274637, 'test/loss': 0.12623666398504158, 'test/num_examples': 95000000, 'score': 6014.791556835175, 'total_duration': 43813.87658715248, 'accumulated_submission_time': 6014.791556835175, 'accumulated_eval_time': 37754.73943400383, 'accumulated_logging_time': 1.2829911708831787}
I0317 00:12:48.647142 140409862878976 logging_writer.py:48] [6140] accumulated_eval_time=37754.7, accumulated_logging_time=1.28299, accumulated_submission_time=6014.79, global_step=6140, preemption_count=0, score=6014.79, test/loss=0.126237, test/num_examples=95000000, total_duration=43813.9, train/loss=0.123568, validation/loss=0.123951, validation/num_examples=83274637
I0317 00:14:49.313733 140452064818368 spec.py:321] Evaluating on the training split.
I0317 00:16:51.760090 140452064818368 spec.py:333] Evaluating on the validation split.
I0317 00:21:28.524852 140452064818368 spec.py:349] Evaluating on the test split.
I0317 00:26:12.220648 140452064818368 submission_runner.py:469] Time since start: 44617.48s, 	Step: 6260, 	{'train/loss': 0.12383372636131551, 'validation/loss': 0.12384292996194815, 'validation/num_examples': 83274637, 'test/loss': 0.126107113277877, 'test/num_examples': 95000000, 'score': 6134.624881029129, 'total_duration': 44617.477257966995, 'accumulated_submission_time': 6134.624881029129, 'accumulated_eval_time': 38437.646401405334, 'accumulated_logging_time': 1.317214012145996}
I0317 00:26:12.231802 140409854486272 logging_writer.py:48] [6260] accumulated_eval_time=38437.6, accumulated_logging_time=1.31721, accumulated_submission_time=6134.62, global_step=6260, preemption_count=0, score=6134.62, test/loss=0.126107, test/num_examples=95000000, total_duration=44617.5, train/loss=0.123834, validation/loss=0.123843, validation/num_examples=83274637
I0317 00:28:13.474867 140452064818368 spec.py:321] Evaluating on the training split.
I0317 00:30:16.395922 140452064818368 spec.py:333] Evaluating on the validation split.
I0317 00:34:58.821625 140452064818368 spec.py:349] Evaluating on the test split.
I0317 00:39:45.430675 140452064818368 submission_runner.py:469] Time since start: 45430.69s, 	Step: 6384, 	{'train/loss': 0.12441772325719283, 'validation/loss': 0.12399173681407082, 'validation/num_examples': 83274637, 'test/loss': 0.12638019873500622, 'test/num_examples': 95000000, 'score': 6255.017929553986, 'total_duration': 45430.68735051155, 'accumulated_submission_time': 6255.017929553986, 'accumulated_eval_time': 39129.602276325226, 'accumulated_logging_time': 1.3354060649871826}
I0317 00:39:45.442115 140409862878976 logging_writer.py:48] [6384] accumulated_eval_time=39129.6, accumulated_logging_time=1.33541, accumulated_submission_time=6255.02, global_step=6384, preemption_count=0, score=6255.02, test/loss=0.12638, test/num_examples=95000000, total_duration=45430.7, train/loss=0.124418, validation/loss=0.123992, validation/num_examples=83274637
I0317 00:41:39.706151 140409854486272 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.00499922, loss=0.115547
I0317 00:41:39.709301 140452064818368 submission.py:265] 6500) loss = 0.116, grad_norm = 0.005
I0317 00:41:46.045768 140452064818368 spec.py:321] Evaluating on the training split.
I0317 00:43:48.523271 140452064818368 spec.py:333] Evaluating on the validation split.
I0317 00:48:22.945689 140452064818368 spec.py:349] Evaluating on the test split.
I0317 00:53:12.103077 140452064818368 submission_runner.py:469] Time since start: 46237.36s, 	Step: 6506, 	{'train/loss': 0.11994014847210142, 'validation/loss': 0.1239905633795382, 'validation/num_examples': 83274637, 'test/loss': 0.1262926558745535, 'test/num_examples': 95000000, 'score': 6374.750734806061, 'total_duration': 46237.35972094536, 'accumulated_submission_time': 6374.750734806061, 'accumulated_eval_time': 39815.65978240967, 'accumulated_logging_time': 1.3647146224975586}
I0317 00:53:12.115331 140409862878976 logging_writer.py:48] [6506] accumulated_eval_time=39815.7, accumulated_logging_time=1.36471, accumulated_submission_time=6374.75, global_step=6506, preemption_count=0, score=6374.75, test/loss=0.126293, test/num_examples=95000000, total_duration=46237.4, train/loss=0.11994, validation/loss=0.123991, validation/num_examples=83274637
I0317 00:55:13.495684 140452064818368 spec.py:321] Evaluating on the training split.
I0317 00:57:19.687494 140452064818368 spec.py:333] Evaluating on the validation split.
I0317 01:01:57.016400 140452064818368 spec.py:349] Evaluating on the test split.
I0317 01:06:48.799810 140452064818368 submission_runner.py:469] Time since start: 47054.06s, 	Step: 6629, 	{'train/loss': 0.12258947015921917, 'validation/loss': 0.12399658988566598, 'validation/num_examples': 83274637, 'test/loss': 0.12630205042427464, 'test/num_examples': 95000000, 'score': 6495.27409529686, 'total_duration': 47054.056389808655, 'accumulated_submission_time': 6495.27409529686, 'accumulated_eval_time': 40510.96397399902, 'accumulated_logging_time': 1.3846375942230225}
I0317 01:06:48.811546 140409854486272 logging_writer.py:48] [6629] accumulated_eval_time=40511, accumulated_logging_time=1.38464, accumulated_submission_time=6495.27, global_step=6629, preemption_count=0, score=6495.27, test/loss=0.126302, test/num_examples=95000000, total_duration=47054.1, train/loss=0.122589, validation/loss=0.123997, validation/num_examples=83274637
I0317 01:08:50.008452 140452064818368 spec.py:321] Evaluating on the training split.
I0317 01:10:53.316861 140452064818368 spec.py:333] Evaluating on the validation split.
I0317 01:15:38.176981 140452064818368 spec.py:349] Evaluating on the test split.
I0317 01:20:31.625435 140452064818368 submission_runner.py:469] Time since start: 47876.88s, 	Step: 6755, 	{'train/loss': 0.12358214803904474, 'validation/loss': 0.12398135631054531, 'validation/num_examples': 83274637, 'test/loss': 0.12635037441028796, 'test/num_examples': 95000000, 'score': 6615.6436059474945, 'total_duration': 47876.8820772171, 'accumulated_submission_time': 6615.6436059474945, 'accumulated_eval_time': 41212.58106684685, 'accumulated_logging_time': 1.4031081199645996}
I0317 01:20:31.637908 140409862878976 logging_writer.py:48] [6755] accumulated_eval_time=41212.6, accumulated_logging_time=1.40311, accumulated_submission_time=6615.64, global_step=6755, preemption_count=0, score=6615.64, test/loss=0.12635, test/num_examples=95000000, total_duration=47876.9, train/loss=0.123582, validation/loss=0.123981, validation/num_examples=83274637
I0317 01:22:32.876854 140452064818368 spec.py:321] Evaluating on the training split.
I0317 01:24:36.778677 140452064818368 spec.py:333] Evaluating on the validation split.
I0317 01:29:14.926236 140452064818368 spec.py:349] Evaluating on the test split.
I0317 01:34:03.887027 140452064818368 submission_runner.py:469] Time since start: 48689.14s, 	Step: 6877, 	{'train/loss': 0.12128183931971055, 'validation/loss': 0.12383261117854659, 'validation/num_examples': 83274637, 'test/loss': 0.12614694684657046, 'test/num_examples': 95000000, 'score': 6735.993963003159, 'total_duration': 48689.14369106293, 'accumulated_submission_time': 6735.993963003159, 'accumulated_eval_time': 41903.59139561653, 'accumulated_logging_time': 1.4224517345428467}
I0317 01:34:03.898141 140409854486272 logging_writer.py:48] [6877] accumulated_eval_time=41903.6, accumulated_logging_time=1.42245, accumulated_submission_time=6735.99, global_step=6877, preemption_count=0, score=6735.99, test/loss=0.126147, test/num_examples=95000000, total_duration=48689.1, train/loss=0.121282, validation/loss=0.123833, validation/num_examples=83274637
I0317 01:36:04.587671 140452064818368 spec.py:321] Evaluating on the training split.
I0317 01:38:07.597738 140452064818368 spec.py:333] Evaluating on the validation split.
I0317 01:42:41.101794 140452064818368 spec.py:349] Evaluating on the test split.
I0317 01:47:38.651269 140452064818368 submission_runner.py:469] Time since start: 49503.91s, 	Step: 6989, 	{'train/loss': 0.12192150906600298, 'validation/loss': 0.12386427305691376, 'validation/num_examples': 83274637, 'test/loss': 0.12617997490491364, 'test/num_examples': 95000000, 'score': 6855.839759349823, 'total_duration': 49503.907885074615, 'accumulated_submission_time': 6855.839759349823, 'accumulated_eval_time': 42597.655114889145, 'accumulated_logging_time': 1.4529142379760742}
I0317 01:47:38.662745 140409862878976 logging_writer.py:48] [6989] accumulated_eval_time=42597.7, accumulated_logging_time=1.45291, accumulated_submission_time=6855.84, global_step=6989, preemption_count=0, score=6855.84, test/loss=0.12618, test/num_examples=95000000, total_duration=49503.9, train/loss=0.121922, validation/loss=0.123864, validation/num_examples=83274637
I0317 01:47:41.478817 140409854486272 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.0240692, loss=0.115801
I0317 01:47:41.482002 140452064818368 submission.py:265] 7000) loss = 0.116, grad_norm = 0.024
I0317 01:49:39.635310 140452064818368 spec.py:321] Evaluating on the training split.
I0317 01:51:42.325106 140452064818368 spec.py:333] Evaluating on the validation split.
I0317 01:56:31.186630 140452064818368 spec.py:349] Evaluating on the test split.
I0317 02:01:27.541467 140452064818368 submission_runner.py:469] Time since start: 50332.80s, 	Step: 7117, 	{'train/loss': 0.12223098075113342, 'validation/loss': 0.12388338028858349, 'validation/num_examples': 83274637, 'test/loss': 0.12626608852643464, 'test/num_examples': 95000000, 'score': 6975.976962566376, 'total_duration': 50332.798134088516, 'accumulated_submission_time': 6975.976962566376, 'accumulated_eval_time': 43305.561421871185, 'accumulated_logging_time': 1.4714164733886719}
I0317 02:01:27.553250 140409862878976 logging_writer.py:48] [7117] accumulated_eval_time=43305.6, accumulated_logging_time=1.47142, accumulated_submission_time=6975.98, global_step=7117, preemption_count=0, score=6975.98, test/loss=0.126266, test/num_examples=95000000, total_duration=50332.8, train/loss=0.122231, validation/loss=0.123883, validation/num_examples=83274637
I0317 02:03:28.253747 140452064818368 spec.py:321] Evaluating on the training split.
I0317 02:05:34.923011 140452064818368 spec.py:333] Evaluating on the validation split.
I0317 02:10:21.742046 140452064818368 spec.py:349] Evaluating on the test split.
I0317 02:15:26.033344 140452064818368 submission_runner.py:469] Time since start: 51171.29s, 	Step: 7239, 	{'train/loss': 0.12214665635452833, 'validation/loss': 0.12386223695397972, 'validation/num_examples': 83274637, 'test/loss': 0.12615486269394724, 'test/num_examples': 95000000, 'score': 7095.832497119904, 'total_duration': 51171.29000234604, 'accumulated_submission_time': 7095.832497119904, 'accumulated_eval_time': 44023.34115815163, 'accumulated_logging_time': 1.4900617599487305}
I0317 02:15:26.045456 140409854486272 logging_writer.py:48] [7239] accumulated_eval_time=44023.3, accumulated_logging_time=1.49006, accumulated_submission_time=7095.83, global_step=7239, preemption_count=0, score=7095.83, test/loss=0.126155, test/num_examples=95000000, total_duration=51171.3, train/loss=0.122147, validation/loss=0.123862, validation/num_examples=83274637
I0317 02:17:27.212045 140452064818368 spec.py:321] Evaluating on the training split.
I0317 02:19:36.756283 140452064818368 spec.py:333] Evaluating on the validation split.
I0317 02:24:08.844829 140452064818368 spec.py:349] Evaluating on the test split.
I0317 02:29:01.024889 140452064818368 submission_runner.py:469] Time since start: 51986.28s, 	Step: 7367, 	{'train/loss': 0.1229977104896685, 'validation/loss': 0.12385124847548573, 'validation/num_examples': 83274637, 'test/loss': 0.1261964888176366, 'test/num_examples': 95000000, 'score': 7216.176561355591, 'total_duration': 51986.28154206276, 'accumulated_submission_time': 7216.176561355591, 'accumulated_eval_time': 44717.15412783623, 'accumulated_logging_time': 1.510237216949463}
I0317 02:29:01.037146 140409862878976 logging_writer.py:48] [7367] accumulated_eval_time=44717.2, accumulated_logging_time=1.51024, accumulated_submission_time=7216.18, global_step=7367, preemption_count=0, score=7216.18, test/loss=0.126196, test/num_examples=95000000, total_duration=51986.3, train/loss=0.122998, validation/loss=0.123851, validation/num_examples=83274637
I0317 02:31:02.760290 140452064818368 spec.py:321] Evaluating on the training split.
I0317 02:33:05.882507 140452064818368 spec.py:333] Evaluating on the validation split.
I0317 02:37:30.951449 140452064818368 spec.py:349] Evaluating on the test split.
I0317 02:42:13.645719 140452064818368 submission_runner.py:469] Time since start: 52778.90s, 	Step: 7489, 	{'train/loss': 0.1236186623881576, 'validation/loss': 0.12387920620097209, 'validation/num_examples': 83274637, 'test/loss': 0.1261783010511298, 'test/num_examples': 95000000, 'score': 7337.0277099609375, 'total_duration': 52778.90236186981, 'accumulated_submission_time': 7337.0277099609375, 'accumulated_eval_time': 45388.039620399475, 'accumulated_logging_time': 1.5504047870635986}
I0317 02:42:13.658066 140409854486272 logging_writer.py:48] [7489] accumulated_eval_time=45388, accumulated_logging_time=1.5504, accumulated_submission_time=7337.03, global_step=7489, preemption_count=0, score=7337.03, test/loss=0.126178, test/num_examples=95000000, total_duration=52778.9, train/loss=0.123619, validation/loss=0.123879, validation/num_examples=83274637
I0317 02:42:16.491541 140409862878976 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.0142487, loss=0.116275
I0317 02:42:16.495368 140452064818368 submission.py:265] 7500) loss = 0.116, grad_norm = 0.014
I0317 02:44:14.179155 140452064818368 spec.py:321] Evaluating on the training split.
I0317 02:46:17.710664 140452064818368 spec.py:333] Evaluating on the validation split.
I0317 02:50:55.682905 140452064818368 spec.py:349] Evaluating on the test split.
I0317 02:55:46.732954 140452064818368 submission_runner.py:469] Time since start: 53591.99s, 	Step: 7610, 	{'train/loss': 0.12245437968558955, 'validation/loss': 0.12388309650992117, 'validation/num_examples': 83274637, 'test/loss': 0.12625280610030326, 'test/num_examples': 95000000, 'score': 7456.7148225307465, 'total_duration': 53591.9894695282, 'accumulated_submission_time': 7456.7148225307465, 'accumulated_eval_time': 46080.59331250191, 'accumulated_logging_time': 1.5701355934143066}
I0317 02:55:46.744729 140409854486272 logging_writer.py:48] [7610] accumulated_eval_time=46080.6, accumulated_logging_time=1.57014, accumulated_submission_time=7456.71, global_step=7610, preemption_count=0, score=7456.71, test/loss=0.126253, test/num_examples=95000000, total_duration=53592, train/loss=0.122454, validation/loss=0.123883, validation/num_examples=83274637
I0317 02:57:48.470223 140452064818368 spec.py:321] Evaluating on the training split.
I0317 02:59:51.500467 140452064818368 spec.py:333] Evaluating on the validation split.
I0317 03:04:28.876251 140452064818368 spec.py:349] Evaluating on the test split.
I0317 03:09:20.512601 140452064818368 submission_runner.py:469] Time since start: 54405.77s, 	Step: 7734, 	{'train/loss': 0.12194246903772589, 'validation/loss': 0.12391655224541505, 'validation/num_examples': 83274637, 'test/loss': 0.12615788330760755, 'test/num_examples': 95000000, 'score': 7577.641162157059, 'total_duration': 54405.769267082214, 'accumulated_submission_time': 7577.641162157059, 'accumulated_eval_time': 46772.640810489655, 'accumulated_logging_time': 1.5887231826782227}
I0317 03:09:20.524687 140409862878976 logging_writer.py:48] [7734] accumulated_eval_time=46772.6, accumulated_logging_time=1.58872, accumulated_submission_time=7577.64, global_step=7734, preemption_count=0, score=7577.64, test/loss=0.126158, test/num_examples=95000000, total_duration=54405.8, train/loss=0.121942, validation/loss=0.123917, validation/num_examples=83274637
I0317 03:11:21.812112 140452064818368 spec.py:321] Evaluating on the training split.
I0317 03:13:25.648305 140452064818368 spec.py:333] Evaluating on the validation split.
I0317 03:18:03.889569 140452064818368 spec.py:349] Evaluating on the test split.
I0317 03:23:15.296504 140452064818368 submission_runner.py:469] Time since start: 55240.55s, 	Step: 7856, 	{'train/loss': 0.12339389920249338, 'validation/loss': 0.12383321178345577, 'validation/num_examples': 83274637, 'test/loss': 0.12613064212228875, 'test/num_examples': 95000000, 'score': 7698.085061073303, 'total_duration': 55240.553169727325, 'accumulated_submission_time': 7698.085061073303, 'accumulated_eval_time': 47486.12541890144, 'accumulated_logging_time': 1.6081855297088623}
I0317 03:23:15.308595 140409854486272 logging_writer.py:48] [7856] accumulated_eval_time=47486.1, accumulated_logging_time=1.60819, accumulated_submission_time=7698.09, global_step=7856, preemption_count=0, score=7698.09, test/loss=0.126131, test/num_examples=95000000, total_duration=55240.6, train/loss=0.123394, validation/loss=0.123833, validation/num_examples=83274637
I0317 03:25:16.033761 140409862878976 logging_writer.py:48] [7976] global_step=7976, preemption_count=0, score=7818.34
I0317 03:25:18.948812 140452064818368 submission_runner.py:646] Tuning trial 4/5
I0317 03:25:18.949061 140452064818368 submission_runner.py:647] Hyperparameters: Hyperparameters(learning_rate=0.0012, one_minus_beta1=0.016610699316537858, one_minus_beta2=0.005888216674053163, epsilon=1e-08, one_minus_momentum=0.5, use_momentum=True, weight_decay=0.00040349948255455174, max_preconditioner_dim=1024, precondition_frequency=100, start_preconditioning_step=-1, inv_root_override=2, exponent_multiplier=1.0, grafting_type='ADAM', grafting_epsilon=1e-08, use_normalized_grafting=False, communication_dtype='FP32', communicate_params=True, use_cosine_decay=True, warmup_factor=0.02, label_smoothing=0.1, dropout_rate=0.1, use_nadam=True, step_hint_factor=1.0)
I0317 03:25:18.950211 140452064818368 submission_runner.py:648] Metrics: {'eval_results': [(1, {'train/loss': 1.5559687839643987, 'validation/loss': 1.5543951495709674, 'validation/num_examples': 83274637, 'test/loss': 1.5525968260523746, 'test/num_examples': 95000000, 'score': 4.20938777923584, 'total_duration': 881.6670415401459, 'accumulated_submission_time': 4.20938777923584, 'accumulated_eval_time': 877.0379030704498, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (123, {'train/loss': 0.13559364615543795, 'validation/loss': 0.1373852469463777, 'validation/num_examples': 83274637, 'test/loss': 0.14072986464502435, 'test/num_examples': 95000000, 'score': 124.32689070701599, 'total_duration': 1893.9639415740967, 'accumulated_submission_time': 124.32689070701599, 'accumulated_eval_time': 1768.2217803001404, 'accumulated_logging_time': 0.0699472427368164, 'global_step': 123, 'preemption_count': 0}), (243, {'train/loss': 0.12789749141411313, 'validation/loss': 0.12938619681209904, 'validation/num_examples': 83274637, 'test/loss': 0.13233890309673108, 'test/num_examples': 95000000, 'score': 244.0833420753479, 'total_duration': 2928.3195593357086, 'accumulated_submission_time': 244.0833420753479, 'accumulated_eval_time': 2681.912724018097, 'accumulated_logging_time': 0.08669519424438477, 'global_step': 243, 'preemption_count': 0}), (363, {'train/loss': 0.12774561294508405, 'validation/loss': 0.12791970920740173, 'validation/num_examples': 83274637, 'test/loss': 0.13051705193533647, 'test/num_examples': 95000000, 'score': 364.39417147636414, 'total_duration': 3944.585338830948, 'accumulated_submission_time': 364.39417147636414, 'accumulated_eval_time': 3576.994199991226, 'accumulated_logging_time': 0.10437130928039551, 'global_step': 363, 'preemption_count': 0}), (487, {'train/loss': 0.125664055511244, 'validation/loss': 0.12727701761173288, 'validation/num_examples': 83274637, 'test/loss': 0.1297727832484195, 'test/num_examples': 95000000, 'score': 485.18765592575073, 'total_duration': 4950.868125915527, 'accumulated_submission_time': 485.18765592575073, 'accumulated_eval_time': 4461.582173585892, 'accumulated_logging_time': 0.12080049514770508, 'global_step': 487, 'preemption_count': 0}), (611, {'train/loss': 0.12389275970780453, 'validation/loss': 0.12656235711494032, 'validation/num_examples': 83274637, 'test/loss': 0.12906700013198852, 'test/num_examples': 95000000, 'score': 605.5009610652924, 'total_duration': 5938.131875276566, 'accumulated_submission_time': 605.5009610652924, 'accumulated_eval_time': 5327.607988834381, 'accumulated_logging_time': 0.17924237251281738, 'global_step': 611, 'preemption_count': 0}), (736, {'train/loss': 0.1250022627300957, 'validation/loss': 0.12647404857221686, 'validation/num_examples': 83274637, 'test/loss': 0.12886777501521862, 'test/num_examples': 95000000, 'score': 725.7015452384949, 'total_duration': 6949.959118127823, 'accumulated_submission_time': 725.7015452384949, 'accumulated_eval_time': 6218.363099813461, 'accumulated_logging_time': 0.19576168060302734, 'global_step': 736, 'preemption_count': 0}), (858, {'train/loss': 0.1242563677986964, 'validation/loss': 0.12606761503658345, 'validation/num_examples': 83274637, 'test/loss': 0.12851110228110865, 'test/num_examples': 95000000, 'score': 845.9896948337555, 'total_duration': 7942.702048063278, 'accumulated_submission_time': 845.9896948337555, 'accumulated_eval_time': 7089.972631692886, 'accumulated_logging_time': 0.21303224563598633, 'global_step': 858, 'preemption_count': 0}), (980, {'train/loss': 0.12546846859210953, 'validation/loss': 0.1259920610072411, 'validation/num_examples': 83274637, 'test/loss': 0.12834299912908453, 'test/num_examples': 95000000, 'score': 965.8628640174866, 'total_duration': 8900.851387739182, 'accumulated_submission_time': 965.8628640174866, 'accumulated_eval_time': 7927.3923354148865, 'accumulated_logging_time': 0.22918033599853516, 'global_step': 980, 'preemption_count': 0}), (1101, {'train/loss': 0.12481992780792518, 'validation/loss': 0.12591176061229148, 'validation/num_examples': 83274637, 'test/loss': 0.12831861611436543, 'test/num_examples': 95000000, 'score': 1086.090036869049, 'total_duration': 9884.677122831345, 'accumulated_submission_time': 1086.090036869049, 'accumulated_eval_time': 8790.061691522598, 'accumulated_logging_time': 0.24562978744506836, 'global_step': 1101, 'preemption_count': 0}), (1223, {'train/loss': 0.12416131637041196, 'validation/loss': 0.1258127702817028, 'validation/num_examples': 83274637, 'test/loss': 0.12815021689762315, 'test/num_examples': 95000000, 'score': 1206.2974948883057, 'total_duration': 10829.661939620972, 'accumulated_submission_time': 1206.2974948883057, 'accumulated_eval_time': 9613.93716931343, 'accumulated_logging_time': 0.2991206645965576, 'global_step': 1223, 'preemption_count': 0}), (1350, {'train/loss': 0.12446779496196424, 'validation/loss': 0.12565363775852995, 'validation/num_examples': 83274637, 'test/loss': 0.12800013631788554, 'test/num_examples': 95000000, 'score': 1326.115199804306, 'total_duration': 11705.92903470993, 'accumulated_submission_time': 1326.115199804306, 'accumulated_eval_time': 10369.500856637955, 'accumulated_logging_time': 0.31755805015563965, 'global_step': 1350, 'preemption_count': 0}), (1471, {'train/loss': 0.12350990747859415, 'validation/loss': 0.12566981729135396, 'validation/num_examples': 83274637, 'test/loss': 0.1281688987270556, 'test/num_examples': 95000000, 'score': 1446.286691904068, 'total_duration': 12532.8699426651, 'accumulated_submission_time': 1446.286691904068, 'accumulated_eval_time': 11075.392145633698, 'accumulated_logging_time': 0.3357665538787842, 'global_step': 1471, 'preemption_count': 0}), (1598, {'train/loss': 0.12554739951582072, 'validation/loss': 0.12540266545437773, 'validation/num_examples': 83274637, 'test/loss': 0.12770052270989668, 'test/num_examples': 95000000, 'score': 1567.140037059784, 'total_duration': 13349.011248588562, 'accumulated_submission_time': 1567.140037059784, 'accumulated_eval_time': 11769.843456029892, 'accumulated_logging_time': 0.35332560539245605, 'global_step': 1598, 'preemption_count': 0}), (1719, {'train/loss': 0.12291021959072142, 'validation/loss': 0.12523784068114227, 'validation/num_examples': 83274637, 'test/loss': 0.12746143530413978, 'test/num_examples': 95000000, 'score': 1687.8334410190582, 'total_duration': 14165.067366600037, 'accumulated_submission_time': 1687.8334410190582, 'accumulated_eval_time': 12464.314994096756, 'accumulated_logging_time': 0.3713796138763428, 'global_step': 1719, 'preemption_count': 0}), (1837, {'train/loss': 0.12418532343900127, 'validation/loss': 0.1251253183451596, 'validation/num_examples': 83274637, 'test/loss': 0.12757429894690261, 'test/num_examples': 95000000, 'score': 1808.9027123451233, 'total_duration': 14999.980171203613, 'accumulated_submission_time': 1808.9027123451233, 'accumulated_eval_time': 13177.352951288223, 'accumulated_logging_time': 0.38906407356262207, 'global_step': 1837, 'preemption_count': 0}), (1960, {'train/loss': 0.1272784827857698, 'validation/loss': 0.12508060406532362, 'validation/num_examples': 83274637, 'test/loss': 0.1274504007788407, 'test/num_examples': 95000000, 'score': 1928.925883769989, 'total_duration': 15831.073868989944, 'accumulated_submission_time': 1928.925883769989, 'accumulated_eval_time': 13887.61410689354, 'accumulated_logging_time': 0.40613317489624023, 'global_step': 1960, 'preemption_count': 0}), (2082, {'train/loss': 0.12435758199888869, 'validation/loss': 0.12518324496061767, 'validation/num_examples': 83274637, 'test/loss': 0.1274853324144062, 'test/num_examples': 95000000, 'score': 2049.305502653122, 'total_duration': 16637.916900634766, 'accumulated_submission_time': 2049.305502653122, 'accumulated_eval_time': 14573.1947016716, 'accumulated_logging_time': 0.42369914054870605, 'global_step': 2082, 'preemption_count': 0}), (2209, {'train/loss': 0.1233498434523425, 'validation/loss': 0.12511047738694356, 'validation/num_examples': 83274637, 'test/loss': 0.12729697007944207, 'test/num_examples': 95000000, 'score': 2169.647258043289, 'total_duration': 17450.68799304962, 'accumulated_submission_time': 2169.647258043289, 'accumulated_eval_time': 15264.68991446495, 'accumulated_logging_time': 0.44046449661254883, 'global_step': 2209, 'preemption_count': 0}), (2331, {'train/loss': 0.12498596727282993, 'validation/loss': 0.12525216967035144, 'validation/num_examples': 83274637, 'test/loss': 0.12758739206695557, 'test/num_examples': 95000000, 'score': 2289.9520835876465, 'total_duration': 18297.37419295311, 'accumulated_submission_time': 2289.9520835876465, 'accumulated_eval_time': 15990.179099321365, 'accumulated_logging_time': 0.4589364528656006, 'global_step': 2331, 'preemption_count': 0}), (2455, {'train/loss': 0.12205388384070318, 'validation/loss': 0.12508738634270553, 'validation/num_examples': 83274637, 'test/loss': 0.12728376804954128, 'test/num_examples': 95000000, 'score': 2411.405754804611, 'total_duration': 19131.480714559555, 'accumulated_submission_time': 2411.405754804611, 'accumulated_eval_time': 16702.017133951187, 'accumulated_logging_time': 0.4765923023223877, 'global_step': 2455, 'preemption_count': 0}), (2576, {'train/loss': 0.12277646065547992, 'validation/loss': 0.12477494465732591, 'validation/num_examples': 83274637, 'test/loss': 0.12702831757860686, 'test/num_examples': 95000000, 'score': 2532.3585402965546, 'total_duration': 19960.977091550827, 'accumulated_submission_time': 2532.3585402965546, 'accumulated_eval_time': 17409.683349847794, 'accumulated_logging_time': 0.494856595993042, 'global_step': 2576, 'preemption_count': 0}), (2696, {'train/loss': 0.12532034974364584, 'validation/loss': 0.12490441414679739, 'validation/num_examples': 83274637, 'test/loss': 0.12728110285451788, 'test/num_examples': 95000000, 'score': 2652.4453897476196, 'total_duration': 20779.95171737671, 'accumulated_submission_time': 2652.4453897476196, 'accumulated_eval_time': 18107.705209493637, 'accumulated_logging_time': 0.5119156837463379, 'global_step': 2696, 'preemption_count': 0}), (2819, {'train/loss': 0.12393260272857393, 'validation/loss': 0.12474691330412799, 'validation/num_examples': 83274637, 'test/loss': 0.12710381994753386, 'test/num_examples': 95000000, 'score': 2772.788913488388, 'total_duration': 21604.029458761215, 'accumulated_submission_time': 2772.788913488388, 'accumulated_eval_time': 18810.561002492905, 'accumulated_logging_time': 0.5638537406921387, 'global_step': 2819, 'preemption_count': 0}), (2940, {'train/loss': 0.12354012859895269, 'validation/loss': 0.124748586791193, 'validation/num_examples': 83274637, 'test/loss': 0.12714746886203163, 'test/num_examples': 95000000, 'score': 2892.5249168872833, 'total_duration': 22411.199244976044, 'accumulated_submission_time': 2892.5249168872833, 'accumulated_eval_time': 19497.097414016724, 'accumulated_logging_time': 0.5820839405059814, 'global_step': 2940, 'preemption_count': 0}), (3064, {'train/loss': 0.12173183175253116, 'validation/loss': 0.12473685723273148, 'validation/num_examples': 83274637, 'test/loss': 0.1271002791008397, 'test/num_examples': 95000000, 'score': 3012.522522211075, 'total_duration': 23218.72576212883, 'accumulated_submission_time': 3012.522522211075, 'accumulated_eval_time': 20183.74552321434, 'accumulated_logging_time': 0.5997507572174072, 'global_step': 3064, 'preemption_count': 0}), (3188, {'train/loss': 0.12475704005351405, 'validation/loss': 0.12467825475592674, 'validation/num_examples': 83274637, 'test/loss': 0.12697045888334577, 'test/num_examples': 95000000, 'score': 3132.9987287521362, 'total_duration': 24034.895886659622, 'accumulated_submission_time': 3132.9987287521362, 'accumulated_eval_time': 20878.48043036461, 'accumulated_logging_time': 0.6780641078948975, 'global_step': 3188, 'preemption_count': 0}), (3313, {'train/loss': 0.1226004052599597, 'validation/loss': 0.12472303841509007, 'validation/num_examples': 83274637, 'test/loss': 0.12707351653482538, 'test/num_examples': 95000000, 'score': 3253.3166873455048, 'total_duration': 24863.836413621902, 'accumulated_submission_time': 3253.3166873455048, 'accumulated_eval_time': 21586.216495513916, 'accumulated_logging_time': 0.6978273391723633, 'global_step': 3313, 'preemption_count': 0}), (3436, {'train/loss': 0.1237160834589522, 'validation/loss': 0.12472562965026103, 'validation/num_examples': 83274637, 'test/loss': 0.12713047632639032, 'test/num_examples': 95000000, 'score': 3373.5131628513336, 'total_duration': 25674.697743415833, 'accumulated_submission_time': 3373.5131628513336, 'accumulated_eval_time': 22276.034877300262, 'accumulated_logging_time': 0.7151679992675781, 'global_step': 3436, 'preemption_count': 0}), (3559, {'train/loss': 0.1260238999328458, 'validation/loss': 0.12473637068531697, 'validation/num_examples': 83274637, 'test/loss': 0.12709235642443206, 'test/num_examples': 95000000, 'score': 3493.560968399048, 'total_duration': 26507.39527463913, 'accumulated_submission_time': 3493.560968399048, 'accumulated_eval_time': 22987.778450489044, 'accumulated_logging_time': 0.7329566478729248, 'global_step': 3559, 'preemption_count': 0}), (3689, {'train/loss': 0.12401712227610169, 'validation/loss': 0.1248227517640353, 'validation/num_examples': 83274637, 'test/loss': 0.12747303791319195, 'test/num_examples': 95000000, 'score': 3613.8832693099976, 'total_duration': 27323.774816036224, 'accumulated_submission_time': 3613.8832693099976, 'accumulated_eval_time': 23682.981556653976, 'accumulated_logging_time': 0.7501275539398193, 'global_step': 3689, 'preemption_count': 0}), (3813, {'train/loss': 0.12498849236151523, 'validation/loss': 0.12459954110834236, 'validation/num_examples': 83274637, 'test/loss': 0.1269656788077505, 'test/num_examples': 95000000, 'score': 3733.454436302185, 'total_duration': 28154.08536338806, 'accumulated_submission_time': 3733.454436302185, 'accumulated_eval_time': 24392.808722019196, 'accumulated_logging_time': 0.7835752964019775, 'global_step': 3813, 'preemption_count': 0}), (3935, {'train/loss': 0.12096645933277825, 'validation/loss': 0.12456880227978906, 'validation/num_examples': 83274637, 'test/loss': 0.12686842103038587, 'test/num_examples': 95000000, 'score': 3853.357095003128, 'total_duration': 28965.94380044937, 'accumulated_submission_time': 3853.357095003128, 'accumulated_eval_time': 25083.908751249313, 'accumulated_logging_time': 0.8017215728759766, 'global_step': 3935, 'preemption_count': 0}), (4054, {'train/loss': 0.12358228968403802, 'validation/loss': 0.12458906232134165, 'validation/num_examples': 83274637, 'test/loss': 0.12680416487446836, 'test/num_examples': 95000000, 'score': 3973.705192089081, 'total_duration': 29801.459738492966, 'accumulated_submission_time': 3973.705192089081, 'accumulated_eval_time': 25798.243510961533, 'accumulated_logging_time': 0.8195385932922363, 'global_step': 4054, 'preemption_count': 0}), (4175, {'train/loss': 0.12271017263085163, 'validation/loss': 0.12455393548087747, 'validation/num_examples': 83274637, 'test/loss': 0.12687363356628417, 'test/num_examples': 95000000, 'score': 4093.462232351303, 'total_duration': 30637.46075129509, 'accumulated_submission_time': 4093.462232351303, 'accumulated_eval_time': 26513.629149198532, 'accumulated_logging_time': 0.8384106159210205, 'global_step': 4175, 'preemption_count': 0}), (4299, {'train/loss': 0.12358242661676445, 'validation/loss': 0.12456168799737823, 'validation/num_examples': 83274637, 'test/loss': 0.1269886673636587, 'test/num_examples': 95000000, 'score': 4213.238229036331, 'total_duration': 31457.948748111725, 'accumulated_submission_time': 4213.238229036331, 'accumulated_eval_time': 27213.448467731476, 'accumulated_logging_time': 0.8781960010528564, 'global_step': 4299, 'preemption_count': 0}), (4420, {'train/loss': 0.1226098140477295, 'validation/loss': 0.12458285201746563, 'validation/num_examples': 83274637, 'test/loss': 0.1269085418898733, 'test/num_examples': 95000000, 'score': 4333.048588752747, 'total_duration': 32278.155025959015, 'accumulated_submission_time': 4333.048588752747, 'accumulated_eval_time': 27912.932423830032, 'accumulated_logging_time': 0.8970916271209717, 'global_step': 4420, 'preemption_count': 0}), (4544, {'train/loss': 0.1207330059746587, 'validation/loss': 0.12442978587903422, 'validation/num_examples': 83274637, 'test/loss': 0.12673042956237793, 'test/num_examples': 95000000, 'score': 4452.692674160004, 'total_duration': 33097.81225705147, 'accumulated_submission_time': 4452.692674160004, 'accumulated_eval_time': 28612.096150636673, 'accumulated_logging_time': 0.9151108264923096, 'global_step': 4544, 'preemption_count': 0}), (4663, {'train/loss': 0.12151488173893556, 'validation/loss': 0.12448371707163437, 'validation/num_examples': 83274637, 'test/loss': 0.12680591815229716, 'test/num_examples': 95000000, 'score': 4572.632852554321, 'total_duration': 33932.28729176521, 'accumulated_submission_time': 4572.632852554321, 'accumulated_eval_time': 29325.760493040085, 'accumulated_logging_time': 0.933140754699707, 'global_step': 4663, 'preemption_count': 0}), (4777, {'train/loss': 0.12454489025776719, 'validation/loss': 0.12450191213574323, 'validation/num_examples': 83274637, 'test/loss': 0.12682452589484766, 'test/num_examples': 95000000, 'score': 4692.687784671783, 'total_duration': 34743.00843644142, 'accumulated_submission_time': 4692.687784671783, 'accumulated_eval_time': 30015.550897598267, 'accumulated_logging_time': 0.952477216720581, 'global_step': 4777, 'preemption_count': 0}), (4899, {'train/loss': 0.12467550781067917, 'validation/loss': 0.12439532669714809, 'validation/num_examples': 83274637, 'test/loss': 0.12677003348890606, 'test/num_examples': 95000000, 'score': 4813.844833612442, 'total_duration': 35568.30336332321, 'accumulated_submission_time': 4813.844833612442, 'accumulated_eval_time': 30718.804966926575, 'accumulated_logging_time': 1.0238473415374756, 'global_step': 4899, 'preemption_count': 0}), (5023, {'train/loss': 0.12479341067553311, 'validation/loss': 0.12436998962142615, 'validation/num_examples': 83274637, 'test/loss': 0.12670422618440327, 'test/num_examples': 95000000, 'score': 4934.32159113884, 'total_duration': 36399.763825416565, 'accumulated_submission_time': 4934.32159113884, 'accumulated_eval_time': 31428.97360754013, 'accumulated_logging_time': 1.0433995723724365, 'global_step': 5023, 'preemption_count': 0}), (5144, {'train/loss': 0.1205214189890715, 'validation/loss': 0.1244104373564381, 'validation/num_examples': 83274637, 'test/loss': 0.12677206951342132, 'test/num_examples': 95000000, 'score': 5053.872928380966, 'total_duration': 37215.428938150406, 'accumulated_submission_time': 5053.872928380966, 'accumulated_eval_time': 32124.225046634674, 'accumulated_logging_time': 1.0621488094329834, 'global_step': 5144, 'preemption_count': 0}), (5267, {'train/loss': 0.12360398883570572, 'validation/loss': 0.12418982825156212, 'validation/num_examples': 83274637, 'test/loss': 0.12649690432634855, 'test/num_examples': 95000000, 'score': 5173.824204921722, 'total_duration': 38044.88124847412, 'accumulated_submission_time': 5173.824204921722, 'accumulated_eval_time': 32832.83056497574, 'accumulated_logging_time': 1.0818254947662354, 'global_step': 5267, 'preemption_count': 0}), (5398, {'train/loss': 0.12261379256125554, 'validation/loss': 0.12412845759045783, 'validation/num_examples': 83274637, 'test/loss': 0.12638324961290862, 'test/num_examples': 95000000, 'score': 5294.796817541122, 'total_duration': 38867.03460764885, 'accumulated_submission_time': 5294.796817541122, 'accumulated_eval_time': 33533.1440243721, 'accumulated_logging_time': 1.11871337890625, 'global_step': 5398, 'preemption_count': 0}), (5519, {'train/loss': 0.12183506858871142, 'validation/loss': 0.12422058120575705, 'validation/num_examples': 83274637, 'test/loss': 0.12656770680927476, 'test/num_examples': 95000000, 'score': 5414.650695800781, 'total_duration': 39694.6936750412, 'accumulated_submission_time': 5414.650695800781, 'accumulated_eval_time': 34240.09390592575, 'accumulated_logging_time': 1.1386075019836426, 'global_step': 5519, 'preemption_count': 0}), (5642, {'train/loss': 0.12353052542233056, 'validation/loss': 0.12394913650254544, 'validation/num_examples': 83274637, 'test/loss': 0.1262620313856426, 'test/num_examples': 95000000, 'score': 5534.940377950668, 'total_duration': 40510.60904979706, 'accumulated_submission_time': 5534.940377950668, 'accumulated_eval_time': 34934.85810256004, 'accumulated_logging_time': 1.1564440727233887, 'global_step': 5642, 'preemption_count': 0}), (5764, {'train/loss': 0.12335673787634026, 'validation/loss': 0.12401565829250386, 'validation/num_examples': 83274637, 'test/loss': 0.1263882255181162, 'test/num_examples': 95000000, 'score': 5654.635269165039, 'total_duration': 41341.200328826904, 'accumulated_submission_time': 5654.635269165039, 'accumulated_eval_time': 35644.85218954086, 'accumulated_logging_time': 1.2147305011749268, 'global_step': 5764, 'preemption_count': 0}), (5891, {'train/loss': 0.12267947374366131, 'validation/loss': 0.12411798511927655, 'validation/num_examples': 83274637, 'test/loss': 0.1265132602184095, 'test/num_examples': 95000000, 'score': 5774.206214666367, 'total_duration': 42169.01492643356, 'accumulated_submission_time': 5774.206214666367, 'accumulated_eval_time': 36352.22287893295, 'accumulated_logging_time': 1.2445228099822998, 'global_step': 5891, 'preemption_count': 0}), (6020, {'train/loss': 0.12151943449482114, 'validation/loss': 0.1240022231342554, 'validation/num_examples': 83274637, 'test/loss': 0.12627774915988318, 'test/num_examples': 95000000, 'score': 5894.2629272937775, 'total_duration': 43002.33083629608, 'accumulated_submission_time': 5894.2629272937775, 'accumulated_eval_time': 37064.58747816086, 'accumulated_logging_time': 1.2636346817016602, 'global_step': 6020, 'preemption_count': 0}), (6140, {'train/loss': 0.12356780287678666, 'validation/loss': 0.1239510134900639, 'validation/num_examples': 83274637, 'test/loss': 0.12623666398504158, 'test/num_examples': 95000000, 'score': 6014.791556835175, 'total_duration': 43813.87658715248, 'accumulated_submission_time': 6014.791556835175, 'accumulated_eval_time': 37754.73943400383, 'accumulated_logging_time': 1.2829911708831787, 'global_step': 6140, 'preemption_count': 0}), (6260, {'train/loss': 0.12383372636131551, 'validation/loss': 0.12384292996194815, 'validation/num_examples': 83274637, 'test/loss': 0.126107113277877, 'test/num_examples': 95000000, 'score': 6134.624881029129, 'total_duration': 44617.477257966995, 'accumulated_submission_time': 6134.624881029129, 'accumulated_eval_time': 38437.646401405334, 'accumulated_logging_time': 1.317214012145996, 'global_step': 6260, 'preemption_count': 0}), (6384, {'train/loss': 0.12441772325719283, 'validation/loss': 0.12399173681407082, 'validation/num_examples': 83274637, 'test/loss': 0.12638019873500622, 'test/num_examples': 95000000, 'score': 6255.017929553986, 'total_duration': 45430.68735051155, 'accumulated_submission_time': 6255.017929553986, 'accumulated_eval_time': 39129.602276325226, 'accumulated_logging_time': 1.3354060649871826, 'global_step': 6384, 'preemption_count': 0}), (6506, {'train/loss': 0.11994014847210142, 'validation/loss': 0.1239905633795382, 'validation/num_examples': 83274637, 'test/loss': 0.1262926558745535, 'test/num_examples': 95000000, 'score': 6374.750734806061, 'total_duration': 46237.35972094536, 'accumulated_submission_time': 6374.750734806061, 'accumulated_eval_time': 39815.65978240967, 'accumulated_logging_time': 1.3647146224975586, 'global_step': 6506, 'preemption_count': 0}), (6629, {'train/loss': 0.12258947015921917, 'validation/loss': 0.12399658988566598, 'validation/num_examples': 83274637, 'test/loss': 0.12630205042427464, 'test/num_examples': 95000000, 'score': 6495.27409529686, 'total_duration': 47054.056389808655, 'accumulated_submission_time': 6495.27409529686, 'accumulated_eval_time': 40510.96397399902, 'accumulated_logging_time': 1.3846375942230225, 'global_step': 6629, 'preemption_count': 0}), (6755, {'train/loss': 0.12358214803904474, 'validation/loss': 0.12398135631054531, 'validation/num_examples': 83274637, 'test/loss': 0.12635037441028796, 'test/num_examples': 95000000, 'score': 6615.6436059474945, 'total_duration': 47876.8820772171, 'accumulated_submission_time': 6615.6436059474945, 'accumulated_eval_time': 41212.58106684685, 'accumulated_logging_time': 1.4031081199645996, 'global_step': 6755, 'preemption_count': 0}), (6877, {'train/loss': 0.12128183931971055, 'validation/loss': 0.12383261117854659, 'validation/num_examples': 83274637, 'test/loss': 0.12614694684657046, 'test/num_examples': 95000000, 'score': 6735.993963003159, 'total_duration': 48689.14369106293, 'accumulated_submission_time': 6735.993963003159, 'accumulated_eval_time': 41903.59139561653, 'accumulated_logging_time': 1.4224517345428467, 'global_step': 6877, 'preemption_count': 0}), (6989, {'train/loss': 0.12192150906600298, 'validation/loss': 0.12386427305691376, 'validation/num_examples': 83274637, 'test/loss': 0.12617997490491364, 'test/num_examples': 95000000, 'score': 6855.839759349823, 'total_duration': 49503.907885074615, 'accumulated_submission_time': 6855.839759349823, 'accumulated_eval_time': 42597.655114889145, 'accumulated_logging_time': 1.4529142379760742, 'global_step': 6989, 'preemption_count': 0}), (7117, {'train/loss': 0.12223098075113342, 'validation/loss': 0.12388338028858349, 'validation/num_examples': 83274637, 'test/loss': 0.12626608852643464, 'test/num_examples': 95000000, 'score': 6975.976962566376, 'total_duration': 50332.798134088516, 'accumulated_submission_time': 6975.976962566376, 'accumulated_eval_time': 43305.561421871185, 'accumulated_logging_time': 1.4714164733886719, 'global_step': 7117, 'preemption_count': 0}), (7239, {'train/loss': 0.12214665635452833, 'validation/loss': 0.12386223695397972, 'validation/num_examples': 83274637, 'test/loss': 0.12615486269394724, 'test/num_examples': 95000000, 'score': 7095.832497119904, 'total_duration': 51171.29000234604, 'accumulated_submission_time': 7095.832497119904, 'accumulated_eval_time': 44023.34115815163, 'accumulated_logging_time': 1.4900617599487305, 'global_step': 7239, 'preemption_count': 0}), (7367, {'train/loss': 0.1229977104896685, 'validation/loss': 0.12385124847548573, 'validation/num_examples': 83274637, 'test/loss': 0.1261964888176366, 'test/num_examples': 95000000, 'score': 7216.176561355591, 'total_duration': 51986.28154206276, 'accumulated_submission_time': 7216.176561355591, 'accumulated_eval_time': 44717.15412783623, 'accumulated_logging_time': 1.510237216949463, 'global_step': 7367, 'preemption_count': 0}), (7489, {'train/loss': 0.1236186623881576, 'validation/loss': 0.12387920620097209, 'validation/num_examples': 83274637, 'test/loss': 0.1261783010511298, 'test/num_examples': 95000000, 'score': 7337.0277099609375, 'total_duration': 52778.90236186981, 'accumulated_submission_time': 7337.0277099609375, 'accumulated_eval_time': 45388.039620399475, 'accumulated_logging_time': 1.5504047870635986, 'global_step': 7489, 'preemption_count': 0}), (7610, {'train/loss': 0.12245437968558955, 'validation/loss': 0.12388309650992117, 'validation/num_examples': 83274637, 'test/loss': 0.12625280610030326, 'test/num_examples': 95000000, 'score': 7456.7148225307465, 'total_duration': 53591.9894695282, 'accumulated_submission_time': 7456.7148225307465, 'accumulated_eval_time': 46080.59331250191, 'accumulated_logging_time': 1.5701355934143066, 'global_step': 7610, 'preemption_count': 0}), (7734, {'train/loss': 0.12194246903772589, 'validation/loss': 0.12391655224541505, 'validation/num_examples': 83274637, 'test/loss': 0.12615788330760755, 'test/num_examples': 95000000, 'score': 7577.641162157059, 'total_duration': 54405.769267082214, 'accumulated_submission_time': 7577.641162157059, 'accumulated_eval_time': 46772.640810489655, 'accumulated_logging_time': 1.5887231826782227, 'global_step': 7734, 'preemption_count': 0}), (7856, {'train/loss': 0.12339389920249338, 'validation/loss': 0.12383321178345577, 'validation/num_examples': 83274637, 'test/loss': 0.12613064212228875, 'test/num_examples': 95000000, 'score': 7698.085061073303, 'total_duration': 55240.553169727325, 'accumulated_submission_time': 7698.085061073303, 'accumulated_eval_time': 47486.12541890144, 'accumulated_logging_time': 1.6081855297088623, 'global_step': 7856, 'preemption_count': 0})], 'global_step': 7976}
I0317 03:25:18.950319 140452064818368 submission_runner.py:649] Timing: 7818.342981815338
I0317 03:25:18.950357 140452064818368 submission_runner.py:651] Total number of evals: 65
I0317 03:25:18.950387 140452064818368 submission_runner.py:652] ====================
I0317 03:25:18.950525 140452064818368 submission_runner.py:750] Final criteo1tb score: 3
