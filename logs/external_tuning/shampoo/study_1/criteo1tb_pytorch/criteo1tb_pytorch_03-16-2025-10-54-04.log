torchrun --redirects 1:0,2:0,3:0,4:0,5:0,6:0,7:0 --standalone --nnodes=1 --nproc_per_node=8 submission_runner.py --framework=pytorch --workload=criteo1tb --submission_path=submissions_algorithms/leaderboard/external_tuning/shampoo_submission/submission.py --data_dir=/data/criteo1tb --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=submissions/rolling_leaderboard/external_tuning/shampoo/study_1 --overwrite=True --save_checkpoints=False --rng_seed=1715668789 --torch_compile=true --tuning_ruleset=external --tuning_search_space=submissions_algorithms/leaderboard/external_tuning/shampoo_submission/tuning_search_space.json --num_tuning_trials=5 --hparam_start_index=0 --hparam_end_index=1 2>&1 | tee -a /logs/criteo1tb_pytorch_03-16-2025-10-54-04.log
W0316 10:54:09.018000 9 site-packages/torch/distributed/run.py:793] 
W0316 10:54:09.018000 9 site-packages/torch/distributed/run.py:793] *****************************************
W0316 10:54:09.018000 9 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0316 10:54:09.018000 9 site-packages/torch/distributed/run.py:793] *****************************************
2025-03-16 10:54:14.094600: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-03-16 10:54:14.094600: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-03-16 10:54:14.094604: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-03-16 10:54:14.094604: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-03-16 10:54:14.094625: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-03-16 10:54:14.094605: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-03-16 10:54:14.094618: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-03-16 10:54:14.094805: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1742122454.116261      45 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1742122454.116255      51 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1742122454.116256      48 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1742122454.116261      50 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1742122454.116255      49 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1742122454.116255      47 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1742122454.116256      46 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1742122454.116989      44 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1742122454.122767      46 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
E0000 00:00:1742122454.122769      51 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
E0000 00:00:1742122454.122770      49 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
E0000 00:00:1742122454.122770      48 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
E0000 00:00:1742122454.122774      45 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
E0000 00:00:1742122454.122774      50 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
E0000 00:00:1742122454.122775      47 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
E0000 00:00:1742122454.123593      44 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
[rank3]:[W316 10:54:33.285112700 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank4]:[W316 10:54:33.296066521 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 4]  using GPU 4 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank6]:[W316 10:54:33.297872143 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 6]  using GPU 6 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank0]:[W316 10:54:33.303261696 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank2]:[W316 10:54:33.304134462 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank7]:[W316 10:54:33.305212577 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 7]  using GPU 7 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank5]:[W316 10:54:33.305993890 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 5]  using GPU 5 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank1]:[W316 10:54:33.306480528 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
I0316 10:54:35.159692 140320504472768 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_1/criteo1tb_pytorch.
I0316 10:54:35.159690 140006680593600 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_1/criteo1tb_pytorch.
I0316 10:54:35.159688 140175741793472 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_1/criteo1tb_pytorch.
I0316 10:54:35.159687 140181865804992 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_1/criteo1tb_pytorch.
I0316 10:54:35.159688 139881148286144 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_1/criteo1tb_pytorch.
I0316 10:54:35.159690 139666147955904 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_1/criteo1tb_pytorch.
I0316 10:54:35.159688 139939800802496 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_1/criteo1tb_pytorch.
I0316 10:54:35.159814 140399016412352 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_1/criteo1tb_pytorch.
I0316 10:54:40.393255 140320504472768 submission_runner.py:606] Using RNG seed 1715668789
I0316 10:54:40.393618 139939800802496 logger_utils.py:97] Saving hparams to /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_1/criteo1tb_pytorch/trial_1/hparams.json.
I0316 10:54:40.393622 140006680593600 logger_utils.py:97] Saving hparams to /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_1/criteo1tb_pytorch/trial_1/hparams.json.
I0316 10:54:40.393658 139881148286144 logger_utils.py:97] Saving hparams to /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_1/criteo1tb_pytorch/trial_1/hparams.json.
I0316 10:54:40.393631 139666147955904 logger_utils.py:97] Saving hparams to /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_1/criteo1tb_pytorch/trial_1/hparams.json.
I0316 10:54:40.393692 140175741793472 logger_utils.py:97] Saving hparams to /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_1/criteo1tb_pytorch/trial_1/hparams.json.
I0316 10:54:40.394595 140320504472768 submission_runner.py:615] --- Tuning run 1/5 ---
I0316 10:54:40.394737 140320504472768 submission_runner.py:620] Creating tuning directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_1/criteo1tb_pytorch/trial_1.
I0316 10:54:40.394648 140181865804992 logger_utils.py:97] Saving hparams to /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_1/criteo1tb_pytorch/trial_1/hparams.json.
I0316 10:54:40.394996 140320504472768 logger_utils.py:97] Saving hparams to /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_1/criteo1tb_pytorch/trial_1/hparams.json.
I0316 10:54:40.397109 140399016412352 logger_utils.py:91] Loading hparams from /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_1/criteo1tb_pytorch/trial_1/hparams.json.
I0316 10:54:40.733462 140320504472768 submission_runner.py:218] Initializing dataset.
I0316 10:54:40.733639 140320504472768 submission_runner.py:229] Initializing model.
W0316 10:54:47.761988 140320504472768 submission_runner.py:254] These workloads cannot be fully compiled under current PyTorch version. Proceeding without `torch.compile`.
I0316 10:54:47.762172 140320504472768 submission_runner.py:272] Initializing optimizer.
W0316 10:54:47.763232 140320504472768 distributed_shampoo.py:341] start_preconditioning_step set to -1. Setting start_preconditioning_step equal to precondition frequency 100 by default.
W0316 10:54:47.763341 140320504472768 distributed_shampoo.py:359] Nesterov flag is enabled but momentum parameter is zero! Continuing without using momentum or Nesterov acceleration...
W0316 10:54:47.763761 139666147955904 submission_runner.py:254] These workloads cannot be fully compiled under current PyTorch version. Proceeding without `torch.compile`.
W0316 10:54:47.764014 140181865804992 submission_runner.py:254] These workloads cannot be fully compiled under current PyTorch version. Proceeding without `torch.compile`.
W0316 10:54:47.764206 139881148286144 submission_runner.py:254] These workloads cannot be fully compiled under current PyTorch version. Proceeding without `torch.compile`.
W0316 10:54:47.764266 140006680593600 submission_runner.py:254] These workloads cannot be fully compiled under current PyTorch version. Proceeding without `torch.compile`.
W0316 10:54:47.764348 139939800802496 submission_runner.py:254] These workloads cannot be fully compiled under current PyTorch version. Proceeding without `torch.compile`.
W0316 10:54:47.764447 140399016412352 submission_runner.py:254] These workloads cannot be fully compiled under current PyTorch version. Proceeding without `torch.compile`.
W0316 10:54:47.764558 140175741793472 submission_runner.py:254] These workloads cannot be fully compiled under current PyTorch version. Proceeding without `torch.compile`.
W0316 10:54:47.764957 139666147955904 distributed_shampoo.py:341] start_preconditioning_step set to -1. Setting start_preconditioning_step equal to precondition frequency 100 by default.
W0316 10:54:47.765083 139666147955904 distributed_shampoo.py:359] Nesterov flag is enabled but momentum parameter is zero! Continuing without using momentum or Nesterov acceleration...
W0316 10:54:47.765189 140181865804992 distributed_shampoo.py:341] start_preconditioning_step set to -1. Setting start_preconditioning_step equal to precondition frequency 100 by default.
W0316 10:54:47.765307 140181865804992 distributed_shampoo.py:359] Nesterov flag is enabled but momentum parameter is zero! Continuing without using momentum or Nesterov acceleration...
W0316 10:54:47.765385 139881148286144 distributed_shampoo.py:341] start_preconditioning_step set to -1. Setting start_preconditioning_step equal to precondition frequency 100 by default.
W0316 10:54:47.765418 140006680593600 distributed_shampoo.py:341] start_preconditioning_step set to -1. Setting start_preconditioning_step equal to precondition frequency 100 by default.
W0316 10:54:47.765514 139881148286144 distributed_shampoo.py:359] Nesterov flag is enabled but momentum parameter is zero! Continuing without using momentum or Nesterov acceleration...
W0316 10:54:47.765533 140006680593600 distributed_shampoo.py:359] Nesterov flag is enabled but momentum parameter is zero! Continuing without using momentum or Nesterov acceleration...
W0316 10:54:47.765513 139939800802496 distributed_shampoo.py:341] start_preconditioning_step set to -1. Setting start_preconditioning_step equal to precondition frequency 100 by default.
I0316 10:54:47.765562 140320504472768 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 0.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 0 (torch.Size([512, 13])), Block block_0 (torch.Size([512, 13])).
W0316 10:54:47.765640 139939800802496 distributed_shampoo.py:359] Nesterov flag is enabled but momentum parameter is zero! Continuing without using momentum or Nesterov acceleration...
I0316 10:54:47.765711 140320504472768 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 1.block_0 ([torch.Size([0])]) for Parameter 1 (torch.Size([512])), Block block_0 (torch.Size([512])).
W0316 10:54:47.765800 140399016412352 distributed_shampoo.py:341] start_preconditioning_step set to -1. Setting start_preconditioning_step equal to precondition frequency 100 by default.
I0316 10:54:47.765887 140320504472768 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 2.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 2 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
W0316 10:54:47.765963 140399016412352 distributed_shampoo.py:359] Nesterov flag is enabled but momentum parameter is zero! Continuing without using momentum or Nesterov acceleration...
I0316 10:54:47.766020 140320504472768 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 3.block_0 ([torch.Size([0])]) for Parameter 3 (torch.Size([256])), Block block_0 (torch.Size([256])).
W0316 10:54:47.765982 140175741793472 distributed_shampoo.py:341] start_preconditioning_step set to -1. Setting start_preconditioning_step equal to precondition frequency 100 by default.
W0316 10:54:47.766136 140175741793472 distributed_shampoo.py:359] Nesterov flag is enabled but momentum parameter is zero! Continuing without using momentum or Nesterov acceleration...
I0316 10:54:47.766168 140320504472768 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 4.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 4 (torch.Size([128, 256])), Block block_0 (torch.Size([128, 256])).
I0316 10:54:47.766265 140320504472768 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 5.block_0 ([torch.Size([0])]) for Parameter 5 (torch.Size([128])), Block block_0 (torch.Size([128])).
I0316 10:54:47.766402 140320504472768 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 6.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 6 (torch.Size([1024, 506])), Block block_0 (torch.Size([1024, 506])).
I0316 10:54:47.766518 140320504472768 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 7.block_0 ([torch.Size([0])]) for Parameter 7 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 10:54:47.767277 139666147955904 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 0.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 0 (torch.Size([512, 13])), Block block_0 (torch.Size([512, 13])).
I0316 10:54:47.767453 139666147955904 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 1.block_0 ([torch.Size([0])]) for Parameter 1 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 10:54:47.767730 140320504472768 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 8.block_0 ([torch.Size([1024, 1024]), torch.Size([1024, 1024])]) for Parameter 8 (torch.Size([1024, 1024])), Block block_0 (torch.Size([1024, 1024])).
I0316 10:54:47.767863 140320504472768 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 9.block_0 ([torch.Size([0])]) for Parameter 9 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 10:54:47.767988 140320504472768 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 10.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 10 (torch.Size([512, 1024])), Block block_0 (torch.Size([512, 1024])).
I0316 10:54:47.768040 139881148286144 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 0.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 0 (torch.Size([512, 13])), Block block_0 (torch.Size([512, 13])).
I0316 10:54:47.768092 140320504472768 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 11.block_0 ([torch.Size([0])]) for Parameter 11 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 10:54:47.768218 140320504472768 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 12.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 12 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 10:54:47.768234 139881148286144 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 1.block_0 ([torch.Size([0])]) for Parameter 1 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 10:54:47.768309 140320504472768 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 13.block_0 ([torch.Size([0])]) for Parameter 13 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 10:54:47.768310 140006680593600 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 0.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 0 (torch.Size([512, 13])), Block block_0 (torch.Size([512, 13])).
I0316 10:54:47.768357 139939800802496 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 0.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 0 (torch.Size([512, 13])), Block block_0 (torch.Size([512, 13])).
I0316 10:54:47.768399 140320504472768 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 14.block_0 ([torch.Size([0])]) for Parameter 14 (torch.Size([1, 256])), Block block_0 (torch.Size([256])).
I0316 10:54:47.768401 139881148286144 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 2.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 2 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 10:54:47.768476 140320504472768 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 15.block_0 ([torch.Size([0])]) for Parameter 15 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0316 10:54:47.768476 140006680593600 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 1.block_0 ([torch.Size([0])]) for Parameter 1 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 10:54:47.768509 139881148286144 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 3.block_0 ([torch.Size([0])]) for Parameter 3 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 10:54:47.768517 139939800802496 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 1.block_0 ([torch.Size([0])]) for Parameter 1 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 10:54:47.768516 139666147955904 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 2.block_0 ([torch.Size([256, 256]), torch.Size([512, 512])]) for Parameter 2 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 10:54:47.768572 140320504472768 shampoo_preconditioner_list.py:612] Rank 0: ShampooPreconditionerList Numel Breakdown: (524626, 524288, 655360, 131072, 163840, 32768, 2609224, 2097152, 4194304, 2097152, 2621440, 524288, 655360, 131072, 131072, 2)
I0316 10:54:47.768528 140181865804992 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 0.block_0 ([torch.Size([512, 512]), torch.Size([13, 13])]) for Parameter 0 (torch.Size([512, 13])), Block block_0 (torch.Size([512, 13])).
I0316 10:54:47.768607 140320504472768 shampoo_preconditioner_list.py:615] Rank 0: ShampooPreconditionerList Bytes Breakdown: (2098504,)
I0316 10:54:47.768618 140006680593600 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 2.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 2 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 10:54:47.768631 139881148286144 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 4.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 4 (torch.Size([128, 256])), Block block_0 (torch.Size([128, 256])).
I0316 10:54:47.768644 140320504472768 shampoo_preconditioner_list.py:618] Rank 0: ShampooPreconditionerList Total Elements: 17093020
I0316 10:54:47.768671 140320504472768 shampoo_preconditioner_list.py:621] Rank 0: ShampooPreconditionerList Total Bytes: 2098504
I0316 10:54:47.768660 139939800802496 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 2.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 2 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 10:54:47.768676 139666147955904 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 3.block_0 ([torch.Size([0])]) for Parameter 3 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 10:54:47.768684 140181865804992 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 1.block_0 ([torch.Size([0])]) for Parameter 1 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 10:54:47.768721 139881148286144 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 5.block_0 ([torch.Size([0])]) for Parameter 5 (torch.Size([128])), Block block_0 (torch.Size([128])).
I0316 10:54:47.768730 140006680593600 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 3.block_0 ([torch.Size([0])]) for Parameter 3 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 10:54:47.768763 139939800802496 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 3.block_0 ([torch.Size([0])]) for Parameter 3 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 10:54:47.768800 139666147955904 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 4.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 4 (torch.Size([128, 256])), Block block_0 (torch.Size([128, 256])).
I0316 10:54:47.768803 140320504472768 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([512, 13])), Block block_0 (torch.Size([512, 13])).
I0316 10:54:47.768834 139881148286144 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 6.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 6 (torch.Size([1024, 506])), Block block_0 (torch.Size([1024, 506])).
I0316 10:54:47.768848 140006680593600 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 4.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 4 (torch.Size([128, 256])), Block block_0 (torch.Size([128, 256])).
I0316 10:54:47.768858 140320504472768 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 10:54:47.768846 140181865804992 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 2.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 2 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 10:54:47.768890 139939800802496 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 4.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 4 (torch.Size([128, 256])), Block block_0 (torch.Size([128, 256])).
I0316 10:54:47.768894 139666147955904 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 5.block_0 ([torch.Size([0])]) for Parameter 5 (torch.Size([128])), Block block_0 (torch.Size([128])).
I0316 10:54:47.768905 140320504472768 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 10:54:47.768950 140320504472768 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 10:54:47.768939 140006680593600 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 5.block_0 ([torch.Size([0])]) for Parameter 5 (torch.Size([128])), Block block_0 (torch.Size([128])).
I0316 10:54:47.768944 139881148286144 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 7.block_0 ([torch.Size([0])]) for Parameter 7 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 10:54:47.768962 140181865804992 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 3.block_0 ([torch.Size([0])]) for Parameter 3 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 10:54:47.769002 139939800802496 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 5.block_0 ([torch.Size([0])]) for Parameter 5 (torch.Size([128])), Block block_0 (torch.Size([128])).
I0316 10:54:47.769010 140320504472768 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 4.block_0 (torch.Size([0])) for Parameter 4 (torch.Size([128, 256])), Block block_0 (torch.Size([128, 256])).
I0316 10:54:47.769024 139666147955904 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 6.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 6 (torch.Size([1024, 506])), Block block_0 (torch.Size([1024, 506])).
I0316 10:54:47.769056 140320504472768 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 5.block_0 (torch.Size([0])) for Parameter 5 (torch.Size([128])), Block block_0 (torch.Size([128])).
I0316 10:54:47.769064 139881148286144 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 8.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 8 (torch.Size([1024, 1024])), Block block_0 (torch.Size([1024, 1024])).
I0316 10:54:47.769083 140181865804992 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 4.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 4 (torch.Size([128, 256])), Block block_0 (torch.Size([128, 256])).
I0316 10:54:47.769109 140320504472768 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 6.block_0 (torch.Size([0])) for Parameter 6 (torch.Size([1024, 506])), Block block_0 (torch.Size([1024, 506])).
I0316 10:54:47.769118 139666147955904 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 7.block_0 ([torch.Size([0])]) for Parameter 7 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 10:54:47.769121 139939800802496 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 6.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 6 (torch.Size([1024, 506])), Block block_0 (torch.Size([1024, 506])).
I0316 10:54:47.769152 139881148286144 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 9.block_0 ([torch.Size([0])]) for Parameter 9 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 10:54:47.769158 140320504472768 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 7.block_0 (torch.Size([0])) for Parameter 7 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 10:54:47.769178 140181865804992 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 5.block_0 ([torch.Size([0])]) for Parameter 5 (torch.Size([128])), Block block_0 (torch.Size([128])).
I0316 10:54:47.769133 140399016412352 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 0.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 0 (torch.Size([512, 13])), Block block_0 (torch.Size([512, 13])).
I0316 10:54:47.769208 139939800802496 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 7.block_0 ([torch.Size([0])]) for Parameter 7 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 10:54:47.769224 139666147955904 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 8.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 8 (torch.Size([1024, 1024])), Block block_0 (torch.Size([1024, 1024])).
I0316 10:54:47.769247 140320504472768 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 8.block_0 (torch.Size([1024, 1024])) for Parameter 8 (torch.Size([1024, 1024])), Block block_0 (torch.Size([1024, 1024])).
I0316 10:54:47.769269 139881148286144 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 10.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 10 (torch.Size([512, 1024])), Block block_0 (torch.Size([512, 1024])).
I0316 10:54:47.769302 139666147955904 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 9.block_0 ([torch.Size([0])]) for Parameter 9 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 10:54:47.769265 140175741793472 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 0.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 0 (torch.Size([512, 13])), Block block_0 (torch.Size([512, 13])).
I0316 10:54:47.769316 140320504472768 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 9.block_0 (torch.Size([0])) for Parameter 9 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 10:54:47.769317 140181865804992 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 6.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 6 (torch.Size([1024, 506])), Block block_0 (torch.Size([1024, 506])).
I0316 10:54:47.769332 139939800802496 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 8.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 8 (torch.Size([1024, 1024])), Block block_0 (torch.Size([1024, 1024])).
I0316 10:54:47.769349 139881148286144 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 11.block_0 ([torch.Size([0])]) for Parameter 11 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 10:54:47.769362 140320504472768 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 10.block_0 (torch.Size([0])) for Parameter 10 (torch.Size([512, 1024])), Block block_0 (torch.Size([512, 1024])).
I0316 10:54:47.769405 140320504472768 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 11.block_0 (torch.Size([0])) for Parameter 11 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 10:54:47.769413 140181865804992 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 7.block_0 ([torch.Size([0])]) for Parameter 7 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 10:54:47.769418 139939800802496 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 9.block_0 ([torch.Size([0])]) for Parameter 9 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 10:54:47.769424 139666147955904 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 10.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 10 (torch.Size([512, 1024])), Block block_0 (torch.Size([512, 1024])).
I0316 10:54:47.769456 140320504472768 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 12.block_0 (torch.Size([0])) for Parameter 12 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 10:54:47.769442 140175741793472 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 1.block_0 ([torch.Size([0])]) for Parameter 1 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 10:54:47.769505 140320504472768 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 13.block_0 (torch.Size([0])) for Parameter 13 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 10:54:47.769500 139666147955904 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 11.block_0 ([torch.Size([0])]) for Parameter 11 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 10:54:47.769515 140181865804992 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 8.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 8 (torch.Size([1024, 1024])), Block block_0 (torch.Size([1024, 1024])).
I0316 10:54:47.769554 140320504472768 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 14.block_0 (torch.Size([0])) for Parameter 14 (torch.Size([1, 256])), Block block_0 (torch.Size([256])).
I0316 10:54:47.769530 140006680593600 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 6.block_0 ([torch.Size([1024, 1024]), torch.Size([506, 506])]) for Parameter 6 (torch.Size([1024, 506])), Block block_0 (torch.Size([1024, 506])).
I0316 10:54:47.769589 140181865804992 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 9.block_0 ([torch.Size([0])]) for Parameter 9 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 10:54:47.769604 140320504472768 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 15.block_0 (torch.Size([0])) for Parameter 15 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0316 10:54:47.769602 140175741793472 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 2.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 2 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 10:54:47.769618 139666147955904 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 12.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 12 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 10:54:47.769659 140320504472768 shampoo_preconditioner_list.py:375] Rank 0: AdaGradPreconditionerList Numel Breakdown: (0, 0, 0, 0, 0, 0, 0, 0, 1048576, 0, 0, 0, 0, 0, 0, 0)
I0316 10:54:47.769658 140006680593600 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 7.block_0 ([torch.Size([0])]) for Parameter 7 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 10:54:47.769695 140320504472768 shampoo_preconditioner_list.py:378] Rank 0: AdaGradPreconditionerList Bytes Breakdown: (0, 0, 0, 0, 0, 0, 0, 0, 4194304, 0, 0, 0, 0, 0, 0, 0)
I0316 10:54:47.769698 139666147955904 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 13.block_0 ([torch.Size([0])]) for Parameter 13 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 10:54:47.769704 140181865804992 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 10.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 10 (torch.Size([512, 1024])), Block block_0 (torch.Size([512, 1024])).
I0316 10:54:47.769717 140175741793472 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 3.block_0 ([torch.Size([0])]) for Parameter 3 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 10:54:47.769729 140320504472768 shampoo_preconditioner_list.py:381] Rank 0: AdaGradPreconditionerList Total Elements: 1048576
I0316 10:54:47.769755 140320504472768 shampoo_preconditioner_list.py:384] Rank 0: AdaGradPreconditionerList Total Bytes: 4194304
I0316 10:54:47.769769 140006680593600 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 8.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 8 (torch.Size([1024, 1024])), Block block_0 (torch.Size([1024, 1024])).
I0316 10:54:47.769776 139666147955904 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 14.block_0 ([torch.Size([0])]) for Parameter 14 (torch.Size([1, 256])), Block block_0 (torch.Size([256])).
I0316 10:54:47.769787 140181865804992 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 11.block_0 ([torch.Size([0])]) for Parameter 11 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 10:54:47.769873 139666147955904 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 15.block_0 ([torch.Size([0])]) for Parameter 15 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0316 10:54:47.769897 140006680593600 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 9.block_0 ([torch.Size([0])]) for Parameter 9 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 10:54:47.769910 140181865804992 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 12.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 12 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 10:54:47.769975 139666147955904 shampoo_preconditioner_list.py:612] Rank 3: ShampooPreconditionerList Numel Breakdown: (524626, 524288, 655360, 131072, 163840, 32768, 2609224, 2097152, 4194304, 2097152, 2621440, 524288, 655360, 131072, 131072, 2)
I0316 10:54:47.770002 140181865804992 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 13.block_0 ([torch.Size([0])]) for Parameter 13 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 10:54:47.770021 139666147955904 shampoo_preconditioner_list.py:615] Rank 3: ShampooPreconditionerList Bytes Breakdown: (2098504,)
I0316 10:54:47.770015 140175741793472 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 4.block_0 ([torch.Size([128, 128]), torch.Size([256, 256])]) for Parameter 4 (torch.Size([128, 256])), Block block_0 (torch.Size([128, 256])).
I0316 10:54:47.770039 140006680593600 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 10.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 10 (torch.Size([512, 1024])), Block block_0 (torch.Size([512, 1024])).
I0316 10:54:47.770052 139666147955904 shampoo_preconditioner_list.py:618] Rank 3: ShampooPreconditionerList Total Elements: 17093020
I0316 10:54:47.769997 140399016412352 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 1.block_0 ([torch.Size([512, 512])]) for Parameter 1 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 10:54:47.770080 139666147955904 shampoo_preconditioner_list.py:621] Rank 3: ShampooPreconditionerList Total Bytes: 2098504
I0316 10:54:47.770090 140181865804992 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 14.block_0 ([torch.Size([0])]) for Parameter 14 (torch.Size([1, 256])), Block block_0 (torch.Size([256])).
I0316 10:54:47.770138 140006680593600 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 11.block_0 ([torch.Size([0])]) for Parameter 11 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 10:54:47.770126 140320504472768 submission.py:105] Large parameters (embedding tables) detected (dim >= 1_000_000). Instantiating Row-Wise AdaGrad...
I0316 10:54:47.770170 140175741793472 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 5.block_0 ([torch.Size([0])]) for Parameter 5 (torch.Size([128])), Block block_0 (torch.Size([128])).
I0316 10:54:47.770185 140181865804992 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 15.block_0 ([torch.Size([0])]) for Parameter 15 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0316 10:54:47.770197 140399016412352 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 2.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 2 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
W0316 10:54:47.770218 140320504472768 distributed_shampoo.py:359] Nesterov flag is enabled but momentum parameter is zero! Continuing without using momentum or Nesterov acceleration...
I0316 10:54:47.770230 139666147955904 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([512, 13])), Block block_0 (torch.Size([512, 13])).
I0316 10:54:47.770228 139881148286144 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 12.block_0 ([torch.Size([256, 256]), torch.Size([512, 512])]) for Parameter 12 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 10:54:47.770255 140006680593600 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 12.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 12 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 10:54:47.770267 140181865804992 shampoo_preconditioner_list.py:612] Rank 6: ShampooPreconditionerList Numel Breakdown: (524626, 524288, 655360, 131072, 163840, 32768, 2609224, 2097152, 4194304, 2097152, 2621440, 524288, 655360, 131072, 131072, 2)
I0316 10:54:47.770286 139666147955904 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 10:54:47.770303 140181865804992 shampoo_preconditioner_list.py:615] Rank 6: ShampooPreconditionerList Bytes Breakdown: (2098504,)
I0316 10:54:47.770322 140175741793472 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 6.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 6 (torch.Size([1024, 506])), Block block_0 (torch.Size([1024, 506])).
I0316 10:54:47.770331 140181865804992 shampoo_preconditioner_list.py:618] Rank 6: ShampooPreconditionerList Total Elements: 17093020
I0316 10:54:47.770305 139939800802496 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 10.block_0 ([torch.Size([512, 512]), torch.Size([1024, 1024])]) for Parameter 10 (torch.Size([512, 1024])), Block block_0 (torch.Size([512, 1024])).
I0316 10:54:47.770330 140006680593600 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 13.block_0 ([torch.Size([0])]) for Parameter 13 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 10:54:47.770358 140181865804992 shampoo_preconditioner_list.py:621] Rank 6: ShampooPreconditionerList Total Bytes: 2098504
I0316 10:54:47.770354 139881148286144 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 13.block_0 ([torch.Size([0])]) for Parameter 13 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 10:54:47.770392 140399016412352 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 3.block_0 ([torch.Size([256, 256])]) for Parameter 3 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 10:54:47.770409 140006680593600 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 14.block_0 ([torch.Size([0])]) for Parameter 14 (torch.Size([1, 256])), Block block_0 (torch.Size([256])).
I0316 10:54:47.770418 139939800802496 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 11.block_0 ([torch.Size([0])]) for Parameter 11 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 10:54:47.770441 139881148286144 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 14.block_0 ([torch.Size([0])]) for Parameter 14 (torch.Size([1, 256])), Block block_0 (torch.Size([256])).
I0316 10:54:47.770445 140175741793472 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 7.block_0 ([torch.Size([0])]) for Parameter 7 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 10:54:47.770488 140006680593600 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 15.block_0 ([torch.Size([0])]) for Parameter 15 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0316 10:54:47.770530 139881148286144 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 15.block_0 ([torch.Size([0])]) for Parameter 15 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0316 10:54:47.770544 139939800802496 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 12.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 12 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 10:54:47.770541 140181865804992 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([512, 13])) for Parameter 0 (torch.Size([512, 13])), Block block_0 (torch.Size([512, 13])).
I0316 10:54:47.770551 140399016412352 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 4.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 4 (torch.Size([128, 256])), Block block_0 (torch.Size([128, 256])).
I0316 10:54:47.770577 140006680593600 shampoo_preconditioner_list.py:612] Rank 2: ShampooPreconditionerList Numel Breakdown: (524626, 524288, 655360, 131072, 163840, 32768, 2609224, 2097152, 4194304, 2097152, 2621440, 524288, 655360, 131072, 131072, 2)
I0316 10:54:47.770591 140175741793472 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 8.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 8 (torch.Size([1024, 1024])), Block block_0 (torch.Size([1024, 1024])).
I0316 10:54:47.770613 140181865804992 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 10:54:47.770617 140006680593600 shampoo_preconditioner_list.py:615] Rank 2: ShampooPreconditionerList Bytes Breakdown: (2098504,)
I0316 10:54:47.770619 139939800802496 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 13.block_0 ([torch.Size([0])]) for Parameter 13 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 10:54:47.770624 139881148286144 shampoo_preconditioner_list.py:612] Rank 4: ShampooPreconditionerList Numel Breakdown: (524626, 524288, 655360, 131072, 163840, 32768, 2609224, 2097152, 4194304, 2097152, 2621440, 524288, 655360, 131072, 131072, 2)
I0316 10:54:47.770649 140006680593600 shampoo_preconditioner_list.py:618] Rank 2: ShampooPreconditionerList Total Elements: 17093020
I0316 10:54:47.770660 139881148286144 shampoo_preconditioner_list.py:615] Rank 4: ShampooPreconditionerList Bytes Breakdown: (2098504,)
I0316 10:54:47.770672 140181865804992 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 10:54:47.770682 140006680593600 shampoo_preconditioner_list.py:621] Rank 2: ShampooPreconditionerList Total Bytes: 2098504
I0316 10:54:47.770689 139881148286144 shampoo_preconditioner_list.py:618] Rank 4: ShampooPreconditionerList Total Elements: 17093020
I0316 10:54:47.770685 140175741793472 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 9.block_0 ([torch.Size([0])]) for Parameter 9 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 10:54:47.770697 139939800802496 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 14.block_0 ([torch.Size([0])]) for Parameter 14 (torch.Size([1, 256])), Block block_0 (torch.Size([256])).
I0316 10:54:47.770681 139666147955904 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([256, 512])) for Parameter 2 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 10:54:47.770716 139881148286144 shampoo_preconditioner_list.py:621] Rank 4: ShampooPreconditionerList Total Bytes: 2098504
I0316 10:54:47.770702 140399016412352 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 5.block_0 ([torch.Size([128, 128])]) for Parameter 5 (torch.Size([128])), Block block_0 (torch.Size([128])).
I0316 10:54:47.770717 140181865804992 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 10:54:47.770761 140181865804992 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 4.block_0 (torch.Size([0])) for Parameter 4 (torch.Size([128, 256])), Block block_0 (torch.Size([128, 256])).
I0316 10:54:47.770771 139666147955904 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 10:54:47.770790 139939800802496 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 15.block_0 ([torch.Size([0])]) for Parameter 15 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0316 10:54:47.770806 140006680593600 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([512, 13])), Block block_0 (torch.Size([512, 13])).
I0316 10:54:47.770814 140181865804992 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 5.block_0 (torch.Size([0])) for Parameter 5 (torch.Size([128])), Block block_0 (torch.Size([128])).
I0316 10:54:47.770820 139666147955904 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 4.block_0 (torch.Size([0])) for Parameter 4 (torch.Size([128, 256])), Block block_0 (torch.Size([128, 256])).
I0316 10:54:47.770818 140175741793472 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 10.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 10 (torch.Size([512, 1024])), Block block_0 (torch.Size([512, 1024])).
I0316 10:54:47.770848 139881148286144 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([512, 13])), Block block_0 (torch.Size([512, 13])).
I0316 10:54:47.770859 140181865804992 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 6.block_0 (torch.Size([0])) for Parameter 6 (torch.Size([1024, 506])), Block block_0 (torch.Size([1024, 506])).
I0316 10:54:47.770860 140006680593600 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 10:54:47.770860 140399016412352 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 6.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 6 (torch.Size([1024, 506])), Block block_0 (torch.Size([1024, 506])).
I0316 10:54:47.770880 139666147955904 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 5.block_0 (torch.Size([0])) for Parameter 5 (torch.Size([128])), Block block_0 (torch.Size([128])).
I0316 10:54:47.770882 139939800802496 shampoo_preconditioner_list.py:612] Rank 1: ShampooPreconditionerList Numel Breakdown: (524626, 524288, 655360, 131072, 163840, 32768, 2609224, 2097152, 4194304, 2097152, 2621440, 524288, 655360, 131072, 131072, 2)
I0316 10:54:47.770906 140006680593600 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 10:54:47.770908 140181865804992 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 7.block_0 (torch.Size([0])) for Parameter 7 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 10:54:47.770916 139939800802496 shampoo_preconditioner_list.py:615] Rank 1: ShampooPreconditionerList Bytes Breakdown: (2098504,)
I0316 10:54:47.770914 139881148286144 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 10:54:47.770911 140175741793472 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 11.block_0 ([torch.Size([0])]) for Parameter 11 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 10:54:47.770928 139666147955904 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 6.block_0 (torch.Size([0])) for Parameter 6 (torch.Size([1024, 506])), Block block_0 (torch.Size([1024, 506])).
I0316 10:54:47.770945 139939800802496 shampoo_preconditioner_list.py:618] Rank 1: ShampooPreconditionerList Total Elements: 17093020
I0316 10:54:47.770963 140181865804992 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 8.block_0 (torch.Size([0])) for Parameter 8 (torch.Size([1024, 1024])), Block block_0 (torch.Size([1024, 1024])).
I0316 10:54:47.770968 139881148286144 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 10:54:47.770968 140006680593600 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 10:54:47.770980 139939800802496 shampoo_preconditioner_list.py:621] Rank 1: ShampooPreconditionerList Total Bytes: 2098504
I0316 10:54:47.770991 139666147955904 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 7.block_0 (torch.Size([0])) for Parameter 7 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 10:54:47.771015 139881148286144 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 10:54:47.771015 140006680593600 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 4.block_0 (torch.Size([0])) for Parameter 4 (torch.Size([128, 256])), Block block_0 (torch.Size([128, 256])).
I0316 10:54:47.771017 140181865804992 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 9.block_0 (torch.Size([0])) for Parameter 9 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 10:54:47.771027 140399016412352 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 7.block_0 ([torch.Size([1024, 1024])]) for Parameter 7 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 10:54:47.771046 139666147955904 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 8.block_0 (torch.Size([0])) for Parameter 8 (torch.Size([1024, 1024])), Block block_0 (torch.Size([1024, 1024])).
I0316 10:54:47.771048 140175741793472 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 12.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 12 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 10:54:47.771059 140006680593600 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 5.block_0 (torch.Size([0])) for Parameter 5 (torch.Size([128])), Block block_0 (torch.Size([128])).
I0316 10:54:47.771061 140181865804992 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 10.block_0 (torch.Size([0])) for Parameter 10 (torch.Size([512, 1024])), Block block_0 (torch.Size([512, 1024])).
I0316 10:54:47.771068 139881148286144 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 4.block_0 (torch.Size([0])) for Parameter 4 (torch.Size([128, 256])), Block block_0 (torch.Size([128, 256])).
I0316 10:54:47.771092 139666147955904 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 9.block_0 (torch.Size([0])) for Parameter 9 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 10:54:47.771105 140181865804992 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 11.block_0 (torch.Size([0])) for Parameter 11 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 10:54:47.771105 139939800802496 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([512, 13])), Block block_0 (torch.Size([512, 13])).
I0316 10:54:47.771120 139881148286144 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 5.block_0 (torch.Size([0])) for Parameter 5 (torch.Size([128])), Block block_0 (torch.Size([128])).
I0316 10:54:47.771145 139666147955904 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 10.block_0 (torch.Size([0])) for Parameter 10 (torch.Size([512, 1024])), Block block_0 (torch.Size([512, 1024])).
I0316 10:54:47.771147 140181865804992 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 12.block_0 (torch.Size([0])) for Parameter 12 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 10:54:47.771143 140175741793472 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 13.block_0 ([torch.Size([0])]) for Parameter 13 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 10:54:47.771160 139939800802496 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 10:54:47.771153 140006680593600 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 6.block_0 (torch.Size([1024, 506])) for Parameter 6 (torch.Size([1024, 506])), Block block_0 (torch.Size([1024, 506])).
I0316 10:54:47.771165 139881148286144 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 6.block_0 (torch.Size([0])) for Parameter 6 (torch.Size([1024, 506])), Block block_0 (torch.Size([1024, 506])).
I0316 10:54:47.771169 140399016412352 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 8.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 8 (torch.Size([1024, 1024])), Block block_0 (torch.Size([1024, 1024])).
I0316 10:54:47.771188 140181865804992 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 13.block_0 (torch.Size([0])) for Parameter 13 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 10:54:47.771197 139666147955904 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 11.block_0 (torch.Size([0])) for Parameter 11 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 10:54:47.771205 139939800802496 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 10:54:47.771215 139881148286144 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 7.block_0 (torch.Size([0])) for Parameter 7 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 10:54:47.771219 140006680593600 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 7.block_0 (torch.Size([0])) for Parameter 7 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 10:54:47.771230 140181865804992 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 14.block_0 (torch.Size([0])) for Parameter 14 (torch.Size([1, 256])), Block block_0 (torch.Size([256])).
I0316 10:54:47.771231 140175741793472 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 14.block_0 ([torch.Size([0])]) for Parameter 14 (torch.Size([1, 256])), Block block_0 (torch.Size([256])).
I0316 10:54:47.771240 139666147955904 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 12.block_0 (torch.Size([0])) for Parameter 12 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 10:54:47.771249 139939800802496 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 10:54:47.771266 140006680593600 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 8.block_0 (torch.Size([0])) for Parameter 8 (torch.Size([1024, 1024])), Block block_0 (torch.Size([1024, 1024])).
I0316 10:54:47.771268 139881148286144 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 8.block_0 (torch.Size([0])) for Parameter 8 (torch.Size([1024, 1024])), Block block_0 (torch.Size([1024, 1024])).
I0316 10:54:47.771271 140181865804992 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 15.block_0 (torch.Size([0])) for Parameter 15 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0316 10:54:47.771283 139666147955904 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 13.block_0 (torch.Size([0])) for Parameter 13 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 10:54:47.771305 139939800802496 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 4.block_0 (torch.Size([0])) for Parameter 4 (torch.Size([128, 256])), Block block_0 (torch.Size([128, 256])).
I0316 10:54:47.771310 140006680593600 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 9.block_0 (torch.Size([0])) for Parameter 9 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 10:54:47.771313 139881148286144 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 9.block_0 (torch.Size([0])) for Parameter 9 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 10:54:47.771321 140175741793472 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 15.block_0 ([torch.Size([0])]) for Parameter 15 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0316 10:54:47.771324 140181865804992 shampoo_preconditioner_list.py:375] Rank 6: AdaGradPreconditionerList Numel Breakdown: (6656, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)
I0316 10:54:47.771326 139666147955904 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 14.block_0 (torch.Size([0])) for Parameter 14 (torch.Size([1, 256])), Block block_0 (torch.Size([256])).
I0316 10:54:47.771355 140181865804992 shampoo_preconditioner_list.py:378] Rank 6: AdaGradPreconditionerList Bytes Breakdown: (26624, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)
I0316 10:54:47.771354 140006680593600 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 10.block_0 (torch.Size([0])) for Parameter 10 (torch.Size([512, 1024])), Block block_0 (torch.Size([512, 1024])).
I0316 10:54:47.771355 139881148286144 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 10.block_0 (torch.Size([0])) for Parameter 10 (torch.Size([512, 1024])), Block block_0 (torch.Size([512, 1024])).
I0316 10:54:47.771356 139939800802496 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 5.block_0 (torch.Size([0])) for Parameter 5 (torch.Size([128])), Block block_0 (torch.Size([128])).
I0316 10:54:47.771368 139666147955904 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 15.block_0 (torch.Size([0])) for Parameter 15 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0316 10:54:47.771390 140181865804992 shampoo_preconditioner_list.py:381] Rank 6: AdaGradPreconditionerList Total Elements: 6656
I0316 10:54:47.771398 140006680593600 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 11.block_0 (torch.Size([0])) for Parameter 11 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 10:54:47.771399 139881148286144 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 11.block_0 (torch.Size([0])) for Parameter 11 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 10:54:47.771402 139939800802496 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 6.block_0 (torch.Size([0])) for Parameter 6 (torch.Size([1024, 506])), Block block_0 (torch.Size([1024, 506])).
I0316 10:54:47.771411 140175741793472 shampoo_preconditioner_list.py:612] Rank 5: ShampooPreconditionerList Numel Breakdown: (524626, 524288, 655360, 131072, 163840, 32768, 2609224, 2097152, 4194304, 2097152, 2621440, 524288, 655360, 131072, 131072, 2)
I0316 10:54:47.771421 139666147955904 shampoo_preconditioner_list.py:375] Rank 3: AdaGradPreconditionerList Numel Breakdown: (0, 0, 131072, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)
I0316 10:54:47.771432 140181865804992 shampoo_preconditioner_list.py:384] Rank 6: AdaGradPreconditionerList Total Bytes: 26624
I0316 10:54:47.771449 140175741793472 shampoo_preconditioner_list.py:615] Rank 5: ShampooPreconditionerList Bytes Breakdown: (2098504,)
I0316 10:54:47.771449 140006680593600 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 12.block_0 (torch.Size([0])) for Parameter 12 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 10:54:47.771453 139666147955904 shampoo_preconditioner_list.py:378] Rank 3: AdaGradPreconditionerList Bytes Breakdown: (0, 0, 524288, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)
I0316 10:54:47.771452 139939800802496 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 7.block_0 (torch.Size([0])) for Parameter 7 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 10:54:47.771480 139666147955904 shampoo_preconditioner_list.py:381] Rank 3: AdaGradPreconditionerList Total Elements: 131072
I0316 10:54:47.771493 140006680593600 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 13.block_0 (torch.Size([0])) for Parameter 13 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 10:54:47.771494 140175741793472 shampoo_preconditioner_list.py:618] Rank 5: ShampooPreconditionerList Total Elements: 17093020
I0316 10:54:47.771497 139939800802496 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 8.block_0 (torch.Size([0])) for Parameter 8 (torch.Size([1024, 1024])), Block block_0 (torch.Size([1024, 1024])).
I0316 10:54:47.771520 139666147955904 shampoo_preconditioner_list.py:384] Rank 3: AdaGradPreconditionerList Total Bytes: 524288
I0316 10:54:47.771524 140175741793472 shampoo_preconditioner_list.py:621] Rank 5: ShampooPreconditionerList Total Bytes: 2098504
I0316 10:54:47.771535 140006680593600 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 14.block_0 (torch.Size([0])) for Parameter 14 (torch.Size([1, 256])), Block block_0 (torch.Size([256])).
I0316 10:54:47.771541 139939800802496 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 9.block_0 (torch.Size([0])) for Parameter 9 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 10:54:47.771576 140006680593600 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 15.block_0 (torch.Size([0])) for Parameter 15 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0316 10:54:47.771629 139939800802496 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 10.block_0 (torch.Size([512, 1024])) for Parameter 10 (torch.Size([512, 1024])), Block block_0 (torch.Size([512, 1024])).
I0316 10:54:47.771641 140006680593600 shampoo_preconditioner_list.py:375] Rank 2: AdaGradPreconditionerList Numel Breakdown: (0, 0, 0, 0, 0, 0, 518144, 0, 0, 0, 0, 0, 0, 0, 0, 0)
I0316 10:54:47.771658 140175741793472 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([512, 13])), Block block_0 (torch.Size([512, 13])).
I0316 10:54:47.771672 140006680593600 shampoo_preconditioner_list.py:378] Rank 2: AdaGradPreconditionerList Bytes Breakdown: (0, 0, 0, 0, 0, 0, 2072576, 0, 0, 0, 0, 0, 0, 0, 0, 0)
I0316 10:54:47.771699 139939800802496 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 11.block_0 (torch.Size([0])) for Parameter 11 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 10:54:47.771699 140006680593600 shampoo_preconditioner_list.py:381] Rank 2: AdaGradPreconditionerList Total Elements: 518144
I0316 10:54:47.771732 140006680593600 shampoo_preconditioner_list.py:384] Rank 2: AdaGradPreconditionerList Total Bytes: 2072576
I0316 10:54:47.771731 140175741793472 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 10:54:47.771754 139939800802496 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 12.block_0 (torch.Size([0])) for Parameter 12 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 10:54:47.771784 140175741793472 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
W0316 10:54:47.771776 140181865804992 distributed_shampoo.py:359] Nesterov flag is enabled but momentum parameter is zero! Continuing without using momentum or Nesterov acceleration...
I0316 10:54:47.771796 139939800802496 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 13.block_0 (torch.Size([0])) for Parameter 13 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 10:54:47.771783 140399016412352 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 9.block_0 ([torch.Size([1024, 1024])]) for Parameter 9 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 10:54:47.771840 139939800802496 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 14.block_0 (torch.Size([0])) for Parameter 14 (torch.Size([1, 256])), Block block_0 (torch.Size([256])).
I0316 10:54:47.771845 140175741793472 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 10:54:47.771883 139939800802496 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 15.block_0 (torch.Size([0])) for Parameter 15 (torch.Size([1])), Block block_0 (torch.Size([1])).
W0316 10:54:47.771872 139666147955904 distributed_shampoo.py:359] Nesterov flag is enabled but momentum parameter is zero! Continuing without using momentum or Nesterov acceleration...
I0316 10:54:47.771944 139939800802496 shampoo_preconditioner_list.py:375] Rank 1: AdaGradPreconditionerList Numel Breakdown: (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 524288, 0, 0, 0, 0, 0)
I0316 10:54:47.771956 140175741793472 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 4.block_0 (torch.Size([128, 256])) for Parameter 4 (torch.Size([128, 256])), Block block_0 (torch.Size([128, 256])).
I0316 10:54:47.771973 140399016412352 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 10.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 10 (torch.Size([512, 1024])), Block block_0 (torch.Size([512, 1024])).
I0316 10:54:47.771989 139939800802496 shampoo_preconditioner_list.py:378] Rank 1: AdaGradPreconditionerList Bytes Breakdown: (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2097152, 0, 0, 0, 0, 0)
I0316 10:54:47.772017 139939800802496 shampoo_preconditioner_list.py:381] Rank 1: AdaGradPreconditionerList Total Elements: 524288
I0316 10:54:47.772030 140175741793472 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 5.block_0 (torch.Size([0])) for Parameter 5 (torch.Size([128])), Block block_0 (torch.Size([128])).
I0316 10:54:47.772058 139939800802496 shampoo_preconditioner_list.py:384] Rank 1: AdaGradPreconditionerList Total Bytes: 2097152
I0316 10:54:47.772037 139881148286144 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 12.block_0 (torch.Size([256, 512])) for Parameter 12 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 10:54:47.772085 140175741793472 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 6.block_0 (torch.Size([0])) for Parameter 6 (torch.Size([1024, 506])), Block block_0 (torch.Size([1024, 506])).
W0316 10:54:47.772094 140006680593600 distributed_shampoo.py:359] Nesterov flag is enabled but momentum parameter is zero! Continuing without using momentum or Nesterov acceleration...
I0316 10:54:47.772090 140320504472768 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([524288, 128])) for Parameter 0 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 10:54:47.772129 139881148286144 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 13.block_0 (torch.Size([0])) for Parameter 13 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 10:54:47.772153 140175741793472 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 7.block_0 (torch.Size([0])) for Parameter 7 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 10:54:47.772196 139881148286144 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 14.block_0 (torch.Size([0])) for Parameter 14 (torch.Size([1, 256])), Block block_0 (torch.Size([256])).
I0316 10:54:47.772209 140320504472768 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_1 (torch.Size([0])) for Parameter 0 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 10:54:47.772233 140175741793472 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 8.block_0 (torch.Size([0])) for Parameter 8 (torch.Size([1024, 1024])), Block block_0 (torch.Size([1024, 1024])).
I0316 10:54:47.772272 139881148286144 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 15.block_0 (torch.Size([0])) for Parameter 15 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0316 10:54:47.772289 140320504472768 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 10:54:47.772315 140175741793472 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 9.block_0 (torch.Size([0])) for Parameter 9 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 10:54:47.772334 139881148286144 shampoo_preconditioner_list.py:375] Rank 4: AdaGradPreconditionerList Numel Breakdown: (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 131072, 0, 0, 0)
I0316 10:54:47.772367 140320504472768 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_1 (torch.Size([0])) for Parameter 1 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 10:54:47.772369 140175741793472 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 10.block_0 (torch.Size([0])) for Parameter 10 (torch.Size([512, 1024])), Block block_0 (torch.Size([512, 1024])).
I0316 10:54:47.772374 139881148286144 shampoo_preconditioner_list.py:378] Rank 4: AdaGradPreconditionerList Bytes Breakdown: (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 524288, 0, 0, 0)
I0316 10:54:47.772403 139881148286144 shampoo_preconditioner_list.py:381] Rank 4: AdaGradPreconditionerList Total Elements: 131072
I0316 10:54:47.772422 140175741793472 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 11.block_0 (torch.Size([0])) for Parameter 11 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 10:54:47.772425 140320504472768 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 10:54:47.772445 139881148286144 shampoo_preconditioner_list.py:384] Rank 4: AdaGradPreconditionerList Total Bytes: 524288
I0316 10:54:47.772474 140320504472768 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_1 (torch.Size([0])) for Parameter 2 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 10:54:47.772474 140175741793472 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 12.block_0 (torch.Size([0])) for Parameter 12 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
W0316 10:54:47.772467 139939800802496 distributed_shampoo.py:359] Nesterov flag is enabled but momentum parameter is zero! Continuing without using momentum or Nesterov acceleration...
I0316 10:54:47.772525 140175741793472 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 13.block_0 (torch.Size([0])) for Parameter 13 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 10:54:47.772529 140320504472768 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 10:54:47.772577 140320504472768 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_1 (torch.Size([0])) for Parameter 3 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 10:54:47.772587 140175741793472 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 14.block_0 (torch.Size([0])) for Parameter 14 (torch.Size([1, 256])), Block block_0 (torch.Size([256])).
I0316 10:54:47.772628 140320504472768 shampoo_preconditioner_list.py:375] Rank 0: AdaGradPreconditionerList Numel Breakdown: (67108864, 0, 0, 0, 0, 0, 0, 0)
I0316 10:54:47.772621 140399016412352 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 11.block_0 ([torch.Size([512, 512])]) for Parameter 11 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 10:54:47.772639 140175741793472 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 15.block_0 (torch.Size([0])) for Parameter 15 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0316 10:54:47.772686 140320504472768 shampoo_preconditioner_list.py:378] Rank 0: AdaGradPreconditionerList Bytes Breakdown: (268435456, 0, 0, 0, 0, 0, 0, 0)
I0316 10:54:47.772709 140175741793472 shampoo_preconditioner_list.py:375] Rank 5: AdaGradPreconditionerList Numel Breakdown: (0, 0, 0, 0, 32768, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)
I0316 10:54:47.772720 140320504472768 shampoo_preconditioner_list.py:381] Rank 0: AdaGradPreconditionerList Total Elements: 67108864
I0316 10:54:47.772747 140320504472768 shampoo_preconditioner_list.py:384] Rank 0: AdaGradPreconditionerList Total Bytes: 268435456
I0316 10:54:47.772753 140175741793472 shampoo_preconditioner_list.py:378] Rank 5: AdaGradPreconditionerList Bytes Breakdown: (0, 0, 0, 0, 131072, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)
I0316 10:54:47.772783 140175741793472 shampoo_preconditioner_list.py:381] Rank 5: AdaGradPreconditionerList Total Elements: 32768
I0316 10:54:47.772780 140399016412352 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 12.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 12 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 10:54:47.772821 140175741793472 shampoo_preconditioner_list.py:384] Rank 5: AdaGradPreconditionerList Total Bytes: 131072
W0316 10:54:47.772845 139881148286144 distributed_shampoo.py:359] Nesterov flag is enabled but momentum parameter is zero! Continuing without using momentum or Nesterov acceleration...
I0316 10:54:47.773391 140181865804992 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 10:54:47.773491 140181865804992 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_1 (torch.Size([0])) for Parameter 0 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 10:54:47.773558 140181865804992 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 10:54:47.773564 139666147955904 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 10:54:47.773615 140181865804992 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_1 (torch.Size([0])) for Parameter 1 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 10:54:47.773648 139666147955904 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_1 (torch.Size([0])) for Parameter 0 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 10:54:47.773671 140181865804992 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 10:54:47.773716 140181865804992 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_1 (torch.Size([0])) for Parameter 2 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 10:54:47.773719 139666147955904 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 10:54:47.774013 140006680593600 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 10:54:47.774137 140006680593600 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_1 (torch.Size([0])) for Parameter 0 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 10:54:47.774189 140399016412352 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 13.block_0 ([torch.Size([256, 256])]) for Parameter 13 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 10:54:47.774392 140399016412352 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 14.block_0 ([torch.Size([256, 256])]) for Parameter 14 (torch.Size([1, 256])), Block block_0 (torch.Size([256])).
I0316 10:54:47.774504 140320504472768 submission_runner.py:279] Initializing metrics bundle.
I0316 10:54:47.774564 140399016412352 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 15.block_0 ([torch.Size([1, 1])]) for Parameter 15 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0316 10:54:47.774663 140320504472768 submission_runner.py:301] Initializing checkpoint and logger.
I0316 10:54:47.774699 140399016412352 shampoo_preconditioner_list.py:612] Rank 7: ShampooPreconditionerList Numel Breakdown: (524626, 524288, 655360, 131072, 163840, 32768, 2609224, 2097152, 4194304, 2097152, 2621440, 524288, 655360, 131072, 131072, 2)
I0316 10:54:47.774748 140399016412352 shampoo_preconditioner_list.py:615] Rank 7: ShampooPreconditionerList Bytes Breakdown: (2098504, 2097152, 2621440, 524288, 655360, 131072, 10436896, 8388608, 16777216)
I0316 10:54:47.774750 139939800802496 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
W0316 10:54:47.774763 140175741793472 distributed_shampoo.py:359] Nesterov flag is enabled but momentum parameter is zero! Continuing without using momentum or Nesterov acceleration...
I0316 10:54:47.774787 140399016412352 shampoo_preconditioner_list.py:618] Rank 7: ShampooPreconditionerList Total Elements: 17093020
I0316 10:54:47.774848 140399016412352 shampoo_preconditioner_list.py:621] Rank 7: ShampooPreconditionerList Total Bytes: 43730536
I0316 10:54:47.775001 140399016412352 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([512, 13])), Block block_0 (torch.Size([512, 13])).
I0316 10:54:47.774996 139666147955904 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_1 (torch.Size([524288, 128])) for Parameter 1 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 10:54:47.775046 140181865804992 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([524288, 128])) for Parameter 3 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 10:54:47.775095 139666147955904 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 10:54:47.775117 140399016412352 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([512])) for Parameter 1 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 10:54:47.775116 140320504472768 submission_runner.py:321] Saving meta data to /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_1/criteo1tb_pytorch/trial_1/meta_data_0.json.
I0316 10:54:47.775114 139881148286144 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 10:54:47.775154 140181865804992 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_1 (torch.Size([0])) for Parameter 3 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 10:54:47.775165 139666147955904 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_1 (torch.Size([0])) for Parameter 2 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 10:54:47.775187 140399016412352 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 10:54:47.775204 140181865804992 shampoo_preconditioner_list.py:375] Rank 6: AdaGradPreconditionerList Numel Breakdown: (0, 0, 0, 0, 0, 0, 67108864, 0)
I0316 10:54:47.775218 139666147955904 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 10:54:47.775216 139881148286144 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_1 (torch.Size([0])) for Parameter 0 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 10:54:47.775215 140006680593600 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([524288, 128])) for Parameter 1 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 10:54:47.775246 140181865804992 shampoo_preconditioner_list.py:378] Rank 6: AdaGradPreconditionerList Bytes Breakdown: (0, 0, 0, 0, 0, 0, 268435456, 0)
I0316 10:54:47.775267 139666147955904 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_1 (torch.Size([0])) for Parameter 3 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 10:54:47.775275 140181865804992 shampoo_preconditioner_list.py:381] Rank 6: AdaGradPreconditionerList Total Elements: 67108864
I0316 10:54:47.775269 140320504472768 logger_utils.py:262] Unable to record workload.train_mean information. Continuing without it.
I0316 10:54:47.775276 139881148286144 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 10:54:47.775285 140399016412352 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([256])) for Parameter 3 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 10:54:47.775302 140181865804992 shampoo_preconditioner_list.py:384] Rank 6: AdaGradPreconditionerList Total Bytes: 268435456
I0316 10:54:47.775307 140320504472768 logger_utils.py:262] Unable to record workload.train_stddev information. Continuing without it.
I0316 10:54:47.775314 139666147955904 shampoo_preconditioner_list.py:375] Rank 3: AdaGradPreconditionerList Numel Breakdown: (0, 0, 0, 67108864, 0, 0, 0, 0)
I0316 10:54:47.775326 139881148286144 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_1 (torch.Size([0])) for Parameter 1 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 10:54:47.775330 140006680593600 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_1 (torch.Size([0])) for Parameter 1 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 10:54:47.775353 140399016412352 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 4.block_0 (torch.Size([0])) for Parameter 4 (torch.Size([128, 256])), Block block_0 (torch.Size([128, 256])).
I0316 10:54:47.775364 139666147955904 shampoo_preconditioner_list.py:378] Rank 3: AdaGradPreconditionerList Bytes Breakdown: (0, 0, 0, 268435456, 0, 0, 0, 0)
I0316 10:54:47.775399 140006680593600 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 10:54:47.775410 139666147955904 shampoo_preconditioner_list.py:381] Rank 3: AdaGradPreconditionerList Total Elements: 67108864
I0316 10:54:47.775435 139666147955904 shampoo_preconditioner_list.py:384] Rank 3: AdaGradPreconditionerList Total Bytes: 268435456
I0316 10:54:47.775431 140399016412352 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 5.block_0 (torch.Size([128])) for Parameter 5 (torch.Size([128])), Block block_0 (torch.Size([128])).
I0316 10:54:47.775456 140006680593600 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_1 (torch.Size([0])) for Parameter 2 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 10:54:47.775455 139939800802496 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_1 (torch.Size([524288, 128])) for Parameter 0 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 10:54:47.775493 140399016412352 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 6.block_0 (torch.Size([0])) for Parameter 6 (torch.Size([1024, 506])), Block block_0 (torch.Size([1024, 506])).
I0316 10:54:47.775510 140006680593600 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 10:54:47.775563 140006680593600 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_1 (torch.Size([0])) for Parameter 3 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 10:54:47.775566 139939800802496 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 10:54:47.775570 140399016412352 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 7.block_0 (torch.Size([1024])) for Parameter 7 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 10:54:47.775629 140399016412352 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 8.block_0 (torch.Size([0])) for Parameter 8 (torch.Size([1024, 1024])), Block block_0 (torch.Size([1024, 1024])).
I0316 10:54:47.775631 139939800802496 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_1 (torch.Size([0])) for Parameter 1 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 10:54:47.775615 140006680593600 shampoo_preconditioner_list.py:375] Rank 2: AdaGradPreconditionerList Numel Breakdown: (0, 0, 67108864, 0, 0, 0, 0, 0)
I0316 10:54:47.775669 140006680593600 shampoo_preconditioner_list.py:378] Rank 2: AdaGradPreconditionerList Bytes Breakdown: (0, 0, 268435456, 0, 0, 0, 0, 0)
I0316 10:54:47.775681 139939800802496 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 10:54:47.775702 140006680593600 shampoo_preconditioner_list.py:381] Rank 2: AdaGradPreconditionerList Total Elements: 67108864
I0316 10:54:47.775712 140399016412352 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 9.block_0 (torch.Size([1024])) for Parameter 9 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 10:54:47.775728 139939800802496 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_1 (torch.Size([0])) for Parameter 2 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 10:54:47.775735 140006680593600 shampoo_preconditioner_list.py:384] Rank 2: AdaGradPreconditionerList Total Bytes: 268435456
I0316 10:54:47.775769 140399016412352 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 10.block_0 (torch.Size([0])) for Parameter 10 (torch.Size([512, 1024])), Block block_0 (torch.Size([512, 1024])).
I0316 10:54:47.775774 139939800802496 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 10:54:47.775824 139939800802496 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_1 (torch.Size([0])) for Parameter 3 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 10:54:47.775861 140399016412352 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 11.block_0 (torch.Size([512])) for Parameter 11 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 10:54:47.775894 139939800802496 shampoo_preconditioner_list.py:375] Rank 1: AdaGradPreconditionerList Numel Breakdown: (0, 67108864, 0, 0, 0, 0, 0, 0)
I0316 10:54:47.775944 140399016412352 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 12.block_0 (torch.Size([0])) for Parameter 12 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 10:54:47.775944 139939800802496 shampoo_preconditioner_list.py:378] Rank 1: AdaGradPreconditionerList Bytes Breakdown: (0, 268435456, 0, 0, 0, 0, 0, 0)
I0316 10:54:47.775985 139939800802496 shampoo_preconditioner_list.py:381] Rank 1: AdaGradPreconditionerList Total Elements: 67108864
I0316 10:54:47.776023 139939800802496 shampoo_preconditioner_list.py:384] Rank 1: AdaGradPreconditionerList Total Bytes: 268435456
I0316 10:54:47.776026 140399016412352 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 13.block_0 (torch.Size([256])) for Parameter 13 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 10:54:47.776099 140399016412352 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 14.block_0 (torch.Size([256])) for Parameter 14 (torch.Size([1, 256])), Block block_0 (torch.Size([256])).
I0316 10:54:47.776193 140399016412352 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 15.block_0 (torch.Size([1])) for Parameter 15 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0316 10:54:47.776276 140399016412352 shampoo_preconditioner_list.py:375] Rank 7: AdaGradPreconditionerList Numel Breakdown: (0, 512, 0, 256, 0, 128, 0, 1024, 0, 1024, 0, 512, 0, 256, 256, 1)
I0316 10:54:47.776282 139881148286144 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([524288, 128])) for Parameter 2 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 10:54:47.776315 140399016412352 shampoo_preconditioner_list.py:378] Rank 7: AdaGradPreconditionerList Bytes Breakdown: (0, 2048, 0, 1024, 0, 512, 0, 4096, 0, 4096, 0, 2048, 0, 1024, 1024, 4)
I0316 10:54:47.776344 140399016412352 shampoo_preconditioner_list.py:381] Rank 7: AdaGradPreconditionerList Total Elements: 3969
I0316 10:54:47.776377 140399016412352 shampoo_preconditioner_list.py:384] Rank 7: AdaGradPreconditionerList Total Bytes: 15876
I0316 10:54:47.776400 139881148286144 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_1 (torch.Size([0])) for Parameter 2 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 10:54:47.776481 139881148286144 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 10:54:47.776547 139881148286144 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_1 (torch.Size([0])) for Parameter 3 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 10:54:47.776518 140175741793472 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 10:54:47.776606 139881148286144 shampoo_preconditioner_list.py:375] Rank 4: AdaGradPreconditionerList Numel Breakdown: (0, 0, 0, 0, 67108864, 0, 0, 0)
I0316 10:54:47.776621 140175741793472 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_1 (torch.Size([0])) for Parameter 0 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 10:54:47.776655 139881148286144 shampoo_preconditioner_list.py:378] Rank 4: AdaGradPreconditionerList Bytes Breakdown: (0, 0, 0, 0, 268435456, 0, 0, 0)
I0316 10:54:47.776694 139881148286144 shampoo_preconditioner_list.py:381] Rank 4: AdaGradPreconditionerList Total Elements: 67108864
I0316 10:54:47.776691 140175741793472 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 10:54:47.776731 139881148286144 shampoo_preconditioner_list.py:384] Rank 4: AdaGradPreconditionerList Total Bytes: 268435456
I0316 10:54:47.776753 140175741793472 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_1 (torch.Size([0])) for Parameter 1 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 10:54:47.776831 140175741793472 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
W0316 10:54:47.776917 140399016412352 distributed_shampoo.py:359] Nesterov flag is enabled but momentum parameter is zero! Continuing without using momentum or Nesterov acceleration...
I0316 10:54:47.777212 140181865804992 logger_utils.py:262] Unable to record workload.train_mean information. Continuing without it.
I0316 10:54:47.777284 140181865804992 logger_utils.py:262] Unable to record workload.train_stddev information. Continuing without it.
I0316 10:54:47.777376 139666147955904 logger_utils.py:262] Unable to record workload.train_mean information. Continuing without it.
I0316 10:54:47.777448 139666147955904 logger_utils.py:262] Unable to record workload.train_stddev information. Continuing without it.
I0316 10:54:47.777519 140175741793472 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_1 (torch.Size([524288, 128])) for Parameter 2 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 10:54:47.777570 140006680593600 logger_utils.py:262] Unable to record workload.train_mean information. Continuing without it.
I0316 10:54:47.777643 140006680593600 logger_utils.py:262] Unable to record workload.train_stddev information. Continuing without it.
I0316 10:54:47.777640 140175741793472 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 10:54:47.777660 139939800802496 logger_utils.py:262] Unable to record workload.train_mean information. Continuing without it.
I0316 10:54:47.777698 140175741793472 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_1 (torch.Size([0])) for Parameter 3 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 10:54:47.777732 139939800802496 logger_utils.py:262] Unable to record workload.train_stddev information. Continuing without it.
I0316 10:54:47.777755 140175741793472 shampoo_preconditioner_list.py:375] Rank 5: AdaGradPreconditionerList Numel Breakdown: (0, 0, 0, 0, 0, 67108864, 0, 0)
I0316 10:54:47.777789 140175741793472 shampoo_preconditioner_list.py:378] Rank 5: AdaGradPreconditionerList Bytes Breakdown: (0, 0, 0, 0, 0, 268435456, 0, 0)
I0316 10:54:47.777829 140175741793472 shampoo_preconditioner_list.py:381] Rank 5: AdaGradPreconditionerList Total Elements: 67108864
I0316 10:54:47.777858 140175741793472 shampoo_preconditioner_list.py:384] Rank 5: AdaGradPreconditionerList Total Bytes: 268435456
I0316 10:54:47.778191 139881148286144 logger_utils.py:262] Unable to record workload.train_mean information. Continuing without it.
I0316 10:54:47.778269 139881148286144 logger_utils.py:262] Unable to record workload.train_stddev information. Continuing without it.
I0316 10:54:47.778350 140399016412352 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 10:54:47.778453 140399016412352 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_1 (torch.Size([0])) for Parameter 0 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 10:54:47.778520 140399016412352 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 10:54:47.778578 140399016412352 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_1 (torch.Size([0])) for Parameter 1 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 10:54:47.778634 140399016412352 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 10:54:47.778688 140399016412352 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_1 (torch.Size([0])) for Parameter 2 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 10:54:47.778741 140399016412352 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 10:54:47.779283 140399016412352 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_1 (torch.Size([524288, 128])) for Parameter 3 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 10:54:47.779322 140175741793472 logger_utils.py:262] Unable to record workload.train_mean information. Continuing without it.
I0316 10:54:47.779379 140399016412352 shampoo_preconditioner_list.py:375] Rank 7: AdaGradPreconditionerList Numel Breakdown: (0, 0, 0, 0, 0, 0, 0, 67108864)
I0316 10:54:47.779424 140399016412352 shampoo_preconditioner_list.py:378] Rank 7: AdaGradPreconditionerList Bytes Breakdown: (0, 0, 0, 0, 0, 0, 0, 268435456)
I0316 10:54:47.779418 140175741793472 logger_utils.py:262] Unable to record workload.train_stddev information. Continuing without it.
I0316 10:54:47.779458 140399016412352 shampoo_preconditioner_list.py:381] Rank 7: AdaGradPreconditionerList Total Elements: 67108864
I0316 10:54:47.779489 140399016412352 shampoo_preconditioner_list.py:384] Rank 7: AdaGradPreconditionerList Total Bytes: 268435456
I0316 10:54:47.780929 140399016412352 logger_utils.py:262] Unable to record workload.train_mean information. Continuing without it.
I0316 10:54:47.781002 140399016412352 logger_utils.py:262] Unable to record workload.train_stddev information. Continuing without it.
I0316 10:54:48.305128 140320504472768 submission_runner.py:325] Saving flags to /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_1/criteo1tb_pytorch/trial_1/flags_0.json.
I0316 10:54:48.337786 140320504472768 submission_runner.py:337] Starting training loop.
I0316 10:54:52.668023 140288690480896 logging_writer.py:48] [0] global_step=0, grad_norm=6.5046, loss=0.839482
I0316 10:54:52.911869 140320504472768 submission.py:265] 0) loss = 0.839, grad_norm = 6.505
I0316 10:54:53.286768 140320504472768 spec.py:321] Evaluating on the training split.
I0316 11:00:05.640738 140320504472768 spec.py:333] Evaluating on the validation split.
I0316 11:04:31.121978 140320504472768 spec.py:349] Evaluating on the test split.
I0316 11:09:32.891691 140320504472768 submission_runner.py:469] Time since start: 884.55s, 	Step: 1, 	{'train/loss': 0.8372680069089006, 'validation/loss': 0.8358118551614604, 'validation/num_examples': 83274637, 'test/loss': 0.8364300465910259, 'test/num_examples': 95000000, 'score': 4.5749499797821045, 'total_duration': 884.554027557373, 'accumulated_submission_time': 4.5749499797821045, 'accumulated_eval_time': 879.6049809455872, 'accumulated_logging_time': 0}
I0316 11:09:32.918046 140277834467072 logging_writer.py:48] [1] accumulated_eval_time=879.605, accumulated_logging_time=0, accumulated_submission_time=4.57495, global_step=1, preemption_count=0, score=4.57495, test/loss=0.83643, test/num_examples=95000000, total_duration=884.554, train/loss=0.837268, validation/loss=0.835812, validation/num_examples=83274637
I0316 11:09:33.608352 140277826074368 logging_writer.py:48] [1] global_step=1, grad_norm=6.51087, loss=0.839349
I0316 11:09:33.611780 140320504472768 submission.py:265] 1) loss = 0.839, grad_norm = 6.511
I0316 11:09:33.803725 140277834467072 logging_writer.py:48] [2] global_step=2, grad_norm=6.21694, loss=0.810188
I0316 11:09:33.806930 140320504472768 submission.py:265] 2) loss = 0.810, grad_norm = 6.217
I0316 11:09:33.999753 140277826074368 logging_writer.py:48] [3] global_step=3, grad_norm=5.70294, loss=0.762278
I0316 11:09:34.002748 140320504472768 submission.py:265] 3) loss = 0.762, grad_norm = 5.703
I0316 11:09:34.193946 140277834467072 logging_writer.py:48] [4] global_step=4, grad_norm=5.05445, loss=0.702902
I0316 11:09:34.196921 140320504472768 submission.py:265] 4) loss = 0.703, grad_norm = 5.054
I0316 11:09:34.389012 140277826074368 logging_writer.py:48] [5] global_step=5, grad_norm=4.38539, loss=0.638495
I0316 11:09:34.392014 140320504472768 submission.py:265] 5) loss = 0.638, grad_norm = 4.385
I0316 11:09:34.583919 140277834467072 logging_writer.py:48] [6] global_step=6, grad_norm=3.79042, loss=0.574268
I0316 11:09:34.586861 140320504472768 submission.py:265] 6) loss = 0.574, grad_norm = 3.790
I0316 11:09:34.777665 140277826074368 logging_writer.py:48] [7] global_step=7, grad_norm=3.46617, loss=0.510684
I0316 11:09:34.780623 140320504472768 submission.py:265] 7) loss = 0.511, grad_norm = 3.466
I0316 11:09:34.970748 140277834467072 logging_writer.py:48] [8] global_step=8, grad_norm=3.14327, loss=0.446529
I0316 11:09:34.973622 140320504472768 submission.py:265] 8) loss = 0.447, grad_norm = 3.143
I0316 11:09:35.166263 140277826074368 logging_writer.py:48] [9] global_step=9, grad_norm=2.80815, loss=0.386334
I0316 11:09:35.169245 140320504472768 submission.py:265] 9) loss = 0.386, grad_norm = 2.808
I0316 11:09:35.359639 140277834467072 logging_writer.py:48] [10] global_step=10, grad_norm=2.49273, loss=0.327505
I0316 11:09:35.363265 140320504472768 submission.py:265] 10) loss = 0.328, grad_norm = 2.493
I0316 11:09:35.553432 140277826074368 logging_writer.py:48] [11] global_step=11, grad_norm=2.09397, loss=0.272756
I0316 11:09:35.556652 140320504472768 submission.py:265] 11) loss = 0.273, grad_norm = 2.094
I0316 11:09:35.746812 140277834467072 logging_writer.py:48] [12] global_step=12, grad_norm=1.58693, loss=0.227422
I0316 11:09:35.750005 140320504472768 submission.py:265] 12) loss = 0.227, grad_norm = 1.587
I0316 11:09:35.942184 140277826074368 logging_writer.py:48] [13] global_step=13, grad_norm=1.06562, loss=0.192437
I0316 11:09:35.945206 140320504472768 submission.py:265] 13) loss = 0.192, grad_norm = 1.066
I0316 11:09:36.135807 140277834467072 logging_writer.py:48] [14] global_step=14, grad_norm=0.56422, loss=0.171861
I0316 11:09:36.139029 140320504472768 submission.py:265] 14) loss = 0.172, grad_norm = 0.564
I0316 11:09:36.338538 140277826074368 logging_writer.py:48] [15] global_step=15, grad_norm=0.149593, loss=0.164567
I0316 11:09:36.355953 140320504472768 submission.py:265] 15) loss = 0.165, grad_norm = 0.150
I0316 11:09:36.550662 140277834467072 logging_writer.py:48] [16] global_step=16, grad_norm=0.334471, loss=0.166675
I0316 11:09:36.554028 140320504472768 submission.py:265] 16) loss = 0.167, grad_norm = 0.334
I0316 11:09:36.745327 140277826074368 logging_writer.py:48] [17] global_step=17, grad_norm=0.615414, loss=0.175644
I0316 11:09:36.748293 140320504472768 submission.py:265] 17) loss = 0.176, grad_norm = 0.615
I0316 11:09:36.939772 140277834467072 logging_writer.py:48] [18] global_step=18, grad_norm=0.829579, loss=0.187772
I0316 11:09:36.942778 140320504472768 submission.py:265] 18) loss = 0.188, grad_norm = 0.830
I0316 11:09:37.132703 140277826074368 logging_writer.py:48] [19] global_step=19, grad_norm=0.884826, loss=0.186796
I0316 11:09:37.135654 140320504472768 submission.py:265] 19) loss = 0.187, grad_norm = 0.885
I0316 11:09:37.327473 140277834467072 logging_writer.py:48] [20] global_step=20, grad_norm=0.9926, loss=0.195429
I0316 11:09:37.331172 140320504472768 submission.py:265] 20) loss = 0.195, grad_norm = 0.993
I0316 11:09:37.525069 140277826074368 logging_writer.py:48] [21] global_step=21, grad_norm=1.05631, loss=0.201518
I0316 11:09:37.528533 140320504472768 submission.py:265] 21) loss = 0.202, grad_norm = 1.056
I0316 11:09:37.725801 140277834467072 logging_writer.py:48] [22] global_step=22, grad_norm=1.07478, loss=0.202849
I0316 11:09:37.730394 140320504472768 submission.py:265] 22) loss = 0.203, grad_norm = 1.075
I0316 11:09:37.923085 140277826074368 logging_writer.py:48] [23] global_step=23, grad_norm=1.07009, loss=0.201616
I0316 11:09:37.926730 140320504472768 submission.py:265] 23) loss = 0.202, grad_norm = 1.070
I0316 11:09:38.122480 140277834467072 logging_writer.py:48] [24] global_step=24, grad_norm=1.05198, loss=0.19938
I0316 11:09:38.125926 140320504472768 submission.py:265] 24) loss = 0.199, grad_norm = 1.052
I0316 11:09:38.326165 140277826074368 logging_writer.py:48] [25] global_step=25, grad_norm=1.01451, loss=0.195561
I0316 11:09:38.329355 140320504472768 submission.py:265] 25) loss = 0.196, grad_norm = 1.015
I0316 11:09:38.522798 140277834467072 logging_writer.py:48] [26] global_step=26, grad_norm=0.933297, loss=0.187473
I0316 11:09:38.525964 140320504472768 submission.py:265] 26) loss = 0.187, grad_norm = 0.933
I0316 11:09:38.718726 140277826074368 logging_writer.py:48] [27] global_step=27, grad_norm=0.854586, loss=0.181487
I0316 11:09:38.722052 140320504472768 submission.py:265] 27) loss = 0.181, grad_norm = 0.855
I0316 11:09:38.916894 140277834467072 logging_writer.py:48] [28] global_step=28, grad_norm=0.720303, loss=0.169963
I0316 11:09:38.920390 140320504472768 submission.py:265] 28) loss = 0.170, grad_norm = 0.720
I0316 11:09:39.110996 140277826074368 logging_writer.py:48] [29] global_step=29, grad_norm=0.585584, loss=0.162664
I0316 11:09:39.113960 140320504472768 submission.py:265] 29) loss = 0.163, grad_norm = 0.586
I0316 11:09:39.302996 140277834467072 logging_writer.py:48] [30] global_step=30, grad_norm=0.404818, loss=0.152384
I0316 11:09:39.305707 140320504472768 submission.py:265] 30) loss = 0.152, grad_norm = 0.405
I0316 11:09:40.293389 140277826074368 logging_writer.py:48] [31] global_step=31, grad_norm=0.244873, loss=0.149556
I0316 11:09:40.296627 140320504472768 submission.py:265] 31) loss = 0.150, grad_norm = 0.245
I0316 11:09:41.313815 140277834467072 logging_writer.py:48] [32] global_step=32, grad_norm=0.08628, loss=0.144922
I0316 11:09:41.316802 140320504472768 submission.py:265] 32) loss = 0.145, grad_norm = 0.086
I0316 11:09:42.580299 140277826074368 logging_writer.py:48] [33] global_step=33, grad_norm=0.101714, loss=0.146044
I0316 11:09:42.583384 140320504472768 submission.py:265] 33) loss = 0.146, grad_norm = 0.102
I0316 11:09:43.593181 140277834467072 logging_writer.py:48] [34] global_step=34, grad_norm=0.146631, loss=0.143654
I0316 11:09:43.596212 140320504472768 submission.py:265] 34) loss = 0.144, grad_norm = 0.147
I0316 11:09:44.732722 140277826074368 logging_writer.py:48] [35] global_step=35, grad_norm=0.11061, loss=0.142762
I0316 11:09:44.735636 140320504472768 submission.py:265] 35) loss = 0.143, grad_norm = 0.111
I0316 11:09:45.737427 140277834467072 logging_writer.py:48] [36] global_step=36, grad_norm=0.065654, loss=0.14136
I0316 11:09:45.740725 140320504472768 submission.py:265] 36) loss = 0.141, grad_norm = 0.066
I0316 11:09:47.101325 140277826074368 logging_writer.py:48] [37] global_step=37, grad_norm=0.0563893, loss=0.141731
I0316 11:09:47.104362 140320504472768 submission.py:265] 37) loss = 0.142, grad_norm = 0.056
I0316 11:09:48.255017 140277834467072 logging_writer.py:48] [38] global_step=38, grad_norm=0.0660505, loss=0.135671
I0316 11:09:48.258110 140320504472768 submission.py:265] 38) loss = 0.136, grad_norm = 0.066
I0316 11:09:49.508384 140277826074368 logging_writer.py:48] [39] global_step=39, grad_norm=0.0696075, loss=0.133155
I0316 11:09:49.511515 140320504472768 submission.py:265] 39) loss = 0.133, grad_norm = 0.070
I0316 11:09:50.707658 140277834467072 logging_writer.py:48] [40] global_step=40, grad_norm=0.068013, loss=0.133182
I0316 11:09:50.710878 140320504472768 submission.py:265] 40) loss = 0.133, grad_norm = 0.068
I0316 11:09:51.628411 140277826074368 logging_writer.py:48] [41] global_step=41, grad_norm=0.0501785, loss=0.133162
I0316 11:09:51.631781 140320504472768 submission.py:265] 41) loss = 0.133, grad_norm = 0.050
I0316 11:09:53.058791 140277834467072 logging_writer.py:48] [42] global_step=42, grad_norm=0.0307971, loss=0.132687
I0316 11:09:53.062041 140320504472768 submission.py:265] 42) loss = 0.133, grad_norm = 0.031
I0316 11:09:54.234171 140277826074368 logging_writer.py:48] [43] global_step=43, grad_norm=0.0250096, loss=0.130169
I0316 11:09:54.237255 140320504472768 submission.py:265] 43) loss = 0.130, grad_norm = 0.025
I0316 11:09:55.367020 140277834467072 logging_writer.py:48] [44] global_step=44, grad_norm=0.0357828, loss=0.131587
I0316 11:09:55.370144 140320504472768 submission.py:265] 44) loss = 0.132, grad_norm = 0.036
I0316 11:09:56.797457 140277826074368 logging_writer.py:48] [45] global_step=45, grad_norm=0.0267615, loss=0.130174
I0316 11:09:56.800669 140320504472768 submission.py:265] 45) loss = 0.130, grad_norm = 0.027
I0316 11:09:57.683637 140277834467072 logging_writer.py:48] [46] global_step=46, grad_norm=0.0277978, loss=0.130457
I0316 11:09:57.687195 140320504472768 submission.py:265] 46) loss = 0.130, grad_norm = 0.028
I0316 11:09:59.700128 140277826074368 logging_writer.py:48] [47] global_step=47, grad_norm=0.0266498, loss=0.128331
I0316 11:09:59.703687 140320504472768 submission.py:265] 47) loss = 0.128, grad_norm = 0.027
I0316 11:10:00.230922 140277834467072 logging_writer.py:48] [48] global_step=48, grad_norm=0.0278811, loss=0.127883
I0316 11:10:00.234462 140320504472768 submission.py:265] 48) loss = 0.128, grad_norm = 0.028
I0316 11:10:01.851069 140277826074368 logging_writer.py:48] [49] global_step=49, grad_norm=0.0240833, loss=0.129116
I0316 11:10:01.854392 140320504472768 submission.py:265] 49) loss = 0.129, grad_norm = 0.024
I0316 11:10:02.952929 140277834467072 logging_writer.py:48] [50] global_step=50, grad_norm=0.0226184, loss=0.129998
I0316 11:10:02.956302 140320504472768 submission.py:265] 50) loss = 0.130, grad_norm = 0.023
I0316 11:10:04.290258 140277826074368 logging_writer.py:48] [51] global_step=51, grad_norm=0.0222767, loss=0.130021
I0316 11:10:04.293335 140320504472768 submission.py:265] 51) loss = 0.130, grad_norm = 0.022
I0316 11:10:05.235524 140277834467072 logging_writer.py:48] [52] global_step=52, grad_norm=0.0172603, loss=0.127479
I0316 11:10:05.238668 140320504472768 submission.py:265] 52) loss = 0.127, grad_norm = 0.017
I0316 11:10:06.412598 140277826074368 logging_writer.py:48] [53] global_step=53, grad_norm=0.0166825, loss=0.125825
I0316 11:10:06.415862 140320504472768 submission.py:265] 53) loss = 0.126, grad_norm = 0.017
I0316 11:10:07.542360 140277834467072 logging_writer.py:48] [54] global_step=54, grad_norm=0.0261354, loss=0.128175
I0316 11:10:07.545510 140320504472768 submission.py:265] 54) loss = 0.128, grad_norm = 0.026
I0316 11:10:08.645613 140277826074368 logging_writer.py:48] [55] global_step=55, grad_norm=0.0561972, loss=0.124397
I0316 11:10:08.648824 140320504472768 submission.py:265] 55) loss = 0.124, grad_norm = 0.056
I0316 11:10:09.874326 140277834467072 logging_writer.py:48] [56] global_step=56, grad_norm=0.0829116, loss=0.125357
I0316 11:10:09.877523 140320504472768 submission.py:265] 56) loss = 0.125, grad_norm = 0.083
I0316 11:10:10.833304 140277826074368 logging_writer.py:48] [57] global_step=57, grad_norm=0.0232131, loss=0.137579
I0316 11:10:10.836353 140320504472768 submission.py:265] 57) loss = 0.138, grad_norm = 0.023
I0316 11:10:11.950447 140277834467072 logging_writer.py:48] [58] global_step=58, grad_norm=0.0137173, loss=0.141247
I0316 11:10:11.953756 140320504472768 submission.py:265] 58) loss = 0.141, grad_norm = 0.014
I0316 11:10:13.259955 140277826074368 logging_writer.py:48] [59] global_step=59, grad_norm=0.0239232, loss=0.141218
I0316 11:10:13.263144 140320504472768 submission.py:265] 59) loss = 0.141, grad_norm = 0.024
I0316 11:10:14.271639 140277834467072 logging_writer.py:48] [60] global_step=60, grad_norm=0.041699, loss=0.143609
I0316 11:10:14.274765 140320504472768 submission.py:265] 60) loss = 0.144, grad_norm = 0.042
I0316 11:10:15.498226 140277826074368 logging_writer.py:48] [61] global_step=61, grad_norm=0.0865034, loss=0.139999
I0316 11:10:15.501608 140320504472768 submission.py:265] 61) loss = 0.140, grad_norm = 0.087
I0316 11:10:16.455755 140277834467072 logging_writer.py:48] [62] global_step=62, grad_norm=0.141553, loss=0.143971
I0316 11:10:16.458796 140320504472768 submission.py:265] 62) loss = 0.144, grad_norm = 0.142
I0316 11:10:17.615463 140277826074368 logging_writer.py:48] [63] global_step=63, grad_norm=0.141167, loss=0.14231
I0316 11:10:17.618470 140320504472768 submission.py:265] 63) loss = 0.142, grad_norm = 0.141
I0316 11:10:18.717206 140277834467072 logging_writer.py:48] [64] global_step=64, grad_norm=0.0854167, loss=0.141868
I0316 11:10:18.720419 140320504472768 submission.py:265] 64) loss = 0.142, grad_norm = 0.085
I0316 11:10:19.723622 140277826074368 logging_writer.py:48] [65] global_step=65, grad_norm=0.0522829, loss=0.137122
I0316 11:10:19.726735 140320504472768 submission.py:265] 65) loss = 0.137, grad_norm = 0.052
I0316 11:10:20.939668 140277834467072 logging_writer.py:48] [66] global_step=66, grad_norm=0.0658319, loss=0.140642
I0316 11:10:20.942916 140320504472768 submission.py:265] 66) loss = 0.141, grad_norm = 0.066
I0316 11:10:21.980828 140277826074368 logging_writer.py:48] [67] global_step=67, grad_norm=0.0500883, loss=0.138571
I0316 11:10:21.984090 140320504472768 submission.py:265] 67) loss = 0.139, grad_norm = 0.050
I0316 11:10:22.959492 140277834467072 logging_writer.py:48] [68] global_step=68, grad_norm=0.0374347, loss=0.139302
I0316 11:10:22.962687 140320504472768 submission.py:265] 68) loss = 0.139, grad_norm = 0.037
I0316 11:10:24.822575 140277826074368 logging_writer.py:48] [69] global_step=69, grad_norm=0.0143305, loss=0.141455
I0316 11:10:24.825814 140320504472768 submission.py:265] 69) loss = 0.141, grad_norm = 0.014
I0316 11:10:25.968299 140277834467072 logging_writer.py:48] [70] global_step=70, grad_norm=0.0275927, loss=0.136648
I0316 11:10:25.971479 140320504472768 submission.py:265] 70) loss = 0.137, grad_norm = 0.028
I0316 11:10:27.758276 140277826074368 logging_writer.py:48] [71] global_step=71, grad_norm=0.0389194, loss=0.136797
I0316 11:10:27.761406 140320504472768 submission.py:265] 71) loss = 0.137, grad_norm = 0.039
I0316 11:10:28.651642 140277834467072 logging_writer.py:48] [72] global_step=72, grad_norm=0.0292761, loss=0.136848
I0316 11:10:28.654904 140320504472768 submission.py:265] 72) loss = 0.137, grad_norm = 0.029
I0316 11:10:30.510845 140277826074368 logging_writer.py:48] [73] global_step=73, grad_norm=0.031828, loss=0.138381
I0316 11:10:30.514099 140320504472768 submission.py:265] 73) loss = 0.138, grad_norm = 0.032
I0316 11:10:31.532601 140277834467072 logging_writer.py:48] [74] global_step=74, grad_norm=0.0480804, loss=0.135852
I0316 11:10:31.535645 140320504472768 submission.py:265] 74) loss = 0.136, grad_norm = 0.048
I0316 11:10:33.315849 140277826074368 logging_writer.py:48] [75] global_step=75, grad_norm=0.0602548, loss=0.135701
I0316 11:10:33.318924 140320504472768 submission.py:265] 75) loss = 0.136, grad_norm = 0.060
I0316 11:10:34.203253 140277834467072 logging_writer.py:48] [76] global_step=76, grad_norm=0.112246, loss=0.134508
I0316 11:10:34.206404 140320504472768 submission.py:265] 76) loss = 0.135, grad_norm = 0.112
I0316 11:10:35.423013 140277826074368 logging_writer.py:48] [77] global_step=77, grad_norm=0.095174, loss=0.131646
I0316 11:10:35.426066 140320504472768 submission.py:265] 77) loss = 0.132, grad_norm = 0.095
I0316 11:10:36.325426 140277834467072 logging_writer.py:48] [78] global_step=78, grad_norm=0.027927, loss=0.131837
I0316 11:10:36.328444 140320504472768 submission.py:265] 78) loss = 0.132, grad_norm = 0.028
I0316 11:10:37.520086 140277826074368 logging_writer.py:48] [79] global_step=79, grad_norm=0.0290052, loss=0.132988
I0316 11:10:37.523155 140320504472768 submission.py:265] 79) loss = 0.133, grad_norm = 0.029
I0316 11:10:38.429430 140277834467072 logging_writer.py:48] [80] global_step=80, grad_norm=0.030339, loss=0.130172
I0316 11:10:38.432435 140320504472768 submission.py:265] 80) loss = 0.130, grad_norm = 0.030
I0316 11:10:39.681611 140277826074368 logging_writer.py:48] [81] global_step=81, grad_norm=0.0428325, loss=0.133425
I0316 11:10:39.684788 140320504472768 submission.py:265] 81) loss = 0.133, grad_norm = 0.043
I0316 11:10:40.531450 140277834467072 logging_writer.py:48] [82] global_step=82, grad_norm=0.0495839, loss=0.13119
I0316 11:10:40.534695 140320504472768 submission.py:265] 82) loss = 0.131, grad_norm = 0.050
I0316 11:10:41.864789 140277826074368 logging_writer.py:48] [83] global_step=83, grad_norm=0.0407173, loss=0.131975
I0316 11:10:41.867736 140320504472768 submission.py:265] 83) loss = 0.132, grad_norm = 0.041
I0316 11:10:42.862224 140277834467072 logging_writer.py:48] [84] global_step=84, grad_norm=0.0318343, loss=0.131671
I0316 11:10:42.865403 140320504472768 submission.py:265] 84) loss = 0.132, grad_norm = 0.032
I0316 11:10:44.240523 140277826074368 logging_writer.py:48] [85] global_step=85, grad_norm=0.0261016, loss=0.13193
I0316 11:10:44.243724 140320504472768 submission.py:265] 85) loss = 0.132, grad_norm = 0.026
I0316 11:10:45.212546 140277834467072 logging_writer.py:48] [86] global_step=86, grad_norm=0.025336, loss=0.131988
I0316 11:10:45.215699 140320504472768 submission.py:265] 86) loss = 0.132, grad_norm = 0.025
I0316 11:10:46.861536 140277826074368 logging_writer.py:48] [87] global_step=87, grad_norm=0.0150817, loss=0.131056
I0316 11:10:46.864785 140320504472768 submission.py:265] 87) loss = 0.131, grad_norm = 0.015
I0316 11:10:47.776546 140277834467072 logging_writer.py:48] [88] global_step=88, grad_norm=0.0107649, loss=0.131443
I0316 11:10:47.779676 140320504472768 submission.py:265] 88) loss = 0.131, grad_norm = 0.011
I0316 11:10:48.980871 140277826074368 logging_writer.py:48] [89] global_step=89, grad_norm=0.00731205, loss=0.131413
I0316 11:10:48.984483 140320504472768 submission.py:265] 89) loss = 0.131, grad_norm = 0.007
I0316 11:10:50.082798 140277834467072 logging_writer.py:48] [90] global_step=90, grad_norm=0.00706544, loss=0.132112
I0316 11:10:50.086203 140320504472768 submission.py:265] 90) loss = 0.132, grad_norm = 0.007
I0316 11:10:51.483312 140277826074368 logging_writer.py:48] [91] global_step=91, grad_norm=0.0115834, loss=0.129314
I0316 11:10:51.486392 140320504472768 submission.py:265] 91) loss = 0.129, grad_norm = 0.012
I0316 11:10:52.356783 140277834467072 logging_writer.py:48] [92] global_step=92, grad_norm=0.0307337, loss=0.132262
I0316 11:10:52.360086 140320504472768 submission.py:265] 92) loss = 0.132, grad_norm = 0.031
I0316 11:10:53.852055 140277826074368 logging_writer.py:48] [93] global_step=93, grad_norm=0.0553284, loss=0.129663
I0316 11:10:53.855359 140320504472768 submission.py:265] 93) loss = 0.130, grad_norm = 0.055
I0316 11:10:54.743212 140277834467072 logging_writer.py:48] [94] global_step=94, grad_norm=0.0792813, loss=0.129729
I0316 11:10:54.746469 140320504472768 submission.py:265] 94) loss = 0.130, grad_norm = 0.079
I0316 11:10:55.883697 140277826074368 logging_writer.py:48] [95] global_step=95, grad_norm=0.0827705, loss=0.128849
I0316 11:10:55.886722 140320504472768 submission.py:265] 95) loss = 0.129, grad_norm = 0.083
I0316 11:10:56.990858 140277834467072 logging_writer.py:48] [96] global_step=96, grad_norm=0.0588707, loss=0.12519
I0316 11:10:56.993934 140320504472768 submission.py:265] 96) loss = 0.125, grad_norm = 0.059
I0316 11:10:58.651950 140277826074368 logging_writer.py:48] [97] global_step=97, grad_norm=0.0362296, loss=0.125981
I0316 11:10:58.655237 140320504472768 submission.py:265] 97) loss = 0.126, grad_norm = 0.036
I0316 11:10:59.723774 140277834467072 logging_writer.py:48] [98] global_step=98, grad_norm=0.0157967, loss=0.123606
I0316 11:10:59.726794 140320504472768 submission.py:265] 98) loss = 0.124, grad_norm = 0.016
I0316 11:11:02.720362 140277826074368 logging_writer.py:48] [99] global_step=99, grad_norm=0.00614934, loss=0.125412
I0316 11:11:02.723953 140320504472768 submission.py:265] 99) loss = 0.125, grad_norm = 0.006
I0316 11:11:02.917253 140277834467072 logging_writer.py:48] [100] global_step=100, grad_norm=0.00679512, loss=0.125362
I0316 11:11:02.920667 140320504472768 submission.py:265] 100) loss = 0.125, grad_norm = 0.007
I0316 11:11:34.399190 140320504472768 spec.py:321] Evaluating on the training split.
I0316 11:16:48.492882 140320504472768 spec.py:333] Evaluating on the validation split.
I0316 11:21:18.428642 140320504472768 spec.py:349] Evaluating on the test split.
I0316 11:26:29.260708 140320504472768 submission_runner.py:469] Time since start: 1900.92s, 	Step: 128, 	{'train/loss': 0.12856768605123584, 'validation/loss': 0.12900573719172526, 'validation/num_examples': 83274637, 'test/loss': 0.13186845210511056, 'test/num_examples': 95000000, 'score': 125.10863494873047, 'total_duration': 1900.9230625629425, 'accumulated_submission_time': 125.10863494873047, 'accumulated_eval_time': 1774.4666442871094, 'accumulated_logging_time': 0.05906033515930176}
I0316 11:26:29.270900 140277826074368 logging_writer.py:48] [128] accumulated_eval_time=1774.47, accumulated_logging_time=0.0590603, accumulated_submission_time=125.109, global_step=128, preemption_count=0, score=125.109, test/loss=0.131868, test/num_examples=95000000, total_duration=1900.92, train/loss=0.128568, validation/loss=0.129006, validation/num_examples=83274637
/usr/local/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
I0316 11:28:30.754664 140320504472768 spec.py:321] Evaluating on the training split.
I0316 11:33:33.237913 140320504472768 spec.py:333] Evaluating on the validation split.
I0316 11:37:57.057554 140320504472768 spec.py:349] Evaluating on the test split.
I0316 11:43:02.821322 140320504472768 submission_runner.py:469] Time since start: 2894.48s, 	Step: 251, 	{'train/loss': 0.1284102053445997, 'validation/loss': 0.12861962093718016, 'validation/num_examples': 83274637, 'test/loss': 0.13107873916581805, 'test/num_examples': 95000000, 'score': 245.6804003715515, 'total_duration': 2894.483717918396, 'accumulated_submission_time': 245.6804003715515, 'accumulated_eval_time': 2646.5334374904633, 'accumulated_logging_time': 0.07641792297363281}
I0316 11:43:02.831402 140277834467072 logging_writer.py:48] [251] accumulated_eval_time=2646.53, accumulated_logging_time=0.0764179, accumulated_submission_time=245.68, global_step=251, preemption_count=0, score=245.68, test/loss=0.131079, test/num_examples=95000000, total_duration=2894.48, train/loss=0.12841, validation/loss=0.12862, validation/num_examples=83274637
I0316 11:45:04.520935 140320504472768 spec.py:321] Evaluating on the training split.
I0316 11:50:16.072262 140320504472768 spec.py:333] Evaluating on the validation split.
I0316 11:54:47.266020 140320504472768 spec.py:349] Evaluating on the test split.
I0316 11:59:56.923297 140320504472768 submission_runner.py:469] Time since start: 3908.59s, 	Step: 376, 	{'train/loss': 0.12735854272199118, 'validation/loss': 0.12753445684334078, 'validation/num_examples': 83274637, 'test/loss': 0.12998620982878836, 'test/num_examples': 95000000, 'score': 366.4601035118103, 'total_duration': 3908.585577249527, 'accumulated_submission_time': 366.4601035118103, 'accumulated_eval_time': 3538.935889482498, 'accumulated_logging_time': 0.09353375434875488}
I0316 11:59:56.933674 140277826074368 logging_writer.py:48] [376] accumulated_eval_time=3538.94, accumulated_logging_time=0.0935338, accumulated_submission_time=366.46, global_step=376, preemption_count=0, score=366.46, test/loss=0.129986, test/num_examples=95000000, total_duration=3908.59, train/loss=0.127359, validation/loss=0.127534, validation/num_examples=83274637
I0316 12:01:57.678669 140320504472768 spec.py:321] Evaluating on the training split.
I0316 12:07:04.243836 140320504472768 spec.py:333] Evaluating on the validation split.
I0316 12:11:30.201770 140320504472768 spec.py:349] Evaluating on the test split.
I0316 12:16:28.364509 140320504472768 submission_runner.py:469] Time since start: 4900.03s, 	Step: 500, 	{'train/loss': 0.12708554649395756, 'validation/loss': 0.12749320697580657, 'validation/num_examples': 83274637, 'test/loss': 0.12992275944294177, 'test/num_examples': 95000000, 'score': 486.2921693325043, 'total_duration': 4900.026757717133, 'accumulated_submission_time': 486.2921693325043, 'accumulated_eval_time': 4409.6216785907745, 'accumulated_logging_time': 0.12847232818603516}
I0316 12:16:28.373783 140277834467072 logging_writer.py:48] [500] accumulated_eval_time=4409.62, accumulated_logging_time=0.128472, accumulated_submission_time=486.292, global_step=500, preemption_count=0, score=486.292, test/loss=0.129923, test/num_examples=95000000, total_duration=4900.03, train/loss=0.127086, validation/loss=0.127493, validation/num_examples=83274637
I0316 12:16:29.007795 140277826074368 logging_writer.py:48] [500] global_step=500, grad_norm=0.079076, loss=0.131652
I0316 12:16:29.011094 140320504472768 submission.py:265] 500) loss = 0.132, grad_norm = 0.079
I0316 12:18:28.962216 140320504472768 spec.py:321] Evaluating on the training split.
I0316 12:23:41.667630 140320504472768 spec.py:333] Evaluating on the validation split.
I0316 12:28:13.922142 140320504472768 spec.py:349] Evaluating on the test split.
I0316 12:33:23.224824 140320504472768 submission_runner.py:469] Time since start: 5914.89s, 	Step: 627, 	{'train/loss': 0.1260084924384286, 'validation/loss': 0.12660912280631412, 'validation/num_examples': 83274637, 'test/loss': 0.12901285073595548, 'test/num_examples': 95000000, 'score': 606.0163090229034, 'total_duration': 5914.887201309204, 'accumulated_submission_time': 606.0163090229034, 'accumulated_eval_time': 5303.884338617325, 'accumulated_logging_time': 0.14438748359680176}
I0316 12:33:23.234657 140277834467072 logging_writer.py:48] [627] accumulated_eval_time=5303.88, accumulated_logging_time=0.144387, accumulated_submission_time=606.016, global_step=627, preemption_count=0, score=606.016, test/loss=0.129013, test/num_examples=95000000, total_duration=5914.89, train/loss=0.126008, validation/loss=0.126609, validation/num_examples=83274637
I0316 12:35:23.765764 140320504472768 spec.py:321] Evaluating on the training split.
I0316 12:40:33.566294 140320504472768 spec.py:333] Evaluating on the validation split.
I0316 12:45:04.251036 140320504472768 spec.py:349] Evaluating on the test split.
I0316 12:50:07.421757 140320504472768 submission_runner.py:469] Time since start: 6919.08s, 	Step: 750, 	{'train/loss': 0.12368760540360996, 'validation/loss': 0.1270288564166763, 'validation/num_examples': 83274637, 'test/loss': 0.12958252743353593, 'test/num_examples': 95000000, 'score': 725.6278722286224, 'total_duration': 6919.084104776382, 'accumulated_submission_time': 725.6278722286224, 'accumulated_eval_time': 6187.54043340683, 'accumulated_logging_time': 0.16140508651733398}
I0316 12:50:07.432157 140277826074368 logging_writer.py:48] [750] accumulated_eval_time=6187.54, accumulated_logging_time=0.161405, accumulated_submission_time=725.628, global_step=750, preemption_count=0, score=725.628, test/loss=0.129583, test/num_examples=95000000, total_duration=6919.08, train/loss=0.123688, validation/loss=0.127029, validation/num_examples=83274637
I0316 12:52:08.248330 140320504472768 spec.py:321] Evaluating on the training split.
I0316 12:57:15.394951 140320504472768 spec.py:333] Evaluating on the validation split.
I0316 13:01:45.651203 140320504472768 spec.py:349] Evaluating on the test split.
I0316 13:06:48.423925 140320504472768 submission_runner.py:469] Time since start: 7920.09s, 	Step: 870, 	{'train/loss': 0.12714357292184325, 'validation/loss': 0.12675777746777478, 'validation/num_examples': 83274637, 'test/loss': 0.12930450026273224, 'test/num_examples': 95000000, 'score': 845.5629544258118, 'total_duration': 7920.086271762848, 'accumulated_submission_time': 845.5629544258118, 'accumulated_eval_time': 7067.716192007065, 'accumulated_logging_time': 0.1791973114013672}
I0316 13:06:48.434077 140277834467072 logging_writer.py:48] [870] accumulated_eval_time=7067.72, accumulated_logging_time=0.179197, accumulated_submission_time=845.563, global_step=870, preemption_count=0, score=845.563, test/loss=0.129305, test/num_examples=95000000, total_duration=7920.09, train/loss=0.127144, validation/loss=0.126758, validation/num_examples=83274637
I0316 13:08:49.490171 140320504472768 spec.py:321] Evaluating on the training split.
I0316 13:14:00.259324 140320504472768 spec.py:333] Evaluating on the validation split.
I0316 13:18:25.079328 140320504472768 spec.py:349] Evaluating on the test split.
I0316 13:23:32.084246 140320504472768 submission_runner.py:469] Time since start: 8923.75s, 	Step: 994, 	{'train/loss': 0.1246893942172337, 'validation/loss': 0.1259469950003855, 'validation/num_examples': 83274637, 'test/loss': 0.1284691206574691, 'test/num_examples': 95000000, 'score': 965.7636876106262, 'total_duration': 8923.74655532837, 'accumulated_submission_time': 965.7636876106262, 'accumulated_eval_time': 7950.310286998749, 'accumulated_logging_time': 0.19660258293151855}
I0316 13:23:32.093486 140277826074368 logging_writer.py:48] [994] accumulated_eval_time=7950.31, accumulated_logging_time=0.196603, accumulated_submission_time=965.764, global_step=994, preemption_count=0, score=965.764, test/loss=0.128469, test/num_examples=95000000, total_duration=8923.75, train/loss=0.124689, validation/loss=0.125947, validation/num_examples=83274637
I0316 13:23:33.881672 140277834467072 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.0445795, loss=0.132684
I0316 13:23:33.884886 140320504472768 submission.py:265] 1000) loss = 0.133, grad_norm = 0.045
I0316 13:25:32.556027 140320504472768 spec.py:321] Evaluating on the training split.
I0316 13:30:32.098458 140320504472768 spec.py:333] Evaluating on the validation split.
I0316 13:34:52.740805 140320504472768 spec.py:349] Evaluating on the test split.
I0316 13:39:46.807108 140320504472768 submission_runner.py:469] Time since start: 9898.47s, 	Step: 1113, 	{'train/loss': 0.12279664347534214, 'validation/loss': 0.12598902059281877, 'validation/num_examples': 83274637, 'test/loss': 0.12829156227509347, 'test/num_examples': 95000000, 'score': 1085.4407052993774, 'total_duration': 9898.469440221786, 'accumulated_submission_time': 1085.4407052993774, 'accumulated_eval_time': 8804.561419010162, 'accumulated_logging_time': 0.2129683494567871}
I0316 13:39:46.816774 140277826074368 logging_writer.py:48] [1113] accumulated_eval_time=8804.56, accumulated_logging_time=0.212968, accumulated_submission_time=1085.44, global_step=1113, preemption_count=0, score=1085.44, test/loss=0.128292, test/num_examples=95000000, total_duration=9898.47, train/loss=0.122797, validation/loss=0.125989, validation/num_examples=83274637
I0316 13:41:47.715478 140320504472768 spec.py:321] Evaluating on the training split.
I0316 13:46:31.167938 140320504472768 spec.py:333] Evaluating on the validation split.
I0316 13:50:50.987864 140320504472768 spec.py:349] Evaluating on the test split.
I0316 13:55:46.194727 140320504472768 submission_runner.py:469] Time since start: 10857.86s, 	Step: 1240, 	{'train/loss': 0.12582157321642376, 'validation/loss': 0.1258041458397921, 'validation/num_examples': 83274637, 'test/loss': 0.12804067429620844, 'test/num_examples': 95000000, 'score': 1205.3944432735443, 'total_duration': 10857.857027292252, 'accumulated_submission_time': 1205.3944432735443, 'accumulated_eval_time': 9643.040719032288, 'accumulated_logging_time': 0.2798757553100586}
I0316 13:55:46.204495 140277834467072 logging_writer.py:48] [1240] accumulated_eval_time=9643.04, accumulated_logging_time=0.279876, accumulated_submission_time=1205.39, global_step=1240, preemption_count=0, score=1205.39, test/loss=0.128041, test/num_examples=95000000, total_duration=10857.9, train/loss=0.125822, validation/loss=0.125804, validation/num_examples=83274637
I0316 13:57:46.955997 140320504472768 spec.py:321] Evaluating on the training split.
I0316 14:01:57.254350 140320504472768 spec.py:333] Evaluating on the validation split.
I0316 14:06:27.735069 140320504472768 spec.py:349] Evaluating on the test split.
I0316 14:11:23.527610 140320504472768 submission_runner.py:469] Time since start: 11795.19s, 	Step: 1361, 	{'train/loss': 0.12558539015069287, 'validation/loss': 0.12655573328842815, 'validation/num_examples': 83274637, 'test/loss': 0.12892446824617887, 'test/num_examples': 95000000, 'score': 1325.249942779541, 'total_duration': 11795.189979553223, 'accumulated_submission_time': 1325.249942779541, 'accumulated_eval_time': 10459.612497806549, 'accumulated_logging_time': 0.29643678665161133}
I0316 14:11:23.538771 140277826074368 logging_writer.py:48] [1361] accumulated_eval_time=10459.6, accumulated_logging_time=0.296437, accumulated_submission_time=1325.25, global_step=1361, preemption_count=0, score=1325.25, test/loss=0.128924, test/num_examples=95000000, total_duration=11795.2, train/loss=0.125585, validation/loss=0.126556, validation/num_examples=83274637
I0316 14:13:24.389712 140320504472768 spec.py:321] Evaluating on the training split.
I0316 14:16:37.409898 140320504472768 spec.py:333] Evaluating on the validation split.
I0316 14:21:09.517911 140320504472768 spec.py:349] Evaluating on the test split.
I0316 14:26:06.639927 140320504472768 submission_runner.py:469] Time since start: 12678.30s, 	Step: 1485, 	{'train/loss': 0.12453780532269473, 'validation/loss': 0.1258493718909249, 'validation/num_examples': 83274637, 'test/loss': 0.12802018090499076, 'test/num_examples': 95000000, 'score': 1445.1929314136505, 'total_duration': 12678.302263259888, 'accumulated_submission_time': 1445.1929314136505, 'accumulated_eval_time': 11221.862795352936, 'accumulated_logging_time': 0.3144991397857666}
I0316 14:26:06.649624 140277834467072 logging_writer.py:48] [1485] accumulated_eval_time=11221.9, accumulated_logging_time=0.314499, accumulated_submission_time=1445.19, global_step=1485, preemption_count=0, score=1445.19, test/loss=0.12802, test/num_examples=95000000, total_duration=12678.3, train/loss=0.124538, validation/loss=0.125849, validation/num_examples=83274637
I0316 14:26:10.118973 140277826074368 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.0777143, loss=0.131659
I0316 14:26:10.122858 140320504472768 submission.py:265] 1500) loss = 0.132, grad_norm = 0.078
I0316 14:28:07.923151 140320504472768 spec.py:321] Evaluating on the training split.
I0316 14:30:14.366627 140320504472768 spec.py:333] Evaluating on the validation split.
I0316 14:34:48.186834 140320504472768 spec.py:349] Evaluating on the test split.
I0316 14:39:39.923599 140320504472768 submission_runner.py:469] Time since start: 13491.59s, 	Step: 1607, 	{'train/loss': 0.1246877809948545, 'validation/loss': 0.12683965089090804, 'validation/num_examples': 83274637, 'test/loss': 0.1292577207273383, 'test/num_examples': 95000000, 'score': 1565.5892806053162, 'total_duration': 13491.585928440094, 'accumulated_submission_time': 1565.5892806053162, 'accumulated_eval_time': 11913.863302707672, 'accumulated_logging_time': 0.3306610584259033}
I0316 14:39:39.935345 140277834467072 logging_writer.py:48] [1607] accumulated_eval_time=11913.9, accumulated_logging_time=0.330661, accumulated_submission_time=1565.59, global_step=1607, preemption_count=0, score=1565.59, test/loss=0.129258, test/num_examples=95000000, total_duration=13491.6, train/loss=0.124688, validation/loss=0.12684, validation/num_examples=83274637
I0316 14:41:40.650606 140320504472768 spec.py:321] Evaluating on the training split.
I0316 14:43:45.166774 140320504472768 spec.py:333] Evaluating on the validation split.
I0316 14:48:25.651127 140320504472768 spec.py:349] Evaluating on the test split.
I0316 14:53:24.615516 140320504472768 submission_runner.py:469] Time since start: 14316.28s, 	Step: 1727, 	{'train/loss': 0.1251219073043905, 'validation/loss': 0.1251358218279607, 'validation/num_examples': 83274637, 'test/loss': 0.12742741135173596, 'test/num_examples': 95000000, 'score': 1685.4304251670837, 'total_duration': 14316.277921676636, 'accumulated_submission_time': 1685.4304251670837, 'accumulated_eval_time': 12617.82832455635, 'accumulated_logging_time': 0.34963393211364746}
I0316 14:53:24.625792 140277826074368 logging_writer.py:48] [1727] accumulated_eval_time=12617.8, accumulated_logging_time=0.349634, accumulated_submission_time=1685.43, global_step=1727, preemption_count=0, score=1685.43, test/loss=0.127427, test/num_examples=95000000, total_duration=14316.3, train/loss=0.125122, validation/loss=0.125136, validation/num_examples=83274637
I0316 14:55:25.720305 140320504472768 spec.py:321] Evaluating on the training split.
I0316 14:57:35.574489 140320504472768 spec.py:333] Evaluating on the validation split.
I0316 15:02:05.007318 140320504472768 spec.py:349] Evaluating on the test split.
I0316 15:07:01.206447 140320504472768 submission_runner.py:469] Time since start: 15132.87s, 	Step: 1851, 	{'train/loss': 0.12354361350308593, 'validation/loss': 0.12588235492896954, 'validation/num_examples': 83274637, 'test/loss': 0.12846187821229635, 'test/num_examples': 95000000, 'score': 1805.6695804595947, 'total_duration': 15132.868842840195, 'accumulated_submission_time': 1805.6695804595947, 'accumulated_eval_time': 13313.314613819122, 'accumulated_logging_time': 0.36646342277526855}
I0316 15:07:01.216334 140277834467072 logging_writer.py:48] [1851] accumulated_eval_time=13313.3, accumulated_logging_time=0.366463, accumulated_submission_time=1805.67, global_step=1851, preemption_count=0, score=1805.67, test/loss=0.128462, test/num_examples=95000000, total_duration=15132.9, train/loss=0.123544, validation/loss=0.125882, validation/num_examples=83274637
I0316 15:09:02.673966 140320504472768 spec.py:321] Evaluating on the training split.
I0316 15:11:13.079546 140320504472768 spec.py:333] Evaluating on the validation split.
I0316 15:15:46.773674 140320504472768 spec.py:349] Evaluating on the test split.
I0316 15:20:44.017203 140320504472768 submission_runner.py:469] Time since start: 15955.68s, 	Step: 1972, 	{'train/loss': 0.12352700453264792, 'validation/loss': 0.12599491511378536, 'validation/num_examples': 83274637, 'test/loss': 0.12979578342084383, 'test/num_examples': 95000000, 'score': 1926.2673420906067, 'total_duration': 15955.679548740387, 'accumulated_submission_time': 1926.2673420906067, 'accumulated_eval_time': 14014.657854557037, 'accumulated_logging_time': 0.38373827934265137}
I0316 15:20:44.027203 140277826074368 logging_writer.py:48] [1972] accumulated_eval_time=14014.7, accumulated_logging_time=0.383738, accumulated_submission_time=1926.27, global_step=1972, preemption_count=0, score=1926.27, test/loss=0.129796, test/num_examples=95000000, total_duration=15955.7, train/loss=0.123527, validation/loss=0.125995, validation/num_examples=83274637
I0316 15:20:49.944572 140277834467072 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.00970946, loss=0.122568
I0316 15:20:49.948040 140320504472768 submission.py:265] 2000) loss = 0.123, grad_norm = 0.010
I0316 15:22:45.592054 140320504472768 spec.py:321] Evaluating on the training split.
I0316 15:24:51.244798 140320504472768 spec.py:333] Evaluating on the validation split.
I0316 15:29:21.696554 140320504472768 spec.py:349] Evaluating on the test split.
I0316 15:34:16.256630 140320504472768 submission_runner.py:469] Time since start: 16767.92s, 	Step: 2095, 	{'train/loss': 0.12361880912496422, 'validation/loss': 0.12570947496492227, 'validation/num_examples': 83274637, 'test/loss': 0.1280773653891714, 'test/num_examples': 95000000, 'score': 2046.9592020511627, 'total_duration': 16767.918994426727, 'accumulated_submission_time': 2046.9592020511627, 'accumulated_eval_time': 14705.322610139847, 'accumulated_logging_time': 0.40008115768432617}
I0316 15:34:16.267003 140277826074368 logging_writer.py:48] [2095] accumulated_eval_time=14705.3, accumulated_logging_time=0.400081, accumulated_submission_time=2046.96, global_step=2095, preemption_count=0, score=2046.96, test/loss=0.128077, test/num_examples=95000000, total_duration=16767.9, train/loss=0.123619, validation/loss=0.125709, validation/num_examples=83274637
I0316 15:36:17.465667 140320504472768 spec.py:321] Evaluating on the training split.
I0316 15:38:27.925175 140320504472768 spec.py:333] Evaluating on the validation split.
I0316 15:43:02.805675 140320504472768 spec.py:349] Evaluating on the test split.
I0316 15:48:08.067842 140320504472768 submission_runner.py:469] Time since start: 17599.73s, 	Step: 2211, 	{'train/loss': 0.12494560092824658, 'validation/loss': 0.1260863386659836, 'validation/num_examples': 83274637, 'test/loss': 0.12839387804621646, 'test/num_examples': 95000000, 'score': 2167.274055957794, 'total_duration': 17599.730130434036, 'accumulated_submission_time': 2167.274055957794, 'accumulated_eval_time': 15415.924892187119, 'accumulated_logging_time': 0.4172022342681885}
I0316 15:48:08.077764 140277834467072 logging_writer.py:48] [2211] accumulated_eval_time=15415.9, accumulated_logging_time=0.417202, accumulated_submission_time=2167.27, global_step=2211, preemption_count=0, score=2167.27, test/loss=0.128394, test/num_examples=95000000, total_duration=17599.7, train/loss=0.124946, validation/loss=0.126086, validation/num_examples=83274637
I0316 15:50:09.434412 140320504472768 spec.py:321] Evaluating on the training split.
I0316 15:52:17.137912 140320504472768 spec.py:333] Evaluating on the validation split.
I0316 15:56:48.951996 140320504472768 spec.py:349] Evaluating on the test split.
I0316 16:01:49.416240 140320504472768 submission_runner.py:469] Time since start: 18421.08s, 	Step: 2330, 	{'train/loss': 0.1252426137627725, 'validation/loss': 0.1250652802651992, 'validation/num_examples': 83274637, 'test/loss': 0.1274694749838578, 'test/num_examples': 95000000, 'score': 2287.8169384002686, 'total_duration': 18421.078630447388, 'accumulated_submission_time': 2287.8169384002686, 'accumulated_eval_time': 16115.90685582161, 'accumulated_logging_time': 0.43357372283935547}
I0316 16:01:49.426254 140277826074368 logging_writer.py:48] [2330] accumulated_eval_time=16115.9, accumulated_logging_time=0.433574, accumulated_submission_time=2287.82, global_step=2330, preemption_count=0, score=2287.82, test/loss=0.127469, test/num_examples=95000000, total_duration=18421.1, train/loss=0.125243, validation/loss=0.125065, validation/num_examples=83274637
I0316 16:03:50.294386 140320504472768 spec.py:321] Evaluating on the training split.
I0316 16:06:02.409731 140320504472768 spec.py:333] Evaluating on the validation split.
I0316 16:10:30.574353 140320504472768 spec.py:349] Evaluating on the test split.
I0316 16:15:22.200620 140320504472768 submission_runner.py:469] Time since start: 19233.86s, 	Step: 2452, 	{'train/loss': 0.12312741085369167, 'validation/loss': 0.1252291737124209, 'validation/num_examples': 83274637, 'test/loss': 0.12765323103344567, 'test/num_examples': 95000000, 'score': 2407.816363096237, 'total_duration': 19233.863020181656, 'accumulated_submission_time': 2407.816363096237, 'accumulated_eval_time': 16807.813301324844, 'accumulated_logging_time': 0.4501988887786865}
I0316 16:15:22.210667 140277834467072 logging_writer.py:48] [2452] accumulated_eval_time=16807.8, accumulated_logging_time=0.450199, accumulated_submission_time=2407.82, global_step=2452, preemption_count=0, score=2407.82, test/loss=0.127653, test/num_examples=95000000, total_duration=19233.9, train/loss=0.123127, validation/loss=0.125229, validation/num_examples=83274637
I0316 16:15:51.763166 140277826074368 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.0320796, loss=0.125811
I0316 16:15:51.766332 140320504472768 submission.py:265] 2500) loss = 0.126, grad_norm = 0.032
I0316 16:17:22.908009 140320504472768 spec.py:321] Evaluating on the training split.
I0316 16:19:39.893437 140320504472768 spec.py:333] Evaluating on the validation split.
I0316 16:24:10.806027 140320504472768 spec.py:349] Evaluating on the test split.
I0316 16:29:24.216195 140320504472768 submission_runner.py:469] Time since start: 20075.88s, 	Step: 2573, 	{'train/loss': 0.12493995157768172, 'validation/loss': 0.1251973874930707, 'validation/num_examples': 83274637, 'test/loss': 0.12758159990892912, 'test/num_examples': 95000000, 'score': 2527.6011111736298, 'total_duration': 20075.878520727158, 'accumulated_submission_time': 2527.6011111736298, 'accumulated_eval_time': 17529.121607780457, 'accumulated_logging_time': 0.46643900871276855}
I0316 16:29:24.227851 140277834467072 logging_writer.py:48] [2573] accumulated_eval_time=17529.1, accumulated_logging_time=0.466439, accumulated_submission_time=2527.6, global_step=2573, preemption_count=0, score=2527.6, test/loss=0.127582, test/num_examples=95000000, total_duration=20075.9, train/loss=0.12494, validation/loss=0.125197, validation/num_examples=83274637
I0316 16:31:24.640363 140320504472768 spec.py:321] Evaluating on the training split.
I0316 16:33:30.617584 140320504472768 spec.py:333] Evaluating on the validation split.
I0316 16:38:02.094758 140320504472768 spec.py:349] Evaluating on the test split.
I0316 16:42:59.066706 140320504472768 submission_runner.py:469] Time since start: 20890.73s, 	Step: 2698, 	{'train/loss': 0.12251138401452467, 'validation/loss': 0.1248545827652636, 'validation/num_examples': 83274637, 'test/loss': 0.12719171517582945, 'test/num_examples': 95000000, 'score': 2647.179775238037, 'total_duration': 20890.72908592224, 'accumulated_submission_time': 2647.179775238037, 'accumulated_eval_time': 18223.54805278778, 'accumulated_logging_time': 0.48497486114501953}
I0316 16:42:59.076602 140277826074368 logging_writer.py:48] [2698] accumulated_eval_time=18223.5, accumulated_logging_time=0.484975, accumulated_submission_time=2647.18, global_step=2698, preemption_count=0, score=2647.18, test/loss=0.127192, test/num_examples=95000000, total_duration=20890.7, train/loss=0.122511, validation/loss=0.124855, validation/num_examples=83274637
I0316 16:44:59.504599 140320504472768 spec.py:321] Evaluating on the training split.
I0316 16:47:05.538307 140320504472768 spec.py:333] Evaluating on the validation split.
I0316 16:51:37.527905 140320504472768 spec.py:349] Evaluating on the test split.
I0316 16:56:35.775716 140320504472768 submission_runner.py:469] Time since start: 21707.44s, 	Step: 2817, 	{'train/loss': 0.1229249759559442, 'validation/loss': 0.12499534662610047, 'validation/num_examples': 83274637, 'test/loss': 0.12727011013436568, 'test/num_examples': 95000000, 'score': 2766.760534763336, 'total_duration': 21707.43790435791, 'accumulated_submission_time': 2766.760534763336, 'accumulated_eval_time': 18919.819051504135, 'accumulated_logging_time': 0.5241272449493408}
I0316 16:56:35.786480 140277834467072 logging_writer.py:48] [2817] accumulated_eval_time=18919.8, accumulated_logging_time=0.524127, accumulated_submission_time=2766.76, global_step=2817, preemption_count=0, score=2766.76, test/loss=0.12727, test/num_examples=95000000, total_duration=21707.4, train/loss=0.122925, validation/loss=0.124995, validation/num_examples=83274637
I0316 16:58:37.587932 140320504472768 spec.py:321] Evaluating on the training split.
I0316 17:00:42.821731 140320504472768 spec.py:333] Evaluating on the validation split.
I0316 17:05:10.007593 140320504472768 spec.py:349] Evaluating on the test split.
I0316 17:10:00.581267 140320504472768 submission_runner.py:469] Time since start: 22512.24s, 	Step: 2942, 	{'train/loss': 0.12361849805740696, 'validation/loss': 0.12535044312703292, 'validation/num_examples': 83274637, 'test/loss': 0.12785298301761025, 'test/num_examples': 95000000, 'score': 2887.668531894684, 'total_duration': 22512.24366760254, 'accumulated_submission_time': 2887.668531894684, 'accumulated_eval_time': 19602.8125808239, 'accumulated_logging_time': 0.5416979789733887}
I0316 17:10:00.591676 140277826074368 logging_writer.py:48] [2942] accumulated_eval_time=19602.8, accumulated_logging_time=0.541698, accumulated_submission_time=2887.67, global_step=2942, preemption_count=0, score=2887.67, test/loss=0.127853, test/num_examples=95000000, total_duration=22512.2, train/loss=0.123618, validation/loss=0.12535, validation/num_examples=83274637
I0316 17:10:43.628786 140277834467072 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.00741561, loss=0.12526
I0316 17:10:43.632065 140320504472768 submission.py:265] 3000) loss = 0.125, grad_norm = 0.007
I0316 17:12:00.987623 140320504472768 spec.py:321] Evaluating on the training split.
I0316 17:14:12.127866 140320504472768 spec.py:333] Evaluating on the validation split.
I0316 17:18:45.047420 140320504472768 spec.py:349] Evaluating on the test split.
I0316 17:23:40.248083 140320504472768 submission_runner.py:469] Time since start: 23331.91s, 	Step: 3065, 	{'train/loss': 0.12105914421864007, 'validation/loss': 0.12494446276259832, 'validation/num_examples': 83274637, 'test/loss': 0.127198951226124, 'test/num_examples': 95000000, 'score': 3007.2029054164886, 'total_duration': 23331.91042947769, 'accumulated_submission_time': 3007.2029054164886, 'accumulated_eval_time': 20302.07315301895, 'accumulated_logging_time': 0.5585799217224121}
I0316 17:23:40.259384 140277826074368 logging_writer.py:48] [3065] accumulated_eval_time=20302.1, accumulated_logging_time=0.55858, accumulated_submission_time=3007.2, global_step=3065, preemption_count=0, score=3007.2, test/loss=0.127199, test/num_examples=95000000, total_duration=23331.9, train/loss=0.121059, validation/loss=0.124944, validation/num_examples=83274637
I0316 17:25:40.868738 140320504472768 spec.py:321] Evaluating on the training split.
I0316 17:27:45.185578 140320504472768 spec.py:333] Evaluating on the validation split.
I0316 17:32:21.237655 140320504472768 spec.py:349] Evaluating on the test split.
I0316 17:37:17.490373 140320504472768 submission_runner.py:469] Time since start: 24149.15s, 	Step: 3188, 	{'train/loss': 0.12278319179804524, 'validation/loss': 0.12548813314093302, 'validation/num_examples': 83274637, 'test/loss': 0.12790892722099706, 'test/num_examples': 95000000, 'score': 3126.876755952835, 'total_duration': 24149.152728557587, 'accumulated_submission_time': 3126.876755952835, 'accumulated_eval_time': 20998.694912672043, 'accumulated_logging_time': 0.5768539905548096}
I0316 17:37:17.501280 140277834467072 logging_writer.py:48] [3188] accumulated_eval_time=20998.7, accumulated_logging_time=0.576854, accumulated_submission_time=3126.88, global_step=3188, preemption_count=0, score=3126.88, test/loss=0.127909, test/num_examples=95000000, total_duration=24149.2, train/loss=0.122783, validation/loss=0.125488, validation/num_examples=83274637
I0316 17:39:18.662205 140320504472768 spec.py:321] Evaluating on the training split.
I0316 17:41:30.276919 140320504472768 spec.py:333] Evaluating on the validation split.
I0316 17:46:03.923306 140320504472768 spec.py:349] Evaluating on the test split.
I0316 17:50:49.816131 140320504472768 submission_runner.py:469] Time since start: 24961.48s, 	Step: 3303, 	{'train/loss': 0.12325024076647867, 'validation/loss': 0.12518418765001246, 'validation/num_examples': 83274637, 'test/loss': 0.12758720750081914, 'test/num_examples': 95000000, 'score': 3247.1840131282806, 'total_duration': 24961.478481531143, 'accumulated_submission_time': 3247.1840131282806, 'accumulated_eval_time': 21689.848920106888, 'accumulated_logging_time': 0.5940737724304199}
I0316 17:50:49.826373 140277826074368 logging_writer.py:48] [3303] accumulated_eval_time=21689.8, accumulated_logging_time=0.594074, accumulated_submission_time=3247.18, global_step=3303, preemption_count=0, score=3247.18, test/loss=0.127587, test/num_examples=95000000, total_duration=24961.5, train/loss=0.12325, validation/loss=0.125184, validation/num_examples=83274637
I0316 17:52:51.339325 140320504472768 spec.py:321] Evaluating on the training split.
I0316 17:54:56.364828 140320504472768 spec.py:333] Evaluating on the validation split.
I0316 17:59:27.917947 140320504472768 spec.py:349] Evaluating on the test split.
I0316 18:04:22.732482 140320504472768 submission_runner.py:469] Time since start: 25774.39s, 	Step: 3426, 	{'train/loss': 0.1255722740276554, 'validation/loss': 0.12492695466929997, 'validation/num_examples': 83274637, 'test/loss': 0.1272743878795423, 'test/num_examples': 95000000, 'score': 3367.8019325733185, 'total_duration': 25774.394868850708, 'accumulated_submission_time': 3367.8019325733185, 'accumulated_eval_time': 22381.242252349854, 'accumulated_logging_time': 0.6110641956329346}
I0316 18:04:22.759715 140293253883648 logging_writer.py:48] [3426] accumulated_eval_time=22381.2, accumulated_logging_time=0.611064, accumulated_submission_time=3367.8, global_step=3426, preemption_count=0, score=3367.8, test/loss=0.127274, test/num_examples=95000000, total_duration=25774.4, train/loss=0.125572, validation/loss=0.124927, validation/num_examples=83274637
I0316 18:05:23.125000 140293245490944 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.033247, loss=0.128878
I0316 18:05:23.128042 140320504472768 submission.py:265] 3500) loss = 0.129, grad_norm = 0.033
I0316 18:06:23.861633 140320504472768 spec.py:321] Evaluating on the training split.
I0316 18:08:30.325466 140320504472768 spec.py:333] Evaluating on the validation split.
I0316 18:13:03.932958 140320504472768 spec.py:349] Evaluating on the test split.
I0316 18:17:53.618589 140320504472768 submission_runner.py:469] Time since start: 26585.28s, 	Step: 3551, 	{'train/loss': 0.12614753305789808, 'validation/loss': 0.12459014488322805, 'validation/num_examples': 83274637, 'test/loss': 0.12683043759492574, 'test/num_examples': 95000000, 'score': 3488.0455770492554, 'total_duration': 26585.280919075012, 'accumulated_submission_time': 3488.0455770492554, 'accumulated_eval_time': 23070.999271154404, 'accumulated_logging_time': 0.6449613571166992}
I0316 18:17:53.639017 140293253883648 logging_writer.py:48] [3551] accumulated_eval_time=23071, accumulated_logging_time=0.644961, accumulated_submission_time=3488.05, global_step=3551, preemption_count=0, score=3488.05, test/loss=0.12683, test/num_examples=95000000, total_duration=26585.3, train/loss=0.126148, validation/loss=0.12459, validation/num_examples=83274637
I0316 18:19:55.151674 140320504472768 spec.py:321] Evaluating on the training split.
I0316 18:21:58.894283 140320504472768 spec.py:333] Evaluating on the validation split.
I0316 18:26:22.387317 140320504472768 spec.py:349] Evaluating on the test split.
I0316 18:31:09.635127 140320504472768 submission_runner.py:469] Time since start: 27381.30s, 	Step: 3671, 	{'train/loss': 0.1248595502015739, 'validation/loss': 0.12464134691031425, 'validation/num_examples': 83274637, 'test/loss': 0.12706937891970183, 'test/num_examples': 95000000, 'score': 3608.677136659622, 'total_duration': 27381.297457695007, 'accumulated_submission_time': 3608.677136659622, 'accumulated_eval_time': 23745.48271226883, 'accumulated_logging_time': 0.6720564365386963}
I0316 18:31:09.646022 140293245490944 logging_writer.py:48] [3671] accumulated_eval_time=23745.5, accumulated_logging_time=0.672056, accumulated_submission_time=3608.68, global_step=3671, preemption_count=0, score=3608.68, test/loss=0.127069, test/num_examples=95000000, total_duration=27381.3, train/loss=0.12486, validation/loss=0.124641, validation/num_examples=83274637
I0316 18:33:10.486250 140320504472768 spec.py:321] Evaluating on the training split.
I0316 18:35:13.945932 140320504472768 spec.py:333] Evaluating on the validation split.
I0316 18:39:44.891649 140320504472768 spec.py:349] Evaluating on the test split.
I0316 18:44:32.573391 140320504472768 submission_runner.py:469] Time since start: 28184.24s, 	Step: 3797, 	{'train/loss': 0.12311814855313295, 'validation/loss': 0.12451377694300199, 'validation/num_examples': 83274637, 'test/loss': 0.12689970806178041, 'test/num_examples': 95000000, 'score': 3728.601909637451, 'total_duration': 28184.235728025436, 'accumulated_submission_time': 3728.601909637451, 'accumulated_eval_time': 24427.56992125511, 'accumulated_logging_time': 0.6898050308227539}
I0316 18:44:32.583991 140293253883648 logging_writer.py:48] [3797] accumulated_eval_time=24427.6, accumulated_logging_time=0.689805, accumulated_submission_time=3728.6, global_step=3797, preemption_count=0, score=3728.6, test/loss=0.1269, test/num_examples=95000000, total_duration=28184.2, train/loss=0.123118, validation/loss=0.124514, validation/num_examples=83274637
I0316 18:46:33.367390 140320504472768 spec.py:321] Evaluating on the training split.
I0316 18:48:36.273612 140320504472768 spec.py:333] Evaluating on the validation split.
I0316 18:53:04.259504 140320504472768 spec.py:349] Evaluating on the test split.
I0316 18:57:54.423380 140320504472768 submission_runner.py:469] Time since start: 28986.09s, 	Step: 3918, 	{'train/loss': 0.12333901923395527, 'validation/loss': 0.12460197909320604, 'validation/num_examples': 83274637, 'test/loss': 0.12702625766975, 'test/num_examples': 95000000, 'score': 3848.489721298218, 'total_duration': 28986.085779428482, 'accumulated_submission_time': 3848.489721298218, 'accumulated_eval_time': 25108.626169204712, 'accumulated_logging_time': 0.7069041728973389}
I0316 18:57:54.434175 140293245490944 logging_writer.py:48] [3918] accumulated_eval_time=25108.6, accumulated_logging_time=0.706904, accumulated_submission_time=3848.49, global_step=3918, preemption_count=0, score=3848.49, test/loss=0.127026, test/num_examples=95000000, total_duration=28986.1, train/loss=0.123339, validation/loss=0.124602, validation/num_examples=83274637
I0316 18:59:05.878333 140293253883648 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.030833, loss=0.114962
I0316 18:59:05.881763 140320504472768 submission.py:265] 4000) loss = 0.115, grad_norm = 0.031
I0316 18:59:56.494347 140320504472768 spec.py:321] Evaluating on the training split.
I0316 19:01:59.547200 140320504472768 spec.py:333] Evaluating on the validation split.
I0316 19:06:40.759176 140320504472768 spec.py:349] Evaluating on the test split.
I0316 19:11:35.268044 140320504472768 submission_runner.py:469] Time since start: 29806.93s, 	Step: 4042, 	{'train/loss': 0.12366455068028312, 'validation/loss': 0.12453124486103, 'validation/num_examples': 83274637, 'test/loss': 0.12696067978491532, 'test/num_examples': 95000000, 'score': 3969.6656963825226, 'total_duration': 29806.930405139923, 'accumulated_submission_time': 3969.6656963825226, 'accumulated_eval_time': 25807.400044441223, 'accumulated_logging_time': 0.724757194519043}
I0316 19:11:35.279857 140293245490944 logging_writer.py:48] [4042] accumulated_eval_time=25807.4, accumulated_logging_time=0.724757, accumulated_submission_time=3969.67, global_step=4042, preemption_count=0, score=3969.67, test/loss=0.126961, test/num_examples=95000000, total_duration=29806.9, train/loss=0.123665, validation/loss=0.124531, validation/num_examples=83274637
I0316 19:13:36.519443 140320504472768 spec.py:321] Evaluating on the training split.
I0316 19:15:50.593389 140320504472768 spec.py:333] Evaluating on the validation split.
I0316 19:20:20.861126 140320504472768 spec.py:349] Evaluating on the test split.
I0316 19:25:15.467269 140320504472768 submission_runner.py:469] Time since start: 30627.13s, 	Step: 4165, 	{'train/loss': 0.12349789245678916, 'validation/loss': 0.12483233581966084, 'validation/num_examples': 83274637, 'test/loss': 0.12742939613350315, 'test/num_examples': 95000000, 'score': 4089.9789078235626, 'total_duration': 30627.12962794304, 'accumulated_submission_time': 4089.9789078235626, 'accumulated_eval_time': 26506.34801030159, 'accumulated_logging_time': 0.7438812255859375}
I0316 19:25:15.478674 140293253883648 logging_writer.py:48] [4165] accumulated_eval_time=26506.3, accumulated_logging_time=0.743881, accumulated_submission_time=4089.98, global_step=4165, preemption_count=0, score=4089.98, test/loss=0.127429, test/num_examples=95000000, total_duration=30627.1, train/loss=0.123498, validation/loss=0.124832, validation/num_examples=83274637
I0316 19:27:16.125536 140320504472768 spec.py:321] Evaluating on the training split.
I0316 19:29:24.545057 140320504472768 spec.py:333] Evaluating on the validation split.
I0316 19:33:57.526231 140320504472768 spec.py:349] Evaluating on the test split.
I0316 19:38:49.617257 140320504472768 submission_runner.py:469] Time since start: 31441.28s, 	Step: 4287, 	{'train/loss': 0.12275991189533036, 'validation/loss': 0.12450317691896538, 'validation/num_examples': 83274637, 'test/loss': 0.1269466377602828, 'test/num_examples': 95000000, 'score': 4209.778605222702, 'total_duration': 31441.27959561348, 'accumulated_submission_time': 4209.778605222702, 'accumulated_eval_time': 27199.839778900146, 'accumulated_logging_time': 0.7620193958282471}
I0316 19:38:49.628489 140293245490944 logging_writer.py:48] [4287] accumulated_eval_time=27199.8, accumulated_logging_time=0.762019, accumulated_submission_time=4209.78, global_step=4287, preemption_count=0, score=4209.78, test/loss=0.126947, test/num_examples=95000000, total_duration=31441.3, train/loss=0.12276, validation/loss=0.124503, validation/num_examples=83274637
I0316 19:40:50.572770 140320504472768 spec.py:321] Evaluating on the training split.
I0316 19:42:54.225343 140320504472768 spec.py:333] Evaluating on the validation split.
I0316 19:47:29.634414 140320504472768 spec.py:349] Evaluating on the test split.
I0316 19:52:28.622984 140320504472768 submission_runner.py:469] Time since start: 32260.29s, 	Step: 4411, 	{'train/loss': 0.12335323706945724, 'validation/loss': 0.12450409915329398, 'validation/num_examples': 83274637, 'test/loss': 0.12686706287259553, 'test/num_examples': 95000000, 'score': 4329.818938732147, 'total_duration': 32260.285347223282, 'accumulated_submission_time': 4329.818938732147, 'accumulated_eval_time': 27897.89021062851, 'accumulated_logging_time': 0.7976374626159668}
I0316 19:52:28.633956 140293253883648 logging_writer.py:48] [4411] accumulated_eval_time=27897.9, accumulated_logging_time=0.797637, accumulated_submission_time=4329.82, global_step=4411, preemption_count=0, score=4329.82, test/loss=0.126867, test/num_examples=95000000, total_duration=32260.3, train/loss=0.123353, validation/loss=0.124504, validation/num_examples=83274637
I0316 19:53:49.791534 140293245490944 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.0223251, loss=0.117619
I0316 19:53:49.794783 140320504472768 submission.py:265] 4500) loss = 0.118, grad_norm = 0.022
I0316 19:54:30.658390 140320504472768 spec.py:321] Evaluating on the training split.
I0316 19:56:39.848404 140320504472768 spec.py:333] Evaluating on the validation split.
I0316 20:01:16.280232 140320504472768 spec.py:349] Evaluating on the test split.
I0316 20:06:21.144899 140320504472768 submission_runner.py:469] Time since start: 33092.81s, 	Step: 4534, 	{'train/loss': 0.12171964065185337, 'validation/loss': 0.12448283646745831, 'validation/num_examples': 83274637, 'test/loss': 0.12681296133808337, 'test/num_examples': 95000000, 'score': 4450.975401163101, 'total_duration': 33092.80724191666, 'accumulated_submission_time': 4450.975401163101, 'accumulated_eval_time': 28608.376883745193, 'accumulated_logging_time': 0.8150219917297363}
I0316 20:06:21.156387 140293253883648 logging_writer.py:48] [4534] accumulated_eval_time=28608.4, accumulated_logging_time=0.815022, accumulated_submission_time=4450.98, global_step=4534, preemption_count=0, score=4450.98, test/loss=0.126813, test/num_examples=95000000, total_duration=33092.8, train/loss=0.12172, validation/loss=0.124483, validation/num_examples=83274637
I0316 20:08:22.260408 140320504472768 spec.py:321] Evaluating on the training split.
I0316 20:10:31.083135 140320504472768 spec.py:333] Evaluating on the validation split.
I0316 20:14:53.396806 140320504472768 spec.py:349] Evaluating on the test split.
I0316 20:19:52.770044 140320504472768 submission_runner.py:469] Time since start: 33904.43s, 	Step: 4661, 	{'train/loss': 0.12468209489470027, 'validation/loss': 0.12502163757292126, 'validation/num_examples': 83274637, 'test/loss': 0.12710753461773522, 'test/num_examples': 95000000, 'score': 4571.179208040237, 'total_duration': 33904.43240189552, 'accumulated_submission_time': 4571.179208040237, 'accumulated_eval_time': 29298.886647224426, 'accumulated_logging_time': 0.8340740203857422}
I0316 20:19:52.781344 140293245490944 logging_writer.py:48] [4661] accumulated_eval_time=29298.9, accumulated_logging_time=0.834074, accumulated_submission_time=4571.18, global_step=4661, preemption_count=0, score=4571.18, test/loss=0.127108, test/num_examples=95000000, total_duration=33904.4, train/loss=0.124682, validation/loss=0.125022, validation/num_examples=83274637
I0316 20:21:54.134525 140320504472768 spec.py:321] Evaluating on the training split.
I0316 20:23:58.744776 140320504472768 spec.py:333] Evaluating on the validation split.
I0316 20:28:39.099335 140320504472768 spec.py:349] Evaluating on the test split.
I0316 20:33:41.871890 140320504472768 submission_runner.py:469] Time since start: 34733.53s, 	Step: 4777, 	{'train/loss': 0.1228803406968257, 'validation/loss': 0.1245294548053785, 'validation/num_examples': 83274637, 'test/loss': 0.12686103972027427, 'test/num_examples': 95000000, 'score': 4691.647433280945, 'total_duration': 34733.53422355652, 'accumulated_submission_time': 4691.647433280945, 'accumulated_eval_time': 30006.6241440773, 'accumulated_logging_time': 0.8519127368927002}
I0316 20:33:41.883503 140293253883648 logging_writer.py:48] [4777] accumulated_eval_time=30006.6, accumulated_logging_time=0.851913, accumulated_submission_time=4691.65, global_step=4777, preemption_count=0, score=4691.65, test/loss=0.126861, test/num_examples=95000000, total_duration=34733.5, train/loss=0.12288, validation/loss=0.124529, validation/num_examples=83274637
I0316 20:35:42.792227 140320504472768 spec.py:321] Evaluating on the training split.
I0316 20:37:48.738109 140320504472768 spec.py:333] Evaluating on the validation split.
I0316 20:42:12.856756 140320504472768 spec.py:349] Evaluating on the test split.
I0316 20:47:05.187815 140320504472768 submission_runner.py:469] Time since start: 35536.85s, 	Step: 4903, 	{'train/loss': 0.12247424278756962, 'validation/loss': 0.12449136599992862, 'validation/num_examples': 83274637, 'test/loss': 0.12693517799526013, 'test/num_examples': 95000000, 'score': 4811.64857172966, 'total_duration': 35536.85018205643, 'accumulated_submission_time': 4811.64857172966, 'accumulated_eval_time': 30689.019859075546, 'accumulated_logging_time': 0.8716294765472412}
I0316 20:47:05.198672 140293245490944 logging_writer.py:48] [4903] accumulated_eval_time=30689, accumulated_logging_time=0.871629, accumulated_submission_time=4811.65, global_step=4903, preemption_count=0, score=4811.65, test/loss=0.126935, test/num_examples=95000000, total_duration=35536.9, train/loss=0.122474, validation/loss=0.124491, validation/num_examples=83274637
I0316 20:48:33.772403 140293253883648 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.0455265, loss=0.122894
I0316 20:48:33.775598 140320504472768 submission.py:265] 5000) loss = 0.123, grad_norm = 0.046
I0316 20:49:05.871466 140320504472768 spec.py:321] Evaluating on the training split.
I0316 20:51:13.377923 140320504472768 spec.py:333] Evaluating on the validation split.
I0316 20:55:58.225455 140320504472768 spec.py:349] Evaluating on the test split.
I0316 21:00:58.158187 140320504472768 submission_runner.py:469] Time since start: 36369.82s, 	Step: 5026, 	{'train/loss': 0.12265547541824516, 'validation/loss': 0.12487215453760045, 'validation/num_examples': 83274637, 'test/loss': 0.12729147794141268, 'test/num_examples': 95000000, 'score': 4931.467498064041, 'total_duration': 36369.820519924164, 'accumulated_submission_time': 4931.467498064041, 'accumulated_eval_time': 31401.306683301926, 'accumulated_logging_time': 0.8954281806945801}
I0316 21:00:58.169525 140293245490944 logging_writer.py:48] [5026] accumulated_eval_time=31401.3, accumulated_logging_time=0.895428, accumulated_submission_time=4931.47, global_step=5026, preemption_count=0, score=4931.47, test/loss=0.127291, test/num_examples=95000000, total_duration=36369.8, train/loss=0.122655, validation/loss=0.124872, validation/num_examples=83274637
I0316 21:02:58.683234 140320504472768 spec.py:321] Evaluating on the training split.
I0316 21:05:01.905459 140320504472768 spec.py:333] Evaluating on the validation split.
I0316 21:09:34.975847 140320504472768 spec.py:349] Evaluating on the test split.
I0316 21:14:36.929486 140320504472768 submission_runner.py:469] Time since start: 37188.59s, 	Step: 5153, 	{'train/loss': 0.12582382031049658, 'validation/loss': 0.12468836991143523, 'validation/num_examples': 83274637, 'test/loss': 0.12711287894455758, 'test/num_examples': 95000000, 'score': 5051.093612670898, 'total_duration': 37188.591873168945, 'accumulated_submission_time': 5051.093612670898, 'accumulated_eval_time': 32099.553120613098, 'accumulated_logging_time': 0.913154125213623}
I0316 21:14:36.940705 140293253883648 logging_writer.py:48] [5153] accumulated_eval_time=32099.6, accumulated_logging_time=0.913154, accumulated_submission_time=5051.09, global_step=5153, preemption_count=0, score=5051.09, test/loss=0.127113, test/num_examples=95000000, total_duration=37188.6, train/loss=0.125824, validation/loss=0.124688, validation/num_examples=83274637
I0316 21:16:38.524779 140320504472768 spec.py:321] Evaluating on the training split.
I0316 21:18:41.613008 140320504472768 spec.py:333] Evaluating on the validation split.
I0316 21:23:19.387821 140320504472768 spec.py:349] Evaluating on the test split.
I0316 21:28:15.193952 140320504472768 submission_runner.py:469] Time since start: 38006.86s, 	Step: 5271, 	{'train/loss': 0.12447069099461709, 'validation/loss': 0.12439834547858102, 'validation/num_examples': 83274637, 'test/loss': 0.1267582923295272, 'test/num_examples': 95000000, 'score': 5171.826122045517, 'total_duration': 38006.85632991791, 'accumulated_submission_time': 5171.826122045517, 'accumulated_eval_time': 32796.22246360779, 'accumulated_logging_time': 0.9319491386413574}
I0316 21:28:15.205967 140293245490944 logging_writer.py:48] [5271] accumulated_eval_time=32796.2, accumulated_logging_time=0.931949, accumulated_submission_time=5171.83, global_step=5271, preemption_count=0, score=5171.83, test/loss=0.126758, test/num_examples=95000000, total_duration=38006.9, train/loss=0.124471, validation/loss=0.124398, validation/num_examples=83274637
I0316 21:30:16.065522 140320504472768 spec.py:321] Evaluating on the training split.
I0316 21:32:25.111076 140320504472768 spec.py:333] Evaluating on the validation split.
I0316 21:36:51.985343 140320504472768 spec.py:349] Evaluating on the test split.
I0316 21:41:50.080628 140320504472768 submission_runner.py:469] Time since start: 38821.74s, 	Step: 5391, 	{'train/loss': 0.122064291826654, 'validation/loss': 0.12428513520444695, 'validation/num_examples': 83274637, 'test/loss': 0.1266025569468448, 'test/num_examples': 95000000, 'score': 5291.774982452393, 'total_duration': 38821.7429959774, 'accumulated_submission_time': 5291.774982452393, 'accumulated_eval_time': 33490.23759698868, 'accumulated_logging_time': 0.9954791069030762}
I0316 21:41:50.092770 140293253883648 logging_writer.py:48] [5391] accumulated_eval_time=33490.2, accumulated_logging_time=0.995479, accumulated_submission_time=5291.77, global_step=5391, preemption_count=0, score=5291.77, test/loss=0.126603, test/num_examples=95000000, total_duration=38821.7, train/loss=0.122064, validation/loss=0.124285, validation/num_examples=83274637
I0316 21:43:31.736438 140293245490944 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.0255075, loss=0.118822
I0316 21:43:31.739739 140320504472768 submission.py:265] 5500) loss = 0.119, grad_norm = 0.026
I0316 21:43:51.152808 140320504472768 spec.py:321] Evaluating on the training split.
I0316 21:45:57.916689 140320504472768 spec.py:333] Evaluating on the validation split.
I0316 21:50:31.084639 140320504472768 spec.py:349] Evaluating on the test split.
I0316 21:55:21.254155 140320504472768 submission_runner.py:469] Time since start: 39632.92s, 	Step: 5516, 	{'train/loss': 0.12324977943694984, 'validation/loss': 0.1242073733451345, 'validation/num_examples': 83274637, 'test/loss': 0.12654180725374722, 'test/num_examples': 95000000, 'score': 5411.982172489166, 'total_duration': 39632.91647362709, 'accumulated_submission_time': 5411.982172489166, 'accumulated_eval_time': 34180.33908724785, 'accumulated_logging_time': 1.014986515045166}
I0316 21:55:21.265019 140293253883648 logging_writer.py:48] [5516] accumulated_eval_time=34180.3, accumulated_logging_time=1.01499, accumulated_submission_time=5411.98, global_step=5516, preemption_count=0, score=5411.98, test/loss=0.126542, test/num_examples=95000000, total_duration=39632.9, train/loss=0.12325, validation/loss=0.124207, validation/num_examples=83274637
I0316 21:57:22.901387 140320504472768 spec.py:321] Evaluating on the training split.
I0316 21:59:26.003547 140320504472768 spec.py:333] Evaluating on the validation split.
I0316 22:03:50.637273 140320504472768 spec.py:349] Evaluating on the test split.
I0316 22:08:45.723122 140320504472768 submission_runner.py:469] Time since start: 40437.39s, 	Step: 5638, 	{'train/loss': 0.12162708275291476, 'validation/loss': 0.12433863352324115, 'validation/num_examples': 83274637, 'test/loss': 0.1267145609060187, 'test/num_examples': 95000000, 'score': 5532.762580633163, 'total_duration': 40437.38548874855, 'accumulated_submission_time': 5532.762580633163, 'accumulated_eval_time': 34863.16097283363, 'accumulated_logging_time': 1.0325391292572021}
I0316 22:08:45.734140 140293245490944 logging_writer.py:48] [5638] accumulated_eval_time=34863.2, accumulated_logging_time=1.03254, accumulated_submission_time=5532.76, global_step=5638, preemption_count=0, score=5532.76, test/loss=0.126715, test/num_examples=95000000, total_duration=40437.4, train/loss=0.121627, validation/loss=0.124339, validation/num_examples=83274637
I0316 22:10:47.346878 140320504472768 spec.py:321] Evaluating on the training split.
I0316 22:12:57.661687 140320504472768 spec.py:333] Evaluating on the validation split.
I0316 22:17:35.399185 140320504472768 spec.py:349] Evaluating on the test split.
I0316 22:22:28.631099 140320504472768 submission_runner.py:469] Time since start: 41260.29s, 	Step: 5763, 	{'train/loss': 0.12389354145526484, 'validation/loss': 0.12436027440766848, 'validation/num_examples': 83274637, 'test/loss': 0.1266813805102298, 'test/num_examples': 95000000, 'score': 5653.480751991272, 'total_duration': 41260.29347062111, 'accumulated_submission_time': 5653.480751991272, 'accumulated_eval_time': 35564.44536995888, 'accumulated_logging_time': 1.0504515171051025}
I0316 22:22:28.642954 140293253883648 logging_writer.py:48] [5763] accumulated_eval_time=35564.4, accumulated_logging_time=1.05045, accumulated_submission_time=5653.48, global_step=5763, preemption_count=0, score=5653.48, test/loss=0.126681, test/num_examples=95000000, total_duration=41260.3, train/loss=0.123894, validation/loss=0.12436, validation/num_examples=83274637
I0316 22:24:29.443526 140320504472768 spec.py:321] Evaluating on the training split.
I0316 22:26:33.858277 140320504472768 spec.py:333] Evaluating on the validation split.
I0316 22:31:00.366382 140320504472768 spec.py:349] Evaluating on the test split.
I0316 22:36:06.628308 140320504472768 submission_runner.py:469] Time since start: 42078.29s, 	Step: 5888, 	{'train/loss': 0.12387756623733388, 'validation/loss': 0.12427081271777321, 'validation/num_examples': 83274637, 'test/loss': 0.1266759790498834, 'test/num_examples': 95000000, 'score': 5773.397820949554, 'total_duration': 42078.29068398476, 'accumulated_submission_time': 5773.397820949554, 'accumulated_eval_time': 36261.63034772873, 'accumulated_logging_time': 1.070716142654419}
I0316 22:36:06.639467 140293245490944 logging_writer.py:48] [5888] accumulated_eval_time=36261.6, accumulated_logging_time=1.07072, accumulated_submission_time=5773.4, global_step=5888, preemption_count=0, score=5773.4, test/loss=0.126676, test/num_examples=95000000, total_duration=42078.3, train/loss=0.123878, validation/loss=0.124271, validation/num_examples=83274637
I0316 22:38:02.318339 140293253883648 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.0244776, loss=0.125603
I0316 22:38:02.321604 140320504472768 submission.py:265] 6000) loss = 0.126, grad_norm = 0.024
I0316 22:38:07.316089 140320504472768 spec.py:321] Evaluating on the training split.
I0316 22:40:12.239341 140320504472768 spec.py:333] Evaluating on the validation split.
I0316 22:44:40.400390 140320504472768 spec.py:349] Evaluating on the test split.
I0316 22:49:33.511335 140320504472768 submission_runner.py:469] Time since start: 42885.17s, 	Step: 6005, 	{'train/loss': 0.12352680325694639, 'validation/loss': 0.12419725468292936, 'validation/num_examples': 83274637, 'test/loss': 0.1266039461065995, 'test/num_examples': 95000000, 'score': 5893.129123210907, 'total_duration': 42885.17369103432, 'accumulated_submission_time': 5893.129123210907, 'accumulated_eval_time': 36947.82575964928, 'accumulated_logging_time': 1.1471734046936035}
I0316 22:49:33.523841 140293245490944 logging_writer.py:48] [6005] accumulated_eval_time=36947.8, accumulated_logging_time=1.14717, accumulated_submission_time=5893.13, global_step=6005, preemption_count=0, score=5893.13, test/loss=0.126604, test/num_examples=95000000, total_duration=42885.2, train/loss=0.123527, validation/loss=0.124197, validation/num_examples=83274637
I0316 22:51:34.424510 140320504472768 spec.py:321] Evaluating on the training split.
I0316 22:53:37.501366 140320504472768 spec.py:333] Evaluating on the validation split.
I0316 22:58:06.897351 140320504472768 spec.py:349] Evaluating on the test split.
I0316 23:03:04.690304 140320504472768 submission_runner.py:469] Time since start: 43696.35s, 	Step: 6127, 	{'train/loss': 0.12135458927183461, 'validation/loss': 0.12408336949541246, 'validation/num_examples': 83274637, 'test/loss': 0.12640728318730404, 'test/num_examples': 95000000, 'score': 6013.1171000003815, 'total_duration': 43696.352658987045, 'accumulated_submission_time': 6013.1171000003815, 'accumulated_eval_time': 37638.091678380966, 'accumulated_logging_time': 1.1666593551635742}
I0316 23:03:04.701436 140293253883648 logging_writer.py:48] [6127] accumulated_eval_time=37638.1, accumulated_logging_time=1.16666, accumulated_submission_time=6013.12, global_step=6127, preemption_count=0, score=6013.12, test/loss=0.126407, test/num_examples=95000000, total_duration=43696.4, train/loss=0.121355, validation/loss=0.124083, validation/num_examples=83274637
I0316 23:05:05.784992 140320504472768 spec.py:321] Evaluating on the training split.
I0316 23:07:08.812882 140320504472768 spec.py:333] Evaluating on the validation split.
I0316 23:11:42.380182 140320504472768 spec.py:349] Evaluating on the test split.
I0316 23:16:39.352806 140320504472768 submission_runner.py:469] Time since start: 44511.01s, 	Step: 6252, 	{'train/loss': 0.12187257939956961, 'validation/loss': 0.12407379121469803, 'validation/num_examples': 83274637, 'test/loss': 0.12639527490491365, 'test/num_examples': 95000000, 'score': 6133.365200519562, 'total_duration': 44511.01498007774, 'accumulated_submission_time': 6133.365200519562, 'accumulated_eval_time': 38331.65941119194, 'accumulated_logging_time': 1.1839931011199951}
I0316 23:16:39.363941 140293245490944 logging_writer.py:48] [6252] accumulated_eval_time=38331.7, accumulated_logging_time=1.18399, accumulated_submission_time=6133.37, global_step=6252, preemption_count=0, score=6133.37, test/loss=0.126395, test/num_examples=95000000, total_duration=44511, train/loss=0.121873, validation/loss=0.124074, validation/num_examples=83274637
I0316 23:18:39.964631 140320504472768 spec.py:321] Evaluating on the training split.
I0316 23:20:44.993929 140320504472768 spec.py:333] Evaluating on the validation split.
I0316 23:25:12.531670 140320504472768 spec.py:349] Evaluating on the test split.
I0316 23:30:11.611530 140320504472768 submission_runner.py:469] Time since start: 45323.27s, 	Step: 6373, 	{'train/loss': 0.12255344376826013, 'validation/loss': 0.12417419743492632, 'validation/num_examples': 83274637, 'test/loss': 0.12658853200105366, 'test/num_examples': 95000000, 'score': 6253.062659740448, 'total_duration': 45323.2739238739, 'accumulated_submission_time': 6253.062659740448, 'accumulated_eval_time': 39023.30649447441, 'accumulated_logging_time': 1.2023210525512695}
I0316 23:30:11.623013 140293253883648 logging_writer.py:48] [6373] accumulated_eval_time=39023.3, accumulated_logging_time=1.20232, accumulated_submission_time=6253.06, global_step=6373, preemption_count=0, score=6253.06, test/loss=0.126589, test/num_examples=95000000, total_duration=45323.3, train/loss=0.122553, validation/loss=0.124174, validation/num_examples=83274637
I0316 23:32:12.347574 140320504472768 spec.py:321] Evaluating on the training split.
I0316 23:34:18.907043 140320504472768 spec.py:333] Evaluating on the validation split.
I0316 23:39:05.139046 140320504472768 spec.py:349] Evaluating on the test split.
I0316 23:43:59.909470 140320504472768 submission_runner.py:469] Time since start: 46151.57s, 	Step: 6497, 	{'train/loss': 0.12098470468406816, 'validation/loss': 0.1239986935889033, 'validation/num_examples': 83274637, 'test/loss': 0.12636945591527035, 'test/num_examples': 95000000, 'score': 6372.951236963272, 'total_duration': 46151.571860075, 'accumulated_submission_time': 6372.951236963272, 'accumulated_eval_time': 39730.8684322834, 'accumulated_logging_time': 1.2206060886383057}
I0316 23:43:59.921059 140293245490944 logging_writer.py:48] [6497] accumulated_eval_time=39730.9, accumulated_logging_time=1.22061, accumulated_submission_time=6372.95, global_step=6497, preemption_count=0, score=6372.95, test/loss=0.126369, test/num_examples=95000000, total_duration=46151.6, train/loss=0.120985, validation/loss=0.123999, validation/num_examples=83274637
I0316 23:44:01.163069 140293253883648 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.00983748, loss=0.125153
I0316 23:44:01.166188 140320504472768 submission.py:265] 6500) loss = 0.125, grad_norm = 0.010
I0316 23:46:00.779187 140320504472768 spec.py:321] Evaluating on the training split.
I0316 23:48:06.080099 140320504472768 spec.py:333] Evaluating on the validation split.
I0316 23:52:29.372256 140320504472768 spec.py:349] Evaluating on the test split.
I0316 23:57:20.026616 140320504472768 submission_runner.py:469] Time since start: 46951.69s, 	Step: 6617, 	{'train/loss': 0.12291126046696182, 'validation/loss': 0.123981588276529, 'validation/num_examples': 83274637, 'test/loss': 0.12629376359799033, 'test/num_examples': 95000000, 'score': 6492.937025308609, 'total_duration': 46951.6889731884, 'accumulated_submission_time': 6492.937025308609, 'accumulated_eval_time': 40410.11602473259, 'accumulated_logging_time': 1.239302635192871}
I0316 23:57:20.039366 140293245490944 logging_writer.py:48] [6617] accumulated_eval_time=40410.1, accumulated_logging_time=1.2393, accumulated_submission_time=6492.94, global_step=6617, preemption_count=0, score=6492.94, test/loss=0.126294, test/num_examples=95000000, total_duration=46951.7, train/loss=0.122911, validation/loss=0.123982, validation/num_examples=83274637
I0316 23:59:21.009891 140320504472768 spec.py:321] Evaluating on the training split.
I0317 00:01:27.470477 140320504472768 spec.py:333] Evaluating on the validation split.
I0317 00:06:05.270810 140320504472768 spec.py:349] Evaluating on the test split.
I0317 00:10:59.440104 140320504472768 submission_runner.py:469] Time since start: 47771.10s, 	Step: 6741, 	{'train/loss': 0.1227758320872222, 'validation/loss': 0.12412662367162039, 'validation/num_examples': 83274637, 'test/loss': 0.12645191842378314, 'test/num_examples': 95000000, 'score': 6613.054425239563, 'total_duration': 47771.10245299339, 'accumulated_submission_time': 6613.054425239563, 'accumulated_eval_time': 41108.54629826546, 'accumulated_logging_time': 1.259434700012207}
I0317 00:10:59.452847 140293253883648 logging_writer.py:48] [6741] accumulated_eval_time=41108.5, accumulated_logging_time=1.25943, accumulated_submission_time=6613.05, global_step=6741, preemption_count=0, score=6613.05, test/loss=0.126452, test/num_examples=95000000, total_duration=47771.1, train/loss=0.122776, validation/loss=0.124127, validation/num_examples=83274637
I0317 00:13:00.090204 140320504472768 spec.py:321] Evaluating on the training split.
I0317 00:15:03.931256 140320504472768 spec.py:333] Evaluating on the validation split.
I0317 00:19:29.405940 140320504472768 spec.py:349] Evaluating on the test split.
I0317 00:24:24.414935 140320504472768 submission_runner.py:469] Time since start: 48576.08s, 	Step: 6866, 	{'train/loss': 0.12219918012042233, 'validation/loss': 0.1240513472247731, 'validation/num_examples': 83274637, 'test/loss': 0.12640641555003115, 'test/num_examples': 95000000, 'score': 6732.8171491622925, 'total_duration': 48576.0773358345, 'accumulated_submission_time': 6732.8171491622925, 'accumulated_eval_time': 41792.87122607231, 'accumulated_logging_time': 1.2788503170013428}
I0317 00:24:24.426361 140293245490944 logging_writer.py:48] [6866] accumulated_eval_time=41792.9, accumulated_logging_time=1.27885, accumulated_submission_time=6732.82, global_step=6866, preemption_count=0, score=6732.82, test/loss=0.126406, test/num_examples=95000000, total_duration=48576.1, train/loss=0.122199, validation/loss=0.124051, validation/num_examples=83274637
I0317 00:26:24.690921 140293253883648 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.0118415, loss=0.127175
I0317 00:26:24.694060 140320504472768 submission.py:265] 7000) loss = 0.127, grad_norm = 0.012
I0317 00:26:25.088556 140320504472768 spec.py:321] Evaluating on the training split.
I0317 00:28:29.768666 140320504472768 spec.py:333] Evaluating on the validation split.
I0317 00:33:08.656525 140320504472768 spec.py:349] Evaluating on the test split.
I0317 00:38:17.548418 140320504472768 submission_runner.py:469] Time since start: 49409.21s, 	Step: 7001, 	{'train/loss': 0.12233406845253007, 'validation/loss': 0.12409298795158048, 'validation/num_examples': 83274637, 'test/loss': 0.12638767789655986, 'test/num_examples': 95000000, 'score': 6852.586658000946, 'total_duration': 49409.21079468727, 'accumulated_submission_time': 6852.586658000946, 'accumulated_eval_time': 42505.331287145615, 'accumulated_logging_time': 1.3298923969268799}
I0317 00:38:17.559982 140293245490944 logging_writer.py:48] [7001] accumulated_eval_time=42505.3, accumulated_logging_time=1.32989, accumulated_submission_time=6852.59, global_step=7001, preemption_count=0, score=6852.59, test/loss=0.126388, test/num_examples=95000000, total_duration=49409.2, train/loss=0.122334, validation/loss=0.124093, validation/num_examples=83274637
I0317 00:40:18.243506 140320504472768 spec.py:321] Evaluating on the training split.
I0317 00:42:21.904274 140320504472768 spec.py:333] Evaluating on the validation split.
I0317 00:46:44.646378 140320504472768 spec.py:349] Evaluating on the test split.
I0317 00:51:35.117964 140320504472768 submission_runner.py:469] Time since start: 50206.78s, 	Step: 7127, 	{'train/loss': 0.1219105096330059, 'validation/loss': 0.12401073884117644, 'validation/num_examples': 83274637, 'test/loss': 0.1263897263436167, 'test/num_examples': 95000000, 'score': 6972.409323453903, 'total_duration': 50206.78033185005, 'accumulated_submission_time': 6972.409323453903, 'accumulated_eval_time': 43182.20598435402, 'accumulated_logging_time': 1.3483171463012695}
I0317 00:51:35.130861 140293253883648 logging_writer.py:48] [7127] accumulated_eval_time=43182.2, accumulated_logging_time=1.34832, accumulated_submission_time=6972.41, global_step=7127, preemption_count=0, score=6972.41, test/loss=0.12639, test/num_examples=95000000, total_duration=50206.8, train/loss=0.121911, validation/loss=0.124011, validation/num_examples=83274637
I0317 00:53:35.994448 140320504472768 spec.py:321] Evaluating on the training split.
I0317 00:55:43.276585 140320504472768 spec.py:333] Evaluating on the validation split.
I0317 01:00:18.053999 140320504472768 spec.py:349] Evaluating on the test split.
I0317 01:05:22.474048 140320504472768 submission_runner.py:469] Time since start: 51034.14s, 	Step: 7258, 	{'train/loss': 0.1220586257115957, 'validation/loss': 0.12406118817850945, 'validation/num_examples': 83274637, 'test/loss': 0.12640187517917031, 'test/num_examples': 95000000, 'score': 7092.411914348602, 'total_duration': 51034.13642644882, 'accumulated_submission_time': 7092.411914348602, 'accumulated_eval_time': 43888.685754776, 'accumulated_logging_time': 1.3685109615325928}
I0317 01:05:22.486386 140293245490944 logging_writer.py:48] [7258] accumulated_eval_time=43888.7, accumulated_logging_time=1.36851, accumulated_submission_time=7092.41, global_step=7258, preemption_count=0, score=7092.41, test/loss=0.126402, test/num_examples=95000000, total_duration=51034.1, train/loss=0.122059, validation/loss=0.124061, validation/num_examples=83274637
I0317 01:07:23.387164 140320504472768 spec.py:321] Evaluating on the training split.
I0317 01:09:32.898839 140320504472768 spec.py:333] Evaluating on the validation split.
I0317 01:14:02.682941 140320504472768 spec.py:349] Evaluating on the test split.
I0317 01:19:03.184013 140320504472768 submission_runner.py:469] Time since start: 51854.85s, 	Step: 7378, 	{'train/loss': 0.12143166089247595, 'validation/loss': 0.12394583034761773, 'validation/num_examples': 83274637, 'test/loss': 0.1263138161034032, 'test/num_examples': 95000000, 'score': 7212.501144170761, 'total_duration': 51854.84638595581, 'accumulated_submission_time': 7212.501144170761, 'accumulated_eval_time': 44588.4827606678, 'accumulated_logging_time': 1.3884644508361816}
I0317 01:19:03.196550 140293253883648 logging_writer.py:48] [7378] accumulated_eval_time=44588.5, accumulated_logging_time=1.38846, accumulated_submission_time=7212.5, global_step=7378, preemption_count=0, score=7212.5, test/loss=0.126314, test/num_examples=95000000, total_duration=51854.8, train/loss=0.121432, validation/loss=0.123946, validation/num_examples=83274637
I0317 01:21:01.499735 140293245490944 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.00794487, loss=0.123857
I0317 01:21:01.503665 140320504472768 submission.py:265] 7500) loss = 0.124, grad_norm = 0.008
I0317 01:21:04.261984 140320504472768 spec.py:321] Evaluating on the training split.
I0317 01:23:12.695003 140320504472768 spec.py:333] Evaluating on the validation split.
I0317 01:27:51.731429 140320504472768 spec.py:349] Evaluating on the test split.
I0317 01:32:45.713379 140320504472768 submission_runner.py:469] Time since start: 52677.38s, 	Step: 7503, 	{'train/loss': 0.12092056417872185, 'validation/loss': 0.12392299552640867, 'validation/num_examples': 83274637, 'test/loss': 0.12625439645441958, 'test/num_examples': 95000000, 'score': 7332.660216093063, 'total_duration': 52677.37572526932, 'accumulated_submission_time': 7332.660216093063, 'accumulated_eval_time': 45289.93433094025, 'accumulated_logging_time': 1.4331932067871094}
I0317 01:32:45.724912 140293253883648 logging_writer.py:48] [7503] accumulated_eval_time=45289.9, accumulated_logging_time=1.43319, accumulated_submission_time=7332.66, global_step=7503, preemption_count=0, score=7332.66, test/loss=0.126254, test/num_examples=95000000, total_duration=52677.4, train/loss=0.120921, validation/loss=0.123923, validation/num_examples=83274637
I0317 01:34:46.952029 140320504472768 spec.py:321] Evaluating on the training split.
I0317 01:36:58.471710 140320504472768 spec.py:333] Evaluating on the validation split.
I0317 01:41:24.795809 140320504472768 spec.py:349] Evaluating on the test split.
I0317 01:46:25.231596 140320504472768 submission_runner.py:469] Time since start: 53496.89s, 	Step: 7625, 	{'train/loss': 0.11975760082860838, 'validation/loss': 0.12378918318079447, 'validation/num_examples': 83274637, 'test/loss': 0.12609952117542467, 'test/num_examples': 95000000, 'score': 7453.00655579567, 'total_duration': 53496.8939807415, 'accumulated_submission_time': 7453.00655579567, 'accumulated_eval_time': 45988.21409392357, 'accumulated_logging_time': 1.4518685340881348}
I0317 01:46:25.243796 140293245490944 logging_writer.py:48] [7625] accumulated_eval_time=45988.2, accumulated_logging_time=1.45187, accumulated_submission_time=7453.01, global_step=7625, preemption_count=0, score=7453.01, test/loss=0.1261, test/num_examples=95000000, total_duration=53496.9, train/loss=0.119758, validation/loss=0.123789, validation/num_examples=83274637
I0317 01:48:26.442307 140320504472768 spec.py:321] Evaluating on the training split.
I0317 01:50:32.229642 140320504472768 spec.py:333] Evaluating on the validation split.
I0317 01:55:05.662211 140320504472768 spec.py:349] Evaluating on the test split.
I0317 01:59:55.384091 140320504472768 submission_runner.py:469] Time since start: 54307.05s, 	Step: 7746, 	{'train/loss': 0.12169647993740319, 'validation/loss': 0.12377082301770063, 'validation/num_examples': 83274637, 'test/loss': 0.12601502458769145, 'test/num_examples': 95000000, 'score': 7573.344976425171, 'total_duration': 54307.04640746117, 'accumulated_submission_time': 7573.344976425171, 'accumulated_eval_time': 46677.155938863754, 'accumulated_logging_time': 1.4708888530731201}
I0317 01:59:55.397105 140293253883648 logging_writer.py:48] [7746] accumulated_eval_time=46677.2, accumulated_logging_time=1.47089, accumulated_submission_time=7573.34, global_step=7746, preemption_count=0, score=7573.34, test/loss=0.126015, test/num_examples=95000000, total_duration=54307, train/loss=0.121696, validation/loss=0.123771, validation/num_examples=83274637
I0317 02:01:56.753819 140320504472768 spec.py:321] Evaluating on the training split.
I0317 02:04:06.362288 140320504472768 spec.py:333] Evaluating on the validation split.
I0317 02:08:32.883653 140320504472768 spec.py:349] Evaluating on the test split.
I0317 02:13:40.858189 140320504472768 submission_runner.py:469] Time since start: 55132.52s, 	Step: 7866, 	{'train/loss': 0.12077552626937695, 'validation/loss': 0.12374843944459823, 'validation/num_examples': 83274637, 'test/loss': 0.12601914355765895, 'test/num_examples': 95000000, 'score': 7693.810689687729, 'total_duration': 55132.520562410355, 'accumulated_submission_time': 7693.810689687729, 'accumulated_eval_time': 47381.26044535637, 'accumulated_logging_time': 1.491452932357788}
I0317 02:13:40.890919 140293245490944 logging_writer.py:48] [7866] accumulated_eval_time=47381.3, accumulated_logging_time=1.49145, accumulated_submission_time=7693.81, global_step=7866, preemption_count=0, score=7693.81, test/loss=0.126019, test/num_examples=95000000, total_duration=55132.5, train/loss=0.120776, validation/loss=0.123748, validation/num_examples=83274637
I0317 02:15:42.016505 140293253883648 logging_writer.py:48] [7988] global_step=7988, preemption_count=0, score=7814.5
I0317 02:15:45.631564 140320504472768 submission_runner.py:646] Tuning trial 1/5
I0317 02:15:45.631806 140320504472768 submission_runner.py:647] Hyperparameters: Hyperparameters(learning_rate=0.0017486387539278373, one_minus_beta1=0.06733926164, one_minus_beta2=0.00448403102, epsilon=1e-08, one_minus_momentum=0.0, use_momentum=False, weight_decay=0.08121616522670176, max_preconditioner_dim=1024, precondition_frequency=100, start_preconditioning_step=-1, inv_root_override=0, exponent_multiplier=1.0, grafting_type='ADAM', grafting_epsilon=1e-08, use_normalized_grafting=False, communication_dtype='FP32', communicate_params=True, use_cosine_decay=True, warmup_factor=0.02, label_smoothing=0.0, dropout_rate=0.1, use_nadam=True, step_hint_factor=1.0)
I0317 02:15:45.633262 140320504472768 submission_runner.py:648] Metrics: {'eval_results': [(1, {'train/loss': 0.8372680069089006, 'validation/loss': 0.8358118551614604, 'validation/num_examples': 83274637, 'test/loss': 0.8364300465910259, 'test/num_examples': 95000000, 'score': 4.5749499797821045, 'total_duration': 884.554027557373, 'accumulated_submission_time': 4.5749499797821045, 'accumulated_eval_time': 879.6049809455872, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (128, {'train/loss': 0.12856768605123584, 'validation/loss': 0.12900573719172526, 'validation/num_examples': 83274637, 'test/loss': 0.13186845210511056, 'test/num_examples': 95000000, 'score': 125.10863494873047, 'total_duration': 1900.9230625629425, 'accumulated_submission_time': 125.10863494873047, 'accumulated_eval_time': 1774.4666442871094, 'accumulated_logging_time': 0.05906033515930176, 'global_step': 128, 'preemption_count': 0}), (251, {'train/loss': 0.1284102053445997, 'validation/loss': 0.12861962093718016, 'validation/num_examples': 83274637, 'test/loss': 0.13107873916581805, 'test/num_examples': 95000000, 'score': 245.6804003715515, 'total_duration': 2894.483717918396, 'accumulated_submission_time': 245.6804003715515, 'accumulated_eval_time': 2646.5334374904633, 'accumulated_logging_time': 0.07641792297363281, 'global_step': 251, 'preemption_count': 0}), (376, {'train/loss': 0.12735854272199118, 'validation/loss': 0.12753445684334078, 'validation/num_examples': 83274637, 'test/loss': 0.12998620982878836, 'test/num_examples': 95000000, 'score': 366.4601035118103, 'total_duration': 3908.585577249527, 'accumulated_submission_time': 366.4601035118103, 'accumulated_eval_time': 3538.935889482498, 'accumulated_logging_time': 0.09353375434875488, 'global_step': 376, 'preemption_count': 0}), (500, {'train/loss': 0.12708554649395756, 'validation/loss': 0.12749320697580657, 'validation/num_examples': 83274637, 'test/loss': 0.12992275944294177, 'test/num_examples': 95000000, 'score': 486.2921693325043, 'total_duration': 4900.026757717133, 'accumulated_submission_time': 486.2921693325043, 'accumulated_eval_time': 4409.6216785907745, 'accumulated_logging_time': 0.12847232818603516, 'global_step': 500, 'preemption_count': 0}), (627, {'train/loss': 0.1260084924384286, 'validation/loss': 0.12660912280631412, 'validation/num_examples': 83274637, 'test/loss': 0.12901285073595548, 'test/num_examples': 95000000, 'score': 606.0163090229034, 'total_duration': 5914.887201309204, 'accumulated_submission_time': 606.0163090229034, 'accumulated_eval_time': 5303.884338617325, 'accumulated_logging_time': 0.14438748359680176, 'global_step': 627, 'preemption_count': 0}), (750, {'train/loss': 0.12368760540360996, 'validation/loss': 0.1270288564166763, 'validation/num_examples': 83274637, 'test/loss': 0.12958252743353593, 'test/num_examples': 95000000, 'score': 725.6278722286224, 'total_duration': 6919.084104776382, 'accumulated_submission_time': 725.6278722286224, 'accumulated_eval_time': 6187.54043340683, 'accumulated_logging_time': 0.16140508651733398, 'global_step': 750, 'preemption_count': 0}), (870, {'train/loss': 0.12714357292184325, 'validation/loss': 0.12675777746777478, 'validation/num_examples': 83274637, 'test/loss': 0.12930450026273224, 'test/num_examples': 95000000, 'score': 845.5629544258118, 'total_duration': 7920.086271762848, 'accumulated_submission_time': 845.5629544258118, 'accumulated_eval_time': 7067.716192007065, 'accumulated_logging_time': 0.1791973114013672, 'global_step': 870, 'preemption_count': 0}), (994, {'train/loss': 0.1246893942172337, 'validation/loss': 0.1259469950003855, 'validation/num_examples': 83274637, 'test/loss': 0.1284691206574691, 'test/num_examples': 95000000, 'score': 965.7636876106262, 'total_duration': 8923.74655532837, 'accumulated_submission_time': 965.7636876106262, 'accumulated_eval_time': 7950.310286998749, 'accumulated_logging_time': 0.19660258293151855, 'global_step': 994, 'preemption_count': 0}), (1113, {'train/loss': 0.12279664347534214, 'validation/loss': 0.12598902059281877, 'validation/num_examples': 83274637, 'test/loss': 0.12829156227509347, 'test/num_examples': 95000000, 'score': 1085.4407052993774, 'total_duration': 9898.469440221786, 'accumulated_submission_time': 1085.4407052993774, 'accumulated_eval_time': 8804.561419010162, 'accumulated_logging_time': 0.2129683494567871, 'global_step': 1113, 'preemption_count': 0}), (1240, {'train/loss': 0.12582157321642376, 'validation/loss': 0.1258041458397921, 'validation/num_examples': 83274637, 'test/loss': 0.12804067429620844, 'test/num_examples': 95000000, 'score': 1205.3944432735443, 'total_duration': 10857.857027292252, 'accumulated_submission_time': 1205.3944432735443, 'accumulated_eval_time': 9643.040719032288, 'accumulated_logging_time': 0.2798757553100586, 'global_step': 1240, 'preemption_count': 0}), (1361, {'train/loss': 0.12558539015069287, 'validation/loss': 0.12655573328842815, 'validation/num_examples': 83274637, 'test/loss': 0.12892446824617887, 'test/num_examples': 95000000, 'score': 1325.249942779541, 'total_duration': 11795.189979553223, 'accumulated_submission_time': 1325.249942779541, 'accumulated_eval_time': 10459.612497806549, 'accumulated_logging_time': 0.29643678665161133, 'global_step': 1361, 'preemption_count': 0}), (1485, {'train/loss': 0.12453780532269473, 'validation/loss': 0.1258493718909249, 'validation/num_examples': 83274637, 'test/loss': 0.12802018090499076, 'test/num_examples': 95000000, 'score': 1445.1929314136505, 'total_duration': 12678.302263259888, 'accumulated_submission_time': 1445.1929314136505, 'accumulated_eval_time': 11221.862795352936, 'accumulated_logging_time': 0.3144991397857666, 'global_step': 1485, 'preemption_count': 0}), (1607, {'train/loss': 0.1246877809948545, 'validation/loss': 0.12683965089090804, 'validation/num_examples': 83274637, 'test/loss': 0.1292577207273383, 'test/num_examples': 95000000, 'score': 1565.5892806053162, 'total_duration': 13491.585928440094, 'accumulated_submission_time': 1565.5892806053162, 'accumulated_eval_time': 11913.863302707672, 'accumulated_logging_time': 0.3306610584259033, 'global_step': 1607, 'preemption_count': 0}), (1727, {'train/loss': 0.1251219073043905, 'validation/loss': 0.1251358218279607, 'validation/num_examples': 83274637, 'test/loss': 0.12742741135173596, 'test/num_examples': 95000000, 'score': 1685.4304251670837, 'total_duration': 14316.277921676636, 'accumulated_submission_time': 1685.4304251670837, 'accumulated_eval_time': 12617.82832455635, 'accumulated_logging_time': 0.34963393211364746, 'global_step': 1727, 'preemption_count': 0}), (1851, {'train/loss': 0.12354361350308593, 'validation/loss': 0.12588235492896954, 'validation/num_examples': 83274637, 'test/loss': 0.12846187821229635, 'test/num_examples': 95000000, 'score': 1805.6695804595947, 'total_duration': 15132.868842840195, 'accumulated_submission_time': 1805.6695804595947, 'accumulated_eval_time': 13313.314613819122, 'accumulated_logging_time': 0.36646342277526855, 'global_step': 1851, 'preemption_count': 0}), (1972, {'train/loss': 0.12352700453264792, 'validation/loss': 0.12599491511378536, 'validation/num_examples': 83274637, 'test/loss': 0.12979578342084383, 'test/num_examples': 95000000, 'score': 1926.2673420906067, 'total_duration': 15955.679548740387, 'accumulated_submission_time': 1926.2673420906067, 'accumulated_eval_time': 14014.657854557037, 'accumulated_logging_time': 0.38373827934265137, 'global_step': 1972, 'preemption_count': 0}), (2095, {'train/loss': 0.12361880912496422, 'validation/loss': 0.12570947496492227, 'validation/num_examples': 83274637, 'test/loss': 0.1280773653891714, 'test/num_examples': 95000000, 'score': 2046.9592020511627, 'total_duration': 16767.918994426727, 'accumulated_submission_time': 2046.9592020511627, 'accumulated_eval_time': 14705.322610139847, 'accumulated_logging_time': 0.40008115768432617, 'global_step': 2095, 'preemption_count': 0}), (2211, {'train/loss': 0.12494560092824658, 'validation/loss': 0.1260863386659836, 'validation/num_examples': 83274637, 'test/loss': 0.12839387804621646, 'test/num_examples': 95000000, 'score': 2167.274055957794, 'total_duration': 17599.730130434036, 'accumulated_submission_time': 2167.274055957794, 'accumulated_eval_time': 15415.924892187119, 'accumulated_logging_time': 0.4172022342681885, 'global_step': 2211, 'preemption_count': 0}), (2330, {'train/loss': 0.1252426137627725, 'validation/loss': 0.1250652802651992, 'validation/num_examples': 83274637, 'test/loss': 0.1274694749838578, 'test/num_examples': 95000000, 'score': 2287.8169384002686, 'total_duration': 18421.078630447388, 'accumulated_submission_time': 2287.8169384002686, 'accumulated_eval_time': 16115.90685582161, 'accumulated_logging_time': 0.43357372283935547, 'global_step': 2330, 'preemption_count': 0}), (2452, {'train/loss': 0.12312741085369167, 'validation/loss': 0.1252291737124209, 'validation/num_examples': 83274637, 'test/loss': 0.12765323103344567, 'test/num_examples': 95000000, 'score': 2407.816363096237, 'total_duration': 19233.863020181656, 'accumulated_submission_time': 2407.816363096237, 'accumulated_eval_time': 16807.813301324844, 'accumulated_logging_time': 0.4501988887786865, 'global_step': 2452, 'preemption_count': 0}), (2573, {'train/loss': 0.12493995157768172, 'validation/loss': 0.1251973874930707, 'validation/num_examples': 83274637, 'test/loss': 0.12758159990892912, 'test/num_examples': 95000000, 'score': 2527.6011111736298, 'total_duration': 20075.878520727158, 'accumulated_submission_time': 2527.6011111736298, 'accumulated_eval_time': 17529.121607780457, 'accumulated_logging_time': 0.46643900871276855, 'global_step': 2573, 'preemption_count': 0}), (2698, {'train/loss': 0.12251138401452467, 'validation/loss': 0.1248545827652636, 'validation/num_examples': 83274637, 'test/loss': 0.12719171517582945, 'test/num_examples': 95000000, 'score': 2647.179775238037, 'total_duration': 20890.72908592224, 'accumulated_submission_time': 2647.179775238037, 'accumulated_eval_time': 18223.54805278778, 'accumulated_logging_time': 0.48497486114501953, 'global_step': 2698, 'preemption_count': 0}), (2817, {'train/loss': 0.1229249759559442, 'validation/loss': 0.12499534662610047, 'validation/num_examples': 83274637, 'test/loss': 0.12727011013436568, 'test/num_examples': 95000000, 'score': 2766.760534763336, 'total_duration': 21707.43790435791, 'accumulated_submission_time': 2766.760534763336, 'accumulated_eval_time': 18919.819051504135, 'accumulated_logging_time': 0.5241272449493408, 'global_step': 2817, 'preemption_count': 0}), (2942, {'train/loss': 0.12361849805740696, 'validation/loss': 0.12535044312703292, 'validation/num_examples': 83274637, 'test/loss': 0.12785298301761025, 'test/num_examples': 95000000, 'score': 2887.668531894684, 'total_duration': 22512.24366760254, 'accumulated_submission_time': 2887.668531894684, 'accumulated_eval_time': 19602.8125808239, 'accumulated_logging_time': 0.5416979789733887, 'global_step': 2942, 'preemption_count': 0}), (3065, {'train/loss': 0.12105914421864007, 'validation/loss': 0.12494446276259832, 'validation/num_examples': 83274637, 'test/loss': 0.127198951226124, 'test/num_examples': 95000000, 'score': 3007.2029054164886, 'total_duration': 23331.91042947769, 'accumulated_submission_time': 3007.2029054164886, 'accumulated_eval_time': 20302.07315301895, 'accumulated_logging_time': 0.5585799217224121, 'global_step': 3065, 'preemption_count': 0}), (3188, {'train/loss': 0.12278319179804524, 'validation/loss': 0.12548813314093302, 'validation/num_examples': 83274637, 'test/loss': 0.12790892722099706, 'test/num_examples': 95000000, 'score': 3126.876755952835, 'total_duration': 24149.152728557587, 'accumulated_submission_time': 3126.876755952835, 'accumulated_eval_time': 20998.694912672043, 'accumulated_logging_time': 0.5768539905548096, 'global_step': 3188, 'preemption_count': 0}), (3303, {'train/loss': 0.12325024076647867, 'validation/loss': 0.12518418765001246, 'validation/num_examples': 83274637, 'test/loss': 0.12758720750081914, 'test/num_examples': 95000000, 'score': 3247.1840131282806, 'total_duration': 24961.478481531143, 'accumulated_submission_time': 3247.1840131282806, 'accumulated_eval_time': 21689.848920106888, 'accumulated_logging_time': 0.5940737724304199, 'global_step': 3303, 'preemption_count': 0}), (3426, {'train/loss': 0.1255722740276554, 'validation/loss': 0.12492695466929997, 'validation/num_examples': 83274637, 'test/loss': 0.1272743878795423, 'test/num_examples': 95000000, 'score': 3367.8019325733185, 'total_duration': 25774.394868850708, 'accumulated_submission_time': 3367.8019325733185, 'accumulated_eval_time': 22381.242252349854, 'accumulated_logging_time': 0.6110641956329346, 'global_step': 3426, 'preemption_count': 0}), (3551, {'train/loss': 0.12614753305789808, 'validation/loss': 0.12459014488322805, 'validation/num_examples': 83274637, 'test/loss': 0.12683043759492574, 'test/num_examples': 95000000, 'score': 3488.0455770492554, 'total_duration': 26585.280919075012, 'accumulated_submission_time': 3488.0455770492554, 'accumulated_eval_time': 23070.999271154404, 'accumulated_logging_time': 0.6449613571166992, 'global_step': 3551, 'preemption_count': 0}), (3671, {'train/loss': 0.1248595502015739, 'validation/loss': 0.12464134691031425, 'validation/num_examples': 83274637, 'test/loss': 0.12706937891970183, 'test/num_examples': 95000000, 'score': 3608.677136659622, 'total_duration': 27381.297457695007, 'accumulated_submission_time': 3608.677136659622, 'accumulated_eval_time': 23745.48271226883, 'accumulated_logging_time': 0.6720564365386963, 'global_step': 3671, 'preemption_count': 0}), (3797, {'train/loss': 0.12311814855313295, 'validation/loss': 0.12451377694300199, 'validation/num_examples': 83274637, 'test/loss': 0.12689970806178041, 'test/num_examples': 95000000, 'score': 3728.601909637451, 'total_duration': 28184.235728025436, 'accumulated_submission_time': 3728.601909637451, 'accumulated_eval_time': 24427.56992125511, 'accumulated_logging_time': 0.6898050308227539, 'global_step': 3797, 'preemption_count': 0}), (3918, {'train/loss': 0.12333901923395527, 'validation/loss': 0.12460197909320604, 'validation/num_examples': 83274637, 'test/loss': 0.12702625766975, 'test/num_examples': 95000000, 'score': 3848.489721298218, 'total_duration': 28986.085779428482, 'accumulated_submission_time': 3848.489721298218, 'accumulated_eval_time': 25108.626169204712, 'accumulated_logging_time': 0.7069041728973389, 'global_step': 3918, 'preemption_count': 0}), (4042, {'train/loss': 0.12366455068028312, 'validation/loss': 0.12453124486103, 'validation/num_examples': 83274637, 'test/loss': 0.12696067978491532, 'test/num_examples': 95000000, 'score': 3969.6656963825226, 'total_duration': 29806.930405139923, 'accumulated_submission_time': 3969.6656963825226, 'accumulated_eval_time': 25807.400044441223, 'accumulated_logging_time': 0.724757194519043, 'global_step': 4042, 'preemption_count': 0}), (4165, {'train/loss': 0.12349789245678916, 'validation/loss': 0.12483233581966084, 'validation/num_examples': 83274637, 'test/loss': 0.12742939613350315, 'test/num_examples': 95000000, 'score': 4089.9789078235626, 'total_duration': 30627.12962794304, 'accumulated_submission_time': 4089.9789078235626, 'accumulated_eval_time': 26506.34801030159, 'accumulated_logging_time': 0.7438812255859375, 'global_step': 4165, 'preemption_count': 0}), (4287, {'train/loss': 0.12275991189533036, 'validation/loss': 0.12450317691896538, 'validation/num_examples': 83274637, 'test/loss': 0.1269466377602828, 'test/num_examples': 95000000, 'score': 4209.778605222702, 'total_duration': 31441.27959561348, 'accumulated_submission_time': 4209.778605222702, 'accumulated_eval_time': 27199.839778900146, 'accumulated_logging_time': 0.7620193958282471, 'global_step': 4287, 'preemption_count': 0}), (4411, {'train/loss': 0.12335323706945724, 'validation/loss': 0.12450409915329398, 'validation/num_examples': 83274637, 'test/loss': 0.12686706287259553, 'test/num_examples': 95000000, 'score': 4329.818938732147, 'total_duration': 32260.285347223282, 'accumulated_submission_time': 4329.818938732147, 'accumulated_eval_time': 27897.89021062851, 'accumulated_logging_time': 0.7976374626159668, 'global_step': 4411, 'preemption_count': 0}), (4534, {'train/loss': 0.12171964065185337, 'validation/loss': 0.12448283646745831, 'validation/num_examples': 83274637, 'test/loss': 0.12681296133808337, 'test/num_examples': 95000000, 'score': 4450.975401163101, 'total_duration': 33092.80724191666, 'accumulated_submission_time': 4450.975401163101, 'accumulated_eval_time': 28608.376883745193, 'accumulated_logging_time': 0.8150219917297363, 'global_step': 4534, 'preemption_count': 0}), (4661, {'train/loss': 0.12468209489470027, 'validation/loss': 0.12502163757292126, 'validation/num_examples': 83274637, 'test/loss': 0.12710753461773522, 'test/num_examples': 95000000, 'score': 4571.179208040237, 'total_duration': 33904.43240189552, 'accumulated_submission_time': 4571.179208040237, 'accumulated_eval_time': 29298.886647224426, 'accumulated_logging_time': 0.8340740203857422, 'global_step': 4661, 'preemption_count': 0}), (4777, {'train/loss': 0.1228803406968257, 'validation/loss': 0.1245294548053785, 'validation/num_examples': 83274637, 'test/loss': 0.12686103972027427, 'test/num_examples': 95000000, 'score': 4691.647433280945, 'total_duration': 34733.53422355652, 'accumulated_submission_time': 4691.647433280945, 'accumulated_eval_time': 30006.6241440773, 'accumulated_logging_time': 0.8519127368927002, 'global_step': 4777, 'preemption_count': 0}), (4903, {'train/loss': 0.12247424278756962, 'validation/loss': 0.12449136599992862, 'validation/num_examples': 83274637, 'test/loss': 0.12693517799526013, 'test/num_examples': 95000000, 'score': 4811.64857172966, 'total_duration': 35536.85018205643, 'accumulated_submission_time': 4811.64857172966, 'accumulated_eval_time': 30689.019859075546, 'accumulated_logging_time': 0.8716294765472412, 'global_step': 4903, 'preemption_count': 0}), (5026, {'train/loss': 0.12265547541824516, 'validation/loss': 0.12487215453760045, 'validation/num_examples': 83274637, 'test/loss': 0.12729147794141268, 'test/num_examples': 95000000, 'score': 4931.467498064041, 'total_duration': 36369.820519924164, 'accumulated_submission_time': 4931.467498064041, 'accumulated_eval_time': 31401.306683301926, 'accumulated_logging_time': 0.8954281806945801, 'global_step': 5026, 'preemption_count': 0}), (5153, {'train/loss': 0.12582382031049658, 'validation/loss': 0.12468836991143523, 'validation/num_examples': 83274637, 'test/loss': 0.12711287894455758, 'test/num_examples': 95000000, 'score': 5051.093612670898, 'total_duration': 37188.591873168945, 'accumulated_submission_time': 5051.093612670898, 'accumulated_eval_time': 32099.553120613098, 'accumulated_logging_time': 0.913154125213623, 'global_step': 5153, 'preemption_count': 0}), (5271, {'train/loss': 0.12447069099461709, 'validation/loss': 0.12439834547858102, 'validation/num_examples': 83274637, 'test/loss': 0.1267582923295272, 'test/num_examples': 95000000, 'score': 5171.826122045517, 'total_duration': 38006.85632991791, 'accumulated_submission_time': 5171.826122045517, 'accumulated_eval_time': 32796.22246360779, 'accumulated_logging_time': 0.9319491386413574, 'global_step': 5271, 'preemption_count': 0}), (5391, {'train/loss': 0.122064291826654, 'validation/loss': 0.12428513520444695, 'validation/num_examples': 83274637, 'test/loss': 0.1266025569468448, 'test/num_examples': 95000000, 'score': 5291.774982452393, 'total_duration': 38821.7429959774, 'accumulated_submission_time': 5291.774982452393, 'accumulated_eval_time': 33490.23759698868, 'accumulated_logging_time': 0.9954791069030762, 'global_step': 5391, 'preemption_count': 0}), (5516, {'train/loss': 0.12324977943694984, 'validation/loss': 0.1242073733451345, 'validation/num_examples': 83274637, 'test/loss': 0.12654180725374722, 'test/num_examples': 95000000, 'score': 5411.982172489166, 'total_duration': 39632.91647362709, 'accumulated_submission_time': 5411.982172489166, 'accumulated_eval_time': 34180.33908724785, 'accumulated_logging_time': 1.014986515045166, 'global_step': 5516, 'preemption_count': 0}), (5638, {'train/loss': 0.12162708275291476, 'validation/loss': 0.12433863352324115, 'validation/num_examples': 83274637, 'test/loss': 0.1267145609060187, 'test/num_examples': 95000000, 'score': 5532.762580633163, 'total_duration': 40437.38548874855, 'accumulated_submission_time': 5532.762580633163, 'accumulated_eval_time': 34863.16097283363, 'accumulated_logging_time': 1.0325391292572021, 'global_step': 5638, 'preemption_count': 0}), (5763, {'train/loss': 0.12389354145526484, 'validation/loss': 0.12436027440766848, 'validation/num_examples': 83274637, 'test/loss': 0.1266813805102298, 'test/num_examples': 95000000, 'score': 5653.480751991272, 'total_duration': 41260.29347062111, 'accumulated_submission_time': 5653.480751991272, 'accumulated_eval_time': 35564.44536995888, 'accumulated_logging_time': 1.0504515171051025, 'global_step': 5763, 'preemption_count': 0}), (5888, {'train/loss': 0.12387756623733388, 'validation/loss': 0.12427081271777321, 'validation/num_examples': 83274637, 'test/loss': 0.1266759790498834, 'test/num_examples': 95000000, 'score': 5773.397820949554, 'total_duration': 42078.29068398476, 'accumulated_submission_time': 5773.397820949554, 'accumulated_eval_time': 36261.63034772873, 'accumulated_logging_time': 1.070716142654419, 'global_step': 5888, 'preemption_count': 0}), (6005, {'train/loss': 0.12352680325694639, 'validation/loss': 0.12419725468292936, 'validation/num_examples': 83274637, 'test/loss': 0.1266039461065995, 'test/num_examples': 95000000, 'score': 5893.129123210907, 'total_duration': 42885.17369103432, 'accumulated_submission_time': 5893.129123210907, 'accumulated_eval_time': 36947.82575964928, 'accumulated_logging_time': 1.1471734046936035, 'global_step': 6005, 'preemption_count': 0}), (6127, {'train/loss': 0.12135458927183461, 'validation/loss': 0.12408336949541246, 'validation/num_examples': 83274637, 'test/loss': 0.12640728318730404, 'test/num_examples': 95000000, 'score': 6013.1171000003815, 'total_duration': 43696.352658987045, 'accumulated_submission_time': 6013.1171000003815, 'accumulated_eval_time': 37638.091678380966, 'accumulated_logging_time': 1.1666593551635742, 'global_step': 6127, 'preemption_count': 0}), (6252, {'train/loss': 0.12187257939956961, 'validation/loss': 0.12407379121469803, 'validation/num_examples': 83274637, 'test/loss': 0.12639527490491365, 'test/num_examples': 95000000, 'score': 6133.365200519562, 'total_duration': 44511.01498007774, 'accumulated_submission_time': 6133.365200519562, 'accumulated_eval_time': 38331.65941119194, 'accumulated_logging_time': 1.1839931011199951, 'global_step': 6252, 'preemption_count': 0}), (6373, {'train/loss': 0.12255344376826013, 'validation/loss': 0.12417419743492632, 'validation/num_examples': 83274637, 'test/loss': 0.12658853200105366, 'test/num_examples': 95000000, 'score': 6253.062659740448, 'total_duration': 45323.2739238739, 'accumulated_submission_time': 6253.062659740448, 'accumulated_eval_time': 39023.30649447441, 'accumulated_logging_time': 1.2023210525512695, 'global_step': 6373, 'preemption_count': 0}), (6497, {'train/loss': 0.12098470468406816, 'validation/loss': 0.1239986935889033, 'validation/num_examples': 83274637, 'test/loss': 0.12636945591527035, 'test/num_examples': 95000000, 'score': 6372.951236963272, 'total_duration': 46151.571860075, 'accumulated_submission_time': 6372.951236963272, 'accumulated_eval_time': 39730.8684322834, 'accumulated_logging_time': 1.2206060886383057, 'global_step': 6497, 'preemption_count': 0}), (6617, {'train/loss': 0.12291126046696182, 'validation/loss': 0.123981588276529, 'validation/num_examples': 83274637, 'test/loss': 0.12629376359799033, 'test/num_examples': 95000000, 'score': 6492.937025308609, 'total_duration': 46951.6889731884, 'accumulated_submission_time': 6492.937025308609, 'accumulated_eval_time': 40410.11602473259, 'accumulated_logging_time': 1.239302635192871, 'global_step': 6617, 'preemption_count': 0}), (6741, {'train/loss': 0.1227758320872222, 'validation/loss': 0.12412662367162039, 'validation/num_examples': 83274637, 'test/loss': 0.12645191842378314, 'test/num_examples': 95000000, 'score': 6613.054425239563, 'total_duration': 47771.10245299339, 'accumulated_submission_time': 6613.054425239563, 'accumulated_eval_time': 41108.54629826546, 'accumulated_logging_time': 1.259434700012207, 'global_step': 6741, 'preemption_count': 0}), (6866, {'train/loss': 0.12219918012042233, 'validation/loss': 0.1240513472247731, 'validation/num_examples': 83274637, 'test/loss': 0.12640641555003115, 'test/num_examples': 95000000, 'score': 6732.8171491622925, 'total_duration': 48576.0773358345, 'accumulated_submission_time': 6732.8171491622925, 'accumulated_eval_time': 41792.87122607231, 'accumulated_logging_time': 1.2788503170013428, 'global_step': 6866, 'preemption_count': 0}), (7001, {'train/loss': 0.12233406845253007, 'validation/loss': 0.12409298795158048, 'validation/num_examples': 83274637, 'test/loss': 0.12638767789655986, 'test/num_examples': 95000000, 'score': 6852.586658000946, 'total_duration': 49409.21079468727, 'accumulated_submission_time': 6852.586658000946, 'accumulated_eval_time': 42505.331287145615, 'accumulated_logging_time': 1.3298923969268799, 'global_step': 7001, 'preemption_count': 0}), (7127, {'train/loss': 0.1219105096330059, 'validation/loss': 0.12401073884117644, 'validation/num_examples': 83274637, 'test/loss': 0.1263897263436167, 'test/num_examples': 95000000, 'score': 6972.409323453903, 'total_duration': 50206.78033185005, 'accumulated_submission_time': 6972.409323453903, 'accumulated_eval_time': 43182.20598435402, 'accumulated_logging_time': 1.3483171463012695, 'global_step': 7127, 'preemption_count': 0}), (7258, {'train/loss': 0.1220586257115957, 'validation/loss': 0.12406118817850945, 'validation/num_examples': 83274637, 'test/loss': 0.12640187517917031, 'test/num_examples': 95000000, 'score': 7092.411914348602, 'total_duration': 51034.13642644882, 'accumulated_submission_time': 7092.411914348602, 'accumulated_eval_time': 43888.685754776, 'accumulated_logging_time': 1.3685109615325928, 'global_step': 7258, 'preemption_count': 0}), (7378, {'train/loss': 0.12143166089247595, 'validation/loss': 0.12394583034761773, 'validation/num_examples': 83274637, 'test/loss': 0.1263138161034032, 'test/num_examples': 95000000, 'score': 7212.501144170761, 'total_duration': 51854.84638595581, 'accumulated_submission_time': 7212.501144170761, 'accumulated_eval_time': 44588.4827606678, 'accumulated_logging_time': 1.3884644508361816, 'global_step': 7378, 'preemption_count': 0}), (7503, {'train/loss': 0.12092056417872185, 'validation/loss': 0.12392299552640867, 'validation/num_examples': 83274637, 'test/loss': 0.12625439645441958, 'test/num_examples': 95000000, 'score': 7332.660216093063, 'total_duration': 52677.37572526932, 'accumulated_submission_time': 7332.660216093063, 'accumulated_eval_time': 45289.93433094025, 'accumulated_logging_time': 1.4331932067871094, 'global_step': 7503, 'preemption_count': 0}), (7625, {'train/loss': 0.11975760082860838, 'validation/loss': 0.12378918318079447, 'validation/num_examples': 83274637, 'test/loss': 0.12609952117542467, 'test/num_examples': 95000000, 'score': 7453.00655579567, 'total_duration': 53496.8939807415, 'accumulated_submission_time': 7453.00655579567, 'accumulated_eval_time': 45988.21409392357, 'accumulated_logging_time': 1.4518685340881348, 'global_step': 7625, 'preemption_count': 0}), (7746, {'train/loss': 0.12169647993740319, 'validation/loss': 0.12377082301770063, 'validation/num_examples': 83274637, 'test/loss': 0.12601502458769145, 'test/num_examples': 95000000, 'score': 7573.344976425171, 'total_duration': 54307.04640746117, 'accumulated_submission_time': 7573.344976425171, 'accumulated_eval_time': 46677.155938863754, 'accumulated_logging_time': 1.4708888530731201, 'global_step': 7746, 'preemption_count': 0}), (7866, {'train/loss': 0.12077552626937695, 'validation/loss': 0.12374843944459823, 'validation/num_examples': 83274637, 'test/loss': 0.12601914355765895, 'test/num_examples': 95000000, 'score': 7693.810689687729, 'total_duration': 55132.520562410355, 'accumulated_submission_time': 7693.810689687729, 'accumulated_eval_time': 47381.26044535637, 'accumulated_logging_time': 1.491452932357788, 'global_step': 7866, 'preemption_count': 0})], 'global_step': 7988}
I0317 02:15:45.633373 140320504472768 submission_runner.py:649] Timing: 7814.502708911896
I0317 02:15:45.633409 140320504472768 submission_runner.py:651] Total number of evals: 65
I0317 02:15:45.633439 140320504472768 submission_runner.py:652] ====================
I0317 02:15:45.633536 140320504472768 submission_runner.py:750] Final criteo1tb score: 0
