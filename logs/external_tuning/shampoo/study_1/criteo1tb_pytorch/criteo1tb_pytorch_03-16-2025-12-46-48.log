torchrun --redirects 1:0,2:0,3:0,4:0,5:0,6:0,7:0 --standalone --nnodes=1 --nproc_per_node=8 submission_runner.py --framework=pytorch --workload=criteo1tb --submission_path=submissions_algorithms/leaderboard/external_tuning/shampoo_submission/submission.py --data_dir=/data/criteo1tb --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=submissions/rolling_leaderboard/external_tuning/shampoo/study_1 --overwrite=True --save_checkpoints=False --rng_seed=1017144525 --torch_compile=true --tuning_ruleset=external --tuning_search_space=submissions_algorithms/leaderboard/external_tuning/shampoo_submission/tuning_search_space.json --num_tuning_trials=5 --hparam_start_index=4 --hparam_end_index=5 2>&1 | tee -a /logs/criteo1tb_pytorch_03-16-2025-12-46-48.log
W0316 12:46:49.721000 9 site-packages/torch/distributed/run.py:793] 
W0316 12:46:49.721000 9 site-packages/torch/distributed/run.py:793] *****************************************
W0316 12:46:49.721000 9 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0316 12:46:49.721000 9 site-packages/torch/distributed/run.py:793] *****************************************
2025-03-16 12:46:50.939239: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-03-16 12:46:50.939238: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-03-16 12:46:50.939237: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-03-16 12:46:50.939234: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-03-16 12:46:50.939234: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-03-16 12:46:50.939246: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-03-16 12:46:50.939238: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-03-16 12:46:50.939231: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1742129210.961689      45 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1742129210.961691      46 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1742129210.961673      50 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1742129210.961674      51 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1742129210.961694      49 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1742129210.961674      48 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1742129210.961673      44 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1742129210.961673      47 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1742129210.968520      50 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
E0000 00:00:1742129210.968520      44 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
E0000 00:00:1742129210.968519      51 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
E0000 00:00:1742129210.968519      49 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
E0000 00:00:1742129210.968528      45 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
E0000 00:00:1742129210.968530      47 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
E0000 00:00:1742129210.968537      46 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
E0000 00:00:1742129210.968536      48 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
[rank7]:[W316 12:46:58.966794970 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 7]  using GPU 7 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank5]:[W316 12:46:58.002700113 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 5]  using GPU 5 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank3]:[W316 12:46:58.064777146 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank4]:[W316 12:46:58.082100765 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 4]  using GPU 4 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank1]:[W316 12:46:58.145283380 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank6]:[W316 12:46:58.145556106 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 6]  using GPU 6 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank2]:[W316 12:46:58.151486169 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank0]:[W316 12:46:58.158440441 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
I0316 12:47:00.064561 139849128674496 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_1/criteo1tb_pytorch.
I0316 12:47:00.064570 140633389098176 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_1/criteo1tb_pytorch.
I0316 12:47:00.064588 140554023089344 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_1/criteo1tb_pytorch.
I0316 12:47:00.064562 139830621443264 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_1/criteo1tb_pytorch.
I0316 12:47:00.064571 139956878537920 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_1/criteo1tb_pytorch.
I0316 12:47:00.064583 140293290714304 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_1/criteo1tb_pytorch.
I0316 12:47:00.064586 139794211816640 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_1/criteo1tb_pytorch.
I0316 12:47:00.064666 140314632471744 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_1/criteo1tb_pytorch.
I0316 12:47:01.241993 140633389098176 logger_utils.py:97] Saving hparams to /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_1/criteo1tb_pytorch/trial_5/hparams.json.
I0316 12:47:01.242012 140314632471744 submission_runner.py:606] Using RNG seed 1017144525
I0316 12:47:01.242207 139956878537920 logger_utils.py:97] Saving hparams to /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_1/criteo1tb_pytorch/trial_5/hparams.json.
I0316 12:47:01.242533 140554023089344 logger_utils.py:97] Saving hparams to /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_1/criteo1tb_pytorch/trial_5/hparams.json.
I0316 12:47:01.242735 139849128674496 logger_utils.py:97] Saving hparams to /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_1/criteo1tb_pytorch/trial_5/hparams.json.
I0316 12:47:01.243225 140314632471744 submission_runner.py:615] --- Tuning run 5/5 ---
I0316 12:47:01.243365 140314632471744 submission_runner.py:620] Creating tuning directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_1/criteo1tb_pytorch/trial_5.
I0316 12:47:01.243601 140314632471744 logger_utils.py:97] Saving hparams to /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_1/criteo1tb_pytorch/trial_5/hparams.json.
I0316 12:47:01.246839 139830621443264 logger_utils.py:91] Loading hparams from /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_1/criteo1tb_pytorch/trial_5/hparams.json.
I0316 12:47:01.347569 140293290714304 logger_utils.py:91] Loading hparams from /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_1/criteo1tb_pytorch/trial_5/hparams.json.
I0316 12:47:01.348796 139794211816640 logger_utils.py:91] Loading hparams from /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_1/criteo1tb_pytorch/trial_5/hparams.json.
I0316 12:47:01.570212 140314632471744 submission_runner.py:218] Initializing dataset.
I0316 12:47:01.570389 140314632471744 submission_runner.py:229] Initializing model.
W0316 12:47:07.516513 140314632471744 submission_runner.py:254] These workloads cannot be fully compiled under current PyTorch version. Proceeding without `torch.compile`.
I0316 12:47:07.516696 140314632471744 submission_runner.py:272] Initializing optimizer.
W0316 12:47:07.517816 140314632471744 distributed_shampoo.py:341] start_preconditioning_step set to -1. Setting start_preconditioning_step equal to precondition frequency 100 by default.
W0316 12:47:07.517919 140314632471744 distributed_shampoo.py:359] Nesterov flag is enabled but momentum parameter is zero! Continuing without using momentum or Nesterov acceleration...
W0316 12:47:07.518210 140554023089344 submission_runner.py:254] These workloads cannot be fully compiled under current PyTorch version. Proceeding without `torch.compile`.
W0316 12:47:07.518299 139956878537920 submission_runner.py:254] These workloads cannot be fully compiled under current PyTorch version. Proceeding without `torch.compile`.
W0316 12:47:07.518570 139830621443264 submission_runner.py:254] These workloads cannot be fully compiled under current PyTorch version. Proceeding without `torch.compile`.
W0316 12:47:07.518667 140633389098176 submission_runner.py:254] These workloads cannot be fully compiled under current PyTorch version. Proceeding without `torch.compile`.
W0316 12:47:07.518725 139849128674496 submission_runner.py:254] These workloads cannot be fully compiled under current PyTorch version. Proceeding without `torch.compile`.
W0316 12:47:07.518892 140293290714304 submission_runner.py:254] These workloads cannot be fully compiled under current PyTorch version. Proceeding without `torch.compile`.
W0316 12:47:07.518963 139794211816640 submission_runner.py:254] These workloads cannot be fully compiled under current PyTorch version. Proceeding without `torch.compile`.
W0316 12:47:07.519327 140554023089344 distributed_shampoo.py:341] start_preconditioning_step set to -1. Setting start_preconditioning_step equal to precondition frequency 100 by default.
W0316 12:47:07.519396 139956878537920 distributed_shampoo.py:341] start_preconditioning_step set to -1. Setting start_preconditioning_step equal to precondition frequency 100 by default.
W0316 12:47:07.519480 140554023089344 distributed_shampoo.py:359] Nesterov flag is enabled but momentum parameter is zero! Continuing without using momentum or Nesterov acceleration...
W0316 12:47:07.519550 139956878537920 distributed_shampoo.py:359] Nesterov flag is enabled but momentum parameter is zero! Continuing without using momentum or Nesterov acceleration...
W0316 12:47:07.519769 139830621443264 distributed_shampoo.py:341] start_preconditioning_step set to -1. Setting start_preconditioning_step equal to precondition frequency 100 by default.
W0316 12:47:07.519840 140633389098176 distributed_shampoo.py:341] start_preconditioning_step set to -1. Setting start_preconditioning_step equal to precondition frequency 100 by default.
W0316 12:47:07.519902 139830621443264 distributed_shampoo.py:359] Nesterov flag is enabled but momentum parameter is zero! Continuing without using momentum or Nesterov acceleration...
W0316 12:47:07.519962 140633389098176 distributed_shampoo.py:359] Nesterov flag is enabled but momentum parameter is zero! Continuing without using momentum or Nesterov acceleration...
W0316 12:47:07.519933 139849128674496 distributed_shampoo.py:341] start_preconditioning_step set to -1. Setting start_preconditioning_step equal to precondition frequency 100 by default.
W0316 12:47:07.520073 139849128674496 distributed_shampoo.py:359] Nesterov flag is enabled but momentum parameter is zero! Continuing without using momentum or Nesterov acceleration...
I0316 12:47:07.520239 140314632471744 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 0.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 0 (torch.Size([512, 13])), Block block_0 (torch.Size([512, 13])).
W0316 12:47:07.520204 140293290714304 distributed_shampoo.py:341] start_preconditioning_step set to -1. Setting start_preconditioning_step equal to precondition frequency 100 by default.
W0316 12:47:07.520246 139794211816640 distributed_shampoo.py:341] start_preconditioning_step set to -1. Setting start_preconditioning_step equal to precondition frequency 100 by default.
W0316 12:47:07.520332 140293290714304 distributed_shampoo.py:359] Nesterov flag is enabled but momentum parameter is zero! Continuing without using momentum or Nesterov acceleration...
W0316 12:47:07.520378 139794211816640 distributed_shampoo.py:359] Nesterov flag is enabled but momentum parameter is zero! Continuing without using momentum or Nesterov acceleration...
I0316 12:47:07.520406 140314632471744 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 1.block_0 ([torch.Size([0])]) for Parameter 1 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 12:47:07.520548 140314632471744 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 2.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 2 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 12:47:07.520703 140314632471744 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 3.block_0 ([torch.Size([0])]) for Parameter 3 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 12:47:07.520821 140314632471744 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 4.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 4 (torch.Size([128, 256])), Block block_0 (torch.Size([128, 256])).
I0316 12:47:07.520912 140314632471744 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 5.block_0 ([torch.Size([0])]) for Parameter 5 (torch.Size([128])), Block block_0 (torch.Size([128])).
I0316 12:47:07.521033 140314632471744 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 6.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 6 (torch.Size([1024, 506])), Block block_0 (torch.Size([1024, 506])).
I0316 12:47:07.521153 140314632471744 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 7.block_0 ([torch.Size([0])]) for Parameter 7 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 12:47:07.521730 140554023089344 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 0.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 0 (torch.Size([512, 13])), Block block_0 (torch.Size([512, 13])).
I0316 12:47:07.521768 139956878537920 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 0.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 0 (torch.Size([512, 13])), Block block_0 (torch.Size([512, 13])).
I0316 12:47:07.521897 140554023089344 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 1.block_0 ([torch.Size([0])]) for Parameter 1 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 12:47:07.521934 139956878537920 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 1.block_0 ([torch.Size([0])]) for Parameter 1 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 12:47:07.522049 140554023089344 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 2.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 2 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 12:47:07.522087 139956878537920 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 2.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 2 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 12:47:07.522164 140554023089344 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 3.block_0 ([torch.Size([0])]) for Parameter 3 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 12:47:07.522206 139956878537920 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 3.block_0 ([torch.Size([0])]) for Parameter 3 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 12:47:07.522297 140554023089344 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 4.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 4 (torch.Size([128, 256])), Block block_0 (torch.Size([128, 256])).
I0316 12:47:07.522245 139830621443264 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 0.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 0 (torch.Size([512, 13])), Block block_0 (torch.Size([512, 13])).
I0316 12:47:07.522295 140314632471744 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 8.block_0 ([torch.Size([1024, 1024]), torch.Size([1024, 1024])]) for Parameter 8 (torch.Size([1024, 1024])), Block block_0 (torch.Size([1024, 1024])).
I0316 12:47:07.522337 139956878537920 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 4.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 4 (torch.Size([128, 256])), Block block_0 (torch.Size([128, 256])).
I0316 12:47:07.522346 140633389098176 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 0.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 0 (torch.Size([512, 13])), Block block_0 (torch.Size([512, 13])).
I0316 12:47:07.522392 140554023089344 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 5.block_0 ([torch.Size([0])]) for Parameter 5 (torch.Size([128])), Block block_0 (torch.Size([128])).
I0316 12:47:07.522428 139956878537920 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 5.block_0 ([torch.Size([0])]) for Parameter 5 (torch.Size([128])), Block block_0 (torch.Size([128])).
I0316 12:47:07.522431 140314632471744 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 9.block_0 ([torch.Size([0])]) for Parameter 9 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 12:47:07.522510 140633389098176 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 1.block_0 ([torch.Size([0])]) for Parameter 1 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 12:47:07.522549 140314632471744 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 10.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 10 (torch.Size([512, 1024])), Block block_0 (torch.Size([512, 1024])).
I0316 12:47:07.522558 139956878537920 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 6.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 6 (torch.Size([1024, 506])), Block block_0 (torch.Size([1024, 506])).
I0316 12:47:07.522578 139849128674496 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 0.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 0 (torch.Size([512, 13])), Block block_0 (torch.Size([512, 13])).
I0316 12:47:07.522644 140314632471744 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 11.block_0 ([torch.Size([0])]) for Parameter 11 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 12:47:07.522658 139956878537920 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 7.block_0 ([torch.Size([0])]) for Parameter 7 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 12:47:07.522746 140314632471744 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 12.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 12 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 12:47:07.522771 139956878537920 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 8.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 8 (torch.Size([1024, 1024])), Block block_0 (torch.Size([1024, 1024])).
I0316 12:47:07.522754 139849128674496 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 1.block_0 ([torch.Size([0])]) for Parameter 1 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 12:47:07.522830 140314632471744 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 13.block_0 ([torch.Size([0])]) for Parameter 13 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 12:47:07.522860 139956878537920 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 9.block_0 ([torch.Size([0])]) for Parameter 9 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 12:47:07.522846 139830621443264 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 1.block_0 ([torch.Size([512, 512])]) for Parameter 1 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 12:47:07.522896 139849128674496 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 2.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 2 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 12:47:07.522927 140314632471744 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 14.block_0 ([torch.Size([0])]) for Parameter 14 (torch.Size([1, 256])), Block block_0 (torch.Size([256])).
I0316 12:47:07.523001 139956878537920 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 10.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 10 (torch.Size([512, 1024])), Block block_0 (torch.Size([512, 1024])).
I0316 12:47:07.523029 140314632471744 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 15.block_0 ([torch.Size([0])]) for Parameter 15 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0316 12:47:07.523045 139849128674496 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 3.block_0 ([torch.Size([0])]) for Parameter 3 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 12:47:07.523070 139830621443264 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 2.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 2 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 12:47:07.523096 139956878537920 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 11.block_0 ([torch.Size([0])]) for Parameter 11 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 12:47:07.523124 140314632471744 shampoo_preconditioner_list.py:612] Rank 0: ShampooPreconditionerList Numel Breakdown: (524626, 524288, 655360, 131072, 163840, 32768, 2609224, 2097152, 4194304, 2097152, 2621440, 524288, 655360, 131072, 131072, 2)
I0316 12:47:07.523168 140314632471744 shampoo_preconditioner_list.py:615] Rank 0: ShampooPreconditionerList Bytes Breakdown: (2098504,)
I0316 12:47:07.523180 139849128674496 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 4.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 4 (torch.Size([128, 256])), Block block_0 (torch.Size([128, 256])).
I0316 12:47:07.523152 139794211816640 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 0.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 0 (torch.Size([512, 13])), Block block_0 (torch.Size([512, 13])).
I0316 12:47:07.523175 140554023089344 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 6.block_0 ([torch.Size([1024, 1024]), torch.Size([506, 506])]) for Parameter 6 (torch.Size([1024, 506])), Block block_0 (torch.Size([1024, 506])).
I0316 12:47:07.523209 140314632471744 shampoo_preconditioner_list.py:618] Rank 0: ShampooPreconditionerList Total Elements: 17093020
I0316 12:47:07.523239 140314632471744 shampoo_preconditioner_list.py:621] Rank 0: ShampooPreconditionerList Total Bytes: 2098504
I0316 12:47:07.523270 139849128674496 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 5.block_0 ([torch.Size([0])]) for Parameter 5 (torch.Size([128])), Block block_0 (torch.Size([128])).
I0316 12:47:07.523262 139830621443264 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 3.block_0 ([torch.Size([256, 256])]) for Parameter 3 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 12:47:07.523295 139794211816640 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 1.block_0 ([torch.Size([0])]) for Parameter 1 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 12:47:07.523310 140554023089344 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 7.block_0 ([torch.Size([0])]) for Parameter 7 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 12:47:07.523295 140633389098176 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 2.block_0 ([torch.Size([256, 256]), torch.Size([512, 512])]) for Parameter 2 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 12:47:07.523370 140314632471744 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([512, 13])), Block block_0 (torch.Size([512, 13])).
I0316 12:47:07.523392 139849128674496 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 6.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 6 (torch.Size([1024, 506])), Block block_0 (torch.Size([1024, 506])).
I0316 12:47:07.523405 139830621443264 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 4.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 4 (torch.Size([128, 256])), Block block_0 (torch.Size([128, 256])).
I0316 12:47:07.523426 140554023089344 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 8.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 8 (torch.Size([1024, 1024])), Block block_0 (torch.Size([1024, 1024])).
I0316 12:47:07.523434 140314632471744 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 12:47:07.523443 139794211816640 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 2.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 2 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 12:47:07.523457 140633389098176 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 3.block_0 ([torch.Size([0])]) for Parameter 3 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 12:47:07.523490 140314632471744 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 12:47:07.523499 139849128674496 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 7.block_0 ([torch.Size([0])]) for Parameter 7 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 12:47:07.523527 140554023089344 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 9.block_0 ([torch.Size([0])]) for Parameter 9 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 12:47:07.523553 139794211816640 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 3.block_0 ([torch.Size([0])]) for Parameter 3 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 12:47:07.523560 140314632471744 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 12:47:07.523557 139830621443264 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 5.block_0 ([torch.Size([128, 128])]) for Parameter 5 (torch.Size([128])), Block block_0 (torch.Size([128])).
I0316 12:47:07.523601 140633389098176 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 4.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 4 (torch.Size([128, 256])), Block block_0 (torch.Size([128, 256])).
I0316 12:47:07.523611 140314632471744 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 4.block_0 (torch.Size([0])) for Parameter 4 (torch.Size([128, 256])), Block block_0 (torch.Size([128, 256])).
I0316 12:47:07.523623 139849128674496 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 8.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 8 (torch.Size([1024, 1024])), Block block_0 (torch.Size([1024, 1024])).
I0316 12:47:07.523658 140314632471744 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 5.block_0 (torch.Size([0])) for Parameter 5 (torch.Size([128])), Block block_0 (torch.Size([128])).
I0316 12:47:07.523660 140554023089344 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 10.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 10 (torch.Size([512, 1024])), Block block_0 (torch.Size([512, 1024])).
I0316 12:47:07.523697 139830621443264 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 6.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 6 (torch.Size([1024, 506])), Block block_0 (torch.Size([1024, 506])).
I0316 12:47:07.523652 140293290714304 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 0.block_0 ([torch.Size([512, 512]), torch.Size([13, 13])]) for Parameter 0 (torch.Size([512, 13])), Block block_0 (torch.Size([512, 13])).
I0316 12:47:07.523709 139849128674496 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 9.block_0 ([torch.Size([0])]) for Parameter 9 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 12:47:07.523712 140314632471744 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 6.block_0 (torch.Size([0])) for Parameter 6 (torch.Size([1024, 506])), Block block_0 (torch.Size([1024, 506])).
I0316 12:47:07.523705 140633389098176 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 5.block_0 ([torch.Size([0])]) for Parameter 5 (torch.Size([128])), Block block_0 (torch.Size([128])).
I0316 12:47:07.523745 140554023089344 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 11.block_0 ([torch.Size([0])]) for Parameter 11 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 12:47:07.523765 140314632471744 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 7.block_0 (torch.Size([0])) for Parameter 7 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 12:47:07.523789 139794211816640 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 4.block_0 ([torch.Size([128, 128]), torch.Size([256, 256])]) for Parameter 4 (torch.Size([128, 256])), Block block_0 (torch.Size([128, 256])).
I0316 12:47:07.523802 139956878537920 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 12.block_0 ([torch.Size([256, 256]), torch.Size([512, 512])]) for Parameter 12 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 12:47:07.523819 140293290714304 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 1.block_0 ([torch.Size([0])]) for Parameter 1 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 12:47:07.523835 140633389098176 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 6.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 6 (torch.Size([1024, 506])), Block block_0 (torch.Size([1024, 506])).
I0316 12:47:07.523860 140554023089344 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 12.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 12 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 12:47:07.523859 140314632471744 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 8.block_0 (torch.Size([1024, 1024])) for Parameter 8 (torch.Size([1024, 1024])), Block block_0 (torch.Size([1024, 1024])).
I0316 12:47:07.523861 139830621443264 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 7.block_0 ([torch.Size([1024, 1024])]) for Parameter 7 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 12:47:07.523921 140314632471744 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 9.block_0 (torch.Size([0])) for Parameter 9 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 12:47:07.523917 139956878537920 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 13.block_0 ([torch.Size([0])]) for Parameter 13 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 12:47:07.523920 139794211816640 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 5.block_0 ([torch.Size([0])]) for Parameter 5 (torch.Size([128])), Block block_0 (torch.Size([128])).
I0316 12:47:07.523927 140633389098176 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 7.block_0 ([torch.Size([0])]) for Parameter 7 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 12:47:07.523945 140554023089344 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 13.block_0 ([torch.Size([0])]) for Parameter 13 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 12:47:07.523961 140293290714304 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 2.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 2 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 12:47:07.523981 140314632471744 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 10.block_0 (torch.Size([0])) for Parameter 10 (torch.Size([512, 1024])), Block block_0 (torch.Size([512, 1024])).
I0316 12:47:07.523989 139830621443264 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 8.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 8 (torch.Size([1024, 1024])), Block block_0 (torch.Size([1024, 1024])).
I0316 12:47:07.524008 139956878537920 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 14.block_0 ([torch.Size([0])]) for Parameter 14 (torch.Size([1, 256])), Block block_0 (torch.Size([256])).
I0316 12:47:07.524029 140314632471744 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 11.block_0 (torch.Size([0])) for Parameter 11 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 12:47:07.524035 140554023089344 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 14.block_0 ([torch.Size([0])]) for Parameter 14 (torch.Size([1, 256])), Block block_0 (torch.Size([256])).
I0316 12:47:07.524039 140633389098176 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 8.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 8 (torch.Size([1024, 1024])), Block block_0 (torch.Size([1024, 1024])).
I0316 12:47:07.524044 139794211816640 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 6.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 6 (torch.Size([1024, 506])), Block block_0 (torch.Size([1024, 506])).
I0316 12:47:07.524082 140314632471744 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 12.block_0 (torch.Size([0])) for Parameter 12 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 12:47:07.524083 140293290714304 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 3.block_0 ([torch.Size([0])]) for Parameter 3 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 12:47:07.524090 139956878537920 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 15.block_0 ([torch.Size([0])]) for Parameter 15 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0316 12:47:07.524116 140554023089344 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 15.block_0 ([torch.Size([0])]) for Parameter 15 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0316 12:47:07.524130 140633389098176 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 9.block_0 ([torch.Size([0])]) for Parameter 9 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 12:47:07.524134 140314632471744 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 13.block_0 (torch.Size([0])) for Parameter 13 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 12:47:07.524133 139794211816640 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 7.block_0 ([torch.Size([0])]) for Parameter 7 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 12:47:07.524178 139956878537920 shampoo_preconditioner_list.py:612] Rank 4: ShampooPreconditionerList Numel Breakdown: (524626, 524288, 655360, 131072, 163840, 32768, 2609224, 2097152, 4194304, 2097152, 2621440, 524288, 655360, 131072, 131072, 2)
I0316 12:47:07.524181 140314632471744 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 14.block_0 (torch.Size([0])) for Parameter 14 (torch.Size([1, 256])), Block block_0 (torch.Size([256])).
I0316 12:47:07.524210 140554023089344 shampoo_preconditioner_list.py:612] Rank 2: ShampooPreconditionerList Numel Breakdown: (524626, 524288, 655360, 131072, 163840, 32768, 2609224, 2097152, 4194304, 2097152, 2621440, 524288, 655360, 131072, 131072, 2)
I0316 12:47:07.524210 140293290714304 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 4.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 4 (torch.Size([128, 256])), Block block_0 (torch.Size([128, 256])).
I0316 12:47:07.524225 139956878537920 shampoo_preconditioner_list.py:615] Rank 4: ShampooPreconditionerList Bytes Breakdown: (2098504,)
I0316 12:47:07.524227 140314632471744 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 15.block_0 (torch.Size([0])) for Parameter 15 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0316 12:47:07.524239 140633389098176 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 10.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 10 (torch.Size([512, 1024])), Block block_0 (torch.Size([512, 1024])).
I0316 12:47:07.524252 140554023089344 shampoo_preconditioner_list.py:615] Rank 2: ShampooPreconditionerList Bytes Breakdown: (2098504,)
I0316 12:47:07.524257 139956878537920 shampoo_preconditioner_list.py:618] Rank 4: ShampooPreconditionerList Total Elements: 17093020
I0316 12:47:07.524248 139794211816640 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 8.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 8 (torch.Size([1024, 1024])), Block block_0 (torch.Size([1024, 1024])).
I0316 12:47:07.524283 140554023089344 shampoo_preconditioner_list.py:618] Rank 2: ShampooPreconditionerList Total Elements: 17093020
I0316 12:47:07.524286 139956878537920 shampoo_preconditioner_list.py:621] Rank 4: ShampooPreconditionerList Total Bytes: 2098504
I0316 12:47:07.524259 139849128674496 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 10.block_0 ([torch.Size([512, 512]), torch.Size([1024, 1024])]) for Parameter 10 (torch.Size([512, 1024])), Block block_0 (torch.Size([512, 1024])).
I0316 12:47:07.524285 140314632471744 shampoo_preconditioner_list.py:375] Rank 0: AdaGradPreconditionerList Numel Breakdown: (0, 0, 0, 0, 0, 0, 0, 0, 1048576, 0, 0, 0, 0, 0, 0, 0)
I0316 12:47:07.524315 140554023089344 shampoo_preconditioner_list.py:621] Rank 2: ShampooPreconditionerList Total Bytes: 2098504
I0316 12:47:07.524317 140314632471744 shampoo_preconditioner_list.py:378] Rank 0: AdaGradPreconditionerList Bytes Breakdown: (0, 0, 0, 0, 0, 0, 0, 0, 4194304, 0, 0, 0, 0, 0, 0, 0)
I0316 12:47:07.524314 140293290714304 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 5.block_0 ([torch.Size([0])]) for Parameter 5 (torch.Size([128])), Block block_0 (torch.Size([128])).
I0316 12:47:07.524345 140314632471744 shampoo_preconditioner_list.py:381] Rank 0: AdaGradPreconditionerList Total Elements: 1048576
I0316 12:47:07.524341 140633389098176 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 11.block_0 ([torch.Size([0])]) for Parameter 11 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 12:47:07.524357 139794211816640 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 9.block_0 ([torch.Size([0])]) for Parameter 9 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 12:47:07.524379 140314632471744 shampoo_preconditioner_list.py:384] Rank 0: AdaGradPreconditionerList Total Bytes: 4194304
I0316 12:47:07.524371 139849128674496 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 11.block_0 ([torch.Size([0])]) for Parameter 11 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 12:47:07.524414 139956878537920 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([512, 13])), Block block_0 (torch.Size([512, 13])).
I0316 12:47:07.524446 140554023089344 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([512, 13])), Block block_0 (torch.Size([512, 13])).
I0316 12:47:07.524451 140633389098176 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 12.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 12 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 12:47:07.524449 140293290714304 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 6.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 6 (torch.Size([1024, 506])), Block block_0 (torch.Size([1024, 506])).
I0316 12:47:07.524456 139830621443264 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 9.block_0 ([torch.Size([1024, 1024])]) for Parameter 9 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 12:47:07.524481 139956878537920 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 12:47:07.524496 139849128674496 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 12.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 12 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 12:47:07.524495 139794211816640 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 10.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 10 (torch.Size([512, 1024])), Block block_0 (torch.Size([512, 1024])).
I0316 12:47:07.524513 140554023089344 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 12:47:07.524530 140633389098176 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 13.block_0 ([torch.Size([0])]) for Parameter 13 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 12:47:07.524531 139956878537920 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 12:47:07.524549 140293290714304 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 7.block_0 ([torch.Size([0])]) for Parameter 7 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 12:47:07.524572 140554023089344 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 12:47:07.524585 139794211816640 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 11.block_0 ([torch.Size([0])]) for Parameter 11 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 12:47:07.524597 139956878537920 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 12:47:07.524609 139849128674496 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 13.block_0 ([torch.Size([0])]) for Parameter 13 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 12:47:07.524627 139830621443264 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 10.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 10 (torch.Size([512, 1024])), Block block_0 (torch.Size([512, 1024])).
I0316 12:47:07.524634 140633389098176 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 14.block_0 ([torch.Size([0])]) for Parameter 14 (torch.Size([1, 256])), Block block_0 (torch.Size([256])).
I0316 12:47:07.524637 140554023089344 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 12:47:07.524646 139956878537920 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 4.block_0 (torch.Size([0])) for Parameter 4 (torch.Size([128, 256])), Block block_0 (torch.Size([128, 256])).
I0316 12:47:07.524659 140293290714304 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 8.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 8 (torch.Size([1024, 1024])), Block block_0 (torch.Size([1024, 1024])).
I0316 12:47:07.524686 140554023089344 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 4.block_0 (torch.Size([0])) for Parameter 4 (torch.Size([128, 256])), Block block_0 (torch.Size([128, 256])).
I0316 12:47:07.524692 139956878537920 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 5.block_0 (torch.Size([0])) for Parameter 5 (torch.Size([128])), Block block_0 (torch.Size([128])).
I0316 12:47:07.524698 139849128674496 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 14.block_0 ([torch.Size([0])]) for Parameter 14 (torch.Size([1, 256])), Block block_0 (torch.Size([256])).
I0316 12:47:07.524698 139794211816640 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 12.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 12 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 12:47:07.524712 140633389098176 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 15.block_0 ([torch.Size([0])]) for Parameter 15 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0316 12:47:07.524732 140554023089344 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 5.block_0 (torch.Size([0])) for Parameter 5 (torch.Size([128])), Block block_0 (torch.Size([128])).
I0316 12:47:07.524716 140314632471744 submission.py:105] Large parameters (embedding tables) detected (dim >= 1_000_000). Instantiating Row-Wise AdaGrad...
I0316 12:47:07.524739 139956878537920 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 6.block_0 (torch.Size([0])) for Parameter 6 (torch.Size([1024, 506])), Block block_0 (torch.Size([1024, 506])).
I0316 12:47:07.524766 140293290714304 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 9.block_0 ([torch.Size([0])]) for Parameter 9 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 12:47:07.524782 139794211816640 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 13.block_0 ([torch.Size([0])]) for Parameter 13 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 12:47:07.524791 139956878537920 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 7.block_0 (torch.Size([0])) for Parameter 7 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 12:47:07.524788 139849128674496 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 15.block_0 ([torch.Size([0])]) for Parameter 15 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0316 12:47:07.524797 140633389098176 shampoo_preconditioner_list.py:612] Rank 3: ShampooPreconditionerList Numel Breakdown: (524626, 524288, 655360, 131072, 163840, 32768, 2609224, 2097152, 4194304, 2097152, 2621440, 524288, 655360, 131072, 131072, 2)
W0316 12:47:07.524802 140314632471744 distributed_shampoo.py:359] Nesterov flag is enabled but momentum parameter is zero! Continuing without using momentum or Nesterov acceleration...
I0316 12:47:07.524824 140554023089344 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 6.block_0 (torch.Size([1024, 506])) for Parameter 6 (torch.Size([1024, 506])), Block block_0 (torch.Size([1024, 506])).
I0316 12:47:07.524841 140633389098176 shampoo_preconditioner_list.py:615] Rank 3: ShampooPreconditionerList Bytes Breakdown: (2098504,)
I0316 12:47:07.524839 139956878537920 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 8.block_0 (torch.Size([0])) for Parameter 8 (torch.Size([1024, 1024])), Block block_0 (torch.Size([1024, 1024])).
I0316 12:47:07.524872 140633389098176 shampoo_preconditioner_list.py:618] Rank 3: ShampooPreconditionerList Total Elements: 17093020
I0316 12:47:07.524866 139794211816640 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 14.block_0 ([torch.Size([0])]) for Parameter 14 (torch.Size([1, 256])), Block block_0 (torch.Size([256])).
I0316 12:47:07.524876 139849128674496 shampoo_preconditioner_list.py:612] Rank 1: ShampooPreconditionerList Numel Breakdown: (524626, 524288, 655360, 131072, 163840, 32768, 2609224, 2097152, 4194304, 2097152, 2621440, 524288, 655360, 131072, 131072, 2)
I0316 12:47:07.524886 139956878537920 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 9.block_0 (torch.Size([0])) for Parameter 9 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 12:47:07.524892 140293290714304 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 10.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 10 (torch.Size([512, 1024])), Block block_0 (torch.Size([512, 1024])).
I0316 12:47:07.524904 140633389098176 shampoo_preconditioner_list.py:621] Rank 3: ShampooPreconditionerList Total Bytes: 2098504
I0316 12:47:07.524901 140554023089344 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 7.block_0 (torch.Size([0])) for Parameter 7 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 12:47:07.524913 139849128674496 shampoo_preconditioner_list.py:615] Rank 1: ShampooPreconditionerList Bytes Breakdown: (2098504,)
I0316 12:47:07.524931 139956878537920 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 10.block_0 (torch.Size([0])) for Parameter 10 (torch.Size([512, 1024])), Block block_0 (torch.Size([512, 1024])).
I0316 12:47:07.524943 139849128674496 shampoo_preconditioner_list.py:618] Rank 1: ShampooPreconditionerList Total Elements: 17093020
I0316 12:47:07.524948 139794211816640 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 15.block_0 ([torch.Size([0])]) for Parameter 15 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0316 12:47:07.524951 140554023089344 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 8.block_0 (torch.Size([0])) for Parameter 8 (torch.Size([1024, 1024])), Block block_0 (torch.Size([1024, 1024])).
I0316 12:47:07.524972 139849128674496 shampoo_preconditioner_list.py:621] Rank 1: ShampooPreconditionerList Total Bytes: 2098504
I0316 12:47:07.524981 140293290714304 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 11.block_0 ([torch.Size([0])]) for Parameter 11 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 12:47:07.524985 139956878537920 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 11.block_0 (torch.Size([0])) for Parameter 11 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 12:47:07.524999 140554023089344 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 9.block_0 (torch.Size([0])) for Parameter 9 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 12:47:07.525033 139794211816640 shampoo_preconditioner_list.py:612] Rank 5: ShampooPreconditionerList Numel Breakdown: (524626, 524288, 655360, 131072, 163840, 32768, 2609224, 2097152, 4194304, 2097152, 2621440, 524288, 655360, 131072, 131072, 2)
I0316 12:47:07.525033 140633389098176 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([512, 13])), Block block_0 (torch.Size([512, 13])).
I0316 12:47:07.525054 140554023089344 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 10.block_0 (torch.Size([0])) for Parameter 10 (torch.Size([512, 1024])), Block block_0 (torch.Size([512, 1024])).
I0316 12:47:07.525080 139794211816640 shampoo_preconditioner_list.py:615] Rank 5: ShampooPreconditionerList Bytes Breakdown: (2098504,)
I0316 12:47:07.525090 140633389098176 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 12:47:07.525090 140293290714304 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 12.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 12 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 12:47:07.525100 140554023089344 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 11.block_0 (torch.Size([0])) for Parameter 11 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 12:47:07.525119 139794211816640 shampoo_preconditioner_list.py:618] Rank 5: ShampooPreconditionerList Total Elements: 17093020
I0316 12:47:07.525111 139849128674496 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([512, 13])), Block block_0 (torch.Size([512, 13])).
I0316 12:47:07.525145 140554023089344 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 12.block_0 (torch.Size([0])) for Parameter 12 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 12:47:07.525155 139794211816640 shampoo_preconditioner_list.py:621] Rank 5: ShampooPreconditionerList Total Bytes: 2098504
I0316 12:47:07.525130 139830621443264 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 11.block_0 ([torch.Size([512, 512])]) for Parameter 11 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 12:47:07.525169 139849128674496 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 12:47:07.525173 140293290714304 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 13.block_0 ([torch.Size([0])]) for Parameter 13 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 12:47:07.525196 140554023089344 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 13.block_0 (torch.Size([0])) for Parameter 13 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 12:47:07.525217 139849128674496 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 12:47:07.525249 140554023089344 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 14.block_0 (torch.Size([0])) for Parameter 14 (torch.Size([1, 256])), Block block_0 (torch.Size([256])).
I0316 12:47:07.525267 140293290714304 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 14.block_0 ([torch.Size([0])]) for Parameter 14 (torch.Size([1, 256])), Block block_0 (torch.Size([256])).
I0316 12:47:07.525287 139849128674496 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 12:47:07.525285 139794211816640 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([512, 13])), Block block_0 (torch.Size([512, 13])).
I0316 12:47:07.525284 139830621443264 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 12.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 12 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 12:47:07.525302 140554023089344 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 15.block_0 (torch.Size([0])) for Parameter 15 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0316 12:47:07.525343 139794211816640 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 12:47:07.525357 139849128674496 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 4.block_0 (torch.Size([0])) for Parameter 4 (torch.Size([128, 256])), Block block_0 (torch.Size([128, 256])).
I0316 12:47:07.525357 140293290714304 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 15.block_0 ([torch.Size([0])]) for Parameter 15 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0316 12:47:07.525380 140554023089344 shampoo_preconditioner_list.py:375] Rank 2: AdaGradPreconditionerList Numel Breakdown: (0, 0, 0, 0, 0, 0, 518144, 0, 0, 0, 0, 0, 0, 0, 0, 0)
I0316 12:47:07.525402 139794211816640 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 12:47:07.525408 139849128674496 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 5.block_0 (torch.Size([0])) for Parameter 5 (torch.Size([128])), Block block_0 (torch.Size([128])).
I0316 12:47:07.525420 140554023089344 shampoo_preconditioner_list.py:378] Rank 2: AdaGradPreconditionerList Bytes Breakdown: (0, 0, 0, 0, 0, 0, 2072576, 0, 0, 0, 0, 0, 0, 0, 0, 0)
I0316 12:47:07.525452 140554023089344 shampoo_preconditioner_list.py:381] Rank 2: AdaGradPreconditionerList Total Elements: 518144
I0316 12:47:07.525452 140293290714304 shampoo_preconditioner_list.py:612] Rank 6: ShampooPreconditionerList Numel Breakdown: (524626, 524288, 655360, 131072, 163840, 32768, 2609224, 2097152, 4194304, 2097152, 2621440, 524288, 655360, 131072, 131072, 2)
I0316 12:47:07.525461 139849128674496 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 6.block_0 (torch.Size([0])) for Parameter 6 (torch.Size([1024, 506])), Block block_0 (torch.Size([1024, 506])).
I0316 12:47:07.525468 139794211816640 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 12:47:07.525487 140554023089344 shampoo_preconditioner_list.py:384] Rank 2: AdaGradPreconditionerList Total Bytes: 2072576
I0316 12:47:07.525490 140293290714304 shampoo_preconditioner_list.py:615] Rank 6: ShampooPreconditionerList Bytes Breakdown: (2098504,)
I0316 12:47:07.525521 140293290714304 shampoo_preconditioner_list.py:618] Rank 6: ShampooPreconditionerList Total Elements: 17093020
I0316 12:47:07.525520 139849128674496 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 7.block_0 (torch.Size([0])) for Parameter 7 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 12:47:07.525558 140293290714304 shampoo_preconditioner_list.py:621] Rank 6: ShampooPreconditionerList Total Bytes: 2098504
I0316 12:47:07.525562 139794211816640 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 4.block_0 (torch.Size([128, 256])) for Parameter 4 (torch.Size([128, 256])), Block block_0 (torch.Size([128, 256])).
I0316 12:47:07.525582 139849128674496 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 8.block_0 (torch.Size([0])) for Parameter 8 (torch.Size([1024, 1024])), Block block_0 (torch.Size([1024, 1024])).
I0316 12:47:07.525571 139956878537920 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 12.block_0 (torch.Size([256, 512])) for Parameter 12 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 12:47:07.525623 139794211816640 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 5.block_0 (torch.Size([0])) for Parameter 5 (torch.Size([128])), Block block_0 (torch.Size([128])).
I0316 12:47:07.525643 139849128674496 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 9.block_0 (torch.Size([0])) for Parameter 9 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 12:47:07.525671 139956878537920 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 13.block_0 (torch.Size([0])) for Parameter 13 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 12:47:07.525683 139794211816640 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 6.block_0 (torch.Size([0])) for Parameter 6 (torch.Size([1024, 506])), Block block_0 (torch.Size([1024, 506])).
I0316 12:47:07.525701 140633389098176 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([256, 512])) for Parameter 2 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 12:47:07.525739 139794211816640 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 7.block_0 (torch.Size([0])) for Parameter 7 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 12:47:07.525740 139956878537920 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 14.block_0 (torch.Size([0])) for Parameter 14 (torch.Size([1, 256])), Block block_0 (torch.Size([256])).
I0316 12:47:07.525743 139849128674496 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 10.block_0 (torch.Size([512, 1024])) for Parameter 10 (torch.Size([512, 1024])), Block block_0 (torch.Size([512, 1024])).
I0316 12:47:07.525744 140293290714304 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([512, 13])) for Parameter 0 (torch.Size([512, 13])), Block block_0 (torch.Size([512, 13])).
I0316 12:47:07.525788 139794211816640 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 8.block_0 (torch.Size([0])) for Parameter 8 (torch.Size([1024, 1024])), Block block_0 (torch.Size([1024, 1024])).
I0316 12:47:07.525794 139956878537920 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 15.block_0 (torch.Size([0])) for Parameter 15 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0316 12:47:07.525810 139849128674496 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 11.block_0 (torch.Size([0])) for Parameter 11 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 12:47:07.525825 140633389098176 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 12:47:07.525839 140293290714304 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 12:47:07.525845 139794211816640 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 9.block_0 (torch.Size([0])) for Parameter 9 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 12:47:07.525860 139956878537920 shampoo_preconditioner_list.py:375] Rank 4: AdaGradPreconditionerList Numel Breakdown: (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 131072, 0, 0, 0)
I0316 12:47:07.525863 139849128674496 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 12.block_0 (torch.Size([0])) for Parameter 12 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 12:47:07.525892 139794211816640 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 10.block_0 (torch.Size([0])) for Parameter 10 (torch.Size([512, 1024])), Block block_0 (torch.Size([512, 1024])).
W0316 12:47:07.525871 140554023089344 distributed_shampoo.py:359] Nesterov flag is enabled but momentum parameter is zero! Continuing without using momentum or Nesterov acceleration...
I0316 12:47:07.525893 140293290714304 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 12:47:07.525894 140633389098176 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 4.block_0 (torch.Size([0])) for Parameter 4 (torch.Size([128, 256])), Block block_0 (torch.Size([128, 256])).
I0316 12:47:07.525903 139956878537920 shampoo_preconditioner_list.py:378] Rank 4: AdaGradPreconditionerList Bytes Breakdown: (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 524288, 0, 0, 0)
I0316 12:47:07.525913 139849128674496 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 13.block_0 (torch.Size([0])) for Parameter 13 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 12:47:07.525935 139956878537920 shampoo_preconditioner_list.py:381] Rank 4: AdaGradPreconditionerList Total Elements: 131072
I0316 12:47:07.525942 140293290714304 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 12:47:07.525948 139794211816640 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 11.block_0 (torch.Size([0])) for Parameter 11 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 12:47:07.525950 140633389098176 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 5.block_0 (torch.Size([0])) for Parameter 5 (torch.Size([128])), Block block_0 (torch.Size([128])).
I0316 12:47:07.525964 139849128674496 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 14.block_0 (torch.Size([0])) for Parameter 14 (torch.Size([1, 256])), Block block_0 (torch.Size([256])).
I0316 12:47:07.525971 139956878537920 shampoo_preconditioner_list.py:384] Rank 4: AdaGradPreconditionerList Total Bytes: 524288
I0316 12:47:07.525990 140293290714304 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 4.block_0 (torch.Size([0])) for Parameter 4 (torch.Size([128, 256])), Block block_0 (torch.Size([128, 256])).
I0316 12:47:07.525994 139794211816640 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 12.block_0 (torch.Size([0])) for Parameter 12 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 12:47:07.526004 140633389098176 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 6.block_0 (torch.Size([0])) for Parameter 6 (torch.Size([1024, 506])), Block block_0 (torch.Size([1024, 506])).
I0316 12:47:07.526024 139849128674496 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 15.block_0 (torch.Size([0])) for Parameter 15 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0316 12:47:07.526058 139794211816640 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 13.block_0 (torch.Size([0])) for Parameter 13 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 12:47:07.526067 140293290714304 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 5.block_0 (torch.Size([0])) for Parameter 5 (torch.Size([128])), Block block_0 (torch.Size([128])).
I0316 12:47:07.526077 140633389098176 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 7.block_0 (torch.Size([0])) for Parameter 7 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 12:47:07.526087 139849128674496 shampoo_preconditioner_list.py:375] Rank 1: AdaGradPreconditionerList Numel Breakdown: (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 524288, 0, 0, 0, 0, 0)
I0316 12:47:07.526115 139794211816640 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 14.block_0 (torch.Size([0])) for Parameter 14 (torch.Size([1, 256])), Block block_0 (torch.Size([256])).
I0316 12:47:07.526123 139849128674496 shampoo_preconditioner_list.py:378] Rank 1: AdaGradPreconditionerList Bytes Breakdown: (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2097152, 0, 0, 0, 0, 0)
I0316 12:47:07.526121 140293290714304 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 6.block_0 (torch.Size([0])) for Parameter 6 (torch.Size([1024, 506])), Block block_0 (torch.Size([1024, 506])).
I0316 12:47:07.526132 140633389098176 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 8.block_0 (torch.Size([0])) for Parameter 8 (torch.Size([1024, 1024])), Block block_0 (torch.Size([1024, 1024])).
I0316 12:47:07.526115 139830621443264 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 13.block_0 ([torch.Size([256, 256])]) for Parameter 13 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 12:47:07.526154 139849128674496 shampoo_preconditioner_list.py:381] Rank 1: AdaGradPreconditionerList Total Elements: 524288
I0316 12:47:07.526174 139794211816640 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 15.block_0 (torch.Size([0])) for Parameter 15 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0316 12:47:07.526180 140293290714304 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 7.block_0 (torch.Size([0])) for Parameter 7 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 12:47:07.526185 140633389098176 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 9.block_0 (torch.Size([0])) for Parameter 9 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 12:47:07.526191 139849128674496 shampoo_preconditioner_list.py:384] Rank 1: AdaGradPreconditionerList Total Bytes: 2097152
I0316 12:47:07.526232 140293290714304 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 8.block_0 (torch.Size([0])) for Parameter 8 (torch.Size([1024, 1024])), Block block_0 (torch.Size([1024, 1024])).
I0316 12:47:07.526237 140633389098176 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 10.block_0 (torch.Size([0])) for Parameter 10 (torch.Size([512, 1024])), Block block_0 (torch.Size([512, 1024])).
I0316 12:47:07.526242 139794211816640 shampoo_preconditioner_list.py:375] Rank 5: AdaGradPreconditionerList Numel Breakdown: (0, 0, 0, 0, 32768, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)
I0316 12:47:07.526279 139794211816640 shampoo_preconditioner_list.py:378] Rank 5: AdaGradPreconditionerList Bytes Breakdown: (0, 0, 0, 0, 131072, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)
I0316 12:47:07.526287 140633389098176 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 11.block_0 (torch.Size([0])) for Parameter 11 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 12:47:07.526280 139830621443264 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 14.block_0 ([torch.Size([256, 256])]) for Parameter 14 (torch.Size([1, 256])), Block block_0 (torch.Size([256])).
I0316 12:47:07.526293 140293290714304 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 9.block_0 (torch.Size([0])) for Parameter 9 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 12:47:07.526317 139794211816640 shampoo_preconditioner_list.py:381] Rank 5: AdaGradPreconditionerList Total Elements: 32768
I0316 12:47:07.526338 140633389098176 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 12.block_0 (torch.Size([0])) for Parameter 12 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 12:47:07.526355 139794211816640 shampoo_preconditioner_list.py:384] Rank 5: AdaGradPreconditionerList Total Bytes: 131072
I0316 12:47:07.526355 140293290714304 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 10.block_0 (torch.Size([0])) for Parameter 10 (torch.Size([512, 1024])), Block block_0 (torch.Size([512, 1024])).
W0316 12:47:07.526350 139956878537920 distributed_shampoo.py:359] Nesterov flag is enabled but momentum parameter is zero! Continuing without using momentum or Nesterov acceleration...
I0316 12:47:07.526396 140633389098176 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 13.block_0 (torch.Size([0])) for Parameter 13 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 12:47:07.526425 140293290714304 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 11.block_0 (torch.Size([0])) for Parameter 11 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 12:47:07.526428 139830621443264 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 15.block_0 ([torch.Size([1, 1])]) for Parameter 15 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0316 12:47:07.526465 140633389098176 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 14.block_0 (torch.Size([0])) for Parameter 14 (torch.Size([1, 256])), Block block_0 (torch.Size([256])).
I0316 12:47:07.526475 140293290714304 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 12.block_0 (torch.Size([0])) for Parameter 12 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 12:47:07.526520 140293290714304 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 13.block_0 (torch.Size([0])) for Parameter 13 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 12:47:07.526514 140633389098176 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 15.block_0 (torch.Size([0])) for Parameter 15 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0316 12:47:07.526574 140293290714304 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 14.block_0 (torch.Size([0])) for Parameter 14 (torch.Size([1, 256])), Block block_0 (torch.Size([256])).
I0316 12:47:07.526592 140633389098176 shampoo_preconditioner_list.py:375] Rank 3: AdaGradPreconditionerList Numel Breakdown: (0, 0, 131072, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)
I0316 12:47:07.526593 139830621443264 shampoo_preconditioner_list.py:612] Rank 7: ShampooPreconditionerList Numel Breakdown: (524626, 524288, 655360, 131072, 163840, 32768, 2609224, 2097152, 4194304, 2097152, 2621440, 524288, 655360, 131072, 131072, 2)
W0316 12:47:07.526596 139849128674496 distributed_shampoo.py:359] Nesterov flag is enabled but momentum parameter is zero! Continuing without using momentum or Nesterov acceleration...
I0316 12:47:07.526624 140633389098176 shampoo_preconditioner_list.py:378] Rank 3: AdaGradPreconditionerList Bytes Breakdown: (0, 0, 524288, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)
I0316 12:47:07.526628 140293290714304 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 15.block_0 (torch.Size([0])) for Parameter 15 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0316 12:47:07.526634 139830621443264 shampoo_preconditioner_list.py:615] Rank 7: ShampooPreconditionerList Bytes Breakdown: (2098504, 2097152, 2621440, 524288, 655360, 131072, 10436896, 8388608, 16777216)
I0316 12:47:07.526652 140633389098176 shampoo_preconditioner_list.py:381] Rank 3: AdaGradPreconditionerList Total Elements: 131072
I0316 12:47:07.526665 139830621443264 shampoo_preconditioner_list.py:618] Rank 7: ShampooPreconditionerList Total Elements: 17093020
I0316 12:47:07.526686 140633389098176 shampoo_preconditioner_list.py:384] Rank 3: AdaGradPreconditionerList Total Bytes: 524288
I0316 12:47:07.526688 140293290714304 shampoo_preconditioner_list.py:375] Rank 6: AdaGradPreconditionerList Numel Breakdown: (6656, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)
I0316 12:47:07.526693 139830621443264 shampoo_preconditioner_list.py:621] Rank 7: ShampooPreconditionerList Total Bytes: 43730536
I0316 12:47:07.526720 140293290714304 shampoo_preconditioner_list.py:378] Rank 6: AdaGradPreconditionerList Bytes Breakdown: (26624, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)
I0316 12:47:07.526756 140293290714304 shampoo_preconditioner_list.py:381] Rank 6: AdaGradPreconditionerList Total Elements: 6656
I0316 12:47:07.526790 140293290714304 shampoo_preconditioner_list.py:384] Rank 6: AdaGradPreconditionerList Total Bytes: 26624
I0316 12:47:07.526832 139830621443264 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([512, 13])), Block block_0 (torch.Size([512, 13])).
I0316 12:47:07.526956 139830621443264 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([512])) for Parameter 1 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 12:47:07.527033 139830621443264 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
W0316 12:47:07.527065 140633389098176 distributed_shampoo.py:359] Nesterov flag is enabled but momentum parameter is zero! Continuing without using momentum or Nesterov acceleration...
I0316 12:47:07.527117 139830621443264 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([256])) for Parameter 3 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 12:47:07.527184 139830621443264 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 4.block_0 (torch.Size([0])) for Parameter 4 (torch.Size([128, 256])), Block block_0 (torch.Size([128, 256])).
W0316 12:47:07.527190 140293290714304 distributed_shampoo.py:359] Nesterov flag is enabled but momentum parameter is zero! Continuing without using momentum or Nesterov acceleration...
I0316 12:47:07.527263 139830621443264 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 5.block_0 (torch.Size([128])) for Parameter 5 (torch.Size([128])), Block block_0 (torch.Size([128])).
I0316 12:47:07.527331 139830621443264 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 6.block_0 (torch.Size([0])) for Parameter 6 (torch.Size([1024, 506])), Block block_0 (torch.Size([1024, 506])).
I0316 12:47:07.527417 139830621443264 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 7.block_0 (torch.Size([1024])) for Parameter 7 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 12:47:07.527411 140554023089344 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 12:47:07.527430 140314632471744 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([524288, 128])) for Parameter 0 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 12:47:07.527485 139830621443264 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 8.block_0 (torch.Size([0])) for Parameter 8 (torch.Size([1024, 1024])), Block block_0 (torch.Size([1024, 1024])).
I0316 12:47:07.527513 140554023089344 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_1 (torch.Size([0])) for Parameter 0 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
W0316 12:47:07.527509 139794211816640 distributed_shampoo.py:359] Nesterov flag is enabled but momentum parameter is zero! Continuing without using momentum or Nesterov acceleration...
I0316 12:47:07.527559 140314632471744 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_1 (torch.Size([0])) for Parameter 0 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 12:47:07.527573 139830621443264 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 9.block_0 (torch.Size([1024])) for Parameter 9 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 12:47:07.527626 140314632471744 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 12:47:07.527641 139830621443264 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 10.block_0 (torch.Size([0])) for Parameter 10 (torch.Size([512, 1024])), Block block_0 (torch.Size([512, 1024])).
I0316 12:47:07.527687 140314632471744 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_1 (torch.Size([0])) for Parameter 1 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 12:47:07.527715 139830621443264 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 11.block_0 (torch.Size([512])) for Parameter 11 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 12:47:07.527744 140314632471744 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 12:47:07.527779 139830621443264 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 12.block_0 (torch.Size([0])) for Parameter 12 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 12:47:07.527798 140314632471744 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_1 (torch.Size([0])) for Parameter 2 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 12:47:07.527853 140314632471744 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 12:47:07.527855 139830621443264 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 13.block_0 (torch.Size([256])) for Parameter 13 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 12:47:07.527922 140314632471744 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_1 (torch.Size([0])) for Parameter 3 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 12:47:07.527923 139830621443264 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 14.block_0 (torch.Size([256])) for Parameter 14 (torch.Size([1, 256])), Block block_0 (torch.Size([256])).
I0316 12:47:07.527975 140314632471744 shampoo_preconditioner_list.py:375] Rank 0: AdaGradPreconditionerList Numel Breakdown: (67108864, 0, 0, 0, 0, 0, 0, 0)
I0316 12:47:07.527987 139830621443264 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 15.block_0 (torch.Size([1])) for Parameter 15 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0316 12:47:07.528012 140314632471744 shampoo_preconditioner_list.py:378] Rank 0: AdaGradPreconditionerList Bytes Breakdown: (268435456, 0, 0, 0, 0, 0, 0, 0)
I0316 12:47:07.528046 140314632471744 shampoo_preconditioner_list.py:381] Rank 0: AdaGradPreconditionerList Total Elements: 67108864
I0316 12:47:07.528048 139830621443264 shampoo_preconditioner_list.py:375] Rank 7: AdaGradPreconditionerList Numel Breakdown: (0, 512, 0, 256, 0, 128, 0, 1024, 0, 1024, 0, 512, 0, 256, 256, 1)
I0316 12:47:07.528078 140314632471744 shampoo_preconditioner_list.py:384] Rank 0: AdaGradPreconditionerList Total Bytes: 268435456
I0316 12:47:07.528080 139830621443264 shampoo_preconditioner_list.py:378] Rank 7: AdaGradPreconditionerList Bytes Breakdown: (0, 2048, 0, 1024, 0, 512, 0, 4096, 0, 4096, 0, 2048, 0, 1024, 1024, 4)
I0316 12:47:07.528119 139830621443264 shampoo_preconditioner_list.py:381] Rank 7: AdaGradPreconditionerList Total Elements: 3969
I0316 12:47:07.528151 139830621443264 shampoo_preconditioner_list.py:384] Rank 7: AdaGradPreconditionerList Total Bytes: 15876
I0316 12:47:07.528181 139956878537920 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 12:47:07.528276 139956878537920 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_1 (torch.Size([0])) for Parameter 0 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 12:47:07.528338 139956878537920 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 12:47:07.528358 139849128674496 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 12:47:07.528392 139956878537920 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_1 (torch.Size([0])) for Parameter 1 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
W0316 12:47:07.528588 139830621443264 distributed_shampoo.py:359] Nesterov flag is enabled but momentum parameter is zero! Continuing without using momentum or Nesterov acceleration...
I0316 12:47:07.528708 140554023089344 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([524288, 128])) for Parameter 1 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 12:47:07.528814 140554023089344 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_1 (torch.Size([0])) for Parameter 1 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 12:47:07.528824 140633389098176 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 12:47:07.528877 140554023089344 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 12:47:07.528921 140633389098176 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_1 (torch.Size([0])) for Parameter 0 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 12:47:07.528931 140554023089344 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_1 (torch.Size([0])) for Parameter 2 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 12:47:07.528978 140554023089344 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 12:47:07.528984 140633389098176 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 12:47:07.529029 140554023089344 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_1 (torch.Size([0])) for Parameter 3 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 12:47:07.529077 140554023089344 shampoo_preconditioner_list.py:375] Rank 2: AdaGradPreconditionerList Numel Breakdown: (0, 0, 67108864, 0, 0, 0, 0, 0)
I0316 12:47:07.529111 140554023089344 shampoo_preconditioner_list.py:378] Rank 2: AdaGradPreconditionerList Bytes Breakdown: (0, 0, 268435456, 0, 0, 0, 0, 0)
I0316 12:47:07.529141 140554023089344 shampoo_preconditioner_list.py:381] Rank 2: AdaGradPreconditionerList Total Elements: 67108864
I0316 12:47:07.529171 140554023089344 shampoo_preconditioner_list.py:384] Rank 2: AdaGradPreconditionerList Total Bytes: 268435456
I0316 12:47:07.529217 140293290714304 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 12:47:07.529319 140293290714304 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_1 (torch.Size([0])) for Parameter 0 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 12:47:07.529385 140293290714304 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 12:47:07.529452 140293290714304 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_1 (torch.Size([0])) for Parameter 1 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 12:47:07.529508 140293290714304 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 12:47:07.529561 140293290714304 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_1 (torch.Size([0])) for Parameter 2 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 12:47:07.529567 139794211816640 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 12:47:07.529601 139956878537920 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([524288, 128])) for Parameter 2 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 12:47:07.529668 139794211816640 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_1 (torch.Size([0])) for Parameter 0 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 12:47:07.529702 139956878537920 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_1 (torch.Size([0])) for Parameter 2 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 12:47:07.529733 139794211816640 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 12:47:07.529762 139956878537920 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 12:47:07.529766 139849128674496 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_1 (torch.Size([524288, 128])) for Parameter 0 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 12:47:07.529791 139794211816640 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_1 (torch.Size([0])) for Parameter 1 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 12:47:07.529815 139956878537920 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_1 (torch.Size([0])) for Parameter 3 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 12:47:07.529839 139794211816640 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 12:47:07.529818 140314632471744 submission_runner.py:279] Initializing metrics bundle.
I0316 12:47:07.529864 139956878537920 shampoo_preconditioner_list.py:375] Rank 4: AdaGradPreconditionerList Numel Breakdown: (0, 0, 0, 0, 67108864, 0, 0, 0)
I0316 12:47:07.529877 139849128674496 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 12:47:07.529895 139956878537920 shampoo_preconditioner_list.py:378] Rank 4: AdaGradPreconditionerList Bytes Breakdown: (0, 0, 0, 0, 268435456, 0, 0, 0)
I0316 12:47:07.529922 139956878537920 shampoo_preconditioner_list.py:381] Rank 4: AdaGradPreconditionerList Total Elements: 67108864
I0316 12:47:07.529939 139849128674496 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_1 (torch.Size([0])) for Parameter 1 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 12:47:07.529952 139956878537920 shampoo_preconditioner_list.py:384] Rank 4: AdaGradPreconditionerList Total Bytes: 268435456
I0316 12:47:07.529964 140314632471744 submission_runner.py:301] Initializing checkpoint and logger.
I0316 12:47:07.529994 139849128674496 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 12:47:07.530046 139849128674496 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_1 (torch.Size([0])) for Parameter 2 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 12:47:07.530098 139849128674496 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 12:47:07.530148 139849128674496 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_1 (torch.Size([0])) for Parameter 3 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 12:47:07.530195 139849128674496 shampoo_preconditioner_list.py:375] Rank 1: AdaGradPreconditionerList Numel Breakdown: (0, 67108864, 0, 0, 0, 0, 0, 0)
I0316 12:47:07.530229 139849128674496 shampoo_preconditioner_list.py:378] Rank 1: AdaGradPreconditionerList Bytes Breakdown: (0, 268435456, 0, 0, 0, 0, 0, 0)
I0316 12:47:07.530260 139849128674496 shampoo_preconditioner_list.py:381] Rank 1: AdaGradPreconditionerList Total Elements: 67108864
I0316 12:47:07.530289 139849128674496 shampoo_preconditioner_list.py:384] Rank 1: AdaGradPreconditionerList Total Bytes: 268435456
I0316 12:47:07.530374 140633389098176 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_1 (torch.Size([524288, 128])) for Parameter 1 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 12:47:07.530403 140314632471744 submission_runner.py:321] Saving meta data to /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_1/criteo1tb_pytorch/trial_5/meta_data_0.json.
I0316 12:47:07.530483 140633389098176 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 12:47:07.530474 139830621443264 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 12:47:07.530549 140633389098176 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_1 (torch.Size([0])) for Parameter 2 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 12:47:07.530547 140314632471744 logger_utils.py:262] Unable to record workload.train_mean information. Continuing without it.
I0316 12:47:07.530566 139830621443264 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_1 (torch.Size([0])) for Parameter 0 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 12:47:07.530594 140314632471744 logger_utils.py:262] Unable to record workload.train_stddev information. Continuing without it.
I0316 12:47:07.530605 140633389098176 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 12:47:07.530622 139830621443264 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 12:47:07.530668 140633389098176 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_1 (torch.Size([0])) for Parameter 3 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 12:47:07.530701 139830621443264 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_1 (torch.Size([0])) for Parameter 1 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 12:47:07.530719 140633389098176 shampoo_preconditioner_list.py:375] Rank 3: AdaGradPreconditionerList Numel Breakdown: (0, 0, 0, 67108864, 0, 0, 0, 0)
I0316 12:47:07.530751 140633389098176 shampoo_preconditioner_list.py:378] Rank 3: AdaGradPreconditionerList Bytes Breakdown: (0, 0, 0, 268435456, 0, 0, 0, 0)
I0316 12:47:07.530758 139830621443264 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 12:47:07.530784 140633389098176 shampoo_preconditioner_list.py:381] Rank 3: AdaGradPreconditionerList Total Elements: 67108864
I0316 12:47:07.530815 140633389098176 shampoo_preconditioner_list.py:384] Rank 3: AdaGradPreconditionerList Total Bytes: 268435456
I0316 12:47:07.530807 139830621443264 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_1 (torch.Size([0])) for Parameter 2 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 12:47:07.530793 140293290714304 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([524288, 128])) for Parameter 3 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 12:47:07.530864 139830621443264 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 12:47:07.530890 140293290714304 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_1 (torch.Size([0])) for Parameter 3 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 12:47:07.530966 140293290714304 shampoo_preconditioner_list.py:375] Rank 6: AdaGradPreconditionerList Numel Breakdown: (0, 0, 0, 0, 0, 0, 67108864, 0)
I0316 12:47:07.531012 140293290714304 shampoo_preconditioner_list.py:378] Rank 6: AdaGradPreconditionerList Bytes Breakdown: (0, 0, 0, 0, 0, 0, 268435456, 0)
I0316 12:47:07.531069 140293290714304 shampoo_preconditioner_list.py:381] Rank 6: AdaGradPreconditionerList Total Elements: 67108864
I0316 12:47:07.531098 140293290714304 shampoo_preconditioner_list.py:384] Rank 6: AdaGradPreconditionerList Total Bytes: 268435456
I0316 12:47:07.531095 139794211816640 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_1 (torch.Size([524288, 128])) for Parameter 2 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 12:47:07.531223 139794211816640 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 12:47:07.531236 140554023089344 logger_utils.py:262] Unable to record workload.train_mean information. Continuing without it.
I0316 12:47:07.531288 139794211816640 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_1 (torch.Size([0])) for Parameter 3 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 12:47:07.531306 140554023089344 logger_utils.py:262] Unable to record workload.train_stddev information. Continuing without it.
I0316 12:47:07.531344 139794211816640 shampoo_preconditioner_list.py:375] Rank 5: AdaGradPreconditionerList Numel Breakdown: (0, 0, 0, 0, 0, 67108864, 0, 0)
I0316 12:47:07.531384 139794211816640 shampoo_preconditioner_list.py:378] Rank 5: AdaGradPreconditionerList Bytes Breakdown: (0, 0, 0, 0, 0, 268435456, 0, 0)
I0316 12:47:07.531419 139794211816640 shampoo_preconditioner_list.py:381] Rank 5: AdaGradPreconditionerList Total Elements: 67108864
I0316 12:47:07.531461 139794211816640 shampoo_preconditioner_list.py:384] Rank 5: AdaGradPreconditionerList Total Bytes: 268435456
I0316 12:47:07.531785 139830621443264 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_1 (torch.Size([524288, 128])) for Parameter 3 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 12:47:07.531872 139830621443264 shampoo_preconditioner_list.py:375] Rank 7: AdaGradPreconditionerList Numel Breakdown: (0, 0, 0, 0, 0, 0, 0, 67108864)
I0316 12:47:07.531864 139956878537920 logger_utils.py:262] Unable to record workload.train_mean information. Continuing without it.
I0316 12:47:07.531914 139830621443264 shampoo_preconditioner_list.py:378] Rank 7: AdaGradPreconditionerList Bytes Breakdown: (0, 0, 0, 0, 0, 0, 0, 268435456)
I0316 12:47:07.531924 139956878537920 logger_utils.py:262] Unable to record workload.train_stddev information. Continuing without it.
I0316 12:47:07.531947 139830621443264 shampoo_preconditioner_list.py:381] Rank 7: AdaGradPreconditionerList Total Elements: 67108864
I0316 12:47:07.531979 139830621443264 shampoo_preconditioner_list.py:384] Rank 7: AdaGradPreconditionerList Total Bytes: 268435456
I0316 12:47:07.532123 139849128674496 logger_utils.py:262] Unable to record workload.train_mean information. Continuing without it.
I0316 12:47:07.532197 139849128674496 logger_utils.py:262] Unable to record workload.train_stddev information. Continuing without it.
I0316 12:47:07.532593 140633389098176 logger_utils.py:262] Unable to record workload.train_mean information. Continuing without it.
I0316 12:47:07.532664 140633389098176 logger_utils.py:262] Unable to record workload.train_stddev information. Continuing without it.
I0316 12:47:07.532896 140293290714304 logger_utils.py:262] Unable to record workload.train_mean information. Continuing without it.
I0316 12:47:07.532972 140293290714304 logger_utils.py:262] Unable to record workload.train_stddev information. Continuing without it.
I0316 12:47:07.533016 139794211816640 logger_utils.py:262] Unable to record workload.train_mean information. Continuing without it.
I0316 12:47:07.533093 139794211816640 logger_utils.py:262] Unable to record workload.train_stddev information. Continuing without it.
I0316 12:47:07.533251 139830621443264 logger_utils.py:262] Unable to record workload.train_mean information. Continuing without it.
I0316 12:47:07.533324 139830621443264 logger_utils.py:262] Unable to record workload.train_stddev information. Continuing without it.
I0316 12:47:07.796655 140314632471744 submission_runner.py:325] Saving flags to /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_1/criteo1tb_pytorch/trial_5/flags_0.json.
I0316 12:47:07.876929 140314632471744 submission_runner.py:337] Starting training loop.
I0316 12:47:13.227739 140282951624448 logging_writer.py:48] [0] global_step=0, grad_norm=4.5253, loss=0.603809
I0316 12:47:13.244044 140314632471744 submission.py:265] 0) loss = 0.604, grad_norm = 4.525
I0316 12:47:13.690804 140314632471744 spec.py:321] Evaluating on the training split.
I0316 12:52:24.007704 140314632471744 spec.py:333] Evaluating on the validation split.
I0316 12:57:25.217823 140314632471744 spec.py:349] Evaluating on the test split.
I0316 13:03:22.273189 140314632471744 submission_runner.py:469] Time since start: 974.40s, 	Step: 1, 	{'train/loss': 0.6023427251093904, 'validation/loss': 0.6009843525960341, 'validation/num_examples': 83274637, 'test/loss': 0.60226138782445, 'test/num_examples': 95000000, 'score': 5.367934465408325, 'total_duration': 974.3964323997498, 'accumulated_submission_time': 5.367934465408325, 'accumulated_eval_time': 968.5826709270477, 'accumulated_logging_time': 0}
I0316 13:03:22.282372 140272860083968 logging_writer.py:48] [1] accumulated_eval_time=968.583, accumulated_logging_time=0, accumulated_submission_time=5.36793, global_step=1, preemption_count=0, score=5.36793, test/loss=0.602261, test/num_examples=95000000, total_duration=974.396, train/loss=0.602343, validation/loss=0.600984, validation/num_examples=83274637
I0316 13:03:22.928517 140272851691264 logging_writer.py:48] [1] global_step=1, grad_norm=4.52878, loss=0.603725
I0316 13:03:22.931897 140314632471744 submission.py:265] 1) loss = 0.604, grad_norm = 4.529
I0316 13:03:23.126453 140272860083968 logging_writer.py:48] [2] global_step=2, grad_norm=4.50663, loss=0.60063
I0316 13:03:23.129871 140314632471744 submission.py:265] 2) loss = 0.601, grad_norm = 4.507
I0316 13:03:23.320883 140272851691264 logging_writer.py:48] [3] global_step=3, grad_norm=4.46247, loss=0.5952
I0316 13:03:23.324007 140314632471744 submission.py:265] 3) loss = 0.595, grad_norm = 4.462
I0316 13:03:23.517577 140272860083968 logging_writer.py:48] [4] global_step=4, grad_norm=4.37545, loss=0.587127
I0316 13:03:23.520735 140314632471744 submission.py:265] 4) loss = 0.587, grad_norm = 4.375
I0316 13:03:23.713320 140272851691264 logging_writer.py:48] [5] global_step=5, grad_norm=4.31896, loss=0.577415
I0316 13:03:23.716180 140314632471744 submission.py:265] 5) loss = 0.577, grad_norm = 4.319
I0316 13:03:23.909326 140272860083968 logging_writer.py:48] [6] global_step=6, grad_norm=4.26864, loss=0.565606
I0316 13:03:23.912850 140314632471744 submission.py:265] 6) loss = 0.566, grad_norm = 4.269
I0316 13:03:24.105253 140272851691264 logging_writer.py:48] [7] global_step=7, grad_norm=4.20185, loss=0.551736
I0316 13:03:24.108380 140314632471744 submission.py:265] 7) loss = 0.552, grad_norm = 4.202
I0316 13:03:24.301379 140272860083968 logging_writer.py:48] [8] global_step=8, grad_norm=4.1133, loss=0.536273
I0316 13:03:24.304325 140314632471744 submission.py:265] 8) loss = 0.536, grad_norm = 4.113
I0316 13:03:24.495189 140272851691264 logging_writer.py:48] [9] global_step=9, grad_norm=4.00914, loss=0.519238
I0316 13:03:24.498355 140314632471744 submission.py:265] 9) loss = 0.519, grad_norm = 4.009
I0316 13:03:24.689873 140272860083968 logging_writer.py:48] [10] global_step=10, grad_norm=3.87556, loss=0.501113
I0316 13:03:24.693374 140314632471744 submission.py:265] 10) loss = 0.501, grad_norm = 3.876
I0316 13:03:24.885824 140272851691264 logging_writer.py:48] [11] global_step=11, grad_norm=3.74002, loss=0.481431
I0316 13:03:24.888993 140314632471744 submission.py:265] 11) loss = 0.481, grad_norm = 3.740
I0316 13:03:25.081558 140272860083968 logging_writer.py:48] [12] global_step=12, grad_norm=3.58908, loss=0.461363
I0316 13:03:25.084802 140314632471744 submission.py:265] 12) loss = 0.461, grad_norm = 3.589
I0316 13:03:25.276359 140272851691264 logging_writer.py:48] [13] global_step=13, grad_norm=3.44203, loss=0.440409
I0316 13:03:25.279627 140314632471744 submission.py:265] 13) loss = 0.440, grad_norm = 3.442
I0316 13:03:25.471427 140272860083968 logging_writer.py:48] [14] global_step=14, grad_norm=3.27733, loss=0.420001
I0316 13:03:25.474449 140314632471744 submission.py:265] 14) loss = 0.420, grad_norm = 3.277
I0316 13:03:25.667202 140272851691264 logging_writer.py:48] [15] global_step=15, grad_norm=3.13769, loss=0.397353
I0316 13:03:25.670881 140314632471744 submission.py:265] 15) loss = 0.397, grad_norm = 3.138
I0316 13:03:25.862964 140272860083968 logging_writer.py:48] [16] global_step=16, grad_norm=2.96222, loss=0.376122
I0316 13:03:25.866528 140314632471744 submission.py:265] 16) loss = 0.376, grad_norm = 2.962
I0316 13:03:26.058668 140272851691264 logging_writer.py:48] [17] global_step=17, grad_norm=2.79272, loss=0.353524
I0316 13:03:26.062527 140314632471744 submission.py:265] 17) loss = 0.354, grad_norm = 2.793
I0316 13:03:26.253910 140272860083968 logging_writer.py:48] [18] global_step=18, grad_norm=2.59429, loss=0.333196
I0316 13:03:26.257030 140314632471744 submission.py:265] 18) loss = 0.333, grad_norm = 2.594
I0316 13:03:26.447141 140272851691264 logging_writer.py:48] [19] global_step=19, grad_norm=2.40979, loss=0.313131
I0316 13:03:26.450103 140314632471744 submission.py:265] 19) loss = 0.313, grad_norm = 2.410
I0316 13:03:26.641994 140272860083968 logging_writer.py:48] [20] global_step=20, grad_norm=2.25791, loss=0.293326
I0316 13:03:26.644939 140314632471744 submission.py:265] 20) loss = 0.293, grad_norm = 2.258
I0316 13:03:26.836724 140272851691264 logging_writer.py:48] [21] global_step=21, grad_norm=2.09793, loss=0.27565
I0316 13:03:26.840236 140314632471744 submission.py:265] 21) loss = 0.276, grad_norm = 2.098
I0316 13:03:27.031337 140272860083968 logging_writer.py:48] [22] global_step=22, grad_norm=1.93189, loss=0.259046
I0316 13:03:27.034464 140314632471744 submission.py:265] 22) loss = 0.259, grad_norm = 1.932
I0316 13:03:27.225615 140272851691264 logging_writer.py:48] [23] global_step=23, grad_norm=1.77587, loss=0.242036
I0316 13:03:27.229468 140314632471744 submission.py:265] 23) loss = 0.242, grad_norm = 1.776
I0316 13:03:27.423301 140272860083968 logging_writer.py:48] [24] global_step=24, grad_norm=1.59929, loss=0.226707
I0316 13:03:27.426182 140314632471744 submission.py:265] 24) loss = 0.227, grad_norm = 1.599
I0316 13:03:27.618232 140272851691264 logging_writer.py:48] [25] global_step=25, grad_norm=1.40844, loss=0.213414
I0316 13:03:27.621523 140314632471744 submission.py:265] 25) loss = 0.213, grad_norm = 1.408
I0316 13:03:27.814815 140272860083968 logging_writer.py:48] [26] global_step=26, grad_norm=1.24162, loss=0.197238
I0316 13:03:27.818419 140314632471744 submission.py:265] 26) loss = 0.197, grad_norm = 1.242
I0316 13:03:28.009152 140272851691264 logging_writer.py:48] [27] global_step=27, grad_norm=1.02106, loss=0.189308
I0316 13:03:28.013063 140314632471744 submission.py:265] 27) loss = 0.189, grad_norm = 1.021
I0316 13:03:28.205253 140272860083968 logging_writer.py:48] [28] global_step=28, grad_norm=0.82849, loss=0.180349
I0316 13:03:28.208361 140314632471744 submission.py:265] 28) loss = 0.180, grad_norm = 0.828
I0316 13:03:28.399427 140272851691264 logging_writer.py:48] [29] global_step=29, grad_norm=0.640102, loss=0.173453
I0316 13:03:28.402609 140314632471744 submission.py:265] 29) loss = 0.173, grad_norm = 0.640
I0316 13:03:28.592822 140272860083968 logging_writer.py:48] [30] global_step=30, grad_norm=0.452343, loss=0.168246
I0316 13:03:28.595789 140314632471744 submission.py:265] 30) loss = 0.168, grad_norm = 0.452
I0316 13:03:29.425831 140272851691264 logging_writer.py:48] [31] global_step=31, grad_norm=0.278315, loss=0.164557
I0316 13:03:29.428959 140314632471744 submission.py:265] 31) loss = 0.165, grad_norm = 0.278
I0316 13:03:30.535600 140272860083968 logging_writer.py:48] [32] global_step=32, grad_norm=0.144837, loss=0.162134
I0316 13:03:30.538747 140314632471744 submission.py:265] 32) loss = 0.162, grad_norm = 0.145
I0316 13:03:31.752343 140272851691264 logging_writer.py:48] [33] global_step=33, grad_norm=0.147443, loss=0.163664
I0316 13:03:31.755565 140314632471744 submission.py:265] 33) loss = 0.164, grad_norm = 0.147
I0316 13:03:32.833445 140272860083968 logging_writer.py:48] [34] global_step=34, grad_norm=0.269786, loss=0.164754
I0316 13:03:32.836639 140314632471744 submission.py:265] 34) loss = 0.165, grad_norm = 0.270
I0316 13:03:34.136649 140272851691264 logging_writer.py:48] [35] global_step=35, grad_norm=0.410794, loss=0.167817
I0316 13:03:34.139886 140314632471744 submission.py:265] 35) loss = 0.168, grad_norm = 0.411
I0316 13:03:34.623981 140272860083968 logging_writer.py:48] [36] global_step=36, grad_norm=0.505713, loss=0.167685
I0316 13:03:34.627602 140314632471744 submission.py:265] 36) loss = 0.168, grad_norm = 0.506
I0316 13:03:36.614452 140272851691264 logging_writer.py:48] [37] global_step=37, grad_norm=0.624596, loss=0.172699
I0316 13:03:36.618091 140314632471744 submission.py:265] 37) loss = 0.173, grad_norm = 0.625
I0316 13:03:37.626424 140272860083968 logging_writer.py:48] [38] global_step=38, grad_norm=0.676386, loss=0.171985
I0316 13:03:37.629509 140314632471744 submission.py:265] 38) loss = 0.172, grad_norm = 0.676
I0316 13:03:38.876516 140272851691264 logging_writer.py:48] [39] global_step=39, grad_norm=0.736707, loss=0.173521
I0316 13:03:38.880425 140314632471744 submission.py:265] 39) loss = 0.174, grad_norm = 0.737
I0316 13:03:39.679799 140272860083968 logging_writer.py:48] [40] global_step=40, grad_norm=0.813401, loss=0.178629
I0316 13:03:39.684003 140314632471744 submission.py:265] 40) loss = 0.179, grad_norm = 0.813
I0316 13:03:41.200670 140272851691264 logging_writer.py:48] [41] global_step=41, grad_norm=0.884356, loss=0.184331
I0316 13:03:41.204014 140314632471744 submission.py:265] 41) loss = 0.184, grad_norm = 0.884
I0316 13:03:42.419875 140272860083968 logging_writer.py:48] [42] global_step=42, grad_norm=0.931067, loss=0.186493
I0316 13:03:42.422996 140314632471744 submission.py:265] 42) loss = 0.186, grad_norm = 0.931
I0316 13:03:43.895415 140272851691264 logging_writer.py:48] [43] global_step=43, grad_norm=1.01476, loss=0.195627
I0316 13:03:43.898415 140314632471744 submission.py:265] 43) loss = 0.196, grad_norm = 1.015
I0316 13:03:44.717678 140272860083968 logging_writer.py:48] [44] global_step=44, grad_norm=1.0276, loss=0.195295
I0316 13:03:44.721261 140314632471744 submission.py:265] 44) loss = 0.195, grad_norm = 1.028
I0316 13:03:46.585072 140272851691264 logging_writer.py:48] [45] global_step=45, grad_norm=1.0866, loss=0.201729
I0316 13:03:46.588242 140314632471744 submission.py:265] 45) loss = 0.202, grad_norm = 1.087
I0316 13:03:47.582682 140272860083968 logging_writer.py:48] [46] global_step=46, grad_norm=1.10695, loss=0.203308
I0316 13:03:47.585779 140314632471744 submission.py:265] 46) loss = 0.203, grad_norm = 1.107
I0316 13:03:49.415482 140272851691264 logging_writer.py:48] [47] global_step=47, grad_norm=1.12735, loss=0.205522
I0316 13:03:49.418619 140314632471744 submission.py:265] 47) loss = 0.206, grad_norm = 1.127
I0316 13:03:50.017028 140272860083968 logging_writer.py:48] [48] global_step=48, grad_norm=1.14395, loss=0.207052
I0316 13:03:50.020117 140314632471744 submission.py:265] 48) loss = 0.207, grad_norm = 1.144
I0316 13:03:52.208466 140272851691264 logging_writer.py:48] [49] global_step=49, grad_norm=1.16443, loss=0.209714
I0316 13:03:52.211706 140314632471744 submission.py:265] 49) loss = 0.210, grad_norm = 1.164
I0316 13:03:52.741275 140272860083968 logging_writer.py:48] [50] global_step=50, grad_norm=1.14255, loss=0.206615
I0316 13:03:52.744713 140314632471744 submission.py:265] 50) loss = 0.207, grad_norm = 1.143
I0316 13:03:54.809911 140272851691264 logging_writer.py:48] [51] global_step=51, grad_norm=1.14968, loss=0.207736
I0316 13:03:54.813019 140314632471744 submission.py:265] 51) loss = 0.208, grad_norm = 1.150
I0316 13:03:55.646291 140272860083968 logging_writer.py:48] [52] global_step=52, grad_norm=1.1489, loss=0.208087
I0316 13:03:55.649310 140314632471744 submission.py:265] 52) loss = 0.208, grad_norm = 1.149
I0316 13:03:57.548883 140272851691264 logging_writer.py:48] [53] global_step=53, grad_norm=1.09995, loss=0.202052
I0316 13:03:57.552196 140314632471744 submission.py:265] 53) loss = 0.202, grad_norm = 1.100
I0316 13:03:58.261108 140272860083968 logging_writer.py:48] [54] global_step=54, grad_norm=1.08535, loss=0.200629
I0316 13:03:58.264236 140314632471744 submission.py:265] 54) loss = 0.201, grad_norm = 1.085
I0316 13:03:59.971841 140272851691264 logging_writer.py:48] [55] global_step=55, grad_norm=1.06061, loss=0.198352
I0316 13:03:59.975531 140314632471744 submission.py:265] 55) loss = 0.198, grad_norm = 1.061
I0316 13:04:01.186637 140272860083968 logging_writer.py:48] [56] global_step=56, grad_norm=1.02318, loss=0.193774
I0316 13:04:01.189934 140314632471744 submission.py:265] 56) loss = 0.194, grad_norm = 1.023
I0316 13:04:02.872020 140272851691264 logging_writer.py:48] [57] global_step=57, grad_norm=1.06226, loss=0.19876
I0316 13:04:02.875177 140314632471744 submission.py:265] 57) loss = 0.199, grad_norm = 1.062
I0316 13:04:03.854970 140272860083968 logging_writer.py:48] [58] global_step=58, grad_norm=1.05317, loss=0.199288
I0316 13:04:03.857975 140314632471744 submission.py:265] 58) loss = 0.199, grad_norm = 1.053
I0316 13:04:05.320864 140272851691264 logging_writer.py:48] [59] global_step=59, grad_norm=1.01731, loss=0.197646
I0316 13:04:05.324036 140314632471744 submission.py:265] 59) loss = 0.198, grad_norm = 1.017
I0316 13:04:06.308844 140272860083968 logging_writer.py:48] [60] global_step=60, grad_norm=0.961181, loss=0.192587
I0316 13:04:06.311986 140314632471744 submission.py:265] 60) loss = 0.193, grad_norm = 0.961
I0316 13:04:07.805631 140272851691264 logging_writer.py:48] [61] global_step=61, grad_norm=0.894071, loss=0.187388
I0316 13:04:07.808714 140314632471744 submission.py:265] 61) loss = 0.187, grad_norm = 0.894
I0316 13:04:08.641505 140272860083968 logging_writer.py:48] [62] global_step=62, grad_norm=0.835042, loss=0.183713
I0316 13:04:08.644638 140314632471744 submission.py:265] 62) loss = 0.184, grad_norm = 0.835
I0316 13:04:10.065129 140272851691264 logging_writer.py:48] [63] global_step=63, grad_norm=0.763285, loss=0.179544
I0316 13:04:10.068502 140314632471744 submission.py:265] 63) loss = 0.180, grad_norm = 0.763
I0316 13:04:10.906616 140272860083968 logging_writer.py:48] [64] global_step=64, grad_norm=0.680528, loss=0.174601
I0316 13:04:10.909786 140314632471744 submission.py:265] 64) loss = 0.175, grad_norm = 0.681
I0316 13:04:12.157964 140272851691264 logging_writer.py:48] [65] global_step=65, grad_norm=0.60056, loss=0.171501
I0316 13:04:12.160993 140314632471744 submission.py:265] 65) loss = 0.172, grad_norm = 0.601
I0316 13:04:13.225602 140272860083968 logging_writer.py:48] [66] global_step=66, grad_norm=0.527236, loss=0.169406
I0316 13:04:13.228939 140314632471744 submission.py:265] 66) loss = 0.169, grad_norm = 0.527
I0316 13:04:14.450051 140272851691264 logging_writer.py:48] [67] global_step=67, grad_norm=0.411179, loss=0.162697
I0316 13:04:14.453174 140314632471744 submission.py:265] 67) loss = 0.163, grad_norm = 0.411
I0316 13:04:15.534634 140272860083968 logging_writer.py:48] [68] global_step=68, grad_norm=0.315703, loss=0.160347
I0316 13:04:15.538186 140314632471744 submission.py:265] 68) loss = 0.160, grad_norm = 0.316
I0316 13:04:16.540600 140272851691264 logging_writer.py:48] [69] global_step=69, grad_norm=0.192435, loss=0.156275
I0316 13:04:16.543805 140314632471744 submission.py:265] 69) loss = 0.156, grad_norm = 0.192
I0316 13:04:17.666466 140272860083968 logging_writer.py:48] [70] global_step=70, grad_norm=0.118701, loss=0.157804
I0316 13:04:17.669847 140314632471744 submission.py:265] 70) loss = 0.158, grad_norm = 0.119
I0316 13:04:18.790967 140272851691264 logging_writer.py:48] [71] global_step=71, grad_norm=0.0734457, loss=0.157578
I0316 13:04:18.794136 140314632471744 submission.py:265] 71) loss = 0.158, grad_norm = 0.073
I0316 13:04:20.103890 140272860083968 logging_writer.py:48] [72] global_step=72, grad_norm=0.127734, loss=0.15684
I0316 13:04:20.108065 140314632471744 submission.py:265] 72) loss = 0.157, grad_norm = 0.128
I0316 13:04:20.829178 140272851691264 logging_writer.py:48] [73] global_step=73, grad_norm=0.204328, loss=0.157401
I0316 13:04:20.833108 140314632471744 submission.py:265] 73) loss = 0.157, grad_norm = 0.204
I0316 13:04:22.657371 140272860083968 logging_writer.py:48] [74] global_step=74, grad_norm=0.282333, loss=0.157307
I0316 13:04:22.660633 140314632471744 submission.py:265] 74) loss = 0.157, grad_norm = 0.282
I0316 13:04:23.645879 140272851691264 logging_writer.py:48] [75] global_step=75, grad_norm=0.344904, loss=0.157859
I0316 13:04:23.649621 140314632471744 submission.py:265] 75) loss = 0.158, grad_norm = 0.345
I0316 13:04:25.534144 140272860083968 logging_writer.py:48] [76] global_step=76, grad_norm=0.426692, loss=0.15206
I0316 13:04:25.537947 140314632471744 submission.py:265] 76) loss = 0.152, grad_norm = 0.427
I0316 13:04:26.584852 140272851691264 logging_writer.py:48] [77] global_step=77, grad_norm=0.479942, loss=0.149092
I0316 13:04:26.588424 140314632471744 submission.py:265] 77) loss = 0.149, grad_norm = 0.480
I0316 13:04:28.310880 140272860083968 logging_writer.py:48] [78] global_step=78, grad_norm=0.504782, loss=0.147791
I0316 13:04:28.314360 140314632471744 submission.py:265] 78) loss = 0.148, grad_norm = 0.505
I0316 13:04:29.173051 140272851691264 logging_writer.py:48] [79] global_step=79, grad_norm=0.495035, loss=0.149048
I0316 13:04:29.176724 140314632471744 submission.py:265] 79) loss = 0.149, grad_norm = 0.495
I0316 13:04:30.575683 140272860083968 logging_writer.py:48] [80] global_step=80, grad_norm=0.467368, loss=0.15009
I0316 13:04:30.579463 140314632471744 submission.py:265] 80) loss = 0.150, grad_norm = 0.467
I0316 13:04:31.590697 140272851691264 logging_writer.py:48] [81] global_step=81, grad_norm=0.453778, loss=0.148167
I0316 13:04:31.594468 140314632471744 submission.py:265] 81) loss = 0.148, grad_norm = 0.454
I0316 13:04:32.739392 140272860083968 logging_writer.py:48] [82] global_step=82, grad_norm=0.419587, loss=0.14666
I0316 13:04:32.743055 140314632471744 submission.py:265] 82) loss = 0.147, grad_norm = 0.420
I0316 13:04:33.765675 140272851691264 logging_writer.py:48] [83] global_step=83, grad_norm=0.375465, loss=0.144983
I0316 13:04:33.769453 140314632471744 submission.py:265] 83) loss = 0.145, grad_norm = 0.375
I0316 13:04:34.533420 140272860083968 logging_writer.py:48] [84] global_step=84, grad_norm=0.31737, loss=0.143913
I0316 13:04:34.537117 140314632471744 submission.py:265] 84) loss = 0.144, grad_norm = 0.317
I0316 13:04:36.316771 140272851691264 logging_writer.py:48] [85] global_step=85, grad_norm=0.265075, loss=0.14166
I0316 13:04:36.320041 140314632471744 submission.py:265] 85) loss = 0.142, grad_norm = 0.265
I0316 13:04:37.458834 140272860083968 logging_writer.py:48] [86] global_step=86, grad_norm=0.199592, loss=0.140158
I0316 13:04:37.461922 140314632471744 submission.py:265] 86) loss = 0.140, grad_norm = 0.200
I0316 13:04:38.911679 140272851691264 logging_writer.py:48] [87] global_step=87, grad_norm=0.131505, loss=0.139266
I0316 13:04:38.914713 140314632471744 submission.py:265] 87) loss = 0.139, grad_norm = 0.132
I0316 13:04:39.898053 140272860083968 logging_writer.py:48] [88] global_step=88, grad_norm=0.0614638, loss=0.140266
I0316 13:04:39.901234 140314632471744 submission.py:265] 88) loss = 0.140, grad_norm = 0.061
I0316 13:04:41.437889 140272851691264 logging_writer.py:48] [89] global_step=89, grad_norm=0.030456, loss=0.138539
I0316 13:04:41.441144 140314632471744 submission.py:265] 89) loss = 0.139, grad_norm = 0.030
I0316 13:04:42.354141 140272860083968 logging_writer.py:48] [90] global_step=90, grad_norm=0.0845098, loss=0.140808
I0316 13:04:42.357348 140314632471744 submission.py:265] 90) loss = 0.141, grad_norm = 0.085
I0316 13:04:44.235098 140272851691264 logging_writer.py:48] [91] global_step=91, grad_norm=0.118966, loss=0.138846
I0316 13:04:44.238176 140314632471744 submission.py:265] 91) loss = 0.139, grad_norm = 0.119
I0316 13:04:44.806350 140272860083968 logging_writer.py:48] [92] global_step=92, grad_norm=0.161794, loss=0.139432
I0316 13:04:44.809720 140314632471744 submission.py:265] 92) loss = 0.139, grad_norm = 0.162
I0316 13:04:46.754218 140272851691264 logging_writer.py:48] [93] global_step=93, grad_norm=0.200222, loss=0.140316
I0316 13:04:46.757761 140314632471744 submission.py:265] 93) loss = 0.140, grad_norm = 0.200
I0316 13:04:47.660465 140272860083968 logging_writer.py:48] [94] global_step=94, grad_norm=0.221709, loss=0.139984
I0316 13:04:47.664056 140314632471744 submission.py:265] 94) loss = 0.140, grad_norm = 0.222
I0316 13:04:49.594571 140272851691264 logging_writer.py:48] [95] global_step=95, grad_norm=0.277467, loss=0.146602
I0316 13:04:49.597781 140314632471744 submission.py:265] 95) loss = 0.147, grad_norm = 0.277
I0316 13:04:50.367114 140272860083968 logging_writer.py:48] [96] global_step=96, grad_norm=0.298954, loss=0.14898
I0316 13:04:50.370191 140314632471744 submission.py:265] 96) loss = 0.149, grad_norm = 0.299
I0316 13:04:52.127319 140272851691264 logging_writer.py:48] [97] global_step=97, grad_norm=0.312435, loss=0.152074
I0316 13:04:52.130322 140314632471744 submission.py:265] 97) loss = 0.152, grad_norm = 0.312
I0316 13:04:53.221824 140272860083968 logging_writer.py:48] [98] global_step=98, grad_norm=0.28214, loss=0.149164
I0316 13:04:53.225014 140314632471744 submission.py:265] 98) loss = 0.149, grad_norm = 0.282
I0316 13:04:54.923778 140272851691264 logging_writer.py:48] [99] global_step=99, grad_norm=0.261578, loss=0.148116
I0316 13:04:54.926959 140314632471744 submission.py:265] 99) loss = 0.148, grad_norm = 0.262
I0316 13:04:55.583331 140272860083968 logging_writer.py:48] [100] global_step=100, grad_norm=0.243298, loss=0.148448
I0316 13:04:55.586740 140314632471744 submission.py:265] 100) loss = 0.148, grad_norm = 0.243
I0316 13:05:23.459771 140314632471744 spec.py:321] Evaluating on the training split.
I0316 13:10:50.740985 140314632471744 spec.py:333] Evaluating on the validation split.
I0316 13:15:18.447186 140314632471744 spec.py:349] Evaluating on the test split.
I0316 13:20:33.775264 140314632471744 submission_runner.py:469] Time since start: 2005.90s, 	Step: 123, 	{'train/loss': 0.13607341417373647, 'validation/loss': 0.13920789513171378, 'validation/num_examples': 83274637, 'test/loss': 0.14271085073579487, 'test/num_examples': 95000000, 'score': 125.63558554649353, 'total_duration': 2005.8985240459442, 'accumulated_submission_time': 125.63558554649353, 'accumulated_eval_time': 1878.8984410762787, 'accumulated_logging_time': 0.016098499298095703}
I0316 13:20:33.785741 140272851691264 logging_writer.py:48] [123] accumulated_eval_time=1878.9, accumulated_logging_time=0.0160985, accumulated_submission_time=125.636, global_step=123, preemption_count=0, score=125.636, test/loss=0.142711, test/num_examples=95000000, total_duration=2005.9, train/loss=0.136073, validation/loss=0.139208, validation/num_examples=83274637
I0316 13:22:35.235450 140314632471744 spec.py:321] Evaluating on the training split.
I0316 13:27:41.442992 140314632471744 spec.py:333] Evaluating on the validation split.
I0316 13:32:03.538260 140314632471744 spec.py:349] Evaluating on the test split.
I0316 13:37:32.151118 140314632471744 submission_runner.py:469] Time since start: 3024.27s, 	Step: 243, 	{'train/loss': 0.12818622331229518, 'validation/loss': 0.12981452871103244, 'validation/num_examples': 83274637, 'test/loss': 0.13210321375836825, 'test/num_examples': 95000000, 'score': 246.1509747505188, 'total_duration': 3024.2743170261383, 'accumulated_submission_time': 246.1509747505188, 'accumulated_eval_time': 2775.8141510486603, 'accumulated_logging_time': 0.03409004211425781}
I0316 13:37:32.160504 140272860083968 logging_writer.py:48] [243] accumulated_eval_time=2775.81, accumulated_logging_time=0.03409, accumulated_submission_time=246.151, global_step=243, preemption_count=0, score=246.151, test/loss=0.132103, test/num_examples=95000000, total_duration=3024.27, train/loss=0.128186, validation/loss=0.129815, validation/num_examples=83274637
I0316 13:39:33.041531 140314632471744 spec.py:321] Evaluating on the training split.
I0316 13:44:45.119019 140314632471744 spec.py:333] Evaluating on the validation split.
I0316 13:49:10.972789 140314632471744 spec.py:349] Evaluating on the test split.
I0316 13:54:54.481389 140314632471744 submission_runner.py:469] Time since start: 4066.60s, 	Step: 368, 	{'train/loss': 0.12628630429185478, 'validation/loss': 0.12798768432529592, 'validation/num_examples': 83274637, 'test/loss': 0.1303522778307463, 'test/num_examples': 95000000, 'score': 366.17584705352783, 'total_duration': 4066.604663848877, 'accumulated_submission_time': 366.17584705352783, 'accumulated_eval_time': 3697.254068374634, 'accumulated_logging_time': 0.05007576942443848}
I0316 13:54:54.490980 140272851691264 logging_writer.py:48] [368] accumulated_eval_time=3697.25, accumulated_logging_time=0.0500758, accumulated_submission_time=366.176, global_step=368, preemption_count=0, score=366.176, test/loss=0.130352, test/num_examples=95000000, total_duration=4066.6, train/loss=0.126286, validation/loss=0.127988, validation/num_examples=83274637
I0316 13:56:55.156185 140314632471744 spec.py:321] Evaluating on the training split.
I0316 14:02:14.095681 140314632471744 spec.py:333] Evaluating on the validation split.
I0316 14:06:36.695640 140314632471744 spec.py:349] Evaluating on the test split.
I0316 14:11:59.461281 140314632471744 submission_runner.py:469] Time since start: 5091.58s, 	Step: 490, 	{'train/loss': 0.12647742901600936, 'validation/loss': 0.12742401315638827, 'validation/num_examples': 83274637, 'test/loss': 0.12976422706716437, 'test/num_examples': 95000000, 'score': 485.9236822128296, 'total_duration': 5091.5845313072205, 'accumulated_submission_time': 485.9236822128296, 'accumulated_eval_time': 4601.559302806854, 'accumulated_logging_time': 0.1294541358947754}
I0316 14:11:59.471075 140272860083968 logging_writer.py:48] [490] accumulated_eval_time=4601.56, accumulated_logging_time=0.129454, accumulated_submission_time=485.924, global_step=490, preemption_count=0, score=485.924, test/loss=0.129764, test/num_examples=95000000, total_duration=5091.58, train/loss=0.126477, validation/loss=0.127424, validation/num_examples=83274637
I0316 14:12:02.035674 140272851691264 logging_writer.py:48] [500] global_step=500, grad_norm=0.0359211, loss=0.128996
I0316 14:12:02.039020 140314632471744 submission.py:265] 500) loss = 0.129, grad_norm = 0.036
I0316 14:14:01.113993 140314632471744 spec.py:321] Evaluating on the training split.
I0316 14:19:14.123386 140314632471744 spec.py:333] Evaluating on the validation split.
I0316 14:23:38.541774 140314632471744 spec.py:349] Evaluating on the test split.
I0316 14:29:20.475173 140314632471744 submission_runner.py:469] Time since start: 6132.60s, 	Step: 614, 	{'train/loss': 0.12424785546139806, 'validation/loss': 0.12715673677421416, 'validation/num_examples': 83274637, 'test/loss': 0.12956325677321584, 'test/num_examples': 95000000, 'score': 606.7232487201691, 'total_duration': 6132.598413228989, 'accumulated_submission_time': 606.7232487201691, 'accumulated_eval_time': 5520.920495033264, 'accumulated_logging_time': 0.14644312858581543}
I0316 14:29:20.484660 140272860083968 logging_writer.py:48] [614] accumulated_eval_time=5520.92, accumulated_logging_time=0.146443, accumulated_submission_time=606.723, global_step=614, preemption_count=0, score=606.723, test/loss=0.129563, test/num_examples=95000000, total_duration=6132.6, train/loss=0.124248, validation/loss=0.127157, validation/num_examples=83274637
I0316 14:31:21.054275 140314632471744 spec.py:321] Evaluating on the training split.
I0316 14:36:29.192849 140314632471744 spec.py:333] Evaluating on the validation split.
I0316 14:41:00.305058 140314632471744 spec.py:349] Evaluating on the test split.
I0316 14:46:05.169442 140314632471744 submission_runner.py:469] Time since start: 7137.29s, 	Step: 741, 	{'train/loss': 0.12509329326120291, 'validation/loss': 0.12684132970780748, 'validation/num_examples': 83274637, 'test/loss': 0.1291754362735949, 'test/num_examples': 95000000, 'score': 726.4880728721619, 'total_duration': 7137.292676210403, 'accumulated_submission_time': 726.4880728721619, 'accumulated_eval_time': 6405.035758495331, 'accumulated_logging_time': 0.16379523277282715}
I0316 14:46:05.179794 140272851691264 logging_writer.py:48] [741] accumulated_eval_time=6405.04, accumulated_logging_time=0.163795, accumulated_submission_time=726.488, global_step=741, preemption_count=0, score=726.488, test/loss=0.129175, test/num_examples=95000000, total_duration=7137.29, train/loss=0.125093, validation/loss=0.126841, validation/num_examples=83274637
I0316 14:48:05.652219 140314632471744 spec.py:321] Evaluating on the training split.
I0316 14:53:10.188325 140314632471744 spec.py:333] Evaluating on the validation split.
I0316 14:57:41.179522 140314632471744 spec.py:349] Evaluating on the test split.
I0316 15:03:09.827780 140314632471744 submission_runner.py:469] Time since start: 8161.95s, 	Step: 862, 	{'train/loss': 0.126744200597174, 'validation/loss': 0.12681966861810842, 'validation/num_examples': 83274637, 'test/loss': 0.12923705452712209, 'test/num_examples': 95000000, 'score': 846.0993120670319, 'total_duration': 8161.951017856598, 'accumulated_submission_time': 846.0993120670319, 'accumulated_eval_time': 7309.211316585541, 'accumulated_logging_time': 0.18069028854370117}
I0316 15:03:09.837187 140272860083968 logging_writer.py:48] [862] accumulated_eval_time=7309.21, accumulated_logging_time=0.18069, accumulated_submission_time=846.099, global_step=862, preemption_count=0, score=846.099, test/loss=0.129237, test/num_examples=95000000, total_duration=8161.95, train/loss=0.126744, validation/loss=0.12682, validation/num_examples=83274637
I0316 15:05:10.793154 140314632471744 spec.py:321] Evaluating on the training split.
I0316 15:10:00.248223 140314632471744 spec.py:333] Evaluating on the validation split.
I0316 15:14:12.005624 140314632471744 spec.py:349] Evaluating on the test split.
I0316 15:19:04.939838 140314632471744 submission_runner.py:469] Time since start: 9117.06s, 	Step: 986, 	{'train/loss': 0.12558371350475164, 'validation/loss': 0.12672920731320914, 'validation/num_examples': 83274637, 'test/loss': 0.12913884590964067, 'test/num_examples': 95000000, 'score': 966.2201406955719, 'total_duration': 9117.063033342361, 'accumulated_submission_time': 966.2201406955719, 'accumulated_eval_time': 8143.358115911484, 'accumulated_logging_time': 0.19673418998718262}
I0316 15:19:04.950884 140272851691264 logging_writer.py:48] [986] accumulated_eval_time=8143.36, accumulated_logging_time=0.196734, accumulated_submission_time=966.22, global_step=986, preemption_count=0, score=966.22, test/loss=0.129139, test/num_examples=95000000, total_duration=9117.06, train/loss=0.125584, validation/loss=0.126729, validation/num_examples=83274637
I0316 15:19:08.273868 140272860083968 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.0301424, loss=0.122809
I0316 15:19:08.277513 140314632471744 submission.py:265] 1000) loss = 0.123, grad_norm = 0.030
/usr/local/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
I0316 15:21:06.117551 140314632471744 spec.py:321] Evaluating on the training split.
I0316 15:25:48.998404 140314632471744 spec.py:333] Evaluating on the validation split.
I0316 15:29:44.288408 140314632471744 spec.py:349] Evaluating on the test split.
I0316 15:34:47.555432 140314632471744 submission_runner.py:469] Time since start: 10059.68s, 	Step: 1103, 	{'train/loss': 0.12646174373415914, 'validation/loss': 0.12650915166209598, 'validation/num_examples': 83274637, 'test/loss': 0.12899481884974429, 'test/num_examples': 95000000, 'score': 1086.4548768997192, 'total_duration': 10059.67867898941, 'accumulated_submission_time': 1086.4548768997192, 'accumulated_eval_time': 8964.796182394028, 'accumulated_logging_time': 0.21472716331481934}
I0316 15:34:47.566418 140272851691264 logging_writer.py:48] [1103] accumulated_eval_time=8964.8, accumulated_logging_time=0.214727, accumulated_submission_time=1086.45, global_step=1103, preemption_count=0, score=1086.45, test/loss=0.128995, test/num_examples=95000000, total_duration=10059.7, train/loss=0.126462, validation/loss=0.126509, validation/num_examples=83274637
I0316 15:36:48.112937 140314632471744 spec.py:321] Evaluating on the training split.
I0316 15:41:05.777563 140314632471744 spec.py:333] Evaluating on the validation split.
I0316 15:44:51.041070 140314632471744 spec.py:349] Evaluating on the test split.
I0316 15:49:48.786708 140314632471744 submission_runner.py:469] Time since start: 10960.91s, 	Step: 1227, 	{'train/loss': 0.12390195336839624, 'validation/loss': 0.12600683487165992, 'validation/num_examples': 83274637, 'test/loss': 0.12853462125035336, 'test/num_examples': 95000000, 'score': 1206.1280629634857, 'total_duration': 10960.909957408905, 'accumulated_submission_time': 1206.1280629634857, 'accumulated_eval_time': 9745.470032691956, 'accumulated_logging_time': 0.2517678737640381}
I0316 15:49:48.797437 140272860083968 logging_writer.py:48] [1227] accumulated_eval_time=9745.47, accumulated_logging_time=0.251768, accumulated_submission_time=1206.13, global_step=1227, preemption_count=0, score=1206.13, test/loss=0.128535, test/num_examples=95000000, total_duration=10960.9, train/loss=0.123902, validation/loss=0.126007, validation/num_examples=83274637
I0316 15:51:51.656256 140314632471744 spec.py:321] Evaluating on the training split.
I0316 15:54:51.103442 140314632471744 spec.py:333] Evaluating on the validation split.
I0316 15:57:43.676269 140314632471744 spec.py:349] Evaluating on the test split.
I0316 16:01:27.780385 140314632471744 submission_runner.py:469] Time since start: 11659.90s, 	Step: 1346, 	{'train/loss': 0.12492355842778348, 'validation/loss': 0.12595288095509197, 'validation/num_examples': 83274637, 'test/loss': 0.12844056025671707, 'test/num_examples': 95000000, 'score': 1328.18146443367, 'total_duration': 11659.903614521027, 'accumulated_submission_time': 1328.18146443367, 'accumulated_eval_time': 10321.594165086746, 'accumulated_logging_time': 0.26906704902648926}
I0316 16:01:27.790173 140272851691264 logging_writer.py:48] [1346] accumulated_eval_time=10321.6, accumulated_logging_time=0.269067, accumulated_submission_time=1328.18, global_step=1346, preemption_count=0, score=1328.18, test/loss=0.128441, test/num_examples=95000000, total_duration=11659.9, train/loss=0.124924, validation/loss=0.125953, validation/num_examples=83274637
I0316 16:03:28.957240 140314632471744 spec.py:321] Evaluating on the training split.
I0316 16:05:33.349132 140314632471744 spec.py:333] Evaluating on the validation split.
I0316 16:07:37.605711 140314632471744 spec.py:349] Evaluating on the test split.
I0316 16:10:35.195211 140314632471744 submission_runner.py:469] Time since start: 12207.32s, 	Step: 1466, 	{'train/loss': 0.12380710599787156, 'validation/loss': 0.12585928425596793, 'validation/num_examples': 83274637, 'test/loss': 0.12837989984765305, 'test/num_examples': 95000000, 'score': 1448.4299774169922, 'total_duration': 12207.318369865417, 'accumulated_submission_time': 1448.4299774169922, 'accumulated_eval_time': 10747.832133293152, 'accumulated_logging_time': 0.28632259368896484}
I0316 16:10:35.206496 140272860083968 logging_writer.py:48] [1466] accumulated_eval_time=10747.8, accumulated_logging_time=0.286323, accumulated_submission_time=1448.43, global_step=1466, preemption_count=0, score=1448.43, test/loss=0.12838, test/num_examples=95000000, total_duration=12207.3, train/loss=0.123807, validation/loss=0.125859, validation/num_examples=83274637
I0316 16:10:47.778676 140272851691264 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.00649441, loss=0.129002
I0316 16:10:47.782029 140314632471744 submission.py:265] 1500) loss = 0.129, grad_norm = 0.006
I0316 16:12:36.067949 140314632471744 spec.py:321] Evaluating on the training split.
I0316 16:14:39.410828 140314632471744 spec.py:333] Evaluating on the validation split.
I0316 16:16:43.407682 140314632471744 spec.py:349] Evaluating on the test split.
I0316 16:19:06.775084 140314632471744 submission_runner.py:469] Time since start: 12718.90s, 	Step: 1589, 	{'train/loss': 0.12569676535372618, 'validation/loss': 0.1260618083854614, 'validation/num_examples': 83274637, 'test/loss': 0.1286645704322012, 'test/num_examples': 95000000, 'score': 1568.39866232872, 'total_duration': 12718.898329734802, 'accumulated_submission_time': 1568.39866232872, 'accumulated_eval_time': 11138.539406061172, 'accumulated_logging_time': 0.3049333095550537}
I0316 16:19:06.785599 140272860083968 logging_writer.py:48] [1589] accumulated_eval_time=11138.5, accumulated_logging_time=0.304933, accumulated_submission_time=1568.4, global_step=1589, preemption_count=0, score=1568.4, test/loss=0.128665, test/num_examples=95000000, total_duration=12718.9, train/loss=0.125697, validation/loss=0.126062, validation/num_examples=83274637
I0316 16:21:07.949361 140314632471744 spec.py:321] Evaluating on the training split.
I0316 16:23:11.797576 140314632471744 spec.py:333] Evaluating on the validation split.
I0316 16:25:16.175806 140314632471744 spec.py:349] Evaluating on the test split.
I0316 16:27:39.203727 140314632471744 submission_runner.py:469] Time since start: 13231.33s, 	Step: 1711, 	{'train/loss': 0.12615529759135666, 'validation/loss': 0.1256201966242171, 'validation/num_examples': 83274637, 'test/loss': 0.12809278509425112, 'test/num_examples': 95000000, 'score': 1688.6981909275055, 'total_duration': 13231.326963424683, 'accumulated_submission_time': 1688.6981909275055, 'accumulated_eval_time': 11529.793808460236, 'accumulated_logging_time': 0.33905696868896484}
I0316 16:27:39.214034 140272851691264 logging_writer.py:48] [1711] accumulated_eval_time=11529.8, accumulated_logging_time=0.339057, accumulated_submission_time=1688.7, global_step=1711, preemption_count=0, score=1688.7, test/loss=0.128093, test/num_examples=95000000, total_duration=13231.3, train/loss=0.126155, validation/loss=0.12562, validation/num_examples=83274637
I0316 16:29:40.350810 140314632471744 spec.py:321] Evaluating on the training split.
I0316 16:31:44.064260 140314632471744 spec.py:333] Evaluating on the validation split.
I0316 16:33:47.849973 140314632471744 spec.py:349] Evaluating on the test split.
I0316 16:36:10.710662 140314632471744 submission_runner.py:469] Time since start: 13742.83s, 	Step: 1830, 	{'train/loss': 0.12508121588572085, 'validation/loss': 0.12570268213533856, 'validation/num_examples': 83274637, 'test/loss': 0.12824256043644955, 'test/num_examples': 95000000, 'score': 1808.953208208084, 'total_duration': 13742.833914995193, 'accumulated_submission_time': 1808.953208208084, 'accumulated_eval_time': 11920.15373635292, 'accumulated_logging_time': 0.3559000492095947}
I0316 16:36:10.721405 140272860083968 logging_writer.py:48] [1830] accumulated_eval_time=11920.2, accumulated_logging_time=0.3559, accumulated_submission_time=1808.95, global_step=1830, preemption_count=0, score=1808.95, test/loss=0.128243, test/num_examples=95000000, total_duration=13742.8, train/loss=0.125081, validation/loss=0.125703, validation/num_examples=83274637
I0316 16:38:11.150690 140314632471744 spec.py:321] Evaluating on the training split.
I0316 16:40:16.021550 140314632471744 spec.py:333] Evaluating on the validation split.
I0316 16:42:19.789688 140314632471744 spec.py:349] Evaluating on the test split.
I0316 16:44:44.465764 140314632471744 submission_runner.py:469] Time since start: 14256.59s, 	Step: 1952, 	{'train/loss': 0.12419040443374973, 'validation/loss': 0.12533212653477432, 'validation/num_examples': 83274637, 'test/loss': 0.12776818721723054, 'test/num_examples': 95000000, 'score': 1928.4589233398438, 'total_duration': 14256.589020252228, 'accumulated_submission_time': 1928.4589233398438, 'accumulated_eval_time': 12313.46892786026, 'accumulated_logging_time': 0.3737156391143799}
I0316 16:44:44.475890 140272851691264 logging_writer.py:48] [1952] accumulated_eval_time=12313.5, accumulated_logging_time=0.373716, accumulated_submission_time=1928.46, global_step=1952, preemption_count=0, score=1928.46, test/loss=0.127768, test/num_examples=95000000, total_duration=14256.6, train/loss=0.12419, validation/loss=0.125332, validation/num_examples=83274637
I0316 16:45:14.589829 140272860083968 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.00569195, loss=0.118133
I0316 16:45:14.593146 140314632471744 submission.py:265] 2000) loss = 0.118, grad_norm = 0.006
I0316 16:46:45.575648 140314632471744 spec.py:321] Evaluating on the training split.
I0316 16:48:49.202870 140314632471744 spec.py:333] Evaluating on the validation split.
I0316 16:50:52.863962 140314632471744 spec.py:349] Evaluating on the test split.
I0316 16:53:15.293949 140314632471744 submission_runner.py:469] Time since start: 14767.42s, 	Step: 2074, 	{'train/loss': 0.12431211828908453, 'validation/loss': 0.125642926583419, 'validation/num_examples': 83274637, 'test/loss': 0.1281478270837884, 'test/num_examples': 95000000, 'score': 2048.6620337963104, 'total_duration': 14767.41723227501, 'accumulated_submission_time': 2048.6620337963104, 'accumulated_eval_time': 12703.187343358994, 'accumulated_logging_time': 0.39081406593322754}
I0316 16:53:15.304608 140272851691264 logging_writer.py:48] [2074] accumulated_eval_time=12703.2, accumulated_logging_time=0.390814, accumulated_submission_time=2048.66, global_step=2074, preemption_count=0, score=2048.66, test/loss=0.128148, test/num_examples=95000000, total_duration=14767.4, train/loss=0.124312, validation/loss=0.125643, validation/num_examples=83274637
I0316 16:55:16.514178 140314632471744 spec.py:321] Evaluating on the training split.
I0316 16:57:20.239182 140314632471744 spec.py:333] Evaluating on the validation split.
I0316 16:59:22.614561 140314632471744 spec.py:349] Evaluating on the test split.
I0316 17:01:44.656358 140314632471744 submission_runner.py:469] Time since start: 15276.78s, 	Step: 2198, 	{'train/loss': 0.12430118498942622, 'validation/loss': 0.12563878545653895, 'validation/num_examples': 83274637, 'test/loss': 0.12826537484604686, 'test/num_examples': 95000000, 'score': 2169.002023458481, 'total_duration': 15276.779505252838, 'accumulated_submission_time': 2169.002023458481, 'accumulated_eval_time': 13091.329463005066, 'accumulated_logging_time': 0.4591939449310303}
I0316 17:01:44.667759 140272860083968 logging_writer.py:48] [2198] accumulated_eval_time=13091.3, accumulated_logging_time=0.459194, accumulated_submission_time=2169, global_step=2198, preemption_count=0, score=2169, test/loss=0.128265, test/num_examples=95000000, total_duration=15276.8, train/loss=0.124301, validation/loss=0.125639, validation/num_examples=83274637
I0316 17:03:46.336589 140314632471744 spec.py:321] Evaluating on the training split.
I0316 17:05:49.814731 140314632471744 spec.py:333] Evaluating on the validation split.
I0316 17:07:53.563715 140314632471744 spec.py:349] Evaluating on the test split.
I0316 17:10:16.297720 140314632471744 submission_runner.py:469] Time since start: 15788.42s, 	Step: 2320, 	{'train/loss': 0.12236742837424715, 'validation/loss': 0.12534673638555208, 'validation/num_examples': 83274637, 'test/loss': 0.12785338479698583, 'test/num_examples': 95000000, 'score': 2289.8345291614532, 'total_duration': 15788.4209856987, 'accumulated_submission_time': 2289.8345291614532, 'accumulated_eval_time': 13481.290761947632, 'accumulated_logging_time': 0.47815465927124023}
I0316 17:10:16.308294 140272851691264 logging_writer.py:48] [2320] accumulated_eval_time=13481.3, accumulated_logging_time=0.478155, accumulated_submission_time=2289.83, global_step=2320, preemption_count=0, score=2289.83, test/loss=0.127853, test/num_examples=95000000, total_duration=15788.4, train/loss=0.122367, validation/loss=0.125347, validation/num_examples=83274637
I0316 17:12:17.485270 140314632471744 spec.py:321] Evaluating on the training split.
I0316 17:14:20.665275 140314632471744 spec.py:333] Evaluating on the validation split.
I0316 17:16:24.475939 140314632471744 spec.py:349] Evaluating on the test split.
I0316 17:18:47.114043 140314632471744 submission_runner.py:469] Time since start: 16299.24s, 	Step: 2442, 	{'train/loss': 0.1251857147346222, 'validation/loss': 0.125357922766593, 'validation/num_examples': 83274637, 'test/loss': 0.1279412816410667, 'test/num_examples': 95000000, 'score': 2410.129479408264, 'total_duration': 16299.23723602295, 'accumulated_submission_time': 2410.129479408264, 'accumulated_eval_time': 13870.919592380524, 'accumulated_logging_time': 0.4958229064941406}
I0316 17:18:47.124692 140272860083968 logging_writer.py:48] [2442] accumulated_eval_time=13870.9, accumulated_logging_time=0.495823, accumulated_submission_time=2410.13, global_step=2442, preemption_count=0, score=2410.13, test/loss=0.127941, test/num_examples=95000000, total_duration=16299.2, train/loss=0.125186, validation/loss=0.125358, validation/num_examples=83274637
I0316 17:19:29.631395 140272851691264 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.0176392, loss=0.126941
I0316 17:19:29.634698 140314632471744 submission.py:265] 2500) loss = 0.127, grad_norm = 0.018
I0316 17:20:47.799103 140314632471744 spec.py:321] Evaluating on the training split.
I0316 17:22:51.469668 140314632471744 spec.py:333] Evaluating on the validation split.
I0316 17:24:54.029764 140314632471744 spec.py:349] Evaluating on the test split.
I0316 17:27:16.970710 140314632471744 submission_runner.py:469] Time since start: 16809.09s, 	Step: 2563, 	{'train/loss': 0.12540921109557934, 'validation/loss': 0.12486918031294199, 'validation/num_examples': 83274637, 'test/loss': 0.12739085005822431, 'test/num_examples': 95000000, 'score': 2529.8876190185547, 'total_duration': 16809.093782901764, 'accumulated_submission_time': 2529.8876190185547, 'accumulated_eval_time': 14260.09124135971, 'accumulated_logging_time': 0.5261611938476562}
I0316 17:27:16.982559 140272860083968 logging_writer.py:48] [2563] accumulated_eval_time=14260.1, accumulated_logging_time=0.526161, accumulated_submission_time=2529.89, global_step=2563, preemption_count=0, score=2529.89, test/loss=0.127391, test/num_examples=95000000, total_duration=16809.1, train/loss=0.125409, validation/loss=0.124869, validation/num_examples=83274637
I0316 17:29:17.820379 140314632471744 spec.py:321] Evaluating on the training split.
I0316 17:31:21.136822 140314632471744 spec.py:333] Evaluating on the validation split.
I0316 17:33:24.926706 140314632471744 spec.py:349] Evaluating on the test split.
I0316 17:35:47.571895 140314632471744 submission_runner.py:469] Time since start: 17319.70s, 	Step: 2686, 	{'train/loss': 0.12393295768871551, 'validation/loss': 0.12494675597592039, 'validation/num_examples': 83274637, 'test/loss': 0.12751430553765547, 'test/num_examples': 95000000, 'score': 2649.84938287735, 'total_duration': 17319.695164442062, 'accumulated_submission_time': 2649.84938287735, 'accumulated_eval_time': 14649.842858076096, 'accumulated_logging_time': 0.544879674911499}
I0316 17:35:47.583241 140272851691264 logging_writer.py:48] [2686] accumulated_eval_time=14649.8, accumulated_logging_time=0.54488, accumulated_submission_time=2649.85, global_step=2686, preemption_count=0, score=2649.85, test/loss=0.127514, test/num_examples=95000000, total_duration=17319.7, train/loss=0.123933, validation/loss=0.124947, validation/num_examples=83274637
I0316 17:37:49.336323 140314632471744 spec.py:321] Evaluating on the training split.
I0316 17:39:52.398864 140314632471744 spec.py:333] Evaluating on the validation split.
I0316 17:41:54.896296 140314632471744 spec.py:349] Evaluating on the test split.
I0316 17:44:15.618004 140314632471744 submission_runner.py:469] Time since start: 17827.74s, 	Step: 2808, 	{'train/loss': 0.12390663669307753, 'validation/loss': 0.1252323258850652, 'validation/num_examples': 83274637, 'test/loss': 0.12785995702659206, 'test/num_examples': 95000000, 'score': 2770.716577529907, 'total_duration': 17827.74126815796, 'accumulated_submission_time': 2770.716577529907, 'accumulated_eval_time': 15036.124640226364, 'accumulated_logging_time': 0.565514326095581}
I0316 17:44:15.629883 140272860083968 logging_writer.py:48] [2808] accumulated_eval_time=15036.1, accumulated_logging_time=0.565514, accumulated_submission_time=2770.72, global_step=2808, preemption_count=0, score=2770.72, test/loss=0.12786, test/num_examples=95000000, total_duration=17827.7, train/loss=0.123907, validation/loss=0.125232, validation/num_examples=83274637
I0316 17:46:17.309094 140314632471744 spec.py:321] Evaluating on the training split.
I0316 17:48:20.923508 140314632471744 spec.py:333] Evaluating on the validation split.
I0316 17:50:23.397939 140314632471744 spec.py:349] Evaluating on the test split.
I0316 17:52:46.141857 140314632471744 submission_runner.py:469] Time since start: 18338.27s, 	Step: 2931, 	{'train/loss': 0.1251271537923938, 'validation/loss': 0.1251822659749817, 'validation/num_examples': 83274637, 'test/loss': 0.12780453318963303, 'test/num_examples': 95000000, 'score': 2891.517518758774, 'total_duration': 18338.265108823776, 'accumulated_submission_time': 2891.517518758774, 'accumulated_eval_time': 15424.957515239716, 'accumulated_logging_time': 0.586129903793335}
I0316 17:52:46.153135 140272851691264 logging_writer.py:48] [2931] accumulated_eval_time=15425, accumulated_logging_time=0.58613, accumulated_submission_time=2891.52, global_step=2931, preemption_count=0, score=2891.52, test/loss=0.127805, test/num_examples=95000000, total_duration=18338.3, train/loss=0.125127, validation/loss=0.125182, validation/num_examples=83274637
I0316 17:53:41.798707 140272860083968 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.00607269, loss=0.123027
I0316 17:53:41.802633 140314632471744 submission.py:265] 3000) loss = 0.123, grad_norm = 0.006
I0316 17:54:47.140119 140314632471744 spec.py:321] Evaluating on the training split.
I0316 17:56:50.712335 140314632471744 spec.py:333] Evaluating on the validation split.
I0316 17:58:54.214771 140314632471744 spec.py:349] Evaluating on the test split.
I0316 18:01:16.932414 140314632471744 submission_runner.py:469] Time since start: 18849.06s, 	Step: 3053, 	{'train/loss': 0.12308701991893098, 'validation/loss': 0.12505920066851514, 'validation/num_examples': 83274637, 'test/loss': 0.1275395722200494, 'test/num_examples': 95000000, 'score': 3011.616305589676, 'total_duration': 18849.05565881729, 'accumulated_submission_time': 3011.616305589676, 'accumulated_eval_time': 15814.7499563694, 'accumulated_logging_time': 0.6048295497894287}
I0316 18:01:16.967455 140272851691264 logging_writer.py:48] [3053] accumulated_eval_time=15814.7, accumulated_logging_time=0.60483, accumulated_submission_time=3011.62, global_step=3053, preemption_count=0, score=3011.62, test/loss=0.12754, test/num_examples=95000000, total_duration=18849.1, train/loss=0.123087, validation/loss=0.125059, validation/num_examples=83274637
I0316 18:03:17.572354 140314632471744 spec.py:321] Evaluating on the training split.
I0316 18:05:20.518758 140314632471744 spec.py:333] Evaluating on the validation split.
I0316 18:07:24.200240 140314632471744 spec.py:349] Evaluating on the test split.
I0316 18:09:46.435028 140314632471744 submission_runner.py:469] Time since start: 19358.56s, 	Step: 3177, 	{'train/loss': 0.12427540939542557, 'validation/loss': 0.12500282603377724, 'validation/num_examples': 83274637, 'test/loss': 0.12737891883922375, 'test/num_examples': 95000000, 'score': 3131.306389093399, 'total_duration': 19358.558275938034, 'accumulated_submission_time': 3131.306389093399, 'accumulated_eval_time': 16203.612781763077, 'accumulated_logging_time': 0.6612179279327393}
I0316 18:09:46.446175 140272860083968 logging_writer.py:48] [3177] accumulated_eval_time=16203.6, accumulated_logging_time=0.661218, accumulated_submission_time=3131.31, global_step=3177, preemption_count=0, score=3131.31, test/loss=0.127379, test/num_examples=95000000, total_duration=19358.6, train/loss=0.124275, validation/loss=0.125003, validation/num_examples=83274637
I0316 18:11:47.397452 140314632471744 spec.py:321] Evaluating on the training split.
I0316 18:13:50.778776 140314632471744 spec.py:333] Evaluating on the validation split.
I0316 18:15:54.066191 140314632471744 spec.py:349] Evaluating on the test split.
I0316 18:18:15.574239 140314632471744 submission_runner.py:469] Time since start: 19867.70s, 	Step: 3300, 	{'train/loss': 0.12350989964205428, 'validation/loss': 0.1246559085930856, 'validation/num_examples': 83274637, 'test/loss': 0.12709904177422773, 'test/num_examples': 95000000, 'score': 3251.3492827415466, 'total_duration': 19867.697481393814, 'accumulated_submission_time': 3251.3492827415466, 'accumulated_eval_time': 16591.789635419846, 'accumulated_logging_time': 0.6795206069946289}
I0316 18:18:15.585790 140272851691264 logging_writer.py:48] [3300] accumulated_eval_time=16591.8, accumulated_logging_time=0.679521, accumulated_submission_time=3251.35, global_step=3300, preemption_count=0, score=3251.35, test/loss=0.127099, test/num_examples=95000000, total_duration=19867.7, train/loss=0.12351, validation/loss=0.124656, validation/num_examples=83274637
I0316 18:20:17.024991 140314632471744 spec.py:321] Evaluating on the training split.
I0316 18:22:21.109065 140314632471744 spec.py:333] Evaluating on the validation split.
I0316 18:24:25.199889 140314632471744 spec.py:349] Evaluating on the test split.
I0316 18:26:48.640469 140314632471744 submission_runner.py:469] Time since start: 20380.76s, 	Step: 3421, 	{'train/loss': 0.12291189380094171, 'validation/loss': 0.12485812227426817, 'validation/num_examples': 83274637, 'test/loss': 0.12729886905830784, 'test/num_examples': 95000000, 'score': 3371.8770208358765, 'total_duration': 20380.763732910156, 'accumulated_submission_time': 3371.8770208358765, 'accumulated_eval_time': 16983.4053440094, 'accumulated_logging_time': 0.6980915069580078}
I0316 18:26:48.652657 140272860083968 logging_writer.py:48] [3421] accumulated_eval_time=16983.4, accumulated_logging_time=0.698092, accumulated_submission_time=3371.88, global_step=3421, preemption_count=0, score=3371.88, test/loss=0.127299, test/num_examples=95000000, total_duration=20380.8, train/loss=0.122912, validation/loss=0.124858, validation/num_examples=83274637
I0316 18:27:53.279016 140272851691264 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.00608879, loss=0.114561
I0316 18:27:53.282224 140314632471744 submission.py:265] 3500) loss = 0.115, grad_norm = 0.006
I0316 18:28:49.304971 140314632471744 spec.py:321] Evaluating on the training split.
I0316 18:30:51.860532 140314632471744 spec.py:333] Evaluating on the validation split.
I0316 18:32:53.919237 140314632471744 spec.py:349] Evaluating on the test split.
I0316 18:35:16.383250 140314632471744 submission_runner.py:469] Time since start: 20888.51s, 	Step: 3547, 	{'train/loss': 0.12342394231312921, 'validation/loss': 0.12491516923598767, 'validation/num_examples': 83274637, 'test/loss': 0.12747071575951827, 'test/num_examples': 95000000, 'score': 3491.616428375244, 'total_duration': 20888.50651907921, 'accumulated_submission_time': 3491.616428375244, 'accumulated_eval_time': 17370.483756780624, 'accumulated_logging_time': 0.717442512512207}
I0316 18:35:16.415622 140272860083968 logging_writer.py:48] [3547] accumulated_eval_time=17370.5, accumulated_logging_time=0.717443, accumulated_submission_time=3491.62, global_step=3547, preemption_count=0, score=3491.62, test/loss=0.127471, test/num_examples=95000000, total_duration=20888.5, train/loss=0.123424, validation/loss=0.124915, validation/num_examples=83274637
I0316 18:37:16.918755 140314632471744 spec.py:321] Evaluating on the training split.
I0316 18:39:20.432110 140314632471744 spec.py:333] Evaluating on the validation split.
I0316 18:41:23.945785 140314632471744 spec.py:349] Evaluating on the test split.
I0316 18:43:45.656373 140314632471744 submission_runner.py:469] Time since start: 21397.78s, 	Step: 3669, 	{'train/loss': 0.12147182959631912, 'validation/loss': 0.12496924203127148, 'validation/num_examples': 83274637, 'test/loss': 0.127382368553523, 'test/num_examples': 95000000, 'score': 3611.23486495018, 'total_duration': 21397.779643297195, 'accumulated_submission_time': 3611.23486495018, 'accumulated_eval_time': 17759.22155022621, 'accumulated_logging_time': 0.7570104598999023}
I0316 18:43:45.667669 140272851691264 logging_writer.py:48] [3669] accumulated_eval_time=17759.2, accumulated_logging_time=0.75701, accumulated_submission_time=3611.23, global_step=3669, preemption_count=0, score=3611.23, test/loss=0.127382, test/num_examples=95000000, total_duration=21397.8, train/loss=0.121472, validation/loss=0.124969, validation/num_examples=83274637
I0316 18:45:46.421594 140314632471744 spec.py:321] Evaluating on the training split.
I0316 18:47:50.155782 140314632471744 spec.py:333] Evaluating on the validation split.
I0316 18:49:52.745131 140314632471744 spec.py:349] Evaluating on the test split.
I0316 18:52:15.205821 140314632471744 submission_runner.py:469] Time since start: 21907.33s, 	Step: 3792, 	{'train/loss': 0.12397411962279974, 'validation/loss': 0.12524913736391968, 'validation/num_examples': 83274637, 'test/loss': 0.12768075185699462, 'test/num_examples': 95000000, 'score': 3731.0944232940674, 'total_duration': 21907.32909178734, 'accumulated_submission_time': 3731.0944232940674, 'accumulated_eval_time': 18148.0059568882, 'accumulated_logging_time': 0.7993669509887695}
I0316 18:52:15.216680 140272860083968 logging_writer.py:48] [3792] accumulated_eval_time=18148, accumulated_logging_time=0.799367, accumulated_submission_time=3731.09, global_step=3792, preemption_count=0, score=3731.09, test/loss=0.127681, test/num_examples=95000000, total_duration=21907.3, train/loss=0.123974, validation/loss=0.125249, validation/num_examples=83274637
I0316 18:54:16.004390 140314632471744 spec.py:321] Evaluating on the training split.
I0316 18:56:19.791636 140314632471744 spec.py:333] Evaluating on the validation split.
I0316 18:58:22.923680 140314632471744 spec.py:349] Evaluating on the test split.
I0316 19:00:45.106006 140314632471744 submission_runner.py:469] Time since start: 22417.23s, 	Step: 3914, 	{'train/loss': 0.12272060813520418, 'validation/loss': 0.12469813042420463, 'validation/num_examples': 83274637, 'test/loss': 0.12706990350655004, 'test/num_examples': 95000000, 'score': 3851.0231878757477, 'total_duration': 22417.229254722595, 'accumulated_submission_time': 3851.0231878757477, 'accumulated_eval_time': 18537.10766172409, 'accumulated_logging_time': 0.8173232078552246}
I0316 19:00:45.117059 140272851691264 logging_writer.py:48] [3914] accumulated_eval_time=18537.1, accumulated_logging_time=0.817323, accumulated_submission_time=3851.02, global_step=3914, preemption_count=0, score=3851.02, test/loss=0.12707, test/num_examples=95000000, total_duration=22417.2, train/loss=0.122721, validation/loss=0.124698, validation/num_examples=83274637
I0316 19:02:01.677049 140272860083968 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.00729011, loss=0.126197
I0316 19:02:01.680865 140314632471744 submission.py:265] 4000) loss = 0.126, grad_norm = 0.007
I0316 19:02:46.118727 140314632471744 spec.py:321] Evaluating on the training split.
I0316 19:04:50.060857 140314632471744 spec.py:333] Evaluating on the validation split.
I0316 19:06:54.362042 140314632471744 spec.py:349] Evaluating on the test split.
I0316 19:09:17.615303 140314632471744 submission_runner.py:469] Time since start: 22929.74s, 	Step: 4037, 	{'train/loss': 0.12208169161161793, 'validation/loss': 0.12467975027801091, 'validation/num_examples': 83274637, 'test/loss': 0.1269883765710128, 'test/num_examples': 95000000, 'score': 3971.104371547699, 'total_duration': 22929.73856472969, 'accumulated_submission_time': 3971.104371547699, 'accumulated_eval_time': 18928.60440993309, 'accumulated_logging_time': 0.8351976871490479}
I0316 19:09:17.654293 140272851691264 logging_writer.py:48] [4037] accumulated_eval_time=18928.6, accumulated_logging_time=0.835198, accumulated_submission_time=3971.1, global_step=4037, preemption_count=0, score=3971.1, test/loss=0.126988, test/num_examples=95000000, total_duration=22929.7, train/loss=0.122082, validation/loss=0.12468, validation/num_examples=83274637
I0316 19:11:19.087808 140314632471744 spec.py:321] Evaluating on the training split.
I0316 19:13:22.711535 140314632471744 spec.py:333] Evaluating on the validation split.
I0316 19:15:26.350220 140314632471744 spec.py:349] Evaluating on the test split.
I0316 19:17:48.483946 140314632471744 submission_runner.py:469] Time since start: 23440.61s, 	Step: 4161, 	{'train/loss': 0.12201703645110486, 'validation/loss': 0.12446358006634335, 'validation/num_examples': 83274637, 'test/loss': 0.12689819835056507, 'test/num_examples': 95000000, 'score': 4091.6834394931793, 'total_duration': 23440.607169628143, 'accumulated_submission_time': 4091.6834394931793, 'accumulated_eval_time': 19318.0007147789, 'accumulated_logging_time': 0.8818163871765137}
I0316 19:17:48.495544 140272860083968 logging_writer.py:48] [4161] accumulated_eval_time=19318, accumulated_logging_time=0.881816, accumulated_submission_time=4091.68, global_step=4161, preemption_count=0, score=4091.68, test/loss=0.126898, test/num_examples=95000000, total_duration=23440.6, train/loss=0.122017, validation/loss=0.124464, validation/num_examples=83274637
I0316 19:19:49.480706 140314632471744 spec.py:321] Evaluating on the training split.
I0316 19:21:52.680810 140314632471744 spec.py:333] Evaluating on the validation split.
I0316 19:23:56.273669 140314632471744 spec.py:349] Evaluating on the test split.
I0316 19:26:17.582112 140314632471744 submission_runner.py:469] Time since start: 23949.71s, 	Step: 4283, 	{'train/loss': 0.12174455662325395, 'validation/loss': 0.12466634198968637, 'validation/num_examples': 83274637, 'test/loss': 0.12712773138371516, 'test/num_examples': 95000000, 'score': 4211.76550245285, 'total_duration': 23949.705335378647, 'accumulated_submission_time': 4211.76550245285, 'accumulated_eval_time': 19706.102294445038, 'accumulated_logging_time': 0.9561691284179688}
I0316 19:26:17.593332 140272851691264 logging_writer.py:48] [4283] accumulated_eval_time=19706.1, accumulated_logging_time=0.956169, accumulated_submission_time=4211.77, global_step=4283, preemption_count=0, score=4211.77, test/loss=0.127128, test/num_examples=95000000, total_duration=23949.7, train/loss=0.121745, validation/loss=0.124666, validation/num_examples=83274637
I0316 19:28:18.299772 140314632471744 spec.py:321] Evaluating on the training split.
I0316 19:30:22.320631 140314632471744 spec.py:333] Evaluating on the validation split.
I0316 19:32:25.406905 140314632471744 spec.py:349] Evaluating on the test split.
I0316 19:34:47.271926 140314632471744 submission_runner.py:469] Time since start: 24459.40s, 	Step: 4404, 	{'train/loss': 0.121421056670739, 'validation/loss': 0.12480549779834164, 'validation/num_examples': 83274637, 'test/loss': 0.12728312139611495, 'test/num_examples': 95000000, 'score': 4331.5587911605835, 'total_duration': 24459.395176887512, 'accumulated_submission_time': 4331.5587911605835, 'accumulated_eval_time': 20095.074667453766, 'accumulated_logging_time': 0.9746382236480713}
I0316 19:34:47.283673 140272860083968 logging_writer.py:48] [4404] accumulated_eval_time=20095.1, accumulated_logging_time=0.974638, accumulated_submission_time=4331.56, global_step=4404, preemption_count=0, score=4331.56, test/loss=0.127283, test/num_examples=95000000, total_duration=24459.4, train/loss=0.121421, validation/loss=0.124805, validation/num_examples=83274637
I0316 19:36:14.942570 140272851691264 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.0154716, loss=0.118061
I0316 19:36:14.946022 140314632471744 submission.py:265] 4500) loss = 0.118, grad_norm = 0.015
I0316 19:36:47.728324 140314632471744 spec.py:321] Evaluating on the training split.
I0316 19:38:51.196289 140314632471744 spec.py:333] Evaluating on the validation split.
I0316 19:40:55.726279 140314632471744 spec.py:349] Evaluating on the test split.
I0316 19:43:18.033085 140314632471744 submission_runner.py:469] Time since start: 24970.16s, 	Step: 4527, 	{'train/loss': 0.12419337521076493, 'validation/loss': 0.1245660732537624, 'validation/num_examples': 83274637, 'test/loss': 0.12691536974603754, 'test/num_examples': 95000000, 'score': 4451.133327960968, 'total_duration': 24970.15636229515, 'accumulated_submission_time': 4451.133327960968, 'accumulated_eval_time': 20485.37953066826, 'accumulated_logging_time': 0.9936356544494629}
I0316 19:43:18.044788 140272860083968 logging_writer.py:48] [4527] accumulated_eval_time=20485.4, accumulated_logging_time=0.993636, accumulated_submission_time=4451.13, global_step=4527, preemption_count=0, score=4451.13, test/loss=0.126915, test/num_examples=95000000, total_duration=24970.2, train/loss=0.124193, validation/loss=0.124566, validation/num_examples=83274637
I0316 19:45:19.919762 140314632471744 spec.py:321] Evaluating on the training split.
I0316 19:47:23.559119 140314632471744 spec.py:333] Evaluating on the validation split.
I0316 19:49:28.105106 140314632471744 spec.py:349] Evaluating on the test split.
I0316 19:51:50.110269 140314632471744 submission_runner.py:469] Time since start: 25482.23s, 	Step: 4646, 	{'train/loss': 0.12101795779973175, 'validation/loss': 0.12464252100884302, 'validation/num_examples': 83274637, 'test/loss': 0.12711420612431576, 'test/num_examples': 95000000, 'score': 4572.162162065506, 'total_duration': 25482.233493328094, 'accumulated_submission_time': 4572.162162065506, 'accumulated_eval_time': 20875.57020521164, 'accumulated_logging_time': 1.0122261047363281}
I0316 19:51:50.122368 140272851691264 logging_writer.py:48] [4646] accumulated_eval_time=20875.6, accumulated_logging_time=1.01223, accumulated_submission_time=4572.16, global_step=4646, preemption_count=0, score=4572.16, test/loss=0.127114, test/num_examples=95000000, total_duration=25482.2, train/loss=0.121018, validation/loss=0.124643, validation/num_examples=83274637
I0316 19:53:50.484768 140314632471744 spec.py:321] Evaluating on the training split.
I0316 19:55:54.160997 140314632471744 spec.py:333] Evaluating on the validation split.
I0316 19:57:57.642136 140314632471744 spec.py:349] Evaluating on the test split.
I0316 20:00:20.233185 140314632471744 submission_runner.py:469] Time since start: 25992.36s, 	Step: 4769, 	{'train/loss': 0.12296707942325512, 'validation/loss': 0.1246650640407425, 'validation/num_examples': 83274637, 'test/loss': 0.1270691764832346, 'test/num_examples': 95000000, 'score': 4691.68408203125, 'total_duration': 25992.356389522552, 'accumulated_submission_time': 4691.68408203125, 'accumulated_eval_time': 21265.318603515625, 'accumulated_logging_time': 1.0570063591003418}
I0316 20:00:20.246456 140272860083968 logging_writer.py:48] [4769] accumulated_eval_time=21265.3, accumulated_logging_time=1.05701, accumulated_submission_time=4691.68, global_step=4769, preemption_count=0, score=4691.68, test/loss=0.127069, test/num_examples=95000000, total_duration=25992.4, train/loss=0.122967, validation/loss=0.124665, validation/num_examples=83274637
I0316 20:02:20.699300 140314632471744 spec.py:321] Evaluating on the training split.
I0316 20:04:23.822947 140314632471744 spec.py:333] Evaluating on the validation split.
I0316 20:06:27.270653 140314632471744 spec.py:349] Evaluating on the test split.
I0316 20:08:47.789497 140314632471744 submission_runner.py:469] Time since start: 26499.91s, 	Step: 4889, 	{'train/loss': 0.12235128447461449, 'validation/loss': 0.12464636718983224, 'validation/num_examples': 83274637, 'test/loss': 0.12717848324568898, 'test/num_examples': 95000000, 'score': 4811.2539319992065, 'total_duration': 26499.91275715828, 'accumulated_submission_time': 4811.2539319992065, 'accumulated_eval_time': 21652.40898513794, 'accumulated_logging_time': 1.0771362781524658}
I0316 20:08:47.801351 140272851691264 logging_writer.py:48] [4889] accumulated_eval_time=21652.4, accumulated_logging_time=1.07714, accumulated_submission_time=4811.25, global_step=4889, preemption_count=0, score=4811.25, test/loss=0.127178, test/num_examples=95000000, total_duration=26499.9, train/loss=0.122351, validation/loss=0.124646, validation/num_examples=83274637
I0316 20:10:37.109905 140272860083968 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.00957658, loss=0.122006
I0316 20:10:37.114026 140314632471744 submission.py:265] 5000) loss = 0.122, grad_norm = 0.010
I0316 20:10:48.678181 140314632471744 spec.py:321] Evaluating on the training split.
I0316 20:12:52.219868 140314632471744 spec.py:333] Evaluating on the validation split.
I0316 20:14:55.320380 140314632471744 spec.py:349] Evaluating on the test split.
I0316 20:17:17.428458 140314632471744 submission_runner.py:469] Time since start: 27009.55s, 	Step: 5010, 	{'train/loss': 0.12137727418016828, 'validation/loss': 0.12464187550922909, 'validation/num_examples': 83274637, 'test/loss': 0.12700683076934816, 'test/num_examples': 95000000, 'score': 4931.263594388962, 'total_duration': 27009.55172228813, 'accumulated_submission_time': 4931.263594388962, 'accumulated_eval_time': 22041.159314393997, 'accumulated_logging_time': 1.095839500427246}
I0316 20:17:17.440664 140272851691264 logging_writer.py:48] [5010] accumulated_eval_time=22041.2, accumulated_logging_time=1.09584, accumulated_submission_time=4931.26, global_step=5010, preemption_count=0, score=4931.26, test/loss=0.127007, test/num_examples=95000000, total_duration=27009.6, train/loss=0.121377, validation/loss=0.124642, validation/num_examples=83274637
I0316 20:19:18.252351 140314632471744 spec.py:321] Evaluating on the training split.
I0316 20:21:20.993000 140314632471744 spec.py:333] Evaluating on the validation split.
I0316 20:23:23.137331 140314632471744 spec.py:349] Evaluating on the test split.
I0316 20:25:44.848217 140314632471744 submission_runner.py:469] Time since start: 27516.97s, 	Step: 5131, 	{'train/loss': 0.12375737924889879, 'validation/loss': 0.12455464018879057, 'validation/num_examples': 83274637, 'test/loss': 0.12689164422117535, 'test/num_examples': 95000000, 'score': 5051.223951339722, 'total_duration': 27516.97146844864, 'accumulated_submission_time': 5051.223951339722, 'accumulated_eval_time': 22427.75536632538, 'accumulated_logging_time': 1.115391492843628}
I0316 20:25:44.860256 140272860083968 logging_writer.py:48] [5131] accumulated_eval_time=22427.8, accumulated_logging_time=1.11539, accumulated_submission_time=5051.22, global_step=5131, preemption_count=0, score=5051.22, test/loss=0.126892, test/num_examples=95000000, total_duration=27517, train/loss=0.123757, validation/loss=0.124555, validation/num_examples=83274637
I0316 20:27:45.842582 140314632471744 spec.py:321] Evaluating on the training split.
I0316 20:29:49.680632 140314632471744 spec.py:333] Evaluating on the validation split.
I0316 20:31:53.258065 140314632471744 spec.py:349] Evaluating on the test split.
I0316 20:34:15.554332 140314632471744 submission_runner.py:469] Time since start: 28027.68s, 	Step: 5251, 	{'train/loss': 0.1231182268605416, 'validation/loss': 0.12442298568227715, 'validation/num_examples': 83274637, 'test/loss': 0.12691711902959724, 'test/num_examples': 95000000, 'score': 5171.286344766617, 'total_duration': 28027.677562236786, 'accumulated_submission_time': 5171.286344766617, 'accumulated_eval_time': 22817.467128753662, 'accumulated_logging_time': 1.220468282699585}
I0316 20:34:15.566762 140272851691264 logging_writer.py:48] [5251] accumulated_eval_time=22817.5, accumulated_logging_time=1.22047, accumulated_submission_time=5171.29, global_step=5251, preemption_count=0, score=5171.29, test/loss=0.126917, test/num_examples=95000000, total_duration=28027.7, train/loss=0.123118, validation/loss=0.124423, validation/num_examples=83274637
I0316 20:36:16.158539 140314632471744 spec.py:321] Evaluating on the training split.
I0316 20:38:19.499942 140314632471744 spec.py:333] Evaluating on the validation split.
I0316 20:40:23.064448 140314632471744 spec.py:349] Evaluating on the test split.
I0316 20:42:44.352291 140314632471744 submission_runner.py:469] Time since start: 28536.48s, 	Step: 5374, 	{'train/loss': 0.12281960833217848, 'validation/loss': 0.12432266427224319, 'validation/num_examples': 83274637, 'test/loss': 0.12666418521258704, 'test/num_examples': 95000000, 'score': 5290.962339878082, 'total_duration': 28536.475568294525, 'accumulated_submission_time': 5290.962339878082, 'accumulated_eval_time': 23205.661076545715, 'accumulated_logging_time': 1.2400925159454346}
I0316 20:42:44.364485 140272860083968 logging_writer.py:48] [5374] accumulated_eval_time=23205.7, accumulated_logging_time=1.24009, accumulated_submission_time=5290.96, global_step=5374, preemption_count=0, score=5290.96, test/loss=0.126664, test/num_examples=95000000, total_duration=28536.5, train/loss=0.12282, validation/loss=0.124323, validation/num_examples=83274637
I0316 20:44:44.882259 140314632471744 spec.py:321] Evaluating on the training split.
I0316 20:46:48.796135 140314632471744 spec.py:333] Evaluating on the validation split.
I0316 20:48:52.371417 140314632471744 spec.py:349] Evaluating on the test split.
I0316 20:51:14.975376 140314632471744 submission_runner.py:469] Time since start: 29047.10s, 	Step: 5496, 	{'train/loss': 0.12215018099038868, 'validation/loss': 0.12428967900400527, 'validation/num_examples': 83274637, 'test/loss': 0.12670933237360904, 'test/num_examples': 95000000, 'score': 5410.650941371918, 'total_duration': 29047.098650693893, 'accumulated_submission_time': 5410.650941371918, 'accumulated_eval_time': 23595.754348278046, 'accumulated_logging_time': 1.259819746017456}
I0316 20:51:14.987284 140272851691264 logging_writer.py:48] [5496] accumulated_eval_time=23595.8, accumulated_logging_time=1.25982, accumulated_submission_time=5410.65, global_step=5496, preemption_count=0, score=5410.65, test/loss=0.126709, test/num_examples=95000000, total_duration=29047.1, train/loss=0.12215, validation/loss=0.12429, validation/num_examples=83274637
I0316 20:51:16.396108 140272860083968 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.0202821, loss=0.126001
I0316 20:51:16.399425 140314632471744 submission.py:265] 5500) loss = 0.126, grad_norm = 0.020
I0316 20:53:15.583667 140314632471744 spec.py:321] Evaluating on the training split.
I0316 20:55:19.612372 140314632471744 spec.py:333] Evaluating on the validation split.
I0316 20:57:22.938271 140314632471744 spec.py:349] Evaluating on the test split.
I0316 20:59:45.030508 140314632471744 submission_runner.py:469] Time since start: 29557.15s, 	Step: 5620, 	{'train/loss': 0.12257585450978489, 'validation/loss': 0.12427999062540275, 'validation/num_examples': 83274637, 'test/loss': 0.1266171048938952, 'test/num_examples': 95000000, 'score': 5530.398602247238, 'total_duration': 29557.153762578964, 'accumulated_submission_time': 5530.398602247238, 'accumulated_eval_time': 23985.201287269592, 'accumulated_logging_time': 1.2788465023040771}
I0316 20:59:45.042892 140272851691264 logging_writer.py:48] [5620] accumulated_eval_time=23985.2, accumulated_logging_time=1.27885, accumulated_submission_time=5530.4, global_step=5620, preemption_count=0, score=5530.4, test/loss=0.126617, test/num_examples=95000000, total_duration=29557.2, train/loss=0.122576, validation/loss=0.12428, validation/num_examples=83274637
I0316 21:01:45.614677 140314632471744 spec.py:321] Evaluating on the training split.
I0316 21:03:49.204205 140314632471744 spec.py:333] Evaluating on the validation split.
I0316 21:05:52.631479 140314632471744 spec.py:349] Evaluating on the test split.
I0316 21:08:14.565130 140314632471744 submission_runner.py:469] Time since start: 30066.69s, 	Step: 5739, 	{'train/loss': 0.12117027378923782, 'validation/loss': 0.12436541189503242, 'validation/num_examples': 83274637, 'test/loss': 0.12693682418602392, 'test/num_examples': 95000000, 'score': 5650.1432592868805, 'total_duration': 30066.6884226799, 'accumulated_submission_time': 5650.1432592868805, 'accumulated_eval_time': 24374.15184855461, 'accumulated_logging_time': 1.298029899597168}
I0316 21:08:14.591955 140272860083968 logging_writer.py:48] [5739] accumulated_eval_time=24374.2, accumulated_logging_time=1.29803, accumulated_submission_time=5650.14, global_step=5739, preemption_count=0, score=5650.14, test/loss=0.126937, test/num_examples=95000000, total_duration=30066.7, train/loss=0.12117, validation/loss=0.124365, validation/num_examples=83274637
I0316 21:10:15.379090 140314632471744 spec.py:321] Evaluating on the training split.
I0316 21:12:19.135461 140314632471744 spec.py:333] Evaluating on the validation split.
I0316 21:14:22.042419 140314632471744 spec.py:349] Evaluating on the test split.
I0316 21:16:44.221983 140314632471744 submission_runner.py:469] Time since start: 30576.35s, 	Step: 5862, 	{'train/loss': 0.12405976619468476, 'validation/loss': 0.12431916739495108, 'validation/num_examples': 83274637, 'test/loss': 0.12682433585694966, 'test/num_examples': 95000000, 'score': 5770.048926353455, 'total_duration': 30576.345227718353, 'accumulated_submission_time': 5770.048926353455, 'accumulated_eval_time': 24762.99488377571, 'accumulated_logging_time': 1.3469254970550537}
I0316 21:16:44.234233 140272851691264 logging_writer.py:48] [5862] accumulated_eval_time=24763, accumulated_logging_time=1.34693, accumulated_submission_time=5770.05, global_step=5862, preemption_count=0, score=5770.05, test/loss=0.126824, test/num_examples=95000000, total_duration=30576.3, train/loss=0.12406, validation/loss=0.124319, validation/num_examples=83274637
I0316 21:18:45.661554 140314632471744 spec.py:321] Evaluating on the training split.
I0316 21:20:48.786486 140314632471744 spec.py:333] Evaluating on the validation split.
I0316 21:22:51.688519 140314632471744 spec.py:349] Evaluating on the test split.
I0316 21:25:14.200298 140314632471744 submission_runner.py:469] Time since start: 31086.32s, 	Step: 5983, 	{'train/loss': 0.12228932108937303, 'validation/loss': 0.12432019993843717, 'validation/num_examples': 83274637, 'test/loss': 0.1267663585537559, 'test/num_examples': 95000000, 'score': 5890.600004673004, 'total_duration': 31086.323573589325, 'accumulated_submission_time': 5890.600004673004, 'accumulated_eval_time': 25151.533830165863, 'accumulated_logging_time': 1.366347074508667}
I0316 21:25:14.212814 140272860083968 logging_writer.py:48] [5983] accumulated_eval_time=25151.5, accumulated_logging_time=1.36635, accumulated_submission_time=5890.6, global_step=5983, preemption_count=0, score=5890.6, test/loss=0.126766, test/num_examples=95000000, total_duration=31086.3, train/loss=0.122289, validation/loss=0.12432, validation/num_examples=83274637
I0316 21:25:18.101601 140272851691264 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.0313296, loss=0.122509
I0316 21:25:18.104916 140314632471744 submission.py:265] 6000) loss = 0.123, grad_norm = 0.031
I0316 21:27:15.045408 140314632471744 spec.py:321] Evaluating on the training split.
I0316 21:29:18.780875 140314632471744 spec.py:333] Evaluating on the validation split.
I0316 21:31:22.672798 140314632471744 spec.py:349] Evaluating on the test split.
I0316 21:33:46.268596 140314632471744 submission_runner.py:469] Time since start: 31598.39s, 	Step: 6107, 	{'train/loss': 0.12189471967864254, 'validation/loss': 0.1241132836685756, 'validation/num_examples': 83274637, 'test/loss': 0.12657543649597167, 'test/num_examples': 95000000, 'score': 6010.596944570541, 'total_duration': 31598.391882896423, 'accumulated_submission_time': 6010.596944570541, 'accumulated_eval_time': 25542.7571041584, 'accumulated_logging_time': 1.3860409259796143}
I0316 21:33:46.281382 140272860083968 logging_writer.py:48] [6107] accumulated_eval_time=25542.8, accumulated_logging_time=1.38604, accumulated_submission_time=6010.6, global_step=6107, preemption_count=0, score=6010.6, test/loss=0.126575, test/num_examples=95000000, total_duration=31598.4, train/loss=0.121895, validation/loss=0.124113, validation/num_examples=83274637
I0316 21:35:46.935552 140314632471744 spec.py:321] Evaluating on the training split.
I0316 21:37:50.470356 140314632471744 spec.py:333] Evaluating on the validation split.
I0316 21:39:54.126152 140314632471744 spec.py:349] Evaluating on the test split.
I0316 21:42:15.901636 140314632471744 submission_runner.py:469] Time since start: 32108.02s, 	Step: 6226, 	{'train/loss': 0.12248965254030204, 'validation/loss': 0.12432265918702687, 'validation/num_examples': 83274637, 'test/loss': 0.12678470596835487, 'test/num_examples': 95000000, 'score': 6130.359548330307, 'total_duration': 32108.024787425995, 'accumulated_submission_time': 6130.359548330307, 'accumulated_eval_time': 25931.72327876091, 'accumulated_logging_time': 1.4055116176605225}
I0316 21:42:15.915901 140272851691264 logging_writer.py:48] [6226] accumulated_eval_time=25931.7, accumulated_logging_time=1.40551, accumulated_submission_time=6130.36, global_step=6226, preemption_count=0, score=6130.36, test/loss=0.126785, test/num_examples=95000000, total_duration=32108, train/loss=0.12249, validation/loss=0.124323, validation/num_examples=83274637
I0316 21:44:16.842594 140314632471744 spec.py:321] Evaluating on the training split.
I0316 21:46:20.203731 140314632471744 spec.py:333] Evaluating on the validation split.
I0316 21:48:22.184697 140314632471744 spec.py:349] Evaluating on the test split.
I0316 21:50:43.592892 140314632471744 submission_runner.py:469] Time since start: 32615.72s, 	Step: 6347, 	{'train/loss': 0.12282445139118629, 'validation/loss': 0.1240554339377771, 'validation/num_examples': 83274637, 'test/loss': 0.12655359052168194, 'test/num_examples': 95000000, 'score': 6250.366380691528, 'total_duration': 32615.716176748276, 'accumulated_submission_time': 6250.366380691528, 'accumulated_eval_time': 26318.473910331726, 'accumulated_logging_time': 1.4273009300231934}
I0316 21:50:43.605397 140272860083968 logging_writer.py:48] [6347] accumulated_eval_time=26318.5, accumulated_logging_time=1.4273, accumulated_submission_time=6250.37, global_step=6347, preemption_count=0, score=6250.37, test/loss=0.126554, test/num_examples=95000000, total_duration=32615.7, train/loss=0.122824, validation/loss=0.124055, validation/num_examples=83274637
I0316 21:52:44.395572 140314632471744 spec.py:321] Evaluating on the training split.
I0316 21:54:47.747925 140314632471744 spec.py:333] Evaluating on the validation split.
I0316 21:56:51.368307 140314632471744 spec.py:349] Evaluating on the test split.
I0316 21:59:13.596296 140314632471744 submission_runner.py:469] Time since start: 33125.72s, 	Step: 6469, 	{'train/loss': 0.12250038614281668, 'validation/loss': 0.12413038120314795, 'validation/num_examples': 83274637, 'test/loss': 0.12661481474007055, 'test/num_examples': 95000000, 'score': 6370.295336961746, 'total_duration': 33125.71952915192, 'accumulated_submission_time': 6370.295336961746, 'accumulated_eval_time': 26707.674695968628, 'accumulated_logging_time': 1.4667718410491943}
I0316 21:59:13.620506 140272851691264 logging_writer.py:48] [6469] accumulated_eval_time=26707.7, accumulated_logging_time=1.46677, accumulated_submission_time=6370.3, global_step=6469, preemption_count=0, score=6370.3, test/loss=0.126615, test/num_examples=95000000, total_duration=33125.7, train/loss=0.1225, validation/loss=0.12413, validation/num_examples=83274637
I0316 21:59:22.688519 140272860083968 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.00760664, loss=0.13323
I0316 21:59:22.691978 140314632471744 submission.py:265] 6500) loss = 0.133, grad_norm = 0.008
I0316 22:01:15.124163 140314632471744 spec.py:321] Evaluating on the training split.
I0316 22:03:18.727556 140314632471744 spec.py:333] Evaluating on the validation split.
I0316 22:05:22.209416 140314632471744 spec.py:349] Evaluating on the test split.
I0316 22:07:42.971440 140314632471744 submission_runner.py:469] Time since start: 33635.09s, 	Step: 6594, 	{'train/loss': 0.12272806245845179, 'validation/loss': 0.12413568151500666, 'validation/num_examples': 83274637, 'test/loss': 0.12658575419564497, 'test/num_examples': 95000000, 'score': 6490.896236419678, 'total_duration': 33635.094723939896, 'accumulated_submission_time': 6490.896236419678, 'accumulated_eval_time': 27095.522090435028, 'accumulated_logging_time': 1.4981791973114014}
I0316 22:07:42.983797 140272851691264 logging_writer.py:48] [6594] accumulated_eval_time=27095.5, accumulated_logging_time=1.49818, accumulated_submission_time=6490.9, global_step=6594, preemption_count=0, score=6490.9, test/loss=0.126586, test/num_examples=95000000, total_duration=33635.1, train/loss=0.122728, validation/loss=0.124136, validation/num_examples=83274637
I0316 22:09:43.456629 140314632471744 spec.py:321] Evaluating on the training split.
I0316 22:11:46.688006 140314632471744 spec.py:333] Evaluating on the validation split.
I0316 22:13:49.182931 140314632471744 spec.py:349] Evaluating on the test split.
I0316 22:16:10.612053 140314632471744 submission_runner.py:469] Time since start: 34142.74s, 	Step: 6716, 	{'train/loss': 0.12023109503475334, 'validation/loss': 0.12419262113262405, 'validation/num_examples': 83274637, 'test/loss': 0.1267059837345324, 'test/num_examples': 95000000, 'score': 6610.495177984238, 'total_duration': 34142.73532629013, 'accumulated_submission_time': 6610.495177984238, 'accumulated_eval_time': 27482.67778801918, 'accumulated_logging_time': 1.517615795135498}
I0316 22:16:10.624337 140272860083968 logging_writer.py:48] [6716] accumulated_eval_time=27482.7, accumulated_logging_time=1.51762, accumulated_submission_time=6610.5, global_step=6716, preemption_count=0, score=6610.5, test/loss=0.126706, test/num_examples=95000000, total_duration=34142.7, train/loss=0.120231, validation/loss=0.124193, validation/num_examples=83274637
I0316 22:18:11.640999 140314632471744 spec.py:321] Evaluating on the training split.
I0316 22:20:15.351236 140314632471744 spec.py:333] Evaluating on the validation split.
I0316 22:22:18.897681 140314632471744 spec.py:349] Evaluating on the test split.
I0316 22:24:40.965840 140314632471744 submission_runner.py:469] Time since start: 34653.09s, 	Step: 6838, 	{'train/loss': 0.12284094374086367, 'validation/loss': 0.1240780483780515, 'validation/num_examples': 83274637, 'test/loss': 0.1265274558647156, 'test/num_examples': 95000000, 'score': 6730.622112035751, 'total_duration': 34653.08913254738, 'accumulated_submission_time': 6730.622112035751, 'accumulated_eval_time': 27872.00283575058, 'accumulated_logging_time': 1.537109375}
I0316 22:24:40.978608 140272851691264 logging_writer.py:48] [6838] accumulated_eval_time=27872, accumulated_logging_time=1.53711, accumulated_submission_time=6730.62, global_step=6838, preemption_count=0, score=6730.62, test/loss=0.126527, test/num_examples=95000000, total_duration=34653.1, train/loss=0.122841, validation/loss=0.124078, validation/num_examples=83274637
I0316 22:26:41.936669 140314632471744 spec.py:321] Evaluating on the training split.
I0316 22:28:45.424677 140314632471744 spec.py:333] Evaluating on the validation split.
I0316 22:30:49.038586 140314632471744 spec.py:349] Evaluating on the test split.
I0316 22:33:11.099435 140314632471744 submission_runner.py:469] Time since start: 35163.22s, 	Step: 6961, 	{'train/loss': 0.12402294331766883, 'validation/loss': 0.124137299844126, 'validation/num_examples': 83274637, 'test/loss': 0.1264878982873214, 'test/num_examples': 95000000, 'score': 6850.681242465973, 'total_duration': 35163.22269845009, 'accumulated_submission_time': 6850.681242465973, 'accumulated_eval_time': 28261.16577243805, 'accumulated_logging_time': 1.6001980304718018}
I0316 22:33:11.112330 140272860083968 logging_writer.py:48] [6961] accumulated_eval_time=28261.2, accumulated_logging_time=1.6002, accumulated_submission_time=6850.68, global_step=6961, preemption_count=0, score=6850.68, test/loss=0.126488, test/num_examples=95000000, total_duration=35163.2, train/loss=0.124023, validation/loss=0.124137, validation/num_examples=83274637
I0316 22:33:29.725113 140272851691264 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.0103443, loss=0.126868
I0316 22:33:29.728321 140314632471744 submission.py:265] 7000) loss = 0.127, grad_norm = 0.010
I0316 22:35:11.863212 140314632471744 spec.py:321] Evaluating on the training split.
I0316 22:37:15.399848 140314632471744 spec.py:333] Evaluating on the validation split.
I0316 22:39:18.788487 140314632471744 spec.py:349] Evaluating on the test split.
I0316 22:41:40.216555 140314632471744 submission_runner.py:469] Time since start: 35672.34s, 	Step: 7084, 	{'train/loss': 0.1225382819896908, 'validation/loss': 0.12394267848807544, 'validation/num_examples': 83274637, 'test/loss': 0.12632085663492304, 'test/num_examples': 95000000, 'score': 6970.557040214539, 'total_duration': 35672.3398103714, 'accumulated_submission_time': 6970.557040214539, 'accumulated_eval_time': 28649.519287586212, 'accumulated_logging_time': 1.6197123527526855}
I0316 22:41:40.229320 140272860083968 logging_writer.py:48] [7084] accumulated_eval_time=28649.5, accumulated_logging_time=1.61971, accumulated_submission_time=6970.56, global_step=7084, preemption_count=0, score=6970.56, test/loss=0.126321, test/num_examples=95000000, total_duration=35672.3, train/loss=0.122538, validation/loss=0.123943, validation/num_examples=83274637
I0316 22:43:41.513415 140314632471744 spec.py:321] Evaluating on the training split.
I0316 22:45:45.108447 140314632471744 spec.py:333] Evaluating on the validation split.
I0316 22:47:48.540133 140314632471744 spec.py:349] Evaluating on the test split.
I0316 22:50:10.308953 140314632471744 submission_runner.py:469] Time since start: 36182.43s, 	Step: 7205, 	{'train/loss': 0.12180562199132977, 'validation/loss': 0.12402680419868596, 'validation/num_examples': 83274637, 'test/loss': 0.12636186293575888, 'test/num_examples': 95000000, 'score': 7090.934632778168, 'total_duration': 36182.432220458984, 'accumulated_submission_time': 7090.934632778168, 'accumulated_eval_time': 29038.315056562424, 'accumulated_logging_time': 1.638972282409668}
I0316 22:50:10.321610 140272851691264 logging_writer.py:48] [7205] accumulated_eval_time=29038.3, accumulated_logging_time=1.63897, accumulated_submission_time=7090.93, global_step=7205, preemption_count=0, score=7090.93, test/loss=0.126362, test/num_examples=95000000, total_duration=36182.4, train/loss=0.121806, validation/loss=0.124027, validation/num_examples=83274637
I0316 22:52:10.730232 140314632471744 spec.py:321] Evaluating on the training split.
I0316 22:54:14.402360 140314632471744 spec.py:333] Evaluating on the validation split.
I0316 22:56:18.095065 140314632471744 spec.py:349] Evaluating on the test split.
I0316 22:58:40.623896 140314632471744 submission_runner.py:469] Time since start: 36692.75s, 	Step: 7327, 	{'train/loss': 0.12260507798422778, 'validation/loss': 0.12403043651254489, 'validation/num_examples': 83274637, 'test/loss': 0.12639017869664243, 'test/num_examples': 95000000, 'score': 7210.477641582489, 'total_duration': 36692.74712920189, 'accumulated_submission_time': 7210.477641582489, 'accumulated_eval_time': 29428.208776474, 'accumulated_logging_time': 1.6589267253875732}
I0316 22:58:40.636315 140272860083968 logging_writer.py:48] [7327] accumulated_eval_time=29428.2, accumulated_logging_time=1.65893, accumulated_submission_time=7210.48, global_step=7327, preemption_count=0, score=7210.48, test/loss=0.12639, test/num_examples=95000000, total_duration=36692.7, train/loss=0.122605, validation/loss=0.12403, validation/num_examples=83274637
I0316 23:00:42.214931 140314632471744 spec.py:321] Evaluating on the training split.
I0316 23:02:45.760810 140314632471744 spec.py:333] Evaluating on the validation split.
I0316 23:04:48.114195 140314632471744 spec.py:349] Evaluating on the test split.
I0316 23:07:09.837084 140314632471744 submission_runner.py:469] Time since start: 37201.96s, 	Step: 7452, 	{'train/loss': 0.12196816254850384, 'validation/loss': 0.12394141269170375, 'validation/num_examples': 83274637, 'test/loss': 0.1263497477012233, 'test/num_examples': 95000000, 'score': 7331.129232168198, 'total_duration': 37201.96035504341, 'accumulated_submission_time': 7331.129232168198, 'accumulated_eval_time': 29815.831134080887, 'accumulated_logging_time': 1.71091890335083}
I0316 23:07:09.849648 140272851691264 logging_writer.py:48] [7452] accumulated_eval_time=29815.8, accumulated_logging_time=1.71092, accumulated_submission_time=7331.13, global_step=7452, preemption_count=0, score=7331.13, test/loss=0.12635, test/num_examples=95000000, total_duration=37202, train/loss=0.121968, validation/loss=0.123941, validation/num_examples=83274637
I0316 23:07:38.757061 140272860083968 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.0195999, loss=0.12886
I0316 23:07:38.760846 140314632471744 submission.py:265] 7500) loss = 0.129, grad_norm = 0.020
I0316 23:09:11.122577 140314632471744 spec.py:321] Evaluating on the training split.
I0316 23:11:14.636617 140314632471744 spec.py:333] Evaluating on the validation split.
I0316 23:13:18.046556 140314632471744 spec.py:349] Evaluating on the test split.
I0316 23:15:39.929543 140314632471744 submission_runner.py:469] Time since start: 37712.05s, 	Step: 7572, 	{'train/loss': 0.12345289780692177, 'validation/loss': 0.12385985597915054, 'validation/num_examples': 83274637, 'test/loss': 0.1262367157002499, 'test/num_examples': 95000000, 'score': 7451.495624780655, 'total_duration': 37712.05278635025, 'accumulated_submission_time': 7451.495624780655, 'accumulated_eval_time': 30204.638250350952, 'accumulated_logging_time': 1.731210708618164}
I0316 23:15:39.942707 140272851691264 logging_writer.py:48] [7572] accumulated_eval_time=30204.6, accumulated_logging_time=1.73121, accumulated_submission_time=7451.5, global_step=7572, preemption_count=0, score=7451.5, test/loss=0.126237, test/num_examples=95000000, total_duration=37712.1, train/loss=0.123453, validation/loss=0.12386, validation/num_examples=83274637
I0316 23:17:40.868942 140314632471744 spec.py:321] Evaluating on the training split.
I0316 23:19:44.596813 140314632471744 spec.py:333] Evaluating on the validation split.
I0316 23:21:48.567744 140314632471744 spec.py:349] Evaluating on the test split.
I0316 23:24:09.432516 140314632471744 submission_runner.py:469] Time since start: 38221.56s, 	Step: 7695, 	{'train/loss': 0.12295619862143316, 'validation/loss': 0.12394925144950633, 'validation/num_examples': 83274637, 'test/loss': 0.126362404895662, 'test/num_examples': 95000000, 'score': 7571.565553665161, 'total_duration': 38221.555780887604, 'accumulated_submission_time': 7571.565553665161, 'accumulated_eval_time': 30593.2019135952, 'accumulated_logging_time': 1.751652479171753}
I0316 23:24:09.445312 140272860083968 logging_writer.py:48] [7695] accumulated_eval_time=30593.2, accumulated_logging_time=1.75165, accumulated_submission_time=7571.57, global_step=7695, preemption_count=0, score=7571.57, test/loss=0.126362, test/num_examples=95000000, total_duration=38221.6, train/loss=0.122956, validation/loss=0.123949, validation/num_examples=83274637
I0316 23:26:11.083755 140314632471744 spec.py:321] Evaluating on the training split.
I0316 23:28:14.882577 140314632471744 spec.py:333] Evaluating on the validation split.
I0316 23:30:18.155717 140314632471744 spec.py:349] Evaluating on the test split.
I0316 23:32:40.112871 140314632471744 submission_runner.py:469] Time since start: 38732.24s, 	Step: 7817, 	{'train/loss': 0.122254133695004, 'validation/loss': 0.12372519953178607, 'validation/num_examples': 83274637, 'test/loss': 0.1261251651818125, 'test/num_examples': 95000000, 'score': 7692.307582139969, 'total_duration': 38732.23611831665, 'accumulated_submission_time': 7692.307582139969, 'accumulated_eval_time': 30982.23121356964, 'accumulated_logging_time': 1.7720067501068115}
I0316 23:32:40.125671 140272851691264 logging_writer.py:48] [7817] accumulated_eval_time=30982.2, accumulated_logging_time=1.77201, accumulated_submission_time=7692.31, global_step=7817, preemption_count=0, score=7692.31, test/loss=0.126125, test/num_examples=95000000, total_duration=38732.2, train/loss=0.122254, validation/loss=0.123725, validation/num_examples=83274637
I0316 23:34:41.359221 140272860083968 logging_writer.py:48] [7942] global_step=7942, preemption_count=0, score=7813.07
I0316 23:34:43.298381 140314632471744 submission_runner.py:646] Tuning trial 5/5
I0316 23:34:43.298595 140314632471744 submission_runner.py:647] Hyperparameters: Hyperparameters(learning_rate=0.001308209823469072, one_minus_beta1=0.02686663061, one_minus_beta2=0.00187670778, epsilon=1e-08, one_minus_momentum=0.0, use_momentum=False, weight_decay=0.16375311233774334, max_preconditioner_dim=1024, precondition_frequency=100, start_preconditioning_step=-1, inv_root_override=0, exponent_multiplier=1.0, grafting_type='ADAM', grafting_epsilon=1e-08, use_normalized_grafting=False, communication_dtype='FP32', communicate_params=True, use_cosine_decay=True, warmup_factor=0.1, label_smoothing=0.1, dropout_rate=0.0, use_nadam=True, step_hint_factor=1.0)
I0316 23:34:43.299816 140314632471744 submission_runner.py:648] Metrics: {'eval_results': [(1, {'train/loss': 0.6023427251093904, 'validation/loss': 0.6009843525960341, 'validation/num_examples': 83274637, 'test/loss': 0.60226138782445, 'test/num_examples': 95000000, 'score': 5.367934465408325, 'total_duration': 974.3964323997498, 'accumulated_submission_time': 5.367934465408325, 'accumulated_eval_time': 968.5826709270477, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (123, {'train/loss': 0.13607341417373647, 'validation/loss': 0.13920789513171378, 'validation/num_examples': 83274637, 'test/loss': 0.14271085073579487, 'test/num_examples': 95000000, 'score': 125.63558554649353, 'total_duration': 2005.8985240459442, 'accumulated_submission_time': 125.63558554649353, 'accumulated_eval_time': 1878.8984410762787, 'accumulated_logging_time': 0.016098499298095703, 'global_step': 123, 'preemption_count': 0}), (243, {'train/loss': 0.12818622331229518, 'validation/loss': 0.12981452871103244, 'validation/num_examples': 83274637, 'test/loss': 0.13210321375836825, 'test/num_examples': 95000000, 'score': 246.1509747505188, 'total_duration': 3024.2743170261383, 'accumulated_submission_time': 246.1509747505188, 'accumulated_eval_time': 2775.8141510486603, 'accumulated_logging_time': 0.03409004211425781, 'global_step': 243, 'preemption_count': 0}), (368, {'train/loss': 0.12628630429185478, 'validation/loss': 0.12798768432529592, 'validation/num_examples': 83274637, 'test/loss': 0.1303522778307463, 'test/num_examples': 95000000, 'score': 366.17584705352783, 'total_duration': 4066.604663848877, 'accumulated_submission_time': 366.17584705352783, 'accumulated_eval_time': 3697.254068374634, 'accumulated_logging_time': 0.05007576942443848, 'global_step': 368, 'preemption_count': 0}), (490, {'train/loss': 0.12647742901600936, 'validation/loss': 0.12742401315638827, 'validation/num_examples': 83274637, 'test/loss': 0.12976422706716437, 'test/num_examples': 95000000, 'score': 485.9236822128296, 'total_duration': 5091.5845313072205, 'accumulated_submission_time': 485.9236822128296, 'accumulated_eval_time': 4601.559302806854, 'accumulated_logging_time': 0.1294541358947754, 'global_step': 490, 'preemption_count': 0}), (614, {'train/loss': 0.12424785546139806, 'validation/loss': 0.12715673677421416, 'validation/num_examples': 83274637, 'test/loss': 0.12956325677321584, 'test/num_examples': 95000000, 'score': 606.7232487201691, 'total_duration': 6132.598413228989, 'accumulated_submission_time': 606.7232487201691, 'accumulated_eval_time': 5520.920495033264, 'accumulated_logging_time': 0.14644312858581543, 'global_step': 614, 'preemption_count': 0}), (741, {'train/loss': 0.12509329326120291, 'validation/loss': 0.12684132970780748, 'validation/num_examples': 83274637, 'test/loss': 0.1291754362735949, 'test/num_examples': 95000000, 'score': 726.4880728721619, 'total_duration': 7137.292676210403, 'accumulated_submission_time': 726.4880728721619, 'accumulated_eval_time': 6405.035758495331, 'accumulated_logging_time': 0.16379523277282715, 'global_step': 741, 'preemption_count': 0}), (862, {'train/loss': 0.126744200597174, 'validation/loss': 0.12681966861810842, 'validation/num_examples': 83274637, 'test/loss': 0.12923705452712209, 'test/num_examples': 95000000, 'score': 846.0993120670319, 'total_duration': 8161.951017856598, 'accumulated_submission_time': 846.0993120670319, 'accumulated_eval_time': 7309.211316585541, 'accumulated_logging_time': 0.18069028854370117, 'global_step': 862, 'preemption_count': 0}), (986, {'train/loss': 0.12558371350475164, 'validation/loss': 0.12672920731320914, 'validation/num_examples': 83274637, 'test/loss': 0.12913884590964067, 'test/num_examples': 95000000, 'score': 966.2201406955719, 'total_duration': 9117.063033342361, 'accumulated_submission_time': 966.2201406955719, 'accumulated_eval_time': 8143.358115911484, 'accumulated_logging_time': 0.19673418998718262, 'global_step': 986, 'preemption_count': 0}), (1103, {'train/loss': 0.12646174373415914, 'validation/loss': 0.12650915166209598, 'validation/num_examples': 83274637, 'test/loss': 0.12899481884974429, 'test/num_examples': 95000000, 'score': 1086.4548768997192, 'total_duration': 10059.67867898941, 'accumulated_submission_time': 1086.4548768997192, 'accumulated_eval_time': 8964.796182394028, 'accumulated_logging_time': 0.21472716331481934, 'global_step': 1103, 'preemption_count': 0}), (1227, {'train/loss': 0.12390195336839624, 'validation/loss': 0.12600683487165992, 'validation/num_examples': 83274637, 'test/loss': 0.12853462125035336, 'test/num_examples': 95000000, 'score': 1206.1280629634857, 'total_duration': 10960.909957408905, 'accumulated_submission_time': 1206.1280629634857, 'accumulated_eval_time': 9745.470032691956, 'accumulated_logging_time': 0.2517678737640381, 'global_step': 1227, 'preemption_count': 0}), (1346, {'train/loss': 0.12492355842778348, 'validation/loss': 0.12595288095509197, 'validation/num_examples': 83274637, 'test/loss': 0.12844056025671707, 'test/num_examples': 95000000, 'score': 1328.18146443367, 'total_duration': 11659.903614521027, 'accumulated_submission_time': 1328.18146443367, 'accumulated_eval_time': 10321.594165086746, 'accumulated_logging_time': 0.26906704902648926, 'global_step': 1346, 'preemption_count': 0}), (1466, {'train/loss': 0.12380710599787156, 'validation/loss': 0.12585928425596793, 'validation/num_examples': 83274637, 'test/loss': 0.12837989984765305, 'test/num_examples': 95000000, 'score': 1448.4299774169922, 'total_duration': 12207.318369865417, 'accumulated_submission_time': 1448.4299774169922, 'accumulated_eval_time': 10747.832133293152, 'accumulated_logging_time': 0.28632259368896484, 'global_step': 1466, 'preemption_count': 0}), (1589, {'train/loss': 0.12569676535372618, 'validation/loss': 0.1260618083854614, 'validation/num_examples': 83274637, 'test/loss': 0.1286645704322012, 'test/num_examples': 95000000, 'score': 1568.39866232872, 'total_duration': 12718.898329734802, 'accumulated_submission_time': 1568.39866232872, 'accumulated_eval_time': 11138.539406061172, 'accumulated_logging_time': 0.3049333095550537, 'global_step': 1589, 'preemption_count': 0}), (1711, {'train/loss': 0.12615529759135666, 'validation/loss': 0.1256201966242171, 'validation/num_examples': 83274637, 'test/loss': 0.12809278509425112, 'test/num_examples': 95000000, 'score': 1688.6981909275055, 'total_duration': 13231.326963424683, 'accumulated_submission_time': 1688.6981909275055, 'accumulated_eval_time': 11529.793808460236, 'accumulated_logging_time': 0.33905696868896484, 'global_step': 1711, 'preemption_count': 0}), (1830, {'train/loss': 0.12508121588572085, 'validation/loss': 0.12570268213533856, 'validation/num_examples': 83274637, 'test/loss': 0.12824256043644955, 'test/num_examples': 95000000, 'score': 1808.953208208084, 'total_duration': 13742.833914995193, 'accumulated_submission_time': 1808.953208208084, 'accumulated_eval_time': 11920.15373635292, 'accumulated_logging_time': 0.3559000492095947, 'global_step': 1830, 'preemption_count': 0}), (1952, {'train/loss': 0.12419040443374973, 'validation/loss': 0.12533212653477432, 'validation/num_examples': 83274637, 'test/loss': 0.12776818721723054, 'test/num_examples': 95000000, 'score': 1928.4589233398438, 'total_duration': 14256.589020252228, 'accumulated_submission_time': 1928.4589233398438, 'accumulated_eval_time': 12313.46892786026, 'accumulated_logging_time': 0.3737156391143799, 'global_step': 1952, 'preemption_count': 0}), (2074, {'train/loss': 0.12431211828908453, 'validation/loss': 0.125642926583419, 'validation/num_examples': 83274637, 'test/loss': 0.1281478270837884, 'test/num_examples': 95000000, 'score': 2048.6620337963104, 'total_duration': 14767.41723227501, 'accumulated_submission_time': 2048.6620337963104, 'accumulated_eval_time': 12703.187343358994, 'accumulated_logging_time': 0.39081406593322754, 'global_step': 2074, 'preemption_count': 0}), (2198, {'train/loss': 0.12430118498942622, 'validation/loss': 0.12563878545653895, 'validation/num_examples': 83274637, 'test/loss': 0.12826537484604686, 'test/num_examples': 95000000, 'score': 2169.002023458481, 'total_duration': 15276.779505252838, 'accumulated_submission_time': 2169.002023458481, 'accumulated_eval_time': 13091.329463005066, 'accumulated_logging_time': 0.4591939449310303, 'global_step': 2198, 'preemption_count': 0}), (2320, {'train/loss': 0.12236742837424715, 'validation/loss': 0.12534673638555208, 'validation/num_examples': 83274637, 'test/loss': 0.12785338479698583, 'test/num_examples': 95000000, 'score': 2289.8345291614532, 'total_duration': 15788.4209856987, 'accumulated_submission_time': 2289.8345291614532, 'accumulated_eval_time': 13481.290761947632, 'accumulated_logging_time': 0.47815465927124023, 'global_step': 2320, 'preemption_count': 0}), (2442, {'train/loss': 0.1251857147346222, 'validation/loss': 0.125357922766593, 'validation/num_examples': 83274637, 'test/loss': 0.1279412816410667, 'test/num_examples': 95000000, 'score': 2410.129479408264, 'total_duration': 16299.23723602295, 'accumulated_submission_time': 2410.129479408264, 'accumulated_eval_time': 13870.919592380524, 'accumulated_logging_time': 0.4958229064941406, 'global_step': 2442, 'preemption_count': 0}), (2563, {'train/loss': 0.12540921109557934, 'validation/loss': 0.12486918031294199, 'validation/num_examples': 83274637, 'test/loss': 0.12739085005822431, 'test/num_examples': 95000000, 'score': 2529.8876190185547, 'total_duration': 16809.093782901764, 'accumulated_submission_time': 2529.8876190185547, 'accumulated_eval_time': 14260.09124135971, 'accumulated_logging_time': 0.5261611938476562, 'global_step': 2563, 'preemption_count': 0}), (2686, {'train/loss': 0.12393295768871551, 'validation/loss': 0.12494675597592039, 'validation/num_examples': 83274637, 'test/loss': 0.12751430553765547, 'test/num_examples': 95000000, 'score': 2649.84938287735, 'total_duration': 17319.695164442062, 'accumulated_submission_time': 2649.84938287735, 'accumulated_eval_time': 14649.842858076096, 'accumulated_logging_time': 0.544879674911499, 'global_step': 2686, 'preemption_count': 0}), (2808, {'train/loss': 0.12390663669307753, 'validation/loss': 0.1252323258850652, 'validation/num_examples': 83274637, 'test/loss': 0.12785995702659206, 'test/num_examples': 95000000, 'score': 2770.716577529907, 'total_duration': 17827.74126815796, 'accumulated_submission_time': 2770.716577529907, 'accumulated_eval_time': 15036.124640226364, 'accumulated_logging_time': 0.565514326095581, 'global_step': 2808, 'preemption_count': 0}), (2931, {'train/loss': 0.1251271537923938, 'validation/loss': 0.1251822659749817, 'validation/num_examples': 83274637, 'test/loss': 0.12780453318963303, 'test/num_examples': 95000000, 'score': 2891.517518758774, 'total_duration': 18338.265108823776, 'accumulated_submission_time': 2891.517518758774, 'accumulated_eval_time': 15424.957515239716, 'accumulated_logging_time': 0.586129903793335, 'global_step': 2931, 'preemption_count': 0}), (3053, {'train/loss': 0.12308701991893098, 'validation/loss': 0.12505920066851514, 'validation/num_examples': 83274637, 'test/loss': 0.1275395722200494, 'test/num_examples': 95000000, 'score': 3011.616305589676, 'total_duration': 18849.05565881729, 'accumulated_submission_time': 3011.616305589676, 'accumulated_eval_time': 15814.7499563694, 'accumulated_logging_time': 0.6048295497894287, 'global_step': 3053, 'preemption_count': 0}), (3177, {'train/loss': 0.12427540939542557, 'validation/loss': 0.12500282603377724, 'validation/num_examples': 83274637, 'test/loss': 0.12737891883922375, 'test/num_examples': 95000000, 'score': 3131.306389093399, 'total_duration': 19358.558275938034, 'accumulated_submission_time': 3131.306389093399, 'accumulated_eval_time': 16203.612781763077, 'accumulated_logging_time': 0.6612179279327393, 'global_step': 3177, 'preemption_count': 0}), (3300, {'train/loss': 0.12350989964205428, 'validation/loss': 0.1246559085930856, 'validation/num_examples': 83274637, 'test/loss': 0.12709904177422773, 'test/num_examples': 95000000, 'score': 3251.3492827415466, 'total_duration': 19867.697481393814, 'accumulated_submission_time': 3251.3492827415466, 'accumulated_eval_time': 16591.789635419846, 'accumulated_logging_time': 0.6795206069946289, 'global_step': 3300, 'preemption_count': 0}), (3421, {'train/loss': 0.12291189380094171, 'validation/loss': 0.12485812227426817, 'validation/num_examples': 83274637, 'test/loss': 0.12729886905830784, 'test/num_examples': 95000000, 'score': 3371.8770208358765, 'total_duration': 20380.763732910156, 'accumulated_submission_time': 3371.8770208358765, 'accumulated_eval_time': 16983.4053440094, 'accumulated_logging_time': 0.6980915069580078, 'global_step': 3421, 'preemption_count': 0}), (3547, {'train/loss': 0.12342394231312921, 'validation/loss': 0.12491516923598767, 'validation/num_examples': 83274637, 'test/loss': 0.12747071575951827, 'test/num_examples': 95000000, 'score': 3491.616428375244, 'total_duration': 20888.50651907921, 'accumulated_submission_time': 3491.616428375244, 'accumulated_eval_time': 17370.483756780624, 'accumulated_logging_time': 0.717442512512207, 'global_step': 3547, 'preemption_count': 0}), (3669, {'train/loss': 0.12147182959631912, 'validation/loss': 0.12496924203127148, 'validation/num_examples': 83274637, 'test/loss': 0.127382368553523, 'test/num_examples': 95000000, 'score': 3611.23486495018, 'total_duration': 21397.779643297195, 'accumulated_submission_time': 3611.23486495018, 'accumulated_eval_time': 17759.22155022621, 'accumulated_logging_time': 0.7570104598999023, 'global_step': 3669, 'preemption_count': 0}), (3792, {'train/loss': 0.12397411962279974, 'validation/loss': 0.12524913736391968, 'validation/num_examples': 83274637, 'test/loss': 0.12768075185699462, 'test/num_examples': 95000000, 'score': 3731.0944232940674, 'total_duration': 21907.32909178734, 'accumulated_submission_time': 3731.0944232940674, 'accumulated_eval_time': 18148.0059568882, 'accumulated_logging_time': 0.7993669509887695, 'global_step': 3792, 'preemption_count': 0}), (3914, {'train/loss': 0.12272060813520418, 'validation/loss': 0.12469813042420463, 'validation/num_examples': 83274637, 'test/loss': 0.12706990350655004, 'test/num_examples': 95000000, 'score': 3851.0231878757477, 'total_duration': 22417.229254722595, 'accumulated_submission_time': 3851.0231878757477, 'accumulated_eval_time': 18537.10766172409, 'accumulated_logging_time': 0.8173232078552246, 'global_step': 3914, 'preemption_count': 0}), (4037, {'train/loss': 0.12208169161161793, 'validation/loss': 0.12467975027801091, 'validation/num_examples': 83274637, 'test/loss': 0.1269883765710128, 'test/num_examples': 95000000, 'score': 3971.104371547699, 'total_duration': 22929.73856472969, 'accumulated_submission_time': 3971.104371547699, 'accumulated_eval_time': 18928.60440993309, 'accumulated_logging_time': 0.8351976871490479, 'global_step': 4037, 'preemption_count': 0}), (4161, {'train/loss': 0.12201703645110486, 'validation/loss': 0.12446358006634335, 'validation/num_examples': 83274637, 'test/loss': 0.12689819835056507, 'test/num_examples': 95000000, 'score': 4091.6834394931793, 'total_duration': 23440.607169628143, 'accumulated_submission_time': 4091.6834394931793, 'accumulated_eval_time': 19318.0007147789, 'accumulated_logging_time': 0.8818163871765137, 'global_step': 4161, 'preemption_count': 0}), (4283, {'train/loss': 0.12174455662325395, 'validation/loss': 0.12466634198968637, 'validation/num_examples': 83274637, 'test/loss': 0.12712773138371516, 'test/num_examples': 95000000, 'score': 4211.76550245285, 'total_duration': 23949.705335378647, 'accumulated_submission_time': 4211.76550245285, 'accumulated_eval_time': 19706.102294445038, 'accumulated_logging_time': 0.9561691284179688, 'global_step': 4283, 'preemption_count': 0}), (4404, {'train/loss': 0.121421056670739, 'validation/loss': 0.12480549779834164, 'validation/num_examples': 83274637, 'test/loss': 0.12728312139611495, 'test/num_examples': 95000000, 'score': 4331.5587911605835, 'total_duration': 24459.395176887512, 'accumulated_submission_time': 4331.5587911605835, 'accumulated_eval_time': 20095.074667453766, 'accumulated_logging_time': 0.9746382236480713, 'global_step': 4404, 'preemption_count': 0}), (4527, {'train/loss': 0.12419337521076493, 'validation/loss': 0.1245660732537624, 'validation/num_examples': 83274637, 'test/loss': 0.12691536974603754, 'test/num_examples': 95000000, 'score': 4451.133327960968, 'total_duration': 24970.15636229515, 'accumulated_submission_time': 4451.133327960968, 'accumulated_eval_time': 20485.37953066826, 'accumulated_logging_time': 0.9936356544494629, 'global_step': 4527, 'preemption_count': 0}), (4646, {'train/loss': 0.12101795779973175, 'validation/loss': 0.12464252100884302, 'validation/num_examples': 83274637, 'test/loss': 0.12711420612431576, 'test/num_examples': 95000000, 'score': 4572.162162065506, 'total_duration': 25482.233493328094, 'accumulated_submission_time': 4572.162162065506, 'accumulated_eval_time': 20875.57020521164, 'accumulated_logging_time': 1.0122261047363281, 'global_step': 4646, 'preemption_count': 0}), (4769, {'train/loss': 0.12296707942325512, 'validation/loss': 0.1246650640407425, 'validation/num_examples': 83274637, 'test/loss': 0.1270691764832346, 'test/num_examples': 95000000, 'score': 4691.68408203125, 'total_duration': 25992.356389522552, 'accumulated_submission_time': 4691.68408203125, 'accumulated_eval_time': 21265.318603515625, 'accumulated_logging_time': 1.0570063591003418, 'global_step': 4769, 'preemption_count': 0}), (4889, {'train/loss': 0.12235128447461449, 'validation/loss': 0.12464636718983224, 'validation/num_examples': 83274637, 'test/loss': 0.12717848324568898, 'test/num_examples': 95000000, 'score': 4811.2539319992065, 'total_duration': 26499.91275715828, 'accumulated_submission_time': 4811.2539319992065, 'accumulated_eval_time': 21652.40898513794, 'accumulated_logging_time': 1.0771362781524658, 'global_step': 4889, 'preemption_count': 0}), (5010, {'train/loss': 0.12137727418016828, 'validation/loss': 0.12464187550922909, 'validation/num_examples': 83274637, 'test/loss': 0.12700683076934816, 'test/num_examples': 95000000, 'score': 4931.263594388962, 'total_duration': 27009.55172228813, 'accumulated_submission_time': 4931.263594388962, 'accumulated_eval_time': 22041.159314393997, 'accumulated_logging_time': 1.095839500427246, 'global_step': 5010, 'preemption_count': 0}), (5131, {'train/loss': 0.12375737924889879, 'validation/loss': 0.12455464018879057, 'validation/num_examples': 83274637, 'test/loss': 0.12689164422117535, 'test/num_examples': 95000000, 'score': 5051.223951339722, 'total_duration': 27516.97146844864, 'accumulated_submission_time': 5051.223951339722, 'accumulated_eval_time': 22427.75536632538, 'accumulated_logging_time': 1.115391492843628, 'global_step': 5131, 'preemption_count': 0}), (5251, {'train/loss': 0.1231182268605416, 'validation/loss': 0.12442298568227715, 'validation/num_examples': 83274637, 'test/loss': 0.12691711902959724, 'test/num_examples': 95000000, 'score': 5171.286344766617, 'total_duration': 28027.677562236786, 'accumulated_submission_time': 5171.286344766617, 'accumulated_eval_time': 22817.467128753662, 'accumulated_logging_time': 1.220468282699585, 'global_step': 5251, 'preemption_count': 0}), (5374, {'train/loss': 0.12281960833217848, 'validation/loss': 0.12432266427224319, 'validation/num_examples': 83274637, 'test/loss': 0.12666418521258704, 'test/num_examples': 95000000, 'score': 5290.962339878082, 'total_duration': 28536.475568294525, 'accumulated_submission_time': 5290.962339878082, 'accumulated_eval_time': 23205.661076545715, 'accumulated_logging_time': 1.2400925159454346, 'global_step': 5374, 'preemption_count': 0}), (5496, {'train/loss': 0.12215018099038868, 'validation/loss': 0.12428967900400527, 'validation/num_examples': 83274637, 'test/loss': 0.12670933237360904, 'test/num_examples': 95000000, 'score': 5410.650941371918, 'total_duration': 29047.098650693893, 'accumulated_submission_time': 5410.650941371918, 'accumulated_eval_time': 23595.754348278046, 'accumulated_logging_time': 1.259819746017456, 'global_step': 5496, 'preemption_count': 0}), (5620, {'train/loss': 0.12257585450978489, 'validation/loss': 0.12427999062540275, 'validation/num_examples': 83274637, 'test/loss': 0.1266171048938952, 'test/num_examples': 95000000, 'score': 5530.398602247238, 'total_duration': 29557.153762578964, 'accumulated_submission_time': 5530.398602247238, 'accumulated_eval_time': 23985.201287269592, 'accumulated_logging_time': 1.2788465023040771, 'global_step': 5620, 'preemption_count': 0}), (5739, {'train/loss': 0.12117027378923782, 'validation/loss': 0.12436541189503242, 'validation/num_examples': 83274637, 'test/loss': 0.12693682418602392, 'test/num_examples': 95000000, 'score': 5650.1432592868805, 'total_duration': 30066.6884226799, 'accumulated_submission_time': 5650.1432592868805, 'accumulated_eval_time': 24374.15184855461, 'accumulated_logging_time': 1.298029899597168, 'global_step': 5739, 'preemption_count': 0}), (5862, {'train/loss': 0.12405976619468476, 'validation/loss': 0.12431916739495108, 'validation/num_examples': 83274637, 'test/loss': 0.12682433585694966, 'test/num_examples': 95000000, 'score': 5770.048926353455, 'total_duration': 30576.345227718353, 'accumulated_submission_time': 5770.048926353455, 'accumulated_eval_time': 24762.99488377571, 'accumulated_logging_time': 1.3469254970550537, 'global_step': 5862, 'preemption_count': 0}), (5983, {'train/loss': 0.12228932108937303, 'validation/loss': 0.12432019993843717, 'validation/num_examples': 83274637, 'test/loss': 0.1267663585537559, 'test/num_examples': 95000000, 'score': 5890.600004673004, 'total_duration': 31086.323573589325, 'accumulated_submission_time': 5890.600004673004, 'accumulated_eval_time': 25151.533830165863, 'accumulated_logging_time': 1.366347074508667, 'global_step': 5983, 'preemption_count': 0}), (6107, {'train/loss': 0.12189471967864254, 'validation/loss': 0.1241132836685756, 'validation/num_examples': 83274637, 'test/loss': 0.12657543649597167, 'test/num_examples': 95000000, 'score': 6010.596944570541, 'total_duration': 31598.391882896423, 'accumulated_submission_time': 6010.596944570541, 'accumulated_eval_time': 25542.7571041584, 'accumulated_logging_time': 1.3860409259796143, 'global_step': 6107, 'preemption_count': 0}), (6226, {'train/loss': 0.12248965254030204, 'validation/loss': 0.12432265918702687, 'validation/num_examples': 83274637, 'test/loss': 0.12678470596835487, 'test/num_examples': 95000000, 'score': 6130.359548330307, 'total_duration': 32108.024787425995, 'accumulated_submission_time': 6130.359548330307, 'accumulated_eval_time': 25931.72327876091, 'accumulated_logging_time': 1.4055116176605225, 'global_step': 6226, 'preemption_count': 0}), (6347, {'train/loss': 0.12282445139118629, 'validation/loss': 0.1240554339377771, 'validation/num_examples': 83274637, 'test/loss': 0.12655359052168194, 'test/num_examples': 95000000, 'score': 6250.366380691528, 'total_duration': 32615.716176748276, 'accumulated_submission_time': 6250.366380691528, 'accumulated_eval_time': 26318.473910331726, 'accumulated_logging_time': 1.4273009300231934, 'global_step': 6347, 'preemption_count': 0}), (6469, {'train/loss': 0.12250038614281668, 'validation/loss': 0.12413038120314795, 'validation/num_examples': 83274637, 'test/loss': 0.12661481474007055, 'test/num_examples': 95000000, 'score': 6370.295336961746, 'total_duration': 33125.71952915192, 'accumulated_submission_time': 6370.295336961746, 'accumulated_eval_time': 26707.674695968628, 'accumulated_logging_time': 1.4667718410491943, 'global_step': 6469, 'preemption_count': 0}), (6594, {'train/loss': 0.12272806245845179, 'validation/loss': 0.12413568151500666, 'validation/num_examples': 83274637, 'test/loss': 0.12658575419564497, 'test/num_examples': 95000000, 'score': 6490.896236419678, 'total_duration': 33635.094723939896, 'accumulated_submission_time': 6490.896236419678, 'accumulated_eval_time': 27095.522090435028, 'accumulated_logging_time': 1.4981791973114014, 'global_step': 6594, 'preemption_count': 0}), (6716, {'train/loss': 0.12023109503475334, 'validation/loss': 0.12419262113262405, 'validation/num_examples': 83274637, 'test/loss': 0.1267059837345324, 'test/num_examples': 95000000, 'score': 6610.495177984238, 'total_duration': 34142.73532629013, 'accumulated_submission_time': 6610.495177984238, 'accumulated_eval_time': 27482.67778801918, 'accumulated_logging_time': 1.517615795135498, 'global_step': 6716, 'preemption_count': 0}), (6838, {'train/loss': 0.12284094374086367, 'validation/loss': 0.1240780483780515, 'validation/num_examples': 83274637, 'test/loss': 0.1265274558647156, 'test/num_examples': 95000000, 'score': 6730.622112035751, 'total_duration': 34653.08913254738, 'accumulated_submission_time': 6730.622112035751, 'accumulated_eval_time': 27872.00283575058, 'accumulated_logging_time': 1.537109375, 'global_step': 6838, 'preemption_count': 0}), (6961, {'train/loss': 0.12402294331766883, 'validation/loss': 0.124137299844126, 'validation/num_examples': 83274637, 'test/loss': 0.1264878982873214, 'test/num_examples': 95000000, 'score': 6850.681242465973, 'total_duration': 35163.22269845009, 'accumulated_submission_time': 6850.681242465973, 'accumulated_eval_time': 28261.16577243805, 'accumulated_logging_time': 1.6001980304718018, 'global_step': 6961, 'preemption_count': 0}), (7084, {'train/loss': 0.1225382819896908, 'validation/loss': 0.12394267848807544, 'validation/num_examples': 83274637, 'test/loss': 0.12632085663492304, 'test/num_examples': 95000000, 'score': 6970.557040214539, 'total_duration': 35672.3398103714, 'accumulated_submission_time': 6970.557040214539, 'accumulated_eval_time': 28649.519287586212, 'accumulated_logging_time': 1.6197123527526855, 'global_step': 7084, 'preemption_count': 0}), (7205, {'train/loss': 0.12180562199132977, 'validation/loss': 0.12402680419868596, 'validation/num_examples': 83274637, 'test/loss': 0.12636186293575888, 'test/num_examples': 95000000, 'score': 7090.934632778168, 'total_duration': 36182.432220458984, 'accumulated_submission_time': 7090.934632778168, 'accumulated_eval_time': 29038.315056562424, 'accumulated_logging_time': 1.638972282409668, 'global_step': 7205, 'preemption_count': 0}), (7327, {'train/loss': 0.12260507798422778, 'validation/loss': 0.12403043651254489, 'validation/num_examples': 83274637, 'test/loss': 0.12639017869664243, 'test/num_examples': 95000000, 'score': 7210.477641582489, 'total_duration': 36692.74712920189, 'accumulated_submission_time': 7210.477641582489, 'accumulated_eval_time': 29428.208776474, 'accumulated_logging_time': 1.6589267253875732, 'global_step': 7327, 'preemption_count': 0}), (7452, {'train/loss': 0.12196816254850384, 'validation/loss': 0.12394141269170375, 'validation/num_examples': 83274637, 'test/loss': 0.1263497477012233, 'test/num_examples': 95000000, 'score': 7331.129232168198, 'total_duration': 37201.96035504341, 'accumulated_submission_time': 7331.129232168198, 'accumulated_eval_time': 29815.831134080887, 'accumulated_logging_time': 1.71091890335083, 'global_step': 7452, 'preemption_count': 0}), (7572, {'train/loss': 0.12345289780692177, 'validation/loss': 0.12385985597915054, 'validation/num_examples': 83274637, 'test/loss': 0.1262367157002499, 'test/num_examples': 95000000, 'score': 7451.495624780655, 'total_duration': 37712.05278635025, 'accumulated_submission_time': 7451.495624780655, 'accumulated_eval_time': 30204.638250350952, 'accumulated_logging_time': 1.731210708618164, 'global_step': 7572, 'preemption_count': 0}), (7695, {'train/loss': 0.12295619862143316, 'validation/loss': 0.12394925144950633, 'validation/num_examples': 83274637, 'test/loss': 0.126362404895662, 'test/num_examples': 95000000, 'score': 7571.565553665161, 'total_duration': 38221.555780887604, 'accumulated_submission_time': 7571.565553665161, 'accumulated_eval_time': 30593.2019135952, 'accumulated_logging_time': 1.751652479171753, 'global_step': 7695, 'preemption_count': 0}), (7817, {'train/loss': 0.122254133695004, 'validation/loss': 0.12372519953178607, 'validation/num_examples': 83274637, 'test/loss': 0.1261251651818125, 'test/num_examples': 95000000, 'score': 7692.307582139969, 'total_duration': 38732.23611831665, 'accumulated_submission_time': 7692.307582139969, 'accumulated_eval_time': 30982.23121356964, 'accumulated_logging_time': 1.7720067501068115, 'global_step': 7817, 'preemption_count': 0})], 'global_step': 7942}
I0316 23:34:43.299951 140314632471744 submission_runner.py:649] Timing: 7813.068846702576
I0316 23:34:43.299995 140314632471744 submission_runner.py:651] Total number of evals: 65
I0316 23:34:43.300028 140314632471744 submission_runner.py:652] ====================
I0316 23:34:43.300148 140314632471744 submission_runner.py:750] Final criteo1tb score: 4
