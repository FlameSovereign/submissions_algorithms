torchrun --redirects 1:0,2:0,3:0,4:0,5:0,6:0,7:0 --standalone --nnodes=1 --nproc_per_node=8 submission_runner.py --framework=pytorch --workload=librispeech_conformer --submission_path=/submissions_algorithms/external_tuning/shampoo_submission/submission.py --data_dir=/data/librispeech --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=submissions/rolling_leaderboard/external_tuning/shampoo/study_1 --overwrite=True --save_checkpoints=False --rng_seed=-321629183 --librispeech_tokenizer_vocab_path=/data/librispeech/spm_model.vocab --torch_compile=true --tuning_ruleset=external --tuning_search_space=/submissions_algorithms/external_tuning/shampoo_submission/tuning_search_space.json --num_tuning_trials=5 --hparam_start_index=1 --hparam_end_index=2 2>&1 | tee -a /logs/librispeech_conformer_pytorch_03-12-2025-21-39-07.log
W0312 21:39:09.453000 9 site-packages/torch/distributed/run.py:793] 
W0312 21:39:09.453000 9 site-packages/torch/distributed/run.py:793] *****************************************
W0312 21:39:09.453000 9 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0312 21:39:09.453000 9 site-packages/torch/distributed/run.py:793] *****************************************
2025-03-12 21:39:10.642269: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-03-12 21:39:10.642263: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-03-12 21:39:10.642263: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-03-12 21:39:10.642264: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-03-12 21:39:10.642276: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-03-12 21:39:10.642263: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-03-12 21:39:10.642268: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-03-12 21:39:10.642591: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1741815550.662599      48 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1741815550.662598      45 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1741815550.662614      49 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1741815550.662596      51 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1741815550.662615      44 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1741815550.662614      47 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1741815550.662619      46 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1741815550.663456      50 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1741815550.668761      44 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
E0000 00:00:1741815550.668765      49 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
E0000 00:00:1741815550.668766      51 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
E0000 00:00:1741815550.668765      48 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
E0000 00:00:1741815550.668771      47 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
E0000 00:00:1741815550.668775      46 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
E0000 00:00:1741815550.668967      45 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
E0000 00:00:1741815550.669716      50 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
[W312 21:39:17.046345962 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
[W312 21:39:17.528802573 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[rank3]:[W312 21:39:17.650667645 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[W312 21:39:17.653493328 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W312 21:39:17.657090738 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W312 21:39:17.666137406 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[rank0]:[W312 21:39:17.709267891 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[W312 21:39:17.753030789 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[rank4]:[W312 21:39:17.763044597 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 4]  using GPU 4 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank6]:[W312 21:39:17.764120835 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 6]  using GPU 6 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[W312 21:39:17.767869441 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[rank2]:[W312 21:39:17.778320639 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[W312 21:39:17.780101642 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[rank7]:[W312 21:39:17.871405518 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 7]  using GPU 7 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank5]:[W312 21:39:17.884348555 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 5]  using GPU 5 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank1]:[W312 21:39:17.897123706 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
I0312 21:39:19.566414 140137614226624 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_1/librispeech_conformer_pytorch.
I0312 21:39:19.566413 139706308916416 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_1/librispeech_conformer_pytorch.
I0312 21:39:19.566413 139647943779520 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_1/librispeech_conformer_pytorch.
I0312 21:39:19.566413 139679044183232 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_1/librispeech_conformer_pytorch.
I0312 21:39:19.566413 139868337816768 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_1/librispeech_conformer_pytorch.
I0312 21:39:19.566417 139629064176832 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_1/librispeech_conformer_pytorch.
I0312 21:39:19.566483 140498639611072 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_1/librispeech_conformer_pytorch.
I0312 21:39:19.566431 140217848812736 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_1/librispeech_conformer_pytorch.
[rank4]: Traceback (most recent call last):
[rank4]:   File "/algorithmic-efficiency/submission_runner.py", line 766, in <module>
[rank4]:     app.run(main)
[rank4]:   File "/usr/local/lib/python3.11/site-packages/absl/app.py", line 308, in run
[rank4]:     _run_main(main, args)
[rank4]:   File "/usr/local/lib/python3.11/site-packages/absl/app.py", line 254, in _run_main
[rank4]:     sys.exit(main(argv))
[rank4]:              ^^^^^^^^^^
[rank4]:   File "/algorithmic-efficiency/submission_runner.py", line 734, in main
[rank4]:     score = score_submission_on_workload(
[rank4]:             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank4]:   File "/algorithmic-efficiency/submission_runner.py", line 558, in score_submission_on_workload
[rank4]:     submission_module = importlib.import_module(submission_module_path)
[rank4]:                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank4]:   File "/usr/local/lib/python3.11/importlib/__init__.py", line 121, in import_module
[rank4]:     raise TypeError(msg.format(name))
[rank4]: TypeError: the 'package' argument is required to perform a relative import for '.submissions_algorithms.external_tuning.shampoo_submission.submission'
[rank1]: Traceback (most recent call last):
[rank1]:   File "/algorithmic-efficiency/submission_runner.py", line 766, in <module>
[rank1]:     app.run(main)
[rank1]:   File "/usr/local/lib/python3.11/site-packages/absl/app.py", line 308, in run
[rank1]:     _run_main(main, args)
[rank1]:   File "/usr/local/lib/python3.11/site-packages/absl/app.py", line 254, in _run_main
[rank1]:     sys.exit(main(argv))
[rank1]:              ^^^^^^^^^^
[rank1]:   File "/algorithmic-efficiency/submission_runner.py", line 734, in main
[rank1]:     score = score_submission_on_workload(
[rank1]:             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/algorithmic-efficiency/submission_runner.py", line 558, in score_submission_on_workload
[rank1]:     submission_module = importlib.import_module(submission_module_path)
[rank1]:                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/usr/local/lib/python3.11/importlib/__init__.py", line 121, in import_module
[rank1]:     raise TypeError(msg.format(name))
[rank1]: TypeError: the 'package' argument is required to perform a relative import for '.submissions_algorithms.external_tuning.shampoo_submission.submission'
[rank3]: Traceback (most recent call last):
[rank3]:   File "/algorithmic-efficiency/submission_runner.py", line 766, in <module>
[rank3]:     app.run(main)
[rank3]:   File "/usr/local/lib/python3.11/site-packages/absl/app.py", line 308, in run
[rank3]:     _run_main(main, args)
[rank3]:   File "/usr/local/lib/python3.11/site-packages/absl/app.py", line 254, in _run_main
[rank3]:     sys.exit(main(argv))
[rank3]:              ^^^^^^^^^^
[rank3]:   File "/algorithmic-efficiency/submission_runner.py", line 734, in main
[rank3]:     score = score_submission_on_workload(
[rank3]:             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/algorithmic-efficiency/submission_runner.py", line 558, in score_submission_on_workload
[rank3]:     submission_module = importlib.import_module(submission_module_path)
[rank3]:                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/usr/local/lib/python3.11/importlib/__init__.py", line 121, in import_module
[rank3]:     raise TypeError(msg.format(name))
[rank3]: TypeError: the 'package' argument is required to perform a relative import for '.submissions_algorithms.external_tuning.shampoo_submission.submission'
[rank2]: Traceback (most recent call last):
[rank2]:   File "/algorithmic-efficiency/submission_runner.py", line 766, in <module>
[rank2]:     app.run(main)
[rank2]:   File "/usr/local/lib/python3.11/site-packages/absl/app.py", line 308, in run
[rank2]:     _run_main(main, args)
[rank2]:   File "/usr/local/lib/python3.11/site-packages/absl/app.py", line 254, in _run_main
[rank2]:     sys.exit(main(argv))
[rank2]:              ^^^^^^^^^^
[rank2]:   File "/algorithmic-efficiency/submission_runner.py", line 734, in main
[rank2]:     score = score_submission_on_workload(
[rank2]:             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/algorithmic-efficiency/submission_runner.py", line 558, in score_submission_on_workload
[rank2]:     submission_module = importlib.import_module(submission_module_path)
[rank2]:                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/usr/local/lib/python3.11/importlib/__init__.py", line 121, in import_module
[rank2]:     raise TypeError(msg.format(name))
[rank2]: TypeError: the 'package' argument is required to perform a relative import for '.submissions_algorithms.external_tuning.shampoo_submission.submission'
[rank5]: Traceback (most recent call last):
[rank5]:   File "/algorithmic-efficiency/submission_runner.py", line 766, in <module>
[rank5]:     app.run(main)
[rank5]:   File "/usr/local/lib/python3.11/site-packages/absl/app.py", line 308, in run
[rank5]:     _run_main(main, args)
[rank5]:   File "/usr/local/lib/python3.11/site-packages/absl/app.py", line 254, in _run_main
[rank5]:     sys.exit(main(argv))
[rank5]:              ^^^^^^^^^^
[rank5]:   File "/algorithmic-efficiency/submission_runner.py", line 734, in main
[rank5]:     score = score_submission_on_workload(
[rank5]:             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank5]:   File "/algorithmic-efficiency/submission_runner.py", line 558, in score_submission_on_workload
[rank5]:     submission_module = importlib.import_module(submission_module_path)
[rank5]:                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank5]:   File "/usr/local/lib/python3.11/importlib/__init__.py", line 121, in import_module
[rank5]:     raise TypeError(msg.format(name))
[rank5]: TypeError: the 'package' argument is required to perform a relative import for '.submissions_algorithms.external_tuning.shampoo_submission.submission'
[rank6]: Traceback (most recent call last):
[rank6]:   File "/algorithmic-efficiency/submission_runner.py", line 766, in <module>
[rank6]:     app.run(main)
[rank6]:   File "/usr/local/lib/python3.11/site-packages/absl/app.py", line 308, in run
[rank6]:     _run_main(main, args)
[rank6]:   File "/usr/local/lib/python3.11/site-packages/absl/app.py", line 254, in _run_main
[rank6]:     sys.exit(main(argv))
[rank6]:              ^^^^^^^^^^
[rank6]:   File "/algorithmic-efficiency/submission_runner.py", line 734, in main
[rank6]:     score = score_submission_on_workload(
[rank6]:             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank6]:   File "/algorithmic-efficiency/submission_runner.py", line 558, in score_submission_on_workload
[rank6]:     submission_module = importlib.import_module(submission_module_path)
[rank6]:                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank6]:   File "/usr/local/lib/python3.11/importlib/__init__.py", line 121, in import_module
[rank6]:     raise TypeError(msg.format(name))
[rank6]: TypeError: the 'package' argument is required to perform a relative import for '.submissions_algorithms.external_tuning.shampoo_submission.submission'
[rank7]: Traceback (most recent call last):
[rank7]:   File "/algorithmic-efficiency/submission_runner.py", line 766, in <module>
[rank7]:     app.run(main)
[rank7]:   File "/usr/local/lib/python3.11/site-packages/absl/app.py", line 308, in run
[rank7]:     _run_main(main, args)
[rank7]:   File "/usr/local/lib/python3.11/site-packages/absl/app.py", line 254, in _run_main
[rank7]:     sys.exit(main(argv))
[rank7]:              ^^^^^^^^^^
[rank7]:   File "/algorithmic-efficiency/submission_runner.py", line 734, in main
[rank7]:     score = score_submission_on_workload(
[rank7]:             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank7]:   File "/algorithmic-efficiency/submission_runner.py", line 558, in score_submission_on_workload
[rank7]:     submission_module = importlib.import_module(submission_module_path)
[rank7]:                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank7]:   File "/usr/local/lib/python3.11/importlib/__init__.py", line 121, in import_module
[rank7]:     raise TypeError(msg.format(name))
[rank7]: TypeError: the 'package' argument is required to perform a relative import for '.submissions_algorithms.external_tuning.shampoo_submission.submission'
[rank0]: Traceback (most recent call last):
[rank0]:   File "/algorithmic-efficiency/submission_runner.py", line 766, in <module>
[rank0]:     app.run(main)
[rank0]:   File "/usr/local/lib/python3.11/site-packages/absl/app.py", line 308, in run
[rank0]:     _run_main(main, args)
[rank0]:   File "/usr/local/lib/python3.11/site-packages/absl/app.py", line 254, in _run_main
[rank0]:     sys.exit(main(argv))
[rank0]:              ^^^^^^^^^^
[rank0]:   File "/algorithmic-efficiency/submission_runner.py", line 734, in main
[rank0]:     score = score_submission_on_workload(
[rank0]:             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/algorithmic-efficiency/submission_runner.py", line 558, in score_submission_on_workload
[rank0]:     submission_module = importlib.import_module(submission_module_path)
[rank0]:                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/usr/local/lib/python3.11/importlib/__init__.py", line 121, in import_module
[rank0]:     raise TypeError(msg.format(name))
[rank0]: TypeError: the 'package' argument is required to perform a relative import for '.submissions_algorithms.external_tuning.shampoo_submission.submission'
[rank0]:[W312 21:39:20.780524388 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
W0312 21:39:21.693000 9 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 44 closing signal SIGTERM
W0312 21:39:21.694000 9 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 45 closing signal SIGTERM
W0312 21:39:21.694000 9 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 46 closing signal SIGTERM
W0312 21:39:21.694000 9 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 47 closing signal SIGTERM
W0312 21:39:21.695000 9 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 48 closing signal SIGTERM
W0312 21:39:21.695000 9 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 50 closing signal SIGTERM
E0312 21:39:22.074000 9 site-packages/torch/distributed/elastic/multiprocessing/api.py:869] failed (exitcode: 1) local_rank: 5 (pid: 49) of binary: /usr/local/bin/python3.11
Traceback (most recent call last):
  File "/usr/local/bin/torchrun", line 8, in <module>
    sys.exit(main())
             ^^^^^^
  File "/usr/local/lib/python3.11/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/site-packages/torch/distributed/run.py", line 919, in main
    run(args)
  File "/usr/local/lib/python3.11/site-packages/torch/distributed/run.py", line 910, in run
    elastic_launch(
  File "/usr/local/lib/python3.11/site-packages/torch/distributed/launcher/api.py", line 138, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/site-packages/torch/distributed/launcher/api.py", line 269, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
submission_runner.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2025-03-12_21:39:21
  host      : 0c0b84ed5ef7
  rank      : 7 (local_rank: 7)
  exitcode  : 1 (pid: 51)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-03-12_21:39:21
  host      : 0c0b84ed5ef7
  rank      : 5 (local_rank: 5)
  exitcode  : 1 (pid: 49)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
