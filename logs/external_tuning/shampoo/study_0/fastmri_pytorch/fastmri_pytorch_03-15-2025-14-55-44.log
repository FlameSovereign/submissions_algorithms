torchrun --redirects 1:0,2:0,3:0,4:0,5:0,6:0,7:0 --standalone --nnodes=1 --nproc_per_node=8 submission_runner.py --framework=pytorch --workload=fastmri --submission_path=submissions_algorithms/leaderboard/external_tuning/shampoo_submission/submission.py --data_dir=/data/fastmri --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=submissions/rolling_leaderboard/external_tuning/shampoo/study_0 --overwrite=True --save_checkpoints=False --rng_seed=-783906668 --torch_compile=true --tuning_ruleset=external --tuning_search_space=submissions_algorithms/leaderboard/external_tuning/shampoo_submission/tuning_search_space.json --num_tuning_trials=5 --hparam_start_index=3 --hparam_end_index=4 2>&1 | tee -a /logs/fastmri_pytorch_03-15-2025-14-55-44.log
W0315 14:55:56.400000 9 site-packages/torch/distributed/run.py:793] 
W0315 14:55:56.400000 9 site-packages/torch/distributed/run.py:793] *****************************************
W0315 14:55:56.400000 9 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0315 14:55:56.400000 9 site-packages/torch/distributed/run.py:793] *****************************************
2025-03-15 14:56:02.329051: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-03-15 14:56:02.329060: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-03-15 14:56:02.329051: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-03-15 14:56:02.329052: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-03-15 14:56:02.329051: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-03-15 14:56:02.329051: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-03-15 14:56:02.329061: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-03-15 14:56:02.329167: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1742050562.352204      51 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1742050562.352206      49 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1742050562.352204      48 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1742050562.352206      46 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1742050562.352204      47 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1742050562.352206      50 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1742050562.352207      45 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1742050562.352323      44 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1742050562.359289      51 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
E0000 00:00:1742050562.359289      48 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
E0000 00:00:1742050562.359289      47 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
E0000 00:00:1742050562.359291      50 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
E0000 00:00:1742050562.359301      45 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
E0000 00:00:1742050562.359300      49 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
E0000 00:00:1742050562.359302      46 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
E0000 00:00:1742050562.359324      44 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
[rank5]:[W315 14:56:39.895511878 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 5]  using GPU 5 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank1]:[W315 14:56:39.895727550 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank2]:[W315 14:56:39.896257448 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank4]:[W315 14:56:39.896369877 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 4]  using GPU 4 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank7]:[W315 14:56:39.896746967 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 7]  using GPU 7 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank6]:[W315 14:56:39.897095046 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 6]  using GPU 6 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank3]:[W315 14:56:39.897176039 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank0]:[W315 14:56:40.056584753 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
I0315 14:56:41.945517 140128648783040 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_0/fastmri_pytorch.
I0315 14:56:41.945516 140545246360768 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_0/fastmri_pytorch.
I0315 14:56:41.945516 140107889456320 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_0/fastmri_pytorch.
I0315 14:56:41.945524 140398947562688 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_0/fastmri_pytorch.
I0315 14:56:41.945515 140599044539584 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_0/fastmri_pytorch.
I0315 14:56:41.945528 139816173798592 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_0/fastmri_pytorch.
I0315 14:56:41.945515 140139585799360 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_0/fastmri_pytorch.
I0315 14:56:41.945623 140204161926336 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_0/fastmri_pytorch.
I0315 14:56:42.372561 140545246360768 submission_runner.py:606] Using RNG seed -783906668
I0315 14:56:42.373413 140107889456320 logger_utils.py:97] Saving hparams to /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_0/fastmri_pytorch/trial_4/hparams.json.
I0315 14:56:42.373415 140599044539584 logger_utils.py:97] Saving hparams to /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_0/fastmri_pytorch/trial_4/hparams.json.
I0315 14:56:42.373416 139816173798592 logger_utils.py:97] Saving hparams to /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_0/fastmri_pytorch/trial_4/hparams.json.
I0315 14:56:42.373414 140398947562688 logger_utils.py:97] Saving hparams to /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_0/fastmri_pytorch/trial_4/hparams.json.
I0315 14:56:42.373435 140204161926336 logger_utils.py:97] Saving hparams to /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_0/fastmri_pytorch/trial_4/hparams.json.
I0315 14:56:42.373430 140128648783040 logger_utils.py:97] Saving hparams to /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_0/fastmri_pytorch/trial_4/hparams.json.
I0315 14:56:42.373851 140545246360768 submission_runner.py:615] --- Tuning run 4/5 ---
I0315 14:56:42.373972 140545246360768 submission_runner.py:620] Creating tuning directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_0/fastmri_pytorch/trial_4.
I0315 14:56:42.373593 140139585799360 logger_utils.py:97] Saving hparams to /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_0/fastmri_pytorch/trial_4/hparams.json.
I0315 14:56:42.374207 140545246360768 logger_utils.py:97] Saving hparams to /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_0/fastmri_pytorch/trial_4/hparams.json.
I0315 14:56:42.722157 140545246360768 submission_runner.py:218] Initializing dataset.
I0315 14:56:42.722344 140545246360768 submission_runner.py:229] Initializing model.
I0315 14:56:42.894470 140545246360768 submission_runner.py:268] Performing `torch.compile`.
I0315 14:56:44.789557 140545246360768 submission_runner.py:272] Initializing optimizer.
W0315 14:56:44.790955 140128648783040 distributed_shampoo.py:341] start_preconditioning_step set to -1. Setting start_preconditioning_step equal to precondition frequency 100 by default.
W0315 14:56:44.790959 140599044539584 distributed_shampoo.py:341] start_preconditioning_step set to -1. Setting start_preconditioning_step equal to precondition frequency 100 by default.
W0315 14:56:44.790960 140204161926336 distributed_shampoo.py:341] start_preconditioning_step set to -1. Setting start_preconditioning_step equal to precondition frequency 100 by default.
W0315 14:56:44.790978 139816173798592 distributed_shampoo.py:341] start_preconditioning_step set to -1. Setting start_preconditioning_step equal to precondition frequency 100 by default.
W0315 14:56:44.791017 140107889456320 distributed_shampoo.py:341] start_preconditioning_step set to -1. Setting start_preconditioning_step equal to precondition frequency 100 by default.
W0315 14:56:44.791043 140398947562688 distributed_shampoo.py:341] start_preconditioning_step set to -1. Setting start_preconditioning_step equal to precondition frequency 100 by default.
W0315 14:56:44.791055 140139585799360 distributed_shampoo.py:341] start_preconditioning_step set to -1. Setting start_preconditioning_step equal to precondition frequency 100 by default.
W0315 14:56:44.791114 140545246360768 distributed_shampoo.py:341] start_preconditioning_step set to -1. Setting start_preconditioning_step equal to precondition frequency 100 by default.
I0315 14:56:44.794862 139816173798592 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 0.block_0 ([torch.Size([0])]) for Parameter 0 (torch.Size([32, 1, 3, 3])), Block block_0 (torch.Size([288])).
I0315 14:56:44.794899 140204161926336 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 0.block_0 ([torch.Size([0])]) for Parameter 0 (torch.Size([32, 1, 3, 3])), Block block_0 (torch.Size([288])).
I0315 14:56:44.794931 140128648783040 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 0.block_0 ([torch.Size([288, 288])]) for Parameter 0 (torch.Size([32, 1, 3, 3])), Block block_0 (torch.Size([288])).
I0315 14:56:44.794974 140599044539584 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 0.block_0 ([torch.Size([0])]) for Parameter 0 (torch.Size([32, 1, 3, 3])), Block block_0 (torch.Size([288])).
I0315 14:56:44.795031 140545246360768 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 0.block_0 ([torch.Size([0])]) for Parameter 0 (torch.Size([32, 1, 3, 3])), Block block_0 (torch.Size([288])).
I0315 14:56:44.795139 140139585799360 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 0.block_0 ([torch.Size([0])]) for Parameter 0 (torch.Size([32, 1, 3, 3])), Block block_0 (torch.Size([288])).
I0315 14:56:44.795086 140107889456320 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 0.block_0 ([torch.Size([0])]) for Parameter 0 (torch.Size([32, 1, 3, 3])), Block block_0 (torch.Size([288])).
I0315 14:56:44.797401 140204161926336 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 1.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 1 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 14:56:44.797414 140128648783040 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 1.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 1 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 14:56:44.795180 140398947562688 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 0.block_0 ([torch.Size([0])]) for Parameter 0 (torch.Size([32, 1, 3, 3])), Block block_0 (torch.Size([288])).
I0315 14:56:44.797441 139816173798592 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 1.block_0 ([torch.Size([1024, 1024]), torch.Size([9, 9])]) for Parameter 1 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 14:56:44.797521 140545246360768 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 1.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 1 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 14:56:44.797506 140599044539584 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 1.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 1 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 14:56:44.797579 140204161926336 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 2.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 2 (torch.Size([64, 32, 3, 3])), Block block_0 (torch.Size([64, 288])).
I0315 14:56:44.797583 140128648783040 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 2.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 2 (torch.Size([64, 32, 3, 3])), Block block_0 (torch.Size([64, 288])).
I0315 14:56:44.797672 140107889456320 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 1.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 1 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 14:56:44.797693 139816173798592 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 2.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 2 (torch.Size([64, 32, 3, 3])), Block block_0 (torch.Size([64, 288])).
I0315 14:56:44.797675 140139585799360 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 1.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 1 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 14:56:44.797742 140545246360768 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 2.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 2 (torch.Size([64, 32, 3, 3])), Block block_0 (torch.Size([64, 288])).
I0315 14:56:44.797742 140599044539584 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 2.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 2 (torch.Size([64, 32, 3, 3])), Block block_0 (torch.Size([64, 288])).
I0315 14:56:44.797751 140398947562688 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 1.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 1 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 14:56:44.797795 140128648783040 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 3.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 3 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 14:56:44.797811 140204161926336 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 3.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 3 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 14:56:44.797881 139816173798592 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 3.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 3 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 14:56:44.797901 140139585799360 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 2.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 2 (torch.Size([64, 32, 3, 3])), Block block_0 (torch.Size([64, 288])).
I0315 14:56:44.797908 140545246360768 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 3.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 3 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 14:56:44.797906 140107889456320 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 2.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 2 (torch.Size([64, 32, 3, 3])), Block block_0 (torch.Size([64, 288])).
I0315 14:56:44.797974 140128648783040 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 4.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 4 (torch.Size([128, 64, 3, 3])), Block block_0 (torch.Size([128, 576])).
I0315 14:56:44.797981 140204161926336 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 4.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 4 (torch.Size([128, 64, 3, 3])), Block block_0 (torch.Size([128, 576])).
I0315 14:56:44.798063 140545246360768 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 4.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 4 (torch.Size([128, 64, 3, 3])), Block block_0 (torch.Size([128, 576])).
I0315 14:56:44.798065 140139585799360 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 3.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 3 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 14:56:44.798068 140107889456320 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 3.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 3 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 14:56:44.798072 140599044539584 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 3.block_0 ([torch.Size([64, 64]), torch.Size([576, 576])]) for Parameter 3 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 14:56:44.798148 140204161926336 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 5.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 5 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 14:56:44.798118 140398947562688 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 2.block_0 ([torch.Size([64, 64]), torch.Size([288, 288])]) for Parameter 2 (torch.Size([64, 32, 3, 3])), Block block_0 (torch.Size([64, 288])).
I0315 14:56:44.798135 139816173798592 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 4.block_0 ([torch.Size([128, 128]), torch.Size([576, 576])]) for Parameter 4 (torch.Size([128, 64, 3, 3])), Block block_0 (torch.Size([128, 576])).
I0315 14:56:44.798207 140139585799360 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 4.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 4 (torch.Size([128, 64, 3, 3])), Block block_0 (torch.Size([128, 576])).
I0315 14:56:44.798211 140107889456320 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 4.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 4 (torch.Size([128, 64, 3, 3])), Block block_0 (torch.Size([128, 576])).
I0315 14:56:44.798225 140545246360768 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 5.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 5 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 14:56:44.798273 140599044539584 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 4.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 4 (torch.Size([128, 64, 3, 3])), Block block_0 (torch.Size([128, 576])).
I0315 14:56:44.798309 140204161926336 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 6.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 6 (torch.Size([256, 128, 3, 3])), Block block_0 (torch.Size([256, 384, 3])).
I0315 14:56:44.798322 139816173798592 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 5.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 5 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 14:56:44.798311 140398947562688 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 3.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 3 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 14:56:44.798373 140545246360768 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 6.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 6 (torch.Size([256, 128, 3, 3])), Block block_0 (torch.Size([256, 384, 3])).
I0315 14:56:44.798377 140107889456320 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 5.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 5 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 14:56:44.798386 140139585799360 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 5.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 5 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 14:56:44.798468 140204161926336 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 7.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 7 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 14:56:44.798469 140599044539584 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 5.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 5 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 14:56:44.798465 140398947562688 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 4.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 4 (torch.Size([128, 64, 3, 3])), Block block_0 (torch.Size([128, 576])).
I0315 14:56:44.798489 139816173798592 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 6.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 6 (torch.Size([256, 128, 3, 3])), Block block_0 (torch.Size([256, 384, 3])).
I0315 14:56:44.798549 140107889456320 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 6.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 6 (torch.Size([256, 128, 3, 3])), Block block_0 (torch.Size([256, 384, 3])).
I0315 14:56:44.798552 140545246360768 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 7.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 7 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 14:56:44.798557 140139585799360 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 6.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 6 (torch.Size([256, 128, 3, 3])), Block block_0 (torch.Size([256, 384, 3])).
I0315 14:56:44.798641 140599044539584 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 6.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 6 (torch.Size([256, 128, 3, 3])), Block block_0 (torch.Size([256, 384, 3])).
I0315 14:56:44.798655 140398947562688 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 5.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 5 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 14:56:44.798653 140204161926336 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 8.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 8 (torch.Size([512, 256, 3, 3])), Block block_0 (torch.Size([512, 768, 3])).
I0315 14:56:44.798675 139816173798592 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 7.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 7 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 14:56:44.798710 140545246360768 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 8.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 8 (torch.Size([512, 256, 3, 3])), Block block_0 (torch.Size([512, 768, 3])).
I0315 14:56:44.798722 140139585799360 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 7.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 7 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 14:56:44.798731 140107889456320 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 7.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 7 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 14:56:44.798807 140398947562688 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 6.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 6 (torch.Size([256, 128, 3, 3])), Block block_0 (torch.Size([256, 384, 3])).
I0315 14:56:44.798813 140204161926336 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 9.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 9 (torch.Size([512, 512, 3, 3])), Block block_0 (torch.Size([512, 512, 9])).
I0315 14:56:44.798817 140599044539584 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 7.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 7 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 14:56:44.798804 140128648783040 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 5.block_0 ([torch.Size([128, 128]), torch.Size([384, 384]), torch.Size([3, 3])]) for Parameter 5 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 14:56:44.798853 139816173798592 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 8.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 8 (torch.Size([512, 256, 3, 3])), Block block_0 (torch.Size([512, 768, 3])).
I0315 14:56:44.798885 140107889456320 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 8.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 8 (torch.Size([512, 256, 3, 3])), Block block_0 (torch.Size([512, 768, 3])).
I0315 14:56:44.798957 140204161926336 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 10.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 10 (torch.Size([256, 512, 3, 3])), Block block_0 (torch.Size([256, 512, 9])).
I0315 14:56:44.798973 140599044539584 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 8.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 8 (torch.Size([512, 256, 3, 3])), Block block_0 (torch.Size([512, 768, 3])).
I0315 14:56:44.799008 139816173798592 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 9.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 9 (torch.Size([512, 512, 3, 3])), Block block_0 (torch.Size([512, 512, 9])).
I0315 14:56:44.799057 140107889456320 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 9.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 9 (torch.Size([512, 512, 3, 3])), Block block_0 (torch.Size([512, 512, 9])).
I0315 14:56:44.799138 140599044539584 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 9.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 9 (torch.Size([512, 512, 3, 3])), Block block_0 (torch.Size([512, 512, 9])).
I0315 14:56:44.799167 139816173798592 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 10.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 10 (torch.Size([256, 512, 3, 3])), Block block_0 (torch.Size([256, 512, 9])).
I0315 14:56:44.799285 140599044539584 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 10.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 10 (torch.Size([256, 512, 3, 3])), Block block_0 (torch.Size([256, 512, 9])).
I0315 14:56:44.799291 140204161926336 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 11.block_0 ([torch.Size([256, 256]), torch.Size([768, 768]), torch.Size([3, 3])]) for Parameter 11 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 14:56:44.799318 139816173798592 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 11.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 11 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 14:56:44.799437 140599044539584 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 11.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 11 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 14:56:44.799460 140204161926336 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 12.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 12 (torch.Size([128, 256, 3, 3])), Block block_0 (torch.Size([128, 768, 3])).
I0315 14:56:44.799583 140599044539584 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 12.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 12 (torch.Size([128, 256, 3, 3])), Block block_0 (torch.Size([128, 768, 3])).
I0315 14:56:44.799620 140204161926336 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 13.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 13 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 14:56:44.799740 140599044539584 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 13.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 13 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 14:56:44.799781 140204161926336 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 14.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 14 (torch.Size([64, 128, 3, 3])), Block block_0 (torch.Size([64, 384, 3])).
I0315 14:56:44.799889 140599044539584 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 14.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 14 (torch.Size([64, 128, 3, 3])), Block block_0 (torch.Size([64, 384, 3])).
I0315 14:56:44.799911 140204161926336 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 15.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 15 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 14:56:44.800124 140204161926336 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 16.block_0 ([torch.Size([32, 32]), torch.Size([576, 576])]) for Parameter 16 (torch.Size([32, 64, 3, 3])), Block block_0 (torch.Size([32, 576])).
I0315 14:56:44.800127 140599044539584 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 15.block_0 ([torch.Size([64, 64]), torch.Size([576, 576])]) for Parameter 15 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 14:56:44.800145 140139585799360 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 8.block_0 ([torch.Size([512, 512]), torch.Size([768, 768]), torch.Size([3, 3])]) for Parameter 8 (torch.Size([512, 256, 3, 3])), Block block_0 (torch.Size([512, 768, 3])).
I0315 14:56:44.800263 140599044539584 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 16.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 16 (torch.Size([32, 64, 3, 3])), Block block_0 (torch.Size([32, 576])).
I0315 14:56:44.800243 140128648783040 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 6.block_0 ([torch.Size([256, 256]), torch.Size([384, 384]), torch.Size([3, 3])]) for Parameter 6 (torch.Size([256, 128, 3, 3])), Block block_0 (torch.Size([256, 384, 3])).
I0315 14:56:44.800266 140204161926336 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 17.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 17 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 14:56:44.800354 140204161926336 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 18.block_0 ([torch.Size([0])]) for Parameter 18 (torch.Size([1, 32, 1, 1])), Block block_0 (torch.Size([32])).
I0315 14:56:44.800356 140139585799360 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 9.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 9 (torch.Size([512, 512, 3, 3])), Block block_0 (torch.Size([512, 512, 9])).
I0315 14:56:44.800431 140128648783040 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 7.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 7 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 14:56:44.800449 140204161926336 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 19.block_0 ([torch.Size([0])]) for Parameter 19 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0315 14:56:44.800507 140139585799360 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 10.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 10 (torch.Size([256, 512, 3, 3])), Block block_0 (torch.Size([256, 512, 9])).
I0315 14:56:44.800514 140398947562688 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 7.block_0 ([torch.Size([256, 256]), torch.Size([768, 768]), torch.Size([3, 3])]) for Parameter 7 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 14:56:44.800570 140204161926336 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 20.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 20 (torch.Size([512, 256, 2, 2])), Block block_0 (torch.Size([512, 1024])).
I0315 14:56:44.800606 140128648783040 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 8.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 8 (torch.Size([512, 256, 3, 3])), Block block_0 (torch.Size([512, 768, 3])).
I0315 14:56:44.800664 140139585799360 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 11.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 11 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 14:56:44.800707 140204161926336 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 21.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 21 (torch.Size([256, 128, 2, 2])), Block block_0 (torch.Size([256, 512])).
I0315 14:56:44.800719 140398947562688 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 8.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 8 (torch.Size([512, 256, 3, 3])), Block block_0 (torch.Size([512, 768, 3])).
I0315 14:56:44.800743 140107889456320 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 10.block_0 ([torch.Size([256, 256]), torch.Size([512, 512]), torch.Size([9, 9])]) for Parameter 10 (torch.Size([256, 512, 3, 3])), Block block_0 (torch.Size([256, 512, 9])).
I0315 14:56:44.800776 140128648783040 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 9.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 9 (torch.Size([512, 512, 3, 3])), Block block_0 (torch.Size([512, 512, 9])).
I0315 14:56:44.800821 140139585799360 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 12.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 12 (torch.Size([128, 256, 3, 3])), Block block_0 (torch.Size([128, 768, 3])).
I0315 14:56:44.800833 140204161926336 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 22.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 22 (torch.Size([128, 64, 2, 2])), Block block_0 (torch.Size([128, 256])).
I0315 14:56:44.800854 139816173798592 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 12.block_0 ([torch.Size([128, 128]), torch.Size([768, 768]), torch.Size([3, 3])]) for Parameter 12 (torch.Size([128, 256, 3, 3])), Block block_0 (torch.Size([128, 768, 3])).
I0315 14:56:44.800884 140398947562688 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 9.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 9 (torch.Size([512, 512, 3, 3])), Block block_0 (torch.Size([512, 512, 9])).
I0315 14:56:44.800874 140545246360768 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 9.block_0 ([torch.Size([512, 512]), torch.Size([512, 512]), torch.Size([9, 9])]) for Parameter 9 (torch.Size([512, 512, 3, 3])), Block block_0 (torch.Size([512, 512, 9])).
I0315 14:56:44.800920 140128648783040 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 10.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 10 (torch.Size([256, 512, 3, 3])), Block block_0 (torch.Size([256, 512, 9])).
I0315 14:56:44.800946 140204161926336 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 23.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 23 (torch.Size([64, 32, 2, 2])), Block block_0 (torch.Size([64, 128])).
I0315 14:56:44.800952 140107889456320 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 11.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 11 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 14:56:44.800977 140139585799360 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 13.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 13 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 14:56:44.801036 140398947562688 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 10.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 10 (torch.Size([256, 512, 3, 3])), Block block_0 (torch.Size([256, 512, 9])).
I0315 14:56:44.801048 140204161926336 shampoo_preconditioner_list.py:612] Rank 4: ShampooPreconditionerList Numel Breakdown: (165888, 2097314, 174080, 671744, 696320, 327698, 426002, 1310738, 1703954, 1048738, 655522, 1310738, 1212434, 327698, 303122, 671744, 665600, 2097314, 2048, 2, 2621440, 655360, 163840, 40960)
I0315 14:56:44.801063 140128648783040 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 11.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 11 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 14:56:44.801073 140545246360768 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 10.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 10 (torch.Size([256, 512, 3, 3])), Block block_0 (torch.Size([256, 512, 9])).
I0315 14:56:44.801089 140204161926336 shampoo_preconditioner_list.py:615] Rank 4: ShampooPreconditionerList Bytes Breakdown: (663552, 8389256)
I0315 14:56:44.801098 140107889456320 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 12.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 12 (torch.Size([128, 256, 3, 3])), Block block_0 (torch.Size([128, 768, 3])).
I0315 14:56:44.801123 140204161926336 shampoo_preconditioner_list.py:618] Rank 4: ShampooPreconditionerList Total Elements: 19350298
I0315 14:56:44.801135 140139585799360 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 14.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 14 (torch.Size([64, 128, 3, 3])), Block block_0 (torch.Size([64, 384, 3])).
I0315 14:56:44.801152 140204161926336 shampoo_preconditioner_list.py:621] Rank 4: ShampooPreconditionerList Total Bytes: 9052808
I0315 14:56:44.801128 140599044539584 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 17.block_0 ([torch.Size([1024, 1024]), torch.Size([9, 9])]) for Parameter 17 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 14:56:44.801194 140398947562688 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 11.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 11 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 14:56:44.801200 140128648783040 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 12.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 12 (torch.Size([128, 256, 3, 3])), Block block_0 (torch.Size([128, 768, 3])).
I0315 14:56:44.801228 140545246360768 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 11.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 11 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 14:56:44.801248 140107889456320 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 13.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 13 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 14:56:44.801253 140599044539584 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 18.block_0 ([torch.Size([0])]) for Parameter 18 (torch.Size([1, 32, 1, 1])), Block block_0 (torch.Size([32])).
I0315 14:56:44.801261 140139585799360 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 15.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 15 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 14:56:44.801342 140599044539584 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 19.block_0 ([torch.Size([0])]) for Parameter 19 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0315 14:56:44.801337 140128648783040 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 13.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 13 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 14:56:44.801337 140204161926336 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([32, 1, 3, 3])), Block block_0 (torch.Size([288])).
I0315 14:56:44.801348 140398947562688 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 12.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 12 (torch.Size([128, 256, 3, 3])), Block block_0 (torch.Size([128, 768, 3])).
I0315 14:56:44.801382 140139585799360 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 16.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 16 (torch.Size([32, 64, 3, 3])), Block block_0 (torch.Size([32, 576])).
I0315 14:56:44.801383 140545246360768 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 12.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 12 (torch.Size([128, 256, 3, 3])), Block block_0 (torch.Size([128, 768, 3])).
I0315 14:56:44.801404 140204161926336 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 14:56:44.801409 140107889456320 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 14.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 14 (torch.Size([64, 128, 3, 3])), Block block_0 (torch.Size([64, 384, 3])).
I0315 14:56:44.801470 140204161926336 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([64, 32, 3, 3])), Block block_0 (torch.Size([64, 288])).
I0315 14:56:44.801486 140398947562688 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 13.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 13 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 14:56:44.801499 140139585799360 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 17.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 17 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 14:56:44.801508 140128648783040 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 14.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 14 (torch.Size([64, 128, 3, 3])), Block block_0 (torch.Size([64, 384, 3])).
I0315 14:56:44.801523 140545246360768 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 13.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 13 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 14:56:44.801531 140204161926336 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 14:56:44.801510 139816173798592 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 13.block_0 ([torch.Size([128, 128]), torch.Size([384, 384]), torch.Size([3, 3])]) for Parameter 13 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 14:56:44.801537 140107889456320 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 15.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 15 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 14:56:44.801582 140204161926336 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 4.block_0 (torch.Size([0])) for Parameter 4 (torch.Size([128, 64, 3, 3])), Block block_0 (torch.Size([128, 576])).
I0315 14:56:44.801613 140139585799360 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 18.block_0 ([torch.Size([0])]) for Parameter 18 (torch.Size([1, 32, 1, 1])), Block block_0 (torch.Size([32])).
I0315 14:56:44.801654 140398947562688 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 14.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 14 (torch.Size([64, 128, 3, 3])), Block block_0 (torch.Size([64, 384, 3])).
I0315 14:56:44.801662 140204161926336 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 5.block_0 (torch.Size([0])) for Parameter 5 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 14:56:44.801668 140128648783040 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 15.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 15 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 14:56:44.801678 140107889456320 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 16.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 16 (torch.Size([32, 64, 3, 3])), Block block_0 (torch.Size([32, 576])).
I0315 14:56:44.801699 140545246360768 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 14.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 14 (torch.Size([64, 128, 3, 3])), Block block_0 (torch.Size([64, 384, 3])).
I0315 14:56:44.801702 140139585799360 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 19.block_0 ([torch.Size([0])]) for Parameter 19 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0315 14:56:44.801715 140204161926336 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 6.block_0 (torch.Size([0])) for Parameter 6 (torch.Size([256, 128, 3, 3])), Block block_0 (torch.Size([256, 384, 3])).
I0315 14:56:44.801782 140204161926336 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 7.block_0 (torch.Size([0])) for Parameter 7 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 14:56:44.801788 140398947562688 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 15.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 15 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 14:56:44.801797 140128648783040 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 16.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 16 (torch.Size([32, 64, 3, 3])), Block block_0 (torch.Size([32, 576])).
I0315 14:56:44.801822 140107889456320 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 17.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 17 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 14:56:44.801812 139816173798592 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 14.block_0 ([torch.Size([64, 64]), torch.Size([384, 384]), torch.Size([3, 3])]) for Parameter 14 (torch.Size([64, 128, 3, 3])), Block block_0 (torch.Size([64, 384, 3])).
I0315 14:56:44.801831 140545246360768 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 15.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 15 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 14:56:44.801836 140139585799360 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 20.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 20 (torch.Size([512, 256, 2, 2])), Block block_0 (torch.Size([512, 1024])).
I0315 14:56:44.801843 140204161926336 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 8.block_0 (torch.Size([0])) for Parameter 8 (torch.Size([512, 256, 3, 3])), Block block_0 (torch.Size([512, 768, 3])).
I0315 14:56:44.801901 140204161926336 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 9.block_0 (torch.Size([0])) for Parameter 9 (torch.Size([512, 512, 3, 3])), Block block_0 (torch.Size([512, 512, 9])).
I0315 14:56:44.801904 140398947562688 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 16.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 16 (torch.Size([32, 64, 3, 3])), Block block_0 (torch.Size([32, 576])).
I0315 14:56:44.801916 140128648783040 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 17.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 17 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 14:56:44.801923 140107889456320 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 18.block_0 ([torch.Size([0])]) for Parameter 18 (torch.Size([1, 32, 1, 1])), Block block_0 (torch.Size([32])).
I0315 14:56:44.801959 140204161926336 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 10.block_0 (torch.Size([0])) for Parameter 10 (torch.Size([256, 512, 3, 3])), Block block_0 (torch.Size([256, 512, 9])).
I0315 14:56:44.801950 140545246360768 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 16.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 16 (torch.Size([32, 64, 3, 3])), Block block_0 (torch.Size([32, 576])).
I0315 14:56:44.801965 140139585799360 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 21.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 21 (torch.Size([256, 128, 2, 2])), Block block_0 (torch.Size([256, 512])).
I0315 14:56:44.801947 140599044539584 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 20.block_0 ([torch.Size([512, 512]), torch.Size([1024, 1024])]) for Parameter 20 (torch.Size([512, 256, 2, 2])), Block block_0 (torch.Size([512, 1024])).
I0315 14:56:44.801974 139816173798592 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 15.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 15 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 14:56:44.802009 140107889456320 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 19.block_0 ([torch.Size([0])]) for Parameter 19 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0315 14:56:44.802041 140398947562688 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 17.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 17 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 14:56:44.802069 140204161926336 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 11.block_0 (torch.Size([256, 768, 3])) for Parameter 11 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 14:56:44.802077 140545246360768 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 17.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 17 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 14:56:44.802082 140139585799360 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 22.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 22 (torch.Size([128, 64, 2, 2])), Block block_0 (torch.Size([128, 256])).
I0315 14:56:44.802087 140128648783040 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 18.block_0 ([torch.Size([32, 32])]) for Parameter 18 (torch.Size([1, 32, 1, 1])), Block block_0 (torch.Size([32])).
I0315 14:56:44.802110 139816173798592 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 16.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 16 (torch.Size([32, 64, 3, 3])), Block block_0 (torch.Size([32, 576])).
I0315 14:56:44.802112 140599044539584 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 21.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 21 (torch.Size([256, 128, 2, 2])), Block block_0 (torch.Size([256, 512])).
I0315 14:56:44.802135 140107889456320 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 20.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 20 (torch.Size([512, 256, 2, 2])), Block block_0 (torch.Size([512, 1024])).
I0315 14:56:44.802147 140204161926336 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 12.block_0 (torch.Size([0])) for Parameter 12 (torch.Size([128, 256, 3, 3])), Block block_0 (torch.Size([128, 768, 3])).
I0315 14:56:44.802145 140398947562688 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 18.block_0 ([torch.Size([0])]) for Parameter 18 (torch.Size([1, 32, 1, 1])), Block block_0 (torch.Size([32])).
I0315 14:56:44.802164 140545246360768 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 18.block_0 ([torch.Size([0])]) for Parameter 18 (torch.Size([1, 32, 1, 1])), Block block_0 (torch.Size([32])).
I0315 14:56:44.802195 140139585799360 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 23.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 23 (torch.Size([64, 32, 2, 2])), Block block_0 (torch.Size([64, 128])).
I0315 14:56:44.802200 140204161926336 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 13.block_0 (torch.Size([0])) for Parameter 13 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 14:56:44.802230 140398947562688 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 19.block_0 ([torch.Size([0])]) for Parameter 19 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0315 14:56:44.802232 139816173798592 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 17.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 17 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 14:56:44.802235 140599044539584 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 22.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 22 (torch.Size([128, 64, 2, 2])), Block block_0 (torch.Size([128, 256])).
I0315 14:56:44.802251 140204161926336 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 14.block_0 (torch.Size([0])) for Parameter 14 (torch.Size([64, 128, 3, 3])), Block block_0 (torch.Size([64, 384, 3])).
I0315 14:56:44.802249 140128648783040 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 19.block_0 ([torch.Size([1, 1])]) for Parameter 19 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0315 14:56:44.802268 140545246360768 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 19.block_0 ([torch.Size([0])]) for Parameter 19 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0315 14:56:44.802270 140107889456320 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 21.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 21 (torch.Size([256, 128, 2, 2])), Block block_0 (torch.Size([256, 512])).
I0315 14:56:44.802309 140204161926336 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 15.block_0 (torch.Size([0])) for Parameter 15 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 14:56:44.802307 140139585799360 shampoo_preconditioner_list.py:612] Rank 1: ShampooPreconditionerList Numel Breakdown: (165888, 2097314, 174080, 671744, 696320, 327698, 426002, 1310738, 1703954, 1048738, 655522, 1310738, 1212434, 327698, 303122, 671744, 665600, 2097314, 2048, 2, 2621440, 655360, 163840, 40960)
I0315 14:56:44.802325 139816173798592 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 18.block_0 ([torch.Size([0])]) for Parameter 18 (torch.Size([1, 32, 1, 1])), Block block_0 (torch.Size([32])).
I0315 14:56:44.802348 140139585799360 shampoo_preconditioner_list.py:615] Rank 1: ShampooPreconditionerList Bytes Breakdown: (663552,)
I0315 14:56:44.802347 140599044539584 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 23.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 23 (torch.Size([64, 32, 2, 2])), Block block_0 (torch.Size([64, 128])).
I0315 14:56:44.802351 140398947562688 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 20.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 20 (torch.Size([512, 256, 2, 2])), Block block_0 (torch.Size([512, 1024])).
I0315 14:56:44.802381 140128648783040 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 20.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 20 (torch.Size([512, 256, 2, 2])), Block block_0 (torch.Size([512, 1024])).
I0315 14:56:44.802388 140139585799360 shampoo_preconditioner_list.py:618] Rank 1: ShampooPreconditionerList Total Elements: 19350298
I0315 14:56:44.802385 140545246360768 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 20.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 20 (torch.Size([512, 256, 2, 2])), Block block_0 (torch.Size([512, 1024])).
I0315 14:56:44.802386 140107889456320 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 22.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 22 (torch.Size([128, 64, 2, 2])), Block block_0 (torch.Size([128, 256])).
I0315 14:56:44.802397 140204161926336 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 16.block_0 (torch.Size([32, 576])) for Parameter 16 (torch.Size([32, 64, 3, 3])), Block block_0 (torch.Size([32, 576])).
I0315 14:56:44.802420 140139585799360 shampoo_preconditioner_list.py:621] Rank 1: ShampooPreconditionerList Total Bytes: 663552
I0315 14:56:44.802419 139816173798592 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 19.block_0 ([torch.Size([0])]) for Parameter 19 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0315 14:56:44.802460 140204161926336 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 17.block_0 (torch.Size([0])) for Parameter 17 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 14:56:44.802460 140599044539584 shampoo_preconditioner_list.py:612] Rank 5: ShampooPreconditionerList Numel Breakdown: (165888, 2097314, 174080, 671744, 696320, 327698, 426002, 1310738, 1703954, 1048738, 655522, 1310738, 1212434, 327698, 303122, 671744, 665600, 2097314, 2048, 2, 2621440, 655360, 163840, 40960)
I0315 14:56:44.802478 140398947562688 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 21.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 21 (torch.Size([256, 128, 2, 2])), Block block_0 (torch.Size([256, 512])).
I0315 14:56:44.802502 140599044539584 shampoo_preconditioner_list.py:615] Rank 5: ShampooPreconditionerList Bytes Breakdown: (663552, 8389256, 696320, 2686976)
I0315 14:56:44.802504 140107889456320 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 23.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 23 (torch.Size([64, 32, 2, 2])), Block block_0 (torch.Size([64, 128])).
I0315 14:56:44.802512 140545246360768 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 21.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 21 (torch.Size([256, 128, 2, 2])), Block block_0 (torch.Size([256, 512])).
I0315 14:56:44.802522 140204161926336 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 18.block_0 (torch.Size([0])) for Parameter 18 (torch.Size([1, 32, 1, 1])), Block block_0 (torch.Size([32])).
I0315 14:56:44.802535 139816173798592 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 20.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 20 (torch.Size([512, 256, 2, 2])), Block block_0 (torch.Size([512, 1024])).
I0315 14:56:44.802549 140599044539584 shampoo_preconditioner_list.py:618] Rank 5: ShampooPreconditionerList Total Elements: 19350298
I0315 14:56:44.802571 140204161926336 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 19.block_0 (torch.Size([0])) for Parameter 19 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0315 14:56:44.802582 140599044539584 shampoo_preconditioner_list.py:621] Rank 5: ShampooPreconditionerList Total Bytes: 12436104
I0315 14:56:44.802602 140398947562688 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 22.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 22 (torch.Size([128, 64, 2, 2])), Block block_0 (torch.Size([128, 256])).
I0315 14:56:44.802610 140107889456320 shampoo_preconditioner_list.py:612] Rank 2: ShampooPreconditionerList Numel Breakdown: (165888, 2097314, 174080, 671744, 696320, 327698, 426002, 1310738, 1703954, 1048738, 655522, 1310738, 1212434, 327698, 303122, 671744, 665600, 2097314, 2048, 2, 2621440, 655360, 163840, 40960)
I0315 14:56:44.802629 140204161926336 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 20.block_0 (torch.Size([0])) for Parameter 20 (torch.Size([512, 256, 2, 2])), Block block_0 (torch.Size([512, 1024])).
I0315 14:56:44.802617 140139585799360 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([32, 1, 3, 3])), Block block_0 (torch.Size([288])).
I0315 14:56:44.802629 140545246360768 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 22.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 22 (torch.Size([128, 64, 2, 2])), Block block_0 (torch.Size([128, 256])).
I0315 14:56:44.802650 140107889456320 shampoo_preconditioner_list.py:615] Rank 2: ShampooPreconditionerList Bytes Breakdown: (663552,)
I0315 14:56:44.802679 140204161926336 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 21.block_0 (torch.Size([0])) for Parameter 21 (torch.Size([256, 128, 2, 2])), Block block_0 (torch.Size([256, 512])).
I0315 14:56:44.802683 139816173798592 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 21.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 21 (torch.Size([256, 128, 2, 2])), Block block_0 (torch.Size([256, 512])).
I0315 14:56:44.802705 140107889456320 shampoo_preconditioner_list.py:618] Rank 2: ShampooPreconditionerList Total Elements: 19350298
I0315 14:56:44.802691 140139585799360 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 14:56:44.802721 140398947562688 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 23.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 23 (torch.Size([64, 32, 2, 2])), Block block_0 (torch.Size([64, 128])).
I0315 14:56:44.802743 140107889456320 shampoo_preconditioner_list.py:621] Rank 2: ShampooPreconditionerList Total Bytes: 663552
I0315 14:56:44.802737 140204161926336 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 22.block_0 (torch.Size([0])) for Parameter 22 (torch.Size([128, 64, 2, 2])), Block block_0 (torch.Size([128, 256])).
I0315 14:56:44.802744 140545246360768 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 23.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 23 (torch.Size([64, 32, 2, 2])), Block block_0 (torch.Size([64, 128])).
I0315 14:56:44.802764 140139585799360 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([64, 32, 3, 3])), Block block_0 (torch.Size([64, 288])).
I0315 14:56:44.802786 140599044539584 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([32, 1, 3, 3])), Block block_0 (torch.Size([288])).
I0315 14:56:44.802799 140204161926336 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 23.block_0 (torch.Size([0])) for Parameter 23 (torch.Size([64, 32, 2, 2])), Block block_0 (torch.Size([64, 128])).
I0315 14:56:44.802803 139816173798592 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 22.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 22 (torch.Size([128, 64, 2, 2])), Block block_0 (torch.Size([128, 256])).
I0315 14:56:44.802828 140139585799360 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 14:56:44.802834 140398947562688 shampoo_preconditioner_list.py:612] Rank 3: ShampooPreconditionerList Numel Breakdown: (165888, 2097314, 174080, 671744, 696320, 327698, 426002, 1310738, 1703954, 1048738, 655522, 1310738, 1212434, 327698, 303122, 671744, 665600, 2097314, 2048, 2, 2621440, 655360, 163840, 40960)
I0315 14:56:44.802853 140545246360768 shampoo_preconditioner_list.py:612] Rank 0: ShampooPreconditionerList Numel Breakdown: (165888, 2097314, 174080, 671744, 696320, 327698, 426002, 1310738, 1703954, 1048738, 655522, 1310738, 1212434, 327698, 303122, 671744, 665600, 2097314, 2048, 2, 2621440, 655360, 163840, 40960)
I0315 14:56:44.802867 140204161926336 shampoo_preconditioner_list.py:375] Rank 4: AdaGradPreconditionerList Numel Breakdown: (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 589824, 0, 0, 0, 0, 18432, 0, 0, 0, 0, 0, 0, 0)
I0315 14:56:44.802868 140599044539584 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 14:56:44.802875 140398947562688 shampoo_preconditioner_list.py:615] Rank 3: ShampooPreconditionerList Bytes Breakdown: (663552, 8389256)
I0315 14:56:44.802887 140139585799360 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 4.block_0 (torch.Size([0])) for Parameter 4 (torch.Size([128, 64, 3, 3])), Block block_0 (torch.Size([128, 576])).
I0315 14:56:44.802894 140545246360768 shampoo_preconditioner_list.py:615] Rank 0: ShampooPreconditionerList Bytes Breakdown: (663552,)
I0315 14:56:44.802904 140204161926336 shampoo_preconditioner_list.py:378] Rank 4: AdaGradPreconditionerList Bytes Breakdown: (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2359296, 0, 0, 0, 0, 73728, 0, 0, 0, 0, 0, 0, 0)
I0315 14:56:44.802909 140398947562688 shampoo_preconditioner_list.py:618] Rank 3: ShampooPreconditionerList Total Elements: 19350298
I0315 14:56:44.802927 140545246360768 shampoo_preconditioner_list.py:618] Rank 0: ShampooPreconditionerList Total Elements: 19350298
I0315 14:56:44.802925 140599044539584 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([64, 32, 3, 3])), Block block_0 (torch.Size([64, 288])).
I0315 14:56:44.802935 140204161926336 shampoo_preconditioner_list.py:381] Rank 4: AdaGradPreconditionerList Total Elements: 608256
I0315 14:56:44.802940 140398947562688 shampoo_preconditioner_list.py:621] Rank 3: ShampooPreconditionerList Total Bytes: 9052808
I0315 14:56:44.802942 140139585799360 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 5.block_0 (torch.Size([0])) for Parameter 5 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 14:56:44.802936 140107889456320 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([32, 1, 3, 3])), Block block_0 (torch.Size([288])).
I0315 14:56:44.802964 140204161926336 shampoo_preconditioner_list.py:384] Rank 4: AdaGradPreconditionerList Total Bytes: 2433024
I0315 14:56:44.802968 140545246360768 shampoo_preconditioner_list.py:621] Rank 0: ShampooPreconditionerList Total Bytes: 663552
I0315 14:56:44.802995 140139585799360 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 6.block_0 (torch.Size([0])) for Parameter 6 (torch.Size([256, 128, 3, 3])), Block block_0 (torch.Size([256, 384, 3])).
I0315 14:56:44.803003 140107889456320 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 14:56:44.802984 140128648783040 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 21.block_0 ([torch.Size([256, 256]), torch.Size([512, 512])]) for Parameter 21 (torch.Size([256, 128, 2, 2])), Block block_0 (torch.Size([256, 512])).
I0315 14:56:44.803008 139816173798592 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 23.block_0 ([torch.Size([64, 64]), torch.Size([128, 128])]) for Parameter 23 (torch.Size([64, 32, 2, 2])), Block block_0 (torch.Size([64, 128])).
I0315 14:56:44.803025 140599044539584 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([64, 576])) for Parameter 3 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 14:56:44.803056 140107889456320 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([64, 32, 3, 3])), Block block_0 (torch.Size([64, 288])).
I0315 14:56:44.803059 140139585799360 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 7.block_0 (torch.Size([0])) for Parameter 7 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 14:56:44.803092 140599044539584 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 4.block_0 (torch.Size([0])) for Parameter 4 (torch.Size([128, 64, 3, 3])), Block block_0 (torch.Size([128, 576])).
I0315 14:56:44.803118 140107889456320 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 14:56:44.803135 140398947562688 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([32, 1, 3, 3])), Block block_0 (torch.Size([288])).
I0315 14:56:44.803148 139816173798592 shampoo_preconditioner_list.py:612] Rank 7: ShampooPreconditionerList Numel Breakdown: (165888, 2097314, 174080, 671744, 696320, 327698, 426002, 1310738, 1703954, 1048738, 655522, 1310738, 1212434, 327698, 303122, 671744, 665600, 2097314, 2048, 2, 2621440, 655360, 163840, 40960)
I0315 14:56:44.803158 140599044539584 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 5.block_0 (torch.Size([0])) for Parameter 5 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 14:56:44.803166 140139585799360 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 8.block_0 (torch.Size([512, 768, 3])) for Parameter 8 (torch.Size([512, 256, 3, 3])), Block block_0 (torch.Size([512, 768, 3])).
I0315 14:56:44.803176 140107889456320 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 4.block_0 (torch.Size([0])) for Parameter 4 (torch.Size([128, 64, 3, 3])), Block block_0 (torch.Size([128, 576])).
I0315 14:56:44.803169 140545246360768 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([32, 1, 3, 3])), Block block_0 (torch.Size([288])).
I0315 14:56:44.803192 139816173798592 shampoo_preconditioner_list.py:615] Rank 7: ShampooPreconditionerList Bytes Breakdown: (663552, 8389256, 696320, 2686976, 2785280, 1310792)
I0315 14:56:44.803211 140599044539584 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 6.block_0 (torch.Size([0])) for Parameter 6 (torch.Size([256, 128, 3, 3])), Block block_0 (torch.Size([256, 384, 3])).
I0315 14:56:44.803214 140398947562688 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 14:56:44.803226 139816173798592 shampoo_preconditioner_list.py:618] Rank 7: ShampooPreconditionerList Total Elements: 19350298
I0315 14:56:44.803211 140128648783040 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 22.block_0 ([torch.Size([128, 128]), torch.Size([256, 256])]) for Parameter 22 (torch.Size([128, 64, 2, 2])), Block block_0 (torch.Size([128, 256])).
I0315 14:56:44.803229 140107889456320 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 5.block_0 (torch.Size([0])) for Parameter 5 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 14:56:44.803244 140545246360768 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 14:56:44.803247 140139585799360 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 9.block_0 (torch.Size([0])) for Parameter 9 (torch.Size([512, 512, 3, 3])), Block block_0 (torch.Size([512, 512, 9])).
I0315 14:56:44.803258 139816173798592 shampoo_preconditioner_list.py:621] Rank 7: ShampooPreconditionerList Total Bytes: 16532176
I0315 14:56:44.803279 140107889456320 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 6.block_0 (torch.Size([0])) for Parameter 6 (torch.Size([256, 128, 3, 3])), Block block_0 (torch.Size([256, 384, 3])).
I0315 14:56:44.803277 140599044539584 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 7.block_0 (torch.Size([0])) for Parameter 7 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 14:56:44.803301 140139585799360 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 10.block_0 (torch.Size([0])) for Parameter 10 (torch.Size([256, 512, 3, 3])), Block block_0 (torch.Size([256, 512, 9])).
I0315 14:56:44.803310 140545246360768 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([64, 32, 3, 3])), Block block_0 (torch.Size([64, 288])).
I0315 14:56:44.803331 140599044539584 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 8.block_0 (torch.Size([0])) for Parameter 8 (torch.Size([512, 256, 3, 3])), Block block_0 (torch.Size([512, 768, 3])).
I0315 14:56:44.803325 140398947562688 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([64, 288])) for Parameter 2 (torch.Size([64, 32, 3, 3])), Block block_0 (torch.Size([64, 288])).
I0315 14:56:44.803342 140107889456320 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 7.block_0 (torch.Size([0])) for Parameter 7 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 14:56:44.803346 140128648783040 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 23.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 23 (torch.Size([64, 32, 2, 2])), Block block_0 (torch.Size([64, 128])).
I0315 14:56:44.803360 140139585799360 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 11.block_0 (torch.Size([0])) for Parameter 11 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 14:56:44.803364 140545246360768 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 14:56:44.803390 140599044539584 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 9.block_0 (torch.Size([0])) for Parameter 9 (torch.Size([512, 512, 3, 3])), Block block_0 (torch.Size([512, 512, 9])).
I0315 14:56:44.803394 140107889456320 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 8.block_0 (torch.Size([0])) for Parameter 8 (torch.Size([512, 256, 3, 3])), Block block_0 (torch.Size([512, 768, 3])).
I0315 14:56:44.803396 140398947562688 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 14:56:44.803416 140545246360768 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 4.block_0 (torch.Size([0])) for Parameter 4 (torch.Size([128, 64, 3, 3])), Block block_0 (torch.Size([128, 576])).
I0315 14:56:44.803418 140139585799360 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 12.block_0 (torch.Size([0])) for Parameter 12 (torch.Size([128, 256, 3, 3])), Block block_0 (torch.Size([128, 768, 3])).
I0315 14:56:44.803444 140107889456320 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 9.block_0 (torch.Size([0])) for Parameter 9 (torch.Size([512, 512, 3, 3])), Block block_0 (torch.Size([512, 512, 9])).
I0315 14:56:44.803447 140599044539584 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 10.block_0 (torch.Size([0])) for Parameter 10 (torch.Size([256, 512, 3, 3])), Block block_0 (torch.Size([256, 512, 9])).
I0315 14:56:44.803452 140398947562688 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 4.block_0 (torch.Size([0])) for Parameter 4 (torch.Size([128, 64, 3, 3])), Block block_0 (torch.Size([128, 576])).
I0315 14:56:44.803441 139816173798592 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([32, 1, 3, 3])), Block block_0 (torch.Size([288])).
I0315 14:56:44.803468 140139585799360 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 13.block_0 (torch.Size([0])) for Parameter 13 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 14:56:44.803466 140128648783040 shampoo_preconditioner_list.py:612] Rank 6: ShampooPreconditionerList Numel Breakdown: (165888, 2097314, 174080, 671744, 696320, 327698, 426002, 1310738, 1703954, 1048738, 655522, 1310738, 1212434, 327698, 303122, 671744, 665600, 2097314, 2048, 2, 2621440, 655360, 163840, 40960)
I0315 14:56:44.803476 140545246360768 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 5.block_0 (torch.Size([0])) for Parameter 5 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 14:56:44.803498 140599044539584 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 11.block_0 (torch.Size([0])) for Parameter 11 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 14:56:44.803505 140398947562688 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 5.block_0 (torch.Size([0])) for Parameter 5 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 14:56:44.803509 140128648783040 shampoo_preconditioner_list.py:615] Rank 6: ShampooPreconditionerList Bytes Breakdown: (663552, 8389256, 696320, 2686976, 2785280, 1310792, 1704008)
I0315 14:56:44.803519 140139585799360 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 14.block_0 (torch.Size([0])) for Parameter 14 (torch.Size([64, 128, 3, 3])), Block block_0 (torch.Size([64, 384, 3])).
I0315 14:56:44.803529 140545246360768 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 6.block_0 (torch.Size([0])) for Parameter 6 (torch.Size([256, 128, 3, 3])), Block block_0 (torch.Size([256, 384, 3])).
I0315 14:56:44.803544 140128648783040 shampoo_preconditioner_list.py:618] Rank 6: ShampooPreconditionerList Total Elements: 19350298
I0315 14:56:44.803555 140599044539584 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 12.block_0 (torch.Size([0])) for Parameter 12 (torch.Size([128, 256, 3, 3])), Block block_0 (torch.Size([128, 768, 3])).
I0315 14:56:44.803556 140398947562688 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 6.block_0 (torch.Size([0])) for Parameter 6 (torch.Size([256, 128, 3, 3])), Block block_0 (torch.Size([256, 384, 3])).
I0315 14:56:44.803555 139816173798592 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([1024, 9])) for Parameter 1 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 14:56:44.803556 140107889456320 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 10.block_0 (torch.Size([256, 512, 9])) for Parameter 10 (torch.Size([256, 512, 3, 3])), Block block_0 (torch.Size([256, 512, 9])).
I0315 14:56:44.803568 140139585799360 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 15.block_0 (torch.Size([0])) for Parameter 15 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 14:56:44.803574 140128648783040 shampoo_preconditioner_list.py:621] Rank 6: ShampooPreconditionerList Total Bytes: 18236184
I0315 14:56:44.803594 140545246360768 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 7.block_0 (torch.Size([0])) for Parameter 7 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 14:56:44.803611 140599044539584 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 13.block_0 (torch.Size([0])) for Parameter 13 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 14:56:44.803617 140139585799360 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 16.block_0 (torch.Size([0])) for Parameter 16 (torch.Size([32, 64, 3, 3])), Block block_0 (torch.Size([32, 576])).
I0315 14:56:44.803626 140107889456320 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 11.block_0 (torch.Size([0])) for Parameter 11 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 14:56:44.803625 139816173798592 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([64, 32, 3, 3])), Block block_0 (torch.Size([64, 288])).
I0315 14:56:44.803645 140398947562688 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 7.block_0 (torch.Size([256, 768, 3])) for Parameter 7 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 14:56:44.803654 140545246360768 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 8.block_0 (torch.Size([0])) for Parameter 8 (torch.Size([512, 256, 3, 3])), Block block_0 (torch.Size([512, 768, 3])).
I0315 14:56:44.803662 140599044539584 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 14.block_0 (torch.Size([0])) for Parameter 14 (torch.Size([64, 128, 3, 3])), Block block_0 (torch.Size([64, 384, 3])).
I0315 14:56:44.803666 140139585799360 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 17.block_0 (torch.Size([0])) for Parameter 17 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 14:56:44.803678 140107889456320 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 12.block_0 (torch.Size([0])) for Parameter 12 (torch.Size([128, 256, 3, 3])), Block block_0 (torch.Size([128, 768, 3])).
I0315 14:56:44.803692 139816173798592 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 14:56:44.803722 140139585799360 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 18.block_0 (torch.Size([0])) for Parameter 18 (torch.Size([1, 32, 1, 1])), Block block_0 (torch.Size([32])).
I0315 14:56:44.803721 140398947562688 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 8.block_0 (torch.Size([0])) for Parameter 8 (torch.Size([512, 256, 3, 3])), Block block_0 (torch.Size([512, 768, 3])).
I0315 14:56:44.803745 140107889456320 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 13.block_0 (torch.Size([0])) for Parameter 13 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 14:56:44.803757 140599044539584 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 15.block_0 (torch.Size([64, 576])) for Parameter 15 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 14:56:44.803775 140398947562688 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 9.block_0 (torch.Size([0])) for Parameter 9 (torch.Size([512, 512, 3, 3])), Block block_0 (torch.Size([512, 512, 9])).
I0315 14:56:44.803779 140139585799360 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 19.block_0 (torch.Size([0])) for Parameter 19 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0315 14:56:44.803777 140545246360768 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 9.block_0 (torch.Size([512, 512, 9])) for Parameter 9 (torch.Size([512, 512, 3, 3])), Block block_0 (torch.Size([512, 512, 9])).
I0315 14:56:44.803804 140107889456320 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 14.block_0 (torch.Size([0])) for Parameter 14 (torch.Size([64, 128, 3, 3])), Block block_0 (torch.Size([64, 384, 3])).
I0315 14:56:44.803826 140398947562688 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 10.block_0 (torch.Size([0])) for Parameter 10 (torch.Size([256, 512, 3, 3])), Block block_0 (torch.Size([256, 512, 9])).
I0315 14:56:44.803828 140139585799360 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 20.block_0 (torch.Size([0])) for Parameter 20 (torch.Size([512, 256, 2, 2])), Block block_0 (torch.Size([512, 1024])).
I0315 14:56:44.803833 140599044539584 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 16.block_0 (torch.Size([0])) for Parameter 16 (torch.Size([32, 64, 3, 3])), Block block_0 (torch.Size([32, 576])).
I0315 14:56:44.803853 140107889456320 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 15.block_0 (torch.Size([0])) for Parameter 15 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 14:56:44.803860 140545246360768 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 10.block_0 (torch.Size([0])) for Parameter 10 (torch.Size([256, 512, 3, 3])), Block block_0 (torch.Size([256, 512, 9])).
I0315 14:56:44.803833 140128648783040 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([288])) for Parameter 0 (torch.Size([32, 1, 3, 3])), Block block_0 (torch.Size([288])).
I0315 14:56:44.803877 140139585799360 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 21.block_0 (torch.Size([0])) for Parameter 21 (torch.Size([256, 128, 2, 2])), Block block_0 (torch.Size([256, 512])).
I0315 14:56:44.803884 140398947562688 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 11.block_0 (torch.Size([0])) for Parameter 11 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 14:56:44.803902 140107889456320 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 16.block_0 (torch.Size([0])) for Parameter 16 (torch.Size([32, 64, 3, 3])), Block block_0 (torch.Size([32, 576])).
I0315 14:56:44.803913 140599044539584 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 17.block_0 (torch.Size([1024, 9])) for Parameter 17 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 14:56:44.803926 140139585799360 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 22.block_0 (torch.Size([0])) for Parameter 22 (torch.Size([128, 64, 2, 2])), Block block_0 (torch.Size([128, 256])).
I0315 14:56:44.803925 140545246360768 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 11.block_0 (torch.Size([0])) for Parameter 11 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 14:56:44.803936 140398947562688 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 12.block_0 (torch.Size([0])) for Parameter 12 (torch.Size([128, 256, 3, 3])), Block block_0 (torch.Size([128, 768, 3])).
I0315 14:56:44.803934 140128648783040 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 14:56:44.803957 140107889456320 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 17.block_0 (torch.Size([0])) for Parameter 17 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 14:56:44.803974 140599044539584 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 18.block_0 (torch.Size([0])) for Parameter 18 (torch.Size([1, 32, 1, 1])), Block block_0 (torch.Size([32])).
I0315 14:56:44.803981 140139585799360 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 23.block_0 (torch.Size([0])) for Parameter 23 (torch.Size([64, 32, 2, 2])), Block block_0 (torch.Size([64, 128])).
I0315 14:56:44.803984 140545246360768 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 12.block_0 (torch.Size([0])) for Parameter 12 (torch.Size([128, 256, 3, 3])), Block block_0 (torch.Size([128, 768, 3])).
I0315 14:56:44.803991 140398947562688 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 13.block_0 (torch.Size([0])) for Parameter 13 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 14:56:44.803991 140128648783040 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([64, 32, 3, 3])), Block block_0 (torch.Size([64, 288])).
I0315 14:56:44.804012 140107889456320 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 18.block_0 (torch.Size([0])) for Parameter 18 (torch.Size([1, 32, 1, 1])), Block block_0 (torch.Size([32])).
I0315 14:56:44.804023 140599044539584 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 19.block_0 (torch.Size([0])) for Parameter 19 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0315 14:56:44.804037 140545246360768 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 13.block_0 (torch.Size([0])) for Parameter 13 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 14:56:44.804042 140398947562688 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 14.block_0 (torch.Size([0])) for Parameter 14 (torch.Size([64, 128, 3, 3])), Block block_0 (torch.Size([64, 384, 3])).
I0315 14:56:44.804045 140128648783040 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 14:56:44.804055 140139585799360 shampoo_preconditioner_list.py:375] Rank 1: AdaGradPreconditionerList Numel Breakdown: (0, 0, 0, 0, 0, 0, 0, 0, 1179648, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)
I0315 14:56:44.804066 140107889456320 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 19.block_0 (torch.Size([0])) for Parameter 19 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0315 14:56:44.804091 140139585799360 shampoo_preconditioner_list.py:378] Rank 1: AdaGradPreconditionerList Bytes Breakdown: (0, 0, 0, 0, 0, 0, 0, 0, 4718592, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)
I0315 14:56:44.804091 140398947562688 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 15.block_0 (torch.Size([0])) for Parameter 15 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 14:56:44.804096 140545246360768 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 14.block_0 (torch.Size([0])) for Parameter 14 (torch.Size([64, 128, 3, 3])), Block block_0 (torch.Size([64, 384, 3])).
I0315 14:56:44.804098 140128648783040 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 4.block_0 (torch.Size([0])) for Parameter 4 (torch.Size([128, 64, 3, 3])), Block block_0 (torch.Size([128, 576])).
I0315 14:56:44.804098 140599044539584 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 20.block_0 (torch.Size([512, 1024])) for Parameter 20 (torch.Size([512, 256, 2, 2])), Block block_0 (torch.Size([512, 1024])).
I0315 14:56:44.804122 140107889456320 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 20.block_0 (torch.Size([0])) for Parameter 20 (torch.Size([512, 256, 2, 2])), Block block_0 (torch.Size([512, 1024])).
I0315 14:56:44.804126 140139585799360 shampoo_preconditioner_list.py:381] Rank 1: AdaGradPreconditionerList Total Elements: 1179648
I0315 14:56:44.804139 140398947562688 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 16.block_0 (torch.Size([0])) for Parameter 16 (torch.Size([32, 64, 3, 3])), Block block_0 (torch.Size([32, 576])).
I0315 14:56:44.804147 140545246360768 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 15.block_0 (torch.Size([0])) for Parameter 15 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 14:56:44.804156 140139585799360 shampoo_preconditioner_list.py:384] Rank 1: AdaGradPreconditionerList Total Bytes: 4718592
I0315 14:56:44.804156 140599044539584 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 21.block_0 (torch.Size([0])) for Parameter 21 (torch.Size([256, 128, 2, 2])), Block block_0 (torch.Size([256, 512])).
I0315 14:56:44.804177 140107889456320 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 21.block_0 (torch.Size([0])) for Parameter 21 (torch.Size([256, 128, 2, 2])), Block block_0 (torch.Size([256, 512])).
I0315 14:56:44.804188 140398947562688 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 17.block_0 (torch.Size([0])) for Parameter 17 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 14:56:44.804175 139816173798592 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 4.block_0 (torch.Size([128, 576])) for Parameter 4 (torch.Size([128, 64, 3, 3])), Block block_0 (torch.Size([128, 576])).
I0315 14:56:44.804203 140545246360768 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 16.block_0 (torch.Size([0])) for Parameter 16 (torch.Size([32, 64, 3, 3])), Block block_0 (torch.Size([32, 576])).
I0315 14:56:44.804207 140599044539584 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 22.block_0 (torch.Size([0])) for Parameter 22 (torch.Size([128, 64, 2, 2])), Block block_0 (torch.Size([128, 256])).
I0315 14:56:44.804231 140107889456320 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 22.block_0 (torch.Size([0])) for Parameter 22 (torch.Size([128, 64, 2, 2])), Block block_0 (torch.Size([128, 256])).
I0315 14:56:44.804237 140398947562688 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 18.block_0 (torch.Size([0])) for Parameter 18 (torch.Size([1, 32, 1, 1])), Block block_0 (torch.Size([32])).
I0315 14:56:44.804254 140545246360768 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 17.block_0 (torch.Size([0])) for Parameter 17 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 14:56:44.804258 140599044539584 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 23.block_0 (torch.Size([0])) for Parameter 23 (torch.Size([64, 32, 2, 2])), Block block_0 (torch.Size([64, 128])).
I0315 14:56:44.804274 139816173798592 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 5.block_0 (torch.Size([0])) for Parameter 5 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 14:56:44.804284 140398947562688 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 19.block_0 (torch.Size([0])) for Parameter 19 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0315 14:56:44.804286 140107889456320 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 23.block_0 (torch.Size([0])) for Parameter 23 (torch.Size([64, 32, 2, 2])), Block block_0 (torch.Size([64, 128])).
I0315 14:56:44.804311 140545246360768 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 18.block_0 (torch.Size([0])) for Parameter 18 (torch.Size([1, 32, 1, 1])), Block block_0 (torch.Size([32])).
I0315 14:56:44.804329 140599044539584 shampoo_preconditioner_list.py:375] Rank 5: AdaGradPreconditionerList Numel Breakdown: (0, 0, 0, 36864, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 36864, 0, 9216, 0, 0, 524288, 0, 0, 0)
I0315 14:56:44.804332 140398947562688 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 20.block_0 (torch.Size([0])) for Parameter 20 (torch.Size([512, 256, 2, 2])), Block block_0 (torch.Size([512, 1024])).
I0315 14:56:44.804346 139816173798592 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 6.block_0 (torch.Size([0])) for Parameter 6 (torch.Size([256, 128, 3, 3])), Block block_0 (torch.Size([256, 384, 3])).
I0315 14:56:44.804352 140107889456320 shampoo_preconditioner_list.py:375] Rank 2: AdaGradPreconditionerList Numel Breakdown: (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1179648, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)
I0315 14:56:44.804365 140599044539584 shampoo_preconditioner_list.py:378] Rank 5: AdaGradPreconditionerList Bytes Breakdown: (0, 0, 0, 147456, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 147456, 0, 36864, 0, 0, 2097152, 0, 0, 0)
I0315 14:56:44.804367 140545246360768 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 19.block_0 (torch.Size([0])) for Parameter 19 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0315 14:56:44.804380 140398947562688 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 21.block_0 (torch.Size([0])) for Parameter 21 (torch.Size([256, 128, 2, 2])), Block block_0 (torch.Size([256, 512])).
I0315 14:56:44.804388 140107889456320 shampoo_preconditioner_list.py:378] Rank 2: AdaGradPreconditionerList Bytes Breakdown: (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4718592, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)
I0315 14:56:44.804395 140599044539584 shampoo_preconditioner_list.py:381] Rank 5: AdaGradPreconditionerList Total Elements: 607232
I0315 14:56:44.804418 140107889456320 shampoo_preconditioner_list.py:381] Rank 2: AdaGradPreconditionerList Total Elements: 1179648
I0315 14:56:44.804415 139816173798592 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 7.block_0 (torch.Size([0])) for Parameter 7 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 14:56:44.804419 140545246360768 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 20.block_0 (torch.Size([0])) for Parameter 20 (torch.Size([512, 256, 2, 2])), Block block_0 (torch.Size([512, 1024])).
I0315 14:56:44.804428 140398947562688 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 22.block_0 (torch.Size([0])) for Parameter 22 (torch.Size([128, 64, 2, 2])), Block block_0 (torch.Size([128, 256])).
I0315 14:56:44.804435 140599044539584 shampoo_preconditioner_list.py:384] Rank 5: AdaGradPreconditionerList Total Bytes: 2428928
I0315 14:56:44.804453 140107889456320 shampoo_preconditioner_list.py:384] Rank 2: AdaGradPreconditionerList Total Bytes: 4718592
I0315 14:56:44.804469 140545246360768 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 21.block_0 (torch.Size([0])) for Parameter 21 (torch.Size([256, 128, 2, 2])), Block block_0 (torch.Size([256, 512])).
I0315 14:56:44.804471 139816173798592 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 8.block_0 (torch.Size([0])) for Parameter 8 (torch.Size([512, 256, 3, 3])), Block block_0 (torch.Size([512, 768, 3])).
I0315 14:56:44.804477 140398947562688 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 23.block_0 (torch.Size([0])) for Parameter 23 (torch.Size([64, 32, 2, 2])), Block block_0 (torch.Size([64, 128])).
I0315 14:56:44.804524 139816173798592 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 9.block_0 (torch.Size([0])) for Parameter 9 (torch.Size([512, 512, 3, 3])), Block block_0 (torch.Size([512, 512, 9])).
I0315 14:56:44.804526 140545246360768 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 22.block_0 (torch.Size([0])) for Parameter 22 (torch.Size([128, 64, 2, 2])), Block block_0 (torch.Size([128, 256])).
I0315 14:56:44.804545 140398947562688 shampoo_preconditioner_list.py:375] Rank 3: AdaGradPreconditionerList Numel Breakdown: (0, 0, 18432, 0, 0, 0, 0, 589824, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)
I0315 14:56:44.804576 140545246360768 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 23.block_0 (torch.Size([0])) for Parameter 23 (torch.Size([64, 32, 2, 2])), Block block_0 (torch.Size([64, 128])).
I0315 14:56:44.804583 139816173798592 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 10.block_0 (torch.Size([0])) for Parameter 10 (torch.Size([256, 512, 3, 3])), Block block_0 (torch.Size([256, 512, 9])).
I0315 14:56:44.804590 140398947562688 shampoo_preconditioner_list.py:378] Rank 3: AdaGradPreconditionerList Bytes Breakdown: (0, 0, 73728, 0, 0, 0, 0, 2359296, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)
I0315 14:56:44.804573 140128648783040 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 5.block_0 (torch.Size([128, 384, 3])) for Parameter 5 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 14:56:44.804621 140398947562688 shampoo_preconditioner_list.py:381] Rank 3: AdaGradPreconditionerList Total Elements: 608256
I0315 14:56:44.804637 139816173798592 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 11.block_0 (torch.Size([0])) for Parameter 11 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 14:56:44.804648 140545246360768 shampoo_preconditioner_list.py:375] Rank 0: AdaGradPreconditionerList Numel Breakdown: (0, 0, 0, 0, 0, 0, 0, 0, 0, 2359296, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)
I0315 14:56:44.804655 140398947562688 shampoo_preconditioner_list.py:384] Rank 3: AdaGradPreconditionerList Total Bytes: 2433024
I0315 14:56:44.804685 140545246360768 shampoo_preconditioner_list.py:378] Rank 0: AdaGradPreconditionerList Bytes Breakdown: (0, 0, 0, 0, 0, 0, 0, 0, 0, 9437184, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)
I0315 14:56:44.804697 140128648783040 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 6.block_0 (torch.Size([256, 384, 3])) for Parameter 6 (torch.Size([256, 128, 3, 3])), Block block_0 (torch.Size([256, 384, 3])).
I0315 14:56:44.804722 140545246360768 shampoo_preconditioner_list.py:381] Rank 0: AdaGradPreconditionerList Total Elements: 2359296
I0315 14:56:44.804752 140545246360768 shampoo_preconditioner_list.py:384] Rank 0: AdaGradPreconditionerList Total Bytes: 9437184
I0315 14:56:44.804746 139816173798592 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 12.block_0 (torch.Size([128, 768, 3])) for Parameter 12 (torch.Size([128, 256, 3, 3])), Block block_0 (torch.Size([128, 768, 3])).
I0315 14:56:44.804800 140128648783040 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 7.block_0 (torch.Size([0])) for Parameter 7 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 14:56:44.804830 139816173798592 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 13.block_0 (torch.Size([128, 384, 3])) for Parameter 13 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 14:56:44.804866 140128648783040 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 8.block_0 (torch.Size([0])) for Parameter 8 (torch.Size([512, 256, 3, 3])), Block block_0 (torch.Size([512, 768, 3])).
I0315 14:56:44.804846 140204161926336 logger_utils.py:262] Unable to record workload.train_mean information. Continuing without it.
I0315 14:56:44.804916 140204161926336 logger_utils.py:262] Unable to record workload.train_stddev information. Continuing without it.
I0315 14:56:44.804918 139816173798592 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 14.block_0 (torch.Size([64, 384, 3])) for Parameter 14 (torch.Size([64, 128, 3, 3])), Block block_0 (torch.Size([64, 384, 3])).
I0315 14:56:44.804926 140128648783040 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 9.block_0 (torch.Size([0])) for Parameter 9 (torch.Size([512, 512, 3, 3])), Block block_0 (torch.Size([512, 512, 9])).
I0315 14:56:44.804978 140128648783040 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 10.block_0 (torch.Size([0])) for Parameter 10 (torch.Size([256, 512, 3, 3])), Block block_0 (torch.Size([256, 512, 9])).
I0315 14:56:44.804981 139816173798592 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 15.block_0 (torch.Size([0])) for Parameter 15 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 14:56:44.805034 139816173798592 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 16.block_0 (torch.Size([0])) for Parameter 16 (torch.Size([32, 64, 3, 3])), Block block_0 (torch.Size([32, 576])).
I0315 14:56:44.805040 140128648783040 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 11.block_0 (torch.Size([0])) for Parameter 11 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 14:56:44.805094 140128648783040 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 12.block_0 (torch.Size([0])) for Parameter 12 (torch.Size([128, 256, 3, 3])), Block block_0 (torch.Size([128, 768, 3])).
I0315 14:56:44.805094 139816173798592 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 17.block_0 (torch.Size([0])) for Parameter 17 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 14:56:44.805145 139816173798592 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 18.block_0 (torch.Size([0])) for Parameter 18 (torch.Size([1, 32, 1, 1])), Block block_0 (torch.Size([32])).
I0315 14:56:44.805151 140128648783040 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 13.block_0 (torch.Size([0])) for Parameter 13 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 14:56:44.805199 139816173798592 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 19.block_0 (torch.Size([0])) for Parameter 19 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0315 14:56:44.805202 140128648783040 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 14.block_0 (torch.Size([0])) for Parameter 14 (torch.Size([64, 128, 3, 3])), Block block_0 (torch.Size([64, 384, 3])).
I0315 14:56:44.805251 140128648783040 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 15.block_0 (torch.Size([0])) for Parameter 15 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 14:56:44.805256 139816173798592 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 20.block_0 (torch.Size([0])) for Parameter 20 (torch.Size([512, 256, 2, 2])), Block block_0 (torch.Size([512, 1024])).
I0315 14:56:44.805306 139816173798592 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 21.block_0 (torch.Size([0])) for Parameter 21 (torch.Size([256, 128, 2, 2])), Block block_0 (torch.Size([256, 512])).
I0315 14:56:44.805306 140128648783040 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 16.block_0 (torch.Size([0])) for Parameter 16 (torch.Size([32, 64, 3, 3])), Block block_0 (torch.Size([32, 576])).
I0315 14:56:44.805355 140128648783040 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 17.block_0 (torch.Size([0])) for Parameter 17 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 14:56:44.805369 139816173798592 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 22.block_0 (torch.Size([0])) for Parameter 22 (torch.Size([128, 64, 2, 2])), Block block_0 (torch.Size([128, 256])).
I0315 14:56:44.805450 140128648783040 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 18.block_0 (torch.Size([32])) for Parameter 18 (torch.Size([1, 32, 1, 1])), Block block_0 (torch.Size([32])).
I0315 14:56:44.805453 139816173798592 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 23.block_0 (torch.Size([64, 128])) for Parameter 23 (torch.Size([64, 32, 2, 2])), Block block_0 (torch.Size([64, 128])).
I0315 14:56:44.805531 139816173798592 shampoo_preconditioner_list.py:375] Rank 7: AdaGradPreconditionerList Numel Breakdown: (0, 9216, 0, 0, 73728, 0, 0, 0, 0, 0, 0, 0, 294912, 147456, 73728, 0, 0, 0, 0, 0, 0, 0, 0, 8192)
I0315 14:56:44.805545 140128648783040 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 19.block_0 (torch.Size([1])) for Parameter 19 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0315 14:56:44.805575 139816173798592 shampoo_preconditioner_list.py:378] Rank 7: AdaGradPreconditionerList Bytes Breakdown: (0, 36864, 0, 0, 294912, 0, 0, 0, 0, 0, 0, 0, 1179648, 589824, 294912, 0, 0, 0, 0, 0, 0, 0, 0, 32768)
I0315 14:56:44.805629 140128648783040 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 20.block_0 (torch.Size([0])) for Parameter 20 (torch.Size([512, 256, 2, 2])), Block block_0 (torch.Size([512, 1024])).
I0315 14:56:44.805633 139816173798592 shampoo_preconditioner_list.py:381] Rank 7: AdaGradPreconditionerList Total Elements: 607232
I0315 14:56:44.805671 139816173798592 shampoo_preconditioner_list.py:384] Rank 7: AdaGradPreconditionerList Total Bytes: 2428928
I0315 14:56:44.805722 140128648783040 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 21.block_0 (torch.Size([256, 512])) for Parameter 21 (torch.Size([256, 128, 2, 2])), Block block_0 (torch.Size([256, 512])).
I0315 14:56:44.805831 140128648783040 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 22.block_0 (torch.Size([128, 256])) for Parameter 22 (torch.Size([128, 64, 2, 2])), Block block_0 (torch.Size([128, 256])).
I0315 14:56:44.805906 140128648783040 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 23.block_0 (torch.Size([0])) for Parameter 23 (torch.Size([64, 32, 2, 2])), Block block_0 (torch.Size([64, 128])).
I0315 14:56:44.805917 140599044539584 logger_utils.py:262] Unable to record workload.train_mean information. Continuing without it.
I0315 14:56:44.805986 140128648783040 shampoo_preconditioner_list.py:375] Rank 6: AdaGradPreconditionerList Numel Breakdown: (288, 0, 0, 0, 0, 147456, 294912, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 32, 1, 0, 131072, 32768, 0)
I0315 14:56:44.805993 140599044539584 logger_utils.py:262] Unable to record workload.train_stddev information. Continuing without it.
I0315 14:56:44.805988 140139585799360 logger_utils.py:262] Unable to record workload.train_mean information. Continuing without it.
I0315 14:56:44.806030 140128648783040 shampoo_preconditioner_list.py:378] Rank 6: AdaGradPreconditionerList Bytes Breakdown: (1152, 0, 0, 0, 0, 589824, 1179648, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 128, 4, 0, 524288, 131072, 0)
I0315 14:56:44.806065 140128648783040 shampoo_preconditioner_list.py:381] Rank 6: AdaGradPreconditionerList Total Elements: 606529
I0315 14:56:44.806066 140139585799360 logger_utils.py:262] Unable to record workload.train_stddev information. Continuing without it.
I0315 14:56:44.806099 140128648783040 shampoo_preconditioner_list.py:384] Rank 6: AdaGradPreconditionerList Total Bytes: 2426116
I0315 14:56:44.806075 140545246360768 submission.py:142] No large parameters detected! Continuing with only Shampoo....
I0315 14:56:44.806096 140398947562688 logger_utils.py:262] Unable to record workload.train_mean information. Continuing without it.
I0315 14:56:44.806171 140398947562688 logger_utils.py:262] Unable to record workload.train_stddev information. Continuing without it.
I0315 14:56:44.806260 140545246360768 submission_runner.py:279] Initializing metrics bundle.
I0315 14:56:44.806401 140545246360768 submission_runner.py:301] Initializing checkpoint and logger.
I0315 14:56:44.806380 140107889456320 logger_utils.py:262] Unable to record workload.train_mean information. Continuing without it.
I0315 14:56:44.806446 140107889456320 logger_utils.py:262] Unable to record workload.train_stddev information. Continuing without it.
I0315 14:56:44.806809 140545246360768 submission_runner.py:321] Saving meta data to /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_0/fastmri_pytorch/trial_4/meta_data_0.json.
I0315 14:56:44.806990 140545246360768 logger_utils.py:262] Unable to record workload.train_mean information. Continuing without it.
I0315 14:56:44.807040 140545246360768 logger_utils.py:262] Unable to record workload.train_stddev information. Continuing without it.
I0315 14:56:44.807600 139816173798592 logger_utils.py:262] Unable to record workload.train_mean information. Continuing without it.
I0315 14:56:44.807685 139816173798592 logger_utils.py:262] Unable to record workload.train_stddev information. Continuing without it.
I0315 14:56:44.808091 140128648783040 logger_utils.py:262] Unable to record workload.train_mean information. Continuing without it.
I0315 14:56:44.808169 140128648783040 logger_utils.py:262] Unable to record workload.train_stddev information. Continuing without it.
I0315 14:56:45.566617 140545246360768 submission_runner.py:325] Saving flags to /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_0/fastmri_pytorch/trial_4/flags_0.json.
I0315 14:56:45.600528 140545246360768 submission_runner.py:337] Starting training loop.
[rank6]:W0315 14:56:45.791000 50 site-packages/torch/_logging/_internal.py:1081] [0/0] Profiler function <class 'torch.autograd.profiler.record_function'> will be ignored
[rank7]:W0315 14:56:45.791000 51 site-packages/torch/_logging/_internal.py:1081] [0/0] Profiler function <class 'torch.autograd.profiler.record_function'> will be ignored
[rank1]:W0315 14:56:45.791000 45 site-packages/torch/_logging/_internal.py:1081] [0/0] Profiler function <class 'torch.autograd.profiler.record_function'> will be ignored
[rank3]:W0315 14:56:45.791000 47 site-packages/torch/_logging/_internal.py:1081] [0/0] Profiler function <class 'torch.autograd.profiler.record_function'> will be ignored
[rank5]:W0315 14:56:45.791000 49 site-packages/torch/_logging/_internal.py:1081] [0/0] Profiler function <class 'torch.autograd.profiler.record_function'> will be ignored
[rank2]:W0315 14:56:45.791000 46 site-packages/torch/_logging/_internal.py:1081] [0/0] Profiler function <class 'torch.autograd.profiler.record_function'> will be ignored
[rank4]:W0315 14:56:45.791000 48 site-packages/torch/_logging/_internal.py:1081] [0/0] Profiler function <class 'torch.autograd.profiler.record_function'> will be ignored
[rank0]:W0315 14:57:13.309000 44 site-packages/torch/_logging/_internal.py:1081] [0/0] Profiler function <class 'torch.autograd.profiler.record_function'> will be ignored
[rank4]:W0315 14:57:49.414000 48 site-packages/torch/_dynamo/convert_frame.py:844] [2/8] torch._dynamo hit config.cache_size_limit (8)
[rank4]:W0315 14:57:49.414000 48 site-packages/torch/_dynamo/convert_frame.py:844] [2/8]    function: 'forward' (/algorithmic-efficiency/algoperf/workloads/fastmri/fastmri_pytorch/models.py:141)
[rank4]:W0315 14:57:49.414000 48 site-packages/torch/_dynamo/convert_frame.py:844] [2/8]    last reason: 2/0: tensor 'L['x']' requires_grad mismatch. expected requires_grad=0
[rank4]:W0315 14:57:49.414000 48 site-packages/torch/_dynamo/convert_frame.py:844] [2/8] To log all recompilation reasons, use TORCH_LOGS="recompiles".
[rank4]:W0315 14:57:49.414000 48 site-packages/torch/_dynamo/convert_frame.py:844] [2/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.
[rank6]:W0315 14:57:49.550000 50 site-packages/torch/_dynamo/convert_frame.py:844] [2/8] torch._dynamo hit config.cache_size_limit (8)
[rank6]:W0315 14:57:49.550000 50 site-packages/torch/_dynamo/convert_frame.py:844] [2/8]    function: 'forward' (/algorithmic-efficiency/algoperf/workloads/fastmri/fastmri_pytorch/models.py:141)
[rank6]:W0315 14:57:49.550000 50 site-packages/torch/_dynamo/convert_frame.py:844] [2/8]    last reason: 2/0: tensor 'L['x']' requires_grad mismatch. expected requires_grad=0
[rank6]:W0315 14:57:49.550000 50 site-packages/torch/_dynamo/convert_frame.py:844] [2/8] To log all recompilation reasons, use TORCH_LOGS="recompiles".
[rank6]:W0315 14:57:49.550000 50 site-packages/torch/_dynamo/convert_frame.py:844] [2/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.
[rank1]:W0315 14:57:49.635000 45 site-packages/torch/_dynamo/convert_frame.py:844] [2/8] torch._dynamo hit config.cache_size_limit (8)
[rank1]:W0315 14:57:49.635000 45 site-packages/torch/_dynamo/convert_frame.py:844] [2/8]    function: 'forward' (/algorithmic-efficiency/algoperf/workloads/fastmri/fastmri_pytorch/models.py:141)
[rank1]:W0315 14:57:49.635000 45 site-packages/torch/_dynamo/convert_frame.py:844] [2/8]    last reason: 2/0: tensor 'L['x']' requires_grad mismatch. expected requires_grad=0
[rank1]:W0315 14:57:49.635000 45 site-packages/torch/_dynamo/convert_frame.py:844] [2/8] To log all recompilation reasons, use TORCH_LOGS="recompiles".
[rank1]:W0315 14:57:49.635000 45 site-packages/torch/_dynamo/convert_frame.py:844] [2/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.
[rank3]:W0315 14:57:49.721000 47 site-packages/torch/_dynamo/convert_frame.py:844] [2/8] torch._dynamo hit config.cache_size_limit (8)
[rank3]:W0315 14:57:49.721000 47 site-packages/torch/_dynamo/convert_frame.py:844] [2/8]    function: 'forward' (/algorithmic-efficiency/algoperf/workloads/fastmri/fastmri_pytorch/models.py:141)
[rank3]:W0315 14:57:49.721000 47 site-packages/torch/_dynamo/convert_frame.py:844] [2/8]    last reason: 2/0: tensor 'L['x']' requires_grad mismatch. expected requires_grad=0
[rank3]:W0315 14:57:49.721000 47 site-packages/torch/_dynamo/convert_frame.py:844] [2/8] To log all recompilation reasons, use TORCH_LOGS="recompiles".
[rank3]:W0315 14:57:49.721000 47 site-packages/torch/_dynamo/convert_frame.py:844] [2/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.
[rank7]:W0315 14:57:49.736000 51 site-packages/torch/_dynamo/convert_frame.py:844] [2/8] torch._dynamo hit config.cache_size_limit (8)
[rank7]:W0315 14:57:49.736000 51 site-packages/torch/_dynamo/convert_frame.py:844] [2/8]    function: 'forward' (/algorithmic-efficiency/algoperf/workloads/fastmri/fastmri_pytorch/models.py:141)
[rank7]:W0315 14:57:49.736000 51 site-packages/torch/_dynamo/convert_frame.py:844] [2/8]    last reason: 2/0: tensor 'L['x']' requires_grad mismatch. expected requires_grad=0
[rank7]:W0315 14:57:49.736000 51 site-packages/torch/_dynamo/convert_frame.py:844] [2/8] To log all recompilation reasons, use TORCH_LOGS="recompiles".
[rank7]:W0315 14:57:49.736000 51 site-packages/torch/_dynamo/convert_frame.py:844] [2/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.
[rank2]:W0315 14:57:49.759000 46 site-packages/torch/_dynamo/convert_frame.py:844] [2/8] torch._dynamo hit config.cache_size_limit (8)
[rank2]:W0315 14:57:49.759000 46 site-packages/torch/_dynamo/convert_frame.py:844] [2/8]    function: 'forward' (/algorithmic-efficiency/algoperf/workloads/fastmri/fastmri_pytorch/models.py:141)
[rank2]:W0315 14:57:49.759000 46 site-packages/torch/_dynamo/convert_frame.py:844] [2/8]    last reason: 2/0: tensor 'L['x']' requires_grad mismatch. expected requires_grad=0
[rank2]:W0315 14:57:49.759000 46 site-packages/torch/_dynamo/convert_frame.py:844] [2/8] To log all recompilation reasons, use TORCH_LOGS="recompiles".
[rank2]:W0315 14:57:49.759000 46 site-packages/torch/_dynamo/convert_frame.py:844] [2/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.
[rank5]:W0315 14:57:49.764000 49 site-packages/torch/_dynamo/convert_frame.py:844] [2/8] torch._dynamo hit config.cache_size_limit (8)
[rank5]:W0315 14:57:49.764000 49 site-packages/torch/_dynamo/convert_frame.py:844] [2/8]    function: 'forward' (/algorithmic-efficiency/algoperf/workloads/fastmri/fastmri_pytorch/models.py:141)
[rank5]:W0315 14:57:49.764000 49 site-packages/torch/_dynamo/convert_frame.py:844] [2/8]    last reason: 2/0: tensor 'L['x']' requires_grad mismatch. expected requires_grad=0
[rank5]:W0315 14:57:49.764000 49 site-packages/torch/_dynamo/convert_frame.py:844] [2/8] To log all recompilation reasons, use TORCH_LOGS="recompiles".
[rank5]:W0315 14:57:49.764000 49 site-packages/torch/_dynamo/convert_frame.py:844] [2/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.
[rank0]:W0315 14:57:51.390000 44 site-packages/torch/_dynamo/convert_frame.py:844] [2/8] torch._dynamo hit config.cache_size_limit (8)
[rank0]:W0315 14:57:51.390000 44 site-packages/torch/_dynamo/convert_frame.py:844] [2/8]    function: 'forward' (/algorithmic-efficiency/algoperf/workloads/fastmri/fastmri_pytorch/models.py:141)
[rank0]:W0315 14:57:51.390000 44 site-packages/torch/_dynamo/convert_frame.py:844] [2/8]    last reason: 2/0: tensor 'L['x']' requires_grad mismatch. expected requires_grad=0
[rank0]:W0315 14:57:51.390000 44 site-packages/torch/_dynamo/convert_frame.py:844] [2/8] To log all recompilation reasons, use TORCH_LOGS="recompiles".
[rank0]:W0315 14:57:51.390000 44 site-packages/torch/_dynamo/convert_frame.py:844] [2/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.
I0315 14:58:03.023374 140517851035392 logging_writer.py:48] [0] global_step=0, grad_norm=4.38269, loss=1.04534
I0315 14:58:03.041267 140545246360768 submission.py:265] 0) loss = 1.045, grad_norm = 4.383
I0315 14:58:03.831708 140545246360768 spec.py:321] Evaluating on the training split.
I0315 14:59:05.241951 140545246360768 spec.py:333] Evaluating on the validation split.
I0315 15:03:06.687709 140545246360768 spec.py:349] Evaluating on the test split.
I0315 15:05:36.015399 140545246360768 submission_runner.py:469] Time since start: 530.42s, 	Step: 1, 	{'train/ssim': 0.2516805785042899, 'train/loss': 0.9709701538085938, 'validation/ssim': 0.24513789892330826, 'validation/loss': 0.978066568369267, 'validation/num_examples': 3554, 'test/ssim': 0.2659577361966804, 'test/loss': 0.9746548488332519, 'test/num_examples': 3581, 'score': 77.44168257713318, 'total_duration': 530.415036201477, 'accumulated_submission_time': 77.44168257713318, 'accumulated_eval_time': 452.1838643550873, 'accumulated_logging_time': 0}
I0315 15:05:36.023549 140503672678144 logging_writer.py:48] [1] accumulated_eval_time=452.184, accumulated_logging_time=0, accumulated_submission_time=77.4417, global_step=1, preemption_count=0, score=77.4417, test/loss=0.974655, test/num_examples=3581, test/ssim=0.265958, total_duration=530.415, train/loss=0.97097, train/ssim=0.251681, validation/loss=0.978067, validation/num_examples=3554, validation/ssim=0.245138
I0315 15:05:37.206052 140503664285440 logging_writer.py:48] [1] global_step=1, grad_norm=5.12274, loss=1.01616
I0315 15:05:37.209504 140545246360768 submission.py:265] 1) loss = 1.016, grad_norm = 5.123
I0315 15:05:37.312690 140503672678144 logging_writer.py:48] [2] global_step=2, grad_norm=4.2673, loss=1.02701
I0315 15:05:37.319664 140545246360768 submission.py:265] 2) loss = 1.027, grad_norm = 4.267
I0315 15:05:37.400834 140503664285440 logging_writer.py:48] [3] global_step=3, grad_norm=3.90379, loss=0.974812
I0315 15:05:37.405819 140545246360768 submission.py:265] 3) loss = 0.975, grad_norm = 3.904
I0315 15:05:37.503831 140503672678144 logging_writer.py:48] [4] global_step=4, grad_norm=4.10369, loss=0.989167
I0315 15:05:37.510406 140545246360768 submission.py:265] 4) loss = 0.989, grad_norm = 4.104
I0315 15:05:37.590109 140503664285440 logging_writer.py:48] [5] global_step=5, grad_norm=4.91539, loss=1.01072
I0315 15:05:37.594108 140545246360768 submission.py:265] 5) loss = 1.011, grad_norm = 4.915
I0315 15:05:37.676955 140503672678144 logging_writer.py:48] [6] global_step=6, grad_norm=4.61677, loss=0.990686
I0315 15:05:37.682021 140545246360768 submission.py:265] 6) loss = 0.991, grad_norm = 4.617
I0315 15:05:37.769269 140503664285440 logging_writer.py:48] [7] global_step=7, grad_norm=4.37755, loss=0.987195
I0315 15:05:37.773786 140545246360768 submission.py:265] 7) loss = 0.987, grad_norm = 4.378
I0315 15:05:37.864044 140503672678144 logging_writer.py:48] [8] global_step=8, grad_norm=3.94087, loss=0.947168
I0315 15:05:37.873587 140545246360768 submission.py:265] 8) loss = 0.947, grad_norm = 3.941
I0315 15:05:37.962129 140503664285440 logging_writer.py:48] [9] global_step=9, grad_norm=3.49501, loss=0.963237
I0315 15:05:37.967241 140545246360768 submission.py:265] 9) loss = 0.963, grad_norm = 3.495
I0315 15:05:38.069014 140503672678144 logging_writer.py:48] [10] global_step=10, grad_norm=3.89052, loss=0.855995
I0315 15:05:38.074513 140545246360768 submission.py:265] 10) loss = 0.856, grad_norm = 3.891
I0315 15:05:38.166615 140503664285440 logging_writer.py:48] [11] global_step=11, grad_norm=3.5338, loss=0.881344
I0315 15:05:38.171245 140545246360768 submission.py:265] 11) loss = 0.881, grad_norm = 3.534
I0315 15:05:38.262264 140503672678144 logging_writer.py:48] [12] global_step=12, grad_norm=3.5595, loss=0.859213
I0315 15:05:38.269167 140545246360768 submission.py:265] 12) loss = 0.859, grad_norm = 3.560
I0315 15:05:38.360626 140503664285440 logging_writer.py:48] [13] global_step=13, grad_norm=3.63254, loss=0.858432
I0315 15:05:38.365503 140545246360768 submission.py:265] 13) loss = 0.858, grad_norm = 3.633
I0315 15:05:38.447274 140503672678144 logging_writer.py:48] [14] global_step=14, grad_norm=3.23939, loss=0.84821
I0315 15:05:38.455060 140545246360768 submission.py:265] 14) loss = 0.848, grad_norm = 3.239
I0315 15:05:38.553634 140503664285440 logging_writer.py:48] [15] global_step=15, grad_norm=3.20854, loss=0.830268
I0315 15:05:38.559235 140545246360768 submission.py:265] 15) loss = 0.830, grad_norm = 3.209
I0315 15:05:38.646786 140503672678144 logging_writer.py:48] [16] global_step=16, grad_norm=2.81391, loss=0.714672
I0315 15:05:38.651282 140545246360768 submission.py:265] 16) loss = 0.715, grad_norm = 2.814
I0315 15:05:38.737156 140503664285440 logging_writer.py:48] [17] global_step=17, grad_norm=2.63286, loss=0.77757
I0315 15:05:38.744176 140545246360768 submission.py:265] 17) loss = 0.778, grad_norm = 2.633
I0315 15:05:38.828400 140503672678144 logging_writer.py:48] [18] global_step=18, grad_norm=2.88723, loss=0.754551
I0315 15:05:38.832445 140545246360768 submission.py:265] 18) loss = 0.755, grad_norm = 2.887
I0315 15:05:38.917549 140503664285440 logging_writer.py:48] [19] global_step=19, grad_norm=2.13001, loss=0.791194
I0315 15:05:38.921794 140545246360768 submission.py:265] 19) loss = 0.791, grad_norm = 2.130
I0315 15:05:39.010979 140503672678144 logging_writer.py:48] [20] global_step=20, grad_norm=2.37328, loss=0.7055
I0315 15:05:39.016943 140545246360768 submission.py:265] 20) loss = 0.706, grad_norm = 2.373
I0315 15:05:39.094231 140503664285440 logging_writer.py:48] [21] global_step=21, grad_norm=1.67401, loss=0.690902
I0315 15:05:39.098822 140545246360768 submission.py:265] 21) loss = 0.691, grad_norm = 1.674
I0315 15:05:39.186490 140503672678144 logging_writer.py:48] [22] global_step=22, grad_norm=1.69766, loss=0.638156
I0315 15:05:39.191350 140545246360768 submission.py:265] 22) loss = 0.638, grad_norm = 1.698
I0315 15:05:39.281866 140503664285440 logging_writer.py:48] [23] global_step=23, grad_norm=1.73227, loss=0.689031
I0315 15:05:39.286984 140545246360768 submission.py:265] 23) loss = 0.689, grad_norm = 1.732
I0315 15:05:39.381830 140503672678144 logging_writer.py:48] [24] global_step=24, grad_norm=1.86879, loss=0.699655
I0315 15:05:39.387219 140545246360768 submission.py:265] 24) loss = 0.700, grad_norm = 1.869
I0315 15:05:39.473169 140503664285440 logging_writer.py:48] [25] global_step=25, grad_norm=1.45671, loss=0.689884
I0315 15:05:39.477752 140545246360768 submission.py:265] 25) loss = 0.690, grad_norm = 1.457
I0315 15:05:39.558234 140503672678144 logging_writer.py:48] [26] global_step=26, grad_norm=1.00708, loss=0.58925
I0315 15:05:39.565371 140545246360768 submission.py:265] 26) loss = 0.589, grad_norm = 1.007
I0315 15:05:39.651300 140503664285440 logging_writer.py:48] [27] global_step=27, grad_norm=1.18756, loss=0.568752
I0315 15:05:39.656936 140545246360768 submission.py:265] 27) loss = 0.569, grad_norm = 1.188
I0315 15:05:39.746856 140503672678144 logging_writer.py:48] [28] global_step=28, grad_norm=1.30547, loss=0.549266
I0315 15:05:39.751597 140545246360768 submission.py:265] 28) loss = 0.549, grad_norm = 1.305
I0315 15:05:39.836795 140503664285440 logging_writer.py:48] [29] global_step=29, grad_norm=1.17916, loss=0.55311
I0315 15:05:39.843128 140545246360768 submission.py:265] 29) loss = 0.553, grad_norm = 1.179
I0315 15:05:39.941481 140503672678144 logging_writer.py:48] [30] global_step=30, grad_norm=1.03707, loss=0.525127
I0315 15:05:39.945828 140545246360768 submission.py:265] 30) loss = 0.525, grad_norm = 1.037
I0315 15:05:40.034466 140503664285440 logging_writer.py:48] [31] global_step=31, grad_norm=1.07098, loss=0.52539
I0315 15:05:40.039877 140545246360768 submission.py:265] 31) loss = 0.525, grad_norm = 1.071
I0315 15:05:40.127575 140503672678144 logging_writer.py:48] [32] global_step=32, grad_norm=1.09964, loss=0.609095
I0315 15:05:40.134650 140545246360768 submission.py:265] 32) loss = 0.609, grad_norm = 1.100
I0315 15:05:40.223320 140503664285440 logging_writer.py:48] [33] global_step=33, grad_norm=1.34569, loss=0.571631
I0315 15:05:40.227469 140545246360768 submission.py:265] 33) loss = 0.572, grad_norm = 1.346
I0315 15:05:40.311220 140503672678144 logging_writer.py:48] [34] global_step=34, grad_norm=1.10506, loss=0.524547
I0315 15:05:40.320125 140545246360768 submission.py:265] 34) loss = 0.525, grad_norm = 1.105
I0315 15:05:40.402138 140503664285440 logging_writer.py:48] [35] global_step=35, grad_norm=1.54402, loss=0.525314
I0315 15:05:40.414541 140545246360768 submission.py:265] 35) loss = 0.525, grad_norm = 1.544
I0315 15:05:40.492516 140503672678144 logging_writer.py:48] [36] global_step=36, grad_norm=0.856024, loss=0.540227
I0315 15:05:40.496954 140545246360768 submission.py:265] 36) loss = 0.540, grad_norm = 0.856
I0315 15:05:40.584978 140503664285440 logging_writer.py:48] [37] global_step=37, grad_norm=1.91374, loss=0.509051
I0315 15:05:40.589855 140545246360768 submission.py:265] 37) loss = 0.509, grad_norm = 1.914
I0315 15:05:40.674961 140503672678144 logging_writer.py:48] [38] global_step=38, grad_norm=1.33121, loss=0.504148
I0315 15:05:40.681142 140545246360768 submission.py:265] 38) loss = 0.504, grad_norm = 1.331
I0315 15:05:40.767117 140503664285440 logging_writer.py:48] [39] global_step=39, grad_norm=1.2319, loss=0.527539
I0315 15:05:40.771807 140545246360768 submission.py:265] 39) loss = 0.528, grad_norm = 1.232
I0315 15:05:40.853440 140503672678144 logging_writer.py:48] [40] global_step=40, grad_norm=1.34293, loss=0.498612
I0315 15:05:40.857811 140545246360768 submission.py:265] 40) loss = 0.499, grad_norm = 1.343
I0315 15:05:40.932848 140503664285440 logging_writer.py:48] [41] global_step=41, grad_norm=1.07704, loss=0.526181
I0315 15:05:40.937584 140545246360768 submission.py:265] 41) loss = 0.526, grad_norm = 1.077
I0315 15:05:41.020655 140503672678144 logging_writer.py:48] [42] global_step=42, grad_norm=1.18906, loss=0.594959
I0315 15:05:41.030510 140545246360768 submission.py:265] 42) loss = 0.595, grad_norm = 1.189
I0315 15:05:41.109611 140503664285440 logging_writer.py:48] [43] global_step=43, grad_norm=1.19886, loss=0.444947
I0315 15:05:41.114067 140545246360768 submission.py:265] 43) loss = 0.445, grad_norm = 1.199
I0315 15:05:41.186550 140503672678144 logging_writer.py:48] [44] global_step=44, grad_norm=1.44275, loss=0.570562
I0315 15:05:41.194627 140545246360768 submission.py:265] 44) loss = 0.571, grad_norm = 1.443
I0315 15:05:41.277751 140503664285440 logging_writer.py:48] [45] global_step=45, grad_norm=1.35173, loss=0.45614
I0315 15:05:41.282700 140545246360768 submission.py:265] 45) loss = 0.456, grad_norm = 1.352
I0315 15:05:41.358250 140503672678144 logging_writer.py:48] [46] global_step=46, grad_norm=1.25833, loss=0.435425
I0315 15:05:41.363278 140545246360768 submission.py:265] 46) loss = 0.435, grad_norm = 1.258
I0315 15:05:41.444874 140503664285440 logging_writer.py:48] [47] global_step=47, grad_norm=1.78822, loss=0.482866
I0315 15:05:41.450353 140545246360768 submission.py:265] 47) loss = 0.483, grad_norm = 1.788
I0315 15:05:41.524651 140503672678144 logging_writer.py:48] [48] global_step=48, grad_norm=1.36114, loss=0.506666
I0315 15:05:41.532170 140545246360768 submission.py:265] 48) loss = 0.507, grad_norm = 1.361
I0315 15:05:41.611870 140503664285440 logging_writer.py:48] [49] global_step=49, grad_norm=1.00424, loss=0.433139
I0315 15:05:41.617726 140545246360768 submission.py:265] 49) loss = 0.433, grad_norm = 1.004
I0315 15:05:41.692842 140503672678144 logging_writer.py:48] [50] global_step=50, grad_norm=1.39319, loss=0.468428
I0315 15:05:41.697985 140545246360768 submission.py:265] 50) loss = 0.468, grad_norm = 1.393
I0315 15:05:41.772166 140503664285440 logging_writer.py:48] [51] global_step=51, grad_norm=1.29988, loss=0.439481
I0315 15:05:41.776840 140545246360768 submission.py:265] 51) loss = 0.439, grad_norm = 1.300
I0315 15:05:41.852105 140503672678144 logging_writer.py:48] [52] global_step=52, grad_norm=0.968632, loss=0.474163
I0315 15:05:41.857856 140545246360768 submission.py:265] 52) loss = 0.474, grad_norm = 0.969
I0315 15:05:42.072312 140503664285440 logging_writer.py:48] [53] global_step=53, grad_norm=1.27405, loss=0.47607
I0315 15:05:42.079006 140545246360768 submission.py:265] 53) loss = 0.476, grad_norm = 1.274
I0315 15:05:42.305503 140503672678144 logging_writer.py:48] [54] global_step=54, grad_norm=1.01854, loss=0.475349
I0315 15:05:42.310094 140545246360768 submission.py:265] 54) loss = 0.475, grad_norm = 1.019
I0315 15:05:42.699280 140503664285440 logging_writer.py:48] [55] global_step=55, grad_norm=1.28122, loss=0.411506
I0315 15:05:42.703045 140545246360768 submission.py:265] 55) loss = 0.412, grad_norm = 1.281
I0315 15:05:43.250012 140503672678144 logging_writer.py:48] [56] global_step=56, grad_norm=1.36809, loss=0.481835
I0315 15:05:43.255314 140545246360768 submission.py:265] 56) loss = 0.482, grad_norm = 1.368
I0315 15:05:43.641240 140503664285440 logging_writer.py:48] [57] global_step=57, grad_norm=1.04544, loss=0.395674
I0315 15:05:43.645258 140545246360768 submission.py:265] 57) loss = 0.396, grad_norm = 1.045
I0315 15:05:43.935450 140503672678144 logging_writer.py:48] [58] global_step=58, grad_norm=1.11906, loss=0.446907
I0315 15:05:43.941323 140545246360768 submission.py:265] 58) loss = 0.447, grad_norm = 1.119
I0315 15:05:44.243613 140503664285440 logging_writer.py:48] [59] global_step=59, grad_norm=0.801201, loss=0.456779
I0315 15:05:44.248869 140545246360768 submission.py:265] 59) loss = 0.457, grad_norm = 0.801
I0315 15:05:44.425559 140503672678144 logging_writer.py:48] [60] global_step=60, grad_norm=0.770987, loss=0.414277
I0315 15:05:44.431261 140545246360768 submission.py:265] 60) loss = 0.414, grad_norm = 0.771
I0315 15:05:44.681445 140503664285440 logging_writer.py:48] [61] global_step=61, grad_norm=0.739486, loss=0.411247
I0315 15:05:44.689985 140545246360768 submission.py:265] 61) loss = 0.411, grad_norm = 0.739
I0315 15:05:45.114524 140503672678144 logging_writer.py:48] [62] global_step=62, grad_norm=1.15874, loss=0.392242
I0315 15:05:45.120548 140545246360768 submission.py:265] 62) loss = 0.392, grad_norm = 1.159
I0315 15:05:45.339264 140503664285440 logging_writer.py:48] [63] global_step=63, grad_norm=1.01035, loss=0.385892
I0315 15:05:45.346597 140545246360768 submission.py:265] 63) loss = 0.386, grad_norm = 1.010
I0315 15:05:45.694709 140503672678144 logging_writer.py:48] [64] global_step=64, grad_norm=1.90416, loss=0.506626
I0315 15:05:45.701329 140545246360768 submission.py:265] 64) loss = 0.507, grad_norm = 1.904
I0315 15:05:45.980844 140503664285440 logging_writer.py:48] [65] global_step=65, grad_norm=0.956857, loss=0.547878
I0315 15:05:45.989134 140545246360768 submission.py:265] 65) loss = 0.548, grad_norm = 0.957
I0315 15:05:46.266391 140503672678144 logging_writer.py:48] [66] global_step=66, grad_norm=1.07468, loss=0.365462
I0315 15:05:46.272151 140545246360768 submission.py:265] 66) loss = 0.365, grad_norm = 1.075
I0315 15:05:46.498841 140503664285440 logging_writer.py:48] [67] global_step=67, grad_norm=2.07071, loss=0.430174
I0315 15:05:46.503911 140545246360768 submission.py:265] 67) loss = 0.430, grad_norm = 2.071
I0315 15:05:46.750670 140503672678144 logging_writer.py:48] [68] global_step=68, grad_norm=1.20057, loss=0.450438
I0315 15:05:46.759956 140545246360768 submission.py:265] 68) loss = 0.450, grad_norm = 1.201
I0315 15:05:46.935047 140503664285440 logging_writer.py:48] [69] global_step=69, grad_norm=1.54053, loss=0.41104
I0315 15:05:46.941968 140545246360768 submission.py:265] 69) loss = 0.411, grad_norm = 1.541
I0315 15:05:47.033392 140503672678144 logging_writer.py:48] [70] global_step=70, grad_norm=0.875034, loss=0.533345
I0315 15:05:47.039274 140545246360768 submission.py:265] 70) loss = 0.533, grad_norm = 0.875
I0315 15:05:47.194783 140503664285440 logging_writer.py:48] [71] global_step=71, grad_norm=0.629998, loss=0.399349
I0315 15:05:47.203043 140545246360768 submission.py:265] 71) loss = 0.399, grad_norm = 0.630
I0315 15:05:47.372193 140503672678144 logging_writer.py:48] [72] global_step=72, grad_norm=1.12254, loss=0.400585
I0315 15:05:47.376818 140545246360768 submission.py:265] 72) loss = 0.401, grad_norm = 1.123
I0315 15:05:47.482496 140503664285440 logging_writer.py:48] [73] global_step=73, grad_norm=1.09249, loss=0.412729
I0315 15:05:47.488192 140545246360768 submission.py:265] 73) loss = 0.413, grad_norm = 1.092
I0315 15:05:47.590274 140503672678144 logging_writer.py:48] [74] global_step=74, grad_norm=0.567006, loss=0.377517
I0315 15:05:47.600296 140545246360768 submission.py:265] 74) loss = 0.378, grad_norm = 0.567
I0315 15:05:47.676619 140503664285440 logging_writer.py:48] [75] global_step=75, grad_norm=0.737878, loss=0.454096
I0315 15:05:47.681782 140545246360768 submission.py:265] 75) loss = 0.454, grad_norm = 0.738
I0315 15:05:47.773689 140503672678144 logging_writer.py:48] [76] global_step=76, grad_norm=0.876906, loss=0.434624
I0315 15:05:47.779828 140545246360768 submission.py:265] 76) loss = 0.435, grad_norm = 0.877
I0315 15:05:47.883320 140503664285440 logging_writer.py:48] [77] global_step=77, grad_norm=0.909789, loss=0.438389
I0315 15:05:47.889304 140545246360768 submission.py:265] 77) loss = 0.438, grad_norm = 0.910
I0315 15:05:48.044084 140503672678144 logging_writer.py:48] [78] global_step=78, grad_norm=1.01242, loss=0.466684
I0315 15:05:48.049417 140545246360768 submission.py:265] 78) loss = 0.467, grad_norm = 1.012
I0315 15:05:48.232719 140503664285440 logging_writer.py:48] [79] global_step=79, grad_norm=0.783934, loss=0.360651
I0315 15:05:48.238025 140545246360768 submission.py:265] 79) loss = 0.361, grad_norm = 0.784
I0315 15:05:48.385291 140503672678144 logging_writer.py:48] [80] global_step=80, grad_norm=0.580958, loss=0.392697
I0315 15:05:48.390492 140545246360768 submission.py:265] 80) loss = 0.393, grad_norm = 0.581
I0315 15:05:48.580502 140503664285440 logging_writer.py:48] [81] global_step=81, grad_norm=0.440209, loss=0.439238
I0315 15:05:48.586521 140545246360768 submission.py:265] 81) loss = 0.439, grad_norm = 0.440
I0315 15:05:48.750577 140503672678144 logging_writer.py:48] [82] global_step=82, grad_norm=0.966036, loss=0.349311
I0315 15:05:48.756952 140545246360768 submission.py:265] 82) loss = 0.349, grad_norm = 0.966
I0315 15:05:48.869376 140503664285440 logging_writer.py:48] [83] global_step=83, grad_norm=0.883533, loss=0.42251
I0315 15:05:48.873508 140545246360768 submission.py:265] 83) loss = 0.423, grad_norm = 0.884
I0315 15:05:49.062397 140503672678144 logging_writer.py:48] [84] global_step=84, grad_norm=0.594499, loss=0.43761
I0315 15:05:49.067772 140545246360768 submission.py:265] 84) loss = 0.438, grad_norm = 0.594
I0315 15:05:49.170057 140503664285440 logging_writer.py:48] [85] global_step=85, grad_norm=0.839223, loss=0.422263
I0315 15:05:49.174567 140545246360768 submission.py:265] 85) loss = 0.422, grad_norm = 0.839
I0315 15:05:49.283311 140503672678144 logging_writer.py:48] [86] global_step=86, grad_norm=1.13475, loss=0.36487
I0315 15:05:49.288368 140545246360768 submission.py:265] 86) loss = 0.365, grad_norm = 1.135
I0315 15:05:49.365187 140503664285440 logging_writer.py:48] [87] global_step=87, grad_norm=1.19537, loss=0.394993
I0315 15:05:49.370993 140545246360768 submission.py:265] 87) loss = 0.395, grad_norm = 1.195
I0315 15:05:49.460162 140503672678144 logging_writer.py:48] [88] global_step=88, grad_norm=0.754461, loss=0.446723
I0315 15:05:49.465461 140545246360768 submission.py:265] 88) loss = 0.447, grad_norm = 0.754
I0315 15:05:49.595333 140503664285440 logging_writer.py:48] [89] global_step=89, grad_norm=1.36826, loss=0.393048
I0315 15:05:49.600537 140545246360768 submission.py:265] 89) loss = 0.393, grad_norm = 1.368
I0315 15:05:49.769766 140503672678144 logging_writer.py:48] [90] global_step=90, grad_norm=0.670463, loss=0.426958
I0315 15:05:49.776334 140545246360768 submission.py:265] 90) loss = 0.427, grad_norm = 0.670
I0315 15:05:49.981639 140503664285440 logging_writer.py:48] [91] global_step=91, grad_norm=0.643442, loss=0.368039
I0315 15:05:49.987789 140545246360768 submission.py:265] 91) loss = 0.368, grad_norm = 0.643
I0315 15:05:50.247790 140503672678144 logging_writer.py:48] [92] global_step=92, grad_norm=0.79588, loss=0.411146
I0315 15:05:50.253577 140545246360768 submission.py:265] 92) loss = 0.411, grad_norm = 0.796
I0315 15:05:50.417189 140503664285440 logging_writer.py:48] [93] global_step=93, grad_norm=0.628845, loss=0.481856
I0315 15:05:50.422455 140545246360768 submission.py:265] 93) loss = 0.482, grad_norm = 0.629
I0315 15:05:50.649089 140503672678144 logging_writer.py:48] [94] global_step=94, grad_norm=0.750283, loss=0.362467
I0315 15:05:50.653742 140545246360768 submission.py:265] 94) loss = 0.362, grad_norm = 0.750
I0315 15:05:51.013712 140503664285440 logging_writer.py:48] [95] global_step=95, grad_norm=0.709721, loss=0.416807
I0315 15:05:51.019451 140545246360768 submission.py:265] 95) loss = 0.417, grad_norm = 0.710
I0315 15:05:51.259703 140503672678144 logging_writer.py:48] [96] global_step=96, grad_norm=1.03012, loss=0.417742
I0315 15:05:51.268678 140545246360768 submission.py:265] 96) loss = 0.418, grad_norm = 1.030
I0315 15:05:51.495309 140503664285440 logging_writer.py:48] [97] global_step=97, grad_norm=0.793652, loss=0.476148
I0315 15:05:51.504162 140545246360768 submission.py:265] 97) loss = 0.476, grad_norm = 0.794
I0315 15:05:51.916717 140503672678144 logging_writer.py:48] [98] global_step=98, grad_norm=0.662762, loss=0.448886
I0315 15:05:51.922106 140545246360768 submission.py:265] 98) loss = 0.449, grad_norm = 0.663
I0315 15:05:52.502250 140503664285440 logging_writer.py:48] [99] global_step=99, grad_norm=0.64718, loss=0.382315
I0315 15:05:52.506974 140545246360768 submission.py:265] 99) loss = 0.382, grad_norm = 0.647
I0315 15:05:52.683621 140503672678144 logging_writer.py:48] [100] global_step=100, grad_norm=0.697227, loss=0.347434
I0315 15:05:52.688683 140545246360768 submission.py:265] 100) loss = 0.347, grad_norm = 0.697
I0315 15:06:59.048620 140545246360768 spec.py:321] Evaluating on the training split.
I0315 15:07:01.113539 140545246360768 spec.py:333] Evaluating on the validation split.
I0315 15:07:03.325762 140545246360768 spec.py:349] Evaluating on the test split.
I0315 15:07:05.460536 140545246360768 submission_runner.py:469] Time since start: 619.86s, 	Step: 349, 	{'train/ssim': 0.6947334834507534, 'train/loss': 0.32060285976954866, 'validation/ssim': 0.674170773688098, 'validation/loss': 0.33712845836117755, 'validation/num_examples': 3554, 'test/ssim': 0.6926874940004538, 'test/loss': 0.3384853300186749, 'test/num_examples': 3581, 'score': 158.6087293624878, 'total_duration': 619.8601343631744, 'accumulated_submission_time': 158.6087293624878, 'accumulated_eval_time': 458.5959515571594, 'accumulated_logging_time': 0.016819000244140625}
I0315 15:07:05.476085 140503664285440 logging_writer.py:48] [349] accumulated_eval_time=458.596, accumulated_logging_time=0.016819, accumulated_submission_time=158.609, global_step=349, preemption_count=0, score=158.609, test/loss=0.338485, test/num_examples=3581, test/ssim=0.692687, total_duration=619.86, train/loss=0.320603, train/ssim=0.694733, validation/loss=0.337128, validation/num_examples=3554, validation/ssim=0.674171
/usr/local/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
I0315 15:07:37.479050 140503672678144 logging_writer.py:48] [500] global_step=500, grad_norm=0.375892, loss=0.45891
I0315 15:07:37.486741 140545246360768 submission.py:265] 500) loss = 0.459, grad_norm = 0.376
I0315 15:08:26.411512 140545246360768 spec.py:321] Evaluating on the training split.
I0315 15:08:28.757084 140545246360768 spec.py:333] Evaluating on the validation split.
I0315 15:08:31.186506 140545246360768 spec.py:349] Evaluating on the test split.
I0315 15:08:33.356710 140545246360768 submission_runner.py:469] Time since start: 707.76s, 	Step: 735, 	{'train/ssim': 0.7143486567905971, 'train/loss': 0.29745735440935406, 'validation/ssim': 0.6939609336531022, 'validation/loss': 0.3132754246975239, 'validation/num_examples': 3554, 'test/ssim': 0.7117130777017593, 'test/loss': 0.3154841606525761, 'test/num_examples': 3581, 'score': 237.71569681167603, 'total_duration': 707.75639128685, 'accumulated_submission_time': 237.71569681167603, 'accumulated_eval_time': 465.54317593574524, 'accumulated_logging_time': 0.05019569396972656}
I0315 15:08:33.366582 140503664285440 logging_writer.py:48] [735] accumulated_eval_time=465.543, accumulated_logging_time=0.0501957, accumulated_submission_time=237.716, global_step=735, preemption_count=0, score=237.716, test/loss=0.315484, test/num_examples=3581, test/ssim=0.711713, total_duration=707.756, train/loss=0.297457, train/ssim=0.714349, validation/loss=0.313275, validation/num_examples=3554, validation/ssim=0.693961
I0315 15:09:19.437936 140503672678144 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.172347, loss=0.41186
I0315 15:09:19.484788 140545246360768 submission.py:265] 1000) loss = 0.412, grad_norm = 0.172
I0315 15:09:51.491916 140503664285440 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.159058, loss=0.36508
I0315 15:09:51.495131 140545246360768 submission.py:265] 1500) loss = 0.365, grad_norm = 0.159
I0315 15:09:54.148771 140545246360768 spec.py:321] Evaluating on the training split.
I0315 15:09:56.145071 140545246360768 spec.py:333] Evaluating on the validation split.
I0315 15:09:58.959296 140545246360768 spec.py:349] Evaluating on the test split.
I0315 15:10:01.412353 140545246360768 submission_runner.py:469] Time since start: 795.81s, 	Step: 1532, 	{'train/ssim': 0.7302229745047433, 'train/loss': 0.28371844972882954, 'validation/ssim': 0.7102234882790518, 'validation/loss': 0.29945980623725027, 'validation/num_examples': 3554, 'test/ssim': 0.7275227682778903, 'test/loss': 0.3014453929484606, 'test/num_examples': 3581, 'score': 316.54540371894836, 'total_duration': 795.8120169639587, 'accumulated_submission_time': 316.54540371894836, 'accumulated_eval_time': 472.80690693855286, 'accumulated_logging_time': 0.06904387474060059}
I0315 15:10:01.482047 140503672678144 logging_writer.py:48] [1532] accumulated_eval_time=472.807, accumulated_logging_time=0.0690439, accumulated_submission_time=316.545, global_step=1532, preemption_count=0, score=316.545, test/loss=0.301445, test/num_examples=3581, test/ssim=0.727523, total_duration=795.812, train/loss=0.283718, train/ssim=0.730223, validation/loss=0.29946, validation/num_examples=3554, validation/ssim=0.710223
I0315 15:10:31.877262 140503664285440 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.45222, loss=0.245556
I0315 15:10:31.881139 140545246360768 submission.py:265] 2000) loss = 0.246, grad_norm = 0.452
I0315 15:11:03.416532 140503672678144 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.148336, loss=0.301102
I0315 15:11:03.419942 140545246360768 submission.py:265] 2500) loss = 0.301, grad_norm = 0.148
I0315 15:11:22.179678 140545246360768 spec.py:321] Evaluating on the training split.
I0315 15:11:24.160551 140545246360768 spec.py:333] Evaluating on the validation split.
I0315 15:11:26.567466 140545246360768 spec.py:349] Evaluating on the test split.
I0315 15:11:28.837259 140545246360768 submission_runner.py:469] Time since start: 883.24s, 	Step: 2789, 	{'train/ssim': 0.7356462478637695, 'train/loss': 0.2776554993220738, 'validation/ssim': 0.7143358220930641, 'validation/loss': 0.29420302030942247, 'validation/num_examples': 3554, 'test/ssim': 0.7317172010262496, 'test/loss': 0.2959561490439996, 'test/num_examples': 3581, 'score': 395.26741456985474, 'total_duration': 883.2369468212128, 'accumulated_submission_time': 395.26741456985474, 'accumulated_eval_time': 479.46471905708313, 'accumulated_logging_time': 0.1469886302947998}
I0315 15:11:28.847816 140503664285440 logging_writer.py:48] [2789] accumulated_eval_time=479.465, accumulated_logging_time=0.146989, accumulated_submission_time=395.267, global_step=2789, preemption_count=0, score=395.267, test/loss=0.295956, test/num_examples=3581, test/ssim=0.731717, total_duration=883.237, train/loss=0.277655, train/ssim=0.735646, validation/loss=0.294203, validation/num_examples=3554, validation/ssim=0.714336
I0315 15:11:42.999292 140503672678144 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.328567, loss=0.247511
I0315 15:11:43.002811 140545246360768 submission.py:265] 3000) loss = 0.248, grad_norm = 0.329
I0315 15:12:14.480277 140503664285440 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.2915, loss=0.270015
I0315 15:12:14.483770 140545246360768 submission.py:265] 3500) loss = 0.270, grad_norm = 0.291
I0315 15:12:45.967444 140503672678144 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.319733, loss=0.302682
I0315 15:12:45.971154 140545246360768 submission.py:265] 4000) loss = 0.303, grad_norm = 0.320
I0315 15:12:49.579639 140545246360768 spec.py:321] Evaluating on the training split.
I0315 15:12:51.540716 140545246360768 spec.py:333] Evaluating on the validation split.
I0315 15:12:54.233052 140545246360768 spec.py:349] Evaluating on the test split.
I0315 15:12:56.612186 140545246360768 submission_runner.py:469] Time since start: 971.01s, 	Step: 4047, 	{'train/ssim': 0.7369195393153599, 'train/loss': 0.2755146196910313, 'validation/ssim': 0.7155651807382527, 'validation/loss': 0.29200544555870145, 'validation/num_examples': 3554, 'test/ssim': 0.7329405630410499, 'test/loss': 0.29360275886405685, 'test/num_examples': 3581, 'score': 474.0532274246216, 'total_duration': 971.0118517875671, 'accumulated_submission_time': 474.0532274246216, 'accumulated_eval_time': 486.49737071990967, 'accumulated_logging_time': 0.1667482852935791}
I0315 15:12:56.622746 140503664285440 logging_writer.py:48] [4047] accumulated_eval_time=486.497, accumulated_logging_time=0.166748, accumulated_submission_time=474.053, global_step=4047, preemption_count=0, score=474.053, test/loss=0.293603, test/num_examples=3581, test/ssim=0.732941, total_duration=971.012, train/loss=0.275515, train/ssim=0.73692, validation/loss=0.292005, validation/num_examples=3554, validation/ssim=0.715565
I0315 15:13:26.030033 140503672678144 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.131169, loss=0.299064
I0315 15:13:26.033803 140545246360768 submission.py:265] 4500) loss = 0.299, grad_norm = 0.131
I0315 15:13:57.566093 140503664285440 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.645729, loss=0.280561
I0315 15:13:57.569437 140545246360768 submission.py:265] 5000) loss = 0.281, grad_norm = 0.646
I0315 15:14:17.297417 140545246360768 spec.py:321] Evaluating on the training split.
I0315 15:14:19.285518 140545246360768 spec.py:333] Evaluating on the validation split.
I0315 15:14:21.566026 140545246360768 spec.py:349] Evaluating on the test split.
I0315 15:14:23.822200 140545246360768 submission_runner.py:469] Time since start: 1058.22s, 	Step: 5304, 	{'train/ssim': 0.7382236889430455, 'train/loss': 0.274721486227853, 'validation/ssim': 0.716497023050612, 'validation/loss': 0.29171802733275887, 'validation/num_examples': 3554, 'test/ssim': 0.733952509228393, 'test/loss': 0.29333601767575396, 'test/num_examples': 3581, 'score': 552.8274099826813, 'total_duration': 1058.2217135429382, 'accumulated_submission_time': 552.8274099826813, 'accumulated_eval_time': 493.0220489501953, 'accumulated_logging_time': 0.18542146682739258}
I0315 15:14:23.839518 140503672678144 logging_writer.py:48] [5304] accumulated_eval_time=493.022, accumulated_logging_time=0.185421, accumulated_submission_time=552.827, global_step=5304, preemption_count=0, score=552.827, test/loss=0.293336, test/num_examples=3581, test/ssim=0.733953, total_duration=1058.22, train/loss=0.274721, train/ssim=0.738224, validation/loss=0.291718, validation/num_examples=3554, validation/ssim=0.716497
I0315 15:14:37.046381 140503664285440 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.134389, loss=0.242929
I0315 15:14:37.049455 140545246360768 submission.py:265] 5500) loss = 0.243, grad_norm = 0.134
I0315 15:15:08.844995 140503672678144 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.364774, loss=0.28665
I0315 15:15:08.848374 140545246360768 submission.py:265] 6000) loss = 0.287, grad_norm = 0.365
I0315 15:15:40.606206 140503664285440 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.262298, loss=0.268027
I0315 15:15:40.609661 140545246360768 submission.py:265] 6500) loss = 0.268, grad_norm = 0.262
I0315 15:15:44.495007 140545246360768 spec.py:321] Evaluating on the training split.
I0315 15:15:46.475389 140545246360768 spec.py:333] Evaluating on the validation split.
I0315 15:15:48.904335 140545246360768 spec.py:349] Evaluating on the test split.
I0315 15:15:51.067692 140545246360768 submission_runner.py:469] Time since start: 1145.47s, 	Step: 6553, 	{'train/ssim': 0.7401924133300781, 'train/loss': 0.2733228547232492, 'validation/ssim': 0.7191440322523917, 'validation/loss': 0.28983424953617404, 'validation/num_examples': 3554, 'test/ssim': 0.7365468357302429, 'test/loss': 0.2913551447745043, 'test/num_examples': 3581, 'score': 631.6341784000397, 'total_duration': 1145.467333316803, 'accumulated_submission_time': 631.6341784000397, 'accumulated_eval_time': 499.5948967933655, 'accumulated_logging_time': 0.21764516830444336}
I0315 15:15:51.077764 140503672678144 logging_writer.py:48] [6553] accumulated_eval_time=499.595, accumulated_logging_time=0.217645, accumulated_submission_time=631.634, global_step=6553, preemption_count=0, score=631.634, test/loss=0.291355, test/num_examples=3581, test/ssim=0.736547, total_duration=1145.47, train/loss=0.273323, train/ssim=0.740192, validation/loss=0.289834, validation/num_examples=3554, validation/ssim=0.719144
I0315 15:16:25.857866 140503664285440 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.160169, loss=0.262461
I0315 15:16:25.861646 140545246360768 submission.py:265] 7000) loss = 0.262, grad_norm = 0.160
I0315 15:16:57.361904 140503672678144 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.3367, loss=0.277593
I0315 15:16:57.365659 140545246360768 submission.py:265] 7500) loss = 0.278, grad_norm = 0.337
I0315 15:17:11.807464 140545246360768 spec.py:321] Evaluating on the training split.
I0315 15:17:13.788066 140545246360768 spec.py:333] Evaluating on the validation split.
I0315 15:17:15.908133 140545246360768 spec.py:349] Evaluating on the test split.
I0315 15:17:17.986393 140545246360768 submission_runner.py:469] Time since start: 1232.39s, 	Step: 7720, 	{'train/ssim': 0.7415270124162946, 'train/loss': 0.2727466481072562, 'validation/ssim': 0.7203075814663056, 'validation/loss': 0.2894570474619267, 'validation/num_examples': 3554, 'test/ssim': 0.7376472070476124, 'test/loss': 0.2909349038327283, 'test/num_examples': 3581, 'score': 705.268030166626, 'total_duration': 1232.3860878944397, 'accumulated_submission_time': 705.268030166626, 'accumulated_eval_time': 505.77395606040955, 'accumulated_logging_time': 5.113474130630493}
I0315 15:17:17.997153 140503664285440 logging_writer.py:48] [7720] accumulated_eval_time=505.774, accumulated_logging_time=5.11347, accumulated_submission_time=705.268, global_step=7720, preemption_count=0, score=705.268, test/loss=0.290935, test/num_examples=3581, test/ssim=0.737647, total_duration=1232.39, train/loss=0.272747, train/ssim=0.741527, validation/loss=0.289457, validation/num_examples=3554, validation/ssim=0.720308
I0315 15:17:36.484572 140503672678144 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.272847, loss=0.328775
I0315 15:17:36.488462 140545246360768 submission.py:265] 8000) loss = 0.329, grad_norm = 0.273
I0315 15:18:07.975722 140503664285440 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.154772, loss=0.334861
I0315 15:18:07.979256 140545246360768 submission.py:265] 8500) loss = 0.335, grad_norm = 0.155
I0315 15:18:38.748487 140545246360768 spec.py:321] Evaluating on the training split.
I0315 15:18:40.752384 140545246360768 spec.py:333] Evaluating on the validation split.
I0315 15:18:42.907489 140545246360768 spec.py:349] Evaluating on the test split.
I0315 15:18:45.026998 140545246360768 submission_runner.py:469] Time since start: 1319.43s, 	Step: 8978, 	{'train/ssim': 0.7397443226405552, 'train/loss': 0.2728936331612723, 'validation/ssim': 0.7184170372511607, 'validation/loss': 0.28952890201841236, 'validation/num_examples': 3554, 'test/ssim': 0.7359562894879224, 'test/loss': 0.290887077904653, 'test/num_examples': 3581, 'score': 784.1282937526703, 'total_duration': 1319.4266967773438, 'accumulated_submission_time': 784.1282937526703, 'accumulated_eval_time': 512.0526666641235, 'accumulated_logging_time': 5.132548570632935}
I0315 15:18:45.038718 140503672678144 logging_writer.py:48] [8978] accumulated_eval_time=512.053, accumulated_logging_time=5.13255, accumulated_submission_time=784.128, global_step=8978, preemption_count=0, score=784.128, test/loss=0.290887, test/num_examples=3581, test/ssim=0.735956, total_duration=1319.43, train/loss=0.272894, train/ssim=0.739744, validation/loss=0.289529, validation/num_examples=3554, validation/ssim=0.718417
I0315 15:18:47.374812 140503664285440 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.343475, loss=0.344101
I0315 15:18:47.378286 140545246360768 submission.py:265] 9000) loss = 0.344, grad_norm = 0.343
I0315 15:19:18.931121 140503672678144 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.330838, loss=0.342146
I0315 15:19:18.934932 140545246360768 submission.py:265] 9500) loss = 0.342, grad_norm = 0.331
I0315 15:19:50.466334 140503664285440 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.231032, loss=0.265652
I0315 15:19:50.469918 140545246360768 submission.py:265] 10000) loss = 0.266, grad_norm = 0.231
I0315 15:20:05.770886 140545246360768 spec.py:321] Evaluating on the training split.
I0315 15:20:07.868936 140545246360768 spec.py:333] Evaluating on the validation split.
I0315 15:20:10.063060 140545246360768 spec.py:349] Evaluating on the test split.
I0315 15:20:12.264244 140545246360768 submission_runner.py:469] Time since start: 1406.66s, 	Step: 10234, 	{'train/ssim': 0.7415193830217633, 'train/loss': 0.2722433124269758, 'validation/ssim': 0.7204449019810425, 'validation/loss': 0.288885233572647, 'validation/num_examples': 3554, 'test/ssim': 0.7378794167568417, 'test/loss': 0.2903023607668947, 'test/num_examples': 3581, 'score': 862.9247846603394, 'total_duration': 1406.6638991832733, 'accumulated_submission_time': 862.9247846603394, 'accumulated_eval_time': 518.5461127758026, 'accumulated_logging_time': 5.153214931488037}
I0315 15:20:12.274955 140503672678144 logging_writer.py:48] [10234] accumulated_eval_time=518.546, accumulated_logging_time=5.15321, accumulated_submission_time=862.925, global_step=10234, preemption_count=0, score=862.925, test/loss=0.290302, test/num_examples=3581, test/ssim=0.737879, total_duration=1406.66, train/loss=0.272243, train/ssim=0.741519, validation/loss=0.288885, validation/num_examples=3554, validation/ssim=0.720445
I0315 15:20:29.837065 140503664285440 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.31224, loss=0.249097
I0315 15:20:29.840420 140545246360768 submission.py:265] 10500) loss = 0.249, grad_norm = 0.312
I0315 15:21:01.251969 140503672678144 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.30975, loss=0.391175
I0315 15:21:01.255939 140545246360768 submission.py:265] 11000) loss = 0.391, grad_norm = 0.310
I0315 15:21:32.952784 140545246360768 spec.py:321] Evaluating on the training split.
I0315 15:21:34.926283 140545246360768 spec.py:333] Evaluating on the validation split.
I0315 15:21:36.994113 140545246360768 spec.py:349] Evaluating on the test split.
I0315 15:21:39.061712 140545246360768 submission_runner.py:469] Time since start: 1493.46s, 	Step: 11496, 	{'train/ssim': 0.7418069839477539, 'train/loss': 0.27199402877262663, 'validation/ssim': 0.7202097603492543, 'validation/loss': 0.2887606215597742, 'validation/num_examples': 3554, 'test/ssim': 0.7376744095355696, 'test/loss': 0.2901994821846202, 'test/num_examples': 3581, 'score': 941.7866492271423, 'total_duration': 1493.461401462555, 'accumulated_submission_time': 941.7866492271423, 'accumulated_eval_time': 524.6551775932312, 'accumulated_logging_time': 5.172511577606201}
I0315 15:21:39.072573 140503664285440 logging_writer.py:48] [11496] accumulated_eval_time=524.655, accumulated_logging_time=5.17251, accumulated_submission_time=941.787, global_step=11496, preemption_count=0, score=941.787, test/loss=0.290199, test/num_examples=3581, test/ssim=0.737674, total_duration=1493.46, train/loss=0.271994, train/ssim=0.741807, validation/loss=0.288761, validation/num_examples=3554, validation/ssim=0.72021
I0315 15:21:40.147056 140503672678144 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.344906, loss=0.305743
I0315 15:21:40.150911 140545246360768 submission.py:265] 11500) loss = 0.306, grad_norm = 0.345
I0315 15:22:11.764888 140503664285440 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.4279, loss=0.280093
I0315 15:22:11.768299 140545246360768 submission.py:265] 12000) loss = 0.280, grad_norm = 0.428
I0315 15:22:43.269035 140503672678144 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.208012, loss=0.356704
I0315 15:22:43.272977 140545246360768 submission.py:265] 12500) loss = 0.357, grad_norm = 0.208
I0315 15:22:59.752282 140545246360768 spec.py:321] Evaluating on the training split.
I0315 15:23:01.723731 140545246360768 spec.py:333] Evaluating on the validation split.
I0315 15:23:03.795186 140545246360768 spec.py:349] Evaluating on the test split.
I0315 15:23:05.850670 140545246360768 submission_runner.py:469] Time since start: 1580.25s, 	Step: 12752, 	{'train/ssim': 0.7423528262547084, 'train/loss': 0.2714330809456961, 'validation/ssim': 0.7208098764156584, 'validation/loss': 0.2882201667575267, 'validation/num_examples': 3554, 'test/ssim': 0.7382207091245462, 'test/loss': 0.2896644999214605, 'test/num_examples': 3581, 'score': 1020.6520760059357, 'total_duration': 1580.2503561973572, 'accumulated_submission_time': 1020.6520760059357, 'accumulated_eval_time': 530.7537386417389, 'accumulated_logging_time': 5.19197940826416}
I0315 15:23:05.861747 140503664285440 logging_writer.py:48] [12752] accumulated_eval_time=530.754, accumulated_logging_time=5.19198, accumulated_submission_time=1020.65, global_step=12752, preemption_count=0, score=1020.65, test/loss=0.289664, test/num_examples=3581, test/ssim=0.738221, total_duration=1580.25, train/loss=0.271433, train/ssim=0.742353, validation/loss=0.28822, validation/num_examples=3554, validation/ssim=0.72081
I0315 15:23:22.408252 140503672678144 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.56109, loss=0.269237
I0315 15:23:22.411703 140545246360768 submission.py:265] 13000) loss = 0.269, grad_norm = 0.561
I0315 15:23:53.839218 140503664285440 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.13219, loss=0.318054
I0315 15:23:53.842501 140545246360768 submission.py:265] 13500) loss = 0.318, grad_norm = 0.132
I0315 15:24:25.297588 140503672678144 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.244486, loss=0.357418
I0315 15:24:25.301229 140545246360768 submission.py:265] 14000) loss = 0.357, grad_norm = 0.244
I0315 15:24:26.599224 140545246360768 spec.py:321] Evaluating on the training split.
I0315 15:24:28.583088 140545246360768 spec.py:333] Evaluating on the validation split.
I0315 15:24:30.674533 140545246360768 spec.py:349] Evaluating on the test split.
I0315 15:24:32.746684 140545246360768 submission_runner.py:469] Time since start: 1667.15s, 	Step: 14010, 	{'train/ssim': 0.7424618176051548, 'train/loss': 0.27162078448704313, 'validation/ssim': 0.7209571576480726, 'validation/loss': 0.28838001910259564, 'validation/num_examples': 3554, 'test/ssim': 0.7383891054785674, 'test/loss': 0.28977126457344315, 'test/num_examples': 3581, 'score': 1099.436635017395, 'total_duration': 1667.1463615894318, 'accumulated_submission_time': 1099.436635017395, 'accumulated_eval_time': 536.9013385772705, 'accumulated_logging_time': 5.2115631103515625}
I0315 15:24:32.757360 140503664285440 logging_writer.py:48] [14010] accumulated_eval_time=536.901, accumulated_logging_time=5.21156, accumulated_submission_time=1099.44, global_step=14010, preemption_count=0, score=1099.44, test/loss=0.289771, test/num_examples=3581, test/ssim=0.738389, total_duration=1667.15, train/loss=0.271621, train/ssim=0.742462, validation/loss=0.28838, validation/num_examples=3554, validation/ssim=0.720957
I0315 15:25:04.546445 140503672678144 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.276426, loss=0.311551
I0315 15:25:04.549843 140545246360768 submission.py:265] 14500) loss = 0.312, grad_norm = 0.276
I0315 15:25:36.085027 140503664285440 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.309679, loss=0.317713
I0315 15:25:36.088818 140545246360768 submission.py:265] 15000) loss = 0.318, grad_norm = 0.310
I0315 15:25:53.525319 140545246360768 spec.py:321] Evaluating on the training split.
I0315 15:25:55.508141 140545246360768 spec.py:333] Evaluating on the validation split.
I0315 15:25:57.625005 140545246360768 spec.py:349] Evaluating on the test split.
I0315 15:25:59.715979 140545246360768 submission_runner.py:469] Time since start: 1754.12s, 	Step: 15268, 	{'train/ssim': 0.7427738734654018, 'train/loss': 0.27121528557368685, 'validation/ssim': 0.7209691105092854, 'validation/loss': 0.2882328409120885, 'validation/num_examples': 3554, 'test/ssim': 0.7383505856647934, 'test/loss': 0.2896905093178407, 'test/num_examples': 3581, 'score': 1178.2382831573486, 'total_duration': 1754.1156792640686, 'accumulated_submission_time': 1178.2382831573486, 'accumulated_eval_time': 543.0921640396118, 'accumulated_logging_time': 5.230418920516968}
I0315 15:25:59.727526 140503672678144 logging_writer.py:48] [15268] accumulated_eval_time=543.092, accumulated_logging_time=5.23042, accumulated_submission_time=1178.24, global_step=15268, preemption_count=0, score=1178.24, test/loss=0.289691, test/num_examples=3581, test/ssim=0.738351, total_duration=1754.12, train/loss=0.271215, train/ssim=0.742774, validation/loss=0.288233, validation/num_examples=3554, validation/ssim=0.720969
I0315 15:26:15.221795 140503664285440 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.305737, loss=0.356833
I0315 15:26:15.225665 140545246360768 submission.py:265] 15500) loss = 0.357, grad_norm = 0.306
I0315 15:26:46.645143 140503672678144 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.165994, loss=0.23567
I0315 15:26:46.648659 140545246360768 submission.py:265] 16000) loss = 0.236, grad_norm = 0.166
I0315 15:27:18.130607 140503664285440 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.334893, loss=0.268896
I0315 15:27:18.134284 140545246360768 submission.py:265] 16500) loss = 0.269, grad_norm = 0.335
I0315 15:27:20.411234 140545246360768 spec.py:321] Evaluating on the training split.
I0315 15:27:22.381084 140545246360768 spec.py:333] Evaluating on the validation split.
I0315 15:27:24.471302 140545246360768 spec.py:349] Evaluating on the test split.
I0315 15:27:26.544020 140545246360768 submission_runner.py:469] Time since start: 1840.94s, 	Step: 16527, 	{'train/ssim': 0.7432101113455636, 'train/loss': 0.2710251808166504, 'validation/ssim': 0.7216244570378447, 'validation/loss': 0.2879347406751196, 'validation/num_examples': 3554, 'test/ssim': 0.7390300342737713, 'test/loss': 0.2893483647418668, 'test/num_examples': 3581, 'score': 1257.0393331050873, 'total_duration': 1840.94371342659, 'accumulated_submission_time': 1257.0393331050873, 'accumulated_eval_time': 549.2250361442566, 'accumulated_logging_time': 5.2502007484436035}
I0315 15:27:26.554692 140503672678144 logging_writer.py:48] [16527] accumulated_eval_time=549.225, accumulated_logging_time=5.2502, accumulated_submission_time=1257.04, global_step=16527, preemption_count=0, score=1257.04, test/loss=0.289348, test/num_examples=3581, test/ssim=0.73903, total_duration=1840.94, train/loss=0.271025, train/ssim=0.74321, validation/loss=0.287935, validation/num_examples=3554, validation/ssim=0.721624
I0315 15:27:57.202645 140503664285440 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.245844, loss=0.258189
I0315 15:27:57.205733 140545246360768 submission.py:265] 17000) loss = 0.258, grad_norm = 0.246
I0315 15:28:28.730880 140503672678144 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.430499, loss=0.268041
I0315 15:28:28.734223 140545246360768 submission.py:265] 17500) loss = 0.268, grad_norm = 0.430
I0315 15:28:47.258673 140545246360768 spec.py:321] Evaluating on the training split.
I0315 15:28:49.227387 140545246360768 spec.py:333] Evaluating on the validation split.
I0315 15:28:51.319470 140545246360768 spec.py:349] Evaluating on the test split.
I0315 15:28:53.401529 140545246360768 submission_runner.py:469] Time since start: 1927.80s, 	Step: 17785, 	{'train/ssim': 0.7428770746503558, 'train/loss': 0.2711802380425589, 'validation/ssim': 0.7211836437596723, 'validation/loss': 0.2881029050673537, 'validation/num_examples': 3554, 'test/ssim': 0.7386128612817648, 'test/loss': 0.2895262035635123, 'test/num_examples': 3581, 'score': 1335.8279929161072, 'total_duration': 1927.80122256279, 'accumulated_submission_time': 1335.8279929161072, 'accumulated_eval_time': 555.3681435585022, 'accumulated_logging_time': 5.269258260726929}
I0315 15:28:53.412622 140503664285440 logging_writer.py:48] [17785] accumulated_eval_time=555.368, accumulated_logging_time=5.26926, accumulated_submission_time=1335.83, global_step=17785, preemption_count=0, score=1335.83, test/loss=0.289526, test/num_examples=3581, test/ssim=0.738613, total_duration=1927.8, train/loss=0.27118, train/ssim=0.742877, validation/loss=0.288103, validation/num_examples=3554, validation/ssim=0.721184
I0315 15:29:07.927440 140503672678144 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.212294, loss=0.277696
I0315 15:29:07.930592 140545246360768 submission.py:265] 18000) loss = 0.278, grad_norm = 0.212
I0315 15:29:39.536791 140503664285440 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.249922, loss=0.27322
I0315 15:29:39.540712 140545246360768 submission.py:265] 18500) loss = 0.273, grad_norm = 0.250
I0315 15:30:11.105776 140503672678144 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.184979, loss=0.27575
I0315 15:30:11.109144 140545246360768 submission.py:265] 19000) loss = 0.276, grad_norm = 0.185
I0315 15:30:14.098524 140545246360768 spec.py:321] Evaluating on the training split.
I0315 15:30:16.090626 140545246360768 spec.py:333] Evaluating on the validation split.
I0315 15:30:18.197578 140545246360768 spec.py:349] Evaluating on the test split.
I0315 15:30:20.295172 140545246360768 submission_runner.py:469] Time since start: 2014.69s, 	Step: 19038, 	{'train/ssim': 0.7430406297956195, 'train/loss': 0.271111113684518, 'validation/ssim': 0.7213683635516319, 'validation/loss': 0.2880374391091024, 'validation/num_examples': 3554, 'test/ssim': 0.7387848028221865, 'test/loss': 0.28946204932499653, 'test/num_examples': 3581, 'score': 1414.650725364685, 'total_duration': 2014.6948294639587, 'accumulated_submission_time': 1414.650725364685, 'accumulated_eval_time': 561.5648484230042, 'accumulated_logging_time': 5.289461135864258}
I0315 15:30:20.307095 140503664285440 logging_writer.py:48] [19038] accumulated_eval_time=561.565, accumulated_logging_time=5.28946, accumulated_submission_time=1414.65, global_step=19038, preemption_count=0, score=1414.65, test/loss=0.289462, test/num_examples=3581, test/ssim=0.738785, total_duration=2014.69, train/loss=0.271111, train/ssim=0.743041, validation/loss=0.288037, validation/num_examples=3554, validation/ssim=0.721368
I0315 15:30:50.149749 140503672678144 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.371601, loss=0.260276
I0315 15:30:50.153685 140545246360768 submission.py:265] 19500) loss = 0.260, grad_norm = 0.372
I0315 15:31:21.736351 140503664285440 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.224827, loss=0.226122
I0315 15:31:21.740169 140545246360768 submission.py:265] 20000) loss = 0.226, grad_norm = 0.225
I0315 15:31:41.018400 140545246360768 spec.py:321] Evaluating on the training split.
I0315 15:31:42.989147 140545246360768 spec.py:333] Evaluating on the validation split.
I0315 15:31:45.072171 140545246360768 spec.py:349] Evaluating on the test split.
I0315 15:31:47.140043 140545246360768 submission_runner.py:469] Time since start: 2101.54s, 	Step: 20297, 	{'train/ssim': 0.7428778239658901, 'train/loss': 0.271324702671596, 'validation/ssim': 0.7211077362215109, 'validation/loss': 0.2882886552783835, 'validation/num_examples': 3554, 'test/ssim': 0.7385570927726194, 'test/loss': 0.28969814510393394, 'test/num_examples': 3581, 'score': 1493.5376181602478, 'total_duration': 2101.5397157669067, 'accumulated_submission_time': 1493.5376181602478, 'accumulated_eval_time': 567.6866729259491, 'accumulated_logging_time': 5.309480905532837}
I0315 15:31:47.151444 140503672678144 logging_writer.py:48] [20297] accumulated_eval_time=567.687, accumulated_logging_time=5.30948, accumulated_submission_time=1493.54, global_step=20297, preemption_count=0, score=1493.54, test/loss=0.289698, test/num_examples=3581, test/ssim=0.738557, total_duration=2101.54, train/loss=0.271325, train/ssim=0.742878, validation/loss=0.288289, validation/num_examples=3554, validation/ssim=0.721108
I0315 15:32:00.833856 140503664285440 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.131671, loss=0.322284
I0315 15:32:00.837104 140545246360768 submission.py:265] 20500) loss = 0.322, grad_norm = 0.132
I0315 15:32:32.373072 140503672678144 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.365402, loss=0.304514
I0315 15:32:32.376508 140545246360768 submission.py:265] 21000) loss = 0.305, grad_norm = 0.365
I0315 15:33:03.968524 140503664285440 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.310995, loss=0.359283
I0315 15:33:03.971904 140545246360768 submission.py:265] 21500) loss = 0.359, grad_norm = 0.311
I0315 15:33:07.838755 140545246360768 spec.py:321] Evaluating on the training split.
I0315 15:33:09.806551 140545246360768 spec.py:333] Evaluating on the validation split.
I0315 15:33:11.875231 140545246360768 spec.py:349] Evaluating on the test split.
I0315 15:33:13.933902 140545246360768 submission_runner.py:469] Time since start: 2188.33s, 	Step: 21552, 	{'train/ssim': 0.7425509861537388, 'train/loss': 0.2716785158429827, 'validation/ssim': 0.7207788264543472, 'validation/loss': 0.28859352193391247, 'validation/num_examples': 3554, 'test/ssim': 0.7382231634843619, 'test/loss': 0.2899849643190624, 'test/num_examples': 3581, 'score': 1572.3682615756989, 'total_duration': 2188.3335797786713, 'accumulated_submission_time': 1572.3682615756989, 'accumulated_eval_time': 573.7820599079132, 'accumulated_logging_time': 5.32880711555481}
I0315 15:33:13.945685 140503672678144 logging_writer.py:48] [21552] accumulated_eval_time=573.782, accumulated_logging_time=5.32881, accumulated_submission_time=1572.37, global_step=21552, preemption_count=0, score=1572.37, test/loss=0.289985, test/num_examples=3581, test/ssim=0.738223, total_duration=2188.33, train/loss=0.271679, train/ssim=0.742551, validation/loss=0.288594, validation/num_examples=3554, validation/ssim=0.720779
I0315 15:33:42.961698 140503664285440 logging_writer.py:48] [22000] global_step=22000, grad_norm=0.358204, loss=0.27693
I0315 15:33:42.964884 140545246360768 submission.py:265] 22000) loss = 0.277, grad_norm = 0.358
I0315 15:34:14.606494 140503672678144 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.221438, loss=0.198101
I0315 15:34:14.610207 140545246360768 submission.py:265] 22500) loss = 0.198, grad_norm = 0.221
I0315 15:34:34.689405 140545246360768 spec.py:321] Evaluating on the training split.
I0315 15:34:36.696801 140545246360768 spec.py:333] Evaluating on the validation split.
I0315 15:34:38.871803 140545246360768 spec.py:349] Evaluating on the test split.
I0315 15:34:41.009252 140545246360768 submission_runner.py:469] Time since start: 2275.41s, 	Step: 22809, 	{'train/ssim': 0.7432858603341239, 'train/loss': 0.2716595104762486, 'validation/ssim': 0.721596429639139, 'validation/loss': 0.28861347771656937, 'validation/num_examples': 3554, 'test/ssim': 0.7389617212589011, 'test/loss': 0.2900386534400307, 'test/num_examples': 3581, 'score': 1651.157345533371, 'total_duration': 2275.4089167118073, 'accumulated_submission_time': 1651.157345533371, 'accumulated_eval_time': 580.1020867824554, 'accumulated_logging_time': 5.348599433898926}
I0315 15:34:41.022153 140503664285440 logging_writer.py:48] [22809] accumulated_eval_time=580.102, accumulated_logging_time=5.3486, accumulated_submission_time=1651.16, global_step=22809, preemption_count=0, score=1651.16, test/loss=0.290039, test/num_examples=3581, test/ssim=0.738962, total_duration=2275.41, train/loss=0.27166, train/ssim=0.743286, validation/loss=0.288613, validation/num_examples=3554, validation/ssim=0.721596
I0315 15:34:53.938686 140503672678144 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.607996, loss=0.286525
I0315 15:34:53.942555 140545246360768 submission.py:265] 23000) loss = 0.287, grad_norm = 0.608
I0315 15:35:25.422074 140503664285440 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.192855, loss=0.326095
I0315 15:35:25.426151 140545246360768 submission.py:265] 23500) loss = 0.326, grad_norm = 0.193
I0315 15:35:56.918378 140503672678144 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.431252, loss=0.333476
I0315 15:35:56.921695 140545246360768 submission.py:265] 24000) loss = 0.333, grad_norm = 0.431
I0315 15:36:01.723537 140545246360768 spec.py:321] Evaluating on the training split.
I0315 15:36:03.826563 140545246360768 spec.py:333] Evaluating on the validation split.
I0315 15:36:06.014589 140545246360768 spec.py:349] Evaluating on the test split.
I0315 15:36:08.187242 140545246360768 submission_runner.py:469] Time since start: 2362.59s, 	Step: 24067, 	{'train/ssim': 0.742661544254848, 'train/loss': 0.2711512190955026, 'validation/ssim': 0.721174507377251, 'validation/loss': 0.2879981801425331, 'validation/num_examples': 3554, 'test/ssim': 0.7386318825703365, 'test/loss': 0.28938381660587126, 'test/num_examples': 3581, 'score': 1729.9371795654297, 'total_duration': 2362.586936712265, 'accumulated_submission_time': 1729.9371795654297, 'accumulated_eval_time': 586.565927028656, 'accumulated_logging_time': 5.370340347290039}
I0315 15:36:08.199544 140503664285440 logging_writer.py:48] [24067] accumulated_eval_time=586.566, accumulated_logging_time=5.37034, accumulated_submission_time=1729.94, global_step=24067, preemption_count=0, score=1729.94, test/loss=0.289384, test/num_examples=3581, test/ssim=0.738632, total_duration=2362.59, train/loss=0.271151, train/ssim=0.742662, validation/loss=0.287998, validation/num_examples=3554, validation/ssim=0.721175
I0315 15:36:36.330702 140503672678144 logging_writer.py:48] [24500] global_step=24500, grad_norm=0.325333, loss=0.311274
I0315 15:36:36.333952 140545246360768 submission.py:265] 24500) loss = 0.311, grad_norm = 0.325
I0315 15:37:07.770315 140503664285440 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.208013, loss=0.355312
I0315 15:37:07.773757 140545246360768 submission.py:265] 25000) loss = 0.355, grad_norm = 0.208
I0315 15:37:28.970428 140545246360768 spec.py:321] Evaluating on the training split.
I0315 15:37:30.948626 140545246360768 spec.py:333] Evaluating on the validation split.
I0315 15:37:33.014312 140545246360768 spec.py:349] Evaluating on the test split.
I0315 15:37:35.060845 140545246360768 submission_runner.py:469] Time since start: 2449.46s, 	Step: 25328, 	{'train/ssim': 0.7427447863987514, 'train/loss': 0.27132456643240793, 'validation/ssim': 0.7209882076093838, 'validation/loss': 0.28846811993308596, 'validation/num_examples': 3554, 'test/ssim': 0.7383222241735897, 'test/loss': 0.28991501506431516, 'test/num_examples': 3581, 'score': 1808.810625076294, 'total_duration': 2449.460305213928, 'accumulated_submission_time': 1808.810625076294, 'accumulated_eval_time': 592.6562523841858, 'accumulated_logging_time': 5.391005516052246}
I0315 15:37:35.072121 140503672678144 logging_writer.py:48] [25328] accumulated_eval_time=592.656, accumulated_logging_time=5.39101, accumulated_submission_time=1808.81, global_step=25328, preemption_count=0, score=1808.81, test/loss=0.289915, test/num_examples=3581, test/ssim=0.738322, total_duration=2449.46, train/loss=0.271325, train/ssim=0.742745, validation/loss=0.288468, validation/num_examples=3554, validation/ssim=0.720988
I0315 15:37:46.756226 140503664285440 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.167997, loss=0.264777
I0315 15:37:46.759514 140545246360768 submission.py:265] 25500) loss = 0.265, grad_norm = 0.168
I0315 15:38:18.148932 140503672678144 logging_writer.py:48] [26000] global_step=26000, grad_norm=0.297789, loss=0.228611
I0315 15:38:18.152259 140545246360768 submission.py:265] 26000) loss = 0.229, grad_norm = 0.298
I0315 15:38:49.624036 140503664285440 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.57673, loss=0.292601
I0315 15:38:49.627416 140545246360768 submission.py:265] 26500) loss = 0.293, grad_norm = 0.577
I0315 15:38:55.823966 140545246360768 spec.py:321] Evaluating on the training split.
I0315 15:38:57.790715 140545246360768 spec.py:333] Evaluating on the validation split.
I0315 15:38:59.856533 140545246360768 spec.py:349] Evaluating on the test split.
I0315 15:39:01.919497 140545246360768 submission_runner.py:469] Time since start: 2536.32s, 	Step: 26589, 	{'train/ssim': 0.7424412454877581, 'train/loss': 0.2718993765967233, 'validation/ssim': 0.7207701022395541, 'validation/loss': 0.28878785897052617, 'validation/num_examples': 3554, 'test/ssim': 0.7381282615714884, 'test/loss': 0.29021229939699106, 'test/num_examples': 3581, 'score': 1887.565128326416, 'total_duration': 2536.3191804885864, 'accumulated_submission_time': 1887.565128326416, 'accumulated_eval_time': 598.7518923282623, 'accumulated_logging_time': 5.410440921783447}
I0315 15:39:01.931151 140503672678144 logging_writer.py:48] [26589] accumulated_eval_time=598.752, accumulated_logging_time=5.41044, accumulated_submission_time=1887.57, global_step=26589, preemption_count=0, score=1887.57, test/loss=0.290212, test/num_examples=3581, test/ssim=0.738128, total_duration=2536.32, train/loss=0.271899, train/ssim=0.742441, validation/loss=0.288788, validation/num_examples=3554, validation/ssim=0.72077
I0315 15:39:28.799529 140503664285440 logging_writer.py:48] [27000] global_step=27000, grad_norm=0.182189, loss=0.26243
I0315 15:39:28.802740 140545246360768 submission.py:265] 27000) loss = 0.262, grad_norm = 0.182
I0315 15:40:00.208602 140503672678144 logging_writer.py:48] [27500] global_step=27500, grad_norm=0.230639, loss=0.284415
I0315 15:40:00.212146 140545246360768 submission.py:265] 27500) loss = 0.284, grad_norm = 0.231
I0315 15:40:22.661051 140545246360768 spec.py:321] Evaluating on the training split.
I0315 15:40:24.654415 140545246360768 spec.py:333] Evaluating on the validation split.
I0315 15:40:26.760692 140545246360768 spec.py:349] Evaluating on the test split.
I0315 15:40:28.842073 140545246360768 submission_runner.py:469] Time since start: 2623.24s, 	Step: 27848, 	{'train/ssim': 0.7426745550973075, 'train/loss': 0.27200726100376676, 'validation/ssim': 0.7210476971370287, 'validation/loss': 0.288901617235861, 'validation/num_examples': 3554, 'test/ssim': 0.7384872116945337, 'test/loss': 0.2903141894176731, 'test/num_examples': 3581, 'score': 1966.3269445896149, 'total_duration': 2623.2417855262756, 'accumulated_submission_time': 1966.3269445896149, 'accumulated_eval_time': 604.9330887794495, 'accumulated_logging_time': 5.43045711517334}
I0315 15:40:28.853224 140503664285440 logging_writer.py:48] [27848] accumulated_eval_time=604.933, accumulated_logging_time=5.43046, accumulated_submission_time=1966.33, global_step=27848, preemption_count=0, score=1966.33, test/loss=0.290314, test/num_examples=3581, test/ssim=0.738487, total_duration=2623.24, train/loss=0.272007, train/ssim=0.742675, validation/loss=0.288902, validation/num_examples=3554, validation/ssim=0.721048
I0315 15:40:39.294292 140503672678144 logging_writer.py:48] [28000] global_step=28000, grad_norm=0.210579, loss=0.210202
I0315 15:40:39.297653 140545246360768 submission.py:265] 28000) loss = 0.210, grad_norm = 0.211
I0315 15:41:10.684053 140503664285440 logging_writer.py:48] [28500] global_step=28500, grad_norm=0.323972, loss=0.312769
I0315 15:41:10.687323 140545246360768 submission.py:265] 28500) loss = 0.313, grad_norm = 0.324
I0315 15:41:42.052829 140503672678144 logging_writer.py:48] [29000] global_step=29000, grad_norm=0.232973, loss=0.249477
I0315 15:41:42.056586 140545246360768 submission.py:265] 29000) loss = 0.249, grad_norm = 0.233
I0315 15:41:49.542346 140545246360768 spec.py:321] Evaluating on the training split.
I0315 15:41:51.497936 140545246360768 spec.py:333] Evaluating on the validation split.
I0315 15:41:53.568242 140545246360768 spec.py:349] Evaluating on the test split.
I0315 15:41:55.630877 140545246360768 submission_runner.py:469] Time since start: 2710.03s, 	Step: 29109, 	{'train/ssim': 0.7419659750802177, 'train/loss': 0.2718642098563058, 'validation/ssim': 0.7202706924635973, 'validation/loss': 0.2888477950131014, 'validation/num_examples': 3554, 'test/ssim': 0.7376720915290771, 'test/loss': 0.2903261544217746, 'test/num_examples': 3581, 'score': 2045.11435008049, 'total_duration': 2710.0305581092834, 'accumulated_submission_time': 2045.11435008049, 'accumulated_eval_time': 611.0218231678009, 'accumulated_logging_time': 5.4498560428619385}
I0315 15:41:55.642747 140503664285440 logging_writer.py:48] [29109] accumulated_eval_time=611.022, accumulated_logging_time=5.44986, accumulated_submission_time=2045.11, global_step=29109, preemption_count=0, score=2045.11, test/loss=0.290326, test/num_examples=3581, test/ssim=0.737672, total_duration=2710.03, train/loss=0.271864, train/ssim=0.741966, validation/loss=0.288848, validation/num_examples=3554, validation/ssim=0.720271
I0315 15:42:21.037584 140503672678144 logging_writer.py:48] [29500] global_step=29500, grad_norm=0.146943, loss=0.271857
I0315 15:42:21.041109 140545246360768 submission.py:265] 29500) loss = 0.272, grad_norm = 0.147
I0315 15:42:52.321108 140503664285440 logging_writer.py:48] [30000] global_step=30000, grad_norm=0.2478, loss=0.29887
I0315 15:42:52.324538 140545246360768 submission.py:265] 30000) loss = 0.299, grad_norm = 0.248
I0315 15:43:16.402150 140545246360768 spec.py:321] Evaluating on the training split.
I0315 15:43:18.364942 140545246360768 spec.py:333] Evaluating on the validation split.
I0315 15:43:20.445292 140545246360768 spec.py:349] Evaluating on the test split.
I0315 15:43:22.516297 140545246360768 submission_runner.py:469] Time since start: 2796.92s, 	Step: 30375, 	{'train/ssim': 0.742027827671596, 'train/loss': 0.27302180017743793, 'validation/ssim': 0.7205737043648002, 'validation/loss': 0.28978643809132315, 'validation/num_examples': 3554, 'test/ssim': 0.7378925748525202, 'test/loss': 0.2911668749236421, 'test/num_examples': 3581, 'score': 2123.941555261612, 'total_duration': 2796.915995836258, 'accumulated_submission_time': 2123.941555261612, 'accumulated_eval_time': 617.1361072063446, 'accumulated_logging_time': 5.469552040100098}
I0315 15:43:22.527752 140503672678144 logging_writer.py:48] [30375] accumulated_eval_time=617.136, accumulated_logging_time=5.46955, accumulated_submission_time=2123.94, global_step=30375, preemption_count=0, score=2123.94, test/loss=0.291167, test/num_examples=3581, test/ssim=0.737893, total_duration=2796.92, train/loss=0.273022, train/ssim=0.742028, validation/loss=0.289786, validation/num_examples=3554, validation/ssim=0.720574
I0315 15:43:31.298674 140503664285440 logging_writer.py:48] [30500] global_step=30500, grad_norm=0.278789, loss=0.218579
I0315 15:43:31.301937 140545246360768 submission.py:265] 30500) loss = 0.219, grad_norm = 0.279
I0315 15:44:02.604161 140503672678144 logging_writer.py:48] [31000] global_step=31000, grad_norm=0.386177, loss=0.296623
I0315 15:44:02.608206 140545246360768 submission.py:265] 31000) loss = 0.297, grad_norm = 0.386
I0315 15:44:33.964539 140503664285440 logging_writer.py:48] [31500] global_step=31500, grad_norm=0.154013, loss=0.243141
I0315 15:44:33.968436 140545246360768 submission.py:265] 31500) loss = 0.243, grad_norm = 0.154
I0315 15:44:43.226077 140545246360768 spec.py:321] Evaluating on the training split.
I0315 15:44:45.231801 140545246360768 spec.py:333] Evaluating on the validation split.
I0315 15:44:47.367682 140545246360768 spec.py:349] Evaluating on the test split.
I0315 15:44:49.481499 140545246360768 submission_runner.py:469] Time since start: 2883.88s, 	Step: 31638, 	{'train/ssim': 0.742133481161935, 'train/loss': 0.2720463786806379, 'validation/ssim': 0.719998455745287, 'validation/loss': 0.28921239162739165, 'validation/num_examples': 3554, 'test/ssim': 0.7374405635864633, 'test/loss': 0.29066287895315557, 'test/num_examples': 3581, 'score': 2202.6985790729523, 'total_duration': 2883.8811655044556, 'accumulated_submission_time': 2202.6985790729523, 'accumulated_eval_time': 623.3917548656464, 'accumulated_logging_time': 5.489339351654053}
I0315 15:44:49.494760 140503672678144 logging_writer.py:48] [31638] accumulated_eval_time=623.392, accumulated_logging_time=5.48934, accumulated_submission_time=2202.7, global_step=31638, preemption_count=0, score=2202.7, test/loss=0.290663, test/num_examples=3581, test/ssim=0.737441, total_duration=2883.88, train/loss=0.272046, train/ssim=0.742133, validation/loss=0.289212, validation/num_examples=3554, validation/ssim=0.719998
I0315 15:45:13.050813 140503664285440 logging_writer.py:48] [32000] global_step=32000, grad_norm=0.58879, loss=0.245778
I0315 15:45:13.053938 140545246360768 submission.py:265] 32000) loss = 0.246, grad_norm = 0.589
I0315 15:45:44.330077 140503672678144 logging_writer.py:48] [32500] global_step=32500, grad_norm=0.0916055, loss=0.306603
I0315 15:45:44.333583 140545246360768 submission.py:265] 32500) loss = 0.307, grad_norm = 0.092
I0315 15:46:10.181526 140545246360768 spec.py:321] Evaluating on the training split.
I0315 15:46:12.264759 140545246360768 spec.py:333] Evaluating on the validation split.
I0315 15:46:14.505749 140545246360768 spec.py:349] Evaluating on the test split.
I0315 15:46:16.728113 140545246360768 submission_runner.py:469] Time since start: 2971.13s, 	Step: 32904, 	{'train/ssim': 0.7413515363420758, 'train/loss': 0.2725895472935268, 'validation/ssim': 0.7191506956290448, 'validation/loss': 0.2898207166990539, 'validation/num_examples': 3554, 'test/ssim': 0.7366467145394093, 'test/loss': 0.29123938080319745, 'test/num_examples': 3581, 'score': 2281.5105352401733, 'total_duration': 2971.127764940262, 'accumulated_submission_time': 2281.5105352401733, 'accumulated_eval_time': 629.9384922981262, 'accumulated_logging_time': 5.511341571807861}
I0315 15:46:16.740800 140503664285440 logging_writer.py:48] [32904] accumulated_eval_time=629.938, accumulated_logging_time=5.51134, accumulated_submission_time=2281.51, global_step=32904, preemption_count=0, score=2281.51, test/loss=0.291239, test/num_examples=3581, test/ssim=0.736647, total_duration=2971.13, train/loss=0.27259, train/ssim=0.741352, validation/loss=0.289821, validation/num_examples=3554, validation/ssim=0.719151
I0315 15:46:23.581203 140503672678144 logging_writer.py:48] [33000] global_step=33000, grad_norm=0.293727, loss=0.303882
I0315 15:46:23.584205 140545246360768 submission.py:265] 33000) loss = 0.304, grad_norm = 0.294
I0315 15:46:54.887068 140503664285440 logging_writer.py:48] [33500] global_step=33500, grad_norm=0.598332, loss=0.314359
I0315 15:46:54.891147 140545246360768 submission.py:265] 33500) loss = 0.314, grad_norm = 0.598
I0315 15:47:26.162850 140503672678144 logging_writer.py:48] [34000] global_step=34000, grad_norm=0.318071, loss=0.344727
I0315 15:47:26.166447 140545246360768 submission.py:265] 34000) loss = 0.345, grad_norm = 0.318
I0315 15:47:37.433000 140545246360768 spec.py:321] Evaluating on the training split.
I0315 15:47:39.390308 140545246360768 spec.py:333] Evaluating on the validation split.
I0315 15:47:41.532909 140545246360768 spec.py:349] Evaluating on the test split.
I0315 15:47:43.657979 140545246360768 submission_runner.py:469] Time since start: 3058.06s, 	Step: 34171, 	{'train/ssim': 0.7425227165222168, 'train/loss': 0.2716927187783377, 'validation/ssim': 0.7208308969646877, 'validation/loss': 0.28863260916397016, 'validation/num_examples': 3554, 'test/ssim': 0.7382098008586987, 'test/loss': 0.29002034800640536, 'test/num_examples': 3581, 'score': 2360.327497959137, 'total_duration': 3058.0576915740967, 'accumulated_submission_time': 2360.327497959137, 'accumulated_eval_time': 636.16370677948, 'accumulated_logging_time': 5.532307863235474}
I0315 15:47:43.670161 140503664285440 logging_writer.py:48] [34171] accumulated_eval_time=636.164, accumulated_logging_time=5.53231, accumulated_submission_time=2360.33, global_step=34171, preemption_count=0, score=2360.33, test/loss=0.29002, test/num_examples=3581, test/ssim=0.73821, total_duration=3058.06, train/loss=0.271693, train/ssim=0.742523, validation/loss=0.288633, validation/num_examples=3554, validation/ssim=0.720831
I0315 15:48:05.216010 140503672678144 logging_writer.py:48] [34500] global_step=34500, grad_norm=0.315817, loss=0.229742
I0315 15:48:05.219191 140545246360768 submission.py:265] 34500) loss = 0.230, grad_norm = 0.316
I0315 15:48:36.600157 140503664285440 logging_writer.py:48] [35000] global_step=35000, grad_norm=0.201688, loss=0.298948
I0315 15:48:36.603398 140545246360768 submission.py:265] 35000) loss = 0.299, grad_norm = 0.202
I0315 15:49:04.331859 140545246360768 spec.py:321] Evaluating on the training split.
I0315 15:49:06.298439 140545246360768 spec.py:333] Evaluating on the validation split.
I0315 15:49:08.441599 140545246360768 spec.py:349] Evaluating on the test split.
I0315 15:49:10.563810 140545246360768 submission_runner.py:469] Time since start: 3144.96s, 	Step: 35433, 	{'train/ssim': 0.7429672649928502, 'train/loss': 0.2712008271898542, 'validation/ssim': 0.7217992848067318, 'validation/loss': 0.28797899717417874, 'validation/num_examples': 3554, 'test/ssim': 0.7392272693556269, 'test/loss': 0.2893286616866797, 'test/num_examples': 3581, 'score': 2439.1304161548615, 'total_duration': 3144.963510990143, 'accumulated_submission_time': 2439.1304161548615, 'accumulated_eval_time': 642.3958656787872, 'accumulated_logging_time': 5.553293704986572}
I0315 15:49:10.575834 140503672678144 logging_writer.py:48] [35433] accumulated_eval_time=642.396, accumulated_logging_time=5.55329, accumulated_submission_time=2439.13, global_step=35433, preemption_count=0, score=2439.13, test/loss=0.289329, test/num_examples=3581, test/ssim=0.739227, total_duration=3144.96, train/loss=0.271201, train/ssim=0.742967, validation/loss=0.287979, validation/num_examples=3554, validation/ssim=0.721799
I0315 15:49:15.596530 140503664285440 logging_writer.py:48] [35500] global_step=35500, grad_norm=0.739485, loss=0.340811
I0315 15:49:15.599906 140545246360768 submission.py:265] 35500) loss = 0.341, grad_norm = 0.739
I0315 15:49:46.918315 140503672678144 logging_writer.py:48] [36000] global_step=36000, grad_norm=0.417782, loss=0.302161
I0315 15:49:46.921465 140545246360768 submission.py:265] 36000) loss = 0.302, grad_norm = 0.418
I0315 15:50:18.332825 140503664285440 logging_writer.py:48] [36500] global_step=36500, grad_norm=0.260351, loss=0.357267
I0315 15:50:18.336044 140545246360768 submission.py:265] 36500) loss = 0.357, grad_norm = 0.260
I0315 15:50:31.226257 140545246360768 spec.py:321] Evaluating on the training split.
I0315 15:50:33.231470 140545246360768 spec.py:333] Evaluating on the validation split.
I0315 15:50:35.400312 140545246360768 spec.py:349] Evaluating on the test split.
I0315 15:50:37.540429 140545246360768 submission_runner.py:469] Time since start: 3231.94s, 	Step: 36697, 	{'train/ssim': 0.7425704683576312, 'train/loss': 0.27088403701782227, 'validation/ssim': 0.7211271081000281, 'validation/loss': 0.28777131621060775, 'validation/num_examples': 3554, 'test/ssim': 0.7385534112328959, 'test/loss': 0.2892036256894024, 'test/num_examples': 3581, 'score': 2517.934933900833, 'total_duration': 3231.9401223659515, 'accumulated_submission_time': 2517.934933900833, 'accumulated_eval_time': 648.7101151943207, 'accumulated_logging_time': 5.573499917984009}
I0315 15:50:37.552428 140503672678144 logging_writer.py:48] [36697] accumulated_eval_time=648.71, accumulated_logging_time=5.5735, accumulated_submission_time=2517.93, global_step=36697, preemption_count=0, score=2517.93, test/loss=0.289204, test/num_examples=3581, test/ssim=0.738553, total_duration=3231.94, train/loss=0.270884, train/ssim=0.74257, validation/loss=0.287771, validation/num_examples=3554, validation/ssim=0.721127
I0315 15:50:57.413813 140503664285440 logging_writer.py:48] [37000] global_step=37000, grad_norm=0.399391, loss=0.228662
I0315 15:50:57.416903 140545246360768 submission.py:265] 37000) loss = 0.229, grad_norm = 0.399
I0315 15:51:28.794210 140503672678144 logging_writer.py:48] [37500] global_step=37500, grad_norm=0.366019, loss=0.256351
I0315 15:51:28.797832 140545246360768 submission.py:265] 37500) loss = 0.256, grad_norm = 0.366
I0315 15:51:58.317387 140545246360768 spec.py:321] Evaluating on the training split.
I0315 15:52:00.321608 140545246360768 spec.py:333] Evaluating on the validation split.
I0315 15:52:02.416984 140545246360768 spec.py:349] Evaluating on the test split.
I0315 15:52:04.495395 140545246360768 submission_runner.py:469] Time since start: 3318.90s, 	Step: 37961, 	{'train/ssim': 0.7427347728184291, 'train/loss': 0.27118442739759174, 'validation/ssim': 0.7212580400165307, 'validation/loss': 0.28813343981912987, 'validation/num_examples': 3554, 'test/ssim': 0.7387291024897026, 'test/loss': 0.28946075396842713, 'test/num_examples': 3581, 'score': 2596.840656042099, 'total_duration': 3318.8950736522675, 'accumulated_submission_time': 2596.840656042099, 'accumulated_eval_time': 654.8883030414581, 'accumulated_logging_time': 5.593853950500488}
I0315 15:52:04.509258 140503664285440 logging_writer.py:48] [37961] accumulated_eval_time=654.888, accumulated_logging_time=5.59385, accumulated_submission_time=2596.84, global_step=37961, preemption_count=0, score=2596.84, test/loss=0.289461, test/num_examples=3581, test/ssim=0.738729, total_duration=3318.9, train/loss=0.271184, train/ssim=0.742735, validation/loss=0.288133, validation/num_examples=3554, validation/ssim=0.721258
I0315 15:52:07.889378 140503672678144 logging_writer.py:48] [38000] global_step=38000, grad_norm=0.537277, loss=0.260694
I0315 15:52:07.892977 140545246360768 submission.py:265] 38000) loss = 0.261, grad_norm = 0.537
I0315 15:52:39.235710 140503664285440 logging_writer.py:48] [38500] global_step=38500, grad_norm=0.202809, loss=0.31767
I0315 15:52:39.239125 140545246360768 submission.py:265] 38500) loss = 0.318, grad_norm = 0.203
I0315 15:53:10.571413 140503672678144 logging_writer.py:48] [39000] global_step=39000, grad_norm=0.506697, loss=0.310048
I0315 15:53:10.574617 140545246360768 submission.py:265] 39000) loss = 0.310, grad_norm = 0.507
I0315 15:53:25.185369 140545246360768 spec.py:321] Evaluating on the training split.
I0315 15:53:27.253380 140545246360768 spec.py:333] Evaluating on the validation split.
I0315 15:53:29.483649 140545246360768 spec.py:349] Evaluating on the test split.
I0315 15:53:31.683146 140545246360768 submission_runner.py:469] Time since start: 3406.08s, 	Step: 39224, 	{'train/ssim': 0.7432337488446917, 'train/loss': 0.27096329416547504, 'validation/ssim': 0.7212225936005205, 'validation/loss': 0.288318297000299, 'validation/num_examples': 3554, 'test/ssim': 0.7386795380567579, 'test/loss': 0.28967172664758445, 'test/num_examples': 3581, 'score': 2675.601131916046, 'total_duration': 3406.0828239917755, 'accumulated_submission_time': 2675.601131916046, 'accumulated_eval_time': 661.3862612247467, 'accumulated_logging_time': 5.616423606872559}
I0315 15:53:31.695493 140503664285440 logging_writer.py:48] [39224] accumulated_eval_time=661.386, accumulated_logging_time=5.61642, accumulated_submission_time=2675.6, global_step=39224, preemption_count=0, score=2675.6, test/loss=0.289672, test/num_examples=3581, test/ssim=0.73868, total_duration=3406.08, train/loss=0.270963, train/ssim=0.743234, validation/loss=0.288318, validation/num_examples=3554, validation/ssim=0.721223
I0315 15:53:49.878799 140503672678144 logging_writer.py:48] [39500] global_step=39500, grad_norm=0.223929, loss=0.246063
I0315 15:53:49.882243 140545246360768 submission.py:265] 39500) loss = 0.246, grad_norm = 0.224
I0315 15:54:21.144512 140503664285440 logging_writer.py:48] [40000] global_step=40000, grad_norm=0.123328, loss=0.291572
I0315 15:54:21.147996 140545246360768 submission.py:265] 40000) loss = 0.292, grad_norm = 0.123
I0315 15:54:52.380511 140545246360768 spec.py:321] Evaluating on the training split.
I0315 15:54:54.363467 140545246360768 spec.py:333] Evaluating on the validation split.
I0315 15:54:56.540701 140545246360768 spec.py:349] Evaluating on the test split.
I0315 15:54:58.666083 140545246360768 submission_runner.py:469] Time since start: 3493.07s, 	Step: 40491, 	{'train/ssim': 0.7435193061828613, 'train/loss': 0.2706965548651559, 'validation/ssim': 0.7213930936093135, 'validation/loss': 0.28800956627325724, 'validation/num_examples': 3554, 'test/ssim': 0.7388467754075329, 'test/loss': 0.2894405395882784, 'test/num_examples': 3581, 'score': 2754.441791534424, 'total_duration': 3493.065782546997, 'accumulated_submission_time': 2754.441791534424, 'accumulated_eval_time': 667.6719498634338, 'accumulated_logging_time': 5.637182235717773}
I0315 15:54:58.678484 140503672678144 logging_writer.py:48] [40491] accumulated_eval_time=667.672, accumulated_logging_time=5.63718, accumulated_submission_time=2754.44, global_step=40491, preemption_count=0, score=2754.44, test/loss=0.289441, test/num_examples=3581, test/ssim=0.738847, total_duration=3493.07, train/loss=0.270697, train/ssim=0.743519, validation/loss=0.28801, validation/num_examples=3554, validation/ssim=0.721393
I0315 15:55:00.102386 140503664285440 logging_writer.py:48] [40500] global_step=40500, grad_norm=0.392327, loss=0.293133
I0315 15:55:00.105545 140545246360768 submission.py:265] 40500) loss = 0.293, grad_norm = 0.392
I0315 15:55:31.452318 140503672678144 logging_writer.py:48] [41000] global_step=41000, grad_norm=0.36603, loss=0.244211
I0315 15:55:31.455891 140545246360768 submission.py:265] 41000) loss = 0.244, grad_norm = 0.366
I0315 15:56:02.785140 140503664285440 logging_writer.py:48] [41500] global_step=41500, grad_norm=0.417278, loss=0.263874
I0315 15:56:02.788417 140545246360768 submission.py:265] 41500) loss = 0.264, grad_norm = 0.417
I0315 15:56:19.343807 140545246360768 spec.py:321] Evaluating on the training split.
I0315 15:56:21.352464 140545246360768 spec.py:333] Evaluating on the validation split.
I0315 15:56:23.532048 140545246360768 spec.py:349] Evaluating on the test split.
I0315 15:56:25.692750 140545246360768 submission_runner.py:469] Time since start: 3580.09s, 	Step: 41755, 	{'train/ssim': 0.7433501652308873, 'train/loss': 0.27086734771728516, 'validation/ssim': 0.7217968118009637, 'validation/loss': 0.2877876311792171, 'validation/num_examples': 3554, 'test/ssim': 0.7392416546312134, 'test/loss': 0.28918153645106115, 'test/num_examples': 3581, 'score': 2833.3289432525635, 'total_duration': 3580.092440366745, 'accumulated_submission_time': 2833.3289432525635, 'accumulated_eval_time': 674.0210111141205, 'accumulated_logging_time': 5.6578850746154785}
I0315 15:56:25.706090 140503672678144 logging_writer.py:48] [41755] accumulated_eval_time=674.021, accumulated_logging_time=5.65789, accumulated_submission_time=2833.33, global_step=41755, preemption_count=0, score=2833.33, test/loss=0.289182, test/num_examples=3581, test/ssim=0.739242, total_duration=3580.09, train/loss=0.270867, train/ssim=0.74335, validation/loss=0.287788, validation/num_examples=3554, validation/ssim=0.721797
I0315 15:56:41.891519 140503664285440 logging_writer.py:48] [42000] global_step=42000, grad_norm=0.677388, loss=0.230779
I0315 15:56:41.894673 140545246360768 submission.py:265] 42000) loss = 0.231, grad_norm = 0.677
I0315 15:57:13.209226 140503672678144 logging_writer.py:48] [42500] global_step=42500, grad_norm=0.371191, loss=0.272152
I0315 15:57:13.212526 140545246360768 submission.py:265] 42500) loss = 0.272, grad_norm = 0.371
I0315 15:57:44.562931 140503664285440 logging_writer.py:48] [43000] global_step=43000, grad_norm=0.333376, loss=0.331172
I0315 15:57:44.566712 140545246360768 submission.py:265] 43000) loss = 0.331, grad_norm = 0.333
I0315 15:57:46.433159 140545246360768 spec.py:321] Evaluating on the training split.
I0315 15:57:48.402659 140545246360768 spec.py:333] Evaluating on the validation split.
I0315 15:57:50.535382 140545246360768 spec.py:349] Evaluating on the test split.
I0315 15:57:52.649426 140545246360768 submission_runner.py:469] Time since start: 3667.05s, 	Step: 43020, 	{'train/ssim': 0.7442851066589355, 'train/loss': 0.27096937383924213, 'validation/ssim': 0.7225210590180079, 'validation/loss': 0.28814212968662073, 'validation/num_examples': 3554, 'test/ssim': 0.7398849014329097, 'test/loss': 0.2895574625628316, 'test/num_examples': 3581, 'score': 2912.218130350113, 'total_duration': 3667.0491468906403, 'accumulated_submission_time': 2912.218130350113, 'accumulated_eval_time': 680.2374379634857, 'accumulated_logging_time': 5.6802918910980225}
I0315 15:57:52.661834 140503672678144 logging_writer.py:48] [43020] accumulated_eval_time=680.237, accumulated_logging_time=5.68029, accumulated_submission_time=2912.22, global_step=43020, preemption_count=0, score=2912.22, test/loss=0.289557, test/num_examples=3581, test/ssim=0.739885, total_duration=3667.05, train/loss=0.270969, train/ssim=0.744285, validation/loss=0.288142, validation/num_examples=3554, validation/ssim=0.722521
I0315 15:58:23.588027 140503664285440 logging_writer.py:48] [43500] global_step=43500, grad_norm=0.557414, loss=0.239185
I0315 15:58:23.591327 140545246360768 submission.py:265] 43500) loss = 0.239, grad_norm = 0.557
I0315 15:58:54.918268 140503672678144 logging_writer.py:48] [44000] global_step=44000, grad_norm=0.290466, loss=0.377211
I0315 15:58:54.921693 140545246360768 submission.py:265] 44000) loss = 0.377, grad_norm = 0.290
I0315 15:59:13.316806 140545246360768 spec.py:321] Evaluating on the training split.
I0315 15:59:15.289851 140545246360768 spec.py:333] Evaluating on the validation split.
I0315 15:59:17.446144 140545246360768 spec.py:349] Evaluating on the test split.
I0315 15:59:19.574800 140545246360768 submission_runner.py:469] Time since start: 3753.97s, 	Step: 44285, 	{'train/ssim': 0.7445543834141323, 'train/loss': 0.27083820956093924, 'validation/ssim': 0.7226071333576604, 'validation/loss': 0.2882185867816193, 'validation/num_examples': 3554, 'test/ssim': 0.7399574414007959, 'test/loss': 0.2896559096621056, 'test/num_examples': 3581, 'score': 2991.0106043815613, 'total_duration': 3753.97447180748, 'accumulated_submission_time': 2991.0106043815613, 'accumulated_eval_time': 686.4955124855042, 'accumulated_logging_time': 5.701359272003174}
I0315 15:59:19.588071 140503664285440 logging_writer.py:48] [44285] accumulated_eval_time=686.496, accumulated_logging_time=5.70136, accumulated_submission_time=2991.01, global_step=44285, preemption_count=0, score=2991.01, test/loss=0.289656, test/num_examples=3581, test/ssim=0.739957, total_duration=3753.97, train/loss=0.270838, train/ssim=0.744554, validation/loss=0.288219, validation/num_examples=3554, validation/ssim=0.722607
I0315 15:59:33.971620 140503672678144 logging_writer.py:48] [44500] global_step=44500, grad_norm=0.102814, loss=0.236049
I0315 15:59:33.975077 140545246360768 submission.py:265] 44500) loss = 0.236, grad_norm = 0.103
I0315 16:00:05.321126 140503664285440 logging_writer.py:48] [45000] global_step=45000, grad_norm=0.304364, loss=0.259136
I0315 16:00:05.324738 140545246360768 submission.py:265] 45000) loss = 0.259, grad_norm = 0.304
I0315 16:00:37.056517 140503672678144 logging_writer.py:48] [45500] global_step=45500, grad_norm=0.146073, loss=0.292592
I0315 16:00:37.059925 140545246360768 submission.py:265] 45500) loss = 0.293, grad_norm = 0.146
I0315 16:00:40.297288 140545246360768 spec.py:321] Evaluating on the training split.
I0315 16:00:42.280929 140545246360768 spec.py:333] Evaluating on the validation split.
I0315 16:00:44.427720 140545246360768 spec.py:349] Evaluating on the test split.
I0315 16:00:46.557244 140545246360768 submission_runner.py:469] Time since start: 3840.96s, 	Step: 45542, 	{'train/ssim': 0.7434676034109933, 'train/loss': 0.27159575053623747, 'validation/ssim': 0.7216228770619373, 'validation/loss': 0.2885452983214336, 'validation/num_examples': 3554, 'test/ssim': 0.7389958095896747, 'test/loss': 0.2899753173214535, 'test/num_examples': 3581, 'score': 3069.872999191284, 'total_duration': 3840.9569351673126, 'accumulated_submission_time': 3069.872999191284, 'accumulated_eval_time': 692.7555980682373, 'accumulated_logging_time': 5.722690582275391}
I0315 16:00:46.569621 140503664285440 logging_writer.py:48] [45542] accumulated_eval_time=692.756, accumulated_logging_time=5.72269, accumulated_submission_time=3069.87, global_step=45542, preemption_count=0, score=3069.87, test/loss=0.289975, test/num_examples=3581, test/ssim=0.738996, total_duration=3840.96, train/loss=0.271596, train/ssim=0.743468, validation/loss=0.288545, validation/num_examples=3554, validation/ssim=0.721623
I0315 16:01:16.098392 140503672678144 logging_writer.py:48] [46000] global_step=46000, grad_norm=0.34884, loss=0.322898
I0315 16:01:16.101626 140545246360768 submission.py:265] 46000) loss = 0.323, grad_norm = 0.349
I0315 16:01:47.439509 140503664285440 logging_writer.py:48] [46500] global_step=46500, grad_norm=0.300863, loss=0.289233
I0315 16:01:47.442715 140545246360768 submission.py:265] 46500) loss = 0.289, grad_norm = 0.301
I0315 16:02:07.327076 140545246360768 spec.py:321] Evaluating on the training split.
I0315 16:02:09.341755 140545246360768 spec.py:333] Evaluating on the validation split.
I0315 16:02:11.453468 140545246360768 spec.py:349] Evaluating on the test split.
I0315 16:02:13.547028 140545246360768 submission_runner.py:469] Time since start: 3927.95s, 	Step: 46808, 	{'train/ssim': 0.7450368063790458, 'train/loss': 0.2699670451028006, 'validation/ssim': 0.7231808706958709, 'validation/loss': 0.2871504200262908, 'validation/num_examples': 3554, 'test/ssim': 0.7404844469945546, 'test/loss': 0.28856143562595993, 'test/num_examples': 3581, 'score': 3148.68093252182, 'total_duration': 3927.9467158317566, 'accumulated_submission_time': 3148.68093252182, 'accumulated_eval_time': 698.9757814407349, 'accumulated_logging_time': 5.74425196647644}
I0315 16:02:13.561313 140503672678144 logging_writer.py:48] [46808] accumulated_eval_time=698.976, accumulated_logging_time=5.74425, accumulated_submission_time=3148.68, global_step=46808, preemption_count=0, score=3148.68, test/loss=0.288561, test/num_examples=3581, test/ssim=0.740484, total_duration=3927.95, train/loss=0.269967, train/ssim=0.745037, validation/loss=0.28715, validation/num_examples=3554, validation/ssim=0.723181
I0315 16:02:26.504488 140503664285440 logging_writer.py:48] [47000] global_step=47000, grad_norm=0.547573, loss=0.262783
I0315 16:02:26.508045 140545246360768 submission.py:265] 47000) loss = 0.263, grad_norm = 0.548
I0315 16:02:57.843324 140503672678144 logging_writer.py:48] [47500] global_step=47500, grad_norm=0.267897, loss=0.261758
I0315 16:02:57.846660 140545246360768 submission.py:265] 47500) loss = 0.262, grad_norm = 0.268
I0315 16:03:29.113642 140503664285440 logging_writer.py:48] [48000] global_step=48000, grad_norm=0.216888, loss=0.301789
I0315 16:03:29.116894 140545246360768 submission.py:265] 48000) loss = 0.302, grad_norm = 0.217
I0315 16:03:34.251461 140545246360768 spec.py:321] Evaluating on the training split.
I0315 16:03:36.311743 140545246360768 spec.py:333] Evaluating on the validation split.
I0315 16:03:38.540401 140545246360768 spec.py:349] Evaluating on the test split.
I0315 16:03:40.751075 140545246360768 submission_runner.py:469] Time since start: 4015.15s, 	Step: 48073, 	{'train/ssim': 0.7455564907618931, 'train/loss': 0.2694148676736014, 'validation/ssim': 0.7235590344945836, 'validation/loss': 0.28677232492218274, 'validation/num_examples': 3554, 'test/ssim': 0.7409184596219631, 'test/loss': 0.2881758284282498, 'test/num_examples': 3581, 'score': 3227.4309837818146, 'total_duration': 4015.150763273239, 'accumulated_submission_time': 3227.4309837818146, 'accumulated_eval_time': 705.4755623340607, 'accumulated_logging_time': 5.767374277114868}
I0315 16:03:40.763971 140503672678144 logging_writer.py:48] [48073] accumulated_eval_time=705.476, accumulated_logging_time=5.76737, accumulated_submission_time=3227.43, global_step=48073, preemption_count=0, score=3227.43, test/loss=0.288176, test/num_examples=3581, test/ssim=0.740918, total_duration=4015.15, train/loss=0.269415, train/ssim=0.745556, validation/loss=0.286772, validation/num_examples=3554, validation/ssim=0.723559
I0315 16:04:08.507371 140503664285440 logging_writer.py:48] [48500] global_step=48500, grad_norm=0.274212, loss=0.270326
I0315 16:04:08.510929 140545246360768 submission.py:265] 48500) loss = 0.270, grad_norm = 0.274
I0315 16:04:39.750342 140503672678144 logging_writer.py:48] [49000] global_step=49000, grad_norm=0.319423, loss=0.315404
I0315 16:04:39.753464 140545246360768 submission.py:265] 49000) loss = 0.315, grad_norm = 0.319
I0315 16:05:01.497618 140545246360768 spec.py:321] Evaluating on the training split.
I0315 16:05:03.475268 140545246360768 spec.py:333] Evaluating on the validation split.
I0315 16:05:05.627101 140545246360768 spec.py:349] Evaluating on the test split.
I0315 16:05:07.758072 140545246360768 submission_runner.py:469] Time since start: 4102.16s, 	Step: 49337, 	{'train/ssim': 0.7451102393014091, 'train/loss': 0.2697339228221348, 'validation/ssim': 0.7232287508353263, 'validation/loss': 0.28702764559409294, 'validation/num_examples': 3554, 'test/ssim': 0.7406065513953853, 'test/loss': 0.28840285671120147, 'test/num_examples': 3581, 'score': 3306.212931871414, 'total_duration': 4102.1577689647675, 'accumulated_submission_time': 3306.212931871414, 'accumulated_eval_time': 711.7361650466919, 'accumulated_logging_time': 5.788323640823364}
I0315 16:05:07.770885 140503664285440 logging_writer.py:48] [49337] accumulated_eval_time=711.736, accumulated_logging_time=5.78832, accumulated_submission_time=3306.21, global_step=49337, preemption_count=0, score=3306.21, test/loss=0.288403, test/num_examples=3581, test/ssim=0.740607, total_duration=4102.16, train/loss=0.269734, train/ssim=0.74511, validation/loss=0.287028, validation/num_examples=3554, validation/ssim=0.723229
I0315 16:05:18.885710 140503672678144 logging_writer.py:48] [49500] global_step=49500, grad_norm=0.728342, loss=0.288873
I0315 16:05:18.888755 140545246360768 submission.py:265] 49500) loss = 0.289, grad_norm = 0.728
I0315 16:05:50.260616 140503664285440 logging_writer.py:48] [50000] global_step=50000, grad_norm=0.212728, loss=0.207612
I0315 16:05:50.263920 140545246360768 submission.py:265] 50000) loss = 0.208, grad_norm = 0.213
I0315 16:06:21.579305 140503672678144 logging_writer.py:48] [50500] global_step=50500, grad_norm=0.0992196, loss=0.285181
I0315 16:06:21.582699 140545246360768 submission.py:265] 50500) loss = 0.285, grad_norm = 0.099
I0315 16:06:28.429763 140545246360768 spec.py:321] Evaluating on the training split.
I0315 16:06:30.404923 140545246360768 spec.py:333] Evaluating on the validation split.
I0315 16:06:32.573284 140545246360768 spec.py:349] Evaluating on the test split.
I0315 16:06:34.718406 140545246360768 submission_runner.py:469] Time since start: 4189.12s, 	Step: 50600, 	{'train/ssim': 0.7457528795514788, 'train/loss': 0.2693368707384382, 'validation/ssim': 0.723742998645892, 'validation/loss': 0.2867471998705244, 'validation/num_examples': 3554, 'test/ssim': 0.7411206716001117, 'test/loss': 0.2881500235618542, 'test/num_examples': 3581, 'score': 3384.9936215877533, 'total_duration': 4189.118111610413, 'accumulated_submission_time': 3384.9936215877533, 'accumulated_eval_time': 718.0249469280243, 'accumulated_logging_time': 5.809622049331665}
I0315 16:06:34.731191 140503664285440 logging_writer.py:48] [50600] accumulated_eval_time=718.025, accumulated_logging_time=5.80962, accumulated_submission_time=3384.99, global_step=50600, preemption_count=0, score=3384.99, test/loss=0.28815, test/num_examples=3581, test/ssim=0.741121, total_duration=4189.12, train/loss=0.269337, train/ssim=0.745753, validation/loss=0.286747, validation/num_examples=3554, validation/ssim=0.723743
I0315 16:06:35.419484 140503672678144 logging_writer.py:48] [50600] global_step=50600, preemption_count=0, score=3384.99
I0315 16:06:36.199056 140545246360768 submission_runner.py:646] Tuning trial 4/5
I0315 16:06:36.199265 140545246360768 submission_runner.py:647] Hyperparameters: Hyperparameters(learning_rate=0.0012, one_minus_beta1=0.016610699316537858, one_minus_beta2=0.005888216674053163, epsilon=1e-08, one_minus_momentum=0.5, use_momentum=True, weight_decay=0.00040349948255455174, max_preconditioner_dim=1024, precondition_frequency=100, start_preconditioning_step=-1, inv_root_override=2, exponent_multiplier=1.0, grafting_type='ADAM', grafting_epsilon=1e-08, use_normalized_grafting=False, communication_dtype='FP32', communicate_params=True, use_cosine_decay=True, warmup_factor=0.02, label_smoothing=0.1, dropout_rate=0.1, use_nadam=True, step_hint_factor=1.0)
I0315 16:06:36.200688 140545246360768 submission_runner.py:648] Metrics: {'eval_results': [(1, {'train/ssim': 0.2516805785042899, 'train/loss': 0.9709701538085938, 'validation/ssim': 0.24513789892330826, 'validation/loss': 0.978066568369267, 'validation/num_examples': 3554, 'test/ssim': 0.2659577361966804, 'test/loss': 0.9746548488332519, 'test/num_examples': 3581, 'score': 77.44168257713318, 'total_duration': 530.415036201477, 'accumulated_submission_time': 77.44168257713318, 'accumulated_eval_time': 452.1838643550873, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (349, {'train/ssim': 0.6947334834507534, 'train/loss': 0.32060285976954866, 'validation/ssim': 0.674170773688098, 'validation/loss': 0.33712845836117755, 'validation/num_examples': 3554, 'test/ssim': 0.6926874940004538, 'test/loss': 0.3384853300186749, 'test/num_examples': 3581, 'score': 158.6087293624878, 'total_duration': 619.8601343631744, 'accumulated_submission_time': 158.6087293624878, 'accumulated_eval_time': 458.5959515571594, 'accumulated_logging_time': 0.016819000244140625, 'global_step': 349, 'preemption_count': 0}), (735, {'train/ssim': 0.7143486567905971, 'train/loss': 0.29745735440935406, 'validation/ssim': 0.6939609336531022, 'validation/loss': 0.3132754246975239, 'validation/num_examples': 3554, 'test/ssim': 0.7117130777017593, 'test/loss': 0.3154841606525761, 'test/num_examples': 3581, 'score': 237.71569681167603, 'total_duration': 707.75639128685, 'accumulated_submission_time': 237.71569681167603, 'accumulated_eval_time': 465.54317593574524, 'accumulated_logging_time': 0.05019569396972656, 'global_step': 735, 'preemption_count': 0}), (1532, {'train/ssim': 0.7302229745047433, 'train/loss': 0.28371844972882954, 'validation/ssim': 0.7102234882790518, 'validation/loss': 0.29945980623725027, 'validation/num_examples': 3554, 'test/ssim': 0.7275227682778903, 'test/loss': 0.3014453929484606, 'test/num_examples': 3581, 'score': 316.54540371894836, 'total_duration': 795.8120169639587, 'accumulated_submission_time': 316.54540371894836, 'accumulated_eval_time': 472.80690693855286, 'accumulated_logging_time': 0.06904387474060059, 'global_step': 1532, 'preemption_count': 0}), (2789, {'train/ssim': 0.7356462478637695, 'train/loss': 0.2776554993220738, 'validation/ssim': 0.7143358220930641, 'validation/loss': 0.29420302030942247, 'validation/num_examples': 3554, 'test/ssim': 0.7317172010262496, 'test/loss': 0.2959561490439996, 'test/num_examples': 3581, 'score': 395.26741456985474, 'total_duration': 883.2369468212128, 'accumulated_submission_time': 395.26741456985474, 'accumulated_eval_time': 479.46471905708313, 'accumulated_logging_time': 0.1469886302947998, 'global_step': 2789, 'preemption_count': 0}), (4047, {'train/ssim': 0.7369195393153599, 'train/loss': 0.2755146196910313, 'validation/ssim': 0.7155651807382527, 'validation/loss': 0.29200544555870145, 'validation/num_examples': 3554, 'test/ssim': 0.7329405630410499, 'test/loss': 0.29360275886405685, 'test/num_examples': 3581, 'score': 474.0532274246216, 'total_duration': 971.0118517875671, 'accumulated_submission_time': 474.0532274246216, 'accumulated_eval_time': 486.49737071990967, 'accumulated_logging_time': 0.1667482852935791, 'global_step': 4047, 'preemption_count': 0}), (5304, {'train/ssim': 0.7382236889430455, 'train/loss': 0.274721486227853, 'validation/ssim': 0.716497023050612, 'validation/loss': 0.29171802733275887, 'validation/num_examples': 3554, 'test/ssim': 0.733952509228393, 'test/loss': 0.29333601767575396, 'test/num_examples': 3581, 'score': 552.8274099826813, 'total_duration': 1058.2217135429382, 'accumulated_submission_time': 552.8274099826813, 'accumulated_eval_time': 493.0220489501953, 'accumulated_logging_time': 0.18542146682739258, 'global_step': 5304, 'preemption_count': 0}), (6553, {'train/ssim': 0.7401924133300781, 'train/loss': 0.2733228547232492, 'validation/ssim': 0.7191440322523917, 'validation/loss': 0.28983424953617404, 'validation/num_examples': 3554, 'test/ssim': 0.7365468357302429, 'test/loss': 0.2913551447745043, 'test/num_examples': 3581, 'score': 631.6341784000397, 'total_duration': 1145.467333316803, 'accumulated_submission_time': 631.6341784000397, 'accumulated_eval_time': 499.5948967933655, 'accumulated_logging_time': 0.21764516830444336, 'global_step': 6553, 'preemption_count': 0}), (7720, {'train/ssim': 0.7415270124162946, 'train/loss': 0.2727466481072562, 'validation/ssim': 0.7203075814663056, 'validation/loss': 0.2894570474619267, 'validation/num_examples': 3554, 'test/ssim': 0.7376472070476124, 'test/loss': 0.2909349038327283, 'test/num_examples': 3581, 'score': 705.268030166626, 'total_duration': 1232.3860878944397, 'accumulated_submission_time': 705.268030166626, 'accumulated_eval_time': 505.77395606040955, 'accumulated_logging_time': 5.113474130630493, 'global_step': 7720, 'preemption_count': 0}), (8978, {'train/ssim': 0.7397443226405552, 'train/loss': 0.2728936331612723, 'validation/ssim': 0.7184170372511607, 'validation/loss': 0.28952890201841236, 'validation/num_examples': 3554, 'test/ssim': 0.7359562894879224, 'test/loss': 0.290887077904653, 'test/num_examples': 3581, 'score': 784.1282937526703, 'total_duration': 1319.4266967773438, 'accumulated_submission_time': 784.1282937526703, 'accumulated_eval_time': 512.0526666641235, 'accumulated_logging_time': 5.132548570632935, 'global_step': 8978, 'preemption_count': 0}), (10234, {'train/ssim': 0.7415193830217633, 'train/loss': 0.2722433124269758, 'validation/ssim': 0.7204449019810425, 'validation/loss': 0.288885233572647, 'validation/num_examples': 3554, 'test/ssim': 0.7378794167568417, 'test/loss': 0.2903023607668947, 'test/num_examples': 3581, 'score': 862.9247846603394, 'total_duration': 1406.6638991832733, 'accumulated_submission_time': 862.9247846603394, 'accumulated_eval_time': 518.5461127758026, 'accumulated_logging_time': 5.153214931488037, 'global_step': 10234, 'preemption_count': 0}), (11496, {'train/ssim': 0.7418069839477539, 'train/loss': 0.27199402877262663, 'validation/ssim': 0.7202097603492543, 'validation/loss': 0.2887606215597742, 'validation/num_examples': 3554, 'test/ssim': 0.7376744095355696, 'test/loss': 0.2901994821846202, 'test/num_examples': 3581, 'score': 941.7866492271423, 'total_duration': 1493.461401462555, 'accumulated_submission_time': 941.7866492271423, 'accumulated_eval_time': 524.6551775932312, 'accumulated_logging_time': 5.172511577606201, 'global_step': 11496, 'preemption_count': 0}), (12752, {'train/ssim': 0.7423528262547084, 'train/loss': 0.2714330809456961, 'validation/ssim': 0.7208098764156584, 'validation/loss': 0.2882201667575267, 'validation/num_examples': 3554, 'test/ssim': 0.7382207091245462, 'test/loss': 0.2896644999214605, 'test/num_examples': 3581, 'score': 1020.6520760059357, 'total_duration': 1580.2503561973572, 'accumulated_submission_time': 1020.6520760059357, 'accumulated_eval_time': 530.7537386417389, 'accumulated_logging_time': 5.19197940826416, 'global_step': 12752, 'preemption_count': 0}), (14010, {'train/ssim': 0.7424618176051548, 'train/loss': 0.27162078448704313, 'validation/ssim': 0.7209571576480726, 'validation/loss': 0.28838001910259564, 'validation/num_examples': 3554, 'test/ssim': 0.7383891054785674, 'test/loss': 0.28977126457344315, 'test/num_examples': 3581, 'score': 1099.436635017395, 'total_duration': 1667.1463615894318, 'accumulated_submission_time': 1099.436635017395, 'accumulated_eval_time': 536.9013385772705, 'accumulated_logging_time': 5.2115631103515625, 'global_step': 14010, 'preemption_count': 0}), (15268, {'train/ssim': 0.7427738734654018, 'train/loss': 0.27121528557368685, 'validation/ssim': 0.7209691105092854, 'validation/loss': 0.2882328409120885, 'validation/num_examples': 3554, 'test/ssim': 0.7383505856647934, 'test/loss': 0.2896905093178407, 'test/num_examples': 3581, 'score': 1178.2382831573486, 'total_duration': 1754.1156792640686, 'accumulated_submission_time': 1178.2382831573486, 'accumulated_eval_time': 543.0921640396118, 'accumulated_logging_time': 5.230418920516968, 'global_step': 15268, 'preemption_count': 0}), (16527, {'train/ssim': 0.7432101113455636, 'train/loss': 0.2710251808166504, 'validation/ssim': 0.7216244570378447, 'validation/loss': 0.2879347406751196, 'validation/num_examples': 3554, 'test/ssim': 0.7390300342737713, 'test/loss': 0.2893483647418668, 'test/num_examples': 3581, 'score': 1257.0393331050873, 'total_duration': 1840.94371342659, 'accumulated_submission_time': 1257.0393331050873, 'accumulated_eval_time': 549.2250361442566, 'accumulated_logging_time': 5.2502007484436035, 'global_step': 16527, 'preemption_count': 0}), (17785, {'train/ssim': 0.7428770746503558, 'train/loss': 0.2711802380425589, 'validation/ssim': 0.7211836437596723, 'validation/loss': 0.2881029050673537, 'validation/num_examples': 3554, 'test/ssim': 0.7386128612817648, 'test/loss': 0.2895262035635123, 'test/num_examples': 3581, 'score': 1335.8279929161072, 'total_duration': 1927.80122256279, 'accumulated_submission_time': 1335.8279929161072, 'accumulated_eval_time': 555.3681435585022, 'accumulated_logging_time': 5.269258260726929, 'global_step': 17785, 'preemption_count': 0}), (19038, {'train/ssim': 0.7430406297956195, 'train/loss': 0.271111113684518, 'validation/ssim': 0.7213683635516319, 'validation/loss': 0.2880374391091024, 'validation/num_examples': 3554, 'test/ssim': 0.7387848028221865, 'test/loss': 0.28946204932499653, 'test/num_examples': 3581, 'score': 1414.650725364685, 'total_duration': 2014.6948294639587, 'accumulated_submission_time': 1414.650725364685, 'accumulated_eval_time': 561.5648484230042, 'accumulated_logging_time': 5.289461135864258, 'global_step': 19038, 'preemption_count': 0}), (20297, {'train/ssim': 0.7428778239658901, 'train/loss': 0.271324702671596, 'validation/ssim': 0.7211077362215109, 'validation/loss': 0.2882886552783835, 'validation/num_examples': 3554, 'test/ssim': 0.7385570927726194, 'test/loss': 0.28969814510393394, 'test/num_examples': 3581, 'score': 1493.5376181602478, 'total_duration': 2101.5397157669067, 'accumulated_submission_time': 1493.5376181602478, 'accumulated_eval_time': 567.6866729259491, 'accumulated_logging_time': 5.309480905532837, 'global_step': 20297, 'preemption_count': 0}), (21552, {'train/ssim': 0.7425509861537388, 'train/loss': 0.2716785158429827, 'validation/ssim': 0.7207788264543472, 'validation/loss': 0.28859352193391247, 'validation/num_examples': 3554, 'test/ssim': 0.7382231634843619, 'test/loss': 0.2899849643190624, 'test/num_examples': 3581, 'score': 1572.3682615756989, 'total_duration': 2188.3335797786713, 'accumulated_submission_time': 1572.3682615756989, 'accumulated_eval_time': 573.7820599079132, 'accumulated_logging_time': 5.32880711555481, 'global_step': 21552, 'preemption_count': 0}), (22809, {'train/ssim': 0.7432858603341239, 'train/loss': 0.2716595104762486, 'validation/ssim': 0.721596429639139, 'validation/loss': 0.28861347771656937, 'validation/num_examples': 3554, 'test/ssim': 0.7389617212589011, 'test/loss': 0.2900386534400307, 'test/num_examples': 3581, 'score': 1651.157345533371, 'total_duration': 2275.4089167118073, 'accumulated_submission_time': 1651.157345533371, 'accumulated_eval_time': 580.1020867824554, 'accumulated_logging_time': 5.348599433898926, 'global_step': 22809, 'preemption_count': 0}), (24067, {'train/ssim': 0.742661544254848, 'train/loss': 0.2711512190955026, 'validation/ssim': 0.721174507377251, 'validation/loss': 0.2879981801425331, 'validation/num_examples': 3554, 'test/ssim': 0.7386318825703365, 'test/loss': 0.28938381660587126, 'test/num_examples': 3581, 'score': 1729.9371795654297, 'total_duration': 2362.586936712265, 'accumulated_submission_time': 1729.9371795654297, 'accumulated_eval_time': 586.565927028656, 'accumulated_logging_time': 5.370340347290039, 'global_step': 24067, 'preemption_count': 0}), (25328, {'train/ssim': 0.7427447863987514, 'train/loss': 0.27132456643240793, 'validation/ssim': 0.7209882076093838, 'validation/loss': 0.28846811993308596, 'validation/num_examples': 3554, 'test/ssim': 0.7383222241735897, 'test/loss': 0.28991501506431516, 'test/num_examples': 3581, 'score': 1808.810625076294, 'total_duration': 2449.460305213928, 'accumulated_submission_time': 1808.810625076294, 'accumulated_eval_time': 592.6562523841858, 'accumulated_logging_time': 5.391005516052246, 'global_step': 25328, 'preemption_count': 0}), (26589, {'train/ssim': 0.7424412454877581, 'train/loss': 0.2718993765967233, 'validation/ssim': 0.7207701022395541, 'validation/loss': 0.28878785897052617, 'validation/num_examples': 3554, 'test/ssim': 0.7381282615714884, 'test/loss': 0.29021229939699106, 'test/num_examples': 3581, 'score': 1887.565128326416, 'total_duration': 2536.3191804885864, 'accumulated_submission_time': 1887.565128326416, 'accumulated_eval_time': 598.7518923282623, 'accumulated_logging_time': 5.410440921783447, 'global_step': 26589, 'preemption_count': 0}), (27848, {'train/ssim': 0.7426745550973075, 'train/loss': 0.27200726100376676, 'validation/ssim': 0.7210476971370287, 'validation/loss': 0.288901617235861, 'validation/num_examples': 3554, 'test/ssim': 0.7384872116945337, 'test/loss': 0.2903141894176731, 'test/num_examples': 3581, 'score': 1966.3269445896149, 'total_duration': 2623.2417855262756, 'accumulated_submission_time': 1966.3269445896149, 'accumulated_eval_time': 604.9330887794495, 'accumulated_logging_time': 5.43045711517334, 'global_step': 27848, 'preemption_count': 0}), (29109, {'train/ssim': 0.7419659750802177, 'train/loss': 0.2718642098563058, 'validation/ssim': 0.7202706924635973, 'validation/loss': 0.2888477950131014, 'validation/num_examples': 3554, 'test/ssim': 0.7376720915290771, 'test/loss': 0.2903261544217746, 'test/num_examples': 3581, 'score': 2045.11435008049, 'total_duration': 2710.0305581092834, 'accumulated_submission_time': 2045.11435008049, 'accumulated_eval_time': 611.0218231678009, 'accumulated_logging_time': 5.4498560428619385, 'global_step': 29109, 'preemption_count': 0}), (30375, {'train/ssim': 0.742027827671596, 'train/loss': 0.27302180017743793, 'validation/ssim': 0.7205737043648002, 'validation/loss': 0.28978643809132315, 'validation/num_examples': 3554, 'test/ssim': 0.7378925748525202, 'test/loss': 0.2911668749236421, 'test/num_examples': 3581, 'score': 2123.941555261612, 'total_duration': 2796.915995836258, 'accumulated_submission_time': 2123.941555261612, 'accumulated_eval_time': 617.1361072063446, 'accumulated_logging_time': 5.469552040100098, 'global_step': 30375, 'preemption_count': 0}), (31638, {'train/ssim': 0.742133481161935, 'train/loss': 0.2720463786806379, 'validation/ssim': 0.719998455745287, 'validation/loss': 0.28921239162739165, 'validation/num_examples': 3554, 'test/ssim': 0.7374405635864633, 'test/loss': 0.29066287895315557, 'test/num_examples': 3581, 'score': 2202.6985790729523, 'total_duration': 2883.8811655044556, 'accumulated_submission_time': 2202.6985790729523, 'accumulated_eval_time': 623.3917548656464, 'accumulated_logging_time': 5.489339351654053, 'global_step': 31638, 'preemption_count': 0}), (32904, {'train/ssim': 0.7413515363420758, 'train/loss': 0.2725895472935268, 'validation/ssim': 0.7191506956290448, 'validation/loss': 0.2898207166990539, 'validation/num_examples': 3554, 'test/ssim': 0.7366467145394093, 'test/loss': 0.29123938080319745, 'test/num_examples': 3581, 'score': 2281.5105352401733, 'total_duration': 2971.127764940262, 'accumulated_submission_time': 2281.5105352401733, 'accumulated_eval_time': 629.9384922981262, 'accumulated_logging_time': 5.511341571807861, 'global_step': 32904, 'preemption_count': 0}), (34171, {'train/ssim': 0.7425227165222168, 'train/loss': 0.2716927187783377, 'validation/ssim': 0.7208308969646877, 'validation/loss': 0.28863260916397016, 'validation/num_examples': 3554, 'test/ssim': 0.7382098008586987, 'test/loss': 0.29002034800640536, 'test/num_examples': 3581, 'score': 2360.327497959137, 'total_duration': 3058.0576915740967, 'accumulated_submission_time': 2360.327497959137, 'accumulated_eval_time': 636.16370677948, 'accumulated_logging_time': 5.532307863235474, 'global_step': 34171, 'preemption_count': 0}), (35433, {'train/ssim': 0.7429672649928502, 'train/loss': 0.2712008271898542, 'validation/ssim': 0.7217992848067318, 'validation/loss': 0.28797899717417874, 'validation/num_examples': 3554, 'test/ssim': 0.7392272693556269, 'test/loss': 0.2893286616866797, 'test/num_examples': 3581, 'score': 2439.1304161548615, 'total_duration': 3144.963510990143, 'accumulated_submission_time': 2439.1304161548615, 'accumulated_eval_time': 642.3958656787872, 'accumulated_logging_time': 5.553293704986572, 'global_step': 35433, 'preemption_count': 0}), (36697, {'train/ssim': 0.7425704683576312, 'train/loss': 0.27088403701782227, 'validation/ssim': 0.7211271081000281, 'validation/loss': 0.28777131621060775, 'validation/num_examples': 3554, 'test/ssim': 0.7385534112328959, 'test/loss': 0.2892036256894024, 'test/num_examples': 3581, 'score': 2517.934933900833, 'total_duration': 3231.9401223659515, 'accumulated_submission_time': 2517.934933900833, 'accumulated_eval_time': 648.7101151943207, 'accumulated_logging_time': 5.573499917984009, 'global_step': 36697, 'preemption_count': 0}), (37961, {'train/ssim': 0.7427347728184291, 'train/loss': 0.27118442739759174, 'validation/ssim': 0.7212580400165307, 'validation/loss': 0.28813343981912987, 'validation/num_examples': 3554, 'test/ssim': 0.7387291024897026, 'test/loss': 0.28946075396842713, 'test/num_examples': 3581, 'score': 2596.840656042099, 'total_duration': 3318.8950736522675, 'accumulated_submission_time': 2596.840656042099, 'accumulated_eval_time': 654.8883030414581, 'accumulated_logging_time': 5.593853950500488, 'global_step': 37961, 'preemption_count': 0}), (39224, {'train/ssim': 0.7432337488446917, 'train/loss': 0.27096329416547504, 'validation/ssim': 0.7212225936005205, 'validation/loss': 0.288318297000299, 'validation/num_examples': 3554, 'test/ssim': 0.7386795380567579, 'test/loss': 0.28967172664758445, 'test/num_examples': 3581, 'score': 2675.601131916046, 'total_duration': 3406.0828239917755, 'accumulated_submission_time': 2675.601131916046, 'accumulated_eval_time': 661.3862612247467, 'accumulated_logging_time': 5.616423606872559, 'global_step': 39224, 'preemption_count': 0}), (40491, {'train/ssim': 0.7435193061828613, 'train/loss': 0.2706965548651559, 'validation/ssim': 0.7213930936093135, 'validation/loss': 0.28800956627325724, 'validation/num_examples': 3554, 'test/ssim': 0.7388467754075329, 'test/loss': 0.2894405395882784, 'test/num_examples': 3581, 'score': 2754.441791534424, 'total_duration': 3493.065782546997, 'accumulated_submission_time': 2754.441791534424, 'accumulated_eval_time': 667.6719498634338, 'accumulated_logging_time': 5.637182235717773, 'global_step': 40491, 'preemption_count': 0}), (41755, {'train/ssim': 0.7433501652308873, 'train/loss': 0.27086734771728516, 'validation/ssim': 0.7217968118009637, 'validation/loss': 0.2877876311792171, 'validation/num_examples': 3554, 'test/ssim': 0.7392416546312134, 'test/loss': 0.28918153645106115, 'test/num_examples': 3581, 'score': 2833.3289432525635, 'total_duration': 3580.092440366745, 'accumulated_submission_time': 2833.3289432525635, 'accumulated_eval_time': 674.0210111141205, 'accumulated_logging_time': 5.6578850746154785, 'global_step': 41755, 'preemption_count': 0}), (43020, {'train/ssim': 0.7442851066589355, 'train/loss': 0.27096937383924213, 'validation/ssim': 0.7225210590180079, 'validation/loss': 0.28814212968662073, 'validation/num_examples': 3554, 'test/ssim': 0.7398849014329097, 'test/loss': 0.2895574625628316, 'test/num_examples': 3581, 'score': 2912.218130350113, 'total_duration': 3667.0491468906403, 'accumulated_submission_time': 2912.218130350113, 'accumulated_eval_time': 680.2374379634857, 'accumulated_logging_time': 5.6802918910980225, 'global_step': 43020, 'preemption_count': 0}), (44285, {'train/ssim': 0.7445543834141323, 'train/loss': 0.27083820956093924, 'validation/ssim': 0.7226071333576604, 'validation/loss': 0.2882185867816193, 'validation/num_examples': 3554, 'test/ssim': 0.7399574414007959, 'test/loss': 0.2896559096621056, 'test/num_examples': 3581, 'score': 2991.0106043815613, 'total_duration': 3753.97447180748, 'accumulated_submission_time': 2991.0106043815613, 'accumulated_eval_time': 686.4955124855042, 'accumulated_logging_time': 5.701359272003174, 'global_step': 44285, 'preemption_count': 0}), (45542, {'train/ssim': 0.7434676034109933, 'train/loss': 0.27159575053623747, 'validation/ssim': 0.7216228770619373, 'validation/loss': 0.2885452983214336, 'validation/num_examples': 3554, 'test/ssim': 0.7389958095896747, 'test/loss': 0.2899753173214535, 'test/num_examples': 3581, 'score': 3069.872999191284, 'total_duration': 3840.9569351673126, 'accumulated_submission_time': 3069.872999191284, 'accumulated_eval_time': 692.7555980682373, 'accumulated_logging_time': 5.722690582275391, 'global_step': 45542, 'preemption_count': 0}), (46808, {'train/ssim': 0.7450368063790458, 'train/loss': 0.2699670451028006, 'validation/ssim': 0.7231808706958709, 'validation/loss': 0.2871504200262908, 'validation/num_examples': 3554, 'test/ssim': 0.7404844469945546, 'test/loss': 0.28856143562595993, 'test/num_examples': 3581, 'score': 3148.68093252182, 'total_duration': 3927.9467158317566, 'accumulated_submission_time': 3148.68093252182, 'accumulated_eval_time': 698.9757814407349, 'accumulated_logging_time': 5.74425196647644, 'global_step': 46808, 'preemption_count': 0}), (48073, {'train/ssim': 0.7455564907618931, 'train/loss': 0.2694148676736014, 'validation/ssim': 0.7235590344945836, 'validation/loss': 0.28677232492218274, 'validation/num_examples': 3554, 'test/ssim': 0.7409184596219631, 'test/loss': 0.2881758284282498, 'test/num_examples': 3581, 'score': 3227.4309837818146, 'total_duration': 4015.150763273239, 'accumulated_submission_time': 3227.4309837818146, 'accumulated_eval_time': 705.4755623340607, 'accumulated_logging_time': 5.767374277114868, 'global_step': 48073, 'preemption_count': 0}), (49337, {'train/ssim': 0.7451102393014091, 'train/loss': 0.2697339228221348, 'validation/ssim': 0.7232287508353263, 'validation/loss': 0.28702764559409294, 'validation/num_examples': 3554, 'test/ssim': 0.7406065513953853, 'test/loss': 0.28840285671120147, 'test/num_examples': 3581, 'score': 3306.212931871414, 'total_duration': 4102.1577689647675, 'accumulated_submission_time': 3306.212931871414, 'accumulated_eval_time': 711.7361650466919, 'accumulated_logging_time': 5.788323640823364, 'global_step': 49337, 'preemption_count': 0}), (50600, {'train/ssim': 0.7457528795514788, 'train/loss': 0.2693368707384382, 'validation/ssim': 0.723742998645892, 'validation/loss': 0.2867471998705244, 'validation/num_examples': 3554, 'test/ssim': 0.7411206716001117, 'test/loss': 0.2881500235618542, 'test/num_examples': 3581, 'score': 3384.9936215877533, 'total_duration': 4189.118111610413, 'accumulated_submission_time': 3384.9936215877533, 'accumulated_eval_time': 718.0249469280243, 'accumulated_logging_time': 5.809622049331665, 'global_step': 50600, 'preemption_count': 0})], 'global_step': 50600}
I0315 16:06:36.200819 140545246360768 submission_runner.py:649] Timing: 3384.9936215877533
I0315 16:06:36.200868 140545246360768 submission_runner.py:651] Total number of evals: 43
I0315 16:06:36.200907 140545246360768 submission_runner.py:652] ====================
I0315 16:06:36.201055 140545246360768 submission_runner.py:750] Final fastmri score: 3
