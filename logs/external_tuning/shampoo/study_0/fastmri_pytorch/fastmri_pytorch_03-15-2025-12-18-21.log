torchrun --redirects 1:0,2:0,3:0,4:0,5:0,6:0,7:0 --standalone --nnodes=1 --nproc_per_node=8 submission_runner.py --framework=pytorch --workload=fastmri --submission_path=submissions_algorithms/leaderboard/external_tuning/shampoo_submission/submission.py --data_dir=/data/fastmri --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=submissions/rolling_leaderboard/external_tuning/shampoo/study_0 --overwrite=True --save_checkpoints=False --rng_seed=-250266841 --torch_compile=true --tuning_ruleset=external --tuning_search_space=submissions_algorithms/leaderboard/external_tuning/shampoo_submission/tuning_search_space.json --num_tuning_trials=5 --hparam_start_index=0 --hparam_end_index=1 2>&1 | tee -a /logs/fastmri_pytorch_03-15-2025-12-18-21.log
W0315 12:18:40.924000 9 site-packages/torch/distributed/run.py:793] 
W0315 12:18:40.924000 9 site-packages/torch/distributed/run.py:793] *****************************************
W0315 12:18:40.924000 9 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0315 12:18:40.924000 9 site-packages/torch/distributed/run.py:793] *****************************************
2025-03-15 12:18:54.693140: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-03-15 12:18:54.693155: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-03-15 12:18:54.693159: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-03-15 12:18:54.693229: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-03-15 12:18:54.693273: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-03-15 12:18:54.693292: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-03-15 12:18:54.693343: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-03-15 12:18:54.693473: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1742041135.408899      50 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1742041135.408929      46 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1742041135.408952      49 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1742041135.408930      51 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1742041135.408891      48 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1742041135.408930      45 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1742041135.408942      47 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1742041135.408918      44 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1742041135.652557      50 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
E0000 00:00:1742041135.652673      51 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
E0000 00:00:1742041135.652726      46 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
E0000 00:00:1742041135.652778      48 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
E0000 00:00:1742041135.652779      49 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
E0000 00:00:1742041135.652810      44 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
E0000 00:00:1742041135.652817      45 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
E0000 00:00:1742041135.652873      47 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
[rank4]:[W315 12:19:39.971741921 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 4]  using GPU 4 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank7]:[W315 12:19:39.971781249 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 7]  using GPU 7 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank2]:[W315 12:19:39.971789573 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank6]:[W315 12:19:39.971790662 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 6]  using GPU 6 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank5]:[W315 12:19:39.971808206 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 5]  using GPU 5 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank3]:[W315 12:19:39.971815499 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank1]:[W315 12:19:39.971821320 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank0]:[W315 12:19:39.097980154 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
I0315 12:19:41.615019 140277216793792 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_0/fastmri_pytorch.
I0315 12:19:41.615019 140508957750464 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_0/fastmri_pytorch.
I0315 12:19:41.615020 139882590454976 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_0/fastmri_pytorch.
I0315 12:19:41.615019 139667941848256 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_0/fastmri_pytorch.
I0315 12:19:41.615022 140631626413248 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_0/fastmri_pytorch.
I0315 12:19:41.615061 140718987674816 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_0/fastmri_pytorch.
I0315 12:19:41.615087 139715604698304 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_0/fastmri_pytorch.
I0315 12:19:41.615172 139680868943040 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_0/fastmri_pytorch.
I0315 12:19:43.551147 140277216793792 submission_runner.py:606] Using RNG seed -250266841
I0315 12:19:43.551958 139882590454976 logger_utils.py:97] Saving hparams to /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_0/fastmri_pytorch/trial_1/hparams.json.
I0315 12:19:43.551960 140508957750464 logger_utils.py:97] Saving hparams to /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_0/fastmri_pytorch/trial_1/hparams.json.
I0315 12:19:43.551959 140631626413248 logger_utils.py:97] Saving hparams to /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_0/fastmri_pytorch/trial_1/hparams.json.
I0315 12:19:43.551969 140718987674816 logger_utils.py:97] Saving hparams to /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_0/fastmri_pytorch/trial_1/hparams.json.
I0315 12:19:43.551972 139667941848256 logger_utils.py:97] Saving hparams to /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_0/fastmri_pytorch/trial_1/hparams.json.
I0315 12:19:43.551983 139680868943040 logger_utils.py:97] Saving hparams to /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_0/fastmri_pytorch/trial_1/hparams.json.
I0315 12:19:43.552375 140277216793792 submission_runner.py:615] --- Tuning run 1/5 ---
I0315 12:19:43.552489 140277216793792 submission_runner.py:620] Creating tuning directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_0/fastmri_pytorch/trial_1.
I0315 12:19:43.552256 139715604698304 logger_utils.py:97] Saving hparams to /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_0/fastmri_pytorch/trial_1/hparams.json.
I0315 12:19:43.552711 140277216793792 logger_utils.py:97] Saving hparams to /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_0/fastmri_pytorch/trial_1/hparams.json.
I0315 12:19:43.900492 140277216793792 submission_runner.py:218] Initializing dataset.
I0315 12:19:43.900684 140277216793792 submission_runner.py:229] Initializing model.
I0315 12:19:44.760040 140277216793792 submission_runner.py:268] Performing `torch.compile`.
I0315 12:19:46.835141 140277216793792 submission_runner.py:272] Initializing optimizer.
W0315 12:19:46.865161 139882590454976 distributed_shampoo.py:341] start_preconditioning_step set to -1. Setting start_preconditioning_step equal to precondition frequency 100 by default.
W0315 12:19:46.865171 139667941848256 distributed_shampoo.py:341] start_preconditioning_step set to -1. Setting start_preconditioning_step equal to precondition frequency 100 by default.
W0315 12:19:46.865179 140718987674816 distributed_shampoo.py:341] start_preconditioning_step set to -1. Setting start_preconditioning_step equal to precondition frequency 100 by default.
W0315 12:19:46.865222 140277216793792 distributed_shampoo.py:341] start_preconditioning_step set to -1. Setting start_preconditioning_step equal to precondition frequency 100 by default.
W0315 12:19:46.865195 140508957750464 distributed_shampoo.py:341] start_preconditioning_step set to -1. Setting start_preconditioning_step equal to precondition frequency 100 by default.
W0315 12:19:46.865207 140631626413248 distributed_shampoo.py:341] start_preconditioning_step set to -1. Setting start_preconditioning_step equal to precondition frequency 100 by default.
W0315 12:19:46.865202 139680868943040 distributed_shampoo.py:341] start_preconditioning_step set to -1. Setting start_preconditioning_step equal to precondition frequency 100 by default.
W0315 12:19:46.865353 140277216793792 distributed_shampoo.py:359] Nesterov flag is enabled but momentum parameter is zero! Continuing without using momentum or Nesterov acceleration...
W0315 12:19:46.865233 139715604698304 distributed_shampoo.py:341] start_preconditioning_step set to -1. Setting start_preconditioning_step equal to precondition frequency 100 by default.
W0315 12:19:46.865358 140718987674816 distributed_shampoo.py:359] Nesterov flag is enabled but momentum parameter is zero! Continuing without using momentum or Nesterov acceleration...
W0315 12:19:46.865361 139667941848256 distributed_shampoo.py:359] Nesterov flag is enabled but momentum parameter is zero! Continuing without using momentum or Nesterov acceleration...
W0315 12:19:46.865370 139882590454976 distributed_shampoo.py:359] Nesterov flag is enabled but momentum parameter is zero! Continuing without using momentum or Nesterov acceleration...
W0315 12:19:46.865376 140508957750464 distributed_shampoo.py:359] Nesterov flag is enabled but momentum parameter is zero! Continuing without using momentum or Nesterov acceleration...
W0315 12:19:46.865387 140631626413248 distributed_shampoo.py:359] Nesterov flag is enabled but momentum parameter is zero! Continuing without using momentum or Nesterov acceleration...
W0315 12:19:46.865401 139680868943040 distributed_shampoo.py:359] Nesterov flag is enabled but momentum parameter is zero! Continuing without using momentum or Nesterov acceleration...
W0315 12:19:46.865416 139715604698304 distributed_shampoo.py:359] Nesterov flag is enabled but momentum parameter is zero! Continuing without using momentum or Nesterov acceleration...
I0315 12:19:46.943754 140718987674816 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 0.block_0 ([torch.Size([0])]) for Parameter 0 (torch.Size([32, 1, 3, 3])), Block block_0 (torch.Size([288])).
I0315 12:19:46.943732 139715604698304 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 0.block_0 ([torch.Size([0])]) for Parameter 0 (torch.Size([32, 1, 3, 3])), Block block_0 (torch.Size([288])).
I0315 12:19:46.943727 139667941848256 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 0.block_0 ([torch.Size([0])]) for Parameter 0 (torch.Size([32, 1, 3, 3])), Block block_0 (torch.Size([288])).
I0315 12:19:46.943812 139882590454976 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 0.block_0 ([torch.Size([0])]) for Parameter 0 (torch.Size([32, 1, 3, 3])), Block block_0 (torch.Size([288])).
I0315 12:19:46.943798 140631626413248 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 0.block_0 ([torch.Size([0])]) for Parameter 0 (torch.Size([32, 1, 3, 3])), Block block_0 (torch.Size([288])).
I0315 12:19:46.943861 140277216793792 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 0.block_0 ([torch.Size([0])]) for Parameter 0 (torch.Size([32, 1, 3, 3])), Block block_0 (torch.Size([288])).
I0315 12:19:46.943873 140508957750464 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 0.block_0 ([torch.Size([288, 288])]) for Parameter 0 (torch.Size([32, 1, 3, 3])), Block block_0 (torch.Size([288])).
I0315 12:19:46.943740 139680868943040 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 0.block_0 ([torch.Size([0])]) for Parameter 0 (torch.Size([32, 1, 3, 3])), Block block_0 (torch.Size([288])).
I0315 12:19:46.946735 140718987674816 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 1.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 1 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 12:19:46.946774 139667941848256 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 1.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 1 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 12:19:46.946768 139715604698304 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 1.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 1 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 12:19:46.946845 139882590454976 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 1.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 1 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 12:19:46.946895 140631626413248 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 1.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 1 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 12:19:46.946942 140718987674816 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 2.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 2 (torch.Size([64, 32, 3, 3])), Block block_0 (torch.Size([64, 288])).
I0315 12:19:46.946958 139667941848256 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 2.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 2 (torch.Size([64, 32, 3, 3])), Block block_0 (torch.Size([64, 288])).
I0315 12:19:46.946947 140277216793792 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 1.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 1 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 12:19:46.947010 140508957750464 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 1.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 1 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 12:19:46.947054 139882590454976 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 2.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 2 (torch.Size([64, 32, 3, 3])), Block block_0 (torch.Size([64, 288])).
I0315 12:19:46.947099 140631626413248 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 2.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 2 (torch.Size([64, 32, 3, 3])), Block block_0 (torch.Size([64, 288])).
I0315 12:19:46.947120 140277216793792 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 2.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 2 (torch.Size([64, 32, 3, 3])), Block block_0 (torch.Size([64, 288])).
I0315 12:19:46.947121 139667941848256 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 3.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 3 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 12:19:46.947138 139715604698304 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 2.block_0 ([torch.Size([64, 64]), torch.Size([288, 288])]) for Parameter 2 (torch.Size([64, 32, 3, 3])), Block block_0 (torch.Size([64, 288])).
I0315 12:19:46.947130 139680868943040 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 1.block_0 ([torch.Size([1024, 1024]), torch.Size([9, 9])]) for Parameter 1 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 12:19:46.947211 139882590454976 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 3.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 3 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 12:19:46.947212 140508957750464 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 2.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 2 (torch.Size([64, 32, 3, 3])), Block block_0 (torch.Size([64, 288])).
I0315 12:19:46.947245 140277216793792 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 3.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 3 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 12:19:46.947256 139667941848256 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 4.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 4 (torch.Size([128, 64, 3, 3])), Block block_0 (torch.Size([128, 576])).
I0315 12:19:46.947248 140718987674816 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 3.block_0 ([torch.Size([64, 64]), torch.Size([576, 576])]) for Parameter 3 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 12:19:46.947268 140631626413248 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 3.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 3 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 12:19:46.947335 139680868943040 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 2.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 2 (torch.Size([64, 32, 3, 3])), Block block_0 (torch.Size([64, 288])).
I0315 12:19:46.947314 139715604698304 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 3.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 3 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 12:19:46.947365 139882590454976 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 4.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 4 (torch.Size([128, 64, 3, 3])), Block block_0 (torch.Size([128, 576])).
I0315 12:19:46.947396 140277216793792 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 4.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 4 (torch.Size([128, 64, 3, 3])), Block block_0 (torch.Size([128, 576])).
I0315 12:19:46.947389 140508957750464 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 3.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 3 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 12:19:46.947417 139667941848256 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 5.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 5 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 12:19:46.947428 140718987674816 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 4.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 4 (torch.Size([128, 64, 3, 3])), Block block_0 (torch.Size([128, 576])).
I0315 12:19:46.947436 140631626413248 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 4.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 4 (torch.Size([128, 64, 3, 3])), Block block_0 (torch.Size([128, 576])).
I0315 12:19:46.947484 139715604698304 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 4.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 4 (torch.Size([128, 64, 3, 3])), Block block_0 (torch.Size([128, 576])).
I0315 12:19:46.947511 139882590454976 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 5.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 5 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 12:19:46.947506 139680868943040 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 3.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 3 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 12:19:46.947531 140508957750464 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 4.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 4 (torch.Size([128, 64, 3, 3])), Block block_0 (torch.Size([128, 576])).
I0315 12:19:46.947559 140277216793792 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 5.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 5 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 12:19:46.947574 139667941848256 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 6.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 6 (torch.Size([256, 128, 3, 3])), Block block_0 (torch.Size([256, 384, 3])).
I0315 12:19:46.947581 140718987674816 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 5.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 5 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 12:19:46.947600 140631626413248 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 5.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 5 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 12:19:46.947652 139715604698304 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 5.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 5 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 12:19:46.947669 139882590454976 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 6.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 6 (torch.Size([256, 128, 3, 3])), Block block_0 (torch.Size([256, 384, 3])).
I0315 12:19:46.947717 140277216793792 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 6.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 6 (torch.Size([256, 128, 3, 3])), Block block_0 (torch.Size([256, 384, 3])).
I0315 12:19:46.947737 139667941848256 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 7.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 7 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 12:19:46.947739 140718987674816 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 6.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 6 (torch.Size([256, 128, 3, 3])), Block block_0 (torch.Size([256, 384, 3])).
I0315 12:19:46.947764 140631626413248 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 6.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 6 (torch.Size([256, 128, 3, 3])), Block block_0 (torch.Size([256, 384, 3])).
I0315 12:19:46.947752 139680868943040 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 4.block_0 ([torch.Size([128, 128]), torch.Size([576, 576])]) for Parameter 4 (torch.Size([128, 64, 3, 3])), Block block_0 (torch.Size([128, 576])).
I0315 12:19:46.947818 139715604698304 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 6.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 6 (torch.Size([256, 128, 3, 3])), Block block_0 (torch.Size([256, 384, 3])).
I0315 12:19:46.947830 139882590454976 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 7.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 7 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 12:19:46.947894 139667941848256 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 8.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 8 (torch.Size([512, 256, 3, 3])), Block block_0 (torch.Size([512, 768, 3])).
I0315 12:19:46.947895 140718987674816 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 7.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 7 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 12:19:46.947902 140277216793792 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 7.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 7 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 12:19:46.947927 139680868943040 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 5.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 5 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 12:19:46.947929 140631626413248 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 7.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 7 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 12:19:46.947983 139882590454976 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 8.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 8 (torch.Size([512, 256, 3, 3])), Block block_0 (torch.Size([512, 768, 3])).
I0315 12:19:46.948051 139667941848256 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 9.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 9 (torch.Size([512, 512, 3, 3])), Block block_0 (torch.Size([512, 512, 9])).
I0315 12:19:46.948050 140718987674816 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 8.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 8 (torch.Size([512, 256, 3, 3])), Block block_0 (torch.Size([512, 768, 3])).
I0315 12:19:46.948058 140277216793792 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 8.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 8 (torch.Size([512, 256, 3, 3])), Block block_0 (torch.Size([512, 768, 3])).
I0315 12:19:46.948076 139680868943040 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 6.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 6 (torch.Size([256, 128, 3, 3])), Block block_0 (torch.Size([256, 384, 3])).
I0315 12:19:46.948136 139882590454976 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 9.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 9 (torch.Size([512, 512, 3, 3])), Block block_0 (torch.Size([512, 512, 9])).
I0315 12:19:46.948192 140718987674816 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 9.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 9 (torch.Size([512, 512, 3, 3])), Block block_0 (torch.Size([512, 512, 9])).
I0315 12:19:46.948200 139667941848256 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 10.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 10 (torch.Size([256, 512, 3, 3])), Block block_0 (torch.Size([256, 512, 9])).
I0315 12:19:46.948221 139680868943040 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 7.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 7 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 12:19:46.948344 140718987674816 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 10.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 10 (torch.Size([256, 512, 3, 3])), Block block_0 (torch.Size([256, 512, 9])).
I0315 12:19:46.948378 139680868943040 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 8.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 8 (torch.Size([512, 256, 3, 3])), Block block_0 (torch.Size([512, 768, 3])).
I0315 12:19:46.948495 140718987674816 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 11.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 11 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 12:19:46.948521 139680868943040 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 9.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 9 (torch.Size([512, 512, 3, 3])), Block block_0 (torch.Size([512, 512, 9])).
I0315 12:19:46.948534 139667941848256 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 11.block_0 ([torch.Size([256, 256]), torch.Size([768, 768]), torch.Size([3, 3])]) for Parameter 11 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 12:19:46.948525 140508957750464 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 5.block_0 ([torch.Size([128, 128]), torch.Size([384, 384]), torch.Size([3, 3])]) for Parameter 5 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 12:19:46.948644 140718987674816 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 12.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 12 (torch.Size([128, 256, 3, 3])), Block block_0 (torch.Size([128, 768, 3])).
I0315 12:19:46.948663 139680868943040 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 10.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 10 (torch.Size([256, 512, 3, 3])), Block block_0 (torch.Size([256, 512, 9])).
I0315 12:19:46.948730 139667941848256 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 12.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 12 (torch.Size([128, 256, 3, 3])), Block block_0 (torch.Size([128, 768, 3])).
I0315 12:19:46.948795 139680868943040 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 11.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 11 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 12:19:46.948807 140718987674816 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 13.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 13 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 12:19:46.948885 139667941848256 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 13.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 13 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 12:19:46.948951 140718987674816 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 14.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 14 (torch.Size([64, 128, 3, 3])), Block block_0 (torch.Size([64, 384, 3])).
I0315 12:19:46.949057 139667941848256 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 14.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 14 (torch.Size([64, 128, 3, 3])), Block block_0 (torch.Size([64, 384, 3])).
I0315 12:19:46.949132 139715604698304 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 7.block_0 ([torch.Size([256, 256]), torch.Size([768, 768]), torch.Size([3, 3])]) for Parameter 7 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 12:19:46.949182 139667941848256 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 15.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 15 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 12:19:46.949189 140718987674816 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 15.block_0 ([torch.Size([64, 64]), torch.Size([576, 576])]) for Parameter 15 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 12:19:46.949344 140718987674816 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 16.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 16 (torch.Size([32, 64, 3, 3])), Block block_0 (torch.Size([32, 576])).
I0315 12:19:46.949353 139715604698304 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 8.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 8 (torch.Size([512, 256, 3, 3])), Block block_0 (torch.Size([512, 768, 3])).
I0315 12:19:46.949345 140631626413248 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 8.block_0 ([torch.Size([512, 512]), torch.Size([768, 768]), torch.Size([3, 3])]) for Parameter 8 (torch.Size([512, 256, 3, 3])), Block block_0 (torch.Size([512, 768, 3])).
I0315 12:19:46.949426 139667941848256 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 16.block_0 ([torch.Size([32, 32]), torch.Size([576, 576])]) for Parameter 16 (torch.Size([32, 64, 3, 3])), Block block_0 (torch.Size([32, 576])).
I0315 12:19:46.949516 139715604698304 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 9.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 9 (torch.Size([512, 512, 3, 3])), Block block_0 (torch.Size([512, 512, 9])).
I0315 12:19:46.949564 140631626413248 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 9.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 9 (torch.Size([512, 512, 3, 3])), Block block_0 (torch.Size([512, 512, 9])).
I0315 12:19:46.949576 139667941848256 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 17.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 17 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 12:19:46.949702 139667941848256 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 18.block_0 ([torch.Size([0])]) for Parameter 18 (torch.Size([1, 32, 1, 1])), Block block_0 (torch.Size([32])).
I0315 12:19:46.949710 139715604698304 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 10.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 10 (torch.Size([256, 512, 3, 3])), Block block_0 (torch.Size([256, 512, 9])).
I0315 12:19:46.949757 140631626413248 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 10.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 10 (torch.Size([256, 512, 3, 3])), Block block_0 (torch.Size([256, 512, 9])).
I0315 12:19:46.949741 139882590454976 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 10.block_0 ([torch.Size([256, 256]), torch.Size([512, 512]), torch.Size([9, 9])]) for Parameter 10 (torch.Size([256, 512, 3, 3])), Block block_0 (torch.Size([256, 512, 9])).
I0315 12:19:46.949805 139667941848256 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 19.block_0 ([torch.Size([0])]) for Parameter 19 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0315 12:19:46.949868 139715604698304 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 11.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 11 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 12:19:46.949919 140631626413248 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 11.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 11 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 12:19:46.949929 139667941848256 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 20.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 20 (torch.Size([512, 256, 2, 2])), Block block_0 (torch.Size([512, 1024])).
I0315 12:19:46.949944 139882590454976 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 11.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 11 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 12:19:46.949925 140508957750464 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 6.block_0 ([torch.Size([256, 256]), torch.Size([384, 384]), torch.Size([3, 3])]) for Parameter 6 (torch.Size([256, 128, 3, 3])), Block block_0 (torch.Size([256, 384, 3])).
I0315 12:19:46.950026 139715604698304 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 12.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 12 (torch.Size([128, 256, 3, 3])), Block block_0 (torch.Size([128, 768, 3])).
I0315 12:19:46.950078 139667941848256 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 21.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 21 (torch.Size([256, 128, 2, 2])), Block block_0 (torch.Size([256, 512])).
I0315 12:19:46.950076 140631626413248 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 12.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 12 (torch.Size([128, 256, 3, 3])), Block block_0 (torch.Size([128, 768, 3])).
I0315 12:19:46.950106 139882590454976 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 12.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 12 (torch.Size([128, 256, 3, 3])), Block block_0 (torch.Size([128, 768, 3])).
I0315 12:19:46.950122 140508957750464 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 7.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 7 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 12:19:46.950186 139715604698304 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 13.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 13 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 12:19:46.950208 139667941848256 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 22.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 22 (torch.Size([128, 64, 2, 2])), Block block_0 (torch.Size([128, 256])).
I0315 12:19:46.950235 140631626413248 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 13.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 13 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 12:19:46.950261 139882590454976 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 13.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 13 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 12:19:46.950293 140508957750464 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 8.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 8 (torch.Size([512, 256, 3, 3])), Block block_0 (torch.Size([512, 768, 3])).
I0315 12:19:46.950331 139667941848256 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 23.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 23 (torch.Size([64, 32, 2, 2])), Block block_0 (torch.Size([64, 128])).
I0315 12:19:46.950360 139715604698304 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 14.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 14 (torch.Size([64, 128, 3, 3])), Block block_0 (torch.Size([64, 384, 3])).
I0315 12:19:46.950394 140631626413248 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 14.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 14 (torch.Size([64, 128, 3, 3])), Block block_0 (torch.Size([64, 384, 3])).
I0315 12:19:46.950427 139882590454976 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 14.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 14 (torch.Size([64, 128, 3, 3])), Block block_0 (torch.Size([64, 384, 3])).
I0315 12:19:46.950410 139680868943040 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 12.block_0 ([torch.Size([128, 128]), torch.Size([768, 768]), torch.Size([3, 3])]) for Parameter 12 (torch.Size([128, 256, 3, 3])), Block block_0 (torch.Size([128, 768, 3])).
I0315 12:19:46.950453 139667941848256 shampoo_preconditioner_list.py:612] Rank 4: ShampooPreconditionerList Numel Breakdown: (165888, 2097314, 174080, 671744, 696320, 327698, 426002, 1310738, 1703954, 1048738, 655522, 1310738, 1212434, 327698, 303122, 671744, 665600, 2097314, 2048, 2, 2621440, 655360, 163840, 40960)
I0315 12:19:46.950461 140508957750464 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 9.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 9 (torch.Size([512, 512, 3, 3])), Block block_0 (torch.Size([512, 512, 9])).
I0315 12:19:46.950502 139667941848256 shampoo_preconditioner_list.py:615] Rank 4: ShampooPreconditionerList Bytes Breakdown: (663552, 8389256)
I0315 12:19:46.950499 139715604698304 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 15.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 15 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 12:19:46.950475 140718987674816 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 17.block_0 ([torch.Size([1024, 1024]), torch.Size([9, 9])]) for Parameter 17 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 12:19:46.950540 139667941848256 shampoo_preconditioner_list.py:618] Rank 4: ShampooPreconditionerList Total Elements: 19350298
I0315 12:19:46.950535 140631626413248 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 15.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 15 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 12:19:46.950505 140277216793792 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 9.block_0 ([torch.Size([512, 512]), torch.Size([512, 512]), torch.Size([9, 9])]) for Parameter 9 (torch.Size([512, 512, 3, 3])), Block block_0 (torch.Size([512, 512, 9])).
I0315 12:19:46.950553 139882590454976 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 15.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 15 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 12:19:46.950576 139667941848256 shampoo_preconditioner_list.py:621] Rank 4: ShampooPreconditionerList Total Bytes: 9052808
I0315 12:19:46.950604 140508957750464 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 10.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 10 (torch.Size([256, 512, 3, 3])), Block block_0 (torch.Size([256, 512, 9])).
I0315 12:19:46.950606 140718987674816 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 18.block_0 ([torch.Size([0])]) for Parameter 18 (torch.Size([1, 32, 1, 1])), Block block_0 (torch.Size([32])).
I0315 12:19:46.950624 139715604698304 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 16.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 16 (torch.Size([32, 64, 3, 3])), Block block_0 (torch.Size([32, 576])).
I0315 12:19:46.950654 140631626413248 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 16.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 16 (torch.Size([32, 64, 3, 3])), Block block_0 (torch.Size([32, 576])).
I0315 12:19:46.950667 139882590454976 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 16.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 16 (torch.Size([32, 64, 3, 3])), Block block_0 (torch.Size([32, 576])).
I0315 12:19:46.950712 140277216793792 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 10.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 10 (torch.Size([256, 512, 3, 3])), Block block_0 (torch.Size([256, 512, 9])).
I0315 12:19:46.950732 140718987674816 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 19.block_0 ([torch.Size([0])]) for Parameter 19 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0315 12:19:46.950754 140508957750464 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 11.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 11 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 12:19:46.950763 139715604698304 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 17.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 17 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 12:19:46.950784 140631626413248 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 17.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 17 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 12:19:46.950803 139882590454976 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 17.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 17 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 12:19:46.950795 139667941848256 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([32, 1, 3, 3])), Block block_0 (torch.Size([288])).
I0315 12:19:46.950852 139715604698304 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 18.block_0 ([torch.Size([0])]) for Parameter 18 (torch.Size([1, 32, 1, 1])), Block block_0 (torch.Size([32])).
I0315 12:19:46.950872 139667941848256 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 12:19:46.950884 140631626413248 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 18.block_0 ([torch.Size([0])]) for Parameter 18 (torch.Size([1, 32, 1, 1])), Block block_0 (torch.Size([32])).
I0315 12:19:46.950892 140277216793792 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 11.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 11 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 12:19:46.950905 139882590454976 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 18.block_0 ([torch.Size([0])]) for Parameter 18 (torch.Size([1, 32, 1, 1])), Block block_0 (torch.Size([32])).
I0315 12:19:46.950910 140508957750464 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 12.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 12 (torch.Size([128, 256, 3, 3])), Block block_0 (torch.Size([128, 768, 3])).
I0315 12:19:46.950936 139667941848256 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([64, 32, 3, 3])), Block block_0 (torch.Size([64, 288])).
I0315 12:19:46.950951 139715604698304 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 19.block_0 ([torch.Size([0])]) for Parameter 19 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0315 12:19:46.950970 140631626413248 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 19.block_0 ([torch.Size([0])]) for Parameter 19 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0315 12:19:46.950991 139882590454976 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 19.block_0 ([torch.Size([0])]) for Parameter 19 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0315 12:19:46.950995 139667941848256 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 12:19:46.951046 139667941848256 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 4.block_0 (torch.Size([0])) for Parameter 4 (torch.Size([128, 64, 3, 3])), Block block_0 (torch.Size([128, 576])).
I0315 12:19:46.951051 140508957750464 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 13.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 13 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 12:19:46.951058 140277216793792 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 12.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 12 (torch.Size([128, 256, 3, 3])), Block block_0 (torch.Size([128, 768, 3])).
I0315 12:19:46.951076 139715604698304 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 20.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 20 (torch.Size([512, 256, 2, 2])), Block block_0 (torch.Size([512, 1024])).
I0315 12:19:46.951093 140631626413248 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 20.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 20 (torch.Size([512, 256, 2, 2])), Block block_0 (torch.Size([512, 1024])).
I0315 12:19:46.951065 139680868943040 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 13.block_0 ([torch.Size([128, 128]), torch.Size([384, 384]), torch.Size([3, 3])]) for Parameter 13 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 12:19:46.951107 139667941848256 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 5.block_0 (torch.Size([0])) for Parameter 5 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 12:19:46.951117 139882590454976 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 20.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 20 (torch.Size([512, 256, 2, 2])), Block block_0 (torch.Size([512, 1024])).
I0315 12:19:46.951159 139667941848256 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 6.block_0 (torch.Size([0])) for Parameter 6 (torch.Size([256, 128, 3, 3])), Block block_0 (torch.Size([256, 384, 3])).
I0315 12:19:46.951199 140508957750464 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 14.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 14 (torch.Size([64, 128, 3, 3])), Block block_0 (torch.Size([64, 384, 3])).
I0315 12:19:46.951200 140277216793792 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 13.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 13 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 12:19:46.951214 139715604698304 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 21.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 21 (torch.Size([256, 128, 2, 2])), Block block_0 (torch.Size([256, 512])).
I0315 12:19:46.951225 139667941848256 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 7.block_0 (torch.Size([0])) for Parameter 7 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 12:19:46.951224 140631626413248 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 21.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 21 (torch.Size([256, 128, 2, 2])), Block block_0 (torch.Size([256, 512])).
I0315 12:19:46.951259 139882590454976 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 21.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 21 (torch.Size([256, 128, 2, 2])), Block block_0 (torch.Size([256, 512])).
I0315 12:19:46.951279 139667941848256 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 8.block_0 (torch.Size([0])) for Parameter 8 (torch.Size([512, 256, 3, 3])), Block block_0 (torch.Size([512, 768, 3])).
I0315 12:19:46.951330 139667941848256 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 9.block_0 (torch.Size([0])) for Parameter 9 (torch.Size([512, 512, 3, 3])), Block block_0 (torch.Size([512, 512, 9])).
I0315 12:19:46.951336 139715604698304 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 22.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 22 (torch.Size([128, 64, 2, 2])), Block block_0 (torch.Size([128, 256])).
I0315 12:19:46.951342 140631626413248 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 22.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 22 (torch.Size([128, 64, 2, 2])), Block block_0 (torch.Size([128, 256])).
I0315 12:19:46.951341 140508957750464 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 15.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 15 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 12:19:46.951344 140277216793792 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 14.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 14 (torch.Size([64, 128, 3, 3])), Block block_0 (torch.Size([64, 384, 3])).
I0315 12:19:46.951337 139680868943040 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 14.block_0 ([torch.Size([64, 64]), torch.Size([384, 384]), torch.Size([3, 3])]) for Parameter 14 (torch.Size([64, 128, 3, 3])), Block block_0 (torch.Size([64, 384, 3])).
I0315 12:19:46.951341 140718987674816 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 20.block_0 ([torch.Size([512, 512]), torch.Size([1024, 1024])]) for Parameter 20 (torch.Size([512, 256, 2, 2])), Block block_0 (torch.Size([512, 1024])).
I0315 12:19:46.951382 139882590454976 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 22.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 22 (torch.Size([128, 64, 2, 2])), Block block_0 (torch.Size([128, 256])).
I0315 12:19:46.951388 139667941848256 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 10.block_0 (torch.Size([0])) for Parameter 10 (torch.Size([256, 512, 3, 3])), Block block_0 (torch.Size([256, 512, 9])).
I0315 12:19:46.951448 139715604698304 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 23.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 23 (torch.Size([64, 32, 2, 2])), Block block_0 (torch.Size([64, 128])).
I0315 12:19:46.951458 140631626413248 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 23.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 23 (torch.Size([64, 32, 2, 2])), Block block_0 (torch.Size([64, 128])).
I0315 12:19:46.951462 140508957750464 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 16.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 16 (torch.Size([32, 64, 3, 3])), Block block_0 (torch.Size([32, 576])).
I0315 12:19:46.951474 140277216793792 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 15.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 15 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 12:19:46.951490 139680868943040 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 15.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 15 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 12:19:46.951500 139882590454976 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 23.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 23 (torch.Size([64, 32, 2, 2])), Block block_0 (torch.Size([64, 128])).
I0315 12:19:46.951496 139667941848256 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 11.block_0 (torch.Size([256, 768, 3])) for Parameter 11 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 12:19:46.951536 140718987674816 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 21.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 21 (torch.Size([256, 128, 2, 2])), Block block_0 (torch.Size([256, 512])).
I0315 12:19:46.951564 139667941848256 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 12.block_0 (torch.Size([0])) for Parameter 12 (torch.Size([128, 256, 3, 3])), Block block_0 (torch.Size([128, 768, 3])).
I0315 12:19:46.951571 140631626413248 shampoo_preconditioner_list.py:612] Rank 1: ShampooPreconditionerList Numel Breakdown: (165888, 2097314, 174080, 671744, 696320, 327698, 426002, 1310738, 1703954, 1048738, 655522, 1310738, 1212434, 327698, 303122, 671744, 665600, 2097314, 2048, 2, 2621440, 655360, 163840, 40960)
I0315 12:19:46.951575 140508957750464 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 17.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 17 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 12:19:46.951573 139715604698304 shampoo_preconditioner_list.py:612] Rank 3: ShampooPreconditionerList Numel Breakdown: (165888, 2097314, 174080, 671744, 696320, 327698, 426002, 1310738, 1703954, 1048738, 655522, 1310738, 1212434, 327698, 303122, 671744, 665600, 2097314, 2048, 2, 2621440, 655360, 163840, 40960)
I0315 12:19:46.951604 140277216793792 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 16.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 16 (torch.Size([32, 64, 3, 3])), Block block_0 (torch.Size([32, 576])).
I0315 12:19:46.951613 140631626413248 shampoo_preconditioner_list.py:615] Rank 1: ShampooPreconditionerList Bytes Breakdown: (663552,)
I0315 12:19:46.951615 139715604698304 shampoo_preconditioner_list.py:615] Rank 3: ShampooPreconditionerList Bytes Breakdown: (663552, 8389256)
I0315 12:19:46.951612 139882590454976 shampoo_preconditioner_list.py:612] Rank 2: ShampooPreconditionerList Numel Breakdown: (165888, 2097314, 174080, 671744, 696320, 327698, 426002, 1310738, 1703954, 1048738, 655522, 1310738, 1212434, 327698, 303122, 671744, 665600, 2097314, 2048, 2, 2621440, 655360, 163840, 40960)
I0315 12:19:46.951627 139667941848256 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 13.block_0 (torch.Size([0])) for Parameter 13 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 12:19:46.951645 140631626413248 shampoo_preconditioner_list.py:618] Rank 1: ShampooPreconditionerList Total Elements: 19350298
I0315 12:19:46.951637 139680868943040 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 16.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 16 (torch.Size([32, 64, 3, 3])), Block block_0 (torch.Size([32, 576])).
I0315 12:19:46.951648 139715604698304 shampoo_preconditioner_list.py:618] Rank 3: ShampooPreconditionerList Total Elements: 19350298
I0315 12:19:46.951653 139882590454976 shampoo_preconditioner_list.py:615] Rank 2: ShampooPreconditionerList Bytes Breakdown: (663552,)
I0315 12:19:46.951685 139882590454976 shampoo_preconditioner_list.py:618] Rank 2: ShampooPreconditionerList Total Elements: 19350298
I0315 12:19:46.951668 140718987674816 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 22.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 22 (torch.Size([128, 64, 2, 2])), Block block_0 (torch.Size([128, 256])).
I0315 12:19:46.951679 139715604698304 shampoo_preconditioner_list.py:621] Rank 3: ShampooPreconditionerList Total Bytes: 9052808
I0315 12:19:46.951686 139667941848256 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 14.block_0 (torch.Size([0])) for Parameter 14 (torch.Size([64, 128, 3, 3])), Block block_0 (torch.Size([64, 384, 3])).
I0315 12:19:46.951693 140631626413248 shampoo_preconditioner_list.py:621] Rank 1: ShampooPreconditionerList Total Bytes: 663552
I0315 12:19:46.951731 139882590454976 shampoo_preconditioner_list.py:621] Rank 2: ShampooPreconditionerList Total Bytes: 663552
I0315 12:19:46.951737 139667941848256 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 15.block_0 (torch.Size([0])) for Parameter 15 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 12:19:46.951734 140277216793792 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 17.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 17 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 12:19:46.951752 139680868943040 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 17.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 17 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 12:19:46.951744 140508957750464 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 18.block_0 ([torch.Size([32, 32])]) for Parameter 18 (torch.Size([1, 32, 1, 1])), Block block_0 (torch.Size([32])).
I0315 12:19:46.951802 140718987674816 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 23.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 23 (torch.Size([64, 32, 2, 2])), Block block_0 (torch.Size([64, 128])).
I0315 12:19:46.951822 139667941848256 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 16.block_0 (torch.Size([32, 576])) for Parameter 16 (torch.Size([32, 64, 3, 3])), Block block_0 (torch.Size([32, 576])).
I0315 12:19:46.951837 140277216793792 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 18.block_0 ([torch.Size([0])]) for Parameter 18 (torch.Size([1, 32, 1, 1])), Block block_0 (torch.Size([32])).
I0315 12:19:46.951847 139680868943040 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 18.block_0 ([torch.Size([0])]) for Parameter 18 (torch.Size([1, 32, 1, 1])), Block block_0 (torch.Size([32])).
I0315 12:19:46.951893 139667941848256 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 17.block_0 (torch.Size([0])) for Parameter 17 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 12:19:46.951889 139715604698304 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([32, 1, 3, 3])), Block block_0 (torch.Size([288])).
I0315 12:19:46.951887 140508957750464 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 19.block_0 ([torch.Size([1, 1])]) for Parameter 19 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0315 12:19:46.951903 140631626413248 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([32, 1, 3, 3])), Block block_0 (torch.Size([288])).
I0315 12:19:46.951922 140277216793792 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 19.block_0 ([torch.Size([0])]) for Parameter 19 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0315 12:19:46.951928 139680868943040 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 19.block_0 ([torch.Size([0])]) for Parameter 19 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0315 12:19:46.951930 140718987674816 shampoo_preconditioner_list.py:612] Rank 5: ShampooPreconditionerList Numel Breakdown: (165888, 2097314, 174080, 671744, 696320, 327698, 426002, 1310738, 1703954, 1048738, 655522, 1310738, 1212434, 327698, 303122, 671744, 665600, 2097314, 2048, 2, 2621440, 655360, 163840, 40960)
I0315 12:19:46.951942 139882590454976 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([32, 1, 3, 3])), Block block_0 (torch.Size([288])).
I0315 12:19:46.951951 139667941848256 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 18.block_0 (torch.Size([0])) for Parameter 18 (torch.Size([1, 32, 1, 1])), Block block_0 (torch.Size([32])).
I0315 12:19:46.951959 139715604698304 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 12:19:46.951973 140718987674816 shampoo_preconditioner_list.py:615] Rank 5: ShampooPreconditionerList Bytes Breakdown: (663552, 8389256, 696320, 2686976)
I0315 12:19:46.951971 140631626413248 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 12:19:46.952001 139667941848256 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 19.block_0 (torch.Size([0])) for Parameter 19 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0315 12:19:46.952006 140718987674816 shampoo_preconditioner_list.py:618] Rank 5: ShampooPreconditionerList Total Elements: 19350298
I0315 12:19:46.952018 139882590454976 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 12:19:46.952026 140631626413248 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([64, 32, 3, 3])), Block block_0 (torch.Size([64, 288])).
I0315 12:19:46.952039 140718987674816 shampoo_preconditioner_list.py:621] Rank 5: ShampooPreconditionerList Total Bytes: 12436104
I0315 12:19:46.952034 140508957750464 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 20.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 20 (torch.Size([512, 256, 2, 2])), Block block_0 (torch.Size([512, 1024])).
I0315 12:19:46.952046 139680868943040 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 20.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 20 (torch.Size([512, 256, 2, 2])), Block block_0 (torch.Size([512, 1024])).
I0315 12:19:46.952049 140277216793792 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 20.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 20 (torch.Size([512, 256, 2, 2])), Block block_0 (torch.Size([512, 1024])).
I0315 12:19:46.952057 139667941848256 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 20.block_0 (torch.Size([0])) for Parameter 20 (torch.Size([512, 256, 2, 2])), Block block_0 (torch.Size([512, 1024])).
I0315 12:19:46.952067 139715604698304 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([64, 288])) for Parameter 2 (torch.Size([64, 32, 3, 3])), Block block_0 (torch.Size([64, 288])).
I0315 12:19:46.952079 140631626413248 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 12:19:46.952080 139882590454976 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([64, 32, 3, 3])), Block block_0 (torch.Size([64, 288])).
I0315 12:19:46.952107 139667941848256 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 21.block_0 (torch.Size([0])) for Parameter 21 (torch.Size([256, 128, 2, 2])), Block block_0 (torch.Size([256, 512])).
I0315 12:19:46.952129 140631626413248 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 4.block_0 (torch.Size([0])) for Parameter 4 (torch.Size([128, 64, 3, 3])), Block block_0 (torch.Size([128, 576])).
I0315 12:19:46.952133 139882590454976 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 12:19:46.952138 139715604698304 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 12:19:46.952162 139680868943040 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 21.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 21 (torch.Size([256, 128, 2, 2])), Block block_0 (torch.Size([256, 512])).
I0315 12:19:46.952163 139667941848256 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 22.block_0 (torch.Size([0])) for Parameter 22 (torch.Size([128, 64, 2, 2])), Block block_0 (torch.Size([128, 256])).
I0315 12:19:46.952184 139882590454976 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 4.block_0 (torch.Size([0])) for Parameter 4 (torch.Size([128, 64, 3, 3])), Block block_0 (torch.Size([128, 576])).
I0315 12:19:46.952186 140277216793792 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 21.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 21 (torch.Size([256, 128, 2, 2])), Block block_0 (torch.Size([256, 512])).
I0315 12:19:46.952191 140631626413248 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 5.block_0 (torch.Size([0])) for Parameter 5 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 12:19:46.952193 139715604698304 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 4.block_0 (torch.Size([0])) for Parameter 4 (torch.Size([128, 64, 3, 3])), Block block_0 (torch.Size([128, 576])).
I0315 12:19:46.952216 139667941848256 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 23.block_0 (torch.Size([0])) for Parameter 23 (torch.Size([64, 32, 2, 2])), Block block_0 (torch.Size([64, 128])).
I0315 12:19:46.952236 139882590454976 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 5.block_0 (torch.Size([0])) for Parameter 5 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 12:19:46.952234 140718987674816 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([32, 1, 3, 3])), Block block_0 (torch.Size([288])).
I0315 12:19:46.952243 140631626413248 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 6.block_0 (torch.Size([0])) for Parameter 6 (torch.Size([256, 128, 3, 3])), Block block_0 (torch.Size([256, 384, 3])).
I0315 12:19:46.952249 139715604698304 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 5.block_0 (torch.Size([0])) for Parameter 5 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 12:19:46.952280 139680868943040 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 22.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 22 (torch.Size([128, 64, 2, 2])), Block block_0 (torch.Size([128, 256])).
I0315 12:19:46.952292 139667941848256 shampoo_preconditioner_list.py:375] Rank 4: AdaGradPreconditionerList Numel Breakdown: (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 589824, 0, 0, 0, 0, 18432, 0, 0, 0, 0, 0, 0, 0)
I0315 12:19:46.952295 139882590454976 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 6.block_0 (torch.Size([0])) for Parameter 6 (torch.Size([256, 128, 3, 3])), Block block_0 (torch.Size([256, 384, 3])).
I0315 12:19:46.952300 140631626413248 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 7.block_0 (torch.Size([0])) for Parameter 7 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 12:19:46.952300 140718987674816 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 12:19:46.952302 139715604698304 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 6.block_0 (torch.Size([0])) for Parameter 6 (torch.Size([256, 128, 3, 3])), Block block_0 (torch.Size([256, 384, 3])).
I0315 12:19:46.952300 140277216793792 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 22.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 22 (torch.Size([128, 64, 2, 2])), Block block_0 (torch.Size([128, 256])).
I0315 12:19:46.952328 139667941848256 shampoo_preconditioner_list.py:378] Rank 4: AdaGradPreconditionerList Bytes Breakdown: (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2359296, 0, 0, 0, 0, 73728, 0, 0, 0, 0, 0, 0, 0)
I0315 12:19:46.952354 139882590454976 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 7.block_0 (torch.Size([0])) for Parameter 7 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 12:19:46.952355 140718987674816 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([64, 32, 3, 3])), Block block_0 (torch.Size([64, 288])).
I0315 12:19:46.952364 139667941848256 shampoo_preconditioner_list.py:381] Rank 4: AdaGradPreconditionerList Total Elements: 608256
I0315 12:19:46.952394 139667941848256 shampoo_preconditioner_list.py:384] Rank 4: AdaGradPreconditionerList Total Bytes: 2433024
I0315 12:19:46.952392 139715604698304 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 7.block_0 (torch.Size([256, 768, 3])) for Parameter 7 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 12:19:46.952412 139882590454976 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 8.block_0 (torch.Size([0])) for Parameter 8 (torch.Size([512, 256, 3, 3])), Block block_0 (torch.Size([512, 768, 3])).
I0315 12:19:46.952414 140631626413248 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 8.block_0 (torch.Size([512, 768, 3])) for Parameter 8 (torch.Size([512, 256, 3, 3])), Block block_0 (torch.Size([512, 768, 3])).
I0315 12:19:46.952421 140277216793792 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 23.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 23 (torch.Size([64, 32, 2, 2])), Block block_0 (torch.Size([64, 128])).
I0315 12:19:46.952456 139715604698304 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 8.block_0 (torch.Size([0])) for Parameter 8 (torch.Size([512, 256, 3, 3])), Block block_0 (torch.Size([512, 768, 3])).
I0315 12:19:46.952453 140718987674816 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([64, 576])) for Parameter 3 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 12:19:46.952470 139882590454976 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 9.block_0 (torch.Size([0])) for Parameter 9 (torch.Size([512, 512, 3, 3])), Block block_0 (torch.Size([512, 512, 9])).
I0315 12:19:46.952481 140631626413248 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 9.block_0 (torch.Size([0])) for Parameter 9 (torch.Size([512, 512, 3, 3])), Block block_0 (torch.Size([512, 512, 9])).
I0315 12:19:46.952478 139680868943040 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 23.block_0 ([torch.Size([64, 64]), torch.Size([128, 128])]) for Parameter 23 (torch.Size([64, 32, 2, 2])), Block block_0 (torch.Size([64, 128])).
I0315 12:19:46.952520 140718987674816 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 4.block_0 (torch.Size([0])) for Parameter 4 (torch.Size([128, 64, 3, 3])), Block block_0 (torch.Size([128, 576])).
I0315 12:19:46.952523 139715604698304 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 9.block_0 (torch.Size([0])) for Parameter 9 (torch.Size([512, 512, 3, 3])), Block block_0 (torch.Size([512, 512, 9])).
I0315 12:19:46.952533 140631626413248 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 10.block_0 (torch.Size([0])) for Parameter 10 (torch.Size([256, 512, 3, 3])), Block block_0 (torch.Size([256, 512, 9])).
I0315 12:19:46.952531 140277216793792 shampoo_preconditioner_list.py:612] Rank 0: ShampooPreconditionerList Numel Breakdown: (165888, 2097314, 174080, 671744, 696320, 327698, 426002, 1310738, 1703954, 1048738, 655522, 1310738, 1212434, 327698, 303122, 671744, 665600, 2097314, 2048, 2, 2621440, 655360, 163840, 40960)
I0315 12:19:46.952571 140277216793792 shampoo_preconditioner_list.py:615] Rank 0: ShampooPreconditionerList Bytes Breakdown: (663552,)
I0315 12:19:46.952576 139715604698304 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 10.block_0 (torch.Size([0])) for Parameter 10 (torch.Size([256, 512, 3, 3])), Block block_0 (torch.Size([256, 512, 9])).
I0315 12:19:46.952583 140631626413248 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 11.block_0 (torch.Size([0])) for Parameter 11 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 12:19:46.952589 140718987674816 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 5.block_0 (torch.Size([0])) for Parameter 5 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 12:19:46.952614 140277216793792 shampoo_preconditioner_list.py:618] Rank 0: ShampooPreconditionerList Total Elements: 19350298
I0315 12:19:46.952577 139882590454976 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 10.block_0 (torch.Size([256, 512, 9])) for Parameter 10 (torch.Size([256, 512, 3, 3])), Block block_0 (torch.Size([256, 512, 9])).
I0315 12:19:46.952628 139715604698304 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 11.block_0 (torch.Size([0])) for Parameter 11 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 12:19:46.952633 140631626413248 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 12.block_0 (torch.Size([0])) for Parameter 12 (torch.Size([128, 256, 3, 3])), Block block_0 (torch.Size([128, 768, 3])).
I0315 12:19:46.952616 140508957750464 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 21.block_0 ([torch.Size([256, 256]), torch.Size([512, 512])]) for Parameter 21 (torch.Size([256, 128, 2, 2])), Block block_0 (torch.Size([256, 512])).
I0315 12:19:46.952644 140718987674816 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 6.block_0 (torch.Size([0])) for Parameter 6 (torch.Size([256, 128, 3, 3])), Block block_0 (torch.Size([256, 384, 3])).
I0315 12:19:46.952650 140277216793792 shampoo_preconditioner_list.py:621] Rank 0: ShampooPreconditionerList Total Bytes: 663552
I0315 12:19:46.952653 139680868943040 shampoo_preconditioner_list.py:612] Rank 7: ShampooPreconditionerList Numel Breakdown: (165888, 2097314, 174080, 671744, 696320, 327698, 426002, 1310738, 1703954, 1048738, 655522, 1310738, 1212434, 327698, 303122, 671744, 665600, 2097314, 2048, 2, 2621440, 655360, 163840, 40960)
I0315 12:19:46.952679 139715604698304 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 12.block_0 (torch.Size([0])) for Parameter 12 (torch.Size([128, 256, 3, 3])), Block block_0 (torch.Size([128, 768, 3])).
I0315 12:19:46.952687 139882590454976 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 11.block_0 (torch.Size([0])) for Parameter 11 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 12:19:46.952691 140631626413248 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 13.block_0 (torch.Size([0])) for Parameter 13 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 12:19:46.952695 139680868943040 shampoo_preconditioner_list.py:615] Rank 7: ShampooPreconditionerList Bytes Breakdown: (663552, 8389256, 696320, 2686976, 2785280, 1310792)
I0315 12:19:46.952713 140718987674816 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 7.block_0 (torch.Size([0])) for Parameter 7 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 12:19:46.952726 139680868943040 shampoo_preconditioner_list.py:618] Rank 7: ShampooPreconditionerList Total Elements: 19350298
I0315 12:19:46.952743 140631626413248 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 14.block_0 (torch.Size([0])) for Parameter 14 (torch.Size([64, 128, 3, 3])), Block block_0 (torch.Size([64, 384, 3])).
I0315 12:19:46.952747 139715604698304 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 13.block_0 (torch.Size([0])) for Parameter 13 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 12:19:46.952754 139680868943040 shampoo_preconditioner_list.py:621] Rank 7: ShampooPreconditionerList Total Bytes: 16532176
I0315 12:19:46.952754 139882590454976 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 12.block_0 (torch.Size([0])) for Parameter 12 (torch.Size([128, 256, 3, 3])), Block block_0 (torch.Size([128, 768, 3])).
I0315 12:19:46.952775 140718987674816 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 8.block_0 (torch.Size([0])) for Parameter 8 (torch.Size([512, 256, 3, 3])), Block block_0 (torch.Size([512, 768, 3])).
I0315 12:19:46.952798 139715604698304 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 14.block_0 (torch.Size([0])) for Parameter 14 (torch.Size([64, 128, 3, 3])), Block block_0 (torch.Size([64, 384, 3])).
I0315 12:19:46.952802 140631626413248 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 15.block_0 (torch.Size([0])) for Parameter 15 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 12:19:46.952806 139882590454976 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 13.block_0 (torch.Size([0])) for Parameter 13 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 12:19:46.952834 140718987674816 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 9.block_0 (torch.Size([0])) for Parameter 9 (torch.Size([512, 512, 3, 3])), Block block_0 (torch.Size([512, 512, 9])).
I0315 12:19:46.952852 140631626413248 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 16.block_0 (torch.Size([0])) for Parameter 16 (torch.Size([32, 64, 3, 3])), Block block_0 (torch.Size([32, 576])).
I0315 12:19:46.952855 139715604698304 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 15.block_0 (torch.Size([0])) for Parameter 15 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 12:19:46.952857 139882590454976 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 14.block_0 (torch.Size([0])) for Parameter 14 (torch.Size([64, 128, 3, 3])), Block block_0 (torch.Size([64, 384, 3])).
I0315 12:19:46.952856 140508957750464 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 22.block_0 ([torch.Size([128, 128]), torch.Size([256, 256])]) for Parameter 22 (torch.Size([128, 64, 2, 2])), Block block_0 (torch.Size([128, 256])).
I0315 12:19:46.952873 140277216793792 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([32, 1, 3, 3])), Block block_0 (torch.Size([288])).
I0315 12:19:46.952891 140718987674816 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 10.block_0 (torch.Size([0])) for Parameter 10 (torch.Size([256, 512, 3, 3])), Block block_0 (torch.Size([256, 512, 9])).
I0315 12:19:46.952902 140631626413248 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 17.block_0 (torch.Size([0])) for Parameter 17 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 12:19:46.952904 139715604698304 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 16.block_0 (torch.Size([0])) for Parameter 16 (torch.Size([32, 64, 3, 3])), Block block_0 (torch.Size([32, 576])).
I0315 12:19:46.952906 139882590454976 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 15.block_0 (torch.Size([0])) for Parameter 15 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 12:19:46.952943 140718987674816 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 11.block_0 (torch.Size([0])) for Parameter 11 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 12:19:46.952938 139680868943040 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([32, 1, 3, 3])), Block block_0 (torch.Size([288])).
I0315 12:19:46.952951 140631626413248 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 18.block_0 (torch.Size([0])) for Parameter 18 (torch.Size([1, 32, 1, 1])), Block block_0 (torch.Size([32])).
I0315 12:19:46.952954 139715604698304 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 17.block_0 (torch.Size([0])) for Parameter 17 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 12:19:46.952955 139882590454976 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 16.block_0 (torch.Size([0])) for Parameter 16 (torch.Size([32, 64, 3, 3])), Block block_0 (torch.Size([32, 576])).
I0315 12:19:46.952953 140277216793792 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 12:19:46.953000 140631626413248 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 19.block_0 (torch.Size([0])) for Parameter 19 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0315 12:19:46.952999 140718987674816 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 12.block_0 (torch.Size([0])) for Parameter 12 (torch.Size([128, 256, 3, 3])), Block block_0 (torch.Size([128, 768, 3])).
I0315 12:19:46.953003 139715604698304 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 18.block_0 (torch.Size([0])) for Parameter 18 (torch.Size([1, 32, 1, 1])), Block block_0 (torch.Size([32])).
I0315 12:19:46.953004 139882590454976 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 17.block_0 (torch.Size([0])) for Parameter 17 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 12:19:46.953011 140277216793792 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([64, 32, 3, 3])), Block block_0 (torch.Size([64, 288])).
I0315 12:19:46.953010 140508957750464 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 23.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 23 (torch.Size([64, 32, 2, 2])), Block block_0 (torch.Size([64, 128])).
I0315 12:19:46.953049 140718987674816 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 13.block_0 (torch.Size([0])) for Parameter 13 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 12:19:46.953044 139680868943040 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([1024, 9])) for Parameter 1 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 12:19:46.953053 139882590454976 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 18.block_0 (torch.Size([0])) for Parameter 18 (torch.Size([1, 32, 1, 1])), Block block_0 (torch.Size([32])).
I0315 12:19:46.953051 139715604698304 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 19.block_0 (torch.Size([0])) for Parameter 19 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0315 12:19:46.953056 140631626413248 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 20.block_0 (torch.Size([0])) for Parameter 20 (torch.Size([512, 256, 2, 2])), Block block_0 (torch.Size([512, 1024])).
I0315 12:19:46.953064 140277216793792 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 12:19:46.953100 139882590454976 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 19.block_0 (torch.Size([0])) for Parameter 19 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0315 12:19:46.953104 140631626413248 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 21.block_0 (torch.Size([0])) for Parameter 21 (torch.Size([256, 128, 2, 2])), Block block_0 (torch.Size([256, 512])).
I0315 12:19:46.953104 139715604698304 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 20.block_0 (torch.Size([0])) for Parameter 20 (torch.Size([512, 256, 2, 2])), Block block_0 (torch.Size([512, 1024])).
I0315 12:19:46.953106 140718987674816 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 14.block_0 (torch.Size([0])) for Parameter 14 (torch.Size([64, 128, 3, 3])), Block block_0 (torch.Size([64, 384, 3])).
I0315 12:19:46.953112 139680868943040 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([64, 32, 3, 3])), Block block_0 (torch.Size([64, 288])).
I0315 12:19:46.953115 140277216793792 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 4.block_0 (torch.Size([0])) for Parameter 4 (torch.Size([128, 64, 3, 3])), Block block_0 (torch.Size([128, 576])).
I0315 12:19:46.953143 140508957750464 shampoo_preconditioner_list.py:612] Rank 6: ShampooPreconditionerList Numel Breakdown: (165888, 2097314, 174080, 671744, 696320, 327698, 426002, 1310738, 1703954, 1048738, 655522, 1310738, 1212434, 327698, 303122, 671744, 665600, 2097314, 2048, 2, 2621440, 655360, 163840, 40960)
I0315 12:19:46.953153 140631626413248 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 22.block_0 (torch.Size([0])) for Parameter 22 (torch.Size([128, 64, 2, 2])), Block block_0 (torch.Size([128, 256])).
I0315 12:19:46.953154 139715604698304 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 21.block_0 (torch.Size([0])) for Parameter 21 (torch.Size([256, 128, 2, 2])), Block block_0 (torch.Size([256, 512])).
I0315 12:19:46.953162 139680868943040 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 12:19:46.953161 139882590454976 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 20.block_0 (torch.Size([0])) for Parameter 20 (torch.Size([512, 256, 2, 2])), Block block_0 (torch.Size([512, 1024])).
I0315 12:19:46.953168 140277216793792 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 5.block_0 (torch.Size([0])) for Parameter 5 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 12:19:46.953192 140718987674816 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 15.block_0 (torch.Size([64, 576])) for Parameter 15 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 12:19:46.953199 140508957750464 shampoo_preconditioner_list.py:615] Rank 6: ShampooPreconditionerList Bytes Breakdown: (663552, 8389256, 696320, 2686976, 2785280, 1310792, 1704008)
I0315 12:19:46.953203 140631626413248 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 23.block_0 (torch.Size([0])) for Parameter 23 (torch.Size([64, 32, 2, 2])), Block block_0 (torch.Size([64, 128])).
I0315 12:19:46.953204 139715604698304 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 22.block_0 (torch.Size([0])) for Parameter 22 (torch.Size([128, 64, 2, 2])), Block block_0 (torch.Size([128, 256])).
I0315 12:19:46.953211 139882590454976 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 21.block_0 (torch.Size([0])) for Parameter 21 (torch.Size([256, 128, 2, 2])), Block block_0 (torch.Size([256, 512])).
I0315 12:19:46.953220 140277216793792 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 6.block_0 (torch.Size([0])) for Parameter 6 (torch.Size([256, 128, 3, 3])), Block block_0 (torch.Size([256, 384, 3])).
I0315 12:19:46.953234 140508957750464 shampoo_preconditioner_list.py:618] Rank 6: ShampooPreconditionerList Total Elements: 19350298
I0315 12:19:46.953255 139715604698304 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 23.block_0 (torch.Size([0])) for Parameter 23 (torch.Size([64, 32, 2, 2])), Block block_0 (torch.Size([64, 128])).
I0315 12:19:46.953261 139882590454976 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 22.block_0 (torch.Size([0])) for Parameter 22 (torch.Size([128, 64, 2, 2])), Block block_0 (torch.Size([128, 256])).
I0315 12:19:46.953263 140718987674816 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 16.block_0 (torch.Size([0])) for Parameter 16 (torch.Size([32, 64, 3, 3])), Block block_0 (torch.Size([32, 576])).
I0315 12:19:46.953271 140508957750464 shampoo_preconditioner_list.py:621] Rank 6: ShampooPreconditionerList Total Bytes: 18236184
I0315 12:19:46.953271 140631626413248 shampoo_preconditioner_list.py:375] Rank 1: AdaGradPreconditionerList Numel Breakdown: (0, 0, 0, 0, 0, 0, 0, 0, 1179648, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)
I0315 12:19:46.953278 140277216793792 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 7.block_0 (torch.Size([0])) for Parameter 7 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 12:19:46.953306 140631626413248 shampoo_preconditioner_list.py:378] Rank 1: AdaGradPreconditionerList Bytes Breakdown: (0, 0, 0, 0, 0, 0, 0, 0, 4718592, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)
I0315 12:19:46.953312 139882590454976 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 23.block_0 (torch.Size([0])) for Parameter 23 (torch.Size([64, 32, 2, 2])), Block block_0 (torch.Size([64, 128])).
I0315 12:19:46.953330 139715604698304 shampoo_preconditioner_list.py:375] Rank 3: AdaGradPreconditionerList Numel Breakdown: (0, 0, 18432, 0, 0, 0, 0, 589824, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)
I0315 12:19:46.953343 140631626413248 shampoo_preconditioner_list.py:381] Rank 1: AdaGradPreconditionerList Total Elements: 1179648
I0315 12:19:46.953341 140277216793792 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 8.block_0 (torch.Size([0])) for Parameter 8 (torch.Size([512, 256, 3, 3])), Block block_0 (torch.Size([512, 768, 3])).
I0315 12:19:46.953341 140718987674816 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 17.block_0 (torch.Size([1024, 9])) for Parameter 17 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 12:19:46.953373 140631626413248 shampoo_preconditioner_list.py:384] Rank 1: AdaGradPreconditionerList Total Bytes: 4718592
I0315 12:19:46.953372 139715604698304 shampoo_preconditioner_list.py:378] Rank 3: AdaGradPreconditionerList Bytes Breakdown: (0, 0, 73728, 0, 0, 0, 0, 2359296, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)
I0315 12:19:46.953380 139882590454976 shampoo_preconditioner_list.py:375] Rank 2: AdaGradPreconditionerList Numel Breakdown: (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1179648, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)
I0315 12:19:46.953403 139715604698304 shampoo_preconditioner_list.py:381] Rank 3: AdaGradPreconditionerList Total Elements: 608256
I0315 12:19:46.953401 140718987674816 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 18.block_0 (torch.Size([0])) for Parameter 18 (torch.Size([1, 32, 1, 1])), Block block_0 (torch.Size([32])).
I0315 12:19:46.953414 139882590454976 shampoo_preconditioner_list.py:378] Rank 2: AdaGradPreconditionerList Bytes Breakdown: (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4718592, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)
I0315 12:19:46.953444 139882590454976 shampoo_preconditioner_list.py:381] Rank 2: AdaGradPreconditionerList Total Elements: 1179648
I0315 12:19:46.953447 139715604698304 shampoo_preconditioner_list.py:384] Rank 3: AdaGradPreconditionerList Total Bytes: 2433024
I0315 12:19:46.953451 140718987674816 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 19.block_0 (torch.Size([0])) for Parameter 19 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0315 12:19:46.953454 140277216793792 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 9.block_0 (torch.Size([512, 512, 9])) for Parameter 9 (torch.Size([512, 512, 3, 3])), Block block_0 (torch.Size([512, 512, 9])).
I0315 12:19:46.953481 139882590454976 shampoo_preconditioner_list.py:384] Rank 2: AdaGradPreconditionerList Total Bytes: 4718592
I0315 12:19:46.953522 140277216793792 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 10.block_0 (torch.Size([0])) for Parameter 10 (torch.Size([256, 512, 3, 3])), Block block_0 (torch.Size([256, 512, 9])).
I0315 12:19:46.953524 140718987674816 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 20.block_0 (torch.Size([512, 1024])) for Parameter 20 (torch.Size([512, 256, 2, 2])), Block block_0 (torch.Size([512, 1024])).
I0315 12:19:46.953516 140508957750464 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([288])) for Parameter 0 (torch.Size([32, 1, 3, 3])), Block block_0 (torch.Size([288])).
I0315 12:19:46.953638 140718987674816 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 21.block_0 (torch.Size([0])) for Parameter 21 (torch.Size([256, 128, 2, 2])), Block block_0 (torch.Size([256, 512])).
I0315 12:19:46.953624 139680868943040 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 4.block_0 (torch.Size([128, 576])) for Parameter 4 (torch.Size([128, 64, 3, 3])), Block block_0 (torch.Size([128, 576])).
I0315 12:19:46.953644 140277216793792 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 11.block_0 (torch.Size([0])) for Parameter 11 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 12:19:46.953655 140508957750464 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 12:19:46.953643 139667941848256 logger_utils.py:262] Unable to record workload.train_mean information. Continuing without it.
I0315 12:19:46.953705 140277216793792 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 12.block_0 (torch.Size([0])) for Parameter 12 (torch.Size([128, 256, 3, 3])), Block block_0 (torch.Size([128, 768, 3])).
I0315 12:19:46.953713 140718987674816 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 22.block_0 (torch.Size([0])) for Parameter 22 (torch.Size([128, 64, 2, 2])), Block block_0 (torch.Size([128, 256])).
I0315 12:19:46.953721 140508957750464 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([64, 32, 3, 3])), Block block_0 (torch.Size([64, 288])).
I0315 12:19:46.953734 139667941848256 logger_utils.py:262] Unable to record workload.train_stddev information. Continuing without it.
I0315 12:19:46.953728 139680868943040 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 5.block_0 (torch.Size([0])) for Parameter 5 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 12:19:46.953770 140718987674816 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 23.block_0 (torch.Size([0])) for Parameter 23 (torch.Size([64, 32, 2, 2])), Block block_0 (torch.Size([64, 128])).
I0315 12:19:46.953772 140277216793792 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 13.block_0 (torch.Size([0])) for Parameter 13 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 12:19:46.953776 140508957750464 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 12:19:46.953792 139680868943040 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 6.block_0 (torch.Size([0])) for Parameter 6 (torch.Size([256, 128, 3, 3])), Block block_0 (torch.Size([256, 384, 3])).
I0315 12:19:46.953827 140277216793792 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 14.block_0 (torch.Size([0])) for Parameter 14 (torch.Size([64, 128, 3, 3])), Block block_0 (torch.Size([64, 384, 3])).
I0315 12:19:46.953830 140508957750464 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 4.block_0 (torch.Size([0])) for Parameter 4 (torch.Size([128, 64, 3, 3])), Block block_0 (torch.Size([128, 576])).
I0315 12:19:46.953849 140718987674816 shampoo_preconditioner_list.py:375] Rank 5: AdaGradPreconditionerList Numel Breakdown: (0, 0, 0, 36864, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 36864, 0, 9216, 0, 0, 524288, 0, 0, 0)
I0315 12:19:46.953858 139680868943040 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 7.block_0 (torch.Size([0])) for Parameter 7 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 12:19:46.953885 140277216793792 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 15.block_0 (torch.Size([0])) for Parameter 15 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 12:19:46.953893 140718987674816 shampoo_preconditioner_list.py:378] Rank 5: AdaGradPreconditionerList Bytes Breakdown: (0, 0, 0, 147456, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 147456, 0, 36864, 0, 0, 2097152, 0, 0, 0)
I0315 12:19:46.953908 139680868943040 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 8.block_0 (torch.Size([0])) for Parameter 8 (torch.Size([512, 256, 3, 3])), Block block_0 (torch.Size([512, 768, 3])).
I0315 12:19:46.953925 140718987674816 shampoo_preconditioner_list.py:381] Rank 5: AdaGradPreconditionerList Total Elements: 607232
I0315 12:19:46.953944 140277216793792 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 16.block_0 (torch.Size([0])) for Parameter 16 (torch.Size([32, 64, 3, 3])), Block block_0 (torch.Size([32, 576])).
I0315 12:19:46.953957 140718987674816 shampoo_preconditioner_list.py:384] Rank 5: AdaGradPreconditionerList Total Bytes: 2428928
I0315 12:19:46.953964 139680868943040 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 9.block_0 (torch.Size([0])) for Parameter 9 (torch.Size([512, 512, 3, 3])), Block block_0 (torch.Size([512, 512, 9])).
I0315 12:19:46.954005 140277216793792 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 17.block_0 (torch.Size([0])) for Parameter 17 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 12:19:46.954011 139680868943040 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 10.block_0 (torch.Size([0])) for Parameter 10 (torch.Size([256, 512, 3, 3])), Block block_0 (torch.Size([256, 512, 9])).
I0315 12:19:46.954057 139680868943040 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 11.block_0 (torch.Size([0])) for Parameter 11 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 12:19:46.954066 140277216793792 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 18.block_0 (torch.Size([0])) for Parameter 18 (torch.Size([1, 32, 1, 1])), Block block_0 (torch.Size([32])).
I0315 12:19:46.954117 140277216793792 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 19.block_0 (torch.Size([0])) for Parameter 19 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0315 12:19:46.954149 139680868943040 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 12.block_0 (torch.Size([128, 768, 3])) for Parameter 12 (torch.Size([128, 256, 3, 3])), Block block_0 (torch.Size([128, 768, 3])).
I0315 12:19:46.954177 140277216793792 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 20.block_0 (torch.Size([0])) for Parameter 20 (torch.Size([512, 256, 2, 2])), Block block_0 (torch.Size([512, 1024])).
I0315 12:19:46.954236 140277216793792 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 21.block_0 (torch.Size([0])) for Parameter 21 (torch.Size([256, 128, 2, 2])), Block block_0 (torch.Size([256, 512])).
I0315 12:19:46.954236 139680868943040 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 13.block_0 (torch.Size([128, 384, 3])) for Parameter 13 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 12:19:46.954297 140277216793792 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 22.block_0 (torch.Size([0])) for Parameter 22 (torch.Size([128, 64, 2, 2])), Block block_0 (torch.Size([128, 256])).
I0315 12:19:46.954306 139680868943040 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 14.block_0 (torch.Size([64, 384, 3])) for Parameter 14 (torch.Size([64, 128, 3, 3])), Block block_0 (torch.Size([64, 384, 3])).
I0315 12:19:46.954332 140508957750464 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 5.block_0 (torch.Size([128, 384, 3])) for Parameter 5 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 12:19:46.954355 140277216793792 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 23.block_0 (torch.Size([0])) for Parameter 23 (torch.Size([64, 32, 2, 2])), Block block_0 (torch.Size([64, 128])).
I0315 12:19:46.954363 139680868943040 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 15.block_0 (torch.Size([0])) for Parameter 15 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 12:19:46.954409 139680868943040 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 16.block_0 (torch.Size([0])) for Parameter 16 (torch.Size([32, 64, 3, 3])), Block block_0 (torch.Size([32, 576])).
I0315 12:19:46.954427 140277216793792 shampoo_preconditioner_list.py:375] Rank 0: AdaGradPreconditionerList Numel Breakdown: (0, 0, 0, 0, 0, 0, 0, 0, 0, 2359296, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)
I0315 12:19:46.954464 139680868943040 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 17.block_0 (torch.Size([0])) for Parameter 17 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 12:19:46.954464 140277216793792 shampoo_preconditioner_list.py:378] Rank 0: AdaGradPreconditionerList Bytes Breakdown: (0, 0, 0, 0, 0, 0, 0, 0, 0, 9437184, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)
I0315 12:19:46.954474 140508957750464 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 6.block_0 (torch.Size([256, 384, 3])) for Parameter 6 (torch.Size([256, 128, 3, 3])), Block block_0 (torch.Size([256, 384, 3])).
I0315 12:19:46.954498 140277216793792 shampoo_preconditioner_list.py:381] Rank 0: AdaGradPreconditionerList Total Elements: 2359296
I0315 12:19:46.954518 139680868943040 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 18.block_0 (torch.Size([0])) for Parameter 18 (torch.Size([1, 32, 1, 1])), Block block_0 (torch.Size([32])).
I0315 12:19:46.954529 140277216793792 shampoo_preconditioner_list.py:384] Rank 0: AdaGradPreconditionerList Total Bytes: 9437184
I0315 12:19:46.954570 140508957750464 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 7.block_0 (torch.Size([0])) for Parameter 7 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 12:19:46.954576 139680868943040 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 19.block_0 (torch.Size([0])) for Parameter 19 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0315 12:19:46.954624 139680868943040 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 20.block_0 (torch.Size([0])) for Parameter 20 (torch.Size([512, 256, 2, 2])), Block block_0 (torch.Size([512, 1024])).
I0315 12:19:46.954637 140508957750464 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 8.block_0 (torch.Size([0])) for Parameter 8 (torch.Size([512, 256, 3, 3])), Block block_0 (torch.Size([512, 768, 3])).
I0315 12:19:46.954678 139680868943040 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 21.block_0 (torch.Size([0])) for Parameter 21 (torch.Size([256, 128, 2, 2])), Block block_0 (torch.Size([256, 512])).
I0315 12:19:46.954693 140508957750464 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 9.block_0 (torch.Size([0])) for Parameter 9 (torch.Size([512, 512, 3, 3])), Block block_0 (torch.Size([512, 512, 9])).
I0315 12:19:46.954675 139882590454976 logger_utils.py:262] Unable to record workload.train_mean information. Continuing without it.
I0315 12:19:46.954730 139680868943040 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 22.block_0 (torch.Size([0])) for Parameter 22 (torch.Size([128, 64, 2, 2])), Block block_0 (torch.Size([128, 256])).
I0315 12:19:46.954715 139715604698304 logger_utils.py:262] Unable to record workload.train_mean information. Continuing without it.
I0315 12:19:46.954746 140508957750464 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 10.block_0 (torch.Size([0])) for Parameter 10 (torch.Size([256, 512, 3, 3])), Block block_0 (torch.Size([256, 512, 9])).
I0315 12:19:46.954764 139882590454976 logger_utils.py:262] Unable to record workload.train_stddev information. Continuing without it.
I0315 12:19:46.954782 139715604698304 logger_utils.py:262] Unable to record workload.train_stddev information. Continuing without it.
I0315 12:19:46.954799 140508957750464 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 11.block_0 (torch.Size([0])) for Parameter 11 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 12:19:46.954807 139680868943040 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 23.block_0 (torch.Size([64, 128])) for Parameter 23 (torch.Size([64, 32, 2, 2])), Block block_0 (torch.Size([64, 128])).
I0315 12:19:46.954859 140508957750464 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 12.block_0 (torch.Size([0])) for Parameter 12 (torch.Size([128, 256, 3, 3])), Block block_0 (torch.Size([128, 768, 3])).
I0315 12:19:46.954879 139680868943040 shampoo_preconditioner_list.py:375] Rank 7: AdaGradPreconditionerList Numel Breakdown: (0, 9216, 0, 0, 73728, 0, 0, 0, 0, 0, 0, 0, 294912, 147456, 73728, 0, 0, 0, 0, 0, 0, 0, 0, 8192)
I0315 12:19:46.954911 139680868943040 shampoo_preconditioner_list.py:378] Rank 7: AdaGradPreconditionerList Bytes Breakdown: (0, 36864, 0, 0, 294912, 0, 0, 0, 0, 0, 0, 0, 1179648, 589824, 294912, 0, 0, 0, 0, 0, 0, 0, 0, 32768)
I0315 12:19:46.954928 140508957750464 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 13.block_0 (torch.Size([0])) for Parameter 13 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 12:19:46.954948 139680868943040 shampoo_preconditioner_list.py:381] Rank 7: AdaGradPreconditionerList Total Elements: 607232
I0315 12:19:46.954981 139680868943040 shampoo_preconditioner_list.py:384] Rank 7: AdaGradPreconditionerList Total Bytes: 2428928
I0315 12:19:46.954984 140508957750464 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 14.block_0 (torch.Size([0])) for Parameter 14 (torch.Size([64, 128, 3, 3])), Block block_0 (torch.Size([64, 384, 3])).
I0315 12:19:46.955036 140508957750464 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 15.block_0 (torch.Size([0])) for Parameter 15 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 12:19:46.955016 140631626413248 logger_utils.py:262] Unable to record workload.train_mean information. Continuing without it.
I0315 12:19:46.955085 140631626413248 logger_utils.py:262] Unable to record workload.train_stddev information. Continuing without it.
I0315 12:19:46.955095 140508957750464 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 16.block_0 (torch.Size([0])) for Parameter 16 (torch.Size([32, 64, 3, 3])), Block block_0 (torch.Size([32, 576])).
I0315 12:19:46.955146 140508957750464 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 17.block_0 (torch.Size([0])) for Parameter 17 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 12:19:46.955157 140718987674816 logger_utils.py:262] Unable to record workload.train_mean information. Continuing without it.
I0315 12:19:46.955235 140718987674816 logger_utils.py:262] Unable to record workload.train_stddev information. Continuing without it.
I0315 12:19:46.955241 140508957750464 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 18.block_0 (torch.Size([32])) for Parameter 18 (torch.Size([1, 32, 1, 1])), Block block_0 (torch.Size([32])).
I0315 12:19:46.955323 140508957750464 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 19.block_0 (torch.Size([1])) for Parameter 19 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0315 12:19:46.955396 140508957750464 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 20.block_0 (torch.Size([0])) for Parameter 20 (torch.Size([512, 256, 2, 2])), Block block_0 (torch.Size([512, 1024])).
I0315 12:19:46.955377 140277216793792 submission.py:142] No large parameters detected! Continuing with only Shampoo....
I0315 12:19:46.955484 140508957750464 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 21.block_0 (torch.Size([256, 512])) for Parameter 21 (torch.Size([256, 128, 2, 2])), Block block_0 (torch.Size([256, 512])).
I0315 12:19:46.955572 140508957750464 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 22.block_0 (torch.Size([128, 256])) for Parameter 22 (torch.Size([128, 64, 2, 2])), Block block_0 (torch.Size([128, 256])).
I0315 12:19:46.955581 140277216793792 submission_runner.py:279] Initializing metrics bundle.
I0315 12:19:46.955647 140508957750464 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 23.block_0 (torch.Size([0])) for Parameter 23 (torch.Size([64, 32, 2, 2])), Block block_0 (torch.Size([64, 128])).
I0315 12:19:46.955719 140277216793792 submission_runner.py:301] Initializing checkpoint and logger.
I0315 12:19:46.955730 140508957750464 shampoo_preconditioner_list.py:375] Rank 6: AdaGradPreconditionerList Numel Breakdown: (288, 0, 0, 0, 0, 147456, 294912, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 32, 1, 0, 131072, 32768, 0)
I0315 12:19:46.955768 140508957750464 shampoo_preconditioner_list.py:378] Rank 6: AdaGradPreconditionerList Bytes Breakdown: (1152, 0, 0, 0, 0, 589824, 1179648, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 128, 4, 0, 524288, 131072, 0)
I0315 12:19:46.955806 140508957750464 shampoo_preconditioner_list.py:381] Rank 6: AdaGradPreconditionerList Total Elements: 606529
I0315 12:19:46.955843 140508957750464 shampoo_preconditioner_list.py:384] Rank 6: AdaGradPreconditionerList Total Bytes: 2426116
I0315 12:19:46.956154 140277216793792 submission_runner.py:321] Saving meta data to /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_0/fastmri_pytorch/trial_1/meta_data_0.json.
I0315 12:19:46.956347 140277216793792 logger_utils.py:262] Unable to record workload.train_mean information. Continuing without it.
I0315 12:19:46.956397 140277216793792 logger_utils.py:262] Unable to record workload.train_stddev information. Continuing without it.
I0315 12:19:46.956580 139680868943040 logger_utils.py:262] Unable to record workload.train_mean information. Continuing without it.
I0315 12:19:46.956655 139680868943040 logger_utils.py:262] Unable to record workload.train_stddev information. Continuing without it.
I0315 12:19:46.957549 140508957750464 logger_utils.py:262] Unable to record workload.train_mean information. Continuing without it.
I0315 12:19:46.957645 140508957750464 logger_utils.py:262] Unable to record workload.train_stddev information. Continuing without it.
I0315 12:19:47.736996 140277216793792 submission_runner.py:325] Saving flags to /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_0/fastmri_pytorch/trial_1/flags_0.json.
I0315 12:19:47.847359 140277216793792 submission_runner.py:337] Starting training loop.
[rank7]:W0315 12:19:47.983000 51 site-packages/torch/_logging/_internal.py:1081] [0/0] Profiler function <class 'torch.autograd.profiler.record_function'> will be ignored
[rank5]:W0315 12:19:47.984000 49 site-packages/torch/_logging/_internal.py:1081] [0/0] Profiler function <class 'torch.autograd.profiler.record_function'> will be ignored
[rank2]:W0315 12:19:47.984000 46 site-packages/torch/_logging/_internal.py:1081] [0/0] Profiler function <class 'torch.autograd.profiler.record_function'> will be ignored
[rank6]:W0315 12:19:47.984000 50 site-packages/torch/_logging/_internal.py:1081] [0/0] Profiler function <class 'torch.autograd.profiler.record_function'> will be ignored
[rank4]:W0315 12:19:47.984000 48 site-packages/torch/_logging/_internal.py:1081] [0/0] Profiler function <class 'torch.autograd.profiler.record_function'> will be ignored
[rank1]:W0315 12:19:47.984000 45 site-packages/torch/_logging/_internal.py:1081] [0/0] Profiler function <class 'torch.autograd.profiler.record_function'> will be ignored
[rank3]:W0315 12:19:47.984000 47 site-packages/torch/_logging/_internal.py:1081] [0/0] Profiler function <class 'torch.autograd.profiler.record_function'> will be ignored
[rank0]:W0315 12:23:57.538000 44 site-packages/torch/_logging/_internal.py:1081] [0/0] Profiler function <class 'torch.autograd.profiler.record_function'> will be ignored
[rank4]:W0315 12:24:46.594000 48 site-packages/torch/_dynamo/convert_frame.py:844] [2/8] torch._dynamo hit config.cache_size_limit (8)
[rank4]:W0315 12:24:46.594000 48 site-packages/torch/_dynamo/convert_frame.py:844] [2/8]    function: 'forward' (/algorithmic-efficiency/algoperf/workloads/fastmri/fastmri_pytorch/models.py:141)
[rank4]:W0315 12:24:46.594000 48 site-packages/torch/_dynamo/convert_frame.py:844] [2/8]    last reason: 2/0: tensor 'L['x']' requires_grad mismatch. expected requires_grad=0
[rank4]:W0315 12:24:46.594000 48 site-packages/torch/_dynamo/convert_frame.py:844] [2/8] To log all recompilation reasons, use TORCH_LOGS="recompiles".
[rank4]:W0315 12:24:46.594000 48 site-packages/torch/_dynamo/convert_frame.py:844] [2/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.
[rank6]:W0315 12:24:46.594000 50 site-packages/torch/_dynamo/convert_frame.py:844] [2/8] torch._dynamo hit config.cache_size_limit (8)
[rank6]:W0315 12:24:46.594000 50 site-packages/torch/_dynamo/convert_frame.py:844] [2/8]    function: 'forward' (/algorithmic-efficiency/algoperf/workloads/fastmri/fastmri_pytorch/models.py:141)
[rank6]:W0315 12:24:46.594000 50 site-packages/torch/_dynamo/convert_frame.py:844] [2/8]    last reason: 2/0: tensor 'L['x']' requires_grad mismatch. expected requires_grad=0
[rank6]:W0315 12:24:46.594000 50 site-packages/torch/_dynamo/convert_frame.py:844] [2/8] To log all recompilation reasons, use TORCH_LOGS="recompiles".
[rank6]:W0315 12:24:46.594000 50 site-packages/torch/_dynamo/convert_frame.py:844] [2/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.
[rank2]:W0315 12:24:46.612000 46 site-packages/torch/_dynamo/convert_frame.py:844] [2/8] torch._dynamo hit config.cache_size_limit (8)
[rank2]:W0315 12:24:46.612000 46 site-packages/torch/_dynamo/convert_frame.py:844] [2/8]    function: 'forward' (/algorithmic-efficiency/algoperf/workloads/fastmri/fastmri_pytorch/models.py:141)
[rank2]:W0315 12:24:46.612000 46 site-packages/torch/_dynamo/convert_frame.py:844] [2/8]    last reason: 2/0: tensor 'L['x']' requires_grad mismatch. expected requires_grad=0
[rank2]:W0315 12:24:46.612000 46 site-packages/torch/_dynamo/convert_frame.py:844] [2/8] To log all recompilation reasons, use TORCH_LOGS="recompiles".
[rank2]:W0315 12:24:46.612000 46 site-packages/torch/_dynamo/convert_frame.py:844] [2/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.
[rank7]:W0315 12:24:46.612000 51 site-packages/torch/_dynamo/convert_frame.py:844] [2/8] torch._dynamo hit config.cache_size_limit (8)
[rank7]:W0315 12:24:46.612000 51 site-packages/torch/_dynamo/convert_frame.py:844] [2/8]    function: 'forward' (/algorithmic-efficiency/algoperf/workloads/fastmri/fastmri_pytorch/models.py:141)
[rank7]:W0315 12:24:46.612000 51 site-packages/torch/_dynamo/convert_frame.py:844] [2/8]    last reason: 2/0: tensor 'L['x']' requires_grad mismatch. expected requires_grad=0
[rank7]:W0315 12:24:46.612000 51 site-packages/torch/_dynamo/convert_frame.py:844] [2/8] To log all recompilation reasons, use TORCH_LOGS="recompiles".
[rank7]:W0315 12:24:46.612000 51 site-packages/torch/_dynamo/convert_frame.py:844] [2/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.
[rank5]:W0315 12:24:46.625000 49 site-packages/torch/_dynamo/convert_frame.py:844] [2/8] torch._dynamo hit config.cache_size_limit (8)
[rank5]:W0315 12:24:46.625000 49 site-packages/torch/_dynamo/convert_frame.py:844] [2/8]    function: 'forward' (/algorithmic-efficiency/algoperf/workloads/fastmri/fastmri_pytorch/models.py:141)
[rank5]:W0315 12:24:46.625000 49 site-packages/torch/_dynamo/convert_frame.py:844] [2/8]    last reason: 2/0: tensor 'L['x']' requires_grad mismatch. expected requires_grad=0
[rank5]:W0315 12:24:46.625000 49 site-packages/torch/_dynamo/convert_frame.py:844] [2/8] To log all recompilation reasons, use TORCH_LOGS="recompiles".
[rank5]:W0315 12:24:46.625000 49 site-packages/torch/_dynamo/convert_frame.py:844] [2/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.
[rank1]:W0315 12:24:46.751000 45 site-packages/torch/_dynamo/convert_frame.py:844] [2/8] torch._dynamo hit config.cache_size_limit (8)
[rank1]:W0315 12:24:46.751000 45 site-packages/torch/_dynamo/convert_frame.py:844] [2/8]    function: 'forward' (/algorithmic-efficiency/algoperf/workloads/fastmri/fastmri_pytorch/models.py:141)
[rank1]:W0315 12:24:46.751000 45 site-packages/torch/_dynamo/convert_frame.py:844] [2/8]    last reason: 2/0: tensor 'L['x']' requires_grad mismatch. expected requires_grad=0
[rank1]:W0315 12:24:46.751000 45 site-packages/torch/_dynamo/convert_frame.py:844] [2/8] To log all recompilation reasons, use TORCH_LOGS="recompiles".
[rank1]:W0315 12:24:46.751000 45 site-packages/torch/_dynamo/convert_frame.py:844] [2/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.
[rank3]:W0315 12:24:46.752000 47 site-packages/torch/_dynamo/convert_frame.py:844] [2/8] torch._dynamo hit config.cache_size_limit (8)
[rank3]:W0315 12:24:46.752000 47 site-packages/torch/_dynamo/convert_frame.py:844] [2/8]    function: 'forward' (/algorithmic-efficiency/algoperf/workloads/fastmri/fastmri_pytorch/models.py:141)
[rank3]:W0315 12:24:46.752000 47 site-packages/torch/_dynamo/convert_frame.py:844] [2/8]    last reason: 2/0: tensor 'L['x']' requires_grad mismatch. expected requires_grad=0
[rank3]:W0315 12:24:46.752000 47 site-packages/torch/_dynamo/convert_frame.py:844] [2/8] To log all recompilation reasons, use TORCH_LOGS="recompiles".
[rank3]:W0315 12:24:46.752000 47 site-packages/torch/_dynamo/convert_frame.py:844] [2/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.
[rank0]:W0315 12:24:46.897000 44 site-packages/torch/_dynamo/convert_frame.py:844] [2/8] torch._dynamo hit config.cache_size_limit (8)
[rank0]:W0315 12:24:46.897000 44 site-packages/torch/_dynamo/convert_frame.py:844] [2/8]    function: 'forward' (/algorithmic-efficiency/algoperf/workloads/fastmri/fastmri_pytorch/models.py:141)
[rank0]:W0315 12:24:46.897000 44 site-packages/torch/_dynamo/convert_frame.py:844] [2/8]    last reason: 2/0: tensor 'L['x']' requires_grad mismatch. expected requires_grad=0
[rank0]:W0315 12:24:46.897000 44 site-packages/torch/_dynamo/convert_frame.py:844] [2/8] To log all recompilation reasons, use TORCH_LOGS="recompiles".
[rank0]:W0315 12:24:46.897000 44 site-packages/torch/_dynamo/convert_frame.py:844] [2/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.
I0315 12:25:02.716985 140248479500032 logging_writer.py:48] [0] global_step=0, grad_norm=4.68282, loss=0.991998
I0315 12:25:03.123783 140277216793792 submission.py:265] 0) loss = 0.992, grad_norm = 4.683
I0315 12:25:03.895418 140277216793792 spec.py:321] Evaluating on the training split.
I0315 12:32:45.250177 140277216793792 spec.py:333] Evaluating on the validation split.
I0315 12:37:04.248796 140277216793792 spec.py:349] Evaluating on the test split.
I0315 12:41:22.914072 140277216793792 submission_runner.py:469] Time since start: 1295.07s, 	Step: 1, 	{'train/ssim': 0.20035191944667272, 'train/loss': 1.0021626608712333, 'validation/ssim': 0.18794720187640687, 'validation/loss': 1.0060458121570766, 'validation/num_examples': 3554, 'test/ssim': 0.21181421777916434, 'test/loss': 1.0037169915875455, 'test/num_examples': 3581, 'score': 315.27763319015503, 'total_duration': 1295.066995382309, 'accumulated_submission_time': 315.27763319015503, 'accumulated_eval_time': 979.0199332237244, 'accumulated_logging_time': 0}
I0315 12:41:22.922326 140225061820160 logging_writer.py:48] [1] accumulated_eval_time=979.02, accumulated_logging_time=0, accumulated_submission_time=315.278, global_step=1, preemption_count=0, score=315.278, test/loss=1.00372, test/num_examples=3581, test/ssim=0.211814, total_duration=1295.07, train/loss=1.00216, train/ssim=0.200352, validation/loss=1.00605, validation/num_examples=3554, validation/ssim=0.187947
I0315 12:41:24.091875 140224977958656 logging_writer.py:48] [1] global_step=1, grad_norm=3.94851, loss=0.968388
I0315 12:41:24.095329 140277216793792 submission.py:265] 1) loss = 0.968, grad_norm = 3.949
I0315 12:41:24.201935 140225061820160 logging_writer.py:48] [2] global_step=2, grad_norm=3.86369, loss=0.99932
I0315 12:41:24.208705 140277216793792 submission.py:265] 2) loss = 0.999, grad_norm = 3.864
I0315 12:41:24.304578 140224977958656 logging_writer.py:48] [3] global_step=3, grad_norm=4.22298, loss=1.02408
I0315 12:41:24.309360 140277216793792 submission.py:265] 3) loss = 1.024, grad_norm = 4.223
I0315 12:41:24.415390 140225061820160 logging_writer.py:48] [4] global_step=4, grad_norm=4.48622, loss=0.992127
I0315 12:41:24.420001 140277216793792 submission.py:265] 4) loss = 0.992, grad_norm = 4.486
I0315 12:41:24.513914 140224977958656 logging_writer.py:48] [5] global_step=5, grad_norm=4.25819, loss=0.898242
I0315 12:41:24.518204 140277216793792 submission.py:265] 5) loss = 0.898, grad_norm = 4.258
I0315 12:41:24.608845 140225061820160 logging_writer.py:48] [6] global_step=6, grad_norm=3.85507, loss=0.915696
I0315 12:41:24.613793 140277216793792 submission.py:265] 6) loss = 0.916, grad_norm = 3.855
I0315 12:41:24.712629 140224977958656 logging_writer.py:48] [7] global_step=7, grad_norm=4.29922, loss=0.869602
I0315 12:41:24.717279 140277216793792 submission.py:265] 7) loss = 0.870, grad_norm = 4.299
I0315 12:41:24.809695 140225061820160 logging_writer.py:48] [8] global_step=8, grad_norm=4.6779, loss=0.927826
I0315 12:41:24.817419 140277216793792 submission.py:265] 8) loss = 0.928, grad_norm = 4.678
I0315 12:41:24.894959 140224977958656 logging_writer.py:48] [9] global_step=9, grad_norm=4.50876, loss=0.964721
I0315 12:41:24.899785 140277216793792 submission.py:265] 9) loss = 0.965, grad_norm = 4.509
I0315 12:41:24.983728 140225061820160 logging_writer.py:48] [10] global_step=10, grad_norm=4.06844, loss=0.897337
I0315 12:41:24.988131 140277216793792 submission.py:265] 10) loss = 0.897, grad_norm = 4.068
I0315 12:41:25.091424 140224977958656 logging_writer.py:48] [11] global_step=11, grad_norm=4.28743, loss=0.762187
I0315 12:41:25.099432 140277216793792 submission.py:265] 11) loss = 0.762, grad_norm = 4.287
I0315 12:41:25.200725 140225061820160 logging_writer.py:48] [12] global_step=12, grad_norm=3.38792, loss=0.699376
I0315 12:41:25.207599 140277216793792 submission.py:265] 12) loss = 0.699, grad_norm = 3.388
I0315 12:41:25.303526 140224977958656 logging_writer.py:48] [13] global_step=13, grad_norm=3.13949, loss=0.721725
I0315 12:41:25.314332 140277216793792 submission.py:265] 13) loss = 0.722, grad_norm = 3.139
I0315 12:41:25.403582 140225061820160 logging_writer.py:48] [14] global_step=14, grad_norm=2.94771, loss=0.787869
I0315 12:41:25.407904 140277216793792 submission.py:265] 14) loss = 0.788, grad_norm = 2.948
I0315 12:41:25.482592 140224977958656 logging_writer.py:48] [15] global_step=15, grad_norm=3.11662, loss=0.672309
I0315 12:41:25.488191 140277216793792 submission.py:265] 15) loss = 0.672, grad_norm = 3.117
I0315 12:41:25.576277 140225061820160 logging_writer.py:48] [16] global_step=16, grad_norm=3.00168, loss=0.710866
I0315 12:41:25.584736 140277216793792 submission.py:265] 16) loss = 0.711, grad_norm = 3.002
I0315 12:41:25.664561 140224977958656 logging_writer.py:48] [17] global_step=17, grad_norm=2.53234, loss=0.69964
I0315 12:41:25.668739 140277216793792 submission.py:265] 17) loss = 0.700, grad_norm = 2.532
I0315 12:41:25.738452 140225061820160 logging_writer.py:48] [18] global_step=18, grad_norm=2.31261, loss=0.617259
I0315 12:41:25.742924 140277216793792 submission.py:265] 18) loss = 0.617, grad_norm = 2.313
I0315 12:41:25.843615 140224977958656 logging_writer.py:48] [19] global_step=19, grad_norm=1.4305, loss=0.685748
I0315 12:41:25.848940 140277216793792 submission.py:265] 19) loss = 0.686, grad_norm = 1.431
I0315 12:41:25.928997 140225061820160 logging_writer.py:48] [20] global_step=20, grad_norm=1.51615, loss=0.60092
I0315 12:41:25.937351 140277216793792 submission.py:265] 20) loss = 0.601, grad_norm = 1.516
I0315 12:41:26.026511 140224977958656 logging_writer.py:48] [21] global_step=21, grad_norm=1.23314, loss=0.570071
I0315 12:41:26.033376 140277216793792 submission.py:265] 21) loss = 0.570, grad_norm = 1.233
I0315 12:41:26.119942 140225061820160 logging_writer.py:48] [22] global_step=22, grad_norm=1.21844, loss=0.591311
I0315 12:41:26.124199 140277216793792 submission.py:265] 22) loss = 0.591, grad_norm = 1.218
I0315 12:41:26.219363 140224977958656 logging_writer.py:48] [23] global_step=23, grad_norm=0.887818, loss=0.559194
I0315 12:41:26.225686 140277216793792 submission.py:265] 23) loss = 0.559, grad_norm = 0.888
I0315 12:41:26.317985 140225061820160 logging_writer.py:48] [24] global_step=24, grad_norm=1.07954, loss=0.672944
I0315 12:41:26.327569 140277216793792 submission.py:265] 24) loss = 0.673, grad_norm = 1.080
I0315 12:41:26.416068 140224977958656 logging_writer.py:48] [25] global_step=25, grad_norm=0.947699, loss=0.510216
I0315 12:41:26.420987 140277216793792 submission.py:265] 25) loss = 0.510, grad_norm = 0.948
I0315 12:41:26.500308 140225061820160 logging_writer.py:48] [26] global_step=26, grad_norm=1.08857, loss=0.531857
I0315 12:41:26.505686 140277216793792 submission.py:265] 26) loss = 0.532, grad_norm = 1.089
I0315 12:41:26.574263 140224977958656 logging_writer.py:48] [27] global_step=27, grad_norm=1.15728, loss=0.492162
I0315 12:41:26.577937 140277216793792 submission.py:265] 27) loss = 0.492, grad_norm = 1.157
I0315 12:41:26.655157 140225061820160 logging_writer.py:48] [28] global_step=28, grad_norm=1.18738, loss=0.529555
I0315 12:41:26.659437 140277216793792 submission.py:265] 28) loss = 0.530, grad_norm = 1.187
I0315 12:41:26.742453 140224977958656 logging_writer.py:48] [29] global_step=29, grad_norm=1.14079, loss=0.534679
I0315 12:41:26.747903 140277216793792 submission.py:265] 29) loss = 0.535, grad_norm = 1.141
I0315 12:41:26.831189 140225061820160 logging_writer.py:48] [30] global_step=30, grad_norm=1.07176, loss=0.547732
I0315 12:41:26.835369 140277216793792 submission.py:265] 30) loss = 0.548, grad_norm = 1.072
I0315 12:41:26.910331 140224977958656 logging_writer.py:48] [31] global_step=31, grad_norm=1.1465, loss=0.58483
I0315 12:41:26.916935 140277216793792 submission.py:265] 31) loss = 0.585, grad_norm = 1.146
I0315 12:41:26.998486 140225061820160 logging_writer.py:48] [32] global_step=32, grad_norm=1.36142, loss=0.609933
I0315 12:41:27.005213 140277216793792 submission.py:265] 32) loss = 0.610, grad_norm = 1.361
I0315 12:41:27.098416 140224977958656 logging_writer.py:48] [33] global_step=33, grad_norm=1.52235, loss=0.601033
I0315 12:41:27.103326 140277216793792 submission.py:265] 33) loss = 0.601, grad_norm = 1.522
I0315 12:41:27.200104 140225061820160 logging_writer.py:48] [34] global_step=34, grad_norm=1.55472, loss=0.471484
I0315 12:41:27.206418 140277216793792 submission.py:265] 34) loss = 0.471, grad_norm = 1.555
I0315 12:41:27.286054 140224977958656 logging_writer.py:48] [35] global_step=35, grad_norm=1.07851, loss=0.475917
I0315 12:41:27.291545 140277216793792 submission.py:265] 35) loss = 0.476, grad_norm = 1.079
I0315 12:41:27.372958 140225061820160 logging_writer.py:48] [36] global_step=36, grad_norm=0.867004, loss=0.507398
I0315 12:41:27.377195 140277216793792 submission.py:265] 36) loss = 0.507, grad_norm = 0.867
I0315 12:41:27.458248 140224977958656 logging_writer.py:48] [37] global_step=37, grad_norm=0.916022, loss=0.516147
I0315 12:41:27.463571 140277216793792 submission.py:265] 37) loss = 0.516, grad_norm = 0.916
I0315 12:41:27.553666 140225061820160 logging_writer.py:48] [38] global_step=38, grad_norm=0.976635, loss=0.545682
I0315 12:41:27.558221 140277216793792 submission.py:265] 38) loss = 0.546, grad_norm = 0.977
I0315 12:41:27.633387 140224977958656 logging_writer.py:48] [39] global_step=39, grad_norm=0.679723, loss=0.501895
I0315 12:41:27.638567 140277216793792 submission.py:265] 39) loss = 0.502, grad_norm = 0.680
I0315 12:41:27.710048 140225061820160 logging_writer.py:48] [40] global_step=40, grad_norm=0.843987, loss=0.427932
I0315 12:41:27.716668 140277216793792 submission.py:265] 40) loss = 0.428, grad_norm = 0.844
I0315 12:41:27.792167 140224977958656 logging_writer.py:48] [41] global_step=41, grad_norm=0.840807, loss=0.423751
I0315 12:41:27.797000 140277216793792 submission.py:265] 41) loss = 0.424, grad_norm = 0.841
I0315 12:41:27.875161 140225061820160 logging_writer.py:48] [42] global_step=42, grad_norm=0.782689, loss=0.424608
I0315 12:41:27.881654 140277216793792 submission.py:265] 42) loss = 0.425, grad_norm = 0.783
I0315 12:41:27.955696 140224977958656 logging_writer.py:48] [43] global_step=43, grad_norm=0.740802, loss=0.425825
I0315 12:41:27.960266 140277216793792 submission.py:265] 43) loss = 0.426, grad_norm = 0.741
I0315 12:41:28.036015 140225061820160 logging_writer.py:48] [44] global_step=44, grad_norm=0.79688, loss=0.446441
I0315 12:41:28.040835 140277216793792 submission.py:265] 44) loss = 0.446, grad_norm = 0.797
I0315 12:41:28.118706 140224977958656 logging_writer.py:48] [45] global_step=45, grad_norm=0.766187, loss=0.500337
I0315 12:41:28.125200 140277216793792 submission.py:265] 45) loss = 0.500, grad_norm = 0.766
I0315 12:41:28.195928 140225061820160 logging_writer.py:48] [46] global_step=46, grad_norm=0.731032, loss=0.437827
I0315 12:41:28.200402 140277216793792 submission.py:265] 46) loss = 0.438, grad_norm = 0.731
I0315 12:41:28.276506 140224977958656 logging_writer.py:48] [47] global_step=47, grad_norm=0.755375, loss=0.398831
I0315 12:41:28.280910 140277216793792 submission.py:265] 47) loss = 0.399, grad_norm = 0.755
I0315 12:41:28.368623 140225061820160 logging_writer.py:48] [48] global_step=48, grad_norm=1.09705, loss=0.474785
I0315 12:41:28.375493 140277216793792 submission.py:265] 48) loss = 0.475, grad_norm = 1.097
I0315 12:41:28.450485 140224977958656 logging_writer.py:48] [49] global_step=49, grad_norm=0.745076, loss=0.358744
I0315 12:41:28.454791 140277216793792 submission.py:265] 49) loss = 0.359, grad_norm = 0.745
I0315 12:41:28.539911 140225061820160 logging_writer.py:48] [50] global_step=50, grad_norm=0.904059, loss=0.353958
I0315 12:41:28.544366 140277216793792 submission.py:265] 50) loss = 0.354, grad_norm = 0.904
I0315 12:41:28.618348 140224977958656 logging_writer.py:48] [51] global_step=51, grad_norm=0.902265, loss=0.414652
I0315 12:41:28.622557 140277216793792 submission.py:265] 51) loss = 0.415, grad_norm = 0.902
I0315 12:41:28.713196 140225061820160 logging_writer.py:48] [52] global_step=52, grad_norm=0.70843, loss=0.426146
I0315 12:41:28.717705 140277216793792 submission.py:265] 52) loss = 0.426, grad_norm = 0.708
I0315 12:41:28.959666 140224977958656 logging_writer.py:48] [53] global_step=53, grad_norm=0.886656, loss=0.355552
I0315 12:41:28.964828 140277216793792 submission.py:265] 53) loss = 0.356, grad_norm = 0.887
I0315 12:41:29.293964 140225061820160 logging_writer.py:48] [54] global_step=54, grad_norm=0.605275, loss=0.423639
I0315 12:41:29.297817 140277216793792 submission.py:265] 54) loss = 0.424, grad_norm = 0.605
I0315 12:41:29.573727 140224977958656 logging_writer.py:48] [55] global_step=55, grad_norm=0.815732, loss=0.375042
I0315 12:41:29.584372 140277216793792 submission.py:265] 55) loss = 0.375, grad_norm = 0.816
I0315 12:41:29.960729 140225061820160 logging_writer.py:48] [56] global_step=56, grad_norm=0.684145, loss=0.329185
I0315 12:41:29.965874 140277216793792 submission.py:265] 56) loss = 0.329, grad_norm = 0.684
I0315 12:41:30.186583 140224977958656 logging_writer.py:48] [57] global_step=57, grad_norm=0.898618, loss=0.343783
I0315 12:41:30.191921 140277216793792 submission.py:265] 57) loss = 0.344, grad_norm = 0.899
I0315 12:41:30.409584 140225061820160 logging_writer.py:48] [58] global_step=58, grad_norm=0.647352, loss=0.320642
I0315 12:41:30.414807 140277216793792 submission.py:265] 58) loss = 0.321, grad_norm = 0.647
I0315 12:41:30.650773 140224977958656 logging_writer.py:48] [59] global_step=59, grad_norm=0.682172, loss=0.401428
I0315 12:41:30.657075 140277216793792 submission.py:265] 59) loss = 0.401, grad_norm = 0.682
I0315 12:41:30.908684 140225061820160 logging_writer.py:48] [60] global_step=60, grad_norm=0.695052, loss=0.307804
I0315 12:41:30.915005 140277216793792 submission.py:265] 60) loss = 0.308, grad_norm = 0.695
I0315 12:41:31.140608 140224977958656 logging_writer.py:48] [61] global_step=61, grad_norm=0.724118, loss=0.343901
I0315 12:41:31.148339 140277216793792 submission.py:265] 61) loss = 0.344, grad_norm = 0.724
I0315 12:41:31.646182 140225061820160 logging_writer.py:48] [62] global_step=62, grad_norm=1.13922, loss=0.389096
I0315 12:41:31.652104 140277216793792 submission.py:265] 62) loss = 0.389, grad_norm = 1.139
I0315 12:41:32.014883 140224977958656 logging_writer.py:48] [63] global_step=63, grad_norm=0.50278, loss=0.365533
I0315 12:41:32.020148 140277216793792 submission.py:265] 63) loss = 0.366, grad_norm = 0.503
I0315 12:41:32.570108 140225061820160 logging_writer.py:48] [64] global_step=64, grad_norm=0.803036, loss=0.32929
I0315 12:41:32.575055 140277216793792 submission.py:265] 64) loss = 0.329, grad_norm = 0.803
I0315 12:41:32.841070 140224977958656 logging_writer.py:48] [65] global_step=65, grad_norm=1.01949, loss=0.381458
I0315 12:41:32.849035 140277216793792 submission.py:265] 65) loss = 0.381, grad_norm = 1.019
I0315 12:41:33.109047 140225061820160 logging_writer.py:48] [66] global_step=66, grad_norm=1.03717, loss=0.440744
I0315 12:41:33.113554 140277216793792 submission.py:265] 66) loss = 0.441, grad_norm = 1.037
I0315 12:41:33.284349 140224977958656 logging_writer.py:48] [67] global_step=67, grad_norm=0.520478, loss=0.398866
I0315 12:41:33.289088 140277216793792 submission.py:265] 67) loss = 0.399, grad_norm = 0.520
I0315 12:41:33.450220 140225061820160 logging_writer.py:48] [68] global_step=68, grad_norm=0.652669, loss=0.425814
I0315 12:41:33.456462 140277216793792 submission.py:265] 68) loss = 0.426, grad_norm = 0.653
I0315 12:41:33.612561 140224977958656 logging_writer.py:48] [69] global_step=69, grad_norm=0.549535, loss=0.362943
I0315 12:41:33.618519 140277216793792 submission.py:265] 69) loss = 0.363, grad_norm = 0.550
I0315 12:41:33.712333 140225061820160 logging_writer.py:48] [70] global_step=70, grad_norm=0.800352, loss=0.394668
I0315 12:41:33.721644 140277216793792 submission.py:265] 70) loss = 0.395, grad_norm = 0.800
I0315 12:41:33.799466 140224977958656 logging_writer.py:48] [71] global_step=71, grad_norm=0.618888, loss=0.340143
I0315 12:41:33.807899 140277216793792 submission.py:265] 71) loss = 0.340, grad_norm = 0.619
I0315 12:41:33.882480 140225061820160 logging_writer.py:48] [72] global_step=72, grad_norm=0.483316, loss=0.417199
I0315 12:41:33.888810 140277216793792 submission.py:265] 72) loss = 0.417, grad_norm = 0.483
I0315 12:41:33.969978 140224977958656 logging_writer.py:48] [73] global_step=73, grad_norm=0.69198, loss=0.321964
I0315 12:41:33.975326 140277216793792 submission.py:265] 73) loss = 0.322, grad_norm = 0.692
I0315 12:41:34.064837 140225061820160 logging_writer.py:48] [74] global_step=74, grad_norm=0.460237, loss=0.384981
I0315 12:41:34.071908 140277216793792 submission.py:265] 74) loss = 0.385, grad_norm = 0.460
I0315 12:41:34.147314 140224977958656 logging_writer.py:48] [75] global_step=75, grad_norm=0.547436, loss=0.353624
I0315 12:41:34.154353 140277216793792 submission.py:265] 75) loss = 0.354, grad_norm = 0.547
I0315 12:41:34.238929 140225061820160 logging_writer.py:48] [76] global_step=76, grad_norm=1.21309, loss=0.354211
I0315 12:41:34.243794 140277216793792 submission.py:265] 76) loss = 0.354, grad_norm = 1.213
I0315 12:41:34.331417 140224977958656 logging_writer.py:48] [77] global_step=77, grad_norm=0.76564, loss=0.381516
I0315 12:41:34.337655 140277216793792 submission.py:265] 77) loss = 0.382, grad_norm = 0.766
I0315 12:41:34.411952 140225061820160 logging_writer.py:48] [78] global_step=78, grad_norm=0.626983, loss=0.348664
I0315 12:41:34.418669 140277216793792 submission.py:265] 78) loss = 0.349, grad_norm = 0.627
I0315 12:41:34.528373 140224977958656 logging_writer.py:48] [79] global_step=79, grad_norm=0.580824, loss=0.309589
I0315 12:41:34.533280 140277216793792 submission.py:265] 79) loss = 0.310, grad_norm = 0.581
I0315 12:41:34.636563 140225061820160 logging_writer.py:48] [80] global_step=80, grad_norm=0.596339, loss=0.403115
I0315 12:41:34.641463 140277216793792 submission.py:265] 80) loss = 0.403, grad_norm = 0.596
I0315 12:41:34.783771 140224977958656 logging_writer.py:48] [81] global_step=81, grad_norm=0.515696, loss=0.27468
I0315 12:41:34.789695 140277216793792 submission.py:265] 81) loss = 0.275, grad_norm = 0.516
I0315 12:41:35.006316 140225061820160 logging_writer.py:48] [82] global_step=82, grad_norm=0.743029, loss=0.314951
I0315 12:41:35.011935 140277216793792 submission.py:265] 82) loss = 0.315, grad_norm = 0.743
I0315 12:41:35.166038 140224977958656 logging_writer.py:48] [83] global_step=83, grad_norm=0.339696, loss=0.409492
I0315 12:41:35.170501 140277216793792 submission.py:265] 83) loss = 0.409, grad_norm = 0.340
I0315 12:41:35.349361 140225061820160 logging_writer.py:48] [84] global_step=84, grad_norm=0.56003, loss=0.52578
I0315 12:41:35.357237 140277216793792 submission.py:265] 84) loss = 0.526, grad_norm = 0.560
I0315 12:41:35.506389 140224977958656 logging_writer.py:48] [85] global_step=85, grad_norm=0.497221, loss=0.450208
I0315 12:41:35.511648 140277216793792 submission.py:265] 85) loss = 0.450, grad_norm = 0.497
I0315 12:41:35.702039 140225061820160 logging_writer.py:48] [86] global_step=86, grad_norm=0.510638, loss=0.36111
I0315 12:41:35.710971 140277216793792 submission.py:265] 86) loss = 0.361, grad_norm = 0.511
I0315 12:41:35.972975 140224977958656 logging_writer.py:48] [87] global_step=87, grad_norm=0.343671, loss=0.341505
I0315 12:41:35.978685 140277216793792 submission.py:265] 87) loss = 0.342, grad_norm = 0.344
I0315 12:41:36.169631 140225061820160 logging_writer.py:48] [88] global_step=88, grad_norm=0.696263, loss=0.399945
I0315 12:41:36.177570 140277216793792 submission.py:265] 88) loss = 0.400, grad_norm = 0.696
I0315 12:41:36.400740 140224977958656 logging_writer.py:48] [89] global_step=89, grad_norm=0.586973, loss=0.402247
I0315 12:41:36.405847 140277216793792 submission.py:265] 89) loss = 0.402, grad_norm = 0.587
I0315 12:41:36.737132 140225061820160 logging_writer.py:48] [90] global_step=90, grad_norm=0.42294, loss=0.373756
I0315 12:41:36.742189 140277216793792 submission.py:265] 90) loss = 0.374, grad_norm = 0.423
I0315 12:41:37.033507 140224977958656 logging_writer.py:48] [91] global_step=91, grad_norm=0.378592, loss=0.386587
I0315 12:41:37.038758 140277216793792 submission.py:265] 91) loss = 0.387, grad_norm = 0.379
I0315 12:41:37.312328 140225061820160 logging_writer.py:48] [92] global_step=92, grad_norm=0.375265, loss=0.345817
I0315 12:41:37.320738 140277216793792 submission.py:265] 92) loss = 0.346, grad_norm = 0.375
I0315 12:41:37.524039 140224977958656 logging_writer.py:48] [93] global_step=93, grad_norm=0.538831, loss=0.350379
I0315 12:41:37.529383 140277216793792 submission.py:265] 93) loss = 0.350, grad_norm = 0.539
I0315 12:41:37.724891 140225061820160 logging_writer.py:48] [94] global_step=94, grad_norm=0.540511, loss=0.420428
I0315 12:41:37.729997 140277216793792 submission.py:265] 94) loss = 0.420, grad_norm = 0.541
I0315 12:41:37.887676 140224977958656 logging_writer.py:48] [95] global_step=95, grad_norm=0.323522, loss=0.358441
I0315 12:41:37.894664 140277216793792 submission.py:265] 95) loss = 0.358, grad_norm = 0.324
I0315 12:41:38.025298 140225061820160 logging_writer.py:48] [96] global_step=96, grad_norm=0.300963, loss=0.353894
I0315 12:41:38.029361 140277216793792 submission.py:265] 96) loss = 0.354, grad_norm = 0.301
I0315 12:41:38.346046 140224977958656 logging_writer.py:48] [97] global_step=97, grad_norm=0.894412, loss=0.320431
I0315 12:41:38.350886 140277216793792 submission.py:265] 97) loss = 0.320, grad_norm = 0.894
I0315 12:41:38.471728 140225061820160 logging_writer.py:48] [98] global_step=98, grad_norm=0.698815, loss=0.422722
I0315 12:41:38.477141 140277216793792 submission.py:265] 98) loss = 0.423, grad_norm = 0.699
I0315 12:41:40.686780 140224977958656 logging_writer.py:48] [99] global_step=99, grad_norm=0.33261, loss=0.37924
I0315 12:41:40.691777 140277216793792 submission.py:265] 99) loss = 0.379, grad_norm = 0.333
I0315 12:41:40.774575 140225061820160 logging_writer.py:48] [100] global_step=100, grad_norm=0.551218, loss=0.421103
I0315 12:41:40.779996 140277216793792 submission.py:265] 100) loss = 0.421, grad_norm = 0.551
I0315 12:42:44.265390 140277216793792 spec.py:321] Evaluating on the training split.
I0315 12:42:46.323940 140277216793792 spec.py:333] Evaluating on the validation split.
I0315 12:42:48.897387 140277216793792 spec.py:349] Evaluating on the test split.
I0315 12:42:51.287909 140277216793792 submission_runner.py:469] Time since start: 1383.44s, 	Step: 238, 	{'train/ssim': 0.692598819732666, 'train/loss': 0.3224166120801653, 'validation/ssim': 0.672557343508195, 'validation/loss': 0.3372185856825056, 'validation/num_examples': 3554, 'test/ssim': 0.6913191202265428, 'test/loss': 0.33819537467711536, 'test/num_examples': 3581, 'score': 394.91109442710876, 'total_duration': 1383.4408266544342, 'accumulated_submission_time': 394.91109442710876, 'accumulated_eval_time': 986.0426678657532, 'accumulated_logging_time': 0.017799854278564453}
I0315 12:42:51.298977 140224977958656 logging_writer.py:48] [238] accumulated_eval_time=986.043, accumulated_logging_time=0.0177999, accumulated_submission_time=394.911, global_step=238, preemption_count=0, score=394.911, test/loss=0.338195, test/num_examples=3581, test/ssim=0.691319, total_duration=1383.44, train/loss=0.322417, train/ssim=0.692599, validation/loss=0.337219, validation/num_examples=3554, validation/ssim=0.672557
I0315 12:44:14.563007 140277216793792 spec.py:321] Evaluating on the training split.
I0315 12:44:16.635026 140277216793792 spec.py:333] Evaluating on the validation split.
I0315 12:44:18.880873 140277216793792 spec.py:349] Evaluating on the test split.
I0315 12:44:21.177705 140277216793792 submission_runner.py:469] Time since start: 1473.33s, 	Step: 280, 	{'train/ssim': 0.6997325760977608, 'train/loss': 0.3164307049342564, 'validation/ssim': 0.6804400493941686, 'validation/loss': 0.3302287379207407, 'validation/num_examples': 3554, 'test/ssim': 0.6989031602336987, 'test/loss': 0.33158155674043566, 'test/num_examples': 3581, 'score': 476.79410314559937, 'total_duration': 1473.330650806427, 'accumulated_submission_time': 476.79410314559937, 'accumulated_eval_time': 992.6574938297272, 'accumulated_logging_time': 0.03784441947937012}
I0315 12:44:21.188203 140225061820160 logging_writer.py:48] [280] accumulated_eval_time=992.657, accumulated_logging_time=0.0378444, accumulated_submission_time=476.794, global_step=280, preemption_count=0, score=476.794, test/loss=0.331582, test/num_examples=3581, test/ssim=0.698903, total_duration=1473.33, train/loss=0.316431, train/ssim=0.699733, validation/loss=0.330229, validation/num_examples=3554, validation/ssim=0.68044
I0315 12:45:44.250382 140277216793792 spec.py:321] Evaluating on the training split.
I0315 12:45:46.245782 140277216793792 spec.py:333] Evaluating on the validation split.
I0315 12:45:48.486306 140277216793792 spec.py:349] Evaluating on the test split.
I0315 12:45:50.656770 140277216793792 submission_runner.py:469] Time since start: 1562.81s, 	Step: 317, 	{'train/ssim': 0.701486315046038, 'train/loss': 0.31354171889168875, 'validation/ssim': 0.682007454188942, 'validation/loss': 0.3271917151009426, 'validation/num_examples': 3554, 'test/ssim': 0.7004668601429419, 'test/loss': 0.3286023388958217, 'test/num_examples': 3581, 'score': 558.4852890968323, 'total_duration': 1562.8097002506256, 'accumulated_submission_time': 558.4852890968323, 'accumulated_eval_time': 999.0640578269958, 'accumulated_logging_time': 0.05714273452758789}
I0315 12:45:50.667510 140224977958656 logging_writer.py:48] [317] accumulated_eval_time=999.064, accumulated_logging_time=0.0571427, accumulated_submission_time=558.485, global_step=317, preemption_count=0, score=558.485, test/loss=0.328602, test/num_examples=3581, test/ssim=0.700467, total_duration=1562.81, train/loss=0.313542, train/ssim=0.701486, validation/loss=0.327192, validation/num_examples=3554, validation/ssim=0.682007
/usr/local/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
I0315 12:47:13.093504 140277216793792 spec.py:321] Evaluating on the training split.
I0315 12:47:15.193161 140277216793792 spec.py:333] Evaluating on the validation split.
I0315 12:47:17.476738 140277216793792 spec.py:349] Evaluating on the test split.
I0315 12:47:19.787887 140277216793792 submission_runner.py:469] Time since start: 1651.94s, 	Step: 364, 	{'train/ssim': 0.701453617640904, 'train/loss': 0.3108607700892857, 'validation/ssim': 0.6822621050884566, 'validation/loss': 0.3240871998320554, 'validation/num_examples': 3554, 'test/ssim': 0.7006460965861491, 'test/loss': 0.3256859117564926, 'test/num_examples': 3581, 'score': 639.4383788108826, 'total_duration': 1651.9408326148987, 'accumulated_submission_time': 639.4383788108826, 'accumulated_eval_time': 1005.7585783004761, 'accumulated_logging_time': 0.07626605033874512}
I0315 12:47:19.798603 140225061820160 logging_writer.py:48] [364] accumulated_eval_time=1005.76, accumulated_logging_time=0.0762661, accumulated_submission_time=639.438, global_step=364, preemption_count=0, score=639.438, test/loss=0.325686, test/num_examples=3581, test/ssim=0.700646, total_duration=1651.94, train/loss=0.310861, train/ssim=0.701454, validation/loss=0.324087, validation/num_examples=3554, validation/ssim=0.682262
I0315 12:48:43.204524 140277216793792 spec.py:321] Evaluating on the training split.
I0315 12:48:45.257124 140277216793792 spec.py:333] Evaluating on the validation split.
I0315 12:48:47.699517 140277216793792 spec.py:349] Evaluating on the test split.
I0315 12:48:50.086955 140277216793792 submission_runner.py:469] Time since start: 1742.24s, 	Step: 402, 	{'train/ssim': 0.7066734177725655, 'train/loss': 0.30737478392464773, 'validation/ssim': 0.688071333027047, 'validation/loss': 0.3203841141253693, 'validation/num_examples': 3554, 'test/ssim': 0.7060993432133134, 'test/loss': 0.3220678104165212, 'test/num_examples': 3581, 'score': 721.4700582027435, 'total_duration': 1742.2398929595947, 'accumulated_submission_time': 721.4700582027435, 'accumulated_eval_time': 1012.6411008834839, 'accumulated_logging_time': 0.09595251083374023}
I0315 12:48:50.097012 140224977958656 logging_writer.py:48] [402] accumulated_eval_time=1012.64, accumulated_logging_time=0.0959525, accumulated_submission_time=721.47, global_step=402, preemption_count=0, score=721.47, test/loss=0.322068, test/num_examples=3581, test/ssim=0.706099, total_duration=1742.24, train/loss=0.307375, train/ssim=0.706673, validation/loss=0.320384, validation/num_examples=3554, validation/ssim=0.688071
I0315 12:50:11.271951 140277216793792 spec.py:321] Evaluating on the training split.
I0315 12:50:13.343817 140277216793792 spec.py:333] Evaluating on the validation split.
I0315 12:50:15.833420 140277216793792 spec.py:349] Evaluating on the test split.
I0315 12:50:18.091395 140277216793792 submission_runner.py:469] Time since start: 1830.24s, 	Step: 440, 	{'train/ssim': 0.7111601148332868, 'train/loss': 0.3036275931767055, 'validation/ssim': 0.6923874837331176, 'validation/loss': 0.31679908009680996, 'validation/num_examples': 3554, 'test/ssim': 0.7101665583112259, 'test/loss': 0.318754186047019, 'test/num_examples': 3581, 'score': 801.35311627388, 'total_duration': 1830.24436211586, 'accumulated_submission_time': 801.35311627388, 'accumulated_eval_time': 1019.4608488082886, 'accumulated_logging_time': 0.11503100395202637}
I0315 12:50:18.102460 140225061820160 logging_writer.py:48] [440] accumulated_eval_time=1019.46, accumulated_logging_time=0.115031, accumulated_submission_time=801.353, global_step=440, preemption_count=0, score=801.353, test/loss=0.318754, test/num_examples=3581, test/ssim=0.710167, total_duration=1830.24, train/loss=0.303628, train/ssim=0.71116, validation/loss=0.316799, validation/num_examples=3554, validation/ssim=0.692387
I0315 12:51:39.789772 140277216793792 spec.py:321] Evaluating on the training split.
I0315 12:51:41.833002 140277216793792 spec.py:333] Evaluating on the validation split.
I0315 12:51:44.400506 140277216793792 spec.py:349] Evaluating on the test split.
I0315 12:51:46.686895 140277216793792 submission_runner.py:469] Time since start: 1918.84s, 	Step: 482, 	{'train/ssim': 0.7122256415230888, 'train/loss': 0.3027702740260533, 'validation/ssim': 0.693476018438731, 'validation/loss': 0.3158179494055993, 'validation/num_examples': 3554, 'test/ssim': 0.7111358259084404, 'test/loss': 0.3177734988589954, 'test/num_examples': 3581, 'score': 881.7598075866699, 'total_duration': 1918.8398296833038, 'accumulated_submission_time': 881.7598075866699, 'accumulated_eval_time': 1026.3583586215973, 'accumulated_logging_time': 0.13406157493591309}
I0315 12:51:46.697413 140224977958656 logging_writer.py:48] [482] accumulated_eval_time=1026.36, accumulated_logging_time=0.134062, accumulated_submission_time=881.76, global_step=482, preemption_count=0, score=881.76, test/loss=0.317773, test/num_examples=3581, test/ssim=0.711136, total_duration=1918.84, train/loss=0.30277, train/ssim=0.712226, validation/loss=0.315818, validation/num_examples=3554, validation/ssim=0.693476
I0315 12:52:12.429753 140225061820160 logging_writer.py:48] [500] global_step=500, grad_norm=0.240451, loss=0.362791
I0315 12:52:12.433027 140277216793792 submission.py:265] 500) loss = 0.363, grad_norm = 0.240
I0315 12:53:07.829703 140277216793792 spec.py:321] Evaluating on the training split.
I0315 12:53:09.850758 140277216793792 spec.py:333] Evaluating on the validation split.
I0315 12:53:12.438627 140277216793792 spec.py:349] Evaluating on the test split.
I0315 12:53:14.807486 140277216793792 submission_runner.py:469] Time since start: 2006.96s, 	Step: 522, 	{'train/ssim': 0.7121241433279855, 'train/loss': 0.3003976345062256, 'validation/ssim': 0.6937393248584341, 'validation/loss': 0.31317039064698227, 'validation/num_examples': 3554, 'test/ssim': 0.7113419921329587, 'test/loss': 0.31510618924095923, 'test/num_examples': 3581, 'score': 961.538745880127, 'total_duration': 2006.9604222774506, 'accumulated_submission_time': 961.538745880127, 'accumulated_eval_time': 1033.336349248886, 'accumulated_logging_time': 0.15690040588378906}
I0315 12:53:14.819296 140224977958656 logging_writer.py:48] [522] accumulated_eval_time=1033.34, accumulated_logging_time=0.1569, accumulated_submission_time=961.539, global_step=522, preemption_count=0, score=961.539, test/loss=0.315106, test/num_examples=3581, test/ssim=0.711342, total_duration=2006.96, train/loss=0.300398, train/ssim=0.712124, validation/loss=0.31317, validation/num_examples=3554, validation/ssim=0.693739
I0315 12:54:36.428542 140277216793792 spec.py:321] Evaluating on the training split.
I0315 12:54:38.467341 140277216793792 spec.py:333] Evaluating on the validation split.
I0315 12:54:41.104208 140277216793792 spec.py:349] Evaluating on the test split.
I0315 12:54:43.673173 140277216793792 submission_runner.py:469] Time since start: 2095.83s, 	Step: 564, 	{'train/ssim': 0.7153910228184291, 'train/loss': 0.298809289932251, 'validation/ssim': 0.6967502780757597, 'validation/loss': 0.31150269172938944, 'validation/num_examples': 3554, 'test/ssim': 0.7143418334176906, 'test/loss': 0.31345358696505865, 'test/num_examples': 3581, 'score': 1041.7175447940826, 'total_duration': 2095.826117515564, 'accumulated_submission_time': 1041.7175447940826, 'accumulated_eval_time': 1040.581214427948, 'accumulated_logging_time': 0.17691636085510254}
I0315 12:54:43.684058 140225061820160 logging_writer.py:48] [564] accumulated_eval_time=1040.58, accumulated_logging_time=0.176916, accumulated_submission_time=1041.72, global_step=564, preemption_count=0, score=1041.72, test/loss=0.313454, test/num_examples=3581, test/ssim=0.714342, total_duration=2095.83, train/loss=0.298809, train/ssim=0.715391, validation/loss=0.311503, validation/num_examples=3554, validation/ssim=0.69675
I0315 12:56:04.672761 140277216793792 spec.py:321] Evaluating on the training split.
I0315 12:56:06.818862 140277216793792 spec.py:333] Evaluating on the validation split.
I0315 12:56:09.484493 140277216793792 spec.py:349] Evaluating on the test split.
I0315 12:56:12.203308 140277216793792 submission_runner.py:469] Time since start: 2184.36s, 	Step: 609, 	{'train/ssim': 0.7189242499215263, 'train/loss': 0.297264678137643, 'validation/ssim': 0.7011448780203644, 'validation/loss': 0.3093167950614624, 'validation/num_examples': 3554, 'test/ssim': 0.7183652109222284, 'test/loss': 0.3116664380475949, 'test/num_examples': 3581, 'score': 1121.3416063785553, 'total_duration': 2184.3562257289886, 'accumulated_submission_time': 1121.3416063785553, 'accumulated_eval_time': 1048.1118938922882, 'accumulated_logging_time': 0.19640278816223145}
I0315 12:56:12.219222 140224977958656 logging_writer.py:48] [609] accumulated_eval_time=1048.11, accumulated_logging_time=0.196403, accumulated_submission_time=1121.34, global_step=609, preemption_count=0, score=1121.34, test/loss=0.311666, test/num_examples=3581, test/ssim=0.718365, total_duration=2184.36, train/loss=0.297265, train/ssim=0.718924, validation/loss=0.309317, validation/num_examples=3554, validation/ssim=0.701145
I0315 12:57:33.768034 140277216793792 spec.py:321] Evaluating on the training split.
I0315 12:57:35.768720 140277216793792 spec.py:333] Evaluating on the validation split.
I0315 12:57:37.888707 140277216793792 spec.py:349] Evaluating on the test split.
I0315 12:57:40.090252 140277216793792 submission_runner.py:469] Time since start: 2272.24s, 	Step: 647, 	{'train/ssim': 0.7178728921072823, 'train/loss': 0.2947742257799421, 'validation/ssim': 0.6993536662035734, 'validation/loss': 0.3074692536688414, 'validation/num_examples': 3554, 'test/ssim': 0.716854825162315, 'test/loss': 0.3094894549794052, 'test/num_examples': 3581, 'score': 1198.9645037651062, 'total_duration': 2272.2431750297546, 'accumulated_submission_time': 1198.9645037651062, 'accumulated_eval_time': 1054.4342849254608, 'accumulated_logging_time': 2.7610230445861816}
I0315 12:57:40.101821 140225061820160 logging_writer.py:48] [647] accumulated_eval_time=1054.43, accumulated_logging_time=2.76102, accumulated_submission_time=1198.96, global_step=647, preemption_count=0, score=1198.96, test/loss=0.309489, test/num_examples=3581, test/ssim=0.716855, total_duration=2272.24, train/loss=0.294774, train/ssim=0.717873, validation/loss=0.307469, validation/num_examples=3554, validation/ssim=0.699354
I0315 12:59:01.177405 140277216793792 spec.py:321] Evaluating on the training split.
I0315 12:59:03.207014 140277216793792 spec.py:333] Evaluating on the validation split.
I0315 12:59:05.332840 140277216793792 spec.py:349] Evaluating on the test split.
I0315 12:59:07.507145 140277216793792 submission_runner.py:469] Time since start: 2359.66s, 	Step: 688, 	{'train/ssim': 0.7154607772827148, 'train/loss': 0.2961082799094064, 'validation/ssim': 0.6972765474421426, 'validation/loss': 0.30846096332917133, 'validation/num_examples': 3554, 'test/ssim': 0.7147528705101578, 'test/loss': 0.31041519177822186, 'test/num_examples': 3581, 'score': 1278.6214230060577, 'total_duration': 2359.660080909729, 'accumulated_submission_time': 1278.6214230060577, 'accumulated_eval_time': 1060.7642035484314, 'accumulated_logging_time': 2.783500909805298}
I0315 12:59:07.518742 140224977958656 logging_writer.py:48] [688] accumulated_eval_time=1060.76, accumulated_logging_time=2.7835, accumulated_submission_time=1278.62, global_step=688, preemption_count=0, score=1278.62, test/loss=0.310415, test/num_examples=3581, test/ssim=0.714753, total_duration=2359.66, train/loss=0.296108, train/ssim=0.715461, validation/loss=0.308461, validation/num_examples=3554, validation/ssim=0.697277
I0315 13:00:28.538274 140277216793792 spec.py:321] Evaluating on the training split.
I0315 13:00:30.586542 140277216793792 spec.py:333] Evaluating on the validation split.
I0315 13:00:32.743327 140277216793792 spec.py:349] Evaluating on the test split.
I0315 13:00:35.003902 140277216793792 submission_runner.py:469] Time since start: 2447.16s, 	Step: 729, 	{'train/ssim': 0.7196531976972308, 'train/loss': 0.2942067895616804, 'validation/ssim': 0.7012525911604882, 'validation/loss': 0.30631367302906937, 'validation/num_examples': 3554, 'test/ssim': 0.7182435155813669, 'test/loss': 0.3086612108066008, 'test/num_examples': 3581, 'score': 1358.1507115364075, 'total_duration': 2447.1568472385406, 'accumulated_submission_time': 1358.1507115364075, 'accumulated_eval_time': 1067.2299966812134, 'accumulated_logging_time': 2.8034799098968506}
I0315 13:00:35.018854 140225061820160 logging_writer.py:48] [729] accumulated_eval_time=1067.23, accumulated_logging_time=2.80348, accumulated_submission_time=1358.15, global_step=729, preemption_count=0, score=1358.15, test/loss=0.308661, test/num_examples=3581, test/ssim=0.718244, total_duration=2447.16, train/loss=0.294207, train/ssim=0.719653, validation/loss=0.306314, validation/num_examples=3554, validation/ssim=0.701253
I0315 13:01:57.043763 140277216793792 spec.py:321] Evaluating on the training split.
I0315 13:01:59.152897 140277216793792 spec.py:333] Evaluating on the validation split.
I0315 13:02:01.380752 140277216793792 spec.py:349] Evaluating on the test split.
I0315 13:02:03.611397 140277216793792 submission_runner.py:469] Time since start: 2535.76s, 	Step: 768, 	{'train/ssim': 0.7208300999232701, 'train/loss': 0.2946338312966483, 'validation/ssim': 0.7022163077694148, 'validation/loss': 0.306895516330631, 'validation/num_examples': 3554, 'test/ssim': 0.7196263428075258, 'test/loss': 0.30893401971778134, 'test/num_examples': 3581, 'score': 1438.7897419929504, 'total_duration': 2535.7643020153046, 'accumulated_submission_time': 1438.7897419929504, 'accumulated_eval_time': 1073.797688961029, 'accumulated_logging_time': 2.831324338912964}
I0315 13:02:03.624368 140224977958656 logging_writer.py:48] [768] accumulated_eval_time=1073.8, accumulated_logging_time=2.83132, accumulated_submission_time=1438.79, global_step=768, preemption_count=0, score=1438.79, test/loss=0.308934, test/num_examples=3581, test/ssim=0.719626, total_duration=2535.76, train/loss=0.294634, train/ssim=0.72083, validation/loss=0.306896, validation/num_examples=3554, validation/ssim=0.702216
I0315 13:03:26.512425 140277216793792 spec.py:321] Evaluating on the training split.
I0315 13:03:28.558155 140277216793792 spec.py:333] Evaluating on the validation split.
I0315 13:03:30.843516 140277216793792 spec.py:349] Evaluating on the test split.
I0315 13:03:33.145767 140277216793792 submission_runner.py:469] Time since start: 2625.30s, 	Step: 809, 	{'train/ssim': 0.7218262127467564, 'train/loss': 0.29218901906694683, 'validation/ssim': 0.7024469155572945, 'validation/loss': 0.3051948440027962, 'validation/num_examples': 3554, 'test/ssim': 0.7199035491133762, 'test/loss': 0.3071372919793703, 'test/num_examples': 3581, 'score': 1520.3666110038757, 'total_duration': 2625.298675060272, 'accumulated_submission_time': 1520.3666110038757, 'accumulated_eval_time': 1080.4310355186462, 'accumulated_logging_time': 2.8585972785949707}
I0315 13:03:33.157913 140225061820160 logging_writer.py:48] [809] accumulated_eval_time=1080.43, accumulated_logging_time=2.8586, accumulated_submission_time=1520.37, global_step=809, preemption_count=0, score=1520.37, test/loss=0.307137, test/num_examples=3581, test/ssim=0.719904, total_duration=2625.3, train/loss=0.292189, train/ssim=0.721826, validation/loss=0.305195, validation/num_examples=3554, validation/ssim=0.702447
I0315 13:04:56.186481 140277216793792 spec.py:321] Evaluating on the training split.
I0315 13:04:58.213715 140277216793792 spec.py:333] Evaluating on the validation split.
I0315 13:05:00.951336 140277216793792 spec.py:349] Evaluating on the test split.
I0315 13:05:03.225435 140277216793792 submission_runner.py:469] Time since start: 2715.38s, 	Step: 848, 	{'train/ssim': 0.7231434413364956, 'train/loss': 0.29054369245256695, 'validation/ssim': 0.7042376465118528, 'validation/loss': 0.30344958887652995, 'validation/num_examples': 3554, 'test/ssim': 0.7214086852703505, 'test/loss': 0.3056276220744031, 'test/num_examples': 3581, 'score': 1602.0433101654053, 'total_duration': 2715.3783752918243, 'accumulated_submission_time': 1602.0433101654053, 'accumulated_eval_time': 1087.4702489376068, 'accumulated_logging_time': 2.8900163173675537}
I0315 13:05:03.236824 140224977958656 logging_writer.py:48] [848] accumulated_eval_time=1087.47, accumulated_logging_time=2.89002, accumulated_submission_time=1602.04, global_step=848, preemption_count=0, score=1602.04, test/loss=0.305628, test/num_examples=3581, test/ssim=0.721409, total_duration=2715.38, train/loss=0.290544, train/ssim=0.723143, validation/loss=0.30345, validation/num_examples=3554, validation/ssim=0.704238
I0315 13:06:24.869074 140277216793792 spec.py:321] Evaluating on the training split.
I0315 13:06:26.899726 140277216793792 spec.py:333] Evaluating on the validation split.
I0315 13:06:29.481003 140277216793792 spec.py:349] Evaluating on the test split.
I0315 13:06:32.428205 140277216793792 submission_runner.py:469] Time since start: 2804.58s, 	Step: 891, 	{'train/ssim': 0.7219435146876744, 'train/loss': 0.29090915407453266, 'validation/ssim': 0.7036784037352279, 'validation/loss': 0.30327963842457445, 'validation/num_examples': 3554, 'test/ssim': 0.7208191616779531, 'test/loss': 0.3052983287991308, 'test/num_examples': 3581, 'score': 1682.279399394989, 'total_duration': 2804.5811598300934, 'accumulated_submission_time': 1682.279399394989, 'accumulated_eval_time': 1095.0295987129211, 'accumulated_logging_time': 2.9098739624023438}
I0315 13:06:32.441514 140225061820160 logging_writer.py:48] [891] accumulated_eval_time=1095.03, accumulated_logging_time=2.90987, accumulated_submission_time=1682.28, global_step=891, preemption_count=0, score=1682.28, test/loss=0.305298, test/num_examples=3581, test/ssim=0.720819, total_duration=2804.58, train/loss=0.290909, train/ssim=0.721944, validation/loss=0.30328, validation/num_examples=3554, validation/ssim=0.703678
I0315 13:07:54.385004 140277216793792 spec.py:321] Evaluating on the training split.
I0315 13:07:56.397401 140277216793792 spec.py:333] Evaluating on the validation split.
I0315 13:07:58.581749 140277216793792 spec.py:349] Evaluating on the test split.
I0315 13:08:00.848780 140277216793792 submission_runner.py:469] Time since start: 2893.00s, 	Step: 933, 	{'train/ssim': 0.7210794176374163, 'train/loss': 0.29396060534885954, 'validation/ssim': 0.7033186500905669, 'validation/loss': 0.30580784030757596, 'validation/num_examples': 3554, 'test/ssim': 0.7207256232983106, 'test/loss': 0.30785410139887603, 'test/num_examples': 3581, 'score': 1762.830902338028, 'total_duration': 2893.0010199546814, 'accumulated_submission_time': 1762.830902338028, 'accumulated_eval_time': 1101.4928722381592, 'accumulated_logging_time': 2.9324727058410645}
I0315 13:08:00.867896 140224977958656 logging_writer.py:48] [933] accumulated_eval_time=1101.49, accumulated_logging_time=2.93247, accumulated_submission_time=1762.83, global_step=933, preemption_count=0, score=1762.83, test/loss=0.307854, test/num_examples=3581, test/ssim=0.720726, total_duration=2893, train/loss=0.293961, train/ssim=0.721079, validation/loss=0.305808, validation/num_examples=3554, validation/ssim=0.703319
I0315 13:09:07.581067 140225061820160 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.203874, loss=0.29431
I0315 13:09:07.584896 140277216793792 submission.py:265] 1000) loss = 0.294, grad_norm = 0.204
I0315 13:09:26.378875 140277216793792 spec.py:321] Evaluating on the training split.
I0315 13:09:28.390396 140277216793792 spec.py:333] Evaluating on the validation split.
I0315 13:09:31.004595 140277216793792 spec.py:349] Evaluating on the test split.
I0315 13:09:33.636710 140277216793792 submission_runner.py:469] Time since start: 2985.79s, 	Step: 1036, 	{'train/ssim': 0.7256838253566197, 'train/loss': 0.2889464242117746, 'validation/ssim': 0.7066664816491981, 'validation/loss': 0.30183907821732553, 'validation/num_examples': 3554, 'test/ssim': 0.723941516423485, 'test/loss': 0.3037558659199595, 'test/num_examples': 3581, 'score': 1846.844562292099, 'total_duration': 2985.789575815201, 'accumulated_submission_time': 1846.844562292099, 'accumulated_eval_time': 1108.750801563263, 'accumulated_logging_time': 2.9599575996398926}
I0315 13:09:33.650044 140224977958656 logging_writer.py:48] [1036] accumulated_eval_time=1108.75, accumulated_logging_time=2.95996, accumulated_submission_time=1846.84, global_step=1036, preemption_count=0, score=1846.84, test/loss=0.303756, test/num_examples=3581, test/ssim=0.723942, total_duration=2985.79, train/loss=0.288946, train/ssim=0.725684, validation/loss=0.301839, validation/num_examples=3554, validation/ssim=0.706666
I0315 13:10:06.484717 140225061820160 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.203018, loss=0.289912
I0315 13:10:06.488452 140277216793792 submission.py:265] 1500) loss = 0.290, grad_norm = 0.203
I0315 13:10:37.812012 140224977958656 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.208146, loss=0.288968
I0315 13:10:37.815258 140277216793792 submission.py:265] 2000) loss = 0.289, grad_norm = 0.208
I0315 13:10:54.379020 140277216793792 spec.py:321] Evaluating on the training split.
I0315 13:10:56.373471 140277216793792 spec.py:333] Evaluating on the validation split.
I0315 13:10:59.080442 140277216793792 spec.py:349] Evaluating on the test split.
I0315 13:11:01.757647 140277216793792 submission_runner.py:469] Time since start: 3073.91s, 	Step: 2254, 	{'train/ssim': 0.7291193008422852, 'train/loss': 0.28545824119022917, 'validation/ssim': 0.7112198348251969, 'validation/loss': 0.2979495897008652, 'validation/num_examples': 3554, 'test/ssim': 0.7283348204935772, 'test/loss': 0.29985138851315973, 'test/num_examples': 3581, 'score': 1925.662005662918, 'total_duration': 3073.910534620285, 'accumulated_submission_time': 1925.662005662918, 'accumulated_eval_time': 1116.1295981407166, 'accumulated_logging_time': 2.9858016967773438}
I0315 13:11:01.770715 140225061820160 logging_writer.py:48] [2254] accumulated_eval_time=1116.13, accumulated_logging_time=2.9858, accumulated_submission_time=1925.66, global_step=2254, preemption_count=0, score=1925.66, test/loss=0.299851, test/num_examples=3581, test/ssim=0.728335, total_duration=3073.91, train/loss=0.285458, train/ssim=0.729119, validation/loss=0.29795, validation/num_examples=3554, validation/ssim=0.71122
I0315 13:11:18.096551 140224977958656 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.598099, loss=0.312378
I0315 13:11:18.100189 140277216793792 submission.py:265] 2500) loss = 0.312, grad_norm = 0.598
I0315 13:11:49.492702 140225061820160 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.796135, loss=0.322791
I0315 13:11:49.496695 140277216793792 submission.py:265] 3000) loss = 0.323, grad_norm = 0.796
I0315 13:12:20.890531 140224977958656 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.403963, loss=0.274991
I0315 13:12:20.893901 140277216793792 submission.py:265] 3500) loss = 0.275, grad_norm = 0.404
I0315 13:12:22.509115 140277216793792 spec.py:321] Evaluating on the training split.
I0315 13:12:24.616555 140277216793792 spec.py:333] Evaluating on the validation split.
I0315 13:12:27.459737 140277216793792 spec.py:349] Evaluating on the test split.
I0315 13:12:30.102411 140277216793792 submission_runner.py:469] Time since start: 3162.26s, 	Step: 3516, 	{'train/ssim': 0.7321172441755023, 'train/loss': 0.28134213175092426, 'validation/ssim': 0.7116756235271877, 'validation/loss': 0.29548668203960327, 'validation/num_examples': 3554, 'test/ssim': 0.7289869984379364, 'test/loss': 0.29729237752199106, 'test/num_examples': 3581, 'score': 2004.4994041919708, 'total_duration': 3162.2553741931915, 'accumulated_submission_time': 2004.4994041919708, 'accumulated_eval_time': 1123.7229735851288, 'accumulated_logging_time': 3.0076403617858887}
I0315 13:12:30.113717 140225061820160 logging_writer.py:48] [3516] accumulated_eval_time=1123.72, accumulated_logging_time=3.00764, accumulated_submission_time=2004.5, global_step=3516, preemption_count=0, score=2004.5, test/loss=0.297292, test/num_examples=3581, test/ssim=0.728987, total_duration=3162.26, train/loss=0.281342, train/ssim=0.732117, validation/loss=0.295487, validation/num_examples=3554, validation/ssim=0.711676
I0315 13:13:01.368420 140224977958656 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.168077, loss=0.252849
I0315 13:13:01.371856 140277216793792 submission.py:265] 4000) loss = 0.253, grad_norm = 0.168
I0315 13:13:32.722787 140225061820160 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.216932, loss=0.257008
I0315 13:13:32.726776 140277216793792 submission.py:265] 4500) loss = 0.257, grad_norm = 0.217
I0315 13:13:50.799658 140277216793792 spec.py:321] Evaluating on the training split.
I0315 13:13:52.801530 140277216793792 spec.py:333] Evaluating on the validation split.
I0315 13:13:54.946938 140277216793792 spec.py:349] Evaluating on the test split.
I0315 13:13:57.161640 140277216793792 submission_runner.py:469] Time since start: 3249.31s, 	Step: 4779, 	{'train/ssim': 0.7375610215323312, 'train/loss': 0.27900091239384245, 'validation/ssim': 0.717322320031127, 'validation/loss': 0.2931784402807576, 'validation/num_examples': 3554, 'test/ssim': 0.7345630312325467, 'test/loss': 0.2950224014874511, 'test/num_examples': 3581, 'score': 2083.332499027252, 'total_duration': 3249.3145785331726, 'accumulated_submission_time': 2083.332499027252, 'accumulated_eval_time': 1130.0850768089294, 'accumulated_logging_time': 3.0274105072021484}
I0315 13:13:57.172518 140224977958656 logging_writer.py:48] [4779] accumulated_eval_time=1130.09, accumulated_logging_time=3.02741, accumulated_submission_time=2083.33, global_step=4779, preemption_count=0, score=2083.33, test/loss=0.295022, test/num_examples=3581, test/ssim=0.734563, total_duration=3249.31, train/loss=0.279001, train/ssim=0.737561, validation/loss=0.293178, validation/num_examples=3554, validation/ssim=0.717322
I0315 13:14:11.948542 140225061820160 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.142654, loss=0.272635
I0315 13:14:11.951852 140277216793792 submission.py:265] 5000) loss = 0.273, grad_norm = 0.143
I0315 13:14:43.368867 140224977958656 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.24408, loss=0.240732
I0315 13:14:43.372096 140277216793792 submission.py:265] 5500) loss = 0.241, grad_norm = 0.244
I0315 13:15:14.697019 140225061820160 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.555325, loss=0.291774
I0315 13:15:14.700457 140277216793792 submission.py:265] 6000) loss = 0.292, grad_norm = 0.555
I0315 13:15:17.842026 140277216793792 spec.py:321] Evaluating on the training split.
I0315 13:15:19.857974 140277216793792 spec.py:333] Evaluating on the validation split.
I0315 13:15:22.031634 140277216793792 spec.py:349] Evaluating on the test split.
I0315 13:15:24.258419 140277216793792 submission_runner.py:469] Time since start: 3336.41s, 	Step: 6041, 	{'train/ssim': 0.7392300878252301, 'train/loss': 0.2778095177241734, 'validation/ssim': 0.7190868096467009, 'validation/loss': 0.29196615224482975, 'validation/num_examples': 3554, 'test/ssim': 0.736458478776878, 'test/loss': 0.29353441176085593, 'test/num_examples': 3581, 'score': 2162.1475336551666, 'total_duration': 3336.4113862514496, 'accumulated_submission_time': 2162.1475336551666, 'accumulated_eval_time': 1136.5017354488373, 'accumulated_logging_time': 3.046607732772827}
I0315 13:15:24.269538 140224977958656 logging_writer.py:48] [6041] accumulated_eval_time=1136.5, accumulated_logging_time=3.04661, accumulated_submission_time=2162.15, global_step=6041, preemption_count=0, score=2162.15, test/loss=0.293534, test/num_examples=3581, test/ssim=0.736458, total_duration=3336.41, train/loss=0.27781, train/ssim=0.73923, validation/loss=0.291966, validation/num_examples=3554, validation/ssim=0.719087
I0315 13:15:53.891892 140225061820160 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.380784, loss=0.212826
I0315 13:15:53.895350 140277216793792 submission.py:265] 6500) loss = 0.213, grad_norm = 0.381
I0315 13:16:25.166368 140224977958656 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.212885, loss=0.388553
I0315 13:16:25.169687 140277216793792 submission.py:265] 7000) loss = 0.389, grad_norm = 0.213
I0315 13:16:44.943732 140277216793792 spec.py:321] Evaluating on the training split.
I0315 13:16:46.949977 140277216793792 spec.py:333] Evaluating on the validation split.
I0315 13:16:49.098338 140277216793792 spec.py:349] Evaluating on the test split.
I0315 13:16:51.310635 140277216793792 submission_runner.py:469] Time since start: 3423.46s, 	Step: 7307, 	{'train/ssim': 0.7380869729178292, 'train/loss': 0.2769366843359811, 'validation/ssim': 0.7177812686849324, 'validation/loss': 0.2907457925928531, 'validation/num_examples': 3554, 'test/ssim': 0.7351685081637461, 'test/loss': 0.29236504566200083, 'test/num_examples': 3581, 'score': 2240.988727092743, 'total_duration': 3423.4635763168335, 'accumulated_submission_time': 2240.988727092743, 'accumulated_eval_time': 1142.8688337802887, 'accumulated_logging_time': 3.065983772277832}
I0315 13:16:51.322429 140225061820160 logging_writer.py:48] [7307] accumulated_eval_time=1142.87, accumulated_logging_time=3.06598, accumulated_submission_time=2240.99, global_step=7307, preemption_count=0, score=2240.99, test/loss=0.292365, test/num_examples=3581, test/ssim=0.735169, total_duration=3423.46, train/loss=0.276937, train/ssim=0.738087, validation/loss=0.290746, validation/num_examples=3554, validation/ssim=0.717781
I0315 13:17:04.243381 140224977958656 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.265571, loss=0.325147
I0315 13:17:04.246692 140277216793792 submission.py:265] 7500) loss = 0.325, grad_norm = 0.266
I0315 13:17:35.661341 140225061820160 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.22253, loss=0.343845
I0315 13:17:35.664941 140277216793792 submission.py:265] 8000) loss = 0.344, grad_norm = 0.223
I0315 13:18:07.111838 140224977958656 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.0958345, loss=0.334409
I0315 13:18:07.115410 140277216793792 submission.py:265] 8500) loss = 0.334, grad_norm = 0.096
I0315 13:18:12.000522 140277216793792 spec.py:321] Evaluating on the training split.
I0315 13:18:14.021859 140277216793792 spec.py:333] Evaluating on the validation split.
I0315 13:18:16.219425 140277216793792 spec.py:349] Evaluating on the test split.
I0315 13:18:18.428236 140277216793792 submission_runner.py:469] Time since start: 3510.58s, 	Step: 8569, 	{'train/ssim': 0.7384975978306362, 'train/loss': 0.27777375493730816, 'validation/ssim': 0.717862946569886, 'validation/loss': 0.2922648707332407, 'validation/num_examples': 3554, 'test/ssim': 0.7352596603602346, 'test/loss': 0.2938173449062762, 'test/num_examples': 3581, 'score': 2319.8484699726105, 'total_duration': 3510.5809857845306, 'accumulated_submission_time': 2319.8484699726105, 'accumulated_eval_time': 1149.296410560608, 'accumulated_logging_time': 3.0861377716064453}
I0315 13:18:18.442035 140225061820160 logging_writer.py:48] [8569] accumulated_eval_time=1149.3, accumulated_logging_time=3.08614, accumulated_submission_time=2319.85, global_step=8569, preemption_count=0, score=2319.85, test/loss=0.293817, test/num_examples=3581, test/ssim=0.73526, total_duration=3510.58, train/loss=0.277774, train/ssim=0.738498, validation/loss=0.292265, validation/num_examples=3554, validation/ssim=0.717863
I0315 13:18:46.399516 140224977958656 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.548354, loss=0.237575
I0315 13:18:46.402893 140277216793792 submission.py:265] 9000) loss = 0.238, grad_norm = 0.548
I0315 13:19:17.804204 140225061820160 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.587401, loss=0.323172
I0315 13:19:17.807700 140277216793792 submission.py:265] 9500) loss = 0.323, grad_norm = 0.587
I0315 13:19:39.155712 140277216793792 spec.py:321] Evaluating on the training split.
I0315 13:19:41.134522 140277216793792 spec.py:333] Evaluating on the validation split.
I0315 13:19:43.815361 140277216793792 spec.py:349] Evaluating on the test split.
I0315 13:19:46.615143 140277216793792 submission_runner.py:469] Time since start: 3598.77s, 	Step: 9830, 	{'train/ssim': 0.7398907797677177, 'train/loss': 0.2754675831113543, 'validation/ssim': 0.7196196050005276, 'validation/loss': 0.28954968213632526, 'validation/num_examples': 3554, 'test/ssim': 0.7371660161704133, 'test/loss': 0.29102455614266265, 'test/num_examples': 3581, 'score': 2398.737003803253, 'total_duration': 3598.768073320389, 'accumulated_submission_time': 2398.737003803253, 'accumulated_eval_time': 1156.756095647812, 'accumulated_logging_time': 3.108621597290039}
I0315 13:19:46.627256 140224977958656 logging_writer.py:48] [9830] accumulated_eval_time=1156.76, accumulated_logging_time=3.10862, accumulated_submission_time=2398.74, global_step=9830, preemption_count=0, score=2398.74, test/loss=0.291025, test/num_examples=3581, test/ssim=0.737166, total_duration=3598.77, train/loss=0.275468, train/ssim=0.739891, validation/loss=0.28955, validation/num_examples=3554, validation/ssim=0.71962
I0315 13:19:58.110629 140225061820160 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.238801, loss=0.248482
I0315 13:19:58.113775 140277216793792 submission.py:265] 10000) loss = 0.248, grad_norm = 0.239
I0315 13:20:29.617964 140224977958656 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.18884, loss=0.287824
I0315 13:20:29.621389 140277216793792 submission.py:265] 10500) loss = 0.288, grad_norm = 0.189
I0315 13:21:01.107949 140225061820160 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.168164, loss=0.311316
I0315 13:21:01.111431 140277216793792 submission.py:265] 11000) loss = 0.311, grad_norm = 0.168
I0315 13:21:07.298165 140277216793792 spec.py:321] Evaluating on the training split.
I0315 13:21:09.379411 140277216793792 spec.py:333] Evaluating on the validation split.
I0315 13:21:11.942337 140277216793792 spec.py:349] Evaluating on the test split.
I0315 13:21:14.666187 140277216793792 submission_runner.py:469] Time since start: 3686.82s, 	Step: 11090, 	{'train/ssim': 0.7411518096923828, 'train/loss': 0.2751688616616385, 'validation/ssim': 0.7209136052687113, 'validation/loss': 0.28905326057567177, 'validation/num_examples': 3554, 'test/ssim': 0.7383291781930675, 'test/loss': 0.29056627262374335, 'test/num_examples': 3581, 'score': 2477.603764295578, 'total_duration': 3686.8191261291504, 'accumulated_submission_time': 2477.603764295578, 'accumulated_eval_time': 1164.124282836914, 'accumulated_logging_time': 3.1302247047424316}
I0315 13:21:14.679475 140224977958656 logging_writer.py:48] [11090] accumulated_eval_time=1164.12, accumulated_logging_time=3.13022, accumulated_submission_time=2477.6, global_step=11090, preemption_count=0, score=2477.6, test/loss=0.290566, test/num_examples=3581, test/ssim=0.738329, total_duration=3686.82, train/loss=0.275169, train/ssim=0.741152, validation/loss=0.289053, validation/num_examples=3554, validation/ssim=0.720914
I0315 13:21:41.480794 140225061820160 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.312125, loss=0.329129
I0315 13:21:41.483951 140277216793792 submission.py:265] 11500) loss = 0.329, grad_norm = 0.312
I0315 13:22:12.857095 140224977958656 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.193161, loss=0.250073
I0315 13:22:12.860493 140277216793792 submission.py:265] 12000) loss = 0.250, grad_norm = 0.193
I0315 13:22:35.398253 140277216793792 spec.py:321] Evaluating on the training split.
I0315 13:22:37.421905 140277216793792 spec.py:333] Evaluating on the validation split.
I0315 13:22:39.694993 140277216793792 spec.py:349] Evaluating on the test split.
I0315 13:22:42.016173 140277216793792 submission_runner.py:469] Time since start: 3774.17s, 	Step: 12349, 	{'train/ssim': 0.7420684950692313, 'train/loss': 0.27451796191079275, 'validation/ssim': 0.7214949677080402, 'validation/loss': 0.28897285354090463, 'validation/num_examples': 3554, 'test/ssim': 0.7389023393866937, 'test/loss': 0.290456303668668, 'test/num_examples': 3581, 'score': 2556.424582004547, 'total_duration': 3774.1689546108246, 'accumulated_submission_time': 2556.424582004547, 'accumulated_eval_time': 1170.7422409057617, 'accumulated_logging_time': 3.1521244049072266}
I0315 13:22:42.027818 140225061820160 logging_writer.py:48] [12349] accumulated_eval_time=1170.74, accumulated_logging_time=3.15212, accumulated_submission_time=2556.42, global_step=12349, preemption_count=0, score=2556.42, test/loss=0.290456, test/num_examples=3581, test/ssim=0.738902, total_duration=3774.17, train/loss=0.274518, train/ssim=0.742068, validation/loss=0.288973, validation/num_examples=3554, validation/ssim=0.721495
I0315 13:22:52.355389 140224977958656 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.571936, loss=0.299969
I0315 13:22:52.358552 140277216793792 submission.py:265] 12500) loss = 0.300, grad_norm = 0.572
I0315 13:23:23.852217 140225061820160 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.318884, loss=0.24354
I0315 13:23:23.855554 140277216793792 submission.py:265] 13000) loss = 0.244, grad_norm = 0.319
I0315 13:23:55.263142 140224977958656 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.182802, loss=0.260856
I0315 13:23:55.266231 140277216793792 submission.py:265] 13500) loss = 0.261, grad_norm = 0.183
I0315 13:24:02.721273 140277216793792 spec.py:321] Evaluating on the training split.
I0315 13:24:04.722919 140277216793792 spec.py:333] Evaluating on the validation split.
I0315 13:24:06.824732 140277216793792 spec.py:349] Evaluating on the test split.
I0315 13:24:08.996188 140277216793792 submission_runner.py:469] Time since start: 3861.15s, 	Step: 13609, 	{'train/ssim': 0.7424231937953404, 'train/loss': 0.27425878388541086, 'validation/ssim': 0.7219842793771103, 'validation/loss': 0.28842892966112127, 'validation/num_examples': 3554, 'test/ssim': 0.7393583048991204, 'test/loss': 0.28991934428232335, 'test/num_examples': 3581, 'score': 2635.2940576076508, 'total_duration': 3861.1491072177887, 'accumulated_submission_time': 2635.2940576076508, 'accumulated_eval_time': 1177.0173680782318, 'accumulated_logging_time': 3.1716830730438232}
I0315 13:24:09.009198 140225061820160 logging_writer.py:48] [13609] accumulated_eval_time=1177.02, accumulated_logging_time=3.17168, accumulated_submission_time=2635.29, global_step=13609, preemption_count=0, score=2635.29, test/loss=0.289919, test/num_examples=3581, test/ssim=0.739358, total_duration=3861.15, train/loss=0.274259, train/ssim=0.742423, validation/loss=0.288429, validation/num_examples=3554, validation/ssim=0.721984
I0315 13:24:34.395373 140224977958656 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.178118, loss=0.285607
I0315 13:24:34.398699 140277216793792 submission.py:265] 14000) loss = 0.286, grad_norm = 0.178
I0315 13:25:05.919873 140225061820160 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.321383, loss=0.28591
I0315 13:25:05.923896 140277216793792 submission.py:265] 14500) loss = 0.286, grad_norm = 0.321
I0315 13:25:29.664123 140277216793792 spec.py:321] Evaluating on the training split.
I0315 13:25:31.760956 140277216793792 spec.py:333] Evaluating on the validation split.
I0315 13:25:35.349887 140277216793792 spec.py:349] Evaluating on the test split.
I0315 13:25:37.797355 140277216793792 submission_runner.py:469] Time since start: 3949.95s, 	Step: 14869, 	{'train/ssim': 0.7424225807189941, 'train/loss': 0.2739834615162441, 'validation/ssim': 0.721989981029298, 'validation/loss': 0.28824094687543966, 'validation/num_examples': 3554, 'test/ssim': 0.7394188457745742, 'test/loss': 0.2896622841799602, 'test/num_examples': 3581, 'score': 2714.133480787277, 'total_duration': 3949.95032787323, 'accumulated_submission_time': 2714.133480787277, 'accumulated_eval_time': 1185.150752544403, 'accumulated_logging_time': 3.1931447982788086}
I0315 13:25:37.809348 140224977958656 logging_writer.py:48] [14869] accumulated_eval_time=1185.15, accumulated_logging_time=3.19314, accumulated_submission_time=2714.13, global_step=14869, preemption_count=0, score=2714.13, test/loss=0.289662, test/num_examples=3581, test/ssim=0.739419, total_duration=3949.95, train/loss=0.273983, train/ssim=0.742423, validation/loss=0.288241, validation/num_examples=3554, validation/ssim=0.72199
I0315 13:25:46.861257 140225061820160 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.266206, loss=0.278271
I0315 13:25:46.864886 140277216793792 submission.py:265] 15000) loss = 0.278, grad_norm = 0.266
I0315 13:26:18.457017 140224977958656 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.277049, loss=0.295695
I0315 13:26:18.460507 140277216793792 submission.py:265] 15500) loss = 0.296, grad_norm = 0.277
I0315 13:26:49.938278 140225061820160 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.252331, loss=0.217926
I0315 13:26:49.941487 140277216793792 submission.py:265] 16000) loss = 0.218, grad_norm = 0.252
I0315 13:26:58.488018 140277216793792 spec.py:321] Evaluating on the training split.
I0315 13:27:00.480254 140277216793792 spec.py:333] Evaluating on the validation split.
I0315 13:27:02.595560 140277216793792 spec.py:349] Evaluating on the test split.
I0315 13:27:04.761574 140277216793792 submission_runner.py:469] Time since start: 4036.91s, 	Step: 16127, 	{'train/ssim': 0.742307322365897, 'train/loss': 0.2739106927599226, 'validation/ssim': 0.7215789812095527, 'validation/loss': 0.28826509302898146, 'validation/num_examples': 3554, 'test/ssim': 0.7390093767453225, 'test/loss': 0.2897056445367041, 'test/num_examples': 3581, 'score': 2793.012162923813, 'total_duration': 4036.9145328998566, 'accumulated_submission_time': 2793.012162923813, 'accumulated_eval_time': 1191.4245555400848, 'accumulated_logging_time': 3.2131543159484863}
I0315 13:27:04.773368 140224977958656 logging_writer.py:48] [16127] accumulated_eval_time=1191.42, accumulated_logging_time=3.21315, accumulated_submission_time=2793.01, global_step=16127, preemption_count=0, score=2793.01, test/loss=0.289706, test/num_examples=3581, test/ssim=0.739009, total_duration=4036.91, train/loss=0.273911, train/ssim=0.742307, validation/loss=0.288265, validation/num_examples=3554, validation/ssim=0.721579
I0315 13:27:29.040648 140225061820160 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.254387, loss=0.2199
I0315 13:27:29.044131 140277216793792 submission.py:265] 16500) loss = 0.220, grad_norm = 0.254
I0315 13:28:00.409539 140224977958656 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.383846, loss=0.207013
I0315 13:28:00.412782 140277216793792 submission.py:265] 17000) loss = 0.207, grad_norm = 0.384
I0315 13:28:25.524217 140277216793792 spec.py:321] Evaluating on the training split.
I0315 13:28:27.529757 140277216793792 spec.py:333] Evaluating on the validation split.
I0315 13:28:29.655530 140277216793792 spec.py:349] Evaluating on the test split.
I0315 13:28:31.854601 140277216793792 submission_runner.py:469] Time since start: 4124.01s, 	Step: 17391, 	{'train/ssim': 0.7427516664777484, 'train/loss': 0.27331994261060444, 'validation/ssim': 0.7221806772518641, 'validation/loss': 0.28761680487105196, 'validation/num_examples': 3554, 'test/ssim': 0.7396388518613864, 'test/loss': 0.2890334908305117, 'test/num_examples': 3581, 'score': 2871.9024336338043, 'total_duration': 4124.007575511932, 'accumulated_submission_time': 2871.9024336338043, 'accumulated_eval_time': 1197.7551157474518, 'accumulated_logging_time': 3.233142137527466}
I0315 13:28:31.866053 140225061820160 logging_writer.py:48] [17391] accumulated_eval_time=1197.76, accumulated_logging_time=3.23314, accumulated_submission_time=2871.9, global_step=17391, preemption_count=0, score=2871.9, test/loss=0.289033, test/num_examples=3581, test/ssim=0.739639, total_duration=4124.01, train/loss=0.27332, train/ssim=0.742752, validation/loss=0.287617, validation/num_examples=3554, validation/ssim=0.722181
I0315 13:28:39.653685 140224977958656 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.217087, loss=0.260781
I0315 13:28:39.656870 140277216793792 submission.py:265] 17500) loss = 0.261, grad_norm = 0.217
I0315 13:29:11.061577 140225061820160 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.085927, loss=0.327131
I0315 13:29:11.065165 140277216793792 submission.py:265] 18000) loss = 0.327, grad_norm = 0.086
I0315 13:29:42.626844 140224977958656 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.243804, loss=0.312606
I0315 13:29:42.630582 140277216793792 submission.py:265] 18500) loss = 0.313, grad_norm = 0.244
I0315 13:29:52.568359 140277216793792 spec.py:321] Evaluating on the training split.
I0315 13:29:54.552125 140277216793792 spec.py:333] Evaluating on the validation split.
I0315 13:29:58.304866 140277216793792 spec.py:349] Evaluating on the test split.
I0315 13:30:01.005903 140277216793792 submission_runner.py:469] Time since start: 4213.16s, 	Step: 18648, 	{'train/ssim': 0.742833001273019, 'train/loss': 0.27330963952200754, 'validation/ssim': 0.7222509518324424, 'validation/loss': 0.2876249795290078, 'validation/num_examples': 3554, 'test/ssim': 0.7396931886606395, 'test/loss': 0.2890536029456681, 'test/num_examples': 3581, 'score': 2950.720981836319, 'total_duration': 4213.158851146698, 'accumulated_submission_time': 2950.720981836319, 'accumulated_eval_time': 1206.1928584575653, 'accumulated_logging_time': 3.2527596950531006}
I0315 13:30:01.017328 140225061820160 logging_writer.py:48] [18648] accumulated_eval_time=1206.19, accumulated_logging_time=3.25276, accumulated_submission_time=2950.72, global_step=18648, preemption_count=0, score=2950.72, test/loss=0.289054, test/num_examples=3581, test/ssim=0.739693, total_duration=4213.16, train/loss=0.27331, train/ssim=0.742833, validation/loss=0.287625, validation/num_examples=3554, validation/ssim=0.722251
I0315 13:30:24.152550 140224977958656 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.100023, loss=0.23504
I0315 13:30:24.155693 140277216793792 submission.py:265] 19000) loss = 0.235, grad_norm = 0.100
I0315 13:30:55.540096 140225061820160 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.54843, loss=0.251452
I0315 13:30:55.543809 140277216793792 submission.py:265] 19500) loss = 0.251, grad_norm = 0.548
I0315 13:31:21.723621 140277216793792 spec.py:321] Evaluating on the training split.
I0315 13:31:23.723165 140277216793792 spec.py:333] Evaluating on the validation split.
I0315 13:31:25.828507 140277216793792 spec.py:349] Evaluating on the test split.
I0315 13:31:27.989838 140277216793792 submission_runner.py:469] Time since start: 4300.14s, 	Step: 19906, 	{'train/ssim': 0.7421615464346749, 'train/loss': 0.2734713043485369, 'validation/ssim': 0.7216377150965462, 'validation/loss': 0.2877413653629713, 'validation/num_examples': 3554, 'test/ssim': 0.7391161413973052, 'test/loss': 0.28914305072561786, 'test/num_examples': 3581, 'score': 3029.5387382507324, 'total_duration': 4300.142775058746, 'accumulated_submission_time': 3029.5387382507324, 'accumulated_eval_time': 1212.459302663803, 'accumulated_logging_time': 3.272318124771118}
I0315 13:31:28.001585 140224977958656 logging_writer.py:48] [19906] accumulated_eval_time=1212.46, accumulated_logging_time=3.27232, accumulated_submission_time=3029.54, global_step=19906, preemption_count=0, score=3029.54, test/loss=0.289143, test/num_examples=3581, test/ssim=0.739116, total_duration=4300.14, train/loss=0.273471, train/ssim=0.742162, validation/loss=0.287741, validation/num_examples=3554, validation/ssim=0.721638
I0315 13:31:34.750586 140225061820160 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.257064, loss=0.22519
I0315 13:31:34.754346 140277216793792 submission.py:265] 20000) loss = 0.225, grad_norm = 0.257
I0315 13:32:06.209522 140224977958656 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.453214, loss=0.304006
I0315 13:32:06.213066 140277216793792 submission.py:265] 20500) loss = 0.304, grad_norm = 0.453
I0315 13:32:37.604736 140225061820160 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.0892244, loss=0.332337
I0315 13:32:37.608074 140277216793792 submission.py:265] 21000) loss = 0.332, grad_norm = 0.089
I0315 13:32:48.701906 140277216793792 spec.py:321] Evaluating on the training split.
I0315 13:32:50.682940 140277216793792 spec.py:333] Evaluating on the validation split.
I0315 13:32:52.777837 140277216793792 spec.py:349] Evaluating on the test split.
I0315 13:32:54.941630 140277216793792 submission_runner.py:469] Time since start: 4387.09s, 	Step: 21168, 	{'train/ssim': 0.7424730573381696, 'train/loss': 0.2737385034561157, 'validation/ssim': 0.7221252407058948, 'validation/loss': 0.28799687494504433, 'validation/num_examples': 3554, 'test/ssim': 0.7395825379389486, 'test/loss': 0.2894143597502443, 'test/num_examples': 3581, 'score': 3108.4140136241913, 'total_duration': 4387.094579219818, 'accumulated_submission_time': 3108.4140136241913, 'accumulated_eval_time': 1218.699146747589, 'accumulated_logging_time': 3.2920308113098145}
I0315 13:32:54.953422 140224977958656 logging_writer.py:48] [21168] accumulated_eval_time=1218.7, accumulated_logging_time=3.29203, accumulated_submission_time=3108.41, global_step=21168, preemption_count=0, score=3108.41, test/loss=0.289414, test/num_examples=3581, test/ssim=0.739583, total_duration=4387.09, train/loss=0.273739, train/ssim=0.742473, validation/loss=0.287997, validation/num_examples=3554, validation/ssim=0.722125
I0315 13:33:16.668623 140225061820160 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.135325, loss=0.270444
I0315 13:33:16.671915 140277216793792 submission.py:265] 21500) loss = 0.270, grad_norm = 0.135
I0315 13:33:48.068515 140224977958656 logging_writer.py:48] [22000] global_step=22000, grad_norm=0.113286, loss=0.270902
I0315 13:33:48.072482 140277216793792 submission.py:265] 22000) loss = 0.271, grad_norm = 0.113
I0315 13:34:15.702451 140277216793792 spec.py:321] Evaluating on the training split.
I0315 13:34:17.713843 140277216793792 spec.py:333] Evaluating on the validation split.
I0315 13:34:19.836135 140277216793792 spec.py:349] Evaluating on the test split.
I0315 13:34:22.025087 140277216793792 submission_runner.py:469] Time since start: 4474.18s, 	Step: 22430, 	{'train/ssim': 0.7422384534563337, 'train/loss': 0.27371529170445036, 'validation/ssim': 0.7217710513242122, 'validation/loss': 0.2879422283870287, 'validation/num_examples': 3554, 'test/ssim': 0.7392724023055711, 'test/loss': 0.28934645579534346, 'test/num_examples': 3581, 'score': 3187.3127830028534, 'total_duration': 4474.1780552864075, 'accumulated_submission_time': 3187.3127830028534, 'accumulated_eval_time': 1225.0219326019287, 'accumulated_logging_time': 3.3118233680725098}
I0315 13:34:22.036860 140225061820160 logging_writer.py:48] [22430] accumulated_eval_time=1225.02, accumulated_logging_time=3.31182, accumulated_submission_time=3187.31, global_step=22430, preemption_count=0, score=3187.31, test/loss=0.289346, test/num_examples=3581, test/ssim=0.739272, total_duration=4474.18, train/loss=0.273715, train/ssim=0.742238, validation/loss=0.287942, validation/num_examples=3554, validation/ssim=0.721771
I0315 13:34:27.311650 140224977958656 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.48111, loss=0.266901
I0315 13:34:27.314700 140277216793792 submission.py:265] 22500) loss = 0.267, grad_norm = 0.481
I0315 13:34:58.669761 140225061820160 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.166364, loss=0.233635
I0315 13:34:58.673043 140277216793792 submission.py:265] 23000) loss = 0.234, grad_norm = 0.166
I0315 13:35:30.106288 140224977958656 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.353005, loss=0.291096
I0315 13:35:30.109776 140277216793792 submission.py:265] 23500) loss = 0.291, grad_norm = 0.353
I0315 13:35:42.706353 140277216793792 spec.py:321] Evaluating on the training split.
I0315 13:35:44.699137 140277216793792 spec.py:333] Evaluating on the validation split.
I0315 13:35:46.794603 140277216793792 spec.py:349] Evaluating on the test split.
I0315 13:35:48.942813 140277216793792 submission_runner.py:469] Time since start: 4561.10s, 	Step: 23692, 	{'train/ssim': 0.7412616184779576, 'train/loss': 0.2742901699883597, 'validation/ssim': 0.7207326636800084, 'validation/loss': 0.2885815003780951, 'validation/num_examples': 3554, 'test/ssim': 0.7382854087763544, 'test/loss': 0.28994474008874965, 'test/num_examples': 3581, 'score': 3266.1214163303375, 'total_duration': 4561.095770120621, 'accumulated_submission_time': 3266.1214163303375, 'accumulated_eval_time': 1231.2584619522095, 'accumulated_logging_time': 3.3319993019104004}
I0315 13:35:48.954714 140225061820160 logging_writer.py:48] [23692] accumulated_eval_time=1231.26, accumulated_logging_time=3.332, accumulated_submission_time=3266.12, global_step=23692, preemption_count=0, score=3266.12, test/loss=0.289945, test/num_examples=3581, test/ssim=0.738285, total_duration=4561.1, train/loss=0.27429, train/ssim=0.741262, validation/loss=0.288582, validation/num_examples=3554, validation/ssim=0.720733
I0315 13:36:09.176535 140224977958656 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.595904, loss=0.324352
I0315 13:36:09.179900 140277216793792 submission.py:265] 24000) loss = 0.324, grad_norm = 0.596
I0315 13:36:40.623333 140225061820160 logging_writer.py:48] [24500] global_step=24500, grad_norm=0.289057, loss=0.317943
I0315 13:36:40.626759 140277216793792 submission.py:265] 24500) loss = 0.318, grad_norm = 0.289
I0315 13:37:09.616809 140277216793792 spec.py:321] Evaluating on the training split.
I0315 13:37:11.604727 140277216793792 spec.py:333] Evaluating on the validation split.
I0315 13:37:14.186042 140277216793792 spec.py:349] Evaluating on the test split.
I0315 13:37:16.511480 140277216793792 submission_runner.py:469] Time since start: 4648.66s, 	Step: 24953, 	{'train/ssim': 0.742664064679827, 'train/loss': 0.2740612540926252, 'validation/ssim': 0.7223258976461382, 'validation/loss': 0.28824850328195345, 'validation/num_examples': 3554, 'test/ssim': 0.7396910070074699, 'test/loss': 0.2897139280010821, 'test/num_examples': 3581, 'score': 3345.0114874839783, 'total_duration': 4648.664395332336, 'accumulated_submission_time': 3345.0114874839783, 'accumulated_eval_time': 1238.153163909912, 'accumulated_logging_time': 3.352001905441284}
I0315 13:37:16.524856 140224977958656 logging_writer.py:48] [24953] accumulated_eval_time=1238.15, accumulated_logging_time=3.352, accumulated_submission_time=3345.01, global_step=24953, preemption_count=0, score=3345.01, test/loss=0.289714, test/num_examples=3581, test/ssim=0.739691, total_duration=4648.66, train/loss=0.274061, train/ssim=0.742664, validation/loss=0.288249, validation/num_examples=3554, validation/ssim=0.722326
I0315 13:37:20.301381 140225061820160 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.293275, loss=0.227793
I0315 13:37:20.304434 140277216793792 submission.py:265] 25000) loss = 0.228, grad_norm = 0.293
I0315 13:37:51.741628 140224977958656 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.305661, loss=0.271758
I0315 13:37:51.744757 140277216793792 submission.py:265] 25500) loss = 0.272, grad_norm = 0.306
I0315 13:38:23.094000 140225061820160 logging_writer.py:48] [26000] global_step=26000, grad_norm=0.452507, loss=0.266433
I0315 13:38:23.097246 140277216793792 submission.py:265] 26000) loss = 0.266, grad_norm = 0.453
I0315 13:38:37.236218 140277216793792 spec.py:321] Evaluating on the training split.
I0315 13:38:39.233111 140277216793792 spec.py:333] Evaluating on the validation split.
I0315 13:38:42.039229 140277216793792 spec.py:349] Evaluating on the test split.
I0315 13:38:45.572555 140277216793792 submission_runner.py:469] Time since start: 4737.73s, 	Step: 26216, 	{'train/ssim': 0.7421885899135044, 'train/loss': 0.2741767168045044, 'validation/ssim': 0.7216151145716094, 'validation/loss': 0.2887100966780388, 'validation/num_examples': 3554, 'test/ssim': 0.7390016046059061, 'test/loss': 0.2901923236351578, 'test/num_examples': 3581, 'score': 3423.9022533893585, 'total_duration': 4737.725488901138, 'accumulated_submission_time': 3423.9022533893585, 'accumulated_eval_time': 1246.489717721939, 'accumulated_logging_time': 3.3736090660095215}
I0315 13:38:45.589760 140224977958656 logging_writer.py:48] [26216] accumulated_eval_time=1246.49, accumulated_logging_time=3.37361, accumulated_submission_time=3423.9, global_step=26216, preemption_count=0, score=3423.9, test/loss=0.290192, test/num_examples=3581, test/ssim=0.739002, total_duration=4737.73, train/loss=0.274177, train/ssim=0.742189, validation/loss=0.28871, validation/num_examples=3554, validation/ssim=0.721615
I0315 13:39:04.344548 140225061820160 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.260204, loss=0.237386
I0315 13:39:04.347731 140277216793792 submission.py:265] 26500) loss = 0.237, grad_norm = 0.260
I0315 13:39:35.826023 140224977958656 logging_writer.py:48] [27000] global_step=27000, grad_norm=0.211434, loss=0.325394
I0315 13:39:35.829568 140277216793792 submission.py:265] 27000) loss = 0.325, grad_norm = 0.211
I0315 13:40:06.340564 140277216793792 spec.py:321] Evaluating on the training split.
I0315 13:40:08.353233 140277216793792 spec.py:333] Evaluating on the validation split.
I0315 13:40:10.487872 140277216793792 spec.py:349] Evaluating on the test split.
I0315 13:40:12.703188 140277216793792 submission_runner.py:469] Time since start: 4824.86s, 	Step: 27477, 	{'train/ssim': 0.7401562418256488, 'train/loss': 0.27516184534345356, 'validation/ssim': 0.7194276035804728, 'validation/loss': 0.28934617436998805, 'validation/num_examples': 3554, 'test/ssim': 0.7370118005619939, 'test/loss': 0.2906777414653728, 'test/num_examples': 3581, 'score': 3502.6981377601624, 'total_duration': 4824.856132268906, 'accumulated_submission_time': 3502.6981377601624, 'accumulated_eval_time': 1252.852541923523, 'accumulated_logging_time': 3.403744697570801}
I0315 13:40:12.716977 140225061820160 logging_writer.py:48] [27477] accumulated_eval_time=1252.85, accumulated_logging_time=3.40374, accumulated_submission_time=3502.7, global_step=27477, preemption_count=0, score=3502.7, test/loss=0.290678, test/num_examples=3581, test/ssim=0.737012, total_duration=4824.86, train/loss=0.275162, train/ssim=0.740156, validation/loss=0.289346, validation/num_examples=3554, validation/ssim=0.719428
I0315 13:40:15.140379 140224977958656 logging_writer.py:48] [27500] global_step=27500, grad_norm=0.262521, loss=0.278038
I0315 13:40:15.143873 140277216793792 submission.py:265] 27500) loss = 0.278, grad_norm = 0.263
I0315 13:40:46.634398 140225061820160 logging_writer.py:48] [28000] global_step=28000, grad_norm=0.307112, loss=0.265221
I0315 13:40:46.637907 140277216793792 submission.py:265] 28000) loss = 0.265, grad_norm = 0.307
I0315 13:41:18.006009 140224977958656 logging_writer.py:48] [28500] global_step=28500, grad_norm=0.28661, loss=0.220665
I0315 13:41:18.009252 140277216793792 submission.py:265] 28500) loss = 0.221, grad_norm = 0.287
I0315 13:41:33.413574 140277216793792 spec.py:321] Evaluating on the training split.
I0315 13:41:35.512153 140277216793792 spec.py:333] Evaluating on the validation split.
I0315 13:41:38.930172 140277216793792 spec.py:349] Evaluating on the test split.
I0315 13:41:41.652306 140277216793792 submission_runner.py:469] Time since start: 4913.81s, 	Step: 28736, 	{'train/ssim': 0.7408768790108817, 'train/loss': 0.2746963160378592, 'validation/ssim': 0.7206333999762592, 'validation/loss': 0.2889705179243458, 'validation/num_examples': 3554, 'test/ssim': 0.7379760230862539, 'test/loss': 0.2904466225827283, 'test/num_examples': 3581, 'score': 3581.4699058532715, 'total_duration': 4913.805272579193, 'accumulated_submission_time': 3581.4699058532715, 'accumulated_eval_time': 1261.0914371013641, 'accumulated_logging_time': 3.426198720932007}
I0315 13:41:41.664333 140225061820160 logging_writer.py:48] [28736] accumulated_eval_time=1261.09, accumulated_logging_time=3.4262, accumulated_submission_time=3581.47, global_step=28736, preemption_count=0, score=3581.47, test/loss=0.290447, test/num_examples=3581, test/ssim=0.737976, total_duration=4913.81, train/loss=0.274696, train/ssim=0.740877, validation/loss=0.288971, validation/num_examples=3554, validation/ssim=0.720633
I0315 13:41:59.115448 140224977958656 logging_writer.py:48] [29000] global_step=29000, grad_norm=0.182809, loss=0.225254
I0315 13:41:59.118639 140277216793792 submission.py:265] 29000) loss = 0.225, grad_norm = 0.183
I0315 13:42:30.616510 140225061820160 logging_writer.py:48] [29500] global_step=29500, grad_norm=0.144486, loss=0.273183
I0315 13:42:30.620580 140277216793792 submission.py:265] 29500) loss = 0.273, grad_norm = 0.144
I0315 13:43:02.429456 140277216793792 spec.py:321] Evaluating on the training split.
I0315 13:43:04.419980 140277216793792 spec.py:333] Evaluating on the validation split.
I0315 13:43:06.522663 140277216793792 spec.py:349] Evaluating on the test split.
I0315 13:43:09.455884 140277216793792 submission_runner.py:469] Time since start: 5001.61s, 	Step: 29998, 	{'train/ssim': 0.7413381167820522, 'train/loss': 0.2749278885977609, 'validation/ssim': 0.7213543498522791, 'validation/loss': 0.2890232066861283, 'validation/num_examples': 3554, 'test/ssim': 0.7386825378298659, 'test/loss': 0.29048773310964116, 'test/num_examples': 3581, 'score': 3660.3380954265594, 'total_duration': 5001.60883307457, 'accumulated_submission_time': 3660.3380954265594, 'accumulated_eval_time': 1268.11807513237, 'accumulated_logging_time': 3.4461731910705566}
I0315 13:43:09.468576 140224977958656 logging_writer.py:48] [29998] accumulated_eval_time=1268.12, accumulated_logging_time=3.44617, accumulated_submission_time=3660.34, global_step=29998, preemption_count=0, score=3660.34, test/loss=0.290488, test/num_examples=3581, test/ssim=0.738683, total_duration=5001.61, train/loss=0.274928, train/ssim=0.741338, validation/loss=0.289023, validation/num_examples=3554, validation/ssim=0.721354
I0315 13:43:10.457974 140225061820160 logging_writer.py:48] [30000] global_step=30000, grad_norm=0.204673, loss=0.356631
I0315 13:43:10.461194 140277216793792 submission.py:265] 30000) loss = 0.357, grad_norm = 0.205
I0315 13:43:42.047849 140224977958656 logging_writer.py:48] [30500] global_step=30500, grad_norm=0.56125, loss=0.269734
I0315 13:43:42.051688 140277216793792 submission.py:265] 30500) loss = 0.270, grad_norm = 0.561
I0315 13:44:13.441678 140225061820160 logging_writer.py:48] [31000] global_step=31000, grad_norm=0.226124, loss=0.278752
I0315 13:44:13.444881 140277216793792 submission.py:265] 31000) loss = 0.279, grad_norm = 0.226
I0315 13:44:30.190963 140277216793792 spec.py:321] Evaluating on the training split.
I0315 13:44:32.290575 140277216793792 spec.py:333] Evaluating on the validation split.
I0315 13:44:35.971351 140277216793792 spec.py:349] Evaluating on the test split.
I0315 13:44:38.212666 140277216793792 submission_runner.py:469] Time since start: 5090.37s, 	Step: 31257, 	{'train/ssim': 0.7406712259565081, 'train/loss': 0.27588135855538504, 'validation/ssim': 0.7202927434316967, 'validation/loss': 0.290072138952149, 'validation/num_examples': 3554, 'test/ssim': 0.7376806136117705, 'test/loss': 0.29146818167934935, 'test/num_examples': 3581, 'score': 3739.2281789779663, 'total_duration': 5090.365613460541, 'accumulated_submission_time': 3739.2281789779663, 'accumulated_eval_time': 1276.1399400234222, 'accumulated_logging_time': 3.467808246612549}
I0315 13:44:38.225209 140224977958656 logging_writer.py:48] [31257] accumulated_eval_time=1276.14, accumulated_logging_time=3.46781, accumulated_submission_time=3739.23, global_step=31257, preemption_count=0, score=3739.23, test/loss=0.291468, test/num_examples=3581, test/ssim=0.737681, total_duration=5090.37, train/loss=0.275881, train/ssim=0.740671, validation/loss=0.290072, validation/num_examples=3554, validation/ssim=0.720293
I0315 13:44:54.367192 140225061820160 logging_writer.py:48] [31500] global_step=31500, grad_norm=0.314245, loss=0.284121
I0315 13:44:54.370360 140277216793792 submission.py:265] 31500) loss = 0.284, grad_norm = 0.314
I0315 13:45:25.785225 140224977958656 logging_writer.py:48] [32000] global_step=32000, grad_norm=0.486156, loss=0.339624
I0315 13:45:25.788695 140277216793792 submission.py:265] 32000) loss = 0.340, grad_norm = 0.486
I0315 13:45:57.132432 140225061820160 logging_writer.py:48] [32500] global_step=32500, grad_norm=0.13275, loss=0.26757
I0315 13:45:57.136100 140277216793792 submission.py:265] 32500) loss = 0.268, grad_norm = 0.133
I0315 13:45:58.939150 140277216793792 spec.py:321] Evaluating on the training split.
I0315 13:46:00.965060 140277216793792 spec.py:333] Evaluating on the validation split.
I0315 13:46:03.087629 140277216793792 spec.py:349] Evaluating on the test split.
I0315 13:46:05.269787 140277216793792 submission_runner.py:469] Time since start: 5177.42s, 	Step: 32519, 	{'train/ssim': 0.736919811793736, 'train/loss': 0.2802351542881557, 'validation/ssim': 0.7159233544070062, 'validation/loss': 0.2944158361946926, 'validation/num_examples': 3554, 'test/ssim': 0.7336405328251536, 'test/loss': 0.29576764057482197, 'test/num_examples': 3581, 'score': 3818.0760264396667, 'total_duration': 5177.422677755356, 'accumulated_submission_time': 3818.0760264396667, 'accumulated_eval_time': 1282.4708037376404, 'accumulated_logging_time': 3.4880316257476807}
I0315 13:46:05.283987 140224977958656 logging_writer.py:48] [32519] accumulated_eval_time=1282.47, accumulated_logging_time=3.48803, accumulated_submission_time=3818.08, global_step=32519, preemption_count=0, score=3818.08, test/loss=0.295768, test/num_examples=3581, test/ssim=0.733641, total_duration=5177.42, train/loss=0.280235, train/ssim=0.73692, validation/loss=0.294416, validation/num_examples=3554, validation/ssim=0.715923
I0315 13:46:36.319852 140225061820160 logging_writer.py:48] [33000] global_step=33000, grad_norm=0.348899, loss=0.298391
I0315 13:46:36.323584 140277216793792 submission.py:265] 33000) loss = 0.298, grad_norm = 0.349
I0315 13:47:07.697630 140224977958656 logging_writer.py:48] [33500] global_step=33500, grad_norm=0.500253, loss=0.342122
I0315 13:47:07.700775 140277216793792 submission.py:265] 33500) loss = 0.342, grad_norm = 0.500
I0315 13:47:25.967359 140277216793792 spec.py:321] Evaluating on the training split.
I0315 13:47:28.042431 140277216793792 spec.py:333] Evaluating on the validation split.
I0315 13:47:30.231894 140277216793792 spec.py:349] Evaluating on the test split.
I0315 13:47:32.462135 140277216793792 submission_runner.py:469] Time since start: 5264.62s, 	Step: 33783, 	{'train/ssim': 0.7405009950910296, 'train/loss': 0.27514195442199707, 'validation/ssim': 0.7197717635498734, 'validation/loss': 0.2895068853976154, 'validation/num_examples': 3554, 'test/ssim': 0.737026526720888, 'test/loss': 0.29110657266650375, 'test/num_examples': 3581, 'score': 3896.9001898765564, 'total_duration': 5264.615083694458, 'accumulated_submission_time': 3896.9001898765564, 'accumulated_eval_time': 1288.9657604694366, 'accumulated_logging_time': 3.5106277465820312}
I0315 13:47:32.475078 140225061820160 logging_writer.py:48] [33783] accumulated_eval_time=1288.97, accumulated_logging_time=3.51063, accumulated_submission_time=3896.9, global_step=33783, preemption_count=0, score=3896.9, test/loss=0.291107, test/num_examples=3581, test/ssim=0.737027, total_duration=5264.62, train/loss=0.275142, train/ssim=0.740501, validation/loss=0.289507, validation/num_examples=3554, validation/ssim=0.719772
I0315 13:47:46.996885 140224977958656 logging_writer.py:48] [34000] global_step=34000, grad_norm=0.115474, loss=0.339853
I0315 13:47:47.000313 140277216793792 submission.py:265] 34000) loss = 0.340, grad_norm = 0.115
I0315 13:48:18.341017 140225061820160 logging_writer.py:48] [34500] global_step=34500, grad_norm=0.179057, loss=0.307025
I0315 13:48:18.344405 140277216793792 submission.py:265] 34500) loss = 0.307, grad_norm = 0.179
I0315 13:48:49.738277 140224977958656 logging_writer.py:48] [35000] global_step=35000, grad_norm=0.293612, loss=0.315679
I0315 13:48:49.741887 140277216793792 submission.py:265] 35000) loss = 0.316, grad_norm = 0.294
I0315 13:48:53.149370 140277216793792 spec.py:321] Evaluating on the training split.
I0315 13:48:55.133752 140277216793792 spec.py:333] Evaluating on the validation split.
I0315 13:48:57.242000 140277216793792 spec.py:349] Evaluating on the test split.
I0315 13:48:59.402443 140277216793792 submission_runner.py:469] Time since start: 5351.56s, 	Step: 35045, 	{'train/ssim': 0.74160064969744, 'train/loss': 0.2746377672467913, 'validation/ssim': 0.7210277070070695, 'validation/loss': 0.2889072845407463, 'validation/num_examples': 3554, 'test/ssim': 0.7383947641414759, 'test/loss': 0.2904036712859536, 'test/num_examples': 3581, 'score': 3975.7700238227844, 'total_duration': 5351.555396080017, 'accumulated_submission_time': 3975.7700238227844, 'accumulated_eval_time': 1295.2190239429474, 'accumulated_logging_time': 3.531580686569214}
I0315 13:48:59.415672 140225061820160 logging_writer.py:48] [35045] accumulated_eval_time=1295.22, accumulated_logging_time=3.53158, accumulated_submission_time=3975.77, global_step=35045, preemption_count=0, score=3975.77, test/loss=0.290404, test/num_examples=3581, test/ssim=0.738395, total_duration=5351.56, train/loss=0.274638, train/ssim=0.741601, validation/loss=0.288907, validation/num_examples=3554, validation/ssim=0.721028
I0315 13:49:28.897585 140224977958656 logging_writer.py:48] [35500] global_step=35500, grad_norm=0.182392, loss=0.27528
I0315 13:49:28.900961 140277216793792 submission.py:265] 35500) loss = 0.275, grad_norm = 0.182
I0315 13:50:00.384869 140225061820160 logging_writer.py:48] [36000] global_step=36000, grad_norm=0.320188, loss=0.25201
I0315 13:50:00.388857 140277216793792 submission.py:265] 36000) loss = 0.252, grad_norm = 0.320
I0315 13:50:20.122492 140277216793792 spec.py:321] Evaluating on the training split.
I0315 13:50:22.131393 140277216793792 spec.py:333] Evaluating on the validation split.
I0315 13:50:24.235296 140277216793792 spec.py:349] Evaluating on the test split.
I0315 13:50:26.424582 140277216793792 submission_runner.py:469] Time since start: 5438.58s, 	Step: 36305, 	{'train/ssim': 0.7415962219238281, 'train/loss': 0.2758822100503104, 'validation/ssim': 0.7213790799099606, 'validation/loss': 0.2900364864523248, 'validation/num_examples': 3554, 'test/ssim': 0.7384332839552499, 'test/loss': 0.2916355212951166, 'test/num_examples': 3581, 'score': 4054.6342635154724, 'total_duration': 5438.57750916481, 'accumulated_submission_time': 4054.6342635154724, 'accumulated_eval_time': 1301.5212895870209, 'accumulated_logging_time': 3.552830219268799}
I0315 13:50:26.437132 140224977958656 logging_writer.py:48] [36305] accumulated_eval_time=1301.52, accumulated_logging_time=3.55283, accumulated_submission_time=4054.63, global_step=36305, preemption_count=0, score=4054.63, test/loss=0.291636, test/num_examples=3581, test/ssim=0.738433, total_duration=5438.58, train/loss=0.275882, train/ssim=0.741596, validation/loss=0.290036, validation/num_examples=3554, validation/ssim=0.721379
I0315 13:50:39.522227 140225061820160 logging_writer.py:48] [36500] global_step=36500, grad_norm=0.244053, loss=0.308772
I0315 13:50:39.525785 140277216793792 submission.py:265] 36500) loss = 0.309, grad_norm = 0.244
I0315 13:51:10.886307 140224977958656 logging_writer.py:48] [37000] global_step=37000, grad_norm=0.23842, loss=0.289679
I0315 13:51:10.889978 140277216793792 submission.py:265] 37000) loss = 0.290, grad_norm = 0.238
I0315 13:51:42.281834 140225061820160 logging_writer.py:48] [37500] global_step=37500, grad_norm=0.309431, loss=0.340446
I0315 13:51:42.285458 140277216793792 submission.py:265] 37500) loss = 0.340, grad_norm = 0.309
I0315 13:51:47.090835 140277216793792 spec.py:321] Evaluating on the training split.
I0315 13:51:49.095170 140277216793792 spec.py:333] Evaluating on the validation split.
I0315 13:51:51.200913 140277216793792 spec.py:349] Evaluating on the test split.
I0315 13:51:53.363554 140277216793792 submission_runner.py:469] Time since start: 5525.52s, 	Step: 37568, 	{'train/ssim': 0.7410045351300921, 'train/loss': 0.2762258563722883, 'validation/ssim': 0.7206177376063942, 'validation/loss': 0.29053359495902503, 'validation/num_examples': 3554, 'test/ssim': 0.7379397531023109, 'test/loss': 0.29203493426679, 'test/num_examples': 3581, 'score': 4133.487586021423, 'total_duration': 5525.51647901535, 'accumulated_submission_time': 4133.487586021423, 'accumulated_eval_time': 1307.7940692901611, 'accumulated_logging_time': 3.5736191272735596}
I0315 13:51:53.406531 140224977958656 logging_writer.py:48] [37568] accumulated_eval_time=1307.79, accumulated_logging_time=3.57362, accumulated_submission_time=4133.49, global_step=37568, preemption_count=0, score=4133.49, test/loss=0.292035, test/num_examples=3581, test/ssim=0.73794, total_duration=5525.52, train/loss=0.276226, train/ssim=0.741005, validation/loss=0.290534, validation/num_examples=3554, validation/ssim=0.720618
I0315 13:52:21.406402 140225061820160 logging_writer.py:48] [38000] global_step=38000, grad_norm=0.229823, loss=0.265509
I0315 13:52:21.410113 140277216793792 submission.py:265] 38000) loss = 0.266, grad_norm = 0.230
I0315 13:52:52.773489 140224977958656 logging_writer.py:48] [38500] global_step=38500, grad_norm=0.317356, loss=0.349628
I0315 13:52:52.777319 140277216793792 submission.py:265] 38500) loss = 0.350, grad_norm = 0.317
I0315 13:53:14.069736 140277216793792 spec.py:321] Evaluating on the training split.
I0315 13:53:16.046157 140277216793792 spec.py:333] Evaluating on the validation split.
I0315 13:53:18.141390 140277216793792 spec.py:349] Evaluating on the test split.
I0315 13:53:20.293977 140277216793792 submission_runner.py:469] Time since start: 5612.45s, 	Step: 38830, 	{'train/ssim': 0.7401762008666992, 'train/loss': 0.275310891015189, 'validation/ssim': 0.7204331552036438, 'validation/loss': 0.28916471757175016, 'validation/num_examples': 3554, 'test/ssim': 0.7378267161974658, 'test/loss': 0.2906204048930117, 'test/num_examples': 3581, 'score': 4212.351359844208, 'total_duration': 5612.446934461594, 'accumulated_submission_time': 4212.351359844208, 'accumulated_eval_time': 1314.0184860229492, 'accumulated_logging_time': 3.6260859966278076}
I0315 13:53:20.306517 140225061820160 logging_writer.py:48] [38830] accumulated_eval_time=1314.02, accumulated_logging_time=3.62609, accumulated_submission_time=4212.35, global_step=38830, preemption_count=0, score=4212.35, test/loss=0.29062, test/num_examples=3581, test/ssim=0.737827, total_duration=5612.45, train/loss=0.275311, train/ssim=0.740176, validation/loss=0.289165, validation/num_examples=3554, validation/ssim=0.720433
I0315 13:53:31.815166 140224977958656 logging_writer.py:48] [39000] global_step=39000, grad_norm=0.38152, loss=0.276693
I0315 13:53:31.818725 140277216793792 submission.py:265] 39000) loss = 0.277, grad_norm = 0.382
I0315 13:54:03.258512 140225061820160 logging_writer.py:48] [39500] global_step=39500, grad_norm=0.440611, loss=0.36086
I0315 13:54:03.261924 140277216793792 submission.py:265] 39500) loss = 0.361, grad_norm = 0.441
I0315 13:54:34.722728 140224977958656 logging_writer.py:48] [40000] global_step=40000, grad_norm=0.171859, loss=0.301259
I0315 13:54:34.726071 140277216793792 submission.py:265] 40000) loss = 0.301, grad_norm = 0.172
I0315 13:54:40.958476 140277216793792 spec.py:321] Evaluating on the training split.
I0315 13:54:42.944661 140277216793792 spec.py:333] Evaluating on the validation split.
I0315 13:54:45.024929 140277216793792 spec.py:349] Evaluating on the test split.
I0315 13:54:47.180404 140277216793792 submission_runner.py:469] Time since start: 5699.33s, 	Step: 40091, 	{'train/ssim': 0.740368161882673, 'train/loss': 0.27571747984204975, 'validation/ssim': 0.7196700955349606, 'validation/loss': 0.2900274187645083, 'validation/num_examples': 3554, 'test/ssim': 0.7371328141362399, 'test/loss': 0.29144595608768503, 'test/num_examples': 3581, 'score': 4291.179814577103, 'total_duration': 5699.333374738693, 'accumulated_submission_time': 4291.179814577103, 'accumulated_eval_time': 1320.2404849529266, 'accumulated_logging_time': 3.64676833152771}
I0315 13:54:47.193220 140225061820160 logging_writer.py:48] [40091] accumulated_eval_time=1320.24, accumulated_logging_time=3.64677, accumulated_submission_time=4291.18, global_step=40091, preemption_count=0, score=4291.18, test/loss=0.291446, test/num_examples=3581, test/ssim=0.737133, total_duration=5699.33, train/loss=0.275717, train/ssim=0.740368, validation/loss=0.290027, validation/num_examples=3554, validation/ssim=0.71967
I0315 13:55:13.801131 140224977958656 logging_writer.py:48] [40500] global_step=40500, grad_norm=0.186179, loss=0.283324
I0315 13:55:13.804258 140277216793792 submission.py:265] 40500) loss = 0.283, grad_norm = 0.186
I0315 13:55:45.204478 140225061820160 logging_writer.py:48] [41000] global_step=41000, grad_norm=0.230795, loss=0.249146
I0315 13:55:45.207789 140277216793792 submission.py:265] 41000) loss = 0.249, grad_norm = 0.231
I0315 13:56:07.948359 140277216793792 spec.py:321] Evaluating on the training split.
I0315 13:56:09.948478 140277216793792 spec.py:333] Evaluating on the validation split.
I0315 13:56:12.082739 140277216793792 spec.py:349] Evaluating on the test split.
I0315 13:56:14.282376 140277216793792 submission_runner.py:469] Time since start: 5786.44s, 	Step: 41352, 	{'train/ssim': 0.7423377718244281, 'train/loss': 0.2738336835588728, 'validation/ssim': 0.7219344070941193, 'validation/loss': 0.28816119243941685, 'validation/num_examples': 3554, 'test/ssim': 0.739262789396293, 'test/loss': 0.28965683004703646, 'test/num_examples': 3581, 'score': 4370.085615634918, 'total_duration': 5786.435327291489, 'accumulated_submission_time': 4370.085615634918, 'accumulated_eval_time': 1326.5746810436249, 'accumulated_logging_time': 3.6684045791625977}
I0315 13:56:14.297103 140224977958656 logging_writer.py:48] [41352] accumulated_eval_time=1326.57, accumulated_logging_time=3.6684, accumulated_submission_time=4370.09, global_step=41352, preemption_count=0, score=4370.09, test/loss=0.289657, test/num_examples=3581, test/ssim=0.739263, total_duration=5786.44, train/loss=0.273834, train/ssim=0.742338, validation/loss=0.288161, validation/num_examples=3554, validation/ssim=0.721934
I0315 13:56:24.528861 140225061820160 logging_writer.py:48] [41500] global_step=41500, grad_norm=0.785825, loss=0.298793
I0315 13:56:24.532072 140277216793792 submission.py:265] 41500) loss = 0.299, grad_norm = 0.786
I0315 13:56:55.846010 140224977958656 logging_writer.py:48] [42000] global_step=42000, grad_norm=0.235478, loss=0.277186
I0315 13:56:55.849226 140277216793792 submission.py:265] 42000) loss = 0.277, grad_norm = 0.235
I0315 13:57:27.186491 140225061820160 logging_writer.py:48] [42500] global_step=42500, grad_norm=0.433448, loss=0.27899
I0315 13:57:27.189666 140277216793792 submission.py:265] 42500) loss = 0.279, grad_norm = 0.433
I0315 13:57:34.325578 140224977958656 logging_writer.py:48] [42615] global_step=42615, preemption_count=0, score=4448.9
I0315 13:57:35.734190 140277216793792 submission_runner.py:646] Tuning trial 1/5
I0315 13:57:35.734438 140277216793792 submission_runner.py:647] Hyperparameters: Hyperparameters(learning_rate=0.0017486387539278373, one_minus_beta1=0.06733926164, one_minus_beta2=0.00448403102, epsilon=1e-08, one_minus_momentum=0.0, use_momentum=False, weight_decay=0.08121616522670176, max_preconditioner_dim=1024, precondition_frequency=100, start_preconditioning_step=-1, inv_root_override=0, exponent_multiplier=1.0, grafting_type='ADAM', grafting_epsilon=1e-08, use_normalized_grafting=False, communication_dtype='FP32', communicate_params=True, use_cosine_decay=True, warmup_factor=0.02, label_smoothing=0.0, dropout_rate=0.1, use_nadam=True, step_hint_factor=1.0)
I0315 13:57:35.735866 140277216793792 submission_runner.py:648] Metrics: {'eval_results': [(1, {'train/ssim': 0.20035191944667272, 'train/loss': 1.0021626608712333, 'validation/ssim': 0.18794720187640687, 'validation/loss': 1.0060458121570766, 'validation/num_examples': 3554, 'test/ssim': 0.21181421777916434, 'test/loss': 1.0037169915875455, 'test/num_examples': 3581, 'score': 315.27763319015503, 'total_duration': 1295.066995382309, 'accumulated_submission_time': 315.27763319015503, 'accumulated_eval_time': 979.0199332237244, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (238, {'train/ssim': 0.692598819732666, 'train/loss': 0.3224166120801653, 'validation/ssim': 0.672557343508195, 'validation/loss': 0.3372185856825056, 'validation/num_examples': 3554, 'test/ssim': 0.6913191202265428, 'test/loss': 0.33819537467711536, 'test/num_examples': 3581, 'score': 394.91109442710876, 'total_duration': 1383.4408266544342, 'accumulated_submission_time': 394.91109442710876, 'accumulated_eval_time': 986.0426678657532, 'accumulated_logging_time': 0.017799854278564453, 'global_step': 238, 'preemption_count': 0}), (280, {'train/ssim': 0.6997325760977608, 'train/loss': 0.3164307049342564, 'validation/ssim': 0.6804400493941686, 'validation/loss': 0.3302287379207407, 'validation/num_examples': 3554, 'test/ssim': 0.6989031602336987, 'test/loss': 0.33158155674043566, 'test/num_examples': 3581, 'score': 476.79410314559937, 'total_duration': 1473.330650806427, 'accumulated_submission_time': 476.79410314559937, 'accumulated_eval_time': 992.6574938297272, 'accumulated_logging_time': 0.03784441947937012, 'global_step': 280, 'preemption_count': 0}), (317, {'train/ssim': 0.701486315046038, 'train/loss': 0.31354171889168875, 'validation/ssim': 0.682007454188942, 'validation/loss': 0.3271917151009426, 'validation/num_examples': 3554, 'test/ssim': 0.7004668601429419, 'test/loss': 0.3286023388958217, 'test/num_examples': 3581, 'score': 558.4852890968323, 'total_duration': 1562.8097002506256, 'accumulated_submission_time': 558.4852890968323, 'accumulated_eval_time': 999.0640578269958, 'accumulated_logging_time': 0.05714273452758789, 'global_step': 317, 'preemption_count': 0}), (364, {'train/ssim': 0.701453617640904, 'train/loss': 0.3108607700892857, 'validation/ssim': 0.6822621050884566, 'validation/loss': 0.3240871998320554, 'validation/num_examples': 3554, 'test/ssim': 0.7006460965861491, 'test/loss': 0.3256859117564926, 'test/num_examples': 3581, 'score': 639.4383788108826, 'total_duration': 1651.9408326148987, 'accumulated_submission_time': 639.4383788108826, 'accumulated_eval_time': 1005.7585783004761, 'accumulated_logging_time': 0.07626605033874512, 'global_step': 364, 'preemption_count': 0}), (402, {'train/ssim': 0.7066734177725655, 'train/loss': 0.30737478392464773, 'validation/ssim': 0.688071333027047, 'validation/loss': 0.3203841141253693, 'validation/num_examples': 3554, 'test/ssim': 0.7060993432133134, 'test/loss': 0.3220678104165212, 'test/num_examples': 3581, 'score': 721.4700582027435, 'total_duration': 1742.2398929595947, 'accumulated_submission_time': 721.4700582027435, 'accumulated_eval_time': 1012.6411008834839, 'accumulated_logging_time': 0.09595251083374023, 'global_step': 402, 'preemption_count': 0}), (440, {'train/ssim': 0.7111601148332868, 'train/loss': 0.3036275931767055, 'validation/ssim': 0.6923874837331176, 'validation/loss': 0.31679908009680996, 'validation/num_examples': 3554, 'test/ssim': 0.7101665583112259, 'test/loss': 0.318754186047019, 'test/num_examples': 3581, 'score': 801.35311627388, 'total_duration': 1830.24436211586, 'accumulated_submission_time': 801.35311627388, 'accumulated_eval_time': 1019.4608488082886, 'accumulated_logging_time': 0.11503100395202637, 'global_step': 440, 'preemption_count': 0}), (482, {'train/ssim': 0.7122256415230888, 'train/loss': 0.3027702740260533, 'validation/ssim': 0.693476018438731, 'validation/loss': 0.3158179494055993, 'validation/num_examples': 3554, 'test/ssim': 0.7111358259084404, 'test/loss': 0.3177734988589954, 'test/num_examples': 3581, 'score': 881.7598075866699, 'total_duration': 1918.8398296833038, 'accumulated_submission_time': 881.7598075866699, 'accumulated_eval_time': 1026.3583586215973, 'accumulated_logging_time': 0.13406157493591309, 'global_step': 482, 'preemption_count': 0}), (522, {'train/ssim': 0.7121241433279855, 'train/loss': 0.3003976345062256, 'validation/ssim': 0.6937393248584341, 'validation/loss': 0.31317039064698227, 'validation/num_examples': 3554, 'test/ssim': 0.7113419921329587, 'test/loss': 0.31510618924095923, 'test/num_examples': 3581, 'score': 961.538745880127, 'total_duration': 2006.9604222774506, 'accumulated_submission_time': 961.538745880127, 'accumulated_eval_time': 1033.336349248886, 'accumulated_logging_time': 0.15690040588378906, 'global_step': 522, 'preemption_count': 0}), (564, {'train/ssim': 0.7153910228184291, 'train/loss': 0.298809289932251, 'validation/ssim': 0.6967502780757597, 'validation/loss': 0.31150269172938944, 'validation/num_examples': 3554, 'test/ssim': 0.7143418334176906, 'test/loss': 0.31345358696505865, 'test/num_examples': 3581, 'score': 1041.7175447940826, 'total_duration': 2095.826117515564, 'accumulated_submission_time': 1041.7175447940826, 'accumulated_eval_time': 1040.581214427948, 'accumulated_logging_time': 0.17691636085510254, 'global_step': 564, 'preemption_count': 0}), (609, {'train/ssim': 0.7189242499215263, 'train/loss': 0.297264678137643, 'validation/ssim': 0.7011448780203644, 'validation/loss': 0.3093167950614624, 'validation/num_examples': 3554, 'test/ssim': 0.7183652109222284, 'test/loss': 0.3116664380475949, 'test/num_examples': 3581, 'score': 1121.3416063785553, 'total_duration': 2184.3562257289886, 'accumulated_submission_time': 1121.3416063785553, 'accumulated_eval_time': 1048.1118938922882, 'accumulated_logging_time': 0.19640278816223145, 'global_step': 609, 'preemption_count': 0}), (647, {'train/ssim': 0.7178728921072823, 'train/loss': 0.2947742257799421, 'validation/ssim': 0.6993536662035734, 'validation/loss': 0.3074692536688414, 'validation/num_examples': 3554, 'test/ssim': 0.716854825162315, 'test/loss': 0.3094894549794052, 'test/num_examples': 3581, 'score': 1198.9645037651062, 'total_duration': 2272.2431750297546, 'accumulated_submission_time': 1198.9645037651062, 'accumulated_eval_time': 1054.4342849254608, 'accumulated_logging_time': 2.7610230445861816, 'global_step': 647, 'preemption_count': 0}), (688, {'train/ssim': 0.7154607772827148, 'train/loss': 0.2961082799094064, 'validation/ssim': 0.6972765474421426, 'validation/loss': 0.30846096332917133, 'validation/num_examples': 3554, 'test/ssim': 0.7147528705101578, 'test/loss': 0.31041519177822186, 'test/num_examples': 3581, 'score': 1278.6214230060577, 'total_duration': 2359.660080909729, 'accumulated_submission_time': 1278.6214230060577, 'accumulated_eval_time': 1060.7642035484314, 'accumulated_logging_time': 2.783500909805298, 'global_step': 688, 'preemption_count': 0}), (729, {'train/ssim': 0.7196531976972308, 'train/loss': 0.2942067895616804, 'validation/ssim': 0.7012525911604882, 'validation/loss': 0.30631367302906937, 'validation/num_examples': 3554, 'test/ssim': 0.7182435155813669, 'test/loss': 0.3086612108066008, 'test/num_examples': 3581, 'score': 1358.1507115364075, 'total_duration': 2447.1568472385406, 'accumulated_submission_time': 1358.1507115364075, 'accumulated_eval_time': 1067.2299966812134, 'accumulated_logging_time': 2.8034799098968506, 'global_step': 729, 'preemption_count': 0}), (768, {'train/ssim': 0.7208300999232701, 'train/loss': 0.2946338312966483, 'validation/ssim': 0.7022163077694148, 'validation/loss': 0.306895516330631, 'validation/num_examples': 3554, 'test/ssim': 0.7196263428075258, 'test/loss': 0.30893401971778134, 'test/num_examples': 3581, 'score': 1438.7897419929504, 'total_duration': 2535.7643020153046, 'accumulated_submission_time': 1438.7897419929504, 'accumulated_eval_time': 1073.797688961029, 'accumulated_logging_time': 2.831324338912964, 'global_step': 768, 'preemption_count': 0}), (809, {'train/ssim': 0.7218262127467564, 'train/loss': 0.29218901906694683, 'validation/ssim': 0.7024469155572945, 'validation/loss': 0.3051948440027962, 'validation/num_examples': 3554, 'test/ssim': 0.7199035491133762, 'test/loss': 0.3071372919793703, 'test/num_examples': 3581, 'score': 1520.3666110038757, 'total_duration': 2625.298675060272, 'accumulated_submission_time': 1520.3666110038757, 'accumulated_eval_time': 1080.4310355186462, 'accumulated_logging_time': 2.8585972785949707, 'global_step': 809, 'preemption_count': 0}), (848, {'train/ssim': 0.7231434413364956, 'train/loss': 0.29054369245256695, 'validation/ssim': 0.7042376465118528, 'validation/loss': 0.30344958887652995, 'validation/num_examples': 3554, 'test/ssim': 0.7214086852703505, 'test/loss': 0.3056276220744031, 'test/num_examples': 3581, 'score': 1602.0433101654053, 'total_duration': 2715.3783752918243, 'accumulated_submission_time': 1602.0433101654053, 'accumulated_eval_time': 1087.4702489376068, 'accumulated_logging_time': 2.8900163173675537, 'global_step': 848, 'preemption_count': 0}), (891, {'train/ssim': 0.7219435146876744, 'train/loss': 0.29090915407453266, 'validation/ssim': 0.7036784037352279, 'validation/loss': 0.30327963842457445, 'validation/num_examples': 3554, 'test/ssim': 0.7208191616779531, 'test/loss': 0.3052983287991308, 'test/num_examples': 3581, 'score': 1682.279399394989, 'total_duration': 2804.5811598300934, 'accumulated_submission_time': 1682.279399394989, 'accumulated_eval_time': 1095.0295987129211, 'accumulated_logging_time': 2.9098739624023438, 'global_step': 891, 'preemption_count': 0}), (933, {'train/ssim': 0.7210794176374163, 'train/loss': 0.29396060534885954, 'validation/ssim': 0.7033186500905669, 'validation/loss': 0.30580784030757596, 'validation/num_examples': 3554, 'test/ssim': 0.7207256232983106, 'test/loss': 0.30785410139887603, 'test/num_examples': 3581, 'score': 1762.830902338028, 'total_duration': 2893.0010199546814, 'accumulated_submission_time': 1762.830902338028, 'accumulated_eval_time': 1101.4928722381592, 'accumulated_logging_time': 2.9324727058410645, 'global_step': 933, 'preemption_count': 0}), (1036, {'train/ssim': 0.7256838253566197, 'train/loss': 0.2889464242117746, 'validation/ssim': 0.7066664816491981, 'validation/loss': 0.30183907821732553, 'validation/num_examples': 3554, 'test/ssim': 0.723941516423485, 'test/loss': 0.3037558659199595, 'test/num_examples': 3581, 'score': 1846.844562292099, 'total_duration': 2985.789575815201, 'accumulated_submission_time': 1846.844562292099, 'accumulated_eval_time': 1108.750801563263, 'accumulated_logging_time': 2.9599575996398926, 'global_step': 1036, 'preemption_count': 0}), (2254, {'train/ssim': 0.7291193008422852, 'train/loss': 0.28545824119022917, 'validation/ssim': 0.7112198348251969, 'validation/loss': 0.2979495897008652, 'validation/num_examples': 3554, 'test/ssim': 0.7283348204935772, 'test/loss': 0.29985138851315973, 'test/num_examples': 3581, 'score': 1925.662005662918, 'total_duration': 3073.910534620285, 'accumulated_submission_time': 1925.662005662918, 'accumulated_eval_time': 1116.1295981407166, 'accumulated_logging_time': 2.9858016967773438, 'global_step': 2254, 'preemption_count': 0}), (3516, {'train/ssim': 0.7321172441755023, 'train/loss': 0.28134213175092426, 'validation/ssim': 0.7116756235271877, 'validation/loss': 0.29548668203960327, 'validation/num_examples': 3554, 'test/ssim': 0.7289869984379364, 'test/loss': 0.29729237752199106, 'test/num_examples': 3581, 'score': 2004.4994041919708, 'total_duration': 3162.2553741931915, 'accumulated_submission_time': 2004.4994041919708, 'accumulated_eval_time': 1123.7229735851288, 'accumulated_logging_time': 3.0076403617858887, 'global_step': 3516, 'preemption_count': 0}), (4779, {'train/ssim': 0.7375610215323312, 'train/loss': 0.27900091239384245, 'validation/ssim': 0.717322320031127, 'validation/loss': 0.2931784402807576, 'validation/num_examples': 3554, 'test/ssim': 0.7345630312325467, 'test/loss': 0.2950224014874511, 'test/num_examples': 3581, 'score': 2083.332499027252, 'total_duration': 3249.3145785331726, 'accumulated_submission_time': 2083.332499027252, 'accumulated_eval_time': 1130.0850768089294, 'accumulated_logging_time': 3.0274105072021484, 'global_step': 4779, 'preemption_count': 0}), (6041, {'train/ssim': 0.7392300878252301, 'train/loss': 0.2778095177241734, 'validation/ssim': 0.7190868096467009, 'validation/loss': 0.29196615224482975, 'validation/num_examples': 3554, 'test/ssim': 0.736458478776878, 'test/loss': 0.29353441176085593, 'test/num_examples': 3581, 'score': 2162.1475336551666, 'total_duration': 3336.4113862514496, 'accumulated_submission_time': 2162.1475336551666, 'accumulated_eval_time': 1136.5017354488373, 'accumulated_logging_time': 3.046607732772827, 'global_step': 6041, 'preemption_count': 0}), (7307, {'train/ssim': 0.7380869729178292, 'train/loss': 0.2769366843359811, 'validation/ssim': 0.7177812686849324, 'validation/loss': 0.2907457925928531, 'validation/num_examples': 3554, 'test/ssim': 0.7351685081637461, 'test/loss': 0.29236504566200083, 'test/num_examples': 3581, 'score': 2240.988727092743, 'total_duration': 3423.4635763168335, 'accumulated_submission_time': 2240.988727092743, 'accumulated_eval_time': 1142.8688337802887, 'accumulated_logging_time': 3.065983772277832, 'global_step': 7307, 'preemption_count': 0}), (8569, {'train/ssim': 0.7384975978306362, 'train/loss': 0.27777375493730816, 'validation/ssim': 0.717862946569886, 'validation/loss': 0.2922648707332407, 'validation/num_examples': 3554, 'test/ssim': 0.7352596603602346, 'test/loss': 0.2938173449062762, 'test/num_examples': 3581, 'score': 2319.8484699726105, 'total_duration': 3510.5809857845306, 'accumulated_submission_time': 2319.8484699726105, 'accumulated_eval_time': 1149.296410560608, 'accumulated_logging_time': 3.0861377716064453, 'global_step': 8569, 'preemption_count': 0}), (9830, {'train/ssim': 0.7398907797677177, 'train/loss': 0.2754675831113543, 'validation/ssim': 0.7196196050005276, 'validation/loss': 0.28954968213632526, 'validation/num_examples': 3554, 'test/ssim': 0.7371660161704133, 'test/loss': 0.29102455614266265, 'test/num_examples': 3581, 'score': 2398.737003803253, 'total_duration': 3598.768073320389, 'accumulated_submission_time': 2398.737003803253, 'accumulated_eval_time': 1156.756095647812, 'accumulated_logging_time': 3.108621597290039, 'global_step': 9830, 'preemption_count': 0}), (11090, {'train/ssim': 0.7411518096923828, 'train/loss': 0.2751688616616385, 'validation/ssim': 0.7209136052687113, 'validation/loss': 0.28905326057567177, 'validation/num_examples': 3554, 'test/ssim': 0.7383291781930675, 'test/loss': 0.29056627262374335, 'test/num_examples': 3581, 'score': 2477.603764295578, 'total_duration': 3686.8191261291504, 'accumulated_submission_time': 2477.603764295578, 'accumulated_eval_time': 1164.124282836914, 'accumulated_logging_time': 3.1302247047424316, 'global_step': 11090, 'preemption_count': 0}), (12349, {'train/ssim': 0.7420684950692313, 'train/loss': 0.27451796191079275, 'validation/ssim': 0.7214949677080402, 'validation/loss': 0.28897285354090463, 'validation/num_examples': 3554, 'test/ssim': 0.7389023393866937, 'test/loss': 0.290456303668668, 'test/num_examples': 3581, 'score': 2556.424582004547, 'total_duration': 3774.1689546108246, 'accumulated_submission_time': 2556.424582004547, 'accumulated_eval_time': 1170.7422409057617, 'accumulated_logging_time': 3.1521244049072266, 'global_step': 12349, 'preemption_count': 0}), (13609, {'train/ssim': 0.7424231937953404, 'train/loss': 0.27425878388541086, 'validation/ssim': 0.7219842793771103, 'validation/loss': 0.28842892966112127, 'validation/num_examples': 3554, 'test/ssim': 0.7393583048991204, 'test/loss': 0.28991934428232335, 'test/num_examples': 3581, 'score': 2635.2940576076508, 'total_duration': 3861.1491072177887, 'accumulated_submission_time': 2635.2940576076508, 'accumulated_eval_time': 1177.0173680782318, 'accumulated_logging_time': 3.1716830730438232, 'global_step': 13609, 'preemption_count': 0}), (14869, {'train/ssim': 0.7424225807189941, 'train/loss': 0.2739834615162441, 'validation/ssim': 0.721989981029298, 'validation/loss': 0.28824094687543966, 'validation/num_examples': 3554, 'test/ssim': 0.7394188457745742, 'test/loss': 0.2896622841799602, 'test/num_examples': 3581, 'score': 2714.133480787277, 'total_duration': 3949.95032787323, 'accumulated_submission_time': 2714.133480787277, 'accumulated_eval_time': 1185.150752544403, 'accumulated_logging_time': 3.1931447982788086, 'global_step': 14869, 'preemption_count': 0}), (16127, {'train/ssim': 0.742307322365897, 'train/loss': 0.2739106927599226, 'validation/ssim': 0.7215789812095527, 'validation/loss': 0.28826509302898146, 'validation/num_examples': 3554, 'test/ssim': 0.7390093767453225, 'test/loss': 0.2897056445367041, 'test/num_examples': 3581, 'score': 2793.012162923813, 'total_duration': 4036.9145328998566, 'accumulated_submission_time': 2793.012162923813, 'accumulated_eval_time': 1191.4245555400848, 'accumulated_logging_time': 3.2131543159484863, 'global_step': 16127, 'preemption_count': 0}), (17391, {'train/ssim': 0.7427516664777484, 'train/loss': 0.27331994261060444, 'validation/ssim': 0.7221806772518641, 'validation/loss': 0.28761680487105196, 'validation/num_examples': 3554, 'test/ssim': 0.7396388518613864, 'test/loss': 0.2890334908305117, 'test/num_examples': 3581, 'score': 2871.9024336338043, 'total_duration': 4124.007575511932, 'accumulated_submission_time': 2871.9024336338043, 'accumulated_eval_time': 1197.7551157474518, 'accumulated_logging_time': 3.233142137527466, 'global_step': 17391, 'preemption_count': 0}), (18648, {'train/ssim': 0.742833001273019, 'train/loss': 0.27330963952200754, 'validation/ssim': 0.7222509518324424, 'validation/loss': 0.2876249795290078, 'validation/num_examples': 3554, 'test/ssim': 0.7396931886606395, 'test/loss': 0.2890536029456681, 'test/num_examples': 3581, 'score': 2950.720981836319, 'total_duration': 4213.158851146698, 'accumulated_submission_time': 2950.720981836319, 'accumulated_eval_time': 1206.1928584575653, 'accumulated_logging_time': 3.2527596950531006, 'global_step': 18648, 'preemption_count': 0}), (19906, {'train/ssim': 0.7421615464346749, 'train/loss': 0.2734713043485369, 'validation/ssim': 0.7216377150965462, 'validation/loss': 0.2877413653629713, 'validation/num_examples': 3554, 'test/ssim': 0.7391161413973052, 'test/loss': 0.28914305072561786, 'test/num_examples': 3581, 'score': 3029.5387382507324, 'total_duration': 4300.142775058746, 'accumulated_submission_time': 3029.5387382507324, 'accumulated_eval_time': 1212.459302663803, 'accumulated_logging_time': 3.272318124771118, 'global_step': 19906, 'preemption_count': 0}), (21168, {'train/ssim': 0.7424730573381696, 'train/loss': 0.2737385034561157, 'validation/ssim': 0.7221252407058948, 'validation/loss': 0.28799687494504433, 'validation/num_examples': 3554, 'test/ssim': 0.7395825379389486, 'test/loss': 0.2894143597502443, 'test/num_examples': 3581, 'score': 3108.4140136241913, 'total_duration': 4387.094579219818, 'accumulated_submission_time': 3108.4140136241913, 'accumulated_eval_time': 1218.699146747589, 'accumulated_logging_time': 3.2920308113098145, 'global_step': 21168, 'preemption_count': 0}), (22430, {'train/ssim': 0.7422384534563337, 'train/loss': 0.27371529170445036, 'validation/ssim': 0.7217710513242122, 'validation/loss': 0.2879422283870287, 'validation/num_examples': 3554, 'test/ssim': 0.7392724023055711, 'test/loss': 0.28934645579534346, 'test/num_examples': 3581, 'score': 3187.3127830028534, 'total_duration': 4474.1780552864075, 'accumulated_submission_time': 3187.3127830028534, 'accumulated_eval_time': 1225.0219326019287, 'accumulated_logging_time': 3.3118233680725098, 'global_step': 22430, 'preemption_count': 0}), (23692, {'train/ssim': 0.7412616184779576, 'train/loss': 0.2742901699883597, 'validation/ssim': 0.7207326636800084, 'validation/loss': 0.2885815003780951, 'validation/num_examples': 3554, 'test/ssim': 0.7382854087763544, 'test/loss': 0.28994474008874965, 'test/num_examples': 3581, 'score': 3266.1214163303375, 'total_duration': 4561.095770120621, 'accumulated_submission_time': 3266.1214163303375, 'accumulated_eval_time': 1231.2584619522095, 'accumulated_logging_time': 3.3319993019104004, 'global_step': 23692, 'preemption_count': 0}), (24953, {'train/ssim': 0.742664064679827, 'train/loss': 0.2740612540926252, 'validation/ssim': 0.7223258976461382, 'validation/loss': 0.28824850328195345, 'validation/num_examples': 3554, 'test/ssim': 0.7396910070074699, 'test/loss': 0.2897139280010821, 'test/num_examples': 3581, 'score': 3345.0114874839783, 'total_duration': 4648.664395332336, 'accumulated_submission_time': 3345.0114874839783, 'accumulated_eval_time': 1238.153163909912, 'accumulated_logging_time': 3.352001905441284, 'global_step': 24953, 'preemption_count': 0}), (26216, {'train/ssim': 0.7421885899135044, 'train/loss': 0.2741767168045044, 'validation/ssim': 0.7216151145716094, 'validation/loss': 0.2887100966780388, 'validation/num_examples': 3554, 'test/ssim': 0.7390016046059061, 'test/loss': 0.2901923236351578, 'test/num_examples': 3581, 'score': 3423.9022533893585, 'total_duration': 4737.725488901138, 'accumulated_submission_time': 3423.9022533893585, 'accumulated_eval_time': 1246.489717721939, 'accumulated_logging_time': 3.3736090660095215, 'global_step': 26216, 'preemption_count': 0}), (27477, {'train/ssim': 0.7401562418256488, 'train/loss': 0.27516184534345356, 'validation/ssim': 0.7194276035804728, 'validation/loss': 0.28934617436998805, 'validation/num_examples': 3554, 'test/ssim': 0.7370118005619939, 'test/loss': 0.2906777414653728, 'test/num_examples': 3581, 'score': 3502.6981377601624, 'total_duration': 4824.856132268906, 'accumulated_submission_time': 3502.6981377601624, 'accumulated_eval_time': 1252.852541923523, 'accumulated_logging_time': 3.403744697570801, 'global_step': 27477, 'preemption_count': 0}), (28736, {'train/ssim': 0.7408768790108817, 'train/loss': 0.2746963160378592, 'validation/ssim': 0.7206333999762592, 'validation/loss': 0.2889705179243458, 'validation/num_examples': 3554, 'test/ssim': 0.7379760230862539, 'test/loss': 0.2904466225827283, 'test/num_examples': 3581, 'score': 3581.4699058532715, 'total_duration': 4913.805272579193, 'accumulated_submission_time': 3581.4699058532715, 'accumulated_eval_time': 1261.0914371013641, 'accumulated_logging_time': 3.426198720932007, 'global_step': 28736, 'preemption_count': 0}), (29998, {'train/ssim': 0.7413381167820522, 'train/loss': 0.2749278885977609, 'validation/ssim': 0.7213543498522791, 'validation/loss': 0.2890232066861283, 'validation/num_examples': 3554, 'test/ssim': 0.7386825378298659, 'test/loss': 0.29048773310964116, 'test/num_examples': 3581, 'score': 3660.3380954265594, 'total_duration': 5001.60883307457, 'accumulated_submission_time': 3660.3380954265594, 'accumulated_eval_time': 1268.11807513237, 'accumulated_logging_time': 3.4461731910705566, 'global_step': 29998, 'preemption_count': 0}), (31257, {'train/ssim': 0.7406712259565081, 'train/loss': 0.27588135855538504, 'validation/ssim': 0.7202927434316967, 'validation/loss': 0.290072138952149, 'validation/num_examples': 3554, 'test/ssim': 0.7376806136117705, 'test/loss': 0.29146818167934935, 'test/num_examples': 3581, 'score': 3739.2281789779663, 'total_duration': 5090.365613460541, 'accumulated_submission_time': 3739.2281789779663, 'accumulated_eval_time': 1276.1399400234222, 'accumulated_logging_time': 3.467808246612549, 'global_step': 31257, 'preemption_count': 0}), (32519, {'train/ssim': 0.736919811793736, 'train/loss': 0.2802351542881557, 'validation/ssim': 0.7159233544070062, 'validation/loss': 0.2944158361946926, 'validation/num_examples': 3554, 'test/ssim': 0.7336405328251536, 'test/loss': 0.29576764057482197, 'test/num_examples': 3581, 'score': 3818.0760264396667, 'total_duration': 5177.422677755356, 'accumulated_submission_time': 3818.0760264396667, 'accumulated_eval_time': 1282.4708037376404, 'accumulated_logging_time': 3.4880316257476807, 'global_step': 32519, 'preemption_count': 0}), (33783, {'train/ssim': 0.7405009950910296, 'train/loss': 0.27514195442199707, 'validation/ssim': 0.7197717635498734, 'validation/loss': 0.2895068853976154, 'validation/num_examples': 3554, 'test/ssim': 0.737026526720888, 'test/loss': 0.29110657266650375, 'test/num_examples': 3581, 'score': 3896.9001898765564, 'total_duration': 5264.615083694458, 'accumulated_submission_time': 3896.9001898765564, 'accumulated_eval_time': 1288.9657604694366, 'accumulated_logging_time': 3.5106277465820312, 'global_step': 33783, 'preemption_count': 0}), (35045, {'train/ssim': 0.74160064969744, 'train/loss': 0.2746377672467913, 'validation/ssim': 0.7210277070070695, 'validation/loss': 0.2889072845407463, 'validation/num_examples': 3554, 'test/ssim': 0.7383947641414759, 'test/loss': 0.2904036712859536, 'test/num_examples': 3581, 'score': 3975.7700238227844, 'total_duration': 5351.555396080017, 'accumulated_submission_time': 3975.7700238227844, 'accumulated_eval_time': 1295.2190239429474, 'accumulated_logging_time': 3.531580686569214, 'global_step': 35045, 'preemption_count': 0}), (36305, {'train/ssim': 0.7415962219238281, 'train/loss': 0.2758822100503104, 'validation/ssim': 0.7213790799099606, 'validation/loss': 0.2900364864523248, 'validation/num_examples': 3554, 'test/ssim': 0.7384332839552499, 'test/loss': 0.2916355212951166, 'test/num_examples': 3581, 'score': 4054.6342635154724, 'total_duration': 5438.57750916481, 'accumulated_submission_time': 4054.6342635154724, 'accumulated_eval_time': 1301.5212895870209, 'accumulated_logging_time': 3.552830219268799, 'global_step': 36305, 'preemption_count': 0}), (37568, {'train/ssim': 0.7410045351300921, 'train/loss': 0.2762258563722883, 'validation/ssim': 0.7206177376063942, 'validation/loss': 0.29053359495902503, 'validation/num_examples': 3554, 'test/ssim': 0.7379397531023109, 'test/loss': 0.29203493426679, 'test/num_examples': 3581, 'score': 4133.487586021423, 'total_duration': 5525.51647901535, 'accumulated_submission_time': 4133.487586021423, 'accumulated_eval_time': 1307.7940692901611, 'accumulated_logging_time': 3.5736191272735596, 'global_step': 37568, 'preemption_count': 0}), (38830, {'train/ssim': 0.7401762008666992, 'train/loss': 0.275310891015189, 'validation/ssim': 0.7204331552036438, 'validation/loss': 0.28916471757175016, 'validation/num_examples': 3554, 'test/ssim': 0.7378267161974658, 'test/loss': 0.2906204048930117, 'test/num_examples': 3581, 'score': 4212.351359844208, 'total_duration': 5612.446934461594, 'accumulated_submission_time': 4212.351359844208, 'accumulated_eval_time': 1314.0184860229492, 'accumulated_logging_time': 3.6260859966278076, 'global_step': 38830, 'preemption_count': 0}), (40091, {'train/ssim': 0.740368161882673, 'train/loss': 0.27571747984204975, 'validation/ssim': 0.7196700955349606, 'validation/loss': 0.2900274187645083, 'validation/num_examples': 3554, 'test/ssim': 0.7371328141362399, 'test/loss': 0.29144595608768503, 'test/num_examples': 3581, 'score': 4291.179814577103, 'total_duration': 5699.333374738693, 'accumulated_submission_time': 4291.179814577103, 'accumulated_eval_time': 1320.2404849529266, 'accumulated_logging_time': 3.64676833152771, 'global_step': 40091, 'preemption_count': 0}), (41352, {'train/ssim': 0.7423377718244281, 'train/loss': 0.2738336835588728, 'validation/ssim': 0.7219344070941193, 'validation/loss': 0.28816119243941685, 'validation/num_examples': 3554, 'test/ssim': 0.739262789396293, 'test/loss': 0.28965683004703646, 'test/num_examples': 3581, 'score': 4370.085615634918, 'total_duration': 5786.435327291489, 'accumulated_submission_time': 4370.085615634918, 'accumulated_eval_time': 1326.5746810436249, 'accumulated_logging_time': 3.6684045791625977, 'global_step': 41352, 'preemption_count': 0})], 'global_step': 42615}
I0315 13:57:35.735988 140277216793792 submission_runner.py:649] Timing: 4448.900188446045
I0315 13:57:35.736025 140277216793792 submission_runner.py:651] Total number of evals: 52
I0315 13:57:35.736053 140277216793792 submission_runner.py:652] ====================
I0315 13:57:35.736214 140277216793792 submission_runner.py:750] Final fastmri score: 0
