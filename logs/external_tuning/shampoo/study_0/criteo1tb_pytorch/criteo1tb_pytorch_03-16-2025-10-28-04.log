torchrun --redirects 1:0,2:0,3:0,4:0,5:0,6:0,7:0 --standalone --nnodes=1 --nproc_per_node=8 submission_runner.py --framework=pytorch --workload=criteo1tb --submission_path=submissions_algorithms/leaderboard/external_tuning/shampoo_submission/submission.py --data_dir=/data/criteo1tb --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=submissions/rolling_leaderboard/external_tuning/shampoo/study_0 --overwrite=True --save_checkpoints=False --rng_seed=-777841126 --torch_compile=true --tuning_ruleset=external --tuning_search_space=submissions_algorithms/leaderboard/external_tuning/shampoo_submission/tuning_search_space.json --num_tuning_trials=5 --hparam_start_index=0 --hparam_end_index=1 2>&1 | tee -a /logs/criteo1tb_pytorch_03-16-2025-10-28-04.log
W0316 10:28:14.705000 9 site-packages/torch/distributed/run.py:793] 
W0316 10:28:14.705000 9 site-packages/torch/distributed/run.py:793] *****************************************
W0316 10:28:14.705000 9 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0316 10:28:14.705000 9 site-packages/torch/distributed/run.py:793] *****************************************
2025-03-16 10:28:19.533606: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-03-16 10:28:19.533612: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-03-16 10:28:19.533603: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-03-16 10:28:19.533598: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-03-16 10:28:19.533619: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-03-16 10:28:19.533597: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-03-16 10:28:19.533620: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-03-16 10:28:19.533597: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1742120899.556166      46 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1742120899.556164      45 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1742120899.556163      44 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1742120899.556163      48 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1742120899.556163      49 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1742120899.556163      50 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1742120899.556165      47 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1742120899.556167      51 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1742120899.562996      48 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
E0000 00:00:1742120899.562996      46 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
E0000 00:00:1742120899.563004      47 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
E0000 00:00:1742120899.563016      44 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
E0000 00:00:1742120899.563026      45 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
E0000 00:00:1742120899.563027      49 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
E0000 00:00:1742120899.563043      51 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
E0000 00:00:1742120899.563045      50 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
[rank4]:[W316 10:28:43.285303178 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 4]  using GPU 4 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank2]:[W316 10:28:43.351603052 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank0]:[W316 10:28:43.358759872 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank6]:[W316 10:28:43.366593374 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 6]  using GPU 6 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank1]:[W316 10:28:43.367682645 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank7]:[W316 10:28:43.368475693 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 7]  using GPU 7 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank3]:[W316 10:28:43.371402177 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank5]:[W316 10:28:43.381204093 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 5]  using GPU 5 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
I0316 10:28:45.049844 140303193433280 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_0/criteo1tb_pytorch.
I0316 10:28:45.049848 140436539851968 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_0/criteo1tb_pytorch.
I0316 10:28:45.049844 139935067108544 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_0/criteo1tb_pytorch.
I0316 10:28:45.049841 140414593897664 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_0/criteo1tb_pytorch.
I0316 10:28:45.049843 140444057306304 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_0/criteo1tb_pytorch.
I0316 10:28:45.049842 140059093185728 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_0/criteo1tb_pytorch.
I0316 10:28:45.049876 140205670307008 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_0/criteo1tb_pytorch.
I0316 10:28:45.049943 139806470304960 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_0/criteo1tb_pytorch.
I0316 10:28:51.340878 140444057306304 submission_runner.py:606] Using RNG seed -777841126
I0316 10:28:51.341213 140059093185728 logger_utils.py:97] Saving hparams to /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_0/criteo1tb_pytorch/trial_1/hparams.json.
I0316 10:28:51.341619 140436539851968 logger_utils.py:97] Saving hparams to /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_0/criteo1tb_pytorch/trial_1/hparams.json.
I0316 10:28:51.341617 140303193433280 logger_utils.py:97] Saving hparams to /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_0/criteo1tb_pytorch/trial_1/hparams.json.
I0316 10:28:51.341649 139935067108544 logger_utils.py:97] Saving hparams to /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_0/criteo1tb_pytorch/trial_1/hparams.json.
I0316 10:28:51.341767 140414593897664 logger_utils.py:97] Saving hparams to /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_0/criteo1tb_pytorch/trial_1/hparams.json.
I0316 10:28:51.341809 139806470304960 logger_utils.py:97] Saving hparams to /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_0/criteo1tb_pytorch/trial_1/hparams.json.
I0316 10:28:51.342180 140444057306304 submission_runner.py:615] --- Tuning run 1/5 ---
I0316 10:28:51.342340 140444057306304 submission_runner.py:620] Creating tuning directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_0/criteo1tb_pytorch/trial_1.
I0316 10:28:51.342591 140444057306304 logger_utils.py:97] Saving hparams to /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_0/criteo1tb_pytorch/trial_1/hparams.json.
I0316 10:28:51.342674 140205670307008 logger_utils.py:97] Saving hparams to /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_0/criteo1tb_pytorch/trial_1/hparams.json.
I0316 10:28:51.686471 140444057306304 submission_runner.py:218] Initializing dataset.
I0316 10:28:51.686653 140444057306304 submission_runner.py:229] Initializing model.
W0316 10:28:58.198064 140444057306304 submission_runner.py:254] These workloads cannot be fully compiled under current PyTorch version. Proceeding without `torch.compile`.
I0316 10:28:58.198248 140444057306304 submission_runner.py:272] Initializing optimizer.
W0316 10:28:58.199352 140444057306304 distributed_shampoo.py:341] start_preconditioning_step set to -1. Setting start_preconditioning_step equal to precondition frequency 100 by default.
W0316 10:28:58.199456 140444057306304 distributed_shampoo.py:359] Nesterov flag is enabled but momentum parameter is zero! Continuing without using momentum or Nesterov acceleration...
W0316 10:28:58.199880 140414593897664 submission_runner.py:254] These workloads cannot be fully compiled under current PyTorch version. Proceeding without `torch.compile`.
W0316 10:28:58.199940 139935067108544 submission_runner.py:254] These workloads cannot be fully compiled under current PyTorch version. Proceeding without `torch.compile`.
W0316 10:28:58.200109 139806470304960 submission_runner.py:254] These workloads cannot be fully compiled under current PyTorch version. Proceeding without `torch.compile`.
W0316 10:28:58.200260 140059093185728 submission_runner.py:254] These workloads cannot be fully compiled under current PyTorch version. Proceeding without `torch.compile`.
W0316 10:28:58.200326 140436539851968 submission_runner.py:254] These workloads cannot be fully compiled under current PyTorch version. Proceeding without `torch.compile`.
W0316 10:28:58.200499 140205670307008 submission_runner.py:254] These workloads cannot be fully compiled under current PyTorch version. Proceeding without `torch.compile`.
W0316 10:28:58.200550 140303193433280 submission_runner.py:254] These workloads cannot be fully compiled under current PyTorch version. Proceeding without `torch.compile`.
W0316 10:28:58.201015 140414593897664 distributed_shampoo.py:341] start_preconditioning_step set to -1. Setting start_preconditioning_step equal to precondition frequency 100 by default.
W0316 10:28:58.201129 140414593897664 distributed_shampoo.py:359] Nesterov flag is enabled but momentum parameter is zero! Continuing without using momentum or Nesterov acceleration...
W0316 10:28:58.201079 139935067108544 distributed_shampoo.py:341] start_preconditioning_step set to -1. Setting start_preconditioning_step equal to precondition frequency 100 by default.
W0316 10:28:58.201195 139935067108544 distributed_shampoo.py:359] Nesterov flag is enabled but momentum parameter is zero! Continuing without using momentum or Nesterov acceleration...
W0316 10:28:58.201219 139806470304960 distributed_shampoo.py:341] start_preconditioning_step set to -1. Setting start_preconditioning_step equal to precondition frequency 100 by default.
W0316 10:28:58.201351 139806470304960 distributed_shampoo.py:359] Nesterov flag is enabled but momentum parameter is zero! Continuing without using momentum or Nesterov acceleration...
W0316 10:28:58.201430 140436539851968 distributed_shampoo.py:341] start_preconditioning_step set to -1. Setting start_preconditioning_step equal to precondition frequency 100 by default.
W0316 10:28:58.201418 140059093185728 distributed_shampoo.py:341] start_preconditioning_step set to -1. Setting start_preconditioning_step equal to precondition frequency 100 by default.
W0316 10:28:58.201535 140059093185728 distributed_shampoo.py:359] Nesterov flag is enabled but momentum parameter is zero! Continuing without using momentum or Nesterov acceleration...
W0316 10:28:58.201542 140436539851968 distributed_shampoo.py:359] Nesterov flag is enabled but momentum parameter is zero! Continuing without using momentum or Nesterov acceleration...
I0316 10:28:58.201622 140444057306304 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 0.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 0 (torch.Size([512, 13])), Block block_0 (torch.Size([512, 13])).
I0316 10:28:58.201778 140444057306304 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 1.block_0 ([torch.Size([0])]) for Parameter 1 (torch.Size([512])), Block block_0 (torch.Size([512])).
W0316 10:28:58.201784 140205670307008 distributed_shampoo.py:341] start_preconditioning_step set to -1. Setting start_preconditioning_step equal to precondition frequency 100 by default.
I0316 10:28:58.201932 140444057306304 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 2.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 2 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
W0316 10:28:58.201938 140205670307008 distributed_shampoo.py:359] Nesterov flag is enabled but momentum parameter is zero! Continuing without using momentum or Nesterov acceleration...
W0316 10:28:58.201907 140303193433280 distributed_shampoo.py:341] start_preconditioning_step set to -1. Setting start_preconditioning_step equal to precondition frequency 100 by default.
I0316 10:28:58.202038 140444057306304 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 3.block_0 ([torch.Size([0])]) for Parameter 3 (torch.Size([256])), Block block_0 (torch.Size([256])).
W0316 10:28:58.202035 140303193433280 distributed_shampoo.py:359] Nesterov flag is enabled but momentum parameter is zero! Continuing without using momentum or Nesterov acceleration...
I0316 10:28:58.202168 140444057306304 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 4.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 4 (torch.Size([128, 256])), Block block_0 (torch.Size([128, 256])).
I0316 10:28:58.202317 140444057306304 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 5.block_0 ([torch.Size([0])]) for Parameter 5 (torch.Size([128])), Block block_0 (torch.Size([128])).
I0316 10:28:58.202452 140444057306304 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 6.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 6 (torch.Size([1024, 506])), Block block_0 (torch.Size([1024, 506])).
I0316 10:28:58.202556 140444057306304 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 7.block_0 ([torch.Size([0])]) for Parameter 7 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 10:28:58.203656 139935067108544 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 0.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 0 (torch.Size([512, 13])), Block block_0 (torch.Size([512, 13])).
I0316 10:28:58.203809 140444057306304 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 8.block_0 ([torch.Size([1024, 1024]), torch.Size([1024, 1024])]) for Parameter 8 (torch.Size([1024, 1024])), Block block_0 (torch.Size([1024, 1024])).
I0316 10:28:58.203825 139935067108544 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 1.block_0 ([torch.Size([0])]) for Parameter 1 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 10:28:58.203947 140444057306304 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 9.block_0 ([torch.Size([0])]) for Parameter 9 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 10:28:58.203968 139935067108544 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 2.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 2 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 10:28:58.203957 139806470304960 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 0.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 0 (torch.Size([512, 13])), Block block_0 (torch.Size([512, 13])).
I0316 10:28:58.204070 139935067108544 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 3.block_0 ([torch.Size([0])]) for Parameter 3 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 10:28:58.204073 140444057306304 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 10.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 10 (torch.Size([512, 1024])), Block block_0 (torch.Size([512, 1024])).
I0316 10:28:58.204160 140444057306304 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 11.block_0 ([torch.Size([0])]) for Parameter 11 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 10:28:58.204188 139935067108544 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 4.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 4 (torch.Size([128, 256])), Block block_0 (torch.Size([128, 256])).
I0316 10:28:58.204157 140436539851968 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 0.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 0 (torch.Size([512, 13])), Block block_0 (torch.Size([512, 13])).
I0316 10:28:58.204277 140444057306304 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 12.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 12 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 10:28:58.204280 139935067108544 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 5.block_0 ([torch.Size([0])]) for Parameter 5 (torch.Size([128])), Block block_0 (torch.Size([128])).
I0316 10:28:58.204290 140414593897664 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 0.block_0 ([torch.Size([512, 512]), torch.Size([13, 13])]) for Parameter 0 (torch.Size([512, 13])), Block block_0 (torch.Size([512, 13])).
I0316 10:28:58.204344 140436539851968 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 1.block_0 ([torch.Size([0])]) for Parameter 1 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 10:28:58.204402 140444057306304 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 13.block_0 ([torch.Size([0])]) for Parameter 13 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 10:28:58.204446 139935067108544 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 6.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 6 (torch.Size([1024, 506])), Block block_0 (torch.Size([1024, 506])).
I0316 10:28:58.204524 140444057306304 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 14.block_0 ([torch.Size([0])]) for Parameter 14 (torch.Size([1, 256])), Block block_0 (torch.Size([256])).
I0316 10:28:58.204489 140059093185728 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 0.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 0 (torch.Size([512, 13])), Block block_0 (torch.Size([512, 13])).
I0316 10:28:58.204519 140414593897664 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 1.block_0 ([torch.Size([0])]) for Parameter 1 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 10:28:58.204561 139935067108544 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 7.block_0 ([torch.Size([0])]) for Parameter 7 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 10:28:58.204622 140444057306304 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 15.block_0 ([torch.Size([0])]) for Parameter 15 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0316 10:28:58.204633 139806470304960 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 1.block_0 ([torch.Size([512, 512])]) for Parameter 1 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 10:28:58.204668 140059093185728 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 1.block_0 ([torch.Size([0])]) for Parameter 1 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 10:28:58.204699 139935067108544 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 8.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 8 (torch.Size([1024, 1024])), Block block_0 (torch.Size([1024, 1024])).
I0316 10:28:58.204692 140414593897664 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 2.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 2 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 10:28:58.204743 140444057306304 shampoo_preconditioner_list.py:612] Rank 0: ShampooPreconditionerList Numel Breakdown: (524626, 524288, 655360, 131072, 163840, 32768, 2609224, 2097152, 4194304, 2097152, 2621440, 524288, 655360, 131072, 131072, 2)
I0316 10:28:58.204791 140444057306304 shampoo_preconditioner_list.py:615] Rank 0: ShampooPreconditionerList Bytes Breakdown: (2098504,)
I0316 10:28:58.204801 139935067108544 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 9.block_0 ([torch.Size([0])]) for Parameter 9 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 10:28:58.204813 140414593897664 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 3.block_0 ([torch.Size([0])]) for Parameter 3 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 10:28:58.204838 140444057306304 shampoo_preconditioner_list.py:618] Rank 0: ShampooPreconditionerList Total Elements: 17093020
I0316 10:28:58.204838 140059093185728 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 2.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 2 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 10:28:58.204844 139806470304960 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 2.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 2 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 10:28:58.204875 140444057306304 shampoo_preconditioner_list.py:621] Rank 0: ShampooPreconditionerList Total Bytes: 2098504
I0316 10:28:58.204923 139935067108544 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 10.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 10 (torch.Size([512, 1024])), Block block_0 (torch.Size([512, 1024])).
I0316 10:28:58.204950 140414593897664 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 4.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 4 (torch.Size([128, 256])), Block block_0 (torch.Size([128, 256])).
I0316 10:28:58.204976 140059093185728 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 3.block_0 ([torch.Size([0])]) for Parameter 3 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 10:28:58.205023 139935067108544 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 11.block_0 ([torch.Size([0])]) for Parameter 11 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 10:28:58.205024 140444057306304 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([512, 13])), Block block_0 (torch.Size([512, 13])).
I0316 10:28:58.205034 139806470304960 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 3.block_0 ([torch.Size([256, 256])]) for Parameter 3 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 10:28:58.205010 140436539851968 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 2.block_0 ([torch.Size([256, 256]), torch.Size([512, 512])]) for Parameter 2 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 10:28:58.205067 140414593897664 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 5.block_0 ([torch.Size([0])]) for Parameter 5 (torch.Size([128])), Block block_0 (torch.Size([128])).
I0316 10:28:58.205090 140444057306304 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 10:28:58.205124 140059093185728 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 4.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 4 (torch.Size([128, 256])), Block block_0 (torch.Size([128, 256])).
I0316 10:28:58.205074 140205670307008 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 0.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 0 (torch.Size([512, 13])), Block block_0 (torch.Size([512, 13])).
I0316 10:28:58.205146 140444057306304 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 10:28:58.205170 140436539851968 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 3.block_0 ([torch.Size([0])]) for Parameter 3 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 10:28:58.205199 140444057306304 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 10:28:58.205199 139806470304960 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 4.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 4 (torch.Size([128, 256])), Block block_0 (torch.Size([128, 256])).
I0316 10:28:58.205208 140414593897664 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 6.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 6 (torch.Size([1024, 506])), Block block_0 (torch.Size([1024, 506])).
I0316 10:28:58.205233 140059093185728 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 5.block_0 ([torch.Size([0])]) for Parameter 5 (torch.Size([128])), Block block_0 (torch.Size([128])).
I0316 10:28:58.205252 140444057306304 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 4.block_0 (torch.Size([0])) for Parameter 4 (torch.Size([128, 256])), Block block_0 (torch.Size([128, 256])).
I0316 10:28:58.205259 140205670307008 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 1.block_0 ([torch.Size([0])]) for Parameter 1 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 10:28:58.205302 140444057306304 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 5.block_0 (torch.Size([0])) for Parameter 5 (torch.Size([128])), Block block_0 (torch.Size([128])).
I0316 10:28:58.205318 140414593897664 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 7.block_0 ([torch.Size([0])]) for Parameter 7 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 10:28:58.205332 140436539851968 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 4.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 4 (torch.Size([128, 256])), Block block_0 (torch.Size([128, 256])).
I0316 10:28:58.205354 140444057306304 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 6.block_0 (torch.Size([0])) for Parameter 6 (torch.Size([1024, 506])), Block block_0 (torch.Size([1024, 506])).
I0316 10:28:58.205363 139806470304960 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 5.block_0 ([torch.Size([128, 128])]) for Parameter 5 (torch.Size([128])), Block block_0 (torch.Size([128])).
I0316 10:28:58.205412 140444057306304 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 7.block_0 (torch.Size([0])) for Parameter 7 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 10:28:58.205455 140414593897664 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 8.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 8 (torch.Size([1024, 1024])), Block block_0 (torch.Size([1024, 1024])).
I0316 10:28:58.205446 140205670307008 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 2.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 2 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 10:28:58.205456 140436539851968 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 5.block_0 ([torch.Size([0])]) for Parameter 5 (torch.Size([128])), Block block_0 (torch.Size([128])).
I0316 10:28:58.205414 140303193433280 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 0.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 0 (torch.Size([512, 13])), Block block_0 (torch.Size([512, 13])).
I0316 10:28:58.205529 139806470304960 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 6.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 6 (torch.Size([1024, 506])), Block block_0 (torch.Size([1024, 506])).
I0316 10:28:58.205548 140444057306304 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 8.block_0 (torch.Size([1024, 1024])) for Parameter 8 (torch.Size([1024, 1024])), Block block_0 (torch.Size([1024, 1024])).
I0316 10:28:58.205566 140414593897664 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 9.block_0 ([torch.Size([0])]) for Parameter 9 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 10:28:58.205577 140205670307008 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 3.block_0 ([torch.Size([0])]) for Parameter 3 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 10:28:58.205594 140436539851968 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 6.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 6 (torch.Size([1024, 506])), Block block_0 (torch.Size([1024, 506])).
I0316 10:28:58.205609 140444057306304 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 9.block_0 (torch.Size([0])) for Parameter 9 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 10:28:58.205618 140303193433280 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 1.block_0 ([torch.Size([0])]) for Parameter 1 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 10:28:58.205655 140444057306304 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 10.block_0 (torch.Size([0])) for Parameter 10 (torch.Size([512, 1024])), Block block_0 (torch.Size([512, 1024])).
I0316 10:28:58.205674 140414593897664 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 10.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 10 (torch.Size([512, 1024])), Block block_0 (torch.Size([512, 1024])).
I0316 10:28:58.205655 139935067108544 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 12.block_0 ([torch.Size([256, 256]), torch.Size([512, 512])]) for Parameter 12 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 10:28:58.205683 140436539851968 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 7.block_0 ([torch.Size([0])]) for Parameter 7 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 10:28:58.205699 140444057306304 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 11.block_0 (torch.Size([0])) for Parameter 11 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 10:28:58.205698 139806470304960 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 7.block_0 ([torch.Size([1024, 1024])]) for Parameter 7 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 10:28:58.205759 140444057306304 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 12.block_0 (torch.Size([0])) for Parameter 12 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 10:28:58.205783 139935067108544 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 13.block_0 ([torch.Size([0])]) for Parameter 13 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 10:28:58.205785 140414593897664 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 11.block_0 ([torch.Size([0])]) for Parameter 11 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 10:28:58.205816 140444057306304 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 13.block_0 (torch.Size([0])) for Parameter 13 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 10:28:58.205815 140436539851968 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 8.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 8 (torch.Size([1024, 1024])), Block block_0 (torch.Size([1024, 1024])).
I0316 10:28:58.205821 140303193433280 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 2.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 2 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 10:28:58.205863 140444057306304 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 14.block_0 (torch.Size([0])) for Parameter 14 (torch.Size([1, 256])), Block block_0 (torch.Size([256])).
I0316 10:28:58.205861 139806470304960 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 8.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 8 (torch.Size([1024, 1024])), Block block_0 (torch.Size([1024, 1024])).
I0316 10:28:58.205845 140205670307008 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 4.block_0 ([torch.Size([128, 128]), torch.Size([256, 256])]) for Parameter 4 (torch.Size([128, 256])), Block block_0 (torch.Size([128, 256])).
I0316 10:28:58.205885 139935067108544 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 14.block_0 ([torch.Size([0])]) for Parameter 14 (torch.Size([1, 256])), Block block_0 (torch.Size([256])).
I0316 10:28:58.205896 140414593897664 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 12.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 12 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 10:28:58.205903 140436539851968 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 9.block_0 ([torch.Size([0])]) for Parameter 9 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 10:28:58.205915 140444057306304 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 15.block_0 (torch.Size([0])) for Parameter 15 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0316 10:28:58.205956 140303193433280 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 3.block_0 ([torch.Size([0])]) for Parameter 3 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 10:28:58.205934 140059093185728 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 6.block_0 ([torch.Size([1024, 1024]), torch.Size([506, 506])]) for Parameter 6 (torch.Size([1024, 506])), Block block_0 (torch.Size([1024, 506])).
I0316 10:28:58.205964 140205670307008 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 5.block_0 ([torch.Size([0])]) for Parameter 5 (torch.Size([128])), Block block_0 (torch.Size([128])).
I0316 10:28:58.205973 140444057306304 shampoo_preconditioner_list.py:375] Rank 0: AdaGradPreconditionerList Numel Breakdown: (0, 0, 0, 0, 0, 0, 0, 0, 1048576, 0, 0, 0, 0, 0, 0, 0)
I0316 10:28:58.205972 139935067108544 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 15.block_0 ([torch.Size([0])]) for Parameter 15 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0316 10:28:58.205979 140414593897664 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 13.block_0 ([torch.Size([0])]) for Parameter 13 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 10:28:58.206007 140444057306304 shampoo_preconditioner_list.py:378] Rank 0: AdaGradPreconditionerList Bytes Breakdown: (0, 0, 0, 0, 0, 0, 0, 0, 4194304, 0, 0, 0, 0, 0, 0, 0)
I0316 10:28:58.206014 140436539851968 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 10.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 10 (torch.Size([512, 1024])), Block block_0 (torch.Size([512, 1024])).
I0316 10:28:58.206041 140444057306304 shampoo_preconditioner_list.py:381] Rank 0: AdaGradPreconditionerList Total Elements: 1048576
I0316 10:28:58.206057 140414593897664 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 14.block_0 ([torch.Size([0])]) for Parameter 14 (torch.Size([1, 256])), Block block_0 (torch.Size([256])).
I0316 10:28:58.206062 139935067108544 shampoo_preconditioner_list.py:612] Rank 4: ShampooPreconditionerList Numel Breakdown: (524626, 524288, 655360, 131072, 163840, 32768, 2609224, 2097152, 4194304, 2097152, 2621440, 524288, 655360, 131072, 131072, 2)
I0316 10:28:58.206073 140444057306304 shampoo_preconditioner_list.py:384] Rank 0: AdaGradPreconditionerList Total Bytes: 4194304
I0316 10:28:58.206084 140205670307008 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 6.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 6 (torch.Size([1024, 506])), Block block_0 (torch.Size([1024, 506])).
I0316 10:28:58.206087 140059093185728 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 7.block_0 ([torch.Size([0])]) for Parameter 7 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 10:28:58.206098 139935067108544 shampoo_preconditioner_list.py:615] Rank 4: ShampooPreconditionerList Bytes Breakdown: (2098504,)
I0316 10:28:58.206098 140436539851968 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 11.block_0 ([torch.Size([0])]) for Parameter 11 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 10:28:58.206128 139935067108544 shampoo_preconditioner_list.py:618] Rank 4: ShampooPreconditionerList Total Elements: 17093020
I0316 10:28:58.206118 140303193433280 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 4.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 4 (torch.Size([128, 256])), Block block_0 (torch.Size([128, 256])).
I0316 10:28:58.206146 140414593897664 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 15.block_0 ([torch.Size([0])]) for Parameter 15 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0316 10:28:58.206164 139935067108544 shampoo_preconditioner_list.py:621] Rank 4: ShampooPreconditionerList Total Bytes: 2098504
I0316 10:28:58.206199 140205670307008 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 7.block_0 ([torch.Size([0])]) for Parameter 7 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 10:28:58.206208 140436539851968 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 12.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 12 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 10:28:58.206207 140059093185728 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 8.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 8 (torch.Size([1024, 1024])), Block block_0 (torch.Size([1024, 1024])).
I0316 10:28:58.206226 140303193433280 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 5.block_0 ([torch.Size([0])]) for Parameter 5 (torch.Size([128])), Block block_0 (torch.Size([128])).
I0316 10:28:58.206235 140414593897664 shampoo_preconditioner_list.py:612] Rank 6: ShampooPreconditionerList Numel Breakdown: (524626, 524288, 655360, 131072, 163840, 32768, 2609224, 2097152, 4194304, 2097152, 2621440, 524288, 655360, 131072, 131072, 2)
I0316 10:28:58.206288 140414593897664 shampoo_preconditioner_list.py:615] Rank 6: ShampooPreconditionerList Bytes Breakdown: (2098504,)
I0316 10:28:58.206304 140436539851968 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 13.block_0 ([torch.Size([0])]) for Parameter 13 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 10:28:58.206311 140059093185728 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 9.block_0 ([torch.Size([0])]) for Parameter 9 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 10:28:58.206311 139935067108544 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([512, 13])), Block block_0 (torch.Size([512, 13])).
I0316 10:28:58.206324 140414593897664 shampoo_preconditioner_list.py:618] Rank 6: ShampooPreconditionerList Total Elements: 17093020
I0316 10:28:58.206353 140414593897664 shampoo_preconditioner_list.py:621] Rank 6: ShampooPreconditionerList Total Bytes: 2098504
I0316 10:28:58.206347 140205670307008 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 8.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 8 (torch.Size([1024, 1024])), Block block_0 (torch.Size([1024, 1024])).
I0316 10:28:58.206370 139935067108544 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 10:28:58.206350 139806470304960 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 9.block_0 ([torch.Size([1024, 1024])]) for Parameter 9 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 10:28:58.206398 140436539851968 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 14.block_0 ([torch.Size([0])]) for Parameter 14 (torch.Size([1, 256])), Block block_0 (torch.Size([256])).
I0316 10:28:58.206413 140303193433280 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 6.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 6 (torch.Size([1024, 506])), Block block_0 (torch.Size([1024, 506])).
I0316 10:28:58.206427 139935067108544 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 10:28:58.206439 140205670307008 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 9.block_0 ([torch.Size([0])]) for Parameter 9 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 10:28:58.206441 140059093185728 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 10.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 10 (torch.Size([512, 1024])), Block block_0 (torch.Size([512, 1024])).
I0316 10:28:58.206440 140444057306304 submission.py:105] Large parameters (embedding tables) detected (dim >= 1_000_000). Instantiating Row-Wise AdaGrad...
I0316 10:28:58.206476 139935067108544 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 10:28:58.206491 140436539851968 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 15.block_0 ([torch.Size([0])]) for Parameter 15 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0316 10:28:58.206523 139935067108544 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 4.block_0 (torch.Size([0])) for Parameter 4 (torch.Size([128, 256])), Block block_0 (torch.Size([128, 256])).
I0316 10:28:58.206520 139806470304960 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 10.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 10 (torch.Size([512, 1024])), Block block_0 (torch.Size([512, 1024])).
W0316 10:28:58.206533 140444057306304 distributed_shampoo.py:359] Nesterov flag is enabled but momentum parameter is zero! Continuing without using momentum or Nesterov acceleration...
I0316 10:28:58.206533 140059093185728 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 11.block_0 ([torch.Size([0])]) for Parameter 11 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 10:28:58.206542 140303193433280 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 7.block_0 ([torch.Size([0])]) for Parameter 7 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 10:28:58.206551 140414593897664 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([512, 13])) for Parameter 0 (torch.Size([512, 13])), Block block_0 (torch.Size([512, 13])).
I0316 10:28:58.206565 140205670307008 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 10.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 10 (torch.Size([512, 1024])), Block block_0 (torch.Size([512, 1024])).
I0316 10:28:58.206579 139935067108544 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 5.block_0 (torch.Size([0])) for Parameter 5 (torch.Size([128])), Block block_0 (torch.Size([128])).
I0316 10:28:58.206584 140436539851968 shampoo_preconditioner_list.py:612] Rank 3: ShampooPreconditionerList Numel Breakdown: (524626, 524288, 655360, 131072, 163840, 32768, 2609224, 2097152, 4194304, 2097152, 2621440, 524288, 655360, 131072, 131072, 2)
I0316 10:28:58.206619 140436539851968 shampoo_preconditioner_list.py:615] Rank 3: ShampooPreconditionerList Bytes Breakdown: (2098504,)
I0316 10:28:58.206626 139935067108544 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 6.block_0 (torch.Size([0])) for Parameter 6 (torch.Size([1024, 506])), Block block_0 (torch.Size([1024, 506])).
I0316 10:28:58.206640 140414593897664 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 10:28:58.206645 140059093185728 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 12.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 12 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 10:28:58.206653 140436539851968 shampoo_preconditioner_list.py:618] Rank 3: ShampooPreconditionerList Total Elements: 17093020
I0316 10:28:58.206656 140205670307008 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 11.block_0 ([torch.Size([0])]) for Parameter 11 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 10:28:58.206681 140436539851968 shampoo_preconditioner_list.py:621] Rank 3: ShampooPreconditionerList Total Bytes: 2098504
I0316 10:28:58.206678 139935067108544 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 7.block_0 (torch.Size([0])) for Parameter 7 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 10:28:58.206689 140303193433280 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 8.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 8 (torch.Size([1024, 1024])), Block block_0 (torch.Size([1024, 1024])).
I0316 10:28:58.206700 140414593897664 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 10:28:58.206725 139935067108544 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 8.block_0 (torch.Size([0])) for Parameter 8 (torch.Size([1024, 1024])), Block block_0 (torch.Size([1024, 1024])).
I0316 10:28:58.206727 140059093185728 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 13.block_0 ([torch.Size([0])]) for Parameter 13 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 10:28:58.206748 140414593897664 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 10:28:58.206773 140205670307008 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 12.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 12 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 10:28:58.206777 139935067108544 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 9.block_0 (torch.Size([0])) for Parameter 9 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 10:28:58.206793 140303193433280 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 9.block_0 ([torch.Size([0])]) for Parameter 9 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 10:28:58.206801 140414593897664 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 4.block_0 (torch.Size([0])) for Parameter 4 (torch.Size([128, 256])), Block block_0 (torch.Size([128, 256])).
I0316 10:28:58.206804 140059093185728 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 14.block_0 ([torch.Size([0])]) for Parameter 14 (torch.Size([1, 256])), Block block_0 (torch.Size([256])).
I0316 10:28:58.206813 140436539851968 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([512, 13])), Block block_0 (torch.Size([512, 13])).
I0316 10:28:58.206823 139935067108544 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 10.block_0 (torch.Size([0])) for Parameter 10 (torch.Size([512, 1024])), Block block_0 (torch.Size([512, 1024])).
I0316 10:28:58.206848 140414593897664 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 5.block_0 (torch.Size([0])) for Parameter 5 (torch.Size([128])), Block block_0 (torch.Size([128])).
I0316 10:28:58.206867 139935067108544 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 11.block_0 (torch.Size([0])) for Parameter 11 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 10:28:58.206875 140205670307008 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 13.block_0 ([torch.Size([0])]) for Parameter 13 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 10:28:58.206878 140436539851968 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 10:28:58.206888 140059093185728 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 15.block_0 ([torch.Size([0])]) for Parameter 15 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0316 10:28:58.206902 140414593897664 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 6.block_0 (torch.Size([0])) for Parameter 6 (torch.Size([1024, 506])), Block block_0 (torch.Size([1024, 506])).
I0316 10:28:58.206958 140414593897664 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 7.block_0 (torch.Size([0])) for Parameter 7 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 10:28:58.206970 140205670307008 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 14.block_0 ([torch.Size([0])]) for Parameter 14 (torch.Size([1, 256])), Block block_0 (torch.Size([256])).
I0316 10:28:58.206974 140059093185728 shampoo_preconditioner_list.py:612] Rank 2: ShampooPreconditionerList Numel Breakdown: (524626, 524288, 655360, 131072, 163840, 32768, 2609224, 2097152, 4194304, 2097152, 2621440, 524288, 655360, 131072, 131072, 2)
I0316 10:28:58.207010 140059093185728 shampoo_preconditioner_list.py:615] Rank 2: ShampooPreconditionerList Bytes Breakdown: (2098504,)
I0316 10:28:58.207012 140414593897664 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 8.block_0 (torch.Size([0])) for Parameter 8 (torch.Size([1024, 1024])), Block block_0 (torch.Size([1024, 1024])).
I0316 10:28:58.207006 139806470304960 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 11.block_0 ([torch.Size([512, 512])]) for Parameter 11 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 10:28:58.207051 140059093185728 shampoo_preconditioner_list.py:618] Rank 2: ShampooPreconditionerList Total Elements: 17093020
I0316 10:28:58.207054 140205670307008 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 15.block_0 ([torch.Size([0])]) for Parameter 15 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0316 10:28:58.207059 140414593897664 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 9.block_0 (torch.Size([0])) for Parameter 9 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 10:28:58.207084 140059093185728 shampoo_preconditioner_list.py:621] Rank 2: ShampooPreconditionerList Total Bytes: 2098504
I0316 10:28:58.207105 140414593897664 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 10.block_0 (torch.Size([0])) for Parameter 10 (torch.Size([512, 1024])), Block block_0 (torch.Size([512, 1024])).
I0316 10:28:58.207145 140205670307008 shampoo_preconditioner_list.py:612] Rank 5: ShampooPreconditionerList Numel Breakdown: (524626, 524288, 655360, 131072, 163840, 32768, 2609224, 2097152, 4194304, 2097152, 2621440, 524288, 655360, 131072, 131072, 2)
I0316 10:28:58.207150 139806470304960 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 12.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 12 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 10:28:58.207157 140414593897664 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 11.block_0 (torch.Size([0])) for Parameter 11 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 10:28:58.207190 140205670307008 shampoo_preconditioner_list.py:615] Rank 5: ShampooPreconditionerList Bytes Breakdown: (2098504,)
I0316 10:28:58.207204 140414593897664 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 12.block_0 (torch.Size([0])) for Parameter 12 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 10:28:58.207228 140205670307008 shampoo_preconditioner_list.py:618] Rank 5: ShampooPreconditionerList Total Elements: 17093020
I0316 10:28:58.207221 140059093185728 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([512, 13])), Block block_0 (torch.Size([512, 13])).
I0316 10:28:58.207249 140414593897664 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 13.block_0 (torch.Size([0])) for Parameter 13 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 10:28:58.207269 140205670307008 shampoo_preconditioner_list.py:621] Rank 5: ShampooPreconditionerList Total Bytes: 2098504
I0316 10:28:58.207282 140059093185728 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 10:28:58.207294 140414593897664 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 14.block_0 (torch.Size([0])) for Parameter 14 (torch.Size([1, 256])), Block block_0 (torch.Size([256])).
I0316 10:28:58.207332 140059093185728 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 10:28:58.207338 140414593897664 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 15.block_0 (torch.Size([0])) for Parameter 15 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0316 10:28:58.207388 140059093185728 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 10:28:58.207395 140414593897664 shampoo_preconditioner_list.py:375] Rank 6: AdaGradPreconditionerList Numel Breakdown: (6656, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)
I0316 10:28:58.207439 140414593897664 shampoo_preconditioner_list.py:378] Rank 6: AdaGradPreconditionerList Bytes Breakdown: (26624, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)
I0316 10:28:58.207434 140205670307008 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([512, 13])), Block block_0 (torch.Size([512, 13])).
I0316 10:28:58.207452 140059093185728 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 4.block_0 (torch.Size([0])) for Parameter 4 (torch.Size([128, 256])), Block block_0 (torch.Size([128, 256])).
I0316 10:28:58.207474 140414593897664 shampoo_preconditioner_list.py:381] Rank 6: AdaGradPreconditionerList Total Elements: 6656
I0316 10:28:58.207501 140059093185728 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 5.block_0 (torch.Size([0])) for Parameter 5 (torch.Size([128])), Block block_0 (torch.Size([128])).
I0316 10:28:58.207499 140205670307008 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 10:28:58.207512 140414593897664 shampoo_preconditioner_list.py:384] Rank 6: AdaGradPreconditionerList Total Bytes: 26624
I0316 10:28:58.207553 140205670307008 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 10:28:58.207605 140059093185728 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 6.block_0 (torch.Size([1024, 506])) for Parameter 6 (torch.Size([1024, 506])), Block block_0 (torch.Size([1024, 506])).
I0316 10:28:58.207614 140205670307008 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 10:28:58.207690 140059093185728 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 7.block_0 (torch.Size([0])) for Parameter 7 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 10:28:58.207727 140205670307008 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 4.block_0 (torch.Size([128, 256])) for Parameter 4 (torch.Size([128, 256])), Block block_0 (torch.Size([128, 256])).
I0316 10:28:58.207717 140303193433280 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 10.block_0 ([torch.Size([512, 512]), torch.Size([1024, 1024])]) for Parameter 10 (torch.Size([512, 1024])), Block block_0 (torch.Size([512, 1024])).
I0316 10:28:58.207749 140059093185728 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 8.block_0 (torch.Size([0])) for Parameter 8 (torch.Size([1024, 1024])), Block block_0 (torch.Size([1024, 1024])).
I0316 10:28:58.207798 140059093185728 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 9.block_0 (torch.Size([0])) for Parameter 9 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 10:28:58.207805 140205670307008 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 5.block_0 (torch.Size([0])) for Parameter 5 (torch.Size([128])), Block block_0 (torch.Size([128])).
I0316 10:28:58.207851 140059093185728 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 10.block_0 (torch.Size([0])) for Parameter 10 (torch.Size([512, 1024])), Block block_0 (torch.Size([512, 1024])).
I0316 10:28:58.207857 140205670307008 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 6.block_0 (torch.Size([0])) for Parameter 6 (torch.Size([1024, 506])), Block block_0 (torch.Size([1024, 506])).
I0316 10:28:58.207859 140303193433280 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 11.block_0 ([torch.Size([0])]) for Parameter 11 (torch.Size([512])), Block block_0 (torch.Size([512])).
W0316 10:28:58.207868 140414593897664 distributed_shampoo.py:359] Nesterov flag is enabled but momentum parameter is zero! Continuing without using momentum or Nesterov acceleration...
I0316 10:28:58.207872 139935067108544 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 12.block_0 (torch.Size([256, 512])) for Parameter 12 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 10:28:58.207904 140059093185728 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 11.block_0 (torch.Size([0])) for Parameter 11 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 10:28:58.207913 140205670307008 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 7.block_0 (torch.Size([0])) for Parameter 7 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 10:28:58.207924 140436539851968 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([256, 512])) for Parameter 2 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 10:28:58.207952 140059093185728 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 12.block_0 (torch.Size([0])) for Parameter 12 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 10:28:58.207962 140205670307008 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 8.block_0 (torch.Size([0])) for Parameter 8 (torch.Size([1024, 1024])), Block block_0 (torch.Size([1024, 1024])).
I0316 10:28:58.207970 139935067108544 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 13.block_0 (torch.Size([0])) for Parameter 13 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 10:28:58.207996 140059093185728 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 13.block_0 (torch.Size([0])) for Parameter 13 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 10:28:58.208003 140303193433280 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 12.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 12 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 10:28:58.208010 140205670307008 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 9.block_0 (torch.Size([0])) for Parameter 9 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 10:28:58.208021 139935067108544 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 14.block_0 (torch.Size([0])) for Parameter 14 (torch.Size([1, 256])), Block block_0 (torch.Size([256])).
I0316 10:28:58.208030 140436539851968 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 10:28:58.208048 140059093185728 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 14.block_0 (torch.Size([0])) for Parameter 14 (torch.Size([1, 256])), Block block_0 (torch.Size([256])).
I0316 10:28:58.208058 140205670307008 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 10.block_0 (torch.Size([0])) for Parameter 10 (torch.Size([512, 1024])), Block block_0 (torch.Size([512, 1024])).
I0316 10:28:58.208067 139935067108544 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 15.block_0 (torch.Size([0])) for Parameter 15 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0316 10:28:58.208090 140436539851968 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 4.block_0 (torch.Size([0])) for Parameter 4 (torch.Size([128, 256])), Block block_0 (torch.Size([128, 256])).
I0316 10:28:58.208094 140059093185728 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 15.block_0 (torch.Size([0])) for Parameter 15 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0316 10:28:58.208104 140205670307008 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 11.block_0 (torch.Size([0])) for Parameter 11 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 10:28:58.208107 140303193433280 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 13.block_0 ([torch.Size([0])]) for Parameter 13 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 10:28:58.208124 139935067108544 shampoo_preconditioner_list.py:375] Rank 4: AdaGradPreconditionerList Numel Breakdown: (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 131072, 0, 0, 0)
I0316 10:28:58.208145 140436539851968 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 5.block_0 (torch.Size([0])) for Parameter 5 (torch.Size([128])), Block block_0 (torch.Size([128])).
I0316 10:28:58.208150 140205670307008 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 12.block_0 (torch.Size([0])) for Parameter 12 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 10:28:58.208151 140059093185728 shampoo_preconditioner_list.py:375] Rank 2: AdaGradPreconditionerList Numel Breakdown: (0, 0, 0, 0, 0, 0, 518144, 0, 0, 0, 0, 0, 0, 0, 0, 0)
I0316 10:28:58.208167 139935067108544 shampoo_preconditioner_list.py:378] Rank 4: AdaGradPreconditionerList Bytes Breakdown: (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 524288, 0, 0, 0)
I0316 10:28:58.208183 140059093185728 shampoo_preconditioner_list.py:378] Rank 2: AdaGradPreconditionerList Bytes Breakdown: (0, 0, 0, 0, 0, 0, 2072576, 0, 0, 0, 0, 0, 0, 0, 0, 0)
I0316 10:28:58.208194 140436539851968 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 6.block_0 (torch.Size([0])) for Parameter 6 (torch.Size([1024, 506])), Block block_0 (torch.Size([1024, 506])).
I0316 10:28:58.208196 140205670307008 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 13.block_0 (torch.Size([0])) for Parameter 13 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 10:28:58.208200 139935067108544 shampoo_preconditioner_list.py:381] Rank 4: AdaGradPreconditionerList Total Elements: 131072
I0316 10:28:58.208202 140303193433280 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 14.block_0 ([torch.Size([0])]) for Parameter 14 (torch.Size([1, 256])), Block block_0 (torch.Size([256])).
I0316 10:28:58.208187 139806470304960 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 13.block_0 ([torch.Size([256, 256])]) for Parameter 13 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 10:28:58.208210 140059093185728 shampoo_preconditioner_list.py:381] Rank 2: AdaGradPreconditionerList Total Elements: 518144
I0316 10:28:58.208235 139935067108544 shampoo_preconditioner_list.py:384] Rank 4: AdaGradPreconditionerList Total Bytes: 524288
I0316 10:28:58.208241 140205670307008 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 14.block_0 (torch.Size([0])) for Parameter 14 (torch.Size([1, 256])), Block block_0 (torch.Size([256])).
I0316 10:28:58.208245 140059093185728 shampoo_preconditioner_list.py:384] Rank 2: AdaGradPreconditionerList Total Bytes: 2072576
I0316 10:28:58.208254 140436539851968 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 7.block_0 (torch.Size([0])) for Parameter 7 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 10:28:58.208297 140205670307008 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 15.block_0 (torch.Size([0])) for Parameter 15 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0316 10:28:58.208303 140436539851968 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 8.block_0 (torch.Size([0])) for Parameter 8 (torch.Size([1024, 1024])), Block block_0 (torch.Size([1024, 1024])).
I0316 10:28:58.208301 140303193433280 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 15.block_0 ([torch.Size([0])]) for Parameter 15 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0316 10:28:58.208352 140436539851968 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 9.block_0 (torch.Size([0])) for Parameter 9 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 10:28:58.208342 139806470304960 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 14.block_0 ([torch.Size([256, 256])]) for Parameter 14 (torch.Size([1, 256])), Block block_0 (torch.Size([256])).
I0316 10:28:58.208364 140205670307008 shampoo_preconditioner_list.py:375] Rank 5: AdaGradPreconditionerList Numel Breakdown: (0, 0, 0, 0, 32768, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)
I0316 10:28:58.208397 140205670307008 shampoo_preconditioner_list.py:378] Rank 5: AdaGradPreconditionerList Bytes Breakdown: (0, 0, 0, 0, 131072, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)
I0316 10:28:58.208405 140436539851968 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 10.block_0 (torch.Size([0])) for Parameter 10 (torch.Size([512, 1024])), Block block_0 (torch.Size([512, 1024])).
I0316 10:28:58.208409 140303193433280 shampoo_preconditioner_list.py:612] Rank 1: ShampooPreconditionerList Numel Breakdown: (524626, 524288, 655360, 131072, 163840, 32768, 2609224, 2097152, 4194304, 2097152, 2621440, 524288, 655360, 131072, 131072, 2)
I0316 10:28:58.208432 140205670307008 shampoo_preconditioner_list.py:381] Rank 5: AdaGradPreconditionerList Total Elements: 32768
I0316 10:28:58.208451 140303193433280 shampoo_preconditioner_list.py:615] Rank 1: ShampooPreconditionerList Bytes Breakdown: (2098504,)
I0316 10:28:58.208463 140436539851968 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 11.block_0 (torch.Size([0])) for Parameter 11 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 10:28:58.208476 140205670307008 shampoo_preconditioner_list.py:384] Rank 5: AdaGradPreconditionerList Total Bytes: 131072
I0316 10:28:58.208484 140303193433280 shampoo_preconditioner_list.py:618] Rank 1: ShampooPreconditionerList Total Elements: 17093020
I0316 10:28:58.208509 140436539851968 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 12.block_0 (torch.Size([0])) for Parameter 12 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 10:28:58.208498 139806470304960 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 15.block_0 ([torch.Size([1, 1])]) for Parameter 15 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0316 10:28:58.208515 140303193433280 shampoo_preconditioner_list.py:621] Rank 1: ShampooPreconditionerList Total Bytes: 2098504
I0316 10:28:58.208560 140436539851968 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 13.block_0 (torch.Size([0])) for Parameter 13 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 10:28:58.208615 139806470304960 shampoo_preconditioner_list.py:612] Rank 7: ShampooPreconditionerList Numel Breakdown: (524626, 524288, 655360, 131072, 163840, 32768, 2609224, 2097152, 4194304, 2097152, 2621440, 524288, 655360, 131072, 131072, 2)
I0316 10:28:58.208621 140436539851968 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 14.block_0 (torch.Size([0])) for Parameter 14 (torch.Size([1, 256])), Block block_0 (torch.Size([256])).
W0316 10:28:58.208623 139935067108544 distributed_shampoo.py:359] Nesterov flag is enabled but momentum parameter is zero! Continuing without using momentum or Nesterov acceleration...
W0316 10:28:58.208636 140059093185728 distributed_shampoo.py:359] Nesterov flag is enabled but momentum parameter is zero! Continuing without using momentum or Nesterov acceleration...
I0316 10:28:58.208658 140303193433280 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([512, 13])), Block block_0 (torch.Size([512, 13])).
I0316 10:28:58.208667 139806470304960 shampoo_preconditioner_list.py:615] Rank 7: ShampooPreconditionerList Bytes Breakdown: (2098504, 2097152, 2621440, 524288, 655360, 131072, 10436896, 8388608, 16777216)
I0316 10:28:58.208668 140436539851968 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 15.block_0 (torch.Size([0])) for Parameter 15 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0316 10:28:58.208698 139806470304960 shampoo_preconditioner_list.py:618] Rank 7: ShampooPreconditionerList Total Elements: 17093020
I0316 10:28:58.208721 140303193433280 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 10:28:58.208727 140436539851968 shampoo_preconditioner_list.py:375] Rank 3: AdaGradPreconditionerList Numel Breakdown: (0, 0, 131072, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)
I0316 10:28:58.208732 139806470304960 shampoo_preconditioner_list.py:621] Rank 7: ShampooPreconditionerList Total Bytes: 43730536
I0316 10:28:58.208758 140436539851968 shampoo_preconditioner_list.py:378] Rank 3: AdaGradPreconditionerList Bytes Breakdown: (0, 0, 524288, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)
I0316 10:28:58.208791 140303193433280 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 10:28:58.208793 140436539851968 shampoo_preconditioner_list.py:381] Rank 3: AdaGradPreconditionerList Total Elements: 131072
I0316 10:28:58.208826 140436539851968 shampoo_preconditioner_list.py:384] Rank 3: AdaGradPreconditionerList Total Bytes: 524288
I0316 10:28:58.208855 140303193433280 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 10:28:58.208861 139806470304960 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([512, 13])), Block block_0 (torch.Size([512, 13])).
I0316 10:28:58.208926 140303193433280 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 4.block_0 (torch.Size([0])) for Parameter 4 (torch.Size([128, 256])), Block block_0 (torch.Size([128, 256])).
I0316 10:28:58.208954 139806470304960 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([512])) for Parameter 1 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 10:28:58.208990 140303193433280 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 5.block_0 (torch.Size([0])) for Parameter 5 (torch.Size([128])), Block block_0 (torch.Size([128])).
I0316 10:28:58.208966 140444057306304 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([524288, 128])) for Parameter 0 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 10:28:58.209027 139806470304960 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 10:28:58.209052 140303193433280 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 6.block_0 (torch.Size([0])) for Parameter 6 (torch.Size([1024, 506])), Block block_0 (torch.Size([1024, 506])).
I0316 10:28:58.209071 140444057306304 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_1 (torch.Size([0])) for Parameter 0 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 10:28:58.209110 139806470304960 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([256])) for Parameter 3 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 10:28:58.209120 140303193433280 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 7.block_0 (torch.Size([0])) for Parameter 7 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 10:28:58.209147 140444057306304 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 10:28:58.209176 140303193433280 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 8.block_0 (torch.Size([0])) for Parameter 8 (torch.Size([1024, 1024])), Block block_0 (torch.Size([1024, 1024])).
I0316 10:28:58.209177 139806470304960 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 4.block_0 (torch.Size([0])) for Parameter 4 (torch.Size([128, 256])), Block block_0 (torch.Size([128, 256])).
I0316 10:28:58.209207 140444057306304 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_1 (torch.Size([0])) for Parameter 1 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
W0316 10:28:58.209202 140436539851968 distributed_shampoo.py:359] Nesterov flag is enabled but momentum parameter is zero! Continuing without using momentum or Nesterov acceleration...
I0316 10:28:58.209238 140303193433280 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 9.block_0 (torch.Size([0])) for Parameter 9 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 10:28:58.209249 139806470304960 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 5.block_0 (torch.Size([128])) for Parameter 5 (torch.Size([128])), Block block_0 (torch.Size([128])).
I0316 10:28:58.209264 140444057306304 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 10:28:58.209271 140414593897664 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 10:28:58.209308 139806470304960 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 6.block_0 (torch.Size([0])) for Parameter 6 (torch.Size([1024, 506])), Block block_0 (torch.Size([1024, 506])).
I0316 10:28:58.209312 140444057306304 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_1 (torch.Size([0])) for Parameter 2 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 10:28:58.209350 140303193433280 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 10.block_0 (torch.Size([512, 1024])) for Parameter 10 (torch.Size([512, 1024])), Block block_0 (torch.Size([512, 1024])).
I0316 10:28:58.209360 140444057306304 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 10:28:58.209371 140414593897664 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_1 (torch.Size([0])) for Parameter 0 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 10:28:58.209383 139806470304960 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 7.block_0 (torch.Size([1024])) for Parameter 7 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 10:28:58.209406 140444057306304 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_1 (torch.Size([0])) for Parameter 3 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
W0316 10:28:58.209403 140205670307008 distributed_shampoo.py:359] Nesterov flag is enabled but momentum parameter is zero! Continuing without using momentum or Nesterov acceleration...
I0316 10:28:58.209440 140414593897664 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 10:28:58.209443 140303193433280 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 11.block_0 (torch.Size([0])) for Parameter 11 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 10:28:58.209452 139806470304960 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 8.block_0 (torch.Size([0])) for Parameter 8 (torch.Size([1024, 1024])), Block block_0 (torch.Size([1024, 1024])).
I0316 10:28:58.209463 140444057306304 shampoo_preconditioner_list.py:375] Rank 0: AdaGradPreconditionerList Numel Breakdown: (67108864, 0, 0, 0, 0, 0, 0, 0)
I0316 10:28:58.209492 140414593897664 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_1 (torch.Size([0])) for Parameter 1 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 10:28:58.209500 140303193433280 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 12.block_0 (torch.Size([0])) for Parameter 12 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 10:28:58.209505 140444057306304 shampoo_preconditioner_list.py:378] Rank 0: AdaGradPreconditionerList Bytes Breakdown: (268435456, 0, 0, 0, 0, 0, 0, 0)
I0316 10:28:58.209525 139806470304960 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 9.block_0 (torch.Size([1024])) for Parameter 9 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 10:28:58.209534 140444057306304 shampoo_preconditioner_list.py:381] Rank 0: AdaGradPreconditionerList Total Elements: 67108864
I0316 10:28:58.209550 140414593897664 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 10:28:58.209552 140303193433280 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 13.block_0 (torch.Size([0])) for Parameter 13 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 10:28:58.209562 140444057306304 shampoo_preconditioner_list.py:384] Rank 0: AdaGradPreconditionerList Total Bytes: 268435456
I0316 10:28:58.209591 139806470304960 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 10.block_0 (torch.Size([0])) for Parameter 10 (torch.Size([512, 1024])), Block block_0 (torch.Size([512, 1024])).
I0316 10:28:58.209605 140414593897664 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_1 (torch.Size([0])) for Parameter 2 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 10:28:58.209606 140303193433280 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 14.block_0 (torch.Size([0])) for Parameter 14 (torch.Size([1, 256])), Block block_0 (torch.Size([256])).
I0316 10:28:58.209658 140303193433280 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 15.block_0 (torch.Size([0])) for Parameter 15 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0316 10:28:58.209669 139806470304960 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 11.block_0 (torch.Size([512])) for Parameter 11 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 10:28:58.209726 139806470304960 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 12.block_0 (torch.Size([0])) for Parameter 12 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 10:28:58.209726 140303193433280 shampoo_preconditioner_list.py:375] Rank 1: AdaGradPreconditionerList Numel Breakdown: (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 524288, 0, 0, 0, 0, 0)
I0316 10:28:58.209762 140303193433280 shampoo_preconditioner_list.py:378] Rank 1: AdaGradPreconditionerList Bytes Breakdown: (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2097152, 0, 0, 0, 0, 0)
I0316 10:28:58.209798 140303193433280 shampoo_preconditioner_list.py:381] Rank 1: AdaGradPreconditionerList Total Elements: 524288
I0316 10:28:58.209803 139806470304960 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 13.block_0 (torch.Size([256])) for Parameter 13 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 10:28:58.209834 140303193433280 shampoo_preconditioner_list.py:384] Rank 1: AdaGradPreconditionerList Total Bytes: 2097152
I0316 10:28:58.209883 139806470304960 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 14.block_0 (torch.Size([256])) for Parameter 14 (torch.Size([1, 256])), Block block_0 (torch.Size([256])).
I0316 10:28:58.209970 139806470304960 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 15.block_0 (torch.Size([1])) for Parameter 15 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0316 10:28:58.210052 139806470304960 shampoo_preconditioner_list.py:375] Rank 7: AdaGradPreconditionerList Numel Breakdown: (0, 512, 0, 256, 0, 128, 0, 1024, 0, 1024, 0, 512, 0, 256, 256, 1)
I0316 10:28:58.210029 139935067108544 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 10:28:58.210092 139806470304960 shampoo_preconditioner_list.py:378] Rank 7: AdaGradPreconditionerList Bytes Breakdown: (0, 2048, 0, 1024, 0, 512, 0, 4096, 0, 4096, 0, 2048, 0, 1024, 1024, 4)
I0316 10:28:58.210116 139935067108544 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_1 (torch.Size([0])) for Parameter 0 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 10:28:58.210125 139806470304960 shampoo_preconditioner_list.py:381] Rank 7: AdaGradPreconditionerList Total Elements: 3969
I0316 10:28:58.210119 140059093185728 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 10:28:58.210152 139806470304960 shampoo_preconditioner_list.py:384] Rank 7: AdaGradPreconditionerList Total Bytes: 15876
I0316 10:28:58.210187 139935067108544 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 10:28:58.210224 140059093185728 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_1 (torch.Size([0])) for Parameter 0 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 10:28:58.210259 139935067108544 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_1 (torch.Size([0])) for Parameter 1 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
W0316 10:28:58.210301 140303193433280 distributed_shampoo.py:359] Nesterov flag is enabled but momentum parameter is zero! Continuing without using momentum or Nesterov acceleration...
I0316 10:28:58.210569 140414593897664 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([524288, 128])) for Parameter 3 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
W0316 10:28:58.210654 139806470304960 distributed_shampoo.py:359] Nesterov flag is enabled but momentum parameter is zero! Continuing without using momentum or Nesterov acceleration...
I0316 10:28:58.210676 140414593897664 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_1 (torch.Size([0])) for Parameter 3 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 10:28:58.210727 140414593897664 shampoo_preconditioner_list.py:375] Rank 6: AdaGradPreconditionerList Numel Breakdown: (0, 0, 0, 0, 0, 0, 67108864, 0)
I0316 10:28:58.210775 140414593897664 shampoo_preconditioner_list.py:378] Rank 6: AdaGradPreconditionerList Bytes Breakdown: (0, 0, 0, 0, 0, 0, 268435456, 0)
I0316 10:28:58.210809 140414593897664 shampoo_preconditioner_list.py:381] Rank 6: AdaGradPreconditionerList Total Elements: 67108864
I0316 10:28:58.210841 140414593897664 shampoo_preconditioner_list.py:384] Rank 6: AdaGradPreconditionerList Total Bytes: 268435456
I0316 10:28:58.210876 140436539851968 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 10:28:58.210903 140444057306304 submission_runner.py:279] Initializing metrics bundle.
I0316 10:28:58.210978 140436539851968 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_1 (torch.Size([0])) for Parameter 0 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 10:28:58.211045 140436539851968 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 10:28:58.211047 140444057306304 submission_runner.py:301] Initializing checkpoint and logger.
I0316 10:28:58.211274 140059093185728 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([524288, 128])) for Parameter 1 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 10:28:58.211359 139935067108544 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([524288, 128])) for Parameter 2 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 10:28:58.211392 140059093185728 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_1 (torch.Size([0])) for Parameter 1 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 10:28:58.211461 140059093185728 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 10:28:58.211486 139935067108544 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_1 (torch.Size([0])) for Parameter 2 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 10:28:58.211503 140444057306304 submission_runner.py:321] Saving meta data to /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_0/criteo1tb_pytorch/trial_1/meta_data_0.json.
I0316 10:28:58.211521 140059093185728 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_1 (torch.Size([0])) for Parameter 2 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 10:28:58.211504 140205670307008 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 10:28:58.211554 139935067108544 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 10:28:58.211572 140059093185728 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 10:28:58.211605 139935067108544 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_1 (torch.Size([0])) for Parameter 3 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 10:28:58.211621 140059093185728 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_1 (torch.Size([0])) for Parameter 3 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 10:28:58.211632 140205670307008 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_1 (torch.Size([0])) for Parameter 0 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 10:28:58.211658 139935067108544 shampoo_preconditioner_list.py:375] Rank 4: AdaGradPreconditionerList Numel Breakdown: (0, 0, 0, 0, 67108864, 0, 0, 0)
I0316 10:28:58.211665 140444057306304 logger_utils.py:262] Unable to record workload.train_mean information. Continuing without it.
I0316 10:28:58.211666 140059093185728 shampoo_preconditioner_list.py:375] Rank 2: AdaGradPreconditionerList Numel Breakdown: (0, 0, 67108864, 0, 0, 0, 0, 0)
I0316 10:28:58.211696 139935067108544 shampoo_preconditioner_list.py:378] Rank 4: AdaGradPreconditionerList Bytes Breakdown: (0, 0, 0, 0, 268435456, 0, 0, 0)
I0316 10:28:58.211704 140444057306304 logger_utils.py:262] Unable to record workload.train_stddev information. Continuing without it.
I0316 10:28:58.211707 140059093185728 shampoo_preconditioner_list.py:378] Rank 2: AdaGradPreconditionerList Bytes Breakdown: (0, 0, 268435456, 0, 0, 0, 0, 0)
I0316 10:28:58.211713 140205670307008 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 10:28:58.211729 139935067108544 shampoo_preconditioner_list.py:381] Rank 4: AdaGradPreconditionerList Total Elements: 67108864
I0316 10:28:58.211735 140059093185728 shampoo_preconditioner_list.py:381] Rank 2: AdaGradPreconditionerList Total Elements: 67108864
I0316 10:28:58.211756 139935067108544 shampoo_preconditioner_list.py:384] Rank 4: AdaGradPreconditionerList Total Bytes: 268435456
I0316 10:28:58.211761 140059093185728 shampoo_preconditioner_list.py:384] Rank 2: AdaGradPreconditionerList Total Bytes: 268435456
I0316 10:28:58.211773 140205670307008 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_1 (torch.Size([0])) for Parameter 1 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 10:28:58.211838 140205670307008 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 10:28:58.212373 140436539851968 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_1 (torch.Size([524288, 128])) for Parameter 1 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 10:28:58.212406 140303193433280 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 10:28:58.212491 140436539851968 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 10:28:58.212555 140436539851968 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_1 (torch.Size([0])) for Parameter 2 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 10:28:58.212613 140436539851968 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 10:28:58.212668 140436539851968 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_1 (torch.Size([0])) for Parameter 3 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 10:28:58.212646 139806470304960 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 10:28:58.212719 140436539851968 shampoo_preconditioner_list.py:375] Rank 3: AdaGradPreconditionerList Numel Breakdown: (0, 0, 0, 67108864, 0, 0, 0, 0)
I0316 10:28:58.212737 139806470304960 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_1 (torch.Size([0])) for Parameter 0 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 10:28:58.212764 140436539851968 shampoo_preconditioner_list.py:378] Rank 3: AdaGradPreconditionerList Bytes Breakdown: (0, 0, 0, 268435456, 0, 0, 0, 0)
I0316 10:28:58.212797 140436539851968 shampoo_preconditioner_list.py:381] Rank 3: AdaGradPreconditionerList Total Elements: 67108864
I0316 10:28:58.212793 139806470304960 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 10:28:58.212830 140436539851968 shampoo_preconditioner_list.py:384] Rank 3: AdaGradPreconditionerList Total Bytes: 268435456
I0316 10:28:58.212819 140414593897664 logger_utils.py:262] Unable to record workload.train_mean information. Continuing without it.
I0316 10:28:58.212856 139806470304960 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_1 (torch.Size([0])) for Parameter 1 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 10:28:58.212888 140414593897664 logger_utils.py:262] Unable to record workload.train_stddev information. Continuing without it.
I0316 10:28:58.212903 139806470304960 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 10:28:58.212948 139806470304960 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_1 (torch.Size([0])) for Parameter 2 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 10:28:58.213004 139806470304960 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 10:28:58.213009 140205670307008 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_1 (torch.Size([524288, 128])) for Parameter 2 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 10:28:58.213126 140205670307008 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 10:28:58.213194 140205670307008 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_1 (torch.Size([0])) for Parameter 3 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 10:28:58.213267 140205670307008 shampoo_preconditioner_list.py:375] Rank 5: AdaGradPreconditionerList Numel Breakdown: (0, 0, 0, 0, 0, 67108864, 0, 0)
I0316 10:28:58.213279 140303193433280 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_1 (torch.Size([524288, 128])) for Parameter 0 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 10:28:58.213309 140205670307008 shampoo_preconditioner_list.py:378] Rank 5: AdaGradPreconditionerList Bytes Breakdown: (0, 0, 0, 0, 0, 268435456, 0, 0)
I0316 10:28:58.213338 140205670307008 shampoo_preconditioner_list.py:381] Rank 5: AdaGradPreconditionerList Total Elements: 67108864
I0316 10:28:58.213372 140205670307008 shampoo_preconditioner_list.py:384] Rank 5: AdaGradPreconditionerList Total Bytes: 268435456
I0316 10:28:58.213416 140303193433280 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 10:28:58.213492 140303193433280 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_1 (torch.Size([0])) for Parameter 1 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 10:28:58.213559 140303193433280 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 10:28:58.213623 140303193433280 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_1 (torch.Size([0])) for Parameter 2 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 10:28:58.213647 140059093185728 logger_utils.py:262] Unable to record workload.train_mean information. Continuing without it.
I0316 10:28:58.213685 140303193433280 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 10:28:58.213667 139806470304960 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_1 (torch.Size([524288, 128])) for Parameter 3 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 10:28:58.213720 140059093185728 logger_utils.py:262] Unable to record workload.train_stddev information. Continuing without it.
I0316 10:28:58.213716 139935067108544 logger_utils.py:262] Unable to record workload.train_mean information. Continuing without it.
I0316 10:28:58.213744 139806470304960 shampoo_preconditioner_list.py:375] Rank 7: AdaGradPreconditionerList Numel Breakdown: (0, 0, 0, 0, 0, 0, 0, 67108864)
I0316 10:28:58.213776 139806470304960 shampoo_preconditioner_list.py:378] Rank 7: AdaGradPreconditionerList Bytes Breakdown: (0, 0, 0, 0, 0, 0, 0, 268435456)
I0316 10:28:58.213768 140303193433280 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_1 (torch.Size([0])) for Parameter 3 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 10:28:58.213776 139935067108544 logger_utils.py:262] Unable to record workload.train_stddev information. Continuing without it.
I0316 10:28:58.213815 139806470304960 shampoo_preconditioner_list.py:381] Rank 7: AdaGradPreconditionerList Total Elements: 67108864
I0316 10:28:58.213826 140303193433280 shampoo_preconditioner_list.py:375] Rank 1: AdaGradPreconditionerList Numel Breakdown: (0, 67108864, 0, 0, 0, 0, 0, 0)
I0316 10:28:58.213846 139806470304960 shampoo_preconditioner_list.py:384] Rank 7: AdaGradPreconditionerList Total Bytes: 268435456
I0316 10:28:58.213860 140303193433280 shampoo_preconditioner_list.py:378] Rank 1: AdaGradPreconditionerList Bytes Breakdown: (0, 268435456, 0, 0, 0, 0, 0, 0)
I0316 10:28:58.213889 140303193433280 shampoo_preconditioner_list.py:381] Rank 1: AdaGradPreconditionerList Total Elements: 67108864
I0316 10:28:58.213926 140303193433280 shampoo_preconditioner_list.py:384] Rank 1: AdaGradPreconditionerList Total Bytes: 268435456
I0316 10:28:58.214444 140436539851968 logger_utils.py:262] Unable to record workload.train_mean information. Continuing without it.
I0316 10:28:58.214521 140436539851968 logger_utils.py:262] Unable to record workload.train_stddev information. Continuing without it.
I0316 10:28:58.214814 140205670307008 logger_utils.py:262] Unable to record workload.train_mean information. Continuing without it.
I0316 10:28:58.214893 140205670307008 logger_utils.py:262] Unable to record workload.train_stddev information. Continuing without it.
I0316 10:28:58.215390 139806470304960 logger_utils.py:262] Unable to record workload.train_mean information. Continuing without it.
I0316 10:28:58.215474 139806470304960 logger_utils.py:262] Unable to record workload.train_stddev information. Continuing without it.
I0316 10:28:58.215618 140303193433280 logger_utils.py:262] Unable to record workload.train_mean information. Continuing without it.
I0316 10:28:58.215698 140303193433280 logger_utils.py:262] Unable to record workload.train_stddev information. Continuing without it.
I0316 10:28:58.983408 140444057306304 submission_runner.py:325] Saving flags to /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_0/criteo1tb_pytorch/trial_1/flags_0.json.
I0316 10:28:59.017379 140444057306304 submission_runner.py:337] Starting training loop.
I0316 10:29:05.520203 140412372117248 logging_writer.py:48] [0] global_step=0, grad_norm=5.22056, loss=0.424114
I0316 10:29:05.657109 140444057306304 submission.py:265] 0) loss = 0.424, grad_norm = 5.221
I0316 10:29:06.070258 140444057306304 spec.py:321] Evaluating on the training split.
I0316 10:34:09.263180 140444057306304 spec.py:333] Evaluating on the validation split.
I0316 10:39:05.831947 140444057306304 spec.py:349] Evaluating on the test split.
I0316 10:44:48.289283 140444057306304 submission_runner.py:469] Time since start: 949.27s, 	Step: 1, 	{'train/loss': 0.42260579897679673, 'validation/loss': 0.42486521993325754, 'validation/num_examples': 83274637, 'test/loss': 0.4235680038511577, 'test/num_examples': 95000000, 'score': 6.640608072280884, 'total_duration': 949.2720501422882, 'accumulated_submission_time': 6.640608072280884, 'accumulated_eval_time': 942.2192132472992, 'accumulated_logging_time': 0}
I0316 10:44:48.356816 140404116662016 logging_writer.py:48] [1] accumulated_eval_time=942.219, accumulated_logging_time=0, accumulated_submission_time=6.64061, global_step=1, preemption_count=0, score=6.64061, test/loss=0.423568, test/num_examples=95000000, total_duration=949.272, train/loss=0.422606, validation/loss=0.424865, validation/num_examples=83274637
I0316 10:44:49.066612 140404108269312 logging_writer.py:48] [1] global_step=1, grad_norm=5.23305, loss=0.423586
I0316 10:44:49.070280 140444057306304 submission.py:265] 1) loss = 0.424, grad_norm = 5.233
I0316 10:44:49.264791 140404116662016 logging_writer.py:48] [2] global_step=2, grad_norm=4.89842, loss=0.400605
I0316 10:44:49.267642 140444057306304 submission.py:265] 2) loss = 0.401, grad_norm = 4.898
I0316 10:44:49.462260 140404108269312 logging_writer.py:48] [3] global_step=3, grad_norm=4.37632, loss=0.36216
I0316 10:44:49.465209 140444057306304 submission.py:265] 3) loss = 0.362, grad_norm = 4.376
I0316 10:44:49.658681 140404116662016 logging_writer.py:48] [4] global_step=4, grad_norm=3.65751, loss=0.316631
I0316 10:44:49.661689 140444057306304 submission.py:265] 4) loss = 0.317, grad_norm = 3.658
I0316 10:44:49.858875 140404108269312 logging_writer.py:48] [5] global_step=5, grad_norm=2.84221, loss=0.268915
I0316 10:44:49.872499 140444057306304 submission.py:265] 5) loss = 0.269, grad_norm = 2.842
I0316 10:44:50.068227 140404116662016 logging_writer.py:48] [6] global_step=6, grad_norm=2.04054, loss=0.228479
I0316 10:44:50.071506 140444057306304 submission.py:265] 6) loss = 0.228, grad_norm = 2.041
I0316 10:44:50.268753 140404108269312 logging_writer.py:48] [7] global_step=7, grad_norm=1.40596, loss=0.196195
I0316 10:44:50.271690 140444057306304 submission.py:265] 7) loss = 0.196, grad_norm = 1.406
I0316 10:44:50.467458 140404116662016 logging_writer.py:48] [8] global_step=8, grad_norm=0.899828, loss=0.176221
I0316 10:44:50.470534 140444057306304 submission.py:265] 8) loss = 0.176, grad_norm = 0.900
I0316 10:44:50.664888 140404108269312 logging_writer.py:48] [9] global_step=9, grad_norm=0.47613, loss=0.163365
I0316 10:44:50.668777 140444057306304 submission.py:265] 9) loss = 0.163, grad_norm = 0.476
I0316 10:44:50.880373 140404116662016 logging_writer.py:48] [10] global_step=10, grad_norm=0.151336, loss=0.157052
I0316 10:44:50.884080 140444057306304 submission.py:265] 10) loss = 0.157, grad_norm = 0.151
I0316 10:44:51.080448 140404108269312 logging_writer.py:48] [11] global_step=11, grad_norm=0.248693, loss=0.158818
I0316 10:44:51.083604 140444057306304 submission.py:265] 11) loss = 0.159, grad_norm = 0.249
I0316 10:44:51.278833 140404116662016 logging_writer.py:48] [12] global_step=12, grad_norm=0.458607, loss=0.160586
I0316 10:44:51.281832 140444057306304 submission.py:265] 12) loss = 0.161, grad_norm = 0.459
I0316 10:44:51.476377 140404108269312 logging_writer.py:48] [13] global_step=13, grad_norm=0.675594, loss=0.174006
I0316 10:44:51.479366 140444057306304 submission.py:265] 13) loss = 0.174, grad_norm = 0.676
I0316 10:44:51.672508 140404116662016 logging_writer.py:48] [14] global_step=14, grad_norm=0.768482, loss=0.176334
I0316 10:44:51.675895 140444057306304 submission.py:265] 14) loss = 0.176, grad_norm = 0.768
I0316 10:44:51.870348 140404108269312 logging_writer.py:48] [15] global_step=15, grad_norm=0.856749, loss=0.183036
I0316 10:44:51.874168 140444057306304 submission.py:265] 15) loss = 0.183, grad_norm = 0.857
I0316 10:44:52.068569 140404116662016 logging_writer.py:48] [16] global_step=16, grad_norm=0.891992, loss=0.185649
I0316 10:44:52.071729 140444057306304 submission.py:265] 16) loss = 0.186, grad_norm = 0.892
I0316 10:44:52.266868 140404108269312 logging_writer.py:48] [17] global_step=17, grad_norm=0.901539, loss=0.188489
I0316 10:44:52.269843 140444057306304 submission.py:265] 17) loss = 0.188, grad_norm = 0.902
I0316 10:44:52.464025 140404116662016 logging_writer.py:48] [18] global_step=18, grad_norm=0.829579, loss=0.180005
I0316 10:44:52.467013 140444057306304 submission.py:265] 18) loss = 0.180, grad_norm = 0.830
I0316 10:44:52.660420 140404108269312 logging_writer.py:48] [19] global_step=19, grad_norm=0.761269, loss=0.175986
I0316 10:44:52.663699 140444057306304 submission.py:265] 19) loss = 0.176, grad_norm = 0.761
I0316 10:44:52.856506 140404116662016 logging_writer.py:48] [20] global_step=20, grad_norm=0.691438, loss=0.171243
I0316 10:44:52.859660 140444057306304 submission.py:265] 20) loss = 0.171, grad_norm = 0.691
I0316 10:44:53.056420 140404108269312 logging_writer.py:48] [21] global_step=21, grad_norm=0.575061, loss=0.167343
I0316 10:44:53.059934 140444057306304 submission.py:265] 21) loss = 0.167, grad_norm = 0.575
I0316 10:44:53.253487 140404116662016 logging_writer.py:48] [22] global_step=22, grad_norm=0.424886, loss=0.163209
I0316 10:44:53.256823 140444057306304 submission.py:265] 22) loss = 0.163, grad_norm = 0.425
I0316 10:44:53.448476 140404108269312 logging_writer.py:48] [23] global_step=23, grad_norm=0.238844, loss=0.157653
I0316 10:44:53.451559 140444057306304 submission.py:265] 23) loss = 0.158, grad_norm = 0.239
I0316 10:44:53.643713 140404116662016 logging_writer.py:48] [24] global_step=24, grad_norm=0.0994103, loss=0.15715
I0316 10:44:53.646924 140444057306304 submission.py:265] 24) loss = 0.157, grad_norm = 0.099
I0316 10:44:53.839792 140404108269312 logging_writer.py:48] [25] global_step=25, grad_norm=0.139877, loss=0.153548
I0316 10:44:53.843040 140444057306304 submission.py:265] 25) loss = 0.154, grad_norm = 0.140
I0316 10:44:54.050188 140404116662016 logging_writer.py:48] [26] global_step=26, grad_norm=0.200584, loss=0.150324
I0316 10:44:54.053483 140444057306304 submission.py:265] 26) loss = 0.150, grad_norm = 0.201
I0316 10:44:54.244303 140404108269312 logging_writer.py:48] [27] global_step=27, grad_norm=0.153672, loss=0.151162
I0316 10:44:54.247283 140444057306304 submission.py:265] 27) loss = 0.151, grad_norm = 0.154
I0316 10:44:54.438457 140404116662016 logging_writer.py:48] [28] global_step=28, grad_norm=0.0908307, loss=0.146764
I0316 10:44:54.441271 140444057306304 submission.py:265] 28) loss = 0.147, grad_norm = 0.091
I0316 10:44:54.632765 140404108269312 logging_writer.py:48] [29] global_step=29, grad_norm=0.0705785, loss=0.147984
I0316 10:44:54.636143 140444057306304 submission.py:265] 29) loss = 0.148, grad_norm = 0.071
I0316 10:44:54.892427 140404116662016 logging_writer.py:48] [30] global_step=30, grad_norm=0.0818952, loss=0.145166
I0316 10:44:54.895514 140444057306304 submission.py:265] 30) loss = 0.145, grad_norm = 0.082
I0316 10:44:55.548196 140404108269312 logging_writer.py:48] [31] global_step=31, grad_norm=0.0746209, loss=0.144599
I0316 10:44:55.551258 140444057306304 submission.py:265] 31) loss = 0.145, grad_norm = 0.075
I0316 10:44:56.852877 140404116662016 logging_writer.py:48] [32] global_step=32, grad_norm=0.042423, loss=0.142394
I0316 10:44:56.855984 140444057306304 submission.py:265] 32) loss = 0.142, grad_norm = 0.042
I0316 10:44:57.763052 140404108269312 logging_writer.py:48] [33] global_step=33, grad_norm=0.0511294, loss=0.146067
I0316 10:44:57.766031 140444057306304 submission.py:265] 33) loss = 0.146, grad_norm = 0.051
I0316 10:44:59.077620 140404116662016 logging_writer.py:48] [34] global_step=34, grad_norm=0.029365, loss=0.144377
I0316 10:44:59.081314 140444057306304 submission.py:265] 34) loss = 0.144, grad_norm = 0.029
I0316 10:45:00.074819 140404108269312 logging_writer.py:48] [35] global_step=35, grad_norm=0.0392767, loss=0.146166
I0316 10:45:00.078363 140444057306304 submission.py:265] 35) loss = 0.146, grad_norm = 0.039
I0316 10:45:01.240118 140404116662016 logging_writer.py:48] [36] global_step=36, grad_norm=0.0268338, loss=0.144687
I0316 10:45:01.243733 140444057306304 submission.py:265] 36) loss = 0.145, grad_norm = 0.027
I0316 10:45:02.453860 140404108269312 logging_writer.py:48] [37] global_step=37, grad_norm=0.0456928, loss=0.147061
I0316 10:45:02.457590 140444057306304 submission.py:265] 37) loss = 0.147, grad_norm = 0.046
I0316 10:45:03.641651 140404116662016 logging_writer.py:48] [38] global_step=38, grad_norm=0.0315482, loss=0.148263
I0316 10:45:03.645400 140444057306304 submission.py:265] 38) loss = 0.148, grad_norm = 0.032
I0316 10:45:04.813233 140404108269312 logging_writer.py:48] [39] global_step=39, grad_norm=0.0253585, loss=0.148357
I0316 10:45:04.816798 140444057306304 submission.py:265] 39) loss = 0.148, grad_norm = 0.025
I0316 10:45:06.309609 140404116662016 logging_writer.py:48] [40] global_step=40, grad_norm=0.0259001, loss=0.149846
I0316 10:45:06.313406 140444057306304 submission.py:265] 40) loss = 0.150, grad_norm = 0.026
I0316 10:45:08.022584 140404108269312 logging_writer.py:48] [41] global_step=41, grad_norm=0.0248778, loss=0.150079
I0316 10:45:08.026192 140444057306304 submission.py:265] 41) loss = 0.150, grad_norm = 0.025
I0316 10:45:09.138269 140404116662016 logging_writer.py:48] [42] global_step=42, grad_norm=0.0237375, loss=0.150483
I0316 10:45:09.141906 140444057306304 submission.py:265] 42) loss = 0.150, grad_norm = 0.024
I0316 10:45:10.942585 140404108269312 logging_writer.py:48] [43] global_step=43, grad_norm=0.0381448, loss=0.148885
I0316 10:45:10.946198 140444057306304 submission.py:265] 43) loss = 0.149, grad_norm = 0.038
I0316 10:45:11.947465 140404116662016 logging_writer.py:48] [44] global_step=44, grad_norm=0.0476172, loss=0.147631
I0316 10:45:11.950968 140444057306304 submission.py:265] 44) loss = 0.148, grad_norm = 0.048
I0316 10:45:13.818915 140404108269312 logging_writer.py:48] [45] global_step=45, grad_norm=0.0576894, loss=0.148249
I0316 10:45:13.822510 140444057306304 submission.py:265] 45) loss = 0.148, grad_norm = 0.058
I0316 10:45:14.202118 140404116662016 logging_writer.py:48] [46] global_step=46, grad_norm=0.0610724, loss=0.147677
I0316 10:45:14.205722 140444057306304 submission.py:265] 46) loss = 0.148, grad_norm = 0.061
I0316 10:45:16.269906 140404108269312 logging_writer.py:48] [47] global_step=47, grad_norm=0.0641479, loss=0.147287
I0316 10:45:16.273054 140444057306304 submission.py:265] 47) loss = 0.147, grad_norm = 0.064
I0316 10:45:17.213887 140404116662016 logging_writer.py:48] [48] global_step=48, grad_norm=0.0572508, loss=0.1461
I0316 10:45:17.217153 140444057306304 submission.py:265] 48) loss = 0.146, grad_norm = 0.057
I0316 10:45:18.687340 140404108269312 logging_writer.py:48] [49] global_step=49, grad_norm=0.0470867, loss=0.145855
I0316 10:45:18.690470 140444057306304 submission.py:265] 49) loss = 0.146, grad_norm = 0.047
I0316 10:45:19.592362 140404116662016 logging_writer.py:48] [50] global_step=50, grad_norm=0.0482191, loss=0.144731
I0316 10:45:19.595714 140444057306304 submission.py:265] 50) loss = 0.145, grad_norm = 0.048
I0316 10:45:21.266645 140404108269312 logging_writer.py:48] [51] global_step=51, grad_norm=0.0248064, loss=0.146368
I0316 10:45:21.269813 140444057306304 submission.py:265] 51) loss = 0.146, grad_norm = 0.025
I0316 10:45:22.615050 140404116662016 logging_writer.py:48] [52] global_step=52, grad_norm=0.0238743, loss=0.144792
I0316 10:45:22.618119 140444057306304 submission.py:265] 52) loss = 0.145, grad_norm = 0.024
I0316 10:45:23.149351 140404108269312 logging_writer.py:48] [53] global_step=53, grad_norm=0.0187012, loss=0.144413
I0316 10:45:23.153054 140444057306304 submission.py:265] 53) loss = 0.144, grad_norm = 0.019
I0316 10:45:24.928385 140404116662016 logging_writer.py:48] [54] global_step=54, grad_norm=0.0565477, loss=0.144777
I0316 10:45:24.931690 140444057306304 submission.py:265] 54) loss = 0.145, grad_norm = 0.057
I0316 10:45:25.979701 140404108269312 logging_writer.py:48] [55] global_step=55, grad_norm=0.0982273, loss=0.145448
I0316 10:45:25.982757 140444057306304 submission.py:265] 55) loss = 0.145, grad_norm = 0.098
I0316 10:45:27.335995 140404116662016 logging_writer.py:48] [56] global_step=56, grad_norm=0.16465, loss=0.145958
I0316 10:45:27.339280 140444057306304 submission.py:265] 56) loss = 0.146, grad_norm = 0.165
I0316 10:45:28.204513 140404108269312 logging_writer.py:48] [57] global_step=57, grad_norm=0.250598, loss=0.133638
I0316 10:45:28.207603 140444057306304 submission.py:265] 57) loss = 0.134, grad_norm = 0.251
I0316 10:45:29.321991 140404116662016 logging_writer.py:48] [58] global_step=58, grad_norm=0.169656, loss=0.130434
I0316 10:45:29.325142 140444057306304 submission.py:265] 58) loss = 0.130, grad_norm = 0.170
I0316 10:45:30.342946 140404108269312 logging_writer.py:48] [59] global_step=59, grad_norm=0.0173855, loss=0.12922
I0316 10:45:30.346112 140444057306304 submission.py:265] 59) loss = 0.129, grad_norm = 0.017
I0316 10:45:31.490442 140404116662016 logging_writer.py:48] [60] global_step=60, grad_norm=0.0182365, loss=0.127717
I0316 10:45:31.493414 140444057306304 submission.py:265] 60) loss = 0.128, grad_norm = 0.018
I0316 10:45:32.709131 140404108269312 logging_writer.py:48] [61] global_step=61, grad_norm=0.0220591, loss=0.128542
I0316 10:45:32.712282 140444057306304 submission.py:265] 61) loss = 0.129, grad_norm = 0.022
I0316 10:45:33.915762 140404116662016 logging_writer.py:48] [62] global_step=62, grad_norm=0.0120956, loss=0.128699
I0316 10:45:33.918787 140444057306304 submission.py:265] 62) loss = 0.129, grad_norm = 0.012
I0316 10:45:35.043608 140404108269312 logging_writer.py:48] [63] global_step=63, grad_norm=0.0104679, loss=0.128603
I0316 10:45:35.046776 140444057306304 submission.py:265] 63) loss = 0.129, grad_norm = 0.010
I0316 10:45:36.218091 140404116662016 logging_writer.py:48] [64] global_step=64, grad_norm=0.00835354, loss=0.12835
I0316 10:45:36.221198 140444057306304 submission.py:265] 64) loss = 0.128, grad_norm = 0.008
I0316 10:45:37.244026 140404108269312 logging_writer.py:48] [65] global_step=65, grad_norm=0.0177532, loss=0.125215
I0316 10:45:37.247273 140444057306304 submission.py:265] 65) loss = 0.125, grad_norm = 0.018
I0316 10:45:38.444755 140404116662016 logging_writer.py:48] [66] global_step=66, grad_norm=0.049224, loss=0.125566
I0316 10:45:38.447908 140444057306304 submission.py:265] 66) loss = 0.126, grad_norm = 0.049
I0316 10:45:39.848855 140404108269312 logging_writer.py:48] [67] global_step=67, grad_norm=0.0633259, loss=0.128834
I0316 10:45:39.851912 140444057306304 submission.py:265] 67) loss = 0.129, grad_norm = 0.063
I0316 10:45:41.069921 140404116662016 logging_writer.py:48] [68] global_step=68, grad_norm=0.0526994, loss=0.124616
I0316 10:45:41.073034 140444057306304 submission.py:265] 68) loss = 0.125, grad_norm = 0.053
I0316 10:45:42.248328 140404108269312 logging_writer.py:48] [69] global_step=69, grad_norm=0.0610704, loss=0.124227
I0316 10:45:42.252022 140444057306304 submission.py:265] 69) loss = 0.124, grad_norm = 0.061
I0316 10:45:43.878053 140404116662016 logging_writer.py:48] [70] global_step=70, grad_norm=0.089961, loss=0.125939
I0316 10:45:43.881166 140444057306304 submission.py:265] 70) loss = 0.126, grad_norm = 0.090
I0316 10:45:44.950504 140404108269312 logging_writer.py:48] [71] global_step=71, grad_norm=0.128494, loss=0.125168
I0316 10:45:44.953577 140444057306304 submission.py:265] 71) loss = 0.125, grad_norm = 0.128
I0316 10:45:46.536675 140404116662016 logging_writer.py:48] [72] global_step=72, grad_norm=0.128777, loss=0.124666
I0316 10:45:46.539937 140444057306304 submission.py:265] 72) loss = 0.125, grad_norm = 0.129
I0316 10:45:47.506030 140404108269312 logging_writer.py:48] [73] global_step=73, grad_norm=0.0901538, loss=0.124228
I0316 10:45:47.509053 140444057306304 submission.py:265] 73) loss = 0.124, grad_norm = 0.090
I0316 10:45:48.819260 140404116662016 logging_writer.py:48] [74] global_step=74, grad_norm=0.0588658, loss=0.123373
I0316 10:45:48.822568 140444057306304 submission.py:265] 74) loss = 0.123, grad_norm = 0.059
I0316 10:45:49.703198 140404108269312 logging_writer.py:48] [75] global_step=75, grad_norm=0.050298, loss=0.122563
I0316 10:45:49.706124 140444057306304 submission.py:265] 75) loss = 0.123, grad_norm = 0.050
I0316 10:45:51.003517 140404116662016 logging_writer.py:48] [76] global_step=76, grad_norm=0.043843, loss=0.127102
I0316 10:45:51.006873 140444057306304 submission.py:265] 76) loss = 0.127, grad_norm = 0.044
I0316 10:45:51.979202 140404108269312 logging_writer.py:48] [77] global_step=77, grad_norm=0.034545, loss=0.12903
I0316 10:45:51.982290 140444057306304 submission.py:265] 77) loss = 0.129, grad_norm = 0.035
I0316 10:45:53.642746 140404116662016 logging_writer.py:48] [78] global_step=78, grad_norm=0.0290216, loss=0.128578
I0316 10:45:53.646197 140444057306304 submission.py:265] 78) loss = 0.129, grad_norm = 0.029
I0316 10:45:54.601193 140404108269312 logging_writer.py:48] [79] global_step=79, grad_norm=0.037688, loss=0.126465
I0316 10:45:54.604316 140444057306304 submission.py:265] 79) loss = 0.126, grad_norm = 0.038
I0316 10:45:56.149775 140404116662016 logging_writer.py:48] [80] global_step=80, grad_norm=0.0393125, loss=0.125797
I0316 10:45:56.152939 140444057306304 submission.py:265] 80) loss = 0.126, grad_norm = 0.039
I0316 10:45:57.239558 140404108269312 logging_writer.py:48] [81] global_step=81, grad_norm=0.021491, loss=0.128256
I0316 10:45:57.242682 140444057306304 submission.py:265] 81) loss = 0.128, grad_norm = 0.021
I0316 10:45:58.775751 140404116662016 logging_writer.py:48] [82] global_step=82, grad_norm=0.00929394, loss=0.126596
I0316 10:45:58.779612 140444057306304 submission.py:265] 82) loss = 0.127, grad_norm = 0.009
I0316 10:45:59.856883 140404108269312 logging_writer.py:48] [83] global_step=83, grad_norm=0.0101066, loss=0.126034
I0316 10:45:59.860019 140444057306304 submission.py:265] 83) loss = 0.126, grad_norm = 0.010
I0316 10:46:01.595989 140404116662016 logging_writer.py:48] [84] global_step=84, grad_norm=0.00778507, loss=0.124133
I0316 10:46:01.599241 140444057306304 submission.py:265] 84) loss = 0.124, grad_norm = 0.008
I0316 10:46:02.480940 140404108269312 logging_writer.py:48] [85] global_step=85, grad_norm=0.0119444, loss=0.125184
I0316 10:46:02.484116 140444057306304 submission.py:265] 85) loss = 0.125, grad_norm = 0.012
I0316 10:46:04.352388 140404116662016 logging_writer.py:48] [86] global_step=86, grad_norm=0.020271, loss=0.125697
I0316 10:46:04.355573 140444057306304 submission.py:265] 86) loss = 0.126, grad_norm = 0.020
I0316 10:46:05.167184 140404108269312 logging_writer.py:48] [87] global_step=87, grad_norm=0.0457242, loss=0.127741
I0316 10:46:05.170232 140444057306304 submission.py:265] 87) loss = 0.128, grad_norm = 0.046
I0316 10:46:07.008255 140404116662016 logging_writer.py:48] [88] global_step=88, grad_norm=0.068171, loss=0.128394
I0316 10:46:07.011416 140444057306304 submission.py:265] 88) loss = 0.128, grad_norm = 0.068
I0316 10:46:07.890871 140404108269312 logging_writer.py:48] [89] global_step=89, grad_norm=0.0856508, loss=0.127538
I0316 10:46:07.893964 140444057306304 submission.py:265] 89) loss = 0.128, grad_norm = 0.086
I0316 10:46:09.528344 140404116662016 logging_writer.py:48] [90] global_step=90, grad_norm=0.102603, loss=0.126547
I0316 10:46:09.531545 140444057306304 submission.py:265] 90) loss = 0.127, grad_norm = 0.103
I0316 10:46:10.315405 140404108269312 logging_writer.py:48] [91] global_step=91, grad_norm=0.0943723, loss=0.12399
I0316 10:46:10.318619 140444057306304 submission.py:265] 91) loss = 0.124, grad_norm = 0.094
I0316 10:46:12.169332 140404116662016 logging_writer.py:48] [92] global_step=92, grad_norm=0.0638994, loss=0.125338
I0316 10:46:12.172535 140444057306304 submission.py:265] 92) loss = 0.125, grad_norm = 0.064
I0316 10:46:12.660832 140404108269312 logging_writer.py:48] [93] global_step=93, grad_norm=0.051562, loss=0.125238
I0316 10:46:12.664284 140444057306304 submission.py:265] 93) loss = 0.125, grad_norm = 0.052
I0316 10:46:14.648291 140404116662016 logging_writer.py:48] [94] global_step=94, grad_norm=0.0435678, loss=0.125752
I0316 10:46:14.651496 140444057306304 submission.py:265] 94) loss = 0.126, grad_norm = 0.044
I0316 10:46:15.636743 140404108269312 logging_writer.py:48] [95] global_step=95, grad_norm=0.0381772, loss=0.127645
I0316 10:46:15.639977 140444057306304 submission.py:265] 95) loss = 0.128, grad_norm = 0.038
I0316 10:46:17.451390 140404116662016 logging_writer.py:48] [96] global_step=96, grad_norm=0.0423598, loss=0.129043
I0316 10:46:17.454436 140444057306304 submission.py:265] 96) loss = 0.129, grad_norm = 0.042
I0316 10:46:17.900098 140404108269312 logging_writer.py:48] [97] global_step=97, grad_norm=0.0397319, loss=0.128852
I0316 10:46:17.903910 140444057306304 submission.py:265] 97) loss = 0.129, grad_norm = 0.040
I0316 10:46:20.045278 140404116662016 logging_writer.py:48] [98] global_step=98, grad_norm=0.0442245, loss=0.128902
I0316 10:46:20.048409 140444057306304 submission.py:265] 98) loss = 0.129, grad_norm = 0.044
I0316 10:46:21.543550 140404108269312 logging_writer.py:48] [99] global_step=99, grad_norm=0.0436851, loss=0.128086
I0316 10:46:21.546638 140444057306304 submission.py:265] 99) loss = 0.128, grad_norm = 0.044
I0316 10:46:22.647625 140404116662016 logging_writer.py:48] [100] global_step=100, grad_norm=0.0625604, loss=0.127751
I0316 10:46:22.650877 140444057306304 submission.py:265] 100) loss = 0.128, grad_norm = 0.063
I0316 10:46:49.318985 140444057306304 spec.py:321] Evaluating on the training split.
I0316 10:52:01.151132 140444057306304 spec.py:333] Evaluating on the validation split.
I0316 10:56:23.101537 140444057306304 spec.py:349] Evaluating on the test split.
I0316 11:01:22.021389 140444057306304 submission_runner.py:469] Time since start: 1943.00s, 	Step: 123, 	{'train/loss': 0.12923081854984755, 'validation/loss': 0.1291103002531186, 'validation/num_examples': 83274637, 'test/loss': 0.1317280954244915, 'test/num_examples': 95000000, 'score': 126.62877297401428, 'total_duration': 1943.0041732788086, 'accumulated_submission_time': 126.62877297401428, 'accumulated_eval_time': 1814.9217031002045, 'accumulated_logging_time': 0.14622998237609863}
I0316 11:01:22.031248 140404108269312 logging_writer.py:48] [123] accumulated_eval_time=1814.92, accumulated_logging_time=0.14623, accumulated_submission_time=126.629, global_step=123, preemption_count=0, score=126.629, test/loss=0.131728, test/num_examples=95000000, total_duration=1943, train/loss=0.129231, validation/loss=0.12911, validation/num_examples=83274637
/usr/local/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
I0316 11:03:23.418470 140444057306304 spec.py:321] Evaluating on the training split.
I0316 11:08:34.797140 140444057306304 spec.py:333] Evaluating on the validation split.
I0316 11:13:03.802572 140444057306304 spec.py:349] Evaluating on the test split.
I0316 11:18:05.772664 140444057306304 submission_runner.py:469] Time since start: 2946.76s, 	Step: 244, 	{'train/loss': 0.12817084404636053, 'validation/loss': 0.1281942983176408, 'validation/num_examples': 83274637, 'test/loss': 0.13094125858977468, 'test/num_examples': 95000000, 'score': 247.0798101425171, 'total_duration': 2946.7554297447205, 'accumulated_submission_time': 247.0798101425171, 'accumulated_eval_time': 2697.2760610580444, 'accumulated_logging_time': 0.16307497024536133}
I0316 11:18:05.782943 140404116662016 logging_writer.py:48] [244] accumulated_eval_time=2697.28, accumulated_logging_time=0.163075, accumulated_submission_time=247.08, global_step=244, preemption_count=0, score=247.08, test/loss=0.130941, test/num_examples=95000000, total_duration=2946.76, train/loss=0.128171, validation/loss=0.128194, validation/num_examples=83274637
I0316 11:20:07.917405 140444057306304 spec.py:321] Evaluating on the training split.
I0316 11:25:20.927079 140444057306304 spec.py:333] Evaluating on the validation split.
I0316 11:29:47.620734 140444057306304 spec.py:349] Evaluating on the test split.
I0316 11:34:44.934947 140444057306304 submission_runner.py:469] Time since start: 3945.92s, 	Step: 365, 	{'train/loss': 0.12562637123752537, 'validation/loss': 0.12756429306450323, 'validation/num_examples': 83274637, 'test/loss': 0.12987155524548982, 'test/num_examples': 95000000, 'score': 368.2872884273529, 'total_duration': 3945.9176907539368, 'accumulated_submission_time': 368.2872884273529, 'accumulated_eval_time': 3574.2936556339264, 'accumulated_logging_time': 0.18044543266296387}
I0316 11:34:44.944452 140404108269312 logging_writer.py:48] [365] accumulated_eval_time=3574.29, accumulated_logging_time=0.180445, accumulated_submission_time=368.287, global_step=365, preemption_count=0, score=368.287, test/loss=0.129872, test/num_examples=95000000, total_duration=3945.92, train/loss=0.125626, validation/loss=0.127564, validation/num_examples=83274637
I0316 11:36:46.554174 140444057306304 spec.py:321] Evaluating on the training split.
I0316 11:41:55.476129 140444057306304 spec.py:333] Evaluating on the validation split.
I0316 11:46:17.110096 140444057306304 spec.py:349] Evaluating on the test split.
I0316 11:51:13.186043 140444057306304 submission_runner.py:469] Time since start: 4934.17s, 	Step: 489, 	{'train/loss': 0.1256584244874236, 'validation/loss': 0.12678173934314615, 'validation/num_examples': 83274637, 'test/loss': 0.12906288906386526, 'test/num_examples': 95000000, 'score': 488.93984842300415, 'total_duration': 4934.168810606003, 'accumulated_submission_time': 488.93984842300415, 'accumulated_eval_time': 4440.925647974014, 'accumulated_logging_time': 0.2370309829711914}
I0316 11:51:13.195446 140404116662016 logging_writer.py:48] [489] accumulated_eval_time=4440.93, accumulated_logging_time=0.237031, accumulated_submission_time=488.94, global_step=489, preemption_count=0, score=488.94, test/loss=0.129063, test/num_examples=95000000, total_duration=4934.17, train/loss=0.125658, validation/loss=0.126782, validation/num_examples=83274637
I0316 11:51:15.965858 140404108269312 logging_writer.py:48] [500] global_step=500, grad_norm=0.0334317, loss=0.129353
I0316 11:51:15.969218 140444057306304 submission.py:265] 500) loss = 0.129, grad_norm = 0.033
I0316 11:53:13.693486 140444057306304 spec.py:321] Evaluating on the training split.
I0316 11:58:20.149028 140444057306304 spec.py:333] Evaluating on the validation split.
I0316 12:02:48.045250 140444057306304 spec.py:349] Evaluating on the test split.
I0316 12:07:46.311334 140444057306304 submission_runner.py:469] Time since start: 5927.29s, 	Step: 613, 	{'train/loss': 0.12787416851060612, 'validation/loss': 0.12742725706237717, 'validation/num_examples': 83274637, 'test/loss': 0.12978037315537302, 'test/num_examples': 95000000, 'score': 608.5112447738647, 'total_duration': 5927.294101715088, 'accumulated_submission_time': 608.5112447738647, 'accumulated_eval_time': 5313.543575286865, 'accumulated_logging_time': 0.25277161598205566}
I0316 12:07:46.321338 140404116662016 logging_writer.py:48] [613] accumulated_eval_time=5313.54, accumulated_logging_time=0.252772, accumulated_submission_time=608.511, global_step=613, preemption_count=0, score=608.511, test/loss=0.12978, test/num_examples=95000000, total_duration=5927.29, train/loss=0.127874, validation/loss=0.127427, validation/num_examples=83274637
I0316 12:09:46.857902 140444057306304 spec.py:321] Evaluating on the training split.
I0316 12:14:52.879072 140444057306304 spec.py:333] Evaluating on the validation split.
I0316 12:19:12.323523 140444057306304 spec.py:349] Evaluating on the test split.
I0316 12:24:14.200179 140444057306304 submission_runner.py:469] Time since start: 6915.18s, 	Step: 736, 	{'train/loss': 0.12637288425641383, 'validation/loss': 0.12620106502029166, 'validation/num_examples': 83274637, 'test/loss': 0.1287449956543772, 'test/num_examples': 95000000, 'score': 728.1306662559509, 'total_duration': 6915.182979345322, 'accumulated_submission_time': 728.1306662559509, 'accumulated_eval_time': 6180.886018753052, 'accumulated_logging_time': 0.2693045139312744}
I0316 12:24:14.210191 140404108269312 logging_writer.py:48] [736] accumulated_eval_time=6180.89, accumulated_logging_time=0.269305, accumulated_submission_time=728.131, global_step=736, preemption_count=0, score=728.131, test/loss=0.128745, test/num_examples=95000000, total_duration=6915.18, train/loss=0.126373, validation/loss=0.126201, validation/num_examples=83274637
I0316 12:26:15.345331 140444057306304 spec.py:321] Evaluating on the training split.
I0316 12:31:17.056740 140444057306304 spec.py:333] Evaluating on the validation split.
I0316 12:35:38.903974 140444057306304 spec.py:349] Evaluating on the test split.
I0316 12:40:32.662232 140444057306304 submission_runner.py:469] Time since start: 7893.65s, 	Step: 862, 	{'train/loss': 0.1255073766603816, 'validation/loss': 0.12635721303307307, 'validation/num_examples': 83274637, 'test/loss': 0.12872989029906423, 'test/num_examples': 95000000, 'score': 848.348484992981, 'total_duration': 7893.6450181007385, 'accumulated_submission_time': 848.348484992981, 'accumulated_eval_time': 7038.203093767166, 'accumulated_logging_time': 0.28646087646484375}
I0316 12:40:32.672011 140404116662016 logging_writer.py:48] [862] accumulated_eval_time=7038.2, accumulated_logging_time=0.286461, accumulated_submission_time=848.348, global_step=862, preemption_count=0, score=848.348, test/loss=0.12873, test/num_examples=95000000, total_duration=7893.65, train/loss=0.125507, validation/loss=0.126357, validation/num_examples=83274637
I0316 12:42:35.016730 140444057306304 spec.py:321] Evaluating on the training split.
I0316 12:47:47.248533 140444057306304 spec.py:333] Evaluating on the validation split.
I0316 12:52:06.139921 140444057306304 spec.py:349] Evaluating on the test split.
I0316 12:56:55.378654 140444057306304 submission_runner.py:469] Time since start: 8876.36s, 	Step: 986, 	{'train/loss': 0.12580540861204095, 'validation/loss': 0.12630198297997977, 'validation/num_examples': 83274637, 'test/loss': 0.12881356923233836, 'test/num_examples': 95000000, 'score': 969.7698495388031, 'total_duration': 8876.361378908157, 'accumulated_submission_time': 969.7698495388031, 'accumulated_eval_time': 7898.565136671066, 'accumulated_logging_time': 0.3030531406402588}
I0316 12:56:55.388405 140404108269312 logging_writer.py:48] [986] accumulated_eval_time=7898.57, accumulated_logging_time=0.303053, accumulated_submission_time=969.77, global_step=986, preemption_count=0, score=969.77, test/loss=0.128814, test/num_examples=95000000, total_duration=8876.36, train/loss=0.125805, validation/loss=0.126302, validation/num_examples=83274637
I0316 12:56:58.757399 140404116662016 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.0249206, loss=0.126702
I0316 12:56:58.760778 140444057306304 submission.py:265] 1000) loss = 0.127, grad_norm = 0.025
I0316 12:58:56.205390 140444057306304 spec.py:321] Evaluating on the training split.
I0316 13:03:50.131770 140444057306304 spec.py:333] Evaluating on the validation split.
I0316 13:08:06.265752 140444057306304 spec.py:349] Evaluating on the test split.
I0316 13:13:05.362794 140444057306304 submission_runner.py:469] Time since start: 9846.35s, 	Step: 1109, 	{'train/loss': 0.1253420001533199, 'validation/loss': 0.1262796894076021, 'validation/num_examples': 83274637, 'test/loss': 0.1286827234249316, 'test/num_examples': 95000000, 'score': 1089.6995077133179, 'total_duration': 9846.345588445663, 'accumulated_submission_time': 1089.6995077133179, 'accumulated_eval_time': 8747.722723960876, 'accumulated_logging_time': 0.31928491592407227}
I0316 13:13:05.372699 140404108269312 logging_writer.py:48] [1109] accumulated_eval_time=8747.72, accumulated_logging_time=0.319285, accumulated_submission_time=1089.7, global_step=1109, preemption_count=0, score=1089.7, test/loss=0.128683, test/num_examples=95000000, total_duration=9846.35, train/loss=0.125342, validation/loss=0.12628, validation/num_examples=83274637
I0316 13:15:07.117802 140444057306304 spec.py:321] Evaluating on the training split.
I0316 13:19:44.176728 140444057306304 spec.py:333] Evaluating on the validation split.
I0316 13:23:44.906203 140444057306304 spec.py:349] Evaluating on the test split.
I0316 13:28:40.616782 140444057306304 submission_runner.py:469] Time since start: 10781.60s, 	Step: 1232, 	{'train/loss': 0.1260237436293702, 'validation/loss': 0.12634931102962746, 'validation/num_examples': 83274637, 'test/loss': 0.1289215160079153, 'test/num_examples': 95000000, 'score': 1210.5205907821655, 'total_duration': 10781.599578619003, 'accumulated_submission_time': 1210.5205907821655, 'accumulated_eval_time': 9561.221871852875, 'accumulated_logging_time': 0.35659146308898926}
I0316 13:28:40.626931 140404116662016 logging_writer.py:48] [1232] accumulated_eval_time=9561.22, accumulated_logging_time=0.356591, accumulated_submission_time=1210.52, global_step=1232, preemption_count=0, score=1210.52, test/loss=0.128922, test/num_examples=95000000, total_duration=10781.6, train/loss=0.126024, validation/loss=0.126349, validation/num_examples=83274637
I0316 13:30:41.997488 140444057306304 spec.py:321] Evaluating on the training split.
I0316 13:34:52.644218 140444057306304 spec.py:333] Evaluating on the validation split.
I0316 13:38:37.323062 140444057306304 spec.py:349] Evaluating on the test split.
I0316 13:43:40.447991 140444057306304 submission_runner.py:469] Time since start: 11681.43s, 	Step: 1353, 	{'train/loss': 0.12375292274936046, 'validation/loss': 0.125648794771064, 'validation/num_examples': 83274637, 'test/loss': 0.12816813929523668, 'test/num_examples': 95000000, 'score': 1330.986073732376, 'total_duration': 11681.43078327179, 'accumulated_submission_time': 1330.986073732376, 'accumulated_eval_time': 10339.672608613968, 'accumulated_logging_time': 0.37366676330566406}
I0316 13:43:40.458256 140404108269312 logging_writer.py:48] [1353] accumulated_eval_time=10339.7, accumulated_logging_time=0.373667, accumulated_submission_time=1330.99, global_step=1353, preemption_count=0, score=1330.99, test/loss=0.128168, test/num_examples=95000000, total_duration=11681.4, train/loss=0.123753, validation/loss=0.125649, validation/num_examples=83274637
I0316 13:45:41.753573 140444057306304 spec.py:321] Evaluating on the training split.
I0316 13:48:44.905546 140444057306304 spec.py:333] Evaluating on the validation split.
I0316 13:51:39.237403 140444057306304 spec.py:349] Evaluating on the test split.
I0316 13:56:56.775028 140444057306304 submission_runner.py:469] Time since start: 12477.76s, 	Step: 1470, 	{'train/loss': 0.12413777748380056, 'validation/loss': 0.12555453776294476, 'validation/num_examples': 83274637, 'test/loss': 0.12792362726175407, 'test/num_examples': 95000000, 'score': 1451.39346575737, 'total_duration': 12477.757830619812, 'accumulated_submission_time': 1451.39346575737, 'accumulated_eval_time': 11014.694277048111, 'accumulated_logging_time': 0.390378475189209}
I0316 13:56:56.785187 140404116662016 logging_writer.py:48] [1470] accumulated_eval_time=11014.7, accumulated_logging_time=0.390378, accumulated_submission_time=1451.39, global_step=1470, preemption_count=0, score=1451.39, test/loss=0.127924, test/num_examples=95000000, total_duration=12477.8, train/loss=0.124138, validation/loss=0.125555, validation/num_examples=83274637
I0316 13:57:04.300214 140404108269312 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.0124587, loss=0.122554
I0316 13:57:04.303534 140444057306304 submission.py:265] 1500) loss = 0.123, grad_norm = 0.012
I0316 13:58:58.022735 140444057306304 spec.py:321] Evaluating on the training split.
I0316 14:01:03.131670 140444057306304 spec.py:333] Evaluating on the validation split.
I0316 14:03:08.064077 140444057306304 spec.py:349] Evaluating on the test split.
I0316 14:08:30.897795 140444057306304 submission_runner.py:469] Time since start: 13171.88s, 	Step: 1592, 	{'train/loss': 0.12496411507200217, 'validation/loss': 0.12553389932064704, 'validation/num_examples': 83274637, 'test/loss': 0.12775306454519975, 'test/num_examples': 95000000, 'score': 1571.725345134735, 'total_duration': 13171.880573272705, 'accumulated_submission_time': 1571.725345134735, 'accumulated_eval_time': 11587.56941151619, 'accumulated_logging_time': 0.4069802761077881}
I0316 14:08:30.908032 140404116662016 logging_writer.py:48] [1592] accumulated_eval_time=11587.6, accumulated_logging_time=0.40698, accumulated_submission_time=1571.73, global_step=1592, preemption_count=0, score=1571.73, test/loss=0.127753, test/num_examples=95000000, total_duration=13171.9, train/loss=0.124964, validation/loss=0.125534, validation/num_examples=83274637
I0316 14:10:31.812993 140444057306304 spec.py:321] Evaluating on the training split.
I0316 14:12:35.139141 140444057306304 spec.py:333] Evaluating on the validation split.
I0316 14:14:39.836739 140444057306304 spec.py:349] Evaluating on the test split.
I0316 14:19:51.695424 140444057306304 submission_runner.py:469] Time since start: 13852.68s, 	Step: 1716, 	{'train/loss': 0.12328752734388342, 'validation/loss': 0.1253680207386878, 'validation/num_examples': 83274637, 'test/loss': 0.12755045540779517, 'test/num_examples': 95000000, 'score': 1691.7474784851074, 'total_duration': 13852.678138494492, 'accumulated_submission_time': 1691.7474784851074, 'accumulated_eval_time': 12147.451938152313, 'accumulated_logging_time': 0.4239315986633301}
I0316 14:19:51.706799 140404108269312 logging_writer.py:48] [1716] accumulated_eval_time=12147.5, accumulated_logging_time=0.423932, accumulated_submission_time=1691.75, global_step=1716, preemption_count=0, score=1691.75, test/loss=0.12755, test/num_examples=95000000, total_duration=13852.7, train/loss=0.123288, validation/loss=0.125368, validation/num_examples=83274637
I0316 14:21:52.524994 140444057306304 spec.py:321] Evaluating on the training split.
I0316 14:23:56.101596 140444057306304 spec.py:333] Evaluating on the validation split.
I0316 14:26:01.014635 140444057306304 spec.py:349] Evaluating on the test split.
I0316 14:31:13.254610 140444057306304 submission_runner.py:469] Time since start: 14534.24s, 	Step: 1838, 	{'train/loss': 0.12550684896312594, 'validation/loss': 0.12594328151546166, 'validation/num_examples': 83274637, 'test/loss': 0.12844214171190763, 'test/num_examples': 95000000, 'score': 1811.6430382728577, 'total_duration': 14534.237362384796, 'accumulated_submission_time': 1811.6430382728577, 'accumulated_eval_time': 12708.181618452072, 'accumulated_logging_time': 0.44202494621276855}
I0316 14:31:13.265640 140404116662016 logging_writer.py:48] [1838] accumulated_eval_time=12708.2, accumulated_logging_time=0.442025, accumulated_submission_time=1811.64, global_step=1838, preemption_count=0, score=1811.64, test/loss=0.128442, test/num_examples=95000000, total_duration=14534.2, train/loss=0.125507, validation/loss=0.125943, validation/num_examples=83274637
I0316 14:33:14.275627 140444057306304 spec.py:321] Evaluating on the training split.
I0316 14:35:17.926654 140444057306304 spec.py:333] Evaluating on the validation split.
I0316 14:37:21.604330 140444057306304 spec.py:349] Evaluating on the test split.
I0316 14:42:38.943795 140444057306304 submission_runner.py:469] Time since start: 15219.93s, 	Step: 1957, 	{'train/loss': 0.12374469050343295, 'validation/loss': 0.12551733489232994, 'validation/num_examples': 83274637, 'test/loss': 0.12775662579646863, 'test/num_examples': 95000000, 'score': 1931.759015083313, 'total_duration': 15219.92656159401, 'accumulated_submission_time': 1931.759015083313, 'accumulated_eval_time': 13272.849855184555, 'accumulated_logging_time': 0.4603264331817627}
I0316 14:42:38.953921 140404108269312 logging_writer.py:48] [1957] accumulated_eval_time=13272.8, accumulated_logging_time=0.460326, accumulated_submission_time=1931.76, global_step=1957, preemption_count=0, score=1931.76, test/loss=0.127757, test/num_examples=95000000, total_duration=15219.9, train/loss=0.123745, validation/loss=0.125517, validation/num_examples=83274637
I0316 14:43:02.474715 140404116662016 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.0187765, loss=0.127892
I0316 14:43:02.477772 140444057306304 submission.py:265] 2000) loss = 0.128, grad_norm = 0.019
I0316 14:44:39.963735 140444057306304 spec.py:321] Evaluating on the training split.
I0316 14:46:44.072086 140444057306304 spec.py:333] Evaluating on the validation split.
I0316 14:48:49.234738 140444057306304 spec.py:349] Evaluating on the test split.
I0316 14:53:59.124315 140444057306304 submission_runner.py:469] Time since start: 15900.11s, 	Step: 2078, 	{'train/loss': 0.12543845503420054, 'validation/loss': 0.1253875596752646, 'validation/num_examples': 83274637, 'test/loss': 0.12767709091226678, 'test/num_examples': 95000000, 'score': 2051.85364818573, 'total_duration': 15900.107107400894, 'accumulated_submission_time': 2051.85364818573, 'accumulated_eval_time': 13832.010572195053, 'accumulated_logging_time': 0.47771286964416504}
I0316 14:53:59.134408 140404108269312 logging_writer.py:48] [2078] accumulated_eval_time=13832, accumulated_logging_time=0.477713, accumulated_submission_time=2051.85, global_step=2078, preemption_count=0, score=2051.85, test/loss=0.127677, test/num_examples=95000000, total_duration=15900.1, train/loss=0.125438, validation/loss=0.125388, validation/num_examples=83274637
I0316 14:55:59.872153 140444057306304 spec.py:321] Evaluating on the training split.
I0316 14:58:04.256614 140444057306304 spec.py:333] Evaluating on the validation split.
I0316 15:00:10.161610 140444057306304 spec.py:349] Evaluating on the test split.
I0316 15:05:15.808386 140444057306304 submission_runner.py:469] Time since start: 16576.79s, 	Step: 2197, 	{'train/loss': 0.1253953185708878, 'validation/loss': 0.1253845837808815, 'validation/num_examples': 83274637, 'test/loss': 0.12765813167467618, 'test/num_examples': 95000000, 'score': 2171.6773347854614, 'total_duration': 16576.79117822647, 'accumulated_submission_time': 2171.6773347854614, 'accumulated_eval_time': 14387.946912050247, 'accumulated_logging_time': 0.49455785751342773}
I0316 15:05:15.818546 140404116662016 logging_writer.py:48] [2197] accumulated_eval_time=14387.9, accumulated_logging_time=0.494558, accumulated_submission_time=2171.68, global_step=2197, preemption_count=0, score=2171.68, test/loss=0.127658, test/num_examples=95000000, total_duration=16576.8, train/loss=0.125395, validation/loss=0.125385, validation/num_examples=83274637
I0316 15:07:16.394340 140444057306304 spec.py:321] Evaluating on the training split.
I0316 15:09:19.860727 140444057306304 spec.py:333] Evaluating on the validation split.
I0316 15:11:24.557111 140444057306304 spec.py:349] Evaluating on the test split.
I0316 15:16:39.623426 140444057306304 submission_runner.py:469] Time since start: 17260.61s, 	Step: 2316, 	{'train/loss': 0.12360621181456106, 'validation/loss': 0.1250902342600912, 'validation/num_examples': 83274637, 'test/loss': 0.12751715908082661, 'test/num_examples': 95000000, 'score': 2291.3299129009247, 'total_duration': 17260.60618519783, 'accumulated_submission_time': 2291.3299129009247, 'accumulated_eval_time': 14951.176084041595, 'accumulated_logging_time': 0.5112204551696777}
I0316 15:16:39.634330 140404108269312 logging_writer.py:48] [2316] accumulated_eval_time=14951.2, accumulated_logging_time=0.51122, accumulated_submission_time=2291.33, global_step=2316, preemption_count=0, score=2291.33, test/loss=0.127517, test/num_examples=95000000, total_duration=17260.6, train/loss=0.123606, validation/loss=0.12509, validation/num_examples=83274637
I0316 15:18:40.940756 140444057306304 spec.py:321] Evaluating on the training split.
I0316 15:20:44.532147 140444057306304 spec.py:333] Evaluating on the validation split.
I0316 15:22:48.504242 140444057306304 spec.py:349] Evaluating on the test split.
I0316 15:27:58.774669 140444057306304 submission_runner.py:469] Time since start: 17939.76s, 	Step: 2441, 	{'train/loss': 0.12343402940989898, 'validation/loss': 0.12496312517448685, 'validation/num_examples': 83274637, 'test/loss': 0.127313794477523, 'test/num_examples': 95000000, 'score': 2411.7292551994324, 'total_duration': 17939.757436990738, 'accumulated_submission_time': 2411.7292551994324, 'accumulated_eval_time': 15509.010079145432, 'accumulated_logging_time': 0.529163122177124}
I0316 15:27:58.785554 140404116662016 logging_writer.py:48] [2441] accumulated_eval_time=15509, accumulated_logging_time=0.529163, accumulated_submission_time=2411.73, global_step=2441, preemption_count=0, score=2411.73, test/loss=0.127314, test/num_examples=95000000, total_duration=17939.8, train/loss=0.123434, validation/loss=0.124963, validation/num_examples=83274637
I0316 15:28:44.898036 140404108269312 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.0237049, loss=0.123093
I0316 15:28:44.901696 140444057306304 submission.py:265] 2500) loss = 0.123, grad_norm = 0.024
I0316 15:29:59.538034 140444057306304 spec.py:321] Evaluating on the training split.
I0316 15:32:03.169899 140444057306304 spec.py:333] Evaluating on the validation split.
I0316 15:34:07.997816 140444057306304 spec.py:349] Evaluating on the test split.
I0316 15:39:23.359415 140444057306304 submission_runner.py:469] Time since start: 18624.34s, 	Step: 2553, 	{'train/loss': 0.12498301337299032, 'validation/loss': 0.12517599720527284, 'validation/num_examples': 83274637, 'test/loss': 0.12745166521871468, 'test/num_examples': 95000000, 'score': 2531.5802681446075, 'total_duration': 18624.34220790863, 'accumulated_submission_time': 2531.5802681446075, 'accumulated_eval_time': 16072.831598758698, 'accumulated_logging_time': 0.5471501350402832}
I0316 15:39:23.369559 140404116662016 logging_writer.py:48] [2553] accumulated_eval_time=16072.8, accumulated_logging_time=0.54715, accumulated_submission_time=2531.58, global_step=2553, preemption_count=0, score=2531.58, test/loss=0.127452, test/num_examples=95000000, total_duration=18624.3, train/loss=0.124983, validation/loss=0.125176, validation/num_examples=83274637
I0316 15:41:24.601997 140444057306304 spec.py:321] Evaluating on the training split.
I0316 15:43:28.442367 140444057306304 spec.py:333] Evaluating on the validation split.
I0316 15:45:33.259017 140444057306304 spec.py:349] Evaluating on the test split.
I0316 15:50:45.589081 140444057306304 submission_runner.py:469] Time since start: 19306.57s, 	Step: 2677, 	{'train/loss': 0.12483020564511906, 'validation/loss': 0.12531357939069035, 'validation/num_examples': 83274637, 'test/loss': 0.1278282787944593, 'test/num_examples': 95000000, 'score': 2651.864834547043, 'total_duration': 19306.571850061417, 'accumulated_submission_time': 2651.864834547043, 'accumulated_eval_time': 16633.81876182556, 'accumulated_logging_time': 0.5744969844818115}
I0316 15:50:45.599835 140404108269312 logging_writer.py:48] [2677] accumulated_eval_time=16633.8, accumulated_logging_time=0.574497, accumulated_submission_time=2651.86, global_step=2677, preemption_count=0, score=2651.86, test/loss=0.127828, test/num_examples=95000000, total_duration=19306.6, train/loss=0.12483, validation/loss=0.125314, validation/num_examples=83274637
I0316 15:52:46.782210 140444057306304 spec.py:321] Evaluating on the training split.
I0316 15:54:50.473933 140444057306304 spec.py:333] Evaluating on the validation split.
I0316 15:56:55.195633 140444057306304 spec.py:349] Evaluating on the test split.
I0316 16:02:08.672933 140444057306304 submission_runner.py:469] Time since start: 19989.66s, 	Step: 2796, 	{'train/loss': 0.12297023407858032, 'validation/loss': 0.12497525951057911, 'validation/num_examples': 83274637, 'test/loss': 0.12736155712713945, 'test/num_examples': 95000000, 'score': 2772.1340565681458, 'total_duration': 19989.655723810196, 'accumulated_submission_time': 2772.1340565681458, 'accumulated_eval_time': 17195.709663152695, 'accumulated_logging_time': 0.5924375057220459}
I0316 16:02:08.683246 140404116662016 logging_writer.py:48] [2796] accumulated_eval_time=17195.7, accumulated_logging_time=0.592438, accumulated_submission_time=2772.13, global_step=2796, preemption_count=0, score=2772.13, test/loss=0.127362, test/num_examples=95000000, total_duration=19989.7, train/loss=0.12297, validation/loss=0.124975, validation/num_examples=83274637
I0316 16:04:09.315541 140444057306304 spec.py:321] Evaluating on the training split.
I0316 16:06:13.095809 140444057306304 spec.py:333] Evaluating on the validation split.
I0316 16:08:18.019233 140444057306304 spec.py:349] Evaluating on the test split.
I0316 16:13:28.704909 140444057306304 submission_runner.py:469] Time since start: 20669.69s, 	Step: 2919, 	{'train/loss': 0.125155661922053, 'validation/loss': 0.12492058564235933, 'validation/num_examples': 83274637, 'test/loss': 0.12729296633377074, 'test/num_examples': 95000000, 'score': 2891.8647694587708, 'total_duration': 20669.687702178955, 'accumulated_submission_time': 2891.8647694587708, 'accumulated_eval_time': 17755.099200487137, 'accumulated_logging_time': 0.6094915866851807}
I0316 16:13:28.714952 140404108269312 logging_writer.py:48] [2919] accumulated_eval_time=17755.1, accumulated_logging_time=0.609492, accumulated_submission_time=2891.86, global_step=2919, preemption_count=0, score=2891.86, test/loss=0.127293, test/num_examples=95000000, total_duration=20669.7, train/loss=0.125156, validation/loss=0.124921, validation/num_examples=83274637
I0316 16:14:37.253242 140404116662016 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.0306851, loss=0.127796
I0316 16:14:37.256806 140444057306304 submission.py:265] 3000) loss = 0.128, grad_norm = 0.031
I0316 16:15:30.715972 140444057306304 spec.py:321] Evaluating on the training split.
I0316 16:17:34.298017 140444057306304 spec.py:333] Evaluating on the validation split.
I0316 16:19:38.731748 140444057306304 spec.py:349] Evaluating on the test split.
I0316 16:24:52.871652 140444057306304 submission_runner.py:469] Time since start: 21353.85s, 	Step: 3044, 	{'train/loss': 0.12239372748261368, 'validation/loss': 0.12520737816925406, 'validation/num_examples': 83274637, 'test/loss': 0.12769722140968726, 'test/num_examples': 95000000, 'score': 3012.9528126716614, 'total_duration': 21353.854426383972, 'accumulated_submission_time': 3012.9528126716614, 'accumulated_eval_time': 18317.255046606064, 'accumulated_logging_time': 0.6261429786682129}
I0316 16:24:52.949800 140404108269312 logging_writer.py:48] [3044] accumulated_eval_time=18317.3, accumulated_logging_time=0.626143, accumulated_submission_time=3012.95, global_step=3044, preemption_count=0, score=3012.95, test/loss=0.127697, test/num_examples=95000000, total_duration=21353.9, train/loss=0.122394, validation/loss=0.125207, validation/num_examples=83274637
I0316 16:26:54.345382 140444057306304 spec.py:321] Evaluating on the training split.
I0316 16:28:57.906139 140444057306304 spec.py:333] Evaluating on the validation split.
I0316 16:31:03.031209 140444057306304 spec.py:349] Evaluating on the test split.
I0316 16:36:16.336369 140444057306304 submission_runner.py:469] Time since start: 22037.32s, 	Step: 3164, 	{'train/loss': 0.12165873570198396, 'validation/loss': 0.1251104070501242, 'validation/num_examples': 83274637, 'test/loss': 0.12766139032014545, 'test/num_examples': 95000000, 'score': 3133.448175430298, 'total_duration': 22037.319144010544, 'accumulated_submission_time': 3133.448175430298, 'accumulated_eval_time': 18879.246157884598, 'accumulated_logging_time': 0.7115600109100342}
I0316 16:36:16.376989 140404116662016 logging_writer.py:48] [3164] accumulated_eval_time=18879.2, accumulated_logging_time=0.71156, accumulated_submission_time=3133.45, global_step=3164, preemption_count=0, score=3133.45, test/loss=0.127661, test/num_examples=95000000, total_duration=22037.3, train/loss=0.121659, validation/loss=0.12511, validation/num_examples=83274637
I0316 16:38:17.768012 140444057306304 spec.py:321] Evaluating on the training split.
I0316 16:40:22.148310 140444057306304 spec.py:333] Evaluating on the validation split.
I0316 16:42:27.813569 140444057306304 spec.py:349] Evaluating on the test split.
I0316 16:47:37.885784 140444057306304 submission_runner.py:469] Time since start: 22718.87s, 	Step: 3286, 	{'train/loss': 0.12399945936411086, 'validation/loss': 0.12500377011387634, 'validation/num_examples': 83274637, 'test/loss': 0.1273210746655113, 'test/num_examples': 95000000, 'score': 3253.9057281017303, 'total_duration': 22718.868587493896, 'accumulated_submission_time': 3253.9057281017303, 'accumulated_eval_time': 19439.364085912704, 'accumulated_logging_time': 0.7750215530395508}
I0316 16:47:37.896279 140404108269312 logging_writer.py:48] [3286] accumulated_eval_time=19439.4, accumulated_logging_time=0.775022, accumulated_submission_time=3253.91, global_step=3286, preemption_count=0, score=3253.91, test/loss=0.127321, test/num_examples=95000000, total_duration=22718.9, train/loss=0.123999, validation/loss=0.125004, validation/num_examples=83274637
I0316 16:49:38.765789 140444057306304 spec.py:321] Evaluating on the training split.
I0316 16:51:42.319088 140444057306304 spec.py:333] Evaluating on the validation split.
I0316 16:53:47.252674 140444057306304 spec.py:349] Evaluating on the test split.
I0316 16:59:01.701558 140444057306304 submission_runner.py:469] Time since start: 23402.68s, 	Step: 3407, 	{'train/loss': 0.12367532372113904, 'validation/loss': 0.12496486381576577, 'validation/num_examples': 83274637, 'test/loss': 0.12737549117459748, 'test/num_examples': 95000000, 'score': 3373.9030833244324, 'total_duration': 23402.684302568436, 'accumulated_submission_time': 3373.9030833244324, 'accumulated_eval_time': 20002.29987668991, 'accumulated_logging_time': 0.7920582294464111}
I0316 16:59:01.711829 140404116662016 logging_writer.py:48] [3407] accumulated_eval_time=20002.3, accumulated_logging_time=0.792058, accumulated_submission_time=3373.9, global_step=3407, preemption_count=0, score=3373.9, test/loss=0.127375, test/num_examples=95000000, total_duration=23402.7, train/loss=0.123675, validation/loss=0.124965, validation/num_examples=83274637
I0316 17:00:29.799604 140404108269312 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.0262906, loss=0.127992
I0316 17:00:29.802851 140444057306304 submission.py:265] 3500) loss = 0.128, grad_norm = 0.026
I0316 17:01:02.441945 140444057306304 spec.py:321] Evaluating on the training split.
I0316 17:03:06.657676 140444057306304 spec.py:333] Evaluating on the validation split.
I0316 17:05:11.500786 140444057306304 spec.py:349] Evaluating on the test split.
I0316 17:10:17.195601 140444057306304 submission_runner.py:469] Time since start: 24078.18s, 	Step: 3528, 	{'train/loss': 0.12306457195711691, 'validation/loss': 0.12476544747447421, 'validation/num_examples': 83274637, 'test/loss': 0.12727453410186768, 'test/num_examples': 95000000, 'score': 3493.7158558368683, 'total_duration': 24078.178349018097, 'accumulated_submission_time': 3493.7158558368683, 'accumulated_eval_time': 20557.05362534523, 'accumulated_logging_time': 0.8090739250183105}
I0316 17:10:17.207292 140404116662016 logging_writer.py:48] [3528] accumulated_eval_time=20557.1, accumulated_logging_time=0.809074, accumulated_submission_time=3493.72, global_step=3528, preemption_count=0, score=3493.72, test/loss=0.127275, test/num_examples=95000000, total_duration=24078.2, train/loss=0.123065, validation/loss=0.124765, validation/num_examples=83274637
I0316 17:12:18.607972 140444057306304 spec.py:321] Evaluating on the training split.
I0316 17:14:22.241568 140444057306304 spec.py:333] Evaluating on the validation split.
I0316 17:16:25.865494 140444057306304 spec.py:349] Evaluating on the test split.
I0316 17:21:42.512789 140444057306304 submission_runner.py:469] Time since start: 24763.50s, 	Step: 3649, 	{'train/loss': 0.1223706443321906, 'validation/loss': 0.12476820883223388, 'validation/num_examples': 83274637, 'test/loss': 0.12720872703335412, 'test/num_examples': 95000000, 'score': 3614.21448469162, 'total_duration': 24763.495587825775, 'accumulated_submission_time': 3614.21448469162, 'accumulated_eval_time': 21120.958647489548, 'accumulated_logging_time': 0.8278121948242188}
I0316 17:21:42.524717 140404108269312 logging_writer.py:48] [3649] accumulated_eval_time=21121, accumulated_logging_time=0.827812, accumulated_submission_time=3614.21, global_step=3649, preemption_count=0, score=3614.21, test/loss=0.127209, test/num_examples=95000000, total_duration=24763.5, train/loss=0.122371, validation/loss=0.124768, validation/num_examples=83274637
I0316 17:23:43.048290 140444057306304 spec.py:321] Evaluating on the training split.
I0316 17:25:46.702704 140444057306304 spec.py:333] Evaluating on the validation split.
I0316 17:27:50.901833 140444057306304 spec.py:349] Evaluating on the test split.
I0316 17:33:07.373068 140444057306304 submission_runner.py:469] Time since start: 25448.36s, 	Step: 3770, 	{'train/loss': 0.12266818273881425, 'validation/loss': 0.12480642446135724, 'validation/num_examples': 83274637, 'test/loss': 0.12722648029616507, 'test/num_examples': 95000000, 'score': 3733.8236577510834, 'total_duration': 25448.355803251266, 'accumulated_submission_time': 3733.8236577510834, 'accumulated_eval_time': 21685.28342938423, 'accumulated_logging_time': 0.87888503074646}
I0316 17:33:07.384101 140404116662016 logging_writer.py:48] [3770] accumulated_eval_time=21685.3, accumulated_logging_time=0.878885, accumulated_submission_time=3733.82, global_step=3770, preemption_count=0, score=3733.82, test/loss=0.127226, test/num_examples=95000000, total_duration=25448.4, train/loss=0.122668, validation/loss=0.124806, validation/num_examples=83274637
I0316 17:35:08.710321 140444057306304 spec.py:321] Evaluating on the training split.
I0316 17:37:12.317730 140444057306304 spec.py:333] Evaluating on the validation split.
I0316 17:39:17.085688 140444057306304 spec.py:349] Evaluating on the test split.
I0316 17:44:36.867028 140444057306304 submission_runner.py:469] Time since start: 26137.85s, 	Step: 3891, 	{'train/loss': 0.12289639798758144, 'validation/loss': 0.1250347260006809, 'validation/num_examples': 83274637, 'test/loss': 0.1271782023138749, 'test/num_examples': 95000000, 'score': 3854.2747886180878, 'total_duration': 26137.849810123444, 'accumulated_submission_time': 3854.2747886180878, 'accumulated_eval_time': 22253.440311193466, 'accumulated_logging_time': 0.896308422088623}
I0316 17:44:36.878015 140404108269312 logging_writer.py:48] [3891] accumulated_eval_time=22253.4, accumulated_logging_time=0.896308, accumulated_submission_time=3854.27, global_step=3891, preemption_count=0, score=3854.27, test/loss=0.127178, test/num_examples=95000000, total_duration=26137.8, train/loss=0.122896, validation/loss=0.125035, validation/num_examples=83274637
I0316 17:46:24.961991 140404116662016 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.0414345, loss=0.129646
I0316 17:46:24.965484 140444057306304 submission.py:265] 4000) loss = 0.130, grad_norm = 0.041
I0316 17:46:37.543567 140444057306304 spec.py:321] Evaluating on the training split.
I0316 17:48:41.159388 140444057306304 spec.py:333] Evaluating on the validation split.
I0316 17:50:45.400003 140444057306304 spec.py:349] Evaluating on the test split.
I0316 17:55:56.914685 140444057306304 submission_runner.py:469] Time since start: 26817.90s, 	Step: 4010, 	{'train/loss': 0.12217565528266447, 'validation/loss': 0.12476903206842678, 'validation/num_examples': 83274637, 'test/loss': 0.12709808628238878, 'test/num_examples': 95000000, 'score': 3974.0467097759247, 'total_duration': 26817.89743757248, 'accumulated_submission_time': 3974.0467097759247, 'accumulated_eval_time': 22812.811592578888, 'accumulated_logging_time': 0.9138050079345703}
I0316 17:55:56.925284 140404108269312 logging_writer.py:48] [4010] accumulated_eval_time=22812.8, accumulated_logging_time=0.913805, accumulated_submission_time=3974.05, global_step=4010, preemption_count=0, score=3974.05, test/loss=0.127098, test/num_examples=95000000, total_duration=26817.9, train/loss=0.122176, validation/loss=0.124769, validation/num_examples=83274637
I0316 17:57:58.254208 140444057306304 spec.py:321] Evaluating on the training split.
I0316 18:00:01.742966 140444057306304 spec.py:333] Evaluating on the validation split.
I0316 18:02:06.544176 140444057306304 spec.py:349] Evaluating on the test split.
I0316 18:07:27.179202 140444057306304 submission_runner.py:469] Time since start: 27508.16s, 	Step: 4131, 	{'train/loss': 0.12429555061748007, 'validation/loss': 0.12479635725805069, 'validation/num_examples': 83274637, 'test/loss': 0.12725053483685944, 'test/num_examples': 95000000, 'score': 4094.4838206768036, 'total_duration': 27508.161992311478, 'accumulated_submission_time': 4094.4838206768036, 'accumulated_eval_time': 23381.736657857895, 'accumulated_logging_time': 0.9307541847229004}
I0316 18:07:27.189685 140404116662016 logging_writer.py:48] [4131] accumulated_eval_time=23381.7, accumulated_logging_time=0.930754, accumulated_submission_time=4094.48, global_step=4131, preemption_count=0, score=4094.48, test/loss=0.127251, test/num_examples=95000000, total_duration=27508.2, train/loss=0.124296, validation/loss=0.124796, validation/num_examples=83274637
I0316 18:09:27.980673 140444057306304 spec.py:321] Evaluating on the training split.
I0316 18:11:31.872197 140444057306304 spec.py:333] Evaluating on the validation split.
I0316 18:13:35.553212 140444057306304 spec.py:349] Evaluating on the test split.
I0316 18:18:46.105488 140444057306304 submission_runner.py:469] Time since start: 28187.09s, 	Step: 4255, 	{'train/loss': 0.12445416633168335, 'validation/loss': 0.12488542223771933, 'validation/num_examples': 83274637, 'test/loss': 0.1273586820873863, 'test/num_examples': 95000000, 'score': 4214.297745704651, 'total_duration': 28187.088273525238, 'accumulated_submission_time': 4214.297745704651, 'accumulated_eval_time': 23939.861600875854, 'accumulated_logging_time': 1.0069353580474854}
I0316 18:18:46.116008 140404108269312 logging_writer.py:48] [4255] accumulated_eval_time=23939.9, accumulated_logging_time=1.00694, accumulated_submission_time=4214.3, global_step=4255, preemption_count=0, score=4214.3, test/loss=0.127359, test/num_examples=95000000, total_duration=28187.1, train/loss=0.124454, validation/loss=0.124885, validation/num_examples=83274637
I0316 18:20:47.758537 140444057306304 spec.py:321] Evaluating on the training split.
I0316 18:22:51.122411 140444057306304 spec.py:333] Evaluating on the validation split.
I0316 18:24:55.040421 140444057306304 spec.py:349] Evaluating on the test split.
I0316 18:30:02.072239 140444057306304 submission_runner.py:469] Time since start: 28863.06s, 	Step: 4377, 	{'train/loss': 0.12200853159640533, 'validation/loss': 0.1248755649208645, 'validation/num_examples': 83274637, 'test/loss': 0.12733502325359145, 'test/num_examples': 95000000, 'score': 4335.038387537003, 'total_duration': 28863.055032014847, 'accumulated_submission_time': 4335.038387537003, 'accumulated_eval_time': 24494.175498962402, 'accumulated_logging_time': 1.0240271091461182}
I0316 18:30:02.083799 140404116662016 logging_writer.py:48] [4377] accumulated_eval_time=24494.2, accumulated_logging_time=1.02403, accumulated_submission_time=4335.04, global_step=4377, preemption_count=0, score=4335.04, test/loss=0.127335, test/num_examples=95000000, total_duration=28863.1, train/loss=0.122009, validation/loss=0.124876, validation/num_examples=83274637
I0316 18:32:02.794933 140444057306304 spec.py:321] Evaluating on the training split.
I0316 18:34:06.562991 140444057306304 spec.py:333] Evaluating on the validation split.
I0316 18:36:10.939307 140444057306304 spec.py:349] Evaluating on the test split.
I0316 18:41:21.900918 140444057306304 submission_runner.py:469] Time since start: 29542.88s, 	Step: 4495, 	{'train/loss': 0.12281809703355497, 'validation/loss': 0.12478628203365254, 'validation/num_examples': 83274637, 'test/loss': 0.12709862981097572, 'test/num_examples': 95000000, 'score': 4454.841799020767, 'total_duration': 29542.88371181488, 'accumulated_submission_time': 4454.841799020767, 'accumulated_eval_time': 25053.281759262085, 'accumulated_logging_time': 1.0424878597259521}
I0316 18:41:21.912019 140404108269312 logging_writer.py:48] [4495] accumulated_eval_time=25053.3, accumulated_logging_time=1.04249, accumulated_submission_time=4454.84, global_step=4495, preemption_count=0, score=4454.84, test/loss=0.127099, test/num_examples=95000000, total_duration=29542.9, train/loss=0.122818, validation/loss=0.124786, validation/num_examples=83274637
I0316 18:41:23.586812 140404116662016 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.0358339, loss=0.125355
I0316 18:41:23.590115 140444057306304 submission.py:265] 4500) loss = 0.125, grad_norm = 0.036
I0316 18:43:22.671840 140444057306304 spec.py:321] Evaluating on the training split.
I0316 18:45:26.348856 140444057306304 spec.py:333] Evaluating on the validation split.
I0316 18:47:29.780609 140444057306304 spec.py:349] Evaluating on the test split.
I0316 18:52:47.914771 140444057306304 submission_runner.py:469] Time since start: 30228.90s, 	Step: 4616, 	{'train/loss': 0.12513383997042857, 'validation/loss': 0.1242933268469449, 'validation/num_examples': 83274637, 'test/loss': 0.12653619451277884, 'test/num_examples': 95000000, 'score': 4574.722317457199, 'total_duration': 30228.89755177498, 'accumulated_submission_time': 4574.722317457199, 'accumulated_eval_time': 25618.52474474907, 'accumulated_logging_time': 1.0606696605682373}
I0316 18:52:47.925106 140404108269312 logging_writer.py:48] [4616] accumulated_eval_time=25618.5, accumulated_logging_time=1.06067, accumulated_submission_time=4574.72, global_step=4616, preemption_count=0, score=4574.72, test/loss=0.126536, test/num_examples=95000000, total_duration=30228.9, train/loss=0.125134, validation/loss=0.124293, validation/num_examples=83274637
I0316 18:54:48.897136 140444057306304 spec.py:321] Evaluating on the training split.
I0316 18:56:52.565345 140444057306304 spec.py:333] Evaluating on the validation split.
I0316 18:58:57.388723 140444057306304 spec.py:349] Evaluating on the test split.
I0316 19:04:14.718283 140444057306304 submission_runner.py:469] Time since start: 30915.70s, 	Step: 4739, 	{'train/loss': 0.1255573124931046, 'validation/loss': 0.12444055599079916, 'validation/num_examples': 83274637, 'test/loss': 0.12670724260193675, 'test/num_examples': 95000000, 'score': 4694.77729678154, 'total_duration': 30915.701027154922, 'accumulated_submission_time': 4694.77729678154, 'accumulated_eval_time': 26184.345997571945, 'accumulated_logging_time': 1.0779473781585693}
I0316 19:04:14.728921 140404116662016 logging_writer.py:48] [4739] accumulated_eval_time=26184.3, accumulated_logging_time=1.07795, accumulated_submission_time=4694.78, global_step=4739, preemption_count=0, score=4694.78, test/loss=0.126707, test/num_examples=95000000, total_duration=30915.7, train/loss=0.125557, validation/loss=0.124441, validation/num_examples=83274637
I0316 19:06:16.751024 140444057306304 spec.py:321] Evaluating on the training split.
I0316 19:08:20.630080 140444057306304 spec.py:333] Evaluating on the validation split.
I0316 19:10:25.508642 140444057306304 spec.py:349] Evaluating on the test split.
I0316 19:15:38.418676 140444057306304 submission_runner.py:469] Time since start: 31599.40s, 	Step: 4860, 	{'train/loss': 0.12298905418369682, 'validation/loss': 0.12452522304964576, 'validation/num_examples': 83274637, 'test/loss': 0.12682551859291477, 'test/num_examples': 95000000, 'score': 4815.899124383926, 'total_duration': 31599.40146780014, 'accumulated_submission_time': 4815.899124383926, 'accumulated_eval_time': 26746.013878583908, 'accumulated_logging_time': 1.1184883117675781}
I0316 19:15:38.430123 140404108269312 logging_writer.py:48] [4860] accumulated_eval_time=26746, accumulated_logging_time=1.11849, accumulated_submission_time=4815.9, global_step=4860, preemption_count=0, score=4815.9, test/loss=0.126826, test/num_examples=95000000, total_duration=31599.4, train/loss=0.122989, validation/loss=0.124525, validation/num_examples=83274637
I0316 19:17:39.586607 140444057306304 spec.py:321] Evaluating on the training split.
I0316 19:19:43.512835 140444057306304 spec.py:333] Evaluating on the validation split.
I0316 19:21:48.825164 140444057306304 spec.py:349] Evaluating on the test split.
I0316 19:26:58.439690 140444057306304 submission_runner.py:469] Time since start: 32279.42s, 	Step: 4984, 	{'train/loss': 0.12510103116910548, 'validation/loss': 0.12449226386012505, 'validation/num_examples': 83274637, 'test/loss': 0.12684085247465435, 'test/num_examples': 95000000, 'score': 4936.159632444382, 'total_duration': 32279.422426462173, 'accumulated_submission_time': 4936.159632444382, 'accumulated_eval_time': 27304.867115020752, 'accumulated_logging_time': 1.1368942260742188}
I0316 19:26:58.450658 140404116662016 logging_writer.py:48] [4984] accumulated_eval_time=27304.9, accumulated_logging_time=1.13689, accumulated_submission_time=4936.16, global_step=4984, preemption_count=0, score=4936.16, test/loss=0.126841, test/num_examples=95000000, total_duration=32279.4, train/loss=0.125101, validation/loss=0.124492, validation/num_examples=83274637
I0316 19:27:02.173861 140404108269312 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.00625152, loss=0.132214
I0316 19:27:02.177470 140444057306304 submission.py:265] 5000) loss = 0.132, grad_norm = 0.006
I0316 19:28:59.153004 140444057306304 spec.py:321] Evaluating on the training split.
I0316 19:31:02.778777 140444057306304 spec.py:333] Evaluating on the validation split.
I0316 19:33:07.700872 140444057306304 spec.py:349] Evaluating on the test split.
I0316 19:38:21.120004 140444057306304 submission_runner.py:469] Time since start: 32962.10s, 	Step: 5108, 	{'train/loss': 0.12338032447225457, 'validation/loss': 0.12451821010678726, 'validation/num_examples': 83274637, 'test/loss': 0.12700690218927485, 'test/num_examples': 95000000, 'score': 5055.941500663757, 'total_duration': 32962.102763175964, 'accumulated_submission_time': 5055.941500663757, 'accumulated_eval_time': 27866.834205150604, 'accumulated_logging_time': 1.1544480323791504}
I0316 19:38:21.131321 140404116662016 logging_writer.py:48] [5108] accumulated_eval_time=27866.8, accumulated_logging_time=1.15445, accumulated_submission_time=5055.94, global_step=5108, preemption_count=0, score=5055.94, test/loss=0.127007, test/num_examples=95000000, total_duration=32962.1, train/loss=0.12338, validation/loss=0.124518, validation/num_examples=83274637
I0316 19:40:21.853901 140444057306304 spec.py:321] Evaluating on the training split.
I0316 19:42:25.314420 140444057306304 spec.py:333] Evaluating on the validation split.
I0316 19:44:29.491534 140444057306304 spec.py:349] Evaluating on the test split.
I0316 19:49:41.956254 140444057306304 submission_runner.py:469] Time since start: 33642.94s, 	Step: 5225, 	{'train/loss': 0.12225708428812629, 'validation/loss': 0.12429744280642469, 'validation/num_examples': 83274637, 'test/loss': 0.12658395074591386, 'test/num_examples': 95000000, 'score': 5175.772595405579, 'total_duration': 33642.93902516365, 'accumulated_submission_time': 5175.772595405579, 'accumulated_eval_time': 28426.93671655655, 'accumulated_logging_time': 1.173121690750122}
I0316 19:49:41.966947 140404108269312 logging_writer.py:48] [5225] accumulated_eval_time=28426.9, accumulated_logging_time=1.17312, accumulated_submission_time=5175.77, global_step=5225, preemption_count=0, score=5175.77, test/loss=0.126584, test/num_examples=95000000, total_duration=33642.9, train/loss=0.122257, validation/loss=0.124297, validation/num_examples=83274637
I0316 19:51:43.096879 140444057306304 spec.py:321] Evaluating on the training split.
I0316 19:53:46.727500 140444057306304 spec.py:333] Evaluating on the validation split.
I0316 19:55:51.618987 140444057306304 spec.py:349] Evaluating on the test split.
I0316 20:00:59.978850 140444057306304 submission_runner.py:469] Time since start: 34320.96s, 	Step: 5348, 	{'train/loss': 0.12177003808142758, 'validation/loss': 0.12441422908969868, 'validation/num_examples': 83274637, 'test/loss': 0.12667920865309865, 'test/num_examples': 95000000, 'score': 5295.953923940659, 'total_duration': 34320.96162438393, 'accumulated_submission_time': 5295.953923940659, 'accumulated_eval_time': 28983.818868398666, 'accumulated_logging_time': 1.219975233078003}
I0316 20:00:59.989353 140404116662016 logging_writer.py:48] [5348] accumulated_eval_time=28983.8, accumulated_logging_time=1.21998, accumulated_submission_time=5295.95, global_step=5348, preemption_count=0, score=5295.95, test/loss=0.126679, test/num_examples=95000000, total_duration=34321, train/loss=0.12177, validation/loss=0.124414, validation/num_examples=83274637
I0316 20:03:01.216115 140444057306304 spec.py:321] Evaluating on the training split.
I0316 20:05:04.628927 140444057306304 spec.py:333] Evaluating on the validation split.
I0316 20:07:09.352334 140444057306304 spec.py:349] Evaluating on the test split.
I0316 20:12:29.599900 140444057306304 submission_runner.py:469] Time since start: 35010.58s, 	Step: 5468, 	{'train/loss': 0.123059707792238, 'validation/loss': 0.12436330404556413, 'validation/num_examples': 83274637, 'test/loss': 0.12666183545765125, 'test/num_examples': 95000000, 'score': 5416.282939195633, 'total_duration': 35010.58262181282, 'accumulated_submission_time': 5416.282939195633, 'accumulated_eval_time': 29552.202750205994, 'accumulated_logging_time': 1.2372703552246094}
I0316 20:12:29.611922 140404108269312 logging_writer.py:48] [5468] accumulated_eval_time=29552.2, accumulated_logging_time=1.23727, accumulated_submission_time=5416.28, global_step=5468, preemption_count=0, score=5416.28, test/loss=0.126662, test/num_examples=95000000, total_duration=35010.6, train/loss=0.12306, validation/loss=0.124363, validation/num_examples=83274637
I0316 20:12:39.131651 140404116662016 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.0139205, loss=0.124226
I0316 20:12:39.135146 140444057306304 submission.py:265] 5500) loss = 0.124, grad_norm = 0.014
I0316 20:14:31.031913 140444057306304 spec.py:321] Evaluating on the training split.
I0316 20:16:35.084161 140444057306304 spec.py:333] Evaluating on the validation split.
I0316 20:18:39.168329 140444057306304 spec.py:349] Evaluating on the test split.
I0316 20:24:02.253238 140444057306304 submission_runner.py:469] Time since start: 35703.24s, 	Step: 5592, 	{'train/loss': 0.12379021071339696, 'validation/loss': 0.12451265965171292, 'validation/num_examples': 83274637, 'test/loss': 0.12695651925024734, 'test/num_examples': 95000000, 'score': 5536.830785274506, 'total_duration': 35703.236033678055, 'accumulated_submission_time': 5536.830785274506, 'accumulated_eval_time': 30123.424327611923, 'accumulated_logging_time': 1.2563667297363281}
I0316 20:24:02.294730 140404108269312 logging_writer.py:48] [5592] accumulated_eval_time=30123.4, accumulated_logging_time=1.25637, accumulated_submission_time=5536.83, global_step=5592, preemption_count=0, score=5536.83, test/loss=0.126957, test/num_examples=95000000, total_duration=35703.2, train/loss=0.12379, validation/loss=0.124513, validation/num_examples=83274637
I0316 20:26:02.875625 140444057306304 spec.py:321] Evaluating on the training split.
I0316 20:28:06.426536 140444057306304 spec.py:333] Evaluating on the validation split.
I0316 20:30:10.742434 140444057306304 spec.py:349] Evaluating on the test split.
I0316 20:35:25.431310 140444057306304 submission_runner.py:469] Time since start: 36386.41s, 	Step: 5710, 	{'train/loss': 0.12495810021401718, 'validation/loss': 0.1240392240992443, 'validation/num_examples': 83274637, 'test/loss': 0.1263329262197394, 'test/num_examples': 95000000, 'score': 5656.58384180069, 'total_duration': 36386.41411447525, 'accumulated_submission_time': 5656.58384180069, 'accumulated_eval_time': 30685.980119228363, 'accumulated_logging_time': 1.304927110671997}
I0316 20:35:25.442947 140404116662016 logging_writer.py:48] [5710] accumulated_eval_time=30686, accumulated_logging_time=1.30493, accumulated_submission_time=5656.58, global_step=5710, preemption_count=0, score=5656.58, test/loss=0.126333, test/num_examples=95000000, total_duration=36386.4, train/loss=0.124958, validation/loss=0.124039, validation/num_examples=83274637
I0316 20:37:26.704652 140444057306304 spec.py:321] Evaluating on the training split.
I0316 20:39:30.862923 140444057306304 spec.py:333] Evaluating on the validation split.
I0316 20:41:36.123205 140444057306304 spec.py:349] Evaluating on the test split.
I0316 20:46:56.268463 140444057306304 submission_runner.py:469] Time since start: 37077.25s, 	Step: 5835, 	{'train/loss': 0.12248819790188367, 'validation/loss': 0.12417376122244979, 'validation/num_examples': 83274637, 'test/loss': 0.12654556902799105, 'test/num_examples': 95000000, 'score': 5776.847810506821, 'total_duration': 37077.25123715401, 'accumulated_submission_time': 5776.847810506821, 'accumulated_eval_time': 31255.544065237045, 'accumulated_logging_time': 1.4017579555511475}
I0316 20:46:56.279698 140404108269312 logging_writer.py:48] [5835] accumulated_eval_time=31255.5, accumulated_logging_time=1.40176, accumulated_submission_time=5776.85, global_step=5835, preemption_count=0, score=5776.85, test/loss=0.126546, test/num_examples=95000000, total_duration=37077.3, train/loss=0.122488, validation/loss=0.124174, validation/num_examples=83274637
I0316 20:48:56.761649 140444057306304 spec.py:321] Evaluating on the training split.
I0316 20:51:00.247614 140444057306304 spec.py:333] Evaluating on the validation split.
I0316 20:53:04.981602 140444057306304 spec.py:349] Evaluating on the test split.
I0316 20:58:21.035046 140444057306304 submission_runner.py:469] Time since start: 37762.02s, 	Step: 5960, 	{'train/loss': 0.12173180879175607, 'validation/loss': 0.12420267849889319, 'validation/num_examples': 83274637, 'test/loss': 0.12660802479906585, 'test/num_examples': 95000000, 'score': 5896.479827642441, 'total_duration': 37762.017844200134, 'accumulated_submission_time': 5896.479827642441, 'accumulated_eval_time': 31819.81757044792, 'accumulated_logging_time': 1.4195375442504883}
I0316 20:58:21.046295 140404116662016 logging_writer.py:48] [5960] accumulated_eval_time=31819.8, accumulated_logging_time=1.41954, accumulated_submission_time=5896.48, global_step=5960, preemption_count=0, score=5896.48, test/loss=0.126608, test/num_examples=95000000, total_duration=37762, train/loss=0.121732, validation/loss=0.124203, validation/num_examples=83274637
I0316 20:58:42.350580 140404108269312 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.042823, loss=0.120734
I0316 20:58:42.353598 140444057306304 submission.py:265] 6000) loss = 0.121, grad_norm = 0.043
I0316 21:00:23.096095 140444057306304 spec.py:321] Evaluating on the training split.
I0316 21:02:26.637055 140444057306304 spec.py:333] Evaluating on the validation split.
I0316 21:04:30.148957 140444057306304 spec.py:349] Evaluating on the test split.
I0316 21:09:45.464578 140444057306304 submission_runner.py:469] Time since start: 38446.45s, 	Step: 6080, 	{'train/loss': 0.12346099074339975, 'validation/loss': 0.12420211540722309, 'validation/num_examples': 83274637, 'test/loss': 0.12658449985046386, 'test/num_examples': 95000000, 'score': 6017.647786855698, 'total_duration': 38446.447353839874, 'accumulated_submission_time': 6017.647786855698, 'accumulated_eval_time': 32382.186200618744, 'accumulated_logging_time': 1.4376697540283203}
I0316 21:09:45.476238 140404116662016 logging_writer.py:48] [6080] accumulated_eval_time=32382.2, accumulated_logging_time=1.43767, accumulated_submission_time=6017.65, global_step=6080, preemption_count=0, score=6017.65, test/loss=0.126584, test/num_examples=95000000, total_duration=38446.4, train/loss=0.123461, validation/loss=0.124202, validation/num_examples=83274637
I0316 21:11:46.828125 140444057306304 spec.py:321] Evaluating on the training split.
I0316 21:13:50.351728 140444057306304 spec.py:333] Evaluating on the validation split.
I0316 21:15:54.433827 140444057306304 spec.py:349] Evaluating on the test split.
I0316 21:21:09.617964 140444057306304 submission_runner.py:469] Time since start: 39130.60s, 	Step: 6203, 	{'train/loss': 0.12221063183970161, 'validation/loss': 0.12480169058318324, 'validation/num_examples': 83274637, 'test/loss': 0.1268822314521388, 'test/num_examples': 95000000, 'score': 6138.099652528763, 'total_duration': 39130.60075163841, 'accumulated_submission_time': 6138.099652528763, 'accumulated_eval_time': 32944.97610402107, 'accumulated_logging_time': 1.4569640159606934}
I0316 21:21:09.630690 140404108269312 logging_writer.py:48] [6203] accumulated_eval_time=32945, accumulated_logging_time=1.45696, accumulated_submission_time=6138.1, global_step=6203, preemption_count=0, score=6138.1, test/loss=0.126882, test/num_examples=95000000, total_duration=39130.6, train/loss=0.122211, validation/loss=0.124802, validation/num_examples=83274637
I0316 21:23:10.419891 140444057306304 spec.py:321] Evaluating on the training split.
I0316 21:25:13.993015 140444057306304 spec.py:333] Evaluating on the validation split.
I0316 21:27:19.005504 140444057306304 spec.py:349] Evaluating on the test split.
I0316 21:32:30.455948 140444057306304 submission_runner.py:469] Time since start: 39811.44s, 	Step: 6322, 	{'train/loss': 0.12339407974985482, 'validation/loss': 0.12410415617036888, 'validation/num_examples': 83274637, 'test/loss': 0.12640449687941702, 'test/num_examples': 95000000, 'score': 6257.9860672950745, 'total_duration': 39811.43871998787, 'accumulated_submission_time': 6257.9860672950745, 'accumulated_eval_time': 33505.01225686073, 'accumulated_logging_time': 1.4767816066741943}
I0316 21:32:30.466964 140404116662016 logging_writer.py:48] [6322] accumulated_eval_time=33505, accumulated_logging_time=1.47678, accumulated_submission_time=6257.99, global_step=6322, preemption_count=0, score=6257.99, test/loss=0.126404, test/num_examples=95000000, total_duration=39811.4, train/loss=0.123394, validation/loss=0.124104, validation/num_examples=83274637
I0316 21:34:31.168269 140444057306304 spec.py:321] Evaluating on the training split.
I0316 21:36:34.662316 140444057306304 spec.py:333] Evaluating on the validation split.
I0316 21:38:39.487871 140444057306304 spec.py:349] Evaluating on the test split.
I0316 21:43:56.197397 140444057306304 submission_runner.py:469] Time since start: 40497.18s, 	Step: 6440, 	{'train/loss': 0.12389660129478791, 'validation/loss': 0.12433307144743286, 'validation/num_examples': 83274637, 'test/loss': 0.126712058998911, 'test/num_examples': 95000000, 'score': 6377.728576660156, 'total_duration': 40497.18017268181, 'accumulated_submission_time': 6377.728576660156, 'accumulated_eval_time': 34070.041502952576, 'accumulated_logging_time': 1.5355265140533447}
I0316 21:43:56.208735 140404108269312 logging_writer.py:48] [6440] accumulated_eval_time=34070, accumulated_logging_time=1.53553, accumulated_submission_time=6377.73, global_step=6440, preemption_count=0, score=6377.73, test/loss=0.126712, test/num_examples=95000000, total_duration=40497.2, train/loss=0.123897, validation/loss=0.124333, validation/num_examples=83274637
I0316 21:44:41.884044 140404116662016 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.00752845, loss=0.126023
I0316 21:44:41.887837 140444057306304 submission.py:265] 6500) loss = 0.126, grad_norm = 0.008
I0316 21:45:57.317139 140444057306304 spec.py:321] Evaluating on the training split.
I0316 21:48:01.082403 140444057306304 spec.py:333] Evaluating on the validation split.
I0316 21:50:06.047100 140444057306304 spec.py:349] Evaluating on the test split.
I0316 21:55:15.891702 140444057306304 submission_runner.py:469] Time since start: 41176.87s, 	Step: 6561, 	{'train/loss': 0.12258541599167436, 'validation/loss': 0.12418120044638162, 'validation/num_examples': 83274637, 'test/loss': 0.1264944525982907, 'test/num_examples': 95000000, 'score': 6497.907428741455, 'total_duration': 41176.87443614006, 'accumulated_submission_time': 6497.907428741455, 'accumulated_eval_time': 34628.61609888077, 'accumulated_logging_time': 1.5543944835662842}
I0316 21:55:15.904008 140404108269312 logging_writer.py:48] [6561] accumulated_eval_time=34628.6, accumulated_logging_time=1.55439, accumulated_submission_time=6497.91, global_step=6561, preemption_count=0, score=6497.91, test/loss=0.126494, test/num_examples=95000000, total_duration=41176.9, train/loss=0.122585, validation/loss=0.124181, validation/num_examples=83274637
I0316 21:57:16.742417 140444057306304 spec.py:321] Evaluating on the training split.
I0316 21:59:20.246665 140444057306304 spec.py:333] Evaluating on the validation split.
I0316 22:01:23.494262 140444057306304 spec.py:349] Evaluating on the test split.
I0316 22:06:44.145972 140444057306304 submission_runner.py:469] Time since start: 41865.13s, 	Step: 6686, 	{'train/loss': 0.12299517381890689, 'validation/loss': 0.12419014631226001, 'validation/num_examples': 83274637, 'test/loss': 0.12661495183780067, 'test/num_examples': 95000000, 'score': 6617.82582616806, 'total_duration': 41865.12874507904, 'accumulated_submission_time': 6617.82582616806, 'accumulated_eval_time': 35196.01975297928, 'accumulated_logging_time': 1.573943853378296}
I0316 22:06:44.159288 140404116662016 logging_writer.py:48] [6686] accumulated_eval_time=35196, accumulated_logging_time=1.57394, accumulated_submission_time=6617.83, global_step=6686, preemption_count=0, score=6617.83, test/loss=0.126615, test/num_examples=95000000, total_duration=41865.1, train/loss=0.122995, validation/loss=0.12419, validation/num_examples=83274637
I0316 22:08:45.562843 140444057306304 spec.py:321] Evaluating on the training split.
I0316 22:10:49.382701 140444057306304 spec.py:333] Evaluating on the validation split.
I0316 22:12:54.160160 140444057306304 spec.py:349] Evaluating on the test split.
I0316 22:18:07.821573 140444057306304 submission_runner.py:469] Time since start: 42548.80s, 	Step: 6807, 	{'train/loss': 0.11998009616813231, 'validation/loss': 0.12404246017713338, 'validation/num_examples': 83274637, 'test/loss': 0.1263536411002711, 'test/num_examples': 95000000, 'score': 6738.3446452617645, 'total_duration': 42548.80435204506, 'accumulated_submission_time': 6738.3446452617645, 'accumulated_eval_time': 35758.278670072556, 'accumulated_logging_time': 1.594161033630371}
I0316 22:18:07.833867 140404108269312 logging_writer.py:48] [6807] accumulated_eval_time=35758.3, accumulated_logging_time=1.59416, accumulated_submission_time=6738.34, global_step=6807, preemption_count=0, score=6738.34, test/loss=0.126354, test/num_examples=95000000, total_duration=42548.8, train/loss=0.11998, validation/loss=0.124042, validation/num_examples=83274637
I0316 22:20:08.413371 140444057306304 spec.py:321] Evaluating on the training split.
I0316 22:22:11.992184 140444057306304 spec.py:333] Evaluating on the validation split.
I0316 22:24:16.840048 140444057306304 spec.py:349] Evaluating on the test split.
I0316 22:29:30.327585 140444057306304 submission_runner.py:469] Time since start: 43231.31s, 	Step: 6928, 	{'train/loss': 0.12219525845444697, 'validation/loss': 0.12404092045851384, 'validation/num_examples': 83274637, 'test/loss': 0.1263211358651412, 'test/num_examples': 95000000, 'score': 6858.057327032089, 'total_duration': 43231.31038093567, 'accumulated_submission_time': 6858.057327032089, 'accumulated_eval_time': 36320.19295883179, 'accumulated_logging_time': 1.6363492012023926}
I0316 22:29:30.340234 140404116662016 logging_writer.py:48] [6928] accumulated_eval_time=36320.2, accumulated_logging_time=1.63635, accumulated_submission_time=6858.06, global_step=6928, preemption_count=0, score=6858.06, test/loss=0.126321, test/num_examples=95000000, total_duration=43231.3, train/loss=0.122195, validation/loss=0.124041, validation/num_examples=83274637
I0316 22:30:31.565876 140404108269312 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.015645, loss=0.13301
I0316 22:30:31.569746 140444057306304 submission.py:265] 7000) loss = 0.133, grad_norm = 0.016
I0316 22:31:30.984901 140444057306304 spec.py:321] Evaluating on the training split.
I0316 22:33:34.472771 140444057306304 spec.py:333] Evaluating on the validation split.
I0316 22:35:39.544370 140444057306304 spec.py:349] Evaluating on the test split.
I0316 22:40:56.889995 140444057306304 submission_runner.py:469] Time since start: 43917.87s, 	Step: 7047, 	{'train/loss': 0.12271226525734344, 'validation/loss': 0.12403221554256239, 'validation/num_examples': 83274637, 'test/loss': 0.12638565355007775, 'test/num_examples': 95000000, 'score': 6977.8189969062805, 'total_duration': 43917.87267661095, 'accumulated_submission_time': 6977.8189969062805, 'accumulated_eval_time': 36886.09798741341, 'accumulated_logging_time': 1.6560561656951904}
I0316 22:40:56.902664 140404116662016 logging_writer.py:48] [7047] accumulated_eval_time=36886.1, accumulated_logging_time=1.65606, accumulated_submission_time=6977.82, global_step=7047, preemption_count=0, score=6977.82, test/loss=0.126386, test/num_examples=95000000, total_duration=43917.9, train/loss=0.122712, validation/loss=0.124032, validation/num_examples=83274637
I0316 22:42:57.456579 140444057306304 spec.py:321] Evaluating on the training split.
I0316 22:45:01.164477 140444057306304 spec.py:333] Evaluating on the validation split.
I0316 22:47:05.064320 140444057306304 spec.py:349] Evaluating on the test split.
I0316 22:52:25.955386 140444057306304 submission_runner.py:469] Time since start: 44606.94s, 	Step: 7169, 	{'train/loss': 0.12223372947953798, 'validation/loss': 0.1240772743536605, 'validation/num_examples': 83274637, 'test/loss': 0.1263718737794575, 'test/num_examples': 95000000, 'score': 7097.526839256287, 'total_duration': 44606.9381415844, 'accumulated_submission_time': 7097.526839256287, 'accumulated_eval_time': 37454.59686207771, 'accumulated_logging_time': 1.675302267074585}
I0316 22:52:25.967991 140404108269312 logging_writer.py:48] [7169] accumulated_eval_time=37454.6, accumulated_logging_time=1.6753, accumulated_submission_time=7097.53, global_step=7169, preemption_count=0, score=7097.53, test/loss=0.126372, test/num_examples=95000000, total_duration=44606.9, train/loss=0.122234, validation/loss=0.124077, validation/num_examples=83274637
I0316 22:54:26.469677 140444057306304 spec.py:321] Evaluating on the training split.
I0316 22:56:30.103833 140444057306304 spec.py:333] Evaluating on the validation split.
I0316 22:58:33.962143 140444057306304 spec.py:349] Evaluating on the test split.
I0316 23:03:40.065571 140444057306304 submission_runner.py:469] Time since start: 45281.05s, 	Step: 7293, 	{'train/loss': 0.1233360518249913, 'validation/loss': 0.12396037555815444, 'validation/num_examples': 83274637, 'test/loss': 0.12618515208941009, 'test/num_examples': 95000000, 'score': 7217.156074523926, 'total_duration': 45281.048209905624, 'accumulated_submission_time': 7217.156074523926, 'accumulated_eval_time': 38008.19267439842, 'accumulated_logging_time': 1.6947665214538574}
I0316 23:03:40.078189 140404116662016 logging_writer.py:48] [7293] accumulated_eval_time=38008.2, accumulated_logging_time=1.69477, accumulated_submission_time=7217.16, global_step=7293, preemption_count=0, score=7217.16, test/loss=0.126185, test/num_examples=95000000, total_duration=45281, train/loss=0.123336, validation/loss=0.12396, validation/num_examples=83274637
I0316 23:05:40.665658 140444057306304 spec.py:321] Evaluating on the training split.
I0316 23:07:44.975732 140444057306304 spec.py:333] Evaluating on the validation split.
I0316 23:09:49.469604 140444057306304 spec.py:349] Evaluating on the test split.
I0316 23:15:03.275969 140444057306304 submission_runner.py:469] Time since start: 45964.26s, 	Step: 7413, 	{'train/loss': 0.12323547033998898, 'validation/loss': 0.12419549813340679, 'validation/num_examples': 83274637, 'test/loss': 0.12659365700201738, 'test/num_examples': 95000000, 'score': 7336.8086721897125, 'total_duration': 45964.25875735283, 'accumulated_submission_time': 7336.8086721897125, 'accumulated_eval_time': 38570.80306315422, 'accumulated_logging_time': 1.7453749179840088}
I0316 23:15:03.288742 140404108269312 logging_writer.py:48] [7413] accumulated_eval_time=38570.8, accumulated_logging_time=1.74537, accumulated_submission_time=7336.81, global_step=7413, preemption_count=0, score=7336.81, test/loss=0.126594, test/num_examples=95000000, total_duration=45964.3, train/loss=0.123235, validation/loss=0.124195, validation/num_examples=83274637
I0316 23:16:21.947949 140404116662016 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.0178012, loss=0.11795
I0316 23:16:21.951286 140444057306304 submission.py:265] 7500) loss = 0.118, grad_norm = 0.018
I0316 23:17:04.389177 140444057306304 spec.py:321] Evaluating on the training split.
I0316 23:19:07.717008 140444057306304 spec.py:333] Evaluating on the validation split.
I0316 23:21:12.535498 140444057306304 spec.py:349] Evaluating on the test split.
I0316 23:26:21.391009 140444057306304 submission_runner.py:469] Time since start: 46642.37s, 	Step: 7534, 	{'train/loss': 0.12166928425837091, 'validation/loss': 0.12398556743787197, 'validation/num_examples': 83274637, 'test/loss': 0.12632333111066316, 'test/num_examples': 95000000, 'score': 7456.990181684494, 'total_duration': 46642.37382555008, 'accumulated_submission_time': 7456.990181684494, 'accumulated_eval_time': 39127.80503773689, 'accumulated_logging_time': 1.7652947902679443}
I0316 23:26:21.402625 140404108269312 logging_writer.py:48] [7534] accumulated_eval_time=39127.8, accumulated_logging_time=1.76529, accumulated_submission_time=7456.99, global_step=7534, preemption_count=0, score=7456.99, test/loss=0.126323, test/num_examples=95000000, total_duration=46642.4, train/loss=0.121669, validation/loss=0.123986, validation/num_examples=83274637
I0316 23:28:22.534894 140444057306304 spec.py:321] Evaluating on the training split.
I0316 23:30:26.053238 140444057306304 spec.py:333] Evaluating on the validation split.
I0316 23:32:29.774631 140444057306304 spec.py:349] Evaluating on the test split.
I0316 23:37:45.996344 140444057306304 submission_runner.py:469] Time since start: 47326.98s, 	Step: 7657, 	{'train/loss': 0.12101246022795017, 'validation/loss': 0.1239581497527394, 'validation/num_examples': 83274637, 'test/loss': 0.12628240522669743, 'test/num_examples': 95000000, 'score': 7577.214041233063, 'total_duration': 47326.9791033268, 'accumulated_submission_time': 7577.214041233063, 'accumulated_eval_time': 39691.26663041115, 'accumulated_logging_time': 1.7834749221801758}
I0316 23:37:46.007953 140404116662016 logging_writer.py:48] [7657] accumulated_eval_time=39691.3, accumulated_logging_time=1.78347, accumulated_submission_time=7577.21, global_step=7657, preemption_count=0, score=7577.21, test/loss=0.126282, test/num_examples=95000000, total_duration=47327, train/loss=0.121012, validation/loss=0.123958, validation/num_examples=83274637
I0316 23:39:47.364967 140444057306304 spec.py:321] Evaluating on the training split.
I0316 23:41:50.872245 140444057306304 spec.py:333] Evaluating on the validation split.
I0316 23:43:55.688087 140444057306304 spec.py:349] Evaluating on the test split.
I0316 23:49:08.656740 140444057306304 submission_runner.py:469] Time since start: 48009.64s, 	Step: 7776, 	{'train/loss': 0.1213882094806677, 'validation/loss': 0.12379910139674512, 'validation/num_examples': 83274637, 'test/loss': 0.12600738070903578, 'test/num_examples': 95000000, 'score': 7697.706192731857, 'total_duration': 48009.639514923096, 'accumulated_submission_time': 7697.706192731857, 'accumulated_eval_time': 40252.558475494385, 'accumulated_logging_time': 1.8019919395446777}
I0316 23:49:08.668112 140404108269312 logging_writer.py:48] [7776] accumulated_eval_time=40252.6, accumulated_logging_time=1.80199, accumulated_submission_time=7697.71, global_step=7776, preemption_count=0, score=7697.71, test/loss=0.126007, test/num_examples=95000000, total_duration=48009.6, train/loss=0.121388, validation/loss=0.123799, validation/num_examples=83274637
I0316 23:51:09.132205 140404116662016 logging_writer.py:48] [7895] global_step=7895, preemption_count=0, score=7817.69
I0316 23:51:14.766201 140444057306304 submission_runner.py:646] Tuning trial 1/5
I0316 23:51:14.766424 140444057306304 submission_runner.py:647] Hyperparameters: Hyperparameters(learning_rate=0.0017486387539278373, one_minus_beta1=0.06733926164, one_minus_beta2=0.00448403102, epsilon=1e-08, one_minus_momentum=0.0, use_momentum=False, weight_decay=0.08121616522670176, max_preconditioner_dim=1024, precondition_frequency=100, start_preconditioning_step=-1, inv_root_override=0, exponent_multiplier=1.0, grafting_type='ADAM', grafting_epsilon=1e-08, use_normalized_grafting=False, communication_dtype='FP32', communicate_params=True, use_cosine_decay=True, warmup_factor=0.02, label_smoothing=0.0, dropout_rate=0.1, use_nadam=True, step_hint_factor=1.0)
I0316 23:51:14.767329 140444057306304 submission_runner.py:648] Metrics: {'eval_results': [(1, {'train/loss': 0.42260579897679673, 'validation/loss': 0.42486521993325754, 'validation/num_examples': 83274637, 'test/loss': 0.4235680038511577, 'test/num_examples': 95000000, 'score': 6.640608072280884, 'total_duration': 949.2720501422882, 'accumulated_submission_time': 6.640608072280884, 'accumulated_eval_time': 942.2192132472992, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (123, {'train/loss': 0.12923081854984755, 'validation/loss': 0.1291103002531186, 'validation/num_examples': 83274637, 'test/loss': 0.1317280954244915, 'test/num_examples': 95000000, 'score': 126.62877297401428, 'total_duration': 1943.0041732788086, 'accumulated_submission_time': 126.62877297401428, 'accumulated_eval_time': 1814.9217031002045, 'accumulated_logging_time': 0.14622998237609863, 'global_step': 123, 'preemption_count': 0}), (244, {'train/loss': 0.12817084404636053, 'validation/loss': 0.1281942983176408, 'validation/num_examples': 83274637, 'test/loss': 0.13094125858977468, 'test/num_examples': 95000000, 'score': 247.0798101425171, 'total_duration': 2946.7554297447205, 'accumulated_submission_time': 247.0798101425171, 'accumulated_eval_time': 2697.2760610580444, 'accumulated_logging_time': 0.16307497024536133, 'global_step': 244, 'preemption_count': 0}), (365, {'train/loss': 0.12562637123752537, 'validation/loss': 0.12756429306450323, 'validation/num_examples': 83274637, 'test/loss': 0.12987155524548982, 'test/num_examples': 95000000, 'score': 368.2872884273529, 'total_duration': 3945.9176907539368, 'accumulated_submission_time': 368.2872884273529, 'accumulated_eval_time': 3574.2936556339264, 'accumulated_logging_time': 0.18044543266296387, 'global_step': 365, 'preemption_count': 0}), (489, {'train/loss': 0.1256584244874236, 'validation/loss': 0.12678173934314615, 'validation/num_examples': 83274637, 'test/loss': 0.12906288906386526, 'test/num_examples': 95000000, 'score': 488.93984842300415, 'total_duration': 4934.168810606003, 'accumulated_submission_time': 488.93984842300415, 'accumulated_eval_time': 4440.925647974014, 'accumulated_logging_time': 0.2370309829711914, 'global_step': 489, 'preemption_count': 0}), (613, {'train/loss': 0.12787416851060612, 'validation/loss': 0.12742725706237717, 'validation/num_examples': 83274637, 'test/loss': 0.12978037315537302, 'test/num_examples': 95000000, 'score': 608.5112447738647, 'total_duration': 5927.294101715088, 'accumulated_submission_time': 608.5112447738647, 'accumulated_eval_time': 5313.543575286865, 'accumulated_logging_time': 0.25277161598205566, 'global_step': 613, 'preemption_count': 0}), (736, {'train/loss': 0.12637288425641383, 'validation/loss': 0.12620106502029166, 'validation/num_examples': 83274637, 'test/loss': 0.1287449956543772, 'test/num_examples': 95000000, 'score': 728.1306662559509, 'total_duration': 6915.182979345322, 'accumulated_submission_time': 728.1306662559509, 'accumulated_eval_time': 6180.886018753052, 'accumulated_logging_time': 0.2693045139312744, 'global_step': 736, 'preemption_count': 0}), (862, {'train/loss': 0.1255073766603816, 'validation/loss': 0.12635721303307307, 'validation/num_examples': 83274637, 'test/loss': 0.12872989029906423, 'test/num_examples': 95000000, 'score': 848.348484992981, 'total_duration': 7893.6450181007385, 'accumulated_submission_time': 848.348484992981, 'accumulated_eval_time': 7038.203093767166, 'accumulated_logging_time': 0.28646087646484375, 'global_step': 862, 'preemption_count': 0}), (986, {'train/loss': 0.12580540861204095, 'validation/loss': 0.12630198297997977, 'validation/num_examples': 83274637, 'test/loss': 0.12881356923233836, 'test/num_examples': 95000000, 'score': 969.7698495388031, 'total_duration': 8876.361378908157, 'accumulated_submission_time': 969.7698495388031, 'accumulated_eval_time': 7898.565136671066, 'accumulated_logging_time': 0.3030531406402588, 'global_step': 986, 'preemption_count': 0}), (1109, {'train/loss': 0.1253420001533199, 'validation/loss': 0.1262796894076021, 'validation/num_examples': 83274637, 'test/loss': 0.1286827234249316, 'test/num_examples': 95000000, 'score': 1089.6995077133179, 'total_duration': 9846.345588445663, 'accumulated_submission_time': 1089.6995077133179, 'accumulated_eval_time': 8747.722723960876, 'accumulated_logging_time': 0.31928491592407227, 'global_step': 1109, 'preemption_count': 0}), (1232, {'train/loss': 0.1260237436293702, 'validation/loss': 0.12634931102962746, 'validation/num_examples': 83274637, 'test/loss': 0.1289215160079153, 'test/num_examples': 95000000, 'score': 1210.5205907821655, 'total_duration': 10781.599578619003, 'accumulated_submission_time': 1210.5205907821655, 'accumulated_eval_time': 9561.221871852875, 'accumulated_logging_time': 0.35659146308898926, 'global_step': 1232, 'preemption_count': 0}), (1353, {'train/loss': 0.12375292274936046, 'validation/loss': 0.125648794771064, 'validation/num_examples': 83274637, 'test/loss': 0.12816813929523668, 'test/num_examples': 95000000, 'score': 1330.986073732376, 'total_duration': 11681.43078327179, 'accumulated_submission_time': 1330.986073732376, 'accumulated_eval_time': 10339.672608613968, 'accumulated_logging_time': 0.37366676330566406, 'global_step': 1353, 'preemption_count': 0}), (1470, {'train/loss': 0.12413777748380056, 'validation/loss': 0.12555453776294476, 'validation/num_examples': 83274637, 'test/loss': 0.12792362726175407, 'test/num_examples': 95000000, 'score': 1451.39346575737, 'total_duration': 12477.757830619812, 'accumulated_submission_time': 1451.39346575737, 'accumulated_eval_time': 11014.694277048111, 'accumulated_logging_time': 0.390378475189209, 'global_step': 1470, 'preemption_count': 0}), (1592, {'train/loss': 0.12496411507200217, 'validation/loss': 0.12553389932064704, 'validation/num_examples': 83274637, 'test/loss': 0.12775306454519975, 'test/num_examples': 95000000, 'score': 1571.725345134735, 'total_duration': 13171.880573272705, 'accumulated_submission_time': 1571.725345134735, 'accumulated_eval_time': 11587.56941151619, 'accumulated_logging_time': 0.4069802761077881, 'global_step': 1592, 'preemption_count': 0}), (1716, {'train/loss': 0.12328752734388342, 'validation/loss': 0.1253680207386878, 'validation/num_examples': 83274637, 'test/loss': 0.12755045540779517, 'test/num_examples': 95000000, 'score': 1691.7474784851074, 'total_duration': 13852.678138494492, 'accumulated_submission_time': 1691.7474784851074, 'accumulated_eval_time': 12147.451938152313, 'accumulated_logging_time': 0.4239315986633301, 'global_step': 1716, 'preemption_count': 0}), (1838, {'train/loss': 0.12550684896312594, 'validation/loss': 0.12594328151546166, 'validation/num_examples': 83274637, 'test/loss': 0.12844214171190763, 'test/num_examples': 95000000, 'score': 1811.6430382728577, 'total_duration': 14534.237362384796, 'accumulated_submission_time': 1811.6430382728577, 'accumulated_eval_time': 12708.181618452072, 'accumulated_logging_time': 0.44202494621276855, 'global_step': 1838, 'preemption_count': 0}), (1957, {'train/loss': 0.12374469050343295, 'validation/loss': 0.12551733489232994, 'validation/num_examples': 83274637, 'test/loss': 0.12775662579646863, 'test/num_examples': 95000000, 'score': 1931.759015083313, 'total_duration': 15219.92656159401, 'accumulated_submission_time': 1931.759015083313, 'accumulated_eval_time': 13272.849855184555, 'accumulated_logging_time': 0.4603264331817627, 'global_step': 1957, 'preemption_count': 0}), (2078, {'train/loss': 0.12543845503420054, 'validation/loss': 0.1253875596752646, 'validation/num_examples': 83274637, 'test/loss': 0.12767709091226678, 'test/num_examples': 95000000, 'score': 2051.85364818573, 'total_duration': 15900.107107400894, 'accumulated_submission_time': 2051.85364818573, 'accumulated_eval_time': 13832.010572195053, 'accumulated_logging_time': 0.47771286964416504, 'global_step': 2078, 'preemption_count': 0}), (2197, {'train/loss': 0.1253953185708878, 'validation/loss': 0.1253845837808815, 'validation/num_examples': 83274637, 'test/loss': 0.12765813167467618, 'test/num_examples': 95000000, 'score': 2171.6773347854614, 'total_duration': 16576.79117822647, 'accumulated_submission_time': 2171.6773347854614, 'accumulated_eval_time': 14387.946912050247, 'accumulated_logging_time': 0.49455785751342773, 'global_step': 2197, 'preemption_count': 0}), (2316, {'train/loss': 0.12360621181456106, 'validation/loss': 0.1250902342600912, 'validation/num_examples': 83274637, 'test/loss': 0.12751715908082661, 'test/num_examples': 95000000, 'score': 2291.3299129009247, 'total_duration': 17260.60618519783, 'accumulated_submission_time': 2291.3299129009247, 'accumulated_eval_time': 14951.176084041595, 'accumulated_logging_time': 0.5112204551696777, 'global_step': 2316, 'preemption_count': 0}), (2441, {'train/loss': 0.12343402940989898, 'validation/loss': 0.12496312517448685, 'validation/num_examples': 83274637, 'test/loss': 0.127313794477523, 'test/num_examples': 95000000, 'score': 2411.7292551994324, 'total_duration': 17939.757436990738, 'accumulated_submission_time': 2411.7292551994324, 'accumulated_eval_time': 15509.010079145432, 'accumulated_logging_time': 0.529163122177124, 'global_step': 2441, 'preemption_count': 0}), (2553, {'train/loss': 0.12498301337299032, 'validation/loss': 0.12517599720527284, 'validation/num_examples': 83274637, 'test/loss': 0.12745166521871468, 'test/num_examples': 95000000, 'score': 2531.5802681446075, 'total_duration': 18624.34220790863, 'accumulated_submission_time': 2531.5802681446075, 'accumulated_eval_time': 16072.831598758698, 'accumulated_logging_time': 0.5471501350402832, 'global_step': 2553, 'preemption_count': 0}), (2677, {'train/loss': 0.12483020564511906, 'validation/loss': 0.12531357939069035, 'validation/num_examples': 83274637, 'test/loss': 0.1278282787944593, 'test/num_examples': 95000000, 'score': 2651.864834547043, 'total_duration': 19306.571850061417, 'accumulated_submission_time': 2651.864834547043, 'accumulated_eval_time': 16633.81876182556, 'accumulated_logging_time': 0.5744969844818115, 'global_step': 2677, 'preemption_count': 0}), (2796, {'train/loss': 0.12297023407858032, 'validation/loss': 0.12497525951057911, 'validation/num_examples': 83274637, 'test/loss': 0.12736155712713945, 'test/num_examples': 95000000, 'score': 2772.1340565681458, 'total_duration': 19989.655723810196, 'accumulated_submission_time': 2772.1340565681458, 'accumulated_eval_time': 17195.709663152695, 'accumulated_logging_time': 0.5924375057220459, 'global_step': 2796, 'preemption_count': 0}), (2919, {'train/loss': 0.125155661922053, 'validation/loss': 0.12492058564235933, 'validation/num_examples': 83274637, 'test/loss': 0.12729296633377074, 'test/num_examples': 95000000, 'score': 2891.8647694587708, 'total_duration': 20669.687702178955, 'accumulated_submission_time': 2891.8647694587708, 'accumulated_eval_time': 17755.099200487137, 'accumulated_logging_time': 0.6094915866851807, 'global_step': 2919, 'preemption_count': 0}), (3044, {'train/loss': 0.12239372748261368, 'validation/loss': 0.12520737816925406, 'validation/num_examples': 83274637, 'test/loss': 0.12769722140968726, 'test/num_examples': 95000000, 'score': 3012.9528126716614, 'total_duration': 21353.854426383972, 'accumulated_submission_time': 3012.9528126716614, 'accumulated_eval_time': 18317.255046606064, 'accumulated_logging_time': 0.6261429786682129, 'global_step': 3044, 'preemption_count': 0}), (3164, {'train/loss': 0.12165873570198396, 'validation/loss': 0.1251104070501242, 'validation/num_examples': 83274637, 'test/loss': 0.12766139032014545, 'test/num_examples': 95000000, 'score': 3133.448175430298, 'total_duration': 22037.319144010544, 'accumulated_submission_time': 3133.448175430298, 'accumulated_eval_time': 18879.246157884598, 'accumulated_logging_time': 0.7115600109100342, 'global_step': 3164, 'preemption_count': 0}), (3286, {'train/loss': 0.12399945936411086, 'validation/loss': 0.12500377011387634, 'validation/num_examples': 83274637, 'test/loss': 0.1273210746655113, 'test/num_examples': 95000000, 'score': 3253.9057281017303, 'total_duration': 22718.868587493896, 'accumulated_submission_time': 3253.9057281017303, 'accumulated_eval_time': 19439.364085912704, 'accumulated_logging_time': 0.7750215530395508, 'global_step': 3286, 'preemption_count': 0}), (3407, {'train/loss': 0.12367532372113904, 'validation/loss': 0.12496486381576577, 'validation/num_examples': 83274637, 'test/loss': 0.12737549117459748, 'test/num_examples': 95000000, 'score': 3373.9030833244324, 'total_duration': 23402.684302568436, 'accumulated_submission_time': 3373.9030833244324, 'accumulated_eval_time': 20002.29987668991, 'accumulated_logging_time': 0.7920582294464111, 'global_step': 3407, 'preemption_count': 0}), (3528, {'train/loss': 0.12306457195711691, 'validation/loss': 0.12476544747447421, 'validation/num_examples': 83274637, 'test/loss': 0.12727453410186768, 'test/num_examples': 95000000, 'score': 3493.7158558368683, 'total_duration': 24078.178349018097, 'accumulated_submission_time': 3493.7158558368683, 'accumulated_eval_time': 20557.05362534523, 'accumulated_logging_time': 0.8090739250183105, 'global_step': 3528, 'preemption_count': 0}), (3649, {'train/loss': 0.1223706443321906, 'validation/loss': 0.12476820883223388, 'validation/num_examples': 83274637, 'test/loss': 0.12720872703335412, 'test/num_examples': 95000000, 'score': 3614.21448469162, 'total_duration': 24763.495587825775, 'accumulated_submission_time': 3614.21448469162, 'accumulated_eval_time': 21120.958647489548, 'accumulated_logging_time': 0.8278121948242188, 'global_step': 3649, 'preemption_count': 0}), (3770, {'train/loss': 0.12266818273881425, 'validation/loss': 0.12480642446135724, 'validation/num_examples': 83274637, 'test/loss': 0.12722648029616507, 'test/num_examples': 95000000, 'score': 3733.8236577510834, 'total_duration': 25448.355803251266, 'accumulated_submission_time': 3733.8236577510834, 'accumulated_eval_time': 21685.28342938423, 'accumulated_logging_time': 0.87888503074646, 'global_step': 3770, 'preemption_count': 0}), (3891, {'train/loss': 0.12289639798758144, 'validation/loss': 0.1250347260006809, 'validation/num_examples': 83274637, 'test/loss': 0.1271782023138749, 'test/num_examples': 95000000, 'score': 3854.2747886180878, 'total_duration': 26137.849810123444, 'accumulated_submission_time': 3854.2747886180878, 'accumulated_eval_time': 22253.440311193466, 'accumulated_logging_time': 0.896308422088623, 'global_step': 3891, 'preemption_count': 0}), (4010, {'train/loss': 0.12217565528266447, 'validation/loss': 0.12476903206842678, 'validation/num_examples': 83274637, 'test/loss': 0.12709808628238878, 'test/num_examples': 95000000, 'score': 3974.0467097759247, 'total_duration': 26817.89743757248, 'accumulated_submission_time': 3974.0467097759247, 'accumulated_eval_time': 22812.811592578888, 'accumulated_logging_time': 0.9138050079345703, 'global_step': 4010, 'preemption_count': 0}), (4131, {'train/loss': 0.12429555061748007, 'validation/loss': 0.12479635725805069, 'validation/num_examples': 83274637, 'test/loss': 0.12725053483685944, 'test/num_examples': 95000000, 'score': 4094.4838206768036, 'total_duration': 27508.161992311478, 'accumulated_submission_time': 4094.4838206768036, 'accumulated_eval_time': 23381.736657857895, 'accumulated_logging_time': 0.9307541847229004, 'global_step': 4131, 'preemption_count': 0}), (4255, {'train/loss': 0.12445416633168335, 'validation/loss': 0.12488542223771933, 'validation/num_examples': 83274637, 'test/loss': 0.1273586820873863, 'test/num_examples': 95000000, 'score': 4214.297745704651, 'total_duration': 28187.088273525238, 'accumulated_submission_time': 4214.297745704651, 'accumulated_eval_time': 23939.861600875854, 'accumulated_logging_time': 1.0069353580474854, 'global_step': 4255, 'preemption_count': 0}), (4377, {'train/loss': 0.12200853159640533, 'validation/loss': 0.1248755649208645, 'validation/num_examples': 83274637, 'test/loss': 0.12733502325359145, 'test/num_examples': 95000000, 'score': 4335.038387537003, 'total_duration': 28863.055032014847, 'accumulated_submission_time': 4335.038387537003, 'accumulated_eval_time': 24494.175498962402, 'accumulated_logging_time': 1.0240271091461182, 'global_step': 4377, 'preemption_count': 0}), (4495, {'train/loss': 0.12281809703355497, 'validation/loss': 0.12478628203365254, 'validation/num_examples': 83274637, 'test/loss': 0.12709862981097572, 'test/num_examples': 95000000, 'score': 4454.841799020767, 'total_duration': 29542.88371181488, 'accumulated_submission_time': 4454.841799020767, 'accumulated_eval_time': 25053.281759262085, 'accumulated_logging_time': 1.0424878597259521, 'global_step': 4495, 'preemption_count': 0}), (4616, {'train/loss': 0.12513383997042857, 'validation/loss': 0.1242933268469449, 'validation/num_examples': 83274637, 'test/loss': 0.12653619451277884, 'test/num_examples': 95000000, 'score': 4574.722317457199, 'total_duration': 30228.89755177498, 'accumulated_submission_time': 4574.722317457199, 'accumulated_eval_time': 25618.52474474907, 'accumulated_logging_time': 1.0606696605682373, 'global_step': 4616, 'preemption_count': 0}), (4739, {'train/loss': 0.1255573124931046, 'validation/loss': 0.12444055599079916, 'validation/num_examples': 83274637, 'test/loss': 0.12670724260193675, 'test/num_examples': 95000000, 'score': 4694.77729678154, 'total_duration': 30915.701027154922, 'accumulated_submission_time': 4694.77729678154, 'accumulated_eval_time': 26184.345997571945, 'accumulated_logging_time': 1.0779473781585693, 'global_step': 4739, 'preemption_count': 0}), (4860, {'train/loss': 0.12298905418369682, 'validation/loss': 0.12452522304964576, 'validation/num_examples': 83274637, 'test/loss': 0.12682551859291477, 'test/num_examples': 95000000, 'score': 4815.899124383926, 'total_duration': 31599.40146780014, 'accumulated_submission_time': 4815.899124383926, 'accumulated_eval_time': 26746.013878583908, 'accumulated_logging_time': 1.1184883117675781, 'global_step': 4860, 'preemption_count': 0}), (4984, {'train/loss': 0.12510103116910548, 'validation/loss': 0.12449226386012505, 'validation/num_examples': 83274637, 'test/loss': 0.12684085247465435, 'test/num_examples': 95000000, 'score': 4936.159632444382, 'total_duration': 32279.422426462173, 'accumulated_submission_time': 4936.159632444382, 'accumulated_eval_time': 27304.867115020752, 'accumulated_logging_time': 1.1368942260742188, 'global_step': 4984, 'preemption_count': 0}), (5108, {'train/loss': 0.12338032447225457, 'validation/loss': 0.12451821010678726, 'validation/num_examples': 83274637, 'test/loss': 0.12700690218927485, 'test/num_examples': 95000000, 'score': 5055.941500663757, 'total_duration': 32962.102763175964, 'accumulated_submission_time': 5055.941500663757, 'accumulated_eval_time': 27866.834205150604, 'accumulated_logging_time': 1.1544480323791504, 'global_step': 5108, 'preemption_count': 0}), (5225, {'train/loss': 0.12225708428812629, 'validation/loss': 0.12429744280642469, 'validation/num_examples': 83274637, 'test/loss': 0.12658395074591386, 'test/num_examples': 95000000, 'score': 5175.772595405579, 'total_duration': 33642.93902516365, 'accumulated_submission_time': 5175.772595405579, 'accumulated_eval_time': 28426.93671655655, 'accumulated_logging_time': 1.173121690750122, 'global_step': 5225, 'preemption_count': 0}), (5348, {'train/loss': 0.12177003808142758, 'validation/loss': 0.12441422908969868, 'validation/num_examples': 83274637, 'test/loss': 0.12667920865309865, 'test/num_examples': 95000000, 'score': 5295.953923940659, 'total_duration': 34320.96162438393, 'accumulated_submission_time': 5295.953923940659, 'accumulated_eval_time': 28983.818868398666, 'accumulated_logging_time': 1.219975233078003, 'global_step': 5348, 'preemption_count': 0}), (5468, {'train/loss': 0.123059707792238, 'validation/loss': 0.12436330404556413, 'validation/num_examples': 83274637, 'test/loss': 0.12666183545765125, 'test/num_examples': 95000000, 'score': 5416.282939195633, 'total_duration': 35010.58262181282, 'accumulated_submission_time': 5416.282939195633, 'accumulated_eval_time': 29552.202750205994, 'accumulated_logging_time': 1.2372703552246094, 'global_step': 5468, 'preemption_count': 0}), (5592, {'train/loss': 0.12379021071339696, 'validation/loss': 0.12451265965171292, 'validation/num_examples': 83274637, 'test/loss': 0.12695651925024734, 'test/num_examples': 95000000, 'score': 5536.830785274506, 'total_duration': 35703.236033678055, 'accumulated_submission_time': 5536.830785274506, 'accumulated_eval_time': 30123.424327611923, 'accumulated_logging_time': 1.2563667297363281, 'global_step': 5592, 'preemption_count': 0}), (5710, {'train/loss': 0.12495810021401718, 'validation/loss': 0.1240392240992443, 'validation/num_examples': 83274637, 'test/loss': 0.1263329262197394, 'test/num_examples': 95000000, 'score': 5656.58384180069, 'total_duration': 36386.41411447525, 'accumulated_submission_time': 5656.58384180069, 'accumulated_eval_time': 30685.980119228363, 'accumulated_logging_time': 1.304927110671997, 'global_step': 5710, 'preemption_count': 0}), (5835, {'train/loss': 0.12248819790188367, 'validation/loss': 0.12417376122244979, 'validation/num_examples': 83274637, 'test/loss': 0.12654556902799105, 'test/num_examples': 95000000, 'score': 5776.847810506821, 'total_duration': 37077.25123715401, 'accumulated_submission_time': 5776.847810506821, 'accumulated_eval_time': 31255.544065237045, 'accumulated_logging_time': 1.4017579555511475, 'global_step': 5835, 'preemption_count': 0}), (5960, {'train/loss': 0.12173180879175607, 'validation/loss': 0.12420267849889319, 'validation/num_examples': 83274637, 'test/loss': 0.12660802479906585, 'test/num_examples': 95000000, 'score': 5896.479827642441, 'total_duration': 37762.017844200134, 'accumulated_submission_time': 5896.479827642441, 'accumulated_eval_time': 31819.81757044792, 'accumulated_logging_time': 1.4195375442504883, 'global_step': 5960, 'preemption_count': 0}), (6080, {'train/loss': 0.12346099074339975, 'validation/loss': 0.12420211540722309, 'validation/num_examples': 83274637, 'test/loss': 0.12658449985046386, 'test/num_examples': 95000000, 'score': 6017.647786855698, 'total_duration': 38446.447353839874, 'accumulated_submission_time': 6017.647786855698, 'accumulated_eval_time': 32382.186200618744, 'accumulated_logging_time': 1.4376697540283203, 'global_step': 6080, 'preemption_count': 0}), (6203, {'train/loss': 0.12221063183970161, 'validation/loss': 0.12480169058318324, 'validation/num_examples': 83274637, 'test/loss': 0.1268822314521388, 'test/num_examples': 95000000, 'score': 6138.099652528763, 'total_duration': 39130.60075163841, 'accumulated_submission_time': 6138.099652528763, 'accumulated_eval_time': 32944.97610402107, 'accumulated_logging_time': 1.4569640159606934, 'global_step': 6203, 'preemption_count': 0}), (6322, {'train/loss': 0.12339407974985482, 'validation/loss': 0.12410415617036888, 'validation/num_examples': 83274637, 'test/loss': 0.12640449687941702, 'test/num_examples': 95000000, 'score': 6257.9860672950745, 'total_duration': 39811.43871998787, 'accumulated_submission_time': 6257.9860672950745, 'accumulated_eval_time': 33505.01225686073, 'accumulated_logging_time': 1.4767816066741943, 'global_step': 6322, 'preemption_count': 0}), (6440, {'train/loss': 0.12389660129478791, 'validation/loss': 0.12433307144743286, 'validation/num_examples': 83274637, 'test/loss': 0.126712058998911, 'test/num_examples': 95000000, 'score': 6377.728576660156, 'total_duration': 40497.18017268181, 'accumulated_submission_time': 6377.728576660156, 'accumulated_eval_time': 34070.041502952576, 'accumulated_logging_time': 1.5355265140533447, 'global_step': 6440, 'preemption_count': 0}), (6561, {'train/loss': 0.12258541599167436, 'validation/loss': 0.12418120044638162, 'validation/num_examples': 83274637, 'test/loss': 0.1264944525982907, 'test/num_examples': 95000000, 'score': 6497.907428741455, 'total_duration': 41176.87443614006, 'accumulated_submission_time': 6497.907428741455, 'accumulated_eval_time': 34628.61609888077, 'accumulated_logging_time': 1.5543944835662842, 'global_step': 6561, 'preemption_count': 0}), (6686, {'train/loss': 0.12299517381890689, 'validation/loss': 0.12419014631226001, 'validation/num_examples': 83274637, 'test/loss': 0.12661495183780067, 'test/num_examples': 95000000, 'score': 6617.82582616806, 'total_duration': 41865.12874507904, 'accumulated_submission_time': 6617.82582616806, 'accumulated_eval_time': 35196.01975297928, 'accumulated_logging_time': 1.573943853378296, 'global_step': 6686, 'preemption_count': 0}), (6807, {'train/loss': 0.11998009616813231, 'validation/loss': 0.12404246017713338, 'validation/num_examples': 83274637, 'test/loss': 0.1263536411002711, 'test/num_examples': 95000000, 'score': 6738.3446452617645, 'total_duration': 42548.80435204506, 'accumulated_submission_time': 6738.3446452617645, 'accumulated_eval_time': 35758.278670072556, 'accumulated_logging_time': 1.594161033630371, 'global_step': 6807, 'preemption_count': 0}), (6928, {'train/loss': 0.12219525845444697, 'validation/loss': 0.12404092045851384, 'validation/num_examples': 83274637, 'test/loss': 0.1263211358651412, 'test/num_examples': 95000000, 'score': 6858.057327032089, 'total_duration': 43231.31038093567, 'accumulated_submission_time': 6858.057327032089, 'accumulated_eval_time': 36320.19295883179, 'accumulated_logging_time': 1.6363492012023926, 'global_step': 6928, 'preemption_count': 0}), (7047, {'train/loss': 0.12271226525734344, 'validation/loss': 0.12403221554256239, 'validation/num_examples': 83274637, 'test/loss': 0.12638565355007775, 'test/num_examples': 95000000, 'score': 6977.8189969062805, 'total_duration': 43917.87267661095, 'accumulated_submission_time': 6977.8189969062805, 'accumulated_eval_time': 36886.09798741341, 'accumulated_logging_time': 1.6560561656951904, 'global_step': 7047, 'preemption_count': 0}), (7169, {'train/loss': 0.12223372947953798, 'validation/loss': 0.1240772743536605, 'validation/num_examples': 83274637, 'test/loss': 0.1263718737794575, 'test/num_examples': 95000000, 'score': 7097.526839256287, 'total_duration': 44606.9381415844, 'accumulated_submission_time': 7097.526839256287, 'accumulated_eval_time': 37454.59686207771, 'accumulated_logging_time': 1.675302267074585, 'global_step': 7169, 'preemption_count': 0}), (7293, {'train/loss': 0.1233360518249913, 'validation/loss': 0.12396037555815444, 'validation/num_examples': 83274637, 'test/loss': 0.12618515208941009, 'test/num_examples': 95000000, 'score': 7217.156074523926, 'total_duration': 45281.048209905624, 'accumulated_submission_time': 7217.156074523926, 'accumulated_eval_time': 38008.19267439842, 'accumulated_logging_time': 1.6947665214538574, 'global_step': 7293, 'preemption_count': 0}), (7413, {'train/loss': 0.12323547033998898, 'validation/loss': 0.12419549813340679, 'validation/num_examples': 83274637, 'test/loss': 0.12659365700201738, 'test/num_examples': 95000000, 'score': 7336.8086721897125, 'total_duration': 45964.25875735283, 'accumulated_submission_time': 7336.8086721897125, 'accumulated_eval_time': 38570.80306315422, 'accumulated_logging_time': 1.7453749179840088, 'global_step': 7413, 'preemption_count': 0}), (7534, {'train/loss': 0.12166928425837091, 'validation/loss': 0.12398556743787197, 'validation/num_examples': 83274637, 'test/loss': 0.12632333111066316, 'test/num_examples': 95000000, 'score': 7456.990181684494, 'total_duration': 46642.37382555008, 'accumulated_submission_time': 7456.990181684494, 'accumulated_eval_time': 39127.80503773689, 'accumulated_logging_time': 1.7652947902679443, 'global_step': 7534, 'preemption_count': 0}), (7657, {'train/loss': 0.12101246022795017, 'validation/loss': 0.1239581497527394, 'validation/num_examples': 83274637, 'test/loss': 0.12628240522669743, 'test/num_examples': 95000000, 'score': 7577.214041233063, 'total_duration': 47326.9791033268, 'accumulated_submission_time': 7577.214041233063, 'accumulated_eval_time': 39691.26663041115, 'accumulated_logging_time': 1.7834749221801758, 'global_step': 7657, 'preemption_count': 0}), (7776, {'train/loss': 0.1213882094806677, 'validation/loss': 0.12379910139674512, 'validation/num_examples': 83274637, 'test/loss': 0.12600738070903578, 'test/num_examples': 95000000, 'score': 7697.706192731857, 'total_duration': 48009.639514923096, 'accumulated_submission_time': 7697.706192731857, 'accumulated_eval_time': 40252.558475494385, 'accumulated_logging_time': 1.8019919395446777, 'global_step': 7776, 'preemption_count': 0})], 'global_step': 7895}
I0316 23:51:14.767438 140444057306304 submission_runner.py:649] Timing: 7817.688884735107
I0316 23:51:14.767477 140444057306304 submission_runner.py:651] Total number of evals: 65
I0316 23:51:14.767506 140444057306304 submission_runner.py:652] ====================
I0316 23:51:14.767616 140444057306304 submission_runner.py:750] Final criteo1tb score: 0
