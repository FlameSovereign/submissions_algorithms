torchrun --redirects 1:0,2:0,3:0,4:0,5:0,6:0,7:0 --standalone --nnodes=1 --nproc_per_node=8 submission_runner.py --framework=pytorch --workload=criteo1tb --submission_path=submissions_algorithms/leaderboard/external_tuning/shampoo_submission/submission.py --data_dir=/data/criteo1tb --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=submissions/rolling_leaderboard/external_tuning/shampoo/study_0 --overwrite=True --save_checkpoints=False --rng_seed=-1741752168 --torch_compile=true --tuning_ruleset=external --tuning_search_space=submissions_algorithms/leaderboard/external_tuning/shampoo_submission/tuning_search_space.json --num_tuning_trials=5 --hparam_start_index=3 --hparam_end_index=4 2>&1 | tee -a /logs/criteo1tb_pytorch_03-16-2025-10-47-25.log
W0316 10:47:41.934000 9 site-packages/torch/distributed/run.py:793] 
W0316 10:47:41.934000 9 site-packages/torch/distributed/run.py:793] *****************************************
W0316 10:47:41.934000 9 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0316 10:47:41.934000 9 site-packages/torch/distributed/run.py:793] *****************************************
2025-03-16 10:47:54.103982: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-03-16 10:47:54.103984: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-03-16 10:47:54.103999: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-03-16 10:47:54.103985: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-03-16 10:47:54.103981: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-03-16 10:47:54.103986: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-03-16 10:47:54.103982: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-03-16 10:47:54.103984: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1742122074.679864      47 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1742122074.679919      49 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1742122074.679866      45 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1742122074.680000      51 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1742122074.680030      50 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1742122074.680003      44 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1742122074.679761      48 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1742122074.680082      46 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1742122074.856638      46 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
E0000 00:00:1742122074.856654      45 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
E0000 00:00:1742122074.856663      49 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
E0000 00:00:1742122074.856667      50 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
E0000 00:00:1742122074.856666      47 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
E0000 00:00:1742122074.856667      51 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
E0000 00:00:1742122074.856669      44 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
E0000 00:00:1742122074.856686      48 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
[rank0]:[W316 10:48:24.776490560 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank6]:[W316 10:48:24.781612681 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 6]  using GPU 6 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank4]:[W316 10:48:24.781702509 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 4]  using GPU 4 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank2]:[W316 10:48:24.789530704 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank3]:[W316 10:48:24.793159017 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank1]:[W316 10:48:24.793369515 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank7]:[W316 10:48:24.795359251 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 7]  using GPU 7 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank5]:[W316 10:48:24.796274440 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 5]  using GPU 5 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
I0316 10:48:27.347014 139975410676928 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_0/criteo1tb_pytorch.
I0316 10:48:27.347012 140126872974528 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_0/criteo1tb_pytorch.
I0316 10:48:27.347013 140316090979520 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_0/criteo1tb_pytorch.
I0316 10:48:27.347013 140446079923392 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_0/criteo1tb_pytorch.
I0316 10:48:27.347012 140443927065792 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_0/criteo1tb_pytorch.
I0316 10:48:27.347012 140446351090880 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_0/criteo1tb_pytorch.
I0316 10:48:27.347048 140025425708224 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_0/criteo1tb_pytorch.
I0316 10:48:27.347155 139763600209088 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_0/criteo1tb_pytorch.
I0316 10:48:36.545357 140316090979520 submission_runner.py:606] Using RNG seed -1741752168
I0316 10:48:36.546303 139975410676928 logger_utils.py:97] Saving hparams to /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_0/criteo1tb_pytorch/trial_4/hparams.json.
I0316 10:48:36.546313 139763600209088 logger_utils.py:97] Saving hparams to /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_0/criteo1tb_pytorch/trial_4/hparams.json.
I0316 10:48:36.546314 140446351090880 logger_utils.py:97] Saving hparams to /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_0/criteo1tb_pytorch/trial_4/hparams.json.
I0316 10:48:36.546302 140443927065792 logger_utils.py:97] Saving hparams to /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_0/criteo1tb_pytorch/trial_4/hparams.json.
I0316 10:48:36.546351 140126872974528 logger_utils.py:97] Saving hparams to /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_0/criteo1tb_pytorch/trial_4/hparams.json.
I0316 10:48:36.546725 140316090979520 submission_runner.py:615] --- Tuning run 4/5 ---
I0316 10:48:36.546342 140025425708224 logger_utils.py:97] Saving hparams to /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_0/criteo1tb_pytorch/trial_4/hparams.json.
I0316 10:48:36.546844 140316090979520 submission_runner.py:620] Creating tuning directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_0/criteo1tb_pytorch/trial_4.
I0316 10:48:36.546721 140446079923392 logger_utils.py:97] Saving hparams to /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_0/criteo1tb_pytorch/trial_4/hparams.json.
I0316 10:48:36.547093 140316090979520 logger_utils.py:97] Saving hparams to /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_0/criteo1tb_pytorch/trial_4/hparams.json.
I0316 10:48:36.887440 140316090979520 submission_runner.py:218] Initializing dataset.
I0316 10:48:36.887644 140316090979520 submission_runner.py:229] Initializing model.
W0316 10:48:44.124290 140443927065792 submission_runner.py:254] These workloads cannot be fully compiled under current PyTorch version. Proceeding without `torch.compile`.
W0316 10:48:44.124336 140025425708224 submission_runner.py:254] These workloads cannot be fully compiled under current PyTorch version. Proceeding without `torch.compile`.
W0316 10:48:44.124289 140126872974528 submission_runner.py:254] These workloads cannot be fully compiled under current PyTorch version. Proceeding without `torch.compile`.
W0316 10:48:44.124393 140316090979520 submission_runner.py:254] These workloads cannot be fully compiled under current PyTorch version. Proceeding without `torch.compile`.
W0316 10:48:44.124403 139975410676928 submission_runner.py:254] These workloads cannot be fully compiled under current PyTorch version. Proceeding without `torch.compile`.
W0316 10:48:44.124426 140446351090880 submission_runner.py:254] These workloads cannot be fully compiled under current PyTorch version. Proceeding without `torch.compile`.
I0316 10:48:44.124537 140316090979520 submission_runner.py:272] Initializing optimizer.
W0316 10:48:44.124448 139763600209088 submission_runner.py:254] These workloads cannot be fully compiled under current PyTorch version. Proceeding without `torch.compile`.
W0316 10:48:44.124508 140446079923392 submission_runner.py:254] These workloads cannot be fully compiled under current PyTorch version. Proceeding without `torch.compile`.
W0316 10:48:44.175422 139763600209088 distributed_shampoo.py:341] start_preconditioning_step set to -1. Setting start_preconditioning_step equal to precondition frequency 100 by default.
W0316 10:48:44.175438 140446351090880 distributed_shampoo.py:341] start_preconditioning_step set to -1. Setting start_preconditioning_step equal to precondition frequency 100 by default.
W0316 10:48:44.175478 140443927065792 distributed_shampoo.py:341] start_preconditioning_step set to -1. Setting start_preconditioning_step equal to precondition frequency 100 by default.
W0316 10:48:44.175525 139975410676928 distributed_shampoo.py:341] start_preconditioning_step set to -1. Setting start_preconditioning_step equal to precondition frequency 100 by default.
W0316 10:48:44.175633 140316090979520 distributed_shampoo.py:341] start_preconditioning_step set to -1. Setting start_preconditioning_step equal to precondition frequency 100 by default.
W0316 10:48:44.175647 140446079923392 distributed_shampoo.py:341] start_preconditioning_step set to -1. Setting start_preconditioning_step equal to precondition frequency 100 by default.
W0316 10:48:44.175691 140126872974528 distributed_shampoo.py:341] start_preconditioning_step set to -1. Setting start_preconditioning_step equal to precondition frequency 100 by default.
W0316 10:48:44.175705 140025425708224 distributed_shampoo.py:341] start_preconditioning_step set to -1. Setting start_preconditioning_step equal to precondition frequency 100 by default.
I0316 10:48:44.275129 140443927065792 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 0.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 0 (torch.Size([512, 13])), Block block_0 (torch.Size([512, 13])).
I0316 10:48:44.275171 140446351090880 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 0.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 0 (torch.Size([512, 13])), Block block_0 (torch.Size([512, 13])).
I0316 10:48:44.275276 139975410676928 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 0.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 0 (torch.Size([512, 13])), Block block_0 (torch.Size([512, 13])).
I0316 10:48:44.275351 140446351090880 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 1.block_0 ([torch.Size([0])]) for Parameter 1 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 10:48:44.275318 140126872974528 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 0.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 0 (torch.Size([512, 13])), Block block_0 (torch.Size([512, 13])).
I0316 10:48:44.275364 140443927065792 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 1.block_0 ([torch.Size([0])]) for Parameter 1 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 10:48:44.275384 140316090979520 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 0.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 0 (torch.Size([512, 13])), Block block_0 (torch.Size([512, 13])).
I0316 10:48:44.275324 140446079923392 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 0.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 0 (torch.Size([512, 13])), Block block_0 (torch.Size([512, 13])).
I0316 10:48:44.275398 139763600209088 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 0.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 0 (torch.Size([512, 13])), Block block_0 (torch.Size([512, 13])).
I0316 10:48:44.275470 139975410676928 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 1.block_0 ([torch.Size([0])]) for Parameter 1 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 10:48:44.275497 140126872974528 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 1.block_0 ([torch.Size([0])]) for Parameter 1 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 10:48:44.275553 140316090979520 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 1.block_0 ([torch.Size([0])]) for Parameter 1 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 10:48:44.275538 140446351090880 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 2.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 2 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 10:48:44.275566 140446079923392 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 1.block_0 ([torch.Size([0])]) for Parameter 1 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 10:48:44.275675 140446351090880 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 3.block_0 ([torch.Size([0])]) for Parameter 3 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 10:48:44.275678 139975410676928 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 2.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 2 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 10:48:44.275683 140126872974528 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 2.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 2 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 10:48:44.275733 140316090979520 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 2.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 2 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 10:48:44.275759 140446079923392 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 2.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 2 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 10:48:44.275804 139975410676928 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 3.block_0 ([torch.Size([0])]) for Parameter 3 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 10:48:44.275808 140126872974528 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 3.block_0 ([torch.Size([0])]) for Parameter 3 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 10:48:44.275852 140316090979520 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 3.block_0 ([torch.Size([0])]) for Parameter 3 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 10:48:44.275886 140446079923392 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 3.block_0 ([torch.Size([0])]) for Parameter 3 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 10:48:44.275957 139975410676928 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 4.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 4 (torch.Size([128, 256])), Block block_0 (torch.Size([128, 256])).
I0316 10:48:44.275956 140446351090880 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 4.block_0 ([torch.Size([128, 128]), torch.Size([256, 256])]) for Parameter 4 (torch.Size([128, 256])), Block block_0 (torch.Size([128, 256])).
I0316 10:48:44.275974 140126872974528 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 4.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 4 (torch.Size([128, 256])), Block block_0 (torch.Size([128, 256])).
I0316 10:48:44.275990 140316090979520 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 4.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 4 (torch.Size([128, 256])), Block block_0 (torch.Size([128, 256])).
I0316 10:48:44.276027 140446079923392 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 4.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 4 (torch.Size([128, 256])), Block block_0 (torch.Size([128, 256])).
I0316 10:48:44.276069 139975410676928 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 5.block_0 ([torch.Size([0])]) for Parameter 5 (torch.Size([128])), Block block_0 (torch.Size([128])).
I0316 10:48:44.276024 140025425708224 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 0.block_0 ([torch.Size([512, 512]), torch.Size([13, 13])]) for Parameter 0 (torch.Size([512, 13])), Block block_0 (torch.Size([512, 13])).
I0316 10:48:44.276074 140126872974528 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 5.block_0 ([torch.Size([0])]) for Parameter 5 (torch.Size([128])), Block block_0 (torch.Size([128])).
I0316 10:48:44.276093 140316090979520 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 5.block_0 ([torch.Size([0])]) for Parameter 5 (torch.Size([128])), Block block_0 (torch.Size([128])).
I0316 10:48:44.276087 140446351090880 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 5.block_0 ([torch.Size([0])]) for Parameter 5 (torch.Size([128])), Block block_0 (torch.Size([128])).
I0316 10:48:44.276137 140446079923392 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 5.block_0 ([torch.Size([0])]) for Parameter 5 (torch.Size([128])), Block block_0 (torch.Size([128])).
I0316 10:48:44.276211 139975410676928 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 6.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 6 (torch.Size([1024, 506])), Block block_0 (torch.Size([1024, 506])).
I0316 10:48:44.276223 140446351090880 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 6.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 6 (torch.Size([1024, 506])), Block block_0 (torch.Size([1024, 506])).
I0316 10:48:44.276206 140025425708224 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 1.block_0 ([torch.Size([0])]) for Parameter 1 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 10:48:44.276233 140316090979520 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 6.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 6 (torch.Size([1024, 506])), Block block_0 (torch.Size([1024, 506])).
I0316 10:48:44.276264 140446079923392 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 6.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 6 (torch.Size([1024, 506])), Block block_0 (torch.Size([1024, 506])).
I0316 10:48:44.276334 139975410676928 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 7.block_0 ([torch.Size([0])]) for Parameter 7 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 10:48:44.276283 140443927065792 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 2.block_0 ([torch.Size([256, 256]), torch.Size([512, 512])]) for Parameter 2 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 10:48:44.276350 140446351090880 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 7.block_0 ([torch.Size([0])]) for Parameter 7 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 10:48:44.276366 140316090979520 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 7.block_0 ([torch.Size([0])]) for Parameter 7 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 10:48:44.276335 139763600209088 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 1.block_0 ([torch.Size([512, 512])]) for Parameter 1 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 10:48:44.276382 140446079923392 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 7.block_0 ([torch.Size([0])]) for Parameter 7 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 10:48:44.276398 140025425708224 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 2.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 2 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 10:48:44.276469 139975410676928 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 8.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 8 (torch.Size([1024, 1024])), Block block_0 (torch.Size([1024, 1024])).
I0316 10:48:44.276476 140446351090880 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 8.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 8 (torch.Size([1024, 1024])), Block block_0 (torch.Size([1024, 1024])).
I0316 10:48:44.276467 140443927065792 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 3.block_0 ([torch.Size([0])]) for Parameter 3 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 10:48:44.276507 140025425708224 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 3.block_0 ([torch.Size([0])]) for Parameter 3 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 10:48:44.276519 140446079923392 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 8.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 8 (torch.Size([1024, 1024])), Block block_0 (torch.Size([1024, 1024])).
I0316 10:48:44.276544 139763600209088 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 2.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 2 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 10:48:44.276556 140446351090880 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 9.block_0 ([torch.Size([0])]) for Parameter 9 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 10:48:44.276558 139975410676928 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 9.block_0 ([torch.Size([0])]) for Parameter 9 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 10:48:44.276611 140446079923392 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 9.block_0 ([torch.Size([0])]) for Parameter 9 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 10:48:44.276637 140443927065792 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 4.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 4 (torch.Size([128, 256])), Block block_0 (torch.Size([128, 256])).
I0316 10:48:44.276668 140025425708224 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 4.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 4 (torch.Size([128, 256])), Block block_0 (torch.Size([128, 256])).
I0316 10:48:44.276675 140446351090880 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 10.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 10 (torch.Size([512, 1024])), Block block_0 (torch.Size([512, 1024])).
I0316 10:48:44.276742 140446079923392 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 10.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 10 (torch.Size([512, 1024])), Block block_0 (torch.Size([512, 1024])).
I0316 10:48:44.276754 140443927065792 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 5.block_0 ([torch.Size([0])]) for Parameter 5 (torch.Size([128])), Block block_0 (torch.Size([128])).
I0316 10:48:44.276763 140446351090880 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 11.block_0 ([torch.Size([0])]) for Parameter 11 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 10:48:44.276741 140126872974528 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 6.block_0 ([torch.Size([1024, 1024]), torch.Size([506, 506])]) for Parameter 6 (torch.Size([1024, 506])), Block block_0 (torch.Size([1024, 506])).
I0316 10:48:44.276747 139763600209088 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 3.block_0 ([torch.Size([256, 256])]) for Parameter 3 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 10:48:44.276775 140025425708224 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 5.block_0 ([torch.Size([0])]) for Parameter 5 (torch.Size([128])), Block block_0 (torch.Size([128])).
I0316 10:48:44.276839 140446079923392 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 11.block_0 ([torch.Size([0])]) for Parameter 11 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 10:48:44.276873 140446351090880 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 12.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 12 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 10:48:44.276884 140443927065792 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 6.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 6 (torch.Size([1024, 506])), Block block_0 (torch.Size([1024, 506])).
I0316 10:48:44.276885 140126872974528 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 7.block_0 ([torch.Size([0])]) for Parameter 7 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 10:48:44.276895 140025425708224 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 6.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 6 (torch.Size([1024, 506])), Block block_0 (torch.Size([1024, 506])).
I0316 10:48:44.276907 139763600209088 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 4.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 4 (torch.Size([128, 256])), Block block_0 (torch.Size([128, 256])).
I0316 10:48:44.276969 140446351090880 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 13.block_0 ([torch.Size([0])]) for Parameter 13 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 10:48:44.276991 140025425708224 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 7.block_0 ([torch.Size([0])]) for Parameter 7 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 10:48:44.276991 140443927065792 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 7.block_0 ([torch.Size([0])]) for Parameter 7 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 10:48:44.277010 140126872974528 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 8.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 8 (torch.Size([1024, 1024])), Block block_0 (torch.Size([1024, 1024])).
I0316 10:48:44.277060 140446351090880 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 14.block_0 ([torch.Size([0])]) for Parameter 14 (torch.Size([1, 256])), Block block_0 (torch.Size([256])).
I0316 10:48:44.277041 140316090979520 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 8.block_0 ([torch.Size([1024, 1024]), torch.Size([1024, 1024])]) for Parameter 8 (torch.Size([1024, 1024])), Block block_0 (torch.Size([1024, 1024])).
I0316 10:48:44.277075 139763600209088 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 5.block_0 ([torch.Size([128, 128])]) for Parameter 5 (torch.Size([128])), Block block_0 (torch.Size([128])).
I0316 10:48:44.277107 140443927065792 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 8.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 8 (torch.Size([1024, 1024])), Block block_0 (torch.Size([1024, 1024])).
I0316 10:48:44.277110 140126872974528 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 9.block_0 ([torch.Size([0])]) for Parameter 9 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 10:48:44.277112 140025425708224 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 8.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 8 (torch.Size([1024, 1024])), Block block_0 (torch.Size([1024, 1024])).
I0316 10:48:44.277153 140446351090880 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 15.block_0 ([torch.Size([0])]) for Parameter 15 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0316 10:48:44.277165 140316090979520 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 9.block_0 ([torch.Size([0])]) for Parameter 9 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 10:48:44.277194 140025425708224 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 9.block_0 ([torch.Size([0])]) for Parameter 9 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 10:48:44.277200 140443927065792 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 9.block_0 ([torch.Size([0])]) for Parameter 9 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 10:48:44.277221 139763600209088 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 6.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 6 (torch.Size([1024, 506])), Block block_0 (torch.Size([1024, 506])).
I0316 10:48:44.277226 140126872974528 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 10.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 10 (torch.Size([512, 1024])), Block block_0 (torch.Size([512, 1024])).
I0316 10:48:44.277248 140446351090880 shampoo_preconditioner_list.py:612] Rank 5: ShampooPreconditionerList Numel Breakdown: (524626, 524288, 655360, 131072, 163840, 32768, 2609224, 2097152, 4194304, 2097152, 2621440, 524288, 655360, 131072, 131072, 2)
I0316 10:48:44.277251 139975410676928 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 10.block_0 ([torch.Size([512, 512]), torch.Size([1024, 1024])]) for Parameter 10 (torch.Size([512, 1024])), Block block_0 (torch.Size([512, 1024])).
I0316 10:48:44.277284 140316090979520 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 10.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 10 (torch.Size([512, 1024])), Block block_0 (torch.Size([512, 1024])).
I0316 10:48:44.277290 140446351090880 shampoo_preconditioner_list.py:615] Rank 5: ShampooPreconditionerList Bytes Breakdown: (2098504,)
I0316 10:48:44.277311 140126872974528 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 11.block_0 ([torch.Size([0])]) for Parameter 11 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 10:48:44.277322 140446351090880 shampoo_preconditioner_list.py:618] Rank 5: ShampooPreconditionerList Total Elements: 17093020
I0316 10:48:44.277318 140025425708224 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 10.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 10 (torch.Size([512, 1024])), Block block_0 (torch.Size([512, 1024])).
I0316 10:48:44.277330 140443927065792 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 10.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 10 (torch.Size([512, 1024])), Block block_0 (torch.Size([512, 1024])).
I0316 10:48:44.277351 140446351090880 shampoo_preconditioner_list.py:621] Rank 5: ShampooPreconditionerList Total Bytes: 2098504
I0316 10:48:44.277369 140316090979520 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 11.block_0 ([torch.Size([0])]) for Parameter 11 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 10:48:44.277385 139763600209088 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 7.block_0 ([torch.Size([1024, 1024])]) for Parameter 7 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 10:48:44.277396 139975410676928 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 11.block_0 ([torch.Size([0])]) for Parameter 11 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 10:48:44.277405 140025425708224 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 11.block_0 ([torch.Size([0])]) for Parameter 11 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 10:48:44.277432 140126872974528 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 12.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 12 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 10:48:44.277433 140443927065792 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 11.block_0 ([torch.Size([0])]) for Parameter 11 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 10:48:44.277483 140446351090880 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([512, 13])), Block block_0 (torch.Size([512, 13])).
I0316 10:48:44.277496 140316090979520 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 12.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 12 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 10:48:44.277510 140025425708224 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 12.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 12 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 10:48:44.277511 139975410676928 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 12.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 12 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 10:48:44.277524 140126872974528 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 13.block_0 ([torch.Size([0])]) for Parameter 13 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 10:48:44.277530 139763600209088 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 8.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 8 (torch.Size([1024, 1024])), Block block_0 (torch.Size([1024, 1024])).
I0316 10:48:44.277548 140446351090880 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 10:48:44.277561 140443927065792 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 12.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 12 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 10:48:44.277582 140316090979520 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 13.block_0 ([torch.Size([0])]) for Parameter 13 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 10:48:44.277589 140025425708224 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 13.block_0 ([torch.Size([0])]) for Parameter 13 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 10:48:44.277601 139975410676928 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 13.block_0 ([torch.Size([0])]) for Parameter 13 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 10:48:44.277613 140446351090880 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 10:48:44.277610 140126872974528 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 14.block_0 ([torch.Size([0])]) for Parameter 14 (torch.Size([1, 256])), Block block_0 (torch.Size([256])).
I0316 10:48:44.277593 140446079923392 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 12.block_0 ([torch.Size([256, 256]), torch.Size([512, 512])]) for Parameter 12 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 10:48:44.277661 140446351090880 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 10:48:44.277662 140443927065792 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 13.block_0 ([torch.Size([0])]) for Parameter 13 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 10:48:44.277687 140316090979520 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 14.block_0 ([torch.Size([0])]) for Parameter 14 (torch.Size([1, 256])), Block block_0 (torch.Size([256])).
I0316 10:48:44.277704 139975410676928 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 14.block_0 ([torch.Size([0])]) for Parameter 14 (torch.Size([1, 256])), Block block_0 (torch.Size([256])).
I0316 10:48:44.277705 140025425708224 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 14.block_0 ([torch.Size([0])]) for Parameter 14 (torch.Size([1, 256])), Block block_0 (torch.Size([256])).
I0316 10:48:44.277722 140126872974528 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 15.block_0 ([torch.Size([0])]) for Parameter 15 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0316 10:48:44.277763 140443927065792 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 14.block_0 ([torch.Size([0])]) for Parameter 14 (torch.Size([1, 256])), Block block_0 (torch.Size([256])).
I0316 10:48:44.277762 140446079923392 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 13.block_0 ([torch.Size([0])]) for Parameter 13 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 10:48:44.277775 140316090979520 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 15.block_0 ([torch.Size([0])]) for Parameter 15 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0316 10:48:44.277785 140446351090880 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 4.block_0 (torch.Size([128, 256])) for Parameter 4 (torch.Size([128, 256])), Block block_0 (torch.Size([128, 256])).
I0316 10:48:44.277792 140025425708224 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 15.block_0 ([torch.Size([0])]) for Parameter 15 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0316 10:48:44.277793 139975410676928 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 15.block_0 ([torch.Size([0])]) for Parameter 15 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0316 10:48:44.277812 140126872974528 shampoo_preconditioner_list.py:612] Rank 2: ShampooPreconditionerList Numel Breakdown: (524626, 524288, 655360, 131072, 163840, 32768, 2609224, 2097152, 4194304, 2097152, 2621440, 524288, 655360, 131072, 131072, 2)
I0316 10:48:44.277855 140126872974528 shampoo_preconditioner_list.py:615] Rank 2: ShampooPreconditionerList Bytes Breakdown: (2098504,)
I0316 10:48:44.277853 140446079923392 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 14.block_0 ([torch.Size([0])]) for Parameter 14 (torch.Size([1, 256])), Block block_0 (torch.Size([256])).
I0316 10:48:44.277855 140446351090880 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 5.block_0 (torch.Size([0])) for Parameter 5 (torch.Size([128])), Block block_0 (torch.Size([128])).
I0316 10:48:44.277856 140443927065792 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 15.block_0 ([torch.Size([0])]) for Parameter 15 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0316 10:48:44.277867 140316090979520 shampoo_preconditioner_list.py:612] Rank 0: ShampooPreconditionerList Numel Breakdown: (524626, 524288, 655360, 131072, 163840, 32768, 2609224, 2097152, 4194304, 2097152, 2621440, 524288, 655360, 131072, 131072, 2)
I0316 10:48:44.277879 140025425708224 shampoo_preconditioner_list.py:612] Rank 6: ShampooPreconditionerList Numel Breakdown: (524626, 524288, 655360, 131072, 163840, 32768, 2609224, 2097152, 4194304, 2097152, 2621440, 524288, 655360, 131072, 131072, 2)
I0316 10:48:44.277884 139975410676928 shampoo_preconditioner_list.py:612] Rank 1: ShampooPreconditionerList Numel Breakdown: (524626, 524288, 655360, 131072, 163840, 32768, 2609224, 2097152, 4194304, 2097152, 2621440, 524288, 655360, 131072, 131072, 2)
I0316 10:48:44.277891 140126872974528 shampoo_preconditioner_list.py:618] Rank 2: ShampooPreconditionerList Total Elements: 17093020
I0316 10:48:44.277904 140316090979520 shampoo_preconditioner_list.py:615] Rank 0: ShampooPreconditionerList Bytes Breakdown: (2098504,)
I0316 10:48:44.277907 140446351090880 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 6.block_0 (torch.Size([0])) for Parameter 6 (torch.Size([1024, 506])), Block block_0 (torch.Size([1024, 506])).
I0316 10:48:44.277919 140126872974528 shampoo_preconditioner_list.py:621] Rank 2: ShampooPreconditionerList Total Bytes: 2098504
I0316 10:48:44.277921 139975410676928 shampoo_preconditioner_list.py:615] Rank 1: ShampooPreconditionerList Bytes Breakdown: (2098504,)
I0316 10:48:44.277923 140025425708224 shampoo_preconditioner_list.py:615] Rank 6: ShampooPreconditionerList Bytes Breakdown: (2098504,)
I0316 10:48:44.277930 140446079923392 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 15.block_0 ([torch.Size([0])]) for Parameter 15 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0316 10:48:44.277944 140316090979520 shampoo_preconditioner_list.py:618] Rank 0: ShampooPreconditionerList Total Elements: 17093020
I0316 10:48:44.277945 140443927065792 shampoo_preconditioner_list.py:612] Rank 3: ShampooPreconditionerList Numel Breakdown: (524626, 524288, 655360, 131072, 163840, 32768, 2609224, 2097152, 4194304, 2097152, 2621440, 524288, 655360, 131072, 131072, 2)
I0316 10:48:44.277952 139975410676928 shampoo_preconditioner_list.py:618] Rank 1: ShampooPreconditionerList Total Elements: 17093020
I0316 10:48:44.277960 140025425708224 shampoo_preconditioner_list.py:618] Rank 6: ShampooPreconditionerList Total Elements: 17093020
I0316 10:48:44.277959 140446351090880 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 7.block_0 (torch.Size([0])) for Parameter 7 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 10:48:44.277978 140316090979520 shampoo_preconditioner_list.py:621] Rank 0: ShampooPreconditionerList Total Bytes: 2098504
I0316 10:48:44.277981 139975410676928 shampoo_preconditioner_list.py:621] Rank 1: ShampooPreconditionerList Total Bytes: 2098504
I0316 10:48:44.277982 140443927065792 shampoo_preconditioner_list.py:615] Rank 3: ShampooPreconditionerList Bytes Breakdown: (2098504,)
I0316 10:48:44.277988 140025425708224 shampoo_preconditioner_list.py:621] Rank 6: ShampooPreconditionerList Total Bytes: 2098504
I0316 10:48:44.278007 140446351090880 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 8.block_0 (torch.Size([0])) for Parameter 8 (torch.Size([1024, 1024])), Block block_0 (torch.Size([1024, 1024])).
I0316 10:48:44.278012 140443927065792 shampoo_preconditioner_list.py:618] Rank 3: ShampooPreconditionerList Total Elements: 17093020
I0316 10:48:44.278022 140446079923392 shampoo_preconditioner_list.py:612] Rank 4: ShampooPreconditionerList Numel Breakdown: (524626, 524288, 655360, 131072, 163840, 32768, 2609224, 2097152, 4194304, 2097152, 2621440, 524288, 655360, 131072, 131072, 2)
I0316 10:48:44.278041 140443927065792 shampoo_preconditioner_list.py:621] Rank 3: ShampooPreconditionerList Total Bytes: 2098504
I0316 10:48:44.278049 140126872974528 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([512, 13])), Block block_0 (torch.Size([512, 13])).
I0316 10:48:44.278060 140446351090880 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 9.block_0 (torch.Size([0])) for Parameter 9 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 10:48:44.278073 140446079923392 shampoo_preconditioner_list.py:615] Rank 4: ShampooPreconditionerList Bytes Breakdown: (2098504,)
I0316 10:48:44.278060 139763600209088 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 9.block_0 ([torch.Size([1024, 1024])]) for Parameter 9 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 10:48:44.278107 140446351090880 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 10.block_0 (torch.Size([0])) for Parameter 10 (torch.Size([512, 1024])), Block block_0 (torch.Size([512, 1024])).
I0316 10:48:44.278107 140126872974528 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 10:48:44.278115 140446079923392 shampoo_preconditioner_list.py:618] Rank 4: ShampooPreconditionerList Total Elements: 17093020
I0316 10:48:44.278109 139975410676928 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([512, 13])), Block block_0 (torch.Size([512, 13])).
I0316 10:48:44.278113 140316090979520 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([512, 13])), Block block_0 (torch.Size([512, 13])).
I0316 10:48:44.278144 140446079923392 shampoo_preconditioner_list.py:621] Rank 4: ShampooPreconditionerList Total Bytes: 2098504
I0316 10:48:44.278156 140126872974528 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 10:48:44.278160 140446351090880 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 11.block_0 (torch.Size([0])) for Parameter 11 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 10:48:44.278167 139975410676928 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 10:48:44.278170 140316090979520 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 10:48:44.278166 140025425708224 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([512, 13])) for Parameter 0 (torch.Size([512, 13])), Block block_0 (torch.Size([512, 13])).
I0316 10:48:44.278181 140443927065792 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([512, 13])), Block block_0 (torch.Size([512, 13])).
I0316 10:48:44.278203 140126872974528 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 10:48:44.278205 140446351090880 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 12.block_0 (torch.Size([0])) for Parameter 12 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 10:48:44.278220 140316090979520 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 10:48:44.278219 139763600209088 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 10.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 10 (torch.Size([512, 1024])), Block block_0 (torch.Size([512, 1024])).
I0316 10:48:44.278227 139975410676928 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 10:48:44.278239 140443927065792 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 10:48:44.278243 140025425708224 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 10:48:44.278249 140446351090880 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 13.block_0 (torch.Size([0])) for Parameter 13 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 10:48:44.278266 140126872974528 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 4.block_0 (torch.Size([0])) for Parameter 4 (torch.Size([128, 256])), Block block_0 (torch.Size([128, 256])).
I0316 10:48:44.278274 139975410676928 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 10:48:44.278276 140316090979520 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 10:48:44.278292 140025425708224 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 10:48:44.278294 140446351090880 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 14.block_0 (torch.Size([0])) for Parameter 14 (torch.Size([1, 256])), Block block_0 (torch.Size([256])).
I0316 10:48:44.278302 140446079923392 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([512, 13])), Block block_0 (torch.Size([512, 13])).
I0316 10:48:44.278320 139975410676928 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 4.block_0 (torch.Size([0])) for Parameter 4 (torch.Size([128, 256])), Block block_0 (torch.Size([128, 256])).
I0316 10:48:44.278322 140126872974528 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 5.block_0 (torch.Size([0])) for Parameter 5 (torch.Size([128])), Block block_0 (torch.Size([128])).
I0316 10:48:44.278331 140316090979520 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 4.block_0 (torch.Size([0])) for Parameter 4 (torch.Size([128, 256])), Block block_0 (torch.Size([128, 256])).
I0316 10:48:44.278338 140446351090880 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 15.block_0 (torch.Size([0])) for Parameter 15 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0316 10:48:44.278351 140025425708224 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 10:48:44.278357 140446079923392 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 10:48:44.278367 139975410676928 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 5.block_0 (torch.Size([0])) for Parameter 5 (torch.Size([128])), Block block_0 (torch.Size([128])).
I0316 10:48:44.278385 140316090979520 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 5.block_0 (torch.Size([0])) for Parameter 5 (torch.Size([128])), Block block_0 (torch.Size([128])).
I0316 10:48:44.278395 140446351090880 shampoo_preconditioner_list.py:375] Rank 5: AdaGradPreconditionerList Numel Breakdown: (0, 0, 0, 0, 32768, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)
I0316 10:48:44.278405 140025425708224 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 4.block_0 (torch.Size([0])) for Parameter 4 (torch.Size([128, 256])), Block block_0 (torch.Size([128, 256])).
I0316 10:48:44.278414 139975410676928 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 6.block_0 (torch.Size([0])) for Parameter 6 (torch.Size([1024, 506])), Block block_0 (torch.Size([1024, 506])).
I0316 10:48:44.278416 140446079923392 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 10:48:44.278426 140446351090880 shampoo_preconditioner_list.py:378] Rank 5: AdaGradPreconditionerList Bytes Breakdown: (0, 0, 0, 0, 131072, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)
I0316 10:48:44.278417 140126872974528 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 6.block_0 (torch.Size([1024, 506])) for Parameter 6 (torch.Size([1024, 506])), Block block_0 (torch.Size([1024, 506])).
I0316 10:48:44.278439 140316090979520 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 6.block_0 (torch.Size([0])) for Parameter 6 (torch.Size([1024, 506])), Block block_0 (torch.Size([1024, 506])).
I0316 10:48:44.278452 140025425708224 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 5.block_0 (torch.Size([0])) for Parameter 5 (torch.Size([128])), Block block_0 (torch.Size([128])).
I0316 10:48:44.278454 140446351090880 shampoo_preconditioner_list.py:381] Rank 5: AdaGradPreconditionerList Total Elements: 32768
I0316 10:48:44.278465 140446079923392 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 10:48:44.278473 139975410676928 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 7.block_0 (torch.Size([0])) for Parameter 7 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 10:48:44.278488 140446351090880 shampoo_preconditioner_list.py:384] Rank 5: AdaGradPreconditionerList Total Bytes: 131072
I0316 10:48:44.278486 140126872974528 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 7.block_0 (torch.Size([0])) for Parameter 7 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 10:48:44.278498 140316090979520 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 7.block_0 (torch.Size([0])) for Parameter 7 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 10:48:44.278500 140025425708224 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 6.block_0 (torch.Size([0])) for Parameter 6 (torch.Size([1024, 506])), Block block_0 (torch.Size([1024, 506])).
I0316 10:48:44.278510 140446079923392 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 4.block_0 (torch.Size([0])) for Parameter 4 (torch.Size([128, 256])), Block block_0 (torch.Size([128, 256])).
I0316 10:48:44.278523 139975410676928 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 8.block_0 (torch.Size([0])) for Parameter 8 (torch.Size([1024, 1024])), Block block_0 (torch.Size([1024, 1024])).
I0316 10:48:44.278536 140126872974528 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 8.block_0 (torch.Size([0])) for Parameter 8 (torch.Size([1024, 1024])), Block block_0 (torch.Size([1024, 1024])).
I0316 10:48:44.278552 140025425708224 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 7.block_0 (torch.Size([0])) for Parameter 7 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 10:48:44.278553 140446079923392 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 5.block_0 (torch.Size([0])) for Parameter 5 (torch.Size([128])), Block block_0 (torch.Size([128])).
I0316 10:48:44.278583 140126872974528 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 9.block_0 (torch.Size([0])) for Parameter 9 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 10:48:44.278584 139975410676928 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 9.block_0 (torch.Size([0])) for Parameter 9 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 10:48:44.278599 140316090979520 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 8.block_0 (torch.Size([1024, 1024])) for Parameter 8 (torch.Size([1024, 1024])), Block block_0 (torch.Size([1024, 1024])).
I0316 10:48:44.278615 140446079923392 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 6.block_0 (torch.Size([0])) for Parameter 6 (torch.Size([1024, 506])), Block block_0 (torch.Size([1024, 506])).
I0316 10:48:44.278614 140025425708224 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 8.block_0 (torch.Size([0])) for Parameter 8 (torch.Size([1024, 1024])), Block block_0 (torch.Size([1024, 1024])).
I0316 10:48:44.278650 140126872974528 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 10.block_0 (torch.Size([0])) for Parameter 10 (torch.Size([512, 1024])), Block block_0 (torch.Size([512, 1024])).
I0316 10:48:44.278663 140025425708224 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 9.block_0 (torch.Size([0])) for Parameter 9 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 10:48:44.278669 140446079923392 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 7.block_0 (torch.Size([0])) for Parameter 7 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 10:48:44.278680 140316090979520 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 9.block_0 (torch.Size([0])) for Parameter 9 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 10:48:44.278680 139975410676928 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 10.block_0 (torch.Size([512, 1024])) for Parameter 10 (torch.Size([512, 1024])), Block block_0 (torch.Size([512, 1024])).
I0316 10:48:44.278709 140025425708224 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 10.block_0 (torch.Size([0])) for Parameter 10 (torch.Size([512, 1024])), Block block_0 (torch.Size([512, 1024])).
I0316 10:48:44.278708 140126872974528 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 11.block_0 (torch.Size([0])) for Parameter 11 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 10:48:44.278719 140446079923392 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 8.block_0 (torch.Size([0])) for Parameter 8 (torch.Size([1024, 1024])), Block block_0 (torch.Size([1024, 1024])).
I0316 10:48:44.278736 140316090979520 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 10.block_0 (torch.Size([0])) for Parameter 10 (torch.Size([512, 1024])), Block block_0 (torch.Size([512, 1024])).
I0316 10:48:44.278743 139975410676928 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 11.block_0 (torch.Size([0])) for Parameter 11 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 10:48:44.278755 140126872974528 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 12.block_0 (torch.Size([0])) for Parameter 12 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 10:48:44.278761 140025425708224 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 11.block_0 (torch.Size([0])) for Parameter 11 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 10:48:44.278765 140446079923392 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 9.block_0 (torch.Size([0])) for Parameter 9 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 10:48:44.278789 140316090979520 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 11.block_0 (torch.Size([0])) for Parameter 11 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 10:48:44.278791 139975410676928 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 12.block_0 (torch.Size([0])) for Parameter 12 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 10:48:44.278799 140126872974528 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 13.block_0 (torch.Size([0])) for Parameter 13 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 10:48:44.278806 140025425708224 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 12.block_0 (torch.Size([0])) for Parameter 12 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 10:48:44.278812 140446079923392 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 10.block_0 (torch.Size([0])) for Parameter 10 (torch.Size([512, 1024])), Block block_0 (torch.Size([512, 1024])).
I0316 10:48:44.278836 139975410676928 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 13.block_0 (torch.Size([0])) for Parameter 13 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 10:48:44.278841 140316090979520 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 12.block_0 (torch.Size([0])) for Parameter 12 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 10:48:44.278844 140126872974528 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 14.block_0 (torch.Size([0])) for Parameter 14 (torch.Size([1, 256])), Block block_0 (torch.Size([256])).
I0316 10:48:44.278850 140025425708224 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 13.block_0 (torch.Size([0])) for Parameter 13 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 10:48:44.278839 140443927065792 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([256, 512])) for Parameter 2 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 10:48:44.278858 140446079923392 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 11.block_0 (torch.Size([0])) for Parameter 11 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 10:48:44.278887 140316090979520 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 13.block_0 (torch.Size([0])) for Parameter 13 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 10:48:44.278887 140126872974528 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 15.block_0 (torch.Size([0])) for Parameter 15 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0316 10:48:44.278890 139975410676928 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 14.block_0 (torch.Size([0])) for Parameter 14 (torch.Size([1, 256])), Block block_0 (torch.Size([256])).
I0316 10:48:44.278869 139763600209088 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 11.block_0 ([torch.Size([512, 512])]) for Parameter 11 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 10:48:44.278895 140025425708224 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 14.block_0 (torch.Size([0])) for Parameter 14 (torch.Size([1, 256])), Block block_0 (torch.Size([256])).
I0316 10:48:44.278935 139975410676928 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 15.block_0 (torch.Size([0])) for Parameter 15 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0316 10:48:44.278939 140025425708224 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 15.block_0 (torch.Size([0])) for Parameter 15 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0316 10:48:44.278939 140316090979520 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 14.block_0 (torch.Size([0])) for Parameter 14 (torch.Size([1, 256])), Block block_0 (torch.Size([256])).
I0316 10:48:44.278945 140126872974528 shampoo_preconditioner_list.py:375] Rank 2: AdaGradPreconditionerList Numel Breakdown: (0, 0, 0, 0, 0, 0, 518144, 0, 0, 0, 0, 0, 0, 0, 0, 0)
I0316 10:48:44.278946 140443927065792 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 10:48:44.278976 140126872974528 shampoo_preconditioner_list.py:378] Rank 2: AdaGradPreconditionerList Bytes Breakdown: (0, 0, 0, 0, 0, 0, 2072576, 0, 0, 0, 0, 0, 0, 0, 0, 0)
I0316 10:48:44.278985 140316090979520 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 15.block_0 (torch.Size([0])) for Parameter 15 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0316 10:48:44.278995 140025425708224 shampoo_preconditioner_list.py:375] Rank 6: AdaGradPreconditionerList Numel Breakdown: (6656, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)
I0316 10:48:44.278996 139975410676928 shampoo_preconditioner_list.py:375] Rank 1: AdaGradPreconditionerList Numel Breakdown: (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 524288, 0, 0, 0, 0, 0)
I0316 10:48:44.279001 140443927065792 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 4.block_0 (torch.Size([0])) for Parameter 4 (torch.Size([128, 256])), Block block_0 (torch.Size([128, 256])).
I0316 10:48:44.279011 140126872974528 shampoo_preconditioner_list.py:381] Rank 2: AdaGradPreconditionerList Total Elements: 518144
I0316 10:48:44.279027 140025425708224 shampoo_preconditioner_list.py:378] Rank 6: AdaGradPreconditionerList Bytes Breakdown: (26624, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)
I0316 10:48:44.279029 139975410676928 shampoo_preconditioner_list.py:378] Rank 1: AdaGradPreconditionerList Bytes Breakdown: (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2097152, 0, 0, 0, 0, 0)
I0316 10:48:44.279019 139763600209088 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 12.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 12 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 10:48:44.279043 140316090979520 shampoo_preconditioner_list.py:375] Rank 0: AdaGradPreconditionerList Numel Breakdown: (0, 0, 0, 0, 0, 0, 0, 0, 1048576, 0, 0, 0, 0, 0, 0, 0)
I0316 10:48:44.279050 140126872974528 shampoo_preconditioner_list.py:384] Rank 2: AdaGradPreconditionerList Total Bytes: 2072576
I0316 10:48:44.279049 140443927065792 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 5.block_0 (torch.Size([0])) for Parameter 5 (torch.Size([128])), Block block_0 (torch.Size([128])).
I0316 10:48:44.279057 139975410676928 shampoo_preconditioner_list.py:381] Rank 1: AdaGradPreconditionerList Total Elements: 524288
I0316 10:48:44.279055 140025425708224 shampoo_preconditioner_list.py:381] Rank 6: AdaGradPreconditionerList Total Elements: 6656
I0316 10:48:44.279081 140316090979520 shampoo_preconditioner_list.py:378] Rank 0: AdaGradPreconditionerList Bytes Breakdown: (0, 0, 0, 0, 0, 0, 0, 0, 4194304, 0, 0, 0, 0, 0, 0, 0)
I0316 10:48:44.279084 139975410676928 shampoo_preconditioner_list.py:384] Rank 1: AdaGradPreconditionerList Total Bytes: 2097152
I0316 10:48:44.279090 140025425708224 shampoo_preconditioner_list.py:384] Rank 6: AdaGradPreconditionerList Total Bytes: 26624
I0316 10:48:44.279097 140443927065792 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 6.block_0 (torch.Size([0])) for Parameter 6 (torch.Size([1024, 506])), Block block_0 (torch.Size([1024, 506])).
I0316 10:48:44.279115 140316090979520 shampoo_preconditioner_list.py:381] Rank 0: AdaGradPreconditionerList Total Elements: 1048576
I0316 10:48:44.279147 140316090979520 shampoo_preconditioner_list.py:384] Rank 0: AdaGradPreconditionerList Total Bytes: 4194304
I0316 10:48:44.279149 140443927065792 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 7.block_0 (torch.Size([0])) for Parameter 7 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 10:48:44.279198 140443927065792 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 8.block_0 (torch.Size([0])) for Parameter 8 (torch.Size([1024, 1024])), Block block_0 (torch.Size([1024, 1024])).
I0316 10:48:44.279256 140443927065792 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 9.block_0 (torch.Size([0])) for Parameter 9 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 10:48:44.279311 140443927065792 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 10.block_0 (torch.Size([0])) for Parameter 10 (torch.Size([512, 1024])), Block block_0 (torch.Size([512, 1024])).
I0316 10:48:44.279363 140443927065792 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 11.block_0 (torch.Size([0])) for Parameter 11 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 10:48:44.279415 140443927065792 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 12.block_0 (torch.Size([0])) for Parameter 12 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 10:48:44.279465 140443927065792 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 13.block_0 (torch.Size([0])) for Parameter 13 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 10:48:44.279516 140443927065792 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 14.block_0 (torch.Size([0])) for Parameter 14 (torch.Size([1, 256])), Block block_0 (torch.Size([256])).
I0316 10:48:44.279578 140443927065792 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 15.block_0 (torch.Size([0])) for Parameter 15 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0316 10:48:44.279598 140446079923392 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 12.block_0 (torch.Size([256, 512])) for Parameter 12 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 10:48:44.279666 140443927065792 shampoo_preconditioner_list.py:375] Rank 3: AdaGradPreconditionerList Numel Breakdown: (0, 0, 131072, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)
I0316 10:48:44.279708 140443927065792 shampoo_preconditioner_list.py:378] Rank 3: AdaGradPreconditionerList Bytes Breakdown: (0, 0, 524288, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)
I0316 10:48:44.279714 140446079923392 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 13.block_0 (torch.Size([0])) for Parameter 13 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 10:48:44.279701 140316090979520 submission.py:105] Large parameters (embedding tables) detected (dim >= 1_000_000). Instantiating Row-Wise AdaGrad...
I0316 10:48:44.279744 140443927065792 shampoo_preconditioner_list.py:381] Rank 3: AdaGradPreconditionerList Total Elements: 131072
I0316 10:48:44.279768 140446079923392 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 14.block_0 (torch.Size([0])) for Parameter 14 (torch.Size([1, 256])), Block block_0 (torch.Size([256])).
I0316 10:48:44.279787 140443927065792 shampoo_preconditioner_list.py:384] Rank 3: AdaGradPreconditionerList Total Bytes: 524288
I0316 10:48:44.279760 139763600209088 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 13.block_0 ([torch.Size([256, 256])]) for Parameter 13 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 10:48:44.279824 140446079923392 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 15.block_0 (torch.Size([0])) for Parameter 15 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0316 10:48:44.279903 140446079923392 shampoo_preconditioner_list.py:375] Rank 4: AdaGradPreconditionerList Numel Breakdown: (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 131072, 0, 0, 0)
I0316 10:48:44.279944 140446079923392 shampoo_preconditioner_list.py:378] Rank 4: AdaGradPreconditionerList Bytes Breakdown: (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 524288, 0, 0, 0)
I0316 10:48:44.279943 139763600209088 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 14.block_0 ([torch.Size([256, 256])]) for Parameter 14 (torch.Size([1, 256])), Block block_0 (torch.Size([256])).
I0316 10:48:44.279979 140446079923392 shampoo_preconditioner_list.py:381] Rank 4: AdaGradPreconditionerList Total Elements: 131072
I0316 10:48:44.280014 140446079923392 shampoo_preconditioner_list.py:384] Rank 4: AdaGradPreconditionerList Total Bytes: 524288
I0316 10:48:44.280075 139763600209088 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 15.block_0 ([torch.Size([1, 1])]) for Parameter 15 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0316 10:48:44.280210 139763600209088 shampoo_preconditioner_list.py:612] Rank 7: ShampooPreconditionerList Numel Breakdown: (524626, 524288, 655360, 131072, 163840, 32768, 2609224, 2097152, 4194304, 2097152, 2621440, 524288, 655360, 131072, 131072, 2)
I0316 10:48:44.280261 139763600209088 shampoo_preconditioner_list.py:615] Rank 7: ShampooPreconditionerList Bytes Breakdown: (2098504, 2097152, 2621440, 524288, 655360, 131072, 10436896, 8388608, 16777216)
I0316 10:48:44.280297 139763600209088 shampoo_preconditioner_list.py:618] Rank 7: ShampooPreconditionerList Total Elements: 17093020
I0316 10:48:44.280331 139763600209088 shampoo_preconditioner_list.py:621] Rank 7: ShampooPreconditionerList Total Bytes: 43730536
I0316 10:48:44.280472 139763600209088 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([512, 13])), Block block_0 (torch.Size([512, 13])).
I0316 10:48:44.280577 139763600209088 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([512])) for Parameter 1 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 10:48:44.280658 139763600209088 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 10:48:44.280745 139763600209088 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([256])) for Parameter 3 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 10:48:44.280811 139763600209088 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 4.block_0 (torch.Size([0])) for Parameter 4 (torch.Size([128, 256])), Block block_0 (torch.Size([128, 256])).
I0316 10:48:44.280891 139763600209088 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 5.block_0 (torch.Size([128])) for Parameter 5 (torch.Size([128])), Block block_0 (torch.Size([128])).
I0316 10:48:44.280956 139763600209088 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 6.block_0 (torch.Size([0])) for Parameter 6 (torch.Size([1024, 506])), Block block_0 (torch.Size([1024, 506])).
I0316 10:48:44.281040 139763600209088 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 7.block_0 (torch.Size([1024])) for Parameter 7 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 10:48:44.281060 140446351090880 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 10:48:44.281106 139763600209088 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 8.block_0 (torch.Size([0])) for Parameter 8 (torch.Size([1024, 1024])), Block block_0 (torch.Size([1024, 1024])).
I0316 10:48:44.281170 140446351090880 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_1 (torch.Size([0])) for Parameter 0 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 10:48:44.281177 139763600209088 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 9.block_0 (torch.Size([1024])) for Parameter 9 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 10:48:44.281147 140025425708224 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 10:48:44.281232 139763600209088 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 10.block_0 (torch.Size([0])) for Parameter 10 (torch.Size([512, 1024])), Block block_0 (torch.Size([512, 1024])).
I0316 10:48:44.281240 140446351090880 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 10:48:44.281240 140025425708224 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_1 (torch.Size([0])) for Parameter 0 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 10:48:44.281293 140446351090880 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_1 (torch.Size([0])) for Parameter 1 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 10:48:44.281298 140025425708224 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 10:48:44.281309 139763600209088 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 11.block_0 (torch.Size([512])) for Parameter 11 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 10:48:44.281347 140025425708224 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_1 (torch.Size([0])) for Parameter 1 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 10:48:44.281349 140446351090880 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 10:48:44.281380 139763600209088 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 12.block_0 (torch.Size([0])) for Parameter 12 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 10:48:44.281410 140025425708224 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 10:48:44.281465 140025425708224 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_1 (torch.Size([0])) for Parameter 2 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 10:48:44.281457 139763600209088 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 13.block_0 (torch.Size([256])) for Parameter 13 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 10:48:44.281533 139763600209088 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 14.block_0 (torch.Size([256])) for Parameter 14 (torch.Size([1, 256])), Block block_0 (torch.Size([256])).
I0316 10:48:44.281607 139763600209088 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 15.block_0 (torch.Size([1])) for Parameter 15 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0316 10:48:44.281708 139763600209088 shampoo_preconditioner_list.py:375] Rank 7: AdaGradPreconditionerList Numel Breakdown: (0, 512, 0, 256, 0, 128, 0, 1024, 0, 1024, 0, 512, 0, 256, 256, 1)
I0316 10:48:44.281751 139763600209088 shampoo_preconditioner_list.py:378] Rank 7: AdaGradPreconditionerList Bytes Breakdown: (0, 2048, 0, 1024, 0, 512, 0, 4096, 0, 4096, 0, 2048, 0, 1024, 1024, 4)
I0316 10:48:44.281784 139763600209088 shampoo_preconditioner_list.py:381] Rank 7: AdaGradPreconditionerList Total Elements: 3969
I0316 10:48:44.281816 139763600209088 shampoo_preconditioner_list.py:384] Rank 7: AdaGradPreconditionerList Total Bytes: 15876
I0316 10:48:44.282061 140126872974528 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 10:48:44.282139 139975410676928 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 10:48:44.282174 140126872974528 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_1 (torch.Size([0])) for Parameter 0 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 10:48:44.282254 140443927065792 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 10:48:44.282361 140443927065792 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_1 (torch.Size([0])) for Parameter 0 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 10:48:44.282429 140443927065792 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 10:48:44.282666 140446351090880 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_1 (torch.Size([524288, 128])) for Parameter 2 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 10:48:44.282704 140446079923392 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 10:48:44.282723 140316090979520 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([524288, 128])) for Parameter 0 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 10:48:44.282776 140446351090880 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 10:48:44.282773 140025425708224 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([524288, 128])) for Parameter 3 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 10:48:44.282809 140446079923392 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_1 (torch.Size([0])) for Parameter 0 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 10:48:44.282830 140316090979520 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_1 (torch.Size([0])) for Parameter 0 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 10:48:44.282838 140446351090880 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_1 (torch.Size([0])) for Parameter 3 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 10:48:44.282866 140446079923392 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 10:48:44.282886 140446351090880 shampoo_preconditioner_list.py:375] Rank 5: AdaGradPreconditionerList Numel Breakdown: (0, 0, 0, 0, 0, 67108864, 0, 0)
I0316 10:48:44.282882 140025425708224 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_1 (torch.Size([0])) for Parameter 3 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 10:48:44.282907 140316090979520 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 10:48:44.282914 140446079923392 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_1 (torch.Size([0])) for Parameter 1 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 10:48:44.282925 140446351090880 shampoo_preconditioner_list.py:378] Rank 5: AdaGradPreconditionerList Bytes Breakdown: (0, 0, 0, 0, 0, 268435456, 0, 0)
I0316 10:48:44.282933 140025425708224 shampoo_preconditioner_list.py:375] Rank 6: AdaGradPreconditionerList Numel Breakdown: (0, 0, 0, 0, 0, 0, 67108864, 0)
I0316 10:48:44.282954 140446351090880 shampoo_preconditioner_list.py:381] Rank 5: AdaGradPreconditionerList Total Elements: 67108864
I0316 10:48:44.282967 140316090979520 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_1 (torch.Size([0])) for Parameter 1 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 10:48:44.282971 140025425708224 shampoo_preconditioner_list.py:378] Rank 6: AdaGradPreconditionerList Bytes Breakdown: (0, 0, 0, 0, 0, 0, 268435456, 0)
I0316 10:48:44.282988 140446351090880 shampoo_preconditioner_list.py:384] Rank 5: AdaGradPreconditionerList Total Bytes: 268435456
I0316 10:48:44.282999 140025425708224 shampoo_preconditioner_list.py:381] Rank 6: AdaGradPreconditionerList Total Elements: 67108864
I0316 10:48:44.283026 140025425708224 shampoo_preconditioner_list.py:384] Rank 6: AdaGradPreconditionerList Total Bytes: 268435456
I0316 10:48:44.283024 140316090979520 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 10:48:44.283074 140316090979520 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_1 (torch.Size([0])) for Parameter 2 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 10:48:44.283128 140316090979520 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 10:48:44.283182 140316090979520 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_1 (torch.Size([0])) for Parameter 3 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 10:48:44.283174 140126872974528 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([524288, 128])) for Parameter 1 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 10:48:44.283234 140316090979520 shampoo_preconditioner_list.py:375] Rank 0: AdaGradPreconditionerList Numel Breakdown: (67108864, 0, 0, 0, 0, 0, 0, 0)
I0316 10:48:44.283230 139975410676928 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_1 (torch.Size([524288, 128])) for Parameter 0 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 10:48:44.283270 140316090979520 shampoo_preconditioner_list.py:378] Rank 0: AdaGradPreconditionerList Bytes Breakdown: (268435456, 0, 0, 0, 0, 0, 0, 0)
I0316 10:48:44.283287 140126872974528 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_1 (torch.Size([0])) for Parameter 1 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 10:48:44.283299 140316090979520 shampoo_preconditioner_list.py:381] Rank 0: AdaGradPreconditionerList Total Elements: 67108864
I0316 10:48:44.283326 140316090979520 shampoo_preconditioner_list.py:384] Rank 0: AdaGradPreconditionerList Total Bytes: 268435456
I0316 10:48:44.283346 139975410676928 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 10:48:44.283353 140126872974528 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 10:48:44.283404 140126872974528 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_1 (torch.Size([0])) for Parameter 2 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 10:48:44.283412 139975410676928 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_1 (torch.Size([0])) for Parameter 1 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 10:48:44.283459 140126872974528 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 10:48:44.283466 139975410676928 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 10:48:44.283513 140126872974528 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_1 (torch.Size([0])) for Parameter 3 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 10:48:44.283515 139975410676928 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_1 (torch.Size([0])) for Parameter 2 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 10:48:44.283563 140126872974528 shampoo_preconditioner_list.py:375] Rank 2: AdaGradPreconditionerList Numel Breakdown: (0, 0, 67108864, 0, 0, 0, 0, 0)
I0316 10:48:44.283573 139975410676928 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 10:48:44.283599 140126872974528 shampoo_preconditioner_list.py:378] Rank 2: AdaGradPreconditionerList Bytes Breakdown: (0, 0, 268435456, 0, 0, 0, 0, 0)
I0316 10:48:44.283624 139975410676928 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_1 (torch.Size([0])) for Parameter 3 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 10:48:44.283640 140126872974528 shampoo_preconditioner_list.py:381] Rank 2: AdaGradPreconditionerList Total Elements: 67108864
I0316 10:48:44.283668 140126872974528 shampoo_preconditioner_list.py:384] Rank 2: AdaGradPreconditionerList Total Bytes: 268435456
I0316 10:48:44.283674 139975410676928 shampoo_preconditioner_list.py:375] Rank 1: AdaGradPreconditionerList Numel Breakdown: (0, 67108864, 0, 0, 0, 0, 0, 0)
I0316 10:48:44.283706 139975410676928 shampoo_preconditioner_list.py:378] Rank 1: AdaGradPreconditionerList Bytes Breakdown: (0, 268435456, 0, 0, 0, 0, 0, 0)
I0316 10:48:44.283704 140443927065792 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_1 (torch.Size([524288, 128])) for Parameter 1 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 10:48:44.283733 139975410676928 shampoo_preconditioner_list.py:381] Rank 1: AdaGradPreconditionerList Total Elements: 67108864
I0316 10:48:44.283760 139975410676928 shampoo_preconditioner_list.py:384] Rank 1: AdaGradPreconditionerList Total Bytes: 268435456
I0316 10:48:44.283816 140443927065792 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 10:48:44.283881 140443927065792 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_1 (torch.Size([0])) for Parameter 2 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 10:48:44.283940 140443927065792 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 10:48:44.283996 140443927065792 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_1 (torch.Size([0])) for Parameter 3 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 10:48:44.284047 140443927065792 shampoo_preconditioner_list.py:375] Rank 3: AdaGradPreconditionerList Numel Breakdown: (0, 0, 0, 67108864, 0, 0, 0, 0)
I0316 10:48:44.284084 140443927065792 shampoo_preconditioner_list.py:378] Rank 3: AdaGradPreconditionerList Bytes Breakdown: (0, 0, 0, 268435456, 0, 0, 0, 0)
I0316 10:48:44.284117 140443927065792 shampoo_preconditioner_list.py:381] Rank 3: AdaGradPreconditionerList Total Elements: 67108864
I0316 10:48:44.284151 140443927065792 shampoo_preconditioner_list.py:384] Rank 3: AdaGradPreconditionerList Total Bytes: 268435456
I0316 10:48:44.284449 140446079923392 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([524288, 128])) for Parameter 2 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 10:48:44.284566 140446079923392 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_1 (torch.Size([0])) for Parameter 2 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 10:48:44.284635 140446079923392 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 10:48:44.284610 139763600209088 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 10:48:44.284696 140446079923392 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_1 (torch.Size([0])) for Parameter 3 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 10:48:44.284713 139763600209088 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_1 (torch.Size([0])) for Parameter 0 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 10:48:44.284752 140446079923392 shampoo_preconditioner_list.py:375] Rank 4: AdaGradPreconditionerList Numel Breakdown: (0, 0, 0, 0, 67108864, 0, 0, 0)
I0316 10:48:44.284773 139763600209088 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 10:48:44.284791 140446079923392 shampoo_preconditioner_list.py:378] Rank 4: AdaGradPreconditionerList Bytes Breakdown: (0, 0, 0, 0, 268435456, 0, 0, 0)
I0316 10:48:44.284821 140446079923392 shampoo_preconditioner_list.py:381] Rank 4: AdaGradPreconditionerList Total Elements: 67108864
I0316 10:48:44.284835 139763600209088 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_1 (torch.Size([0])) for Parameter 1 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 10:48:44.284855 140446079923392 shampoo_preconditioner_list.py:384] Rank 4: AdaGradPreconditionerList Total Bytes: 268435456
I0316 10:48:44.284884 139763600209088 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 10:48:44.284938 139763600209088 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_1 (torch.Size([0])) for Parameter 2 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 10:48:44.284991 139763600209088 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 10:48:44.286887 139763600209088 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_1 (torch.Size([524288, 128])) for Parameter 3 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 10:48:44.286979 139763600209088 shampoo_preconditioner_list.py:375] Rank 7: AdaGradPreconditionerList Numel Breakdown: (0, 0, 0, 0, 0, 0, 0, 67108864)
I0316 10:48:44.286965 140446351090880 logger_utils.py:262] Unable to record workload.train_mean information. Continuing without it.
I0316 10:48:44.286962 140316090979520 submission_runner.py:279] Initializing metrics bundle.
I0316 10:48:44.287017 139763600209088 shampoo_preconditioner_list.py:378] Rank 7: AdaGradPreconditionerList Bytes Breakdown: (0, 0, 0, 0, 0, 0, 0, 268435456)
I0316 10:48:44.287022 140446351090880 logger_utils.py:262] Unable to record workload.train_stddev information. Continuing without it.
I0316 10:48:44.287048 139763600209088 shampoo_preconditioner_list.py:381] Rank 7: AdaGradPreconditionerList Total Elements: 67108864
I0316 10:48:44.287080 139763600209088 shampoo_preconditioner_list.py:384] Rank 7: AdaGradPreconditionerList Total Bytes: 268435456
I0316 10:48:44.287102 140316090979520 submission_runner.py:301] Initializing checkpoint and logger.
I0316 10:48:44.287152 140025425708224 logger_utils.py:262] Unable to record workload.train_mean information. Continuing without it.
I0316 10:48:44.287234 140025425708224 logger_utils.py:262] Unable to record workload.train_stddev information. Continuing without it.
I0316 10:48:44.287505 140316090979520 submission_runner.py:321] Saving meta data to /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_0/criteo1tb_pytorch/trial_4/meta_data_0.json.
I0316 10:48:44.287675 140316090979520 logger_utils.py:262] Unable to record workload.train_mean information. Continuing without it.
I0316 10:48:44.287722 140316090979520 logger_utils.py:262] Unable to record workload.train_stddev information. Continuing without it.
I0316 10:48:44.287783 140126872974528 logger_utils.py:262] Unable to record workload.train_mean information. Continuing without it.
I0316 10:48:44.287857 140126872974528 logger_utils.py:262] Unable to record workload.train_stddev information. Continuing without it.
I0316 10:48:44.287907 139975410676928 logger_utils.py:262] Unable to record workload.train_mean information. Continuing without it.
I0316 10:48:44.287980 139975410676928 logger_utils.py:262] Unable to record workload.train_stddev information. Continuing without it.
I0316 10:48:44.287997 140443927065792 logger_utils.py:262] Unable to record workload.train_mean information. Continuing without it.
I0316 10:48:44.288070 140443927065792 logger_utils.py:262] Unable to record workload.train_stddev information. Continuing without it.
I0316 10:48:44.288334 140446079923392 logger_utils.py:262] Unable to record workload.train_mean information. Continuing without it.
I0316 10:48:44.288418 140446079923392 logger_utils.py:262] Unable to record workload.train_stddev information. Continuing without it.
I0316 10:48:44.288961 139763600209088 logger_utils.py:262] Unable to record workload.train_mean information. Continuing without it.
I0316 10:48:44.289036 139763600209088 logger_utils.py:262] Unable to record workload.train_stddev information. Continuing without it.
I0316 10:48:45.033214 140316090979520 submission_runner.py:325] Saving flags to /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_0/criteo1tb_pytorch/trial_4/flags_0.json.
I0316 10:48:45.123218 140316090979520 submission_runner.py:337] Starting training loop.
I0316 10:48:54.050583 140287758300928 logging_writer.py:48] [0] global_step=0, grad_norm=11.1599, loss=1.69475
I0316 10:48:54.277316 140316090979520 submission.py:265] 0) loss = 1.695, grad_norm = 11.160
I0316 10:48:54.720898 140316090979520 spec.py:321] Evaluating on the training split.
I0316 10:54:10.033756 140316090979520 spec.py:333] Evaluating on the validation split.
I0316 10:59:04.298037 140316090979520 spec.py:349] Evaluating on the test split.
I0316 11:04:46.834006 140316090979520 submission_runner.py:469] Time since start: 961.71s, 	Step: 1, 	{'train/loss': 1.6914659254641344, 'validation/loss': 1.6855084564093616, 'validation/num_examples': 83274637, 'test/loss': 1.6899221253790604, 'test/num_examples': 95000000, 'score': 9.155137777328491, 'total_duration': 961.7108941078186, 'accumulated_submission_time': 9.155137777328491, 'accumulated_eval_time': 952.1133000850677, 'accumulated_logging_time': 0}
I0316 11:04:46.912052 140274135119616 logging_writer.py:48] [1] accumulated_eval_time=952.113, accumulated_logging_time=0, accumulated_submission_time=9.15514, global_step=1, preemption_count=0, score=9.15514, test/loss=1.68992, test/num_examples=95000000, total_duration=961.711, train/loss=1.69147, validation/loss=1.68551, validation/num_examples=83274637
I0316 11:04:47.591290 140274126726912 logging_writer.py:48] [1] global_step=1, grad_norm=11.1662, loss=1.69438
I0316 11:04:47.594718 140316090979520 submission.py:265] 1) loss = 1.694, grad_norm = 11.166
I0316 11:04:47.789543 140274135119616 logging_writer.py:48] [2] global_step=2, grad_norm=10.974, loss=1.65765
I0316 11:04:47.792907 140316090979520 submission.py:265] 2) loss = 1.658, grad_norm = 10.974
I0316 11:04:47.988403 140274126726912 logging_writer.py:48] [3] global_step=3, grad_norm=10.729, loss=1.59714
I0316 11:04:47.991498 140316090979520 submission.py:265] 3) loss = 1.597, grad_norm = 10.729
I0316 11:04:48.188521 140274135119616 logging_writer.py:48] [4] global_step=4, grad_norm=10.2674, loss=1.51272
I0316 11:04:48.191467 140316090979520 submission.py:265] 4) loss = 1.513, grad_norm = 10.267
I0316 11:04:48.388674 140274126726912 logging_writer.py:48] [5] global_step=5, grad_norm=9.75658, loss=1.40977
I0316 11:04:48.391508 140316090979520 submission.py:265] 5) loss = 1.410, grad_norm = 9.757
I0316 11:04:48.587457 140274135119616 logging_writer.py:48] [6] global_step=6, grad_norm=9.08078, loss=1.29345
I0316 11:04:48.590620 140316090979520 submission.py:265] 6) loss = 1.293, grad_norm = 9.081
I0316 11:04:48.787475 140274126726912 logging_writer.py:48] [7] global_step=7, grad_norm=8.28549, loss=1.17431
I0316 11:04:48.790279 140316090979520 submission.py:265] 7) loss = 1.174, grad_norm = 8.285
I0316 11:04:48.984534 140274135119616 logging_writer.py:48] [8] global_step=8, grad_norm=7.59688, loss=1.0566
I0316 11:04:48.987238 140316090979520 submission.py:265] 8) loss = 1.057, grad_norm = 7.597
I0316 11:04:49.182360 140274126726912 logging_writer.py:48] [9] global_step=9, grad_norm=7.06535, loss=0.943726
I0316 11:04:49.185469 140316090979520 submission.py:265] 9) loss = 0.944, grad_norm = 7.065
I0316 11:04:49.379644 140274135119616 logging_writer.py:48] [10] global_step=10, grad_norm=6.636, loss=0.829287
I0316 11:04:49.382529 140316090979520 submission.py:265] 10) loss = 0.829, grad_norm = 6.636
I0316 11:04:49.577228 140274126726912 logging_writer.py:48] [11] global_step=11, grad_norm=6.04624, loss=0.719181
I0316 11:04:49.580145 140316090979520 submission.py:265] 11) loss = 0.719, grad_norm = 6.046
I0316 11:04:49.776602 140274135119616 logging_writer.py:48] [12] global_step=12, grad_norm=5.4696, loss=0.619345
I0316 11:04:49.779943 140316090979520 submission.py:265] 12) loss = 0.619, grad_norm = 5.470
I0316 11:04:49.977576 140274126726912 logging_writer.py:48] [13] global_step=13, grad_norm=4.93142, loss=0.52745
I0316 11:04:49.980783 140316090979520 submission.py:265] 13) loss = 0.527, grad_norm = 4.931
I0316 11:04:50.177786 140274135119616 logging_writer.py:48] [14] global_step=14, grad_norm=4.34939, loss=0.445006
I0316 11:04:50.181576 140316090979520 submission.py:265] 14) loss = 0.445, grad_norm = 4.349
I0316 11:04:50.378242 140274126726912 logging_writer.py:48] [15] global_step=15, grad_norm=3.64305, loss=0.369624
I0316 11:04:50.381377 140316090979520 submission.py:265] 15) loss = 0.370, grad_norm = 3.643
I0316 11:04:50.576800 140274135119616 logging_writer.py:48] [16] global_step=16, grad_norm=2.85519, loss=0.307026
I0316 11:04:50.579991 140316090979520 submission.py:265] 16) loss = 0.307, grad_norm = 2.855
I0316 11:04:50.775288 140274126726912 logging_writer.py:48] [17] global_step=17, grad_norm=2.06371, loss=0.258524
I0316 11:04:50.778659 140316090979520 submission.py:265] 17) loss = 0.259, grad_norm = 2.064
I0316 11:04:50.975888 140274135119616 logging_writer.py:48] [18] global_step=18, grad_norm=1.32433, loss=0.225682
I0316 11:04:50.979459 140316090979520 submission.py:265] 18) loss = 0.226, grad_norm = 1.324
I0316 11:04:51.172631 140274126726912 logging_writer.py:48] [19] global_step=19, grad_norm=0.661355, loss=0.21024
I0316 11:04:51.176156 140316090979520 submission.py:265] 19) loss = 0.210, grad_norm = 0.661
I0316 11:04:51.372663 140274135119616 logging_writer.py:48] [20] global_step=20, grad_norm=0.231267, loss=0.197882
I0316 11:04:51.376543 140316090979520 submission.py:265] 20) loss = 0.198, grad_norm = 0.231
I0316 11:04:51.571196 140274126726912 logging_writer.py:48] [21] global_step=21, grad_norm=0.463155, loss=0.199903
I0316 11:04:51.575188 140316090979520 submission.py:265] 21) loss = 0.200, grad_norm = 0.463
I0316 11:04:51.769984 140274135119616 logging_writer.py:48] [22] global_step=22, grad_norm=0.845851, loss=0.210407
I0316 11:04:51.773729 140316090979520 submission.py:265] 22) loss = 0.210, grad_norm = 0.846
I0316 11:04:51.968588 140274126726912 logging_writer.py:48] [23] global_step=23, grad_norm=1.22361, loss=0.23412
I0316 11:04:51.971743 140316090979520 submission.py:265] 23) loss = 0.234, grad_norm = 1.224
I0316 11:04:52.164129 140274135119616 logging_writer.py:48] [24] global_step=24, grad_norm=1.43345, loss=0.248242
I0316 11:04:52.168057 140316090979520 submission.py:265] 24) loss = 0.248, grad_norm = 1.433
I0316 11:04:52.363994 140274126726912 logging_writer.py:48] [25] global_step=25, grad_norm=1.68876, loss=0.274184
I0316 11:04:52.367520 140316090979520 submission.py:265] 25) loss = 0.274, grad_norm = 1.689
I0316 11:04:52.564649 140274135119616 logging_writer.py:48] [26] global_step=26, grad_norm=1.84955, loss=0.293177
I0316 11:04:52.568402 140316090979520 submission.py:265] 26) loss = 0.293, grad_norm = 1.850
I0316 11:04:52.765231 140274126726912 logging_writer.py:48] [27] global_step=27, grad_norm=2.10505, loss=0.326227
I0316 11:04:52.768857 140316090979520 submission.py:265] 27) loss = 0.326, grad_norm = 2.105
I0316 11:04:52.963873 140274135119616 logging_writer.py:48] [28] global_step=28, grad_norm=2.27994, loss=0.351702
I0316 11:04:52.967260 140316090979520 submission.py:265] 28) loss = 0.352, grad_norm = 2.280
I0316 11:04:53.162204 140274126726912 logging_writer.py:48] [29] global_step=29, grad_norm=2.41593, loss=0.373547
I0316 11:04:53.165749 140316090979520 submission.py:265] 29) loss = 0.374, grad_norm = 2.416
I0316 11:04:53.360250 140274135119616 logging_writer.py:48] [30] global_step=30, grad_norm=2.53451, loss=0.393557
I0316 11:04:53.363425 140316090979520 submission.py:265] 30) loss = 0.394, grad_norm = 2.535
I0316 11:04:54.181519 140274126726912 logging_writer.py:48] [31] global_step=31, grad_norm=2.76468, loss=0.428477
I0316 11:04:54.184938 140316090979520 submission.py:265] 31) loss = 0.428, grad_norm = 2.765
I0316 11:04:55.497550 140274135119616 logging_writer.py:48] [32] global_step=32, grad_norm=2.77298, loss=0.43119
I0316 11:04:55.500764 140316090979520 submission.py:265] 32) loss = 0.431, grad_norm = 2.773
I0316 11:04:56.556723 140274126726912 logging_writer.py:48] [33] global_step=33, grad_norm=2.80719, loss=0.437455
I0316 11:04:56.559940 140316090979520 submission.py:265] 33) loss = 0.437, grad_norm = 2.807
I0316 11:04:58.070587 140274135119616 logging_writer.py:48] [34] global_step=34, grad_norm=2.801, loss=0.43737
I0316 11:04:58.073916 140316090979520 submission.py:265] 34) loss = 0.437, grad_norm = 2.801
I0316 11:04:58.722473 140274126726912 logging_writer.py:48] [35] global_step=35, grad_norm=2.9111, loss=0.453169
I0316 11:04:58.726062 140316090979520 submission.py:265] 35) loss = 0.453, grad_norm = 2.911
I0316 11:05:00.107033 140274135119616 logging_writer.py:48] [36] global_step=36, grad_norm=2.86287, loss=0.445003
I0316 11:05:00.110247 140316090979520 submission.py:265] 36) loss = 0.445, grad_norm = 2.863
I0316 11:05:01.421448 140274126726912 logging_writer.py:48] [37] global_step=37, grad_norm=2.82416, loss=0.436067
I0316 11:05:01.424744 140316090979520 submission.py:265] 37) loss = 0.436, grad_norm = 2.824
I0316 11:05:02.408825 140274135119616 logging_writer.py:48] [38] global_step=38, grad_norm=2.65071, loss=0.411653
I0316 11:05:02.411862 140316090979520 submission.py:265] 38) loss = 0.412, grad_norm = 2.651
I0316 11:05:03.577989 140274126726912 logging_writer.py:48] [39] global_step=39, grad_norm=2.5355, loss=0.391045
I0316 11:05:03.581748 140316090979520 submission.py:265] 39) loss = 0.391, grad_norm = 2.536
I0316 11:05:04.308699 140274135119616 logging_writer.py:48] [40] global_step=40, grad_norm=2.4311, loss=0.372661
I0316 11:05:04.312067 140316090979520 submission.py:265] 40) loss = 0.373, grad_norm = 2.431
I0316 11:05:06.316013 140274126726912 logging_writer.py:48] [41] global_step=41, grad_norm=2.33856, loss=0.356248
I0316 11:05:06.319124 140316090979520 submission.py:265] 41) loss = 0.356, grad_norm = 2.339
I0316 11:05:07.197342 140274135119616 logging_writer.py:48] [42] global_step=42, grad_norm=2.17603, loss=0.330419
I0316 11:05:07.200535 140316090979520 submission.py:265] 42) loss = 0.330, grad_norm = 2.176
I0316 11:05:09.214643 140274126726912 logging_writer.py:48] [43] global_step=43, grad_norm=2.03613, loss=0.307413
I0316 11:05:09.217784 140316090979520 submission.py:265] 43) loss = 0.307, grad_norm = 2.036
I0316 11:05:10.270472 140274135119616 logging_writer.py:48] [44] global_step=44, grad_norm=1.82384, loss=0.276294
I0316 11:05:10.273749 140316090979520 submission.py:265] 44) loss = 0.276, grad_norm = 1.824
I0316 11:05:11.900807 140274126726912 logging_writer.py:48] [45] global_step=45, grad_norm=1.72325, loss=0.261777
I0316 11:05:11.903969 140316090979520 submission.py:265] 45) loss = 0.262, grad_norm = 1.723
I0316 11:05:12.451275 140274135119616 logging_writer.py:48] [46] global_step=46, grad_norm=1.57524, loss=0.24313
I0316 11:05:12.454429 140316090979520 submission.py:265] 46) loss = 0.243, grad_norm = 1.575
I0316 11:05:14.301607 140274126726912 logging_writer.py:48] [47] global_step=47, grad_norm=1.39798, loss=0.223357
I0316 11:05:14.304615 140316090979520 submission.py:265] 47) loss = 0.223, grad_norm = 1.398
I0316 11:05:15.377739 140274135119616 logging_writer.py:48] [48] global_step=48, grad_norm=1.17907, loss=0.202897
I0316 11:05:15.380793 140316090979520 submission.py:265] 48) loss = 0.203, grad_norm = 1.179
I0316 11:05:16.418583 140274126726912 logging_writer.py:48] [49] global_step=49, grad_norm=0.963839, loss=0.187558
I0316 11:05:16.421800 140316090979520 submission.py:265] 49) loss = 0.188, grad_norm = 0.964
I0316 11:05:17.777169 140274135119616 logging_writer.py:48] [50] global_step=50, grad_norm=0.730215, loss=0.176533
I0316 11:05:17.780220 140316090979520 submission.py:265] 50) loss = 0.177, grad_norm = 0.730
I0316 11:05:18.745163 140274126726912 logging_writer.py:48] [51] global_step=51, grad_norm=0.448461, loss=0.165521
I0316 11:05:18.748313 140316090979520 submission.py:265] 51) loss = 0.166, grad_norm = 0.448
I0316 11:05:20.414003 140274135119616 logging_writer.py:48] [52] global_step=52, grad_norm=0.197339, loss=0.158017
I0316 11:05:20.417478 140316090979520 submission.py:265] 52) loss = 0.158, grad_norm = 0.197
I0316 11:05:21.449383 140274126726912 logging_writer.py:48] [53] global_step=53, grad_norm=0.370737, loss=0.159129
I0316 11:05:21.452493 140316090979520 submission.py:265] 53) loss = 0.159, grad_norm = 0.371
I0316 11:05:22.500123 140274135119616 logging_writer.py:48] [54] global_step=54, grad_norm=0.701327, loss=0.163586
I0316 11:05:22.503150 140316090979520 submission.py:265] 54) loss = 0.164, grad_norm = 0.701
I0316 11:05:23.435858 140274126726912 logging_writer.py:48] [55] global_step=55, grad_norm=1.01402, loss=0.167922
I0316 11:05:23.439004 140316090979520 submission.py:265] 55) loss = 0.168, grad_norm = 1.014
I0316 11:05:24.504229 140274135119616 logging_writer.py:48] [56] global_step=56, grad_norm=1.23653, loss=0.173221
I0316 11:05:24.507715 140316090979520 submission.py:265] 56) loss = 0.173, grad_norm = 1.237
I0316 11:05:26.096087 140274126726912 logging_writer.py:48] [57] global_step=57, grad_norm=1.24665, loss=0.173606
I0316 11:05:26.099248 140316090979520 submission.py:265] 57) loss = 0.174, grad_norm = 1.247
I0316 11:05:27.398180 140274135119616 logging_writer.py:48] [58] global_step=58, grad_norm=1.21956, loss=0.171047
I0316 11:05:27.401946 140316090979520 submission.py:265] 58) loss = 0.171, grad_norm = 1.220
I0316 11:05:28.879482 140274126726912 logging_writer.py:48] [59] global_step=59, grad_norm=1.06949, loss=0.169439
I0316 11:05:28.882706 140316090979520 submission.py:265] 59) loss = 0.169, grad_norm = 1.069
I0316 11:05:29.993965 140274135119616 logging_writer.py:48] [60] global_step=60, grad_norm=0.910134, loss=0.160835
I0316 11:05:29.997168 140316090979520 submission.py:265] 60) loss = 0.161, grad_norm = 0.910
I0316 11:05:32.187266 140274126726912 logging_writer.py:48] [61] global_step=61, grad_norm=0.652451, loss=0.154997
I0316 11:05:32.190431 140316090979520 submission.py:265] 61) loss = 0.155, grad_norm = 0.652
I0316 11:05:33.019235 140274135119616 logging_writer.py:48] [62] global_step=62, grad_norm=0.389634, loss=0.149332
I0316 11:05:33.022390 140316090979520 submission.py:265] 62) loss = 0.149, grad_norm = 0.390
I0316 11:05:34.605703 140274126726912 logging_writer.py:48] [63] global_step=63, grad_norm=0.130739, loss=0.146199
I0316 11:05:34.609244 140316090979520 submission.py:265] 63) loss = 0.146, grad_norm = 0.131
I0316 11:05:35.520964 140274135119616 logging_writer.py:48] [64] global_step=64, grad_norm=0.139454, loss=0.146511
I0316 11:05:35.524316 140316090979520 submission.py:265] 64) loss = 0.147, grad_norm = 0.139
I0316 11:05:37.451692 140274126726912 logging_writer.py:48] [65] global_step=65, grad_norm=0.328973, loss=0.148431
I0316 11:05:37.454932 140316090979520 submission.py:265] 65) loss = 0.148, grad_norm = 0.329
I0316 11:05:38.412337 140274135119616 logging_writer.py:48] [66] global_step=66, grad_norm=0.4875, loss=0.153415
I0316 11:05:38.415633 140316090979520 submission.py:265] 66) loss = 0.153, grad_norm = 0.488
I0316 11:05:40.439013 140274126726912 logging_writer.py:48] [67] global_step=67, grad_norm=0.542573, loss=0.151579
I0316 11:05:40.442067 140316090979520 submission.py:265] 67) loss = 0.152, grad_norm = 0.543
I0316 11:05:41.049032 140274135119616 logging_writer.py:48] [68] global_step=68, grad_norm=0.601456, loss=0.154164
I0316 11:05:41.052302 140316090979520 submission.py:265] 68) loss = 0.154, grad_norm = 0.601
I0316 11:05:42.582989 140274126726912 logging_writer.py:48] [69] global_step=69, grad_norm=0.629524, loss=0.156531
I0316 11:05:42.586095 140316090979520 submission.py:265] 69) loss = 0.157, grad_norm = 0.630
I0316 11:05:43.521083 140274135119616 logging_writer.py:48] [70] global_step=70, grad_norm=0.607082, loss=0.154339
I0316 11:05:43.524359 140316090979520 submission.py:265] 70) loss = 0.154, grad_norm = 0.607
I0316 11:05:44.976246 140274126726912 logging_writer.py:48] [71] global_step=71, grad_norm=0.574742, loss=0.153485
I0316 11:05:44.979470 140316090979520 submission.py:265] 71) loss = 0.153, grad_norm = 0.575
I0316 11:05:45.715796 140274135119616 logging_writer.py:48] [72] global_step=72, grad_norm=0.530486, loss=0.152686
I0316 11:05:45.719072 140316090979520 submission.py:265] 72) loss = 0.153, grad_norm = 0.530
I0316 11:05:47.275740 140274126726912 logging_writer.py:48] [73] global_step=73, grad_norm=0.476052, loss=0.153574
I0316 11:05:47.279058 140316090979520 submission.py:265] 73) loss = 0.154, grad_norm = 0.476
I0316 11:05:48.265552 140274135119616 logging_writer.py:48] [74] global_step=74, grad_norm=0.359551, loss=0.148575
I0316 11:05:48.268740 140316090979520 submission.py:265] 74) loss = 0.149, grad_norm = 0.360
I0316 11:05:49.696948 140274126726912 logging_writer.py:48] [75] global_step=75, grad_norm=0.250326, loss=0.146699
I0316 11:05:49.700079 140316090979520 submission.py:265] 75) loss = 0.147, grad_norm = 0.250
I0316 11:05:50.845311 140274135119616 logging_writer.py:48] [76] global_step=76, grad_norm=0.157256, loss=0.153121
I0316 11:05:50.848515 140316090979520 submission.py:265] 76) loss = 0.153, grad_norm = 0.157
I0316 11:05:51.753270 140274126726912 logging_writer.py:48] [77] global_step=77, grad_norm=0.0639618, loss=0.153493
I0316 11:05:51.756522 140316090979520 submission.py:265] 77) loss = 0.153, grad_norm = 0.064
I0316 11:05:52.929592 140274135119616 logging_writer.py:48] [78] global_step=78, grad_norm=0.169461, loss=0.154489
I0316 11:05:52.932563 140316090979520 submission.py:265] 78) loss = 0.154, grad_norm = 0.169
I0316 11:05:54.050765 140274126726912 logging_writer.py:48] [79] global_step=79, grad_norm=0.306902, loss=0.155304
I0316 11:05:54.053913 140316090979520 submission.py:265] 79) loss = 0.155, grad_norm = 0.307
I0316 11:05:54.922251 140274135119616 logging_writer.py:48] [80] global_step=80, grad_norm=0.42185, loss=0.156411
I0316 11:05:54.925264 140316090979520 submission.py:265] 80) loss = 0.156, grad_norm = 0.422
I0316 11:05:56.487221 140274126726912 logging_writer.py:48] [81] global_step=81, grad_norm=0.491962, loss=0.15958
I0316 11:05:56.490431 140316090979520 submission.py:265] 81) loss = 0.160, grad_norm = 0.492
I0316 11:05:57.356817 140274135119616 logging_writer.py:48] [82] global_step=82, grad_norm=0.529503, loss=0.159273
I0316 11:05:57.360148 140316090979520 submission.py:265] 82) loss = 0.159, grad_norm = 0.530
I0316 11:05:58.601581 140274126726912 logging_writer.py:48] [83] global_step=83, grad_norm=0.524292, loss=0.15914
I0316 11:05:58.604732 140316090979520 submission.py:265] 83) loss = 0.159, grad_norm = 0.524
I0316 11:05:59.463594 140274135119616 logging_writer.py:48] [84] global_step=84, grad_norm=0.482429, loss=0.156917
I0316 11:05:59.466820 140316090979520 submission.py:265] 84) loss = 0.157, grad_norm = 0.482
I0316 11:06:01.080270 140274126726912 logging_writer.py:48] [85] global_step=85, grad_norm=0.409184, loss=0.153813
I0316 11:06:01.083669 140316090979520 submission.py:265] 85) loss = 0.154, grad_norm = 0.409
I0316 11:06:01.890979 140274135119616 logging_writer.py:48] [86] global_step=86, grad_norm=0.290277, loss=0.154453
I0316 11:06:01.894183 140316090979520 submission.py:265] 86) loss = 0.154, grad_norm = 0.290
I0316 11:06:03.221952 140274126726912 logging_writer.py:48] [87] global_step=87, grad_norm=0.190044, loss=0.151212
I0316 11:06:03.225033 140316090979520 submission.py:265] 87) loss = 0.151, grad_norm = 0.190
I0316 11:06:04.446835 140274135119616 logging_writer.py:48] [88] global_step=88, grad_norm=0.0656431, loss=0.151637
I0316 11:06:04.449903 140316090979520 submission.py:265] 88) loss = 0.152, grad_norm = 0.066
I0316 11:06:05.690948 140274126726912 logging_writer.py:48] [89] global_step=89, grad_norm=0.0728543, loss=0.149932
I0316 11:06:05.694068 140316090979520 submission.py:265] 89) loss = 0.150, grad_norm = 0.073
I0316 11:06:07.428194 140274135119616 logging_writer.py:48] [90] global_step=90, grad_norm=0.170075, loss=0.15003
I0316 11:06:07.431435 140316090979520 submission.py:265] 90) loss = 0.150, grad_norm = 0.170
I0316 11:06:08.144853 140274126726912 logging_writer.py:48] [91] global_step=91, grad_norm=0.240779, loss=0.148812
I0316 11:06:08.147931 140316090979520 submission.py:265] 91) loss = 0.149, grad_norm = 0.241
I0316 11:06:09.702329 140274135119616 logging_writer.py:48] [92] global_step=92, grad_norm=0.310877, loss=0.15053
I0316 11:06:09.705477 140316090979520 submission.py:265] 92) loss = 0.151, grad_norm = 0.311
I0316 11:06:10.482555 140274126726912 logging_writer.py:48] [93] global_step=93, grad_norm=0.3652, loss=0.152771
I0316 11:06:10.486360 140316090979520 submission.py:265] 93) loss = 0.153, grad_norm = 0.365
I0316 11:06:11.883499 140274135119616 logging_writer.py:48] [94] global_step=94, grad_norm=0.377732, loss=0.151661
I0316 11:06:11.886700 140316090979520 submission.py:265] 94) loss = 0.152, grad_norm = 0.378
I0316 11:06:13.105835 140274126726912 logging_writer.py:48] [95] global_step=95, grad_norm=0.32649, loss=0.140577
I0316 11:06:13.109100 140316090979520 submission.py:265] 95) loss = 0.141, grad_norm = 0.326
I0316 11:06:14.038188 140274135119616 logging_writer.py:48] [96] global_step=96, grad_norm=0.306163, loss=0.138355
I0316 11:06:14.041409 140316090979520 submission.py:265] 96) loss = 0.138, grad_norm = 0.306
I0316 11:06:15.132212 140274126726912 logging_writer.py:48] [97] global_step=97, grad_norm=0.267772, loss=0.136197
I0316 11:06:15.135343 140316090979520 submission.py:265] 97) loss = 0.136, grad_norm = 0.268
I0316 11:06:16.125760 140274135119616 logging_writer.py:48] [98] global_step=98, grad_norm=0.2145, loss=0.134948
I0316 11:06:16.128851 140316090979520 submission.py:265] 98) loss = 0.135, grad_norm = 0.214
I0316 11:06:19.908972 140274126726912 logging_writer.py:48] [99] global_step=99, grad_norm=0.17186, loss=0.136775
I0316 11:06:19.912380 140316090979520 submission.py:265] 99) loss = 0.137, grad_norm = 0.172
I0316 11:06:20.107349 140274135119616 logging_writer.py:48] [100] global_step=100, grad_norm=0.152038, loss=0.136246
I0316 11:06:20.111107 140316090979520 submission.py:265] 100) loss = 0.136, grad_norm = 0.152
I0316 11:06:47.959444 140316090979520 spec.py:321] Evaluating on the training split.
I0316 11:12:04.644621 140316090979520 spec.py:333] Evaluating on the validation split.
I0316 11:16:29.573018 140316090979520 spec.py:349] Evaluating on the test split.
I0316 11:21:40.213128 140316090979520 submission_runner.py:469] Time since start: 1975.09s, 	Step: 123, 	{'train/loss': 0.1336745599514896, 'validation/loss': 0.13410219117680397, 'validation/num_examples': 83274637, 'test/loss': 0.13706516503769725, 'test/num_examples': 95000000, 'score': 129.32037258148193, 'total_duration': 1975.090059041977, 'accumulated_submission_time': 129.32037258148193, 'accumulated_eval_time': 1844.3670508861542, 'accumulated_logging_time': 0.10162639617919922}
I0316 11:21:40.222849 140274126726912 logging_writer.py:48] [123] accumulated_eval_time=1844.37, accumulated_logging_time=0.101626, accumulated_submission_time=129.32, global_step=123, preemption_count=0, score=129.32, test/loss=0.137065, test/num_examples=95000000, total_duration=1975.09, train/loss=0.133675, validation/loss=0.134102, validation/num_examples=83274637
/usr/local/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
I0316 11:23:41.301998 140316090979520 spec.py:321] Evaluating on the training split.
I0316 11:28:48.486420 140316090979520 spec.py:333] Evaluating on the validation split.
I0316 11:33:12.959858 140316090979520 spec.py:349] Evaluating on the test split.
I0316 11:38:18.167954 140316090979520 submission_runner.py:469] Time since start: 2973.04s, 	Step: 243, 	{'train/loss': 0.12853585001803677, 'validation/loss': 0.1290671640119229, 'validation/num_examples': 83274637, 'test/loss': 0.13162405343764455, 'test/num_examples': 95000000, 'score': 249.47972059249878, 'total_duration': 2973.0449056625366, 'accumulated_submission_time': 249.47972059249878, 'accumulated_eval_time': 2721.2331125736237, 'accumulated_logging_time': 0.11838936805725098}
I0316 11:38:18.179166 140274135119616 logging_writer.py:48] [243] accumulated_eval_time=2721.23, accumulated_logging_time=0.118389, accumulated_submission_time=249.48, global_step=243, preemption_count=0, score=249.48, test/loss=0.131624, test/num_examples=95000000, total_duration=2973.04, train/loss=0.128536, validation/loss=0.129067, validation/num_examples=83274637
I0316 11:40:18.830324 140316090979520 spec.py:321] Evaluating on the training split.
I0316 11:45:27.547243 140316090979520 spec.py:333] Evaluating on the validation split.
I0316 11:49:56.242594 140316090979520 spec.py:349] Evaluating on the test split.
I0316 11:55:02.897570 140316090979520 submission_runner.py:469] Time since start: 3977.77s, 	Step: 365, 	{'train/loss': 0.1251490287833244, 'validation/loss': 0.1276909515587593, 'validation/num_examples': 83274637, 'test/loss': 0.13026014517059326, 'test/num_examples': 95000000, 'score': 369.2020356655121, 'total_duration': 3977.7744965553284, 'accumulated_submission_time': 369.2020356655121, 'accumulated_eval_time': 3605.3004066944122, 'accumulated_logging_time': 0.17025971412658691}
I0316 11:55:02.906841 140274126726912 logging_writer.py:48] [365] accumulated_eval_time=3605.3, accumulated_logging_time=0.17026, accumulated_submission_time=369.202, global_step=365, preemption_count=0, score=369.202, test/loss=0.13026, test/num_examples=95000000, total_duration=3977.77, train/loss=0.125149, validation/loss=0.127691, validation/num_examples=83274637
I0316 11:57:03.336657 140316090979520 spec.py:321] Evaluating on the training split.
I0316 12:02:17.397490 140316090979520 spec.py:333] Evaluating on the validation split.
I0316 12:06:50.464672 140316090979520 spec.py:349] Evaluating on the test split.
I0316 12:11:51.186951 140316090979520 submission_runner.py:469] Time since start: 4986.06s, 	Step: 492, 	{'train/loss': 0.12687969088847356, 'validation/loss': 0.12747136064057743, 'validation/num_examples': 83274637, 'test/loss': 0.12994236653402227, 'test/num_examples': 95000000, 'score': 488.7521860599518, 'total_duration': 4986.063929080963, 'accumulated_submission_time': 488.7521860599518, 'accumulated_eval_time': 4493.150989294052, 'accumulated_logging_time': 0.1867082118988037}
I0316 12:11:51.196497 140274135119616 logging_writer.py:48] [492] accumulated_eval_time=4493.15, accumulated_logging_time=0.186708, accumulated_submission_time=488.752, global_step=492, preemption_count=0, score=488.752, test/loss=0.129942, test/num_examples=95000000, total_duration=4986.06, train/loss=0.12688, validation/loss=0.127471, validation/num_examples=83274637
I0316 12:11:53.474960 140274126726912 logging_writer.py:48] [500] global_step=500, grad_norm=0.0154381, loss=0.125608
I0316 12:11:53.478350 140316090979520 submission.py:265] 500) loss = 0.126, grad_norm = 0.015
I0316 12:13:52.452806 140316090979520 spec.py:321] Evaluating on the training split.
I0316 12:18:56.988196 140316090979520 spec.py:333] Evaluating on the validation split.
I0316 12:23:22.345494 140316090979520 spec.py:349] Evaluating on the test split.
I0316 12:28:31.668434 140316090979520 submission_runner.py:469] Time since start: 5986.55s, 	Step: 615, 	{'train/loss': 0.1254724831503202, 'validation/loss': 0.12690448655203887, 'validation/num_examples': 83274637, 'test/loss': 0.12926871547076577, 'test/num_examples': 95000000, 'score': 609.1161625385284, 'total_duration': 5986.5454018116, 'accumulated_submission_time': 609.1161625385284, 'accumulated_eval_time': 5372.366828680038, 'accumulated_logging_time': 0.2030930519104004}
I0316 12:28:31.678467 140274135119616 logging_writer.py:48] [615] accumulated_eval_time=5372.37, accumulated_logging_time=0.203093, accumulated_submission_time=609.116, global_step=615, preemption_count=0, score=609.116, test/loss=0.129269, test/num_examples=95000000, total_duration=5986.55, train/loss=0.125472, validation/loss=0.126904, validation/num_examples=83274637
I0316 12:30:32.442160 140316090979520 spec.py:321] Evaluating on the training split.
I0316 12:35:35.943314 140316090979520 spec.py:333] Evaluating on the validation split.
I0316 12:40:07.715928 140316090979520 spec.py:349] Evaluating on the test split.
I0316 12:45:10.434630 140316090979520 submission_runner.py:469] Time since start: 6985.31s, 	Step: 739, 	{'train/loss': 0.12587102871595604, 'validation/loss': 0.12678134647278236, 'validation/num_examples': 83274637, 'test/loss': 0.12911691553906893, 'test/num_examples': 95000000, 'score': 729.0497987270355, 'total_duration': 6985.311563253403, 'accumulated_submission_time': 729.0497987270355, 'accumulated_eval_time': 6250.359308242798, 'accumulated_logging_time': 0.22115182876586914}
I0316 12:45:10.444287 140274126726912 logging_writer.py:48] [739] accumulated_eval_time=6250.36, accumulated_logging_time=0.221152, accumulated_submission_time=729.05, global_step=739, preemption_count=0, score=729.05, test/loss=0.129117, test/num_examples=95000000, total_duration=6985.31, train/loss=0.125871, validation/loss=0.126781, validation/num_examples=83274637
I0316 12:47:11.499584 140316090979520 spec.py:321] Evaluating on the training split.
I0316 12:52:20.277642 140316090979520 spec.py:333] Evaluating on the validation split.
I0316 12:56:42.490576 140316090979520 spec.py:349] Evaluating on the test split.
I0316 13:01:50.841063 140316090979520 submission_runner.py:469] Time since start: 7985.72s, 	Step: 862, 	{'train/loss': 0.12525209514327096, 'validation/loss': 0.12654671168370157, 'validation/num_examples': 83274637, 'test/loss': 0.12898656450926127, 'test/num_examples': 95000000, 'score': 849.2402403354645, 'total_duration': 7985.717970132828, 'accumulated_submission_time': 849.2402403354645, 'accumulated_eval_time': 7129.700910329819, 'accumulated_logging_time': 0.23888373374938965}
I0316 13:01:50.850621 140274135119616 logging_writer.py:48] [862] accumulated_eval_time=7129.7, accumulated_logging_time=0.238884, accumulated_submission_time=849.24, global_step=862, preemption_count=0, score=849.24, test/loss=0.128987, test/num_examples=95000000, total_duration=7985.72, train/loss=0.125252, validation/loss=0.126547, validation/num_examples=83274637
I0316 13:03:51.831167 140316090979520 spec.py:321] Evaluating on the training split.
I0316 13:08:51.765859 140316090979520 spec.py:333] Evaluating on the validation split.
I0316 13:13:03.148533 140316090979520 spec.py:349] Evaluating on the test split.
I0316 13:17:55.755367 140316090979520 submission_runner.py:469] Time since start: 8950.63s, 	Step: 982, 	{'train/loss': 0.12360756684209365, 'validation/loss': 0.1261834019488302, 'validation/num_examples': 83274637, 'test/loss': 0.1286987404245477, 'test/num_examples': 95000000, 'score': 969.3288962841034, 'total_duration': 8950.632301568985, 'accumulated_submission_time': 969.3288962841034, 'accumulated_eval_time': 7973.625199079514, 'accumulated_logging_time': 0.2551608085632324}
I0316 13:17:55.764763 140274126726912 logging_writer.py:48] [982] accumulated_eval_time=7973.63, accumulated_logging_time=0.255161, accumulated_submission_time=969.329, global_step=982, preemption_count=0, score=969.329, test/loss=0.128699, test/num_examples=95000000, total_duration=8950.63, train/loss=0.123608, validation/loss=0.126183, validation/num_examples=83274637
I0316 13:17:59.912123 140274135119616 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.0158807, loss=0.125313
I0316 13:17:59.915354 140316090979520 submission.py:265] 1000) loss = 0.125, grad_norm = 0.016
I0316 13:19:57.159081 140316090979520 spec.py:321] Evaluating on the training split.
I0316 13:24:45.970797 140316090979520 spec.py:333] Evaluating on the validation split.
I0316 13:28:54.019916 140316090979520 spec.py:349] Evaluating on the test split.
I0316 13:33:37.414936 140316090979520 submission_runner.py:469] Time since start: 9892.29s, 	Step: 1107, 	{'train/loss': 0.12320177029305639, 'validation/loss': 0.1260728514303284, 'validation/num_examples': 83274637, 'test/loss': 0.12855530603288348, 'test/num_examples': 95000000, 'score': 1089.8160667419434, 'total_duration': 9892.291828870773, 'accumulated_submission_time': 1089.8160667419434, 'accumulated_eval_time': 8793.881187677383, 'accumulated_logging_time': 0.27149128913879395}
I0316 13:33:37.424459 140274126726912 logging_writer.py:48] [1107] accumulated_eval_time=8793.88, accumulated_logging_time=0.271491, accumulated_submission_time=1089.82, global_step=1107, preemption_count=0, score=1089.82, test/loss=0.128555, test/num_examples=95000000, total_duration=9892.29, train/loss=0.123202, validation/loss=0.126073, validation/num_examples=83274637
I0316 13:35:38.200170 140316090979520 spec.py:321] Evaluating on the training split.
I0316 13:40:19.547592 140316090979520 spec.py:333] Evaluating on the validation split.
I0316 13:43:53.561484 140316090979520 spec.py:349] Evaluating on the test split.
I0316 13:48:52.909474 140316090979520 submission_runner.py:469] Time since start: 10807.79s, 	Step: 1230, 	{'train/loss': 0.12491417499368132, 'validation/loss': 0.125767355947103, 'validation/num_examples': 83274637, 'test/loss': 0.12818754992218018, 'test/num_examples': 95000000, 'score': 1209.6269445419312, 'total_duration': 10807.786437034607, 'accumulated_submission_time': 1209.6269445419312, 'accumulated_eval_time': 9588.590577363968, 'accumulated_logging_time': 0.35022616386413574}
I0316 13:48:52.919670 140274135119616 logging_writer.py:48] [1230] accumulated_eval_time=9588.59, accumulated_logging_time=0.350226, accumulated_submission_time=1209.63, global_step=1230, preemption_count=0, score=1209.63, test/loss=0.128188, test/num_examples=95000000, total_duration=10807.8, train/loss=0.124914, validation/loss=0.125767, validation/num_examples=83274637
I0316 13:50:53.364793 140316090979520 spec.py:321] Evaluating on the training split.
I0316 13:55:01.207251 140316090979520 spec.py:333] Evaluating on the validation split.
I0316 13:57:50.722439 140316090979520 spec.py:349] Evaluating on the test split.
I0316 14:02:43.418524 140316090979520 submission_runner.py:469] Time since start: 11638.30s, 	Step: 1354, 	{'train/loss': 0.12436340965553949, 'validation/loss': 0.12568193117219675, 'validation/num_examples': 83274637, 'test/loss': 0.12797348085471705, 'test/num_examples': 95000000, 'score': 1329.2002420425415, 'total_duration': 11638.295469760895, 'accumulated_submission_time': 1329.2002420425415, 'accumulated_eval_time': 10298.64454627037, 'accumulated_logging_time': 0.3673419952392578}
I0316 14:02:43.429987 140274126726912 logging_writer.py:48] [1354] accumulated_eval_time=10298.6, accumulated_logging_time=0.367342, accumulated_submission_time=1329.2, global_step=1354, preemption_count=0, score=1329.2, test/loss=0.127973, test/num_examples=95000000, total_duration=11638.3, train/loss=0.124363, validation/loss=0.125682, validation/num_examples=83274637
I0316 14:04:44.590112 140316090979520 spec.py:321] Evaluating on the training split.
I0316 14:07:50.522568 140316090979520 spec.py:333] Evaluating on the validation split.
I0316 14:10:46.815046 140316090979520 spec.py:349] Evaluating on the test split.
I0316 14:15:42.584083 140316090979520 submission_runner.py:469] Time since start: 12417.46s, 	Step: 1472, 	{'train/loss': 0.12431990869619765, 'validation/loss': 0.12579194413289782, 'validation/num_examples': 83274637, 'test/loss': 0.12843951437221326, 'test/num_examples': 95000000, 'score': 1449.4603264331818, 'total_duration': 12417.460972547531, 'accumulated_submission_time': 1449.4603264331818, 'accumulated_eval_time': 10956.638572692871, 'accumulated_logging_time': 0.3860495090484619}
I0316 14:15:42.595473 140274135119616 logging_writer.py:48] [1472] accumulated_eval_time=10956.6, accumulated_logging_time=0.38605, accumulated_submission_time=1449.46, global_step=1472, preemption_count=0, score=1449.46, test/loss=0.12844, test/num_examples=95000000, total_duration=12417.5, train/loss=0.12432, validation/loss=0.125792, validation/num_examples=83274637
I0316 14:15:48.955628 140274126726912 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.0346522, loss=0.132664
I0316 14:15:48.959397 140316090979520 submission.py:265] 1500) loss = 0.133, grad_norm = 0.035
I0316 14:17:43.151319 140316090979520 spec.py:321] Evaluating on the training split.
I0316 14:19:46.829504 140316090979520 spec.py:333] Evaluating on the validation split.
I0316 14:22:41.966017 140316090979520 spec.py:349] Evaluating on the test split.
I0316 14:27:34.998041 140316090979520 submission_runner.py:469] Time since start: 13129.88s, 	Step: 1593, 	{'train/loss': 0.12573929438983114, 'validation/loss': 0.12562391440594492, 'validation/num_examples': 83274637, 'test/loss': 0.12815064461091694, 'test/num_examples': 95000000, 'score': 1569.1198790073395, 'total_duration': 13129.875000476837, 'accumulated_submission_time': 1569.1198790073395, 'accumulated_eval_time': 11548.485477924347, 'accumulated_logging_time': 0.4048593044281006}
I0316 14:27:35.007668 140274135119616 logging_writer.py:48] [1593] accumulated_eval_time=11548.5, accumulated_logging_time=0.404859, accumulated_submission_time=1569.12, global_step=1593, preemption_count=0, score=1569.12, test/loss=0.128151, test/num_examples=95000000, total_duration=13129.9, train/loss=0.125739, validation/loss=0.125624, validation/num_examples=83274637
I0316 14:29:35.776090 140316090979520 spec.py:321] Evaluating on the training split.
I0316 14:31:40.535608 140316090979520 spec.py:333] Evaluating on the validation split.
I0316 14:34:37.071100 140316090979520 spec.py:349] Evaluating on the test split.
I0316 14:39:44.485993 140316090979520 submission_runner.py:469] Time since start: 13859.36s, 	Step: 1715, 	{'train/loss': 0.12510872138593584, 'validation/loss': 0.12555031365922348, 'validation/num_examples': 83274637, 'test/loss': 0.1280176452840303, 'test/num_examples': 95000000, 'score': 1689.012228012085, 'total_duration': 13859.362963676453, 'accumulated_submission_time': 1689.012228012085, 'accumulated_eval_time': 12157.19554901123, 'accumulated_logging_time': 0.4211466312408447}
I0316 14:39:44.496381 140274126726912 logging_writer.py:48] [1715] accumulated_eval_time=12157.2, accumulated_logging_time=0.421147, accumulated_submission_time=1689.01, global_step=1715, preemption_count=0, score=1689.01, test/loss=0.128018, test/num_examples=95000000, total_duration=13859.4, train/loss=0.125109, validation/loss=0.12555, validation/num_examples=83274637
I0316 14:41:46.448355 140316090979520 spec.py:321] Evaluating on the training split.
I0316 14:43:49.988455 140316090979520 spec.py:333] Evaluating on the validation split.
I0316 14:46:45.375429 140316090979520 spec.py:349] Evaluating on the test split.
I0316 14:51:42.915210 140316090979520 submission_runner.py:469] Time since start: 14577.79s, 	Step: 1837, 	{'train/loss': 0.12324692388352819, 'validation/loss': 0.12541339006992477, 'validation/num_examples': 83274637, 'test/loss': 0.1277747086371171, 'test/num_examples': 95000000, 'score': 1810.0823884010315, 'total_duration': 14577.792183160782, 'accumulated_submission_time': 1810.0823884010315, 'accumulated_eval_time': 12753.662537574768, 'accumulated_logging_time': 0.4382638931274414}
I0316 14:51:42.925974 140274135119616 logging_writer.py:48] [1837] accumulated_eval_time=12753.7, accumulated_logging_time=0.438264, accumulated_submission_time=1810.08, global_step=1837, preemption_count=0, score=1810.08, test/loss=0.127775, test/num_examples=95000000, total_duration=14577.8, train/loss=0.123247, validation/loss=0.125413, validation/num_examples=83274637
I0316 14:53:43.756413 140316090979520 spec.py:321] Evaluating on the training split.
I0316 14:55:47.500012 140316090979520 spec.py:333] Evaluating on the validation split.
I0316 14:58:43.887278 140316090979520 spec.py:349] Evaluating on the test split.
I0316 15:03:38.030792 140316090979520 submission_runner.py:469] Time since start: 15292.91s, 	Step: 1962, 	{'train/loss': 0.123373683914872, 'validation/loss': 0.12549998279694066, 'validation/num_examples': 83274637, 'test/loss': 0.127937661266608, 'test/num_examples': 95000000, 'score': 1930.0053222179413, 'total_duration': 15292.907705783844, 'accumulated_submission_time': 1930.0053222179413, 'accumulated_eval_time': 13347.936957597733, 'accumulated_logging_time': 0.4560530185699463}
I0316 15:03:38.041374 140274126726912 logging_writer.py:48] [1962] accumulated_eval_time=13347.9, accumulated_logging_time=0.456053, accumulated_submission_time=1930.01, global_step=1962, preemption_count=0, score=1930.01, test/loss=0.127938, test/num_examples=95000000, total_duration=15292.9, train/loss=0.123374, validation/loss=0.1255, validation/num_examples=83274637
I0316 15:03:53.343392 140274135119616 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.0177436, loss=0.124303
I0316 15:03:53.347428 140316090979520 submission.py:265] 2000) loss = 0.124, grad_norm = 0.018
I0316 15:05:39.445957 140316090979520 spec.py:321] Evaluating on the training split.
I0316 15:07:43.139848 140316090979520 spec.py:333] Evaluating on the validation split.
I0316 15:10:40.921453 140316090979520 spec.py:349] Evaluating on the test split.
I0316 15:15:34.565075 140316090979520 submission_runner.py:469] Time since start: 16009.44s, 	Step: 2087, 	{'train/loss': 0.12530907141806027, 'validation/loss': 0.12532904585076166, 'validation/num_examples': 83274637, 'test/loss': 0.12776454719013414, 'test/num_examples': 95000000, 'score': 2050.553661108017, 'total_duration': 16009.442063808441, 'accumulated_submission_time': 2050.553661108017, 'accumulated_eval_time': 13943.056151866913, 'accumulated_logging_time': 0.4734678268432617}
I0316 15:15:34.640082 140274126726912 logging_writer.py:48] [2087] accumulated_eval_time=13943.1, accumulated_logging_time=0.473468, accumulated_submission_time=2050.55, global_step=2087, preemption_count=0, score=2050.55, test/loss=0.127765, test/num_examples=95000000, total_duration=16009.4, train/loss=0.125309, validation/loss=0.125329, validation/num_examples=83274637
I0316 15:17:36.087288 140316090979520 spec.py:321] Evaluating on the training split.
I0316 15:19:39.538631 140316090979520 spec.py:333] Evaluating on the validation split.
I0316 15:22:33.415861 140316090979520 spec.py:349] Evaluating on the test split.
I0316 15:27:29.261049 140316090979520 submission_runner.py:469] Time since start: 16724.14s, 	Step: 2209, 	{'train/loss': 0.12281970480981333, 'validation/loss': 0.12515759089254677, 'validation/num_examples': 83274637, 'test/loss': 0.1275742136479428, 'test/num_examples': 95000000, 'score': 2171.174302339554, 'total_duration': 16724.13800740242, 'accumulated_submission_time': 2171.174302339554, 'accumulated_eval_time': 14536.2299721241, 'accumulated_logging_time': 0.5555760860443115}
I0316 15:27:29.271393 140274135119616 logging_writer.py:48] [2209] accumulated_eval_time=14536.2, accumulated_logging_time=0.555576, accumulated_submission_time=2171.17, global_step=2209, preemption_count=0, score=2171.17, test/loss=0.127574, test/num_examples=95000000, total_duration=16724.1, train/loss=0.12282, validation/loss=0.125158, validation/num_examples=83274637
I0316 15:29:30.105620 140316090979520 spec.py:321] Evaluating on the training split.
I0316 15:31:33.406492 140316090979520 spec.py:333] Evaluating on the validation split.
I0316 15:34:22.489091 140316090979520 spec.py:349] Evaluating on the test split.
I0316 15:39:12.961635 140316090979520 submission_runner.py:469] Time since start: 17427.84s, 	Step: 2328, 	{'train/loss': 0.12590030080312872, 'validation/loss': 0.12496603486550097, 'validation/num_examples': 83274637, 'test/loss': 0.1273209322377255, 'test/num_examples': 95000000, 'score': 2291.155768632889, 'total_duration': 17427.838596343994, 'accumulated_submission_time': 2291.155768632889, 'accumulated_eval_time': 15119.086103439331, 'accumulated_logging_time': 0.5724442005157471}
I0316 15:39:12.971429 140274126726912 logging_writer.py:48] [2328] accumulated_eval_time=15119.1, accumulated_logging_time=0.572444, accumulated_submission_time=2291.16, global_step=2328, preemption_count=0, score=2291.16, test/loss=0.127321, test/num_examples=95000000, total_duration=17427.8, train/loss=0.1259, validation/loss=0.124966, validation/num_examples=83274637
I0316 15:41:13.728308 140316090979520 spec.py:321] Evaluating on the training split.
I0316 15:43:17.147982 140316090979520 spec.py:333] Evaluating on the validation split.
I0316 15:46:14.966910 140316090979520 spec.py:349] Evaluating on the test split.
I0316 15:51:07.234143 140316090979520 submission_runner.py:469] Time since start: 18142.11s, 	Step: 2446, 	{'train/loss': 0.12238951843294579, 'validation/loss': 0.1251013117454916, 'validation/num_examples': 83274637, 'test/loss': 0.12744433217267487, 'test/num_examples': 95000000, 'score': 2411.025289297104, 'total_duration': 18142.11105132103, 'accumulated_submission_time': 2411.025289297104, 'accumulated_eval_time': 15712.592066287994, 'accumulated_logging_time': 0.5888779163360596}
I0316 15:51:07.244174 140274135119616 logging_writer.py:48] [2446] accumulated_eval_time=15712.6, accumulated_logging_time=0.588878, accumulated_submission_time=2411.03, global_step=2446, preemption_count=0, score=2411.03, test/loss=0.127444, test/num_examples=95000000, total_duration=18142.1, train/loss=0.12239, validation/loss=0.125101, validation/num_examples=83274637
I0316 15:51:45.156039 140274126726912 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.0113397, loss=0.116742
I0316 15:51:45.159424 140316090979520 submission.py:265] 2500) loss = 0.117, grad_norm = 0.011
I0316 15:53:08.169787 140316090979520 spec.py:321] Evaluating on the training split.
I0316 15:55:11.725249 140316090979520 spec.py:333] Evaluating on the validation split.
I0316 15:58:06.072400 140316090979520 spec.py:349] Evaluating on the test split.
I0316 16:03:01.339112 140316090979520 submission_runner.py:469] Time since start: 18856.22s, 	Step: 2567, 	{'train/loss': 0.12313673958834975, 'validation/loss': 0.12490526938013705, 'validation/num_examples': 83274637, 'test/loss': 0.12726186002791556, 'test/num_examples': 95000000, 'score': 2531.083735227585, 'total_duration': 18856.21607899666, 'accumulated_submission_time': 2531.083735227585, 'accumulated_eval_time': 16305.761589050293, 'accumulated_logging_time': 0.6052801609039307}
I0316 16:03:01.349280 140274135119616 logging_writer.py:48] [2567] accumulated_eval_time=16305.8, accumulated_logging_time=0.60528, accumulated_submission_time=2531.08, global_step=2567, preemption_count=0, score=2531.08, test/loss=0.127262, test/num_examples=95000000, total_duration=18856.2, train/loss=0.123137, validation/loss=0.124905, validation/num_examples=83274637
I0316 16:05:01.861298 140316090979520 spec.py:321] Evaluating on the training split.
I0316 16:07:05.501137 140316090979520 spec.py:333] Evaluating on the validation split.
I0316 16:09:58.077943 140316090979520 spec.py:349] Evaluating on the test split.
I0316 16:14:55.368748 140316090979520 submission_runner.py:469] Time since start: 19570.25s, 	Step: 2686, 	{'train/loss': 0.1229286024180856, 'validation/loss': 0.12498980783715645, 'validation/num_examples': 83274637, 'test/loss': 0.12733083697682432, 'test/num_examples': 95000000, 'score': 2650.7251060009003, 'total_duration': 19570.245700120926, 'accumulated_submission_time': 2650.7251060009003, 'accumulated_eval_time': 16899.269127368927, 'accumulated_logging_time': 0.6533441543579102}
I0316 16:14:55.380481 140274126726912 logging_writer.py:48] [2686] accumulated_eval_time=16899.3, accumulated_logging_time=0.653344, accumulated_submission_time=2650.73, global_step=2686, preemption_count=0, score=2650.73, test/loss=0.127331, test/num_examples=95000000, total_duration=19570.2, train/loss=0.122929, validation/loss=0.12499, validation/num_examples=83274637
I0316 16:16:56.162941 140316090979520 spec.py:321] Evaluating on the training split.
I0316 16:18:59.752226 140316090979520 spec.py:333] Evaluating on the validation split.
I0316 16:21:56.129409 140316090979520 spec.py:349] Evaluating on the test split.
I0316 16:26:54.979205 140316090979520 submission_runner.py:469] Time since start: 20289.86s, 	Step: 2807, 	{'train/loss': 0.12423641924512176, 'validation/loss': 0.1249243081619366, 'validation/num_examples': 83274637, 'test/loss': 0.12727789703489606, 'test/num_examples': 95000000, 'score': 2770.5773775577545, 'total_duration': 20289.85614323616, 'accumulated_submission_time': 2770.5773775577545, 'accumulated_eval_time': 17498.085548877716, 'accumulated_logging_time': 0.6740763187408447}
I0316 16:26:54.990619 140274135119616 logging_writer.py:48] [2807] accumulated_eval_time=17498.1, accumulated_logging_time=0.674076, accumulated_submission_time=2770.58, global_step=2807, preemption_count=0, score=2770.58, test/loss=0.127278, test/num_examples=95000000, total_duration=20289.9, train/loss=0.124236, validation/loss=0.124924, validation/num_examples=83274637
I0316 16:28:56.337648 140316090979520 spec.py:321] Evaluating on the training split.
I0316 16:31:00.471742 140316090979520 spec.py:333] Evaluating on the validation split.
I0316 16:33:53.358361 140316090979520 spec.py:349] Evaluating on the test split.
I0316 16:38:44.790391 140316090979520 submission_runner.py:469] Time since start: 20999.67s, 	Step: 2925, 	{'train/loss': 0.12339795641703691, 'validation/loss': 0.12475231609553267, 'validation/num_examples': 83274637, 'test/loss': 0.1270237113834582, 'test/num_examples': 95000000, 'score': 2891.0601370334625, 'total_duration': 20999.667350053787, 'accumulated_submission_time': 2891.0601370334625, 'accumulated_eval_time': 18086.53838968277, 'accumulated_logging_time': 0.6929118633270264}
I0316 16:38:44.801103 140274126726912 logging_writer.py:48] [2925] accumulated_eval_time=18086.5, accumulated_logging_time=0.692912, accumulated_submission_time=2891.06, global_step=2925, preemption_count=0, score=2891.06, test/loss=0.127024, test/num_examples=95000000, total_duration=20999.7, train/loss=0.123398, validation/loss=0.124752, validation/num_examples=83274637
I0316 16:39:55.115153 140274135119616 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.00981869, loss=0.126979
I0316 16:39:55.118364 140316090979520 submission.py:265] 3000) loss = 0.127, grad_norm = 0.010
I0316 16:40:45.469815 140316090979520 spec.py:321] Evaluating on the training split.
I0316 16:42:48.930422 140316090979520 spec.py:333] Evaluating on the validation split.
I0316 16:45:44.473191 140316090979520 spec.py:349] Evaluating on the test split.
I0316 16:50:40.419764 140316090979520 submission_runner.py:469] Time since start: 21715.30s, 	Step: 3040, 	{'train/loss': 0.12548221487982686, 'validation/loss': 0.12464914671662411, 'validation/num_examples': 83274637, 'test/loss': 0.12690119336616115, 'test/num_examples': 95000000, 'score': 3010.8247060775757, 'total_duration': 21715.29672884941, 'accumulated_submission_time': 3010.8247060775757, 'accumulated_eval_time': 18681.488548994064, 'accumulated_logging_time': 0.710813045501709}
I0316 16:50:40.430112 140274126726912 logging_writer.py:48] [3040] accumulated_eval_time=18681.5, accumulated_logging_time=0.710813, accumulated_submission_time=3010.82, global_step=3040, preemption_count=0, score=3010.82, test/loss=0.126901, test/num_examples=95000000, total_duration=21715.3, train/loss=0.125482, validation/loss=0.124649, validation/num_examples=83274637
I0316 16:52:41.336256 140316090979520 spec.py:321] Evaluating on the training split.
I0316 16:54:44.730104 140316090979520 spec.py:333] Evaluating on the validation split.
I0316 16:57:36.675588 140316090979520 spec.py:349] Evaluating on the test split.
I0316 17:02:33.376811 140316090979520 submission_runner.py:469] Time since start: 22428.25s, 	Step: 3160, 	{'train/loss': 0.12487915662666421, 'validation/loss': 0.12456766276283851, 'validation/num_examples': 83274637, 'test/loss': 0.12691639732943083, 'test/num_examples': 95000000, 'score': 3130.8704998493195, 'total_duration': 22428.253749608994, 'accumulated_submission_time': 3130.8704998493195, 'accumulated_eval_time': 19273.529145002365, 'accumulated_logging_time': 0.7279255390167236}
I0316 17:02:33.386923 140274135119616 logging_writer.py:48] [3160] accumulated_eval_time=19273.5, accumulated_logging_time=0.727926, accumulated_submission_time=3130.87, global_step=3160, preemption_count=0, score=3130.87, test/loss=0.126916, test/num_examples=95000000, total_duration=22428.3, train/loss=0.124879, validation/loss=0.124568, validation/num_examples=83274637
I0316 17:04:35.152820 140316090979520 spec.py:321] Evaluating on the training split.
I0316 17:06:38.425271 140316090979520 spec.py:333] Evaluating on the validation split.
I0316 17:09:32.602034 140316090979520 spec.py:349] Evaluating on the test split.
I0316 17:14:19.333761 140316090979520 submission_runner.py:469] Time since start: 23134.21s, 	Step: 3281, 	{'train/loss': 0.12201861854971148, 'validation/loss': 0.12456298143683192, 'validation/num_examples': 83274637, 'test/loss': 0.12688183504775197, 'test/num_examples': 95000000, 'score': 3251.7499673366547, 'total_duration': 23134.210708141327, 'accumulated_submission_time': 3251.7499673366547, 'accumulated_eval_time': 19857.710265636444, 'accumulated_logging_time': 0.7697329521179199}
I0316 17:14:19.343939 140274126726912 logging_writer.py:48] [3281] accumulated_eval_time=19857.7, accumulated_logging_time=0.769733, accumulated_submission_time=3251.75, global_step=3281, preemption_count=0, score=3251.75, test/loss=0.126882, test/num_examples=95000000, total_duration=23134.2, train/loss=0.122019, validation/loss=0.124563, validation/num_examples=83274637
I0316 17:16:20.651423 140316090979520 spec.py:321] Evaluating on the training split.
I0316 17:18:24.376891 140316090979520 spec.py:333] Evaluating on the validation split.
I0316 17:21:18.464675 140316090979520 spec.py:349] Evaluating on the test split.
I0316 17:26:07.723792 140316090979520 submission_runner.py:469] Time since start: 23842.60s, 	Step: 3404, 	{'train/loss': 0.12220520174235967, 'validation/loss': 0.12466059268671235, 'validation/num_examples': 83274637, 'test/loss': 0.1269929662708885, 'test/num_examples': 95000000, 'score': 3372.1971917152405, 'total_duration': 23842.600779771805, 'accumulated_submission_time': 3372.1971917152405, 'accumulated_eval_time': 20444.78269481659, 'accumulated_logging_time': 0.7867560386657715}
I0316 17:26:07.734732 140274135119616 logging_writer.py:48] [3404] accumulated_eval_time=20444.8, accumulated_logging_time=0.786756, accumulated_submission_time=3372.2, global_step=3404, preemption_count=0, score=3372.2, test/loss=0.126993, test/num_examples=95000000, total_duration=23842.6, train/loss=0.122205, validation/loss=0.124661, validation/num_examples=83274637
I0316 17:27:42.383753 140274126726912 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.0109217, loss=0.119137
I0316 17:27:42.387485 140316090979520 submission.py:265] 3500) loss = 0.119, grad_norm = 0.011
I0316 17:28:09.439446 140316090979520 spec.py:321] Evaluating on the training split.
I0316 17:30:12.959253 140316090979520 spec.py:333] Evaluating on the validation split.
I0316 17:33:07.351475 140316090979520 spec.py:349] Evaluating on the test split.
I0316 17:37:55.443675 140316090979520 submission_runner.py:469] Time since start: 24550.32s, 	Step: 3520, 	{'train/loss': 0.12193732728544764, 'validation/loss': 0.12468958312820012, 'validation/num_examples': 83274637, 'test/loss': 0.1271003201581453, 'test/num_examples': 95000000, 'score': 3493.0354900360107, 'total_duration': 24550.320616960526, 'accumulated_submission_time': 3493.0354900360107, 'accumulated_eval_time': 21030.787079572678, 'accumulated_logging_time': 0.804595947265625}
I0316 17:37:55.454278 140274135119616 logging_writer.py:48] [3520] accumulated_eval_time=21030.8, accumulated_logging_time=0.804596, accumulated_submission_time=3493.04, global_step=3520, preemption_count=0, score=3493.04, test/loss=0.1271, test/num_examples=95000000, total_duration=24550.3, train/loss=0.121937, validation/loss=0.12469, validation/num_examples=83274637
I0316 17:39:56.702737 140316090979520 spec.py:321] Evaluating on the training split.
I0316 17:42:00.484954 140316090979520 spec.py:333] Evaluating on the validation split.
I0316 17:44:54.518046 140316090979520 spec.py:349] Evaluating on the test split.
I0316 17:49:54.613885 140316090979520 submission_runner.py:469] Time since start: 25269.49s, 	Step: 3638, 	{'train/loss': 0.12348222200423091, 'validation/loss': 0.12477326875605435, 'validation/num_examples': 83274637, 'test/loss': 0.12709243507284868, 'test/num_examples': 95000000, 'score': 3613.4135015010834, 'total_duration': 25269.490796804428, 'accumulated_submission_time': 3613.4135015010834, 'accumulated_eval_time': 21628.69826388359, 'accumulated_logging_time': 0.8218832015991211}
I0316 17:49:54.625222 140274126726912 logging_writer.py:48] [3638] accumulated_eval_time=21628.7, accumulated_logging_time=0.821883, accumulated_submission_time=3613.41, global_step=3638, preemption_count=0, score=3613.41, test/loss=0.127092, test/num_examples=95000000, total_duration=25269.5, train/loss=0.123482, validation/loss=0.124773, validation/num_examples=83274637
I0316 17:51:55.923277 140316090979520 spec.py:321] Evaluating on the training split.
I0316 17:54:00.121123 140316090979520 spec.py:333] Evaluating on the validation split.
I0316 17:56:56.899279 140316090979520 spec.py:349] Evaluating on the test split.
I0316 18:01:47.415744 140316090979520 submission_runner.py:469] Time since start: 25982.29s, 	Step: 3760, 	{'train/loss': 0.12351843513645117, 'validation/loss': 0.12473606955086308, 'validation/num_examples': 83274637, 'test/loss': 0.12707489401823344, 'test/num_examples': 95000000, 'score': 3733.831976890564, 'total_duration': 25982.292655467987, 'accumulated_submission_time': 3733.831976890564, 'accumulated_eval_time': 22220.19090819359, 'accumulated_logging_time': 0.8633899688720703}
I0316 18:01:47.426380 140274135119616 logging_writer.py:48] [3760] accumulated_eval_time=22220.2, accumulated_logging_time=0.86339, accumulated_submission_time=3733.83, global_step=3760, preemption_count=0, score=3733.83, test/loss=0.127075, test/num_examples=95000000, total_duration=25982.3, train/loss=0.123518, validation/loss=0.124736, validation/num_examples=83274637
I0316 18:03:47.865401 140316090979520 spec.py:321] Evaluating on the training split.
I0316 18:05:51.324192 140316090979520 spec.py:333] Evaluating on the validation split.
I0316 18:08:46.338972 140316090979520 spec.py:349] Evaluating on the test split.
I0316 18:13:41.886390 140316090979520 submission_runner.py:469] Time since start: 26696.76s, 	Step: 3879, 	{'train/loss': 0.12400023174951648, 'validation/loss': 0.12454301445055628, 'validation/num_examples': 83274637, 'test/loss': 0.12688940321872108, 'test/num_examples': 95000000, 'score': 3853.4002549648285, 'total_duration': 26696.76336503029, 'accumulated_submission_time': 3853.4002549648285, 'accumulated_eval_time': 22814.211965322495, 'accumulated_logging_time': 0.8812885284423828}
I0316 18:13:41.897274 140274126726912 logging_writer.py:48] [3879] accumulated_eval_time=22814.2, accumulated_logging_time=0.881289, accumulated_submission_time=3853.4, global_step=3879, preemption_count=0, score=3853.4, test/loss=0.126889, test/num_examples=95000000, total_duration=26696.8, train/loss=0.124, validation/loss=0.124543, validation/num_examples=83274637
I0316 18:15:43.441685 140316090979520 spec.py:321] Evaluating on the training split.
I0316 18:17:47.130643 140316090979520 spec.py:333] Evaluating on the validation split.
I0316 18:20:40.789305 140316090979520 spec.py:349] Evaluating on the test split.
I0316 18:25:33.519228 140316090979520 submission_runner.py:469] Time since start: 27408.40s, 	Step: 4000, 	{'train/loss': 0.12321602280818574, 'validation/loss': 0.12470204006925245, 'validation/num_examples': 83274637, 'test/loss': 0.127063418793086, 'test/num_examples': 95000000, 'score': 3974.107750415802, 'total_duration': 27408.39618563652, 'accumulated_submission_time': 3974.107750415802, 'accumulated_eval_time': 23404.28955936432, 'accumulated_logging_time': 0.9000418186187744}
I0316 18:25:33.529832 140274135119616 logging_writer.py:48] [4000] accumulated_eval_time=23404.3, accumulated_logging_time=0.900042, accumulated_submission_time=3974.11, global_step=4000, preemption_count=0, score=3974.11, test/loss=0.127063, test/num_examples=95000000, total_duration=27408.4, train/loss=0.123216, validation/loss=0.124702, validation/num_examples=83274637
I0316 18:25:34.155772 140274126726912 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.0137573, loss=0.120846
I0316 18:25:34.160003 140316090979520 submission.py:265] 4000) loss = 0.121, grad_norm = 0.014
I0316 18:27:34.894354 140316090979520 spec.py:321] Evaluating on the training split.
I0316 18:29:38.464357 140316090979520 spec.py:333] Evaluating on the validation split.
I0316 18:32:34.760784 140316090979520 spec.py:349] Evaluating on the test split.
I0316 18:37:30.543337 140316090979520 submission_runner.py:469] Time since start: 28125.42s, 	Step: 4123, 	{'train/loss': 0.12494646464995769, 'validation/loss': 0.12454506671894455, 'validation/num_examples': 83274637, 'test/loss': 0.1269056560419183, 'test/num_examples': 95000000, 'score': 4094.5785994529724, 'total_duration': 28125.420327186584, 'accumulated_submission_time': 4094.5785994529724, 'accumulated_eval_time': 23999.93874502182, 'accumulated_logging_time': 0.9177372455596924}
I0316 18:37:30.554112 140274135119616 logging_writer.py:48] [4123] accumulated_eval_time=23999.9, accumulated_logging_time=0.917737, accumulated_submission_time=4094.58, global_step=4123, preemption_count=0, score=4094.58, test/loss=0.126906, test/num_examples=95000000, total_duration=28125.4, train/loss=0.124946, validation/loss=0.124545, validation/num_examples=83274637
I0316 18:39:31.078289 140316090979520 spec.py:321] Evaluating on the training split.
I0316 18:41:34.464720 140316090979520 spec.py:333] Evaluating on the validation split.
I0316 18:44:30.335228 140316090979520 spec.py:349] Evaluating on the test split.
I0316 18:49:27.539144 140316090979520 submission_runner.py:469] Time since start: 28842.42s, 	Step: 4243, 	{'train/loss': 0.1236824732960537, 'validation/loss': 0.12434002573242871, 'validation/num_examples': 83274637, 'test/loss': 0.12669102755656994, 'test/num_examples': 95000000, 'score': 4214.180819511414, 'total_duration': 28842.416075706482, 'accumulated_submission_time': 4214.180819511414, 'accumulated_eval_time': 24596.399737358093, 'accumulated_logging_time': 0.9645049571990967}
I0316 18:49:27.550189 140274126726912 logging_writer.py:48] [4243] accumulated_eval_time=24596.4, accumulated_logging_time=0.964505, accumulated_submission_time=4214.18, global_step=4243, preemption_count=0, score=4214.18, test/loss=0.126691, test/num_examples=95000000, total_duration=28842.4, train/loss=0.123682, validation/loss=0.12434, validation/num_examples=83274637
I0316 18:51:28.576227 140316090979520 spec.py:321] Evaluating on the training split.
I0316 18:53:32.052841 140316090979520 spec.py:333] Evaluating on the validation split.
I0316 18:56:28.433655 140316090979520 spec.py:349] Evaluating on the test split.
I0316 19:01:19.476869 140316090979520 submission_runner.py:469] Time since start: 29554.35s, 	Step: 4371, 	{'train/loss': 0.12477601389496853, 'validation/loss': 0.12439455839123195, 'validation/num_examples': 83274637, 'test/loss': 0.126817789969113, 'test/num_examples': 95000000, 'score': 4334.3733921051025, 'total_duration': 29554.353815555573, 'accumulated_submission_time': 4334.3733921051025, 'accumulated_eval_time': 25187.300411701202, 'accumulated_logging_time': 0.9832148551940918}
I0316 19:01:19.488965 140274135119616 logging_writer.py:48] [4371] accumulated_eval_time=25187.3, accumulated_logging_time=0.983215, accumulated_submission_time=4334.37, global_step=4371, preemption_count=0, score=4334.37, test/loss=0.126818, test/num_examples=95000000, total_duration=29554.4, train/loss=0.124776, validation/loss=0.124395, validation/num_examples=83274637
I0316 19:03:20.412789 140316090979520 spec.py:321] Evaluating on the training split.
I0316 19:05:24.327862 140316090979520 spec.py:333] Evaluating on the validation split.
I0316 19:08:20.307120 140316090979520 spec.py:349] Evaluating on the test split.
I0316 19:13:14.850060 140316090979520 submission_runner.py:469] Time since start: 30269.73s, 	Step: 4489, 	{'train/loss': 0.12347488179850533, 'validation/loss': 0.12444481230302823, 'validation/num_examples': 83274637, 'test/loss': 0.12686246164125142, 'test/num_examples': 95000000, 'score': 4454.375955104828, 'total_duration': 30269.727014303207, 'accumulated_submission_time': 4454.375955104828, 'accumulated_eval_time': 25781.7378385067, 'accumulated_logging_time': 1.0025665760040283}
I0316 19:13:14.860820 140274126726912 logging_writer.py:48] [4489] accumulated_eval_time=25781.7, accumulated_logging_time=1.00257, accumulated_submission_time=4454.38, global_step=4489, preemption_count=0, score=4454.38, test/loss=0.126862, test/num_examples=95000000, total_duration=30269.7, train/loss=0.123475, validation/loss=0.124445, validation/num_examples=83274637
I0316 19:13:17.669809 140274135119616 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.0270321, loss=0.11571
I0316 19:13:17.673022 140316090979520 submission.py:265] 4500) loss = 0.116, grad_norm = 0.027
I0316 19:15:15.846916 140316090979520 spec.py:321] Evaluating on the training split.
I0316 19:17:19.540980 140316090979520 spec.py:333] Evaluating on the validation split.
I0316 19:20:15.471151 140316090979520 spec.py:349] Evaluating on the test split.
I0316 19:25:09.268207 140316090979520 submission_runner.py:469] Time since start: 30984.15s, 	Step: 4611, 	{'train/loss': 0.12178743560363312, 'validation/loss': 0.12431409818975192, 'validation/num_examples': 83274637, 'test/loss': 0.12670614152711568, 'test/num_examples': 95000000, 'score': 4574.481848239899, 'total_duration': 30984.145166873932, 'accumulated_submission_time': 4574.481848239899, 'accumulated_eval_time': 26375.159276247025, 'accumulated_logging_time': 1.020167350769043}
I0316 19:25:09.280089 140274126726912 logging_writer.py:48] [4611] accumulated_eval_time=26375.2, accumulated_logging_time=1.02017, accumulated_submission_time=4574.48, global_step=4611, preemption_count=0, score=4574.48, test/loss=0.126706, test/num_examples=95000000, total_duration=30984.1, train/loss=0.121787, validation/loss=0.124314, validation/num_examples=83274637
I0316 19:27:10.381810 140316090979520 spec.py:321] Evaluating on the training split.
I0316 19:29:14.401144 140316090979520 spec.py:333] Evaluating on the validation split.
I0316 19:32:06.010358 140316090979520 spec.py:349] Evaluating on the test split.
I0316 19:37:00.948154 140316090979520 submission_runner.py:469] Time since start: 31695.83s, 	Step: 4736, 	{'train/loss': 0.12349593777775862, 'validation/loss': 0.1244248759138043, 'validation/num_examples': 83274637, 'test/loss': 0.12677910185679386, 'test/num_examples': 95000000, 'score': 4694.710900068283, 'total_duration': 31695.825081825256, 'accumulated_submission_time': 4694.710900068283, 'accumulated_eval_time': 26965.72564649582, 'accumulated_logging_time': 1.0392565727233887}
I0316 19:37:00.999523 140274135119616 logging_writer.py:48] [4736] accumulated_eval_time=26965.7, accumulated_logging_time=1.03926, accumulated_submission_time=4694.71, global_step=4736, preemption_count=0, score=4694.71, test/loss=0.126779, test/num_examples=95000000, total_duration=31695.8, train/loss=0.123496, validation/loss=0.124425, validation/num_examples=83274637
I0316 19:39:02.738049 140316090979520 spec.py:321] Evaluating on the training split.
I0316 19:41:06.467180 140316090979520 spec.py:333] Evaluating on the validation split.
I0316 19:44:02.897855 140316090979520 spec.py:349] Evaluating on the test split.
I0316 19:49:08.048385 140316090979520 submission_runner.py:469] Time since start: 32422.93s, 	Step: 4856, 	{'train/loss': 0.12249251771435184, 'validation/loss': 0.12418454966554691, 'validation/num_examples': 83274637, 'test/loss': 0.12657576785812377, 'test/num_examples': 95000000, 'score': 4815.565217494965, 'total_duration': 32422.925330877304, 'accumulated_submission_time': 4815.565217494965, 'accumulated_eval_time': 27571.03613305092, 'accumulated_logging_time': 1.1310560703277588}
I0316 19:49:08.059451 140274126726912 logging_writer.py:48] [4856] accumulated_eval_time=27571, accumulated_logging_time=1.13106, accumulated_submission_time=4815.57, global_step=4856, preemption_count=0, score=4815.57, test/loss=0.126576, test/num_examples=95000000, total_duration=32422.9, train/loss=0.122493, validation/loss=0.124185, validation/num_examples=83274637
I0316 19:51:09.435267 140316090979520 spec.py:321] Evaluating on the training split.
I0316 19:53:12.846605 140316090979520 spec.py:333] Evaluating on the validation split.
I0316 19:56:09.362015 140316090979520 spec.py:349] Evaluating on the test split.
I0316 20:01:04.315753 140316090979520 submission_runner.py:469] Time since start: 33139.19s, 	Step: 4980, 	{'train/loss': 0.12091516114176991, 'validation/loss': 0.12430536763086077, 'validation/num_examples': 83274637, 'test/loss': 0.1265838744643362, 'test/num_examples': 95000000, 'score': 4936.084424734116, 'total_duration': 33139.19271397591, 'accumulated_submission_time': 4936.084424734116, 'accumulated_eval_time': 28165.91676592827, 'accumulated_logging_time': 1.1487882137298584}
I0316 20:01:04.326414 140274135119616 logging_writer.py:48] [4980] accumulated_eval_time=28165.9, accumulated_logging_time=1.14879, accumulated_submission_time=4936.08, global_step=4980, preemption_count=0, score=4936.08, test/loss=0.126584, test/num_examples=95000000, total_duration=33139.2, train/loss=0.120915, validation/loss=0.124305, validation/num_examples=83274637
I0316 20:01:08.840333 140274126726912 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.0173021, loss=0.121959
I0316 20:01:08.843734 140316090979520 submission.py:265] 5000) loss = 0.122, grad_norm = 0.017
I0316 20:03:05.512145 140316090979520 spec.py:321] Evaluating on the training split.
I0316 20:05:09.183084 140316090979520 spec.py:333] Evaluating on the validation split.
I0316 20:08:01.035371 140316090979520 spec.py:349] Evaluating on the test split.
I0316 20:12:50.417153 140316090979520 submission_runner.py:469] Time since start: 33845.29s, 	Step: 5102, 	{'train/loss': 0.12306761653865199, 'validation/loss': 0.12433111198935017, 'validation/num_examples': 83274637, 'test/loss': 0.12666641557569003, 'test/num_examples': 95000000, 'score': 5056.399609327316, 'total_duration': 33845.294115543365, 'accumulated_submission_time': 5056.399609327316, 'accumulated_eval_time': 28750.82194542885, 'accumulated_logging_time': 1.1661632061004639}
I0316 20:12:50.427621 140274135119616 logging_writer.py:48] [5102] accumulated_eval_time=28750.8, accumulated_logging_time=1.16616, accumulated_submission_time=5056.4, global_step=5102, preemption_count=0, score=5056.4, test/loss=0.126666, test/num_examples=95000000, total_duration=33845.3, train/loss=0.123068, validation/loss=0.124331, validation/num_examples=83274637
I0316 20:14:51.360990 140316090979520 spec.py:321] Evaluating on the training split.
I0316 20:16:54.923453 140316090979520 spec.py:333] Evaluating on the validation split.
I0316 20:19:51.097416 140316090979520 spec.py:349] Evaluating on the test split.
I0316 20:24:40.798993 140316090979520 submission_runner.py:469] Time since start: 34555.68s, 	Step: 5227, 	{'train/loss': 0.12152092947913017, 'validation/loss': 0.12426910410057163, 'validation/num_examples': 83274637, 'test/loss': 0.12658650282199257, 'test/num_examples': 95000000, 'score': 5176.470336437225, 'total_duration': 34555.67594027519, 'accumulated_submission_time': 5176.470336437225, 'accumulated_eval_time': 29340.26001882553, 'accumulated_logging_time': 1.1835932731628418}
I0316 20:24:40.810301 140274126726912 logging_writer.py:48] [5227] accumulated_eval_time=29340.3, accumulated_logging_time=1.18359, accumulated_submission_time=5176.47, global_step=5227, preemption_count=0, score=5176.47, test/loss=0.126587, test/num_examples=95000000, total_duration=34555.7, train/loss=0.121521, validation/loss=0.124269, validation/num_examples=83274637
I0316 20:26:41.745805 140316090979520 spec.py:321] Evaluating on the training split.
I0316 20:28:45.287655 140316090979520 spec.py:333] Evaluating on the validation split.
I0316 20:31:40.207482 140316090979520 spec.py:349] Evaluating on the test split.
I0316 20:36:34.152370 140316090979520 submission_runner.py:469] Time since start: 35269.03s, 	Step: 5346, 	{'train/loss': 0.12289815132673658, 'validation/loss': 0.12423726484169596, 'validation/num_examples': 83274637, 'test/loss': 0.12656987285357024, 'test/num_examples': 95000000, 'score': 5296.519563674927, 'total_duration': 35269.029329538345, 'accumulated_submission_time': 5296.519563674927, 'accumulated_eval_time': 29932.666643857956, 'accumulated_logging_time': 1.2349815368652344}
I0316 20:36:34.181188 140274135119616 logging_writer.py:48] [5346] accumulated_eval_time=29932.7, accumulated_logging_time=1.23498, accumulated_submission_time=5296.52, global_step=5346, preemption_count=0, score=5296.52, test/loss=0.12657, test/num_examples=95000000, total_duration=35269, train/loss=0.122898, validation/loss=0.124237, validation/num_examples=83274637
I0316 20:38:36.849270 140316090979520 spec.py:321] Evaluating on the training split.
I0316 20:40:40.471726 140316090979520 spec.py:333] Evaluating on the validation split.
I0316 20:43:34.296091 140316090979520 spec.py:349] Evaluating on the test split.
I0316 20:48:21.017336 140316090979520 submission_runner.py:469] Time since start: 35975.89s, 	Step: 5469, 	{'train/loss': 0.12385378785483787, 'validation/loss': 0.12432040902713155, 'validation/num_examples': 83274637, 'test/loss': 0.12666277193105596, 'test/num_examples': 95000000, 'score': 5418.273940086365, 'total_duration': 35975.89430117607, 'accumulated_submission_time': 5418.273940086365, 'accumulated_eval_time': 30516.834812402725, 'accumulated_logging_time': 1.2715435028076172}
I0316 20:48:21.028203 140274126726912 logging_writer.py:48] [5469] accumulated_eval_time=30516.8, accumulated_logging_time=1.27154, accumulated_submission_time=5418.27, global_step=5469, preemption_count=0, score=5418.27, test/loss=0.126663, test/num_examples=95000000, total_duration=35975.9, train/loss=0.123854, validation/loss=0.12432, validation/num_examples=83274637
I0316 20:48:29.312618 140274135119616 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.0188652, loss=0.121411
I0316 20:48:29.315836 140316090979520 submission.py:265] 5500) loss = 0.121, grad_norm = 0.019
I0316 20:50:22.458082 140316090979520 spec.py:321] Evaluating on the training split.
I0316 20:52:26.194828 140316090979520 spec.py:333] Evaluating on the validation split.
I0316 20:55:24.132213 140316090979520 spec.py:349] Evaluating on the test split.
I0316 21:00:14.447310 140316090979520 submission_runner.py:469] Time since start: 36689.32s, 	Step: 5593, 	{'train/loss': 0.12262887904932053, 'validation/loss': 0.124285524009432, 'validation/num_examples': 83274637, 'test/loss': 0.12659249295108194, 'test/num_examples': 95000000, 'score': 5538.821206569672, 'total_duration': 36689.324254989624, 'accumulated_submission_time': 5538.821206569672, 'accumulated_eval_time': 31108.824098587036, 'accumulated_logging_time': 1.2895972728729248}
I0316 21:00:14.458194 140274126726912 logging_writer.py:48] [5593] accumulated_eval_time=31108.8, accumulated_logging_time=1.2896, accumulated_submission_time=5538.82, global_step=5593, preemption_count=0, score=5538.82, test/loss=0.126592, test/num_examples=95000000, total_duration=36689.3, train/loss=0.122629, validation/loss=0.124286, validation/num_examples=83274637
I0316 21:02:15.475179 140316090979520 spec.py:321] Evaluating on the training split.
I0316 21:04:19.102611 140316090979520 spec.py:333] Evaluating on the validation split.
I0316 21:07:11.728585 140316090979520 spec.py:349] Evaluating on the test split.
I0316 21:11:55.722923 140316090979520 submission_runner.py:469] Time since start: 37390.60s, 	Step: 5713, 	{'train/loss': 0.12335170685269119, 'validation/loss': 0.12408856139422461, 'validation/num_examples': 83274637, 'test/loss': 0.12643102782323737, 'test/num_examples': 95000000, 'score': 5658.927039861679, 'total_duration': 37390.599875450134, 'accumulated_submission_time': 5658.927039861679, 'accumulated_eval_time': 31689.071981668472, 'accumulated_logging_time': 1.3071010112762451}
I0316 21:11:55.734373 140274135119616 logging_writer.py:48] [5713] accumulated_eval_time=31689.1, accumulated_logging_time=1.3071, accumulated_submission_time=5658.93, global_step=5713, preemption_count=0, score=5658.93, test/loss=0.126431, test/num_examples=95000000, total_duration=37390.6, train/loss=0.123352, validation/loss=0.124089, validation/num_examples=83274637
I0316 21:13:57.569797 140316090979520 spec.py:321] Evaluating on the training split.
I0316 21:16:01.248854 140316090979520 spec.py:333] Evaluating on the validation split.
I0316 21:18:55.545705 140316090979520 spec.py:349] Evaluating on the test split.
I0316 21:23:54.006623 140316090979520 submission_runner.py:469] Time since start: 38108.88s, 	Step: 5835, 	{'train/loss': 0.12492505367039115, 'validation/loss': 0.12409976888864761, 'validation/num_examples': 83274637, 'test/loss': 0.1263984246964304, 'test/num_examples': 95000000, 'score': 5779.881299257278, 'total_duration': 38108.883584976196, 'accumulated_submission_time': 5779.881299257278, 'accumulated_eval_time': 32285.512724876404, 'accumulated_logging_time': 1.332024335861206}
I0316 21:23:54.049404 140274126726912 logging_writer.py:48] [5835] accumulated_eval_time=32285.5, accumulated_logging_time=1.33202, accumulated_submission_time=5779.88, global_step=5835, preemption_count=0, score=5779.88, test/loss=0.126398, test/num_examples=95000000, total_duration=38108.9, train/loss=0.124925, validation/loss=0.1241, validation/num_examples=83274637
I0316 21:25:55.292776 140316090979520 spec.py:321] Evaluating on the training split.
I0316 21:27:58.884322 140316090979520 spec.py:333] Evaluating on the validation split.
I0316 21:30:54.447110 140316090979520 spec.py:349] Evaluating on the test split.
I0316 21:35:51.746543 140316090979520 submission_runner.py:469] Time since start: 38826.62s, 	Step: 5953, 	{'train/loss': 0.12177773625709919, 'validation/loss': 0.12418310090569382, 'validation/num_examples': 83274637, 'test/loss': 0.12647612680688156, 'test/num_examples': 95000000, 'score': 5900.304789543152, 'total_duration': 38826.62350821495, 'accumulated_submission_time': 5900.304789543152, 'accumulated_eval_time': 32881.96655488014, 'accumulated_logging_time': 1.3828198909759521}
I0316 21:35:51.758432 140274135119616 logging_writer.py:48] [5953] accumulated_eval_time=32882, accumulated_logging_time=1.38282, accumulated_submission_time=5900.3, global_step=5953, preemption_count=0, score=5900.3, test/loss=0.126476, test/num_examples=95000000, total_duration=38826.6, train/loss=0.121778, validation/loss=0.124183, validation/num_examples=83274637
I0316 21:36:19.407186 140274126726912 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.0060316, loss=0.117691
I0316 21:36:19.411041 140316090979520 submission.py:265] 6000) loss = 0.118, grad_norm = 0.006
I0316 21:37:52.629400 140316090979520 spec.py:321] Evaluating on the training split.
I0316 21:39:56.555245 140316090979520 spec.py:333] Evaluating on the validation split.
I0316 21:42:53.940871 140316090979520 spec.py:349] Evaluating on the test split.
I0316 21:47:49.826386 140316090979520 submission_runner.py:469] Time since start: 39544.70s, 	Step: 6077, 	{'train/loss': 0.12457566169847646, 'validation/loss': 0.12407643838898928, 'validation/num_examples': 83274637, 'test/loss': 0.12633634523556359, 'test/num_examples': 95000000, 'score': 6020.305463552475, 'total_duration': 39544.70331072807, 'accumulated_submission_time': 6020.305463552475, 'accumulated_eval_time': 33479.16358399391, 'accumulated_logging_time': 1.4015510082244873}
I0316 21:47:49.839140 140274135119616 logging_writer.py:48] [6077] accumulated_eval_time=33479.2, accumulated_logging_time=1.40155, accumulated_submission_time=6020.31, global_step=6077, preemption_count=0, score=6020.31, test/loss=0.126336, test/num_examples=95000000, total_duration=39544.7, train/loss=0.124576, validation/loss=0.124076, validation/num_examples=83274637
I0316 21:49:50.336378 140316090979520 spec.py:321] Evaluating on the training split.
I0316 21:51:53.877656 140316090979520 spec.py:333] Evaluating on the validation split.
I0316 21:54:48.724974 140316090979520 spec.py:349] Evaluating on the test split.
I0316 21:59:37.500271 140316090979520 submission_runner.py:469] Time since start: 40252.38s, 	Step: 6197, 	{'train/loss': 0.12349874591631674, 'validation/loss': 0.1239790603084395, 'validation/num_examples': 83274637, 'test/loss': 0.1263344498046072, 'test/num_examples': 95000000, 'score': 6139.944213151932, 'total_duration': 40252.37722277641, 'accumulated_submission_time': 6139.944213151932, 'accumulated_eval_time': 34066.32761693001, 'accumulated_logging_time': 1.421529769897461}
I0316 21:59:37.558105 140274126726912 logging_writer.py:48] [6197] accumulated_eval_time=34066.3, accumulated_logging_time=1.42153, accumulated_submission_time=6139.94, global_step=6197, preemption_count=0, score=6139.94, test/loss=0.126334, test/num_examples=95000000, total_duration=40252.4, train/loss=0.123499, validation/loss=0.123979, validation/num_examples=83274637
I0316 22:01:38.964865 140316090979520 spec.py:321] Evaluating on the training split.
I0316 22:03:42.708719 140316090979520 spec.py:333] Evaluating on the validation split.
I0316 22:06:36.427846 140316090979520 spec.py:349] Evaluating on the test split.
I0316 22:11:18.074909 140316090979520 submission_runner.py:469] Time since start: 40952.95s, 	Step: 6319, 	{'train/loss': 0.12374429377691848, 'validation/loss': 0.12402362345904533, 'validation/num_examples': 83274637, 'test/loss': 0.12633860092251425, 'test/num_examples': 95000000, 'score': 6260.475841760635, 'total_duration': 40952.951867341995, 'accumulated_submission_time': 6260.475841760635, 'accumulated_eval_time': 34645.437871694565, 'accumulated_logging_time': 1.4870057106018066}
I0316 22:11:18.086267 140274135119616 logging_writer.py:48] [6319] accumulated_eval_time=34645.4, accumulated_logging_time=1.48701, accumulated_submission_time=6260.48, global_step=6319, preemption_count=0, score=6260.48, test/loss=0.126339, test/num_examples=95000000, total_duration=40953, train/loss=0.123744, validation/loss=0.124024, validation/num_examples=83274637
I0316 22:13:19.873923 140316090979520 spec.py:321] Evaluating on the training split.
I0316 22:15:23.938036 140316090979520 spec.py:333] Evaluating on the validation split.
I0316 22:18:18.378124 140316090979520 spec.py:349] Evaluating on the test split.
I0316 22:23:06.717251 140316090979520 submission_runner.py:469] Time since start: 41661.59s, 	Step: 6443, 	{'train/loss': 0.12281658205663785, 'validation/loss': 0.12411128135280645, 'validation/num_examples': 83274637, 'test/loss': 0.12650998271066766, 'test/num_examples': 95000000, 'score': 6381.317385196686, 'total_duration': 41661.5942132473, 'accumulated_submission_time': 6381.317385196686, 'accumulated_eval_time': 35232.28139257431, 'accumulated_logging_time': 1.5306048393249512}
I0316 22:23:06.728454 140274126726912 logging_writer.py:48] [6443] accumulated_eval_time=35232.3, accumulated_logging_time=1.5306, accumulated_submission_time=6381.32, global_step=6443, preemption_count=0, score=6381.32, test/loss=0.12651, test/num_examples=95000000, total_duration=41661.6, train/loss=0.122817, validation/loss=0.124111, validation/num_examples=83274637
I0316 22:23:51.719922 140274135119616 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.0394048, loss=0.125376
I0316 22:23:51.723656 140316090979520 submission.py:265] 6500) loss = 0.125, grad_norm = 0.039
I0316 22:25:07.554421 140316090979520 spec.py:321] Evaluating on the training split.
I0316 22:27:10.958323 140316090979520 spec.py:333] Evaluating on the validation split.
I0316 22:30:03.557332 140316090979520 spec.py:349] Evaluating on the test split.
I0316 22:34:49.561443 140316090979520 submission_runner.py:469] Time since start: 42364.44s, 	Step: 6565, 	{'train/loss': 0.1245094004382872, 'validation/loss': 0.12418290584305308, 'validation/num_examples': 83274637, 'test/loss': 0.12658433545977943, 'test/num_examples': 95000000, 'score': 6501.29265499115, 'total_duration': 42364.43837285042, 'accumulated_submission_time': 6501.29265499115, 'accumulated_eval_time': 35814.288455724716, 'accumulated_logging_time': 1.548649549484253}
I0316 22:34:49.572921 140274126726912 logging_writer.py:48] [6565] accumulated_eval_time=35814.3, accumulated_logging_time=1.54865, accumulated_submission_time=6501.29, global_step=6565, preemption_count=0, score=6501.29, test/loss=0.126584, test/num_examples=95000000, total_duration=42364.4, train/loss=0.124509, validation/loss=0.124183, validation/num_examples=83274637
I0316 22:36:50.063530 140316090979520 spec.py:321] Evaluating on the training split.
I0316 22:38:53.718212 140316090979520 spec.py:333] Evaluating on the validation split.
I0316 22:41:49.650010 140316090979520 spec.py:349] Evaluating on the test split.
I0316 22:46:46.640207 140316090979520 submission_runner.py:469] Time since start: 43081.52s, 	Step: 6684, 	{'train/loss': 0.12135978226632191, 'validation/loss': 0.12408610312374084, 'validation/num_examples': 83274637, 'test/loss': 0.1264122324943141, 'test/num_examples': 95000000, 'score': 6620.930959224701, 'total_duration': 43081.51717925072, 'accumulated_submission_time': 6620.930959224701, 'accumulated_eval_time': 36410.86519289017, 'accumulated_logging_time': 1.5670886039733887}
I0316 22:46:46.700063 140274135119616 logging_writer.py:48] [6684] accumulated_eval_time=36410.9, accumulated_logging_time=1.56709, accumulated_submission_time=6620.93, global_step=6684, preemption_count=0, score=6620.93, test/loss=0.126412, test/num_examples=95000000, total_duration=43081.5, train/loss=0.12136, validation/loss=0.124086, validation/num_examples=83274637
I0316 22:48:48.063652 140316090979520 spec.py:321] Evaluating on the training split.
I0316 22:50:51.550402 140316090979520 spec.py:333] Evaluating on the validation split.
I0316 22:53:46.417093 140316090979520 spec.py:349] Evaluating on the test split.
I0316 22:58:40.685999 140316090979520 submission_runner.py:469] Time since start: 43795.56s, 	Step: 6806, 	{'train/loss': 0.12217675080150403, 'validation/loss': 0.12389699781965685, 'validation/num_examples': 83274637, 'test/loss': 0.12621866573365864, 'test/num_examples': 95000000, 'score': 6741.424641609192, 'total_duration': 43795.56293511391, 'accumulated_submission_time': 6741.424641609192, 'accumulated_eval_time': 37003.48772931099, 'accumulated_logging_time': 1.634108304977417}
I0316 22:58:40.697884 140274126726912 logging_writer.py:48] [6806] accumulated_eval_time=37003.5, accumulated_logging_time=1.63411, accumulated_submission_time=6741.42, global_step=6806, preemption_count=0, score=6741.42, test/loss=0.126219, test/num_examples=95000000, total_duration=43795.6, train/loss=0.122177, validation/loss=0.123897, validation/num_examples=83274637
I0316 23:00:42.041200 140316090979520 spec.py:321] Evaluating on the training split.
I0316 23:02:45.690735 140316090979520 spec.py:333] Evaluating on the validation split.
I0316 23:05:38.937731 140316090979520 spec.py:349] Evaluating on the test split.
I0316 23:10:25.628560 140316090979520 submission_runner.py:469] Time since start: 44500.51s, 	Step: 6929, 	{'train/loss': 0.12207356933701308, 'validation/loss': 0.12389437092823717, 'validation/num_examples': 83274637, 'test/loss': 0.12615772895957544, 'test/num_examples': 95000000, 'score': 6861.888075828552, 'total_duration': 44500.50545787811, 'accumulated_submission_time': 6861.888075828552, 'accumulated_eval_time': 37587.07506299019, 'accumulated_logging_time': 1.672605037689209}
I0316 23:10:25.671367 140274135119616 logging_writer.py:48] [6929] accumulated_eval_time=37587.1, accumulated_logging_time=1.67261, accumulated_submission_time=6861.89, global_step=6929, preemption_count=0, score=6861.89, test/loss=0.126158, test/num_examples=95000000, total_duration=44500.5, train/loss=0.122074, validation/loss=0.123894, validation/num_examples=83274637
I0316 23:11:19.905979 140274126726912 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.0137866, loss=0.118729
I0316 23:11:19.909218 140316090979520 submission.py:265] 7000) loss = 0.119, grad_norm = 0.014
I0316 23:12:26.880666 140316090979520 spec.py:321] Evaluating on the training split.
I0316 23:14:30.378678 140316090979520 spec.py:333] Evaluating on the validation split.
I0316 23:17:24.415290 140316090979520 spec.py:349] Evaluating on the test split.
I0316 23:22:13.610111 140316090979520 submission_runner.py:469] Time since start: 45208.49s, 	Step: 7056, 	{'train/loss': 0.12183470156640341, 'validation/loss': 0.12388657351680264, 'validation/num_examples': 83274637, 'test/loss': 0.1262012250270241, 'test/num_examples': 95000000, 'score': 6982.203358411789, 'total_duration': 45208.48706960678, 'accumulated_submission_time': 6982.203358411789, 'accumulated_eval_time': 38173.80462884903, 'accumulated_logging_time': 1.722271203994751}
I0316 23:22:13.621785 140274135119616 logging_writer.py:48] [7056] accumulated_eval_time=38173.8, accumulated_logging_time=1.72227, accumulated_submission_time=6982.2, global_step=7056, preemption_count=0, score=6982.2, test/loss=0.126201, test/num_examples=95000000, total_duration=45208.5, train/loss=0.121835, validation/loss=0.123887, validation/num_examples=83274637
I0316 23:24:14.122759 140316090979520 spec.py:321] Evaluating on the training split.
I0316 23:26:17.837448 140316090979520 spec.py:333] Evaluating on the validation split.
I0316 23:29:10.098402 140316090979520 spec.py:349] Evaluating on the test split.
I0316 23:33:58.835233 140316090979520 submission_runner.py:469] Time since start: 45913.71s, 	Step: 7180, 	{'train/loss': 0.12181081806538367, 'validation/loss': 0.12390784551159381, 'validation/num_examples': 83274637, 'test/loss': 0.12622587926025391, 'test/num_examples': 95000000, 'score': 7101.829274892807, 'total_duration': 45913.71219420433, 'accumulated_submission_time': 7101.829274892807, 'accumulated_eval_time': 38758.51727771759, 'accumulated_logging_time': 1.7416551113128662}
I0316 23:33:58.846369 140274126726912 logging_writer.py:48] [7180] accumulated_eval_time=38758.5, accumulated_logging_time=1.74166, accumulated_submission_time=7101.83, global_step=7180, preemption_count=0, score=7101.83, test/loss=0.126226, test/num_examples=95000000, total_duration=45913.7, train/loss=0.121811, validation/loss=0.123908, validation/num_examples=83274637
I0316 23:36:00.338350 140316090979520 spec.py:321] Evaluating on the training split.
I0316 23:38:04.693611 140316090979520 spec.py:333] Evaluating on the validation split.
I0316 23:40:57.381894 140316090979520 spec.py:349] Evaluating on the test split.
I0316 23:45:40.890182 140316090979520 submission_runner.py:469] Time since start: 46615.77s, 	Step: 7307, 	{'train/loss': 0.1209750078587745, 'validation/loss': 0.12399048429949994, 'validation/num_examples': 83274637, 'test/loss': 0.12642958378922312, 'test/num_examples': 95000000, 'score': 7222.41815829277, 'total_duration': 46615.76712656021, 'accumulated_submission_time': 7222.41815829277, 'accumulated_eval_time': 39339.0692551136, 'accumulated_logging_time': 1.7600204944610596}
I0316 23:45:40.901273 140274135119616 logging_writer.py:48] [7307] accumulated_eval_time=39339.1, accumulated_logging_time=1.76002, accumulated_submission_time=7222.42, global_step=7307, preemption_count=0, score=7222.42, test/loss=0.12643, test/num_examples=95000000, total_duration=46615.8, train/loss=0.120975, validation/loss=0.12399, validation/num_examples=83274637
I0316 23:47:41.714725 140316090979520 spec.py:321] Evaluating on the training split.
I0316 23:49:45.776555 140316090979520 spec.py:333] Evaluating on the validation split.
I0316 23:52:40.243378 140316090979520 spec.py:349] Evaluating on the test split.
I0316 23:57:28.744033 140316090979520 submission_runner.py:469] Time since start: 47323.62s, 	Step: 7433, 	{'train/loss': 0.12309050338581709, 'validation/loss': 0.12401101793407374, 'validation/num_examples': 83274637, 'test/loss': 0.1264056586326599, 'test/num_examples': 95000000, 'score': 7342.293780565262, 'total_duration': 47323.620936870575, 'accumulated_submission_time': 7342.293780565262, 'accumulated_eval_time': 39926.09876227379, 'accumulated_logging_time': 1.8216722011566162}
I0316 23:57:28.757164 140274126726912 logging_writer.py:48] [7433] accumulated_eval_time=39926.1, accumulated_logging_time=1.82167, accumulated_submission_time=7342.29, global_step=7433, preemption_count=0, score=7342.29, test/loss=0.126406, test/num_examples=95000000, total_duration=47323.6, train/loss=0.123091, validation/loss=0.124011, validation/num_examples=83274637
I0316 23:58:23.368483 140274135119616 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.0291781, loss=0.118335
I0316 23:58:23.371936 140316090979520 submission.py:265] 7500) loss = 0.118, grad_norm = 0.029
I0316 23:59:29.355273 140316090979520 spec.py:321] Evaluating on the training split.
I0317 00:01:32.828539 140316090979520 spec.py:333] Evaluating on the validation split.
I0317 00:04:26.557995 140316090979520 spec.py:349] Evaluating on the test split.
I0317 00:09:18.720426 140316090979520 submission_runner.py:469] Time since start: 48033.60s, 	Step: 7556, 	{'train/loss': 0.12299006779332974, 'validation/loss': 0.12415381261409372, 'validation/num_examples': 83274637, 'test/loss': 0.12655942339116147, 'test/num_examples': 95000000, 'score': 7461.994308948517, 'total_duration': 48033.597408771515, 'accumulated_submission_time': 7461.994308948517, 'accumulated_eval_time': 40515.46410560608, 'accumulated_logging_time': 1.842362403869629}
I0317 00:09:18.731575 140274126726912 logging_writer.py:48] [7556] accumulated_eval_time=40515.5, accumulated_logging_time=1.84236, accumulated_submission_time=7461.99, global_step=7556, preemption_count=0, score=7461.99, test/loss=0.126559, test/num_examples=95000000, total_duration=48033.6, train/loss=0.12299, validation/loss=0.124154, validation/num_examples=83274637
I0317 00:11:19.496304 140316090979520 spec.py:321] Evaluating on the training split.
I0317 00:13:22.932463 140316090979520 spec.py:333] Evaluating on the validation split.
I0317 00:16:16.684707 140316090979520 spec.py:349] Evaluating on the test split.
I0317 00:21:17.562373 140316090979520 submission_runner.py:469] Time since start: 48752.44s, 	Step: 7678, 	{'train/loss': 0.12230060196963705, 'validation/loss': 0.12392577255072075, 'validation/num_examples': 83274637, 'test/loss': 0.12629193977532638, 'test/num_examples': 95000000, 'score': 7581.8922510147095, 'total_duration': 48752.439274311066, 'accumulated_submission_time': 7581.8922510147095, 'accumulated_eval_time': 41113.530271053314, 'accumulated_logging_time': 1.860534906387329}
I0317 00:21:17.575364 140274135119616 logging_writer.py:48] [7678] accumulated_eval_time=41113.5, accumulated_logging_time=1.86053, accumulated_submission_time=7581.89, global_step=7678, preemption_count=0, score=7581.89, test/loss=0.126292, test/num_examples=95000000, total_duration=48752.4, train/loss=0.122301, validation/loss=0.123926, validation/num_examples=83274637
I0317 00:23:18.118650 140316090979520 spec.py:321] Evaluating on the training split.
I0317 00:25:21.964530 140316090979520 spec.py:333] Evaluating on the validation split.
I0317 00:28:15.714149 140316090979520 spec.py:349] Evaluating on the test split.
I0317 00:33:07.167783 140316090979520 submission_runner.py:469] Time since start: 49462.04s, 	Step: 7803, 	{'train/loss': 0.12340452999939998, 'validation/loss': 0.12387520195074485, 'validation/num_examples': 83274637, 'test/loss': 0.12618098711335032, 'test/num_examples': 95000000, 'score': 7701.55692076683, 'total_duration': 49462.04470896721, 'accumulated_submission_time': 7701.55692076683, 'accumulated_eval_time': 41702.579486846924, 'accumulated_logging_time': 1.8810551166534424}
I0317 00:33:07.181630 140274126726912 logging_writer.py:48] [7803] accumulated_eval_time=41702.6, accumulated_logging_time=1.88106, accumulated_submission_time=7701.56, global_step=7803, preemption_count=0, score=7701.56, test/loss=0.126181, test/num_examples=95000000, total_duration=49462, train/loss=0.123405, validation/loss=0.123875, validation/num_examples=83274637
I0317 00:35:07.368045 140274135119616 logging_writer.py:48] [7924] global_step=7924, preemption_count=0, score=7821.24
I0317 00:35:09.134222 140316090979520 submission_runner.py:646] Tuning trial 4/5
I0317 00:35:09.134401 140316090979520 submission_runner.py:647] Hyperparameters: Hyperparameters(learning_rate=0.0012, one_minus_beta1=0.016610699316537858, one_minus_beta2=0.005888216674053163, epsilon=1e-08, one_minus_momentum=0.5, use_momentum=True, weight_decay=0.00040349948255455174, max_preconditioner_dim=1024, precondition_frequency=100, start_preconditioning_step=-1, inv_root_override=2, exponent_multiplier=1.0, grafting_type='ADAM', grafting_epsilon=1e-08, use_normalized_grafting=False, communication_dtype='FP32', communicate_params=True, use_cosine_decay=True, warmup_factor=0.02, label_smoothing=0.1, dropout_rate=0.1, use_nadam=True, step_hint_factor=1.0)
I0317 00:35:09.135676 140316090979520 submission_runner.py:648] Metrics: {'eval_results': [(1, {'train/loss': 1.6914659254641344, 'validation/loss': 1.6855084564093616, 'validation/num_examples': 83274637, 'test/loss': 1.6899221253790604, 'test/num_examples': 95000000, 'score': 9.155137777328491, 'total_duration': 961.7108941078186, 'accumulated_submission_time': 9.155137777328491, 'accumulated_eval_time': 952.1133000850677, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (123, {'train/loss': 0.1336745599514896, 'validation/loss': 0.13410219117680397, 'validation/num_examples': 83274637, 'test/loss': 0.13706516503769725, 'test/num_examples': 95000000, 'score': 129.32037258148193, 'total_duration': 1975.090059041977, 'accumulated_submission_time': 129.32037258148193, 'accumulated_eval_time': 1844.3670508861542, 'accumulated_logging_time': 0.10162639617919922, 'global_step': 123, 'preemption_count': 0}), (243, {'train/loss': 0.12853585001803677, 'validation/loss': 0.1290671640119229, 'validation/num_examples': 83274637, 'test/loss': 0.13162405343764455, 'test/num_examples': 95000000, 'score': 249.47972059249878, 'total_duration': 2973.0449056625366, 'accumulated_submission_time': 249.47972059249878, 'accumulated_eval_time': 2721.2331125736237, 'accumulated_logging_time': 0.11838936805725098, 'global_step': 243, 'preemption_count': 0}), (365, {'train/loss': 0.1251490287833244, 'validation/loss': 0.1276909515587593, 'validation/num_examples': 83274637, 'test/loss': 0.13026014517059326, 'test/num_examples': 95000000, 'score': 369.2020356655121, 'total_duration': 3977.7744965553284, 'accumulated_submission_time': 369.2020356655121, 'accumulated_eval_time': 3605.3004066944122, 'accumulated_logging_time': 0.17025971412658691, 'global_step': 365, 'preemption_count': 0}), (492, {'train/loss': 0.12687969088847356, 'validation/loss': 0.12747136064057743, 'validation/num_examples': 83274637, 'test/loss': 0.12994236653402227, 'test/num_examples': 95000000, 'score': 488.7521860599518, 'total_duration': 4986.063929080963, 'accumulated_submission_time': 488.7521860599518, 'accumulated_eval_time': 4493.150989294052, 'accumulated_logging_time': 0.1867082118988037, 'global_step': 492, 'preemption_count': 0}), (615, {'train/loss': 0.1254724831503202, 'validation/loss': 0.12690448655203887, 'validation/num_examples': 83274637, 'test/loss': 0.12926871547076577, 'test/num_examples': 95000000, 'score': 609.1161625385284, 'total_duration': 5986.5454018116, 'accumulated_submission_time': 609.1161625385284, 'accumulated_eval_time': 5372.366828680038, 'accumulated_logging_time': 0.2030930519104004, 'global_step': 615, 'preemption_count': 0}), (739, {'train/loss': 0.12587102871595604, 'validation/loss': 0.12678134647278236, 'validation/num_examples': 83274637, 'test/loss': 0.12911691553906893, 'test/num_examples': 95000000, 'score': 729.0497987270355, 'total_duration': 6985.311563253403, 'accumulated_submission_time': 729.0497987270355, 'accumulated_eval_time': 6250.359308242798, 'accumulated_logging_time': 0.22115182876586914, 'global_step': 739, 'preemption_count': 0}), (862, {'train/loss': 0.12525209514327096, 'validation/loss': 0.12654671168370157, 'validation/num_examples': 83274637, 'test/loss': 0.12898656450926127, 'test/num_examples': 95000000, 'score': 849.2402403354645, 'total_duration': 7985.717970132828, 'accumulated_submission_time': 849.2402403354645, 'accumulated_eval_time': 7129.700910329819, 'accumulated_logging_time': 0.23888373374938965, 'global_step': 862, 'preemption_count': 0}), (982, {'train/loss': 0.12360756684209365, 'validation/loss': 0.1261834019488302, 'validation/num_examples': 83274637, 'test/loss': 0.1286987404245477, 'test/num_examples': 95000000, 'score': 969.3288962841034, 'total_duration': 8950.632301568985, 'accumulated_submission_time': 969.3288962841034, 'accumulated_eval_time': 7973.625199079514, 'accumulated_logging_time': 0.2551608085632324, 'global_step': 982, 'preemption_count': 0}), (1107, {'train/loss': 0.12320177029305639, 'validation/loss': 0.1260728514303284, 'validation/num_examples': 83274637, 'test/loss': 0.12855530603288348, 'test/num_examples': 95000000, 'score': 1089.8160667419434, 'total_duration': 9892.291828870773, 'accumulated_submission_time': 1089.8160667419434, 'accumulated_eval_time': 8793.881187677383, 'accumulated_logging_time': 0.27149128913879395, 'global_step': 1107, 'preemption_count': 0}), (1230, {'train/loss': 0.12491417499368132, 'validation/loss': 0.125767355947103, 'validation/num_examples': 83274637, 'test/loss': 0.12818754992218018, 'test/num_examples': 95000000, 'score': 1209.6269445419312, 'total_duration': 10807.786437034607, 'accumulated_submission_time': 1209.6269445419312, 'accumulated_eval_time': 9588.590577363968, 'accumulated_logging_time': 0.35022616386413574, 'global_step': 1230, 'preemption_count': 0}), (1354, {'train/loss': 0.12436340965553949, 'validation/loss': 0.12568193117219675, 'validation/num_examples': 83274637, 'test/loss': 0.12797348085471705, 'test/num_examples': 95000000, 'score': 1329.2002420425415, 'total_duration': 11638.295469760895, 'accumulated_submission_time': 1329.2002420425415, 'accumulated_eval_time': 10298.64454627037, 'accumulated_logging_time': 0.3673419952392578, 'global_step': 1354, 'preemption_count': 0}), (1472, {'train/loss': 0.12431990869619765, 'validation/loss': 0.12579194413289782, 'validation/num_examples': 83274637, 'test/loss': 0.12843951437221326, 'test/num_examples': 95000000, 'score': 1449.4603264331818, 'total_duration': 12417.460972547531, 'accumulated_submission_time': 1449.4603264331818, 'accumulated_eval_time': 10956.638572692871, 'accumulated_logging_time': 0.3860495090484619, 'global_step': 1472, 'preemption_count': 0}), (1593, {'train/loss': 0.12573929438983114, 'validation/loss': 0.12562391440594492, 'validation/num_examples': 83274637, 'test/loss': 0.12815064461091694, 'test/num_examples': 95000000, 'score': 1569.1198790073395, 'total_duration': 13129.875000476837, 'accumulated_submission_time': 1569.1198790073395, 'accumulated_eval_time': 11548.485477924347, 'accumulated_logging_time': 0.4048593044281006, 'global_step': 1593, 'preemption_count': 0}), (1715, {'train/loss': 0.12510872138593584, 'validation/loss': 0.12555031365922348, 'validation/num_examples': 83274637, 'test/loss': 0.1280176452840303, 'test/num_examples': 95000000, 'score': 1689.012228012085, 'total_duration': 13859.362963676453, 'accumulated_submission_time': 1689.012228012085, 'accumulated_eval_time': 12157.19554901123, 'accumulated_logging_time': 0.4211466312408447, 'global_step': 1715, 'preemption_count': 0}), (1837, {'train/loss': 0.12324692388352819, 'validation/loss': 0.12541339006992477, 'validation/num_examples': 83274637, 'test/loss': 0.1277747086371171, 'test/num_examples': 95000000, 'score': 1810.0823884010315, 'total_duration': 14577.792183160782, 'accumulated_submission_time': 1810.0823884010315, 'accumulated_eval_time': 12753.662537574768, 'accumulated_logging_time': 0.4382638931274414, 'global_step': 1837, 'preemption_count': 0}), (1962, {'train/loss': 0.123373683914872, 'validation/loss': 0.12549998279694066, 'validation/num_examples': 83274637, 'test/loss': 0.127937661266608, 'test/num_examples': 95000000, 'score': 1930.0053222179413, 'total_duration': 15292.907705783844, 'accumulated_submission_time': 1930.0053222179413, 'accumulated_eval_time': 13347.936957597733, 'accumulated_logging_time': 0.4560530185699463, 'global_step': 1962, 'preemption_count': 0}), (2087, {'train/loss': 0.12530907141806027, 'validation/loss': 0.12532904585076166, 'validation/num_examples': 83274637, 'test/loss': 0.12776454719013414, 'test/num_examples': 95000000, 'score': 2050.553661108017, 'total_duration': 16009.442063808441, 'accumulated_submission_time': 2050.553661108017, 'accumulated_eval_time': 13943.056151866913, 'accumulated_logging_time': 0.4734678268432617, 'global_step': 2087, 'preemption_count': 0}), (2209, {'train/loss': 0.12281970480981333, 'validation/loss': 0.12515759089254677, 'validation/num_examples': 83274637, 'test/loss': 0.1275742136479428, 'test/num_examples': 95000000, 'score': 2171.174302339554, 'total_duration': 16724.13800740242, 'accumulated_submission_time': 2171.174302339554, 'accumulated_eval_time': 14536.2299721241, 'accumulated_logging_time': 0.5555760860443115, 'global_step': 2209, 'preemption_count': 0}), (2328, {'train/loss': 0.12590030080312872, 'validation/loss': 0.12496603486550097, 'validation/num_examples': 83274637, 'test/loss': 0.1273209322377255, 'test/num_examples': 95000000, 'score': 2291.155768632889, 'total_duration': 17427.838596343994, 'accumulated_submission_time': 2291.155768632889, 'accumulated_eval_time': 15119.086103439331, 'accumulated_logging_time': 0.5724442005157471, 'global_step': 2328, 'preemption_count': 0}), (2446, {'train/loss': 0.12238951843294579, 'validation/loss': 0.1251013117454916, 'validation/num_examples': 83274637, 'test/loss': 0.12744433217267487, 'test/num_examples': 95000000, 'score': 2411.025289297104, 'total_duration': 18142.11105132103, 'accumulated_submission_time': 2411.025289297104, 'accumulated_eval_time': 15712.592066287994, 'accumulated_logging_time': 0.5888779163360596, 'global_step': 2446, 'preemption_count': 0}), (2567, {'train/loss': 0.12313673958834975, 'validation/loss': 0.12490526938013705, 'validation/num_examples': 83274637, 'test/loss': 0.12726186002791556, 'test/num_examples': 95000000, 'score': 2531.083735227585, 'total_duration': 18856.21607899666, 'accumulated_submission_time': 2531.083735227585, 'accumulated_eval_time': 16305.761589050293, 'accumulated_logging_time': 0.6052801609039307, 'global_step': 2567, 'preemption_count': 0}), (2686, {'train/loss': 0.1229286024180856, 'validation/loss': 0.12498980783715645, 'validation/num_examples': 83274637, 'test/loss': 0.12733083697682432, 'test/num_examples': 95000000, 'score': 2650.7251060009003, 'total_duration': 19570.245700120926, 'accumulated_submission_time': 2650.7251060009003, 'accumulated_eval_time': 16899.269127368927, 'accumulated_logging_time': 0.6533441543579102, 'global_step': 2686, 'preemption_count': 0}), (2807, {'train/loss': 0.12423641924512176, 'validation/loss': 0.1249243081619366, 'validation/num_examples': 83274637, 'test/loss': 0.12727789703489606, 'test/num_examples': 95000000, 'score': 2770.5773775577545, 'total_duration': 20289.85614323616, 'accumulated_submission_time': 2770.5773775577545, 'accumulated_eval_time': 17498.085548877716, 'accumulated_logging_time': 0.6740763187408447, 'global_step': 2807, 'preemption_count': 0}), (2925, {'train/loss': 0.12339795641703691, 'validation/loss': 0.12475231609553267, 'validation/num_examples': 83274637, 'test/loss': 0.1270237113834582, 'test/num_examples': 95000000, 'score': 2891.0601370334625, 'total_duration': 20999.667350053787, 'accumulated_submission_time': 2891.0601370334625, 'accumulated_eval_time': 18086.53838968277, 'accumulated_logging_time': 0.6929118633270264, 'global_step': 2925, 'preemption_count': 0}), (3040, {'train/loss': 0.12548221487982686, 'validation/loss': 0.12464914671662411, 'validation/num_examples': 83274637, 'test/loss': 0.12690119336616115, 'test/num_examples': 95000000, 'score': 3010.8247060775757, 'total_duration': 21715.29672884941, 'accumulated_submission_time': 3010.8247060775757, 'accumulated_eval_time': 18681.488548994064, 'accumulated_logging_time': 0.710813045501709, 'global_step': 3040, 'preemption_count': 0}), (3160, {'train/loss': 0.12487915662666421, 'validation/loss': 0.12456766276283851, 'validation/num_examples': 83274637, 'test/loss': 0.12691639732943083, 'test/num_examples': 95000000, 'score': 3130.8704998493195, 'total_duration': 22428.253749608994, 'accumulated_submission_time': 3130.8704998493195, 'accumulated_eval_time': 19273.529145002365, 'accumulated_logging_time': 0.7279255390167236, 'global_step': 3160, 'preemption_count': 0}), (3281, {'train/loss': 0.12201861854971148, 'validation/loss': 0.12456298143683192, 'validation/num_examples': 83274637, 'test/loss': 0.12688183504775197, 'test/num_examples': 95000000, 'score': 3251.7499673366547, 'total_duration': 23134.210708141327, 'accumulated_submission_time': 3251.7499673366547, 'accumulated_eval_time': 19857.710265636444, 'accumulated_logging_time': 0.7697329521179199, 'global_step': 3281, 'preemption_count': 0}), (3404, {'train/loss': 0.12220520174235967, 'validation/loss': 0.12466059268671235, 'validation/num_examples': 83274637, 'test/loss': 0.1269929662708885, 'test/num_examples': 95000000, 'score': 3372.1971917152405, 'total_duration': 23842.600779771805, 'accumulated_submission_time': 3372.1971917152405, 'accumulated_eval_time': 20444.78269481659, 'accumulated_logging_time': 0.7867560386657715, 'global_step': 3404, 'preemption_count': 0}), (3520, {'train/loss': 0.12193732728544764, 'validation/loss': 0.12468958312820012, 'validation/num_examples': 83274637, 'test/loss': 0.1271003201581453, 'test/num_examples': 95000000, 'score': 3493.0354900360107, 'total_duration': 24550.320616960526, 'accumulated_submission_time': 3493.0354900360107, 'accumulated_eval_time': 21030.787079572678, 'accumulated_logging_time': 0.804595947265625, 'global_step': 3520, 'preemption_count': 0}), (3638, {'train/loss': 0.12348222200423091, 'validation/loss': 0.12477326875605435, 'validation/num_examples': 83274637, 'test/loss': 0.12709243507284868, 'test/num_examples': 95000000, 'score': 3613.4135015010834, 'total_duration': 25269.490796804428, 'accumulated_submission_time': 3613.4135015010834, 'accumulated_eval_time': 21628.69826388359, 'accumulated_logging_time': 0.8218832015991211, 'global_step': 3638, 'preemption_count': 0}), (3760, {'train/loss': 0.12351843513645117, 'validation/loss': 0.12473606955086308, 'validation/num_examples': 83274637, 'test/loss': 0.12707489401823344, 'test/num_examples': 95000000, 'score': 3733.831976890564, 'total_duration': 25982.292655467987, 'accumulated_submission_time': 3733.831976890564, 'accumulated_eval_time': 22220.19090819359, 'accumulated_logging_time': 0.8633899688720703, 'global_step': 3760, 'preemption_count': 0}), (3879, {'train/loss': 0.12400023174951648, 'validation/loss': 0.12454301445055628, 'validation/num_examples': 83274637, 'test/loss': 0.12688940321872108, 'test/num_examples': 95000000, 'score': 3853.4002549648285, 'total_duration': 26696.76336503029, 'accumulated_submission_time': 3853.4002549648285, 'accumulated_eval_time': 22814.211965322495, 'accumulated_logging_time': 0.8812885284423828, 'global_step': 3879, 'preemption_count': 0}), (4000, {'train/loss': 0.12321602280818574, 'validation/loss': 0.12470204006925245, 'validation/num_examples': 83274637, 'test/loss': 0.127063418793086, 'test/num_examples': 95000000, 'score': 3974.107750415802, 'total_duration': 27408.39618563652, 'accumulated_submission_time': 3974.107750415802, 'accumulated_eval_time': 23404.28955936432, 'accumulated_logging_time': 0.9000418186187744, 'global_step': 4000, 'preemption_count': 0}), (4123, {'train/loss': 0.12494646464995769, 'validation/loss': 0.12454506671894455, 'validation/num_examples': 83274637, 'test/loss': 0.1269056560419183, 'test/num_examples': 95000000, 'score': 4094.5785994529724, 'total_duration': 28125.420327186584, 'accumulated_submission_time': 4094.5785994529724, 'accumulated_eval_time': 23999.93874502182, 'accumulated_logging_time': 0.9177372455596924, 'global_step': 4123, 'preemption_count': 0}), (4243, {'train/loss': 0.1236824732960537, 'validation/loss': 0.12434002573242871, 'validation/num_examples': 83274637, 'test/loss': 0.12669102755656994, 'test/num_examples': 95000000, 'score': 4214.180819511414, 'total_duration': 28842.416075706482, 'accumulated_submission_time': 4214.180819511414, 'accumulated_eval_time': 24596.399737358093, 'accumulated_logging_time': 0.9645049571990967, 'global_step': 4243, 'preemption_count': 0}), (4371, {'train/loss': 0.12477601389496853, 'validation/loss': 0.12439455839123195, 'validation/num_examples': 83274637, 'test/loss': 0.126817789969113, 'test/num_examples': 95000000, 'score': 4334.3733921051025, 'total_duration': 29554.353815555573, 'accumulated_submission_time': 4334.3733921051025, 'accumulated_eval_time': 25187.300411701202, 'accumulated_logging_time': 0.9832148551940918, 'global_step': 4371, 'preemption_count': 0}), (4489, {'train/loss': 0.12347488179850533, 'validation/loss': 0.12444481230302823, 'validation/num_examples': 83274637, 'test/loss': 0.12686246164125142, 'test/num_examples': 95000000, 'score': 4454.375955104828, 'total_duration': 30269.727014303207, 'accumulated_submission_time': 4454.375955104828, 'accumulated_eval_time': 25781.7378385067, 'accumulated_logging_time': 1.0025665760040283, 'global_step': 4489, 'preemption_count': 0}), (4611, {'train/loss': 0.12178743560363312, 'validation/loss': 0.12431409818975192, 'validation/num_examples': 83274637, 'test/loss': 0.12670614152711568, 'test/num_examples': 95000000, 'score': 4574.481848239899, 'total_duration': 30984.145166873932, 'accumulated_submission_time': 4574.481848239899, 'accumulated_eval_time': 26375.159276247025, 'accumulated_logging_time': 1.020167350769043, 'global_step': 4611, 'preemption_count': 0}), (4736, {'train/loss': 0.12349593777775862, 'validation/loss': 0.1244248759138043, 'validation/num_examples': 83274637, 'test/loss': 0.12677910185679386, 'test/num_examples': 95000000, 'score': 4694.710900068283, 'total_duration': 31695.825081825256, 'accumulated_submission_time': 4694.710900068283, 'accumulated_eval_time': 26965.72564649582, 'accumulated_logging_time': 1.0392565727233887, 'global_step': 4736, 'preemption_count': 0}), (4856, {'train/loss': 0.12249251771435184, 'validation/loss': 0.12418454966554691, 'validation/num_examples': 83274637, 'test/loss': 0.12657576785812377, 'test/num_examples': 95000000, 'score': 4815.565217494965, 'total_duration': 32422.925330877304, 'accumulated_submission_time': 4815.565217494965, 'accumulated_eval_time': 27571.03613305092, 'accumulated_logging_time': 1.1310560703277588, 'global_step': 4856, 'preemption_count': 0}), (4980, {'train/loss': 0.12091516114176991, 'validation/loss': 0.12430536763086077, 'validation/num_examples': 83274637, 'test/loss': 0.1265838744643362, 'test/num_examples': 95000000, 'score': 4936.084424734116, 'total_duration': 33139.19271397591, 'accumulated_submission_time': 4936.084424734116, 'accumulated_eval_time': 28165.91676592827, 'accumulated_logging_time': 1.1487882137298584, 'global_step': 4980, 'preemption_count': 0}), (5102, {'train/loss': 0.12306761653865199, 'validation/loss': 0.12433111198935017, 'validation/num_examples': 83274637, 'test/loss': 0.12666641557569003, 'test/num_examples': 95000000, 'score': 5056.399609327316, 'total_duration': 33845.294115543365, 'accumulated_submission_time': 5056.399609327316, 'accumulated_eval_time': 28750.82194542885, 'accumulated_logging_time': 1.1661632061004639, 'global_step': 5102, 'preemption_count': 0}), (5227, {'train/loss': 0.12152092947913017, 'validation/loss': 0.12426910410057163, 'validation/num_examples': 83274637, 'test/loss': 0.12658650282199257, 'test/num_examples': 95000000, 'score': 5176.470336437225, 'total_duration': 34555.67594027519, 'accumulated_submission_time': 5176.470336437225, 'accumulated_eval_time': 29340.26001882553, 'accumulated_logging_time': 1.1835932731628418, 'global_step': 5227, 'preemption_count': 0}), (5346, {'train/loss': 0.12289815132673658, 'validation/loss': 0.12423726484169596, 'validation/num_examples': 83274637, 'test/loss': 0.12656987285357024, 'test/num_examples': 95000000, 'score': 5296.519563674927, 'total_duration': 35269.029329538345, 'accumulated_submission_time': 5296.519563674927, 'accumulated_eval_time': 29932.666643857956, 'accumulated_logging_time': 1.2349815368652344, 'global_step': 5346, 'preemption_count': 0}), (5469, {'train/loss': 0.12385378785483787, 'validation/loss': 0.12432040902713155, 'validation/num_examples': 83274637, 'test/loss': 0.12666277193105596, 'test/num_examples': 95000000, 'score': 5418.273940086365, 'total_duration': 35975.89430117607, 'accumulated_submission_time': 5418.273940086365, 'accumulated_eval_time': 30516.834812402725, 'accumulated_logging_time': 1.2715435028076172, 'global_step': 5469, 'preemption_count': 0}), (5593, {'train/loss': 0.12262887904932053, 'validation/loss': 0.124285524009432, 'validation/num_examples': 83274637, 'test/loss': 0.12659249295108194, 'test/num_examples': 95000000, 'score': 5538.821206569672, 'total_duration': 36689.324254989624, 'accumulated_submission_time': 5538.821206569672, 'accumulated_eval_time': 31108.824098587036, 'accumulated_logging_time': 1.2895972728729248, 'global_step': 5593, 'preemption_count': 0}), (5713, {'train/loss': 0.12335170685269119, 'validation/loss': 0.12408856139422461, 'validation/num_examples': 83274637, 'test/loss': 0.12643102782323737, 'test/num_examples': 95000000, 'score': 5658.927039861679, 'total_duration': 37390.599875450134, 'accumulated_submission_time': 5658.927039861679, 'accumulated_eval_time': 31689.071981668472, 'accumulated_logging_time': 1.3071010112762451, 'global_step': 5713, 'preemption_count': 0}), (5835, {'train/loss': 0.12492505367039115, 'validation/loss': 0.12409976888864761, 'validation/num_examples': 83274637, 'test/loss': 0.1263984246964304, 'test/num_examples': 95000000, 'score': 5779.881299257278, 'total_duration': 38108.883584976196, 'accumulated_submission_time': 5779.881299257278, 'accumulated_eval_time': 32285.512724876404, 'accumulated_logging_time': 1.332024335861206, 'global_step': 5835, 'preemption_count': 0}), (5953, {'train/loss': 0.12177773625709919, 'validation/loss': 0.12418310090569382, 'validation/num_examples': 83274637, 'test/loss': 0.12647612680688156, 'test/num_examples': 95000000, 'score': 5900.304789543152, 'total_duration': 38826.62350821495, 'accumulated_submission_time': 5900.304789543152, 'accumulated_eval_time': 32881.96655488014, 'accumulated_logging_time': 1.3828198909759521, 'global_step': 5953, 'preemption_count': 0}), (6077, {'train/loss': 0.12457566169847646, 'validation/loss': 0.12407643838898928, 'validation/num_examples': 83274637, 'test/loss': 0.12633634523556359, 'test/num_examples': 95000000, 'score': 6020.305463552475, 'total_duration': 39544.70331072807, 'accumulated_submission_time': 6020.305463552475, 'accumulated_eval_time': 33479.16358399391, 'accumulated_logging_time': 1.4015510082244873, 'global_step': 6077, 'preemption_count': 0}), (6197, {'train/loss': 0.12349874591631674, 'validation/loss': 0.1239790603084395, 'validation/num_examples': 83274637, 'test/loss': 0.1263344498046072, 'test/num_examples': 95000000, 'score': 6139.944213151932, 'total_duration': 40252.37722277641, 'accumulated_submission_time': 6139.944213151932, 'accumulated_eval_time': 34066.32761693001, 'accumulated_logging_time': 1.421529769897461, 'global_step': 6197, 'preemption_count': 0}), (6319, {'train/loss': 0.12374429377691848, 'validation/loss': 0.12402362345904533, 'validation/num_examples': 83274637, 'test/loss': 0.12633860092251425, 'test/num_examples': 95000000, 'score': 6260.475841760635, 'total_duration': 40952.951867341995, 'accumulated_submission_time': 6260.475841760635, 'accumulated_eval_time': 34645.437871694565, 'accumulated_logging_time': 1.4870057106018066, 'global_step': 6319, 'preemption_count': 0}), (6443, {'train/loss': 0.12281658205663785, 'validation/loss': 0.12411128135280645, 'validation/num_examples': 83274637, 'test/loss': 0.12650998271066766, 'test/num_examples': 95000000, 'score': 6381.317385196686, 'total_duration': 41661.5942132473, 'accumulated_submission_time': 6381.317385196686, 'accumulated_eval_time': 35232.28139257431, 'accumulated_logging_time': 1.5306048393249512, 'global_step': 6443, 'preemption_count': 0}), (6565, {'train/loss': 0.1245094004382872, 'validation/loss': 0.12418290584305308, 'validation/num_examples': 83274637, 'test/loss': 0.12658433545977943, 'test/num_examples': 95000000, 'score': 6501.29265499115, 'total_duration': 42364.43837285042, 'accumulated_submission_time': 6501.29265499115, 'accumulated_eval_time': 35814.288455724716, 'accumulated_logging_time': 1.548649549484253, 'global_step': 6565, 'preemption_count': 0}), (6684, {'train/loss': 0.12135978226632191, 'validation/loss': 0.12408610312374084, 'validation/num_examples': 83274637, 'test/loss': 0.1264122324943141, 'test/num_examples': 95000000, 'score': 6620.930959224701, 'total_duration': 43081.51717925072, 'accumulated_submission_time': 6620.930959224701, 'accumulated_eval_time': 36410.86519289017, 'accumulated_logging_time': 1.5670886039733887, 'global_step': 6684, 'preemption_count': 0}), (6806, {'train/loss': 0.12217675080150403, 'validation/loss': 0.12389699781965685, 'validation/num_examples': 83274637, 'test/loss': 0.12621866573365864, 'test/num_examples': 95000000, 'score': 6741.424641609192, 'total_duration': 43795.56293511391, 'accumulated_submission_time': 6741.424641609192, 'accumulated_eval_time': 37003.48772931099, 'accumulated_logging_time': 1.634108304977417, 'global_step': 6806, 'preemption_count': 0}), (6929, {'train/loss': 0.12207356933701308, 'validation/loss': 0.12389437092823717, 'validation/num_examples': 83274637, 'test/loss': 0.12615772895957544, 'test/num_examples': 95000000, 'score': 6861.888075828552, 'total_duration': 44500.50545787811, 'accumulated_submission_time': 6861.888075828552, 'accumulated_eval_time': 37587.07506299019, 'accumulated_logging_time': 1.672605037689209, 'global_step': 6929, 'preemption_count': 0}), (7056, {'train/loss': 0.12183470156640341, 'validation/loss': 0.12388657351680264, 'validation/num_examples': 83274637, 'test/loss': 0.1262012250270241, 'test/num_examples': 95000000, 'score': 6982.203358411789, 'total_duration': 45208.48706960678, 'accumulated_submission_time': 6982.203358411789, 'accumulated_eval_time': 38173.80462884903, 'accumulated_logging_time': 1.722271203994751, 'global_step': 7056, 'preemption_count': 0}), (7180, {'train/loss': 0.12181081806538367, 'validation/loss': 0.12390784551159381, 'validation/num_examples': 83274637, 'test/loss': 0.12622587926025391, 'test/num_examples': 95000000, 'score': 7101.829274892807, 'total_duration': 45913.71219420433, 'accumulated_submission_time': 7101.829274892807, 'accumulated_eval_time': 38758.51727771759, 'accumulated_logging_time': 1.7416551113128662, 'global_step': 7180, 'preemption_count': 0}), (7307, {'train/loss': 0.1209750078587745, 'validation/loss': 0.12399048429949994, 'validation/num_examples': 83274637, 'test/loss': 0.12642958378922312, 'test/num_examples': 95000000, 'score': 7222.41815829277, 'total_duration': 46615.76712656021, 'accumulated_submission_time': 7222.41815829277, 'accumulated_eval_time': 39339.0692551136, 'accumulated_logging_time': 1.7600204944610596, 'global_step': 7307, 'preemption_count': 0}), (7433, {'train/loss': 0.12309050338581709, 'validation/loss': 0.12401101793407374, 'validation/num_examples': 83274637, 'test/loss': 0.1264056586326599, 'test/num_examples': 95000000, 'score': 7342.293780565262, 'total_duration': 47323.620936870575, 'accumulated_submission_time': 7342.293780565262, 'accumulated_eval_time': 39926.09876227379, 'accumulated_logging_time': 1.8216722011566162, 'global_step': 7433, 'preemption_count': 0}), (7556, {'train/loss': 0.12299006779332974, 'validation/loss': 0.12415381261409372, 'validation/num_examples': 83274637, 'test/loss': 0.12655942339116147, 'test/num_examples': 95000000, 'score': 7461.994308948517, 'total_duration': 48033.597408771515, 'accumulated_submission_time': 7461.994308948517, 'accumulated_eval_time': 40515.46410560608, 'accumulated_logging_time': 1.842362403869629, 'global_step': 7556, 'preemption_count': 0}), (7678, {'train/loss': 0.12230060196963705, 'validation/loss': 0.12392577255072075, 'validation/num_examples': 83274637, 'test/loss': 0.12629193977532638, 'test/num_examples': 95000000, 'score': 7581.8922510147095, 'total_duration': 48752.439274311066, 'accumulated_submission_time': 7581.8922510147095, 'accumulated_eval_time': 41113.530271053314, 'accumulated_logging_time': 1.860534906387329, 'global_step': 7678, 'preemption_count': 0}), (7803, {'train/loss': 0.12340452999939998, 'validation/loss': 0.12387520195074485, 'validation/num_examples': 83274637, 'test/loss': 0.12618098711335032, 'test/num_examples': 95000000, 'score': 7701.55692076683, 'total_duration': 49462.04470896721, 'accumulated_submission_time': 7701.55692076683, 'accumulated_eval_time': 41702.579486846924, 'accumulated_logging_time': 1.8810551166534424, 'global_step': 7803, 'preemption_count': 0})], 'global_step': 7924}
I0317 00:35:09.135792 140316090979520 submission_runner.py:649] Timing: 7821.244575023651
I0317 00:35:09.135836 140316090979520 submission_runner.py:651] Total number of evals: 65
I0317 00:35:09.135869 140316090979520 submission_runner.py:652] ====================
I0317 00:35:09.135984 140316090979520 submission_runner.py:750] Final criteo1tb score: 3
