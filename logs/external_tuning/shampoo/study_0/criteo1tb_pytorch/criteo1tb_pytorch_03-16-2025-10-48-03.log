torchrun --redirects 1:0,2:0,3:0,4:0,5:0,6:0,7:0 --standalone --nnodes=1 --nproc_per_node=8 submission_runner.py --framework=pytorch --workload=criteo1tb --submission_path=submissions_algorithms/leaderboard/external_tuning/shampoo_submission/submission.py --data_dir=/data/criteo1tb --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=submissions/rolling_leaderboard/external_tuning/shampoo/study_0 --overwrite=True --save_checkpoints=False --rng_seed=544009331 --torch_compile=true --tuning_ruleset=external --tuning_search_space=submissions_algorithms/leaderboard/external_tuning/shampoo_submission/tuning_search_space.json --num_tuning_trials=5 --hparam_start_index=4 --hparam_end_index=5 2>&1 | tee -a /logs/criteo1tb_pytorch_03-16-2025-10-48-03.log
W0316 10:48:19.533000 9 site-packages/torch/distributed/run.py:793] 
W0316 10:48:19.533000 9 site-packages/torch/distributed/run.py:793] *****************************************
W0316 10:48:19.533000 9 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0316 10:48:19.533000 9 site-packages/torch/distributed/run.py:793] *****************************************
2025-03-16 10:48:32.267630: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-03-16 10:48:32.267630: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-03-16 10:48:32.267626: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-03-16 10:48:32.267627: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-03-16 10:48:32.267626: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-03-16 10:48:32.267625: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-03-16 10:48:32.267628: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-03-16 10:48:32.267625: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1742122112.739373      44 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1742122112.739401      51 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1742122112.739385      49 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1742122112.739406      47 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1742122112.739405      50 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1742122112.739415      48 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1742122112.739389      46 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1742122112.739400      45 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1742122112.909364      44 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
E0000 00:00:1742122112.909373      50 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
E0000 00:00:1742122112.909392      48 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
E0000 00:00:1742122112.909399      51 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
E0000 00:00:1742122112.909400      46 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
E0000 00:00:1742122112.909402      49 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
E0000 00:00:1742122112.909399      45 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
E0000 00:00:1742122112.909412      47 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
[rank0]:[W316 10:49:05.391309909 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank5]:[W316 10:49:05.391444894 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 5]  using GPU 5 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank4]:[W316 10:49:05.391501950 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 4]  using GPU 4 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank7]:[W316 10:49:05.393652445 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 7]  using GPU 7 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank3]:[W316 10:49:05.395614009 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank2]:[W316 10:49:05.396065538 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank1]:[W316 10:49:05.397187067 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank6]:[W316 10:49:05.397305331 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 6]  using GPU 6 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
I0316 10:49:07.991467 139901304898752 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_0/criteo1tb_pytorch.
I0316 10:49:07.991470 140193402266816 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_0/criteo1tb_pytorch.
I0316 10:49:07.991470 140575969412288 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_0/criteo1tb_pytorch.
I0316 10:49:07.991467 140007357965504 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_0/criteo1tb_pytorch.
I0316 10:49:07.991466 139752247178432 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_0/criteo1tb_pytorch.
I0316 10:49:07.991471 139896858342592 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_0/criteo1tb_pytorch.
I0316 10:49:07.991488 139623498142912 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_0/criteo1tb_pytorch.
I0316 10:49:07.991614 140218016072896 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_0/criteo1tb_pytorch.
I0316 10:49:17.527701 139623498142912 submission_runner.py:606] Using RNG seed 544009331
I0316 10:49:17.528456 139896858342592 logger_utils.py:97] Saving hparams to /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_0/criteo1tb_pytorch/trial_5/hparams.json.
I0316 10:49:17.528457 140007357965504 logger_utils.py:97] Saving hparams to /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_0/criteo1tb_pytorch/trial_5/hparams.json.
I0316 10:49:17.528455 140575969412288 logger_utils.py:97] Saving hparams to /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_0/criteo1tb_pytorch/trial_5/hparams.json.
I0316 10:49:17.528471 139752247178432 logger_utils.py:97] Saving hparams to /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_0/criteo1tb_pytorch/trial_5/hparams.json.
I0316 10:49:17.528458 140218016072896 logger_utils.py:97] Saving hparams to /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_0/criteo1tb_pytorch/trial_5/hparams.json.
I0316 10:49:17.528475 140193402266816 logger_utils.py:97] Saving hparams to /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_0/criteo1tb_pytorch/trial_5/hparams.json.
I0316 10:49:17.529166 139623498142912 submission_runner.py:615] --- Tuning run 5/5 ---
I0316 10:49:17.529304 139623498142912 submission_runner.py:620] Creating tuning directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_0/criteo1tb_pytorch/trial_5.
I0316 10:49:17.528892 139901304898752 logger_utils.py:97] Saving hparams to /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_0/criteo1tb_pytorch/trial_5/hparams.json.
I0316 10:49:17.529590 139623498142912 logger_utils.py:97] Saving hparams to /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_0/criteo1tb_pytorch/trial_5/hparams.json.
I0316 10:49:17.924299 139623498142912 submission_runner.py:218] Initializing dataset.
I0316 10:49:17.924523 139623498142912 submission_runner.py:229] Initializing model.
W0316 10:49:25.149476 140575969412288 submission_runner.py:254] These workloads cannot be fully compiled under current PyTorch version. Proceeding without `torch.compile`.
W0316 10:49:25.149482 139896858342592 submission_runner.py:254] These workloads cannot be fully compiled under current PyTorch version. Proceeding without `torch.compile`.
W0316 10:49:25.149484 139752247178432 submission_runner.py:254] These workloads cannot be fully compiled under current PyTorch version. Proceeding without `torch.compile`.
W0316 10:49:25.149481 140193402266816 submission_runner.py:254] These workloads cannot be fully compiled under current PyTorch version. Proceeding without `torch.compile`.
W0316 10:49:25.149485 140007357965504 submission_runner.py:254] These workloads cannot be fully compiled under current PyTorch version. Proceeding without `torch.compile`.
W0316 10:49:25.149508 139623498142912 submission_runner.py:254] These workloads cannot be fully compiled under current PyTorch version. Proceeding without `torch.compile`.
W0316 10:49:25.149516 139901304898752 submission_runner.py:254] These workloads cannot be fully compiled under current PyTorch version. Proceeding without `torch.compile`.
W0316 10:49:25.149574 140218016072896 submission_runner.py:254] These workloads cannot be fully compiled under current PyTorch version. Proceeding without `torch.compile`.
I0316 10:49:25.149690 139623498142912 submission_runner.py:272] Initializing optimizer.
W0316 10:49:25.210561 140193402266816 distributed_shampoo.py:341] start_preconditioning_step set to -1. Setting start_preconditioning_step equal to precondition frequency 100 by default.
W0316 10:49:25.210558 139896858342592 distributed_shampoo.py:341] start_preconditioning_step set to -1. Setting start_preconditioning_step equal to precondition frequency 100 by default.
W0316 10:49:25.210596 140218016072896 distributed_shampoo.py:341] start_preconditioning_step set to -1. Setting start_preconditioning_step equal to precondition frequency 100 by default.
W0316 10:49:25.210592 140575969412288 distributed_shampoo.py:341] start_preconditioning_step set to -1. Setting start_preconditioning_step equal to precondition frequency 100 by default.
W0316 10:49:25.210585 139901304898752 distributed_shampoo.py:341] start_preconditioning_step set to -1. Setting start_preconditioning_step equal to precondition frequency 100 by default.
W0316 10:49:25.210608 140007357965504 distributed_shampoo.py:341] start_preconditioning_step set to -1. Setting start_preconditioning_step equal to precondition frequency 100 by default.
W0316 10:49:25.210615 139752247178432 distributed_shampoo.py:341] start_preconditioning_step set to -1. Setting start_preconditioning_step equal to precondition frequency 100 by default.
W0316 10:49:25.210685 139896858342592 distributed_shampoo.py:359] Nesterov flag is enabled but momentum parameter is zero! Continuing without using momentum or Nesterov acceleration...
W0316 10:49:25.210691 140193402266816 distributed_shampoo.py:359] Nesterov flag is enabled but momentum parameter is zero! Continuing without using momentum or Nesterov acceleration...
W0316 10:49:25.210712 140218016072896 distributed_shampoo.py:359] Nesterov flag is enabled but momentum parameter is zero! Continuing without using momentum or Nesterov acceleration...
W0316 10:49:25.210714 140575969412288 distributed_shampoo.py:359] Nesterov flag is enabled but momentum parameter is zero! Continuing without using momentum or Nesterov acceleration...
W0316 10:49:25.210723 139901304898752 distributed_shampoo.py:359] Nesterov flag is enabled but momentum parameter is zero! Continuing without using momentum or Nesterov acceleration...
W0316 10:49:25.210731 140007357965504 distributed_shampoo.py:359] Nesterov flag is enabled but momentum parameter is zero! Continuing without using momentum or Nesterov acceleration...
W0316 10:49:25.210735 139752247178432 distributed_shampoo.py:359] Nesterov flag is enabled but momentum parameter is zero! Continuing without using momentum or Nesterov acceleration...
W0316 10:49:25.210732 139623498142912 distributed_shampoo.py:341] start_preconditioning_step set to -1. Setting start_preconditioning_step equal to precondition frequency 100 by default.
W0316 10:49:25.210857 139623498142912 distributed_shampoo.py:359] Nesterov flag is enabled but momentum parameter is zero! Continuing without using momentum or Nesterov acceleration...
I0316 10:49:25.291627 139896858342592 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 0.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 0 (torch.Size([512, 13])), Block block_0 (torch.Size([512, 13])).
I0316 10:49:25.291654 140575969412288 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 0.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 0 (torch.Size([512, 13])), Block block_0 (torch.Size([512, 13])).
I0316 10:49:25.291675 140193402266816 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 0.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 0 (torch.Size([512, 13])), Block block_0 (torch.Size([512, 13])).
I0316 10:49:25.291680 140007357965504 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 0.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 0 (torch.Size([512, 13])), Block block_0 (torch.Size([512, 13])).
I0316 10:49:25.291699 140218016072896 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 0.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 0 (torch.Size([512, 13])), Block block_0 (torch.Size([512, 13])).
I0316 10:49:25.291746 139623498142912 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 0.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 0 (torch.Size([512, 13])), Block block_0 (torch.Size([512, 13])).
I0316 10:49:25.291820 139896858342592 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 1.block_0 ([torch.Size([0])]) for Parameter 1 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 10:49:25.291826 140575969412288 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 1.block_0 ([torch.Size([0])]) for Parameter 1 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 10:49:25.291843 140193402266816 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 1.block_0 ([torch.Size([0])]) for Parameter 1 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 10:49:25.291858 140007357965504 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 1.block_0 ([torch.Size([0])]) for Parameter 1 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 10:49:25.291938 139623498142912 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 1.block_0 ([torch.Size([0])]) for Parameter 1 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 10:49:25.291980 139896858342592 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 2.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 2 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 10:49:25.291985 140575969412288 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 2.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 2 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 10:49:25.291924 139901304898752 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 0.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 0 (torch.Size([512, 13])), Block block_0 (torch.Size([512, 13])).
I0316 10:49:25.292011 140193402266816 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 2.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 2 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 10:49:25.292033 140007357965504 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 2.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 2 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 10:49:25.292087 140575969412288 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 3.block_0 ([torch.Size([0])]) for Parameter 3 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 10:49:25.292087 139623498142912 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 2.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 2 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 10:49:25.292094 139896858342592 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 3.block_0 ([torch.Size([0])]) for Parameter 3 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 10:49:25.292124 140193402266816 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 3.block_0 ([torch.Size([0])]) for Parameter 3 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 10:49:25.292120 139901304898752 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 1.block_0 ([torch.Size([0])]) for Parameter 1 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 10:49:25.292149 140007357965504 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 3.block_0 ([torch.Size([0])]) for Parameter 3 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 10:49:25.292176 139623498142912 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 3.block_0 ([torch.Size([0])]) for Parameter 3 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 10:49:25.292216 140575969412288 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 4.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 4 (torch.Size([128, 256])), Block block_0 (torch.Size([128, 256])).
I0316 10:49:25.292211 139896858342592 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 4.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 4 (torch.Size([128, 256])), Block block_0 (torch.Size([128, 256])).
I0316 10:49:25.292157 139752247178432 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 0.block_0 ([torch.Size([512, 512]), torch.Size([13, 13])]) for Parameter 0 (torch.Size([512, 13])), Block block_0 (torch.Size([512, 13])).
I0316 10:49:25.292260 140193402266816 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 4.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 4 (torch.Size([128, 256])), Block block_0 (torch.Size([128, 256])).
I0316 10:49:25.292312 139623498142912 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 4.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 4 (torch.Size([128, 256])), Block block_0 (torch.Size([128, 256])).
I0316 10:49:25.292323 139896858342592 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 5.block_0 ([torch.Size([0])]) for Parameter 5 (torch.Size([128])), Block block_0 (torch.Size([128])).
I0316 10:49:25.292330 140575969412288 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 5.block_0 ([torch.Size([0])]) for Parameter 5 (torch.Size([128])), Block block_0 (torch.Size([128])).
I0316 10:49:25.292370 139752247178432 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 1.block_0 ([torch.Size([0])]) for Parameter 1 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 10:49:25.292388 140193402266816 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 5.block_0 ([torch.Size([0])]) for Parameter 5 (torch.Size([128])), Block block_0 (torch.Size([128])).
I0316 10:49:25.292411 139623498142912 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 5.block_0 ([torch.Size([0])]) for Parameter 5 (torch.Size([128])), Block block_0 (torch.Size([128])).
I0316 10:49:25.292387 140218016072896 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 1.block_0 ([torch.Size([512, 512])]) for Parameter 1 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 10:49:25.292412 140007357965504 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 4.block_0 ([torch.Size([128, 128]), torch.Size([256, 256])]) for Parameter 4 (torch.Size([128, 256])), Block block_0 (torch.Size([128, 256])).
I0316 10:49:25.292449 139896858342592 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 6.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 6 (torch.Size([1024, 506])), Block block_0 (torch.Size([1024, 506])).
I0316 10:49:25.292512 140193402266816 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 6.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 6 (torch.Size([1024, 506])), Block block_0 (torch.Size([1024, 506])).
I0316 10:49:25.292534 139623498142912 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 6.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 6 (torch.Size([1024, 506])), Block block_0 (torch.Size([1024, 506])).
I0316 10:49:25.292529 140007357965504 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 5.block_0 ([torch.Size([0])]) for Parameter 5 (torch.Size([128])), Block block_0 (torch.Size([128])).
I0316 10:49:25.292530 139752247178432 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 2.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 2 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 10:49:25.292544 139896858342592 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 7.block_0 ([torch.Size([0])]) for Parameter 7 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 10:49:25.292609 140218016072896 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 2.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 2 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 10:49:25.292630 139623498142912 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 7.block_0 ([torch.Size([0])]) for Parameter 7 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 10:49:25.292628 140193402266816 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 7.block_0 ([torch.Size([0])]) for Parameter 7 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 10:49:25.292645 139752247178432 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 3.block_0 ([torch.Size([0])]) for Parameter 3 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 10:49:25.292661 139896858342592 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 8.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 8 (torch.Size([1024, 1024])), Block block_0 (torch.Size([1024, 1024])).
I0316 10:49:25.292670 140007357965504 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 6.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 6 (torch.Size([1024, 506])), Block block_0 (torch.Size([1024, 506])).
I0316 10:49:25.292763 139896858342592 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 9.block_0 ([torch.Size([0])]) for Parameter 9 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 10:49:25.292762 140193402266816 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 8.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 8 (torch.Size([1024, 1024])), Block block_0 (torch.Size([1024, 1024])).
I0316 10:49:25.292790 140007357965504 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 7.block_0 ([torch.Size([0])]) for Parameter 7 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 10:49:25.292801 139752247178432 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 4.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 4 (torch.Size([128, 256])), Block block_0 (torch.Size([128, 256])).
I0316 10:49:25.292819 140218016072896 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 3.block_0 ([torch.Size([256, 256])]) for Parameter 3 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 10:49:25.292845 140193402266816 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 9.block_0 ([torch.Size([0])]) for Parameter 9 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 10:49:25.292881 139896858342592 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 10.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 10 (torch.Size([512, 1024])), Block block_0 (torch.Size([512, 1024])).
I0316 10:49:25.292905 139752247178432 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 5.block_0 ([torch.Size([0])]) for Parameter 5 (torch.Size([128])), Block block_0 (torch.Size([128])).
I0316 10:49:25.292911 140007357965504 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 8.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 8 (torch.Size([1024, 1024])), Block block_0 (torch.Size([1024, 1024])).
I0316 10:49:25.292976 139896858342592 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 11.block_0 ([torch.Size([0])]) for Parameter 11 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 10:49:25.292973 140218016072896 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 4.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 4 (torch.Size([128, 256])), Block block_0 (torch.Size([128, 256])).
I0316 10:49:25.292997 140007357965504 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 9.block_0 ([torch.Size([0])]) for Parameter 9 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 10:49:25.292983 139901304898752 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 2.block_0 ([torch.Size([256, 256]), torch.Size([512, 512])]) for Parameter 2 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 10:49:25.293031 139752247178432 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 6.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 6 (torch.Size([1024, 506])), Block block_0 (torch.Size([1024, 506])).
I0316 10:49:25.293105 140007357965504 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 10.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 10 (torch.Size([512, 1024])), Block block_0 (torch.Size([512, 1024])).
I0316 10:49:25.293125 139752247178432 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 7.block_0 ([torch.Size([0])]) for Parameter 7 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 10:49:25.293127 140218016072896 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 5.block_0 ([torch.Size([128, 128])]) for Parameter 5 (torch.Size([128])), Block block_0 (torch.Size([128])).
I0316 10:49:25.293166 139901304898752 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 3.block_0 ([torch.Size([0])]) for Parameter 3 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 10:49:25.293157 140575969412288 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 6.block_0 ([torch.Size([1024, 1024]), torch.Size([506, 506])]) for Parameter 6 (torch.Size([1024, 506])), Block block_0 (torch.Size([1024, 506])).
I0316 10:49:25.293195 140007357965504 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 11.block_0 ([torch.Size([0])]) for Parameter 11 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 10:49:25.293247 139752247178432 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 8.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 8 (torch.Size([1024, 1024])), Block block_0 (torch.Size([1024, 1024])).
I0316 10:49:25.293265 140218016072896 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 6.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 6 (torch.Size([1024, 506])), Block block_0 (torch.Size([1024, 506])).
I0316 10:49:25.293299 140575969412288 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 7.block_0 ([torch.Size([0])]) for Parameter 7 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 10:49:25.293317 140007357965504 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 12.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 12 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 10:49:25.293321 139901304898752 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 4.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 4 (torch.Size([128, 256])), Block block_0 (torch.Size([128, 256])).
I0316 10:49:25.293344 139752247178432 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 9.block_0 ([torch.Size([0])]) for Parameter 9 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 10:49:25.293403 140007357965504 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 13.block_0 ([torch.Size([0])]) for Parameter 13 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 10:49:25.293426 139901304898752 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 5.block_0 ([torch.Size([0])]) for Parameter 5 (torch.Size([128])), Block block_0 (torch.Size([128])).
I0316 10:49:25.293462 139752247178432 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 10.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 10 (torch.Size([512, 1024])), Block block_0 (torch.Size([512, 1024])).
I0316 10:49:25.293459 140575969412288 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 8.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 8 (torch.Size([1024, 1024])), Block block_0 (torch.Size([1024, 1024])).
I0316 10:49:25.293452 140218016072896 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 7.block_0 ([torch.Size([1024, 1024])]) for Parameter 7 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 10:49:25.293496 140007357965504 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 14.block_0 ([torch.Size([0])]) for Parameter 14 (torch.Size([1, 256])), Block block_0 (torch.Size([256])).
I0316 10:49:25.293473 139623498142912 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 8.block_0 ([torch.Size([1024, 1024]), torch.Size([1024, 1024])]) for Parameter 8 (torch.Size([1024, 1024])), Block block_0 (torch.Size([1024, 1024])).
I0316 10:49:25.293545 140575969412288 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 9.block_0 ([torch.Size([0])]) for Parameter 9 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 10:49:25.293552 139752247178432 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 11.block_0 ([torch.Size([0])]) for Parameter 11 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 10:49:25.293555 139901304898752 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 6.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 6 (torch.Size([1024, 506])), Block block_0 (torch.Size([1024, 506])).
I0316 10:49:25.293587 140007357965504 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 15.block_0 ([torch.Size([0])]) for Parameter 15 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0316 10:49:25.293590 140218016072896 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 8.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 8 (torch.Size([1024, 1024])), Block block_0 (torch.Size([1024, 1024])).
I0316 10:49:25.293607 139623498142912 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 9.block_0 ([torch.Size([0])]) for Parameter 9 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 10:49:25.293654 139901304898752 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 7.block_0 ([torch.Size([0])]) for Parameter 7 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 10:49:25.293665 140575969412288 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 10.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 10 (torch.Size([512, 1024])), Block block_0 (torch.Size([512, 1024])).
I0316 10:49:25.293666 139752247178432 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 12.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 12 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 10:49:25.293682 140007357965504 shampoo_preconditioner_list.py:612] Rank 5: ShampooPreconditionerList Numel Breakdown: (524626, 524288, 655360, 131072, 163840, 32768, 2609224, 2097152, 4194304, 2097152, 2621440, 524288, 655360, 131072, 131072, 2)
I0316 10:49:25.293724 140007357965504 shampoo_preconditioner_list.py:615] Rank 5: ShampooPreconditionerList Bytes Breakdown: (2098504,)
I0316 10:49:25.293744 140575969412288 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 11.block_0 ([torch.Size([0])]) for Parameter 11 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 10:49:25.293745 139623498142912 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 10.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 10 (torch.Size([512, 1024])), Block block_0 (torch.Size([512, 1024])).
I0316 10:49:25.293749 139752247178432 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 13.block_0 ([torch.Size([0])]) for Parameter 13 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 10:49:25.293758 140007357965504 shampoo_preconditioner_list.py:618] Rank 5: ShampooPreconditionerList Total Elements: 17093020
I0316 10:49:25.293786 140007357965504 shampoo_preconditioner_list.py:621] Rank 5: ShampooPreconditionerList Total Bytes: 2098504
I0316 10:49:25.293786 139901304898752 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 8.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 8 (torch.Size([1024, 1024])), Block block_0 (torch.Size([1024, 1024])).
I0316 10:49:25.293765 140193402266816 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 10.block_0 ([torch.Size([512, 512]), torch.Size([1024, 1024])]) for Parameter 10 (torch.Size([512, 1024])), Block block_0 (torch.Size([512, 1024])).
I0316 10:49:25.293832 139623498142912 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 11.block_0 ([torch.Size([0])]) for Parameter 11 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 10:49:25.293833 139752247178432 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 14.block_0 ([torch.Size([0])]) for Parameter 14 (torch.Size([1, 256])), Block block_0 (torch.Size([256])).
I0316 10:49:25.293864 140575969412288 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 12.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 12 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 10:49:25.293870 139901304898752 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 9.block_0 ([torch.Size([0])]) for Parameter 9 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 10:49:25.293843 139896858342592 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 12.block_0 ([torch.Size([256, 256]), torch.Size([512, 512])]) for Parameter 12 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 10:49:25.293893 140193402266816 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 11.block_0 ([torch.Size([0])]) for Parameter 11 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 10:49:25.293914 139752247178432 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 15.block_0 ([torch.Size([0])]) for Parameter 15 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0316 10:49:25.293923 140007357965504 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([512, 13])), Block block_0 (torch.Size([512, 13])).
I0316 10:49:25.293950 139623498142912 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 12.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 12 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 10:49:25.293955 140575969412288 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 13.block_0 ([torch.Size([0])]) for Parameter 13 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 10:49:25.293982 139896858342592 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 13.block_0 ([torch.Size([0])]) for Parameter 13 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 10:49:25.293985 139901304898752 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 10.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 10 (torch.Size([512, 1024])), Block block_0 (torch.Size([512, 1024])).
I0316 10:49:25.293998 140007357965504 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 10:49:25.294016 139752247178432 shampoo_preconditioner_list.py:612] Rank 6: ShampooPreconditionerList Numel Breakdown: (524626, 524288, 655360, 131072, 163840, 32768, 2609224, 2097152, 4194304, 2097152, 2621440, 524288, 655360, 131072, 131072, 2)
I0316 10:49:25.294032 139623498142912 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 13.block_0 ([torch.Size([0])]) for Parameter 13 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 10:49:25.294033 140193402266816 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 12.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 12 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 10:49:25.294045 140575969412288 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 14.block_0 ([torch.Size([0])]) for Parameter 14 (torch.Size([1, 256])), Block block_0 (torch.Size([256])).
I0316 10:49:25.294045 140007357965504 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 10:49:25.294058 139752247178432 shampoo_preconditioner_list.py:615] Rank 6: ShampooPreconditionerList Bytes Breakdown: (2098504,)
I0316 10:49:25.294065 139901304898752 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 11.block_0 ([torch.Size([0])]) for Parameter 11 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 10:49:25.294080 139896858342592 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 14.block_0 ([torch.Size([0])]) for Parameter 14 (torch.Size([1, 256])), Block block_0 (torch.Size([256])).
I0316 10:49:25.294097 139752247178432 shampoo_preconditioner_list.py:618] Rank 6: ShampooPreconditionerList Total Elements: 17093020
I0316 10:49:25.294099 140007357965504 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 10:49:25.294083 140218016072896 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 9.block_0 ([torch.Size([1024, 1024])]) for Parameter 9 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 10:49:25.294115 140193402266816 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 13.block_0 ([torch.Size([0])]) for Parameter 13 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 10:49:25.294119 140575969412288 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 15.block_0 ([torch.Size([0])]) for Parameter 15 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0316 10:49:25.294111 139623498142912 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 14.block_0 ([torch.Size([0])]) for Parameter 14 (torch.Size([1, 256])), Block block_0 (torch.Size([256])).
I0316 10:49:25.294135 139752247178432 shampoo_preconditioner_list.py:621] Rank 6: ShampooPreconditionerList Total Bytes: 2098504
I0316 10:49:25.294167 139896858342592 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 15.block_0 ([torch.Size([0])]) for Parameter 15 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0316 10:49:25.294204 139901304898752 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 12.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 12 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 10:49:25.294221 140193402266816 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 14.block_0 ([torch.Size([0])]) for Parameter 14 (torch.Size([1, 256])), Block block_0 (torch.Size([256])).
I0316 10:49:25.294218 140007357965504 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 4.block_0 (torch.Size([128, 256])) for Parameter 4 (torch.Size([128, 256])), Block block_0 (torch.Size([128, 256])).
I0316 10:49:25.294227 140575969412288 shampoo_preconditioner_list.py:612] Rank 2: ShampooPreconditionerList Numel Breakdown: (524626, 524288, 655360, 131072, 163840, 32768, 2609224, 2097152, 4194304, 2097152, 2621440, 524288, 655360, 131072, 131072, 2)
I0316 10:49:25.294238 139623498142912 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 15.block_0 ([torch.Size([0])]) for Parameter 15 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0316 10:49:25.294281 140575969412288 shampoo_preconditioner_list.py:615] Rank 2: ShampooPreconditionerList Bytes Breakdown: (2098504,)
I0316 10:49:25.294270 140218016072896 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 10.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 10 (torch.Size([512, 1024])), Block block_0 (torch.Size([512, 1024])).
I0316 10:49:25.294303 139901304898752 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 13.block_0 ([torch.Size([0])]) for Parameter 13 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 10:49:25.294304 139896858342592 shampoo_preconditioner_list.py:612] Rank 4: ShampooPreconditionerList Numel Breakdown: (524626, 524288, 655360, 131072, 163840, 32768, 2609224, 2097152, 4194304, 2097152, 2621440, 524288, 655360, 131072, 131072, 2)
I0316 10:49:25.294315 140007357965504 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 5.block_0 (torch.Size([0])) for Parameter 5 (torch.Size([128])), Block block_0 (torch.Size([128])).
I0316 10:49:25.294329 140575969412288 shampoo_preconditioner_list.py:618] Rank 2: ShampooPreconditionerList Total Elements: 17093020
I0316 10:49:25.294344 139896858342592 shampoo_preconditioner_list.py:615] Rank 4: ShampooPreconditionerList Bytes Breakdown: (2098504,)
I0316 10:49:25.294334 140193402266816 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 15.block_0 ([torch.Size([0])]) for Parameter 15 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0316 10:49:25.294351 139623498142912 shampoo_preconditioner_list.py:612] Rank 0: ShampooPreconditionerList Numel Breakdown: (524626, 524288, 655360, 131072, 163840, 32768, 2609224, 2097152, 4194304, 2097152, 2621440, 524288, 655360, 131072, 131072, 2)
I0316 10:49:25.294362 140575969412288 shampoo_preconditioner_list.py:621] Rank 2: ShampooPreconditionerList Total Bytes: 2098504
I0316 10:49:25.294382 139901304898752 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 14.block_0 ([torch.Size([0])]) for Parameter 14 (torch.Size([1, 256])), Block block_0 (torch.Size([256])).
I0316 10:49:25.294386 139896858342592 shampoo_preconditioner_list.py:618] Rank 4: ShampooPreconditionerList Total Elements: 17093020
I0316 10:49:25.294382 140007357965504 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 6.block_0 (torch.Size([0])) for Parameter 6 (torch.Size([1024, 506])), Block block_0 (torch.Size([1024, 506])).
I0316 10:49:25.294371 139752247178432 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([512, 13])) for Parameter 0 (torch.Size([512, 13])), Block block_0 (torch.Size([512, 13])).
I0316 10:49:25.294389 139623498142912 shampoo_preconditioner_list.py:615] Rank 0: ShampooPreconditionerList Bytes Breakdown: (2098504,)
I0316 10:49:25.294418 139896858342592 shampoo_preconditioner_list.py:621] Rank 4: ShampooPreconditionerList Total Bytes: 2098504
I0316 10:49:25.294423 139623498142912 shampoo_preconditioner_list.py:618] Rank 0: ShampooPreconditionerList Total Elements: 17093020
I0316 10:49:25.294444 140007357965504 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 7.block_0 (torch.Size([0])) for Parameter 7 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 10:49:25.294453 139623498142912 shampoo_preconditioner_list.py:621] Rank 0: ShampooPreconditionerList Total Bytes: 2098504
I0316 10:49:25.294448 140193402266816 shampoo_preconditioner_list.py:612] Rank 1: ShampooPreconditionerList Numel Breakdown: (524626, 524288, 655360, 131072, 163840, 32768, 2609224, 2097152, 4194304, 2097152, 2621440, 524288, 655360, 131072, 131072, 2)
I0316 10:49:25.294455 139752247178432 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 10:49:25.294478 139901304898752 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 15.block_0 ([torch.Size([0])]) for Parameter 15 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0316 10:49:25.294489 140193402266816 shampoo_preconditioner_list.py:615] Rank 1: ShampooPreconditionerList Bytes Breakdown: (2098504,)
I0316 10:49:25.294505 140007357965504 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 8.block_0 (torch.Size([0])) for Parameter 8 (torch.Size([1024, 1024])), Block block_0 (torch.Size([1024, 1024])).
I0316 10:49:25.294506 140575969412288 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([512, 13])), Block block_0 (torch.Size([512, 13])).
I0316 10:49:25.294512 139752247178432 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 10:49:25.294523 140193402266816 shampoo_preconditioner_list.py:618] Rank 1: ShampooPreconditionerList Total Elements: 17093020
I0316 10:49:25.294554 140193402266816 shampoo_preconditioner_list.py:621] Rank 1: ShampooPreconditionerList Total Bytes: 2098504
I0316 10:49:25.294559 140007357965504 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 9.block_0 (torch.Size([0])) for Parameter 9 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 10:49:25.294565 139752247178432 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 10:49:25.294567 140575969412288 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 10:49:25.294571 139896858342592 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([512, 13])), Block block_0 (torch.Size([512, 13])).
I0316 10:49:25.294581 139901304898752 shampoo_preconditioner_list.py:612] Rank 3: ShampooPreconditionerList Numel Breakdown: (524626, 524288, 655360, 131072, 163840, 32768, 2609224, 2097152, 4194304, 2097152, 2621440, 524288, 655360, 131072, 131072, 2)
I0316 10:49:25.294588 139623498142912 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([512, 13])), Block block_0 (torch.Size([512, 13])).
I0316 10:49:25.294610 140007357965504 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 10.block_0 (torch.Size([0])) for Parameter 10 (torch.Size([512, 1024])), Block block_0 (torch.Size([512, 1024])).
I0316 10:49:25.294618 139752247178432 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 4.block_0 (torch.Size([0])) for Parameter 4 (torch.Size([128, 256])), Block block_0 (torch.Size([128, 256])).
I0316 10:49:25.294620 140575969412288 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 10:49:25.294618 139901304898752 shampoo_preconditioner_list.py:615] Rank 3: ShampooPreconditionerList Bytes Breakdown: (2098504,)
I0316 10:49:25.294634 139896858342592 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 10:49:25.294664 139901304898752 shampoo_preconditioner_list.py:618] Rank 3: ShampooPreconditionerList Total Elements: 17093020
I0316 10:49:25.294666 140007357965504 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 11.block_0 (torch.Size([0])) for Parameter 11 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 10:49:25.294669 139623498142912 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 10:49:25.294670 139752247178432 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 5.block_0 (torch.Size([0])) for Parameter 5 (torch.Size([128])), Block block_0 (torch.Size([128])).
I0316 10:49:25.294682 140575969412288 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 10:49:25.294687 139896858342592 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 10:49:25.294700 139901304898752 shampoo_preconditioner_list.py:621] Rank 3: ShampooPreconditionerList Total Bytes: 2098504
I0316 10:49:25.294705 140193402266816 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([512, 13])), Block block_0 (torch.Size([512, 13])).
I0316 10:49:25.294716 140007357965504 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 12.block_0 (torch.Size([0])) for Parameter 12 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 10:49:25.294718 139623498142912 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 10:49:25.294724 139752247178432 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 6.block_0 (torch.Size([0])) for Parameter 6 (torch.Size([1024, 506])), Block block_0 (torch.Size([1024, 506])).
I0316 10:49:25.294734 140575969412288 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 4.block_0 (torch.Size([0])) for Parameter 4 (torch.Size([128, 256])), Block block_0 (torch.Size([128, 256])).
I0316 10:49:25.294739 139896858342592 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 10:49:25.294765 139623498142912 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 10:49:25.294765 140007357965504 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 13.block_0 (torch.Size([0])) for Parameter 13 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 10:49:25.294778 140193402266816 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 10:49:25.294784 140575969412288 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 5.block_0 (torch.Size([0])) for Parameter 5 (torch.Size([128])), Block block_0 (torch.Size([128])).
I0316 10:49:25.294784 139752247178432 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 7.block_0 (torch.Size([0])) for Parameter 7 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 10:49:25.294792 139896858342592 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 4.block_0 (torch.Size([0])) for Parameter 4 (torch.Size([128, 256])), Block block_0 (torch.Size([128, 256])).
I0316 10:49:25.294815 140007357965504 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 14.block_0 (torch.Size([0])) for Parameter 14 (torch.Size([1, 256])), Block block_0 (torch.Size([256])).
I0316 10:49:25.294818 139623498142912 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 4.block_0 (torch.Size([0])) for Parameter 4 (torch.Size([128, 256])), Block block_0 (torch.Size([128, 256])).
I0316 10:49:25.294810 140218016072896 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 11.block_0 ([torch.Size([512, 512])]) for Parameter 11 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 10:49:25.294834 140193402266816 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 10:49:25.294837 139752247178432 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 8.block_0 (torch.Size([0])) for Parameter 8 (torch.Size([1024, 1024])), Block block_0 (torch.Size([1024, 1024])).
I0316 10:49:25.294836 139901304898752 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([512, 13])), Block block_0 (torch.Size([512, 13])).
I0316 10:49:25.294843 139896858342592 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 5.block_0 (torch.Size([0])) for Parameter 5 (torch.Size([128])), Block block_0 (torch.Size([128])).
I0316 10:49:25.294862 139623498142912 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 5.block_0 (torch.Size([0])) for Parameter 5 (torch.Size([128])), Block block_0 (torch.Size([128])).
I0316 10:49:25.294872 140007357965504 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 15.block_0 (torch.Size([0])) for Parameter 15 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0316 10:49:25.294887 140193402266816 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 10:49:25.294890 139901304898752 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 10:49:25.294889 139752247178432 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 9.block_0 (torch.Size([0])) for Parameter 9 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 10:49:25.294885 140575969412288 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 6.block_0 (torch.Size([1024, 506])) for Parameter 6 (torch.Size([1024, 506])), Block block_0 (torch.Size([1024, 506])).
I0316 10:49:25.294895 139896858342592 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 6.block_0 (torch.Size([0])) for Parameter 6 (torch.Size([1024, 506])), Block block_0 (torch.Size([1024, 506])).
I0316 10:49:25.294907 139623498142912 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 6.block_0 (torch.Size([0])) for Parameter 6 (torch.Size([1024, 506])), Block block_0 (torch.Size([1024, 506])).
I0316 10:49:25.294936 140007357965504 shampoo_preconditioner_list.py:375] Rank 5: AdaGradPreconditionerList Numel Breakdown: (0, 0, 0, 0, 32768, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)
I0316 10:49:25.294941 139752247178432 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 10.block_0 (torch.Size([0])) for Parameter 10 (torch.Size([512, 1024])), Block block_0 (torch.Size([512, 1024])).
I0316 10:49:25.294949 140193402266816 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 4.block_0 (torch.Size([0])) for Parameter 4 (torch.Size([128, 256])), Block block_0 (torch.Size([128, 256])).
I0316 10:49:25.294956 139623498142912 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 7.block_0 (torch.Size([0])) for Parameter 7 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 10:49:25.294955 139896858342592 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 7.block_0 (torch.Size([0])) for Parameter 7 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 10:49:25.294958 140575969412288 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 7.block_0 (torch.Size([0])) for Parameter 7 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 10:49:25.294976 140007357965504 shampoo_preconditioner_list.py:378] Rank 5: AdaGradPreconditionerList Bytes Breakdown: (0, 0, 0, 0, 131072, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)
I0316 10:49:25.294991 139752247178432 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 11.block_0 (torch.Size([0])) for Parameter 11 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 10:49:25.294986 140218016072896 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 12.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 12 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 10:49:25.295001 140193402266816 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 5.block_0 (torch.Size([0])) for Parameter 5 (torch.Size([128])), Block block_0 (torch.Size([128])).
I0316 10:49:25.295008 139896858342592 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 8.block_0 (torch.Size([0])) for Parameter 8 (torch.Size([1024, 1024])), Block block_0 (torch.Size([1024, 1024])).
I0316 10:49:25.295012 140575969412288 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 8.block_0 (torch.Size([0])) for Parameter 8 (torch.Size([1024, 1024])), Block block_0 (torch.Size([1024, 1024])).
I0316 10:49:25.295013 140007357965504 shampoo_preconditioner_list.py:381] Rank 5: AdaGradPreconditionerList Total Elements: 32768
I0316 10:49:25.295053 140007357965504 shampoo_preconditioner_list.py:384] Rank 5: AdaGradPreconditionerList Total Bytes: 131072
I0316 10:49:25.295053 139752247178432 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 12.block_0 (torch.Size([0])) for Parameter 12 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 10:49:25.295049 139623498142912 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 8.block_0 (torch.Size([1024, 1024])) for Parameter 8 (torch.Size([1024, 1024])), Block block_0 (torch.Size([1024, 1024])).
I0316 10:49:25.295055 140193402266816 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 6.block_0 (torch.Size([0])) for Parameter 6 (torch.Size([1024, 506])), Block block_0 (torch.Size([1024, 506])).
I0316 10:49:25.295060 139896858342592 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 9.block_0 (torch.Size([0])) for Parameter 9 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 10:49:25.295063 140575969412288 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 9.block_0 (torch.Size([0])) for Parameter 9 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 10:49:25.295102 139752247178432 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 13.block_0 (torch.Size([0])) for Parameter 13 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 10:49:25.295110 139623498142912 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 9.block_0 (torch.Size([0])) for Parameter 9 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 10:49:25.295111 139896858342592 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 10.block_0 (torch.Size([0])) for Parameter 10 (torch.Size([512, 1024])), Block block_0 (torch.Size([512, 1024])).
I0316 10:49:25.295114 140575969412288 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 10.block_0 (torch.Size([0])) for Parameter 10 (torch.Size([512, 1024])), Block block_0 (torch.Size([512, 1024])).
I0316 10:49:25.295114 140193402266816 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 7.block_0 (torch.Size([0])) for Parameter 7 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 10:49:25.295155 139623498142912 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 10.block_0 (torch.Size([0])) for Parameter 10 (torch.Size([512, 1024])), Block block_0 (torch.Size([512, 1024])).
I0316 10:49:25.295160 139752247178432 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 14.block_0 (torch.Size([0])) for Parameter 14 (torch.Size([1, 256])), Block block_0 (torch.Size([256])).
I0316 10:49:25.295162 139896858342592 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 11.block_0 (torch.Size([0])) for Parameter 11 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 10:49:25.295163 140575969412288 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 11.block_0 (torch.Size([0])) for Parameter 11 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 10:49:25.295166 140193402266816 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 8.block_0 (torch.Size([0])) for Parameter 8 (torch.Size([1024, 1024])), Block block_0 (torch.Size([1024, 1024])).
I0316 10:49:25.295211 139752247178432 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 15.block_0 (torch.Size([0])) for Parameter 15 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0316 10:49:25.295213 140575969412288 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 12.block_0 (torch.Size([0])) for Parameter 12 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 10:49:25.295219 139623498142912 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 11.block_0 (torch.Size([0])) for Parameter 11 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 10:49:25.295218 140193402266816 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 9.block_0 (torch.Size([0])) for Parameter 9 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 10:49:25.295261 140575969412288 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 13.block_0 (torch.Size([0])) for Parameter 13 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 10:49:25.295268 139623498142912 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 12.block_0 (torch.Size([0])) for Parameter 12 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 10:49:25.295277 139752247178432 shampoo_preconditioner_list.py:375] Rank 6: AdaGradPreconditionerList Numel Breakdown: (6656, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)
I0316 10:49:25.295311 139623498142912 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 13.block_0 (torch.Size([0])) for Parameter 13 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 10:49:25.295321 139752247178432 shampoo_preconditioner_list.py:378] Rank 6: AdaGradPreconditionerList Bytes Breakdown: (26624, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)
I0316 10:49:25.295323 140193402266816 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 10.block_0 (torch.Size([512, 1024])) for Parameter 10 (torch.Size([512, 1024])), Block block_0 (torch.Size([512, 1024])).
I0316 10:49:25.295330 140575969412288 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 14.block_0 (torch.Size([0])) for Parameter 14 (torch.Size([1, 256])), Block block_0 (torch.Size([256])).
I0316 10:49:25.295353 139752247178432 shampoo_preconditioner_list.py:381] Rank 6: AdaGradPreconditionerList Total Elements: 6656
I0316 10:49:25.295364 139623498142912 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 14.block_0 (torch.Size([0])) for Parameter 14 (torch.Size([1, 256])), Block block_0 (torch.Size([256])).
I0316 10:49:25.295382 140575969412288 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 15.block_0 (torch.Size([0])) for Parameter 15 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0316 10:49:25.295390 139752247178432 shampoo_preconditioner_list.py:384] Rank 6: AdaGradPreconditionerList Total Bytes: 26624
I0316 10:49:25.295407 139623498142912 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 15.block_0 (torch.Size([0])) for Parameter 15 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0316 10:49:25.295410 140193402266816 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 11.block_0 (torch.Size([0])) for Parameter 11 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 10:49:25.295406 139901304898752 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([256, 512])) for Parameter 2 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 10:49:25.295468 139623498142912 shampoo_preconditioner_list.py:375] Rank 0: AdaGradPreconditionerList Numel Breakdown: (0, 0, 0, 0, 0, 0, 0, 0, 1048576, 0, 0, 0, 0, 0, 0, 0)
I0316 10:49:25.295472 140575969412288 shampoo_preconditioner_list.py:375] Rank 2: AdaGradPreconditionerList Numel Breakdown: (0, 0, 0, 0, 0, 0, 518144, 0, 0, 0, 0, 0, 0, 0, 0, 0)
I0316 10:49:25.295491 140193402266816 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 12.block_0 (torch.Size([0])) for Parameter 12 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 10:49:25.295502 140575969412288 shampoo_preconditioner_list.py:378] Rank 2: AdaGradPreconditionerList Bytes Breakdown: (0, 0, 0, 0, 0, 0, 2072576, 0, 0, 0, 0, 0, 0, 0, 0, 0)
I0316 10:49:25.295497 139901304898752 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 10:49:25.295507 139623498142912 shampoo_preconditioner_list.py:378] Rank 0: AdaGradPreconditionerList Bytes Breakdown: (0, 0, 0, 0, 0, 0, 0, 0, 4194304, 0, 0, 0, 0, 0, 0, 0)
I0316 10:49:25.295535 139623498142912 shampoo_preconditioner_list.py:381] Rank 0: AdaGradPreconditionerList Total Elements: 1048576
I0316 10:49:25.295534 140193402266816 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 13.block_0 (torch.Size([0])) for Parameter 13 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 10:49:25.295534 140575969412288 shampoo_preconditioner_list.py:381] Rank 2: AdaGradPreconditionerList Total Elements: 518144
I0316 10:49:25.295546 139901304898752 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 4.block_0 (torch.Size([0])) for Parameter 4 (torch.Size([128, 256])), Block block_0 (torch.Size([128, 256])).
I0316 10:49:25.295565 140575969412288 shampoo_preconditioner_list.py:384] Rank 2: AdaGradPreconditionerList Total Bytes: 2072576
I0316 10:49:25.295568 139623498142912 shampoo_preconditioner_list.py:384] Rank 0: AdaGradPreconditionerList Total Bytes: 4194304
I0316 10:49:25.295577 140193402266816 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 14.block_0 (torch.Size([0])) for Parameter 14 (torch.Size([1, 256])), Block block_0 (torch.Size([256])).
I0316 10:49:25.295591 139901304898752 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 5.block_0 (torch.Size([0])) for Parameter 5 (torch.Size([128])), Block block_0 (torch.Size([128])).
I0316 10:49:25.295619 140193402266816 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 15.block_0 (torch.Size([0])) for Parameter 15 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0316 10:49:25.295651 139901304898752 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 6.block_0 (torch.Size([0])) for Parameter 6 (torch.Size([1024, 506])), Block block_0 (torch.Size([1024, 506])).
I0316 10:49:25.295679 140193402266816 shampoo_preconditioner_list.py:375] Rank 1: AdaGradPreconditionerList Numel Breakdown: (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 524288, 0, 0, 0, 0, 0)
I0316 10:49:25.295714 140193402266816 shampoo_preconditioner_list.py:378] Rank 1: AdaGradPreconditionerList Bytes Breakdown: (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2097152, 0, 0, 0, 0, 0)
I0316 10:49:25.295712 139901304898752 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 7.block_0 (torch.Size([0])) for Parameter 7 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 10:49:25.295745 140193402266816 shampoo_preconditioner_list.py:381] Rank 1: AdaGradPreconditionerList Total Elements: 524288
I0316 10:49:25.295759 139901304898752 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 8.block_0 (torch.Size([0])) for Parameter 8 (torch.Size([1024, 1024])), Block block_0 (torch.Size([1024, 1024])).
I0316 10:49:25.295775 140193402266816 shampoo_preconditioner_list.py:384] Rank 1: AdaGradPreconditionerList Total Bytes: 2097152
W0316 10:49:25.295758 139752247178432 distributed_shampoo.py:359] Nesterov flag is enabled but momentum parameter is zero! Continuing without using momentum or Nesterov acceleration...
I0316 10:49:25.295812 139901304898752 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 9.block_0 (torch.Size([0])) for Parameter 9 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 10:49:25.295837 140218016072896 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 13.block_0 ([torch.Size([256, 256])]) for Parameter 13 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 10:49:25.295874 139901304898752 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 10.block_0 (torch.Size([0])) for Parameter 10 (torch.Size([512, 1024])), Block block_0 (torch.Size([512, 1024])).
I0316 10:49:25.295904 139623498142912 submission.py:105] Large parameters (embedding tables) detected (dim >= 1_000_000). Instantiating Row-Wise AdaGrad...
I0316 10:49:25.295937 139901304898752 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 11.block_0 (torch.Size([0])) for Parameter 11 (torch.Size([512])), Block block_0 (torch.Size([512])).
W0316 10:49:25.295923 140575969412288 distributed_shampoo.py:359] Nesterov flag is enabled but momentum parameter is zero! Continuing without using momentum or Nesterov acceleration...
I0316 10:49:25.295955 139896858342592 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 12.block_0 (torch.Size([256, 512])) for Parameter 12 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 10:49:25.295980 139901304898752 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 12.block_0 (torch.Size([0])) for Parameter 12 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
W0316 10:49:25.296003 139623498142912 distributed_shampoo.py:359] Nesterov flag is enabled but momentum parameter is zero! Continuing without using momentum or Nesterov acceleration...
I0316 10:49:25.296023 139901304898752 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 13.block_0 (torch.Size([0])) for Parameter 13 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 10:49:25.296044 140218016072896 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 14.block_0 ([torch.Size([256, 256])]) for Parameter 14 (torch.Size([1, 256])), Block block_0 (torch.Size([256])).
I0316 10:49:25.296065 139901304898752 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 14.block_0 (torch.Size([0])) for Parameter 14 (torch.Size([1, 256])), Block block_0 (torch.Size([256])).
I0316 10:49:25.296072 139896858342592 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 13.block_0 (torch.Size([0])) for Parameter 13 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 10:49:25.296108 139901304898752 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 15.block_0 (torch.Size([0])) for Parameter 15 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0316 10:49:25.296123 139896858342592 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 14.block_0 (torch.Size([0])) for Parameter 14 (torch.Size([1, 256])), Block block_0 (torch.Size([256])).
W0316 10:49:25.296144 140193402266816 distributed_shampoo.py:359] Nesterov flag is enabled but momentum parameter is zero! Continuing without using momentum or Nesterov acceleration...
I0316 10:49:25.296171 139896858342592 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 15.block_0 (torch.Size([0])) for Parameter 15 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0316 10:49:25.296172 139901304898752 shampoo_preconditioner_list.py:375] Rank 3: AdaGradPreconditionerList Numel Breakdown: (0, 0, 131072, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)
I0316 10:49:25.296205 140218016072896 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 15.block_0 ([torch.Size([1, 1])]) for Parameter 15 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0316 10:49:25.296230 139896858342592 shampoo_preconditioner_list.py:375] Rank 4: AdaGradPreconditionerList Numel Breakdown: (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 131072, 0, 0, 0)
I0316 10:49:25.296236 139901304898752 shampoo_preconditioner_list.py:378] Rank 3: AdaGradPreconditionerList Bytes Breakdown: (0, 0, 524288, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)
W0316 10:49:25.296223 140007357965504 distributed_shampoo.py:359] Nesterov flag is enabled but momentum parameter is zero! Continuing without using momentum or Nesterov acceleration...
I0316 10:49:25.296262 139896858342592 shampoo_preconditioner_list.py:378] Rank 4: AdaGradPreconditionerList Bytes Breakdown: (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 524288, 0, 0, 0)
I0316 10:49:25.296265 139901304898752 shampoo_preconditioner_list.py:381] Rank 3: AdaGradPreconditionerList Total Elements: 131072
I0316 10:49:25.296290 139896858342592 shampoo_preconditioner_list.py:381] Rank 4: AdaGradPreconditionerList Total Elements: 131072
I0316 10:49:25.296300 139901304898752 shampoo_preconditioner_list.py:384] Rank 3: AdaGradPreconditionerList Total Bytes: 524288
I0316 10:49:25.296331 139896858342592 shampoo_preconditioner_list.py:384] Rank 4: AdaGradPreconditionerList Total Bytes: 524288
I0316 10:49:25.296343 140218016072896 shampoo_preconditioner_list.py:612] Rank 7: ShampooPreconditionerList Numel Breakdown: (524626, 524288, 655360, 131072, 163840, 32768, 2609224, 2097152, 4194304, 2097152, 2621440, 524288, 655360, 131072, 131072, 2)
I0316 10:49:25.296392 140218016072896 shampoo_preconditioner_list.py:615] Rank 7: ShampooPreconditionerList Bytes Breakdown: (2098504, 2097152, 2621440, 524288, 655360, 131072, 10436896, 8388608, 16777216)
I0316 10:49:25.296431 140218016072896 shampoo_preconditioner_list.py:618] Rank 7: ShampooPreconditionerList Total Elements: 17093020
I0316 10:49:25.296472 140218016072896 shampoo_preconditioner_list.py:621] Rank 7: ShampooPreconditionerList Total Bytes: 43730536
I0316 10:49:25.296615 140218016072896 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([512, 13])), Block block_0 (torch.Size([512, 13])).
W0316 10:49:25.296706 139901304898752 distributed_shampoo.py:359] Nesterov flag is enabled but momentum parameter is zero! Continuing without using momentum or Nesterov acceleration...
W0316 10:49:25.296713 139896858342592 distributed_shampoo.py:359] Nesterov flag is enabled but momentum parameter is zero! Continuing without using momentum or Nesterov acceleration...
I0316 10:49:25.296736 140218016072896 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([512])) for Parameter 1 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 10:49:25.296803 140218016072896 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 10:49:25.296890 140218016072896 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([256])) for Parameter 3 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 10:49:25.296959 140218016072896 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 4.block_0 (torch.Size([0])) for Parameter 4 (torch.Size([128, 256])), Block block_0 (torch.Size([128, 256])).
I0316 10:49:25.297040 140218016072896 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 5.block_0 (torch.Size([128])) for Parameter 5 (torch.Size([128])), Block block_0 (torch.Size([128])).
I0316 10:49:25.297108 140218016072896 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 6.block_0 (torch.Size([0])) for Parameter 6 (torch.Size([1024, 506])), Block block_0 (torch.Size([1024, 506])).
I0316 10:49:25.297194 140218016072896 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 7.block_0 (torch.Size([1024])) for Parameter 7 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 10:49:25.297210 139752247178432 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 10:49:25.297262 140218016072896 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 8.block_0 (torch.Size([0])) for Parameter 8 (torch.Size([1024, 1024])), Block block_0 (torch.Size([1024, 1024])).
I0316 10:49:25.297318 139752247178432 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_1 (torch.Size([0])) for Parameter 0 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 10:49:25.297357 140218016072896 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 9.block_0 (torch.Size([1024])) for Parameter 9 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 10:49:25.297387 139752247178432 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 10:49:25.297424 140218016072896 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 10.block_0 (torch.Size([0])) for Parameter 10 (torch.Size([512, 1024])), Block block_0 (torch.Size([512, 1024])).
I0316 10:49:25.297445 139752247178432 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_1 (torch.Size([0])) for Parameter 1 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 10:49:25.297501 139752247178432 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 10:49:25.297503 140218016072896 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 11.block_0 (torch.Size([512])) for Parameter 11 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 10:49:25.297577 139752247178432 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_1 (torch.Size([0])) for Parameter 2 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 10:49:25.297582 140218016072896 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 12.block_0 (torch.Size([0])) for Parameter 12 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 10:49:25.297636 140575969412288 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 10:49:25.297671 140218016072896 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 13.block_0 (torch.Size([256])) for Parameter 13 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 10:49:25.297741 140575969412288 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_1 (torch.Size([0])) for Parameter 0 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 10:49:25.297747 140218016072896 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 14.block_0 (torch.Size([256])) for Parameter 14 (torch.Size([1, 256])), Block block_0 (torch.Size([256])).
I0316 10:49:25.297818 140218016072896 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 15.block_0 (torch.Size([1])) for Parameter 15 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0316 10:49:25.297905 140218016072896 shampoo_preconditioner_list.py:375] Rank 7: AdaGradPreconditionerList Numel Breakdown: (0, 512, 0, 256, 0, 128, 0, 1024, 0, 1024, 0, 512, 0, 256, 256, 1)
I0316 10:49:25.297949 140218016072896 shampoo_preconditioner_list.py:378] Rank 7: AdaGradPreconditionerList Bytes Breakdown: (0, 2048, 0, 1024, 0, 512, 0, 4096, 0, 4096, 0, 2048, 0, 1024, 1024, 4)
I0316 10:49:25.297985 140218016072896 shampoo_preconditioner_list.py:381] Rank 7: AdaGradPreconditionerList Total Elements: 3969
I0316 10:49:25.298020 140218016072896 shampoo_preconditioner_list.py:384] Rank 7: AdaGradPreconditionerList Total Bytes: 15876
I0316 10:49:25.298342 140193402266816 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 10:49:25.298545 140007357965504 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
W0316 10:49:25.298559 140218016072896 distributed_shampoo.py:359] Nesterov flag is enabled but momentum parameter is zero! Continuing without using momentum or Nesterov acceleration...
I0316 10:49:25.298652 140007357965504 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_1 (torch.Size([0])) for Parameter 0 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 10:49:25.298724 140007357965504 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 10:49:25.298786 140007357965504 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_1 (torch.Size([0])) for Parameter 1 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 10:49:25.298845 140007357965504 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 10:49:25.298995 139896858342592 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 10:49:25.299061 139901304898752 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 10:49:25.299081 139752247178432 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([524288, 128])) for Parameter 3 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 10:49:25.299104 139896858342592 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_1 (torch.Size([0])) for Parameter 0 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 10:49:25.299168 139896858342592 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 10:49:25.299144 140575969412288 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([524288, 128])) for Parameter 1 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 10:49:25.299167 139901304898752 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_1 (torch.Size([0])) for Parameter 0 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 10:49:25.299182 139752247178432 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_1 (torch.Size([0])) for Parameter 3 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 10:49:25.299229 139896858342592 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_1 (torch.Size([0])) for Parameter 1 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 10:49:25.299236 139752247178432 shampoo_preconditioner_list.py:375] Rank 6: AdaGradPreconditionerList Numel Breakdown: (0, 0, 0, 0, 0, 0, 67108864, 0)
I0316 10:49:25.299240 139901304898752 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 10:49:25.299248 140575969412288 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_1 (torch.Size([0])) for Parameter 1 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 10:49:25.299270 139752247178432 shampoo_preconditioner_list.py:378] Rank 6: AdaGradPreconditionerList Bytes Breakdown: (0, 0, 0, 0, 0, 0, 268435456, 0)
I0316 10:49:25.299290 139623498142912 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([524288, 128])) for Parameter 0 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 10:49:25.299330 139752247178432 shampoo_preconditioner_list.py:381] Rank 6: AdaGradPreconditionerList Total Elements: 67108864
I0316 10:49:25.299328 140575969412288 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 10:49:25.299359 139752247178432 shampoo_preconditioner_list.py:384] Rank 6: AdaGradPreconditionerList Total Bytes: 268435456
I0316 10:49:25.299380 140575969412288 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_1 (torch.Size([0])) for Parameter 2 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 10:49:25.299410 139623498142912 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_1 (torch.Size([0])) for Parameter 0 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 10:49:25.299436 140575969412288 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 10:49:25.299480 139623498142912 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 10:49:25.299484 140575969412288 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_1 (torch.Size([0])) for Parameter 3 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 10:49:25.299530 140575969412288 shampoo_preconditioner_list.py:375] Rank 2: AdaGradPreconditionerList Numel Breakdown: (0, 0, 67108864, 0, 0, 0, 0, 0)
I0316 10:49:25.299540 139623498142912 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_1 (torch.Size([0])) for Parameter 1 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 10:49:25.299566 140575969412288 shampoo_preconditioner_list.py:378] Rank 2: AdaGradPreconditionerList Bytes Breakdown: (0, 0, 268435456, 0, 0, 0, 0, 0)
I0316 10:49:25.299592 139623498142912 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 10:49:25.299599 140575969412288 shampoo_preconditioner_list.py:381] Rank 2: AdaGradPreconditionerList Total Elements: 67108864
I0316 10:49:25.299626 140575969412288 shampoo_preconditioner_list.py:384] Rank 2: AdaGradPreconditionerList Total Bytes: 268435456
I0316 10:49:25.299648 139623498142912 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_1 (torch.Size([0])) for Parameter 2 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 10:49:25.299705 139623498142912 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 10:49:25.299710 140193402266816 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_1 (torch.Size([524288, 128])) for Parameter 0 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 10:49:25.299760 139623498142912 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_1 (torch.Size([0])) for Parameter 3 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 10:49:25.299823 139623498142912 shampoo_preconditioner_list.py:375] Rank 0: AdaGradPreconditionerList Numel Breakdown: (67108864, 0, 0, 0, 0, 0, 0, 0)
I0316 10:49:25.299825 140193402266816 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 10:49:25.299906 139623498142912 shampoo_preconditioner_list.py:378] Rank 0: AdaGradPreconditionerList Bytes Breakdown: (268435456, 0, 0, 0, 0, 0, 0, 0)
I0316 10:49:25.299916 140193402266816 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_1 (torch.Size([0])) for Parameter 1 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 10:49:25.299943 139623498142912 shampoo_preconditioner_list.py:381] Rank 0: AdaGradPreconditionerList Total Elements: 67108864
I0316 10:49:25.299971 140193402266816 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 10:49:25.299976 139623498142912 shampoo_preconditioner_list.py:384] Rank 0: AdaGradPreconditionerList Total Bytes: 268435456
I0316 10:49:25.300028 140193402266816 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_1 (torch.Size([0])) for Parameter 2 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 10:49:25.300083 140193402266816 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 10:49:25.300135 140193402266816 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_1 (torch.Size([0])) for Parameter 3 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 10:49:25.300184 140193402266816 shampoo_preconditioner_list.py:375] Rank 1: AdaGradPreconditionerList Numel Breakdown: (0, 67108864, 0, 0, 0, 0, 0, 0)
I0316 10:49:25.300190 140007357965504 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_1 (torch.Size([524288, 128])) for Parameter 2 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 10:49:25.300221 140193402266816 shampoo_preconditioner_list.py:378] Rank 1: AdaGradPreconditionerList Bytes Breakdown: (0, 268435456, 0, 0, 0, 0, 0, 0)
I0316 10:49:25.300249 140193402266816 shampoo_preconditioner_list.py:381] Rank 1: AdaGradPreconditionerList Total Elements: 67108864
I0316 10:49:25.300280 140193402266816 shampoo_preconditioner_list.py:384] Rank 1: AdaGradPreconditionerList Total Bytes: 268435456
I0316 10:49:25.300305 140007357965504 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 10:49:25.300379 140007357965504 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_1 (torch.Size([0])) for Parameter 3 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 10:49:25.300432 140007357965504 shampoo_preconditioner_list.py:375] Rank 5: AdaGradPreconditionerList Numel Breakdown: (0, 0, 0, 0, 0, 67108864, 0, 0)
I0316 10:49:25.300428 139901304898752 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_1 (torch.Size([524288, 128])) for Parameter 1 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 10:49:25.300469 140007357965504 shampoo_preconditioner_list.py:378] Rank 5: AdaGradPreconditionerList Bytes Breakdown: (0, 0, 0, 0, 0, 268435456, 0, 0)
I0316 10:49:25.300498 140007357965504 shampoo_preconditioner_list.py:381] Rank 5: AdaGradPreconditionerList Total Elements: 67108864
I0316 10:49:25.300529 140007357965504 shampoo_preconditioner_list.py:384] Rank 5: AdaGradPreconditionerList Total Bytes: 268435456
I0316 10:49:25.300536 139901304898752 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 10:49:25.300589 139901304898752 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_1 (torch.Size([0])) for Parameter 2 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 10:49:25.300645 139901304898752 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 10:49:25.300699 139901304898752 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_1 (torch.Size([0])) for Parameter 3 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 10:49:25.300690 140218016072896 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 10:49:25.300749 139901304898752 shampoo_preconditioner_list.py:375] Rank 3: AdaGradPreconditionerList Numel Breakdown: (0, 0, 0, 67108864, 0, 0, 0, 0)
I0316 10:49:25.300740 139896858342592 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([524288, 128])) for Parameter 2 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 10:49:25.300777 140218016072896 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_1 (torch.Size([0])) for Parameter 0 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 10:49:25.300785 139901304898752 shampoo_preconditioner_list.py:378] Rank 3: AdaGradPreconditionerList Bytes Breakdown: (0, 0, 0, 268435456, 0, 0, 0, 0)
I0316 10:49:25.300813 139901304898752 shampoo_preconditioner_list.py:381] Rank 3: AdaGradPreconditionerList Total Elements: 67108864
I0316 10:49:25.300844 139901304898752 shampoo_preconditioner_list.py:384] Rank 3: AdaGradPreconditionerList Total Bytes: 268435456
I0316 10:49:25.300838 139896858342592 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_1 (torch.Size([0])) for Parameter 2 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 10:49:25.300850 140218016072896 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 10:49:25.300892 139896858342592 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 10:49:25.300904 140218016072896 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_1 (torch.Size([0])) for Parameter 1 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 10:49:25.300953 140218016072896 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 10:49:25.300961 139896858342592 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_1 (torch.Size([0])) for Parameter 3 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 10:49:25.301008 139896858342592 shampoo_preconditioner_list.py:375] Rank 4: AdaGradPreconditionerList Numel Breakdown: (0, 0, 0, 0, 67108864, 0, 0, 0)
I0316 10:49:25.301009 140218016072896 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_1 (torch.Size([0])) for Parameter 2 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 10:49:25.301045 139896858342592 shampoo_preconditioner_list.py:378] Rank 4: AdaGradPreconditionerList Bytes Breakdown: (0, 0, 0, 0, 268435456, 0, 0, 0)
I0316 10:49:25.301059 140218016072896 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 10:49:25.301077 139896858342592 shampoo_preconditioner_list.py:381] Rank 4: AdaGradPreconditionerList Total Elements: 67108864
I0316 10:49:25.301108 139896858342592 shampoo_preconditioner_list.py:384] Rank 4: AdaGradPreconditionerList Total Bytes: 268435456
I0316 10:49:25.301751 139623498142912 submission_runner.py:279] Initializing metrics bundle.
I0316 10:49:25.301787 139752247178432 logger_utils.py:262] Unable to record workload.train_mean information. Continuing without it.
I0316 10:49:25.301849 139752247178432 logger_utils.py:262] Unable to record workload.train_stddev information. Continuing without it.
I0316 10:49:25.301917 139623498142912 submission_runner.py:301] Initializing checkpoint and logger.
I0316 10:49:25.302031 140575969412288 logger_utils.py:262] Unable to record workload.train_mean information. Continuing without it.
I0316 10:49:25.302100 140575969412288 logger_utils.py:262] Unable to record workload.train_stddev information. Continuing without it.
I0316 10:49:25.302271 140218016072896 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_1 (torch.Size([524288, 128])) for Parameter 3 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 10:49:25.302364 140218016072896 shampoo_preconditioner_list.py:375] Rank 7: AdaGradPreconditionerList Numel Breakdown: (0, 0, 0, 0, 0, 0, 0, 67108864)
I0316 10:49:25.302345 140193402266816 logger_utils.py:262] Unable to record workload.train_mean information. Continuing without it.
I0316 10:49:25.302403 140218016072896 shampoo_preconditioner_list.py:378] Rank 7: AdaGradPreconditionerList Bytes Breakdown: (0, 0, 0, 0, 0, 0, 0, 268435456)
I0316 10:49:25.302386 139623498142912 submission_runner.py:321] Saving meta data to /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_0/criteo1tb_pytorch/trial_5/meta_data_0.json.
I0316 10:49:25.302414 140193402266816 logger_utils.py:262] Unable to record workload.train_stddev information. Continuing without it.
I0316 10:49:25.302436 140218016072896 shampoo_preconditioner_list.py:381] Rank 7: AdaGradPreconditionerList Total Elements: 67108864
I0316 10:49:25.302466 140218016072896 shampoo_preconditioner_list.py:384] Rank 7: AdaGradPreconditionerList Total Bytes: 268435456
I0316 10:49:25.302546 139623498142912 logger_utils.py:262] Unable to record workload.train_mean information. Continuing without it.
I0316 10:49:25.302593 139623498142912 logger_utils.py:262] Unable to record workload.train_stddev information. Continuing without it.
I0316 10:49:25.302665 140007357965504 logger_utils.py:262] Unable to record workload.train_mean information. Continuing without it.
I0316 10:49:25.302732 140007357965504 logger_utils.py:262] Unable to record workload.train_stddev information. Continuing without it.
I0316 10:49:25.303048 139901304898752 logger_utils.py:262] Unable to record workload.train_mean information. Continuing without it.
I0316 10:49:25.303063 139896858342592 logger_utils.py:262] Unable to record workload.train_mean information. Continuing without it.
I0316 10:49:25.303121 139896858342592 logger_utils.py:262] Unable to record workload.train_stddev information. Continuing without it.
I0316 10:49:25.303122 139901304898752 logger_utils.py:262] Unable to record workload.train_stddev information. Continuing without it.
I0316 10:49:25.303756 140218016072896 logger_utils.py:262] Unable to record workload.train_mean information. Continuing without it.
I0316 10:49:25.303829 140218016072896 logger_utils.py:262] Unable to record workload.train_stddev information. Continuing without it.
I0316 10:49:26.041246 139623498142912 submission_runner.py:325] Saving flags to /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_0/criteo1tb_pytorch/trial_5/flags_0.json.
I0316 10:49:26.145856 139623498142912 submission_runner.py:337] Starting training loop.
I0316 10:49:35.220493 139595455915776 logging_writer.py:48] [0] global_step=0, grad_norm=11.8323, loss=0.860071
I0316 10:49:35.489675 139623498142912 submission.py:265] 0) loss = 0.860, grad_norm = 11.832
I0316 10:49:35.876551 139623498142912 spec.py:321] Evaluating on the training split.
I0316 10:54:48.924271 139623498142912 spec.py:333] Evaluating on the validation split.
I0316 10:59:47.210293 139623498142912 spec.py:349] Evaluating on the test split.
I0316 11:05:33.941209 139623498142912 submission_runner.py:469] Time since start: 967.80s, 	Step: 1, 	{'train/loss': 0.8509040423757784, 'validation/loss': 0.8602193090610964, 'validation/num_examples': 83274637, 'test/loss': 0.8528632051263106, 'test/num_examples': 95000000, 'score': 9.34485936164856, 'total_duration': 967.795530796051, 'accumulated_submission_time': 9.34485936164856, 'accumulated_eval_time': 958.064713716507, 'accumulated_logging_time': 0}
I0316 11:05:33.996923 139582158903040 logging_writer.py:48] [1] accumulated_eval_time=958.065, accumulated_logging_time=0, accumulated_submission_time=9.34486, global_step=1, preemption_count=0, score=9.34486, test/loss=0.852863, test/num_examples=95000000, total_duration=967.796, train/loss=0.850904, validation/loss=0.860219, validation/num_examples=83274637
I0316 11:05:34.652674 139581538170624 logging_writer.py:48] [1] global_step=1, grad_norm=11.8408, loss=0.860237
I0316 11:05:34.656015 139623498142912 submission.py:265] 1) loss = 0.860, grad_norm = 11.841
I0316 11:05:34.846987 139582158903040 logging_writer.py:48] [2] global_step=2, grad_norm=11.7471, loss=0.851675
I0316 11:05:34.850184 139623498142912 submission.py:265] 2) loss = 0.852, grad_norm = 11.747
I0316 11:05:35.041356 139581538170624 logging_writer.py:48] [3] global_step=3, grad_norm=11.6416, loss=0.836898
I0316 11:05:35.044362 139623498142912 submission.py:265] 3) loss = 0.837, grad_norm = 11.642
I0316 11:05:35.236778 139582158903040 logging_writer.py:48] [4] global_step=4, grad_norm=11.4067, loss=0.816893
I0316 11:05:35.239804 139623498142912 submission.py:265] 4) loss = 0.817, grad_norm = 11.407
I0316 11:05:35.431101 139581538170624 logging_writer.py:48] [5] global_step=5, grad_norm=11.1643, loss=0.790662
I0316 11:05:35.434769 139623498142912 submission.py:265] 5) loss = 0.791, grad_norm = 11.164
I0316 11:05:35.628656 139582158903040 logging_writer.py:48] [6] global_step=6, grad_norm=10.8193, loss=0.760665
I0316 11:05:35.631825 139623498142912 submission.py:265] 6) loss = 0.761, grad_norm = 10.819
I0316 11:05:35.824170 139581538170624 logging_writer.py:48] [7] global_step=7, grad_norm=10.4292, loss=0.725962
I0316 11:05:35.826784 139623498142912 submission.py:265] 7) loss = 0.726, grad_norm = 10.429
I0316 11:05:36.018554 139582158903040 logging_writer.py:48] [8] global_step=8, grad_norm=9.97005, loss=0.687621
I0316 11:05:36.021400 139623498142912 submission.py:265] 8) loss = 0.688, grad_norm = 9.970
I0316 11:05:36.211325 139581538170624 logging_writer.py:48] [9] global_step=9, grad_norm=9.44963, loss=0.646525
I0316 11:05:36.214114 139623498142912 submission.py:265] 9) loss = 0.647, grad_norm = 9.450
I0316 11:05:36.406373 139582158903040 logging_writer.py:48] [10] global_step=10, grad_norm=8.8322, loss=0.605229
I0316 11:05:36.409743 139623498142912 submission.py:265] 10) loss = 0.605, grad_norm = 8.832
I0316 11:05:36.601996 139581538170624 logging_writer.py:48] [11] global_step=11, grad_norm=8.15388, loss=0.56258
I0316 11:05:36.605299 139623498142912 submission.py:265] 11) loss = 0.563, grad_norm = 8.154
I0316 11:05:36.796398 139582158903040 logging_writer.py:48] [12] global_step=12, grad_norm=7.47249, loss=0.519098
I0316 11:05:36.799573 139623498142912 submission.py:265] 12) loss = 0.519, grad_norm = 7.472
I0316 11:05:36.992568 139581538170624 logging_writer.py:48] [13] global_step=13, grad_norm=6.74134, loss=0.47667
I0316 11:05:36.995836 139623498142912 submission.py:265] 13) loss = 0.477, grad_norm = 6.741
I0316 11:05:37.186583 139582158903040 logging_writer.py:48] [14] global_step=14, grad_norm=6.02126, loss=0.436289
I0316 11:05:37.189585 139623498142912 submission.py:265] 14) loss = 0.436, grad_norm = 6.021
I0316 11:05:37.381394 139581538170624 logging_writer.py:48] [15] global_step=15, grad_norm=5.28165, loss=0.398802
I0316 11:05:37.384619 139623498142912 submission.py:265] 15) loss = 0.399, grad_norm = 5.282
I0316 11:05:37.577769 139582158903040 logging_writer.py:48] [16] global_step=16, grad_norm=4.55877, loss=0.364238
I0316 11:05:37.582123 139623498142912 submission.py:265] 16) loss = 0.364, grad_norm = 4.559
I0316 11:05:37.774432 139581538170624 logging_writer.py:48] [17] global_step=17, grad_norm=3.92838, loss=0.332649
I0316 11:05:37.777410 139623498142912 submission.py:265] 17) loss = 0.333, grad_norm = 3.928
I0316 11:05:37.968114 139582158903040 logging_writer.py:48] [18] global_step=18, grad_norm=3.34438, loss=0.304442
I0316 11:05:37.971759 139623498142912 submission.py:265] 18) loss = 0.304, grad_norm = 3.344
I0316 11:05:38.162628 139581538170624 logging_writer.py:48] [19] global_step=19, grad_norm=2.58237, loss=0.265246
I0316 11:05:38.165625 139623498142912 submission.py:265] 19) loss = 0.265, grad_norm = 2.582
I0316 11:05:38.358024 139582158903040 logging_writer.py:48] [20] global_step=20, grad_norm=2.05306, loss=0.244858
I0316 11:05:38.361221 139623498142912 submission.py:265] 20) loss = 0.245, grad_norm = 2.053
I0316 11:05:38.554452 139581538170624 logging_writer.py:48] [21] global_step=21, grad_norm=1.61606, loss=0.228607
I0316 11:05:38.558089 139623498142912 submission.py:265] 21) loss = 0.229, grad_norm = 1.616
I0316 11:05:38.757812 139582158903040 logging_writer.py:48] [22] global_step=22, grad_norm=1.1897, loss=0.220796
I0316 11:05:38.761273 139623498142912 submission.py:265] 22) loss = 0.221, grad_norm = 1.190
I0316 11:05:38.952787 139581538170624 logging_writer.py:48] [23] global_step=23, grad_norm=0.850861, loss=0.210946
I0316 11:05:38.956467 139623498142912 submission.py:265] 23) loss = 0.211, grad_norm = 0.851
I0316 11:05:39.147114 139582158903040 logging_writer.py:48] [24] global_step=24, grad_norm=0.556439, loss=0.205019
I0316 11:05:39.150671 139623498142912 submission.py:265] 24) loss = 0.205, grad_norm = 0.556
I0316 11:05:39.341782 139581538170624 logging_writer.py:48] [25] global_step=25, grad_norm=0.34297, loss=0.20264
I0316 11:05:39.345154 139623498142912 submission.py:265] 25) loss = 0.203, grad_norm = 0.343
I0316 11:05:39.534336 139582158903040 logging_writer.py:48] [26] global_step=26, grad_norm=0.255986, loss=0.19654
I0316 11:05:39.537390 139623498142912 submission.py:265] 26) loss = 0.197, grad_norm = 0.256
I0316 11:05:39.746734 139581538170624 logging_writer.py:48] [27] global_step=27, grad_norm=0.300072, loss=0.194997
I0316 11:05:39.749962 139623498142912 submission.py:265] 27) loss = 0.195, grad_norm = 0.300
I0316 11:05:39.937687 139582158903040 logging_writer.py:48] [28] global_step=28, grad_norm=0.459385, loss=0.197953
I0316 11:05:39.940740 139623498142912 submission.py:265] 28) loss = 0.198, grad_norm = 0.459
I0316 11:05:40.129785 139581538170624 logging_writer.py:48] [29] global_step=29, grad_norm=0.618983, loss=0.202291
I0316 11:05:40.133103 139623498142912 submission.py:265] 29) loss = 0.202, grad_norm = 0.619
I0316 11:05:40.405222 139582158903040 logging_writer.py:48] [30] global_step=30, grad_norm=0.767947, loss=0.206054
I0316 11:05:40.408269 139623498142912 submission.py:265] 30) loss = 0.206, grad_norm = 0.768
I0316 11:05:41.451934 139581538170624 logging_writer.py:48] [31] global_step=31, grad_norm=0.860626, loss=0.206295
I0316 11:05:41.454935 139623498142912 submission.py:265] 31) loss = 0.206, grad_norm = 0.861
I0316 11:05:42.607254 139582158903040 logging_writer.py:48] [32] global_step=32, grad_norm=1.02597, loss=0.217029
I0316 11:05:42.610272 139623498142912 submission.py:265] 32) loss = 0.217, grad_norm = 1.026
I0316 11:05:43.993360 139581538170624 logging_writer.py:48] [33] global_step=33, grad_norm=1.13122, loss=0.223942
I0316 11:05:43.996422 139623498142912 submission.py:265] 33) loss = 0.224, grad_norm = 1.131
I0316 11:05:45.300418 139582158903040 logging_writer.py:48] [34] global_step=34, grad_norm=1.20396, loss=0.228132
I0316 11:05:45.303552 139623498142912 submission.py:265] 34) loss = 0.228, grad_norm = 1.204
I0316 11:05:46.462145 139581538170624 logging_writer.py:48] [35] global_step=35, grad_norm=1.28745, loss=0.233909
I0316 11:05:46.465130 139623498142912 submission.py:265] 35) loss = 0.234, grad_norm = 1.287
I0316 11:05:47.645746 139582158903040 logging_writer.py:48] [36] global_step=36, grad_norm=1.3646, loss=0.24025
I0316 11:05:47.649064 139623498142912 submission.py:265] 36) loss = 0.240, grad_norm = 1.365
I0316 11:05:48.898671 139581538170624 logging_writer.py:48] [37] global_step=37, grad_norm=1.4186, loss=0.244294
I0316 11:05:48.902498 139623498142912 submission.py:265] 37) loss = 0.244, grad_norm = 1.419
I0316 11:05:50.245241 139582158903040 logging_writer.py:48] [38] global_step=38, grad_norm=1.5526, loss=0.26086
I0316 11:05:50.248262 139623498142912 submission.py:265] 38) loss = 0.261, grad_norm = 1.553
I0316 11:05:51.323899 139581538170624 logging_writer.py:48] [39] global_step=39, grad_norm=1.60481, loss=0.266961
I0316 11:05:51.326990 139623498142912 submission.py:265] 39) loss = 0.267, grad_norm = 1.605
I0316 11:05:52.494038 139582158903040 logging_writer.py:48] [40] global_step=40, grad_norm=1.69107, loss=0.276708
I0316 11:05:52.497122 139623498142912 submission.py:265] 40) loss = 0.277, grad_norm = 1.691
I0316 11:05:53.399220 139581538170624 logging_writer.py:48] [41] global_step=41, grad_norm=1.68522, loss=0.27433
I0316 11:05:53.402164 139623498142912 submission.py:265] 41) loss = 0.274, grad_norm = 1.685
I0316 11:05:54.855562 139582158903040 logging_writer.py:48] [42] global_step=42, grad_norm=1.74856, loss=0.281711
I0316 11:05:54.858600 139623498142912 submission.py:265] 42) loss = 0.282, grad_norm = 1.749
I0316 11:05:55.938665 139581538170624 logging_writer.py:48] [43] global_step=43, grad_norm=1.75899, loss=0.283049
I0316 11:05:55.941736 139623498142912 submission.py:265] 43) loss = 0.283, grad_norm = 1.759
I0316 11:05:57.554689 139582158903040 logging_writer.py:48] [44] global_step=44, grad_norm=1.79085, loss=0.286631
I0316 11:05:57.557728 139623498142912 submission.py:265] 44) loss = 0.287, grad_norm = 1.791
I0316 11:05:59.054673 139581538170624 logging_writer.py:48] [45] global_step=45, grad_norm=1.80174, loss=0.287686
I0316 11:05:59.057779 139623498142912 submission.py:265] 45) loss = 0.288, grad_norm = 1.802
I0316 11:05:59.878923 139582158903040 logging_writer.py:48] [46] global_step=46, grad_norm=1.82826, loss=0.291264
I0316 11:05:59.882191 139623498142912 submission.py:265] 46) loss = 0.291, grad_norm = 1.828
I0316 11:06:01.449588 139581538170624 logging_writer.py:48] [47] global_step=47, grad_norm=1.81697, loss=0.289374
I0316 11:06:01.452865 139623498142912 submission.py:265] 47) loss = 0.289, grad_norm = 1.817
I0316 11:06:02.101394 139582158903040 logging_writer.py:48] [48] global_step=48, grad_norm=1.76967, loss=0.283832
I0316 11:06:02.104510 139623498142912 submission.py:265] 48) loss = 0.284, grad_norm = 1.770
I0316 11:06:03.647305 139581538170624 logging_writer.py:48] [49] global_step=49, grad_norm=1.7978, loss=0.288433
I0316 11:06:03.650381 139623498142912 submission.py:265] 49) loss = 0.288, grad_norm = 1.798
I0316 11:06:04.900448 139582158903040 logging_writer.py:48] [50] global_step=50, grad_norm=1.76523, loss=0.283705
I0316 11:06:04.903453 139623498142912 submission.py:265] 50) loss = 0.284, grad_norm = 1.765
I0316 11:06:05.951826 139581538170624 logging_writer.py:48] [51] global_step=51, grad_norm=1.69783, loss=0.275439
I0316 11:06:05.954895 139623498142912 submission.py:265] 51) loss = 0.275, grad_norm = 1.698
I0316 11:06:07.153367 139582158903040 logging_writer.py:48] [52] global_step=52, grad_norm=1.66249, loss=0.271436
I0316 11:06:07.156738 139623498142912 submission.py:265] 52) loss = 0.271, grad_norm = 1.662
I0316 11:06:08.517404 139581538170624 logging_writer.py:48] [53] global_step=53, grad_norm=1.64535, loss=0.268812
I0316 11:06:08.520467 139623498142912 submission.py:265] 53) loss = 0.269, grad_norm = 1.645
I0316 11:06:09.659687 139582158903040 logging_writer.py:48] [54] global_step=54, grad_norm=1.61961, loss=0.265205
I0316 11:06:09.662784 139623498142912 submission.py:265] 54) loss = 0.265, grad_norm = 1.620
I0316 11:06:11.608138 139581538170624 logging_writer.py:48] [55] global_step=55, grad_norm=1.5277, loss=0.254563
I0316 11:06:11.611346 139623498142912 submission.py:265] 55) loss = 0.255, grad_norm = 1.528
I0316 11:06:12.634278 139582158903040 logging_writer.py:48] [56] global_step=56, grad_norm=1.51291, loss=0.253729
I0316 11:06:12.637413 139623498142912 submission.py:265] 56) loss = 0.254, grad_norm = 1.513
I0316 11:06:14.454731 139581538170624 logging_writer.py:48] [57] global_step=57, grad_norm=1.26834, loss=0.223313
I0316 11:06:14.457776 139623498142912 submission.py:265] 57) loss = 0.223, grad_norm = 1.268
I0316 11:06:15.266566 139582158903040 logging_writer.py:48] [58] global_step=58, grad_norm=1.24667, loss=0.221785
I0316 11:06:15.269520 139623498142912 submission.py:265] 58) loss = 0.222, grad_norm = 1.247
I0316 11:06:17.061410 139581538170624 logging_writer.py:48] [59] global_step=59, grad_norm=1.19002, loss=0.217527
I0316 11:06:17.064518 139623498142912 submission.py:265] 59) loss = 0.218, grad_norm = 1.190
I0316 11:06:18.189307 139582158903040 logging_writer.py:48] [60] global_step=60, grad_norm=1.13955, loss=0.212255
I0316 11:06:18.192766 139623498142912 submission.py:265] 60) loss = 0.212, grad_norm = 1.140
I0316 11:06:20.136045 139581538170624 logging_writer.py:48] [61] global_step=61, grad_norm=1.06773, loss=0.205994
I0316 11:06:20.139704 139623498142912 submission.py:265] 61) loss = 0.206, grad_norm = 1.068
I0316 11:06:21.195420 139582158903040 logging_writer.py:48] [62] global_step=62, grad_norm=0.971412, loss=0.198061
I0316 11:06:21.198553 139623498142912 submission.py:265] 62) loss = 0.198, grad_norm = 0.971
I0316 11:06:23.204673 139581538170624 logging_writer.py:48] [63] global_step=63, grad_norm=0.94359, loss=0.196159
I0316 11:06:23.207814 139623498142912 submission.py:265] 63) loss = 0.196, grad_norm = 0.944
I0316 11:06:24.370744 139582158903040 logging_writer.py:48] [64] global_step=64, grad_norm=0.797527, loss=0.182545
I0316 11:06:24.374192 139623498142912 submission.py:265] 64) loss = 0.183, grad_norm = 0.798
I0316 11:06:25.306579 139581538170624 logging_writer.py:48] [65] global_step=65, grad_norm=0.795025, loss=0.186557
I0316 11:06:25.309787 139623498142912 submission.py:265] 65) loss = 0.187, grad_norm = 0.795
I0316 11:06:27.464494 139582158903040 logging_writer.py:48] [66] global_step=66, grad_norm=0.673851, loss=0.176267
I0316 11:06:27.467768 139623498142912 submission.py:265] 66) loss = 0.176, grad_norm = 0.674
I0316 11:06:28.488502 139581538170624 logging_writer.py:48] [67] global_step=67, grad_norm=0.59197, loss=0.17319
I0316 11:06:28.491518 139623498142912 submission.py:265] 67) loss = 0.173, grad_norm = 0.592
I0316 11:06:30.070789 139582158903040 logging_writer.py:48] [68] global_step=68, grad_norm=0.532157, loss=0.172237
I0316 11:06:30.073847 139623498142912 submission.py:265] 68) loss = 0.172, grad_norm = 0.532
I0316 11:06:31.438206 139581538170624 logging_writer.py:48] [69] global_step=69, grad_norm=0.432272, loss=0.168061
I0316 11:06:31.441346 139623498142912 submission.py:265] 69) loss = 0.168, grad_norm = 0.432
I0316 11:06:32.305683 139582158903040 logging_writer.py:48] [70] global_step=70, grad_norm=0.330394, loss=0.164143
I0316 11:06:32.308860 139623498142912 submission.py:265] 70) loss = 0.164, grad_norm = 0.330
I0316 11:06:33.488244 139581538170624 logging_writer.py:48] [71] global_step=71, grad_norm=0.265017, loss=0.164327
I0316 11:06:33.491520 139623498142912 submission.py:265] 71) loss = 0.164, grad_norm = 0.265
I0316 11:06:35.061506 139582158903040 logging_writer.py:48] [72] global_step=72, grad_norm=0.20282, loss=0.163593
I0316 11:06:35.064725 139623498142912 submission.py:265] 72) loss = 0.164, grad_norm = 0.203
I0316 11:06:37.331225 139581538170624 logging_writer.py:48] [73] global_step=73, grad_norm=0.177432, loss=0.160599
I0316 11:06:37.334469 139623498142912 submission.py:265] 73) loss = 0.161, grad_norm = 0.177
I0316 11:06:38.156358 139582158903040 logging_writer.py:48] [74] global_step=74, grad_norm=0.226538, loss=0.158451
I0316 11:06:38.159729 139623498142912 submission.py:265] 74) loss = 0.158, grad_norm = 0.227
I0316 11:06:39.949220 139581538170624 logging_writer.py:48] [75] global_step=75, grad_norm=0.243447, loss=0.161305
I0316 11:06:39.952352 139623498142912 submission.py:265] 75) loss = 0.161, grad_norm = 0.243
I0316 11:06:40.820602 139582158903040 logging_writer.py:48] [76] global_step=76, grad_norm=0.302282, loss=0.160311
I0316 11:06:40.823596 139623498142912 submission.py:265] 76) loss = 0.160, grad_norm = 0.302
I0316 11:06:42.251332 139581538170624 logging_writer.py:48] [77] global_step=77, grad_norm=0.3323, loss=0.160152
I0316 11:06:42.254291 139623498142912 submission.py:265] 77) loss = 0.160, grad_norm = 0.332
I0316 11:06:43.473707 139582158903040 logging_writer.py:48] [78] global_step=78, grad_norm=0.339443, loss=0.159092
I0316 11:06:43.476733 139623498142912 submission.py:265] 78) loss = 0.159, grad_norm = 0.339
I0316 11:06:44.601654 139581538170624 logging_writer.py:48] [79] global_step=79, grad_norm=0.321257, loss=0.158347
I0316 11:06:44.604764 139623498142912 submission.py:265] 79) loss = 0.158, grad_norm = 0.321
I0316 11:06:46.270772 139582158903040 logging_writer.py:48] [80] global_step=80, grad_norm=0.292989, loss=0.155953
I0316 11:06:46.273840 139623498142912 submission.py:265] 80) loss = 0.156, grad_norm = 0.293
I0316 11:06:47.346957 139581538170624 logging_writer.py:48] [81] global_step=81, grad_norm=0.213808, loss=0.156164
I0316 11:06:47.350649 139623498142912 submission.py:265] 81) loss = 0.156, grad_norm = 0.214
I0316 11:06:49.141136 139582158903040 logging_writer.py:48] [82] global_step=82, grad_norm=0.157748, loss=0.154288
I0316 11:06:49.144230 139623498142912 submission.py:265] 82) loss = 0.154, grad_norm = 0.158
I0316 11:06:50.214782 139581538170624 logging_writer.py:48] [83] global_step=83, grad_norm=0.111265, loss=0.154149
I0316 11:06:50.218225 139623498142912 submission.py:265] 83) loss = 0.154, grad_norm = 0.111
I0316 11:06:51.730354 139582158903040 logging_writer.py:48] [84] global_step=84, grad_norm=0.122406, loss=0.153172
I0316 11:06:51.733598 139623498142912 submission.py:265] 84) loss = 0.153, grad_norm = 0.122
I0316 11:06:52.733466 139581538170624 logging_writer.py:48] [85] global_step=85, grad_norm=0.153978, loss=0.15194
I0316 11:06:52.736491 139623498142912 submission.py:265] 85) loss = 0.152, grad_norm = 0.154
I0316 11:06:54.101885 139582158903040 logging_writer.py:48] [86] global_step=86, grad_norm=0.183354, loss=0.150196
I0316 11:06:54.105045 139623498142912 submission.py:265] 86) loss = 0.150, grad_norm = 0.183
I0316 11:06:54.908619 139581538170624 logging_writer.py:48] [87] global_step=87, grad_norm=0.228624, loss=0.150233
I0316 11:06:54.911672 139623498142912 submission.py:265] 87) loss = 0.150, grad_norm = 0.229
I0316 11:06:56.496654 139582158903040 logging_writer.py:48] [88] global_step=88, grad_norm=0.248432, loss=0.149802
I0316 11:06:56.499911 139623498142912 submission.py:265] 88) loss = 0.150, grad_norm = 0.248
I0316 11:06:57.432817 139581538170624 logging_writer.py:48] [89] global_step=89, grad_norm=0.245459, loss=0.148851
I0316 11:06:57.435865 139623498142912 submission.py:265] 89) loss = 0.149, grad_norm = 0.245
I0316 11:06:58.823119 139582158903040 logging_writer.py:48] [90] global_step=90, grad_norm=0.240177, loss=0.148727
I0316 11:06:58.826193 139623498142912 submission.py:265] 90) loss = 0.149, grad_norm = 0.240
I0316 11:06:59.555087 139581538170624 logging_writer.py:48] [91] global_step=91, grad_norm=0.199237, loss=0.146689
I0316 11:06:59.558472 139623498142912 submission.py:265] 91) loss = 0.147, grad_norm = 0.199
I0316 11:07:01.185543 139582158903040 logging_writer.py:48] [92] global_step=92, grad_norm=0.167798, loss=0.146087
I0316 11:07:01.189001 139623498142912 submission.py:265] 92) loss = 0.146, grad_norm = 0.168
I0316 11:07:02.359720 139581538170624 logging_writer.py:48] [93] global_step=93, grad_norm=0.157698, loss=0.14855
I0316 11:07:02.362964 139623498142912 submission.py:265] 93) loss = 0.149, grad_norm = 0.158
I0316 11:07:03.431075 139582158903040 logging_writer.py:48] [94] global_step=94, grad_norm=0.0985507, loss=0.146087
I0316 11:07:03.434013 139623498142912 submission.py:265] 94) loss = 0.146, grad_norm = 0.099
I0316 11:07:04.998343 139581538170624 logging_writer.py:48] [95] global_step=95, grad_norm=0.0593323, loss=0.138734
I0316 11:07:05.001569 139623498142912 submission.py:265] 95) loss = 0.139, grad_norm = 0.059
I0316 11:07:05.723574 139582158903040 logging_writer.py:48] [96] global_step=96, grad_norm=0.0574258, loss=0.140178
I0316 11:07:05.726612 139623498142912 submission.py:265] 96) loss = 0.140, grad_norm = 0.057
I0316 11:07:07.000861 139581538170624 logging_writer.py:48] [97] global_step=97, grad_norm=0.0698461, loss=0.13894
I0316 11:07:07.003833 139623498142912 submission.py:265] 97) loss = 0.139, grad_norm = 0.070
I0316 11:07:08.134889 139582158903040 logging_writer.py:48] [98] global_step=98, grad_norm=0.0812648, loss=0.137071
I0316 11:07:08.138013 139623498142912 submission.py:265] 98) loss = 0.137, grad_norm = 0.081
I0316 11:07:11.765954 139581538170624 logging_writer.py:48] [99] global_step=99, grad_norm=0.0710655, loss=0.136845
I0316 11:07:11.769162 139623498142912 submission.py:265] 99) loss = 0.137, grad_norm = 0.071
I0316 11:07:11.962368 139582158903040 logging_writer.py:48] [100] global_step=100, grad_norm=0.0854117, loss=0.137107
I0316 11:07:11.965501 139623498142912 submission.py:265] 100) loss = 0.137, grad_norm = 0.085
I0316 11:07:35.032115 139623498142912 spec.py:321] Evaluating on the training split.
I0316 11:12:40.592338 139623498142912 spec.py:333] Evaluating on the validation split.
I0316 11:16:59.176585 139623498142912 spec.py:349] Evaluating on the test split.
I0316 11:22:02.727925 139623498142912 submission_runner.py:469] Time since start: 1956.58s, 	Step: 122, 	{'train/loss': 0.13806551262839292, 'validation/loss': 0.13844054850217832, 'validation/num_examples': 83274637, 'test/loss': 0.14160634178262008, 'test/num_examples': 95000000, 'score': 129.47463846206665, 'total_duration': 1956.582330942154, 'accumulated_submission_time': 129.47463846206665, 'accumulated_eval_time': 1825.7606344223022, 'accumulated_logging_time': 0.0716865062713623}
I0316 11:22:02.738353 139581538170624 logging_writer.py:48] [122] accumulated_eval_time=1825.76, accumulated_logging_time=0.0716865, accumulated_submission_time=129.475, global_step=122, preemption_count=0, score=129.475, test/loss=0.141606, test/num_examples=95000000, total_duration=1956.58, train/loss=0.138066, validation/loss=0.138441, validation/num_examples=83274637
I0316 11:24:04.299553 139623498142912 spec.py:321] Evaluating on the training split.
I0316 11:29:19.633211 139623498142912 spec.py:333] Evaluating on the validation split.
I0316 11:33:36.794627 139623498142912 spec.py:349] Evaluating on the test split.
I0316 11:38:44.178384 139623498142912 submission_runner.py:469] Time since start: 2958.03s, 	Step: 245, 	{'train/loss': 0.12827721130914185, 'validation/loss': 0.1292842964798221, 'validation/num_examples': 83274637, 'test/loss': 0.13239581693460564, 'test/num_examples': 95000000, 'score': 250.12658643722534, 'total_duration': 2958.032777070999, 'accumulated_submission_time': 250.12658643722534, 'accumulated_eval_time': 2705.6396358013153, 'accumulated_logging_time': 0.09008240699768066}
I0316 11:38:44.187308 139582158903040 logging_writer.py:48] [245] accumulated_eval_time=2705.64, accumulated_logging_time=0.0900824, accumulated_submission_time=250.127, global_step=245, preemption_count=0, score=250.127, test/loss=0.132396, test/num_examples=95000000, total_duration=2958.03, train/loss=0.128277, validation/loss=0.129284, validation/num_examples=83274637
I0316 11:40:44.984741 139623498142912 spec.py:321] Evaluating on the training split.
I0316 11:45:55.537107 139623498142912 spec.py:333] Evaluating on the validation split.
I0316 11:50:09.037488 139623498142912 spec.py:349] Evaluating on the test split.
I0316 11:55:17.225461 139623498142912 submission_runner.py:469] Time since start: 3951.08s, 	Step: 369, 	{'train/loss': 0.12884185960768815, 'validation/loss': 0.128320491409564, 'validation/num_examples': 83274637, 'test/loss': 0.13086834462432861, 'test/num_examples': 95000000, 'score': 370.00406861305237, 'total_duration': 3951.0798490047455, 'accumulated_submission_time': 370.00406861305237, 'accumulated_eval_time': 3577.880513191223, 'accumulated_logging_time': 0.1324470043182373}
I0316 11:55:17.234680 139581538170624 logging_writer.py:48] [369] accumulated_eval_time=3577.88, accumulated_logging_time=0.132447, accumulated_submission_time=370.004, global_step=369, preemption_count=0, score=370.004, test/loss=0.130868, test/num_examples=95000000, total_duration=3951.08, train/loss=0.128842, validation/loss=0.12832, validation/num_examples=83274637
I0316 11:57:18.293406 139623498142912 spec.py:321] Evaluating on the training split.
I0316 12:02:39.384061 139623498142912 spec.py:333] Evaluating on the validation split.
I0316 12:07:01.269504 139623498142912 spec.py:349] Evaluating on the test split.
I0316 12:12:14.397262 139623498142912 submission_runner.py:469] Time since start: 4968.25s, 	Step: 495, 	{'train/loss': 0.12716185972564503, 'validation/loss': 0.12788654373844685, 'validation/num_examples': 83274637, 'test/loss': 0.130423721377443, 'test/num_examples': 95000000, 'score': 490.14915013313293, 'total_duration': 4968.251679420471, 'accumulated_submission_time': 490.14915013313293, 'accumulated_eval_time': 4473.984615802765, 'accumulated_logging_time': 0.14891862869262695}
I0316 12:12:14.406713 139582158903040 logging_writer.py:48] [495] accumulated_eval_time=4473.98, accumulated_logging_time=0.148919, accumulated_submission_time=490.149, global_step=495, preemption_count=0, score=490.149, test/loss=0.130424, test/num_examples=95000000, total_duration=4968.25, train/loss=0.127162, validation/loss=0.127887, validation/num_examples=83274637
I0316 12:12:16.031379 139581538170624 logging_writer.py:48] [500] global_step=500, grad_norm=0.0577471, loss=0.116577
I0316 12:12:16.034410 139623498142912 submission.py:265] 500) loss = 0.117, grad_norm = 0.058
I0316 12:14:15.874091 139623498142912 spec.py:321] Evaluating on the training split.
I0316 12:19:12.111352 139623498142912 spec.py:333] Evaluating on the validation split.
I0316 12:23:27.913221 139623498142912 spec.py:349] Evaluating on the test split.
I0316 12:28:35.656044 139623498142912 submission_runner.py:469] Time since start: 5949.51s, 	Step: 613, 	{'train/loss': 0.12621784877750641, 'validation/loss': 0.1272189770564625, 'validation/num_examples': 83274637, 'test/loss': 0.1297390959479081, 'test/num_examples': 95000000, 'score': 610.804224729538, 'total_duration': 5949.510470867157, 'accumulated_submission_time': 610.804224729538, 'accumulated_eval_time': 5333.766624212265, 'accumulated_logging_time': 0.16493654251098633}
I0316 12:28:35.665609 139582158903040 logging_writer.py:48] [613] accumulated_eval_time=5333.77, accumulated_logging_time=0.164937, accumulated_submission_time=610.804, global_step=613, preemption_count=0, score=610.804, test/loss=0.129739, test/num_examples=95000000, total_duration=5949.51, train/loss=0.126218, validation/loss=0.127219, validation/num_examples=83274637
I0316 12:30:36.232779 139623498142912 spec.py:321] Evaluating on the training split.
I0316 12:35:57.468566 139623498142912 spec.py:333] Evaluating on the validation split.
I0316 12:40:17.090237 139623498142912 spec.py:349] Evaluating on the test split.
I0316 12:45:35.423645 139623498142912 submission_runner.py:469] Time since start: 6969.28s, 	Step: 733, 	{'train/loss': 0.12875681522297017, 'validation/loss': 0.12715421042353586, 'validation/num_examples': 83274637, 'test/loss': 0.12978260829283061, 'test/num_examples': 95000000, 'score': 730.5265727043152, 'total_duration': 6969.278084278107, 'accumulated_submission_time': 730.5265727043152, 'accumulated_eval_time': 6232.9575753211975, 'accumulated_logging_time': 0.18106508255004883}
I0316 12:45:35.433449 139581538170624 logging_writer.py:48] [733] accumulated_eval_time=6232.96, accumulated_logging_time=0.181065, accumulated_submission_time=730.527, global_step=733, preemption_count=0, score=730.527, test/loss=0.129783, test/num_examples=95000000, total_duration=6969.28, train/loss=0.128757, validation/loss=0.127154, validation/num_examples=83274637
I0316 12:47:36.629140 139623498142912 spec.py:321] Evaluating on the training split.
I0316 12:52:47.321335 139623498142912 spec.py:333] Evaluating on the validation split.
I0316 12:56:59.470099 139623498142912 spec.py:349] Evaluating on the test split.
I0316 13:01:58.736018 139623498142912 submission_runner.py:469] Time since start: 7952.59s, 	Step: 858, 	{'train/loss': 0.12436651434590328, 'validation/loss': 0.1269641372722766, 'validation/num_examples': 83274637, 'test/loss': 0.12938452772851242, 'test/num_examples': 95000000, 'score': 850.8974733352661, 'total_duration': 7952.590391874313, 'accumulated_submission_time': 850.8974733352661, 'accumulated_eval_time': 7095.0645706653595, 'accumulated_logging_time': 0.1976301670074463}
I0316 13:01:58.747316 139582158903040 logging_writer.py:48] [858] accumulated_eval_time=7095.06, accumulated_logging_time=0.19763, accumulated_submission_time=850.897, global_step=858, preemption_count=0, score=850.897, test/loss=0.129385, test/num_examples=95000000, total_duration=7952.59, train/loss=0.124367, validation/loss=0.126964, validation/num_examples=83274637
I0316 13:03:59.802522 139623498142912 spec.py:321] Evaluating on the training split.
I0316 13:08:57.592519 139623498142912 spec.py:333] Evaluating on the validation split.
I0316 13:13:08.859173 139623498142912 spec.py:349] Evaluating on the test split.
I0316 13:18:12.419427 139623498142912 submission_runner.py:469] Time since start: 8926.27s, 	Step: 982, 	{'train/loss': 0.12369887852066912, 'validation/loss': 0.1262706936406851, 'validation/num_examples': 83274637, 'test/loss': 0.12865905622763382, 'test/num_examples': 95000000, 'score': 971.0415787696838, 'total_duration': 8926.273813962936, 'accumulated_submission_time': 971.0415787696838, 'accumulated_eval_time': 7947.68154168129, 'accumulated_logging_time': 0.2162764072418213}
I0316 13:18:12.428947 139581538170624 logging_writer.py:48] [982] accumulated_eval_time=7947.68, accumulated_logging_time=0.216276, accumulated_submission_time=971.042, global_step=982, preemption_count=0, score=971.042, test/loss=0.128659, test/num_examples=95000000, total_duration=8926.27, train/loss=0.123699, validation/loss=0.126271, validation/num_examples=83274637
I0316 13:18:16.471369 139582158903040 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.0445679, loss=0.124434
I0316 13:18:16.474691 139623498142912 submission.py:265] 1000) loss = 0.124, grad_norm = 0.045
/usr/local/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
I0316 13:20:12.903285 139623498142912 spec.py:321] Evaluating on the training split.
I0316 13:25:00.198502 139623498142912 spec.py:333] Evaluating on the validation split.
I0316 13:29:00.953096 139623498142912 spec.py:349] Evaluating on the test split.
I0316 13:34:09.758034 139623498142912 submission_runner.py:469] Time since start: 9883.61s, 	Step: 1102, 	{'train/loss': 0.12635882149972483, 'validation/loss': 0.12613673984355572, 'validation/num_examples': 83274637, 'test/loss': 0.12872901026864303, 'test/num_examples': 95000000, 'score': 1090.6885466575623, 'total_duration': 9883.612463235855, 'accumulated_submission_time': 1090.6885466575623, 'accumulated_eval_time': 8784.5363342762, 'accumulated_logging_time': 0.23322176933288574}
I0316 13:34:09.767511 139581538170624 logging_writer.py:48] [1102] accumulated_eval_time=8784.54, accumulated_logging_time=0.233222, accumulated_submission_time=1090.69, global_step=1102, preemption_count=0, score=1090.69, test/loss=0.128729, test/num_examples=95000000, total_duration=9883.61, train/loss=0.126359, validation/loss=0.126137, validation/num_examples=83274637
I0316 13:36:10.236330 139623498142912 spec.py:321] Evaluating on the training split.
I0316 13:40:24.521218 139623498142912 spec.py:333] Evaluating on the validation split.
I0316 13:44:02.712412 139623498142912 spec.py:349] Evaluating on the test split.
I0316 13:48:52.888702 139623498142912 submission_runner.py:469] Time since start: 10766.74s, 	Step: 1226, 	{'train/loss': 0.1246948797551568, 'validation/loss': 0.12619120496811176, 'validation/num_examples': 83274637, 'test/loss': 0.12861853735801296, 'test/num_examples': 95000000, 'score': 1210.2924740314484, 'total_duration': 10766.743099212646, 'accumulated_submission_time': 1210.2924740314484, 'accumulated_eval_time': 9547.188748598099, 'accumulated_logging_time': 0.2787351608276367}
I0316 13:48:52.898237 139582158903040 logging_writer.py:48] [1226] accumulated_eval_time=9547.19, accumulated_logging_time=0.278735, accumulated_submission_time=1210.29, global_step=1226, preemption_count=0, score=1210.29, test/loss=0.128619, test/num_examples=95000000, total_duration=10766.7, train/loss=0.124695, validation/loss=0.126191, validation/num_examples=83274637
I0316 13:50:53.328885 139623498142912 spec.py:321] Evaluating on the training split.
I0316 13:54:00.090580 139623498142912 spec.py:333] Evaluating on the validation split.
I0316 13:56:50.008570 139623498142912 spec.py:349] Evaluating on the test split.
I0316 14:02:14.828080 139623498142912 submission_runner.py:469] Time since start: 11568.68s, 	Step: 1347, 	{'train/loss': 0.12487287523480363, 'validation/loss': 0.12618856752650587, 'validation/num_examples': 83274637, 'test/loss': 0.12854281655739233, 'test/num_examples': 95000000, 'score': 1329.868495464325, 'total_duration': 11568.682466983795, 'accumulated_submission_time': 1329.868495464325, 'accumulated_eval_time': 10228.687976837158, 'accumulated_logging_time': 0.29607248306274414}
I0316 14:02:14.838483 139581538170624 logging_writer.py:48] [1347] accumulated_eval_time=10228.7, accumulated_logging_time=0.296072, accumulated_submission_time=1329.87, global_step=1347, preemption_count=0, score=1329.87, test/loss=0.128543, test/num_examples=95000000, total_duration=11568.7, train/loss=0.124873, validation/loss=0.126189, validation/num_examples=83274637
I0316 14:04:16.581383 139623498142912 spec.py:321] Evaluating on the training split.
I0316 14:06:20.770689 139623498142912 spec.py:333] Evaluating on the validation split.
I0316 14:09:10.238422 139623498142912 spec.py:349] Evaluating on the test split.
I0316 14:15:28.372512 139623498142912 submission_runner.py:469] Time since start: 12362.23s, 	Step: 1470, 	{'train/loss': 0.12634461405283026, 'validation/loss': 0.12564555551167375, 'validation/num_examples': 83274637, 'test/loss': 0.1279155673620525, 'test/num_examples': 95000000, 'score': 1450.7699003219604, 'total_duration': 12362.226927995682, 'accumulated_submission_time': 1450.7699003219604, 'accumulated_eval_time': 10900.479144573212, 'accumulated_logging_time': 0.31328725814819336}
I0316 14:15:28.383590 139582158903040 logging_writer.py:48] [1470] accumulated_eval_time=10900.5, accumulated_logging_time=0.313287, accumulated_submission_time=1450.77, global_step=1470, preemption_count=0, score=1450.77, test/loss=0.127916, test/num_examples=95000000, total_duration=12362.2, train/loss=0.126345, validation/loss=0.125646, validation/num_examples=83274637
I0316 14:15:35.949409 139581538170624 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.0224877, loss=0.128592
I0316 14:15:35.952965 139623498142912 submission.py:265] 1500) loss = 0.129, grad_norm = 0.022
I0316 14:17:29.077349 139623498142912 spec.py:321] Evaluating on the training split.
I0316 14:19:32.185104 139623498142912 spec.py:333] Evaluating on the validation split.
I0316 14:22:19.631431 139623498142912 spec.py:349] Evaluating on the test split.
I0316 14:27:43.342466 139623498142912 submission_runner.py:469] Time since start: 13097.20s, 	Step: 1584, 	{'train/loss': 0.1256123556407451, 'validation/loss': 0.12596999658357036, 'validation/num_examples': 83274637, 'test/loss': 0.12837742085422715, 'test/num_examples': 95000000, 'score': 1570.5822021961212, 'total_duration': 13097.19689154625, 'accumulated_submission_time': 1570.5822021961212, 'accumulated_eval_time': 11514.74446439743, 'accumulated_logging_time': 0.33088231086730957}
I0316 14:27:43.352697 139582158903040 logging_writer.py:48] [1584] accumulated_eval_time=11514.7, accumulated_logging_time=0.330882, accumulated_submission_time=1570.58, global_step=1584, preemption_count=0, score=1570.58, test/loss=0.128377, test/num_examples=95000000, total_duration=13097.2, train/loss=0.125612, validation/loss=0.12597, validation/num_examples=83274637
I0316 14:29:43.780044 139623498142912 spec.py:321] Evaluating on the training split.
I0316 14:31:46.830295 139623498142912 spec.py:333] Evaluating on the validation split.
I0316 14:34:35.888716 139623498142912 spec.py:349] Evaluating on the test split.
I0316 14:40:04.470881 139623498142912 submission_runner.py:469] Time since start: 13838.33s, 	Step: 1701, 	{'train/loss': 0.12518251146817042, 'validation/loss': 0.1257016998374178, 'validation/num_examples': 83274637, 'test/loss': 0.12792220989580658, 'test/num_examples': 95000000, 'score': 1690.1199131011963, 'total_duration': 13838.325284719467, 'accumulated_submission_time': 1690.1199131011963, 'accumulated_eval_time': 12135.435537338257, 'accumulated_logging_time': 0.34783506393432617}
I0316 14:40:04.481688 139581538170624 logging_writer.py:48] [1701] accumulated_eval_time=12135.4, accumulated_logging_time=0.347835, accumulated_submission_time=1690.12, global_step=1701, preemption_count=0, score=1690.12, test/loss=0.127922, test/num_examples=95000000, total_duration=13838.3, train/loss=0.125183, validation/loss=0.125702, validation/num_examples=83274637
I0316 14:42:05.476840 139623498142912 spec.py:321] Evaluating on the training split.
I0316 14:44:08.447145 139623498142912 spec.py:333] Evaluating on the validation split.
I0316 14:46:54.405218 139623498142912 spec.py:349] Evaluating on the test split.
I0316 14:51:46.013747 139623498142912 submission_runner.py:469] Time since start: 14539.87s, 	Step: 1827, 	{'train/loss': 0.1234934760007319, 'validation/loss': 0.12550876013258422, 'validation/num_examples': 83274637, 'test/loss': 0.12794491630791113, 'test/num_examples': 95000000, 'score': 1810.2585709095001, 'total_duration': 14539.868153572083, 'accumulated_submission_time': 1810.2585709095001, 'accumulated_eval_time': 12715.97259235382, 'accumulated_logging_time': 0.3658175468444824}
I0316 14:51:46.025704 139582158903040 logging_writer.py:48] [1827] accumulated_eval_time=12716, accumulated_logging_time=0.365818, accumulated_submission_time=1810.26, global_step=1827, preemption_count=0, score=1810.26, test/loss=0.127945, test/num_examples=95000000, total_duration=14539.9, train/loss=0.123493, validation/loss=0.125509, validation/num_examples=83274637
I0316 14:53:46.887162 139623498142912 spec.py:321] Evaluating on the training split.
I0316 14:55:49.735715 139623498142912 spec.py:333] Evaluating on the validation split.
I0316 14:58:40.493605 139623498142912 spec.py:349] Evaluating on the test split.
I0316 15:03:34.935388 139623498142912 submission_runner.py:469] Time since start: 15248.79s, 	Step: 1945, 	{'train/loss': 0.12417711971297248, 'validation/loss': 0.12558359346735032, 'validation/num_examples': 83274637, 'test/loss': 0.12786557774770635, 'test/num_examples': 95000000, 'score': 1930.198768377304, 'total_duration': 15248.789781332016, 'accumulated_submission_time': 1930.198768377304, 'accumulated_eval_time': 13304.020874261856, 'accumulated_logging_time': 0.38471341133117676}
I0316 15:03:34.946692 139581538170624 logging_writer.py:48] [1945] accumulated_eval_time=13304, accumulated_logging_time=0.384713, accumulated_submission_time=1930.2, global_step=1945, preemption_count=0, score=1930.2, test/loss=0.127866, test/num_examples=95000000, total_duration=15248.8, train/loss=0.124177, validation/loss=0.125584, validation/num_examples=83274637
I0316 15:04:13.606451 139582158903040 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.0217325, loss=0.124009
I0316 15:04:13.609787 139623498142912 submission.py:265] 2000) loss = 0.124, grad_norm = 0.022
I0316 15:05:35.836793 139623498142912 spec.py:321] Evaluating on the training split.
I0316 15:07:38.463724 139623498142912 spec.py:333] Evaluating on the validation split.
I0316 15:10:31.404821 139623498142912 spec.py:349] Evaluating on the test split.
I0316 15:15:24.177480 139623498142912 submission_runner.py:469] Time since start: 15958.03s, 	Step: 2064, 	{'train/loss': 0.12218235091693053, 'validation/loss': 0.12554543336754323, 'validation/num_examples': 83274637, 'test/loss': 0.12794666495851215, 'test/num_examples': 95000000, 'score': 2050.230681657791, 'total_duration': 15958.031909704208, 'accumulated_submission_time': 2050.230681657791, 'accumulated_eval_time': 13892.361625671387, 'accumulated_logging_time': 0.40320491790771484}
I0316 15:15:24.187809 139581538170624 logging_writer.py:48] [2064] accumulated_eval_time=13892.4, accumulated_logging_time=0.403205, accumulated_submission_time=2050.23, global_step=2064, preemption_count=0, score=2050.23, test/loss=0.127947, test/num_examples=95000000, total_duration=15958, train/loss=0.122182, validation/loss=0.125545, validation/num_examples=83274637
I0316 15:17:24.750918 139623498142912 spec.py:321] Evaluating on the training split.
I0316 15:19:27.425466 139623498142912 spec.py:333] Evaluating on the validation split.
I0316 15:22:14.320845 139623498142912 spec.py:349] Evaluating on the test split.
I0316 15:27:05.540862 139623498142912 submission_runner.py:469] Time since start: 16659.40s, 	Step: 2186, 	{'train/loss': 0.12422673145684515, 'validation/loss': 0.12547692201677138, 'validation/num_examples': 83274637, 'test/loss': 0.12792899866007754, 'test/num_examples': 95000000, 'score': 2169.9307646751404, 'total_duration': 16659.395295143127, 'accumulated_submission_time': 2169.9307646751404, 'accumulated_eval_time': 14473.151748418808, 'accumulated_logging_time': 0.42026329040527344}
I0316 15:27:05.551517 139582158903040 logging_writer.py:48] [2186] accumulated_eval_time=14473.2, accumulated_logging_time=0.420263, accumulated_submission_time=2169.93, global_step=2186, preemption_count=0, score=2169.93, test/loss=0.127929, test/num_examples=95000000, total_duration=16659.4, train/loss=0.124227, validation/loss=0.125477, validation/num_examples=83274637
I0316 15:29:06.462820 139623498142912 spec.py:321] Evaluating on the training split.
I0316 15:31:09.625933 139623498142912 spec.py:333] Evaluating on the validation split.
I0316 15:34:01.093797 139623498142912 spec.py:349] Evaluating on the test split.
I0316 15:38:55.384843 139623498142912 submission_runner.py:469] Time since start: 17369.24s, 	Step: 2310, 	{'train/loss': 0.12408775379289452, 'validation/loss': 0.12574555858318046, 'validation/num_examples': 83274637, 'test/loss': 0.1281450340647647, 'test/num_examples': 95000000, 'score': 2289.9896636009216, 'total_duration': 17369.239292383194, 'accumulated_submission_time': 2289.9896636009216, 'accumulated_eval_time': 15062.073969602585, 'accumulated_logging_time': 0.4383575916290283}
I0316 15:38:55.395066 139581538170624 logging_writer.py:48] [2310] accumulated_eval_time=15062.1, accumulated_logging_time=0.438358, accumulated_submission_time=2289.99, global_step=2310, preemption_count=0, score=2289.99, test/loss=0.128145, test/num_examples=95000000, total_duration=17369.2, train/loss=0.124088, validation/loss=0.125746, validation/num_examples=83274637
I0316 15:40:56.643604 139623498142912 spec.py:321] Evaluating on the training split.
I0316 15:42:59.427642 139623498142912 spec.py:333] Evaluating on the validation split.
I0316 15:45:48.878127 139623498142912 spec.py:349] Evaluating on the test split.
I0316 15:51:18.463748 139623498142912 submission_runner.py:469] Time since start: 18112.32s, 	Step: 2432, 	{'train/loss': 0.12384349610116908, 'validation/loss': 0.1254531870440181, 'validation/num_examples': 83274637, 'test/loss': 0.12771855525472542, 'test/num_examples': 95000000, 'score': 2410.4140713214874, 'total_duration': 18112.318170547485, 'accumulated_submission_time': 2410.4140713214874, 'accumulated_eval_time': 15683.894177913666, 'accumulated_logging_time': 0.45590782165527344}
I0316 15:51:18.475023 139582158903040 logging_writer.py:48] [2432] accumulated_eval_time=15683.9, accumulated_logging_time=0.455908, accumulated_submission_time=2410.41, global_step=2432, preemption_count=0, score=2410.41, test/loss=0.127719, test/num_examples=95000000, total_duration=18112.3, train/loss=0.123843, validation/loss=0.125453, validation/num_examples=83274637
I0316 15:52:13.672824 139581538170624 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.00747495, loss=0.126456
I0316 15:52:13.676630 139623498142912 submission.py:265] 2500) loss = 0.126, grad_norm = 0.007
I0316 15:53:19.724287 139623498142912 spec.py:321] Evaluating on the training split.
I0316 15:55:22.604052 139623498142912 spec.py:333] Evaluating on the validation split.
I0316 15:58:10.867580 139623498142912 spec.py:349] Evaluating on the test split.
I0316 16:03:40.196907 139623498142912 submission_runner.py:469] Time since start: 18854.05s, 	Step: 2555, 	{'train/loss': 0.12397566745316047, 'validation/loss': 0.12586067334073187, 'validation/num_examples': 83274637, 'test/loss': 0.12820455992692648, 'test/num_examples': 95000000, 'score': 2530.7806046009064, 'total_duration': 18854.05129623413, 'accumulated_submission_time': 2530.7806046009064, 'accumulated_eval_time': 16304.36704492569, 'accumulated_logging_time': 0.4743473529815674}
I0316 16:03:40.207520 139582158903040 logging_writer.py:48] [2555] accumulated_eval_time=16304.4, accumulated_logging_time=0.474347, accumulated_submission_time=2530.78, global_step=2555, preemption_count=0, score=2530.78, test/loss=0.128205, test/num_examples=95000000, total_duration=18854.1, train/loss=0.123976, validation/loss=0.125861, validation/num_examples=83274637
I0316 16:05:41.271828 139623498142912 spec.py:321] Evaluating on the training split.
I0316 16:07:44.124483 139623498142912 spec.py:333] Evaluating on the validation split.
I0316 16:10:32.479734 139623498142912 spec.py:349] Evaluating on the test split.
I0316 16:15:57.999244 139623498142912 submission_runner.py:469] Time since start: 19591.85s, 	Step: 2678, 	{'train/loss': 0.12354351886968232, 'validation/loss': 0.12534764879768526, 'validation/num_examples': 83274637, 'test/loss': 0.1278186221743935, 'test/num_examples': 95000000, 'score': 2650.943372964859, 'total_duration': 19591.85366177559, 'accumulated_submission_time': 2650.943372964859, 'accumulated_eval_time': 16921.09463620186, 'accumulated_logging_time': 0.4915468692779541}
I0316 16:15:58.009716 139581538170624 logging_writer.py:48] [2678] accumulated_eval_time=16921.1, accumulated_logging_time=0.491547, accumulated_submission_time=2650.94, global_step=2678, preemption_count=0, score=2650.94, test/loss=0.127819, test/num_examples=95000000, total_duration=19591.9, train/loss=0.123544, validation/loss=0.125348, validation/num_examples=83274637
I0316 16:17:58.719761 139623498142912 spec.py:321] Evaluating on the training split.
I0316 16:20:02.317457 139623498142912 spec.py:333] Evaluating on the validation split.
I0316 16:22:48.575901 139623498142912 spec.py:349] Evaluating on the test split.
I0316 16:28:03.104531 139623498142912 submission_runner.py:469] Time since start: 20316.96s, 	Step: 2796, 	{'train/loss': 0.12463771776474292, 'validation/loss': 0.12546456245493184, 'validation/num_examples': 83274637, 'test/loss': 0.1279700280576204, 'test/num_examples': 95000000, 'score': 2770.7136147022247, 'total_duration': 20316.958945035934, 'accumulated_submission_time': 2770.7136147022247, 'accumulated_eval_time': 17525.47950720787, 'accumulated_logging_time': 0.5335021018981934}
I0316 16:28:03.114978 139582158903040 logging_writer.py:48] [2796] accumulated_eval_time=17525.5, accumulated_logging_time=0.533502, accumulated_submission_time=2770.71, global_step=2796, preemption_count=0, score=2770.71, test/loss=0.12797, test/num_examples=95000000, total_duration=20317, train/loss=0.124638, validation/loss=0.125465, validation/num_examples=83274637
I0316 16:30:03.732342 139623498142912 spec.py:321] Evaluating on the training split.
I0316 16:32:06.517632 139623498142912 spec.py:333] Evaluating on the validation split.
I0316 16:34:54.504104 139623498142912 spec.py:349] Evaluating on the test split.
I0316 16:40:22.815786 139623498142912 submission_runner.py:469] Time since start: 21056.67s, 	Step: 2915, 	{'train/loss': 0.1254480393663358, 'validation/loss': 0.12527963424859034, 'validation/num_examples': 83274637, 'test/loss': 0.12764556552148115, 'test/num_examples': 95000000, 'score': 2890.4767711162567, 'total_duration': 21056.670206069946, 'accumulated_submission_time': 2890.4767711162567, 'accumulated_eval_time': 18144.56298518181, 'accumulated_logging_time': 0.5507709980010986}
I0316 16:40:22.826475 139581538170624 logging_writer.py:48] [2915] accumulated_eval_time=18144.6, accumulated_logging_time=0.550771, accumulated_submission_time=2890.48, global_step=2915, preemption_count=0, score=2890.48, test/loss=0.127646, test/num_examples=95000000, total_duration=21056.7, train/loss=0.125448, validation/loss=0.12528, validation/num_examples=83274637
I0316 16:41:40.836753 139582158903040 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.0430866, loss=0.126058
I0316 16:41:40.840086 139623498142912 submission.py:265] 3000) loss = 0.126, grad_norm = 0.043
I0316 16:42:24.186610 139623498142912 spec.py:321] Evaluating on the training split.
I0316 16:44:27.338808 139623498142912 spec.py:333] Evaluating on the validation split.
I0316 16:47:15.221667 139623498142912 spec.py:349] Evaluating on the test split.
I0316 16:52:45.866762 139623498142912 submission_runner.py:469] Time since start: 21799.72s, 	Step: 3035, 	{'train/loss': 0.12493569724820225, 'validation/loss': 0.12493672870038014, 'validation/num_examples': 83274637, 'test/loss': 0.12718467562480726, 'test/num_examples': 95000000, 'score': 3010.984120607376, 'total_duration': 21799.721139907837, 'accumulated_submission_time': 3010.984120607376, 'accumulated_eval_time': 18766.243144989014, 'accumulated_logging_time': 0.5684123039245605}
I0316 16:52:45.877531 139581538170624 logging_writer.py:48] [3035] accumulated_eval_time=18766.2, accumulated_logging_time=0.568412, accumulated_submission_time=3010.98, global_step=3035, preemption_count=0, score=3010.98, test/loss=0.127185, test/num_examples=95000000, total_duration=21799.7, train/loss=0.124936, validation/loss=0.124937, validation/num_examples=83274637
I0316 16:54:46.646883 139623498142912 spec.py:321] Evaluating on the training split.
I0316 16:56:49.434675 139623498142912 spec.py:333] Evaluating on the validation split.
I0316 16:59:37.228248 139623498142912 spec.py:349] Evaluating on the test split.
I0316 17:04:57.932454 139623498142912 submission_runner.py:469] Time since start: 22531.79s, 	Step: 3155, 	{'train/loss': 0.12367085986341031, 'validation/loss': 0.12487770361126967, 'validation/num_examples': 83274637, 'test/loss': 0.1271944655845642, 'test/num_examples': 95000000, 'score': 3130.9257090091705, 'total_duration': 22531.786817789078, 'accumulated_submission_time': 3130.9257090091705, 'accumulated_eval_time': 19377.528700590134, 'accumulated_logging_time': 0.5857093334197998}
I0316 17:04:57.943391 139582158903040 logging_writer.py:48] [3155] accumulated_eval_time=19377.5, accumulated_logging_time=0.585709, accumulated_submission_time=3130.93, global_step=3155, preemption_count=0, score=3130.93, test/loss=0.127194, test/num_examples=95000000, total_duration=22531.8, train/loss=0.123671, validation/loss=0.124878, validation/num_examples=83274637
I0316 17:06:58.636322 139623498142912 spec.py:321] Evaluating on the training split.
I0316 17:09:01.368944 139623498142912 spec.py:333] Evaluating on the validation split.
I0316 17:11:48.735634 139623498142912 spec.py:349] Evaluating on the test split.
I0316 17:17:14.142382 139623498142912 submission_runner.py:469] Time since start: 23268.00s, 	Step: 3273, 	{'train/loss': 0.12271482987071386, 'validation/loss': 0.1248615626999788, 'validation/num_examples': 83274637, 'test/loss': 0.12731455375719572, 'test/num_examples': 95000000, 'score': 3250.7646675109863, 'total_duration': 23267.99680829048, 'accumulated_submission_time': 3250.7646675109863, 'accumulated_eval_time': 19993.034805059433, 'accumulated_logging_time': 0.609102725982666}
I0316 17:17:14.152889 139581538170624 logging_writer.py:48] [3273] accumulated_eval_time=19993, accumulated_logging_time=0.609103, accumulated_submission_time=3250.76, global_step=3273, preemption_count=0, score=3250.76, test/loss=0.127315, test/num_examples=95000000, total_duration=23268, train/loss=0.122715, validation/loss=0.124862, validation/num_examples=83274637
I0316 17:19:14.922305 139623498142912 spec.py:321] Evaluating on the training split.
I0316 17:21:17.803232 139623498142912 spec.py:333] Evaluating on the validation split.
I0316 17:24:06.425334 139623498142912 spec.py:349] Evaluating on the test split.
I0316 17:29:33.045523 139623498142912 submission_runner.py:469] Time since start: 24006.90s, 	Step: 3393, 	{'train/loss': 0.12401396967814189, 'validation/loss': 0.12501035104564287, 'validation/num_examples': 83274637, 'test/loss': 0.12742764343285812, 'test/num_examples': 95000000, 'score': 3370.6447043418884, 'total_duration': 24006.899938106537, 'accumulated_submission_time': 3370.6447043418884, 'accumulated_eval_time': 20611.158259153366, 'accumulated_logging_time': 0.6263463497161865}
I0316 17:29:33.056307 139582158903040 logging_writer.py:48] [3393] accumulated_eval_time=20611.2, accumulated_logging_time=0.626346, accumulated_submission_time=3370.64, global_step=3393, preemption_count=0, score=3370.64, test/loss=0.127428, test/num_examples=95000000, total_duration=24006.9, train/loss=0.124014, validation/loss=0.12501, validation/num_examples=83274637
I0316 17:31:26.501701 139581538170624 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.0222362, loss=0.120603
I0316 17:31:26.505020 139623498142912 submission.py:265] 3500) loss = 0.121, grad_norm = 0.022
I0316 17:31:33.479429 139623498142912 spec.py:321] Evaluating on the training split.
I0316 17:33:37.767431 139623498142912 spec.py:333] Evaluating on the validation split.
I0316 17:36:30.033988 139623498142912 spec.py:349] Evaluating on the test split.
I0316 17:41:50.370682 139623498142912 submission_runner.py:469] Time since start: 24744.23s, 	Step: 3506, 	{'train/loss': 0.1243174592763722, 'validation/loss': 0.12495218051923368, 'validation/num_examples': 83274637, 'test/loss': 0.12738912677556088, 'test/num_examples': 95000000, 'score': 3490.1970858573914, 'total_duration': 24744.22510766983, 'accumulated_submission_time': 3490.1970858573914, 'accumulated_eval_time': 21228.049637317657, 'accumulated_logging_time': 0.6437513828277588}
I0316 17:41:50.382391 139582158903040 logging_writer.py:48] [3506] accumulated_eval_time=21228, accumulated_logging_time=0.643751, accumulated_submission_time=3490.2, global_step=3506, preemption_count=0, score=3490.2, test/loss=0.127389, test/num_examples=95000000, total_duration=24744.2, train/loss=0.124317, validation/loss=0.124952, validation/num_examples=83274637
I0316 17:43:50.824741 139623498142912 spec.py:321] Evaluating on the training split.
I0316 17:45:53.617146 139623498142912 spec.py:333] Evaluating on the validation split.
I0316 17:48:45.511414 139623498142912 spec.py:349] Evaluating on the test split.
I0316 17:54:17.029397 139623498142912 submission_runner.py:469] Time since start: 25490.88s, 	Step: 3628, 	{'train/loss': 0.12398448433491034, 'validation/loss': 0.12500903585409734, 'validation/num_examples': 83274637, 'test/loss': 0.127495093280471, 'test/num_examples': 95000000, 'score': 3609.8278489112854, 'total_duration': 25490.883791446686, 'accumulated_submission_time': 3609.8278489112854, 'accumulated_eval_time': 21854.254301548004, 'accumulated_logging_time': 0.662665843963623}
I0316 17:54:17.040328 139581538170624 logging_writer.py:48] [3628] accumulated_eval_time=21854.3, accumulated_logging_time=0.662666, accumulated_submission_time=3609.83, global_step=3628, preemption_count=0, score=3609.83, test/loss=0.127495, test/num_examples=95000000, total_duration=25490.9, train/loss=0.123984, validation/loss=0.125009, validation/num_examples=83274637
I0316 17:56:17.827570 139623498142912 spec.py:321] Evaluating on the training split.
I0316 17:58:20.532658 139623498142912 spec.py:333] Evaluating on the validation split.
I0316 18:01:09.881391 139623498142912 spec.py:349] Evaluating on the test split.
I0316 18:06:43.933521 139623498142912 submission_runner.py:469] Time since start: 26237.79s, 	Step: 3746, 	{'train/loss': 0.12119297679258463, 'validation/loss': 0.12482603595670116, 'validation/num_examples': 83274637, 'test/loss': 0.12727339539064106, 'test/num_examples': 95000000, 'score': 3729.7345538139343, 'total_duration': 26237.787905216217, 'accumulated_submission_time': 3729.7345538139343, 'accumulated_eval_time': 22480.36032795906, 'accumulated_logging_time': 0.7010600566864014}
I0316 18:06:43.944685 139582158903040 logging_writer.py:48] [3746] accumulated_eval_time=22480.4, accumulated_logging_time=0.70106, accumulated_submission_time=3729.73, global_step=3746, preemption_count=0, score=3729.73, test/loss=0.127273, test/num_examples=95000000, total_duration=26237.8, train/loss=0.121193, validation/loss=0.124826, validation/num_examples=83274637
I0316 18:08:44.822481 139623498142912 spec.py:321] Evaluating on the training split.
I0316 18:10:48.072957 139623498142912 spec.py:333] Evaluating on the validation split.
I0316 18:13:37.279381 139623498142912 spec.py:349] Evaluating on the test split.
I0316 18:19:10.795024 139623498142912 submission_runner.py:469] Time since start: 26984.65s, 	Step: 3867, 	{'train/loss': 0.12453811648245897, 'validation/loss': 0.12493507347785834, 'validation/num_examples': 83274637, 'test/loss': 0.12725578205052426, 'test/num_examples': 95000000, 'score': 3849.7473697662354, 'total_duration': 26984.64945745468, 'accumulated_submission_time': 3849.7473697662354, 'accumulated_eval_time': 23106.33301258087, 'accumulated_logging_time': 0.7192695140838623}
I0316 18:19:10.823337 139581538170624 logging_writer.py:48] [3867] accumulated_eval_time=23106.3, accumulated_logging_time=0.71927, accumulated_submission_time=3849.75, global_step=3867, preemption_count=0, score=3849.75, test/loss=0.127256, test/num_examples=95000000, total_duration=26984.6, train/loss=0.124538, validation/loss=0.124935, validation/num_examples=83274637
I0316 18:21:12.098463 139623498142912 spec.py:321] Evaluating on the training split.
I0316 18:23:15.745759 139623498142912 spec.py:333] Evaluating on the validation split.
I0316 18:26:04.801558 139623498142912 spec.py:349] Evaluating on the test split.
I0316 18:31:34.160291 139623498142912 submission_runner.py:469] Time since start: 27728.01s, 	Step: 3992, 	{'train/loss': 0.12382641563337855, 'validation/loss': 0.12484541726380283, 'validation/num_examples': 83274637, 'test/loss': 0.12720820917928596, 'test/num_examples': 95000000, 'score': 3970.1233274936676, 'total_duration': 27728.01472067833, 'accumulated_submission_time': 3970.1233274936676, 'accumulated_eval_time': 23728.395102262497, 'accumulated_logging_time': 0.7544982433319092}
I0316 18:31:34.171184 139582158903040 logging_writer.py:48] [3992] accumulated_eval_time=23728.4, accumulated_logging_time=0.754498, accumulated_submission_time=3970.12, global_step=3992, preemption_count=0, score=3970.12, test/loss=0.127208, test/num_examples=95000000, total_duration=27728, train/loss=0.123826, validation/loss=0.124845, validation/num_examples=83274637
I0316 18:31:36.356427 139581538170624 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.0134094, loss=0.12188
I0316 18:31:36.360244 139623498142912 submission.py:265] 4000) loss = 0.122, grad_norm = 0.013
I0316 18:33:34.965411 139623498142912 spec.py:321] Evaluating on the training split.
I0316 18:35:38.585882 139623498142912 spec.py:333] Evaluating on the validation split.
I0316 18:38:25.688170 139623498142912 spec.py:349] Evaluating on the test split.
I0316 18:43:52.071071 139623498142912 submission_runner.py:469] Time since start: 28465.93s, 	Step: 4116, 	{'train/loss': 0.12723006315902388, 'validation/loss': 0.12456052426140352, 'validation/num_examples': 83274637, 'test/loss': 0.1269691666005988, 'test/num_examples': 95000000, 'score': 4090.101422071457, 'total_duration': 28465.92551803589, 'accumulated_submission_time': 4090.101422071457, 'accumulated_eval_time': 24345.50083732605, 'accumulated_logging_time': 0.7718675136566162}
I0316 18:43:52.082606 139582158903040 logging_writer.py:48] [4116] accumulated_eval_time=24345.5, accumulated_logging_time=0.771868, accumulated_submission_time=4090.1, global_step=4116, preemption_count=0, score=4090.1, test/loss=0.126969, test/num_examples=95000000, total_duration=28465.9, train/loss=0.12723, validation/loss=0.124561, validation/num_examples=83274637
I0316 18:45:53.292897 139623498142912 spec.py:321] Evaluating on the training split.
I0316 18:47:56.751231 139623498142912 spec.py:333] Evaluating on the validation split.
I0316 18:50:46.961826 139623498142912 spec.py:349] Evaluating on the test split.
I0316 18:56:13.694501 139623498142912 submission_runner.py:469] Time since start: 29207.55s, 	Step: 4239, 	{'train/loss': 0.12348023308782753, 'validation/loss': 0.12476303320561008, 'validation/num_examples': 83274637, 'test/loss': 0.12712341542037162, 'test/num_examples': 95000000, 'score': 4210.39488530159, 'total_duration': 29207.548905849457, 'accumulated_submission_time': 4210.39488530159, 'accumulated_eval_time': 24965.902668237686, 'accumulated_logging_time': 0.8361103534698486}
I0316 18:56:13.705475 139581538170624 logging_writer.py:48] [4239] accumulated_eval_time=24965.9, accumulated_logging_time=0.83611, accumulated_submission_time=4210.39, global_step=4239, preemption_count=0, score=4210.39, test/loss=0.127123, test/num_examples=95000000, total_duration=29207.5, train/loss=0.12348, validation/loss=0.124763, validation/num_examples=83274637
I0316 18:58:14.937094 139623498142912 spec.py:321] Evaluating on the training split.
I0316 19:00:18.562911 139623498142912 spec.py:333] Evaluating on the validation split.
I0316 19:03:06.815675 139623498142912 spec.py:349] Evaluating on the test split.
I0316 19:07:50.584809 139623498142912 submission_runner.py:469] Time since start: 29904.44s, 	Step: 4355, 	{'train/loss': 0.1224880539965597, 'validation/loss': 0.12471488873449076, 'validation/num_examples': 83274637, 'test/loss': 0.12713790291491056, 'test/num_examples': 95000000, 'score': 4330.726969718933, 'total_duration': 29904.439235925674, 'accumulated_submission_time': 4330.726969718933, 'accumulated_eval_time': 25541.55055999756, 'accumulated_logging_time': 0.8540787696838379}
I0316 19:07:50.595247 139582158903040 logging_writer.py:48] [4355] accumulated_eval_time=25541.6, accumulated_logging_time=0.854079, accumulated_submission_time=4330.73, global_step=4355, preemption_count=0, score=4330.73, test/loss=0.127138, test/num_examples=95000000, total_duration=29904.4, train/loss=0.122488, validation/loss=0.124715, validation/num_examples=83274637
I0316 19:09:51.018515 139623498142912 spec.py:321] Evaluating on the training split.
I0316 19:11:54.430269 139623498142912 spec.py:333] Evaluating on the validation split.
I0316 19:14:41.571052 139623498142912 spec.py:349] Evaluating on the test split.
I0316 19:19:34.113329 139623498142912 submission_runner.py:469] Time since start: 30607.97s, 	Step: 4478, 	{'train/loss': 0.12386821500069113, 'validation/loss': 0.12472198941448619, 'validation/num_examples': 83274637, 'test/loss': 0.1268932403778076, 'test/num_examples': 95000000, 'score': 4450.2883644104, 'total_duration': 30607.967757701874, 'accumulated_submission_time': 4450.2883644104, 'accumulated_eval_time': 26124.645430088043, 'accumulated_logging_time': 0.8714489936828613}
I0316 19:19:34.125268 139581538170624 logging_writer.py:48] [4478] accumulated_eval_time=26124.6, accumulated_logging_time=0.871449, accumulated_submission_time=4450.29, global_step=4478, preemption_count=0, score=4450.29, test/loss=0.126893, test/num_examples=95000000, total_duration=30608, train/loss=0.123868, validation/loss=0.124722, validation/num_examples=83274637
I0316 19:19:38.992563 139582158903040 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.0203957, loss=0.132572
I0316 19:19:38.995862 139623498142912 submission.py:265] 4500) loss = 0.133, grad_norm = 0.020
I0316 19:21:35.277410 139623498142912 spec.py:321] Evaluating on the training split.
I0316 19:23:38.738620 139623498142912 spec.py:333] Evaluating on the validation split.
I0316 19:26:29.180057 139623498142912 spec.py:349] Evaluating on the test split.
I0316 19:31:22.582190 139623498142912 submission_runner.py:469] Time since start: 31316.44s, 	Step: 4603, 	{'train/loss': 0.12313326203136774, 'validation/loss': 0.12463869377952648, 'validation/num_examples': 83274637, 'test/loss': 0.12698132943147358, 'test/num_examples': 95000000, 'score': 4570.556317567825, 'total_duration': 31316.436642885208, 'accumulated_submission_time': 4570.556317567825, 'accumulated_eval_time': 26711.950392961502, 'accumulated_logging_time': 0.8907320499420166}
I0316 19:31:22.593095 139581538170624 logging_writer.py:48] [4603] accumulated_eval_time=26712, accumulated_logging_time=0.890732, accumulated_submission_time=4570.56, global_step=4603, preemption_count=0, score=4570.56, test/loss=0.126981, test/num_examples=95000000, total_duration=31316.4, train/loss=0.123133, validation/loss=0.124639, validation/num_examples=83274637
I0316 19:33:23.504934 139623498142912 spec.py:321] Evaluating on the training split.
I0316 19:35:26.997817 139623498142912 spec.py:333] Evaluating on the validation split.
I0316 19:38:18.992411 139623498142912 spec.py:349] Evaluating on the test split.
I0316 19:43:14.434775 139623498142912 submission_runner.py:469] Time since start: 32028.29s, 	Step: 4726, 	{'train/loss': 0.12414826662742304, 'validation/loss': 0.12451841272615748, 'validation/num_examples': 83274637, 'test/loss': 0.12705076838133963, 'test/num_examples': 95000000, 'score': 4690.617697477341, 'total_duration': 32028.28920149803, 'accumulated_submission_time': 4690.617697477341, 'accumulated_eval_time': 27302.880423545837, 'accumulated_logging_time': 0.9081294536590576}
I0316 19:43:14.447185 139582158903040 logging_writer.py:48] [4726] accumulated_eval_time=27302.9, accumulated_logging_time=0.908129, accumulated_submission_time=4690.62, global_step=4726, preemption_count=0, score=4690.62, test/loss=0.127051, test/num_examples=95000000, total_duration=32028.3, train/loss=0.124148, validation/loss=0.124518, validation/num_examples=83274637
I0316 19:45:15.732774 139623498142912 spec.py:321] Evaluating on the training split.
I0316 19:47:20.190183 139623498142912 spec.py:333] Evaluating on the validation split.
I0316 19:50:12.165421 139623498142912 spec.py:349] Evaluating on the test split.
I0316 19:55:01.689044 139623498142912 submission_runner.py:469] Time since start: 32735.54s, 	Step: 4851, 	{'train/loss': 0.12358901492563137, 'validation/loss': 0.1244741529996126, 'validation/num_examples': 83274637, 'test/loss': 0.1268417391059474, 'test/num_examples': 95000000, 'score': 4810.997691869736, 'total_duration': 32735.54348564148, 'accumulated_submission_time': 4810.997691869736, 'accumulated_eval_time': 27888.83693909645, 'accumulated_logging_time': 0.9752039909362793}
I0316 19:55:01.700434 139581538170624 logging_writer.py:48] [4851] accumulated_eval_time=27888.8, accumulated_logging_time=0.975204, accumulated_submission_time=4811, global_step=4851, preemption_count=0, score=4811, test/loss=0.126842, test/num_examples=95000000, total_duration=32735.5, train/loss=0.123589, validation/loss=0.124474, validation/num_examples=83274637
I0316 19:57:03.064293 139623498142912 spec.py:321] Evaluating on the training split.
I0316 19:59:07.272649 139623498142912 spec.py:333] Evaluating on the validation split.
I0316 20:01:52.292757 139623498142912 spec.py:349] Evaluating on the test split.
I0316 20:06:42.281223 139623498142912 submission_runner.py:469] Time since start: 33436.14s, 	Step: 4975, 	{'train/loss': 0.12279934357800808, 'validation/loss': 0.12448895181058828, 'validation/num_examples': 83274637, 'test/loss': 0.1268811529253508, 'test/num_examples': 95000000, 'score': 4931.4783935546875, 'total_duration': 33436.135644197464, 'accumulated_submission_time': 4931.4783935546875, 'accumulated_eval_time': 28468.054042577744, 'accumulated_logging_time': 1.0002901554107666}
I0316 20:06:42.292408 139582158903040 logging_writer.py:48] [4975] accumulated_eval_time=28468.1, accumulated_logging_time=1.00029, accumulated_submission_time=4931.48, global_step=4975, preemption_count=0, score=4931.48, test/loss=0.126881, test/num_examples=95000000, total_duration=33436.1, train/loss=0.122799, validation/loss=0.124489, validation/num_examples=83274637
I0316 20:06:47.700148 139581538170624 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.00771615, loss=0.128168
I0316 20:06:47.703189 139623498142912 submission.py:265] 5000) loss = 0.128, grad_norm = 0.008
I0316 20:08:43.559021 139623498142912 spec.py:321] Evaluating on the training split.
I0316 20:10:47.727100 139623498142912 spec.py:333] Evaluating on the validation split.
I0316 20:13:39.212035 139623498142912 spec.py:349] Evaluating on the test split.
I0316 20:18:36.120788 139623498142912 submission_runner.py:469] Time since start: 34149.98s, 	Step: 5099, 	{'train/loss': 0.12242894497598897, 'validation/loss': 0.12452565325299166, 'validation/num_examples': 83274637, 'test/loss': 0.1268863835630718, 'test/num_examples': 95000000, 'score': 5051.892138957977, 'total_duration': 34149.97521138191, 'accumulated_submission_time': 5051.892138957977, 'accumulated_eval_time': 29060.615869998932, 'accumulated_logging_time': 1.0188190937042236}
I0316 20:18:36.131901 139582158903040 logging_writer.py:48] [5099] accumulated_eval_time=29060.6, accumulated_logging_time=1.01882, accumulated_submission_time=5051.89, global_step=5099, preemption_count=0, score=5051.89, test/loss=0.126886, test/num_examples=95000000, total_duration=34150, train/loss=0.122429, validation/loss=0.124526, validation/num_examples=83274637
I0316 20:20:36.900496 139623498142912 spec.py:321] Evaluating on the training split.
I0316 20:22:40.246567 139623498142912 spec.py:333] Evaluating on the validation split.
I0316 20:25:29.144842 139623498142912 spec.py:349] Evaluating on the test split.
I0316 20:30:19.223212 139623498142912 submission_runner.py:469] Time since start: 34853.08s, 	Step: 5220, 	{'train/loss': 0.1224124448533501, 'validation/loss': 0.12466407096583698, 'validation/num_examples': 83274637, 'test/loss': 0.1271119116803621, 'test/num_examples': 95000000, 'score': 5171.803838014603, 'total_duration': 34853.07766509056, 'accumulated_submission_time': 5171.803838014603, 'accumulated_eval_time': 29642.93873500824, 'accumulated_logging_time': 1.0363359451293945}
I0316 20:30:19.234191 139581538170624 logging_writer.py:48] [5220] accumulated_eval_time=29642.9, accumulated_logging_time=1.03634, accumulated_submission_time=5171.8, global_step=5220, preemption_count=0, score=5171.8, test/loss=0.127112, test/num_examples=95000000, total_duration=34853.1, train/loss=0.122412, validation/loss=0.124664, validation/num_examples=83274637
I0316 20:32:19.657127 139623498142912 spec.py:321] Evaluating on the training split.
I0316 20:34:22.362214 139623498142912 spec.py:333] Evaluating on the validation split.
I0316 20:37:11.897984 139623498142912 spec.py:349] Evaluating on the test split.
I0316 20:42:04.859874 139623498142912 submission_runner.py:469] Time since start: 35558.71s, 	Step: 5337, 	{'train/loss': 0.12586168701011638, 'validation/loss': 0.12455311032708939, 'validation/num_examples': 83274637, 'test/loss': 0.12683932440884238, 'test/num_examples': 95000000, 'score': 5291.309556722641, 'total_duration': 35558.71432232857, 'accumulated_submission_time': 5291.309556722641, 'accumulated_eval_time': 30228.141699790955, 'accumulated_logging_time': 1.068800687789917}
I0316 20:42:04.871243 139582158903040 logging_writer.py:48] [5337] accumulated_eval_time=30228.1, accumulated_logging_time=1.0688, accumulated_submission_time=5291.31, global_step=5337, preemption_count=0, score=5291.31, test/loss=0.126839, test/num_examples=95000000, total_duration=35558.7, train/loss=0.125862, validation/loss=0.124553, validation/num_examples=83274637
I0316 20:44:05.973654 139623498142912 spec.py:321] Evaluating on the training split.
I0316 20:46:09.640672 139623498142912 spec.py:333] Evaluating on the validation split.
I0316 20:49:00.418126 139623498142912 spec.py:349] Evaluating on the test split.
I0316 20:54:17.624021 139623498142912 submission_runner.py:469] Time since start: 36291.48s, 	Step: 5456, 	{'train/loss': 0.12444582107050346, 'validation/loss': 0.12460705216016457, 'validation/num_examples': 83274637, 'test/loss': 0.12692160073442962, 'test/num_examples': 95000000, 'score': 5411.586529016495, 'total_duration': 36291.478420734406, 'accumulated_submission_time': 5411.586529016495, 'accumulated_eval_time': 30839.792104244232, 'accumulated_logging_time': 1.0868875980377197}
I0316 20:54:17.635334 139581538170624 logging_writer.py:48] [5456] accumulated_eval_time=30839.8, accumulated_logging_time=1.08689, accumulated_submission_time=5411.59, global_step=5456, preemption_count=0, score=5411.59, test/loss=0.126922, test/num_examples=95000000, total_duration=36291.5, train/loss=0.124446, validation/loss=0.124607, validation/num_examples=83274637
I0316 20:54:42.776823 139582158903040 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.00929712, loss=0.123549
I0316 20:54:42.780355 139623498142912 submission.py:265] 5500) loss = 0.124, grad_norm = 0.009
I0316 20:56:18.623192 139623498142912 spec.py:321] Evaluating on the training split.
I0316 20:58:22.313297 139623498142912 spec.py:333] Evaluating on the validation split.
I0316 21:01:12.585500 139623498142912 spec.py:349] Evaluating on the test split.
I0316 21:06:04.966156 139623498142912 submission_runner.py:469] Time since start: 36998.82s, 	Step: 5583, 	{'train/loss': 0.12487630052911604, 'validation/loss': 0.12456235497230822, 'validation/num_examples': 83274637, 'test/loss': 0.12687887167193765, 'test/num_examples': 95000000, 'score': 5531.7102954387665, 'total_duration': 36998.82051920891, 'accumulated_submission_time': 5531.7102954387665, 'accumulated_eval_time': 31426.135251760483, 'accumulated_logging_time': 1.1053354740142822}
I0316 21:06:04.977062 139581538170624 logging_writer.py:48] [5583] accumulated_eval_time=31426.1, accumulated_logging_time=1.10534, accumulated_submission_time=5531.71, global_step=5583, preemption_count=0, score=5531.71, test/loss=0.126879, test/num_examples=95000000, total_duration=36998.8, train/loss=0.124876, validation/loss=0.124562, validation/num_examples=83274637
I0316 21:08:05.754211 139623498142912 spec.py:321] Evaluating on the training split.
I0316 21:10:08.460741 139623498142912 spec.py:333] Evaluating on the validation split.
I0316 21:12:58.215027 139623498142912 spec.py:349] Evaluating on the test split.
I0316 21:17:50.349891 139623498142912 submission_runner.py:469] Time since start: 37704.20s, 	Step: 5698, 	{'train/loss': 0.12230296136095707, 'validation/loss': 0.12467129503642374, 'validation/num_examples': 83274637, 'test/loss': 0.1270730264054951, 'test/num_examples': 95000000, 'score': 5651.632122039795, 'total_duration': 37704.20431923866, 'accumulated_submission_time': 5651.632122039795, 'accumulated_eval_time': 32010.730974674225, 'accumulated_logging_time': 1.1238155364990234}
I0316 21:17:50.361488 139582158903040 logging_writer.py:48] [5698] accumulated_eval_time=32010.7, accumulated_logging_time=1.12382, accumulated_submission_time=5651.63, global_step=5698, preemption_count=0, score=5651.63, test/loss=0.127073, test/num_examples=95000000, total_duration=37704.2, train/loss=0.122303, validation/loss=0.124671, validation/num_examples=83274637
I0316 21:19:51.658000 139623498142912 spec.py:321] Evaluating on the training split.
I0316 21:21:54.236658 139623498142912 spec.py:333] Evaluating on the validation split.
I0316 21:24:45.608582 139623498142912 spec.py:349] Evaluating on the test split.
I0316 21:29:43.005969 139623498142912 submission_runner.py:469] Time since start: 38416.86s, 	Step: 5821, 	{'train/loss': 0.12034476714199745, 'validation/loss': 0.12452320305980667, 'validation/num_examples': 83274637, 'test/loss': 0.12692175081670662, 'test/num_examples': 95000000, 'score': 5772.05837893486, 'total_duration': 38416.86041235924, 'accumulated_submission_time': 5772.05837893486, 'accumulated_eval_time': 32602.07928943634, 'accumulated_logging_time': 1.1638603210449219}
I0316 21:29:43.022843 139581538170624 logging_writer.py:48] [5821] accumulated_eval_time=32602.1, accumulated_logging_time=1.16386, accumulated_submission_time=5772.06, global_step=5821, preemption_count=0, score=5772.06, test/loss=0.126922, test/num_examples=95000000, total_duration=38416.9, train/loss=0.120345, validation/loss=0.124523, validation/num_examples=83274637
I0316 21:31:43.801134 139623498142912 spec.py:321] Evaluating on the training split.
I0316 21:33:46.569944 139623498142912 spec.py:333] Evaluating on the validation split.
I0316 21:36:34.170948 139623498142912 spec.py:349] Evaluating on the test split.
I0316 21:41:31.717082 139623498142912 submission_runner.py:469] Time since start: 39125.57s, 	Step: 5939, 	{'train/loss': 0.12554640548342932, 'validation/loss': 0.12441661363324939, 'validation/num_examples': 83274637, 'test/loss': 0.12671987745015997, 'test/num_examples': 95000000, 'score': 5891.966375827789, 'total_duration': 39125.57147192955, 'accumulated_submission_time': 5891.966375827789, 'accumulated_eval_time': 33189.99530720711, 'accumulated_logging_time': 1.187654733657837}
I0316 21:41:31.729108 139582158903040 logging_writer.py:48] [5939] accumulated_eval_time=33190, accumulated_logging_time=1.18765, accumulated_submission_time=5891.97, global_step=5939, preemption_count=0, score=5891.97, test/loss=0.12672, test/num_examples=95000000, total_duration=39125.6, train/loss=0.125546, validation/loss=0.124417, validation/num_examples=83274637
I0316 21:42:17.898634 139581538170624 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.0129671, loss=0.124713
I0316 21:42:17.901876 139623498142912 submission.py:265] 6000) loss = 0.125, grad_norm = 0.013
I0316 21:43:32.402924 139623498142912 spec.py:321] Evaluating on the training split.
I0316 21:45:35.347766 139623498142912 spec.py:333] Evaluating on the validation split.
I0316 21:48:22.059931 139623498142912 spec.py:349] Evaluating on the test split.
I0316 21:53:12.695571 139623498142912 submission_runner.py:469] Time since start: 39826.55s, 	Step: 6065, 	{'train/loss': 0.12189872058967031, 'validation/loss': 0.12438576817461314, 'validation/num_examples': 83274637, 'test/loss': 0.1267103862325568, 'test/num_examples': 95000000, 'score': 6011.780951023102, 'total_duration': 39826.5500292778, 'accumulated_submission_time': 6011.780951023102, 'accumulated_eval_time': 33770.28807473183, 'accumulated_logging_time': 1.2077231407165527}
I0316 21:53:12.706666 139582158903040 logging_writer.py:48] [6065] accumulated_eval_time=33770.3, accumulated_logging_time=1.20772, accumulated_submission_time=6011.78, global_step=6065, preemption_count=0, score=6011.78, test/loss=0.12671, test/num_examples=95000000, total_duration=39826.6, train/loss=0.121899, validation/loss=0.124386, validation/num_examples=83274637
I0316 21:55:14.493071 139623498142912 spec.py:321] Evaluating on the training split.
I0316 21:57:17.175250 139623498142912 spec.py:333] Evaluating on the validation split.
I0316 22:00:07.455012 139623498142912 spec.py:349] Evaluating on the test split.
I0316 22:04:59.443197 139623498142912 submission_runner.py:469] Time since start: 40533.30s, 	Step: 6190, 	{'train/loss': 0.12181094933125133, 'validation/loss': 0.12432047389998883, 'validation/num_examples': 83274637, 'test/loss': 0.1265786778611434, 'test/num_examples': 95000000, 'score': 6132.735398054123, 'total_duration': 40533.29756641388, 'accumulated_submission_time': 6132.735398054123, 'accumulated_eval_time': 34355.238230228424, 'accumulated_logging_time': 1.22560453414917}
I0316 22:04:59.456239 139581538170624 logging_writer.py:48] [6190] accumulated_eval_time=34355.2, accumulated_logging_time=1.2256, accumulated_submission_time=6132.74, global_step=6190, preemption_count=0, score=6132.74, test/loss=0.126579, test/num_examples=95000000, total_duration=40533.3, train/loss=0.121811, validation/loss=0.12432, validation/num_examples=83274637
I0316 22:07:00.742854 139623498142912 spec.py:321] Evaluating on the training split.
I0316 22:09:03.604292 139623498142912 spec.py:333] Evaluating on the validation split.
I0316 22:11:51.617091 139623498142912 spec.py:349] Evaluating on the test split.
I0316 22:16:43.388293 139623498142912 submission_runner.py:469] Time since start: 41237.24s, 	Step: 6312, 	{'train/loss': 0.12188161369938072, 'validation/loss': 0.1243671047714258, 'validation/num_examples': 83274637, 'test/loss': 0.12664027764511107, 'test/num_examples': 95000000, 'score': 6253.155354976654, 'total_duration': 41237.242715120316, 'accumulated_submission_time': 6253.155354976654, 'accumulated_eval_time': 34937.88372421265, 'accumulated_logging_time': 1.2463791370391846}
I0316 22:16:43.400210 139582158903040 logging_writer.py:48] [6312] accumulated_eval_time=34937.9, accumulated_logging_time=1.24638, accumulated_submission_time=6253.16, global_step=6312, preemption_count=0, score=6253.16, test/loss=0.12664, test/num_examples=95000000, total_duration=41237.2, train/loss=0.121882, validation/loss=0.124367, validation/num_examples=83274637
I0316 22:18:43.944047 139623498142912 spec.py:321] Evaluating on the training split.
I0316 22:20:47.045724 139623498142912 spec.py:333] Evaluating on the validation split.
I0316 22:23:31.077640 139623498142912 spec.py:349] Evaluating on the test split.
I0316 22:28:58.373992 139623498142912 submission_runner.py:469] Time since start: 41972.23s, 	Step: 6438, 	{'train/loss': 0.12138663749134498, 'validation/loss': 0.12419374209859002, 'validation/num_examples': 83274637, 'test/loss': 0.12647966727387278, 'test/num_examples': 95000000, 'score': 6372.776814937592, 'total_duration': 41972.22842526436, 'accumulated_submission_time': 6372.776814937592, 'accumulated_eval_time': 35552.31383776665, 'accumulated_logging_time': 1.319519281387329}
I0316 22:28:58.385650 139581538170624 logging_writer.py:48] [6438] accumulated_eval_time=35552.3, accumulated_logging_time=1.31952, accumulated_submission_time=6372.78, global_step=6438, preemption_count=0, score=6372.78, test/loss=0.12648, test/num_examples=95000000, total_duration=41972.2, train/loss=0.121387, validation/loss=0.124194, validation/num_examples=83274637
I0316 22:29:44.191536 139582158903040 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.0163035, loss=0.121058
I0316 22:29:44.194891 139623498142912 submission.py:265] 6500) loss = 0.121, grad_norm = 0.016
I0316 22:30:59.399135 139623498142912 spec.py:321] Evaluating on the training split.
I0316 22:33:02.048416 139623498142912 spec.py:333] Evaluating on the validation split.
I0316 22:35:51.877642 139623498142912 spec.py:349] Evaluating on the test split.
I0316 22:41:22.595603 139623498142912 submission_runner.py:469] Time since start: 42716.45s, 	Step: 6561, 	{'train/loss': 0.12388614861117084, 'validation/loss': 0.12429494085782183, 'validation/num_examples': 83274637, 'test/loss': 0.12658443023392527, 'test/num_examples': 95000000, 'score': 6492.889795780182, 'total_duration': 42716.45002245903, 'accumulated_submission_time': 6492.889795780182, 'accumulated_eval_time': 36175.510634183884, 'accumulated_logging_time': 1.3381297588348389}
I0316 22:41:22.607000 139581538170624 logging_writer.py:48] [6561] accumulated_eval_time=36175.5, accumulated_logging_time=1.33813, accumulated_submission_time=6492.89, global_step=6561, preemption_count=0, score=6492.89, test/loss=0.126584, test/num_examples=95000000, total_duration=42716.5, train/loss=0.123886, validation/loss=0.124295, validation/num_examples=83274637
I0316 22:43:23.585728 139623498142912 spec.py:321] Evaluating on the training split.
I0316 22:45:26.346100 139623498142912 spec.py:333] Evaluating on the validation split.
I0316 22:48:13.314423 139623498142912 spec.py:349] Evaluating on the test split.
I0316 22:53:39.472643 139623498142912 submission_runner.py:469] Time since start: 43453.33s, 	Step: 6679, 	{'train/loss': 0.12248985227922056, 'validation/loss': 0.1242328017527342, 'validation/num_examples': 83274637, 'test/loss': 0.12640517909622193, 'test/num_examples': 95000000, 'score': 6613.025106430054, 'total_duration': 43453.327095746994, 'accumulated_submission_time': 6613.025106430054, 'accumulated_eval_time': 36791.397654533386, 'accumulated_logging_time': 1.3561313152313232}
I0316 22:53:39.483571 139582158903040 logging_writer.py:48] [6679] accumulated_eval_time=36791.4, accumulated_logging_time=1.35613, accumulated_submission_time=6613.03, global_step=6679, preemption_count=0, score=6613.03, test/loss=0.126405, test/num_examples=95000000, total_duration=43453.3, train/loss=0.12249, validation/loss=0.124233, validation/num_examples=83274637
I0316 22:55:40.918963 139623498142912 spec.py:321] Evaluating on the training split.
I0316 22:57:43.861391 139623498142912 spec.py:333] Evaluating on the validation split.
I0316 23:00:32.774199 139623498142912 spec.py:349] Evaluating on the test split.
I0316 23:06:00.652837 139623498142912 submission_runner.py:469] Time since start: 44194.51s, 	Step: 6805, 	{'train/loss': 0.1214268349995406, 'validation/loss': 0.12413877724797974, 'validation/num_examples': 83274637, 'test/loss': 0.12642759158477784, 'test/num_examples': 95000000, 'score': 6733.60741519928, 'total_duration': 44194.50724029541, 'accumulated_submission_time': 6733.60741519928, 'accumulated_eval_time': 37411.13156104088, 'accumulated_logging_time': 1.3740413188934326}
I0316 23:06:00.664844 139581538170624 logging_writer.py:48] [6805] accumulated_eval_time=37411.1, accumulated_logging_time=1.37404, accumulated_submission_time=6733.61, global_step=6805, preemption_count=0, score=6733.61, test/loss=0.126428, test/num_examples=95000000, total_duration=44194.5, train/loss=0.121427, validation/loss=0.124139, validation/num_examples=83274637
I0316 23:08:02.115239 139623498142912 spec.py:321] Evaluating on the training split.
I0316 23:10:04.901095 139623498142912 spec.py:333] Evaluating on the validation split.
I0316 23:12:52.937027 139623498142912 spec.py:349] Evaluating on the test split.
I0316 23:18:05.017380 139623498142912 submission_runner.py:469] Time since start: 44918.87s, 	Step: 6929, 	{'train/loss': 0.1232362847617924, 'validation/loss': 0.12402385417912581, 'validation/num_examples': 83274637, 'test/loss': 0.12628147105604473, 'test/num_examples': 95000000, 'score': 6854.153039455414, 'total_duration': 44918.87181663513, 'accumulated_submission_time': 6854.153039455414, 'accumulated_eval_time': 38014.03385639191, 'accumulated_logging_time': 1.4484620094299316}
I0316 23:18:05.047767 139582158903040 logging_writer.py:48] [6929] accumulated_eval_time=38014, accumulated_logging_time=1.44846, accumulated_submission_time=6854.15, global_step=6929, preemption_count=0, score=6854.15, test/loss=0.126281, test/num_examples=95000000, total_duration=44918.9, train/loss=0.123236, validation/loss=0.124024, validation/num_examples=83274637
I0316 23:19:02.580583 139581538170624 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.0175248, loss=0.142365
I0316 23:19:02.583961 139623498142912 submission.py:265] 7000) loss = 0.142, grad_norm = 0.018
I0316 23:20:05.458534 139623498142912 spec.py:321] Evaluating on the training split.
I0316 23:22:08.493563 139623498142912 spec.py:333] Evaluating on the validation split.
I0316 23:24:59.964257 139623498142912 spec.py:349] Evaluating on the test split.
I0316 23:30:22.072772 139623498142912 submission_runner.py:469] Time since start: 45655.93s, 	Step: 7050, 	{'train/loss': 0.12292576135668816, 'validation/loss': 0.12411629573487525, 'validation/num_examples': 83274637, 'test/loss': 0.12639887875221653, 'test/num_examples': 95000000, 'score': 6973.70441865921, 'total_duration': 45655.927186727524, 'accumulated_submission_time': 6973.70441865921, 'accumulated_eval_time': 38630.64813756943, 'accumulated_logging_time': 1.4854764938354492}
I0316 23:30:22.085448 139582158903040 logging_writer.py:48] [7050] accumulated_eval_time=38630.6, accumulated_logging_time=1.48548, accumulated_submission_time=6973.7, global_step=7050, preemption_count=0, score=6973.7, test/loss=0.126399, test/num_examples=95000000, total_duration=45655.9, train/loss=0.122926, validation/loss=0.124116, validation/num_examples=83274637
I0316 23:32:22.605769 139623498142912 spec.py:321] Evaluating on the training split.
I0316 23:34:25.210839 139623498142912 spec.py:333] Evaluating on the validation split.
I0316 23:37:15.430851 139623498142912 spec.py:349] Evaluating on the test split.
I0316 23:42:41.462416 139623498142912 submission_runner.py:469] Time since start: 46395.32s, 	Step: 7178, 	{'train/loss': 0.12228470035619227, 'validation/loss': 0.12390800129603464, 'validation/num_examples': 83274637, 'test/loss': 0.12617333150185034, 'test/num_examples': 95000000, 'score': 7093.348165273666, 'total_duration': 46395.31681418419, 'accumulated_submission_time': 7093.348165273666, 'accumulated_eval_time': 39249.50490236282, 'accumulated_logging_time': 1.5052170753479004}
I0316 23:42:41.474866 139581538170624 logging_writer.py:48] [7178] accumulated_eval_time=39249.5, accumulated_logging_time=1.50522, accumulated_submission_time=7093.35, global_step=7178, preemption_count=0, score=7093.35, test/loss=0.126173, test/num_examples=95000000, total_duration=46395.3, train/loss=0.122285, validation/loss=0.123908, validation/num_examples=83274637
I0316 23:44:42.013973 139623498142912 spec.py:321] Evaluating on the training split.
I0316 23:46:44.885140 139623498142912 spec.py:333] Evaluating on the validation split.
I0316 23:49:33.752291 139623498142912 spec.py:349] Evaluating on the test split.
I0316 23:55:00.343406 139623498142912 submission_runner.py:469] Time since start: 47134.20s, 	Step: 7303, 	{'train/loss': 0.12345492185294825, 'validation/loss': 0.12398254758181432, 'validation/num_examples': 83274637, 'test/loss': 0.12618481661694175, 'test/num_examples': 95000000, 'score': 7213.02393579483, 'total_duration': 47134.19782781601, 'accumulated_submission_time': 7213.02393579483, 'accumulated_eval_time': 39867.8343808651, 'accumulated_logging_time': 1.5247740745544434}
I0316 23:55:00.355280 139582158903040 logging_writer.py:48] [7303] accumulated_eval_time=39867.8, accumulated_logging_time=1.52477, accumulated_submission_time=7213.02, global_step=7303, preemption_count=0, score=7213.02, test/loss=0.126185, test/num_examples=95000000, total_duration=47134.2, train/loss=0.123455, validation/loss=0.123983, validation/num_examples=83274637
I0316 23:57:00.747789 139623498142912 spec.py:321] Evaluating on the training split.
I0316 23:59:03.581617 139623498142912 spec.py:333] Evaluating on the validation split.
I0317 00:01:49.362266 139623498142912 spec.py:349] Evaluating on the test split.
I0317 00:07:04.477731 139623498142912 submission_runner.py:469] Time since start: 47858.33s, 	Step: 7424, 	{'train/loss': 0.12247920945958629, 'validation/loss': 0.12387737668118345, 'validation/num_examples': 83274637, 'test/loss': 0.12614726133382698, 'test/num_examples': 95000000, 'score': 7332.562870502472, 'total_duration': 47858.33213329315, 'accumulated_submission_time': 7332.562870502472, 'accumulated_eval_time': 40471.56435704231, 'accumulated_logging_time': 1.5759985446929932}
I0317 00:07:04.490940 139581538170624 logging_writer.py:48] [7424] accumulated_eval_time=40471.6, accumulated_logging_time=1.576, accumulated_submission_time=7332.56, global_step=7424, preemption_count=0, score=7332.56, test/loss=0.126147, test/num_examples=95000000, total_duration=47858.3, train/loss=0.122479, validation/loss=0.123877, validation/num_examples=83274637
I0317 00:08:06.613429 139582158903040 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.0327682, loss=0.122028
I0317 00:08:06.616714 139623498142912 submission.py:265] 7500) loss = 0.122, grad_norm = 0.033
I0317 00:09:05.198589 139623498142912 spec.py:321] Evaluating on the training split.
I0317 00:11:07.833583 139623498142912 spec.py:333] Evaluating on the validation split.
I0317 00:13:55.145316 139623498142912 spec.py:349] Evaluating on the test split.
I0317 00:19:12.772348 139623498142912 submission_runner.py:469] Time since start: 48586.63s, 	Step: 7547, 	{'train/loss': 0.12366073518434839, 'validation/loss': 0.12389000564290518, 'validation/num_examples': 83274637, 'test/loss': 0.1262302362401862, 'test/num_examples': 95000000, 'score': 7452.361122369766, 'total_duration': 48586.62675046921, 'accumulated_submission_time': 7452.361122369766, 'accumulated_eval_time': 41079.138211250305, 'accumulated_logging_time': 1.5963597297668457}
I0317 00:19:12.785615 139581538170624 logging_writer.py:48] [7547] accumulated_eval_time=41079.1, accumulated_logging_time=1.59636, accumulated_submission_time=7452.36, global_step=7547, preemption_count=0, score=7452.36, test/loss=0.12623, test/num_examples=95000000, total_duration=48586.6, train/loss=0.123661, validation/loss=0.12389, validation/num_examples=83274637
I0317 00:21:13.233115 139623498142912 spec.py:321] Evaluating on the training split.
I0317 00:23:16.157393 139623498142912 spec.py:333] Evaluating on the validation split.
I0317 00:26:08.389596 139623498142912 spec.py:349] Evaluating on the test split.
I0317 00:30:55.523470 139623498142912 submission_runner.py:469] Time since start: 49289.38s, 	Step: 7673, 	{'train/loss': 0.12266639767257896, 'validation/loss': 0.12396128231282985, 'validation/num_examples': 83274637, 'test/loss': 0.1263820700949418, 'test/num_examples': 95000000, 'score': 7571.937877416611, 'total_duration': 49289.377920627594, 'accumulated_submission_time': 7571.937877416611, 'accumulated_eval_time': 41661.42863559723, 'accumulated_logging_time': 1.6163709163665771}
I0317 00:30:55.552896 139582158903040 logging_writer.py:48] [7673] accumulated_eval_time=41661.4, accumulated_logging_time=1.61637, accumulated_submission_time=7571.94, global_step=7673, preemption_count=0, score=7571.94, test/loss=0.126382, test/num_examples=95000000, total_duration=49289.4, train/loss=0.122666, validation/loss=0.123961, validation/num_examples=83274637
I0317 00:32:56.092842 139623498142912 spec.py:321] Evaluating on the training split.
I0317 00:34:59.387966 139623498142912 spec.py:333] Evaluating on the validation split.
I0317 00:37:47.261380 139623498142912 spec.py:349] Evaluating on the test split.
I0317 00:42:39.418414 139623498142912 submission_runner.py:469] Time since start: 49993.27s, 	Step: 7791, 	{'train/loss': 0.12233210956143149, 'validation/loss': 0.12374006380865757, 'validation/num_examples': 83274637, 'test/loss': 0.12606681039725856, 'test/num_examples': 95000000, 'score': 7691.582696914673, 'total_duration': 49993.27279305458, 'accumulated_submission_time': 7691.582696914673, 'accumulated_eval_time': 42244.75424671173, 'accumulated_logging_time': 1.652780532836914}
I0317 00:42:39.430142 139581538170624 logging_writer.py:48] [7791] accumulated_eval_time=42244.8, accumulated_logging_time=1.65278, accumulated_submission_time=7691.58, global_step=7791, preemption_count=0, score=7691.58, test/loss=0.126067, test/num_examples=95000000, total_duration=49993.3, train/loss=0.122332, validation/loss=0.12374, validation/num_examples=83274637
I0317 00:44:40.949749 139582158903040 logging_writer.py:48] [7916] global_step=7916, preemption_count=0, score=7812.63
I0317 00:44:44.144626 139623498142912 submission_runner.py:646] Tuning trial 5/5
I0317 00:44:44.144864 139623498142912 submission_runner.py:647] Hyperparameters: Hyperparameters(learning_rate=0.001308209823469072, one_minus_beta1=0.02686663061, one_minus_beta2=0.00187670778, epsilon=1e-08, one_minus_momentum=0.0, use_momentum=False, weight_decay=0.16375311233774334, max_preconditioner_dim=1024, precondition_frequency=100, start_preconditioning_step=-1, inv_root_override=0, exponent_multiplier=1.0, grafting_type='ADAM', grafting_epsilon=1e-08, use_normalized_grafting=False, communication_dtype='FP32', communicate_params=True, use_cosine_decay=True, warmup_factor=0.1, label_smoothing=0.1, dropout_rate=0.0, use_nadam=True, step_hint_factor=1.0)
I0317 00:44:44.146447 139623498142912 submission_runner.py:648] Metrics: {'eval_results': [(1, {'train/loss': 0.8509040423757784, 'validation/loss': 0.8602193090610964, 'validation/num_examples': 83274637, 'test/loss': 0.8528632051263106, 'test/num_examples': 95000000, 'score': 9.34485936164856, 'total_duration': 967.795530796051, 'accumulated_submission_time': 9.34485936164856, 'accumulated_eval_time': 958.064713716507, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (122, {'train/loss': 0.13806551262839292, 'validation/loss': 0.13844054850217832, 'validation/num_examples': 83274637, 'test/loss': 0.14160634178262008, 'test/num_examples': 95000000, 'score': 129.47463846206665, 'total_duration': 1956.582330942154, 'accumulated_submission_time': 129.47463846206665, 'accumulated_eval_time': 1825.7606344223022, 'accumulated_logging_time': 0.0716865062713623, 'global_step': 122, 'preemption_count': 0}), (245, {'train/loss': 0.12827721130914185, 'validation/loss': 0.1292842964798221, 'validation/num_examples': 83274637, 'test/loss': 0.13239581693460564, 'test/num_examples': 95000000, 'score': 250.12658643722534, 'total_duration': 2958.032777070999, 'accumulated_submission_time': 250.12658643722534, 'accumulated_eval_time': 2705.6396358013153, 'accumulated_logging_time': 0.09008240699768066, 'global_step': 245, 'preemption_count': 0}), (369, {'train/loss': 0.12884185960768815, 'validation/loss': 0.128320491409564, 'validation/num_examples': 83274637, 'test/loss': 0.13086834462432861, 'test/num_examples': 95000000, 'score': 370.00406861305237, 'total_duration': 3951.0798490047455, 'accumulated_submission_time': 370.00406861305237, 'accumulated_eval_time': 3577.880513191223, 'accumulated_logging_time': 0.1324470043182373, 'global_step': 369, 'preemption_count': 0}), (495, {'train/loss': 0.12716185972564503, 'validation/loss': 0.12788654373844685, 'validation/num_examples': 83274637, 'test/loss': 0.130423721377443, 'test/num_examples': 95000000, 'score': 490.14915013313293, 'total_duration': 4968.251679420471, 'accumulated_submission_time': 490.14915013313293, 'accumulated_eval_time': 4473.984615802765, 'accumulated_logging_time': 0.14891862869262695, 'global_step': 495, 'preemption_count': 0}), (613, {'train/loss': 0.12621784877750641, 'validation/loss': 0.1272189770564625, 'validation/num_examples': 83274637, 'test/loss': 0.1297390959479081, 'test/num_examples': 95000000, 'score': 610.804224729538, 'total_duration': 5949.510470867157, 'accumulated_submission_time': 610.804224729538, 'accumulated_eval_time': 5333.766624212265, 'accumulated_logging_time': 0.16493654251098633, 'global_step': 613, 'preemption_count': 0}), (733, {'train/loss': 0.12875681522297017, 'validation/loss': 0.12715421042353586, 'validation/num_examples': 83274637, 'test/loss': 0.12978260829283061, 'test/num_examples': 95000000, 'score': 730.5265727043152, 'total_duration': 6969.278084278107, 'accumulated_submission_time': 730.5265727043152, 'accumulated_eval_time': 6232.9575753211975, 'accumulated_logging_time': 0.18106508255004883, 'global_step': 733, 'preemption_count': 0}), (858, {'train/loss': 0.12436651434590328, 'validation/loss': 0.1269641372722766, 'validation/num_examples': 83274637, 'test/loss': 0.12938452772851242, 'test/num_examples': 95000000, 'score': 850.8974733352661, 'total_duration': 7952.590391874313, 'accumulated_submission_time': 850.8974733352661, 'accumulated_eval_time': 7095.0645706653595, 'accumulated_logging_time': 0.1976301670074463, 'global_step': 858, 'preemption_count': 0}), (982, {'train/loss': 0.12369887852066912, 'validation/loss': 0.1262706936406851, 'validation/num_examples': 83274637, 'test/loss': 0.12865905622763382, 'test/num_examples': 95000000, 'score': 971.0415787696838, 'total_duration': 8926.273813962936, 'accumulated_submission_time': 971.0415787696838, 'accumulated_eval_time': 7947.68154168129, 'accumulated_logging_time': 0.2162764072418213, 'global_step': 982, 'preemption_count': 0}), (1102, {'train/loss': 0.12635882149972483, 'validation/loss': 0.12613673984355572, 'validation/num_examples': 83274637, 'test/loss': 0.12872901026864303, 'test/num_examples': 95000000, 'score': 1090.6885466575623, 'total_duration': 9883.612463235855, 'accumulated_submission_time': 1090.6885466575623, 'accumulated_eval_time': 8784.5363342762, 'accumulated_logging_time': 0.23322176933288574, 'global_step': 1102, 'preemption_count': 0}), (1226, {'train/loss': 0.1246948797551568, 'validation/loss': 0.12619120496811176, 'validation/num_examples': 83274637, 'test/loss': 0.12861853735801296, 'test/num_examples': 95000000, 'score': 1210.2924740314484, 'total_duration': 10766.743099212646, 'accumulated_submission_time': 1210.2924740314484, 'accumulated_eval_time': 9547.188748598099, 'accumulated_logging_time': 0.2787351608276367, 'global_step': 1226, 'preemption_count': 0}), (1347, {'train/loss': 0.12487287523480363, 'validation/loss': 0.12618856752650587, 'validation/num_examples': 83274637, 'test/loss': 0.12854281655739233, 'test/num_examples': 95000000, 'score': 1329.868495464325, 'total_duration': 11568.682466983795, 'accumulated_submission_time': 1329.868495464325, 'accumulated_eval_time': 10228.687976837158, 'accumulated_logging_time': 0.29607248306274414, 'global_step': 1347, 'preemption_count': 0}), (1470, {'train/loss': 0.12634461405283026, 'validation/loss': 0.12564555551167375, 'validation/num_examples': 83274637, 'test/loss': 0.1279155673620525, 'test/num_examples': 95000000, 'score': 1450.7699003219604, 'total_duration': 12362.226927995682, 'accumulated_submission_time': 1450.7699003219604, 'accumulated_eval_time': 10900.479144573212, 'accumulated_logging_time': 0.31328725814819336, 'global_step': 1470, 'preemption_count': 0}), (1584, {'train/loss': 0.1256123556407451, 'validation/loss': 0.12596999658357036, 'validation/num_examples': 83274637, 'test/loss': 0.12837742085422715, 'test/num_examples': 95000000, 'score': 1570.5822021961212, 'total_duration': 13097.19689154625, 'accumulated_submission_time': 1570.5822021961212, 'accumulated_eval_time': 11514.74446439743, 'accumulated_logging_time': 0.33088231086730957, 'global_step': 1584, 'preemption_count': 0}), (1701, {'train/loss': 0.12518251146817042, 'validation/loss': 0.1257016998374178, 'validation/num_examples': 83274637, 'test/loss': 0.12792220989580658, 'test/num_examples': 95000000, 'score': 1690.1199131011963, 'total_duration': 13838.325284719467, 'accumulated_submission_time': 1690.1199131011963, 'accumulated_eval_time': 12135.435537338257, 'accumulated_logging_time': 0.34783506393432617, 'global_step': 1701, 'preemption_count': 0}), (1827, {'train/loss': 0.1234934760007319, 'validation/loss': 0.12550876013258422, 'validation/num_examples': 83274637, 'test/loss': 0.12794491630791113, 'test/num_examples': 95000000, 'score': 1810.2585709095001, 'total_duration': 14539.868153572083, 'accumulated_submission_time': 1810.2585709095001, 'accumulated_eval_time': 12715.97259235382, 'accumulated_logging_time': 0.3658175468444824, 'global_step': 1827, 'preemption_count': 0}), (1945, {'train/loss': 0.12417711971297248, 'validation/loss': 0.12558359346735032, 'validation/num_examples': 83274637, 'test/loss': 0.12786557774770635, 'test/num_examples': 95000000, 'score': 1930.198768377304, 'total_duration': 15248.789781332016, 'accumulated_submission_time': 1930.198768377304, 'accumulated_eval_time': 13304.020874261856, 'accumulated_logging_time': 0.38471341133117676, 'global_step': 1945, 'preemption_count': 0}), (2064, {'train/loss': 0.12218235091693053, 'validation/loss': 0.12554543336754323, 'validation/num_examples': 83274637, 'test/loss': 0.12794666495851215, 'test/num_examples': 95000000, 'score': 2050.230681657791, 'total_duration': 15958.031909704208, 'accumulated_submission_time': 2050.230681657791, 'accumulated_eval_time': 13892.361625671387, 'accumulated_logging_time': 0.40320491790771484, 'global_step': 2064, 'preemption_count': 0}), (2186, {'train/loss': 0.12422673145684515, 'validation/loss': 0.12547692201677138, 'validation/num_examples': 83274637, 'test/loss': 0.12792899866007754, 'test/num_examples': 95000000, 'score': 2169.9307646751404, 'total_duration': 16659.395295143127, 'accumulated_submission_time': 2169.9307646751404, 'accumulated_eval_time': 14473.151748418808, 'accumulated_logging_time': 0.42026329040527344, 'global_step': 2186, 'preemption_count': 0}), (2310, {'train/loss': 0.12408775379289452, 'validation/loss': 0.12574555858318046, 'validation/num_examples': 83274637, 'test/loss': 0.1281450340647647, 'test/num_examples': 95000000, 'score': 2289.9896636009216, 'total_duration': 17369.239292383194, 'accumulated_submission_time': 2289.9896636009216, 'accumulated_eval_time': 15062.073969602585, 'accumulated_logging_time': 0.4383575916290283, 'global_step': 2310, 'preemption_count': 0}), (2432, {'train/loss': 0.12384349610116908, 'validation/loss': 0.1254531870440181, 'validation/num_examples': 83274637, 'test/loss': 0.12771855525472542, 'test/num_examples': 95000000, 'score': 2410.4140713214874, 'total_duration': 18112.318170547485, 'accumulated_submission_time': 2410.4140713214874, 'accumulated_eval_time': 15683.894177913666, 'accumulated_logging_time': 0.45590782165527344, 'global_step': 2432, 'preemption_count': 0}), (2555, {'train/loss': 0.12397566745316047, 'validation/loss': 0.12586067334073187, 'validation/num_examples': 83274637, 'test/loss': 0.12820455992692648, 'test/num_examples': 95000000, 'score': 2530.7806046009064, 'total_duration': 18854.05129623413, 'accumulated_submission_time': 2530.7806046009064, 'accumulated_eval_time': 16304.36704492569, 'accumulated_logging_time': 0.4743473529815674, 'global_step': 2555, 'preemption_count': 0}), (2678, {'train/loss': 0.12354351886968232, 'validation/loss': 0.12534764879768526, 'validation/num_examples': 83274637, 'test/loss': 0.1278186221743935, 'test/num_examples': 95000000, 'score': 2650.943372964859, 'total_duration': 19591.85366177559, 'accumulated_submission_time': 2650.943372964859, 'accumulated_eval_time': 16921.09463620186, 'accumulated_logging_time': 0.4915468692779541, 'global_step': 2678, 'preemption_count': 0}), (2796, {'train/loss': 0.12463771776474292, 'validation/loss': 0.12546456245493184, 'validation/num_examples': 83274637, 'test/loss': 0.1279700280576204, 'test/num_examples': 95000000, 'score': 2770.7136147022247, 'total_duration': 20316.958945035934, 'accumulated_submission_time': 2770.7136147022247, 'accumulated_eval_time': 17525.47950720787, 'accumulated_logging_time': 0.5335021018981934, 'global_step': 2796, 'preemption_count': 0}), (2915, {'train/loss': 0.1254480393663358, 'validation/loss': 0.12527963424859034, 'validation/num_examples': 83274637, 'test/loss': 0.12764556552148115, 'test/num_examples': 95000000, 'score': 2890.4767711162567, 'total_duration': 21056.670206069946, 'accumulated_submission_time': 2890.4767711162567, 'accumulated_eval_time': 18144.56298518181, 'accumulated_logging_time': 0.5507709980010986, 'global_step': 2915, 'preemption_count': 0}), (3035, {'train/loss': 0.12493569724820225, 'validation/loss': 0.12493672870038014, 'validation/num_examples': 83274637, 'test/loss': 0.12718467562480726, 'test/num_examples': 95000000, 'score': 3010.984120607376, 'total_duration': 21799.721139907837, 'accumulated_submission_time': 3010.984120607376, 'accumulated_eval_time': 18766.243144989014, 'accumulated_logging_time': 0.5684123039245605, 'global_step': 3035, 'preemption_count': 0}), (3155, {'train/loss': 0.12367085986341031, 'validation/loss': 0.12487770361126967, 'validation/num_examples': 83274637, 'test/loss': 0.1271944655845642, 'test/num_examples': 95000000, 'score': 3130.9257090091705, 'total_duration': 22531.786817789078, 'accumulated_submission_time': 3130.9257090091705, 'accumulated_eval_time': 19377.528700590134, 'accumulated_logging_time': 0.5857093334197998, 'global_step': 3155, 'preemption_count': 0}), (3273, {'train/loss': 0.12271482987071386, 'validation/loss': 0.1248615626999788, 'validation/num_examples': 83274637, 'test/loss': 0.12731455375719572, 'test/num_examples': 95000000, 'score': 3250.7646675109863, 'total_duration': 23267.99680829048, 'accumulated_submission_time': 3250.7646675109863, 'accumulated_eval_time': 19993.034805059433, 'accumulated_logging_time': 0.609102725982666, 'global_step': 3273, 'preemption_count': 0}), (3393, {'train/loss': 0.12401396967814189, 'validation/loss': 0.12501035104564287, 'validation/num_examples': 83274637, 'test/loss': 0.12742764343285812, 'test/num_examples': 95000000, 'score': 3370.6447043418884, 'total_duration': 24006.899938106537, 'accumulated_submission_time': 3370.6447043418884, 'accumulated_eval_time': 20611.158259153366, 'accumulated_logging_time': 0.6263463497161865, 'global_step': 3393, 'preemption_count': 0}), (3506, {'train/loss': 0.1243174592763722, 'validation/loss': 0.12495218051923368, 'validation/num_examples': 83274637, 'test/loss': 0.12738912677556088, 'test/num_examples': 95000000, 'score': 3490.1970858573914, 'total_duration': 24744.22510766983, 'accumulated_submission_time': 3490.1970858573914, 'accumulated_eval_time': 21228.049637317657, 'accumulated_logging_time': 0.6437513828277588, 'global_step': 3506, 'preemption_count': 0}), (3628, {'train/loss': 0.12398448433491034, 'validation/loss': 0.12500903585409734, 'validation/num_examples': 83274637, 'test/loss': 0.127495093280471, 'test/num_examples': 95000000, 'score': 3609.8278489112854, 'total_duration': 25490.883791446686, 'accumulated_submission_time': 3609.8278489112854, 'accumulated_eval_time': 21854.254301548004, 'accumulated_logging_time': 0.662665843963623, 'global_step': 3628, 'preemption_count': 0}), (3746, {'train/loss': 0.12119297679258463, 'validation/loss': 0.12482603595670116, 'validation/num_examples': 83274637, 'test/loss': 0.12727339539064106, 'test/num_examples': 95000000, 'score': 3729.7345538139343, 'total_duration': 26237.787905216217, 'accumulated_submission_time': 3729.7345538139343, 'accumulated_eval_time': 22480.36032795906, 'accumulated_logging_time': 0.7010600566864014, 'global_step': 3746, 'preemption_count': 0}), (3867, {'train/loss': 0.12453811648245897, 'validation/loss': 0.12493507347785834, 'validation/num_examples': 83274637, 'test/loss': 0.12725578205052426, 'test/num_examples': 95000000, 'score': 3849.7473697662354, 'total_duration': 26984.64945745468, 'accumulated_submission_time': 3849.7473697662354, 'accumulated_eval_time': 23106.33301258087, 'accumulated_logging_time': 0.7192695140838623, 'global_step': 3867, 'preemption_count': 0}), (3992, {'train/loss': 0.12382641563337855, 'validation/loss': 0.12484541726380283, 'validation/num_examples': 83274637, 'test/loss': 0.12720820917928596, 'test/num_examples': 95000000, 'score': 3970.1233274936676, 'total_duration': 27728.01472067833, 'accumulated_submission_time': 3970.1233274936676, 'accumulated_eval_time': 23728.395102262497, 'accumulated_logging_time': 0.7544982433319092, 'global_step': 3992, 'preemption_count': 0}), (4116, {'train/loss': 0.12723006315902388, 'validation/loss': 0.12456052426140352, 'validation/num_examples': 83274637, 'test/loss': 0.1269691666005988, 'test/num_examples': 95000000, 'score': 4090.101422071457, 'total_duration': 28465.92551803589, 'accumulated_submission_time': 4090.101422071457, 'accumulated_eval_time': 24345.50083732605, 'accumulated_logging_time': 0.7718675136566162, 'global_step': 4116, 'preemption_count': 0}), (4239, {'train/loss': 0.12348023308782753, 'validation/loss': 0.12476303320561008, 'validation/num_examples': 83274637, 'test/loss': 0.12712341542037162, 'test/num_examples': 95000000, 'score': 4210.39488530159, 'total_duration': 29207.548905849457, 'accumulated_submission_time': 4210.39488530159, 'accumulated_eval_time': 24965.902668237686, 'accumulated_logging_time': 0.8361103534698486, 'global_step': 4239, 'preemption_count': 0}), (4355, {'train/loss': 0.1224880539965597, 'validation/loss': 0.12471488873449076, 'validation/num_examples': 83274637, 'test/loss': 0.12713790291491056, 'test/num_examples': 95000000, 'score': 4330.726969718933, 'total_duration': 29904.439235925674, 'accumulated_submission_time': 4330.726969718933, 'accumulated_eval_time': 25541.55055999756, 'accumulated_logging_time': 0.8540787696838379, 'global_step': 4355, 'preemption_count': 0}), (4478, {'train/loss': 0.12386821500069113, 'validation/loss': 0.12472198941448619, 'validation/num_examples': 83274637, 'test/loss': 0.1268932403778076, 'test/num_examples': 95000000, 'score': 4450.2883644104, 'total_duration': 30607.967757701874, 'accumulated_submission_time': 4450.2883644104, 'accumulated_eval_time': 26124.645430088043, 'accumulated_logging_time': 0.8714489936828613, 'global_step': 4478, 'preemption_count': 0}), (4603, {'train/loss': 0.12313326203136774, 'validation/loss': 0.12463869377952648, 'validation/num_examples': 83274637, 'test/loss': 0.12698132943147358, 'test/num_examples': 95000000, 'score': 4570.556317567825, 'total_duration': 31316.436642885208, 'accumulated_submission_time': 4570.556317567825, 'accumulated_eval_time': 26711.950392961502, 'accumulated_logging_time': 0.8907320499420166, 'global_step': 4603, 'preemption_count': 0}), (4726, {'train/loss': 0.12414826662742304, 'validation/loss': 0.12451841272615748, 'validation/num_examples': 83274637, 'test/loss': 0.12705076838133963, 'test/num_examples': 95000000, 'score': 4690.617697477341, 'total_duration': 32028.28920149803, 'accumulated_submission_time': 4690.617697477341, 'accumulated_eval_time': 27302.880423545837, 'accumulated_logging_time': 0.9081294536590576, 'global_step': 4726, 'preemption_count': 0}), (4851, {'train/loss': 0.12358901492563137, 'validation/loss': 0.1244741529996126, 'validation/num_examples': 83274637, 'test/loss': 0.1268417391059474, 'test/num_examples': 95000000, 'score': 4810.997691869736, 'total_duration': 32735.54348564148, 'accumulated_submission_time': 4810.997691869736, 'accumulated_eval_time': 27888.83693909645, 'accumulated_logging_time': 0.9752039909362793, 'global_step': 4851, 'preemption_count': 0}), (4975, {'train/loss': 0.12279934357800808, 'validation/loss': 0.12448895181058828, 'validation/num_examples': 83274637, 'test/loss': 0.1268811529253508, 'test/num_examples': 95000000, 'score': 4931.4783935546875, 'total_duration': 33436.135644197464, 'accumulated_submission_time': 4931.4783935546875, 'accumulated_eval_time': 28468.054042577744, 'accumulated_logging_time': 1.0002901554107666, 'global_step': 4975, 'preemption_count': 0}), (5099, {'train/loss': 0.12242894497598897, 'validation/loss': 0.12452565325299166, 'validation/num_examples': 83274637, 'test/loss': 0.1268863835630718, 'test/num_examples': 95000000, 'score': 5051.892138957977, 'total_duration': 34149.97521138191, 'accumulated_submission_time': 5051.892138957977, 'accumulated_eval_time': 29060.615869998932, 'accumulated_logging_time': 1.0188190937042236, 'global_step': 5099, 'preemption_count': 0}), (5220, {'train/loss': 0.1224124448533501, 'validation/loss': 0.12466407096583698, 'validation/num_examples': 83274637, 'test/loss': 0.1271119116803621, 'test/num_examples': 95000000, 'score': 5171.803838014603, 'total_duration': 34853.07766509056, 'accumulated_submission_time': 5171.803838014603, 'accumulated_eval_time': 29642.93873500824, 'accumulated_logging_time': 1.0363359451293945, 'global_step': 5220, 'preemption_count': 0}), (5337, {'train/loss': 0.12586168701011638, 'validation/loss': 0.12455311032708939, 'validation/num_examples': 83274637, 'test/loss': 0.12683932440884238, 'test/num_examples': 95000000, 'score': 5291.309556722641, 'total_duration': 35558.71432232857, 'accumulated_submission_time': 5291.309556722641, 'accumulated_eval_time': 30228.141699790955, 'accumulated_logging_time': 1.068800687789917, 'global_step': 5337, 'preemption_count': 0}), (5456, {'train/loss': 0.12444582107050346, 'validation/loss': 0.12460705216016457, 'validation/num_examples': 83274637, 'test/loss': 0.12692160073442962, 'test/num_examples': 95000000, 'score': 5411.586529016495, 'total_duration': 36291.478420734406, 'accumulated_submission_time': 5411.586529016495, 'accumulated_eval_time': 30839.792104244232, 'accumulated_logging_time': 1.0868875980377197, 'global_step': 5456, 'preemption_count': 0}), (5583, {'train/loss': 0.12487630052911604, 'validation/loss': 0.12456235497230822, 'validation/num_examples': 83274637, 'test/loss': 0.12687887167193765, 'test/num_examples': 95000000, 'score': 5531.7102954387665, 'total_duration': 36998.82051920891, 'accumulated_submission_time': 5531.7102954387665, 'accumulated_eval_time': 31426.135251760483, 'accumulated_logging_time': 1.1053354740142822, 'global_step': 5583, 'preemption_count': 0}), (5698, {'train/loss': 0.12230296136095707, 'validation/loss': 0.12467129503642374, 'validation/num_examples': 83274637, 'test/loss': 0.1270730264054951, 'test/num_examples': 95000000, 'score': 5651.632122039795, 'total_duration': 37704.20431923866, 'accumulated_submission_time': 5651.632122039795, 'accumulated_eval_time': 32010.730974674225, 'accumulated_logging_time': 1.1238155364990234, 'global_step': 5698, 'preemption_count': 0}), (5821, {'train/loss': 0.12034476714199745, 'validation/loss': 0.12452320305980667, 'validation/num_examples': 83274637, 'test/loss': 0.12692175081670662, 'test/num_examples': 95000000, 'score': 5772.05837893486, 'total_duration': 38416.86041235924, 'accumulated_submission_time': 5772.05837893486, 'accumulated_eval_time': 32602.07928943634, 'accumulated_logging_time': 1.1638603210449219, 'global_step': 5821, 'preemption_count': 0}), (5939, {'train/loss': 0.12554640548342932, 'validation/loss': 0.12441661363324939, 'validation/num_examples': 83274637, 'test/loss': 0.12671987745015997, 'test/num_examples': 95000000, 'score': 5891.966375827789, 'total_duration': 39125.57147192955, 'accumulated_submission_time': 5891.966375827789, 'accumulated_eval_time': 33189.99530720711, 'accumulated_logging_time': 1.187654733657837, 'global_step': 5939, 'preemption_count': 0}), (6065, {'train/loss': 0.12189872058967031, 'validation/loss': 0.12438576817461314, 'validation/num_examples': 83274637, 'test/loss': 0.1267103862325568, 'test/num_examples': 95000000, 'score': 6011.780951023102, 'total_duration': 39826.5500292778, 'accumulated_submission_time': 6011.780951023102, 'accumulated_eval_time': 33770.28807473183, 'accumulated_logging_time': 1.2077231407165527, 'global_step': 6065, 'preemption_count': 0}), (6190, {'train/loss': 0.12181094933125133, 'validation/loss': 0.12432047389998883, 'validation/num_examples': 83274637, 'test/loss': 0.1265786778611434, 'test/num_examples': 95000000, 'score': 6132.735398054123, 'total_duration': 40533.29756641388, 'accumulated_submission_time': 6132.735398054123, 'accumulated_eval_time': 34355.238230228424, 'accumulated_logging_time': 1.22560453414917, 'global_step': 6190, 'preemption_count': 0}), (6312, {'train/loss': 0.12188161369938072, 'validation/loss': 0.1243671047714258, 'validation/num_examples': 83274637, 'test/loss': 0.12664027764511107, 'test/num_examples': 95000000, 'score': 6253.155354976654, 'total_duration': 41237.242715120316, 'accumulated_submission_time': 6253.155354976654, 'accumulated_eval_time': 34937.88372421265, 'accumulated_logging_time': 1.2463791370391846, 'global_step': 6312, 'preemption_count': 0}), (6438, {'train/loss': 0.12138663749134498, 'validation/loss': 0.12419374209859002, 'validation/num_examples': 83274637, 'test/loss': 0.12647966727387278, 'test/num_examples': 95000000, 'score': 6372.776814937592, 'total_duration': 41972.22842526436, 'accumulated_submission_time': 6372.776814937592, 'accumulated_eval_time': 35552.31383776665, 'accumulated_logging_time': 1.319519281387329, 'global_step': 6438, 'preemption_count': 0}), (6561, {'train/loss': 0.12388614861117084, 'validation/loss': 0.12429494085782183, 'validation/num_examples': 83274637, 'test/loss': 0.12658443023392527, 'test/num_examples': 95000000, 'score': 6492.889795780182, 'total_duration': 42716.45002245903, 'accumulated_submission_time': 6492.889795780182, 'accumulated_eval_time': 36175.510634183884, 'accumulated_logging_time': 1.3381297588348389, 'global_step': 6561, 'preemption_count': 0}), (6679, {'train/loss': 0.12248985227922056, 'validation/loss': 0.1242328017527342, 'validation/num_examples': 83274637, 'test/loss': 0.12640517909622193, 'test/num_examples': 95000000, 'score': 6613.025106430054, 'total_duration': 43453.327095746994, 'accumulated_submission_time': 6613.025106430054, 'accumulated_eval_time': 36791.397654533386, 'accumulated_logging_time': 1.3561313152313232, 'global_step': 6679, 'preemption_count': 0}), (6805, {'train/loss': 0.1214268349995406, 'validation/loss': 0.12413877724797974, 'validation/num_examples': 83274637, 'test/loss': 0.12642759158477784, 'test/num_examples': 95000000, 'score': 6733.60741519928, 'total_duration': 44194.50724029541, 'accumulated_submission_time': 6733.60741519928, 'accumulated_eval_time': 37411.13156104088, 'accumulated_logging_time': 1.3740413188934326, 'global_step': 6805, 'preemption_count': 0}), (6929, {'train/loss': 0.1232362847617924, 'validation/loss': 0.12402385417912581, 'validation/num_examples': 83274637, 'test/loss': 0.12628147105604473, 'test/num_examples': 95000000, 'score': 6854.153039455414, 'total_duration': 44918.87181663513, 'accumulated_submission_time': 6854.153039455414, 'accumulated_eval_time': 38014.03385639191, 'accumulated_logging_time': 1.4484620094299316, 'global_step': 6929, 'preemption_count': 0}), (7050, {'train/loss': 0.12292576135668816, 'validation/loss': 0.12411629573487525, 'validation/num_examples': 83274637, 'test/loss': 0.12639887875221653, 'test/num_examples': 95000000, 'score': 6973.70441865921, 'total_duration': 45655.927186727524, 'accumulated_submission_time': 6973.70441865921, 'accumulated_eval_time': 38630.64813756943, 'accumulated_logging_time': 1.4854764938354492, 'global_step': 7050, 'preemption_count': 0}), (7178, {'train/loss': 0.12228470035619227, 'validation/loss': 0.12390800129603464, 'validation/num_examples': 83274637, 'test/loss': 0.12617333150185034, 'test/num_examples': 95000000, 'score': 7093.348165273666, 'total_duration': 46395.31681418419, 'accumulated_submission_time': 7093.348165273666, 'accumulated_eval_time': 39249.50490236282, 'accumulated_logging_time': 1.5052170753479004, 'global_step': 7178, 'preemption_count': 0}), (7303, {'train/loss': 0.12345492185294825, 'validation/loss': 0.12398254758181432, 'validation/num_examples': 83274637, 'test/loss': 0.12618481661694175, 'test/num_examples': 95000000, 'score': 7213.02393579483, 'total_duration': 47134.19782781601, 'accumulated_submission_time': 7213.02393579483, 'accumulated_eval_time': 39867.8343808651, 'accumulated_logging_time': 1.5247740745544434, 'global_step': 7303, 'preemption_count': 0}), (7424, {'train/loss': 0.12247920945958629, 'validation/loss': 0.12387737668118345, 'validation/num_examples': 83274637, 'test/loss': 0.12614726133382698, 'test/num_examples': 95000000, 'score': 7332.562870502472, 'total_duration': 47858.33213329315, 'accumulated_submission_time': 7332.562870502472, 'accumulated_eval_time': 40471.56435704231, 'accumulated_logging_time': 1.5759985446929932, 'global_step': 7424, 'preemption_count': 0}), (7547, {'train/loss': 0.12366073518434839, 'validation/loss': 0.12389000564290518, 'validation/num_examples': 83274637, 'test/loss': 0.1262302362401862, 'test/num_examples': 95000000, 'score': 7452.361122369766, 'total_duration': 48586.62675046921, 'accumulated_submission_time': 7452.361122369766, 'accumulated_eval_time': 41079.138211250305, 'accumulated_logging_time': 1.5963597297668457, 'global_step': 7547, 'preemption_count': 0}), (7673, {'train/loss': 0.12266639767257896, 'validation/loss': 0.12396128231282985, 'validation/num_examples': 83274637, 'test/loss': 0.1263820700949418, 'test/num_examples': 95000000, 'score': 7571.937877416611, 'total_duration': 49289.377920627594, 'accumulated_submission_time': 7571.937877416611, 'accumulated_eval_time': 41661.42863559723, 'accumulated_logging_time': 1.6163709163665771, 'global_step': 7673, 'preemption_count': 0}), (7791, {'train/loss': 0.12233210956143149, 'validation/loss': 0.12374006380865757, 'validation/num_examples': 83274637, 'test/loss': 0.12606681039725856, 'test/num_examples': 95000000, 'score': 7691.582696914673, 'total_duration': 49993.27279305458, 'accumulated_submission_time': 7691.582696914673, 'accumulated_eval_time': 42244.75424671173, 'accumulated_logging_time': 1.652780532836914, 'global_step': 7791, 'preemption_count': 0})], 'global_step': 7916}
I0317 00:44:44.146576 139623498142912 submission_runner.py:649] Timing: 7812.630542516708
I0317 00:44:44.146617 139623498142912 submission_runner.py:651] Total number of evals: 65
I0317 00:44:44.146648 139623498142912 submission_runner.py:652] ====================
I0317 00:44:44.146768 139623498142912 submission_runner.py:750] Final criteo1tb score: 4
