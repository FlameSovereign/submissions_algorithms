torchrun --redirects 1:0,2:0,3:0,4:0,5:0,6:0,7:0 --standalone --nnodes=1 --nproc_per_node=8 submission_runner.py --framework=pytorch --workload=criteo1tb --submission_path=submissions_algorithms/leaderboard/external_tuning/shampoo_submission/submission.py --data_dir=/data/criteo1tb --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=submissions/rolling_leaderboard/external_tuning/shampoo/study_0 --overwrite=True --save_checkpoints=False --rng_seed=-73440200 --torch_compile=true --tuning_ruleset=external --tuning_search_space=submissions_algorithms/leaderboard/external_tuning/shampoo_submission/tuning_search_space.json --num_tuning_trials=5 --hparam_start_index=1 --hparam_end_index=2 2>&1 | tee -a /logs/criteo1tb_pytorch_03-16-2025-10-28-28.log
W0316 10:28:44.991000 9 site-packages/torch/distributed/run.py:793] 
W0316 10:28:44.991000 9 site-packages/torch/distributed/run.py:793] *****************************************
W0316 10:28:44.991000 9 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0316 10:28:44.991000 9 site-packages/torch/distributed/run.py:793] *****************************************
2025-03-16 10:28:55.573239: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-03-16 10:28:55.573238: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-03-16 10:28:55.573243: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-03-16 10:28:55.573239: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-03-16 10:28:55.573239: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-03-16 10:28:55.573238: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-03-16 10:28:55.573240: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-03-16 10:28:55.573239: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1742120935.952924      45 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1742120935.952959      48 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1742120935.952950      47 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1742120935.952944      49 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1742120935.952957      50 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1742120935.952972      46 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1742120935.952954      51 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1742120935.952942      44 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1742120936.137491      50 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
E0000 00:00:1742120936.137512      46 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
E0000 00:00:1742120936.137524      48 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
E0000 00:00:1742120936.137528      45 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
E0000 00:00:1742120936.137532      51 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
E0000 00:00:1742120936.137536      47 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
E0000 00:00:1742120936.137542      44 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
E0000 00:00:1742120936.137547      49 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
[rank0]:[W316 10:29:26.861136736 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank3]:[W316 10:29:26.863099471 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank1]:[W316 10:29:26.864615946 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank4]:[W316 10:29:26.864687203 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 4]  using GPU 4 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank5]:[W316 10:29:26.868151750 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 5]  using GPU 5 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank6]:[W316 10:29:26.870316467 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 6]  using GPU 6 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank2]:[W316 10:29:26.871286418 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank7]:[W316 10:29:26.872400773 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 7]  using GPU 7 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
I0316 10:29:29.352014 139621354939584 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_0/criteo1tb_pytorch.
I0316 10:29:29.352013 140701078574272 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_0/criteo1tb_pytorch.
I0316 10:29:29.352013 140574539490496 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_0/criteo1tb_pytorch.
I0316 10:29:29.352014 140649173091520 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_0/criteo1tb_pytorch.
I0316 10:29:29.352013 140638539949248 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_0/criteo1tb_pytorch.
I0316 10:29:29.352017 140702629090496 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_0/criteo1tb_pytorch.
I0316 10:29:29.352038 140552051844288 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_0/criteo1tb_pytorch.
I0316 10:29:29.352159 139927921173696 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_0/criteo1tb_pytorch.
I0316 10:29:38.100472 140702629090496 submission_runner.py:606] Using RNG seed -73440200
I0316 10:29:38.101313 139621354939584 logger_utils.py:97] Saving hparams to /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_0/criteo1tb_pytorch/trial_2/hparams.json.
I0316 10:29:38.101316 140574539490496 logger_utils.py:97] Saving hparams to /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_0/criteo1tb_pytorch/trial_2/hparams.json.
I0316 10:29:38.101320 140638539949248 logger_utils.py:97] Saving hparams to /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_0/criteo1tb_pytorch/trial_2/hparams.json.
I0316 10:29:38.101324 140649173091520 logger_utils.py:97] Saving hparams to /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_0/criteo1tb_pytorch/trial_2/hparams.json.
I0316 10:29:38.101332 140552051844288 logger_utils.py:97] Saving hparams to /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_0/criteo1tb_pytorch/trial_2/hparams.json.
I0316 10:29:38.101341 140701078574272 logger_utils.py:97] Saving hparams to /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_0/criteo1tb_pytorch/trial_2/hparams.json.
I0316 10:29:38.101741 140702629090496 submission_runner.py:615] --- Tuning run 2/5 ---
I0316 10:29:38.101869 140702629090496 submission_runner.py:620] Creating tuning directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_0/criteo1tb_pytorch/trial_2.
I0316 10:29:38.101680 139927921173696 logger_utils.py:97] Saving hparams to /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_0/criteo1tb_pytorch/trial_2/hparams.json.
I0316 10:29:38.102123 140702629090496 logger_utils.py:97] Saving hparams to /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_0/criteo1tb_pytorch/trial_2/hparams.json.
I0316 10:29:38.443592 140702629090496 submission_runner.py:218] Initializing dataset.
I0316 10:29:38.443791 140702629090496 submission_runner.py:229] Initializing model.
W0316 10:29:45.266154 139621354939584 submission_runner.py:254] These workloads cannot be fully compiled under current PyTorch version. Proceeding without `torch.compile`.
W0316 10:29:45.266159 140574539490496 submission_runner.py:254] These workloads cannot be fully compiled under current PyTorch version. Proceeding without `torch.compile`.
W0316 10:29:45.266159 140701078574272 submission_runner.py:254] These workloads cannot be fully compiled under current PyTorch version. Proceeding without `torch.compile`.
W0316 10:29:45.266164 140552051844288 submission_runner.py:254] These workloads cannot be fully compiled under current PyTorch version. Proceeding without `torch.compile`.
W0316 10:29:45.266179 140649173091520 submission_runner.py:254] These workloads cannot be fully compiled under current PyTorch version. Proceeding without `torch.compile`.
W0316 10:29:45.266218 140702629090496 submission_runner.py:254] These workloads cannot be fully compiled under current PyTorch version. Proceeding without `torch.compile`.
W0316 10:29:45.266218 139927921173696 submission_runner.py:254] These workloads cannot be fully compiled under current PyTorch version. Proceeding without `torch.compile`.
I0316 10:29:45.266366 140702629090496 submission_runner.py:272] Initializing optimizer.
W0316 10:29:45.266298 140638539949248 submission_runner.py:254] These workloads cannot be fully compiled under current PyTorch version. Proceeding without `torch.compile`.
W0316 10:29:45.320483 140649173091520 distributed_shampoo.py:341] start_preconditioning_step set to -1. Setting start_preconditioning_step equal to precondition frequency 100 by default.
W0316 10:29:45.320638 140649173091520 distributed_shampoo.py:359] Nesterov flag is enabled but momentum parameter is zero! Continuing without using momentum or Nesterov acceleration...
W0316 10:29:45.320559 140552051844288 distributed_shampoo.py:341] start_preconditioning_step set to -1. Setting start_preconditioning_step equal to precondition frequency 100 by default.
W0316 10:29:45.320605 140638539949248 distributed_shampoo.py:341] start_preconditioning_step set to -1. Setting start_preconditioning_step equal to precondition frequency 100 by default.
W0316 10:29:45.320608 139621354939584 distributed_shampoo.py:341] start_preconditioning_step set to -1. Setting start_preconditioning_step equal to precondition frequency 100 by default.
W0316 10:29:45.320611 140574539490496 distributed_shampoo.py:341] start_preconditioning_step set to -1. Setting start_preconditioning_step equal to precondition frequency 100 by default.
W0316 10:29:45.320695 140552051844288 distributed_shampoo.py:359] Nesterov flag is enabled but momentum parameter is zero! Continuing without using momentum or Nesterov acceleration...
W0316 10:29:45.320617 139927921173696 distributed_shampoo.py:341] start_preconditioning_step set to -1. Setting start_preconditioning_step equal to precondition frequency 100 by default.
W0316 10:29:45.320743 140638539949248 distributed_shampoo.py:359] Nesterov flag is enabled but momentum parameter is zero! Continuing without using momentum or Nesterov acceleration...
W0316 10:29:45.320746 139621354939584 distributed_shampoo.py:359] Nesterov flag is enabled but momentum parameter is zero! Continuing without using momentum or Nesterov acceleration...
W0316 10:29:45.320749 140574539490496 distributed_shampoo.py:359] Nesterov flag is enabled but momentum parameter is zero! Continuing without using momentum or Nesterov acceleration...
W0316 10:29:45.320697 140702629090496 distributed_shampoo.py:341] start_preconditioning_step set to -1. Setting start_preconditioning_step equal to precondition frequency 100 by default.
W0316 10:29:45.320784 139927921173696 distributed_shampoo.py:359] Nesterov flag is enabled but momentum parameter is zero! Continuing without using momentum or Nesterov acceleration...
W0316 10:29:45.320825 140702629090496 distributed_shampoo.py:359] Nesterov flag is enabled but momentum parameter is zero! Continuing without using momentum or Nesterov acceleration...
W0316 10:29:45.320732 140701078574272 distributed_shampoo.py:341] start_preconditioning_step set to -1. Setting start_preconditioning_step equal to precondition frequency 100 by default.
W0316 10:29:45.320879 140701078574272 distributed_shampoo.py:359] Nesterov flag is enabled but momentum parameter is zero! Continuing without using momentum or Nesterov acceleration...
I0316 10:29:45.364952 140701078574272 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 0.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 0 (torch.Size([512, 13])), Block block_0 (torch.Size([512, 13])).
I0316 10:29:45.364989 140702629090496 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 0.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 0 (torch.Size([512, 13])), Block block_0 (torch.Size([512, 13])).
I0316 10:29:45.364999 139621354939584 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 0.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 0 (torch.Size([512, 13])), Block block_0 (torch.Size([512, 13])).
I0316 10:29:45.365004 140649173091520 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 0.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 0 (torch.Size([512, 13])), Block block_0 (torch.Size([512, 13])).
I0316 10:29:45.365024 140638539949248 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 0.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 0 (torch.Size([512, 13])), Block block_0 (torch.Size([512, 13])).
I0316 10:29:45.365036 140552051844288 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 0.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 0 (torch.Size([512, 13])), Block block_0 (torch.Size([512, 13])).
I0316 10:29:45.365059 140574539490496 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 0.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 0 (torch.Size([512, 13])), Block block_0 (torch.Size([512, 13])).
I0316 10:29:45.365140 140702629090496 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 1.block_0 ([torch.Size([0])]) for Parameter 1 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 10:29:45.365140 140701078574272 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 1.block_0 ([torch.Size([0])]) for Parameter 1 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 10:29:45.365178 139621354939584 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 1.block_0 ([torch.Size([0])]) for Parameter 1 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 10:29:45.365180 140649173091520 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 1.block_0 ([torch.Size([0])]) for Parameter 1 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 10:29:45.365215 140552051844288 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 1.block_0 ([torch.Size([0])]) for Parameter 1 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 10:29:45.365262 140702629090496 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 2.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 2 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 10:29:45.365252 140574539490496 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 1.block_0 ([torch.Size([0])]) for Parameter 1 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 10:29:45.365277 140701078574272 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 2.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 2 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 10:29:45.365334 140649173091520 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 2.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 2 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 10:29:45.365350 139621354939584 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 2.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 2 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 10:29:45.365376 140702629090496 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 3.block_0 ([torch.Size([0])]) for Parameter 3 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 10:29:45.365397 140701078574272 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 3.block_0 ([torch.Size([0])]) for Parameter 3 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 10:29:45.365396 140574539490496 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 2.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 2 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 10:29:45.365452 140649173091520 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 3.block_0 ([torch.Size([0])]) for Parameter 3 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 10:29:45.365455 139621354939584 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 3.block_0 ([torch.Size([0])]) for Parameter 3 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 10:29:45.365498 140574539490496 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 3.block_0 ([torch.Size([0])]) for Parameter 3 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 10:29:45.365511 140702629090496 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 4.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 4 (torch.Size([128, 256])), Block block_0 (torch.Size([128, 256])).
I0316 10:29:45.365569 140649173091520 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 4.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 4 (torch.Size([128, 256])), Block block_0 (torch.Size([128, 256])).
I0316 10:29:45.365573 139621354939584 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 4.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 4 (torch.Size([128, 256])), Block block_0 (torch.Size([128, 256])).
I0316 10:29:45.365604 140702629090496 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 5.block_0 ([torch.Size([0])]) for Parameter 5 (torch.Size([128])), Block block_0 (torch.Size([128])).
I0316 10:29:45.365549 139927921173696 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 0.block_0 ([torch.Size([512, 512]), torch.Size([13, 13])]) for Parameter 0 (torch.Size([512, 13])), Block block_0 (torch.Size([512, 13])).
I0316 10:29:45.365628 140574539490496 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 4.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 4 (torch.Size([128, 256])), Block block_0 (torch.Size([128, 256])).
I0316 10:29:45.365624 140701078574272 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 4.block_0 ([torch.Size([128, 128]), torch.Size([256, 256])]) for Parameter 4 (torch.Size([128, 256])), Block block_0 (torch.Size([128, 256])).
I0316 10:29:45.365679 139621354939584 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 5.block_0 ([torch.Size([0])]) for Parameter 5 (torch.Size([128])), Block block_0 (torch.Size([128])).
I0316 10:29:45.365692 140649173091520 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 5.block_0 ([torch.Size([0])]) for Parameter 5 (torch.Size([128])), Block block_0 (torch.Size([128])).
I0316 10:29:45.365740 140702629090496 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 6.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 6 (torch.Size([1024, 506])), Block block_0 (torch.Size([1024, 506])).
I0316 10:29:45.365745 140574539490496 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 5.block_0 ([torch.Size([0])]) for Parameter 5 (torch.Size([128])), Block block_0 (torch.Size([128])).
I0316 10:29:45.365752 140701078574272 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 5.block_0 ([torch.Size([0])]) for Parameter 5 (torch.Size([128])), Block block_0 (torch.Size([128])).
I0316 10:29:45.365805 139621354939584 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 6.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 6 (torch.Size([1024, 506])), Block block_0 (torch.Size([1024, 506])).
I0316 10:29:45.365796 139927921173696 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 1.block_0 ([torch.Size([0])]) for Parameter 1 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 10:29:45.365849 140702629090496 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 7.block_0 ([torch.Size([0])]) for Parameter 7 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 10:29:45.365860 140574539490496 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 6.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 6 (torch.Size([1024, 506])), Block block_0 (torch.Size([1024, 506])).
I0316 10:29:45.365868 140701078574272 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 6.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 6 (torch.Size([1024, 506])), Block block_0 (torch.Size([1024, 506])).
I0316 10:29:45.365829 140638539949248 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 1.block_0 ([torch.Size([512, 512])]) for Parameter 1 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 10:29:45.365911 139621354939584 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 7.block_0 ([torch.Size([0])]) for Parameter 7 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 10:29:45.365958 140574539490496 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 7.block_0 ([torch.Size([0])]) for Parameter 7 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 10:29:45.365949 139927921173696 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 2.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 2 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 10:29:45.365965 140701078574272 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 7.block_0 ([torch.Size([0])]) for Parameter 7 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 10:29:45.365976 140552051844288 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 2.block_0 ([torch.Size([256, 256]), torch.Size([512, 512])]) for Parameter 2 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 10:29:45.366034 139621354939584 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 8.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 8 (torch.Size([1024, 1024])), Block block_0 (torch.Size([1024, 1024])).
I0316 10:29:45.366028 140638539949248 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 2.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 2 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 10:29:45.366074 140701078574272 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 8.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 8 (torch.Size([1024, 1024])), Block block_0 (torch.Size([1024, 1024])).
I0316 10:29:45.366070 139927921173696 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 3.block_0 ([torch.Size([0])]) for Parameter 3 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 10:29:45.366081 140574539490496 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 8.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 8 (torch.Size([1024, 1024])), Block block_0 (torch.Size([1024, 1024])).
I0316 10:29:45.366116 139621354939584 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 9.block_0 ([torch.Size([0])]) for Parameter 9 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 10:29:45.366152 140552051844288 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 3.block_0 ([torch.Size([0])]) for Parameter 3 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 10:29:45.366169 140574539490496 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 9.block_0 ([torch.Size([0])]) for Parameter 9 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 10:29:45.366171 140701078574272 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 9.block_0 ([torch.Size([0])]) for Parameter 9 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 10:29:45.366199 140638539949248 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 3.block_0 ([torch.Size([256, 256])]) for Parameter 3 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 10:29:45.366221 139927921173696 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 4.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 4 (torch.Size([128, 256])), Block block_0 (torch.Size([128, 256])).
I0316 10:29:45.366252 139621354939584 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 10.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 10 (torch.Size([512, 1024])), Block block_0 (torch.Size([512, 1024])).
I0316 10:29:45.366289 140701078574272 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 10.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 10 (torch.Size([512, 1024])), Block block_0 (torch.Size([512, 1024])).
I0316 10:29:45.366303 140552051844288 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 4.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 4 (torch.Size([128, 256])), Block block_0 (torch.Size([128, 256])).
I0316 10:29:45.366318 139927921173696 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 5.block_0 ([torch.Size([0])]) for Parameter 5 (torch.Size([128])), Block block_0 (torch.Size([128])).
I0316 10:29:45.366347 139621354939584 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 11.block_0 ([torch.Size([0])]) for Parameter 11 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 10:29:45.366386 140701078574272 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 11.block_0 ([torch.Size([0])]) for Parameter 11 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 10:29:45.366369 140649173091520 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 6.block_0 ([torch.Size([1024, 1024]), torch.Size([506, 506])]) for Parameter 6 (torch.Size([1024, 506])), Block block_0 (torch.Size([1024, 506])).
I0316 10:29:45.366386 140638539949248 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 4.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 4 (torch.Size([128, 256])), Block block_0 (torch.Size([128, 256])).
I0316 10:29:45.366405 140552051844288 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 5.block_0 ([torch.Size([0])]) for Parameter 5 (torch.Size([128])), Block block_0 (torch.Size([128])).
I0316 10:29:45.366435 139927921173696 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 6.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 6 (torch.Size([1024, 506])), Block block_0 (torch.Size([1024, 506])).
I0316 10:29:45.366501 140649173091520 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 7.block_0 ([torch.Size([0])]) for Parameter 7 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 10:29:45.366524 140552051844288 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 6.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 6 (torch.Size([1024, 506])), Block block_0 (torch.Size([1024, 506])).
I0316 10:29:45.366538 140701078574272 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 12.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 12 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 10:29:45.366538 140638539949248 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 5.block_0 ([torch.Size([128, 128])]) for Parameter 5 (torch.Size([128])), Block block_0 (torch.Size([128])).
I0316 10:29:45.366549 139927921173696 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 7.block_0 ([torch.Size([0])]) for Parameter 7 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 10:29:45.366579 140702629090496 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 8.block_0 ([torch.Size([1024, 1024]), torch.Size([1024, 1024])]) for Parameter 8 (torch.Size([1024, 1024])), Block block_0 (torch.Size([1024, 1024])).
I0316 10:29:45.366617 140552051844288 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 7.block_0 ([torch.Size([0])]) for Parameter 7 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 10:29:45.366636 140701078574272 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 13.block_0 ([torch.Size([0])]) for Parameter 13 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 10:29:45.366653 140649173091520 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 8.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 8 (torch.Size([1024, 1024])), Block block_0 (torch.Size([1024, 1024])).
I0316 10:29:45.366665 140638539949248 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 6.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 6 (torch.Size([1024, 506])), Block block_0 (torch.Size([1024, 506])).
I0316 10:29:45.366671 139927921173696 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 8.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 8 (torch.Size([1024, 1024])), Block block_0 (torch.Size([1024, 1024])).
I0316 10:29:45.366715 140702629090496 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 9.block_0 ([torch.Size([0])]) for Parameter 9 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 10:29:45.366727 140552051844288 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 8.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 8 (torch.Size([1024, 1024])), Block block_0 (torch.Size([1024, 1024])).
I0316 10:29:45.366727 140701078574272 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 14.block_0 ([torch.Size([0])]) for Parameter 14 (torch.Size([1, 256])), Block block_0 (torch.Size([256])).
I0316 10:29:45.366755 140649173091520 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 9.block_0 ([torch.Size([0])]) for Parameter 9 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 10:29:45.366758 139927921173696 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 9.block_0 ([torch.Size([0])]) for Parameter 9 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 10:29:45.366810 140701078574272 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 15.block_0 ([torch.Size([0])]) for Parameter 15 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0316 10:29:45.366811 140552051844288 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 9.block_0 ([torch.Size([0])]) for Parameter 9 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 10:29:45.366814 140638539949248 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 7.block_0 ([torch.Size([1024, 1024])]) for Parameter 7 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 10:29:45.366847 140702629090496 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 10.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 10 (torch.Size([512, 1024])), Block block_0 (torch.Size([512, 1024])).
I0316 10:29:45.366871 139927921173696 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 10.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 10 (torch.Size([512, 1024])), Block block_0 (torch.Size([512, 1024])).
I0316 10:29:45.366877 140649173091520 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 10.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 10 (torch.Size([512, 1024])), Block block_0 (torch.Size([512, 1024])).
I0316 10:29:45.366906 140701078574272 shampoo_preconditioner_list.py:612] Rank 5: ShampooPreconditionerList Numel Breakdown: (524626, 524288, 655360, 131072, 163840, 32768, 2609224, 2097152, 4194304, 2097152, 2621440, 524288, 655360, 131072, 131072, 2)
I0316 10:29:45.366924 140552051844288 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 10.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 10 (torch.Size([512, 1024])), Block block_0 (torch.Size([512, 1024])).
I0316 10:29:45.366943 140702629090496 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 11.block_0 ([torch.Size([0])]) for Parameter 11 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 10:29:45.366949 140701078574272 shampoo_preconditioner_list.py:615] Rank 5: ShampooPreconditionerList Bytes Breakdown: (2098504,)
I0316 10:29:45.366948 140638539949248 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 8.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 8 (torch.Size([1024, 1024])), Block block_0 (torch.Size([1024, 1024])).
I0316 10:29:45.366964 139927921173696 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 11.block_0 ([torch.Size([0])]) for Parameter 11 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 10:29:45.366963 140649173091520 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 11.block_0 ([torch.Size([0])]) for Parameter 11 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 10:29:45.366981 140701078574272 shampoo_preconditioner_list.py:618] Rank 5: ShampooPreconditionerList Total Elements: 17093020
I0316 10:29:45.367009 140701078574272 shampoo_preconditioner_list.py:621] Rank 5: ShampooPreconditionerList Total Bytes: 2098504
I0316 10:29:45.366975 140574539490496 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 10.block_0 ([torch.Size([512, 512]), torch.Size([1024, 1024])]) for Parameter 10 (torch.Size([512, 1024])), Block block_0 (torch.Size([512, 1024])).
I0316 10:29:45.367020 140552051844288 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 11.block_0 ([torch.Size([0])]) for Parameter 11 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 10:29:45.367033 139621354939584 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 12.block_0 ([torch.Size([256, 256]), torch.Size([512, 512])]) for Parameter 12 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 10:29:45.367060 140702629090496 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 12.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 12 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 10:29:45.367074 140649173091520 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 12.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 12 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 10:29:45.367087 139927921173696 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 12.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 12 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 10:29:45.367100 140574539490496 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 11.block_0 ([torch.Size([0])]) for Parameter 11 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 10:29:45.367141 140552051844288 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 12.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 12 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 10:29:45.367144 140702629090496 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 13.block_0 ([torch.Size([0])]) for Parameter 13 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 10:29:45.367142 140701078574272 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([512, 13])), Block block_0 (torch.Size([512, 13])).
I0316 10:29:45.367154 140649173091520 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 13.block_0 ([torch.Size([0])]) for Parameter 13 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 10:29:45.367166 139621354939584 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 13.block_0 ([torch.Size([0])]) for Parameter 13 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 10:29:45.367190 139927921173696 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 13.block_0 ([torch.Size([0])]) for Parameter 13 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 10:29:45.367199 140701078574272 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 10:29:45.367221 140702629090496 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 14.block_0 ([torch.Size([0])]) for Parameter 14 (torch.Size([1, 256])), Block block_0 (torch.Size([256])).
I0316 10:29:45.367228 140552051844288 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 13.block_0 ([torch.Size([0])]) for Parameter 13 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 10:29:45.367222 140574539490496 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 12.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 12 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 10:29:45.367244 140649173091520 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 14.block_0 ([torch.Size([0])]) for Parameter 14 (torch.Size([1, 256])), Block block_0 (torch.Size([256])).
I0316 10:29:45.367249 140701078574272 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 10:29:45.367278 139927921173696 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 14.block_0 ([torch.Size([0])]) for Parameter 14 (torch.Size([1, 256])), Block block_0 (torch.Size([256])).
I0316 10:29:45.367274 139621354939584 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 14.block_0 ([torch.Size([0])]) for Parameter 14 (torch.Size([1, 256])), Block block_0 (torch.Size([256])).
I0316 10:29:45.367327 140701078574272 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 10:29:45.367341 140552051844288 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 14.block_0 ([torch.Size([0])]) for Parameter 14 (torch.Size([1, 256])), Block block_0 (torch.Size([256])).
I0316 10:29:45.367343 140574539490496 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 13.block_0 ([torch.Size([0])]) for Parameter 13 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 10:29:45.367345 140702629090496 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 15.block_0 ([torch.Size([0])]) for Parameter 15 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0316 10:29:45.367357 140649173091520 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 15.block_0 ([torch.Size([0])]) for Parameter 15 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0316 10:29:45.367374 139927921173696 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 15.block_0 ([torch.Size([0])]) for Parameter 15 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0316 10:29:45.367388 139621354939584 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 15.block_0 ([torch.Size([0])]) for Parameter 15 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0316 10:29:45.367429 140574539490496 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 14.block_0 ([torch.Size([0])]) for Parameter 14 (torch.Size([1, 256])), Block block_0 (torch.Size([256])).
I0316 10:29:45.367430 140552051844288 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 15.block_0 ([torch.Size([0])]) for Parameter 15 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0316 10:29:45.367441 140702629090496 shampoo_preconditioner_list.py:612] Rank 0: ShampooPreconditionerList Numel Breakdown: (524626, 524288, 655360, 131072, 163840, 32768, 2609224, 2097152, 4194304, 2097152, 2621440, 524288, 655360, 131072, 131072, 2)
I0316 10:29:45.367440 140701078574272 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 4.block_0 (torch.Size([128, 256])) for Parameter 4 (torch.Size([128, 256])), Block block_0 (torch.Size([128, 256])).
I0316 10:29:45.367466 139927921173696 shampoo_preconditioner_list.py:612] Rank 6: ShampooPreconditionerList Numel Breakdown: (524626, 524288, 655360, 131072, 163840, 32768, 2609224, 2097152, 4194304, 2097152, 2621440, 524288, 655360, 131072, 131072, 2)
I0316 10:29:45.367469 140649173091520 shampoo_preconditioner_list.py:612] Rank 2: ShampooPreconditionerList Numel Breakdown: (524626, 524288, 655360, 131072, 163840, 32768, 2609224, 2097152, 4194304, 2097152, 2621440, 524288, 655360, 131072, 131072, 2)
I0316 10:29:45.367479 140702629090496 shampoo_preconditioner_list.py:615] Rank 0: ShampooPreconditionerList Bytes Breakdown: (2098504,)
I0316 10:29:45.367463 140638539949248 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 9.block_0 ([torch.Size([1024, 1024])]) for Parameter 9 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 10:29:45.367496 139621354939584 shampoo_preconditioner_list.py:612] Rank 4: ShampooPreconditionerList Numel Breakdown: (524626, 524288, 655360, 131072, 163840, 32768, 2609224, 2097152, 4194304, 2097152, 2621440, 524288, 655360, 131072, 131072, 2)
I0316 10:29:45.367510 139927921173696 shampoo_preconditioner_list.py:615] Rank 6: ShampooPreconditionerList Bytes Breakdown: (2098504,)
I0316 10:29:45.367511 140702629090496 shampoo_preconditioner_list.py:618] Rank 0: ShampooPreconditionerList Total Elements: 17093020
I0316 10:29:45.367509 140701078574272 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 5.block_0 (torch.Size([0])) for Parameter 5 (torch.Size([128])), Block block_0 (torch.Size([128])).
I0316 10:29:45.367509 140649173091520 shampoo_preconditioner_list.py:615] Rank 2: ShampooPreconditionerList Bytes Breakdown: (2098504,)
I0316 10:29:45.367524 140552051844288 shampoo_preconditioner_list.py:612] Rank 3: ShampooPreconditionerList Numel Breakdown: (524626, 524288, 655360, 131072, 163840, 32768, 2609224, 2097152, 4194304, 2097152, 2621440, 524288, 655360, 131072, 131072, 2)
I0316 10:29:45.367540 139927921173696 shampoo_preconditioner_list.py:618] Rank 6: ShampooPreconditionerList Total Elements: 17093020
I0316 10:29:45.367535 139621354939584 shampoo_preconditioner_list.py:615] Rank 4: ShampooPreconditionerList Bytes Breakdown: (2098504,)
I0316 10:29:45.367535 140574539490496 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 15.block_0 ([torch.Size([0])]) for Parameter 15 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0316 10:29:45.367542 140702629090496 shampoo_preconditioner_list.py:621] Rank 0: ShampooPreconditionerList Total Bytes: 2098504
I0316 10:29:45.367545 140649173091520 shampoo_preconditioner_list.py:618] Rank 2: ShampooPreconditionerList Total Elements: 17093020
I0316 10:29:45.367567 139927921173696 shampoo_preconditioner_list.py:621] Rank 6: ShampooPreconditionerList Total Bytes: 2098504
I0316 10:29:45.367563 140701078574272 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 6.block_0 (torch.Size([0])) for Parameter 6 (torch.Size([1024, 506])), Block block_0 (torch.Size([1024, 506])).
I0316 10:29:45.367569 140552051844288 shampoo_preconditioner_list.py:615] Rank 3: ShampooPreconditionerList Bytes Breakdown: (2098504,)
I0316 10:29:45.367571 139621354939584 shampoo_preconditioner_list.py:618] Rank 4: ShampooPreconditionerList Total Elements: 17093020
I0316 10:29:45.367576 140649173091520 shampoo_preconditioner_list.py:621] Rank 2: ShampooPreconditionerList Total Bytes: 2098504
I0316 10:29:45.367601 139621354939584 shampoo_preconditioner_list.py:621] Rank 4: ShampooPreconditionerList Total Bytes: 2098504
I0316 10:29:45.367602 140552051844288 shampoo_preconditioner_list.py:618] Rank 3: ShampooPreconditionerList Total Elements: 17093020
I0316 10:29:45.367621 140701078574272 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 7.block_0 (torch.Size([0])) for Parameter 7 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 10:29:45.367632 140552051844288 shampoo_preconditioner_list.py:621] Rank 3: ShampooPreconditionerList Total Bytes: 2098504
I0316 10:29:45.367635 140574539490496 shampoo_preconditioner_list.py:612] Rank 1: ShampooPreconditionerList Numel Breakdown: (524626, 524288, 655360, 131072, 163840, 32768, 2609224, 2097152, 4194304, 2097152, 2621440, 524288, 655360, 131072, 131072, 2)
I0316 10:29:45.367631 140638539949248 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 10.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 10 (torch.Size([512, 1024])), Block block_0 (torch.Size([512, 1024])).
I0316 10:29:45.367674 140574539490496 shampoo_preconditioner_list.py:615] Rank 1: ShampooPreconditionerList Bytes Breakdown: (2098504,)
I0316 10:29:45.367684 140701078574272 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 8.block_0 (torch.Size([0])) for Parameter 8 (torch.Size([1024, 1024])), Block block_0 (torch.Size([1024, 1024])).
I0316 10:29:45.367681 140702629090496 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([512, 13])), Block block_0 (torch.Size([512, 13])).
I0316 10:29:45.367714 140574539490496 shampoo_preconditioner_list.py:618] Rank 1: ShampooPreconditionerList Total Elements: 17093020
I0316 10:29:45.367714 140649173091520 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([512, 13])), Block block_0 (torch.Size([512, 13])).
I0316 10:29:45.367734 140701078574272 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 9.block_0 (torch.Size([0])) for Parameter 9 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 10:29:45.367741 140702629090496 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 10:29:45.367750 140574539490496 shampoo_preconditioner_list.py:621] Rank 1: ShampooPreconditionerList Total Bytes: 2098504
I0316 10:29:45.367749 139621354939584 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([512, 13])), Block block_0 (torch.Size([512, 13])).
I0316 10:29:45.367767 140552051844288 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([512, 13])), Block block_0 (torch.Size([512, 13])).
I0316 10:29:45.367775 140649173091520 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 10:29:45.367760 139927921173696 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([512, 13])) for Parameter 0 (torch.Size([512, 13])), Block block_0 (torch.Size([512, 13])).
I0316 10:29:45.367784 140701078574272 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 10.block_0 (torch.Size([0])) for Parameter 10 (torch.Size([512, 1024])), Block block_0 (torch.Size([512, 1024])).
I0316 10:29:45.367808 140702629090496 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 10:29:45.367812 139621354939584 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 10:29:45.367848 140649173091520 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 10:29:45.367855 140701078574272 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 11.block_0 (torch.Size([0])) for Parameter 11 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 10:29:45.367854 140552051844288 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 10:29:45.367877 139927921173696 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 10:29:45.367889 139621354939584 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 10:29:45.367893 140702629090496 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 10:29:45.367904 140649173091520 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 10:29:45.367907 140701078574272 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 12.block_0 (torch.Size([0])) for Parameter 12 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 10:29:45.367918 140574539490496 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([512, 13])), Block block_0 (torch.Size([512, 13])).
I0316 10:29:45.367942 139927921173696 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 10:29:45.367942 139621354939584 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 10:29:45.367952 140702629090496 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 4.block_0 (torch.Size([0])) for Parameter 4 (torch.Size([128, 256])), Block block_0 (torch.Size([128, 256])).
I0316 10:29:45.367955 140701078574272 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 13.block_0 (torch.Size([0])) for Parameter 13 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 10:29:45.367954 140649173091520 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 4.block_0 (torch.Size([0])) for Parameter 4 (torch.Size([128, 256])), Block block_0 (torch.Size([128, 256])).
I0316 10:29:45.367994 139621354939584 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 4.block_0 (torch.Size([0])) for Parameter 4 (torch.Size([128, 256])), Block block_0 (torch.Size([128, 256])).
I0316 10:29:45.367993 140574539490496 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 10:29:45.368003 140701078574272 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 14.block_0 (torch.Size([0])) for Parameter 14 (torch.Size([1, 256])), Block block_0 (torch.Size([256])).
I0316 10:29:45.368005 140702629090496 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 5.block_0 (torch.Size([0])) for Parameter 5 (torch.Size([128])), Block block_0 (torch.Size([128])).
I0316 10:29:45.368005 140649173091520 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 5.block_0 (torch.Size([0])) for Parameter 5 (torch.Size([128])), Block block_0 (torch.Size([128])).
I0316 10:29:45.368007 139927921173696 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 10:29:45.368048 140574539490496 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 10:29:45.368051 140701078574272 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 15.block_0 (torch.Size([0])) for Parameter 15 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0316 10:29:45.368053 139621354939584 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 5.block_0 (torch.Size([0])) for Parameter 5 (torch.Size([128])), Block block_0 (torch.Size([128])).
I0316 10:29:45.368056 139927921173696 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 4.block_0 (torch.Size([0])) for Parameter 4 (torch.Size([128, 256])), Block block_0 (torch.Size([128, 256])).
I0316 10:29:45.368057 140702629090496 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 6.block_0 (torch.Size([0])) for Parameter 6 (torch.Size([1024, 506])), Block block_0 (torch.Size([1024, 506])).
I0316 10:29:45.368105 139621354939584 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 6.block_0 (torch.Size([0])) for Parameter 6 (torch.Size([1024, 506])), Block block_0 (torch.Size([1024, 506])).
I0316 10:29:45.368108 140574539490496 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 10:29:45.368104 139927921173696 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 5.block_0 (torch.Size([0])) for Parameter 5 (torch.Size([128])), Block block_0 (torch.Size([128])).
I0316 10:29:45.368112 140701078574272 shampoo_preconditioner_list.py:375] Rank 5: AdaGradPreconditionerList Numel Breakdown: (0, 0, 0, 0, 32768, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)
I0316 10:29:45.368114 140702629090496 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 7.block_0 (torch.Size([0])) for Parameter 7 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 10:29:45.368108 140649173091520 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 6.block_0 (torch.Size([1024, 506])) for Parameter 6 (torch.Size([1024, 506])), Block block_0 (torch.Size([1024, 506])).
I0316 10:29:45.368146 140701078574272 shampoo_preconditioner_list.py:378] Rank 5: AdaGradPreconditionerList Bytes Breakdown: (0, 0, 0, 0, 131072, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)
I0316 10:29:45.368161 140574539490496 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 4.block_0 (torch.Size([0])) for Parameter 4 (torch.Size([128, 256])), Block block_0 (torch.Size([128, 256])).
I0316 10:29:45.368164 139927921173696 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 6.block_0 (torch.Size([0])) for Parameter 6 (torch.Size([1024, 506])), Block block_0 (torch.Size([1024, 506])).
I0316 10:29:45.368171 139621354939584 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 7.block_0 (torch.Size([0])) for Parameter 7 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 10:29:45.368181 140649173091520 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 7.block_0 (torch.Size([0])) for Parameter 7 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 10:29:45.368157 140638539949248 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 11.block_0 ([torch.Size([512, 512])]) for Parameter 11 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 10:29:45.368184 140701078574272 shampoo_preconditioner_list.py:381] Rank 5: AdaGradPreconditionerList Total Elements: 32768
I0316 10:29:45.368223 140701078574272 shampoo_preconditioner_list.py:384] Rank 5: AdaGradPreconditionerList Total Bytes: 131072
I0316 10:29:45.368221 139927921173696 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 7.block_0 (torch.Size([0])) for Parameter 7 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 10:29:45.368214 140702629090496 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 8.block_0 (torch.Size([1024, 1024])) for Parameter 8 (torch.Size([1024, 1024])), Block block_0 (torch.Size([1024, 1024])).
I0316 10:29:45.368211 140574539490496 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 5.block_0 (torch.Size([0])) for Parameter 5 (torch.Size([128])), Block block_0 (torch.Size([128])).
I0316 10:29:45.368233 139621354939584 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 8.block_0 (torch.Size([0])) for Parameter 8 (torch.Size([1024, 1024])), Block block_0 (torch.Size([1024, 1024])).
I0316 10:29:45.368257 140649173091520 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 8.block_0 (torch.Size([0])) for Parameter 8 (torch.Size([1024, 1024])), Block block_0 (torch.Size([1024, 1024])).
I0316 10:29:45.368272 139927921173696 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 8.block_0 (torch.Size([0])) for Parameter 8 (torch.Size([1024, 1024])), Block block_0 (torch.Size([1024, 1024])).
I0316 10:29:45.368278 140574539490496 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 6.block_0 (torch.Size([0])) for Parameter 6 (torch.Size([1024, 506])), Block block_0 (torch.Size([1024, 506])).
I0316 10:29:45.368281 140702629090496 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 9.block_0 (torch.Size([0])) for Parameter 9 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 10:29:45.368287 139621354939584 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 9.block_0 (torch.Size([0])) for Parameter 9 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 10:29:45.368317 140649173091520 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 9.block_0 (torch.Size([0])) for Parameter 9 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 10:29:45.368320 139927921173696 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 9.block_0 (torch.Size([0])) for Parameter 9 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 10:29:45.368323 140638539949248 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 12.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 12 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 10:29:45.368337 139621354939584 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 10.block_0 (torch.Size([0])) for Parameter 10 (torch.Size([512, 1024])), Block block_0 (torch.Size([512, 1024])).
I0316 10:29:45.368337 140574539490496 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 7.block_0 (torch.Size([0])) for Parameter 7 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 10:29:45.368343 140702629090496 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 10.block_0 (torch.Size([0])) for Parameter 10 (torch.Size([512, 1024])), Block block_0 (torch.Size([512, 1024])).
I0316 10:29:45.368340 140552051844288 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([256, 512])) for Parameter 2 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 10:29:45.368367 139927921173696 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 10.block_0 (torch.Size([0])) for Parameter 10 (torch.Size([512, 1024])), Block block_0 (torch.Size([512, 1024])).
I0316 10:29:45.368376 140649173091520 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 10.block_0 (torch.Size([0])) for Parameter 10 (torch.Size([512, 1024])), Block block_0 (torch.Size([512, 1024])).
I0316 10:29:45.368387 139621354939584 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 11.block_0 (torch.Size([0])) for Parameter 11 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 10:29:45.368388 140574539490496 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 8.block_0 (torch.Size([0])) for Parameter 8 (torch.Size([1024, 1024])), Block block_0 (torch.Size([1024, 1024])).
I0316 10:29:45.368395 140702629090496 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 11.block_0 (torch.Size([0])) for Parameter 11 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 10:29:45.368414 139927921173696 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 11.block_0 (torch.Size([0])) for Parameter 11 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 10:29:45.368426 140649173091520 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 11.block_0 (torch.Size([0])) for Parameter 11 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 10:29:45.368438 140574539490496 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 9.block_0 (torch.Size([0])) for Parameter 9 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 10:29:45.368444 140702629090496 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 12.block_0 (torch.Size([0])) for Parameter 12 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 10:29:45.368451 140552051844288 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 10:29:45.368470 139927921173696 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 12.block_0 (torch.Size([0])) for Parameter 12 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 10:29:45.368476 140649173091520 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 12.block_0 (torch.Size([0])) for Parameter 12 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 10:29:45.368491 140702629090496 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 13.block_0 (torch.Size([0])) for Parameter 13 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 10:29:45.368509 140552051844288 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 4.block_0 (torch.Size([0])) for Parameter 4 (torch.Size([128, 256])), Block block_0 (torch.Size([128, 256])).
I0316 10:29:45.368524 139927921173696 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 13.block_0 (torch.Size([0])) for Parameter 13 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 10:29:45.368524 140649173091520 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 13.block_0 (torch.Size([0])) for Parameter 13 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 10:29:45.368540 140702629090496 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 14.block_0 (torch.Size([0])) for Parameter 14 (torch.Size([1, 256])), Block block_0 (torch.Size([256])).
I0316 10:29:45.368538 140574539490496 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 10.block_0 (torch.Size([512, 1024])) for Parameter 10 (torch.Size([512, 1024])), Block block_0 (torch.Size([512, 1024])).
I0316 10:29:45.368569 140552051844288 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 5.block_0 (torch.Size([0])) for Parameter 5 (torch.Size([128])), Block block_0 (torch.Size([128])).
I0316 10:29:45.368572 139927921173696 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 14.block_0 (torch.Size([0])) for Parameter 14 (torch.Size([1, 256])), Block block_0 (torch.Size([256])).
I0316 10:29:45.368574 140649173091520 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 14.block_0 (torch.Size([0])) for Parameter 14 (torch.Size([1, 256])), Block block_0 (torch.Size([256])).
I0316 10:29:45.368599 140702629090496 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 15.block_0 (torch.Size([0])) for Parameter 15 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0316 10:29:45.368604 140574539490496 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 11.block_0 (torch.Size([0])) for Parameter 11 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 10:29:45.368618 139927921173696 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 15.block_0 (torch.Size([0])) for Parameter 15 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0316 10:29:45.368622 140649173091520 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 15.block_0 (torch.Size([0])) for Parameter 15 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0316 10:29:45.368629 140552051844288 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 6.block_0 (torch.Size([0])) for Parameter 6 (torch.Size([1024, 506])), Block block_0 (torch.Size([1024, 506])).
I0316 10:29:45.368666 140574539490496 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 12.block_0 (torch.Size([0])) for Parameter 12 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 10:29:45.368678 140702629090496 shampoo_preconditioner_list.py:375] Rank 0: AdaGradPreconditionerList Numel Breakdown: (0, 0, 0, 0, 0, 0, 0, 0, 1048576, 0, 0, 0, 0, 0, 0, 0)
I0316 10:29:45.368685 139927921173696 shampoo_preconditioner_list.py:375] Rank 6: AdaGradPreconditionerList Numel Breakdown: (6656, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)
I0316 10:29:45.368693 140649173091520 shampoo_preconditioner_list.py:375] Rank 2: AdaGradPreconditionerList Numel Breakdown: (0, 0, 0, 0, 0, 0, 518144, 0, 0, 0, 0, 0, 0, 0, 0, 0)
I0316 10:29:45.368697 140552051844288 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 7.block_0 (torch.Size([0])) for Parameter 7 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 10:29:45.368710 140702629090496 shampoo_preconditioner_list.py:378] Rank 0: AdaGradPreconditionerList Bytes Breakdown: (0, 0, 0, 0, 0, 0, 0, 0, 4194304, 0, 0, 0, 0, 0, 0, 0)
I0316 10:29:45.368718 139927921173696 shampoo_preconditioner_list.py:378] Rank 6: AdaGradPreconditionerList Bytes Breakdown: (26624, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)
I0316 10:29:45.368725 140649173091520 shampoo_preconditioner_list.py:378] Rank 2: AdaGradPreconditionerList Bytes Breakdown: (0, 0, 0, 0, 0, 0, 2072576, 0, 0, 0, 0, 0, 0, 0, 0, 0)
I0316 10:29:45.368723 140574539490496 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 13.block_0 (torch.Size([0])) for Parameter 13 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 10:29:45.368743 140702629090496 shampoo_preconditioner_list.py:381] Rank 0: AdaGradPreconditionerList Total Elements: 1048576
I0316 10:29:45.368746 140552051844288 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 8.block_0 (torch.Size([0])) for Parameter 8 (torch.Size([1024, 1024])), Block block_0 (torch.Size([1024, 1024])).
I0316 10:29:45.368752 140649173091520 shampoo_preconditioner_list.py:381] Rank 2: AdaGradPreconditionerList Total Elements: 518144
I0316 10:29:45.368746 139927921173696 shampoo_preconditioner_list.py:381] Rank 6: AdaGradPreconditionerList Total Elements: 6656
I0316 10:29:45.368769 140574539490496 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 14.block_0 (torch.Size([0])) for Parameter 14 (torch.Size([1, 256])), Block block_0 (torch.Size([256])).
I0316 10:29:45.368775 140702629090496 shampoo_preconditioner_list.py:384] Rank 0: AdaGradPreconditionerList Total Bytes: 4194304
I0316 10:29:45.368779 140649173091520 shampoo_preconditioner_list.py:384] Rank 2: AdaGradPreconditionerList Total Bytes: 2072576
I0316 10:29:45.368783 139927921173696 shampoo_preconditioner_list.py:384] Rank 6: AdaGradPreconditionerList Total Bytes: 26624
I0316 10:29:45.368793 140552051844288 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 9.block_0 (torch.Size([0])) for Parameter 9 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 10:29:45.368822 140574539490496 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 15.block_0 (torch.Size([0])) for Parameter 15 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0316 10:29:45.368846 140552051844288 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 10.block_0 (torch.Size([0])) for Parameter 10 (torch.Size([512, 1024])), Block block_0 (torch.Size([512, 1024])).
I0316 10:29:45.368884 140574539490496 shampoo_preconditioner_list.py:375] Rank 1: AdaGradPreconditionerList Numel Breakdown: (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 524288, 0, 0, 0, 0, 0)
I0316 10:29:45.368898 140552051844288 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 11.block_0 (torch.Size([0])) for Parameter 11 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 10:29:45.368921 140574539490496 shampoo_preconditioner_list.py:378] Rank 1: AdaGradPreconditionerList Bytes Breakdown: (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2097152, 0, 0, 0, 0, 0)
I0316 10:29:45.368945 140552051844288 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 12.block_0 (torch.Size([0])) for Parameter 12 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 10:29:45.368954 140574539490496 shampoo_preconditioner_list.py:381] Rank 1: AdaGradPreconditionerList Total Elements: 524288
I0316 10:29:45.368982 140574539490496 shampoo_preconditioner_list.py:384] Rank 1: AdaGradPreconditionerList Total Bytes: 2097152
I0316 10:29:45.368995 140552051844288 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 13.block_0 (torch.Size([0])) for Parameter 13 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 10:29:45.369041 140552051844288 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 14.block_0 (torch.Size([0])) for Parameter 14 (torch.Size([1, 256])), Block block_0 (torch.Size([256])).
I0316 10:29:45.369093 140552051844288 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 15.block_0 (torch.Size([0])) for Parameter 15 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0316 10:29:45.369077 140638539949248 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 13.block_0 ([torch.Size([256, 256])]) for Parameter 13 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 10:29:45.369111 140702629090496 submission.py:105] Large parameters (embedding tables) detected (dim >= 1_000_000). Instantiating Row-Wise AdaGrad...
I0316 10:29:45.369156 140552051844288 shampoo_preconditioner_list.py:375] Rank 3: AdaGradPreconditionerList Numel Breakdown: (0, 0, 131072, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)
W0316 10:29:45.369146 140649173091520 distributed_shampoo.py:359] Nesterov flag is enabled but momentum parameter is zero! Continuing without using momentum or Nesterov acceleration...
I0316 10:29:45.369193 140552051844288 shampoo_preconditioner_list.py:378] Rank 3: AdaGradPreconditionerList Bytes Breakdown: (0, 0, 524288, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)
W0316 10:29:45.369174 139927921173696 distributed_shampoo.py:359] Nesterov flag is enabled but momentum parameter is zero! Continuing without using momentum or Nesterov acceleration...
I0316 10:29:45.369177 139621354939584 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 12.block_0 (torch.Size([256, 512])) for Parameter 12 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
W0316 10:29:45.369211 140702629090496 distributed_shampoo.py:359] Nesterov flag is enabled but momentum parameter is zero! Continuing without using momentum or Nesterov acceleration...
I0316 10:29:45.369225 140552051844288 shampoo_preconditioner_list.py:381] Rank 3: AdaGradPreconditionerList Total Elements: 131072
I0316 10:29:45.369274 140552051844288 shampoo_preconditioner_list.py:384] Rank 3: AdaGradPreconditionerList Total Bytes: 524288
I0316 10:29:45.369261 140638539949248 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 14.block_0 ([torch.Size([256, 256])]) for Parameter 14 (torch.Size([1, 256])), Block block_0 (torch.Size([256])).
I0316 10:29:45.369281 139621354939584 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 13.block_0 (torch.Size([0])) for Parameter 13 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 10:29:45.369333 139621354939584 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 14.block_0 (torch.Size([0])) for Parameter 14 (torch.Size([1, 256])), Block block_0 (torch.Size([256])).
W0316 10:29:45.369328 140701078574272 distributed_shampoo.py:359] Nesterov flag is enabled but momentum parameter is zero! Continuing without using momentum or Nesterov acceleration...
W0316 10:29:45.369359 140574539490496 distributed_shampoo.py:359] Nesterov flag is enabled but momentum parameter is zero! Continuing without using momentum or Nesterov acceleration...
I0316 10:29:45.369392 139621354939584 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 15.block_0 (torch.Size([0])) for Parameter 15 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0316 10:29:45.369402 140638539949248 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 15.block_0 ([torch.Size([1, 1])]) for Parameter 15 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0316 10:29:45.369454 139621354939584 shampoo_preconditioner_list.py:375] Rank 4: AdaGradPreconditionerList Numel Breakdown: (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 131072, 0, 0, 0)
I0316 10:29:45.369493 139621354939584 shampoo_preconditioner_list.py:378] Rank 4: AdaGradPreconditionerList Bytes Breakdown: (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 524288, 0, 0, 0)
I0316 10:29:45.369527 139621354939584 shampoo_preconditioner_list.py:381] Rank 4: AdaGradPreconditionerList Total Elements: 131072
I0316 10:29:45.369538 140638539949248 shampoo_preconditioner_list.py:612] Rank 7: ShampooPreconditionerList Numel Breakdown: (524626, 524288, 655360, 131072, 163840, 32768, 2609224, 2097152, 4194304, 2097152, 2621440, 524288, 655360, 131072, 131072, 2)
I0316 10:29:45.369558 139621354939584 shampoo_preconditioner_list.py:384] Rank 4: AdaGradPreconditionerList Total Bytes: 524288
I0316 10:29:45.369587 140638539949248 shampoo_preconditioner_list.py:615] Rank 7: ShampooPreconditionerList Bytes Breakdown: (2098504, 2097152, 2621440, 524288, 655360, 131072, 10436896, 8388608, 16777216)
I0316 10:29:45.369623 140638539949248 shampoo_preconditioner_list.py:618] Rank 7: ShampooPreconditionerList Total Elements: 17093020
I0316 10:29:45.369657 140638539949248 shampoo_preconditioner_list.py:621] Rank 7: ShampooPreconditionerList Total Bytes: 43730536
W0316 10:29:45.369644 140552051844288 distributed_shampoo.py:359] Nesterov flag is enabled but momentum parameter is zero! Continuing without using momentum or Nesterov acceleration...
I0316 10:29:45.369797 140638539949248 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([512, 13])), Block block_0 (torch.Size([512, 13])).
I0316 10:29:45.369905 140638539949248 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([512])) for Parameter 1 (torch.Size([512])), Block block_0 (torch.Size([512])).
W0316 10:29:45.369929 139621354939584 distributed_shampoo.py:359] Nesterov flag is enabled but momentum parameter is zero! Continuing without using momentum or Nesterov acceleration...
I0316 10:29:45.369987 140638539949248 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 10:29:45.370075 140638539949248 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([256])) for Parameter 3 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 10:29:45.370144 140638539949248 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 4.block_0 (torch.Size([0])) for Parameter 4 (torch.Size([128, 256])), Block block_0 (torch.Size([128, 256])).
I0316 10:29:45.370234 140638539949248 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 5.block_0 (torch.Size([128])) for Parameter 5 (torch.Size([128])), Block block_0 (torch.Size([128])).
I0316 10:29:45.370325 140638539949248 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 6.block_0 (torch.Size([0])) for Parameter 6 (torch.Size([1024, 506])), Block block_0 (torch.Size([1024, 506])).
I0316 10:29:45.370416 140638539949248 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 7.block_0 (torch.Size([1024])) for Parameter 7 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 10:29:45.370484 140638539949248 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 8.block_0 (torch.Size([0])) for Parameter 8 (torch.Size([1024, 1024])), Block block_0 (torch.Size([1024, 1024])).
I0316 10:29:45.370573 140638539949248 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 9.block_0 (torch.Size([1024])) for Parameter 9 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 10:29:45.370640 140638539949248 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 10.block_0 (torch.Size([0])) for Parameter 10 (torch.Size([512, 1024])), Block block_0 (torch.Size([512, 1024])).
I0316 10:29:45.370738 140638539949248 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 11.block_0 (torch.Size([512])) for Parameter 11 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 10:29:45.370815 140638539949248 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 12.block_0 (torch.Size([0])) for Parameter 12 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 10:29:45.370891 140638539949248 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 13.block_0 (torch.Size([256])) for Parameter 13 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 10:29:45.370966 140638539949248 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 14.block_0 (torch.Size([256])) for Parameter 14 (torch.Size([1, 256])), Block block_0 (torch.Size([256])).
I0316 10:29:45.371039 140638539949248 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 15.block_0 (torch.Size([1])) for Parameter 15 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0316 10:29:45.371045 140649173091520 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 10:29:45.371116 140638539949248 shampoo_preconditioner_list.py:375] Rank 7: AdaGradPreconditionerList Numel Breakdown: (0, 512, 0, 256, 0, 128, 0, 1024, 0, 1024, 0, 512, 0, 256, 256, 1)
I0316 10:29:45.371155 140638539949248 shampoo_preconditioner_list.py:378] Rank 7: AdaGradPreconditionerList Bytes Breakdown: (0, 2048, 0, 1024, 0, 512, 0, 4096, 0, 4096, 0, 2048, 0, 1024, 1024, 4)
I0316 10:29:45.371147 140649173091520 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_1 (torch.Size([0])) for Parameter 0 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 10:29:45.371188 140638539949248 shampoo_preconditioner_list.py:381] Rank 7: AdaGradPreconditionerList Total Elements: 3969
I0316 10:29:45.371247 140638539949248 shampoo_preconditioner_list.py:384] Rank 7: AdaGradPreconditionerList Total Bytes: 15876
I0316 10:29:45.371553 139927921173696 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 10:29:45.371657 139927921173696 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_1 (torch.Size([0])) for Parameter 0 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 10:29:45.371723 139927921173696 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 10:29:45.371695 140701078574272 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
W0316 10:29:45.371743 140638539949248 distributed_shampoo.py:359] Nesterov flag is enabled but momentum parameter is zero! Continuing without using momentum or Nesterov acceleration...
I0316 10:29:45.371782 139927921173696 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_1 (torch.Size([0])) for Parameter 1 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 10:29:45.371794 140701078574272 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_1 (torch.Size([0])) for Parameter 0 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 10:29:45.371852 139927921173696 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 10:29:45.371909 139927921173696 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_1 (torch.Size([0])) for Parameter 2 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 10:29:45.371892 140701078574272 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 10:29:45.371966 140701078574272 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_1 (torch.Size([0])) for Parameter 1 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 10:29:45.371966 140574539490496 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 10:29:45.372027 140701078574272 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 10:29:45.372109 140552051844288 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 10:29:45.372185 140702629090496 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([524288, 128])) for Parameter 0 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 10:29:45.372216 140552051844288 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_1 (torch.Size([0])) for Parameter 0 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 10:29:45.372292 140552051844288 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 10:29:45.372297 140649173091520 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([524288, 128])) for Parameter 1 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 10:29:45.372312 140702629090496 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_1 (torch.Size([0])) for Parameter 0 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 10:29:45.372390 140702629090496 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 10:29:45.372377 139621354939584 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 10:29:45.372416 140649173091520 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_1 (torch.Size([0])) for Parameter 1 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 10:29:45.372454 140702629090496 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_1 (torch.Size([0])) for Parameter 1 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 10:29:45.372470 139621354939584 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_1 (torch.Size([0])) for Parameter 0 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 10:29:45.372485 140649173091520 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 10:29:45.372516 140702629090496 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 10:29:45.372546 139621354939584 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 10:29:45.372549 140649173091520 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_1 (torch.Size([0])) for Parameter 2 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 10:29:45.372574 140702629090496 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_1 (torch.Size([0])) for Parameter 2 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 10:29:45.372603 140649173091520 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 10:29:45.372609 139621354939584 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_1 (torch.Size([0])) for Parameter 1 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 10:29:45.372632 140702629090496 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 10:29:45.372661 140649173091520 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_1 (torch.Size([0])) for Parameter 3 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 10:29:45.372690 140702629090496 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_1 (torch.Size([0])) for Parameter 3 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 10:29:45.372715 140649173091520 shampoo_preconditioner_list.py:375] Rank 2: AdaGradPreconditionerList Numel Breakdown: (0, 0, 67108864, 0, 0, 0, 0, 0)
I0316 10:29:45.372744 140702629090496 shampoo_preconditioner_list.py:375] Rank 0: AdaGradPreconditionerList Numel Breakdown: (67108864, 0, 0, 0, 0, 0, 0, 0)
I0316 10:29:45.372753 140649173091520 shampoo_preconditioner_list.py:378] Rank 2: AdaGradPreconditionerList Bytes Breakdown: (0, 0, 268435456, 0, 0, 0, 0, 0)
I0316 10:29:45.372783 140649173091520 shampoo_preconditioner_list.py:381] Rank 2: AdaGradPreconditionerList Total Elements: 67108864
I0316 10:29:45.372783 140702629090496 shampoo_preconditioner_list.py:378] Rank 0: AdaGradPreconditionerList Bytes Breakdown: (268435456, 0, 0, 0, 0, 0, 0, 0)
I0316 10:29:45.372814 140702629090496 shampoo_preconditioner_list.py:381] Rank 0: AdaGradPreconditionerList Total Elements: 67108864
I0316 10:29:45.372816 140649173091520 shampoo_preconditioner_list.py:384] Rank 2: AdaGradPreconditionerList Total Bytes: 268435456
I0316 10:29:45.372848 140702629090496 shampoo_preconditioner_list.py:384] Rank 0: AdaGradPreconditionerList Total Bytes: 268435456
I0316 10:29:45.373006 139927921173696 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([524288, 128])) for Parameter 3 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 10:29:45.373117 139927921173696 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_1 (torch.Size([0])) for Parameter 3 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 10:29:45.373184 139927921173696 shampoo_preconditioner_list.py:375] Rank 6: AdaGradPreconditionerList Numel Breakdown: (0, 0, 0, 0, 0, 0, 67108864, 0)
I0316 10:29:45.373222 139927921173696 shampoo_preconditioner_list.py:378] Rank 6: AdaGradPreconditionerList Bytes Breakdown: (0, 0, 0, 0, 0, 0, 268435456, 0)
I0316 10:29:45.373189 140574539490496 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_1 (torch.Size([524288, 128])) for Parameter 0 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 10:29:45.373256 139927921173696 shampoo_preconditioner_list.py:381] Rank 6: AdaGradPreconditionerList Total Elements: 67108864
I0316 10:29:45.373241 140701078574272 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_1 (torch.Size([524288, 128])) for Parameter 2 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 10:29:45.373289 139927921173696 shampoo_preconditioner_list.py:384] Rank 6: AdaGradPreconditionerList Total Bytes: 268435456
I0316 10:29:45.373310 140574539490496 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 10:29:45.373352 140701078574272 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 10:29:45.373404 140574539490496 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_1 (torch.Size([0])) for Parameter 1 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 10:29:45.373439 140701078574272 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_1 (torch.Size([0])) for Parameter 3 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 10:29:45.373467 140574539490496 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 10:29:45.373496 140701078574272 shampoo_preconditioner_list.py:375] Rank 5: AdaGradPreconditionerList Numel Breakdown: (0, 0, 0, 0, 0, 67108864, 0, 0)
I0316 10:29:45.373527 140574539490496 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_1 (torch.Size([0])) for Parameter 2 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 10:29:45.373537 140701078574272 shampoo_preconditioner_list.py:378] Rank 5: AdaGradPreconditionerList Bytes Breakdown: (0, 0, 0, 0, 0, 268435456, 0, 0)
I0316 10:29:45.373568 140701078574272 shampoo_preconditioner_list.py:381] Rank 5: AdaGradPreconditionerList Total Elements: 67108864
I0316 10:29:45.373585 140574539490496 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 10:29:45.373604 140701078574272 shampoo_preconditioner_list.py:384] Rank 5: AdaGradPreconditionerList Total Bytes: 268435456
I0316 10:29:45.373665 140574539490496 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_1 (torch.Size([0])) for Parameter 3 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 10:29:45.373722 140574539490496 shampoo_preconditioner_list.py:375] Rank 1: AdaGradPreconditionerList Numel Breakdown: (0, 67108864, 0, 0, 0, 0, 0, 0)
I0316 10:29:45.373764 140574539490496 shampoo_preconditioner_list.py:378] Rank 1: AdaGradPreconditionerList Bytes Breakdown: (0, 268435456, 0, 0, 0, 0, 0, 0)
I0316 10:29:45.373766 140552051844288 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_1 (torch.Size([524288, 128])) for Parameter 1 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 10:29:45.373800 140574539490496 shampoo_preconditioner_list.py:381] Rank 1: AdaGradPreconditionerList Total Elements: 67108864
I0316 10:29:45.373830 140574539490496 shampoo_preconditioner_list.py:384] Rank 1: AdaGradPreconditionerList Total Bytes: 268435456
I0316 10:29:45.373885 140552051844288 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 10:29:45.373955 140552051844288 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_1 (torch.Size([0])) for Parameter 2 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 10:29:45.374030 140552051844288 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 10:29:45.374093 140552051844288 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_1 (torch.Size([0])) for Parameter 3 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 10:29:45.374149 140552051844288 shampoo_preconditioner_list.py:375] Rank 3: AdaGradPreconditionerList Numel Breakdown: (0, 0, 0, 67108864, 0, 0, 0, 0)
I0316 10:29:45.374124 140638539949248 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 10:29:45.374192 140552051844288 shampoo_preconditioner_list.py:378] Rank 3: AdaGradPreconditionerList Bytes Breakdown: (0, 0, 0, 268435456, 0, 0, 0, 0)
I0316 10:29:45.374196 139621354939584 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([524288, 128])) for Parameter 2 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 10:29:45.374217 140638539949248 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_1 (torch.Size([0])) for Parameter 0 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 10:29:45.374228 140552051844288 shampoo_preconditioner_list.py:381] Rank 3: AdaGradPreconditionerList Total Elements: 67108864
I0316 10:29:45.374270 140552051844288 shampoo_preconditioner_list.py:384] Rank 3: AdaGradPreconditionerList Total Bytes: 268435456
I0316 10:29:45.374297 140638539949248 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 10:29:45.374321 139621354939584 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_1 (torch.Size([0])) for Parameter 2 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 10:29:45.374367 140638539949248 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_1 (torch.Size([0])) for Parameter 1 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 10:29:45.374402 139621354939584 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 10:29:45.374430 140638539949248 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 10:29:45.374465 139621354939584 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_1 (torch.Size([0])) for Parameter 3 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 10:29:45.374491 140638539949248 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_1 (torch.Size([0])) for Parameter 2 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 10:29:45.374522 139621354939584 shampoo_preconditioner_list.py:375] Rank 4: AdaGradPreconditionerList Numel Breakdown: (0, 0, 0, 0, 67108864, 0, 0, 0)
I0316 10:29:45.374552 140638539949248 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 10:29:45.374562 139621354939584 shampoo_preconditioner_list.py:378] Rank 4: AdaGradPreconditionerList Bytes Breakdown: (0, 0, 0, 0, 268435456, 0, 0, 0)
I0316 10:29:45.374593 139621354939584 shampoo_preconditioner_list.py:381] Rank 4: AdaGradPreconditionerList Total Elements: 67108864
I0316 10:29:45.374628 139621354939584 shampoo_preconditioner_list.py:384] Rank 4: AdaGradPreconditionerList Total Bytes: 268435456
I0316 10:29:45.374699 140702629090496 submission_runner.py:279] Initializing metrics bundle.
I0316 10:29:45.374880 140702629090496 submission_runner.py:301] Initializing checkpoint and logger.
I0316 10:29:45.375242 140649173091520 logger_utils.py:262] Unable to record workload.train_mean information. Continuing without it.
I0316 10:29:45.375316 140649173091520 logger_utils.py:262] Unable to record workload.train_stddev information. Continuing without it.
I0316 10:29:45.375308 140702629090496 submission_runner.py:321] Saving meta data to /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_0/criteo1tb_pytorch/trial_2/meta_data_0.json.
I0316 10:29:45.375472 140702629090496 logger_utils.py:262] Unable to record workload.train_mean information. Continuing without it.
I0316 10:29:45.375520 140702629090496 logger_utils.py:262] Unable to record workload.train_stddev information. Continuing without it.
I0316 10:29:45.375580 140638539949248 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_1 (torch.Size([524288, 128])) for Parameter 3 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 10:29:45.375671 140638539949248 shampoo_preconditioner_list.py:375] Rank 7: AdaGradPreconditionerList Numel Breakdown: (0, 0, 0, 0, 0, 0, 0, 67108864)
I0316 10:29:45.375687 139927921173696 logger_utils.py:262] Unable to record workload.train_mean information. Continuing without it.
I0316 10:29:45.375722 140638539949248 shampoo_preconditioner_list.py:378] Rank 7: AdaGradPreconditionerList Bytes Breakdown: (0, 0, 0, 0, 0, 0, 0, 268435456)
I0316 10:29:45.375724 140701078574272 logger_utils.py:262] Unable to record workload.train_mean information. Continuing without it.
I0316 10:29:45.375752 140638539949248 shampoo_preconditioner_list.py:381] Rank 7: AdaGradPreconditionerList Total Elements: 67108864
I0316 10:29:45.375758 139927921173696 logger_utils.py:262] Unable to record workload.train_stddev information. Continuing without it.
I0316 10:29:45.375779 140638539949248 shampoo_preconditioner_list.py:384] Rank 7: AdaGradPreconditionerList Total Bytes: 268435456
I0316 10:29:45.375798 140701078574272 logger_utils.py:262] Unable to record workload.train_stddev information. Continuing without it.
I0316 10:29:45.375966 140574539490496 logger_utils.py:262] Unable to record workload.train_mean information. Continuing without it.
I0316 10:29:45.376039 140574539490496 logger_utils.py:262] Unable to record workload.train_stddev information. Continuing without it.
I0316 10:29:45.376173 140552051844288 logger_utils.py:262] Unable to record workload.train_mean information. Continuing without it.
I0316 10:29:45.376243 140552051844288 logger_utils.py:262] Unable to record workload.train_stddev information. Continuing without it.
I0316 10:29:45.376431 139621354939584 logger_utils.py:262] Unable to record workload.train_mean information. Continuing without it.
I0316 10:29:45.376502 139621354939584 logger_utils.py:262] Unable to record workload.train_stddev information. Continuing without it.
I0316 10:29:45.377129 140638539949248 logger_utils.py:262] Unable to record workload.train_mean information. Continuing without it.
I0316 10:29:45.377207 140638539949248 logger_utils.py:262] Unable to record workload.train_stddev information. Continuing without it.
I0316 10:29:46.071380 140702629090496 submission_runner.py:325] Saving flags to /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_0/criteo1tb_pytorch/trial_2/flags_0.json.
I0316 10:29:46.154773 140702629090496 submission_runner.py:337] Starting training loop.
I0316 10:29:55.219437 140670875461376 logging_writer.py:48] [0] global_step=0, grad_norm=6.01641, loss=0.61089
I0316 10:29:55.418498 140702629090496 submission.py:265] 0) loss = 0.611, grad_norm = 6.016
I0316 10:29:55.871556 140702629090496 spec.py:321] Evaluating on the training split.
I0316 10:34:58.718746 140702629090496 spec.py:333] Evaluating on the validation split.
I0316 10:39:56.524356 140702629090496 spec.py:349] Evaluating on the test split.
I0316 10:45:45.286155 140702629090496 submission_runner.py:469] Time since start: 959.13s, 	Step: 1, 	{'train/loss': 0.6098753428854343, 'validation/loss': 0.6070518915552142, 'validation/num_examples': 83274637, 'test/loss': 0.6086389612169768, 'test/num_examples': 95000000, 'score': 9.26477837562561, 'total_duration': 959.13156914711, 'accumulated_submission_time': 9.26477837562561, 'accumulated_eval_time': 949.4147655963898, 'accumulated_logging_time': 0}
I0316 10:45:45.316712 140661093189376 logging_writer.py:48] [1] accumulated_eval_time=949.415, accumulated_logging_time=0, accumulated_submission_time=9.26478, global_step=1, preemption_count=0, score=9.26478, test/loss=0.608639, test/num_examples=95000000, total_duration=959.132, train/loss=0.609875, validation/loss=0.607052, validation/num_examples=83274637
I0316 10:45:45.982401 140661084796672 logging_writer.py:48] [1] global_step=1, grad_norm=6.01352, loss=0.610835
I0316 10:45:45.985675 140702629090496 submission.py:265] 1) loss = 0.611, grad_norm = 6.014
I0316 10:45:46.177788 140661093189376 logging_writer.py:48] [2] global_step=2, grad_norm=5.82206, loss=0.59593
I0316 10:45:46.181028 140702629090496 submission.py:265] 2) loss = 0.596, grad_norm = 5.822
I0316 10:45:46.374406 140661084796672 logging_writer.py:48] [3] global_step=3, grad_norm=5.49887, loss=0.570549
I0316 10:45:46.377133 140702629090496 submission.py:265] 3) loss = 0.571, grad_norm = 5.499
I0316 10:45:46.571376 140661093189376 logging_writer.py:48] [4] global_step=4, grad_norm=5.05319, loss=0.537426
I0316 10:45:46.574570 140702629090496 submission.py:265] 4) loss = 0.537, grad_norm = 5.053
I0316 10:45:46.781969 140661084796672 logging_writer.py:48] [5] global_step=5, grad_norm=4.5101, loss=0.500241
I0316 10:45:46.784988 140702629090496 submission.py:265] 5) loss = 0.500, grad_norm = 4.510
I0316 10:45:46.977056 140661093189376 logging_writer.py:48] [6] global_step=6, grad_norm=4.00281, loss=0.460802
I0316 10:45:46.980306 140702629090496 submission.py:265] 6) loss = 0.461, grad_norm = 4.003
I0316 10:45:47.172307 140661084796672 logging_writer.py:48] [7] global_step=7, grad_norm=3.48357, loss=0.421662
I0316 10:45:47.175299 140702629090496 submission.py:265] 7) loss = 0.422, grad_norm = 3.484
I0316 10:45:47.366255 140661093189376 logging_writer.py:48] [8] global_step=8, grad_norm=3.09057, loss=0.383908
I0316 10:45:47.369136 140702629090496 submission.py:265] 8) loss = 0.384, grad_norm = 3.091
I0316 10:45:47.560301 140661084796672 logging_writer.py:48] [9] global_step=9, grad_norm=2.73022, loss=0.345955
I0316 10:45:47.563306 140702629090496 submission.py:265] 9) loss = 0.346, grad_norm = 2.730
I0316 10:45:47.757406 140661093189376 logging_writer.py:48] [10] global_step=10, grad_norm=2.38419, loss=0.311163
I0316 10:45:47.760792 140702629090496 submission.py:265] 10) loss = 0.311, grad_norm = 2.384
I0316 10:45:47.955758 140661084796672 logging_writer.py:48] [11] global_step=11, grad_norm=2.08053, loss=0.278586
I0316 10:45:47.959404 140702629090496 submission.py:265] 11) loss = 0.279, grad_norm = 2.081
I0316 10:45:48.151291 140661093189376 logging_writer.py:48] [12] global_step=12, grad_norm=1.76074, loss=0.250619
I0316 10:45:48.154355 140702629090496 submission.py:265] 12) loss = 0.251, grad_norm = 1.761
I0316 10:45:48.346143 140661084796672 logging_writer.py:48] [13] global_step=13, grad_norm=1.46097, loss=0.224607
I0316 10:45:48.349433 140702629090496 submission.py:265] 13) loss = 0.225, grad_norm = 1.461
I0316 10:45:48.567982 140661093189376 logging_writer.py:48] [14] global_step=14, grad_norm=1.16671, loss=0.202804
I0316 10:45:48.571758 140702629090496 submission.py:265] 14) loss = 0.203, grad_norm = 1.167
I0316 10:45:48.765679 140661084796672 logging_writer.py:48] [15] global_step=15, grad_norm=0.872174, loss=0.185669
I0316 10:45:48.769057 140702629090496 submission.py:265] 15) loss = 0.186, grad_norm = 0.872
I0316 10:45:48.962694 140661093189376 logging_writer.py:48] [16] global_step=16, grad_norm=0.568281, loss=0.175998
I0316 10:45:48.965875 140702629090496 submission.py:265] 16) loss = 0.176, grad_norm = 0.568
I0316 10:45:49.156847 140661084796672 logging_writer.py:48] [17] global_step=17, grad_norm=0.308959, loss=0.167899
I0316 10:45:49.159679 140702629090496 submission.py:265] 17) loss = 0.168, grad_norm = 0.309
I0316 10:45:49.350934 140661093189376 logging_writer.py:48] [18] global_step=18, grad_norm=0.106039, loss=0.162844
I0316 10:45:49.353841 140702629090496 submission.py:265] 18) loss = 0.163, grad_norm = 0.106
I0316 10:45:49.546030 140661084796672 logging_writer.py:48] [19] global_step=19, grad_norm=0.23659, loss=0.159212
I0316 10:45:49.549004 140702629090496 submission.py:265] 19) loss = 0.159, grad_norm = 0.237
I0316 10:45:49.751303 140661093189376 logging_writer.py:48] [20] global_step=20, grad_norm=0.453288, loss=0.164907
I0316 10:45:49.754968 140702629090496 submission.py:265] 20) loss = 0.165, grad_norm = 0.453
I0316 10:45:49.947555 140661084796672 logging_writer.py:48] [21] global_step=21, grad_norm=0.613051, loss=0.169469
I0316 10:45:49.966618 140702629090496 submission.py:265] 21) loss = 0.169, grad_norm = 0.613
I0316 10:45:50.162352 140661093189376 logging_writer.py:48] [22] global_step=22, grad_norm=0.736283, loss=0.175222
I0316 10:45:50.165467 140702629090496 submission.py:265] 22) loss = 0.175, grad_norm = 0.736
I0316 10:45:50.355995 140661084796672 logging_writer.py:48] [23] global_step=23, grad_norm=0.911228, loss=0.190874
I0316 10:45:50.359018 140702629090496 submission.py:265] 23) loss = 0.191, grad_norm = 0.911
I0316 10:45:50.550345 140661093189376 logging_writer.py:48] [24] global_step=24, grad_norm=1.01403, loss=0.19902
I0316 10:45:50.553652 140702629090496 submission.py:265] 24) loss = 0.199, grad_norm = 1.014
I0316 10:45:50.746329 140661084796672 logging_writer.py:48] [25] global_step=25, grad_norm=1.05399, loss=0.200759
I0316 10:45:50.749435 140702629090496 submission.py:265] 25) loss = 0.201, grad_norm = 1.054
I0316 10:45:50.940344 140661093189376 logging_writer.py:48] [26] global_step=26, grad_norm=1.14968, loss=0.211466
I0316 10:45:50.943285 140702629090496 submission.py:265] 26) loss = 0.211, grad_norm = 1.150
I0316 10:45:51.135148 140661084796672 logging_writer.py:48] [27] global_step=27, grad_norm=1.14585, loss=0.209287
I0316 10:45:51.138743 140702629090496 submission.py:265] 27) loss = 0.209, grad_norm = 1.146
I0316 10:45:51.330382 140661093189376 logging_writer.py:48] [28] global_step=28, grad_norm=1.22443, loss=0.219392
I0316 10:45:51.333464 140702629090496 submission.py:265] 28) loss = 0.219, grad_norm = 1.224
I0316 10:45:51.524354 140661084796672 logging_writer.py:48] [29] global_step=29, grad_norm=1.18831, loss=0.213747
I0316 10:45:51.527307 140702629090496 submission.py:265] 29) loss = 0.214, grad_norm = 1.188
I0316 10:45:51.718226 140661093189376 logging_writer.py:48] [30] global_step=30, grad_norm=1.18979, loss=0.213767
I0316 10:45:51.722342 140702629090496 submission.py:265] 30) loss = 0.214, grad_norm = 1.190
I0316 10:45:52.370083 140661084796672 logging_writer.py:48] [31] global_step=31, grad_norm=1.15206, loss=0.20909
I0316 10:45:52.373360 140702629090496 submission.py:265] 31) loss = 0.209, grad_norm = 1.152
I0316 10:45:53.342574 140661093189376 logging_writer.py:48] [32] global_step=32, grad_norm=1.14045, loss=0.20872
I0316 10:45:53.345689 140702629090496 submission.py:265] 32) loss = 0.209, grad_norm = 1.140
I0316 10:45:54.468072 140661084796672 logging_writer.py:48] [33] global_step=33, grad_norm=1.04312, loss=0.197269
I0316 10:45:54.471091 140702629090496 submission.py:265] 33) loss = 0.197, grad_norm = 1.043
I0316 10:45:55.497119 140661093189376 logging_writer.py:48] [34] global_step=34, grad_norm=0.971392, loss=0.189577
I0316 10:45:55.500398 140702629090496 submission.py:265] 34) loss = 0.190, grad_norm = 0.971
I0316 10:45:56.529468 140661084796672 logging_writer.py:48] [35] global_step=35, grad_norm=0.904583, loss=0.183906
I0316 10:45:56.532540 140702629090496 submission.py:265] 35) loss = 0.184, grad_norm = 0.905
I0316 10:45:57.425273 140661093189376 logging_writer.py:48] [36] global_step=36, grad_norm=0.823875, loss=0.17938
I0316 10:45:57.428641 140702629090496 submission.py:265] 36) loss = 0.179, grad_norm = 0.824
I0316 10:45:58.616200 140661084796672 logging_writer.py:48] [37] global_step=37, grad_norm=0.676706, loss=0.168485
I0316 10:45:58.619326 140702629090496 submission.py:265] 37) loss = 0.168, grad_norm = 0.677
I0316 10:45:59.607883 140661093189376 logging_writer.py:48] [38] global_step=38, grad_norm=0.372242, loss=0.149678
I0316 10:45:59.611044 140702629090496 submission.py:265] 38) loss = 0.150, grad_norm = 0.372
I0316 10:46:00.465031 140661084796672 logging_writer.py:48] [39] global_step=39, grad_norm=0.247205, loss=0.148901
I0316 10:46:00.468144 140702629090496 submission.py:265] 39) loss = 0.149, grad_norm = 0.247
I0316 10:46:01.731790 140661093189376 logging_writer.py:48] [40] global_step=40, grad_norm=0.10554, loss=0.144638
I0316 10:46:01.735124 140702629090496 submission.py:265] 40) loss = 0.145, grad_norm = 0.106
I0316 10:46:02.442979 140661084796672 logging_writer.py:48] [41] global_step=41, grad_norm=0.0786417, loss=0.145161
I0316 10:46:02.446216 140702629090496 submission.py:265] 41) loss = 0.145, grad_norm = 0.079
I0316 10:46:04.447752 140661093189376 logging_writer.py:48] [42] global_step=42, grad_norm=0.191544, loss=0.143765
I0316 10:46:04.450874 140702629090496 submission.py:265] 42) loss = 0.144, grad_norm = 0.192
I0316 10:46:05.202215 140661084796672 logging_writer.py:48] [43] global_step=43, grad_norm=0.292742, loss=0.144763
I0316 10:46:05.205619 140702629090496 submission.py:265] 43) loss = 0.145, grad_norm = 0.293
I0316 10:46:06.738522 140661093189376 logging_writer.py:48] [44] global_step=44, grad_norm=0.365928, loss=0.14462
I0316 10:46:06.741856 140702629090496 submission.py:265] 44) loss = 0.145, grad_norm = 0.366
I0316 10:46:07.557451 140661084796672 logging_writer.py:48] [45] global_step=45, grad_norm=0.40091, loss=0.144444
I0316 10:46:07.561172 140702629090496 submission.py:265] 45) loss = 0.144, grad_norm = 0.401
I0316 10:46:08.692595 140661093189376 logging_writer.py:48] [46] global_step=46, grad_norm=0.37724, loss=0.147068
I0316 10:46:08.695925 140702629090496 submission.py:265] 46) loss = 0.147, grad_norm = 0.377
I0316 10:46:09.699426 140661084796672 logging_writer.py:48] [47] global_step=47, grad_norm=0.359159, loss=0.143219
I0316 10:46:09.702819 140702629090496 submission.py:265] 47) loss = 0.143, grad_norm = 0.359
I0316 10:46:10.802772 140661093189376 logging_writer.py:48] [48] global_step=48, grad_norm=0.277355, loss=0.143164
I0316 10:46:10.806114 140702629090496 submission.py:265] 48) loss = 0.143, grad_norm = 0.277
I0316 10:46:11.911597 140661084796672 logging_writer.py:48] [49] global_step=49, grad_norm=0.211859, loss=0.140085
I0316 10:46:11.914657 140702629090496 submission.py:265] 49) loss = 0.140, grad_norm = 0.212
I0316 10:46:13.024232 140661093189376 logging_writer.py:48] [50] global_step=50, grad_norm=0.109871, loss=0.140213
I0316 10:46:13.027467 140702629090496 submission.py:265] 50) loss = 0.140, grad_norm = 0.110
I0316 10:46:14.332129 140661084796672 logging_writer.py:48] [51] global_step=51, grad_norm=0.037062, loss=0.13894
I0316 10:46:14.335270 140702629090496 submission.py:265] 51) loss = 0.139, grad_norm = 0.037
I0316 10:46:15.520919 140661093189376 logging_writer.py:48] [52] global_step=52, grad_norm=0.05324, loss=0.137341
I0316 10:46:15.524013 140702629090496 submission.py:265] 52) loss = 0.137, grad_norm = 0.053
I0316 10:46:17.467328 140661084796672 logging_writer.py:48] [53] global_step=53, grad_norm=0.128717, loss=0.139603
I0316 10:46:17.470489 140702629090496 submission.py:265] 53) loss = 0.140, grad_norm = 0.129
I0316 10:46:17.949421 140661093189376 logging_writer.py:48] [54] global_step=54, grad_norm=0.16404, loss=0.138662
I0316 10:46:17.952723 140702629090496 submission.py:265] 54) loss = 0.139, grad_norm = 0.164
I0316 10:46:19.496170 140661084796672 logging_writer.py:48] [55] global_step=55, grad_norm=0.183377, loss=0.138096
I0316 10:46:19.499458 140702629090496 submission.py:265] 55) loss = 0.138, grad_norm = 0.183
I0316 10:46:20.690256 140661093189376 logging_writer.py:48] [56] global_step=56, grad_norm=0.189654, loss=0.137998
I0316 10:46:20.693620 140702629090496 submission.py:265] 56) loss = 0.138, grad_norm = 0.190
I0316 10:46:21.875684 140661084796672 logging_writer.py:48] [57] global_step=57, grad_norm=0.214084, loss=0.143338
I0316 10:46:21.879027 140702629090496 submission.py:265] 57) loss = 0.143, grad_norm = 0.214
I0316 10:46:23.298988 140661093189376 logging_writer.py:48] [58] global_step=58, grad_norm=0.179349, loss=0.142104
I0316 10:46:23.302253 140702629090496 submission.py:265] 58) loss = 0.142, grad_norm = 0.179
I0316 10:46:24.488881 140661084796672 logging_writer.py:48] [59] global_step=59, grad_norm=0.137771, loss=0.141993
I0316 10:46:24.492028 140702629090496 submission.py:265] 59) loss = 0.142, grad_norm = 0.138
I0316 10:46:25.807425 140661093189376 logging_writer.py:48] [60] global_step=60, grad_norm=0.0965128, loss=0.142877
I0316 10:46:25.810444 140702629090496 submission.py:265] 60) loss = 0.143, grad_norm = 0.097
I0316 10:46:26.638584 140661084796672 logging_writer.py:48] [61] global_step=61, grad_norm=0.0459434, loss=0.142333
I0316 10:46:26.641808 140702629090496 submission.py:265] 61) loss = 0.142, grad_norm = 0.046
I0316 10:46:28.327735 140661093189376 logging_writer.py:48] [62] global_step=62, grad_norm=0.0261331, loss=0.143477
I0316 10:46:28.330825 140702629090496 submission.py:265] 62) loss = 0.143, grad_norm = 0.026
I0316 10:46:29.428170 140661084796672 logging_writer.py:48] [63] global_step=63, grad_norm=0.0657078, loss=0.140738
I0316 10:46:29.431254 140702629090496 submission.py:265] 63) loss = 0.141, grad_norm = 0.066
I0316 10:46:30.959583 140661093189376 logging_writer.py:48] [64] global_step=64, grad_norm=0.0789243, loss=0.142629
I0316 10:46:30.962584 140702629090496 submission.py:265] 64) loss = 0.143, grad_norm = 0.079
I0316 10:46:31.456295 140661084796672 logging_writer.py:48] [65] global_step=65, grad_norm=0.084677, loss=0.142733
I0316 10:46:31.459625 140702629090496 submission.py:265] 65) loss = 0.143, grad_norm = 0.085
I0316 10:46:33.198266 140661093189376 logging_writer.py:48] [66] global_step=66, grad_norm=0.0810349, loss=0.143339
I0316 10:46:33.201509 140702629090496 submission.py:265] 66) loss = 0.143, grad_norm = 0.081
I0316 10:46:34.153319 140661084796672 logging_writer.py:48] [67] global_step=67, grad_norm=0.0788612, loss=0.141264
I0316 10:46:34.156420 140702629090496 submission.py:265] 67) loss = 0.141, grad_norm = 0.079
I0316 10:46:35.300516 140661093189376 logging_writer.py:48] [68] global_step=68, grad_norm=0.051613, loss=0.14329
I0316 10:46:35.303632 140702629090496 submission.py:265] 68) loss = 0.143, grad_norm = 0.052
I0316 10:46:36.291082 140661084796672 logging_writer.py:48] [69] global_step=69, grad_norm=0.0364049, loss=0.142905
I0316 10:46:36.294634 140702629090496 submission.py:265] 69) loss = 0.143, grad_norm = 0.036
I0316 10:46:37.452930 140661093189376 logging_writer.py:48] [70] global_step=70, grad_norm=0.0276294, loss=0.142282
I0316 10:46:37.456559 140702629090496 submission.py:265] 70) loss = 0.142, grad_norm = 0.028
I0316 10:46:38.609448 140661084796672 logging_writer.py:48] [71] global_step=71, grad_norm=0.0282555, loss=0.141414
I0316 10:46:38.613147 140702629090496 submission.py:265] 71) loss = 0.141, grad_norm = 0.028
I0316 10:46:39.805576 140661093189376 logging_writer.py:48] [72] global_step=72, grad_norm=0.0332557, loss=0.140204
I0316 10:46:39.808829 140702629090496 submission.py:265] 72) loss = 0.140, grad_norm = 0.033
I0316 10:46:41.095868 140661084796672 logging_writer.py:48] [73] global_step=73, grad_norm=0.0419854, loss=0.141166
I0316 10:46:41.099293 140702629090496 submission.py:265] 73) loss = 0.141, grad_norm = 0.042
I0316 10:46:42.584464 140661093189376 logging_writer.py:48] [74] global_step=74, grad_norm=0.0444299, loss=0.141625
I0316 10:46:42.587970 140702629090496 submission.py:265] 74) loss = 0.142, grad_norm = 0.044
I0316 10:46:43.627750 140661084796672 logging_writer.py:48] [75] global_step=75, grad_norm=0.0357511, loss=0.141468
I0316 10:46:43.631060 140702629090496 submission.py:265] 75) loss = 0.141, grad_norm = 0.036
I0316 10:46:45.705713 140661093189376 logging_writer.py:48] [76] global_step=76, grad_norm=0.0289753, loss=0.138346
I0316 10:46:45.709060 140702629090496 submission.py:265] 76) loss = 0.138, grad_norm = 0.029
I0316 10:46:46.319610 140661084796672 logging_writer.py:48] [77] global_step=77, grad_norm=0.0245648, loss=0.134582
I0316 10:46:46.323033 140702629090496 submission.py:265] 77) loss = 0.135, grad_norm = 0.025
I0316 10:46:47.982872 140661093189376 logging_writer.py:48] [78] global_step=78, grad_norm=0.0256188, loss=0.134914
I0316 10:46:47.986006 140702629090496 submission.py:265] 78) loss = 0.135, grad_norm = 0.026
I0316 10:46:48.895012 140661084796672 logging_writer.py:48] [79] global_step=79, grad_norm=0.0324003, loss=0.133982
I0316 10:46:48.898085 140702629090496 submission.py:265] 79) loss = 0.134, grad_norm = 0.032
I0316 10:46:50.354877 140661093189376 logging_writer.py:48] [80] global_step=80, grad_norm=0.0243905, loss=0.134566
I0316 10:46:50.358053 140702629090496 submission.py:265] 80) loss = 0.135, grad_norm = 0.024
I0316 10:46:50.926042 140661084796672 logging_writer.py:48] [81] global_step=81, grad_norm=0.0229359, loss=0.13437
I0316 10:46:50.929821 140702629090496 submission.py:265] 81) loss = 0.134, grad_norm = 0.023
I0316 10:46:52.694998 140661093189376 logging_writer.py:48] [82] global_step=82, grad_norm=0.0193495, loss=0.133794
I0316 10:46:52.698626 140702629090496 submission.py:265] 82) loss = 0.134, grad_norm = 0.019
I0316 10:46:53.788492 140661084796672 logging_writer.py:48] [83] global_step=83, grad_norm=0.0165834, loss=0.132225
I0316 10:46:53.792171 140702629090496 submission.py:265] 83) loss = 0.132, grad_norm = 0.017
I0316 10:46:55.246798 140661093189376 logging_writer.py:48] [84] global_step=84, grad_norm=0.0166135, loss=0.133362
I0316 10:46:55.250563 140702629090496 submission.py:265] 84) loss = 0.133, grad_norm = 0.017
I0316 10:46:56.366076 140661084796672 logging_writer.py:48] [85] global_step=85, grad_norm=0.0273456, loss=0.135162
I0316 10:46:56.369607 140702629090496 submission.py:265] 85) loss = 0.135, grad_norm = 0.027
I0316 10:46:57.781183 140661093189376 logging_writer.py:48] [86] global_step=86, grad_norm=0.0163347, loss=0.133119
I0316 10:46:57.784693 140702629090496 submission.py:265] 86) loss = 0.133, grad_norm = 0.016
I0316 10:46:58.711208 140661084796672 logging_writer.py:48] [87] global_step=87, grad_norm=0.0130418, loss=0.132268
I0316 10:46:58.714902 140702629090496 submission.py:265] 87) loss = 0.132, grad_norm = 0.013
I0316 10:47:00.588783 140661093189376 logging_writer.py:48] [88] global_step=88, grad_norm=0.0135895, loss=0.133452
I0316 10:47:00.592427 140702629090496 submission.py:265] 88) loss = 0.133, grad_norm = 0.014
I0316 10:47:01.172635 140661084796672 logging_writer.py:48] [89] global_step=89, grad_norm=0.0101352, loss=0.132265
I0316 10:47:01.176185 140702629090496 submission.py:265] 89) loss = 0.132, grad_norm = 0.010
I0316 10:47:03.242516 140661093189376 logging_writer.py:48] [90] global_step=90, grad_norm=0.0112771, loss=0.132819
I0316 10:47:03.246173 140702629090496 submission.py:265] 90) loss = 0.133, grad_norm = 0.011
I0316 10:47:04.010897 140661084796672 logging_writer.py:48] [91] global_step=91, grad_norm=0.0146574, loss=0.130839
I0316 10:47:04.014669 140702629090496 submission.py:265] 91) loss = 0.131, grad_norm = 0.015
I0316 10:47:05.461300 140661093189376 logging_writer.py:48] [92] global_step=92, grad_norm=0.0128091, loss=0.132402
I0316 10:47:05.464849 140702629090496 submission.py:265] 92) loss = 0.132, grad_norm = 0.013
I0316 10:47:06.254688 140661084796672 logging_writer.py:48] [93] global_step=93, grad_norm=0.0126191, loss=0.132138
I0316 10:47:06.258277 140702629090496 submission.py:265] 93) loss = 0.132, grad_norm = 0.013
I0316 10:47:08.033559 140661093189376 logging_writer.py:48] [94] global_step=94, grad_norm=0.0100618, loss=0.131489
I0316 10:47:08.037575 140702629090496 submission.py:265] 94) loss = 0.131, grad_norm = 0.010
I0316 10:47:09.066845 140661084796672 logging_writer.py:48] [95] global_step=95, grad_norm=0.0196172, loss=0.142075
I0316 10:47:09.070464 140702629090496 submission.py:265] 95) loss = 0.142, grad_norm = 0.020
I0316 10:47:10.728911 140661093189376 logging_writer.py:48] [96] global_step=96, grad_norm=0.0164611, loss=0.145492
I0316 10:47:10.732687 140702629090496 submission.py:265] 96) loss = 0.145, grad_norm = 0.016
I0316 10:47:11.538678 140661084796672 logging_writer.py:48] [97] global_step=97, grad_norm=0.0158212, loss=0.144
I0316 10:47:11.542492 140702629090496 submission.py:265] 97) loss = 0.144, grad_norm = 0.016
I0316 10:47:13.409108 140661093189376 logging_writer.py:48] [98] global_step=98, grad_norm=0.0152733, loss=0.142958
I0316 10:47:13.412879 140702629090496 submission.py:265] 98) loss = 0.143, grad_norm = 0.015
I0316 10:47:16.653826 140661084796672 logging_writer.py:48] [99] global_step=99, grad_norm=0.0189315, loss=0.141098
I0316 10:47:16.656995 140702629090496 submission.py:265] 99) loss = 0.141, grad_norm = 0.019
I0316 10:47:16.857363 140661093189376 logging_writer.py:48] [100] global_step=100, grad_norm=0.0179517, loss=0.142155
I0316 10:47:16.861849 140702629090496 submission.py:265] 100) loss = 0.142, grad_norm = 0.018
I0316 10:47:46.412932 140702629090496 spec.py:321] Evaluating on the training split.
I0316 10:53:05.294947 140702629090496 spec.py:333] Evaluating on the validation split.
I0316 10:57:25.461167 140702629090496 spec.py:349] Evaluating on the test split.
I0316 11:02:35.646780 140702629090496 submission_runner.py:469] Time since start: 1969.49s, 	Step: 124, 	{'train/loss': 0.13124836394424066, 'validation/loss': 0.13223318855414934, 'validation/num_examples': 83274637, 'test/loss': 0.13501571895157663, 'test/num_examples': 95000000, 'score': 129.43750596046448, 'total_duration': 1969.492205619812, 'accumulated_submission_time': 129.43750596046448, 'accumulated_eval_time': 1838.648716211319, 'accumulated_logging_time': 0.06248927116394043}
I0316 11:02:35.657414 140661084796672 logging_writer.py:48] [124] accumulated_eval_time=1838.65, accumulated_logging_time=0.0624893, accumulated_submission_time=129.438, global_step=124, preemption_count=0, score=129.438, test/loss=0.135016, test/num_examples=95000000, total_duration=1969.49, train/loss=0.131248, validation/loss=0.132233, validation/num_examples=83274637
I0316 11:04:36.288885 140702629090496 spec.py:321] Evaluating on the training split.
I0316 11:09:50.347212 140702629090496 spec.py:333] Evaluating on the validation split.
I0316 11:14:17.348761 140702629090496 spec.py:349] Evaluating on the test split.
I0316 11:19:29.286438 140702629090496 submission_runner.py:469] Time since start: 2983.13s, 	Step: 244, 	{'train/loss': 0.1288338746732061, 'validation/loss': 0.128730590634081, 'validation/num_examples': 83274637, 'test/loss': 0.13145081003064607, 'test/num_examples': 95000000, 'score': 249.1492781639099, 'total_duration': 2983.131873846054, 'accumulated_submission_time': 249.1492781639099, 'accumulated_eval_time': 2731.6464343070984, 'accumulated_logging_time': 0.08045792579650879}
I0316 11:19:29.295863 140661093189376 logging_writer.py:48] [244] accumulated_eval_time=2731.65, accumulated_logging_time=0.0804579, accumulated_submission_time=249.149, global_step=244, preemption_count=0, score=249.149, test/loss=0.131451, test/num_examples=95000000, total_duration=2983.13, train/loss=0.128834, validation/loss=0.128731, validation/num_examples=83274637
/usr/local/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
I0316 11:21:30.071974 140702629090496 spec.py:321] Evaluating on the training split.
I0316 11:26:36.591810 140702629090496 spec.py:333] Evaluating on the validation split.
I0316 11:30:55.606046 140702629090496 spec.py:349] Evaluating on the test split.
I0316 11:36:04.011816 140702629090496 submission_runner.py:469] Time since start: 3977.86s, 	Step: 366, 	{'train/loss': 0.12788641748973886, 'validation/loss': 0.12773614130041228, 'validation/num_examples': 83274637, 'test/loss': 0.13056377778095446, 'test/num_examples': 95000000, 'score': 368.95321464538574, 'total_duration': 3977.8572521209717, 'accumulated_submission_time': 368.95321464538574, 'accumulated_eval_time': 3605.5976700782776, 'accumulated_logging_time': 0.17122840881347656}
I0316 11:36:04.021836 140661084796672 logging_writer.py:48] [366] accumulated_eval_time=3605.6, accumulated_logging_time=0.171228, accumulated_submission_time=368.953, global_step=366, preemption_count=0, score=368.953, test/loss=0.130564, test/num_examples=95000000, total_duration=3977.86, train/loss=0.127886, validation/loss=0.127736, validation/num_examples=83274637
I0316 11:38:04.677840 140702629090496 spec.py:321] Evaluating on the training split.
I0316 11:43:19.599257 140702629090496 spec.py:333] Evaluating on the validation split.
I0316 11:47:43.195314 140702629090496 spec.py:349] Evaluating on the test split.
I0316 11:52:53.110018 140702629090496 submission_runner.py:469] Time since start: 4986.96s, 	Step: 489, 	{'train/loss': 0.12818378242681444, 'validation/loss': 0.12692039395845084, 'validation/num_examples': 83274637, 'test/loss': 0.1294551247552169, 'test/num_examples': 95000000, 'score': 488.71590924263, 'total_duration': 4986.955470323563, 'accumulated_submission_time': 488.71590924263, 'accumulated_eval_time': 4494.02990937233, 'accumulated_logging_time': 0.1878490447998047}
I0316 11:52:53.119649 140661093189376 logging_writer.py:48] [489] accumulated_eval_time=4494.03, accumulated_logging_time=0.187849, accumulated_submission_time=488.716, global_step=489, preemption_count=0, score=488.716, test/loss=0.129455, test/num_examples=95000000, total_duration=4986.96, train/loss=0.128184, validation/loss=0.12692, validation/num_examples=83274637
I0316 11:52:55.892808 140661084796672 logging_writer.py:48] [500] global_step=500, grad_norm=0.013281, loss=0.121867
I0316 11:52:55.897340 140702629090496 submission.py:265] 500) loss = 0.122, grad_norm = 0.013
I0316 11:54:54.121199 140702629090496 spec.py:321] Evaluating on the training split.
I0316 12:00:04.956365 140702629090496 spec.py:333] Evaluating on the validation split.
I0316 12:04:30.484970 140702629090496 spec.py:349] Evaluating on the test split.
I0316 12:09:30.775850 140702629090496 submission_runner.py:469] Time since start: 5984.62s, 	Step: 610, 	{'train/loss': 0.12522959846694673, 'validation/loss': 0.12677021676856395, 'validation/num_examples': 83274637, 'test/loss': 0.12919498835304663, 'test/num_examples': 95000000, 'score': 608.8842985630035, 'total_duration': 5984.621225118637, 'accumulated_submission_time': 608.8842985630035, 'accumulated_eval_time': 5370.684534311295, 'accumulated_logging_time': 0.20443010330200195}
I0316 12:09:30.786434 140661093189376 logging_writer.py:48] [610] accumulated_eval_time=5370.68, accumulated_logging_time=0.20443, accumulated_submission_time=608.884, global_step=610, preemption_count=0, score=608.884, test/loss=0.129195, test/num_examples=95000000, total_duration=5984.62, train/loss=0.12523, validation/loss=0.12677, validation/num_examples=83274637
I0316 12:11:31.893158 140702629090496 spec.py:321] Evaluating on the training split.
I0316 12:16:45.894094 140702629090496 spec.py:333] Evaluating on the validation split.
I0316 12:21:02.777854 140702629090496 spec.py:349] Evaluating on the test split.
I0316 12:26:03.193835 140702629090496 submission_runner.py:469] Time since start: 6977.04s, 	Step: 734, 	{'train/loss': 0.12445888383322808, 'validation/loss': 0.12648132692213973, 'validation/num_examples': 83274637, 'test/loss': 0.12900676912119013, 'test/num_examples': 95000000, 'score': 729.0558650493622, 'total_duration': 6977.039262056351, 'accumulated_submission_time': 729.0558650493622, 'accumulated_eval_time': 6241.985344171524, 'accumulated_logging_time': 0.2216479778289795}
I0316 12:26:03.203715 140661084796672 logging_writer.py:48] [734] accumulated_eval_time=6241.99, accumulated_logging_time=0.221648, accumulated_submission_time=729.056, global_step=734, preemption_count=0, score=729.056, test/loss=0.129007, test/num_examples=95000000, total_duration=6977.04, train/loss=0.124459, validation/loss=0.126481, validation/num_examples=83274637
I0316 12:28:03.920117 140702629090496 spec.py:321] Evaluating on the training split.
I0316 12:33:16.098978 140702629090496 spec.py:333] Evaluating on the validation split.
I0316 12:37:36.696735 140702629090496 spec.py:349] Evaluating on the test split.
I0316 12:42:41.933659 140702629090496 submission_runner.py:469] Time since start: 7975.78s, 	Step: 855, 	{'train/loss': 0.12679202938590697, 'validation/loss': 0.1264477009151791, 'validation/num_examples': 83274637, 'test/loss': 0.12859334473033704, 'test/num_examples': 95000000, 'score': 848.8339562416077, 'total_duration': 7975.779079437256, 'accumulated_submission_time': 848.8339562416077, 'accumulated_eval_time': 7119.999000787735, 'accumulated_logging_time': 0.2384810447692871}
I0316 12:42:41.943970 140661093189376 logging_writer.py:48] [855] accumulated_eval_time=7120, accumulated_logging_time=0.238481, accumulated_submission_time=848.834, global_step=855, preemption_count=0, score=848.834, test/loss=0.128593, test/num_examples=95000000, total_duration=7975.78, train/loss=0.126792, validation/loss=0.126448, validation/num_examples=83274637
I0316 12:44:43.135881 140702629090496 spec.py:321] Evaluating on the training split.
I0316 12:49:44.408838 140702629090496 spec.py:333] Evaluating on the validation split.
I0316 12:54:05.378663 140702629090496 spec.py:349] Evaluating on the test split.
I0316 12:59:14.212023 140702629090496 submission_runner.py:469] Time since start: 8968.06s, 	Step: 977, 	{'train/loss': 0.12449914697186111, 'validation/loss': 0.12636989387220104, 'validation/num_examples': 83274637, 'test/loss': 0.12862598498687744, 'test/num_examples': 95000000, 'score': 969.1742653846741, 'total_duration': 8968.05740904808, 'accumulated_submission_time': 969.1742653846741, 'accumulated_eval_time': 7991.075145483017, 'accumulated_logging_time': 0.25598788261413574}
I0316 12:59:14.223515 140661084796672 logging_writer.py:48] [977] accumulated_eval_time=7991.08, accumulated_logging_time=0.255988, accumulated_submission_time=969.174, global_step=977, preemption_count=0, score=969.174, test/loss=0.128626, test/num_examples=95000000, total_duration=8968.06, train/loss=0.124499, validation/loss=0.12637, validation/num_examples=83274637
I0316 12:59:19.302916 140661093189376 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.00553832, loss=0.128195
I0316 12:59:19.306668 140702629090496 submission.py:265] 1000) loss = 0.128, grad_norm = 0.006
I0316 13:01:15.160853 140702629090496 spec.py:321] Evaluating on the training split.
I0316 13:06:04.462116 140702629090496 spec.py:333] Evaluating on the validation split.
I0316 13:10:14.650029 140702629090496 spec.py:349] Evaluating on the test split.
I0316 13:15:17.843481 140702629090496 submission_runner.py:469] Time since start: 9931.69s, 	Step: 1106, 	{'train/loss': 0.12420708116967713, 'validation/loss': 0.1261259112938554, 'validation/num_examples': 83274637, 'test/loss': 0.12836988119406448, 'test/num_examples': 95000000, 'score': 1089.1597654819489, 'total_duration': 9931.688895702362, 'accumulated_submission_time': 1089.1597654819489, 'accumulated_eval_time': 8833.757845163345, 'accumulated_logging_time': 0.2758200168609619}
I0316 13:15:17.854030 140661084796672 logging_writer.py:48] [1106] accumulated_eval_time=8833.76, accumulated_logging_time=0.27582, accumulated_submission_time=1089.16, global_step=1106, preemption_count=0, score=1089.16, test/loss=0.12837, test/num_examples=95000000, total_duration=9931.69, train/loss=0.124207, validation/loss=0.126126, validation/num_examples=83274637
I0316 13:17:19.908579 140702629090496 spec.py:321] Evaluating on the training split.
I0316 13:21:32.306721 140702629090496 spec.py:333] Evaluating on the validation split.
I0316 13:25:40.217634 140702629090496 spec.py:349] Evaluating on the test split.
I0316 13:30:39.927224 140702629090496 submission_runner.py:469] Time since start: 10853.77s, 	Step: 1225, 	{'train/loss': 0.12378487705113632, 'validation/loss': 0.12606058928828812, 'validation/num_examples': 83274637, 'test/loss': 0.12842468525551243, 'test/num_examples': 95000000, 'score': 1210.2358293533325, 'total_duration': 10853.77265381813, 'accumulated_submission_time': 1210.2358293533325, 'accumulated_eval_time': 9633.776673555374, 'accumulated_logging_time': 0.341350793838501}
I0316 13:30:39.937129 140661093189376 logging_writer.py:48] [1225] accumulated_eval_time=9633.78, accumulated_logging_time=0.341351, accumulated_submission_time=1210.24, global_step=1225, preemption_count=0, score=1210.24, test/loss=0.128425, test/num_examples=95000000, total_duration=10853.8, train/loss=0.123785, validation/loss=0.126061, validation/num_examples=83274637
I0316 13:32:40.361355 140702629090496 spec.py:321] Evaluating on the training split.
I0316 13:35:45.182398 140702629090496 spec.py:333] Evaluating on the validation split.
I0316 13:39:30.446461 140702629090496 spec.py:349] Evaluating on the test split.
I0316 13:44:43.558254 140702629090496 submission_runner.py:469] Time since start: 11697.40s, 	Step: 1347, 	{'train/loss': 0.12548079147342003, 'validation/loss': 0.1256814093186031, 'validation/num_examples': 83274637, 'test/loss': 0.12795344565690694, 'test/num_examples': 95000000, 'score': 1329.7490818500519, 'total_duration': 11697.40369796753, 'accumulated_submission_time': 1329.7490818500519, 'accumulated_eval_time': 10356.973821163177, 'accumulated_logging_time': 0.35823607444763184}
I0316 13:44:43.568508 140661084796672 logging_writer.py:48] [1347] accumulated_eval_time=10357, accumulated_logging_time=0.358236, accumulated_submission_time=1329.75, global_step=1347, preemption_count=0, score=1329.75, test/loss=0.127953, test/num_examples=95000000, total_duration=11697.4, train/loss=0.125481, validation/loss=0.125681, validation/num_examples=83274637
I0316 13:46:44.548278 140702629090496 spec.py:321] Evaluating on the training split.
I0316 13:48:47.631803 140702629090496 spec.py:333] Evaluating on the validation split.
I0316 13:51:43.409972 140702629090496 spec.py:349] Evaluating on the test split.
I0316 13:57:20.465792 140702629090496 submission_runner.py:469] Time since start: 12454.31s, 	Step: 1471, 	{'train/loss': 0.12575430116379283, 'validation/loss': 0.12576707111397176, 'validation/num_examples': 83274637, 'test/loss': 0.12802002936220672, 'test/num_examples': 95000000, 'score': 1449.8142113685608, 'total_duration': 12454.31123638153, 'accumulated_submission_time': 1449.8142113685608, 'accumulated_eval_time': 10992.891634464264, 'accumulated_logging_time': 0.37509870529174805}
I0316 13:57:20.476496 140661093189376 logging_writer.py:48] [1471] accumulated_eval_time=10992.9, accumulated_logging_time=0.375099, accumulated_submission_time=1449.81, global_step=1471, preemption_count=0, score=1449.81, test/loss=0.12802, test/num_examples=95000000, total_duration=12454.3, train/loss=0.125754, validation/loss=0.125767, validation/num_examples=83274637
I0316 13:57:26.680876 140661084796672 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.00601101, loss=0.127129
I0316 13:57:26.684113 140702629090496 submission.py:265] 1500) loss = 0.127, grad_norm = 0.006
I0316 13:59:20.997590 140702629090496 spec.py:321] Evaluating on the training split.
I0316 14:01:23.900675 140702629090496 spec.py:333] Evaluating on the validation split.
I0316 14:04:15.540995 140702629090496 spec.py:349] Evaluating on the test split.
I0316 14:09:43.751945 140702629090496 submission_runner.py:469] Time since start: 13197.60s, 	Step: 1592, 	{'train/loss': 0.12329995826552925, 'validation/loss': 0.125730390169145, 'validation/num_examples': 83274637, 'test/loss': 0.1281536034189726, 'test/num_examples': 95000000, 'score': 1569.4414608478546, 'total_duration': 13197.597396612167, 'accumulated_submission_time': 1569.4414608478546, 'accumulated_eval_time': 11615.646130561829, 'accumulated_logging_time': 0.3927338123321533}
I0316 14:09:43.762568 140661093189376 logging_writer.py:48] [1592] accumulated_eval_time=11615.6, accumulated_logging_time=0.392734, accumulated_submission_time=1569.44, global_step=1592, preemption_count=0, score=1569.44, test/loss=0.128154, test/num_examples=95000000, total_duration=13197.6, train/loss=0.1233, validation/loss=0.12573, validation/num_examples=83274637
I0316 14:11:44.767604 140702629090496 spec.py:321] Evaluating on the training split.
I0316 14:13:56.860989 140702629090496 spec.py:333] Evaluating on the validation split.
I0316 14:16:48.189040 140702629090496 spec.py:349] Evaluating on the test split.
I0316 14:23:47.064706 140702629090496 submission_runner.py:469] Time since start: 14040.91s, 	Step: 1715, 	{'train/loss': 0.12470665026475172, 'validation/loss': 0.12565270585422803, 'validation/num_examples': 83274637, 'test/loss': 0.12791637865612632, 'test/num_examples': 95000000, 'score': 1689.565319776535, 'total_duration': 14040.910097360611, 'accumulated_submission_time': 1689.565319776535, 'accumulated_eval_time': 12337.943231582642, 'accumulated_logging_time': 0.41123485565185547}
I0316 14:23:47.076442 140661084796672 logging_writer.py:48] [1715] accumulated_eval_time=12337.9, accumulated_logging_time=0.411235, accumulated_submission_time=1689.57, global_step=1715, preemption_count=0, score=1689.57, test/loss=0.127916, test/num_examples=95000000, total_duration=14040.9, train/loss=0.124707, validation/loss=0.125653, validation/num_examples=83274637
I0316 14:25:48.379797 140702629090496 spec.py:321] Evaluating on the training split.
I0316 14:27:51.237595 140702629090496 spec.py:333] Evaluating on the validation split.
I0316 14:30:45.291981 140702629090496 spec.py:349] Evaluating on the test split.
I0316 14:36:28.546943 140702629090496 submission_runner.py:469] Time since start: 14802.39s, 	Step: 1838, 	{'train/loss': 0.1252224388548464, 'validation/loss': 0.12556255251966747, 'validation/num_examples': 83274637, 'test/loss': 0.12783369584856535, 'test/num_examples': 95000000, 'score': 1809.962423324585, 'total_duration': 14802.39233660698, 'accumulated_submission_time': 1809.962423324585, 'accumulated_eval_time': 12978.11045384407, 'accumulated_logging_time': 0.43041372299194336}
I0316 14:36:28.558018 140661093189376 logging_writer.py:48] [1838] accumulated_eval_time=12978.1, accumulated_logging_time=0.430414, accumulated_submission_time=1809.96, global_step=1838, preemption_count=0, score=1809.96, test/loss=0.127834, test/num_examples=95000000, total_duration=14802.4, train/loss=0.125222, validation/loss=0.125563, validation/num_examples=83274637
I0316 14:38:29.878912 140702629090496 spec.py:321] Evaluating on the training split.
I0316 14:40:32.761414 140702629090496 spec.py:333] Evaluating on the validation split.
I0316 14:43:23.043231 140702629090496 spec.py:349] Evaluating on the test split.
I0316 14:48:55.960124 140702629090496 submission_runner.py:469] Time since start: 15549.81s, 	Step: 1962, 	{'train/loss': 0.12431994612036834, 'validation/loss': 0.12563795277707962, 'validation/num_examples': 83274637, 'test/loss': 0.12796454144656533, 'test/num_examples': 95000000, 'score': 1930.368491411209, 'total_duration': 15549.805566549301, 'accumulated_submission_time': 1930.368491411209, 'accumulated_eval_time': 13604.191866397858, 'accumulated_logging_time': 0.4485151767730713}
I0316 14:48:55.970321 140661084796672 logging_writer.py:48] [1962] accumulated_eval_time=13604.2, accumulated_logging_time=0.448515, accumulated_submission_time=1930.37, global_step=1962, preemption_count=0, score=1930.37, test/loss=0.127965, test/num_examples=95000000, total_duration=15549.8, train/loss=0.12432, validation/loss=0.125638, validation/num_examples=83274637
I0316 14:49:13.900588 140661093189376 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.0246933, loss=0.117286
I0316 14:49:13.903683 140702629090496 submission.py:265] 2000) loss = 0.117, grad_norm = 0.025
I0316 14:50:57.031563 140702629090496 spec.py:321] Evaluating on the training split.
I0316 14:53:00.479268 140702629090496 spec.py:333] Evaluating on the validation split.
I0316 14:55:53.533447 140702629090496 spec.py:349] Evaluating on the test split.
I0316 15:01:23.648501 140702629090496 submission_runner.py:469] Time since start: 16297.49s, 	Step: 2081, 	{'train/loss': 0.12487185268269674, 'validation/loss': 0.12561407526623042, 'validation/num_examples': 83274637, 'test/loss': 0.12791434818014846, 'test/num_examples': 95000000, 'score': 2050.523274898529, 'total_duration': 16297.493948936462, 'accumulated_submission_time': 2050.523274898529, 'accumulated_eval_time': 14230.808935880661, 'accumulated_logging_time': 0.465512752532959}
I0316 15:01:23.659662 140661084796672 logging_writer.py:48] [2081] accumulated_eval_time=14230.8, accumulated_logging_time=0.465513, accumulated_submission_time=2050.52, global_step=2081, preemption_count=0, score=2050.52, test/loss=0.127914, test/num_examples=95000000, total_duration=16297.5, train/loss=0.124872, validation/loss=0.125614, validation/num_examples=83274637
I0316 15:03:24.538083 140702629090496 spec.py:321] Evaluating on the training split.
I0316 15:05:27.916734 140702629090496 spec.py:333] Evaluating on the validation split.
I0316 15:08:22.072196 140702629090496 spec.py:349] Evaluating on the test split.
I0316 15:14:06.048943 140702629090496 submission_runner.py:469] Time since start: 17059.89s, 	Step: 2210, 	{'train/loss': 0.12451703663062126, 'validation/loss': 0.12535060022279168, 'validation/num_examples': 83274637, 'test/loss': 0.1275554156149613, 'test/num_examples': 95000000, 'score': 2170.494159936905, 'total_duration': 17059.894397497177, 'accumulated_submission_time': 2170.494159936905, 'accumulated_eval_time': 14872.319916009903, 'accumulated_logging_time': 0.4838600158691406}
I0316 15:14:06.059075 140661093189376 logging_writer.py:48] [2210] accumulated_eval_time=14872.3, accumulated_logging_time=0.48386, accumulated_submission_time=2170.49, global_step=2210, preemption_count=0, score=2170.49, test/loss=0.127555, test/num_examples=95000000, total_duration=17059.9, train/loss=0.124517, validation/loss=0.125351, validation/num_examples=83274637
I0316 15:16:06.970154 140702629090496 spec.py:321] Evaluating on the training split.
I0316 15:18:10.903951 140702629090496 spec.py:333] Evaluating on the validation split.
I0316 15:20:56.789420 140702629090496 spec.py:349] Evaluating on the test split.
I0316 15:26:32.365647 140702629090496 submission_runner.py:469] Time since start: 17806.21s, 	Step: 2330, 	{'train/loss': 0.12439698996362222, 'validation/loss': 0.12518763557844423, 'validation/num_examples': 83274637, 'test/loss': 0.12757364848407945, 'test/num_examples': 95000000, 'score': 2290.490604877472, 'total_duration': 17806.21106648445, 'accumulated_submission_time': 2290.490604877472, 'accumulated_eval_time': 15497.715490818024, 'accumulated_logging_time': 0.5006890296936035}
I0316 15:26:32.377239 140661084796672 logging_writer.py:48] [2330] accumulated_eval_time=15497.7, accumulated_logging_time=0.500689, accumulated_submission_time=2290.49, global_step=2330, preemption_count=0, score=2290.49, test/loss=0.127574, test/num_examples=95000000, total_duration=17806.2, train/loss=0.124397, validation/loss=0.125188, validation/num_examples=83274637
I0316 15:28:34.051811 140702629090496 spec.py:321] Evaluating on the training split.
I0316 15:30:37.394444 140702629090496 spec.py:333] Evaluating on the validation split.
I0316 15:33:27.378730 140702629090496 spec.py:349] Evaluating on the test split.
I0316 15:39:16.723865 140702629090496 submission_runner.py:469] Time since start: 18570.57s, 	Step: 2455, 	{'train/loss': 0.12416529124888503, 'validation/loss': 0.12523676962702573, 'validation/num_examples': 83274637, 'test/loss': 0.12759205634492574, 'test/num_examples': 95000000, 'score': 2411.2543354034424, 'total_duration': 18570.56929206848, 'accumulated_submission_time': 2411.2543354034424, 'accumulated_eval_time': 16140.387684106827, 'accumulated_logging_time': 0.5200259685516357}
I0316 15:39:16.734696 140661093189376 logging_writer.py:48] [2455] accumulated_eval_time=16140.4, accumulated_logging_time=0.520026, accumulated_submission_time=2411.25, global_step=2455, preemption_count=0, score=2411.25, test/loss=0.127592, test/num_examples=95000000, total_duration=18570.6, train/loss=0.124165, validation/loss=0.125237, validation/num_examples=83274637
I0316 15:39:43.017896 140661084796672 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.00564656, loss=0.119336
I0316 15:39:43.021499 140702629090496 submission.py:265] 2500) loss = 0.119, grad_norm = 0.006
I0316 15:41:18.848869 140702629090496 spec.py:321] Evaluating on the training split.
I0316 15:43:22.108126 140702629090496 spec.py:333] Evaluating on the validation split.
I0316 15:46:13.363570 140702629090496 spec.py:349] Evaluating on the test split.
I0316 15:51:46.286203 140702629090496 submission_runner.py:469] Time since start: 19320.13s, 	Step: 2575, 	{'train/loss': 0.12424639102888516, 'validation/loss': 0.1250569067993509, 'validation/num_examples': 83274637, 'test/loss': 0.12737227748220342, 'test/num_examples': 95000000, 'score': 2532.5077307224274, 'total_duration': 19320.131658792496, 'accumulated_submission_time': 2532.5077307224274, 'accumulated_eval_time': 16767.825222730637, 'accumulated_logging_time': 0.5384321212768555}
I0316 15:51:46.296647 140661093189376 logging_writer.py:48] [2575] accumulated_eval_time=16767.8, accumulated_logging_time=0.538432, accumulated_submission_time=2532.51, global_step=2575, preemption_count=0, score=2532.51, test/loss=0.127372, test/num_examples=95000000, total_duration=19320.1, train/loss=0.124246, validation/loss=0.125057, validation/num_examples=83274637
I0316 15:53:46.754543 140702629090496 spec.py:321] Evaluating on the training split.
I0316 15:55:49.929456 140702629090496 spec.py:333] Evaluating on the validation split.
I0316 15:58:40.984889 140702629090496 spec.py:349] Evaluating on the test split.
I0316 16:04:24.219741 140702629090496 submission_runner.py:469] Time since start: 20078.07s, 	Step: 2695, 	{'train/loss': 0.12166257764561095, 'validation/loss': 0.1251845160291965, 'validation/num_examples': 83274637, 'test/loss': 0.1276684805275365, 'test/num_examples': 95000000, 'score': 2652.070769071579, 'total_duration': 20078.06515789032, 'accumulated_submission_time': 2652.070769071579, 'accumulated_eval_time': 17405.29059100151, 'accumulated_logging_time': 0.5559756755828857}
I0316 16:04:24.231825 140661084796672 logging_writer.py:48] [2695] accumulated_eval_time=17405.3, accumulated_logging_time=0.555976, accumulated_submission_time=2652.07, global_step=2695, preemption_count=0, score=2652.07, test/loss=0.127668, test/num_examples=95000000, total_duration=20078.1, train/loss=0.121663, validation/loss=0.125185, validation/num_examples=83274637
I0316 16:06:24.929273 140702629090496 spec.py:321] Evaluating on the training split.
I0316 16:08:27.658959 140702629090496 spec.py:333] Evaluating on the validation split.
I0316 16:11:21.590539 140702629090496 spec.py:349] Evaluating on the test split.
I0316 16:17:16.765623 140702629090496 submission_runner.py:469] Time since start: 20850.61s, 	Step: 2815, 	{'train/loss': 0.12469269409131704, 'validation/loss': 0.12500161620978292, 'validation/num_examples': 83274637, 'test/loss': 0.12745133505646555, 'test/num_examples': 95000000, 'score': 2771.846065759659, 'total_duration': 20850.611068487167, 'accumulated_submission_time': 2771.846065759659, 'accumulated_eval_time': 18057.12704205513, 'accumulated_logging_time': 0.6204884052276611}
I0316 16:17:16.776458 140661093189376 logging_writer.py:48] [2815] accumulated_eval_time=18057.1, accumulated_logging_time=0.620488, accumulated_submission_time=2771.85, global_step=2815, preemption_count=0, score=2771.85, test/loss=0.127451, test/num_examples=95000000, total_duration=20850.6, train/loss=0.124693, validation/loss=0.125002, validation/num_examples=83274637
I0316 16:19:17.988504 140702629090496 spec.py:321] Evaluating on the training split.
I0316 16:21:20.745575 140702629090496 spec.py:333] Evaluating on the validation split.
I0316 16:24:12.990167 140702629090496 spec.py:349] Evaluating on the test split.
I0316 16:30:03.651085 140702629090496 submission_runner.py:469] Time since start: 21617.50s, 	Step: 2936, 	{'train/loss': 0.12415228537305825, 'validation/loss': 0.12510976263608323, 'validation/num_examples': 83274637, 'test/loss': 0.12757824414809377, 'test/num_examples': 95000000, 'score': 2892.1906571388245, 'total_duration': 21617.496493577957, 'accumulated_submission_time': 2892.1906571388245, 'accumulated_eval_time': 18702.789763212204, 'accumulated_logging_time': 0.638481616973877}
I0316 16:30:03.662748 140661084796672 logging_writer.py:48] [2936] accumulated_eval_time=18702.8, accumulated_logging_time=0.638482, accumulated_submission_time=2892.19, global_step=2936, preemption_count=0, score=2892.19, test/loss=0.127578, test/num_examples=95000000, total_duration=21617.5, train/loss=0.124152, validation/loss=0.12511, validation/num_examples=83274637
I0316 16:30:54.649495 140661093189376 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.0109244, loss=0.124585
I0316 16:30:54.653261 140702629090496 submission.py:265] 3000) loss = 0.125, grad_norm = 0.011
I0316 16:32:04.487192 140702629090496 spec.py:321] Evaluating on the training split.
I0316 16:34:07.588325 140702629090496 spec.py:333] Evaluating on the validation split.
I0316 16:36:55.273201 140702629090496 spec.py:349] Evaluating on the test split.
I0316 16:42:46.949795 140702629090496 submission_runner.py:469] Time since start: 22380.80s, 	Step: 3054, 	{'train/loss': 0.12262518637363448, 'validation/loss': 0.12482498632283875, 'validation/num_examples': 83274637, 'test/loss': 0.12712922440277902, 'test/num_examples': 95000000, 'score': 3012.10649061203, 'total_duration': 22380.7952439785, 'accumulated_submission_time': 3012.10649061203, 'accumulated_eval_time': 19345.252594709396, 'accumulated_logging_time': 0.6572251319885254}
I0316 16:42:46.960975 140661084796672 logging_writer.py:48] [3054] accumulated_eval_time=19345.3, accumulated_logging_time=0.657225, accumulated_submission_time=3012.11, global_step=3054, preemption_count=0, score=3012.11, test/loss=0.127129, test/num_examples=95000000, total_duration=22380.8, train/loss=0.122625, validation/loss=0.124825, validation/num_examples=83274637
I0316 16:44:47.576719 140702629090496 spec.py:321] Evaluating on the training split.
I0316 16:46:51.028596 140702629090496 spec.py:333] Evaluating on the validation split.
I0316 16:49:44.296816 140702629090496 spec.py:349] Evaluating on the test split.
I0316 16:55:17.192266 140702629090496 submission_runner.py:469] Time since start: 23131.04s, 	Step: 3177, 	{'train/loss': 0.12406284202453824, 'validation/loss': 0.12486488107742849, 'validation/num_examples': 83274637, 'test/loss': 0.12735339092632092, 'test/num_examples': 95000000, 'score': 3131.811572790146, 'total_duration': 23131.037689447403, 'accumulated_submission_time': 3131.811572790146, 'accumulated_eval_time': 19974.868231534958, 'accumulated_logging_time': 0.6752736568450928}
I0316 16:55:17.203366 140661093189376 logging_writer.py:48] [3177] accumulated_eval_time=19974.9, accumulated_logging_time=0.675274, accumulated_submission_time=3131.81, global_step=3177, preemption_count=0, score=3131.81, test/loss=0.127353, test/num_examples=95000000, total_duration=23131, train/loss=0.124063, validation/loss=0.124865, validation/num_examples=83274637
I0316 16:57:17.673018 140702629090496 spec.py:321] Evaluating on the training split.
I0316 16:59:21.281089 140702629090496 spec.py:333] Evaluating on the validation split.
I0316 17:02:12.241790 140702629090496 spec.py:349] Evaluating on the test split.
I0316 17:07:53.438491 140702629090496 submission_runner.py:469] Time since start: 23887.28s, 	Step: 3299, 	{'train/loss': 0.12362515559301261, 'validation/loss': 0.12484479089955423, 'validation/num_examples': 83274637, 'test/loss': 0.12728781454937582, 'test/num_examples': 95000000, 'score': 3251.3215448856354, 'total_duration': 23887.28389453888, 'accumulated_submission_time': 3251.3215448856354, 'accumulated_eval_time': 20610.633850097656, 'accumulated_logging_time': 0.7231490612030029}
I0316 17:07:53.450634 140661084796672 logging_writer.py:48] [3299] accumulated_eval_time=20610.6, accumulated_logging_time=0.723149, accumulated_submission_time=3251.32, global_step=3299, preemption_count=0, score=3251.32, test/loss=0.127288, test/num_examples=95000000, total_duration=23887.3, train/loss=0.123625, validation/loss=0.124845, validation/num_examples=83274637
I0316 17:09:54.689699 140702629090496 spec.py:321] Evaluating on the training split.
I0316 17:11:57.882354 140702629090496 spec.py:333] Evaluating on the validation split.
I0316 17:14:51.549290 140702629090496 spec.py:349] Evaluating on the test split.
I0316 17:20:41.251032 140702629090496 submission_runner.py:469] Time since start: 24655.10s, 	Step: 3420, 	{'train/loss': 0.12185102077092907, 'validation/loss': 0.12480690702016828, 'validation/num_examples': 83274637, 'test/loss': 0.12722606708390086, 'test/num_examples': 95000000, 'score': 3371.685770511627, 'total_duration': 24655.09649705887, 'accumulated_submission_time': 3371.685770511627, 'accumulated_eval_time': 21257.195376634598, 'accumulated_logging_time': 0.7423291206359863}
I0316 17:20:41.262043 140661093189376 logging_writer.py:48] [3420] accumulated_eval_time=21257.2, accumulated_logging_time=0.742329, accumulated_submission_time=3371.69, global_step=3420, preemption_count=0, score=3371.69, test/loss=0.127226, test/num_examples=95000000, total_duration=24655.1, train/loss=0.121851, validation/loss=0.124807, validation/num_examples=83274637
I0316 17:21:47.947839 140661084796672 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.00659211, loss=0.122079
I0316 17:21:47.951096 140702629090496 submission.py:265] 3500) loss = 0.122, grad_norm = 0.007
I0316 17:22:43.299426 140702629090496 spec.py:321] Evaluating on the training split.
I0316 17:24:46.212711 140702629090496 spec.py:333] Evaluating on the validation split.
I0316 17:27:36.893226 140702629090496 spec.py:349] Evaluating on the test split.
I0316 17:33:22.193039 140702629090496 submission_runner.py:469] Time since start: 25416.04s, 	Step: 3548, 	{'train/loss': 0.12141043000079335, 'validation/loss': 0.12493563601487752, 'validation/num_examples': 83274637, 'test/loss': 0.1273770106413992, 'test/num_examples': 95000000, 'score': 3492.819480419159, 'total_duration': 25416.03848528862, 'accumulated_submission_time': 3492.819480419159, 'accumulated_eval_time': 21896.089182138443, 'accumulated_logging_time': 0.7598333358764648}
I0316 17:33:22.204305 140661093189376 logging_writer.py:48] [3548] accumulated_eval_time=21896.1, accumulated_logging_time=0.759833, accumulated_submission_time=3492.82, global_step=3548, preemption_count=0, score=3492.82, test/loss=0.127377, test/num_examples=95000000, total_duration=25416, train/loss=0.12141, validation/loss=0.124936, validation/num_examples=83274637
I0316 17:35:22.993991 140702629090496 spec.py:321] Evaluating on the training split.
I0316 17:37:25.927322 140702629090496 spec.py:333] Evaluating on the validation split.
I0316 17:40:19.549243 140702629090496 spec.py:349] Evaluating on the test split.
I0316 17:46:10.317706 140702629090496 submission_runner.py:469] Time since start: 26184.16s, 	Step: 3676, 	{'train/loss': 0.12383861687993908, 'validation/loss': 0.12482832716534584, 'validation/num_examples': 83274637, 'test/loss': 0.12724863797639546, 'test/num_examples': 95000000, 'score': 3612.7181606292725, 'total_duration': 26184.163151025772, 'accumulated_submission_time': 3612.7181606292725, 'accumulated_eval_time': 22543.41310787201, 'accumulated_logging_time': 0.777552604675293}
I0316 17:46:10.329483 140661084796672 logging_writer.py:48] [3676] accumulated_eval_time=22543.4, accumulated_logging_time=0.777553, accumulated_submission_time=3612.72, global_step=3676, preemption_count=0, score=3612.72, test/loss=0.127249, test/num_examples=95000000, total_duration=26184.2, train/loss=0.123839, validation/loss=0.124828, validation/num_examples=83274637
I0316 17:48:11.491889 140702629090496 spec.py:321] Evaluating on the training split.
I0316 17:50:14.352732 140702629090496 spec.py:333] Evaluating on the validation split.
I0316 17:53:09.088909 140702629090496 spec.py:349] Evaluating on the test split.
I0316 17:58:48.410288 140702629090496 submission_runner.py:469] Time since start: 26942.26s, 	Step: 3798, 	{'train/loss': 0.12318282376287001, 'validation/loss': 0.12518009998954852, 'validation/num_examples': 83274637, 'test/loss': 0.12757547708932976, 'test/num_examples': 95000000, 'score': 3732.9887068271637, 'total_duration': 26942.25573325157, 'accumulated_submission_time': 3732.9887068271637, 'accumulated_eval_time': 23180.331609010696, 'accumulated_logging_time': 0.8071057796478271}
I0316 17:58:48.421382 140661093189376 logging_writer.py:48] [3798] accumulated_eval_time=23180.3, accumulated_logging_time=0.807106, accumulated_submission_time=3732.99, global_step=3798, preemption_count=0, score=3732.99, test/loss=0.127575, test/num_examples=95000000, total_duration=26942.3, train/loss=0.123183, validation/loss=0.12518, validation/num_examples=83274637
I0316 18:00:50.467261 140702629090496 spec.py:321] Evaluating on the training split.
I0316 18:02:54.933277 140702629090496 spec.py:333] Evaluating on the validation split.
I0316 18:05:46.578639 140702629090496 spec.py:349] Evaluating on the test split.
I0316 18:11:17.203065 140702629090496 submission_runner.py:469] Time since start: 27691.05s, 	Step: 3920, 	{'train/loss': 0.12345981750131231, 'validation/loss': 0.12461510392774308, 'validation/num_examples': 83274637, 'test/loss': 0.12694141767722683, 'test/num_examples': 95000000, 'score': 3854.1386528015137, 'total_duration': 27691.048473358154, 'accumulated_submission_time': 3854.1386528015137, 'accumulated_eval_time': 23807.06750369072, 'accumulated_logging_time': 0.8250265121459961}
I0316 18:11:17.214811 140661084796672 logging_writer.py:48] [3920] accumulated_eval_time=23807.1, accumulated_logging_time=0.825027, accumulated_submission_time=3854.14, global_step=3920, preemption_count=0, score=3854.14, test/loss=0.126941, test/num_examples=95000000, total_duration=27691, train/loss=0.12346, validation/loss=0.124615, validation/num_examples=83274637
I0316 18:12:27.445786 140661093189376 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.00542054, loss=0.126657
I0316 18:12:27.449195 140702629090496 submission.py:265] 4000) loss = 0.127, grad_norm = 0.005
I0316 18:13:18.142321 140702629090496 spec.py:321] Evaluating on the training split.
I0316 18:15:22.648821 140702629090496 spec.py:333] Evaluating on the validation split.
I0316 18:18:17.195220 140702629090496 spec.py:349] Evaluating on the test split.
I0316 18:23:23.733681 140702629090496 submission_runner.py:469] Time since start: 28417.58s, 	Step: 4038, 	{'train/loss': 0.12296825992523379, 'validation/loss': 0.12474789349031658, 'validation/num_examples': 83274637, 'test/loss': 0.12713553524515253, 'test/num_examples': 95000000, 'score': 3974.1648695468903, 'total_duration': 28417.579128026962, 'accumulated_submission_time': 3974.1648695468903, 'accumulated_eval_time': 24412.659071922302, 'accumulated_logging_time': 0.8443238735198975}
I0316 18:23:23.744533 140661084796672 logging_writer.py:48] [4038] accumulated_eval_time=24412.7, accumulated_logging_time=0.844324, accumulated_submission_time=3974.16, global_step=4038, preemption_count=0, score=3974.16, test/loss=0.127136, test/num_examples=95000000, total_duration=28417.6, train/loss=0.122968, validation/loss=0.124748, validation/num_examples=83274637
I0316 18:25:24.386374 140702629090496 spec.py:321] Evaluating on the training split.
I0316 18:27:27.316823 140702629090496 spec.py:333] Evaluating on the validation split.
I0316 18:30:18.863560 140702629090496 spec.py:349] Evaluating on the test split.
I0316 18:35:24.132150 140702629090496 submission_runner.py:469] Time since start: 29137.98s, 	Step: 4156, 	{'train/loss': 0.12415823455360912, 'validation/loss': 0.12463490666730334, 'validation/num_examples': 83274637, 'test/loss': 0.12696073825061197, 'test/num_examples': 95000000, 'score': 4093.932314157486, 'total_duration': 29137.977610349655, 'accumulated_submission_time': 4093.932314157486, 'accumulated_eval_time': 25012.404972076416, 'accumulated_logging_time': 0.8629729747772217}
I0316 18:35:24.143048 140661093189376 logging_writer.py:48] [4156] accumulated_eval_time=25012.4, accumulated_logging_time=0.862973, accumulated_submission_time=4093.93, global_step=4156, preemption_count=0, score=4093.93, test/loss=0.126961, test/num_examples=95000000, total_duration=29138, train/loss=0.124158, validation/loss=0.124635, validation/num_examples=83274637
I0316 18:37:25.240245 140702629090496 spec.py:321] Evaluating on the training split.
I0316 18:39:27.933513 140702629090496 spec.py:333] Evaluating on the validation split.
I0316 18:42:22.687760 140702629090496 spec.py:349] Evaluating on the test split.
I0316 18:47:30.737354 140702629090496 submission_runner.py:469] Time since start: 29864.58s, 	Step: 4279, 	{'train/loss': 0.12156910188613376, 'validation/loss': 0.12452666402019893, 'validation/num_examples': 83274637, 'test/loss': 0.12688028505192808, 'test/num_examples': 95000000, 'score': 4214.08628153801, 'total_duration': 29864.582788944244, 'accumulated_submission_time': 4214.08628153801, 'accumulated_eval_time': 25617.902287006378, 'accumulated_logging_time': 0.9446344375610352}
I0316 18:47:30.762305 140661084796672 logging_writer.py:48] [4279] accumulated_eval_time=25617.9, accumulated_logging_time=0.944634, accumulated_submission_time=4214.09, global_step=4279, preemption_count=0, score=4214.09, test/loss=0.12688, test/num_examples=95000000, total_duration=29864.6, train/loss=0.121569, validation/loss=0.124527, validation/num_examples=83274637
I0316 18:49:31.240741 140702629090496 spec.py:321] Evaluating on the training split.
I0316 18:51:34.332653 140702629090496 spec.py:333] Evaluating on the validation split.
I0316 18:54:23.719346 140702629090496 spec.py:349] Evaluating on the test split.
I0316 18:59:32.309963 140702629090496 submission_runner.py:469] Time since start: 30586.16s, 	Step: 4404, 	{'train/loss': 0.12358754758907953, 'validation/loss': 0.12464335414162456, 'validation/num_examples': 83274637, 'test/loss': 0.1269981241361919, 'test/num_examples': 95000000, 'score': 4333.651380777359, 'total_duration': 30586.155426502228, 'accumulated_submission_time': 4333.651380777359, 'accumulated_eval_time': 26218.971644878387, 'accumulated_logging_time': 0.9763240814208984}
I0316 18:59:32.321497 140661093189376 logging_writer.py:48] [4404] accumulated_eval_time=26219, accumulated_logging_time=0.976324, accumulated_submission_time=4333.65, global_step=4404, preemption_count=0, score=4333.65, test/loss=0.126998, test/num_examples=95000000, total_duration=30586.2, train/loss=0.123588, validation/loss=0.124643, validation/num_examples=83274637
I0316 19:00:59.579102 140661084796672 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.00522505, loss=0.120815
I0316 19:00:59.582823 140702629090496 submission.py:265] 4500) loss = 0.121, grad_norm = 0.005
I0316 19:01:33.386161 140702629090496 spec.py:321] Evaluating on the training split.
I0316 19:03:36.431434 140702629090496 spec.py:333] Evaluating on the validation split.
I0316 19:06:29.831337 140702629090496 spec.py:349] Evaluating on the test split.
I0316 19:11:29.290216 140702629090496 submission_runner.py:469] Time since start: 31303.14s, 	Step: 4529, 	{'train/loss': 0.12320134710025255, 'validation/loss': 0.12457518142123, 'validation/num_examples': 83274637, 'test/loss': 0.12692401949852392, 'test/num_examples': 95000000, 'score': 4453.7957055568695, 'total_duration': 31303.135671138763, 'accumulated_submission_time': 4453.7957055568695, 'accumulated_eval_time': 26814.875884771347, 'accumulated_logging_time': 0.9947173595428467}
I0316 19:11:29.301751 140661093189376 logging_writer.py:48] [4529] accumulated_eval_time=26814.9, accumulated_logging_time=0.994717, accumulated_submission_time=4453.8, global_step=4529, preemption_count=0, score=4453.8, test/loss=0.126924, test/num_examples=95000000, total_duration=31303.1, train/loss=0.123201, validation/loss=0.124575, validation/num_examples=83274637
I0316 19:13:30.478492 140702629090496 spec.py:321] Evaluating on the training split.
I0316 19:15:34.372555 140702629090496 spec.py:333] Evaluating on the validation split.
I0316 19:18:28.349468 140702629090496 spec.py:349] Evaluating on the test split.
I0316 19:23:33.324778 140702629090496 submission_runner.py:469] Time since start: 32027.17s, 	Step: 4657, 	{'train/loss': 0.12118336303190193, 'validation/loss': 0.12452832958333246, 'validation/num_examples': 83274637, 'test/loss': 0.12687285996832598, 'test/num_examples': 95000000, 'score': 4574.034925699234, 'total_duration': 32027.17025089264, 'accumulated_submission_time': 4574.034925699234, 'accumulated_eval_time': 27417.722332000732, 'accumulated_logging_time': 1.013540506362915}
I0316 19:23:33.335930 140661084796672 logging_writer.py:48] [4657] accumulated_eval_time=27417.7, accumulated_logging_time=1.01354, accumulated_submission_time=4574.03, global_step=4657, preemption_count=0, score=4574.03, test/loss=0.126873, test/num_examples=95000000, total_duration=32027.2, train/loss=0.121183, validation/loss=0.124528, validation/num_examples=83274637
I0316 19:25:33.813309 140702629090496 spec.py:321] Evaluating on the training split.
I0316 19:27:37.013296 140702629090496 spec.py:333] Evaluating on the validation split.
I0316 19:30:28.122147 140702629090496 spec.py:349] Evaluating on the test split.
I0316 19:35:32.737003 140702629090496 submission_runner.py:469] Time since start: 32746.58s, 	Step: 4781, 	{'train/loss': 0.12298070345532545, 'validation/loss': 0.12452254336996096, 'validation/num_examples': 83274637, 'test/loss': 0.12690960756631148, 'test/num_examples': 95000000, 'score': 4693.625509738922, 'total_duration': 32746.582434654236, 'accumulated_submission_time': 4693.625509738922, 'accumulated_eval_time': 28016.646154403687, 'accumulated_logging_time': 1.0324487686157227}
I0316 19:35:32.748338 140661093189376 logging_writer.py:48] [4781] accumulated_eval_time=28016.6, accumulated_logging_time=1.03245, accumulated_submission_time=4693.63, global_step=4781, preemption_count=0, score=4693.63, test/loss=0.12691, test/num_examples=95000000, total_duration=32746.6, train/loss=0.122981, validation/loss=0.124523, validation/num_examples=83274637
I0316 19:37:33.449631 140702629090496 spec.py:321] Evaluating on the training split.
I0316 19:39:36.464154 140702629090496 spec.py:333] Evaluating on the validation split.
I0316 19:42:26.296365 140702629090496 spec.py:349] Evaluating on the test split.
I0316 19:47:50.910327 140702629090496 submission_runner.py:469] Time since start: 33484.76s, 	Step: 4906, 	{'train/loss': 0.12259106093009166, 'validation/loss': 0.12442503001757221, 'validation/num_examples': 83274637, 'test/loss': 0.1267324877390008, 'test/num_examples': 95000000, 'score': 4813.428468465805, 'total_duration': 33484.75571727753, 'accumulated_submission_time': 4813.428468465805, 'accumulated_eval_time': 28634.106946706772, 'accumulated_logging_time': 1.0872466564178467}
I0316 19:47:50.988205 140661084796672 logging_writer.py:48] [4906] accumulated_eval_time=28634.1, accumulated_logging_time=1.08725, accumulated_submission_time=4813.43, global_step=4906, preemption_count=0, score=4813.43, test/loss=0.126732, test/num_examples=95000000, total_duration=33484.8, train/loss=0.122591, validation/loss=0.124425, validation/num_examples=83274637
I0316 19:49:13.025435 140661093189376 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.00973362, loss=0.12642
I0316 19:49:13.029436 140702629090496 submission.py:265] 5000) loss = 0.126, grad_norm = 0.010
I0316 19:49:51.456072 140702629090496 spec.py:321] Evaluating on the training split.
I0316 19:51:54.274831 140702629090496 spec.py:333] Evaluating on the validation split.
I0316 19:54:47.526735 140702629090496 spec.py:349] Evaluating on the test split.
I0316 20:00:04.728430 140702629090496 submission_runner.py:469] Time since start: 34218.57s, 	Step: 5029, 	{'train/loss': 0.12241567427887208, 'validation/loss': 0.12432338831937002, 'validation/num_examples': 83274637, 'test/loss': 0.1266615741187246, 'test/num_examples': 95000000, 'score': 4933.03212594986, 'total_duration': 34218.5738825798, 'accumulated_submission_time': 4933.03212594986, 'accumulated_eval_time': 29247.379501581192, 'accumulated_logging_time': 1.1794037818908691}
I0316 20:00:04.739567 140661084796672 logging_writer.py:48] [5029] accumulated_eval_time=29247.4, accumulated_logging_time=1.1794, accumulated_submission_time=4933.03, global_step=5029, preemption_count=0, score=4933.03, test/loss=0.126662, test/num_examples=95000000, total_duration=34218.6, train/loss=0.122416, validation/loss=0.124323, validation/num_examples=83274637
I0316 20:02:05.842626 140702629090496 spec.py:321] Evaluating on the training split.
I0316 20:04:09.464611 140702629090496 spec.py:333] Evaluating on the validation split.
I0316 20:06:58.293146 140702629090496 spec.py:349] Evaluating on the test split.
I0316 20:12:03.271417 140702629090496 submission_runner.py:469] Time since start: 34937.12s, 	Step: 5151, 	{'train/loss': 0.12123211520200347, 'validation/loss': 0.12424395430648187, 'validation/num_examples': 83274637, 'test/loss': 0.1265959284518995, 'test/num_examples': 95000000, 'score': 5053.287846088409, 'total_duration': 34937.1168653965, 'accumulated_submission_time': 5053.287846088409, 'accumulated_eval_time': 29844.808381557465, 'accumulated_logging_time': 1.1974542140960693}
I0316 20:12:03.282499 140661093189376 logging_writer.py:48] [5151] accumulated_eval_time=29844.8, accumulated_logging_time=1.19745, accumulated_submission_time=5053.29, global_step=5151, preemption_count=0, score=5053.29, test/loss=0.126596, test/num_examples=95000000, total_duration=34937.1, train/loss=0.121232, validation/loss=0.124244, validation/num_examples=83274637
I0316 20:14:04.354456 140702629090496 spec.py:321] Evaluating on the training split.
I0316 20:16:07.438917 140702629090496 spec.py:333] Evaluating on the validation split.
I0316 20:19:00.433011 140702629090496 spec.py:349] Evaluating on the test split.
I0316 20:24:06.422576 140702629090496 submission_runner.py:469] Time since start: 35660.27s, 	Step: 5269, 	{'train/loss': 0.12509037689894903, 'validation/loss': 0.12426798424583148, 'validation/num_examples': 83274637, 'test/loss': 0.12659053121390093, 'test/num_examples': 95000000, 'score': 5173.468491077423, 'total_duration': 35660.26803445816, 'accumulated_submission_time': 5173.468491077423, 'accumulated_eval_time': 30446.876648902893, 'accumulated_logging_time': 1.215188980102539}
I0316 20:24:06.433541 140661084796672 logging_writer.py:48] [5269] accumulated_eval_time=30446.9, accumulated_logging_time=1.21519, accumulated_submission_time=5173.47, global_step=5269, preemption_count=0, score=5173.47, test/loss=0.126591, test/num_examples=95000000, total_duration=35660.3, train/loss=0.12509, validation/loss=0.124268, validation/num_examples=83274637
I0316 20:26:07.521197 140702629090496 spec.py:321] Evaluating on the training split.
I0316 20:28:10.615652 140702629090496 spec.py:333] Evaluating on the validation split.
I0316 20:31:01.759426 140702629090496 spec.py:349] Evaluating on the test split.
I0316 20:36:12.070091 140702629090496 submission_runner.py:469] Time since start: 36385.92s, 	Step: 5393, 	{'train/loss': 0.12178219871734018, 'validation/loss': 0.12432358432983717, 'validation/num_examples': 83274637, 'test/loss': 0.1266910321995384, 'test/num_examples': 95000000, 'score': 5293.621126651764, 'total_duration': 36385.915556907654, 'accumulated_submission_time': 5293.621126651764, 'accumulated_eval_time': 31051.425617933273, 'accumulated_logging_time': 1.2968759536743164}
I0316 20:36:12.082068 140661093189376 logging_writer.py:48] [5393] accumulated_eval_time=31051.4, accumulated_logging_time=1.29688, accumulated_submission_time=5293.62, global_step=5393, preemption_count=0, score=5293.62, test/loss=0.126691, test/num_examples=95000000, total_duration=36385.9, train/loss=0.121782, validation/loss=0.124324, validation/num_examples=83274637
I0316 20:37:58.237431 140661084796672 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.007953, loss=0.11924
I0316 20:37:58.240856 140702629090496 submission.py:265] 5500) loss = 0.119, grad_norm = 0.008
I0316 20:38:13.861686 140702629090496 spec.py:321] Evaluating on the training split.
I0316 20:40:17.625320 140702629090496 spec.py:333] Evaluating on the validation split.
I0316 20:43:11.502986 140702629090496 spec.py:349] Evaluating on the test split.
I0316 20:48:19.900947 140702629090496 submission_runner.py:469] Time since start: 37113.75s, 	Step: 5513, 	{'train/loss': 0.12355015100013467, 'validation/loss': 0.12422394327538905, 'validation/num_examples': 83274637, 'test/loss': 0.12653632827449598, 'test/num_examples': 95000000, 'score': 5414.488468408585, 'total_duration': 37113.74638748169, 'accumulated_submission_time': 5414.488468408585, 'accumulated_eval_time': 31657.4650182724, 'accumulated_logging_time': 1.316572904586792}
I0316 20:48:19.912207 140661093189376 logging_writer.py:48] [5513] accumulated_eval_time=31657.5, accumulated_logging_time=1.31657, accumulated_submission_time=5414.49, global_step=5513, preemption_count=0, score=5414.49, test/loss=0.126536, test/num_examples=95000000, total_duration=37113.7, train/loss=0.12355, validation/loss=0.124224, validation/num_examples=83274637
I0316 20:50:20.929454 140702629090496 spec.py:321] Evaluating on the training split.
I0316 20:52:23.796862 140702629090496 spec.py:333] Evaluating on the validation split.
I0316 20:55:15.848093 140702629090496 spec.py:349] Evaluating on the test split.
I0316 21:00:16.355450 140702629090496 submission_runner.py:469] Time since start: 37830.20s, 	Step: 5635, 	{'train/loss': 0.11979429599703144, 'validation/loss': 0.12427267291697748, 'validation/num_examples': 83274637, 'test/loss': 0.12664412569961547, 'test/num_examples': 95000000, 'score': 5534.580418348312, 'total_duration': 37830.20088362694, 'accumulated_submission_time': 5534.580418348312, 'accumulated_eval_time': 32252.891203165054, 'accumulated_logging_time': 1.3341755867004395}
I0316 21:00:16.367135 140661084796672 logging_writer.py:48] [5635] accumulated_eval_time=32252.9, accumulated_logging_time=1.33418, accumulated_submission_time=5534.58, global_step=5635, preemption_count=0, score=5534.58, test/loss=0.126644, test/num_examples=95000000, total_duration=37830.2, train/loss=0.119794, validation/loss=0.124273, validation/num_examples=83274637
I0316 21:02:18.138872 140702629090496 spec.py:321] Evaluating on the training split.
I0316 21:04:21.241963 140702629090496 spec.py:333] Evaluating on the validation split.
I0316 21:07:13.664286 140702629090496 spec.py:349] Evaluating on the test split.
I0316 21:12:22.735237 140702629090496 submission_runner.py:469] Time since start: 38556.58s, 	Step: 5756, 	{'train/loss': 0.12260690676926037, 'validation/loss': 0.12424491943944983, 'validation/num_examples': 83274637, 'test/loss': 0.12654745349880017, 'test/num_examples': 95000000, 'score': 5655.4560878276825, 'total_duration': 38556.5806992054, 'accumulated_submission_time': 5655.4560878276825, 'accumulated_eval_time': 32857.48768544197, 'accumulated_logging_time': 1.3526008129119873}
I0316 21:12:22.746616 140661093189376 logging_writer.py:48] [5756] accumulated_eval_time=32857.5, accumulated_logging_time=1.3526, accumulated_submission_time=5655.46, global_step=5756, preemption_count=0, score=5655.46, test/loss=0.126547, test/num_examples=95000000, total_duration=38556.6, train/loss=0.122607, validation/loss=0.124245, validation/num_examples=83274637
I0316 21:14:23.626415 140702629090496 spec.py:321] Evaluating on the training split.
I0316 21:16:27.253526 140702629090496 spec.py:333] Evaluating on the validation split.
I0316 21:19:19.871023 140702629090496 spec.py:349] Evaluating on the test split.
I0316 21:24:22.177745 140702629090496 submission_runner.py:469] Time since start: 39276.02s, 	Step: 5879, 	{'train/loss': 0.12280554271162218, 'validation/loss': 0.12427680801493757, 'validation/num_examples': 83274637, 'test/loss': 0.12666824759148046, 'test/num_examples': 95000000, 'score': 5775.442489147186, 'total_duration': 39276.02319073677, 'accumulated_submission_time': 5775.442489147186, 'accumulated_eval_time': 33456.03910827637, 'accumulated_logging_time': 1.3953797817230225}
I0316 21:24:22.189012 140661084796672 logging_writer.py:48] [5879] accumulated_eval_time=33456, accumulated_logging_time=1.39538, accumulated_submission_time=5775.44, global_step=5879, preemption_count=0, score=5775.44, test/loss=0.126668, test/num_examples=95000000, total_duration=39276, train/loss=0.122806, validation/loss=0.124277, validation/num_examples=83274637
I0316 21:26:23.501769 140661093189376 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.0101497, loss=0.133565
I0316 21:26:23.505256 140702629090496 submission.py:265] 6000) loss = 0.134, grad_norm = 0.010
I0316 21:26:23.897747 140702629090496 spec.py:321] Evaluating on the training split.
I0316 21:28:27.019673 140702629090496 spec.py:333] Evaluating on the validation split.
I0316 21:31:18.955864 140702629090496 spec.py:349] Evaluating on the test split.
I0316 21:36:20.814869 140702629090496 submission_runner.py:469] Time since start: 39994.66s, 	Step: 6001, 	{'train/loss': 0.1212167995272799, 'validation/loss': 0.12428201228208055, 'validation/num_examples': 83274637, 'test/loss': 0.12663389754811338, 'test/num_examples': 95000000, 'score': 5896.274694919586, 'total_duration': 39994.6602768898, 'accumulated_submission_time': 5896.274694919586, 'accumulated_eval_time': 34052.95636820793, 'accumulated_logging_time': 1.4134395122528076}
I0316 21:36:20.827157 140661084796672 logging_writer.py:48] [6001] accumulated_eval_time=34053, accumulated_logging_time=1.41344, accumulated_submission_time=5896.27, global_step=6001, preemption_count=0, score=5896.27, test/loss=0.126634, test/num_examples=95000000, total_duration=39994.7, train/loss=0.121217, validation/loss=0.124282, validation/num_examples=83274637
I0316 21:38:22.196711 140702629090496 spec.py:321] Evaluating on the training split.
I0316 21:40:26.033697 140702629090496 spec.py:333] Evaluating on the validation split.
I0316 21:43:19.710636 140702629090496 spec.py:349] Evaluating on the test split.
I0316 21:48:27.859323 140702629090496 submission_runner.py:469] Time since start: 40721.70s, 	Step: 6121, 	{'train/loss': 0.12344111239575341, 'validation/loss': 0.12421188668282238, 'validation/num_examples': 83274637, 'test/loss': 0.12655992651343095, 'test/num_examples': 95000000, 'score': 6016.74103140831, 'total_duration': 40721.7047085762, 'accumulated_submission_time': 6016.74103140831, 'accumulated_eval_time': 34658.61904358864, 'accumulated_logging_time': 1.433138370513916}
I0316 21:48:27.870818 140661093189376 logging_writer.py:48] [6121] accumulated_eval_time=34658.6, accumulated_logging_time=1.43314, accumulated_submission_time=6016.74, global_step=6121, preemption_count=0, score=6016.74, test/loss=0.12656, test/num_examples=95000000, total_duration=40721.7, train/loss=0.123441, validation/loss=0.124212, validation/num_examples=83274637
I0316 21:50:28.532209 140702629090496 spec.py:321] Evaluating on the training split.
I0316 21:52:31.426132 140702629090496 spec.py:333] Evaluating on the validation split.
I0316 21:55:27.273511 140702629090496 spec.py:349] Evaluating on the test split.
I0316 22:00:34.097929 140702629090496 submission_runner.py:469] Time since start: 41447.94s, 	Step: 6242, 	{'train/loss': 0.12342496271025864, 'validation/loss': 0.12417234765626985, 'validation/num_examples': 83274637, 'test/loss': 0.12648455640041953, 'test/num_examples': 95000000, 'score': 6136.521651506424, 'total_duration': 41447.94339489937, 'accumulated_submission_time': 6136.521651506424, 'accumulated_eval_time': 35264.18491625786, 'accumulated_logging_time': 1.4514319896697998}
I0316 22:00:34.109635 140661084796672 logging_writer.py:48] [6242] accumulated_eval_time=35264.2, accumulated_logging_time=1.45143, accumulated_submission_time=6136.52, global_step=6242, preemption_count=0, score=6136.52, test/loss=0.126485, test/num_examples=95000000, total_duration=41447.9, train/loss=0.123425, validation/loss=0.124172, validation/num_examples=83274637
I0316 22:02:35.987023 140702629090496 spec.py:321] Evaluating on the training split.
I0316 22:04:38.918810 140702629090496 spec.py:333] Evaluating on the validation split.
I0316 22:07:31.025021 140702629090496 spec.py:349] Evaluating on the test split.
I0316 22:12:38.149030 140702629090496 submission_runner.py:469] Time since start: 42171.99s, 	Step: 6365, 	{'train/loss': 0.12303176307123644, 'validation/loss': 0.12422289932115577, 'validation/num_examples': 83274637, 'test/loss': 0.1265676910182752, 'test/num_examples': 95000000, 'score': 6257.517626047134, 'total_duration': 42171.99449419975, 'accumulated_submission_time': 6257.517626047134, 'accumulated_eval_time': 35866.34700489044, 'accumulated_logging_time': 1.4695441722869873}
I0316 22:12:38.160781 140661093189376 logging_writer.py:48] [6365] accumulated_eval_time=35866.3, accumulated_logging_time=1.46954, accumulated_submission_time=6257.52, global_step=6365, preemption_count=0, score=6257.52, test/loss=0.126568, test/num_examples=95000000, total_duration=42172, train/loss=0.123032, validation/loss=0.124223, validation/num_examples=83274637
I0316 22:14:39.084242 140702629090496 spec.py:321] Evaluating on the training split.
I0316 22:16:42.137125 140702629090496 spec.py:333] Evaluating on the validation split.
I0316 22:19:33.563510 140702629090496 spec.py:349] Evaluating on the test split.
I0316 22:24:45.219305 140702629090496 submission_runner.py:469] Time since start: 42899.06s, 	Step: 6486, 	{'train/loss': 0.1217422130491644, 'validation/loss': 0.12417963862353927, 'validation/num_examples': 83274637, 'test/loss': 0.12650026302630776, 'test/num_examples': 95000000, 'score': 6377.566786766052, 'total_duration': 42899.06469845772, 'accumulated_submission_time': 6377.566786766052, 'accumulated_eval_time': 36472.48222899437, 'accumulated_logging_time': 1.5057408809661865}
I0316 22:24:45.230882 140661084796672 logging_writer.py:48] [6486] accumulated_eval_time=36472.5, accumulated_logging_time=1.50574, accumulated_submission_time=6377.57, global_step=6486, preemption_count=0, score=6377.57, test/loss=0.1265, test/num_examples=95000000, total_duration=42899.1, train/loss=0.121742, validation/loss=0.12418, validation/num_examples=83274637
I0316 22:24:48.576793 140661093189376 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.00882382, loss=0.12738
I0316 22:24:48.580089 140702629090496 submission.py:265] 6500) loss = 0.127, grad_norm = 0.009
I0316 22:26:45.896242 140702629090496 spec.py:321] Evaluating on the training split.
I0316 22:28:50.687428 140702629090496 spec.py:333] Evaluating on the validation split.
I0316 22:31:41.593930 140702629090496 spec.py:349] Evaluating on the test split.
I0316 22:36:49.791770 140702629090496 submission_runner.py:469] Time since start: 43623.64s, 	Step: 6608, 	{'train/loss': 0.12000183391810958, 'validation/loss': 0.12417836679206345, 'validation/num_examples': 83274637, 'test/loss': 0.12649507836392052, 'test/num_examples': 95000000, 'score': 6497.345421791077, 'total_duration': 43623.63721680641, 'accumulated_submission_time': 6497.345421791077, 'accumulated_eval_time': 37076.37792158127, 'accumulated_logging_time': 1.523942232131958}
I0316 22:36:49.803971 140661084796672 logging_writer.py:48] [6608] accumulated_eval_time=37076.4, accumulated_logging_time=1.52394, accumulated_submission_time=6497.35, global_step=6608, preemption_count=0, score=6497.35, test/loss=0.126495, test/num_examples=95000000, total_duration=43623.6, train/loss=0.120002, validation/loss=0.124178, validation/num_examples=83274637
I0316 22:38:50.930338 140702629090496 spec.py:321] Evaluating on the training split.
I0316 22:40:55.512728 140702629090496 spec.py:333] Evaluating on the validation split.
I0316 22:43:46.183907 140702629090496 spec.py:349] Evaluating on the test split.
I0316 22:48:54.639204 140702629090496 submission_runner.py:469] Time since start: 44348.48s, 	Step: 6734, 	{'train/loss': 0.1241756232561459, 'validation/loss': 0.12416904401412887, 'validation/num_examples': 83274637, 'test/loss': 0.12646157559991134, 'test/num_examples': 95000000, 'score': 6617.546154975891, 'total_duration': 44348.4846303463, 'accumulated_submission_time': 6617.546154975891, 'accumulated_eval_time': 37680.086958408356, 'accumulated_logging_time': 1.5434362888336182}
I0316 22:48:54.690567 140661093189376 logging_writer.py:48] [6734] accumulated_eval_time=37680.1, accumulated_logging_time=1.54344, accumulated_submission_time=6617.55, global_step=6734, preemption_count=0, score=6617.55, test/loss=0.126462, test/num_examples=95000000, total_duration=44348.5, train/loss=0.124176, validation/loss=0.124169, validation/num_examples=83274637
I0316 22:50:55.583927 140702629090496 spec.py:321] Evaluating on the training split.
I0316 22:53:00.449469 140702629090496 spec.py:333] Evaluating on the validation split.
I0316 22:55:53.314163 140702629090496 spec.py:349] Evaluating on the test split.
I0316 23:00:59.331416 140702629090496 submission_runner.py:469] Time since start: 45073.18s, 	Step: 6861, 	{'train/loss': 0.12373367113320694, 'validation/loss': 0.12418523547982685, 'validation/num_examples': 83274637, 'test/loss': 0.12646296601397866, 'test/num_examples': 95000000, 'score': 6737.567309617996, 'total_duration': 45073.176850795746, 'accumulated_submission_time': 6737.567309617996, 'accumulated_eval_time': 38283.83467888832, 'accumulated_logging_time': 1.6019697189331055}
I0316 23:00:59.343347 140661084796672 logging_writer.py:48] [6861] accumulated_eval_time=38283.8, accumulated_logging_time=1.60197, accumulated_submission_time=6737.57, global_step=6861, preemption_count=0, score=6737.57, test/loss=0.126463, test/num_examples=95000000, total_duration=45073.2, train/loss=0.123734, validation/loss=0.124185, validation/num_examples=83274637
I0316 23:03:00.891205 140702629090496 spec.py:321] Evaluating on the training split.
I0316 23:05:04.511060 140702629090496 spec.py:333] Evaluating on the validation split.
I0316 23:07:58.696765 140702629090496 spec.py:349] Evaluating on the test split.
I0316 23:13:06.771817 140702629090496 submission_runner.py:469] Time since start: 45800.62s, 	Step: 6984, 	{'train/loss': 0.12216044485252207, 'validation/loss': 0.12419126732769929, 'validation/num_examples': 83274637, 'test/loss': 0.1264812626517848, 'test/num_examples': 95000000, 'score': 6858.191623449326, 'total_duration': 45800.617264032364, 'accumulated_submission_time': 6858.191623449326, 'accumulated_eval_time': 38889.71549201012, 'accumulated_logging_time': 1.662550926208496}
I0316 23:13:06.783729 140661093189376 logging_writer.py:48] [6984] accumulated_eval_time=38889.7, accumulated_logging_time=1.66255, accumulated_submission_time=6858.19, global_step=6984, preemption_count=0, score=6858.19, test/loss=0.126481, test/num_examples=95000000, total_duration=45800.6, train/loss=0.12216, validation/loss=0.124191, validation/num_examples=83274637
I0316 23:13:10.539391 140661084796672 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.0063847, loss=0.120552
I0316 23:13:10.542935 140702629090496 submission.py:265] 7000) loss = 0.121, grad_norm = 0.006
I0316 23:15:08.143976 140702629090496 spec.py:321] Evaluating on the training split.
I0316 23:17:11.350095 140702629090496 spec.py:333] Evaluating on the validation split.
I0316 23:20:03.835033 140702629090496 spec.py:349] Evaluating on the test split.
I0316 23:25:09.504806 140702629090496 submission_runner.py:469] Time since start: 46523.35s, 	Step: 7107, 	{'train/loss': 0.1229978238164, 'validation/loss': 0.12421125179496148, 'validation/num_examples': 83274637, 'test/loss': 0.12655037126585308, 'test/num_examples': 95000000, 'score': 6978.689979314804, 'total_duration': 46523.35026836395, 'accumulated_submission_time': 6978.689979314804, 'accumulated_eval_time': 39491.07646870613, 'accumulated_logging_time': 1.6811580657958984}
I0316 23:25:09.516301 140661093189376 logging_writer.py:48] [7107] accumulated_eval_time=39491.1, accumulated_logging_time=1.68116, accumulated_submission_time=6978.69, global_step=7107, preemption_count=0, score=6978.69, test/loss=0.12655, test/num_examples=95000000, total_duration=46523.4, train/loss=0.122998, validation/loss=0.124211, validation/num_examples=83274637
I0316 23:27:10.613823 140702629090496 spec.py:321] Evaluating on the training split.
I0316 23:29:13.761269 140702629090496 spec.py:333] Evaluating on the validation split.
I0316 23:32:06.961425 140702629090496 spec.py:349] Evaluating on the test split.
I0316 23:37:04.042641 140702629090496 submission_runner.py:469] Time since start: 47237.89s, 	Step: 7229, 	{'train/loss': 0.12500174550783713, 'validation/loss': 0.12423201872305313, 'validation/num_examples': 83274637, 'test/loss': 0.12654745270144813, 'test/num_examples': 95000000, 'score': 7098.922003984451, 'total_duration': 47237.8880674839, 'accumulated_submission_time': 7098.922003984451, 'accumulated_eval_time': 40084.505352020264, 'accumulated_logging_time': 1.7001678943634033}
I0316 23:37:04.099922 140661084796672 logging_writer.py:48] [7229] accumulated_eval_time=40084.5, accumulated_logging_time=1.70017, accumulated_submission_time=7098.92, global_step=7229, preemption_count=0, score=7098.92, test/loss=0.126547, test/num_examples=95000000, total_duration=47237.9, train/loss=0.125002, validation/loss=0.124232, validation/num_examples=83274637
I0316 23:39:04.604862 140702629090496 spec.py:321] Evaluating on the training split.
I0316 23:41:07.572817 140702629090496 spec.py:333] Evaluating on the validation split.
I0316 23:43:58.261273 140702629090496 spec.py:349] Evaluating on the test split.
I0316 23:49:04.717124 140702629090496 submission_runner.py:469] Time since start: 47958.56s, 	Step: 7355, 	{'train/loss': 0.12273843095846915, 'validation/loss': 0.12427873582301131, 'validation/num_examples': 83274637, 'test/loss': 0.12661800879693283, 'test/num_examples': 95000000, 'score': 7218.50971698761, 'total_duration': 47958.562549352646, 'accumulated_submission_time': 7218.50971698761, 'accumulated_eval_time': 40684.61773586273, 'accumulated_logging_time': 1.7644495964050293}
I0316 23:49:04.730515 140661093189376 logging_writer.py:48] [7355] accumulated_eval_time=40684.6, accumulated_logging_time=1.76445, accumulated_submission_time=7218.51, global_step=7355, preemption_count=0, score=7218.51, test/loss=0.126618, test/num_examples=95000000, total_duration=47958.6, train/loss=0.122738, validation/loss=0.124279, validation/num_examples=83274637
I0316 23:51:05.612301 140702629090496 spec.py:321] Evaluating on the training split.
I0316 23:53:08.829091 140702629090496 spec.py:333] Evaluating on the validation split.
I0316 23:56:01.722805 140702629090496 spec.py:349] Evaluating on the test split.
I0317 00:01:11.477396 140702629090496 submission_runner.py:469] Time since start: 48685.32s, 	Step: 7483, 	{'train/loss': 0.123977120932052, 'validation/loss': 0.12435836819909801, 'validation/num_examples': 83274637, 'test/loss': 0.12668651290058336, 'test/num_examples': 95000000, 'score': 7338.380881309509, 'total_duration': 48685.32284593582, 'accumulated_submission_time': 7338.380881309509, 'accumulated_eval_time': 41290.482941150665, 'accumulated_logging_time': 1.8675270080566406}
I0317 00:01:11.489579 140661084796672 logging_writer.py:48] [7483] accumulated_eval_time=41290.5, accumulated_logging_time=1.86753, accumulated_submission_time=7338.38, global_step=7483, preemption_count=0, score=7338.38, test/loss=0.126687, test/num_examples=95000000, total_duration=48685.3, train/loss=0.123977, validation/loss=0.124358, validation/num_examples=83274637
I0317 00:01:15.399396 140661093189376 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.0143296, loss=0.120786
I0317 00:01:15.403253 140702629090496 submission.py:265] 7500) loss = 0.121, grad_norm = 0.014
I0317 00:03:12.526763 140702629090496 spec.py:321] Evaluating on the training split.
I0317 00:05:16.210463 140702629090496 spec.py:333] Evaluating on the validation split.
I0317 00:08:09.055719 140702629090496 spec.py:349] Evaluating on the test split.
I0317 00:13:19.855124 140702629090496 submission_runner.py:469] Time since start: 49413.70s, 	Step: 7607, 	{'train/loss': 0.12380439678141812, 'validation/loss': 0.12441053703721403, 'validation/num_examples': 83274637, 'test/loss': 0.12675144480526573, 'test/num_examples': 95000000, 'score': 7458.580144882202, 'total_duration': 49413.70057964325, 'accumulated_submission_time': 7458.580144882202, 'accumulated_eval_time': 41897.81136226654, 'accumulated_logging_time': 1.8874342441558838}
I0317 00:13:19.866922 140661084796672 logging_writer.py:48] [7607] accumulated_eval_time=41897.8, accumulated_logging_time=1.88743, accumulated_submission_time=7458.58, global_step=7607, preemption_count=0, score=7458.58, test/loss=0.126751, test/num_examples=95000000, total_duration=49413.7, train/loss=0.123804, validation/loss=0.124411, validation/num_examples=83274637
I0317 00:15:21.831178 140702629090496 spec.py:321] Evaluating on the training split.
I0317 00:17:24.794269 140702629090496 spec.py:333] Evaluating on the validation split.
I0317 00:20:16.602629 140702629090496 spec.py:349] Evaluating on the test split.
I0317 00:25:27.438356 140702629090496 submission_runner.py:469] Time since start: 50141.28s, 	Step: 7733, 	{'train/loss': 0.12274766096258223, 'validation/loss': 0.1242992761180687, 'validation/num_examples': 83274637, 'test/loss': 0.12666050209535298, 'test/num_examples': 95000000, 'score': 7579.643595457077, 'total_duration': 50141.2838037014, 'accumulated_submission_time': 7579.643595457077, 'accumulated_eval_time': 42503.41873574257, 'accumulated_logging_time': 1.9065778255462646}
I0317 00:25:27.451357 140661093189376 logging_writer.py:48] [7733] accumulated_eval_time=42503.4, accumulated_logging_time=1.90658, accumulated_submission_time=7579.64, global_step=7733, preemption_count=0, score=7579.64, test/loss=0.126661, test/num_examples=95000000, total_duration=50141.3, train/loss=0.122748, validation/loss=0.124299, validation/num_examples=83274637
I0317 00:27:28.531489 140702629090496 spec.py:321] Evaluating on the training split.
I0317 00:29:31.408488 140702629090496 spec.py:333] Evaluating on the validation split.
I0317 00:32:26.075990 140702629090496 spec.py:349] Evaluating on the test split.
I0317 00:37:33.185138 140702629090496 submission_runner.py:469] Time since start: 50867.03s, 	Step: 7858, 	{'train/loss': 0.12231447797709463, 'validation/loss': 0.12433469593730939, 'validation/num_examples': 83274637, 'test/loss': 0.12671314309672305, 'test/num_examples': 95000000, 'score': 7699.83989572525, 'total_duration': 50867.03058600426, 'accumulated_submission_time': 7699.83989572525, 'accumulated_eval_time': 43108.07256412506, 'accumulated_logging_time': 1.9271306991577148}
I0317 00:37:33.196697 140661084796672 logging_writer.py:48] [7858] accumulated_eval_time=43108.1, accumulated_logging_time=1.92713, accumulated_submission_time=7699.84, global_step=7858, preemption_count=0, score=7699.84, test/loss=0.126713, test/num_examples=95000000, total_duration=50867, train/loss=0.122314, validation/loss=0.124335, validation/num_examples=83274637
I0317 00:39:33.936861 140661093189376 logging_writer.py:48] [7988] global_step=7988, preemption_count=0, score=7820.09
I0317 00:39:45.925403 140702629090496 submission_runner.py:646] Tuning trial 2/5
I0317 00:39:45.925774 140702629090496 submission_runner.py:647] Hyperparameters: Hyperparameters(learning_rate=0.0014381744028656841, one_minus_beta1=0.025337537053408913, one_minus_beta2=0.02508024059481679, epsilon=1e-08, one_minus_momentum=0.0, use_momentum=False, weight_decay=0.00019716633625688372, max_preconditioner_dim=1024, precondition_frequency=100, start_preconditioning_step=-1, inv_root_override=0, exponent_multiplier=1.0, grafting_type='ADAM', grafting_epsilon=1e-08, use_normalized_grafting=False, communication_dtype='FP32', communicate_params=True, use_cosine_decay=True, warmup_factor=0.05, label_smoothing=0.2, dropout_rate=0.0, use_nadam=True, step_hint_factor=0.6)
I0317 00:39:45.927334 140702629090496 submission_runner.py:648] Metrics: {'eval_results': [(1, {'train/loss': 0.6098753428854343, 'validation/loss': 0.6070518915552142, 'validation/num_examples': 83274637, 'test/loss': 0.6086389612169768, 'test/num_examples': 95000000, 'score': 9.26477837562561, 'total_duration': 959.13156914711, 'accumulated_submission_time': 9.26477837562561, 'accumulated_eval_time': 949.4147655963898, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (124, {'train/loss': 0.13124836394424066, 'validation/loss': 0.13223318855414934, 'validation/num_examples': 83274637, 'test/loss': 0.13501571895157663, 'test/num_examples': 95000000, 'score': 129.43750596046448, 'total_duration': 1969.492205619812, 'accumulated_submission_time': 129.43750596046448, 'accumulated_eval_time': 1838.648716211319, 'accumulated_logging_time': 0.06248927116394043, 'global_step': 124, 'preemption_count': 0}), (244, {'train/loss': 0.1288338746732061, 'validation/loss': 0.128730590634081, 'validation/num_examples': 83274637, 'test/loss': 0.13145081003064607, 'test/num_examples': 95000000, 'score': 249.1492781639099, 'total_duration': 2983.131873846054, 'accumulated_submission_time': 249.1492781639099, 'accumulated_eval_time': 2731.6464343070984, 'accumulated_logging_time': 0.08045792579650879, 'global_step': 244, 'preemption_count': 0}), (366, {'train/loss': 0.12788641748973886, 'validation/loss': 0.12773614130041228, 'validation/num_examples': 83274637, 'test/loss': 0.13056377778095446, 'test/num_examples': 95000000, 'score': 368.95321464538574, 'total_duration': 3977.8572521209717, 'accumulated_submission_time': 368.95321464538574, 'accumulated_eval_time': 3605.5976700782776, 'accumulated_logging_time': 0.17122840881347656, 'global_step': 366, 'preemption_count': 0}), (489, {'train/loss': 0.12818378242681444, 'validation/loss': 0.12692039395845084, 'validation/num_examples': 83274637, 'test/loss': 0.1294551247552169, 'test/num_examples': 95000000, 'score': 488.71590924263, 'total_duration': 4986.955470323563, 'accumulated_submission_time': 488.71590924263, 'accumulated_eval_time': 4494.02990937233, 'accumulated_logging_time': 0.1878490447998047, 'global_step': 489, 'preemption_count': 0}), (610, {'train/loss': 0.12522959846694673, 'validation/loss': 0.12677021676856395, 'validation/num_examples': 83274637, 'test/loss': 0.12919498835304663, 'test/num_examples': 95000000, 'score': 608.8842985630035, 'total_duration': 5984.621225118637, 'accumulated_submission_time': 608.8842985630035, 'accumulated_eval_time': 5370.684534311295, 'accumulated_logging_time': 0.20443010330200195, 'global_step': 610, 'preemption_count': 0}), (734, {'train/loss': 0.12445888383322808, 'validation/loss': 0.12648132692213973, 'validation/num_examples': 83274637, 'test/loss': 0.12900676912119013, 'test/num_examples': 95000000, 'score': 729.0558650493622, 'total_duration': 6977.039262056351, 'accumulated_submission_time': 729.0558650493622, 'accumulated_eval_time': 6241.985344171524, 'accumulated_logging_time': 0.2216479778289795, 'global_step': 734, 'preemption_count': 0}), (855, {'train/loss': 0.12679202938590697, 'validation/loss': 0.1264477009151791, 'validation/num_examples': 83274637, 'test/loss': 0.12859334473033704, 'test/num_examples': 95000000, 'score': 848.8339562416077, 'total_duration': 7975.779079437256, 'accumulated_submission_time': 848.8339562416077, 'accumulated_eval_time': 7119.999000787735, 'accumulated_logging_time': 0.2384810447692871, 'global_step': 855, 'preemption_count': 0}), (977, {'train/loss': 0.12449914697186111, 'validation/loss': 0.12636989387220104, 'validation/num_examples': 83274637, 'test/loss': 0.12862598498687744, 'test/num_examples': 95000000, 'score': 969.1742653846741, 'total_duration': 8968.05740904808, 'accumulated_submission_time': 969.1742653846741, 'accumulated_eval_time': 7991.075145483017, 'accumulated_logging_time': 0.25598788261413574, 'global_step': 977, 'preemption_count': 0}), (1106, {'train/loss': 0.12420708116967713, 'validation/loss': 0.1261259112938554, 'validation/num_examples': 83274637, 'test/loss': 0.12836988119406448, 'test/num_examples': 95000000, 'score': 1089.1597654819489, 'total_duration': 9931.688895702362, 'accumulated_submission_time': 1089.1597654819489, 'accumulated_eval_time': 8833.757845163345, 'accumulated_logging_time': 0.2758200168609619, 'global_step': 1106, 'preemption_count': 0}), (1225, {'train/loss': 0.12378487705113632, 'validation/loss': 0.12606058928828812, 'validation/num_examples': 83274637, 'test/loss': 0.12842468525551243, 'test/num_examples': 95000000, 'score': 1210.2358293533325, 'total_duration': 10853.77265381813, 'accumulated_submission_time': 1210.2358293533325, 'accumulated_eval_time': 9633.776673555374, 'accumulated_logging_time': 0.341350793838501, 'global_step': 1225, 'preemption_count': 0}), (1347, {'train/loss': 0.12548079147342003, 'validation/loss': 0.1256814093186031, 'validation/num_examples': 83274637, 'test/loss': 0.12795344565690694, 'test/num_examples': 95000000, 'score': 1329.7490818500519, 'total_duration': 11697.40369796753, 'accumulated_submission_time': 1329.7490818500519, 'accumulated_eval_time': 10356.973821163177, 'accumulated_logging_time': 0.35823607444763184, 'global_step': 1347, 'preemption_count': 0}), (1471, {'train/loss': 0.12575430116379283, 'validation/loss': 0.12576707111397176, 'validation/num_examples': 83274637, 'test/loss': 0.12802002936220672, 'test/num_examples': 95000000, 'score': 1449.8142113685608, 'total_duration': 12454.31123638153, 'accumulated_submission_time': 1449.8142113685608, 'accumulated_eval_time': 10992.891634464264, 'accumulated_logging_time': 0.37509870529174805, 'global_step': 1471, 'preemption_count': 0}), (1592, {'train/loss': 0.12329995826552925, 'validation/loss': 0.125730390169145, 'validation/num_examples': 83274637, 'test/loss': 0.1281536034189726, 'test/num_examples': 95000000, 'score': 1569.4414608478546, 'total_duration': 13197.597396612167, 'accumulated_submission_time': 1569.4414608478546, 'accumulated_eval_time': 11615.646130561829, 'accumulated_logging_time': 0.3927338123321533, 'global_step': 1592, 'preemption_count': 0}), (1715, {'train/loss': 0.12470665026475172, 'validation/loss': 0.12565270585422803, 'validation/num_examples': 83274637, 'test/loss': 0.12791637865612632, 'test/num_examples': 95000000, 'score': 1689.565319776535, 'total_duration': 14040.910097360611, 'accumulated_submission_time': 1689.565319776535, 'accumulated_eval_time': 12337.943231582642, 'accumulated_logging_time': 0.41123485565185547, 'global_step': 1715, 'preemption_count': 0}), (1838, {'train/loss': 0.1252224388548464, 'validation/loss': 0.12556255251966747, 'validation/num_examples': 83274637, 'test/loss': 0.12783369584856535, 'test/num_examples': 95000000, 'score': 1809.962423324585, 'total_duration': 14802.39233660698, 'accumulated_submission_time': 1809.962423324585, 'accumulated_eval_time': 12978.11045384407, 'accumulated_logging_time': 0.43041372299194336, 'global_step': 1838, 'preemption_count': 0}), (1962, {'train/loss': 0.12431994612036834, 'validation/loss': 0.12563795277707962, 'validation/num_examples': 83274637, 'test/loss': 0.12796454144656533, 'test/num_examples': 95000000, 'score': 1930.368491411209, 'total_duration': 15549.805566549301, 'accumulated_submission_time': 1930.368491411209, 'accumulated_eval_time': 13604.191866397858, 'accumulated_logging_time': 0.4485151767730713, 'global_step': 1962, 'preemption_count': 0}), (2081, {'train/loss': 0.12487185268269674, 'validation/loss': 0.12561407526623042, 'validation/num_examples': 83274637, 'test/loss': 0.12791434818014846, 'test/num_examples': 95000000, 'score': 2050.523274898529, 'total_duration': 16297.493948936462, 'accumulated_submission_time': 2050.523274898529, 'accumulated_eval_time': 14230.808935880661, 'accumulated_logging_time': 0.465512752532959, 'global_step': 2081, 'preemption_count': 0}), (2210, {'train/loss': 0.12451703663062126, 'validation/loss': 0.12535060022279168, 'validation/num_examples': 83274637, 'test/loss': 0.1275554156149613, 'test/num_examples': 95000000, 'score': 2170.494159936905, 'total_duration': 17059.894397497177, 'accumulated_submission_time': 2170.494159936905, 'accumulated_eval_time': 14872.319916009903, 'accumulated_logging_time': 0.4838600158691406, 'global_step': 2210, 'preemption_count': 0}), (2330, {'train/loss': 0.12439698996362222, 'validation/loss': 0.12518763557844423, 'validation/num_examples': 83274637, 'test/loss': 0.12757364848407945, 'test/num_examples': 95000000, 'score': 2290.490604877472, 'total_duration': 17806.21106648445, 'accumulated_submission_time': 2290.490604877472, 'accumulated_eval_time': 15497.715490818024, 'accumulated_logging_time': 0.5006890296936035, 'global_step': 2330, 'preemption_count': 0}), (2455, {'train/loss': 0.12416529124888503, 'validation/loss': 0.12523676962702573, 'validation/num_examples': 83274637, 'test/loss': 0.12759205634492574, 'test/num_examples': 95000000, 'score': 2411.2543354034424, 'total_duration': 18570.56929206848, 'accumulated_submission_time': 2411.2543354034424, 'accumulated_eval_time': 16140.387684106827, 'accumulated_logging_time': 0.5200259685516357, 'global_step': 2455, 'preemption_count': 0}), (2575, {'train/loss': 0.12424639102888516, 'validation/loss': 0.1250569067993509, 'validation/num_examples': 83274637, 'test/loss': 0.12737227748220342, 'test/num_examples': 95000000, 'score': 2532.5077307224274, 'total_duration': 19320.131658792496, 'accumulated_submission_time': 2532.5077307224274, 'accumulated_eval_time': 16767.825222730637, 'accumulated_logging_time': 0.5384321212768555, 'global_step': 2575, 'preemption_count': 0}), (2695, {'train/loss': 0.12166257764561095, 'validation/loss': 0.1251845160291965, 'validation/num_examples': 83274637, 'test/loss': 0.1276684805275365, 'test/num_examples': 95000000, 'score': 2652.070769071579, 'total_duration': 20078.06515789032, 'accumulated_submission_time': 2652.070769071579, 'accumulated_eval_time': 17405.29059100151, 'accumulated_logging_time': 0.5559756755828857, 'global_step': 2695, 'preemption_count': 0}), (2815, {'train/loss': 0.12469269409131704, 'validation/loss': 0.12500161620978292, 'validation/num_examples': 83274637, 'test/loss': 0.12745133505646555, 'test/num_examples': 95000000, 'score': 2771.846065759659, 'total_duration': 20850.611068487167, 'accumulated_submission_time': 2771.846065759659, 'accumulated_eval_time': 18057.12704205513, 'accumulated_logging_time': 0.6204884052276611, 'global_step': 2815, 'preemption_count': 0}), (2936, {'train/loss': 0.12415228537305825, 'validation/loss': 0.12510976263608323, 'validation/num_examples': 83274637, 'test/loss': 0.12757824414809377, 'test/num_examples': 95000000, 'score': 2892.1906571388245, 'total_duration': 21617.496493577957, 'accumulated_submission_time': 2892.1906571388245, 'accumulated_eval_time': 18702.789763212204, 'accumulated_logging_time': 0.638481616973877, 'global_step': 2936, 'preemption_count': 0}), (3054, {'train/loss': 0.12262518637363448, 'validation/loss': 0.12482498632283875, 'validation/num_examples': 83274637, 'test/loss': 0.12712922440277902, 'test/num_examples': 95000000, 'score': 3012.10649061203, 'total_duration': 22380.7952439785, 'accumulated_submission_time': 3012.10649061203, 'accumulated_eval_time': 19345.252594709396, 'accumulated_logging_time': 0.6572251319885254, 'global_step': 3054, 'preemption_count': 0}), (3177, {'train/loss': 0.12406284202453824, 'validation/loss': 0.12486488107742849, 'validation/num_examples': 83274637, 'test/loss': 0.12735339092632092, 'test/num_examples': 95000000, 'score': 3131.811572790146, 'total_duration': 23131.037689447403, 'accumulated_submission_time': 3131.811572790146, 'accumulated_eval_time': 19974.868231534958, 'accumulated_logging_time': 0.6752736568450928, 'global_step': 3177, 'preemption_count': 0}), (3299, {'train/loss': 0.12362515559301261, 'validation/loss': 0.12484479089955423, 'validation/num_examples': 83274637, 'test/loss': 0.12728781454937582, 'test/num_examples': 95000000, 'score': 3251.3215448856354, 'total_duration': 23887.28389453888, 'accumulated_submission_time': 3251.3215448856354, 'accumulated_eval_time': 20610.633850097656, 'accumulated_logging_time': 0.7231490612030029, 'global_step': 3299, 'preemption_count': 0}), (3420, {'train/loss': 0.12185102077092907, 'validation/loss': 0.12480690702016828, 'validation/num_examples': 83274637, 'test/loss': 0.12722606708390086, 'test/num_examples': 95000000, 'score': 3371.685770511627, 'total_duration': 24655.09649705887, 'accumulated_submission_time': 3371.685770511627, 'accumulated_eval_time': 21257.195376634598, 'accumulated_logging_time': 0.7423291206359863, 'global_step': 3420, 'preemption_count': 0}), (3548, {'train/loss': 0.12141043000079335, 'validation/loss': 0.12493563601487752, 'validation/num_examples': 83274637, 'test/loss': 0.1273770106413992, 'test/num_examples': 95000000, 'score': 3492.819480419159, 'total_duration': 25416.03848528862, 'accumulated_submission_time': 3492.819480419159, 'accumulated_eval_time': 21896.089182138443, 'accumulated_logging_time': 0.7598333358764648, 'global_step': 3548, 'preemption_count': 0}), (3676, {'train/loss': 0.12383861687993908, 'validation/loss': 0.12482832716534584, 'validation/num_examples': 83274637, 'test/loss': 0.12724863797639546, 'test/num_examples': 95000000, 'score': 3612.7181606292725, 'total_duration': 26184.163151025772, 'accumulated_submission_time': 3612.7181606292725, 'accumulated_eval_time': 22543.41310787201, 'accumulated_logging_time': 0.777552604675293, 'global_step': 3676, 'preemption_count': 0}), (3798, {'train/loss': 0.12318282376287001, 'validation/loss': 0.12518009998954852, 'validation/num_examples': 83274637, 'test/loss': 0.12757547708932976, 'test/num_examples': 95000000, 'score': 3732.9887068271637, 'total_duration': 26942.25573325157, 'accumulated_submission_time': 3732.9887068271637, 'accumulated_eval_time': 23180.331609010696, 'accumulated_logging_time': 0.8071057796478271, 'global_step': 3798, 'preemption_count': 0}), (3920, {'train/loss': 0.12345981750131231, 'validation/loss': 0.12461510392774308, 'validation/num_examples': 83274637, 'test/loss': 0.12694141767722683, 'test/num_examples': 95000000, 'score': 3854.1386528015137, 'total_duration': 27691.048473358154, 'accumulated_submission_time': 3854.1386528015137, 'accumulated_eval_time': 23807.06750369072, 'accumulated_logging_time': 0.8250265121459961, 'global_step': 3920, 'preemption_count': 0}), (4038, {'train/loss': 0.12296825992523379, 'validation/loss': 0.12474789349031658, 'validation/num_examples': 83274637, 'test/loss': 0.12713553524515253, 'test/num_examples': 95000000, 'score': 3974.1648695468903, 'total_duration': 28417.579128026962, 'accumulated_submission_time': 3974.1648695468903, 'accumulated_eval_time': 24412.659071922302, 'accumulated_logging_time': 0.8443238735198975, 'global_step': 4038, 'preemption_count': 0}), (4156, {'train/loss': 0.12415823455360912, 'validation/loss': 0.12463490666730334, 'validation/num_examples': 83274637, 'test/loss': 0.12696073825061197, 'test/num_examples': 95000000, 'score': 4093.932314157486, 'total_duration': 29137.977610349655, 'accumulated_submission_time': 4093.932314157486, 'accumulated_eval_time': 25012.404972076416, 'accumulated_logging_time': 0.8629729747772217, 'global_step': 4156, 'preemption_count': 0}), (4279, {'train/loss': 0.12156910188613376, 'validation/loss': 0.12452666402019893, 'validation/num_examples': 83274637, 'test/loss': 0.12688028505192808, 'test/num_examples': 95000000, 'score': 4214.08628153801, 'total_duration': 29864.582788944244, 'accumulated_submission_time': 4214.08628153801, 'accumulated_eval_time': 25617.902287006378, 'accumulated_logging_time': 0.9446344375610352, 'global_step': 4279, 'preemption_count': 0}), (4404, {'train/loss': 0.12358754758907953, 'validation/loss': 0.12464335414162456, 'validation/num_examples': 83274637, 'test/loss': 0.1269981241361919, 'test/num_examples': 95000000, 'score': 4333.651380777359, 'total_duration': 30586.155426502228, 'accumulated_submission_time': 4333.651380777359, 'accumulated_eval_time': 26218.971644878387, 'accumulated_logging_time': 0.9763240814208984, 'global_step': 4404, 'preemption_count': 0}), (4529, {'train/loss': 0.12320134710025255, 'validation/loss': 0.12457518142123, 'validation/num_examples': 83274637, 'test/loss': 0.12692401949852392, 'test/num_examples': 95000000, 'score': 4453.7957055568695, 'total_duration': 31303.135671138763, 'accumulated_submission_time': 4453.7957055568695, 'accumulated_eval_time': 26814.875884771347, 'accumulated_logging_time': 0.9947173595428467, 'global_step': 4529, 'preemption_count': 0}), (4657, {'train/loss': 0.12118336303190193, 'validation/loss': 0.12452832958333246, 'validation/num_examples': 83274637, 'test/loss': 0.12687285996832598, 'test/num_examples': 95000000, 'score': 4574.034925699234, 'total_duration': 32027.17025089264, 'accumulated_submission_time': 4574.034925699234, 'accumulated_eval_time': 27417.722332000732, 'accumulated_logging_time': 1.013540506362915, 'global_step': 4657, 'preemption_count': 0}), (4781, {'train/loss': 0.12298070345532545, 'validation/loss': 0.12452254336996096, 'validation/num_examples': 83274637, 'test/loss': 0.12690960756631148, 'test/num_examples': 95000000, 'score': 4693.625509738922, 'total_duration': 32746.582434654236, 'accumulated_submission_time': 4693.625509738922, 'accumulated_eval_time': 28016.646154403687, 'accumulated_logging_time': 1.0324487686157227, 'global_step': 4781, 'preemption_count': 0}), (4906, {'train/loss': 0.12259106093009166, 'validation/loss': 0.12442503001757221, 'validation/num_examples': 83274637, 'test/loss': 0.1267324877390008, 'test/num_examples': 95000000, 'score': 4813.428468465805, 'total_duration': 33484.75571727753, 'accumulated_submission_time': 4813.428468465805, 'accumulated_eval_time': 28634.106946706772, 'accumulated_logging_time': 1.0872466564178467, 'global_step': 4906, 'preemption_count': 0}), (5029, {'train/loss': 0.12241567427887208, 'validation/loss': 0.12432338831937002, 'validation/num_examples': 83274637, 'test/loss': 0.1266615741187246, 'test/num_examples': 95000000, 'score': 4933.03212594986, 'total_duration': 34218.5738825798, 'accumulated_submission_time': 4933.03212594986, 'accumulated_eval_time': 29247.379501581192, 'accumulated_logging_time': 1.1794037818908691, 'global_step': 5029, 'preemption_count': 0}), (5151, {'train/loss': 0.12123211520200347, 'validation/loss': 0.12424395430648187, 'validation/num_examples': 83274637, 'test/loss': 0.1265959284518995, 'test/num_examples': 95000000, 'score': 5053.287846088409, 'total_duration': 34937.1168653965, 'accumulated_submission_time': 5053.287846088409, 'accumulated_eval_time': 29844.808381557465, 'accumulated_logging_time': 1.1974542140960693, 'global_step': 5151, 'preemption_count': 0}), (5269, {'train/loss': 0.12509037689894903, 'validation/loss': 0.12426798424583148, 'validation/num_examples': 83274637, 'test/loss': 0.12659053121390093, 'test/num_examples': 95000000, 'score': 5173.468491077423, 'total_duration': 35660.26803445816, 'accumulated_submission_time': 5173.468491077423, 'accumulated_eval_time': 30446.876648902893, 'accumulated_logging_time': 1.215188980102539, 'global_step': 5269, 'preemption_count': 0}), (5393, {'train/loss': 0.12178219871734018, 'validation/loss': 0.12432358432983717, 'validation/num_examples': 83274637, 'test/loss': 0.1266910321995384, 'test/num_examples': 95000000, 'score': 5293.621126651764, 'total_duration': 36385.915556907654, 'accumulated_submission_time': 5293.621126651764, 'accumulated_eval_time': 31051.425617933273, 'accumulated_logging_time': 1.2968759536743164, 'global_step': 5393, 'preemption_count': 0}), (5513, {'train/loss': 0.12355015100013467, 'validation/loss': 0.12422394327538905, 'validation/num_examples': 83274637, 'test/loss': 0.12653632827449598, 'test/num_examples': 95000000, 'score': 5414.488468408585, 'total_duration': 37113.74638748169, 'accumulated_submission_time': 5414.488468408585, 'accumulated_eval_time': 31657.4650182724, 'accumulated_logging_time': 1.316572904586792, 'global_step': 5513, 'preemption_count': 0}), (5635, {'train/loss': 0.11979429599703144, 'validation/loss': 0.12427267291697748, 'validation/num_examples': 83274637, 'test/loss': 0.12664412569961547, 'test/num_examples': 95000000, 'score': 5534.580418348312, 'total_duration': 37830.20088362694, 'accumulated_submission_time': 5534.580418348312, 'accumulated_eval_time': 32252.891203165054, 'accumulated_logging_time': 1.3341755867004395, 'global_step': 5635, 'preemption_count': 0}), (5756, {'train/loss': 0.12260690676926037, 'validation/loss': 0.12424491943944983, 'validation/num_examples': 83274637, 'test/loss': 0.12654745349880017, 'test/num_examples': 95000000, 'score': 5655.4560878276825, 'total_duration': 38556.5806992054, 'accumulated_submission_time': 5655.4560878276825, 'accumulated_eval_time': 32857.48768544197, 'accumulated_logging_time': 1.3526008129119873, 'global_step': 5756, 'preemption_count': 0}), (5879, {'train/loss': 0.12280554271162218, 'validation/loss': 0.12427680801493757, 'validation/num_examples': 83274637, 'test/loss': 0.12666824759148046, 'test/num_examples': 95000000, 'score': 5775.442489147186, 'total_duration': 39276.02319073677, 'accumulated_submission_time': 5775.442489147186, 'accumulated_eval_time': 33456.03910827637, 'accumulated_logging_time': 1.3953797817230225, 'global_step': 5879, 'preemption_count': 0}), (6001, {'train/loss': 0.1212167995272799, 'validation/loss': 0.12428201228208055, 'validation/num_examples': 83274637, 'test/loss': 0.12663389754811338, 'test/num_examples': 95000000, 'score': 5896.274694919586, 'total_duration': 39994.6602768898, 'accumulated_submission_time': 5896.274694919586, 'accumulated_eval_time': 34052.95636820793, 'accumulated_logging_time': 1.4134395122528076, 'global_step': 6001, 'preemption_count': 0}), (6121, {'train/loss': 0.12344111239575341, 'validation/loss': 0.12421188668282238, 'validation/num_examples': 83274637, 'test/loss': 0.12655992651343095, 'test/num_examples': 95000000, 'score': 6016.74103140831, 'total_duration': 40721.7047085762, 'accumulated_submission_time': 6016.74103140831, 'accumulated_eval_time': 34658.61904358864, 'accumulated_logging_time': 1.433138370513916, 'global_step': 6121, 'preemption_count': 0}), (6242, {'train/loss': 0.12342496271025864, 'validation/loss': 0.12417234765626985, 'validation/num_examples': 83274637, 'test/loss': 0.12648455640041953, 'test/num_examples': 95000000, 'score': 6136.521651506424, 'total_duration': 41447.94339489937, 'accumulated_submission_time': 6136.521651506424, 'accumulated_eval_time': 35264.18491625786, 'accumulated_logging_time': 1.4514319896697998, 'global_step': 6242, 'preemption_count': 0}), (6365, {'train/loss': 0.12303176307123644, 'validation/loss': 0.12422289932115577, 'validation/num_examples': 83274637, 'test/loss': 0.1265676910182752, 'test/num_examples': 95000000, 'score': 6257.517626047134, 'total_duration': 42171.99449419975, 'accumulated_submission_time': 6257.517626047134, 'accumulated_eval_time': 35866.34700489044, 'accumulated_logging_time': 1.4695441722869873, 'global_step': 6365, 'preemption_count': 0}), (6486, {'train/loss': 0.1217422130491644, 'validation/loss': 0.12417963862353927, 'validation/num_examples': 83274637, 'test/loss': 0.12650026302630776, 'test/num_examples': 95000000, 'score': 6377.566786766052, 'total_duration': 42899.06469845772, 'accumulated_submission_time': 6377.566786766052, 'accumulated_eval_time': 36472.48222899437, 'accumulated_logging_time': 1.5057408809661865, 'global_step': 6486, 'preemption_count': 0}), (6608, {'train/loss': 0.12000183391810958, 'validation/loss': 0.12417836679206345, 'validation/num_examples': 83274637, 'test/loss': 0.12649507836392052, 'test/num_examples': 95000000, 'score': 6497.345421791077, 'total_duration': 43623.63721680641, 'accumulated_submission_time': 6497.345421791077, 'accumulated_eval_time': 37076.37792158127, 'accumulated_logging_time': 1.523942232131958, 'global_step': 6608, 'preemption_count': 0}), (6734, {'train/loss': 0.1241756232561459, 'validation/loss': 0.12416904401412887, 'validation/num_examples': 83274637, 'test/loss': 0.12646157559991134, 'test/num_examples': 95000000, 'score': 6617.546154975891, 'total_duration': 44348.4846303463, 'accumulated_submission_time': 6617.546154975891, 'accumulated_eval_time': 37680.086958408356, 'accumulated_logging_time': 1.5434362888336182, 'global_step': 6734, 'preemption_count': 0}), (6861, {'train/loss': 0.12373367113320694, 'validation/loss': 0.12418523547982685, 'validation/num_examples': 83274637, 'test/loss': 0.12646296601397866, 'test/num_examples': 95000000, 'score': 6737.567309617996, 'total_duration': 45073.176850795746, 'accumulated_submission_time': 6737.567309617996, 'accumulated_eval_time': 38283.83467888832, 'accumulated_logging_time': 1.6019697189331055, 'global_step': 6861, 'preemption_count': 0}), (6984, {'train/loss': 0.12216044485252207, 'validation/loss': 0.12419126732769929, 'validation/num_examples': 83274637, 'test/loss': 0.1264812626517848, 'test/num_examples': 95000000, 'score': 6858.191623449326, 'total_duration': 45800.617264032364, 'accumulated_submission_time': 6858.191623449326, 'accumulated_eval_time': 38889.71549201012, 'accumulated_logging_time': 1.662550926208496, 'global_step': 6984, 'preemption_count': 0}), (7107, {'train/loss': 0.1229978238164, 'validation/loss': 0.12421125179496148, 'validation/num_examples': 83274637, 'test/loss': 0.12655037126585308, 'test/num_examples': 95000000, 'score': 6978.689979314804, 'total_duration': 46523.35026836395, 'accumulated_submission_time': 6978.689979314804, 'accumulated_eval_time': 39491.07646870613, 'accumulated_logging_time': 1.6811580657958984, 'global_step': 7107, 'preemption_count': 0}), (7229, {'train/loss': 0.12500174550783713, 'validation/loss': 0.12423201872305313, 'validation/num_examples': 83274637, 'test/loss': 0.12654745270144813, 'test/num_examples': 95000000, 'score': 7098.922003984451, 'total_duration': 47237.8880674839, 'accumulated_submission_time': 7098.922003984451, 'accumulated_eval_time': 40084.505352020264, 'accumulated_logging_time': 1.7001678943634033, 'global_step': 7229, 'preemption_count': 0}), (7355, {'train/loss': 0.12273843095846915, 'validation/loss': 0.12427873582301131, 'validation/num_examples': 83274637, 'test/loss': 0.12661800879693283, 'test/num_examples': 95000000, 'score': 7218.50971698761, 'total_duration': 47958.562549352646, 'accumulated_submission_time': 7218.50971698761, 'accumulated_eval_time': 40684.61773586273, 'accumulated_logging_time': 1.7644495964050293, 'global_step': 7355, 'preemption_count': 0}), (7483, {'train/loss': 0.123977120932052, 'validation/loss': 0.12435836819909801, 'validation/num_examples': 83274637, 'test/loss': 0.12668651290058336, 'test/num_examples': 95000000, 'score': 7338.380881309509, 'total_duration': 48685.32284593582, 'accumulated_submission_time': 7338.380881309509, 'accumulated_eval_time': 41290.482941150665, 'accumulated_logging_time': 1.8675270080566406, 'global_step': 7483, 'preemption_count': 0}), (7607, {'train/loss': 0.12380439678141812, 'validation/loss': 0.12441053703721403, 'validation/num_examples': 83274637, 'test/loss': 0.12675144480526573, 'test/num_examples': 95000000, 'score': 7458.580144882202, 'total_duration': 49413.70057964325, 'accumulated_submission_time': 7458.580144882202, 'accumulated_eval_time': 41897.81136226654, 'accumulated_logging_time': 1.8874342441558838, 'global_step': 7607, 'preemption_count': 0}), (7733, {'train/loss': 0.12274766096258223, 'validation/loss': 0.1242992761180687, 'validation/num_examples': 83274637, 'test/loss': 0.12666050209535298, 'test/num_examples': 95000000, 'score': 7579.643595457077, 'total_duration': 50141.2838037014, 'accumulated_submission_time': 7579.643595457077, 'accumulated_eval_time': 42503.41873574257, 'accumulated_logging_time': 1.9065778255462646, 'global_step': 7733, 'preemption_count': 0}), (7858, {'train/loss': 0.12231447797709463, 'validation/loss': 0.12433469593730939, 'validation/num_examples': 83274637, 'test/loss': 0.12671314309672305, 'test/num_examples': 95000000, 'score': 7699.83989572525, 'total_duration': 50867.03058600426, 'accumulated_submission_time': 7699.83989572525, 'accumulated_eval_time': 43108.07256412506, 'accumulated_logging_time': 1.9271306991577148, 'global_step': 7858, 'preemption_count': 0})], 'global_step': 7988}
I0317 00:39:45.927462 140702629090496 submission_runner.py:649] Timing: 7820.088207244873
I0317 00:39:45.927500 140702629090496 submission_runner.py:651] Total number of evals: 65
I0317 00:39:45.927530 140702629090496 submission_runner.py:652] ====================
I0317 00:39:45.927652 140702629090496 submission_runner.py:750] Final criteo1tb score: 1
