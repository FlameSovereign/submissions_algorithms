torchrun --redirects 1:0,2:0,3:0,4:0,5:0,6:0,7:0 --standalone --nnodes=1 --nproc_per_node=8 submission_runner.py --framework=pytorch --workload=fastmri --submission_path=submissions_algorithms/leaderboard/external_tuning/shampoo_submission/submission.py --data_dir=/data/fastmri --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=submissions/rolling_leaderboard/external_tuning/shampoo/study_2 --overwrite=True --save_checkpoints=False --rng_seed=342425588 --torch_compile=true --tuning_ruleset=external --tuning_search_space=submissions_algorithms/leaderboard/external_tuning/shampoo_submission/tuning_search_space.json --num_tuning_trials=5 --hparam_start_index=0 --hparam_end_index=1 2>&1 | tee -a /logs/fastmri_pytorch_03-15-2025-18-57-53.log
W0315 18:57:56.974000 9 site-packages/torch/distributed/run.py:793] 
W0315 18:57:56.974000 9 site-packages/torch/distributed/run.py:793] *****************************************
W0315 18:57:56.974000 9 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0315 18:57:56.974000 9 site-packages/torch/distributed/run.py:793] *****************************************
2025-03-15 18:57:59.064231: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-03-15 18:57:59.064222: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-03-15 18:57:59.064222: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-03-15 18:57:59.064222: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-03-15 18:57:59.064222: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-03-15 18:57:59.064222: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-03-15 18:57:59.064273: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-03-15 18:57:59.064336: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1742065079.085878      44 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1742065079.085879      48 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1742065079.085874      51 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1742065079.085874      47 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1742065079.085881      46 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1742065079.085878      49 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1742065079.086110      45 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1742065079.086309      50 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1742065079.092480      48 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
E0000 00:00:1742065079.092480      49 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
E0000 00:00:1742065079.092481      46 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
E0000 00:00:1742065079.092481      51 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
E0000 00:00:1742065079.092487      47 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
E0000 00:00:1742065079.092497      44 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
E0000 00:00:1742065079.092756      45 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
E0000 00:00:1742065079.092986      50 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
[rank0]:[W315 18:58:08.365635180 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank3]:[W315 18:58:08.366356381 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank1]:[W315 18:58:08.459050228 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank7]:[W315 18:58:08.459144782 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 7]  using GPU 7 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank6]:[W315 18:58:08.459195205 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 6]  using GPU 6 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank5]:[W315 18:58:08.459865718 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 5]  using GPU 5 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank2]:[W315 18:58:08.461408793 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank4]:[W315 18:58:08.515261821 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 4]  using GPU 4 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
I0315 18:58:10.279017 140137582884032 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_2/fastmri_pytorch.
I0315 18:58:10.279020 140146293318848 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_2/fastmri_pytorch.
I0315 18:58:10.279017 140014493193408 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_2/fastmri_pytorch.
I0315 18:58:10.279017 139666531239104 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_2/fastmri_pytorch.
I0315 18:58:10.279041 140364750652608 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_2/fastmri_pytorch.
I0315 18:58:10.279025 140167162942656 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_2/fastmri_pytorch.
I0315 18:58:10.279045 140662492013760 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_2/fastmri_pytorch.
I0315 18:58:10.279159 140247122023616 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_2/fastmri_pytorch.
I0315 18:58:10.586314 140014493193408 logger_utils.py:97] Saving hparams to /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_2/fastmri_pytorch/trial_1/hparams.json.
I0315 18:58:10.586318 140364750652608 logger_utils.py:97] Saving hparams to /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_2/fastmri_pytorch/trial_1/hparams.json.
I0315 18:58:10.586335 139666531239104 logger_utils.py:97] Saving hparams to /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_2/fastmri_pytorch/trial_1/hparams.json.
I0315 18:58:10.586863 140146293318848 submission_runner.py:606] Using RNG seed 342425588
I0315 18:58:10.587210 140662492013760 logger_utils.py:97] Saving hparams to /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_2/fastmri_pytorch/trial_1/hparams.json.
I0315 18:58:10.588200 140146293318848 submission_runner.py:615] --- Tuning run 1/5 ---
I0315 18:58:10.587849 140247122023616 logger_utils.py:97] Saving hparams to /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_2/fastmri_pytorch/trial_1/hparams.json.
I0315 18:58:10.587949 140137582884032 logger_utils.py:97] Saving hparams to /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_2/fastmri_pytorch/trial_1/hparams.json.
I0315 18:58:10.588350 140146293318848 submission_runner.py:620] Creating tuning directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_2/fastmri_pytorch/trial_1.
I0315 18:58:10.588251 140167162942656 logger_utils.py:97] Saving hparams to /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_2/fastmri_pytorch/trial_1/hparams.json.
I0315 18:58:10.588604 140146293318848 logger_utils.py:97] Saving hparams to /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_2/fastmri_pytorch/trial_1/hparams.json.
I0315 18:58:11.016147 140146293318848 submission_runner.py:218] Initializing dataset.
I0315 18:58:11.016365 140146293318848 submission_runner.py:229] Initializing model.
I0315 18:58:11.191715 140146293318848 submission_runner.py:268] Performing `torch.compile`.
W0315 18:58:12.175038 139666531239104 distributed_shampoo.py:341] start_preconditioning_step set to -1. Setting start_preconditioning_step equal to precondition frequency 100 by default.
W0315 18:58:12.175063 140662492013760 distributed_shampoo.py:341] start_preconditioning_step set to -1. Setting start_preconditioning_step equal to precondition frequency 100 by default.
W0315 18:58:12.175078 140137582884032 distributed_shampoo.py:341] start_preconditioning_step set to -1. Setting start_preconditioning_step equal to precondition frequency 100 by default.
W0315 18:58:12.175204 139666531239104 distributed_shampoo.py:359] Nesterov flag is enabled but momentum parameter is zero! Continuing without using momentum or Nesterov acceleration...
W0315 18:58:12.175214 140662492013760 distributed_shampoo.py:359] Nesterov flag is enabled but momentum parameter is zero! Continuing without using momentum or Nesterov acceleration...
W0315 18:58:12.175223 140137582884032 distributed_shampoo.py:359] Nesterov flag is enabled but momentum parameter is zero! Continuing without using momentum or Nesterov acceleration...
W0315 18:58:12.175151 140014493193408 distributed_shampoo.py:341] start_preconditioning_step set to -1. Setting start_preconditioning_step equal to precondition frequency 100 by default.
W0315 18:58:12.175303 140014493193408 distributed_shampoo.py:359] Nesterov flag is enabled but momentum parameter is zero! Continuing without using momentum or Nesterov acceleration...
W0315 18:58:12.177471 140364750652608 distributed_shampoo.py:341] start_preconditioning_step set to -1. Setting start_preconditioning_step equal to precondition frequency 100 by default.
W0315 18:58:12.177654 140364750652608 distributed_shampoo.py:359] Nesterov flag is enabled but momentum parameter is zero! Continuing without using momentum or Nesterov acceleration...
I0315 18:58:12.178302 140662492013760 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 0.block_0 ([torch.Size([0])]) for Parameter 0 (torch.Size([32, 1, 3, 3])), Block block_0 (torch.Size([288])).
I0315 18:58:12.178319 139666531239104 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 0.block_0 ([torch.Size([0])]) for Parameter 0 (torch.Size([32, 1, 3, 3])), Block block_0 (torch.Size([288])).
I0315 18:58:12.178316 140137582884032 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 0.block_0 ([torch.Size([0])]) for Parameter 0 (torch.Size([32, 1, 3, 3])), Block block_0 (torch.Size([288])).
I0315 18:58:12.178450 140014493193408 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 0.block_0 ([torch.Size([288, 288])]) for Parameter 0 (torch.Size([32, 1, 3, 3])), Block block_0 (torch.Size([288])).
I0315 18:58:12.180840 139666531239104 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 1.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 1 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 18:58:12.180872 140137582884032 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 1.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 1 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 18:58:12.180950 140662492013760 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 1.block_0 ([torch.Size([1024, 1024]), torch.Size([9, 9])]) for Parameter 1 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 18:58:12.181035 139666531239104 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 2.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 2 (torch.Size([64, 32, 3, 3])), Block block_0 (torch.Size([64, 288])).
I0315 18:58:12.181049 140014493193408 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 1.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 1 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 18:58:12.181086 140137582884032 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 2.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 2 (torch.Size([64, 32, 3, 3])), Block block_0 (torch.Size([64, 288])).
I0315 18:58:12.181178 140662492013760 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 2.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 2 (torch.Size([64, 32, 3, 3])), Block block_0 (torch.Size([64, 288])).
I0315 18:58:12.181211 139666531239104 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 3.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 3 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 18:58:12.181262 140014493193408 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 2.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 2 (torch.Size([64, 32, 3, 3])), Block block_0 (torch.Size([64, 288])).
I0315 18:58:12.181352 139666531239104 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 4.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 4 (torch.Size([128, 64, 3, 3])), Block block_0 (torch.Size([128, 576])).
I0315 18:58:12.181348 140662492013760 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 3.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 3 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 18:58:12.181381 140137582884032 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 3.block_0 ([torch.Size([64, 64]), torch.Size([576, 576])]) for Parameter 3 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 18:58:12.181432 140014493193408 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 3.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 3 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 18:58:12.181518 139666531239104 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 5.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 5 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 18:58:12.181562 140137582884032 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 4.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 4 (torch.Size([128, 64, 3, 3])), Block block_0 (torch.Size([128, 576])).
I0315 18:58:12.181622 140014493193408 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 4.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 4 (torch.Size([128, 64, 3, 3])), Block block_0 (torch.Size([128, 576])).
I0315 18:58:12.181584 140662492013760 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 4.block_0 ([torch.Size([128, 128]), torch.Size([576, 576])]) for Parameter 4 (torch.Size([128, 64, 3, 3])), Block block_0 (torch.Size([128, 576])).
I0315 18:58:12.181703 139666531239104 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 6.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 6 (torch.Size([256, 128, 3, 3])), Block block_0 (torch.Size([256, 384, 3])).
I0315 18:58:12.181774 140137582884032 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 5.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 5 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 18:58:12.181809 140662492013760 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 5.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 5 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 18:58:12.181860 139666531239104 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 7.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 7 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 18:58:12.181940 140137582884032 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 6.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 6 (torch.Size([256, 128, 3, 3])), Block block_0 (torch.Size([256, 384, 3])).
I0315 18:58:12.181969 140662492013760 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 6.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 6 (torch.Size([256, 128, 3, 3])), Block block_0 (torch.Size([256, 384, 3])).
I0315 18:58:12.182110 140137582884032 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 7.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 7 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 18:58:12.182131 140662492013760 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 7.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 7 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 18:58:12.182270 140137582884032 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 8.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 8 (torch.Size([512, 256, 3, 3])), Block block_0 (torch.Size([512, 768, 3])).
I0315 18:58:12.182275 140662492013760 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 8.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 8 (torch.Size([512, 256, 3, 3])), Block block_0 (torch.Size([512, 768, 3])).
I0315 18:58:12.182354 140014493193408 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 5.block_0 ([torch.Size([128, 128]), torch.Size([384, 384]), torch.Size([3, 3])]) for Parameter 5 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 18:58:12.182418 140662492013760 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 9.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 9 (torch.Size([512, 512, 3, 3])), Block block_0 (torch.Size([512, 512, 9])).
I0315 18:58:12.182424 140137582884032 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 9.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 9 (torch.Size([512, 512, 3, 3])), Block block_0 (torch.Size([512, 512, 9])).
I0315 18:58:12.182564 140137582884032 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 10.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 10 (torch.Size([256, 512, 3, 3])), Block block_0 (torch.Size([256, 512, 9])).
I0315 18:58:12.182570 140662492013760 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 10.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 10 (torch.Size([256, 512, 3, 3])), Block block_0 (torch.Size([256, 512, 9])).
I0315 18:58:12.182602 139666531239104 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 8.block_0 ([torch.Size([512, 512]), torch.Size([768, 768]), torch.Size([3, 3])]) for Parameter 8 (torch.Size([512, 256, 3, 3])), Block block_0 (torch.Size([512, 768, 3])).
I0315 18:58:12.182710 140137582884032 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 11.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 11 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 18:58:12.182717 140662492013760 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 11.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 11 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 18:58:12.182795 139666531239104 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 9.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 9 (torch.Size([512, 512, 3, 3])), Block block_0 (torch.Size([512, 512, 9])).
I0315 18:58:12.182865 140137582884032 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 12.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 12 (torch.Size([128, 256, 3, 3])), Block block_0 (torch.Size([128, 768, 3])).
I0315 18:58:12.180457 140364750652608 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 0.block_0 ([torch.Size([0])]) for Parameter 0 (torch.Size([32, 1, 3, 3])), Block block_0 (torch.Size([288])).
I0315 18:58:12.182957 139666531239104 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 10.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 10 (torch.Size([256, 512, 3, 3])), Block block_0 (torch.Size([256, 512, 9])).
I0315 18:58:12.182964 140662492013760 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 12.block_0 ([torch.Size([128, 128]), torch.Size([768, 768]), torch.Size([3, 3])]) for Parameter 12 (torch.Size([128, 256, 3, 3])), Block block_0 (torch.Size([128, 768, 3])).
I0315 18:58:12.183011 140137582884032 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 13.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 13 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 18:58:12.183054 140014493193408 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 6.block_0 ([torch.Size([256, 256]), torch.Size([384, 384]), torch.Size([3, 3])]) for Parameter 6 (torch.Size([256, 128, 3, 3])), Block block_0 (torch.Size([256, 384, 3])).
I0315 18:58:12.183103 139666531239104 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 11.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 11 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 18:58:12.183170 140137582884032 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 14.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 14 (torch.Size([64, 128, 3, 3])), Block block_0 (torch.Size([64, 384, 3])).
I0315 18:58:12.183138 140364750652608 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 1.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 1 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 18:58:12.183246 139666531239104 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 12.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 12 (torch.Size([128, 256, 3, 3])), Block block_0 (torch.Size([128, 768, 3])).
I0315 18:58:12.183260 140014493193408 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 7.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 7 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 18:58:12.183355 140364750652608 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 2.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 2 (torch.Size([64, 32, 3, 3])), Block block_0 (torch.Size([64, 288])).
I0315 18:58:12.183390 139666531239104 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 13.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 13 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 18:58:12.183395 140137582884032 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 15.block_0 ([torch.Size([64, 64]), torch.Size([576, 576])]) for Parameter 15 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 18:58:12.183430 140014493193408 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 8.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 8 (torch.Size([512, 256, 3, 3])), Block block_0 (torch.Size([512, 768, 3])).
I0315 18:58:12.183534 139666531239104 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 14.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 14 (torch.Size([64, 128, 3, 3])), Block block_0 (torch.Size([64, 384, 3])).
I0315 18:58:12.183548 140137582884032 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 16.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 16 (torch.Size([32, 64, 3, 3])), Block block_0 (torch.Size([32, 576])).
I0315 18:58:12.183556 140364750652608 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 3.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 3 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 18:58:12.183587 140014493193408 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 9.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 9 (torch.Size([512, 512, 3, 3])), Block block_0 (torch.Size([512, 512, 9])).
I0315 18:58:12.183658 139666531239104 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 15.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 15 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 18:58:12.183629 140662492013760 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 13.block_0 ([torch.Size([128, 128]), torch.Size([384, 384]), torch.Size([3, 3])]) for Parameter 13 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 18:58:12.183740 140014493193408 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 10.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 10 (torch.Size([256, 512, 3, 3])), Block block_0 (torch.Size([256, 512, 9])).
I0315 18:58:12.183737 140364750652608 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 4.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 4 (torch.Size([128, 64, 3, 3])), Block block_0 (torch.Size([128, 576])).
I0315 18:58:12.183753 140137582884032 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 17.block_0 ([torch.Size([1024, 1024]), torch.Size([9, 9])]) for Parameter 17 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 18:58:12.183794 139666531239104 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 16.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 16 (torch.Size([32, 64, 3, 3])), Block block_0 (torch.Size([32, 576])).
I0315 18:58:12.183881 140137582884032 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 18.block_0 ([torch.Size([0])]) for Parameter 18 (torch.Size([1, 32, 1, 1])), Block block_0 (torch.Size([32])).
I0315 18:58:12.183898 140014493193408 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 11.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 11 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 18:58:12.183910 140662492013760 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 14.block_0 ([torch.Size([64, 64]), torch.Size([384, 384]), torch.Size([3, 3])]) for Parameter 14 (torch.Size([64, 128, 3, 3])), Block block_0 (torch.Size([64, 384, 3])).
I0315 18:58:12.183928 139666531239104 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 17.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 17 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 18:58:12.183948 140364750652608 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 5.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 5 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 18:58:12.183980 140137582884032 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 19.block_0 ([torch.Size([0])]) for Parameter 19 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0315 18:58:12.184017 139666531239104 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 18.block_0 ([torch.Size([0])]) for Parameter 18 (torch.Size([1, 32, 1, 1])), Block block_0 (torch.Size([32])).
I0315 18:58:12.184044 140014493193408 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 12.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 12 (torch.Size([128, 256, 3, 3])), Block block_0 (torch.Size([128, 768, 3])).
I0315 18:58:12.184068 140662492013760 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 15.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 15 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 18:58:12.184114 139666531239104 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 19.block_0 ([torch.Size([0])]) for Parameter 19 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0315 18:58:12.184150 140364750652608 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 6.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 6 (torch.Size([256, 128, 3, 3])), Block block_0 (torch.Size([256, 384, 3])).
I0315 18:58:12.184195 140014493193408 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 13.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 13 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 18:58:12.184211 140662492013760 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 16.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 16 (torch.Size([32, 64, 3, 3])), Block block_0 (torch.Size([32, 576])).
I0315 18:58:12.184236 139666531239104 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 20.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 20 (torch.Size([512, 256, 2, 2])), Block block_0 (torch.Size([512, 1024])).
I0315 18:58:12.184333 140662492013760 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 17.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 17 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 18:58:12.184354 140364750652608 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 7.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 7 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 18:58:12.184356 140014493193408 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 14.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 14 (torch.Size([64, 128, 3, 3])), Block block_0 (torch.Size([64, 384, 3])).
I0315 18:58:12.184364 139666531239104 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 21.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 21 (torch.Size([256, 128, 2, 2])), Block block_0 (torch.Size([256, 512])).
I0315 18:58:12.184428 140662492013760 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 18.block_0 ([torch.Size([0])]) for Parameter 18 (torch.Size([1, 32, 1, 1])), Block block_0 (torch.Size([32])).
I0315 18:58:12.184476 140014493193408 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 15.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 15 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 18:58:12.184505 139666531239104 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 22.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 22 (torch.Size([128, 64, 2, 2])), Block block_0 (torch.Size([128, 256])).
I0315 18:58:12.184521 140662492013760 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 19.block_0 ([torch.Size([0])]) for Parameter 19 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0315 18:58:12.184556 140364750652608 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 8.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 8 (torch.Size([512, 256, 3, 3])), Block block_0 (torch.Size([512, 768, 3])).
I0315 18:58:12.184608 140014493193408 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 16.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 16 (torch.Size([32, 64, 3, 3])), Block block_0 (torch.Size([32, 576])).
I0315 18:58:12.184623 139666531239104 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 23.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 23 (torch.Size([64, 32, 2, 2])), Block block_0 (torch.Size([64, 128])).
I0315 18:58:12.184626 140137582884032 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 20.block_0 ([torch.Size([512, 512]), torch.Size([1024, 1024])]) for Parameter 20 (torch.Size([512, 256, 2, 2])), Block block_0 (torch.Size([512, 1024])).
I0315 18:58:12.184638 140662492013760 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 20.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 20 (torch.Size([512, 256, 2, 2])), Block block_0 (torch.Size([512, 1024])).
I0315 18:58:12.184732 139666531239104 shampoo_preconditioner_list.py:612] Rank 1: ShampooPreconditionerList Numel Breakdown: (165888, 2097314, 174080, 671744, 696320, 327698, 426002, 1310738, 1703954, 1048738, 655522, 1310738, 1212434, 327698, 303122, 671744, 665600, 2097314, 2048, 2, 2621440, 655360, 163840, 40960)
I0315 18:58:12.184742 140014493193408 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 17.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 17 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 18:58:12.184757 140364750652608 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 9.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 9 (torch.Size([512, 512, 3, 3])), Block block_0 (torch.Size([512, 512, 9])).
I0315 18:58:12.184784 139666531239104 shampoo_preconditioner_list.py:615] Rank 1: ShampooPreconditionerList Bytes Breakdown: (663552,)
I0315 18:58:12.184796 140662492013760 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 21.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 21 (torch.Size([256, 128, 2, 2])), Block block_0 (torch.Size([256, 512])).
I0315 18:58:12.184823 139666531239104 shampoo_preconditioner_list.py:618] Rank 1: ShampooPreconditionerList Total Elements: 19350298
I0315 18:58:12.184816 140137582884032 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 21.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 21 (torch.Size([256, 128, 2, 2])), Block block_0 (torch.Size([256, 512])).
I0315 18:58:12.184859 139666531239104 shampoo_preconditioner_list.py:621] Rank 1: ShampooPreconditionerList Total Bytes: 663552
I0315 18:58:12.184915 140662492013760 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 22.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 22 (torch.Size([128, 64, 2, 2])), Block block_0 (torch.Size([128, 256])).
I0315 18:58:12.184919 140014493193408 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 18.block_0 ([torch.Size([32, 32])]) for Parameter 18 (torch.Size([1, 32, 1, 1])), Block block_0 (torch.Size([32])).
I0315 18:58:12.184953 140137582884032 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 22.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 22 (torch.Size([128, 64, 2, 2])), Block block_0 (torch.Size([128, 256])).
I0315 18:58:12.185072 139666531239104 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([32, 1, 3, 3])), Block block_0 (torch.Size([288])).
I0315 18:58:12.185085 140014493193408 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 19.block_0 ([torch.Size([1, 1])]) for Parameter 19 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0315 18:58:12.185109 140137582884032 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 23.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 23 (torch.Size([64, 32, 2, 2])), Block block_0 (torch.Size([64, 128])).
I0315 18:58:12.185152 139666531239104 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 18:58:12.185150 140662492013760 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 23.block_0 ([torch.Size([64, 64]), torch.Size([128, 128])]) for Parameter 23 (torch.Size([64, 32, 2, 2])), Block block_0 (torch.Size([64, 128])).
I0315 18:58:12.185217 139666531239104 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([64, 32, 3, 3])), Block block_0 (torch.Size([64, 288])).
I0315 18:58:12.185242 140014493193408 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 20.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 20 (torch.Size([512, 256, 2, 2])), Block block_0 (torch.Size([512, 1024])).
I0315 18:58:12.185240 140137582884032 shampoo_preconditioner_list.py:612] Rank 5: ShampooPreconditionerList Numel Breakdown: (165888, 2097314, 174080, 671744, 696320, 327698, 426002, 1310738, 1703954, 1048738, 655522, 1310738, 1212434, 327698, 303122, 671744, 665600, 2097314, 2048, 2, 2621440, 655360, 163840, 40960)
I0315 18:58:12.185279 139666531239104 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 18:58:12.185288 140137582884032 shampoo_preconditioner_list.py:615] Rank 5: ShampooPreconditionerList Bytes Breakdown: (663552, 8389256, 696320, 2686976)
I0315 18:58:12.185287 140662492013760 shampoo_preconditioner_list.py:612] Rank 7: ShampooPreconditionerList Numel Breakdown: (165888, 2097314, 174080, 671744, 696320, 327698, 426002, 1310738, 1703954, 1048738, 655522, 1310738, 1212434, 327698, 303122, 671744, 665600, 2097314, 2048, 2, 2621440, 655360, 163840, 40960)
I0315 18:58:12.185324 140137582884032 shampoo_preconditioner_list.py:618] Rank 5: ShampooPreconditionerList Total Elements: 19350298
I0315 18:58:12.185334 140662492013760 shampoo_preconditioner_list.py:615] Rank 7: ShampooPreconditionerList Bytes Breakdown: (663552, 8389256, 696320, 2686976, 2785280, 1310792)
I0315 18:58:12.185340 139666531239104 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 4.block_0 (torch.Size([0])) for Parameter 4 (torch.Size([128, 64, 3, 3])), Block block_0 (torch.Size([128, 576])).
I0315 18:58:12.185363 140137582884032 shampoo_preconditioner_list.py:621] Rank 5: ShampooPreconditionerList Total Bytes: 12436104
I0315 18:58:12.185370 140662492013760 shampoo_preconditioner_list.py:618] Rank 7: ShampooPreconditionerList Total Elements: 19350298
I0315 18:58:12.185395 139666531239104 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 5.block_0 (torch.Size([0])) for Parameter 5 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 18:58:12.185403 140662492013760 shampoo_preconditioner_list.py:621] Rank 7: ShampooPreconditionerList Total Bytes: 16532176
I0315 18:58:12.185448 139666531239104 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 6.block_0 (torch.Size([0])) for Parameter 6 (torch.Size([256, 128, 3, 3])), Block block_0 (torch.Size([256, 384, 3])).
I0315 18:58:12.185516 139666531239104 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 7.block_0 (torch.Size([0])) for Parameter 7 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 18:58:12.185555 140137582884032 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([32, 1, 3, 3])), Block block_0 (torch.Size([288])).
I0315 18:58:12.185580 140662492013760 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([32, 1, 3, 3])), Block block_0 (torch.Size([288])).
I0315 18:58:12.185651 140137582884032 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 18:58:12.185649 139666531239104 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 8.block_0 (torch.Size([512, 768, 3])) for Parameter 8 (torch.Size([512, 256, 3, 3])), Block block_0 (torch.Size([512, 768, 3])).
I0315 18:58:12.185646 140364750652608 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 10.block_0 ([torch.Size([256, 256]), torch.Size([512, 512]), torch.Size([9, 9])]) for Parameter 10 (torch.Size([256, 512, 3, 3])), Block block_0 (torch.Size([256, 512, 9])).
I0315 18:58:12.185719 140137582884032 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([64, 32, 3, 3])), Block block_0 (torch.Size([64, 288])).
I0315 18:58:12.185728 139666531239104 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 9.block_0 (torch.Size([0])) for Parameter 9 (torch.Size([512, 512, 3, 3])), Block block_0 (torch.Size([512, 512, 9])).
I0315 18:58:12.185736 140662492013760 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([1024, 9])) for Parameter 1 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 18:58:12.185784 139666531239104 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 10.block_0 (torch.Size([0])) for Parameter 10 (torch.Size([256, 512, 3, 3])), Block block_0 (torch.Size([256, 512, 9])).
I0315 18:58:12.185825 140662492013760 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([64, 32, 3, 3])), Block block_0 (torch.Size([64, 288])).
I0315 18:58:12.185849 139666531239104 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 11.block_0 (torch.Size([0])) for Parameter 11 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 18:58:12.185842 140137582884032 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([64, 576])) for Parameter 3 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 18:58:12.185890 140662492013760 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 18:58:12.185888 140014493193408 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 21.block_0 ([torch.Size([256, 256]), torch.Size([512, 512])]) for Parameter 21 (torch.Size([256, 128, 2, 2])), Block block_0 (torch.Size([256, 512])).
I0315 18:58:12.185916 140137582884032 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 4.block_0 (torch.Size([0])) for Parameter 4 (torch.Size([128, 64, 3, 3])), Block block_0 (torch.Size([128, 576])).
I0315 18:58:12.185918 139666531239104 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 12.block_0 (torch.Size([0])) for Parameter 12 (torch.Size([128, 256, 3, 3])), Block block_0 (torch.Size([128, 768, 3])).
I0315 18:58:12.185920 140364750652608 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 11.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 11 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 18:58:12.185972 139666531239104 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 13.block_0 (torch.Size([0])) for Parameter 13 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 18:58:12.185972 140137582884032 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 5.block_0 (torch.Size([0])) for Parameter 5 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 18:58:12.186027 140137582884032 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 6.block_0 (torch.Size([0])) for Parameter 6 (torch.Size([256, 128, 3, 3])), Block block_0 (torch.Size([256, 384, 3])).
I0315 18:58:12.186031 139666531239104 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 14.block_0 (torch.Size([0])) for Parameter 14 (torch.Size([64, 128, 3, 3])), Block block_0 (torch.Size([64, 384, 3])).
I0315 18:58:12.186083 139666531239104 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 15.block_0 (torch.Size([0])) for Parameter 15 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 18:58:12.186099 140137582884032 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 7.block_0 (torch.Size([0])) for Parameter 7 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 18:58:12.186129 140364750652608 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 12.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 12 (torch.Size([128, 256, 3, 3])), Block block_0 (torch.Size([128, 768, 3])).
I0315 18:58:12.186141 139666531239104 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 16.block_0 (torch.Size([0])) for Parameter 16 (torch.Size([32, 64, 3, 3])), Block block_0 (torch.Size([32, 576])).
I0315 18:58:12.186135 140014493193408 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 22.block_0 ([torch.Size([128, 128]), torch.Size([256, 256])]) for Parameter 22 (torch.Size([128, 64, 2, 2])), Block block_0 (torch.Size([128, 256])).
I0315 18:58:12.186154 140137582884032 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 8.block_0 (torch.Size([0])) for Parameter 8 (torch.Size([512, 256, 3, 3])), Block block_0 (torch.Size([512, 768, 3])).
I0315 18:58:12.186192 139666531239104 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 17.block_0 (torch.Size([0])) for Parameter 17 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 18:58:12.186208 140137582884032 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 9.block_0 (torch.Size([0])) for Parameter 9 (torch.Size([512, 512, 3, 3])), Block block_0 (torch.Size([512, 512, 9])).
I0315 18:58:12.186251 139666531239104 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 18.block_0 (torch.Size([0])) for Parameter 18 (torch.Size([1, 32, 1, 1])), Block block_0 (torch.Size([32])).
I0315 18:58:12.186260 140137582884032 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 10.block_0 (torch.Size([0])) for Parameter 10 (torch.Size([256, 512, 3, 3])), Block block_0 (torch.Size([256, 512, 9])).
I0315 18:58:12.186277 140014493193408 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 23.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 23 (torch.Size([64, 32, 2, 2])), Block block_0 (torch.Size([64, 128])).
I0315 18:58:12.186308 139666531239104 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 19.block_0 (torch.Size([0])) for Parameter 19 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0315 18:58:12.186312 140137582884032 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 11.block_0 (torch.Size([0])) for Parameter 11 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 18:58:12.186330 140364750652608 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 13.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 13 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 18:58:12.186377 140137582884032 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 12.block_0 (torch.Size([0])) for Parameter 12 (torch.Size([128, 256, 3, 3])), Block block_0 (torch.Size([128, 768, 3])).
I0315 18:58:12.186354 140662492013760 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 4.block_0 (torch.Size([128, 576])) for Parameter 4 (torch.Size([128, 64, 3, 3])), Block block_0 (torch.Size([128, 576])).
I0315 18:58:12.186378 139666531239104 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 20.block_0 (torch.Size([0])) for Parameter 20 (torch.Size([512, 256, 2, 2])), Block block_0 (torch.Size([512, 1024])).
I0315 18:58:12.186427 139666531239104 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 21.block_0 (torch.Size([0])) for Parameter 21 (torch.Size([256, 128, 2, 2])), Block block_0 (torch.Size([256, 512])).
I0315 18:58:12.186425 140014493193408 shampoo_preconditioner_list.py:612] Rank 6: ShampooPreconditionerList Numel Breakdown: (165888, 2097314, 174080, 671744, 696320, 327698, 426002, 1310738, 1703954, 1048738, 655522, 1310738, 1212434, 327698, 303122, 671744, 665600, 2097314, 2048, 2, 2621440, 655360, 163840, 40960)
I0315 18:58:12.186435 140137582884032 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 13.block_0 (torch.Size([0])) for Parameter 13 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 18:58:12.186454 140662492013760 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 5.block_0 (torch.Size([0])) for Parameter 5 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 18:58:12.186469 140014493193408 shampoo_preconditioner_list.py:615] Rank 6: ShampooPreconditionerList Bytes Breakdown: (663552, 8389256, 696320, 2686976, 2785280, 1310792, 1704008)
I0315 18:58:12.186483 139666531239104 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 22.block_0 (torch.Size([0])) for Parameter 22 (torch.Size([128, 64, 2, 2])), Block block_0 (torch.Size([128, 256])).
I0315 18:58:12.186486 140137582884032 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 14.block_0 (torch.Size([0])) for Parameter 14 (torch.Size([64, 128, 3, 3])), Block block_0 (torch.Size([64, 384, 3])).
I0315 18:58:12.186503 140014493193408 shampoo_preconditioner_list.py:618] Rank 6: ShampooPreconditionerList Total Elements: 19350298
I0315 18:58:12.186526 140662492013760 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 6.block_0 (torch.Size([0])) for Parameter 6 (torch.Size([256, 128, 3, 3])), Block block_0 (torch.Size([256, 384, 3])).
I0315 18:58:12.186536 140014493193408 shampoo_preconditioner_list.py:621] Rank 6: ShampooPreconditionerList Total Bytes: 18236184
I0315 18:58:12.186540 139666531239104 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 23.block_0 (torch.Size([0])) for Parameter 23 (torch.Size([64, 32, 2, 2])), Block block_0 (torch.Size([64, 128])).
I0315 18:58:12.186552 140364750652608 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 14.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 14 (torch.Size([64, 128, 3, 3])), Block block_0 (torch.Size([64, 384, 3])).
I0315 18:58:12.186580 140137582884032 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 15.block_0 (torch.Size([64, 576])) for Parameter 15 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 18:58:12.186593 140662492013760 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 7.block_0 (torch.Size([0])) for Parameter 7 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 18:58:12.186608 139666531239104 shampoo_preconditioner_list.py:375] Rank 1: AdaGradPreconditionerList Numel Breakdown: (0, 0, 0, 0, 0, 0, 0, 0, 1179648, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)
I0315 18:58:12.186649 139666531239104 shampoo_preconditioner_list.py:378] Rank 1: AdaGradPreconditionerList Bytes Breakdown: (0, 0, 0, 0, 0, 0, 0, 0, 4718592, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)
I0315 18:58:12.186655 140662492013760 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 8.block_0 (torch.Size([0])) for Parameter 8 (torch.Size([512, 256, 3, 3])), Block block_0 (torch.Size([512, 768, 3])).
I0315 18:58:12.186657 140137582884032 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 16.block_0 (torch.Size([0])) for Parameter 16 (torch.Size([32, 64, 3, 3])), Block block_0 (torch.Size([32, 576])).
I0315 18:58:12.186684 139666531239104 shampoo_preconditioner_list.py:381] Rank 1: AdaGradPreconditionerList Total Elements: 1179648
I0315 18:58:12.186709 140662492013760 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 9.block_0 (torch.Size([0])) for Parameter 9 (torch.Size([512, 512, 3, 3])), Block block_0 (torch.Size([512, 512, 9])).
I0315 18:58:12.186708 140364750652608 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 15.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 15 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 18:58:12.186719 139666531239104 shampoo_preconditioner_list.py:384] Rank 1: AdaGradPreconditionerList Total Bytes: 4718592
I0315 18:58:12.186750 140137582884032 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 17.block_0 (torch.Size([1024, 9])) for Parameter 17 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 18:58:12.186774 140662492013760 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 10.block_0 (torch.Size([0])) for Parameter 10 (torch.Size([256, 512, 3, 3])), Block block_0 (torch.Size([256, 512, 9])).
I0315 18:58:12.186778 140014493193408 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([288])) for Parameter 0 (torch.Size([32, 1, 3, 3])), Block block_0 (torch.Size([288])).
I0315 18:58:12.186824 140137582884032 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 18.block_0 (torch.Size([0])) for Parameter 18 (torch.Size([1, 32, 1, 1])), Block block_0 (torch.Size([32])).
I0315 18:58:12.186835 140662492013760 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 11.block_0 (torch.Size([0])) for Parameter 11 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 18:58:12.186881 140137582884032 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 19.block_0 (torch.Size([0])) for Parameter 19 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0315 18:58:12.186877 140014493193408 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 18:58:12.186882 140364750652608 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 16.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 16 (torch.Size([32, 64, 3, 3])), Block block_0 (torch.Size([32, 576])).
I0315 18:58:12.186927 140662492013760 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 12.block_0 (torch.Size([128, 768, 3])) for Parameter 12 (torch.Size([128, 256, 3, 3])), Block block_0 (torch.Size([128, 768, 3])).
I0315 18:58:12.186937 140014493193408 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([64, 32, 3, 3])), Block block_0 (torch.Size([64, 288])).
I0315 18:58:12.186969 140137582884032 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 20.block_0 (torch.Size([512, 1024])) for Parameter 20 (torch.Size([512, 256, 2, 2])), Block block_0 (torch.Size([512, 1024])).
I0315 18:58:12.186990 140014493193408 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 18:58:12.187020 140662492013760 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 13.block_0 (torch.Size([128, 384, 3])) for Parameter 13 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 18:58:12.187032 140364750652608 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 17.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 17 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 18:58:12.187040 140137582884032 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 21.block_0 (torch.Size([0])) for Parameter 21 (torch.Size([256, 128, 2, 2])), Block block_0 (torch.Size([256, 512])).
I0315 18:58:12.187043 140014493193408 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 4.block_0 (torch.Size([0])) for Parameter 4 (torch.Size([128, 64, 3, 3])), Block block_0 (torch.Size([128, 576])).
I0315 18:58:12.187093 140137582884032 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 22.block_0 (torch.Size([0])) for Parameter 22 (torch.Size([128, 64, 2, 2])), Block block_0 (torch.Size([128, 256])).
I0315 18:58:12.187106 140662492013760 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 14.block_0 (torch.Size([64, 384, 3])) for Parameter 14 (torch.Size([64, 128, 3, 3])), Block block_0 (torch.Size([64, 384, 3])).
I0315 18:58:12.187144 140364750652608 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 18.block_0 ([torch.Size([0])]) for Parameter 18 (torch.Size([1, 32, 1, 1])), Block block_0 (torch.Size([32])).
I0315 18:58:12.187154 140137582884032 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 23.block_0 (torch.Size([0])) for Parameter 23 (torch.Size([64, 32, 2, 2])), Block block_0 (torch.Size([64, 128])).
I0315 18:58:12.187169 140662492013760 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 15.block_0 (torch.Size([0])) for Parameter 15 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 18:58:12.187222 140137582884032 shampoo_preconditioner_list.py:375] Rank 5: AdaGradPreconditionerList Numel Breakdown: (0, 0, 0, 36864, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 36864, 0, 9216, 0, 0, 524288, 0, 0, 0)
I0315 18:58:12.187230 140662492013760 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 16.block_0 (torch.Size([0])) for Parameter 16 (torch.Size([32, 64, 3, 3])), Block block_0 (torch.Size([32, 576])).
I0315 18:58:12.187257 140364750652608 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 19.block_0 ([torch.Size([0])]) for Parameter 19 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0315 18:58:12.187264 140137582884032 shampoo_preconditioner_list.py:378] Rank 5: AdaGradPreconditionerList Bytes Breakdown: (0, 0, 0, 147456, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 147456, 0, 36864, 0, 0, 2097152, 0, 0, 0)
I0315 18:58:12.187281 140662492013760 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 17.block_0 (torch.Size([0])) for Parameter 17 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 18:58:12.187295 140137582884032 shampoo_preconditioner_list.py:381] Rank 5: AdaGradPreconditionerList Total Elements: 607232
I0315 18:58:12.187325 140137582884032 shampoo_preconditioner_list.py:384] Rank 5: AdaGradPreconditionerList Total Bytes: 2428928
I0315 18:58:12.187339 140662492013760 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 18.block_0 (torch.Size([0])) for Parameter 18 (torch.Size([1, 32, 1, 1])), Block block_0 (torch.Size([32])).
I0315 18:58:12.187392 140662492013760 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 19.block_0 (torch.Size([0])) for Parameter 19 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0315 18:58:12.187417 140364750652608 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 20.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 20 (torch.Size([512, 256, 2, 2])), Block block_0 (torch.Size([512, 1024])).
I0315 18:58:12.187447 140662492013760 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 20.block_0 (torch.Size([0])) for Parameter 20 (torch.Size([512, 256, 2, 2])), Block block_0 (torch.Size([512, 1024])).
I0315 18:58:12.187503 140662492013760 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 21.block_0 (torch.Size([0])) for Parameter 21 (torch.Size([256, 128, 2, 2])), Block block_0 (torch.Size([256, 512])).
I0315 18:58:12.187498 140014493193408 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 5.block_0 (torch.Size([128, 384, 3])) for Parameter 5 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 18:58:12.187558 140662492013760 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 22.block_0 (torch.Size([0])) for Parameter 22 (torch.Size([128, 64, 2, 2])), Block block_0 (torch.Size([128, 256])).
I0315 18:58:12.187585 140364750652608 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 21.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 21 (torch.Size([256, 128, 2, 2])), Block block_0 (torch.Size([256, 512])).
I0315 18:58:12.187635 140014493193408 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 6.block_0 (torch.Size([256, 384, 3])) for Parameter 6 (torch.Size([256, 128, 3, 3])), Block block_0 (torch.Size([256, 384, 3])).
I0315 18:58:12.187654 140662492013760 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 23.block_0 (torch.Size([64, 128])) for Parameter 23 (torch.Size([64, 32, 2, 2])), Block block_0 (torch.Size([64, 128])).
I0315 18:58:12.187735 140662492013760 shampoo_preconditioner_list.py:375] Rank 7: AdaGradPreconditionerList Numel Breakdown: (0, 9216, 0, 0, 73728, 0, 0, 0, 0, 0, 0, 0, 294912, 147456, 73728, 0, 0, 0, 0, 0, 0, 0, 0, 8192)
I0315 18:58:12.187737 140014493193408 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 7.block_0 (torch.Size([0])) for Parameter 7 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 18:58:12.187747 140364750652608 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 22.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 22 (torch.Size([128, 64, 2, 2])), Block block_0 (torch.Size([128, 256])).
I0315 18:58:12.187778 140662492013760 shampoo_preconditioner_list.py:378] Rank 7: AdaGradPreconditionerList Bytes Breakdown: (0, 36864, 0, 0, 294912, 0, 0, 0, 0, 0, 0, 0, 1179648, 589824, 294912, 0, 0, 0, 0, 0, 0, 0, 0, 32768)
I0315 18:58:12.187797 140014493193408 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 8.block_0 (torch.Size([0])) for Parameter 8 (torch.Size([512, 256, 3, 3])), Block block_0 (torch.Size([512, 768, 3])).
I0315 18:58:12.187815 140662492013760 shampoo_preconditioner_list.py:381] Rank 7: AdaGradPreconditionerList Total Elements: 607232
I0315 18:58:12.187846 140662492013760 shampoo_preconditioner_list.py:384] Rank 7: AdaGradPreconditionerList Total Bytes: 2428928
I0315 18:58:12.187826 139666531239104 logger_utils.py:262] Unable to record workload.train_mean information. Continuing without it.
I0315 18:58:12.187858 140014493193408 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 9.block_0 (torch.Size([0])) for Parameter 9 (torch.Size([512, 512, 3, 3])), Block block_0 (torch.Size([512, 512, 9])).
I0315 18:58:12.187900 139666531239104 logger_utils.py:262] Unable to record workload.train_stddev information. Continuing without it.
I0315 18:58:12.187905 140364750652608 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 23.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 23 (torch.Size([64, 32, 2, 2])), Block block_0 (torch.Size([64, 128])).
I0315 18:58:12.187912 140014493193408 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 10.block_0 (torch.Size([0])) for Parameter 10 (torch.Size([256, 512, 3, 3])), Block block_0 (torch.Size([256, 512, 9])).
I0315 18:58:12.187964 140014493193408 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 11.block_0 (torch.Size([0])) for Parameter 11 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 18:58:12.188015 140364750652608 shampoo_preconditioner_list.py:612] Rank 2: ShampooPreconditionerList Numel Breakdown: (165888, 2097314, 174080, 671744, 696320, 327698, 426002, 1310738, 1703954, 1048738, 655522, 1310738, 1212434, 327698, 303122, 671744, 665600, 2097314, 2048, 2, 2621440, 655360, 163840, 40960)
I0315 18:58:12.188024 140014493193408 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 12.block_0 (torch.Size([0])) for Parameter 12 (torch.Size([128, 256, 3, 3])), Block block_0 (torch.Size([128, 768, 3])).
I0315 18:58:12.188069 140364750652608 shampoo_preconditioner_list.py:615] Rank 2: ShampooPreconditionerList Bytes Breakdown: (663552,)
I0315 18:58:12.188077 140014493193408 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 13.block_0 (torch.Size([0])) for Parameter 13 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 18:58:12.188111 140364750652608 shampoo_preconditioner_list.py:618] Rank 2: ShampooPreconditionerList Total Elements: 19350298
I0315 18:58:12.188128 140014493193408 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 14.block_0 (torch.Size([0])) for Parameter 14 (torch.Size([64, 128, 3, 3])), Block block_0 (torch.Size([64, 384, 3])).
I0315 18:58:12.188152 140364750652608 shampoo_preconditioner_list.py:621] Rank 2: ShampooPreconditionerList Total Bytes: 663552
I0315 18:58:12.188179 140014493193408 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 15.block_0 (torch.Size([0])) for Parameter 15 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 18:58:12.188235 140014493193408 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 16.block_0 (torch.Size([0])) for Parameter 16 (torch.Size([32, 64, 3, 3])), Block block_0 (torch.Size([32, 576])).
I0315 18:58:12.188292 140014493193408 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 17.block_0 (torch.Size([0])) for Parameter 17 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 18:58:12.188359 140364750652608 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([32, 1, 3, 3])), Block block_0 (torch.Size([288])).
I0315 18:58:12.188387 140014493193408 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 18.block_0 (torch.Size([32])) for Parameter 18 (torch.Size([1, 32, 1, 1])), Block block_0 (torch.Size([32])).
I0315 18:58:12.188443 140364750652608 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 18:58:12.188480 140014493193408 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 19.block_0 (torch.Size([1])) for Parameter 19 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0315 18:58:12.188481 140137582884032 logger_utils.py:262] Unable to record workload.train_mean information. Continuing without it.
I0315 18:58:12.188512 140364750652608 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([64, 32, 3, 3])), Block block_0 (torch.Size([64, 288])).
I0315 18:58:12.188552 140014493193408 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 20.block_0 (torch.Size([0])) for Parameter 20 (torch.Size([512, 256, 2, 2])), Block block_0 (torch.Size([512, 1024])).
I0315 18:58:12.188558 140137582884032 logger_utils.py:262] Unable to record workload.train_stddev information. Continuing without it.
I0315 18:58:12.188572 140364750652608 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 18:58:12.188631 140364750652608 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 4.block_0 (torch.Size([0])) for Parameter 4 (torch.Size([128, 64, 3, 3])), Block block_0 (torch.Size([128, 576])).
I0315 18:58:12.188639 140014493193408 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 21.block_0 (torch.Size([256, 512])) for Parameter 21 (torch.Size([256, 128, 2, 2])), Block block_0 (torch.Size([256, 512])).
I0315 18:58:12.188700 140364750652608 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 5.block_0 (torch.Size([0])) for Parameter 5 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 18:58:12.188738 140014493193408 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 22.block_0 (torch.Size([128, 256])) for Parameter 22 (torch.Size([128, 64, 2, 2])), Block block_0 (torch.Size([128, 256])).
I0315 18:58:12.188767 140364750652608 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 6.block_0 (torch.Size([0])) for Parameter 6 (torch.Size([256, 128, 3, 3])), Block block_0 (torch.Size([256, 384, 3])).
I0315 18:58:12.188814 140014493193408 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 23.block_0 (torch.Size([0])) for Parameter 23 (torch.Size([64, 32, 2, 2])), Block block_0 (torch.Size([64, 128])).
I0315 18:58:12.188840 140364750652608 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 7.block_0 (torch.Size([0])) for Parameter 7 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 18:58:12.188891 140014493193408 shampoo_preconditioner_list.py:375] Rank 6: AdaGradPreconditionerList Numel Breakdown: (288, 0, 0, 0, 0, 147456, 294912, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 32, 1, 0, 131072, 32768, 0)
I0315 18:58:12.188914 140364750652608 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 8.block_0 (torch.Size([0])) for Parameter 8 (torch.Size([512, 256, 3, 3])), Block block_0 (torch.Size([512, 768, 3])).
I0315 18:58:12.188934 140014493193408 shampoo_preconditioner_list.py:378] Rank 6: AdaGradPreconditionerList Bytes Breakdown: (1152, 0, 0, 0, 0, 589824, 1179648, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 128, 4, 0, 524288, 131072, 0)
I0315 18:58:12.188966 140014493193408 shampoo_preconditioner_list.py:381] Rank 6: AdaGradPreconditionerList Total Elements: 606529
I0315 18:58:12.188982 140364750652608 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 9.block_0 (torch.Size([0])) for Parameter 9 (torch.Size([512, 512, 3, 3])), Block block_0 (torch.Size([512, 512, 9])).
I0315 18:58:12.189000 140014493193408 shampoo_preconditioner_list.py:384] Rank 6: AdaGradPreconditionerList Total Bytes: 2426116
I0315 18:58:12.189113 140364750652608 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 10.block_0 (torch.Size([256, 512, 9])) for Parameter 10 (torch.Size([256, 512, 3, 3])), Block block_0 (torch.Size([256, 512, 9])).
I0315 18:58:12.189203 140364750652608 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 11.block_0 (torch.Size([0])) for Parameter 11 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 18:58:12.189271 140364750652608 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 12.block_0 (torch.Size([0])) for Parameter 12 (torch.Size([128, 256, 3, 3])), Block block_0 (torch.Size([128, 768, 3])).
I0315 18:58:12.189335 140364750652608 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 13.block_0 (torch.Size([0])) for Parameter 13 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 18:58:12.189399 140364750652608 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 14.block_0 (torch.Size([0])) for Parameter 14 (torch.Size([64, 128, 3, 3])), Block block_0 (torch.Size([64, 384, 3])).
I0315 18:58:12.189407 140662492013760 logger_utils.py:262] Unable to record workload.train_mean information. Continuing without it.
I0315 18:58:12.189462 140364750652608 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 15.block_0 (torch.Size([0])) for Parameter 15 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 18:58:12.189483 140662492013760 logger_utils.py:262] Unable to record workload.train_stddev information. Continuing without it.
I0315 18:58:12.189526 140364750652608 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 16.block_0 (torch.Size([0])) for Parameter 16 (torch.Size([32, 64, 3, 3])), Block block_0 (torch.Size([32, 576])).
I0315 18:58:12.189623 140364750652608 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 17.block_0 (torch.Size([0])) for Parameter 17 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 18:58:12.189717 140364750652608 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 18.block_0 (torch.Size([0])) for Parameter 18 (torch.Size([1, 32, 1, 1])), Block block_0 (torch.Size([32])).
I0315 18:58:12.189783 140364750652608 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 19.block_0 (torch.Size([0])) for Parameter 19 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0315 18:58:12.189850 140364750652608 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 20.block_0 (torch.Size([0])) for Parameter 20 (torch.Size([512, 256, 2, 2])), Block block_0 (torch.Size([512, 1024])).
I0315 18:58:12.189925 140364750652608 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 21.block_0 (torch.Size([0])) for Parameter 21 (torch.Size([256, 128, 2, 2])), Block block_0 (torch.Size([256, 512])).
I0315 18:58:12.189991 140364750652608 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 22.block_0 (torch.Size([0])) for Parameter 22 (torch.Size([128, 64, 2, 2])), Block block_0 (torch.Size([128, 256])).
I0315 18:58:12.190060 140364750652608 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 23.block_0 (torch.Size([0])) for Parameter 23 (torch.Size([64, 32, 2, 2])), Block block_0 (torch.Size([64, 128])).
I0315 18:58:12.190140 140364750652608 shampoo_preconditioner_list.py:375] Rank 2: AdaGradPreconditionerList Numel Breakdown: (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1179648, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)
I0315 18:58:12.190186 140364750652608 shampoo_preconditioner_list.py:378] Rank 2: AdaGradPreconditionerList Bytes Breakdown: (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4718592, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)
I0315 18:58:12.190227 140364750652608 shampoo_preconditioner_list.py:381] Rank 2: AdaGradPreconditionerList Total Elements: 1179648
I0315 18:58:12.190267 140364750652608 shampoo_preconditioner_list.py:384] Rank 2: AdaGradPreconditionerList Total Bytes: 4718592
I0315 18:58:12.190707 140014493193408 logger_utils.py:262] Unable to record workload.train_mean information. Continuing without it.
I0315 18:58:12.190785 140014493193408 logger_utils.py:262] Unable to record workload.train_stddev information. Continuing without it.
I0315 18:58:12.191540 140364750652608 logger_utils.py:262] Unable to record workload.train_mean information. Continuing without it.
I0315 18:58:12.191631 140364750652608 logger_utils.py:262] Unable to record workload.train_stddev information. Continuing without it.
W0315 18:58:12.265212 140167162942656 distributed_shampoo.py:341] start_preconditioning_step set to -1. Setting start_preconditioning_step equal to precondition frequency 100 by default.
W0315 18:58:12.265396 140167162942656 distributed_shampoo.py:359] Nesterov flag is enabled but momentum parameter is zero! Continuing without using momentum or Nesterov acceleration...
I0315 18:58:12.268425 140167162942656 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 0.block_0 ([torch.Size([0])]) for Parameter 0 (torch.Size([32, 1, 3, 3])), Block block_0 (torch.Size([288])).
I0315 18:58:12.270755 140167162942656 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 1.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 1 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 18:58:12.270936 140167162942656 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 2.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 2 (torch.Size([64, 32, 3, 3])), Block block_0 (torch.Size([64, 288])).
I0315 18:58:12.271016 140146293318848 submission_runner.py:272] Initializing optimizer.
I0315 18:58:12.271098 140167162942656 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 3.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 3 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 18:58:12.271234 140167162942656 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 4.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 4 (torch.Size([128, 64, 3, 3])), Block block_0 (torch.Size([128, 576])).
I0315 18:58:12.271392 140167162942656 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 5.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 5 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 18:58:12.271557 140167162942656 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 6.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 6 (torch.Size([256, 128, 3, 3])), Block block_0 (torch.Size([256, 384, 3])).
I0315 18:58:12.271703 140167162942656 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 7.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 7 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 18:58:12.271873 140167162942656 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 8.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 8 (torch.Size([512, 256, 3, 3])), Block block_0 (torch.Size([512, 768, 3])).
I0315 18:58:12.272038 140167162942656 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 9.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 9 (torch.Size([512, 512, 3, 3])), Block block_0 (torch.Size([512, 512, 9])).
I0315 18:58:12.272177 140167162942656 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 10.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 10 (torch.Size([256, 512, 3, 3])), Block block_0 (torch.Size([256, 512, 9])).
I0315 18:58:12.272462 140167162942656 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 11.block_0 ([torch.Size([256, 256]), torch.Size([768, 768]), torch.Size([3, 3])]) for Parameter 11 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
W0315 18:58:12.272530 140146293318848 distributed_shampoo.py:341] start_preconditioning_step set to -1. Setting start_preconditioning_step equal to precondition frequency 100 by default.
W0315 18:58:12.272648 140146293318848 distributed_shampoo.py:359] Nesterov flag is enabled but momentum parameter is zero! Continuing without using momentum or Nesterov acceleration...
I0315 18:58:12.272644 140167162942656 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 12.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 12 (torch.Size([128, 256, 3, 3])), Block block_0 (torch.Size([128, 768, 3])).
I0315 18:58:12.272779 140167162942656 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 13.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 13 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 18:58:12.272930 140167162942656 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 14.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 14 (torch.Size([64, 128, 3, 3])), Block block_0 (torch.Size([64, 384, 3])).
I0315 18:58:12.273060 140167162942656 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 15.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 15 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 18:58:12.273276 140167162942656 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 16.block_0 ([torch.Size([32, 32]), torch.Size([576, 576])]) for Parameter 16 (torch.Size([32, 64, 3, 3])), Block block_0 (torch.Size([32, 576])).
I0315 18:58:12.273421 140167162942656 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 17.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 17 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 18:58:12.273527 140167162942656 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 18.block_0 ([torch.Size([0])]) for Parameter 18 (torch.Size([1, 32, 1, 1])), Block block_0 (torch.Size([32])).
I0315 18:58:12.273642 140167162942656 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 19.block_0 ([torch.Size([0])]) for Parameter 19 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0315 18:58:12.273769 140167162942656 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 20.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 20 (torch.Size([512, 256, 2, 2])), Block block_0 (torch.Size([512, 1024])).
I0315 18:58:12.273900 140167162942656 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 21.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 21 (torch.Size([256, 128, 2, 2])), Block block_0 (torch.Size([256, 512])).
I0315 18:58:12.274025 140167162942656 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 22.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 22 (torch.Size([128, 64, 2, 2])), Block block_0 (torch.Size([128, 256])).
I0315 18:58:12.274143 140167162942656 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 23.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 23 (torch.Size([64, 32, 2, 2])), Block block_0 (torch.Size([64, 128])).
I0315 18:58:12.274248 140167162942656 shampoo_preconditioner_list.py:612] Rank 4: ShampooPreconditionerList Numel Breakdown: (165888, 2097314, 174080, 671744, 696320, 327698, 426002, 1310738, 1703954, 1048738, 655522, 1310738, 1212434, 327698, 303122, 671744, 665600, 2097314, 2048, 2, 2621440, 655360, 163840, 40960)
I0315 18:58:12.274306 140167162942656 shampoo_preconditioner_list.py:615] Rank 4: ShampooPreconditionerList Bytes Breakdown: (663552, 8389256)
I0315 18:58:12.274341 140167162942656 shampoo_preconditioner_list.py:618] Rank 4: ShampooPreconditionerList Total Elements: 19350298
I0315 18:58:12.274376 140167162942656 shampoo_preconditioner_list.py:621] Rank 4: ShampooPreconditionerList Total Bytes: 9052808
I0315 18:58:12.274552 140167162942656 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([32, 1, 3, 3])), Block block_0 (torch.Size([288])).
I0315 18:58:12.274626 140167162942656 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 18:58:12.274685 140167162942656 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([64, 32, 3, 3])), Block block_0 (torch.Size([64, 288])).
I0315 18:58:12.274739 140167162942656 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 18:58:12.274794 140167162942656 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 4.block_0 (torch.Size([0])) for Parameter 4 (torch.Size([128, 64, 3, 3])), Block block_0 (torch.Size([128, 576])).
I0315 18:58:12.274867 140167162942656 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 5.block_0 (torch.Size([0])) for Parameter 5 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 18:58:12.274932 140167162942656 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 6.block_0 (torch.Size([0])) for Parameter 6 (torch.Size([256, 128, 3, 3])), Block block_0 (torch.Size([256, 384, 3])).
I0315 18:58:12.274996 140167162942656 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 7.block_0 (torch.Size([0])) for Parameter 7 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 18:58:12.275053 140167162942656 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 8.block_0 (torch.Size([0])) for Parameter 8 (torch.Size([512, 256, 3, 3])), Block block_0 (torch.Size([512, 768, 3])).
I0315 18:58:12.275110 140167162942656 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 9.block_0 (torch.Size([0])) for Parameter 9 (torch.Size([512, 512, 3, 3])), Block block_0 (torch.Size([512, 512, 9])).
I0315 18:58:12.275167 140167162942656 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 10.block_0 (torch.Size([0])) for Parameter 10 (torch.Size([256, 512, 3, 3])), Block block_0 (torch.Size([256, 512, 9])).
I0315 18:58:12.275271 140167162942656 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 11.block_0 (torch.Size([256, 768, 3])) for Parameter 11 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 18:58:12.275347 140167162942656 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 12.block_0 (torch.Size([0])) for Parameter 12 (torch.Size([128, 256, 3, 3])), Block block_0 (torch.Size([128, 768, 3])).
I0315 18:58:12.275404 140167162942656 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 13.block_0 (torch.Size([0])) for Parameter 13 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 18:58:12.275461 140167162942656 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 14.block_0 (torch.Size([0])) for Parameter 14 (torch.Size([64, 128, 3, 3])), Block block_0 (torch.Size([64, 384, 3])).
I0315 18:58:12.275515 140167162942656 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 15.block_0 (torch.Size([0])) for Parameter 15 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 18:58:12.275612 140167162942656 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 16.block_0 (torch.Size([32, 576])) for Parameter 16 (torch.Size([32, 64, 3, 3])), Block block_0 (torch.Size([32, 576])).
W0315 18:58:12.275538 140247122023616 distributed_shampoo.py:341] start_preconditioning_step set to -1. Setting start_preconditioning_step equal to precondition frequency 100 by default.
I0315 18:58:12.275683 140167162942656 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 17.block_0 (torch.Size([0])) for Parameter 17 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 18:58:12.275732 140167162942656 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 18.block_0 (torch.Size([0])) for Parameter 18 (torch.Size([1, 32, 1, 1])), Block block_0 (torch.Size([32])).
W0315 18:58:12.275731 140247122023616 distributed_shampoo.py:359] Nesterov flag is enabled but momentum parameter is zero! Continuing without using momentum or Nesterov acceleration...
I0315 18:58:12.275787 140167162942656 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 19.block_0 (torch.Size([0])) for Parameter 19 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0315 18:58:12.275842 140167162942656 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 20.block_0 (torch.Size([0])) for Parameter 20 (torch.Size([512, 256, 2, 2])), Block block_0 (torch.Size([512, 1024])).
I0315 18:58:12.275897 140167162942656 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 21.block_0 (torch.Size([0])) for Parameter 21 (torch.Size([256, 128, 2, 2])), Block block_0 (torch.Size([256, 512])).
I0315 18:58:12.275951 140167162942656 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 22.block_0 (torch.Size([0])) for Parameter 22 (torch.Size([128, 64, 2, 2])), Block block_0 (torch.Size([128, 256])).
I0315 18:58:12.276007 140167162942656 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 23.block_0 (torch.Size([0])) for Parameter 23 (torch.Size([64, 32, 2, 2])), Block block_0 (torch.Size([64, 128])).
I0315 18:58:12.276090 140167162942656 shampoo_preconditioner_list.py:375] Rank 4: AdaGradPreconditionerList Numel Breakdown: (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 589824, 0, 0, 0, 0, 18432, 0, 0, 0, 0, 0, 0, 0)
I0315 18:58:12.276146 140167162942656 shampoo_preconditioner_list.py:378] Rank 4: AdaGradPreconditionerList Bytes Breakdown: (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2359296, 0, 0, 0, 0, 73728, 0, 0, 0, 0, 0, 0, 0)
I0315 18:58:12.276181 140167162942656 shampoo_preconditioner_list.py:381] Rank 4: AdaGradPreconditionerList Total Elements: 608256
I0315 18:58:12.276217 140167162942656 shampoo_preconditioner_list.py:384] Rank 4: AdaGradPreconditionerList Total Bytes: 2433024
I0315 18:58:12.277273 140167162942656 logger_utils.py:262] Unable to record workload.train_mean information. Continuing without it.
I0315 18:58:12.277350 140167162942656 logger_utils.py:262] Unable to record workload.train_stddev information. Continuing without it.
I0315 18:58:12.275785 140146293318848 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 0.block_0 ([torch.Size([0])]) for Parameter 0 (torch.Size([32, 1, 3, 3])), Block block_0 (torch.Size([288])).
I0315 18:58:12.278157 140146293318848 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 1.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 1 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 18:58:12.278327 140146293318848 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 2.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 2 (torch.Size([64, 32, 3, 3])), Block block_0 (torch.Size([64, 288])).
I0315 18:58:12.278468 140146293318848 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 3.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 3 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 18:58:12.278631 140146293318848 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 4.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 4 (torch.Size([128, 64, 3, 3])), Block block_0 (torch.Size([128, 576])).
I0315 18:58:12.278788 140146293318848 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 5.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 5 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 18:58:12.278934 140146293318848 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 6.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 6 (torch.Size([256, 128, 3, 3])), Block block_0 (torch.Size([256, 384, 3])).
I0315 18:58:12.279091 140146293318848 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 7.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 7 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 18:58:12.279239 140146293318848 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 8.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 8 (torch.Size([512, 256, 3, 3])), Block block_0 (torch.Size([512, 768, 3])).
I0315 18:58:12.278809 140247122023616 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 0.block_0 ([torch.Size([0])]) for Parameter 0 (torch.Size([32, 1, 3, 3])), Block block_0 (torch.Size([288])).
I0315 18:58:12.281193 140247122023616 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 1.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 1 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 18:58:12.281481 140247122023616 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 2.block_0 ([torch.Size([64, 64]), torch.Size([288, 288])]) for Parameter 2 (torch.Size([64, 32, 3, 3])), Block block_0 (torch.Size([64, 288])).
I0315 18:58:12.281719 140247122023616 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 3.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 3 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 18:58:12.281880 140247122023616 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 4.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 4 (torch.Size([128, 64, 3, 3])), Block block_0 (torch.Size([128, 576])).
I0315 18:58:12.282040 140247122023616 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 5.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 5 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 18:58:12.282199 140247122023616 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 6.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 6 (torch.Size([256, 128, 3, 3])), Block block_0 (torch.Size([256, 384, 3])).
I0315 18:58:12.282242 140146293318848 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 9.block_0 ([torch.Size([512, 512]), torch.Size([512, 512]), torch.Size([9, 9])]) for Parameter 9 (torch.Size([512, 512, 3, 3])), Block block_0 (torch.Size([512, 512, 9])).
I0315 18:58:12.282455 140146293318848 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 10.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 10 (torch.Size([256, 512, 3, 3])), Block block_0 (torch.Size([256, 512, 9])).
I0315 18:58:12.282614 140146293318848 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 11.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 11 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 18:58:12.282769 140146293318848 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 12.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 12 (torch.Size([128, 256, 3, 3])), Block block_0 (torch.Size([128, 768, 3])).
I0315 18:58:12.282919 140146293318848 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 13.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 13 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 18:58:12.283063 140146293318848 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 14.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 14 (torch.Size([64, 128, 3, 3])), Block block_0 (torch.Size([64, 384, 3])).
I0315 18:58:12.283183 140146293318848 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 15.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 15 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 18:58:12.283310 140146293318848 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 16.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 16 (torch.Size([32, 64, 3, 3])), Block block_0 (torch.Size([32, 576])).
I0315 18:58:12.283435 140146293318848 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 17.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 17 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 18:58:12.283531 140146293318848 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 18.block_0 ([torch.Size([0])]) for Parameter 18 (torch.Size([1, 32, 1, 1])), Block block_0 (torch.Size([32])).
I0315 18:58:12.283627 140146293318848 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 19.block_0 ([torch.Size([0])]) for Parameter 19 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0315 18:58:12.283751 140146293318848 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 20.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 20 (torch.Size([512, 256, 2, 2])), Block block_0 (torch.Size([512, 1024])).
I0315 18:58:12.283879 140146293318848 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 21.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 21 (torch.Size([256, 128, 2, 2])), Block block_0 (torch.Size([256, 512])).
I0315 18:58:12.283967 140247122023616 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 7.block_0 ([torch.Size([256, 256]), torch.Size([768, 768]), torch.Size([3, 3])]) for Parameter 7 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 18:58:12.284002 140146293318848 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 22.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 22 (torch.Size([128, 64, 2, 2])), Block block_0 (torch.Size([128, 256])).
I0315 18:58:12.284112 140146293318848 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 23.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 23 (torch.Size([64, 32, 2, 2])), Block block_0 (torch.Size([64, 128])).
I0315 18:58:12.284168 140247122023616 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 8.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 8 (torch.Size([512, 256, 3, 3])), Block block_0 (torch.Size([512, 768, 3])).
I0315 18:58:12.284213 140146293318848 shampoo_preconditioner_list.py:612] Rank 0: ShampooPreconditionerList Numel Breakdown: (165888, 2097314, 174080, 671744, 696320, 327698, 426002, 1310738, 1703954, 1048738, 655522, 1310738, 1212434, 327698, 303122, 671744, 665600, 2097314, 2048, 2, 2621440, 655360, 163840, 40960)
I0315 18:58:12.284259 140146293318848 shampoo_preconditioner_list.py:615] Rank 0: ShampooPreconditionerList Bytes Breakdown: (663552,)
I0315 18:58:12.284297 140146293318848 shampoo_preconditioner_list.py:618] Rank 0: ShampooPreconditionerList Total Elements: 19350298
I0315 18:58:12.284321 140247122023616 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 9.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 9 (torch.Size([512, 512, 3, 3])), Block block_0 (torch.Size([512, 512, 9])).
I0315 18:58:12.284334 140146293318848 shampoo_preconditioner_list.py:621] Rank 0: ShampooPreconditionerList Total Bytes: 663552
I0315 18:58:12.284464 140247122023616 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 10.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 10 (torch.Size([256, 512, 3, 3])), Block block_0 (torch.Size([256, 512, 9])).
I0315 18:58:12.284500 140146293318848 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([32, 1, 3, 3])), Block block_0 (torch.Size([288])).
I0315 18:58:12.284582 140146293318848 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 18:58:12.284613 140247122023616 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 11.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 11 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 18:58:12.284644 140146293318848 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([64, 32, 3, 3])), Block block_0 (torch.Size([64, 288])).
I0315 18:58:12.284702 140146293318848 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 18:58:12.284759 140146293318848 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 4.block_0 (torch.Size([0])) for Parameter 4 (torch.Size([128, 64, 3, 3])), Block block_0 (torch.Size([128, 576])).
I0315 18:58:12.284760 140247122023616 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 12.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 12 (torch.Size([128, 256, 3, 3])), Block block_0 (torch.Size([128, 768, 3])).
I0315 18:58:12.284825 140146293318848 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 5.block_0 (torch.Size([0])) for Parameter 5 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 18:58:12.284884 140146293318848 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 6.block_0 (torch.Size([0])) for Parameter 6 (torch.Size([256, 128, 3, 3])), Block block_0 (torch.Size([256, 384, 3])).
I0315 18:58:12.284889 140247122023616 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 13.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 13 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 18:58:12.284959 140146293318848 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 7.block_0 (torch.Size([0])) for Parameter 7 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 18:58:12.285013 140146293318848 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 8.block_0 (torch.Size([0])) for Parameter 8 (torch.Size([512, 256, 3, 3])), Block block_0 (torch.Size([512, 768, 3])).
I0315 18:58:12.285037 140247122023616 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 14.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 14 (torch.Size([64, 128, 3, 3])), Block block_0 (torch.Size([64, 384, 3])).
I0315 18:58:12.285114 140146293318848 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 9.block_0 (torch.Size([512, 512, 9])) for Parameter 9 (torch.Size([512, 512, 3, 3])), Block block_0 (torch.Size([512, 512, 9])).
I0315 18:58:12.285169 140247122023616 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 15.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 15 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 18:58:12.285188 140146293318848 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 10.block_0 (torch.Size([0])) for Parameter 10 (torch.Size([256, 512, 3, 3])), Block block_0 (torch.Size([256, 512, 9])).
I0315 18:58:12.285238 140146293318848 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 11.block_0 (torch.Size([0])) for Parameter 11 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 18:58:12.285283 140247122023616 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 16.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 16 (torch.Size([32, 64, 3, 3])), Block block_0 (torch.Size([32, 576])).
I0315 18:58:12.285292 140146293318848 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 12.block_0 (torch.Size([0])) for Parameter 12 (torch.Size([128, 256, 3, 3])), Block block_0 (torch.Size([128, 768, 3])).
I0315 18:58:12.285337 140146293318848 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 13.block_0 (torch.Size([0])) for Parameter 13 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 18:58:12.285391 140146293318848 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 14.block_0 (torch.Size([0])) for Parameter 14 (torch.Size([64, 128, 3, 3])), Block block_0 (torch.Size([64, 384, 3])).
I0315 18:58:12.285399 140247122023616 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 17.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 17 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 18:58:12.285443 140146293318848 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 15.block_0 (torch.Size([0])) for Parameter 15 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 18:58:12.285483 140247122023616 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 18.block_0 ([torch.Size([0])]) for Parameter 18 (torch.Size([1, 32, 1, 1])), Block block_0 (torch.Size([32])).
I0315 18:58:12.285494 140146293318848 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 16.block_0 (torch.Size([0])) for Parameter 16 (torch.Size([32, 64, 3, 3])), Block block_0 (torch.Size([32, 576])).
I0315 18:58:12.285541 140146293318848 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 17.block_0 (torch.Size([0])) for Parameter 17 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 18:58:12.285574 140247122023616 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 19.block_0 ([torch.Size([0])]) for Parameter 19 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0315 18:58:12.285618 140146293318848 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 18.block_0 (torch.Size([0])) for Parameter 18 (torch.Size([1, 32, 1, 1])), Block block_0 (torch.Size([32])).
I0315 18:58:12.285685 140146293318848 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 19.block_0 (torch.Size([0])) for Parameter 19 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0315 18:58:12.285753 140146293318848 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 20.block_0 (torch.Size([0])) for Parameter 20 (torch.Size([512, 256, 2, 2])), Block block_0 (torch.Size([512, 1024])).
I0315 18:58:12.285752 140247122023616 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 20.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 20 (torch.Size([512, 256, 2, 2])), Block block_0 (torch.Size([512, 1024])).
I0315 18:58:12.285806 140146293318848 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 21.block_0 (torch.Size([0])) for Parameter 21 (torch.Size([256, 128, 2, 2])), Block block_0 (torch.Size([256, 512])).
I0315 18:58:12.285859 140146293318848 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 22.block_0 (torch.Size([0])) for Parameter 22 (torch.Size([128, 64, 2, 2])), Block block_0 (torch.Size([128, 256])).
I0315 18:58:12.285871 140247122023616 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 21.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 21 (torch.Size([256, 128, 2, 2])), Block block_0 (torch.Size([256, 512])).
I0315 18:58:12.285912 140146293318848 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 23.block_0 (torch.Size([0])) for Parameter 23 (torch.Size([64, 32, 2, 2])), Block block_0 (torch.Size([64, 128])).
I0315 18:58:12.285974 140247122023616 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 22.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 22 (torch.Size([128, 64, 2, 2])), Block block_0 (torch.Size([128, 256])).
I0315 18:58:12.285979 140146293318848 shampoo_preconditioner_list.py:375] Rank 0: AdaGradPreconditionerList Numel Breakdown: (0, 0, 0, 0, 0, 0, 0, 0, 0, 2359296, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)
I0315 18:58:12.286012 140146293318848 shampoo_preconditioner_list.py:378] Rank 0: AdaGradPreconditionerList Bytes Breakdown: (0, 0, 0, 0, 0, 0, 0, 0, 0, 9437184, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)
I0315 18:58:12.286047 140146293318848 shampoo_preconditioner_list.py:381] Rank 0: AdaGradPreconditionerList Total Elements: 2359296
I0315 18:58:12.286081 140146293318848 shampoo_preconditioner_list.py:384] Rank 0: AdaGradPreconditionerList Total Bytes: 9437184
I0315 18:58:12.286085 140247122023616 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 23.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 23 (torch.Size([64, 32, 2, 2])), Block block_0 (torch.Size([64, 128])).
I0315 18:58:12.286178 140247122023616 shampoo_preconditioner_list.py:612] Rank 3: ShampooPreconditionerList Numel Breakdown: (165888, 2097314, 174080, 671744, 696320, 327698, 426002, 1310738, 1703954, 1048738, 655522, 1310738, 1212434, 327698, 303122, 671744, 665600, 2097314, 2048, 2, 2621440, 655360, 163840, 40960)
I0315 18:58:12.286223 140247122023616 shampoo_preconditioner_list.py:615] Rank 3: ShampooPreconditionerList Bytes Breakdown: (663552, 8389256)
I0315 18:58:12.286259 140247122023616 shampoo_preconditioner_list.py:618] Rank 3: ShampooPreconditionerList Total Elements: 19350298
I0315 18:58:12.286293 140247122023616 shampoo_preconditioner_list.py:621] Rank 3: ShampooPreconditionerList Total Bytes: 9052808
I0315 18:58:12.286457 140247122023616 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([32, 1, 3, 3])), Block block_0 (torch.Size([288])).
I0315 18:58:12.286533 140247122023616 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 18:58:12.286647 140247122023616 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([64, 288])) for Parameter 2 (torch.Size([64, 32, 3, 3])), Block block_0 (torch.Size([64, 288])).
I0315 18:58:12.286724 140247122023616 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 18:58:12.286781 140247122023616 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 4.block_0 (torch.Size([0])) for Parameter 4 (torch.Size([128, 64, 3, 3])), Block block_0 (torch.Size([128, 576])).
I0315 18:58:12.286855 140247122023616 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 5.block_0 (torch.Size([0])) for Parameter 5 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 18:58:12.286935 140247122023616 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 6.block_0 (torch.Size([0])) for Parameter 6 (torch.Size([256, 128, 3, 3])), Block block_0 (torch.Size([256, 384, 3])).
I0315 18:58:12.287028 140247122023616 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 7.block_0 (torch.Size([256, 768, 3])) for Parameter 7 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 18:58:12.287098 140247122023616 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 8.block_0 (torch.Size([0])) for Parameter 8 (torch.Size([512, 256, 3, 3])), Block block_0 (torch.Size([512, 768, 3])).
I0315 18:58:12.287161 140247122023616 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 9.block_0 (torch.Size([0])) for Parameter 9 (torch.Size([512, 512, 3, 3])), Block block_0 (torch.Size([512, 512, 9])).
I0315 18:58:12.287212 140247122023616 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 10.block_0 (torch.Size([0])) for Parameter 10 (torch.Size([256, 512, 3, 3])), Block block_0 (torch.Size([256, 512, 9])).
I0315 18:58:12.287264 140247122023616 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 11.block_0 (torch.Size([0])) for Parameter 11 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 18:58:12.287330 140247122023616 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 12.block_0 (torch.Size([0])) for Parameter 12 (torch.Size([128, 256, 3, 3])), Block block_0 (torch.Size([128, 768, 3])).
I0315 18:58:12.287316 140146293318848 submission.py:142] No large parameters detected! Continuing with only Shampoo....
I0315 18:58:12.287388 140247122023616 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 13.block_0 (torch.Size([0])) for Parameter 13 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 18:58:12.287447 140247122023616 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 14.block_0 (torch.Size([0])) for Parameter 14 (torch.Size([64, 128, 3, 3])), Block block_0 (torch.Size([64, 384, 3])).
I0315 18:58:12.287487 140146293318848 submission_runner.py:279] Initializing metrics bundle.
I0315 18:58:12.287503 140247122023616 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 15.block_0 (torch.Size([0])) for Parameter 15 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 18:58:12.287550 140247122023616 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 16.block_0 (torch.Size([0])) for Parameter 16 (torch.Size([32, 64, 3, 3])), Block block_0 (torch.Size([32, 576])).
I0315 18:58:12.287615 140247122023616 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 17.block_0 (torch.Size([0])) for Parameter 17 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 18:58:12.287615 140146293318848 submission_runner.py:301] Initializing checkpoint and logger.
I0315 18:58:12.287672 140247122023616 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 18.block_0 (torch.Size([0])) for Parameter 18 (torch.Size([1, 32, 1, 1])), Block block_0 (torch.Size([32])).
I0315 18:58:12.287727 140247122023616 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 19.block_0 (torch.Size([0])) for Parameter 19 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0315 18:58:12.287782 140247122023616 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 20.block_0 (torch.Size([0])) for Parameter 20 (torch.Size([512, 256, 2, 2])), Block block_0 (torch.Size([512, 1024])).
I0315 18:58:12.287837 140247122023616 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 21.block_0 (torch.Size([0])) for Parameter 21 (torch.Size([256, 128, 2, 2])), Block block_0 (torch.Size([256, 512])).
I0315 18:58:12.287892 140247122023616 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 22.block_0 (torch.Size([0])) for Parameter 22 (torch.Size([128, 64, 2, 2])), Block block_0 (torch.Size([128, 256])).
I0315 18:58:12.287949 140247122023616 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 23.block_0 (torch.Size([0])) for Parameter 23 (torch.Size([64, 32, 2, 2])), Block block_0 (torch.Size([64, 128])).
I0315 18:58:12.288022 140247122023616 shampoo_preconditioner_list.py:375] Rank 3: AdaGradPreconditionerList Numel Breakdown: (0, 0, 18432, 0, 0, 0, 0, 589824, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)
I0315 18:58:12.288015 140146293318848 submission_runner.py:321] Saving meta data to /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_2/fastmri_pytorch/trial_1/meta_data_0.json.
I0315 18:58:12.288063 140247122023616 shampoo_preconditioner_list.py:378] Rank 3: AdaGradPreconditionerList Bytes Breakdown: (0, 0, 73728, 0, 0, 0, 0, 2359296, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)
I0315 18:58:12.288113 140247122023616 shampoo_preconditioner_list.py:381] Rank 3: AdaGradPreconditionerList Total Elements: 608256
I0315 18:58:12.288148 140247122023616 shampoo_preconditioner_list.py:384] Rank 3: AdaGradPreconditionerList Total Bytes: 2433024
I0315 18:58:12.288181 140146293318848 logger_utils.py:262] Unable to record workload.train_mean information. Continuing without it.
I0315 18:58:12.288233 140146293318848 logger_utils.py:262] Unable to record workload.train_stddev information. Continuing without it.
I0315 18:58:12.289197 140247122023616 logger_utils.py:262] Unable to record workload.train_mean information. Continuing without it.
I0315 18:58:12.289270 140247122023616 logger_utils.py:262] Unable to record workload.train_stddev information. Continuing without it.
I0315 18:58:12.579346 140146293318848 submission_runner.py:325] Saving flags to /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_2/fastmri_pytorch/trial_1/flags_0.json.
I0315 18:58:12.618879 140146293318848 submission_runner.py:337] Starting training loop.
[rank1]:W0315 18:58:12.653000 45 site-packages/torch/_logging/_internal.py:1081] [0/0] Profiler function <class 'torch.autograd.profiler.record_function'> will be ignored
[rank2]:W0315 18:58:12.653000 46 site-packages/torch/_logging/_internal.py:1081] [0/0] Profiler function <class 'torch.autograd.profiler.record_function'> will be ignored
[rank6]:W0315 18:58:12.653000 50 site-packages/torch/_logging/_internal.py:1081] [0/0] Profiler function <class 'torch.autograd.profiler.record_function'> will be ignored
[rank5]:W0315 18:58:12.654000 49 site-packages/torch/_logging/_internal.py:1081] [0/0] Profiler function <class 'torch.autograd.profiler.record_function'> will be ignored
[rank7]:W0315 18:58:12.654000 51 site-packages/torch/_logging/_internal.py:1081] [0/0] Profiler function <class 'torch.autograd.profiler.record_function'> will be ignored
[rank4]:W0315 18:58:12.654000 48 site-packages/torch/_logging/_internal.py:1081] [0/0] Profiler function <class 'torch.autograd.profiler.record_function'> will be ignored
[rank3]:W0315 18:58:12.654000 47 site-packages/torch/_logging/_internal.py:1081] [0/0] Profiler function <class 'torch.autograd.profiler.record_function'> will be ignored
[rank0]:W0315 18:58:38.115000 44 site-packages/torch/_logging/_internal.py:1081] [0/0] Profiler function <class 'torch.autograd.profiler.record_function'> will be ignored
[rank4]:W0315 18:59:13.578000 48 site-packages/torch/_dynamo/convert_frame.py:844] [2/8] torch._dynamo hit config.cache_size_limit (8)
[rank4]:W0315 18:59:13.578000 48 site-packages/torch/_dynamo/convert_frame.py:844] [2/8]    function: 'forward' (/algorithmic-efficiency/algoperf/workloads/fastmri/fastmri_pytorch/models.py:141)
[rank4]:W0315 18:59:13.578000 48 site-packages/torch/_dynamo/convert_frame.py:844] [2/8]    last reason: 2/0: tensor 'L['x']' requires_grad mismatch. expected requires_grad=0
[rank4]:W0315 18:59:13.578000 48 site-packages/torch/_dynamo/convert_frame.py:844] [2/8] To log all recompilation reasons, use TORCH_LOGS="recompiles".
[rank4]:W0315 18:59:13.578000 48 site-packages/torch/_dynamo/convert_frame.py:844] [2/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.
[rank1]:W0315 18:59:13.580000 45 site-packages/torch/_dynamo/convert_frame.py:844] [2/8] torch._dynamo hit config.cache_size_limit (8)
[rank1]:W0315 18:59:13.580000 45 site-packages/torch/_dynamo/convert_frame.py:844] [2/8]    function: 'forward' (/algorithmic-efficiency/algoperf/workloads/fastmri/fastmri_pytorch/models.py:141)
[rank1]:W0315 18:59:13.580000 45 site-packages/torch/_dynamo/convert_frame.py:844] [2/8]    last reason: 2/0: tensor 'L['x']' requires_grad mismatch. expected requires_grad=0
[rank1]:W0315 18:59:13.580000 45 site-packages/torch/_dynamo/convert_frame.py:844] [2/8] To log all recompilation reasons, use TORCH_LOGS="recompiles".
[rank1]:W0315 18:59:13.580000 45 site-packages/torch/_dynamo/convert_frame.py:844] [2/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.
[rank2]:W0315 18:59:13.869000 46 site-packages/torch/_dynamo/convert_frame.py:844] [2/8] torch._dynamo hit config.cache_size_limit (8)
[rank2]:W0315 18:59:13.869000 46 site-packages/torch/_dynamo/convert_frame.py:844] [2/8]    function: 'forward' (/algorithmic-efficiency/algoperf/workloads/fastmri/fastmri_pytorch/models.py:141)
[rank2]:W0315 18:59:13.869000 46 site-packages/torch/_dynamo/convert_frame.py:844] [2/8]    last reason: 2/0: tensor 'L['x']' requires_grad mismatch. expected requires_grad=0
[rank2]:W0315 18:59:13.869000 46 site-packages/torch/_dynamo/convert_frame.py:844] [2/8] To log all recompilation reasons, use TORCH_LOGS="recompiles".
[rank2]:W0315 18:59:13.869000 46 site-packages/torch/_dynamo/convert_frame.py:844] [2/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.
[rank3]:W0315 18:59:13.927000 47 site-packages/torch/_dynamo/convert_frame.py:844] [2/8] torch._dynamo hit config.cache_size_limit (8)
[rank3]:W0315 18:59:13.927000 47 site-packages/torch/_dynamo/convert_frame.py:844] [2/8]    function: 'forward' (/algorithmic-efficiency/algoperf/workloads/fastmri/fastmri_pytorch/models.py:141)
[rank3]:W0315 18:59:13.927000 47 site-packages/torch/_dynamo/convert_frame.py:844] [2/8]    last reason: 2/0: tensor 'L['x']' requires_grad mismatch. expected requires_grad=0
[rank3]:W0315 18:59:13.927000 47 site-packages/torch/_dynamo/convert_frame.py:844] [2/8] To log all recompilation reasons, use TORCH_LOGS="recompiles".
[rank3]:W0315 18:59:13.927000 47 site-packages/torch/_dynamo/convert_frame.py:844] [2/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.
[rank5]:W0315 18:59:13.956000 49 site-packages/torch/_dynamo/convert_frame.py:844] [2/8] torch._dynamo hit config.cache_size_limit (8)
[rank5]:W0315 18:59:13.956000 49 site-packages/torch/_dynamo/convert_frame.py:844] [2/8]    function: 'forward' (/algorithmic-efficiency/algoperf/workloads/fastmri/fastmri_pytorch/models.py:141)
[rank5]:W0315 18:59:13.956000 49 site-packages/torch/_dynamo/convert_frame.py:844] [2/8]    last reason: 2/0: tensor 'L['x']' requires_grad mismatch. expected requires_grad=0
[rank5]:W0315 18:59:13.956000 49 site-packages/torch/_dynamo/convert_frame.py:844] [2/8] To log all recompilation reasons, use TORCH_LOGS="recompiles".
[rank5]:W0315 18:59:13.956000 49 site-packages/torch/_dynamo/convert_frame.py:844] [2/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.
[rank6]:W0315 18:59:14.150000 50 site-packages/torch/_dynamo/convert_frame.py:844] [2/8] torch._dynamo hit config.cache_size_limit (8)
[rank6]:W0315 18:59:14.150000 50 site-packages/torch/_dynamo/convert_frame.py:844] [2/8]    function: 'forward' (/algorithmic-efficiency/algoperf/workloads/fastmri/fastmri_pytorch/models.py:141)
[rank6]:W0315 18:59:14.150000 50 site-packages/torch/_dynamo/convert_frame.py:844] [2/8]    last reason: 2/0: tensor 'L['x']' requires_grad mismatch. expected requires_grad=0
[rank6]:W0315 18:59:14.150000 50 site-packages/torch/_dynamo/convert_frame.py:844] [2/8] To log all recompilation reasons, use TORCH_LOGS="recompiles".
[rank6]:W0315 18:59:14.150000 50 site-packages/torch/_dynamo/convert_frame.py:844] [2/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.
[rank7]:W0315 18:59:14.150000 51 site-packages/torch/_dynamo/convert_frame.py:844] [2/8] torch._dynamo hit config.cache_size_limit (8)
[rank7]:W0315 18:59:14.150000 51 site-packages/torch/_dynamo/convert_frame.py:844] [2/8]    function: 'forward' (/algorithmic-efficiency/algoperf/workloads/fastmri/fastmri_pytorch/models.py:141)
[rank7]:W0315 18:59:14.150000 51 site-packages/torch/_dynamo/convert_frame.py:844] [2/8]    last reason: 2/0: tensor 'L['x']' requires_grad mismatch. expected requires_grad=0
[rank7]:W0315 18:59:14.150000 51 site-packages/torch/_dynamo/convert_frame.py:844] [2/8] To log all recompilation reasons, use TORCH_LOGS="recompiles".
[rank7]:W0315 18:59:14.150000 51 site-packages/torch/_dynamo/convert_frame.py:844] [2/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.
[rank0]:W0315 18:59:15.932000 44 site-packages/torch/_dynamo/convert_frame.py:844] [2/8] torch._dynamo hit config.cache_size_limit (8)
[rank0]:W0315 18:59:15.932000 44 site-packages/torch/_dynamo/convert_frame.py:844] [2/8]    function: 'forward' (/algorithmic-efficiency/algoperf/workloads/fastmri/fastmri_pytorch/models.py:141)
[rank0]:W0315 18:59:15.932000 44 site-packages/torch/_dynamo/convert_frame.py:844] [2/8]    last reason: 2/0: tensor 'L['x']' requires_grad mismatch. expected requires_grad=0
[rank0]:W0315 18:59:15.932000 44 site-packages/torch/_dynamo/convert_frame.py:844] [2/8] To log all recompilation reasons, use TORCH_LOGS="recompiles".
[rank0]:W0315 18:59:15.932000 44 site-packages/torch/_dynamo/convert_frame.py:844] [2/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.
I0315 18:59:27.020057 140118489630464 logging_writer.py:48] [0] global_step=0, grad_norm=2.4655, loss=1.18924
I0315 18:59:27.038589 140146293318848 submission.py:265] 0) loss = 1.189, grad_norm = 2.465
I0315 18:59:27.831945 140146293318848 spec.py:321] Evaluating on the training split.
I0315 19:00:33.221415 140146293318848 spec.py:333] Evaluating on the validation split.
I0315 19:01:01.292028 140146293318848 spec.py:349] Evaluating on the test split.
I0315 19:01:41.221901 140146293318848 submission_runner.py:469] Time since start: 208.60s, 	Step: 1, 	{'train/ssim': 0.1951230423791068, 'train/loss': 0.9957185472760882, 'validation/ssim': 0.18485184016359207, 'validation/loss': 1.0091675697717362, 'validation/num_examples': 3554, 'test/ssim': 0.20717075649369066, 'test/loss': 1.0065030990383272, 'test/num_examples': 3581, 'score': 74.42088913917542, 'total_duration': 208.60333847999573, 'accumulated_submission_time': 74.42088913917542, 'accumulated_eval_time': 133.39040756225586, 'accumulated_logging_time': 0}
I0315 19:01:41.230335 140096531592960 logging_writer.py:48] [1] accumulated_eval_time=133.39, accumulated_logging_time=0, accumulated_submission_time=74.4209, global_step=1, preemption_count=0, score=74.4209, test/loss=1.0065, test/num_examples=3581, test/ssim=0.207171, total_duration=208.603, train/loss=0.995719, train/ssim=0.195123, validation/loss=1.00917, validation/num_examples=3554, validation/ssim=0.184852
I0315 19:01:42.357834 140096523200256 logging_writer.py:48] [1] global_step=1, grad_norm=3.16345, loss=1.04865
I0315 19:01:42.361808 140146293318848 submission.py:265] 1) loss = 1.049, grad_norm = 3.163
I0315 19:01:42.470875 140096531592960 logging_writer.py:48] [2] global_step=2, grad_norm=2.51896, loss=1.05925
I0315 19:01:42.478227 140146293318848 submission.py:265] 2) loss = 1.059, grad_norm = 2.519
I0315 19:01:42.566763 140096523200256 logging_writer.py:48] [3] global_step=3, grad_norm=3.00501, loss=0.972261
I0315 19:01:42.573561 140146293318848 submission.py:265] 3) loss = 0.972, grad_norm = 3.005
I0315 19:01:42.676972 140096531592960 logging_writer.py:48] [4] global_step=4, grad_norm=3.00856, loss=1.02254
I0315 19:01:42.684711 140146293318848 submission.py:265] 4) loss = 1.023, grad_norm = 3.009
I0315 19:01:42.769811 140096523200256 logging_writer.py:48] [5] global_step=5, grad_norm=2.92237, loss=0.954677
I0315 19:01:42.774322 140146293318848 submission.py:265] 5) loss = 0.955, grad_norm = 2.922
I0315 19:01:42.856032 140096531592960 logging_writer.py:48] [6] global_step=6, grad_norm=3.34489, loss=0.942737
I0315 19:01:42.860806 140146293318848 submission.py:265] 6) loss = 0.943, grad_norm = 3.345
I0315 19:01:42.956310 140096523200256 logging_writer.py:48] [7] global_step=7, grad_norm=2.42955, loss=0.981707
I0315 19:01:42.960778 140146293318848 submission.py:265] 7) loss = 0.982, grad_norm = 2.430
I0315 19:01:43.055795 140096531592960 logging_writer.py:48] [8] global_step=8, grad_norm=2.68478, loss=0.975356
I0315 19:01:43.061045 140146293318848 submission.py:265] 8) loss = 0.975, grad_norm = 2.685
I0315 19:01:43.161156 140096523200256 logging_writer.py:48] [9] global_step=9, grad_norm=2.23429, loss=1.00978
I0315 19:01:43.166172 140146293318848 submission.py:265] 9) loss = 1.010, grad_norm = 2.234
I0315 19:01:43.250137 140096531592960 logging_writer.py:48] [10] global_step=10, grad_norm=2.0261, loss=0.849817
I0315 19:01:43.254361 140146293318848 submission.py:265] 10) loss = 0.850, grad_norm = 2.026
I0315 19:01:43.337680 140096523200256 logging_writer.py:48] [11] global_step=11, grad_norm=1.80334, loss=0.858694
I0315 19:01:43.345745 140146293318848 submission.py:265] 11) loss = 0.859, grad_norm = 1.803
I0315 19:01:43.434525 140096531592960 logging_writer.py:48] [12] global_step=12, grad_norm=2.00214, loss=0.821625
I0315 19:01:43.438216 140146293318848 submission.py:265] 12) loss = 0.822, grad_norm = 2.002
I0315 19:01:43.526584 140096523200256 logging_writer.py:48] [13] global_step=13, grad_norm=1.88977, loss=0.809585
I0315 19:01:43.531624 140146293318848 submission.py:265] 13) loss = 0.810, grad_norm = 1.890
I0315 19:01:43.631996 140096531592960 logging_writer.py:48] [14] global_step=14, grad_norm=1.71751, loss=0.932392
I0315 19:01:43.637193 140146293318848 submission.py:265] 14) loss = 0.932, grad_norm = 1.718
I0315 19:01:43.741560 140096523200256 logging_writer.py:48] [15] global_step=15, grad_norm=1.78592, loss=0.764336
I0315 19:01:43.750091 140146293318848 submission.py:265] 15) loss = 0.764, grad_norm = 1.786
I0315 19:01:43.839001 140096531592960 logging_writer.py:48] [16] global_step=16, grad_norm=1.5895, loss=0.892166
I0315 19:01:43.844767 140146293318848 submission.py:265] 16) loss = 0.892, grad_norm = 1.590
I0315 19:01:43.932397 140096523200256 logging_writer.py:48] [17] global_step=17, grad_norm=1.63885, loss=0.840244
I0315 19:01:43.936917 140146293318848 submission.py:265] 17) loss = 0.840, grad_norm = 1.639
I0315 19:01:44.031344 140096531592960 logging_writer.py:48] [18] global_step=18, grad_norm=1.58166, loss=0.894617
I0315 19:01:44.035975 140146293318848 submission.py:265] 18) loss = 0.895, grad_norm = 1.582
I0315 19:01:44.115268 140096523200256 logging_writer.py:48] [19] global_step=19, grad_norm=1.57593, loss=0.739855
I0315 19:01:44.124305 140146293318848 submission.py:265] 19) loss = 0.740, grad_norm = 1.576
I0315 19:01:44.205711 140096531592960 logging_writer.py:48] [20] global_step=20, grad_norm=1.57356, loss=0.831911
I0315 19:01:44.210943 140146293318848 submission.py:265] 20) loss = 0.832, grad_norm = 1.574
I0315 19:01:44.293702 140096523200256 logging_writer.py:48] [21] global_step=21, grad_norm=1.45322, loss=0.749005
I0315 19:01:44.298393 140146293318848 submission.py:265] 21) loss = 0.749, grad_norm = 1.453
I0315 19:01:44.379072 140096531592960 logging_writer.py:48] [22] global_step=22, grad_norm=1.51472, loss=0.685159
I0315 19:01:44.384404 140146293318848 submission.py:265] 22) loss = 0.685, grad_norm = 1.515
I0315 19:01:44.467013 140096523200256 logging_writer.py:48] [23] global_step=23, grad_norm=1.61671, loss=0.71136
I0315 19:01:44.472019 140146293318848 submission.py:265] 23) loss = 0.711, grad_norm = 1.617
I0315 19:01:44.569027 140096531592960 logging_writer.py:48] [24] global_step=24, grad_norm=1.44159, loss=0.735246
I0315 19:01:44.579276 140146293318848 submission.py:265] 24) loss = 0.735, grad_norm = 1.442
I0315 19:01:44.656786 140096523200256 logging_writer.py:48] [25] global_step=25, grad_norm=1.66188, loss=0.836893
I0315 19:01:44.662631 140146293318848 submission.py:265] 25) loss = 0.837, grad_norm = 1.662
I0315 19:01:44.748029 140096531592960 logging_writer.py:48] [26] global_step=26, grad_norm=1.63485, loss=0.659231
I0315 19:01:44.752671 140146293318848 submission.py:265] 26) loss = 0.659, grad_norm = 1.635
I0315 19:01:44.840388 140096523200256 logging_writer.py:48] [27] global_step=27, grad_norm=1.26613, loss=0.574995
I0315 19:01:44.847262 140146293318848 submission.py:265] 27) loss = 0.575, grad_norm = 1.266
I0315 19:01:44.934031 140096531592960 logging_writer.py:48] [28] global_step=28, grad_norm=1.40954, loss=0.657392
I0315 19:01:44.941126 140146293318848 submission.py:265] 28) loss = 0.657, grad_norm = 1.410
I0315 19:01:45.012291 140096523200256 logging_writer.py:48] [29] global_step=29, grad_norm=1.49205, loss=0.587717
I0315 19:01:45.016934 140146293318848 submission.py:265] 29) loss = 0.588, grad_norm = 1.492
I0315 19:01:45.108407 140096531592960 logging_writer.py:48] [30] global_step=30, grad_norm=1.38581, loss=0.596311
I0315 19:01:45.112589 140146293318848 submission.py:265] 30) loss = 0.596, grad_norm = 1.386
I0315 19:01:45.192008 140096523200256 logging_writer.py:48] [31] global_step=31, grad_norm=1.35208, loss=0.585182
I0315 19:01:45.195706 140146293318848 submission.py:265] 31) loss = 0.585, grad_norm = 1.352
I0315 19:01:45.288451 140096531592960 logging_writer.py:48] [32] global_step=32, grad_norm=1.31164, loss=0.588143
I0315 19:01:45.293034 140146293318848 submission.py:265] 32) loss = 0.588, grad_norm = 1.312
I0315 19:01:45.383228 140096523200256 logging_writer.py:48] [33] global_step=33, grad_norm=1.36178, loss=0.562191
I0315 19:01:45.388864 140146293318848 submission.py:265] 33) loss = 0.562, grad_norm = 1.362
I0315 19:01:45.483173 140096531592960 logging_writer.py:48] [34] global_step=34, grad_norm=1.42866, loss=0.560041
I0315 19:01:45.487892 140146293318848 submission.py:265] 34) loss = 0.560, grad_norm = 1.429
I0315 19:01:45.567757 140096523200256 logging_writer.py:48] [35] global_step=35, grad_norm=1.3205, loss=0.585439
I0315 19:01:45.573794 140146293318848 submission.py:265] 35) loss = 0.585, grad_norm = 1.321
I0315 19:01:45.658518 140096531592960 logging_writer.py:48] [36] global_step=36, grad_norm=1.09098, loss=0.532155
I0315 19:01:45.663099 140146293318848 submission.py:265] 36) loss = 0.532, grad_norm = 1.091
I0315 19:01:45.747831 140096523200256 logging_writer.py:48] [37] global_step=37, grad_norm=0.977892, loss=0.565037
I0315 19:01:45.753543 140146293318848 submission.py:265] 37) loss = 0.565, grad_norm = 0.978
I0315 19:01:45.837066 140096531592960 logging_writer.py:48] [38] global_step=38, grad_norm=1.22191, loss=0.56885
I0315 19:01:45.842658 140146293318848 submission.py:265] 38) loss = 0.569, grad_norm = 1.222
I0315 19:01:45.926593 140096523200256 logging_writer.py:48] [39] global_step=39, grad_norm=1.25326, loss=0.570189
I0315 19:01:45.931001 140146293318848 submission.py:265] 39) loss = 0.570, grad_norm = 1.253
I0315 19:01:46.012012 140096531592960 logging_writer.py:48] [40] global_step=40, grad_norm=0.992996, loss=0.489297
I0315 19:01:46.016213 140146293318848 submission.py:265] 40) loss = 0.489, grad_norm = 0.993
I0315 19:01:46.087204 140096523200256 logging_writer.py:48] [41] global_step=41, grad_norm=0.890287, loss=0.487195
I0315 19:01:46.094009 140146293318848 submission.py:265] 41) loss = 0.487, grad_norm = 0.890
I0315 19:01:46.179283 140096531592960 logging_writer.py:48] [42] global_step=42, grad_norm=1.21672, loss=0.39835
I0315 19:01:46.184908 140146293318848 submission.py:265] 42) loss = 0.398, grad_norm = 1.217
I0315 19:01:46.265683 140096523200256 logging_writer.py:48] [43] global_step=43, grad_norm=1.20533, loss=0.478658
I0315 19:01:46.270856 140146293318848 submission.py:265] 43) loss = 0.479, grad_norm = 1.205
I0315 19:01:46.340334 140096531592960 logging_writer.py:48] [44] global_step=44, grad_norm=0.968915, loss=0.440387
I0315 19:01:46.344937 140146293318848 submission.py:265] 44) loss = 0.440, grad_norm = 0.969
I0315 19:01:46.420618 140096523200256 logging_writer.py:48] [45] global_step=45, grad_norm=0.816886, loss=0.429533
I0315 19:01:46.425458 140146293318848 submission.py:265] 45) loss = 0.430, grad_norm = 0.817
I0315 19:01:46.500051 140096531592960 logging_writer.py:48] [46] global_step=46, grad_norm=0.701765, loss=0.520858
I0315 19:01:46.505215 140146293318848 submission.py:265] 46) loss = 0.521, grad_norm = 0.702
I0315 19:01:46.584391 140096523200256 logging_writer.py:48] [47] global_step=47, grad_norm=0.901046, loss=0.422405
I0315 19:01:46.588407 140146293318848 submission.py:265] 47) loss = 0.422, grad_norm = 0.901
I0315 19:01:46.661171 140096531592960 logging_writer.py:48] [48] global_step=48, grad_norm=1.4237, loss=0.389726
I0315 19:01:46.666020 140146293318848 submission.py:265] 48) loss = 0.390, grad_norm = 1.424
I0315 19:01:46.748834 140096523200256 logging_writer.py:48] [49] global_step=49, grad_norm=0.796771, loss=0.439641
I0315 19:01:46.757396 140146293318848 submission.py:265] 49) loss = 0.440, grad_norm = 0.797
I0315 19:01:46.838733 140096531592960 logging_writer.py:48] [50] global_step=50, grad_norm=0.747986, loss=0.421599
I0315 19:01:46.844279 140146293318848 submission.py:265] 50) loss = 0.422, grad_norm = 0.748
I0315 19:01:46.919044 140096523200256 logging_writer.py:48] [51] global_step=51, grad_norm=0.526238, loss=0.430119
I0315 19:01:46.927568 140146293318848 submission.py:265] 51) loss = 0.430, grad_norm = 0.526
I0315 19:01:47.018066 140096531592960 logging_writer.py:48] [52] global_step=52, grad_norm=0.695401, loss=0.393654
I0315 19:01:47.022446 140146293318848 submission.py:265] 52) loss = 0.394, grad_norm = 0.695
I0315 19:01:47.304563 140096523200256 logging_writer.py:48] [53] global_step=53, grad_norm=0.599007, loss=0.360748
I0315 19:01:47.313397 140146293318848 submission.py:265] 53) loss = 0.361, grad_norm = 0.599
I0315 19:01:47.548778 140096531592960 logging_writer.py:48] [54] global_step=54, grad_norm=0.546523, loss=0.366988
I0315 19:01:47.558137 140146293318848 submission.py:265] 54) loss = 0.367, grad_norm = 0.547
I0315 19:01:47.794275 140096523200256 logging_writer.py:48] [55] global_step=55, grad_norm=0.540525, loss=0.35789
I0315 19:01:47.799440 140146293318848 submission.py:265] 55) loss = 0.358, grad_norm = 0.541
I0315 19:01:48.044311 140096531592960 logging_writer.py:48] [56] global_step=56, grad_norm=0.681124, loss=0.340857
I0315 19:01:48.049398 140146293318848 submission.py:265] 56) loss = 0.341, grad_norm = 0.681
I0315 19:01:48.302722 140096523200256 logging_writer.py:48] [57] global_step=57, grad_norm=1.00069, loss=0.305293
I0315 19:01:48.309753 140146293318848 submission.py:265] 57) loss = 0.305, grad_norm = 1.001
I0315 19:01:48.747252 140096531592960 logging_writer.py:48] [58] global_step=58, grad_norm=0.500967, loss=0.294725
I0315 19:01:48.759909 140146293318848 submission.py:265] 58) loss = 0.295, grad_norm = 0.501
I0315 19:01:49.140054 140096523200256 logging_writer.py:48] [59] global_step=59, grad_norm=1.04488, loss=0.357678
I0315 19:01:49.147634 140146293318848 submission.py:265] 59) loss = 0.358, grad_norm = 1.045
I0315 19:01:49.432082 140096531592960 logging_writer.py:48] [60] global_step=60, grad_norm=0.563679, loss=0.332251
I0315 19:01:49.439074 140146293318848 submission.py:265] 60) loss = 0.332, grad_norm = 0.564
I0315 19:01:49.723321 140096523200256 logging_writer.py:48] [61] global_step=61, grad_norm=0.416805, loss=0.374489
I0315 19:01:49.732715 140146293318848 submission.py:265] 61) loss = 0.374, grad_norm = 0.417
I0315 19:01:50.050758 140096531592960 logging_writer.py:48] [62] global_step=62, grad_norm=0.397117, loss=0.323545
I0315 19:01:50.056962 140146293318848 submission.py:265] 62) loss = 0.324, grad_norm = 0.397
I0315 19:01:50.518003 140096523200256 logging_writer.py:48] [63] global_step=63, grad_norm=0.547147, loss=0.324698
I0315 19:01:50.527119 140146293318848 submission.py:265] 63) loss = 0.325, grad_norm = 0.547
I0315 19:01:50.837992 140096531592960 logging_writer.py:48] [64] global_step=64, grad_norm=1.2955, loss=0.51836
I0315 19:01:50.846552 140146293318848 submission.py:265] 64) loss = 0.518, grad_norm = 1.296
I0315 19:01:51.112523 140096523200256 logging_writer.py:48] [65] global_step=65, grad_norm=0.479436, loss=0.322631
I0315 19:01:51.117411 140146293318848 submission.py:265] 65) loss = 0.323, grad_norm = 0.479
I0315 19:01:51.384986 140096531592960 logging_writer.py:48] [66] global_step=66, grad_norm=0.594447, loss=0.307217
I0315 19:01:51.389850 140146293318848 submission.py:265] 66) loss = 0.307, grad_norm = 0.594
I0315 19:01:51.644638 140096523200256 logging_writer.py:48] [67] global_step=67, grad_norm=1.14249, loss=0.388643
I0315 19:01:51.648417 140146293318848 submission.py:265] 67) loss = 0.389, grad_norm = 1.142
I0315 19:01:51.932315 140096531592960 logging_writer.py:48] [68] global_step=68, grad_norm=0.473948, loss=0.464003
I0315 19:01:51.936573 140146293318848 submission.py:265] 68) loss = 0.464, grad_norm = 0.474
I0315 19:01:52.150415 140096523200256 logging_writer.py:48] [69] global_step=69, grad_norm=0.390996, loss=0.347905
I0315 19:01:52.156553 140146293318848 submission.py:265] 69) loss = 0.348, grad_norm = 0.391
I0315 19:01:52.248487 140096531592960 logging_writer.py:48] [70] global_step=70, grad_norm=0.321995, loss=0.32899
I0315 19:01:52.254519 140146293318848 submission.py:265] 70) loss = 0.329, grad_norm = 0.322
I0315 19:01:52.331234 140096523200256 logging_writer.py:48] [71] global_step=71, grad_norm=0.310366, loss=0.360328
I0315 19:01:52.336746 140146293318848 submission.py:265] 71) loss = 0.360, grad_norm = 0.310
I0315 19:01:52.443341 140096531592960 logging_writer.py:48] [72] global_step=72, grad_norm=0.395427, loss=0.3052
I0315 19:01:52.449496 140146293318848 submission.py:265] 72) loss = 0.305, grad_norm = 0.395
I0315 19:01:52.519536 140096523200256 logging_writer.py:48] [73] global_step=73, grad_norm=0.40881, loss=0.403041
I0315 19:01:52.524661 140146293318848 submission.py:265] 73) loss = 0.403, grad_norm = 0.409
I0315 19:01:52.669422 140096531592960 logging_writer.py:48] [74] global_step=74, grad_norm=0.814929, loss=0.342968
I0315 19:01:52.674247 140146293318848 submission.py:265] 74) loss = 0.343, grad_norm = 0.815
I0315 19:01:52.792304 140096523200256 logging_writer.py:48] [75] global_step=75, grad_norm=0.651689, loss=0.335826
I0315 19:01:52.797743 140146293318848 submission.py:265] 75) loss = 0.336, grad_norm = 0.652
I0315 19:01:52.937228 140096531592960 logging_writer.py:48] [76] global_step=76, grad_norm=0.536457, loss=0.458026
I0315 19:01:52.941901 140146293318848 submission.py:265] 76) loss = 0.458, grad_norm = 0.536
I0315 19:01:53.104156 140096523200256 logging_writer.py:48] [77] global_step=77, grad_norm=0.392643, loss=0.396435
I0315 19:01:53.108615 140146293318848 submission.py:265] 77) loss = 0.396, grad_norm = 0.393
I0315 19:01:53.264439 140096531592960 logging_writer.py:48] [78] global_step=78, grad_norm=0.683238, loss=0.403999
I0315 19:01:53.270053 140146293318848 submission.py:265] 78) loss = 0.404, grad_norm = 0.683
I0315 19:01:53.347439 140096523200256 logging_writer.py:48] [79] global_step=79, grad_norm=0.647047, loss=0.367963
I0315 19:01:53.351994 140146293318848 submission.py:265] 79) loss = 0.368, grad_norm = 0.647
I0315 19:01:53.510214 140096531592960 logging_writer.py:48] [80] global_step=80, grad_norm=0.448054, loss=0.35358
I0315 19:01:53.524383 140146293318848 submission.py:265] 80) loss = 0.354, grad_norm = 0.448
I0315 19:01:53.604599 140096523200256 logging_writer.py:48] [81] global_step=81, grad_norm=0.808236, loss=0.338414
I0315 19:01:53.609718 140146293318848 submission.py:265] 81) loss = 0.338, grad_norm = 0.808
I0315 19:01:53.728679 140096531592960 logging_writer.py:48] [82] global_step=82, grad_norm=1.01346, loss=0.360239
I0315 19:01:53.736385 140146293318848 submission.py:265] 82) loss = 0.360, grad_norm = 1.013
I0315 19:01:53.915364 140096523200256 logging_writer.py:48] [83] global_step=83, grad_norm=0.729045, loss=0.400802
I0315 19:01:53.922331 140146293318848 submission.py:265] 83) loss = 0.401, grad_norm = 0.729
I0315 19:01:54.175809 140096531592960 logging_writer.py:48] [84] global_step=84, grad_norm=0.48934, loss=0.398102
I0315 19:01:54.180488 140146293318848 submission.py:265] 84) loss = 0.398, grad_norm = 0.489
I0315 19:01:54.314603 140096523200256 logging_writer.py:48] [85] global_step=85, grad_norm=0.615829, loss=0.362528
I0315 19:01:54.319219 140146293318848 submission.py:265] 85) loss = 0.363, grad_norm = 0.616
I0315 19:01:54.464363 140096531592960 logging_writer.py:48] [86] global_step=86, grad_norm=0.599589, loss=0.401231
I0315 19:01:54.469227 140146293318848 submission.py:265] 86) loss = 0.401, grad_norm = 0.600
I0315 19:01:54.548018 140096523200256 logging_writer.py:48] [87] global_step=87, grad_norm=0.292113, loss=0.39945
I0315 19:01:54.553643 140146293318848 submission.py:265] 87) loss = 0.399, grad_norm = 0.292
I0315 19:01:54.657501 140096531592960 logging_writer.py:48] [88] global_step=88, grad_norm=0.72191, loss=0.318789
I0315 19:01:54.661860 140146293318848 submission.py:265] 88) loss = 0.319, grad_norm = 0.722
I0315 19:01:54.835671 140096523200256 logging_writer.py:48] [89] global_step=89, grad_norm=0.654932, loss=0.429483
I0315 19:01:54.840626 140146293318848 submission.py:265] 89) loss = 0.429, grad_norm = 0.655
I0315 19:01:54.959907 140096531592960 logging_writer.py:48] [90] global_step=90, grad_norm=0.61746, loss=0.441862
I0315 19:01:54.965010 140146293318848 submission.py:265] 90) loss = 0.442, grad_norm = 0.617
I0315 19:01:55.224187 140096523200256 logging_writer.py:48] [91] global_step=91, grad_norm=0.631687, loss=0.403715
I0315 19:01:55.228851 140146293318848 submission.py:265] 91) loss = 0.404, grad_norm = 0.632
I0315 19:01:55.472039 140096531592960 logging_writer.py:48] [92] global_step=92, grad_norm=0.256438, loss=0.308835
I0315 19:01:55.476948 140146293318848 submission.py:265] 92) loss = 0.309, grad_norm = 0.256
I0315 19:01:55.697040 140096523200256 logging_writer.py:48] [93] global_step=93, grad_norm=0.74442, loss=0.308102
I0315 19:01:55.702587 140146293318848 submission.py:265] 93) loss = 0.308, grad_norm = 0.744
I0315 19:01:55.934014 140096531592960 logging_writer.py:48] [94] global_step=94, grad_norm=0.300262, loss=0.327037
I0315 19:01:55.938599 140146293318848 submission.py:265] 94) loss = 0.327, grad_norm = 0.300
I0315 19:01:56.103141 140096523200256 logging_writer.py:48] [95] global_step=95, grad_norm=0.652784, loss=0.382596
I0315 19:01:56.108380 140146293318848 submission.py:265] 95) loss = 0.383, grad_norm = 0.653
I0315 19:01:56.384664 140096531592960 logging_writer.py:48] [96] global_step=96, grad_norm=0.541576, loss=0.328262
I0315 19:01:56.390504 140146293318848 submission.py:265] 96) loss = 0.328, grad_norm = 0.542
I0315 19:01:56.748808 140096523200256 logging_writer.py:48] [97] global_step=97, grad_norm=0.374149, loss=0.494065
I0315 19:01:56.754367 140146293318848 submission.py:265] 97) loss = 0.494, grad_norm = 0.374
I0315 19:01:57.033708 140096531592960 logging_writer.py:48] [98] global_step=98, grad_norm=0.332046, loss=0.407687
I0315 19:01:57.037912 140146293318848 submission.py:265] 98) loss = 0.408, grad_norm = 0.332
I0315 19:01:57.464525 140096523200256 logging_writer.py:48] [99] global_step=99, grad_norm=0.300426, loss=0.355781
I0315 19:01:57.470242 140146293318848 submission.py:265] 99) loss = 0.356, grad_norm = 0.300
I0315 19:01:57.539757 140096531592960 logging_writer.py:48] [100] global_step=100, grad_norm=0.904899, loss=0.307671
I0315 19:01:57.546189 140146293318848 submission.py:265] 100) loss = 0.308, grad_norm = 0.905
/usr/local/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
I0315 19:03:02.265354 140146293318848 spec.py:321] Evaluating on the training split.
I0315 19:03:04.486024 140146293318848 spec.py:333] Evaluating on the validation split.
I0315 19:03:06.920871 140146293318848 spec.py:349] Evaluating on the test split.
I0315 19:03:09.276628 140146293318848 submission_runner.py:469] Time since start: 296.66s, 	Step: 420, 	{'train/ssim': 0.7073582240513393, 'train/loss': 0.3078629629952567, 'validation/ssim': 0.6866867932699071, 'validation/loss': 0.3234609111212718, 'validation/num_examples': 3554, 'test/ssim': 0.704605183498848, 'test/loss': 0.3252015165762008, 'test/num_examples': 3581, 'score': 153.7083330154419, 'total_duration': 296.65807461738586, 'accumulated_submission_time': 153.7083330154419, 'accumulated_eval_time': 140.40176963806152, 'accumulated_logging_time': 0.017005205154418945}
I0315 19:03:09.287405 140096523200256 logging_writer.py:48] [420] accumulated_eval_time=140.402, accumulated_logging_time=0.0170052, accumulated_submission_time=153.708, global_step=420, preemption_count=0, score=153.708, test/loss=0.325202, test/num_examples=3581, test/ssim=0.704605, total_duration=296.658, train/loss=0.307863, train/ssim=0.707358, validation/loss=0.323461, validation/num_examples=3554, validation/ssim=0.686687
I0315 19:03:25.974086 140096531592960 logging_writer.py:48] [500] global_step=500, grad_norm=0.198038, loss=0.359019
I0315 19:03:25.980715 140146293318848 submission.py:265] 500) loss = 0.359, grad_norm = 0.198
I0315 19:04:30.144767 140146293318848 spec.py:321] Evaluating on the training split.
I0315 19:04:32.407294 140146293318848 spec.py:333] Evaluating on the validation split.
I0315 19:04:34.816819 140146293318848 spec.py:349] Evaluating on the test split.
I0315 19:04:37.028682 140146293318848 submission_runner.py:469] Time since start: 384.41s, 	Step: 773, 	{'train/ssim': 0.7213031223842076, 'train/loss': 0.29395880017961773, 'validation/ssim': 0.7008039466973832, 'validation/loss': 0.3086146331598199, 'validation/num_examples': 3554, 'test/ssim': 0.7180585522985897, 'test/loss': 0.31090797268788395, 'test/num_examples': 3581, 'score': 232.7646780014038, 'total_duration': 384.41015434265137, 'accumulated_submission_time': 232.7646780014038, 'accumulated_eval_time': 147.2859878540039, 'accumulated_logging_time': 0.036096811294555664}
I0315 19:04:37.038354 140096523200256 logging_writer.py:48] [773] accumulated_eval_time=147.286, accumulated_logging_time=0.0360968, accumulated_submission_time=232.765, global_step=773, preemption_count=0, score=232.765, test/loss=0.310908, test/num_examples=3581, test/ssim=0.718059, total_duration=384.41, train/loss=0.293959, train/ssim=0.721303, validation/loss=0.308615, validation/num_examples=3554, validation/ssim=0.700804
I0315 19:05:48.550788 140096531592960 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.0883271, loss=0.293513
I0315 19:05:48.556152 140146293318848 submission.py:265] 1000) loss = 0.294, grad_norm = 0.088
I0315 19:05:58.036002 140146293318848 spec.py:321] Evaluating on the training split.
I0315 19:06:00.075519 140146293318848 spec.py:333] Evaluating on the validation split.
I0315 19:06:02.601523 140146293318848 spec.py:349] Evaluating on the test split.
I0315 19:06:04.726368 140146293318848 submission_runner.py:469] Time since start: 472.11s, 	Step: 1061, 	{'train/ssim': 0.7181926454816546, 'train/loss': 0.2963809626443045, 'validation/ssim': 0.6970765774479459, 'validation/loss': 0.3119490349232379, 'validation/num_examples': 3554, 'test/ssim': 0.7140766943809341, 'test/loss': 0.31424201596751955, 'test/num_examples': 3581, 'score': 312.0791611671448, 'total_duration': 472.1078178882599, 'accumulated_submission_time': 312.0791611671448, 'accumulated_eval_time': 153.97653031349182, 'accumulated_logging_time': 0.053810834884643555}
I0315 19:06:04.770972 140096523200256 logging_writer.py:48] [1061] accumulated_eval_time=153.977, accumulated_logging_time=0.0538108, accumulated_submission_time=312.079, global_step=1061, preemption_count=0, score=312.079, test/loss=0.314242, test/num_examples=3581, test/ssim=0.714077, total_duration=472.108, train/loss=0.296381, train/ssim=0.718193, validation/loss=0.311949, validation/num_examples=3554, validation/ssim=0.697077
I0315 19:06:33.578528 140096531592960 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.187891, loss=0.30149
I0315 19:06:33.581918 140146293318848 submission.py:265] 1500) loss = 0.301, grad_norm = 0.188
I0315 19:07:05.273107 140096523200256 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.53825, loss=0.323885
I0315 19:07:05.277237 140146293318848 submission.py:265] 2000) loss = 0.324, grad_norm = 0.538
I0315 19:07:25.448826 140146293318848 spec.py:321] Evaluating on the training split.
I0315 19:07:27.447935 140146293318848 spec.py:333] Evaluating on the validation split.
I0315 19:07:29.566664 140146293318848 spec.py:349] Evaluating on the test split.
I0315 19:07:31.652930 140146293318848 submission_runner.py:469] Time since start: 559.03s, 	Step: 2309, 	{'train/ssim': 0.7365969930376325, 'train/loss': 0.28225016593933105, 'validation/ssim': 0.7153720115099184, 'validation/loss': 0.2975515731614026, 'validation/num_examples': 3554, 'test/ssim': 0.732381991652995, 'test/loss': 0.2997153078967118, 'test/num_examples': 3581, 'score': 390.8544578552246, 'total_duration': 559.0344023704529, 'accumulated_submission_time': 390.8544578552246, 'accumulated_eval_time': 160.18073964118958, 'accumulated_logging_time': 0.10712218284606934}
I0315 19:07:31.663639 140096531592960 logging_writer.py:48] [2309] accumulated_eval_time=160.181, accumulated_logging_time=0.107122, accumulated_submission_time=390.854, global_step=2309, preemption_count=0, score=390.854, test/loss=0.299715, test/num_examples=3581, test/ssim=0.732382, total_duration=559.034, train/loss=0.28225, train/ssim=0.736597, validation/loss=0.297552, validation/num_examples=3554, validation/ssim=0.715372
I0315 19:07:44.610701 140096523200256 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.269498, loss=0.275568
I0315 19:07:44.614334 140146293318848 submission.py:265] 2500) loss = 0.276, grad_norm = 0.269
I0315 19:08:16.392852 140096531592960 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.385996, loss=0.385632
I0315 19:08:16.396405 140146293318848 submission.py:265] 3000) loss = 0.386, grad_norm = 0.386
I0315 19:08:48.106962 140096523200256 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.314462, loss=0.273738
I0315 19:08:48.110854 140146293318848 submission.py:265] 3500) loss = 0.274, grad_norm = 0.314
I0315 19:08:52.419660 140146293318848 spec.py:321] Evaluating on the training split.
I0315 19:08:54.422795 140146293318848 spec.py:333] Evaluating on the validation split.
I0315 19:08:56.550241 140146293318848 spec.py:349] Evaluating on the test split.
I0315 19:08:58.662992 140146293318848 submission_runner.py:469] Time since start: 646.04s, 	Step: 3558, 	{'train/ssim': 0.7361230850219727, 'train/loss': 0.27949919019426617, 'validation/ssim': 0.7142424661253165, 'validation/loss': 0.29481141407568934, 'validation/num_examples': 3554, 'test/ssim': 0.7316964071444778, 'test/loss': 0.29660778157506634, 'test/num_examples': 3581, 'score': 469.7804834842682, 'total_duration': 646.0444688796997, 'accumulated_submission_time': 469.7804834842682, 'accumulated_eval_time': 166.4242959022522, 'accumulated_logging_time': 0.12713193893432617}
I0315 19:08:58.673039 140096531592960 logging_writer.py:48] [3558] accumulated_eval_time=166.424, accumulated_logging_time=0.127132, accumulated_submission_time=469.78, global_step=3558, preemption_count=0, score=469.78, test/loss=0.296608, test/num_examples=3581, test/ssim=0.731696, total_duration=646.044, train/loss=0.279499, train/ssim=0.736123, validation/loss=0.294811, validation/num_examples=3554, validation/ssim=0.714242
I0315 19:09:27.571180 140096523200256 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.151151, loss=0.30497
I0315 19:09:27.574446 140146293318848 submission.py:265] 4000) loss = 0.305, grad_norm = 0.151
I0315 19:09:59.331089 140096531592960 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.303454, loss=0.268672
I0315 19:09:59.335018 140146293318848 submission.py:265] 4500) loss = 0.269, grad_norm = 0.303
I0315 19:10:19.396999 140146293318848 spec.py:321] Evaluating on the training split.
I0315 19:10:21.420083 140146293318848 spec.py:333] Evaluating on the validation split.
I0315 19:10:23.568593 140146293318848 spec.py:349] Evaluating on the test split.
I0315 19:10:25.693754 140146293318848 submission_runner.py:469] Time since start: 733.08s, 	Step: 4806, 	{'train/ssim': 0.7377322741917202, 'train/loss': 0.2781505584716797, 'validation/ssim': 0.7159214309580754, 'validation/loss': 0.2933562219176456, 'validation/num_examples': 3554, 'test/ssim': 0.7332921500846481, 'test/loss': 0.2951722878778623, 'test/num_examples': 3581, 'score': 548.6473343372345, 'total_duration': 733.0752110481262, 'accumulated_submission_time': 548.6473343372345, 'accumulated_eval_time': 172.72110199928284, 'accumulated_logging_time': 0.14561867713928223}
I0315 19:10:25.704226 140096523200256 logging_writer.py:48] [4806] accumulated_eval_time=172.721, accumulated_logging_time=0.145619, accumulated_submission_time=548.647, global_step=4806, preemption_count=0, score=548.647, test/loss=0.295172, test/num_examples=3581, test/ssim=0.733292, total_duration=733.075, train/loss=0.278151, train/ssim=0.737732, validation/loss=0.293356, validation/num_examples=3554, validation/ssim=0.715921
I0315 19:10:38.830687 140096531592960 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.17557, loss=0.359933
I0315 19:10:38.833967 140146293318848 submission.py:265] 5000) loss = 0.360, grad_norm = 0.176
I0315 19:11:10.544585 140096523200256 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.566994, loss=0.27686
I0315 19:11:10.548400 140146293318848 submission.py:265] 5500) loss = 0.277, grad_norm = 0.567
I0315 19:11:42.279368 140096531592960 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.394421, loss=0.28099
I0315 19:11:42.283355 140146293318848 submission.py:265] 6000) loss = 0.281, grad_norm = 0.394
I0315 19:11:46.495097 140146293318848 spec.py:321] Evaluating on the training split.
I0315 19:11:48.494678 140146293318848 spec.py:333] Evaluating on the validation split.
I0315 19:11:50.617333 140146293318848 spec.py:349] Evaluating on the test split.
I0315 19:11:52.723104 140146293318848 submission_runner.py:469] Time since start: 820.10s, 	Step: 6056, 	{'train/ssim': 0.7392733437674386, 'train/loss': 0.27616015502384733, 'validation/ssim': 0.7175813673853405, 'validation/loss': 0.2914715167438977, 'validation/num_examples': 3554, 'test/ssim': 0.7351534411215442, 'test/loss': 0.29316363298703224, 'test/num_examples': 3581, 'score': 627.5366609096527, 'total_duration': 820.1045751571655, 'accumulated_submission_time': 627.5366609096527, 'accumulated_eval_time': 178.94923162460327, 'accumulated_logging_time': 0.16474676132202148}
I0315 19:11:52.734165 140096523200256 logging_writer.py:48] [6056] accumulated_eval_time=178.949, accumulated_logging_time=0.164747, accumulated_submission_time=627.537, global_step=6056, preemption_count=0, score=627.537, test/loss=0.293164, test/num_examples=3581, test/ssim=0.735153, total_duration=820.105, train/loss=0.27616, train/ssim=0.739273, validation/loss=0.291472, validation/num_examples=3554, validation/ssim=0.717581
I0315 19:12:21.797374 140096531592960 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.457485, loss=0.280735
I0315 19:12:25.314980 140146293318848 submission.py:265] 6500) loss = 0.281, grad_norm = 0.457
I0315 19:12:57.940612 140096523200256 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.739365, loss=0.215887
I0315 19:12:57.943989 140146293318848 submission.py:265] 7000) loss = 0.216, grad_norm = 0.739
I0315 19:13:13.450258 140146293318848 spec.py:321] Evaluating on the training split.
I0315 19:13:15.452794 140146293318848 spec.py:333] Evaluating on the validation split.
I0315 19:13:17.595978 140146293318848 spec.py:349] Evaluating on the test split.
I0315 19:13:19.702221 140146293318848 submission_runner.py:469] Time since start: 907.08s, 	Step: 7235, 	{'train/ssim': 0.7409513337271554, 'train/loss': 0.2761353424617222, 'validation/ssim': 0.7189920110922552, 'validation/loss': 0.2914939111850204, 'validation/num_examples': 3554, 'test/ssim': 0.7365817421809551, 'test/loss': 0.29323975222964954, 'test/num_examples': 3581, 'score': 706.273845911026, 'total_duration': 907.0837149620056, 'accumulated_submission_time': 706.273845911026, 'accumulated_eval_time': 185.20140671730042, 'accumulated_logging_time': 0.18425345420837402}
I0315 19:13:19.712997 140096531592960 logging_writer.py:48] [7235] accumulated_eval_time=185.201, accumulated_logging_time=0.184253, accumulated_submission_time=706.274, global_step=7235, preemption_count=0, score=706.274, test/loss=0.29324, test/num_examples=3581, test/ssim=0.736582, total_duration=907.084, train/loss=0.276135, train/ssim=0.740951, validation/loss=0.291494, validation/num_examples=3554, validation/ssim=0.718992
I0315 19:13:37.368805 140096523200256 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.460472, loss=0.265363
I0315 19:13:37.372431 140146293318848 submission.py:265] 7500) loss = 0.265, grad_norm = 0.460
I0315 19:14:09.066601 140096531592960 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.17125, loss=0.369233
I0315 19:14:09.070281 140146293318848 submission.py:265] 8000) loss = 0.369, grad_norm = 0.171
I0315 19:14:40.441977 140146293318848 spec.py:321] Evaluating on the training split.
I0315 19:14:42.454455 140146293318848 spec.py:333] Evaluating on the validation split.
I0315 19:14:44.649975 140146293318848 spec.py:349] Evaluating on the test split.
I0315 19:14:46.776460 140146293318848 submission_runner.py:469] Time since start: 994.16s, 	Step: 8487, 	{'train/ssim': 0.7388312476021903, 'train/loss': 0.27721461227961947, 'validation/ssim': 0.7164191233689153, 'validation/loss': 0.2927209342136501, 'validation/num_examples': 3554, 'test/ssim': 0.7340626145367914, 'test/loss': 0.2942671063425021, 'test/num_examples': 3581, 'score': 785.1298532485962, 'total_duration': 994.1579301357269, 'accumulated_submission_time': 785.1298532485962, 'accumulated_eval_time': 191.53596210479736, 'accumulated_logging_time': 0.20326852798461914}
I0315 19:14:46.786961 140096523200256 logging_writer.py:48] [8487] accumulated_eval_time=191.536, accumulated_logging_time=0.203269, accumulated_submission_time=785.13, global_step=8487, preemption_count=0, score=785.13, test/loss=0.294267, test/num_examples=3581, test/ssim=0.734063, total_duration=994.158, train/loss=0.277215, train/ssim=0.738831, validation/loss=0.292721, validation/num_examples=3554, validation/ssim=0.716419
I0315 19:14:48.448590 140096531592960 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.348521, loss=0.286918
I0315 19:14:48.452572 140146293318848 submission.py:265] 8500) loss = 0.287, grad_norm = 0.349
I0315 19:15:20.121100 140096523200256 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.504248, loss=0.282359
I0315 19:15:20.124329 140146293318848 submission.py:265] 9000) loss = 0.282, grad_norm = 0.504
I0315 19:15:51.687582 140096531592960 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.192986, loss=0.242624
I0315 19:15:51.690761 140146293318848 submission.py:265] 9500) loss = 0.243, grad_norm = 0.193
I0315 19:16:07.542033 140146293318848 spec.py:321] Evaluating on the training split.
I0315 19:16:09.553900 140146293318848 spec.py:333] Evaluating on the validation split.
I0315 19:16:11.688506 140146293318848 spec.py:349] Evaluating on the test split.
I0315 19:16:13.803474 140146293318848 submission_runner.py:469] Time since start: 1081.18s, 	Step: 9740, 	{'train/ssim': 0.7421254430498395, 'train/loss': 0.274355343409947, 'validation/ssim': 0.720264303865363, 'validation/loss': 0.28981030946644626, 'validation/num_examples': 3554, 'test/ssim': 0.7377496083932561, 'test/loss': 0.2913263060466699, 'test/num_examples': 3581, 'score': 864.0050904750824, 'total_duration': 1081.1849565505981, 'accumulated_submission_time': 864.0050904750824, 'accumulated_eval_time': 197.7975823879242, 'accumulated_logging_time': 0.22165226936340332}
I0315 19:16:13.814023 140096523200256 logging_writer.py:48] [9740] accumulated_eval_time=197.798, accumulated_logging_time=0.221652, accumulated_submission_time=864.005, global_step=9740, preemption_count=0, score=864.005, test/loss=0.291326, test/num_examples=3581, test/ssim=0.73775, total_duration=1081.18, train/loss=0.274355, train/ssim=0.742125, validation/loss=0.28981, validation/num_examples=3554, validation/ssim=0.720264
I0315 19:16:31.112560 140096531592960 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.381157, loss=0.332067
I0315 19:16:31.115906 140146293318848 submission.py:265] 10000) loss = 0.332, grad_norm = 0.381
I0315 19:17:02.829122 140096523200256 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.390587, loss=0.283186
I0315 19:17:02.832754 140146293318848 submission.py:265] 10500) loss = 0.283, grad_norm = 0.391
I0315 19:17:34.513515 140146293318848 spec.py:321] Evaluating on the training split.
I0315 19:17:36.495684 140146293318848 spec.py:333] Evaluating on the validation split.
I0315 19:17:38.592582 140146293318848 spec.py:349] Evaluating on the test split.
I0315 19:17:40.676599 140146293318848 submission_runner.py:469] Time since start: 1168.06s, 	Step: 10992, 	{'train/ssim': 0.743058613368443, 'train/loss': 0.27450621128082275, 'validation/ssim': 0.7218092455244092, 'validation/loss': 0.28988666351953785, 'validation/num_examples': 3554, 'test/ssim': 0.739201362224239, 'test/loss': 0.29142850286232896, 'test/num_examples': 3581, 'score': 942.7845983505249, 'total_duration': 1168.0580859184265, 'accumulated_submission_time': 942.7845983505249, 'accumulated_eval_time': 203.96089482307434, 'accumulated_logging_time': 0.2402787208557129}
I0315 19:17:40.687251 140096531592960 logging_writer.py:48] [10992] accumulated_eval_time=203.961, accumulated_logging_time=0.240279, accumulated_submission_time=942.785, global_step=10992, preemption_count=0, score=942.785, test/loss=0.291429, test/num_examples=3581, test/ssim=0.739201, total_duration=1168.06, train/loss=0.274506, train/ssim=0.743059, validation/loss=0.289887, validation/num_examples=3554, validation/ssim=0.721809
I0315 19:17:42.083770 140096523200256 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.191053, loss=0.355089
I0315 19:17:42.087119 140146293318848 submission.py:265] 11000) loss = 0.355, grad_norm = 0.191
I0315 19:18:13.804107 140096531592960 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.106324, loss=0.373496
I0315 19:18:13.807442 140146293318848 submission.py:265] 11500) loss = 0.373, grad_norm = 0.106
I0315 19:18:45.403045 140096523200256 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.235012, loss=0.223746
I0315 19:18:45.406893 140146293318848 submission.py:265] 12000) loss = 0.224, grad_norm = 0.235
I0315 19:19:01.455995 140146293318848 spec.py:321] Evaluating on the training split.
I0315 19:19:03.448677 140146293318848 spec.py:333] Evaluating on the validation split.
I0315 19:19:05.563667 140146293318848 spec.py:349] Evaluating on the test split.
I0315 19:19:07.649145 140146293318848 submission_runner.py:469] Time since start: 1255.03s, 	Step: 12244, 	{'train/ssim': 0.7433741433279855, 'train/loss': 0.27305638790130615, 'validation/ssim': 0.7212775492842571, 'validation/loss': 0.28888681354855444, 'validation/num_examples': 3554, 'test/ssim': 0.7388109826602206, 'test/loss': 0.2903327675579447, 'test/num_examples': 3581, 'score': 1021.6256160736084, 'total_duration': 1255.0305979251862, 'accumulated_submission_time': 1021.6256160736084, 'accumulated_eval_time': 210.15417909622192, 'accumulated_logging_time': 0.25931668281555176}
I0315 19:19:07.659982 140096531592960 logging_writer.py:48] [12244] accumulated_eval_time=210.154, accumulated_logging_time=0.259317, accumulated_submission_time=1021.63, global_step=12244, preemption_count=0, score=1021.63, test/loss=0.290333, test/num_examples=3581, test/ssim=0.738811, total_duration=1255.03, train/loss=0.273056, train/ssim=0.743374, validation/loss=0.288887, validation/num_examples=3554, validation/ssim=0.721278
I0315 19:19:24.775951 140096523200256 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.206469, loss=0.249354
I0315 19:19:24.779742 140146293318848 submission.py:265] 12500) loss = 0.249, grad_norm = 0.206
I0315 19:19:56.392093 140096531592960 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.151185, loss=0.314869
I0315 19:19:56.395509 140146293318848 submission.py:265] 13000) loss = 0.315, grad_norm = 0.151
I0315 19:20:28.390698 140146293318848 spec.py:321] Evaluating on the training split.
I0315 19:20:30.402895 140146293318848 spec.py:333] Evaluating on the validation split.
I0315 19:20:32.534022 140146293318848 spec.py:349] Evaluating on the test split.
I0315 19:20:34.640536 140146293318848 submission_runner.py:469] Time since start: 1342.02s, 	Step: 13497, 	{'train/ssim': 0.7453458649771554, 'train/loss': 0.27282229491642546, 'validation/ssim': 0.7231907627189434, 'validation/loss': 0.2888733837533413, 'validation/num_examples': 3554, 'test/ssim': 0.7404148386231151, 'test/loss': 0.2904724274491238, 'test/num_examples': 3581, 'score': 1100.437126159668, 'total_duration': 1342.0219945907593, 'accumulated_submission_time': 1100.437126159668, 'accumulated_eval_time': 216.40420413017273, 'accumulated_logging_time': 0.27797579765319824}
I0315 19:20:34.651728 140096523200256 logging_writer.py:48] [13497] accumulated_eval_time=216.404, accumulated_logging_time=0.277976, accumulated_submission_time=1100.44, global_step=13497, preemption_count=0, score=1100.44, test/loss=0.290472, test/num_examples=3581, test/ssim=0.740415, total_duration=1342.02, train/loss=0.272822, train/ssim=0.745346, validation/loss=0.288873, validation/num_examples=3554, validation/ssim=0.723191
I0315 19:20:35.754215 140096531592960 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.539893, loss=0.313806
I0315 19:20:35.757520 140146293318848 submission.py:265] 13500) loss = 0.314, grad_norm = 0.540
I0315 19:21:07.339511 140096523200256 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.194517, loss=0.297729
I0315 19:21:07.342904 140146293318848 submission.py:265] 14000) loss = 0.298, grad_norm = 0.195
I0315 19:21:38.854034 140096531592960 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.236752, loss=0.279105
I0315 19:21:38.857176 140146293318848 submission.py:265] 14500) loss = 0.279, grad_norm = 0.237
I0315 19:21:55.400679 140146293318848 spec.py:321] Evaluating on the training split.
I0315 19:21:57.393361 140146293318848 spec.py:333] Evaluating on the validation split.
I0315 19:21:59.544730 140146293318848 spec.py:349] Evaluating on the test split.
I0315 19:22:01.689586 140146293318848 submission_runner.py:469] Time since start: 1429.07s, 	Step: 14752, 	{'train/ssim': 0.7441869463239398, 'train/loss': 0.2726684127535139, 'validation/ssim': 0.721874986261079, 'validation/loss': 0.28866005265853617, 'validation/num_examples': 3554, 'test/ssim': 0.7392880147610653, 'test/loss': 0.29016675738707765, 'test/num_examples': 3581, 'score': 1179.262228012085, 'total_duration': 1429.0709912776947, 'accumulated_submission_time': 1179.262228012085, 'accumulated_eval_time': 222.69362831115723, 'accumulated_logging_time': 0.2986726760864258}
I0315 19:22:01.703024 140096523200256 logging_writer.py:48] [14752] accumulated_eval_time=222.694, accumulated_logging_time=0.298673, accumulated_submission_time=1179.26, global_step=14752, preemption_count=0, score=1179.26, test/loss=0.290167, test/num_examples=3581, test/ssim=0.739288, total_duration=1429.07, train/loss=0.272668, train/ssim=0.744187, validation/loss=0.28866, validation/num_examples=3554, validation/ssim=0.721875
I0315 19:22:18.361780 140096531592960 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.234168, loss=0.271776
I0315 19:22:18.365012 140146293318848 submission.py:265] 15000) loss = 0.272, grad_norm = 0.234
I0315 19:22:49.887295 140096523200256 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.183997, loss=0.296126
I0315 19:22:49.891100 140146293318848 submission.py:265] 15500) loss = 0.296, grad_norm = 0.184
I0315 19:23:21.501493 140096531592960 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.574776, loss=0.22964
I0315 19:23:21.505019 140146293318848 submission.py:265] 16000) loss = 0.230, grad_norm = 0.575
I0315 19:23:22.452848 140146293318848 spec.py:321] Evaluating on the training split.
I0315 19:23:24.437552 140146293318848 spec.py:333] Evaluating on the validation split.
I0315 19:23:26.536470 140146293318848 spec.py:349] Evaluating on the test split.
I0315 19:23:28.624649 140146293318848 submission_runner.py:469] Time since start: 1516.01s, 	Step: 16005, 	{'train/ssim': 0.7440193721226284, 'train/loss': 0.2725036655153547, 'validation/ssim': 0.7218023073693374, 'validation/loss': 0.2883543273204488, 'validation/num_examples': 3554, 'test/ssim': 0.7392959914304663, 'test/loss': 0.28983016920901983, 'test/num_examples': 3581, 'score': 1258.061113834381, 'total_duration': 1516.0060744285583, 'accumulated_submission_time': 1258.061113834381, 'accumulated_eval_time': 228.8656632900238, 'accumulated_logging_time': 0.3210608959197998}
I0315 19:23:28.636679 140096523200256 logging_writer.py:48] [16005] accumulated_eval_time=228.866, accumulated_logging_time=0.321061, accumulated_submission_time=1258.06, global_step=16005, preemption_count=0, score=1258.06, test/loss=0.28983, test/num_examples=3581, test/ssim=0.739296, total_duration=1516.01, train/loss=0.272504, train/ssim=0.744019, validation/loss=0.288354, validation/num_examples=3554, validation/ssim=0.721802
I0315 19:24:00.746322 140096531592960 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.156907, loss=0.283615
I0315 19:24:00.749460 140146293318848 submission.py:265] 16500) loss = 0.284, grad_norm = 0.157
I0315 19:24:32.324588 140096523200256 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.143355, loss=0.315196
I0315 19:24:32.327964 140146293318848 submission.py:265] 17000) loss = 0.315, grad_norm = 0.143
I0315 19:24:49.346200 140146293318848 spec.py:321] Evaluating on the training split.
I0315 19:24:51.445508 140146293318848 spec.py:333] Evaluating on the validation split.
I0315 19:24:53.662784 140146293318848 spec.py:349] Evaluating on the test split.
I0315 19:24:55.853028 140146293318848 submission_runner.py:469] Time since start: 1603.23s, 	Step: 17261, 	{'train/ssim': 0.7447244099208287, 'train/loss': 0.27191308566502165, 'validation/ssim': 0.722501000193444, 'validation/loss': 0.2878226310802969, 'validation/num_examples': 3554, 'test/ssim': 0.7399323523893465, 'test/loss': 0.28932382114370986, 'test/num_examples': 3581, 'score': 1336.8884885311127, 'total_duration': 1603.2345094680786, 'accumulated_submission_time': 1336.8884885311127, 'accumulated_eval_time': 235.3726086616516, 'accumulated_logging_time': 0.3421640396118164}
I0315 19:24:55.864213 140096531592960 logging_writer.py:48] [17261] accumulated_eval_time=235.373, accumulated_logging_time=0.342164, accumulated_submission_time=1336.89, global_step=17261, preemption_count=0, score=1336.89, test/loss=0.289324, test/num_examples=3581, test/ssim=0.739932, total_duration=1603.23, train/loss=0.271913, train/ssim=0.744724, validation/loss=0.287823, validation/num_examples=3554, validation/ssim=0.722501
I0315 19:25:11.807009 140096523200256 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.258287, loss=0.27653
I0315 19:25:11.810188 140146293318848 submission.py:265] 17500) loss = 0.277, grad_norm = 0.258
I0315 19:25:43.390934 140096531592960 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.27551, loss=0.281972
I0315 19:25:43.394763 140146293318848 submission.py:265] 18000) loss = 0.282, grad_norm = 0.276
I0315 19:26:15.013626 140096523200256 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.224216, loss=0.294749
I0315 19:26:15.016839 140146293318848 submission.py:265] 18500) loss = 0.295, grad_norm = 0.224
I0315 19:26:16.571791 140146293318848 spec.py:321] Evaluating on the training split.
I0315 19:26:18.593325 140146293318848 spec.py:333] Evaluating on the validation split.
I0315 19:26:20.738747 140146293318848 spec.py:349] Evaluating on the test split.
I0315 19:26:22.853940 140146293318848 submission_runner.py:469] Time since start: 1690.24s, 	Step: 18515, 	{'train/ssim': 0.7446695736476353, 'train/loss': 0.27191478865487234, 'validation/ssim': 0.7223735030071751, 'validation/loss': 0.2878624739510059, 'validation/num_examples': 3554, 'test/ssim': 0.7398161793580704, 'test/loss': 0.2893651021122766, 'test/num_examples': 3581, 'score': 1415.7927322387695, 'total_duration': 1690.2354412078857, 'accumulated_submission_time': 1415.7927322387695, 'accumulated_eval_time': 241.6549108028412, 'accumulated_logging_time': 0.3613853454589844}
I0315 19:26:22.865193 140096531592960 logging_writer.py:48] [18515] accumulated_eval_time=241.655, accumulated_logging_time=0.361385, accumulated_submission_time=1415.79, global_step=18515, preemption_count=0, score=1415.79, test/loss=0.289365, test/num_examples=3581, test/ssim=0.739816, total_duration=1690.24, train/loss=0.271915, train/ssim=0.74467, validation/loss=0.287862, validation/num_examples=3554, validation/ssim=0.722374
I0315 19:26:54.288951 140096523200256 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.234274, loss=0.291568
I0315 19:26:54.292123 140146293318848 submission.py:265] 19000) loss = 0.292, grad_norm = 0.234
I0315 19:27:25.911807 140096531592960 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.204999, loss=0.304333
I0315 19:27:25.915190 140146293318848 submission.py:265] 19500) loss = 0.304, grad_norm = 0.205
I0315 19:27:43.582789 140146293318848 spec.py:321] Evaluating on the training split.
I0315 19:27:45.560088 140146293318848 spec.py:333] Evaluating on the validation split.
I0315 19:27:47.671358 140146293318848 spec.py:349] Evaluating on the test split.
I0315 19:27:49.755895 140146293318848 submission_runner.py:469] Time since start: 1777.14s, 	Step: 19771, 	{'train/ssim': 0.7445036343165806, 'train/loss': 0.27186196190970285, 'validation/ssim': 0.7223541998232625, 'validation/loss': 0.2877795423895171, 'validation/num_examples': 3554, 'test/ssim': 0.7398193836611631, 'test/loss': 0.2892660755113795, 'test/num_examples': 3581, 'score': 1494.6639420986176, 'total_duration': 1777.1373591423035, 'accumulated_submission_time': 1494.6639420986176, 'accumulated_eval_time': 247.82817602157593, 'accumulated_logging_time': 0.3811984062194824}
I0315 19:27:49.767223 140096523200256 logging_writer.py:48] [19771] accumulated_eval_time=247.828, accumulated_logging_time=0.381198, accumulated_submission_time=1494.66, global_step=19771, preemption_count=0, score=1494.66, test/loss=0.289266, test/num_examples=3581, test/ssim=0.739819, total_duration=1777.14, train/loss=0.271862, train/ssim=0.744504, validation/loss=0.28778, validation/num_examples=3554, validation/ssim=0.722354
I0315 19:28:05.102805 140096531592960 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.241305, loss=0.316843
I0315 19:28:05.106688 140146293318848 submission.py:265] 20000) loss = 0.317, grad_norm = 0.241
I0315 19:28:36.673289 140096523200256 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.14972, loss=0.295111
I0315 19:28:36.676424 140146293318848 submission.py:265] 20500) loss = 0.295, grad_norm = 0.150
I0315 19:29:08.215508 140096531592960 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.102373, loss=0.337756
I0315 19:29:08.218664 140146293318848 submission.py:265] 21000) loss = 0.338, grad_norm = 0.102
I0315 19:29:10.430636 140146293318848 spec.py:321] Evaluating on the training split.
I0315 19:29:12.456366 140146293318848 spec.py:333] Evaluating on the validation split.
I0315 19:29:14.585371 140146293318848 spec.py:349] Evaluating on the test split.
I0315 19:29:16.683029 140146293318848 submission_runner.py:469] Time since start: 1864.06s, 	Step: 21026, 	{'train/ssim': 0.7441000938415527, 'train/loss': 0.2721811192376273, 'validation/ssim': 0.7219677926719893, 'validation/loss': 0.2880178267994689, 'validation/num_examples': 3554, 'test/ssim': 0.7393942339997557, 'test/loss': 0.2895158747992879, 'test/num_examples': 3581, 'score': 1573.4895980358124, 'total_duration': 1864.0645184516907, 'accumulated_submission_time': 1573.4895980358124, 'accumulated_eval_time': 254.0806529521942, 'accumulated_logging_time': 0.4006764888763428}
I0315 19:29:16.694582 140096523200256 logging_writer.py:48] [21026] accumulated_eval_time=254.081, accumulated_logging_time=0.400676, accumulated_submission_time=1573.49, global_step=21026, preemption_count=0, score=1573.49, test/loss=0.289516, test/num_examples=3581, test/ssim=0.739394, total_duration=1864.06, train/loss=0.272181, train/ssim=0.7441, validation/loss=0.288018, validation/num_examples=3554, validation/ssim=0.721968
I0315 19:29:47.497963 140096531592960 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.213141, loss=0.328761
I0315 19:29:47.501807 140146293318848 submission.py:265] 21500) loss = 0.329, grad_norm = 0.213
I0315 19:30:19.014468 140096523200256 logging_writer.py:48] [22000] global_step=22000, grad_norm=0.138497, loss=0.273447
I0315 19:30:19.017683 140146293318848 submission.py:265] 22000) loss = 0.273, grad_norm = 0.138
I0315 19:30:37.394020 140146293318848 spec.py:321] Evaluating on the training split.
I0315 19:30:39.403609 140146293318848 spec.py:333] Evaluating on the validation split.
I0315 19:30:41.522330 140146293318848 spec.py:349] Evaluating on the test split.
I0315 19:30:43.625786 140146293318848 submission_runner.py:469] Time since start: 1951.01s, 	Step: 22282, 	{'train/ssim': 0.7433674676077706, 'train/loss': 0.2728325298854283, 'validation/ssim': 0.7210479032208427, 'validation/loss': 0.28870679933701465, 'validation/num_examples': 3554, 'test/ssim': 0.7385365715974938, 'test/loss': 0.290172722844963, 'test/num_examples': 3581, 'score': 1652.3817439079285, 'total_duration': 1951.0072338581085, 'accumulated_submission_time': 1652.3817439079285, 'accumulated_eval_time': 260.3125813007355, 'accumulated_logging_time': 0.4203329086303711}
I0315 19:30:43.637086 140096531592960 logging_writer.py:48] [22282] accumulated_eval_time=260.313, accumulated_logging_time=0.420333, accumulated_submission_time=1652.38, global_step=22282, preemption_count=0, score=1652.38, test/loss=0.290173, test/num_examples=3581, test/ssim=0.738537, total_duration=1951.01, train/loss=0.272833, train/ssim=0.743367, validation/loss=0.288707, validation/num_examples=3554, validation/ssim=0.721048
I0315 19:30:58.295424 140096523200256 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.662857, loss=0.314164
I0315 19:30:58.298741 140146293318848 submission.py:265] 22500) loss = 0.314, grad_norm = 0.663
I0315 19:31:29.856606 140096531592960 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.319044, loss=0.265183
I0315 19:31:29.859888 140146293318848 submission.py:265] 23000) loss = 0.265, grad_norm = 0.319
I0315 19:32:01.443449 140096523200256 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.185979, loss=0.258445
I0315 19:32:01.447428 140146293318848 submission.py:265] 23500) loss = 0.258, grad_norm = 0.186
I0315 19:32:04.421792 140146293318848 spec.py:321] Evaluating on the training split.
I0315 19:32:06.423127 140146293318848 spec.py:333] Evaluating on the validation split.
I0315 19:32:08.530447 140146293318848 spec.py:349] Evaluating on the test split.
I0315 19:32:10.634500 140146293318848 submission_runner.py:469] Time since start: 2038.02s, 	Step: 23537, 	{'train/ssim': 0.743966920035226, 'train/loss': 0.27261509214128765, 'validation/ssim': 0.7218409137371623, 'validation/loss': 0.2883646658584517, 'validation/num_examples': 3554, 'test/ssim': 0.7393314432944709, 'test/loss': 0.2898451680745602, 'test/num_examples': 3581, 'score': 1731.2403120994568, 'total_duration': 2038.015981912613, 'accumulated_submission_time': 1731.2403120994568, 'accumulated_eval_time': 266.5254509449005, 'accumulated_logging_time': 0.43962788581848145}
I0315 19:32:10.646311 140096531592960 logging_writer.py:48] [23537] accumulated_eval_time=266.525, accumulated_logging_time=0.439628, accumulated_submission_time=1731.24, global_step=23537, preemption_count=0, score=1731.24, test/loss=0.289845, test/num_examples=3581, test/ssim=0.739331, total_duration=2038.02, train/loss=0.272615, train/ssim=0.743967, validation/loss=0.288365, validation/num_examples=3554, validation/ssim=0.721841
I0315 19:32:40.776653 140096523200256 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.125056, loss=0.272439
I0315 19:32:40.780454 140146293318848 submission.py:265] 24000) loss = 0.272, grad_norm = 0.125
I0315 19:33:12.290865 140096531592960 logging_writer.py:48] [24500] global_step=24500, grad_norm=0.278191, loss=0.260622
I0315 19:33:12.294153 140146293318848 submission.py:265] 24500) loss = 0.261, grad_norm = 0.278
I0315 19:33:31.379640 140146293318848 spec.py:321] Evaluating on the training split.
I0315 19:33:33.365082 140146293318848 spec.py:333] Evaluating on the validation split.
I0315 19:33:35.461236 140146293318848 spec.py:349] Evaluating on the test split.
I0315 19:33:37.535804 140146293318848 submission_runner.py:469] Time since start: 2124.92s, 	Step: 24794, 	{'train/ssim': 0.7427633830479213, 'train/loss': 0.2737788302557809, 'validation/ssim': 0.720846490639948, 'validation/loss': 0.2895091523195695, 'validation/num_examples': 3554, 'test/ssim': 0.7384386017348507, 'test/loss': 0.29087170406747415, 'test/num_examples': 3581, 'score': 1810.0445396900177, 'total_duration': 2124.917290210724, 'accumulated_submission_time': 1810.0445396900177, 'accumulated_eval_time': 272.68181681632996, 'accumulated_logging_time': 0.45932912826538086}
I0315 19:33:37.547374 140096523200256 logging_writer.py:48] [24794] accumulated_eval_time=272.682, accumulated_logging_time=0.459329, accumulated_submission_time=1810.04, global_step=24794, preemption_count=0, score=1810.04, test/loss=0.290872, test/num_examples=3581, test/ssim=0.738439, total_duration=2124.92, train/loss=0.273779, train/ssim=0.742763, validation/loss=0.289509, validation/num_examples=3554, validation/ssim=0.720846
I0315 19:33:51.424166 140096531592960 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.340925, loss=0.261038
I0315 19:33:51.427595 140146293318848 submission.py:265] 25000) loss = 0.261, grad_norm = 0.341
I0315 19:34:22.971815 140096523200256 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.261415, loss=0.297939
I0315 19:34:22.975219 140146293318848 submission.py:265] 25500) loss = 0.298, grad_norm = 0.261
I0315 19:34:54.549862 140096531592960 logging_writer.py:48] [26000] global_step=26000, grad_norm=0.186626, loss=0.271841
I0315 19:34:54.553617 140146293318848 submission.py:265] 26000) loss = 0.272, grad_norm = 0.187
I0315 19:34:58.204121 140146293318848 spec.py:321] Evaluating on the training split.
I0315 19:35:00.215513 140146293318848 spec.py:333] Evaluating on the validation split.
I0315 19:35:02.329685 140146293318848 spec.py:349] Evaluating on the test split.
I0315 19:35:04.428940 140146293318848 submission_runner.py:469] Time since start: 2211.81s, 	Step: 26049, 	{'train/ssim': 0.7435014588492257, 'train/loss': 0.2732975482940674, 'validation/ssim': 0.7215003945818093, 'validation/loss': 0.2890255079553848, 'validation/num_examples': 3554, 'test/ssim': 0.7389908326933817, 'test/loss': 0.29043803232337334, 'test/num_examples': 3581, 'score': 1888.8450696468353, 'total_duration': 2211.810420036316, 'accumulated_submission_time': 1888.8450696468353, 'accumulated_eval_time': 278.9067015647888, 'accumulated_logging_time': 0.47882509231567383}
I0315 19:35:04.440876 140096523200256 logging_writer.py:48] [26049] accumulated_eval_time=278.907, accumulated_logging_time=0.478825, accumulated_submission_time=1888.85, global_step=26049, preemption_count=0, score=1888.85, test/loss=0.290438, test/num_examples=3581, test/ssim=0.738991, total_duration=2211.81, train/loss=0.273298, train/ssim=0.743501, validation/loss=0.289026, validation/num_examples=3554, validation/ssim=0.7215
I0315 19:35:33.675496 140096531592960 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.232081, loss=0.331283
I0315 19:35:33.678701 140146293318848 submission.py:265] 26500) loss = 0.331, grad_norm = 0.232
I0315 19:36:05.310881 140096523200256 logging_writer.py:48] [27000] global_step=27000, grad_norm=0.384853, loss=0.2917
I0315 19:36:05.314160 140146293318848 submission.py:265] 27000) loss = 0.292, grad_norm = 0.385
I0315 19:36:25.193280 140146293318848 spec.py:321] Evaluating on the training split.
I0315 19:36:27.204695 140146293318848 spec.py:333] Evaluating on the validation split.
I0315 19:36:29.330854 140146293318848 spec.py:349] Evaluating on the test split.
I0315 19:36:31.433150 140146293318848 submission_runner.py:469] Time since start: 2298.81s, 	Step: 27305, 	{'train/ssim': 0.7441748891557965, 'train/loss': 0.27299850327628, 'validation/ssim': 0.7218036125668261, 'validation/loss': 0.2890189132733364, 'validation/num_examples': 3554, 'test/ssim': 0.7391586836341106, 'test/loss': 0.29056719300867423, 'test/num_examples': 3581, 'score': 1967.7468943595886, 'total_duration': 2298.814632177353, 'accumulated_submission_time': 1967.7468943595886, 'accumulated_eval_time': 285.1467297077179, 'accumulated_logging_time': 0.4989640712738037}
I0315 19:36:31.444720 140096531592960 logging_writer.py:48] [27305] accumulated_eval_time=285.147, accumulated_logging_time=0.498964, accumulated_submission_time=1967.75, global_step=27305, preemption_count=0, score=1967.75, test/loss=0.290567, test/num_examples=3581, test/ssim=0.739159, total_duration=2298.81, train/loss=0.272999, train/ssim=0.744175, validation/loss=0.289019, validation/num_examples=3554, validation/ssim=0.721804
I0315 19:36:44.600667 140096523200256 logging_writer.py:48] [27500] global_step=27500, grad_norm=0.267995, loss=0.262339
I0315 19:36:44.603829 140146293318848 submission.py:265] 27500) loss = 0.262, grad_norm = 0.268
I0315 19:37:16.156820 140096531592960 logging_writer.py:48] [28000] global_step=28000, grad_norm=0.253677, loss=0.257884
I0315 19:37:16.160165 140146293318848 submission.py:265] 28000) loss = 0.258, grad_norm = 0.254
I0315 19:37:47.605953 140096523200256 logging_writer.py:48] [28500] global_step=28500, grad_norm=0.493234, loss=0.323749
I0315 19:37:47.609194 140146293318848 submission.py:265] 28500) loss = 0.324, grad_norm = 0.493
I0315 19:37:52.135027 140146293318848 spec.py:321] Evaluating on the training split.
I0315 19:37:54.142193 140146293318848 spec.py:333] Evaluating on the validation split.
I0315 19:37:56.252233 140146293318848 spec.py:349] Evaluating on the test split.
I0315 19:37:58.350026 140146293318848 submission_runner.py:469] Time since start: 2385.73s, 	Step: 28563, 	{'train/ssim': 0.7423482622419085, 'train/loss': 0.27453681400844027, 'validation/ssim': 0.7201113896753658, 'validation/loss': 0.2904934086152926, 'validation/num_examples': 3554, 'test/ssim': 0.7375633497539095, 'test/loss': 0.2918937063123953, 'test/num_examples': 3581, 'score': 2046.5627608299255, 'total_duration': 2385.7315006256104, 'accumulated_submission_time': 2046.5627608299255, 'accumulated_eval_time': 291.3618280887604, 'accumulated_logging_time': 0.5184464454650879}
I0315 19:37:58.361703 140096531592960 logging_writer.py:48] [28563] accumulated_eval_time=291.362, accumulated_logging_time=0.518446, accumulated_submission_time=2046.56, global_step=28563, preemption_count=0, score=2046.56, test/loss=0.291894, test/num_examples=3581, test/ssim=0.737563, total_duration=2385.73, train/loss=0.274537, train/ssim=0.742348, validation/loss=0.290493, validation/num_examples=3554, validation/ssim=0.720111
I0315 19:38:26.714264 140096523200256 logging_writer.py:48] [29000] global_step=29000, grad_norm=0.146534, loss=0.261761
I0315 19:38:26.717691 140146293318848 submission.py:265] 29000) loss = 0.262, grad_norm = 0.147
I0315 19:38:58.152745 140096531592960 logging_writer.py:48] [29500] global_step=29500, grad_norm=0.154197, loss=0.274223
I0315 19:38:58.156525 140146293318848 submission.py:265] 29500) loss = 0.274, grad_norm = 0.154
I0315 19:39:19.003317 140146293318848 spec.py:321] Evaluating on the training split.
I0315 19:39:21.014005 140146293318848 spec.py:333] Evaluating on the validation split.
I0315 19:39:23.115142 140146293318848 spec.py:349] Evaluating on the test split.
I0315 19:39:25.196893 140146293318848 submission_runner.py:469] Time since start: 2472.58s, 	Step: 29822, 	{'train/ssim': 0.7403538567679269, 'train/loss': 0.275881256375994, 'validation/ssim': 0.7177984423361001, 'validation/loss': 0.2920175014618212, 'validation/num_examples': 3554, 'test/ssim': 0.7352285036259075, 'test/loss': 0.29359437313468656, 'test/num_examples': 3581, 'score': 2125.4383203983307, 'total_duration': 2472.5783755779266, 'accumulated_submission_time': 2125.4383203983307, 'accumulated_eval_time': 297.5554790496826, 'accumulated_logging_time': 0.5380234718322754}
I0315 19:39:25.209212 140096523200256 logging_writer.py:48] [29822] accumulated_eval_time=297.555, accumulated_logging_time=0.538023, accumulated_submission_time=2125.44, global_step=29822, preemption_count=0, score=2125.44, test/loss=0.293594, test/num_examples=3581, test/ssim=0.735229, total_duration=2472.58, train/loss=0.275881, train/ssim=0.740354, validation/loss=0.292018, validation/num_examples=3554, validation/ssim=0.717798
I0315 19:39:37.213157 140096531592960 logging_writer.py:48] [30000] global_step=30000, grad_norm=0.421533, loss=0.347121
I0315 19:39:37.217149 140146293318848 submission.py:265] 30000) loss = 0.347, grad_norm = 0.422
I0315 19:40:08.738933 140096523200256 logging_writer.py:48] [30500] global_step=30500, grad_norm=0.129388, loss=0.243647
I0315 19:40:08.742568 140146293318848 submission.py:265] 30500) loss = 0.244, grad_norm = 0.129
I0315 19:40:40.275600 140096531592960 logging_writer.py:48] [31000] global_step=31000, grad_norm=0.427209, loss=0.30214
I0315 19:40:40.279896 140146293318848 submission.py:265] 31000) loss = 0.302, grad_norm = 0.427
I0315 19:40:45.911401 140146293318848 spec.py:321] Evaluating on the training split.
I0315 19:40:47.914200 140146293318848 spec.py:333] Evaluating on the validation split.
I0315 19:40:50.070789 140146293318848 spec.py:349] Evaluating on the test split.
I0315 19:40:52.211522 140146293318848 submission_runner.py:469] Time since start: 2559.59s, 	Step: 31080, 	{'train/ssim': 0.7451820373535156, 'train/loss': 0.27517989703587126, 'validation/ssim': 0.7233183972944218, 'validation/loss': 0.29080517907859105, 'validation/num_examples': 3554, 'test/ssim': 0.7403774778125872, 'test/loss': 0.29264531991762077, 'test/num_examples': 3581, 'score': 2204.319138288498, 'total_duration': 2559.593016386032, 'accumulated_submission_time': 2204.319138288498, 'accumulated_eval_time': 303.85582995414734, 'accumulated_logging_time': 0.5590479373931885}
I0315 19:40:52.223644 140096523200256 logging_writer.py:48] [31080] accumulated_eval_time=303.856, accumulated_logging_time=0.559048, accumulated_submission_time=2204.32, global_step=31080, preemption_count=0, score=2204.32, test/loss=0.292645, test/num_examples=3581, test/ssim=0.740377, total_duration=2559.59, train/loss=0.27518, train/ssim=0.745182, validation/loss=0.290805, validation/num_examples=3554, validation/ssim=0.723318
I0315 19:41:19.529133 140096531592960 logging_writer.py:48] [31500] global_step=31500, grad_norm=0.134078, loss=0.270537
I0315 19:41:19.532339 140146293318848 submission.py:265] 31500) loss = 0.271, grad_norm = 0.134
I0315 19:41:50.957444 140096523200256 logging_writer.py:48] [32000] global_step=32000, grad_norm=0.589316, loss=0.328955
I0315 19:41:50.960546 140146293318848 submission.py:265] 32000) loss = 0.329, grad_norm = 0.589
I0315 19:42:12.864980 140146293318848 spec.py:321] Evaluating on the training split.
I0315 19:42:14.897976 140146293318848 spec.py:333] Evaluating on the validation split.
I0315 19:42:17.112135 140146293318848 spec.py:349] Evaluating on the test split.
I0315 19:42:19.276881 140146293318848 submission_runner.py:469] Time since start: 2646.66s, 	Step: 32339, 	{'train/ssim': 0.7411383220127651, 'train/loss': 0.27474245003291536, 'validation/ssim': 0.718848508063098, 'validation/loss': 0.29066651901906304, 'validation/num_examples': 3554, 'test/ssim': 0.7363913929419157, 'test/loss': 0.2922037396807805, 'test/num_examples': 3581, 'score': 2283.1173145771027, 'total_duration': 2646.658350944519, 'accumulated_submission_time': 2283.1173145771027, 'accumulated_eval_time': 310.2677991390228, 'accumulated_logging_time': 0.5792012214660645}
I0315 19:42:19.288814 140096531592960 logging_writer.py:48] [32339] accumulated_eval_time=310.268, accumulated_logging_time=0.579201, accumulated_submission_time=2283.12, global_step=32339, preemption_count=0, score=2283.12, test/loss=0.292204, test/num_examples=3581, test/ssim=0.736391, total_duration=2646.66, train/loss=0.274742, train/ssim=0.741138, validation/loss=0.290667, validation/num_examples=3554, validation/ssim=0.718849
I0315 19:42:30.231624 140096523200256 logging_writer.py:48] [32500] global_step=32500, grad_norm=0.231845, loss=0.312572
I0315 19:42:30.234782 140146293318848 submission.py:265] 32500) loss = 0.313, grad_norm = 0.232
I0315 19:43:01.674015 140096531592960 logging_writer.py:48] [33000] global_step=33000, grad_norm=0.429515, loss=0.250571
I0315 19:43:01.677265 140146293318848 submission.py:265] 33000) loss = 0.251, grad_norm = 0.430
I0315 19:43:33.066049 140096523200256 logging_writer.py:48] [33500] global_step=33500, grad_norm=0.232138, loss=0.305976
I0315 19:43:33.069456 140146293318848 submission.py:265] 33500) loss = 0.306, grad_norm = 0.232
I0315 19:43:39.930012 140146293318848 spec.py:321] Evaluating on the training split.
I0315 19:43:41.952903 140146293318848 spec.py:333] Evaluating on the validation split.
I0315 19:43:44.139886 140146293318848 spec.py:349] Evaluating on the test split.
I0315 19:43:46.290927 140146293318848 submission_runner.py:469] Time since start: 2733.67s, 	Step: 33600, 	{'train/ssim': 0.7411050796508789, 'train/loss': 0.2742088351930891, 'validation/ssim': 0.7188852596765968, 'validation/loss': 0.28989212474060916, 'validation/num_examples': 3554, 'test/ssim': 0.736381098266022, 'test/loss': 0.29146132992486384, 'test/num_examples': 3581, 'score': 2361.9817926883698, 'total_duration': 2733.6724212169647, 'accumulated_submission_time': 2361.9817926883698, 'accumulated_eval_time': 316.6288607120514, 'accumulated_logging_time': 0.5992915630340576}
I0315 19:43:46.303359 140096531592960 logging_writer.py:48] [33600] accumulated_eval_time=316.629, accumulated_logging_time=0.599292, accumulated_submission_time=2361.98, global_step=33600, preemption_count=0, score=2361.98, test/loss=0.291461, test/num_examples=3581, test/ssim=0.736381, total_duration=2733.67, train/loss=0.274209, train/ssim=0.741105, validation/loss=0.289892, validation/num_examples=3554, validation/ssim=0.718885
I0315 19:44:12.264603 140096523200256 logging_writer.py:48] [34000] global_step=34000, grad_norm=0.407443, loss=0.340463
I0315 19:44:12.267836 140146293318848 submission.py:265] 34000) loss = 0.340, grad_norm = 0.407
I0315 19:44:43.753834 140096531592960 logging_writer.py:48] [34500] global_step=34500, grad_norm=0.282193, loss=0.257187
I0315 19:44:43.757035 140146293318848 submission.py:265] 34500) loss = 0.257, grad_norm = 0.282
I0315 19:45:07.032326 140146293318848 spec.py:321] Evaluating on the training split.
I0315 19:45:09.018428 140146293318848 spec.py:333] Evaluating on the validation split.
I0315 19:45:11.102814 140146293318848 spec.py:349] Evaluating on the test split.
I0315 19:45:13.152719 140146293318848 submission_runner.py:469] Time since start: 2820.53s, 	Step: 34860, 	{'train/ssim': 0.74284485408238, 'train/loss': 0.2736622095108032, 'validation/ssim': 0.7207758038917417, 'validation/loss': 0.28963245913495356, 'validation/num_examples': 3554, 'test/ssim': 0.7381798713042795, 'test/loss': 0.2911903617835451, 'test/num_examples': 3581, 'score': 2440.8483040332794, 'total_duration': 2820.5341522693634, 'accumulated_submission_time': 2440.8483040332794, 'accumulated_eval_time': 322.749392747879, 'accumulated_logging_time': 0.6201107501983643}
I0315 19:45:13.167291 140096523200256 logging_writer.py:48] [34860] accumulated_eval_time=322.749, accumulated_logging_time=0.620111, accumulated_submission_time=2440.85, global_step=34860, preemption_count=0, score=2440.85, test/loss=0.29119, test/num_examples=3581, test/ssim=0.73818, total_duration=2820.53, train/loss=0.273662, train/ssim=0.742845, validation/loss=0.289632, validation/num_examples=3554, validation/ssim=0.720776
I0315 19:45:22.957682 140096531592960 logging_writer.py:48] [35000] global_step=35000, grad_norm=0.164191, loss=0.245756
I0315 19:45:22.961204 140146293318848 submission.py:265] 35000) loss = 0.246, grad_norm = 0.164
I0315 19:45:54.422240 140096523200256 logging_writer.py:48] [35500] global_step=35500, grad_norm=0.178272, loss=0.385801
I0315 19:45:54.425349 140146293318848 submission.py:265] 35500) loss = 0.386, grad_norm = 0.178
I0315 19:46:25.913887 140096531592960 logging_writer.py:48] [36000] global_step=36000, grad_norm=0.179949, loss=0.233571
I0315 19:46:25.917022 140146293318848 submission.py:265] 36000) loss = 0.234, grad_norm = 0.180
I0315 19:46:33.858009 140146293318848 spec.py:321] Evaluating on the training split.
I0315 19:46:35.985544 140146293318848 spec.py:333] Evaluating on the validation split.
I0315 19:46:38.262072 140146293318848 spec.py:349] Evaluating on the test split.
I0315 19:46:40.495230 140146293318848 submission_runner.py:469] Time since start: 2907.88s, 	Step: 36116, 	{'train/ssim': 0.7414957455226353, 'train/loss': 0.27564564773014616, 'validation/ssim': 0.7189575264007104, 'validation/loss': 0.2916531796259496, 'validation/num_examples': 3554, 'test/ssim': 0.736527269028379, 'test/loss': 0.29303679030822394, 'test/num_examples': 3581, 'score': 2519.6292798519135, 'total_duration': 2907.8767132759094, 'accumulated_submission_time': 2519.6292798519135, 'accumulated_eval_time': 329.38674092292786, 'accumulated_logging_time': 0.6433703899383545}
I0315 19:46:40.507367 140096523200256 logging_writer.py:48] [36116] accumulated_eval_time=329.387, accumulated_logging_time=0.64337, accumulated_submission_time=2519.63, global_step=36116, preemption_count=0, score=2519.63, test/loss=0.293037, test/num_examples=3581, test/ssim=0.736527, total_duration=2907.88, train/loss=0.275646, train/ssim=0.741496, validation/loss=0.291653, validation/num_examples=3554, validation/ssim=0.718958
I0315 19:47:05.506045 140096531592960 logging_writer.py:48] [36500] global_step=36500, grad_norm=0.156364, loss=0.306568
I0315 19:47:05.509499 140146293318848 submission.py:265] 36500) loss = 0.307, grad_norm = 0.156
I0315 19:47:36.934948 140096523200256 logging_writer.py:48] [37000] global_step=37000, grad_norm=0.280421, loss=0.230488
I0315 19:47:36.938413 140146293318848 submission.py:265] 37000) loss = 0.230, grad_norm = 0.280
I0315 19:48:01.296506 140146293318848 spec.py:321] Evaluating on the training split.
I0315 19:48:03.289871 140146293318848 spec.py:333] Evaluating on the validation split.
I0315 19:48:05.492170 140146293318848 spec.py:349] Evaluating on the test split.
I0315 19:48:07.651981 140146293318848 submission_runner.py:469] Time since start: 2995.03s, 	Step: 37377, 	{'train/ssim': 0.7422714233398438, 'train/loss': 0.2742816720690046, 'validation/ssim': 0.7197518421145188, 'validation/loss': 0.2903473295384602, 'validation/num_examples': 3554, 'test/ssim': 0.7371814922725844, 'test/loss': 0.29188330937150936, 'test/num_examples': 3581, 'score': 2598.540436267853, 'total_duration': 2995.0334854125977, 'accumulated_submission_time': 2598.540436267853, 'accumulated_eval_time': 335.74242997169495, 'accumulated_logging_time': 0.6638855934143066}
I0315 19:48:07.664058 140096531592960 logging_writer.py:48] [37377] accumulated_eval_time=335.742, accumulated_logging_time=0.663886, accumulated_submission_time=2598.54, global_step=37377, preemption_count=0, score=2598.54, test/loss=0.291883, test/num_examples=3581, test/ssim=0.737181, total_duration=2995.03, train/loss=0.274282, train/ssim=0.742271, validation/loss=0.290347, validation/num_examples=3554, validation/ssim=0.719752
I0315 19:48:16.293174 140096523200256 logging_writer.py:48] [37500] global_step=37500, grad_norm=0.298621, loss=0.284192
I0315 19:48:16.296385 140146293318848 submission.py:265] 37500) loss = 0.284, grad_norm = 0.299
I0315 19:48:47.779774 140096531592960 logging_writer.py:48] [38000] global_step=38000, grad_norm=0.570058, loss=0.262537
I0315 19:48:47.783099 140146293318848 submission.py:265] 38000) loss = 0.263, grad_norm = 0.570
I0315 19:49:19.283743 140096523200256 logging_writer.py:48] [38500] global_step=38500, grad_norm=0.211427, loss=0.30058
I0315 19:49:19.287040 140146293318848 submission.py:265] 38500) loss = 0.301, grad_norm = 0.211
I0315 19:49:28.369322 140146293318848 spec.py:321] Evaluating on the training split.
I0315 19:49:30.372795 140146293318848 spec.py:333] Evaluating on the validation split.
I0315 19:49:32.545935 140146293318848 spec.py:349] Evaluating on the test split.
I0315 19:49:34.684532 140146293318848 submission_runner.py:469] Time since start: 3082.07s, 	Step: 38635, 	{'train/ssim': 0.7446798597063337, 'train/loss': 0.2733526570456369, 'validation/ssim': 0.7226071333576604, 'validation/loss': 0.2892033926341798, 'validation/num_examples': 3554, 'test/ssim': 0.7398272921539025, 'test/loss': 0.2908303890105767, 'test/num_examples': 3581, 'score': 2677.3347611427307, 'total_duration': 3082.0659799575806, 'accumulated_submission_time': 2677.3347611427307, 'accumulated_eval_time': 342.05778336524963, 'accumulated_logging_time': 0.6843328475952148}
I0315 19:49:34.698214 140096531592960 logging_writer.py:48] [38635] accumulated_eval_time=342.058, accumulated_logging_time=0.684333, accumulated_submission_time=2677.33, global_step=38635, preemption_count=0, score=2677.33, test/loss=0.29083, test/num_examples=3581, test/ssim=0.739827, total_duration=3082.07, train/loss=0.273353, train/ssim=0.74468, validation/loss=0.289203, validation/num_examples=3554, validation/ssim=0.722607
I0315 19:49:58.519047 140096523200256 logging_writer.py:48] [39000] global_step=39000, grad_norm=0.253601, loss=0.388969
I0315 19:49:58.522865 140146293318848 submission.py:265] 39000) loss = 0.389, grad_norm = 0.254
I0315 19:50:29.940057 140096531592960 logging_writer.py:48] [39500] global_step=39500, grad_norm=0.241452, loss=0.324705
I0315 19:50:29.943398 140146293318848 submission.py:265] 39500) loss = 0.325, grad_norm = 0.241
I0315 19:50:55.426340 140146293318848 spec.py:321] Evaluating on the training split.
I0315 19:50:57.412409 140146293318848 spec.py:333] Evaluating on the validation split.
I0315 19:50:59.593090 140146293318848 spec.py:349] Evaluating on the test split.
I0315 19:51:01.751653 140146293318848 submission_runner.py:469] Time since start: 3169.13s, 	Step: 39897, 	{'train/ssim': 0.7435912404741559, 'train/loss': 0.2731384038925171, 'validation/ssim': 0.7217312771481078, 'validation/loss': 0.288757633344471, 'validation/num_examples': 3554, 'test/ssim': 0.7391068011946733, 'test/loss': 0.29031026925963416, 'test/num_examples': 3581, 'score': 2756.181461572647, 'total_duration': 3169.133134365082, 'accumulated_submission_time': 2756.181461572647, 'accumulated_eval_time': 348.38329219818115, 'accumulated_logging_time': 0.7059872150421143}
I0315 19:51:01.764345 140096523200256 logging_writer.py:48] [39897] accumulated_eval_time=348.383, accumulated_logging_time=0.705987, accumulated_submission_time=2756.18, global_step=39897, preemption_count=0, score=2756.18, test/loss=0.29031, test/num_examples=3581, test/ssim=0.739107, total_duration=3169.13, train/loss=0.273138, train/ssim=0.743591, validation/loss=0.288758, validation/num_examples=3554, validation/ssim=0.721731
I0315 19:51:09.166843 140096531592960 logging_writer.py:48] [40000] global_step=40000, grad_norm=0.384478, loss=0.245344
I0315 19:51:09.170159 140146293318848 submission.py:265] 40000) loss = 0.245, grad_norm = 0.384
I0315 19:51:40.579590 140096523200256 logging_writer.py:48] [40500] global_step=40500, grad_norm=0.0990645, loss=0.367635
I0315 19:51:40.583260 140146293318848 submission.py:265] 40500) loss = 0.368, grad_norm = 0.099
I0315 19:52:12.024387 140096531592960 logging_writer.py:48] [41000] global_step=41000, grad_norm=0.14985, loss=0.290693
I0315 19:52:12.027986 140146293318848 submission.py:265] 41000) loss = 0.291, grad_norm = 0.150
I0315 19:52:22.483579 140146293318848 spec.py:321] Evaluating on the training split.
I0315 19:52:24.506683 140146293318848 spec.py:333] Evaluating on the validation split.
I0315 19:52:26.709545 140146293318848 spec.py:349] Evaluating on the test split.
I0315 19:52:28.882489 140146293318848 submission_runner.py:469] Time since start: 3256.26s, 	Step: 41157, 	{'train/ssim': 0.7411667960030692, 'train/loss': 0.2745837824685233, 'validation/ssim': 0.7188650634628235, 'validation/loss': 0.29048598959798816, 'validation/num_examples': 3554, 'test/ssim': 0.7359778333129713, 'test/loss': 0.2921823322090547, 'test/num_examples': 3581, 'score': 2835.008713245392, 'total_duration': 3256.2639620304108, 'accumulated_submission_time': 2835.008713245392, 'accumulated_eval_time': 354.78237223625183, 'accumulated_logging_time': 0.7274649143218994}
I0315 19:52:28.895188 140096523200256 logging_writer.py:48] [41157] accumulated_eval_time=354.782, accumulated_logging_time=0.727465, accumulated_submission_time=2835.01, global_step=41157, preemption_count=0, score=2835.01, test/loss=0.292182, test/num_examples=3581, test/ssim=0.735978, total_duration=3256.26, train/loss=0.274584, train/ssim=0.741167, validation/loss=0.290486, validation/num_examples=3554, validation/ssim=0.718865
I0315 19:52:51.285123 140096531592960 logging_writer.py:48] [41500] global_step=41500, grad_norm=0.58667, loss=0.229319
I0315 19:52:51.288842 140146293318848 submission.py:265] 41500) loss = 0.229, grad_norm = 0.587
I0315 19:53:22.801703 140096523200256 logging_writer.py:48] [42000] global_step=42000, grad_norm=0.262414, loss=0.225199
I0315 19:53:22.804957 140146293318848 submission.py:265] 42000) loss = 0.225, grad_norm = 0.262
I0315 19:53:49.549446 140146293318848 spec.py:321] Evaluating on the training split.
I0315 19:53:51.547352 140146293318848 spec.py:333] Evaluating on the validation split.
I0315 19:53:53.718156 140146293318848 spec.py:349] Evaluating on the test split.
I0315 19:53:55.869781 140146293318848 submission_runner.py:469] Time since start: 3343.25s, 	Step: 42417, 	{'train/ssim': 0.7425377028329032, 'train/loss': 0.2732114621571132, 'validation/ssim': 0.7201099470886677, 'validation/loss': 0.289322921246307, 'validation/num_examples': 3554, 'test/ssim': 0.7376958170072955, 'test/loss': 0.2907759499463313, 'test/num_examples': 3581, 'score': 2913.799752473831, 'total_duration': 3343.2511398792267, 'accumulated_submission_time': 2913.799752473831, 'accumulated_eval_time': 361.10269474983215, 'accumulated_logging_time': 0.7485363483428955}
I0315 19:53:55.883275 140096531592960 logging_writer.py:48] [42417] accumulated_eval_time=361.103, accumulated_logging_time=0.748536, accumulated_submission_time=2913.8, global_step=42417, preemption_count=0, score=2913.8, test/loss=0.290776, test/num_examples=3581, test/ssim=0.737696, total_duration=3343.25, train/loss=0.273211, train/ssim=0.742538, validation/loss=0.289323, validation/num_examples=3554, validation/ssim=0.72011
I0315 19:54:01.863753 140096523200256 logging_writer.py:48] [42500] global_step=42500, grad_norm=0.102876, loss=0.322445
I0315 19:54:01.866785 140146293318848 submission.py:265] 42500) loss = 0.322, grad_norm = 0.103
I0315 19:54:33.269786 140096531592960 logging_writer.py:48] [43000] global_step=43000, grad_norm=0.108581, loss=0.273406
I0315 19:54:33.273248 140146293318848 submission.py:265] 43000) loss = 0.273, grad_norm = 0.109
I0315 19:55:04.684620 140096523200256 logging_writer.py:48] [43500] global_step=43500, grad_norm=0.55644, loss=0.276427
I0315 19:55:04.687940 140146293318848 submission.py:265] 43500) loss = 0.276, grad_norm = 0.556
I0315 19:55:16.545785 140146293318848 spec.py:321] Evaluating on the training split.
I0315 19:55:18.562258 140146293318848 spec.py:333] Evaluating on the validation split.
I0315 19:55:20.738351 140146293318848 spec.py:349] Evaluating on the test split.
I0315 19:55:22.872641 140146293318848 submission_runner.py:469] Time since start: 3430.25s, 	Step: 43681, 	{'train/ssim': 0.7433334759303502, 'train/loss': 0.27305974279131207, 'validation/ssim': 0.7203966096739589, 'validation/loss': 0.28945502097108894, 'validation/num_examples': 3554, 'test/ssim': 0.7379518203714046, 'test/loss': 0.29092280247530367, 'test/num_examples': 3581, 'score': 2992.668555498123, 'total_duration': 3430.2541375160217, 'accumulated_submission_time': 2992.668555498123, 'accumulated_eval_time': 367.4296371936798, 'accumulated_logging_time': 0.7704877853393555}
I0315 19:55:22.885547 140096531592960 logging_writer.py:48] [43681] accumulated_eval_time=367.43, accumulated_logging_time=0.770488, accumulated_submission_time=2992.67, global_step=43681, preemption_count=0, score=2992.67, test/loss=0.290923, test/num_examples=3581, test/ssim=0.737952, total_duration=3430.25, train/loss=0.27306, train/ssim=0.743333, validation/loss=0.289455, validation/num_examples=3554, validation/ssim=0.720397
I0315 19:55:43.759343 140096523200256 logging_writer.py:48] [44000] global_step=44000, grad_norm=0.134418, loss=0.362206
I0315 19:55:43.763186 140146293318848 submission.py:265] 44000) loss = 0.362, grad_norm = 0.134
I0315 19:56:15.202481 140096531592960 logging_writer.py:48] [44500] global_step=44500, grad_norm=0.115453, loss=0.322048
I0315 19:56:15.205715 140146293318848 submission.py:265] 44500) loss = 0.322, grad_norm = 0.115
I0315 19:56:43.555400 140146293318848 spec.py:321] Evaluating on the training split.
I0315 19:56:45.566019 140146293318848 spec.py:333] Evaluating on the validation split.
I0315 19:56:47.750213 140146293318848 spec.py:349] Evaluating on the test split.
I0315 19:56:49.928533 140146293318848 submission_runner.py:469] Time since start: 3517.31s, 	Step: 44942, 	{'train/ssim': 0.7441113335745675, 'train/loss': 0.27217819009508404, 'validation/ssim': 0.7217832102692389, 'validation/loss': 0.2881997644599395, 'validation/num_examples': 3554, 'test/ssim': 0.7391518659679559, 'test/loss': 0.28976690126710414, 'test/num_examples': 3581, 'score': 3071.5357432365417, 'total_duration': 3517.3099768161774, 'accumulated_submission_time': 3071.5357432365417, 'accumulated_eval_time': 373.80280804634094, 'accumulated_logging_time': 0.7916111946105957}
I0315 19:56:49.941940 140096523200256 logging_writer.py:48] [44942] accumulated_eval_time=373.803, accumulated_logging_time=0.791611, accumulated_submission_time=3071.54, global_step=44942, preemption_count=0, score=3071.54, test/loss=0.289767, test/num_examples=3581, test/ssim=0.739152, total_duration=3517.31, train/loss=0.272178, train/ssim=0.744111, validation/loss=0.2882, validation/num_examples=3554, validation/ssim=0.721783
I0315 19:56:54.443514 140096531592960 logging_writer.py:48] [45000] global_step=45000, grad_norm=0.393562, loss=0.262917
I0315 19:56:54.446775 140146293318848 submission.py:265] 45000) loss = 0.263, grad_norm = 0.394
I0315 19:57:25.904683 140096523200256 logging_writer.py:48] [45500] global_step=45500, grad_norm=0.122766, loss=0.331337
I0315 19:57:25.908207 140146293318848 submission.py:265] 45500) loss = 0.331, grad_norm = 0.123
I0315 19:57:57.298609 140096531592960 logging_writer.py:48] [46000] global_step=46000, grad_norm=0.173566, loss=0.311104
I0315 19:57:57.302314 140146293318848 submission.py:265] 46000) loss = 0.311, grad_norm = 0.174
I0315 19:58:10.633926 140146293318848 spec.py:321] Evaluating on the training split.
I0315 19:58:12.654056 140146293318848 spec.py:333] Evaluating on the validation split.
I0315 19:58:14.864481 140146293318848 spec.py:349] Evaluating on the test split.
I0315 19:58:17.044235 140146293318848 submission_runner.py:469] Time since start: 3604.43s, 	Step: 46201, 	{'train/ssim': 0.7446784973144531, 'train/loss': 0.2719996656690325, 'validation/ssim': 0.7225244937482415, 'validation/loss': 0.2879621154750809, 'validation/num_examples': 3554, 'test/ssim': 0.739901127478358, 'test/loss': 0.28942669972598434, 'test/num_examples': 3581, 'score': 3150.4060542583466, 'total_duration': 3604.42573261261, 'accumulated_submission_time': 3150.4060542583466, 'accumulated_eval_time': 380.2132203578949, 'accumulated_logging_time': 0.8131155967712402}
I0315 19:58:17.057094 140096523200256 logging_writer.py:48] [46201] accumulated_eval_time=380.213, accumulated_logging_time=0.813116, accumulated_submission_time=3150.41, global_step=46201, preemption_count=0, score=3150.41, test/loss=0.289427, test/num_examples=3581, test/ssim=0.739901, total_duration=3604.43, train/loss=0.272, train/ssim=0.744678, validation/loss=0.287962, validation/num_examples=3554, validation/ssim=0.722524
I0315 19:58:36.641186 140096531592960 logging_writer.py:48] [46500] global_step=46500, grad_norm=0.323336, loss=0.253723
I0315 19:58:36.644399 140146293318848 submission.py:265] 46500) loss = 0.254, grad_norm = 0.323
I0315 19:59:08.121921 140096523200256 logging_writer.py:48] [47000] global_step=47000, grad_norm=0.142645, loss=0.218871
I0315 19:59:08.125090 140146293318848 submission.py:265] 47000) loss = 0.219, grad_norm = 0.143
I0315 19:59:37.756547 140146293318848 spec.py:321] Evaluating on the training split.
I0315 19:59:39.747625 140146293318848 spec.py:333] Evaluating on the validation split.
I0315 19:59:41.920425 140146293318848 spec.py:349] Evaluating on the test split.
I0315 19:59:44.080423 140146293318848 submission_runner.py:469] Time since start: 3691.46s, 	Step: 47462, 	{'train/ssim': 0.7452500888279506, 'train/loss': 0.2719661167689732, 'validation/ssim': 0.7226425797736705, 'validation/loss': 0.2882189302546427, 'validation/num_examples': 3554, 'test/ssim': 0.740064001522794, 'test/loss': 0.28965284171233596, 'test/num_examples': 3581, 'score': 3229.265285730362, 'total_duration': 3691.4618928432465, 'accumulated_submission_time': 3229.265285730362, 'accumulated_eval_time': 386.5372929573059, 'accumulated_logging_time': 0.8343875408172607}
I0315 19:59:44.094003 140096531592960 logging_writer.py:48] [47462] accumulated_eval_time=386.537, accumulated_logging_time=0.834388, accumulated_submission_time=3229.27, global_step=47462, preemption_count=0, score=3229.27, test/loss=0.289653, test/num_examples=3581, test/ssim=0.740064, total_duration=3691.46, train/loss=0.271966, train/ssim=0.74525, validation/loss=0.288219, validation/num_examples=3554, validation/ssim=0.722643
I0315 19:59:47.352916 140096523200256 logging_writer.py:48] [47500] global_step=47500, grad_norm=0.260086, loss=0.316506
I0315 19:59:47.356771 140146293318848 submission.py:265] 47500) loss = 0.317, grad_norm = 0.260
I0315 20:00:18.789930 140096531592960 logging_writer.py:48] [48000] global_step=48000, grad_norm=0.461564, loss=0.278237
I0315 20:00:18.793372 140146293318848 submission.py:265] 48000) loss = 0.278, grad_norm = 0.462
I0315 20:00:50.196948 140096523200256 logging_writer.py:48] [48500] global_step=48500, grad_norm=0.496301, loss=0.244573
I0315 20:00:50.200226 140146293318848 submission.py:265] 48500) loss = 0.245, grad_norm = 0.496
I0315 20:01:04.790722 140146293318848 spec.py:321] Evaluating on the training split.
I0315 20:01:06.788144 140146293318848 spec.py:333] Evaluating on the validation split.
I0315 20:01:08.944512 140146293318848 spec.py:349] Evaluating on the test split.
I0315 20:01:11.086130 140146293318848 submission_runner.py:469] Time since start: 3778.47s, 	Step: 48723, 	{'train/ssim': 0.7448912348066058, 'train/loss': 0.27262304510389057, 'validation/ssim': 0.722528546729917, 'validation/loss': 0.28874289835176914, 'validation/num_examples': 3554, 'test/ssim': 0.7398832651930327, 'test/loss': 0.29020043665788187, 'test/num_examples': 3581, 'score': 3308.1266119480133, 'total_duration': 3778.4676311016083, 'accumulated_submission_time': 3308.1266119480133, 'accumulated_eval_time': 392.832781791687, 'accumulated_logging_time': 0.856410026550293}
I0315 20:01:11.099410 140096531592960 logging_writer.py:48] [48723] accumulated_eval_time=392.833, accumulated_logging_time=0.85641, accumulated_submission_time=3308.13, global_step=48723, preemption_count=0, score=3308.13, test/loss=0.2902, test/num_examples=3581, test/ssim=0.739883, total_duration=3778.47, train/loss=0.272623, train/ssim=0.744891, validation/loss=0.288743, validation/num_examples=3554, validation/ssim=0.722529
I0315 20:01:29.283509 140096523200256 logging_writer.py:48] [49000] global_step=49000, grad_norm=0.210118, loss=0.39125
I0315 20:01:29.287231 140146293318848 submission.py:265] 49000) loss = 0.391, grad_norm = 0.210
I0315 20:02:00.692291 140096531592960 logging_writer.py:48] [49500] global_step=49500, grad_norm=0.227034, loss=0.322141
I0315 20:02:00.695469 140146293318848 submission.py:265] 49500) loss = 0.322, grad_norm = 0.227
I0315 20:02:31.811320 140146293318848 spec.py:321] Evaluating on the training split.
I0315 20:02:33.843537 140146293318848 spec.py:333] Evaluating on the validation split.
I0315 20:02:36.053091 140146293318848 spec.py:349] Evaluating on the test split.
I0315 20:02:38.228579 140146293318848 submission_runner.py:469] Time since start: 3865.61s, 	Step: 49987, 	{'train/ssim': 0.7459492002214704, 'train/loss': 0.2708491597856794, 'validation/ssim': 0.7232905759795301, 'validation/loss': 0.28727645745221053, 'validation/num_examples': 3554, 'test/ssim': 0.7406545477651145, 'test/loss': 0.28873409302132785, 'test/num_examples': 3581, 'score': 3386.9748871326447, 'total_duration': 3865.610069513321, 'accumulated_submission_time': 3386.9748871326447, 'accumulated_eval_time': 399.2502522468567, 'accumulated_logging_time': 0.8782632350921631}
I0315 20:02:38.241648 140096523200256 logging_writer.py:48] [49987] accumulated_eval_time=399.25, accumulated_logging_time=0.878263, accumulated_submission_time=3386.97, global_step=49987, preemption_count=0, score=3386.97, test/loss=0.288734, test/num_examples=3581, test/ssim=0.740655, total_duration=3865.61, train/loss=0.270849, train/ssim=0.745949, validation/loss=0.287276, validation/num_examples=3554, validation/ssim=0.723291
I0315 20:02:39.940625 140096531592960 logging_writer.py:48] [50000] global_step=50000, grad_norm=0.280667, loss=0.269557
I0315 20:02:39.943745 140146293318848 submission.py:265] 50000) loss = 0.270, grad_norm = 0.281
I0315 20:03:11.364125 140096523200256 logging_writer.py:48] [50500] global_step=50500, grad_norm=0.236584, loss=0.267326
I0315 20:03:11.367603 140146293318848 submission.py:265] 50500) loss = 0.267, grad_norm = 0.237
I0315 20:03:42.860792 140096531592960 logging_writer.py:48] [51000] global_step=51000, grad_norm=0.40233, loss=0.278623
I0315 20:03:42.864372 140146293318848 submission.py:265] 51000) loss = 0.279, grad_norm = 0.402
I0315 20:03:58.940963 140146293318848 spec.py:321] Evaluating on the training split.
I0315 20:04:00.959722 140146293318848 spec.py:333] Evaluating on the validation split.
I0315 20:04:03.161548 140146293318848 spec.py:349] Evaluating on the test split.
I0315 20:04:05.335390 140146293318848 submission_runner.py:469] Time since start: 3952.72s, 	Step: 51247, 	{'train/ssim': 0.7458992004394531, 'train/loss': 0.270669903073992, 'validation/ssim': 0.7232503896357977, 'validation/loss': 0.28706898257245356, 'validation/num_examples': 3554, 'test/ssim': 0.7406816820764103, 'test/loss': 0.28848596406162735, 'test/num_examples': 3581, 'score': 3465.8320820331573, 'total_duration': 3952.716889858246, 'accumulated_submission_time': 3465.8320820331573, 'accumulated_eval_time': 405.64476919174194, 'accumulated_logging_time': 0.9005115032196045}
I0315 20:04:05.348551 140096523200256 logging_writer.py:48] [51247] accumulated_eval_time=405.645, accumulated_logging_time=0.900512, accumulated_submission_time=3465.83, global_step=51247, preemption_count=0, score=3465.83, test/loss=0.288486, test/num_examples=3581, test/ssim=0.740682, total_duration=3952.72, train/loss=0.27067, train/ssim=0.745899, validation/loss=0.287069, validation/num_examples=3554, validation/ssim=0.72325
I0315 20:04:22.046764 140096531592960 logging_writer.py:48] [51500] global_step=51500, grad_norm=0.257864, loss=0.309862
I0315 20:04:22.050494 140146293318848 submission.py:265] 51500) loss = 0.310, grad_norm = 0.258
I0315 20:04:54.269572 140096523200256 logging_writer.py:48] [52000] global_step=52000, grad_norm=0.102227, loss=0.341691
I0315 20:04:54.272626 140146293318848 submission.py:265] 52000) loss = 0.342, grad_norm = 0.102
I0315 20:05:26.055134 140146293318848 spec.py:321] Evaluating on the training split.
I0315 20:05:28.065207 140146293318848 spec.py:333] Evaluating on the validation split.
I0315 20:05:30.189479 140146293318848 spec.py:349] Evaluating on the test split.
I0315 20:05:32.271168 140146293318848 submission_runner.py:469] Time since start: 4039.65s, 	Step: 52497, 	{'train/ssim': 0.7465798514229911, 'train/loss': 0.27028936999184744, 'validation/ssim': 0.7240425758168613, 'validation/loss': 0.2866542388967537, 'validation/num_examples': 3554, 'test/ssim': 0.7413828108637601, 'test/loss': 0.2881275934402053, 'test/num_examples': 3581, 'score': 3544.689062833786, 'total_duration': 4039.652656555176, 'accumulated_submission_time': 3544.689062833786, 'accumulated_eval_time': 411.8609836101532, 'accumulated_logging_time': 0.9216856956481934}
I0315 20:05:32.284279 140096531592960 logging_writer.py:48] [52497] accumulated_eval_time=411.861, accumulated_logging_time=0.921686, accumulated_submission_time=3544.69, global_step=52497, preemption_count=0, score=3544.69, test/loss=0.288128, test/num_examples=3581, test/ssim=0.741383, total_duration=4039.65, train/loss=0.270289, train/ssim=0.74658, validation/loss=0.286654, validation/num_examples=3554, validation/ssim=0.724043
I0315 20:05:32.937327 140096523200256 logging_writer.py:48] [52497] global_step=52497, preemption_count=0, score=3544.69
I0315 20:05:33.699038 140146293318848 submission_runner.py:646] Tuning trial 1/5
I0315 20:05:33.699244 140146293318848 submission_runner.py:647] Hyperparameters: Hyperparameters(learning_rate=0.0017486387539278373, one_minus_beta1=0.06733926164, one_minus_beta2=0.00448403102, epsilon=1e-08, one_minus_momentum=0.0, use_momentum=False, weight_decay=0.08121616522670176, max_preconditioner_dim=1024, precondition_frequency=100, start_preconditioning_step=-1, inv_root_override=0, exponent_multiplier=1.0, grafting_type='ADAM', grafting_epsilon=1e-08, use_normalized_grafting=False, communication_dtype='FP32', communicate_params=True, use_cosine_decay=True, warmup_factor=0.02, label_smoothing=0.0, dropout_rate=0.1, use_nadam=True, step_hint_factor=1.0)
I0315 20:05:33.700457 140146293318848 submission_runner.py:648] Metrics: {'eval_results': [(1, {'train/ssim': 0.1951230423791068, 'train/loss': 0.9957185472760882, 'validation/ssim': 0.18485184016359207, 'validation/loss': 1.0091675697717362, 'validation/num_examples': 3554, 'test/ssim': 0.20717075649369066, 'test/loss': 1.0065030990383272, 'test/num_examples': 3581, 'score': 74.42088913917542, 'total_duration': 208.60333847999573, 'accumulated_submission_time': 74.42088913917542, 'accumulated_eval_time': 133.39040756225586, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (420, {'train/ssim': 0.7073582240513393, 'train/loss': 0.3078629629952567, 'validation/ssim': 0.6866867932699071, 'validation/loss': 0.3234609111212718, 'validation/num_examples': 3554, 'test/ssim': 0.704605183498848, 'test/loss': 0.3252015165762008, 'test/num_examples': 3581, 'score': 153.7083330154419, 'total_duration': 296.65807461738586, 'accumulated_submission_time': 153.7083330154419, 'accumulated_eval_time': 140.40176963806152, 'accumulated_logging_time': 0.017005205154418945, 'global_step': 420, 'preemption_count': 0}), (773, {'train/ssim': 0.7213031223842076, 'train/loss': 0.29395880017961773, 'validation/ssim': 0.7008039466973832, 'validation/loss': 0.3086146331598199, 'validation/num_examples': 3554, 'test/ssim': 0.7180585522985897, 'test/loss': 0.31090797268788395, 'test/num_examples': 3581, 'score': 232.7646780014038, 'total_duration': 384.41015434265137, 'accumulated_submission_time': 232.7646780014038, 'accumulated_eval_time': 147.2859878540039, 'accumulated_logging_time': 0.036096811294555664, 'global_step': 773, 'preemption_count': 0}), (1061, {'train/ssim': 0.7181926454816546, 'train/loss': 0.2963809626443045, 'validation/ssim': 0.6970765774479459, 'validation/loss': 0.3119490349232379, 'validation/num_examples': 3554, 'test/ssim': 0.7140766943809341, 'test/loss': 0.31424201596751955, 'test/num_examples': 3581, 'score': 312.0791611671448, 'total_duration': 472.1078178882599, 'accumulated_submission_time': 312.0791611671448, 'accumulated_eval_time': 153.97653031349182, 'accumulated_logging_time': 0.053810834884643555, 'global_step': 1061, 'preemption_count': 0}), (2309, {'train/ssim': 0.7365969930376325, 'train/loss': 0.28225016593933105, 'validation/ssim': 0.7153720115099184, 'validation/loss': 0.2975515731614026, 'validation/num_examples': 3554, 'test/ssim': 0.732381991652995, 'test/loss': 0.2997153078967118, 'test/num_examples': 3581, 'score': 390.8544578552246, 'total_duration': 559.0344023704529, 'accumulated_submission_time': 390.8544578552246, 'accumulated_eval_time': 160.18073964118958, 'accumulated_logging_time': 0.10712218284606934, 'global_step': 2309, 'preemption_count': 0}), (3558, {'train/ssim': 0.7361230850219727, 'train/loss': 0.27949919019426617, 'validation/ssim': 0.7142424661253165, 'validation/loss': 0.29481141407568934, 'validation/num_examples': 3554, 'test/ssim': 0.7316964071444778, 'test/loss': 0.29660778157506634, 'test/num_examples': 3581, 'score': 469.7804834842682, 'total_duration': 646.0444688796997, 'accumulated_submission_time': 469.7804834842682, 'accumulated_eval_time': 166.4242959022522, 'accumulated_logging_time': 0.12713193893432617, 'global_step': 3558, 'preemption_count': 0}), (4806, {'train/ssim': 0.7377322741917202, 'train/loss': 0.2781505584716797, 'validation/ssim': 0.7159214309580754, 'validation/loss': 0.2933562219176456, 'validation/num_examples': 3554, 'test/ssim': 0.7332921500846481, 'test/loss': 0.2951722878778623, 'test/num_examples': 3581, 'score': 548.6473343372345, 'total_duration': 733.0752110481262, 'accumulated_submission_time': 548.6473343372345, 'accumulated_eval_time': 172.72110199928284, 'accumulated_logging_time': 0.14561867713928223, 'global_step': 4806, 'preemption_count': 0}), (6056, {'train/ssim': 0.7392733437674386, 'train/loss': 0.27616015502384733, 'validation/ssim': 0.7175813673853405, 'validation/loss': 0.2914715167438977, 'validation/num_examples': 3554, 'test/ssim': 0.7351534411215442, 'test/loss': 0.29316363298703224, 'test/num_examples': 3581, 'score': 627.5366609096527, 'total_duration': 820.1045751571655, 'accumulated_submission_time': 627.5366609096527, 'accumulated_eval_time': 178.94923162460327, 'accumulated_logging_time': 0.16474676132202148, 'global_step': 6056, 'preemption_count': 0}), (7235, {'train/ssim': 0.7409513337271554, 'train/loss': 0.2761353424617222, 'validation/ssim': 0.7189920110922552, 'validation/loss': 0.2914939111850204, 'validation/num_examples': 3554, 'test/ssim': 0.7365817421809551, 'test/loss': 0.29323975222964954, 'test/num_examples': 3581, 'score': 706.273845911026, 'total_duration': 907.0837149620056, 'accumulated_submission_time': 706.273845911026, 'accumulated_eval_time': 185.20140671730042, 'accumulated_logging_time': 0.18425345420837402, 'global_step': 7235, 'preemption_count': 0}), (8487, {'train/ssim': 0.7388312476021903, 'train/loss': 0.27721461227961947, 'validation/ssim': 0.7164191233689153, 'validation/loss': 0.2927209342136501, 'validation/num_examples': 3554, 'test/ssim': 0.7340626145367914, 'test/loss': 0.2942671063425021, 'test/num_examples': 3581, 'score': 785.1298532485962, 'total_duration': 994.1579301357269, 'accumulated_submission_time': 785.1298532485962, 'accumulated_eval_time': 191.53596210479736, 'accumulated_logging_time': 0.20326852798461914, 'global_step': 8487, 'preemption_count': 0}), (9740, {'train/ssim': 0.7421254430498395, 'train/loss': 0.274355343409947, 'validation/ssim': 0.720264303865363, 'validation/loss': 0.28981030946644626, 'validation/num_examples': 3554, 'test/ssim': 0.7377496083932561, 'test/loss': 0.2913263060466699, 'test/num_examples': 3581, 'score': 864.0050904750824, 'total_duration': 1081.1849565505981, 'accumulated_submission_time': 864.0050904750824, 'accumulated_eval_time': 197.7975823879242, 'accumulated_logging_time': 0.22165226936340332, 'global_step': 9740, 'preemption_count': 0}), (10992, {'train/ssim': 0.743058613368443, 'train/loss': 0.27450621128082275, 'validation/ssim': 0.7218092455244092, 'validation/loss': 0.28988666351953785, 'validation/num_examples': 3554, 'test/ssim': 0.739201362224239, 'test/loss': 0.29142850286232896, 'test/num_examples': 3581, 'score': 942.7845983505249, 'total_duration': 1168.0580859184265, 'accumulated_submission_time': 942.7845983505249, 'accumulated_eval_time': 203.96089482307434, 'accumulated_logging_time': 0.2402787208557129, 'global_step': 10992, 'preemption_count': 0}), (12244, {'train/ssim': 0.7433741433279855, 'train/loss': 0.27305638790130615, 'validation/ssim': 0.7212775492842571, 'validation/loss': 0.28888681354855444, 'validation/num_examples': 3554, 'test/ssim': 0.7388109826602206, 'test/loss': 0.2903327675579447, 'test/num_examples': 3581, 'score': 1021.6256160736084, 'total_duration': 1255.0305979251862, 'accumulated_submission_time': 1021.6256160736084, 'accumulated_eval_time': 210.15417909622192, 'accumulated_logging_time': 0.25931668281555176, 'global_step': 12244, 'preemption_count': 0}), (13497, {'train/ssim': 0.7453458649771554, 'train/loss': 0.27282229491642546, 'validation/ssim': 0.7231907627189434, 'validation/loss': 0.2888733837533413, 'validation/num_examples': 3554, 'test/ssim': 0.7404148386231151, 'test/loss': 0.2904724274491238, 'test/num_examples': 3581, 'score': 1100.437126159668, 'total_duration': 1342.0219945907593, 'accumulated_submission_time': 1100.437126159668, 'accumulated_eval_time': 216.40420413017273, 'accumulated_logging_time': 0.27797579765319824, 'global_step': 13497, 'preemption_count': 0}), (14752, {'train/ssim': 0.7441869463239398, 'train/loss': 0.2726684127535139, 'validation/ssim': 0.721874986261079, 'validation/loss': 0.28866005265853617, 'validation/num_examples': 3554, 'test/ssim': 0.7392880147610653, 'test/loss': 0.29016675738707765, 'test/num_examples': 3581, 'score': 1179.262228012085, 'total_duration': 1429.0709912776947, 'accumulated_submission_time': 1179.262228012085, 'accumulated_eval_time': 222.69362831115723, 'accumulated_logging_time': 0.2986726760864258, 'global_step': 14752, 'preemption_count': 0}), (16005, {'train/ssim': 0.7440193721226284, 'train/loss': 0.2725036655153547, 'validation/ssim': 0.7218023073693374, 'validation/loss': 0.2883543273204488, 'validation/num_examples': 3554, 'test/ssim': 0.7392959914304663, 'test/loss': 0.28983016920901983, 'test/num_examples': 3581, 'score': 1258.061113834381, 'total_duration': 1516.0060744285583, 'accumulated_submission_time': 1258.061113834381, 'accumulated_eval_time': 228.8656632900238, 'accumulated_logging_time': 0.3210608959197998, 'global_step': 16005, 'preemption_count': 0}), (17261, {'train/ssim': 0.7447244099208287, 'train/loss': 0.27191308566502165, 'validation/ssim': 0.722501000193444, 'validation/loss': 0.2878226310802969, 'validation/num_examples': 3554, 'test/ssim': 0.7399323523893465, 'test/loss': 0.28932382114370986, 'test/num_examples': 3581, 'score': 1336.8884885311127, 'total_duration': 1603.2345094680786, 'accumulated_submission_time': 1336.8884885311127, 'accumulated_eval_time': 235.3726086616516, 'accumulated_logging_time': 0.3421640396118164, 'global_step': 17261, 'preemption_count': 0}), (18515, {'train/ssim': 0.7446695736476353, 'train/loss': 0.27191478865487234, 'validation/ssim': 0.7223735030071751, 'validation/loss': 0.2878624739510059, 'validation/num_examples': 3554, 'test/ssim': 0.7398161793580704, 'test/loss': 0.2893651021122766, 'test/num_examples': 3581, 'score': 1415.7927322387695, 'total_duration': 1690.2354412078857, 'accumulated_submission_time': 1415.7927322387695, 'accumulated_eval_time': 241.6549108028412, 'accumulated_logging_time': 0.3613853454589844, 'global_step': 18515, 'preemption_count': 0}), (19771, {'train/ssim': 0.7445036343165806, 'train/loss': 0.27186196190970285, 'validation/ssim': 0.7223541998232625, 'validation/loss': 0.2877795423895171, 'validation/num_examples': 3554, 'test/ssim': 0.7398193836611631, 'test/loss': 0.2892660755113795, 'test/num_examples': 3581, 'score': 1494.6639420986176, 'total_duration': 1777.1373591423035, 'accumulated_submission_time': 1494.6639420986176, 'accumulated_eval_time': 247.82817602157593, 'accumulated_logging_time': 0.3811984062194824, 'global_step': 19771, 'preemption_count': 0}), (21026, {'train/ssim': 0.7441000938415527, 'train/loss': 0.2721811192376273, 'validation/ssim': 0.7219677926719893, 'validation/loss': 0.2880178267994689, 'validation/num_examples': 3554, 'test/ssim': 0.7393942339997557, 'test/loss': 0.2895158747992879, 'test/num_examples': 3581, 'score': 1573.4895980358124, 'total_duration': 1864.0645184516907, 'accumulated_submission_time': 1573.4895980358124, 'accumulated_eval_time': 254.0806529521942, 'accumulated_logging_time': 0.4006764888763428, 'global_step': 21026, 'preemption_count': 0}), (22282, {'train/ssim': 0.7433674676077706, 'train/loss': 0.2728325298854283, 'validation/ssim': 0.7210479032208427, 'validation/loss': 0.28870679933701465, 'validation/num_examples': 3554, 'test/ssim': 0.7385365715974938, 'test/loss': 0.290172722844963, 'test/num_examples': 3581, 'score': 1652.3817439079285, 'total_duration': 1951.0072338581085, 'accumulated_submission_time': 1652.3817439079285, 'accumulated_eval_time': 260.3125813007355, 'accumulated_logging_time': 0.4203329086303711, 'global_step': 22282, 'preemption_count': 0}), (23537, {'train/ssim': 0.743966920035226, 'train/loss': 0.27261509214128765, 'validation/ssim': 0.7218409137371623, 'validation/loss': 0.2883646658584517, 'validation/num_examples': 3554, 'test/ssim': 0.7393314432944709, 'test/loss': 0.2898451680745602, 'test/num_examples': 3581, 'score': 1731.2403120994568, 'total_duration': 2038.015981912613, 'accumulated_submission_time': 1731.2403120994568, 'accumulated_eval_time': 266.5254509449005, 'accumulated_logging_time': 0.43962788581848145, 'global_step': 23537, 'preemption_count': 0}), (24794, {'train/ssim': 0.7427633830479213, 'train/loss': 0.2737788302557809, 'validation/ssim': 0.720846490639948, 'validation/loss': 0.2895091523195695, 'validation/num_examples': 3554, 'test/ssim': 0.7384386017348507, 'test/loss': 0.29087170406747415, 'test/num_examples': 3581, 'score': 1810.0445396900177, 'total_duration': 2124.917290210724, 'accumulated_submission_time': 1810.0445396900177, 'accumulated_eval_time': 272.68181681632996, 'accumulated_logging_time': 0.45932912826538086, 'global_step': 24794, 'preemption_count': 0}), (26049, {'train/ssim': 0.7435014588492257, 'train/loss': 0.2732975482940674, 'validation/ssim': 0.7215003945818093, 'validation/loss': 0.2890255079553848, 'validation/num_examples': 3554, 'test/ssim': 0.7389908326933817, 'test/loss': 0.29043803232337334, 'test/num_examples': 3581, 'score': 1888.8450696468353, 'total_duration': 2211.810420036316, 'accumulated_submission_time': 1888.8450696468353, 'accumulated_eval_time': 278.9067015647888, 'accumulated_logging_time': 0.47882509231567383, 'global_step': 26049, 'preemption_count': 0}), (27305, {'train/ssim': 0.7441748891557965, 'train/loss': 0.27299850327628, 'validation/ssim': 0.7218036125668261, 'validation/loss': 0.2890189132733364, 'validation/num_examples': 3554, 'test/ssim': 0.7391586836341106, 'test/loss': 0.29056719300867423, 'test/num_examples': 3581, 'score': 1967.7468943595886, 'total_duration': 2298.814632177353, 'accumulated_submission_time': 1967.7468943595886, 'accumulated_eval_time': 285.1467297077179, 'accumulated_logging_time': 0.4989640712738037, 'global_step': 27305, 'preemption_count': 0}), (28563, {'train/ssim': 0.7423482622419085, 'train/loss': 0.27453681400844027, 'validation/ssim': 0.7201113896753658, 'validation/loss': 0.2904934086152926, 'validation/num_examples': 3554, 'test/ssim': 0.7375633497539095, 'test/loss': 0.2918937063123953, 'test/num_examples': 3581, 'score': 2046.5627608299255, 'total_duration': 2385.7315006256104, 'accumulated_submission_time': 2046.5627608299255, 'accumulated_eval_time': 291.3618280887604, 'accumulated_logging_time': 0.5184464454650879, 'global_step': 28563, 'preemption_count': 0}), (29822, {'train/ssim': 0.7403538567679269, 'train/loss': 0.275881256375994, 'validation/ssim': 0.7177984423361001, 'validation/loss': 0.2920175014618212, 'validation/num_examples': 3554, 'test/ssim': 0.7352285036259075, 'test/loss': 0.29359437313468656, 'test/num_examples': 3581, 'score': 2125.4383203983307, 'total_duration': 2472.5783755779266, 'accumulated_submission_time': 2125.4383203983307, 'accumulated_eval_time': 297.5554790496826, 'accumulated_logging_time': 0.5380234718322754, 'global_step': 29822, 'preemption_count': 0}), (31080, {'train/ssim': 0.7451820373535156, 'train/loss': 0.27517989703587126, 'validation/ssim': 0.7233183972944218, 'validation/loss': 0.29080517907859105, 'validation/num_examples': 3554, 'test/ssim': 0.7403774778125872, 'test/loss': 0.29264531991762077, 'test/num_examples': 3581, 'score': 2204.319138288498, 'total_duration': 2559.593016386032, 'accumulated_submission_time': 2204.319138288498, 'accumulated_eval_time': 303.85582995414734, 'accumulated_logging_time': 0.5590479373931885, 'global_step': 31080, 'preemption_count': 0}), (32339, {'train/ssim': 0.7411383220127651, 'train/loss': 0.27474245003291536, 'validation/ssim': 0.718848508063098, 'validation/loss': 0.29066651901906304, 'validation/num_examples': 3554, 'test/ssim': 0.7363913929419157, 'test/loss': 0.2922037396807805, 'test/num_examples': 3581, 'score': 2283.1173145771027, 'total_duration': 2646.658350944519, 'accumulated_submission_time': 2283.1173145771027, 'accumulated_eval_time': 310.2677991390228, 'accumulated_logging_time': 0.5792012214660645, 'global_step': 32339, 'preemption_count': 0}), (33600, {'train/ssim': 0.7411050796508789, 'train/loss': 0.2742088351930891, 'validation/ssim': 0.7188852596765968, 'validation/loss': 0.28989212474060916, 'validation/num_examples': 3554, 'test/ssim': 0.736381098266022, 'test/loss': 0.29146132992486384, 'test/num_examples': 3581, 'score': 2361.9817926883698, 'total_duration': 2733.6724212169647, 'accumulated_submission_time': 2361.9817926883698, 'accumulated_eval_time': 316.6288607120514, 'accumulated_logging_time': 0.5992915630340576, 'global_step': 33600, 'preemption_count': 0}), (34860, {'train/ssim': 0.74284485408238, 'train/loss': 0.2736622095108032, 'validation/ssim': 0.7207758038917417, 'validation/loss': 0.28963245913495356, 'validation/num_examples': 3554, 'test/ssim': 0.7381798713042795, 'test/loss': 0.2911903617835451, 'test/num_examples': 3581, 'score': 2440.8483040332794, 'total_duration': 2820.5341522693634, 'accumulated_submission_time': 2440.8483040332794, 'accumulated_eval_time': 322.749392747879, 'accumulated_logging_time': 0.6201107501983643, 'global_step': 34860, 'preemption_count': 0}), (36116, {'train/ssim': 0.7414957455226353, 'train/loss': 0.27564564773014616, 'validation/ssim': 0.7189575264007104, 'validation/loss': 0.2916531796259496, 'validation/num_examples': 3554, 'test/ssim': 0.736527269028379, 'test/loss': 0.29303679030822394, 'test/num_examples': 3581, 'score': 2519.6292798519135, 'total_duration': 2907.8767132759094, 'accumulated_submission_time': 2519.6292798519135, 'accumulated_eval_time': 329.38674092292786, 'accumulated_logging_time': 0.6433703899383545, 'global_step': 36116, 'preemption_count': 0}), (37377, {'train/ssim': 0.7422714233398438, 'train/loss': 0.2742816720690046, 'validation/ssim': 0.7197518421145188, 'validation/loss': 0.2903473295384602, 'validation/num_examples': 3554, 'test/ssim': 0.7371814922725844, 'test/loss': 0.29188330937150936, 'test/num_examples': 3581, 'score': 2598.540436267853, 'total_duration': 2995.0334854125977, 'accumulated_submission_time': 2598.540436267853, 'accumulated_eval_time': 335.74242997169495, 'accumulated_logging_time': 0.6638855934143066, 'global_step': 37377, 'preemption_count': 0}), (38635, {'train/ssim': 0.7446798597063337, 'train/loss': 0.2733526570456369, 'validation/ssim': 0.7226071333576604, 'validation/loss': 0.2892033926341798, 'validation/num_examples': 3554, 'test/ssim': 0.7398272921539025, 'test/loss': 0.2908303890105767, 'test/num_examples': 3581, 'score': 2677.3347611427307, 'total_duration': 3082.0659799575806, 'accumulated_submission_time': 2677.3347611427307, 'accumulated_eval_time': 342.05778336524963, 'accumulated_logging_time': 0.6843328475952148, 'global_step': 38635, 'preemption_count': 0}), (39897, {'train/ssim': 0.7435912404741559, 'train/loss': 0.2731384038925171, 'validation/ssim': 0.7217312771481078, 'validation/loss': 0.288757633344471, 'validation/num_examples': 3554, 'test/ssim': 0.7391068011946733, 'test/loss': 0.29031026925963416, 'test/num_examples': 3581, 'score': 2756.181461572647, 'total_duration': 3169.133134365082, 'accumulated_submission_time': 2756.181461572647, 'accumulated_eval_time': 348.38329219818115, 'accumulated_logging_time': 0.7059872150421143, 'global_step': 39897, 'preemption_count': 0}), (41157, {'train/ssim': 0.7411667960030692, 'train/loss': 0.2745837824685233, 'validation/ssim': 0.7188650634628235, 'validation/loss': 0.29048598959798816, 'validation/num_examples': 3554, 'test/ssim': 0.7359778333129713, 'test/loss': 0.2921823322090547, 'test/num_examples': 3581, 'score': 2835.008713245392, 'total_duration': 3256.2639620304108, 'accumulated_submission_time': 2835.008713245392, 'accumulated_eval_time': 354.78237223625183, 'accumulated_logging_time': 0.7274649143218994, 'global_step': 41157, 'preemption_count': 0}), (42417, {'train/ssim': 0.7425377028329032, 'train/loss': 0.2732114621571132, 'validation/ssim': 0.7201099470886677, 'validation/loss': 0.289322921246307, 'validation/num_examples': 3554, 'test/ssim': 0.7376958170072955, 'test/loss': 0.2907759499463313, 'test/num_examples': 3581, 'score': 2913.799752473831, 'total_duration': 3343.2511398792267, 'accumulated_submission_time': 2913.799752473831, 'accumulated_eval_time': 361.10269474983215, 'accumulated_logging_time': 0.7485363483428955, 'global_step': 42417, 'preemption_count': 0}), (43681, {'train/ssim': 0.7433334759303502, 'train/loss': 0.27305974279131207, 'validation/ssim': 0.7203966096739589, 'validation/loss': 0.28945502097108894, 'validation/num_examples': 3554, 'test/ssim': 0.7379518203714046, 'test/loss': 0.29092280247530367, 'test/num_examples': 3581, 'score': 2992.668555498123, 'total_duration': 3430.2541375160217, 'accumulated_submission_time': 2992.668555498123, 'accumulated_eval_time': 367.4296371936798, 'accumulated_logging_time': 0.7704877853393555, 'global_step': 43681, 'preemption_count': 0}), (44942, {'train/ssim': 0.7441113335745675, 'train/loss': 0.27217819009508404, 'validation/ssim': 0.7217832102692389, 'validation/loss': 0.2881997644599395, 'validation/num_examples': 3554, 'test/ssim': 0.7391518659679559, 'test/loss': 0.28976690126710414, 'test/num_examples': 3581, 'score': 3071.5357432365417, 'total_duration': 3517.3099768161774, 'accumulated_submission_time': 3071.5357432365417, 'accumulated_eval_time': 373.80280804634094, 'accumulated_logging_time': 0.7916111946105957, 'global_step': 44942, 'preemption_count': 0}), (46201, {'train/ssim': 0.7446784973144531, 'train/loss': 0.2719996656690325, 'validation/ssim': 0.7225244937482415, 'validation/loss': 0.2879621154750809, 'validation/num_examples': 3554, 'test/ssim': 0.739901127478358, 'test/loss': 0.28942669972598434, 'test/num_examples': 3581, 'score': 3150.4060542583466, 'total_duration': 3604.42573261261, 'accumulated_submission_time': 3150.4060542583466, 'accumulated_eval_time': 380.2132203578949, 'accumulated_logging_time': 0.8131155967712402, 'global_step': 46201, 'preemption_count': 0}), (47462, {'train/ssim': 0.7452500888279506, 'train/loss': 0.2719661167689732, 'validation/ssim': 0.7226425797736705, 'validation/loss': 0.2882189302546427, 'validation/num_examples': 3554, 'test/ssim': 0.740064001522794, 'test/loss': 0.28965284171233596, 'test/num_examples': 3581, 'score': 3229.265285730362, 'total_duration': 3691.4618928432465, 'accumulated_submission_time': 3229.265285730362, 'accumulated_eval_time': 386.5372929573059, 'accumulated_logging_time': 0.8343875408172607, 'global_step': 47462, 'preemption_count': 0}), (48723, {'train/ssim': 0.7448912348066058, 'train/loss': 0.27262304510389057, 'validation/ssim': 0.722528546729917, 'validation/loss': 0.28874289835176914, 'validation/num_examples': 3554, 'test/ssim': 0.7398832651930327, 'test/loss': 0.29020043665788187, 'test/num_examples': 3581, 'score': 3308.1266119480133, 'total_duration': 3778.4676311016083, 'accumulated_submission_time': 3308.1266119480133, 'accumulated_eval_time': 392.832781791687, 'accumulated_logging_time': 0.856410026550293, 'global_step': 48723, 'preemption_count': 0}), (49987, {'train/ssim': 0.7459492002214704, 'train/loss': 0.2708491597856794, 'validation/ssim': 0.7232905759795301, 'validation/loss': 0.28727645745221053, 'validation/num_examples': 3554, 'test/ssim': 0.7406545477651145, 'test/loss': 0.28873409302132785, 'test/num_examples': 3581, 'score': 3386.9748871326447, 'total_duration': 3865.610069513321, 'accumulated_submission_time': 3386.9748871326447, 'accumulated_eval_time': 399.2502522468567, 'accumulated_logging_time': 0.8782632350921631, 'global_step': 49987, 'preemption_count': 0}), (51247, {'train/ssim': 0.7458992004394531, 'train/loss': 0.270669903073992, 'validation/ssim': 0.7232503896357977, 'validation/loss': 0.28706898257245356, 'validation/num_examples': 3554, 'test/ssim': 0.7406816820764103, 'test/loss': 0.28848596406162735, 'test/num_examples': 3581, 'score': 3465.8320820331573, 'total_duration': 3952.716889858246, 'accumulated_submission_time': 3465.8320820331573, 'accumulated_eval_time': 405.64476919174194, 'accumulated_logging_time': 0.9005115032196045, 'global_step': 51247, 'preemption_count': 0}), (52497, {'train/ssim': 0.7465798514229911, 'train/loss': 0.27028936999184744, 'validation/ssim': 0.7240425758168613, 'validation/loss': 0.2866542388967537, 'validation/num_examples': 3554, 'test/ssim': 0.7413828108637601, 'test/loss': 0.2881275934402053, 'test/num_examples': 3581, 'score': 3544.689062833786, 'total_duration': 4039.652656555176, 'accumulated_submission_time': 3544.689062833786, 'accumulated_eval_time': 411.8609836101532, 'accumulated_logging_time': 0.9216856956481934, 'global_step': 52497, 'preemption_count': 0})], 'global_step': 52497}
I0315 20:05:33.700570 140146293318848 submission_runner.py:649] Timing: 3544.689062833786
I0315 20:05:33.700621 140146293318848 submission_runner.py:651] Total number of evals: 45
I0315 20:05:33.700657 140146293318848 submission_runner.py:652] ====================
I0315 20:05:33.700804 140146293318848 submission_runner.py:750] Final fastmri score: 0
