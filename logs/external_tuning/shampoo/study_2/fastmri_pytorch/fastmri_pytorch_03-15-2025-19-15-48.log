torchrun --redirects 1:0,2:0,3:0,4:0,5:0,6:0,7:0 --standalone --nnodes=1 --nproc_per_node=8 submission_runner.py --framework=pytorch --workload=fastmri --submission_path=submissions_algorithms/leaderboard/external_tuning/shampoo_submission/submission.py --data_dir=/data/fastmri --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=submissions/rolling_leaderboard/external_tuning/shampoo/study_2 --overwrite=True --save_checkpoints=False --rng_seed=-1809214569 --torch_compile=true --tuning_ruleset=external --tuning_search_space=submissions_algorithms/leaderboard/external_tuning/shampoo_submission/tuning_search_space.json --num_tuning_trials=5 --hparam_start_index=4 --hparam_end_index=5 2>&1 | tee -a /logs/fastmri_pytorch_03-15-2025-19-15-48.log
W0315 19:16:09.128000 9 site-packages/torch/distributed/run.py:793] 
W0315 19:16:09.128000 9 site-packages/torch/distributed/run.py:793] *****************************************
W0315 19:16:09.128000 9 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0315 19:16:09.128000 9 site-packages/torch/distributed/run.py:793] *****************************************
2025-03-15 19:16:22.988266: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-03-15 19:16:22.988299: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-03-15 19:16:22.988347: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-03-15 19:16:22.988390: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-03-15 19:16:22.988407: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-03-15 19:16:22.988514: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-03-15 19:16:22.988536: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-03-15 19:16:22.988576: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1742066183.660350      48 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1742066183.660311      51 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1742066183.660272      47 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1742066183.660344      49 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1742066183.660281      50 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1742066183.660313      45 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1742066183.660339      44 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1742066183.660207      46 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1742066183.864455      49 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
E0000 00:00:1742066183.864463      44 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
E0000 00:00:1742066183.864484      47 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
E0000 00:00:1742066183.864487      50 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
E0000 00:00:1742066183.864487      45 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
E0000 00:00:1742066183.864489      46 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
E0000 00:00:1742066183.864490      51 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
E0000 00:00:1742066183.864520      48 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
[rank2]:[W315 19:17:07.862467878 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank3]:[W315 19:17:07.862520986 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank7]:[W315 19:17:07.862547013 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 7]  using GPU 7 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank4]:[W315 19:17:07.862558568 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 4]  using GPU 4 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank6]:[W315 19:17:07.862567059 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 6]  using GPU 6 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank1]:[W315 19:17:07.862581561 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank5]:[W315 19:17:07.862705537 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 5]  using GPU 5 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank0]:[W315 19:17:08.994517721 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
I0315 19:17:10.439342 140286885487808 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_2/fastmri_pytorch.
I0315 19:17:10.439343 140066777728192 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_2/fastmri_pytorch.
I0315 19:17:10.439344 139802597225664 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_2/fastmri_pytorch.
I0315 19:17:10.439342 140198727365824 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_2/fastmri_pytorch.
I0315 19:17:10.439341 139716304848064 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_2/fastmri_pytorch.
I0315 19:17:10.439343 140282512307392 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_2/fastmri_pytorch.
I0315 19:17:10.439342 140571517158592 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_2/fastmri_pytorch.
I0315 19:17:10.439427 139647719654592 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_2/fastmri_pytorch.
I0315 19:17:12.206470 140066777728192 submission_runner.py:606] Using RNG seed -1809214569
I0315 19:17:12.207346 139647719654592 logger_utils.py:97] Saving hparams to /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_2/fastmri_pytorch/trial_5/hparams.json.
I0315 19:17:12.207331 140198727365824 logger_utils.py:97] Saving hparams to /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_2/fastmri_pytorch/trial_5/hparams.json.
I0315 19:17:12.207353 139802597225664 logger_utils.py:97] Saving hparams to /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_2/fastmri_pytorch/trial_5/hparams.json.
I0315 19:17:12.207333 139716304848064 logger_utils.py:97] Saving hparams to /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_2/fastmri_pytorch/trial_5/hparams.json.
I0315 19:17:12.207344 140282512307392 logger_utils.py:97] Saving hparams to /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_2/fastmri_pytorch/trial_5/hparams.json.
I0315 19:17:12.207360 140571517158592 logger_utils.py:97] Saving hparams to /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_2/fastmri_pytorch/trial_5/hparams.json.
I0315 19:17:12.207696 140066777728192 submission_runner.py:615] --- Tuning run 5/5 ---
I0315 19:17:12.207818 140066777728192 submission_runner.py:620] Creating tuning directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_2/fastmri_pytorch/trial_5.
I0315 19:17:12.207699 140286885487808 logger_utils.py:97] Saving hparams to /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_2/fastmri_pytorch/trial_5/hparams.json.
I0315 19:17:12.208075 140066777728192 logger_utils.py:97] Saving hparams to /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_2/fastmri_pytorch/trial_5/hparams.json.
I0315 19:17:12.549941 140066777728192 submission_runner.py:218] Initializing dataset.
I0315 19:17:12.550122 140066777728192 submission_runner.py:229] Initializing model.
I0315 19:17:13.671999 140066777728192 submission_runner.py:268] Performing `torch.compile`.
I0315 19:17:15.936568 140066777728192 submission_runner.py:272] Initializing optimizer.
W0315 19:17:16.004003 139716304848064 distributed_shampoo.py:341] start_preconditioning_step set to -1. Setting start_preconditioning_step equal to precondition frequency 100 by default.
W0315 19:17:16.004050 140066777728192 distributed_shampoo.py:341] start_preconditioning_step set to -1. Setting start_preconditioning_step equal to precondition frequency 100 by default.
W0315 19:17:16.004032 139802597225664 distributed_shampoo.py:341] start_preconditioning_step set to -1. Setting start_preconditioning_step equal to precondition frequency 100 by default.
W0315 19:17:16.004042 140286885487808 distributed_shampoo.py:341] start_preconditioning_step set to -1. Setting start_preconditioning_step equal to precondition frequency 100 by default.
W0315 19:17:16.004054 139647719654592 distributed_shampoo.py:341] start_preconditioning_step set to -1. Setting start_preconditioning_step equal to precondition frequency 100 by default.
W0315 19:17:16.004180 140066777728192 distributed_shampoo.py:359] Nesterov flag is enabled but momentum parameter is zero! Continuing without using momentum or Nesterov acceleration...
W0315 19:17:16.004064 140571517158592 distributed_shampoo.py:341] start_preconditioning_step set to -1. Setting start_preconditioning_step equal to precondition frequency 100 by default.
W0315 19:17:16.004200 139716304848064 distributed_shampoo.py:359] Nesterov flag is enabled but momentum parameter is zero! Continuing without using momentum or Nesterov acceleration...
W0315 19:17:16.004205 139802597225664 distributed_shampoo.py:359] Nesterov flag is enabled but momentum parameter is zero! Continuing without using momentum or Nesterov acceleration...
W0315 19:17:16.004087 140198727365824 distributed_shampoo.py:341] start_preconditioning_step set to -1. Setting start_preconditioning_step equal to precondition frequency 100 by default.
W0315 19:17:16.004102 140282512307392 distributed_shampoo.py:341] start_preconditioning_step set to -1. Setting start_preconditioning_step equal to precondition frequency 100 by default.
W0315 19:17:16.004223 140286885487808 distributed_shampoo.py:359] Nesterov flag is enabled but momentum parameter is zero! Continuing without using momentum or Nesterov acceleration...
W0315 19:17:16.004226 139647719654592 distributed_shampoo.py:359] Nesterov flag is enabled but momentum parameter is zero! Continuing without using momentum or Nesterov acceleration...
W0315 19:17:16.004243 140571517158592 distributed_shampoo.py:359] Nesterov flag is enabled but momentum parameter is zero! Continuing without using momentum or Nesterov acceleration...
W0315 19:17:16.004273 140198727365824 distributed_shampoo.py:359] Nesterov flag is enabled but momentum parameter is zero! Continuing without using momentum or Nesterov acceleration...
W0315 19:17:16.004282 140282512307392 distributed_shampoo.py:359] Nesterov flag is enabled but momentum parameter is zero! Continuing without using momentum or Nesterov acceleration...
I0315 19:17:16.114186 139647719654592 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 0.block_0 ([torch.Size([0])]) for Parameter 0 (torch.Size([32, 1, 3, 3])), Block block_0 (torch.Size([288])).
I0315 19:17:16.114167 139716304848064 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 0.block_0 ([torch.Size([0])]) for Parameter 0 (torch.Size([32, 1, 3, 3])), Block block_0 (torch.Size([288])).
I0315 19:17:16.114192 140198727365824 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 0.block_0 ([torch.Size([0])]) for Parameter 0 (torch.Size([32, 1, 3, 3])), Block block_0 (torch.Size([288])).
I0315 19:17:16.114194 140066777728192 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 0.block_0 ([torch.Size([0])]) for Parameter 0 (torch.Size([32, 1, 3, 3])), Block block_0 (torch.Size([288])).
I0315 19:17:16.114096 140286885487808 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 0.block_0 ([torch.Size([0])]) for Parameter 0 (torch.Size([32, 1, 3, 3])), Block block_0 (torch.Size([288])).
I0315 19:17:16.114211 139802597225664 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 0.block_0 ([torch.Size([0])]) for Parameter 0 (torch.Size([32, 1, 3, 3])), Block block_0 (torch.Size([288])).
I0315 19:17:16.114236 140282512307392 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 0.block_0 ([torch.Size([0])]) for Parameter 0 (torch.Size([32, 1, 3, 3])), Block block_0 (torch.Size([288])).
I0315 19:17:16.114298 140571517158592 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 0.block_0 ([torch.Size([288, 288])]) for Parameter 0 (torch.Size([32, 1, 3, 3])), Block block_0 (torch.Size([288])).
I0315 19:17:16.117113 140066777728192 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 1.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 1 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 19:17:16.117112 139647719654592 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 1.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 1 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 19:17:16.117125 140286885487808 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 1.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 1 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 19:17:16.117117 139716304848064 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 1.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 1 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 19:17:16.117145 139802597225664 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 1.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 1 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 19:17:16.117181 140282512307392 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 1.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 1 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 19:17:16.117244 140571517158592 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 1.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 1 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 19:17:16.117300 139647719654592 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 2.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 2 (torch.Size([64, 32, 3, 3])), Block block_0 (torch.Size([64, 288])).
I0315 19:17:16.117309 140066777728192 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 2.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 2 (torch.Size([64, 32, 3, 3])), Block block_0 (torch.Size([64, 288])).
I0315 19:17:16.117310 139716304848064 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 2.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 2 (torch.Size([64, 32, 3, 3])), Block block_0 (torch.Size([64, 288])).
I0315 19:17:16.117270 140198727365824 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 1.block_0 ([torch.Size([1024, 1024]), torch.Size([9, 9])]) for Parameter 1 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 19:17:16.117332 139802597225664 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 2.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 2 (torch.Size([64, 32, 3, 3])), Block block_0 (torch.Size([64, 288])).
I0315 19:17:16.117401 140282512307392 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 2.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 2 (torch.Size([64, 32, 3, 3])), Block block_0 (torch.Size([64, 288])).
I0315 19:17:16.117468 140066777728192 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 3.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 3 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 19:17:16.117468 140571517158592 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 2.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 2 (torch.Size([64, 32, 3, 3])), Block block_0 (torch.Size([64, 288])).
I0315 19:17:16.117470 140286885487808 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 2.block_0 ([torch.Size([64, 64]), torch.Size([288, 288])]) for Parameter 2 (torch.Size([64, 32, 3, 3])), Block block_0 (torch.Size([64, 288])).
I0315 19:17:16.117495 139716304848064 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 3.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 3 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 19:17:16.117516 140198727365824 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 2.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 2 (torch.Size([64, 32, 3, 3])), Block block_0 (torch.Size([64, 288])).
I0315 19:17:16.117545 139802597225664 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 3.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 3 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 19:17:16.117558 140282512307392 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 3.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 3 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 19:17:16.117618 140066777728192 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 4.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 4 (torch.Size([128, 64, 3, 3])), Block block_0 (torch.Size([128, 576])).
I0315 19:17:16.117637 140286885487808 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 3.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 3 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 19:17:16.117657 139716304848064 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 4.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 4 (torch.Size([128, 64, 3, 3])), Block block_0 (torch.Size([128, 576])).
I0315 19:17:16.117638 140571517158592 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 3.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 3 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 19:17:16.117718 140198727365824 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 3.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 3 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 19:17:16.117707 139647719654592 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 3.block_0 ([torch.Size([64, 64]), torch.Size([576, 576])]) for Parameter 3 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 19:17:16.117738 139802597225664 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 4.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 4 (torch.Size([128, 64, 3, 3])), Block block_0 (torch.Size([128, 576])).
I0315 19:17:16.117752 140282512307392 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 4.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 4 (torch.Size([128, 64, 3, 3])), Block block_0 (torch.Size([128, 576])).
I0315 19:17:16.117822 140571517158592 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 4.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 4 (torch.Size([128, 64, 3, 3])), Block block_0 (torch.Size([128, 576])).
I0315 19:17:16.117823 140286885487808 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 4.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 4 (torch.Size([128, 64, 3, 3])), Block block_0 (torch.Size([128, 576])).
I0315 19:17:16.117834 140066777728192 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 5.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 5 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 19:17:16.117851 139716304848064 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 5.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 5 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 19:17:16.117901 139802597225664 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 5.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 5 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 19:17:16.117902 139647719654592 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 4.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 4 (torch.Size([128, 64, 3, 3])), Block block_0 (torch.Size([128, 576])).
I0315 19:17:16.117915 140282512307392 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 5.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 5 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 19:17:16.117971 140286885487808 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 5.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 5 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 19:17:16.117990 140066777728192 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 6.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 6 (torch.Size([256, 128, 3, 3])), Block block_0 (torch.Size([256, 384, 3])).
I0315 19:17:16.117992 140198727365824 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 4.block_0 ([torch.Size([128, 128]), torch.Size([576, 576])]) for Parameter 4 (torch.Size([128, 64, 3, 3])), Block block_0 (torch.Size([128, 576])).
I0315 19:17:16.118015 139716304848064 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 6.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 6 (torch.Size([256, 128, 3, 3])), Block block_0 (torch.Size([256, 384, 3])).
I0315 19:17:16.118067 139802597225664 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 6.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 6 (torch.Size([256, 128, 3, 3])), Block block_0 (torch.Size([256, 384, 3])).
I0315 19:17:16.118066 139647719654592 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 5.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 5 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 19:17:16.118069 140282512307392 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 6.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 6 (torch.Size([256, 128, 3, 3])), Block block_0 (torch.Size([256, 384, 3])).
I0315 19:17:16.118124 140286885487808 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 6.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 6 (torch.Size([256, 128, 3, 3])), Block block_0 (torch.Size([256, 384, 3])).
I0315 19:17:16.118154 140066777728192 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 7.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 7 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 19:17:16.118179 139716304848064 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 7.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 7 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 19:17:16.118196 140198727365824 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 5.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 5 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 19:17:16.118231 140282512307392 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 7.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 7 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 19:17:16.118232 139647719654592 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 6.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 6 (torch.Size([256, 128, 3, 3])), Block block_0 (torch.Size([256, 384, 3])).
I0315 19:17:16.118241 139802597225664 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 7.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 7 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 19:17:16.118317 140066777728192 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 8.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 8 (torch.Size([512, 256, 3, 3])), Block block_0 (torch.Size([512, 768, 3])).
I0315 19:17:16.118357 140198727365824 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 6.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 6 (torch.Size([256, 128, 3, 3])), Block block_0 (torch.Size([256, 384, 3])).
I0315 19:17:16.118353 139716304848064 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 8.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 8 (torch.Size([512, 256, 3, 3])), Block block_0 (torch.Size([512, 768, 3])).
I0315 19:17:16.118394 139647719654592 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 7.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 7 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 19:17:16.118396 140282512307392 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 8.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 8 (torch.Size([512, 256, 3, 3])), Block block_0 (torch.Size([512, 768, 3])).
I0315 19:17:16.118508 139716304848064 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 9.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 9 (torch.Size([512, 512, 3, 3])), Block block_0 (torch.Size([512, 512, 9])).
I0315 19:17:16.118527 140198727365824 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 7.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 7 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 19:17:16.118546 140282512307392 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 9.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 9 (torch.Size([512, 512, 3, 3])), Block block_0 (torch.Size([512, 512, 9])).
I0315 19:17:16.118564 139647719654592 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 8.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 8 (torch.Size([512, 256, 3, 3])), Block block_0 (torch.Size([512, 768, 3])).
I0315 19:17:16.118682 139716304848064 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 10.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 10 (torch.Size([256, 512, 3, 3])), Block block_0 (torch.Size([256, 512, 9])).
I0315 19:17:16.118691 140198727365824 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 8.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 8 (torch.Size([512, 256, 3, 3])), Block block_0 (torch.Size([512, 768, 3])).
I0315 19:17:16.118719 139647719654592 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 9.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 9 (torch.Size([512, 512, 3, 3])), Block block_0 (torch.Size([512, 512, 9])).
I0315 19:17:16.118844 140198727365824 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 9.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 9 (torch.Size([512, 512, 3, 3])), Block block_0 (torch.Size([512, 512, 9])).
I0315 19:17:16.118873 139647719654592 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 10.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 10 (torch.Size([256, 512, 3, 3])), Block block_0 (torch.Size([256, 512, 9])).
I0315 19:17:16.118871 140571517158592 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 5.block_0 ([torch.Size([128, 128]), torch.Size([384, 384]), torch.Size([3, 3])]) for Parameter 5 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 19:17:16.119007 140198727365824 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 10.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 10 (torch.Size([256, 512, 3, 3])), Block block_0 (torch.Size([256, 512, 9])).
I0315 19:17:16.119028 139647719654592 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 11.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 11 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 19:17:16.119022 139716304848064 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 11.block_0 ([torch.Size([256, 256]), torch.Size([768, 768]), torch.Size([3, 3])]) for Parameter 11 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 19:17:16.119167 140198727365824 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 11.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 11 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 19:17:16.119186 139647719654592 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 12.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 12 (torch.Size([128, 256, 3, 3])), Block block_0 (torch.Size([128, 768, 3])).
I0315 19:17:16.119196 139716304848064 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 12.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 12 (torch.Size([128, 256, 3, 3])), Block block_0 (torch.Size([128, 768, 3])).
I0315 19:17:16.119336 139647719654592 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 13.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 13 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 19:17:16.119355 139716304848064 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 13.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 13 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 19:17:16.119494 139647719654592 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 14.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 14 (torch.Size([64, 128, 3, 3])), Block block_0 (torch.Size([64, 384, 3])).
I0315 19:17:16.119506 139716304848064 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 14.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 14 (torch.Size([64, 128, 3, 3])), Block block_0 (torch.Size([64, 384, 3])).
I0315 19:17:16.119588 140286885487808 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 7.block_0 ([torch.Size([256, 256]), torch.Size([768, 768]), torch.Size([3, 3])]) for Parameter 7 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 19:17:16.119651 139716304848064 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 15.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 15 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 19:17:16.119760 139647719654592 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 15.block_0 ([torch.Size([64, 64]), torch.Size([576, 576])]) for Parameter 15 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 19:17:16.119780 139802597225664 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 8.block_0 ([torch.Size([512, 512]), torch.Size([768, 768]), torch.Size([3, 3])]) for Parameter 8 (torch.Size([512, 256, 3, 3])), Block block_0 (torch.Size([512, 768, 3])).
I0315 19:17:16.119835 140286885487808 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 8.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 8 (torch.Size([512, 256, 3, 3])), Block block_0 (torch.Size([512, 768, 3])).
I0315 19:17:16.119878 139716304848064 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 16.block_0 ([torch.Size([32, 32]), torch.Size([576, 576])]) for Parameter 16 (torch.Size([32, 64, 3, 3])), Block block_0 (torch.Size([32, 576])).
I0315 19:17:16.119911 139647719654592 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 16.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 16 (torch.Size([32, 64, 3, 3])), Block block_0 (torch.Size([32, 576])).
I0315 19:17:16.120003 140286885487808 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 9.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 9 (torch.Size([512, 512, 3, 3])), Block block_0 (torch.Size([512, 512, 9])).
I0315 19:17:16.120013 139802597225664 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 9.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 9 (torch.Size([512, 512, 3, 3])), Block block_0 (torch.Size([512, 512, 9])).
I0315 19:17:16.120031 139716304848064 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 17.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 17 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 19:17:16.120130 139716304848064 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 18.block_0 ([torch.Size([0])]) for Parameter 18 (torch.Size([1, 32, 1, 1])), Block block_0 (torch.Size([32])).
I0315 19:17:16.120124 140282512307392 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 10.block_0 ([torch.Size([256, 256]), torch.Size([512, 512]), torch.Size([9, 9])]) for Parameter 10 (torch.Size([256, 512, 3, 3])), Block block_0 (torch.Size([256, 512, 9])).
I0315 19:17:16.120167 139802597225664 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 10.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 10 (torch.Size([256, 512, 3, 3])), Block block_0 (torch.Size([256, 512, 9])).
I0315 19:17:16.120177 140286885487808 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 10.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 10 (torch.Size([256, 512, 3, 3])), Block block_0 (torch.Size([256, 512, 9])).
I0315 19:17:16.120226 139716304848064 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 19.block_0 ([torch.Size([0])]) for Parameter 19 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0315 19:17:16.120310 140286885487808 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 11.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 11 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 19:17:16.120290 140571517158592 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 6.block_0 ([torch.Size([256, 256]), torch.Size([384, 384]), torch.Size([3, 3])]) for Parameter 6 (torch.Size([256, 128, 3, 3])), Block block_0 (torch.Size([256, 384, 3])).
I0315 19:17:16.120320 139802597225664 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 11.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 11 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 19:17:16.120348 140282512307392 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 11.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 11 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 19:17:16.120357 139716304848064 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 20.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 20 (torch.Size([512, 256, 2, 2])), Block block_0 (torch.Size([512, 1024])).
I0315 19:17:16.120450 140286885487808 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 12.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 12 (torch.Size([128, 256, 3, 3])), Block block_0 (torch.Size([128, 768, 3])).
I0315 19:17:16.120465 139802597225664 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 12.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 12 (torch.Size([128, 256, 3, 3])), Block block_0 (torch.Size([128, 768, 3])).
I0315 19:17:16.120491 139716304848064 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 21.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 21 (torch.Size([256, 128, 2, 2])), Block block_0 (torch.Size([256, 512])).
I0315 19:17:16.120461 140198727365824 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 12.block_0 ([torch.Size([128, 128]), torch.Size([768, 768]), torch.Size([3, 3])]) for Parameter 12 (torch.Size([128, 256, 3, 3])), Block block_0 (torch.Size([128, 768, 3])).
I0315 19:17:16.120491 140571517158592 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 7.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 7 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 19:17:16.120515 140282512307392 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 12.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 12 (torch.Size([128, 256, 3, 3])), Block block_0 (torch.Size([128, 768, 3])).
I0315 19:17:16.120589 140286885487808 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 13.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 13 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 19:17:16.120610 139802597225664 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 13.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 13 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 19:17:16.120620 139716304848064 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 22.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 22 (torch.Size([128, 64, 2, 2])), Block block_0 (torch.Size([128, 256])).
I0315 19:17:16.120676 140571517158592 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 8.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 8 (torch.Size([512, 256, 3, 3])), Block block_0 (torch.Size([512, 768, 3])).
I0315 19:17:16.120688 140282512307392 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 13.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 13 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 19:17:16.120737 140286885487808 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 14.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 14 (torch.Size([64, 128, 3, 3])), Block block_0 (torch.Size([64, 384, 3])).
I0315 19:17:16.120759 139716304848064 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 23.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 23 (torch.Size([64, 32, 2, 2])), Block block_0 (torch.Size([64, 128])).
I0315 19:17:16.120773 139802597225664 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 14.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 14 (torch.Size([64, 128, 3, 3])), Block block_0 (torch.Size([64, 384, 3])).
I0315 19:17:16.120775 140066777728192 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 9.block_0 ([torch.Size([512, 512]), torch.Size([512, 512]), torch.Size([9, 9])]) for Parameter 9 (torch.Size([512, 512, 3, 3])), Block block_0 (torch.Size([512, 512, 9])).
I0315 19:17:16.120841 140282512307392 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 14.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 14 (torch.Size([64, 128, 3, 3])), Block block_0 (torch.Size([64, 384, 3])).
I0315 19:17:16.120845 140571517158592 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 9.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 9 (torch.Size([512, 512, 3, 3])), Block block_0 (torch.Size([512, 512, 9])).
I0315 19:17:16.120866 140286885487808 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 15.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 15 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 19:17:16.120850 139647719654592 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 17.block_0 ([torch.Size([1024, 1024]), torch.Size([9, 9])]) for Parameter 17 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 19:17:16.120887 139716304848064 shampoo_preconditioner_list.py:612] Rank 4: ShampooPreconditionerList Numel Breakdown: (165888, 2097314, 174080, 671744, 696320, 327698, 426002, 1310738, 1703954, 1048738, 655522, 1310738, 1212434, 327698, 303122, 671744, 665600, 2097314, 2048, 2, 2621440, 655360, 163840, 40960)
I0315 19:17:16.120910 139802597225664 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 15.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 15 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 19:17:16.120932 139716304848064 shampoo_preconditioner_list.py:615] Rank 4: ShampooPreconditionerList Bytes Breakdown: (663552, 8389256)
I0315 19:17:16.120966 139716304848064 shampoo_preconditioner_list.py:618] Rank 4: ShampooPreconditionerList Total Elements: 19350298
I0315 19:17:16.120977 140286885487808 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 16.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 16 (torch.Size([32, 64, 3, 3])), Block block_0 (torch.Size([32, 576])).
I0315 19:17:16.120978 139647719654592 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 18.block_0 ([torch.Size([0])]) for Parameter 18 (torch.Size([1, 32, 1, 1])), Block block_0 (torch.Size([32])).
I0315 19:17:16.120988 140282512307392 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 15.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 15 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 19:17:16.120992 140571517158592 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 10.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 10 (torch.Size([256, 512, 3, 3])), Block block_0 (torch.Size([256, 512, 9])).
I0315 19:17:16.121005 139716304848064 shampoo_preconditioner_list.py:621] Rank 4: ShampooPreconditionerList Total Bytes: 9052808
I0315 19:17:16.120995 140066777728192 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 10.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 10 (torch.Size([256, 512, 3, 3])), Block block_0 (torch.Size([256, 512, 9])).
I0315 19:17:16.121037 139802597225664 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 16.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 16 (torch.Size([32, 64, 3, 3])), Block block_0 (torch.Size([32, 576])).
I0315 19:17:16.121080 139647719654592 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 19.block_0 ([torch.Size([0])]) for Parameter 19 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0315 19:17:16.121089 140286885487808 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 17.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 17 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 19:17:16.121125 140282512307392 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 16.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 16 (torch.Size([32, 64, 3, 3])), Block block_0 (torch.Size([32, 576])).
I0315 19:17:16.121143 140571517158592 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 11.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 11 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 19:17:16.121168 140286885487808 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 18.block_0 ([torch.Size([0])]) for Parameter 18 (torch.Size([1, 32, 1, 1])), Block block_0 (torch.Size([32])).
I0315 19:17:16.121161 140066777728192 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 11.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 11 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 19:17:16.121133 140198727365824 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 13.block_0 ([torch.Size([128, 128]), torch.Size([384, 384]), torch.Size([3, 3])]) for Parameter 13 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 19:17:16.121174 139802597225664 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 17.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 17 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 19:17:16.121213 139716304848064 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([32, 1, 3, 3])), Block block_0 (torch.Size([288])).
I0315 19:17:16.121253 140282512307392 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 17.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 17 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 19:17:16.121259 140286885487808 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 19.block_0 ([torch.Size([0])]) for Parameter 19 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0315 19:17:16.121266 139802597225664 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 18.block_0 ([torch.Size([0])]) for Parameter 18 (torch.Size([1, 32, 1, 1])), Block block_0 (torch.Size([32])).
I0315 19:17:16.121286 140571517158592 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 12.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 12 (torch.Size([128, 256, 3, 3])), Block block_0 (torch.Size([128, 768, 3])).
I0315 19:17:16.121293 139716304848064 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 19:17:16.121311 140066777728192 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 12.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 12 (torch.Size([128, 256, 3, 3])), Block block_0 (torch.Size([128, 768, 3])).
I0315 19:17:16.121352 139716304848064 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([64, 32, 3, 3])), Block block_0 (torch.Size([64, 288])).
I0315 19:17:16.121360 139802597225664 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 19.block_0 ([torch.Size([0])]) for Parameter 19 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0315 19:17:16.121365 140286885487808 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 20.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 20 (torch.Size([512, 256, 2, 2])), Block block_0 (torch.Size([512, 1024])).
I0315 19:17:16.121363 140282512307392 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 18.block_0 ([torch.Size([0])]) for Parameter 18 (torch.Size([1, 32, 1, 1])), Block block_0 (torch.Size([32])).
I0315 19:17:16.121414 139716304848064 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 19:17:16.121428 140198727365824 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 14.block_0 ([torch.Size([64, 64]), torch.Size([384, 384]), torch.Size([3, 3])]) for Parameter 14 (torch.Size([64, 128, 3, 3])), Block block_0 (torch.Size([64, 384, 3])).
I0315 19:17:16.121443 140571517158592 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 13.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 13 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 19:17:16.121454 140282512307392 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 19.block_0 ([torch.Size([0])]) for Parameter 19 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0315 19:17:16.121468 140066777728192 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 13.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 13 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 19:17:16.121474 139716304848064 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 4.block_0 (torch.Size([0])) for Parameter 4 (torch.Size([128, 64, 3, 3])), Block block_0 (torch.Size([128, 576])).
I0315 19:17:16.121480 139802597225664 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 20.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 20 (torch.Size([512, 256, 2, 2])), Block block_0 (torch.Size([512, 1024])).
I0315 19:17:16.121484 140286885487808 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 21.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 21 (torch.Size([256, 128, 2, 2])), Block block_0 (torch.Size([256, 512])).
I0315 19:17:16.121530 139716304848064 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 5.block_0 (torch.Size([0])) for Parameter 5 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 19:17:16.121584 140286885487808 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 22.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 22 (torch.Size([128, 64, 2, 2])), Block block_0 (torch.Size([128, 256])).
I0315 19:17:16.121591 139716304848064 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 6.block_0 (torch.Size([0])) for Parameter 6 (torch.Size([256, 128, 3, 3])), Block block_0 (torch.Size([256, 384, 3])).
I0315 19:17:16.121590 140282512307392 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 20.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 20 (torch.Size([512, 256, 2, 2])), Block block_0 (torch.Size([512, 1024])).
I0315 19:17:16.121599 140571517158592 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 14.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 14 (torch.Size([64, 128, 3, 3])), Block block_0 (torch.Size([64, 384, 3])).
I0315 19:17:16.121602 140198727365824 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 15.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 15 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 19:17:16.121613 139802597225664 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 21.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 21 (torch.Size([256, 128, 2, 2])), Block block_0 (torch.Size([256, 512])).
I0315 19:17:16.121643 140066777728192 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 14.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 14 (torch.Size([64, 128, 3, 3])), Block block_0 (torch.Size([64, 384, 3])).
I0315 19:17:16.121660 139716304848064 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 7.block_0 (torch.Size([0])) for Parameter 7 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 19:17:16.121715 140286885487808 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 23.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 23 (torch.Size([64, 32, 2, 2])), Block block_0 (torch.Size([64, 128])).
I0315 19:17:16.121736 139716304848064 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 8.block_0 (torch.Size([0])) for Parameter 8 (torch.Size([512, 256, 3, 3])), Block block_0 (torch.Size([512, 768, 3])).
I0315 19:17:16.121748 140198727365824 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 16.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 16 (torch.Size([32, 64, 3, 3])), Block block_0 (torch.Size([32, 576])).
I0315 19:17:16.121733 139647719654592 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 20.block_0 ([torch.Size([512, 512]), torch.Size([1024, 1024])]) for Parameter 20 (torch.Size([512, 256, 2, 2])), Block block_0 (torch.Size([512, 1024])).
I0315 19:17:16.121757 140282512307392 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 21.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 21 (torch.Size([256, 128, 2, 2])), Block block_0 (torch.Size([256, 512])).
I0315 19:17:16.121757 140571517158592 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 15.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 15 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 19:17:16.121761 139802597225664 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 22.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 22 (torch.Size([128, 64, 2, 2])), Block block_0 (torch.Size([128, 256])).
I0315 19:17:16.121792 139716304848064 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 9.block_0 (torch.Size([0])) for Parameter 9 (torch.Size([512, 512, 3, 3])), Block block_0 (torch.Size([512, 512, 9])).
I0315 19:17:16.121810 140066777728192 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 15.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 15 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 19:17:16.121835 140286885487808 shampoo_preconditioner_list.py:612] Rank 3: ShampooPreconditionerList Numel Breakdown: (165888, 2097314, 174080, 671744, 696320, 327698, 426002, 1310738, 1703954, 1048738, 655522, 1310738, 1212434, 327698, 303122, 671744, 665600, 2097314, 2048, 2, 2621440, 655360, 163840, 40960)
I0315 19:17:16.121853 139716304848064 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 10.block_0 (torch.Size([0])) for Parameter 10 (torch.Size([256, 512, 3, 3])), Block block_0 (torch.Size([256, 512, 9])).
I0315 19:17:16.121881 140286885487808 shampoo_preconditioner_list.py:615] Rank 3: ShampooPreconditionerList Bytes Breakdown: (663552, 8389256)
I0315 19:17:16.121876 140282512307392 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 22.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 22 (torch.Size([128, 64, 2, 2])), Block block_0 (torch.Size([128, 256])).
I0315 19:17:16.121876 140571517158592 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 16.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 16 (torch.Size([32, 64, 3, 3])), Block block_0 (torch.Size([32, 576])).
I0315 19:17:16.121886 139802597225664 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 23.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 23 (torch.Size([64, 32, 2, 2])), Block block_0 (torch.Size([64, 128])).
I0315 19:17:16.121890 140198727365824 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 17.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 17 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 19:17:16.121918 140286885487808 shampoo_preconditioner_list.py:618] Rank 3: ShampooPreconditionerList Total Elements: 19350298
I0315 19:17:16.121919 139647719654592 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 21.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 21 (torch.Size([256, 128, 2, 2])), Block block_0 (torch.Size([256, 512])).
I0315 19:17:16.121953 140286885487808 shampoo_preconditioner_list.py:621] Rank 3: ShampooPreconditionerList Total Bytes: 9052808
I0315 19:17:16.121948 140066777728192 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 16.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 16 (torch.Size([32, 64, 3, 3])), Block block_0 (torch.Size([32, 576])).
I0315 19:17:16.121959 139716304848064 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 11.block_0 (torch.Size([256, 768, 3])) for Parameter 11 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 19:17:16.121988 140198727365824 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 18.block_0 ([torch.Size([0])]) for Parameter 18 (torch.Size([1, 32, 1, 1])), Block block_0 (torch.Size([32])).
I0315 19:17:16.121996 140282512307392 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 23.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 23 (torch.Size([64, 32, 2, 2])), Block block_0 (torch.Size([64, 128])).
I0315 19:17:16.122000 140571517158592 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 17.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 17 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 19:17:16.122004 139802597225664 shampoo_preconditioner_list.py:612] Rank 1: ShampooPreconditionerList Numel Breakdown: (165888, 2097314, 174080, 671744, 696320, 327698, 426002, 1310738, 1703954, 1048738, 655522, 1310738, 1212434, 327698, 303122, 671744, 665600, 2097314, 2048, 2, 2621440, 655360, 163840, 40960)
I0315 19:17:16.122030 139716304848064 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 12.block_0 (torch.Size([0])) for Parameter 12 (torch.Size([128, 256, 3, 3])), Block block_0 (torch.Size([128, 768, 3])).
I0315 19:17:16.122049 139802597225664 shampoo_preconditioner_list.py:615] Rank 1: ShampooPreconditionerList Bytes Breakdown: (663552,)
I0315 19:17:16.122044 139647719654592 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 22.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 22 (torch.Size([128, 64, 2, 2])), Block block_0 (torch.Size([128, 256])).
I0315 19:17:16.122074 140066777728192 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 17.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 17 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 19:17:16.122084 139802597225664 shampoo_preconditioner_list.py:618] Rank 1: ShampooPreconditionerList Total Elements: 19350298
I0315 19:17:16.122085 140198727365824 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 19.block_0 ([torch.Size([0])]) for Parameter 19 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0315 19:17:16.122095 139716304848064 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 13.block_0 (torch.Size([0])) for Parameter 13 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 19:17:16.122117 139802597225664 shampoo_preconditioner_list.py:621] Rank 1: ShampooPreconditionerList Total Bytes: 663552
I0315 19:17:16.122117 140282512307392 shampoo_preconditioner_list.py:612] Rank 2: ShampooPreconditionerList Numel Breakdown: (165888, 2097314, 174080, 671744, 696320, 327698, 426002, 1310738, 1703954, 1048738, 655522, 1310738, 1212434, 327698, 303122, 671744, 665600, 2097314, 2048, 2, 2621440, 655360, 163840, 40960)
I0315 19:17:16.122136 140286885487808 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([32, 1, 3, 3])), Block block_0 (torch.Size([288])).
I0315 19:17:16.122149 139716304848064 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 14.block_0 (torch.Size([0])) for Parameter 14 (torch.Size([64, 128, 3, 3])), Block block_0 (torch.Size([64, 384, 3])).
I0315 19:17:16.122160 140282512307392 shampoo_preconditioner_list.py:615] Rank 2: ShampooPreconditionerList Bytes Breakdown: (663552,)
I0315 19:17:16.122168 139647719654592 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 23.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 23 (torch.Size([64, 32, 2, 2])), Block block_0 (torch.Size([64, 128])).
I0315 19:17:16.122163 140571517158592 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 18.block_0 ([torch.Size([32, 32])]) for Parameter 18 (torch.Size([1, 32, 1, 1])), Block block_0 (torch.Size([32])).
I0315 19:17:16.122179 140066777728192 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 18.block_0 ([torch.Size([0])]) for Parameter 18 (torch.Size([1, 32, 1, 1])), Block block_0 (torch.Size([32])).
I0315 19:17:16.122195 140282512307392 shampoo_preconditioner_list.py:618] Rank 2: ShampooPreconditionerList Total Elements: 19350298
I0315 19:17:16.122201 139716304848064 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 15.block_0 (torch.Size([0])) for Parameter 15 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 19:17:16.122206 140286885487808 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 19:17:16.122204 140198727365824 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 20.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 20 (torch.Size([512, 256, 2, 2])), Block block_0 (torch.Size([512, 1024])).
I0315 19:17:16.122228 140282512307392 shampoo_preconditioner_list.py:621] Rank 2: ShampooPreconditionerList Total Bytes: 663552
I0315 19:17:16.122268 140066777728192 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 19.block_0 ([torch.Size([0])]) for Parameter 19 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0315 19:17:16.122287 139716304848064 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 16.block_0 (torch.Size([32, 576])) for Parameter 16 (torch.Size([32, 64, 3, 3])), Block block_0 (torch.Size([32, 576])).
I0315 19:17:16.122299 139647719654592 shampoo_preconditioner_list.py:612] Rank 5: ShampooPreconditionerList Numel Breakdown: (165888, 2097314, 174080, 671744, 696320, 327698, 426002, 1310738, 1703954, 1048738, 655522, 1310738, 1212434, 327698, 303122, 671744, 665600, 2097314, 2048, 2, 2621440, 655360, 163840, 40960)
I0315 19:17:16.122304 140286885487808 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([64, 288])) for Parameter 2 (torch.Size([64, 32, 3, 3])), Block block_0 (torch.Size([64, 288])).
I0315 19:17:16.122324 140571517158592 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 19.block_0 ([torch.Size([1, 1])]) for Parameter 19 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0315 19:17:16.122339 140198727365824 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 21.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 21 (torch.Size([256, 128, 2, 2])), Block block_0 (torch.Size([256, 512])).
I0315 19:17:16.122345 139647719654592 shampoo_preconditioner_list.py:615] Rank 5: ShampooPreconditionerList Bytes Breakdown: (663552, 8389256, 696320, 2686976)
I0315 19:17:16.122342 139802597225664 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([32, 1, 3, 3])), Block block_0 (torch.Size([288])).
I0315 19:17:16.122360 139716304848064 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 17.block_0 (torch.Size([0])) for Parameter 17 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 19:17:16.122367 140286885487808 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 19:17:16.122380 139647719654592 shampoo_preconditioner_list.py:618] Rank 5: ShampooPreconditionerList Total Elements: 19350298
I0315 19:17:16.122396 140066777728192 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 20.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 20 (torch.Size([512, 256, 2, 2])), Block block_0 (torch.Size([512, 1024])).
I0315 19:17:16.122416 140286885487808 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 4.block_0 (torch.Size([0])) for Parameter 4 (torch.Size([128, 64, 3, 3])), Block block_0 (torch.Size([128, 576])).
I0315 19:17:16.122414 139716304848064 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 18.block_0 (torch.Size([0])) for Parameter 18 (torch.Size([1, 32, 1, 1])), Block block_0 (torch.Size([32])).
I0315 19:17:16.122414 139802597225664 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 19:17:16.122423 139647719654592 shampoo_preconditioner_list.py:621] Rank 5: ShampooPreconditionerList Total Bytes: 12436104
I0315 19:17:16.122466 139716304848064 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 19.block_0 (torch.Size([0])) for Parameter 19 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0315 19:17:16.122453 140282512307392 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([32, 1, 3, 3])), Block block_0 (torch.Size([288])).
I0315 19:17:16.122472 140286885487808 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 5.block_0 (torch.Size([0])) for Parameter 5 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 19:17:16.122468 140198727365824 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 22.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 22 (torch.Size([128, 64, 2, 2])), Block block_0 (torch.Size([128, 256])).
I0315 19:17:16.122471 139802597225664 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([64, 32, 3, 3])), Block block_0 (torch.Size([64, 288])).
I0315 19:17:16.122476 140571517158592 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 20.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 20 (torch.Size([512, 256, 2, 2])), Block block_0 (torch.Size([512, 1024])).
I0315 19:17:16.122520 140286885487808 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 6.block_0 (torch.Size([0])) for Parameter 6 (torch.Size([256, 128, 3, 3])), Block block_0 (torch.Size([256, 384, 3])).
I0315 19:17:16.122526 139716304848064 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 20.block_0 (torch.Size([0])) for Parameter 20 (torch.Size([512, 256, 2, 2])), Block block_0 (torch.Size([512, 1024])).
I0315 19:17:16.122526 139802597225664 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 19:17:16.122528 140282512307392 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 19:17:16.122542 140066777728192 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 21.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 21 (torch.Size([256, 128, 2, 2])), Block block_0 (torch.Size([256, 512])).
I0315 19:17:16.122577 139716304848064 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 21.block_0 (torch.Size([0])) for Parameter 21 (torch.Size([256, 128, 2, 2])), Block block_0 (torch.Size([256, 512])).
I0315 19:17:16.122581 139802597225664 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 4.block_0 (torch.Size([0])) for Parameter 4 (torch.Size([128, 64, 3, 3])), Block block_0 (torch.Size([128, 576])).
I0315 19:17:16.122597 140282512307392 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([64, 32, 3, 3])), Block block_0 (torch.Size([64, 288])).
I0315 19:17:16.122618 140286885487808 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 7.block_0 (torch.Size([256, 768, 3])) for Parameter 7 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 19:17:16.122637 139802597225664 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 5.block_0 (torch.Size([0])) for Parameter 5 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 19:17:16.122636 139716304848064 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 22.block_0 (torch.Size([0])) for Parameter 22 (torch.Size([128, 64, 2, 2])), Block block_0 (torch.Size([128, 256])).
I0315 19:17:16.122641 139647719654592 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([32, 1, 3, 3])), Block block_0 (torch.Size([288])).
I0315 19:17:16.122663 140282512307392 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 19:17:16.122677 140066777728192 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 22.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 22 (torch.Size([128, 64, 2, 2])), Block block_0 (torch.Size([128, 256])).
I0315 19:17:16.122688 140286885487808 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 8.block_0 (torch.Size([0])) for Parameter 8 (torch.Size([512, 256, 3, 3])), Block block_0 (torch.Size([512, 768, 3])).
I0315 19:17:16.122700 139716304848064 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 23.block_0 (torch.Size([0])) for Parameter 23 (torch.Size([64, 32, 2, 2])), Block block_0 (torch.Size([64, 128])).
I0315 19:17:16.122686 140198727365824 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 23.block_0 ([torch.Size([64, 64]), torch.Size([128, 128])]) for Parameter 23 (torch.Size([64, 32, 2, 2])), Block block_0 (torch.Size([64, 128])).
I0315 19:17:16.122701 139802597225664 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 6.block_0 (torch.Size([0])) for Parameter 6 (torch.Size([256, 128, 3, 3])), Block block_0 (torch.Size([256, 384, 3])).
I0315 19:17:16.122718 140282512307392 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 4.block_0 (torch.Size([0])) for Parameter 4 (torch.Size([128, 64, 3, 3])), Block block_0 (torch.Size([128, 576])).
I0315 19:17:16.122731 139647719654592 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 19:17:16.122745 140286885487808 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 9.block_0 (torch.Size([0])) for Parameter 9 (torch.Size([512, 512, 3, 3])), Block block_0 (torch.Size([512, 512, 9])).
I0315 19:17:16.122763 139802597225664 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 7.block_0 (torch.Size([0])) for Parameter 7 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 19:17:16.122772 139716304848064 shampoo_preconditioner_list.py:375] Rank 4: AdaGradPreconditionerList Numel Breakdown: (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 589824, 0, 0, 0, 0, 18432, 0, 0, 0, 0, 0, 0, 0)
I0315 19:17:16.122782 140282512307392 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 5.block_0 (torch.Size([0])) for Parameter 5 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 19:17:16.122792 139647719654592 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([64, 32, 3, 3])), Block block_0 (torch.Size([64, 288])).
I0315 19:17:16.122795 140286885487808 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 10.block_0 (torch.Size([0])) for Parameter 10 (torch.Size([256, 512, 3, 3])), Block block_0 (torch.Size([256, 512, 9])).
I0315 19:17:16.122804 140066777728192 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 23.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 23 (torch.Size([64, 32, 2, 2])), Block block_0 (torch.Size([64, 128])).
I0315 19:17:16.122810 139716304848064 shampoo_preconditioner_list.py:378] Rank 4: AdaGradPreconditionerList Bytes Breakdown: (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2359296, 0, 0, 0, 0, 73728, 0, 0, 0, 0, 0, 0, 0)
I0315 19:17:16.122838 140282512307392 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 6.block_0 (torch.Size([0])) for Parameter 6 (torch.Size([256, 128, 3, 3])), Block block_0 (torch.Size([256, 384, 3])).
I0315 19:17:16.122842 140286885487808 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 11.block_0 (torch.Size([0])) for Parameter 11 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 19:17:16.122843 139716304848064 shampoo_preconditioner_list.py:381] Rank 4: AdaGradPreconditionerList Total Elements: 608256
I0315 19:17:16.122843 140198727365824 shampoo_preconditioner_list.py:612] Rank 7: ShampooPreconditionerList Numel Breakdown: (165888, 2097314, 174080, 671744, 696320, 327698, 426002, 1310738, 1703954, 1048738, 655522, 1310738, 1212434, 327698, 303122, 671744, 665600, 2097314, 2048, 2, 2621440, 655360, 163840, 40960)
I0315 19:17:16.122886 139716304848064 shampoo_preconditioner_list.py:384] Rank 4: AdaGradPreconditionerList Total Bytes: 2433024
I0315 19:17:16.122887 140286885487808 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 12.block_0 (torch.Size([0])) for Parameter 12 (torch.Size([128, 256, 3, 3])), Block block_0 (torch.Size([128, 768, 3])).
I0315 19:17:16.122906 140198727365824 shampoo_preconditioner_list.py:615] Rank 7: ShampooPreconditionerList Bytes Breakdown: (663552, 8389256, 696320, 2686976, 2785280, 1310792)
I0315 19:17:16.122901 139802597225664 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 8.block_0 (torch.Size([512, 768, 3])) for Parameter 8 (torch.Size([512, 256, 3, 3])), Block block_0 (torch.Size([512, 768, 3])).
I0315 19:17:16.122916 140282512307392 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 7.block_0 (torch.Size([0])) for Parameter 7 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 19:17:16.122915 139647719654592 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([64, 576])) for Parameter 3 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 19:17:16.122931 140286885487808 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 13.block_0 (torch.Size([0])) for Parameter 13 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 19:17:16.122932 140066777728192 shampoo_preconditioner_list.py:612] Rank 0: ShampooPreconditionerList Numel Breakdown: (165888, 2097314, 174080, 671744, 696320, 327698, 426002, 1310738, 1703954, 1048738, 655522, 1310738, 1212434, 327698, 303122, 671744, 665600, 2097314, 2048, 2, 2621440, 655360, 163840, 40960)
I0315 19:17:16.122949 140198727365824 shampoo_preconditioner_list.py:618] Rank 7: ShampooPreconditionerList Total Elements: 19350298
I0315 19:17:16.122973 140066777728192 shampoo_preconditioner_list.py:615] Rank 0: ShampooPreconditionerList Bytes Breakdown: (663552,)
I0315 19:17:16.122971 140282512307392 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 8.block_0 (torch.Size([0])) for Parameter 8 (torch.Size([512, 256, 3, 3])), Block block_0 (torch.Size([512, 768, 3])).
I0315 19:17:16.122971 139802597225664 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 9.block_0 (torch.Size([0])) for Parameter 9 (torch.Size([512, 512, 3, 3])), Block block_0 (torch.Size([512, 512, 9])).
I0315 19:17:16.122976 140286885487808 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 14.block_0 (torch.Size([0])) for Parameter 14 (torch.Size([64, 128, 3, 3])), Block block_0 (torch.Size([64, 384, 3])).
I0315 19:17:16.122987 140198727365824 shampoo_preconditioner_list.py:621] Rank 7: ShampooPreconditionerList Total Bytes: 16532176
I0315 19:17:16.122988 139647719654592 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 4.block_0 (torch.Size([0])) for Parameter 4 (torch.Size([128, 64, 3, 3])), Block block_0 (torch.Size([128, 576])).
I0315 19:17:16.123014 140066777728192 shampoo_preconditioner_list.py:618] Rank 0: ShampooPreconditionerList Total Elements: 19350298
I0315 19:17:16.123020 140286885487808 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 15.block_0 (torch.Size([0])) for Parameter 15 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 19:17:16.123024 139802597225664 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 10.block_0 (torch.Size([0])) for Parameter 10 (torch.Size([256, 512, 3, 3])), Block block_0 (torch.Size([256, 512, 9])).
I0315 19:17:16.123024 140282512307392 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 9.block_0 (torch.Size([0])) for Parameter 9 (torch.Size([512, 512, 3, 3])), Block block_0 (torch.Size([512, 512, 9])).
I0315 19:17:16.123043 139647719654592 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 5.block_0 (torch.Size([0])) for Parameter 5 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 19:17:16.123050 140066777728192 shampoo_preconditioner_list.py:621] Rank 0: ShampooPreconditionerList Total Bytes: 663552
I0315 19:17:16.123063 140286885487808 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 16.block_0 (torch.Size([0])) for Parameter 16 (torch.Size([32, 64, 3, 3])), Block block_0 (torch.Size([32, 576])).
I0315 19:17:16.123076 139802597225664 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 11.block_0 (torch.Size([0])) for Parameter 11 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 19:17:16.123107 140286885487808 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 17.block_0 (torch.Size([0])) for Parameter 17 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 19:17:16.123106 139647719654592 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 6.block_0 (torch.Size([0])) for Parameter 6 (torch.Size([256, 128, 3, 3])), Block block_0 (torch.Size([256, 384, 3])).
I0315 19:17:16.123101 140571517158592 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 21.block_0 ([torch.Size([256, 256]), torch.Size([512, 512])]) for Parameter 21 (torch.Size([256, 128, 2, 2])), Block block_0 (torch.Size([256, 512])).
I0315 19:17:16.123138 139802597225664 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 12.block_0 (torch.Size([0])) for Parameter 12 (torch.Size([128, 256, 3, 3])), Block block_0 (torch.Size([128, 768, 3])).
I0315 19:17:16.123138 140282512307392 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 10.block_0 (torch.Size([256, 512, 9])) for Parameter 10 (torch.Size([256, 512, 3, 3])), Block block_0 (torch.Size([256, 512, 9])).
I0315 19:17:16.123161 140286885487808 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 18.block_0 (torch.Size([0])) for Parameter 18 (torch.Size([1, 32, 1, 1])), Block block_0 (torch.Size([32])).
I0315 19:17:16.123166 139647719654592 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 7.block_0 (torch.Size([0])) for Parameter 7 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 19:17:16.123190 139802597225664 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 13.block_0 (torch.Size([0])) for Parameter 13 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 19:17:16.123209 140282512307392 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 11.block_0 (torch.Size([0])) for Parameter 11 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 19:17:16.123213 140286885487808 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 19.block_0 (torch.Size([0])) for Parameter 19 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0315 19:17:16.123196 140198727365824 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([32, 1, 3, 3])), Block block_0 (torch.Size([288])).
I0315 19:17:16.123226 139647719654592 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 8.block_0 (torch.Size([0])) for Parameter 8 (torch.Size([512, 256, 3, 3])), Block block_0 (torch.Size([512, 768, 3])).
I0315 19:17:16.123248 139802597225664 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 14.block_0 (torch.Size([0])) for Parameter 14 (torch.Size([64, 128, 3, 3])), Block block_0 (torch.Size([64, 384, 3])).
I0315 19:17:16.123257 140286885487808 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 20.block_0 (torch.Size([0])) for Parameter 20 (torch.Size([512, 256, 2, 2])), Block block_0 (torch.Size([512, 1024])).
I0315 19:17:16.123262 140066777728192 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([32, 1, 3, 3])), Block block_0 (torch.Size([288])).
I0315 19:17:16.123273 140282512307392 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 12.block_0 (torch.Size([0])) for Parameter 12 (torch.Size([128, 256, 3, 3])), Block block_0 (torch.Size([128, 768, 3])).
I0315 19:17:16.123280 139647719654592 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 9.block_0 (torch.Size([0])) for Parameter 9 (torch.Size([512, 512, 3, 3])), Block block_0 (torch.Size([512, 512, 9])).
I0315 19:17:16.123305 139802597225664 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 15.block_0 (torch.Size([0])) for Parameter 15 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 19:17:16.123321 140286885487808 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 21.block_0 (torch.Size([0])) for Parameter 21 (torch.Size([256, 128, 2, 2])), Block block_0 (torch.Size([256, 512])).
I0315 19:17:16.123314 140198727365824 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([1024, 9])) for Parameter 1 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 19:17:16.123326 140282512307392 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 13.block_0 (torch.Size([0])) for Parameter 13 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 19:17:16.123330 139647719654592 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 10.block_0 (torch.Size([0])) for Parameter 10 (torch.Size([256, 512, 3, 3])), Block block_0 (torch.Size([256, 512, 9])).
I0315 19:17:16.123337 140066777728192 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 19:17:16.123332 140571517158592 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 22.block_0 ([torch.Size([128, 128]), torch.Size([256, 256])]) for Parameter 22 (torch.Size([128, 64, 2, 2])), Block block_0 (torch.Size([128, 256])).
I0315 19:17:16.123362 139802597225664 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 16.block_0 (torch.Size([0])) for Parameter 16 (torch.Size([32, 64, 3, 3])), Block block_0 (torch.Size([32, 576])).
I0315 19:17:16.123368 140286885487808 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 22.block_0 (torch.Size([0])) for Parameter 22 (torch.Size([128, 64, 2, 2])), Block block_0 (torch.Size([128, 256])).
I0315 19:17:16.123377 140282512307392 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 14.block_0 (torch.Size([0])) for Parameter 14 (torch.Size([64, 128, 3, 3])), Block block_0 (torch.Size([64, 384, 3])).
I0315 19:17:16.123381 139647719654592 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 11.block_0 (torch.Size([0])) for Parameter 11 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 19:17:16.123386 140198727365824 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([64, 32, 3, 3])), Block block_0 (torch.Size([64, 288])).
I0315 19:17:16.123392 140066777728192 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([64, 32, 3, 3])), Block block_0 (torch.Size([64, 288])).
I0315 19:17:16.123413 139802597225664 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 17.block_0 (torch.Size([0])) for Parameter 17 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 19:17:16.123418 140286885487808 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 23.block_0 (torch.Size([0])) for Parameter 23 (torch.Size([64, 32, 2, 2])), Block block_0 (torch.Size([64, 128])).
I0315 19:17:16.123432 139647719654592 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 12.block_0 (torch.Size([0])) for Parameter 12 (torch.Size([128, 256, 3, 3])), Block block_0 (torch.Size([128, 768, 3])).
I0315 19:17:16.123435 140282512307392 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 15.block_0 (torch.Size([0])) for Parameter 15 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 19:17:16.123441 140198727365824 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 19:17:16.123443 140066777728192 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 19:17:16.123469 139802597225664 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 18.block_0 (torch.Size([0])) for Parameter 18 (torch.Size([1, 32, 1, 1])), Block block_0 (torch.Size([32])).
I0315 19:17:16.123481 139647719654592 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 13.block_0 (torch.Size([0])) for Parameter 13 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 19:17:16.123476 140571517158592 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 23.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 23 (torch.Size([64, 32, 2, 2])), Block block_0 (torch.Size([64, 128])).
I0315 19:17:16.123484 140286885487808 shampoo_preconditioner_list.py:375] Rank 3: AdaGradPreconditionerList Numel Breakdown: (0, 0, 18432, 0, 0, 0, 0, 589824, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)
I0315 19:17:16.123486 140282512307392 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 16.block_0 (torch.Size([0])) for Parameter 16 (torch.Size([32, 64, 3, 3])), Block block_0 (torch.Size([32, 576])).
I0315 19:17:16.123493 140066777728192 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 4.block_0 (torch.Size([0])) for Parameter 4 (torch.Size([128, 64, 3, 3])), Block block_0 (torch.Size([128, 576])).
I0315 19:17:16.123520 140286885487808 shampoo_preconditioner_list.py:378] Rank 3: AdaGradPreconditionerList Bytes Breakdown: (0, 0, 73728, 0, 0, 0, 0, 2359296, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)
I0315 19:17:16.123523 139802597225664 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 19.block_0 (torch.Size([0])) for Parameter 19 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0315 19:17:16.123532 139647719654592 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 14.block_0 (torch.Size([0])) for Parameter 14 (torch.Size([64, 128, 3, 3])), Block block_0 (torch.Size([64, 384, 3])).
I0315 19:17:16.123538 140282512307392 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 17.block_0 (torch.Size([0])) for Parameter 17 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 19:17:16.123546 140066777728192 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 5.block_0 (torch.Size([0])) for Parameter 5 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 19:17:16.123557 140286885487808 shampoo_preconditioner_list.py:381] Rank 3: AdaGradPreconditionerList Total Elements: 608256
I0315 19:17:16.123574 139802597225664 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 20.block_0 (torch.Size([0])) for Parameter 20 (torch.Size([512, 256, 2, 2])), Block block_0 (torch.Size([512, 1024])).
I0315 19:17:16.123585 140286885487808 shampoo_preconditioner_list.py:384] Rank 3: AdaGradPreconditionerList Total Bytes: 2433024
I0315 19:17:16.123598 140282512307392 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 18.block_0 (torch.Size([0])) for Parameter 18 (torch.Size([1, 32, 1, 1])), Block block_0 (torch.Size([32])).
I0315 19:17:16.123609 140066777728192 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 6.block_0 (torch.Size([0])) for Parameter 6 (torch.Size([256, 128, 3, 3])), Block block_0 (torch.Size([256, 384, 3])).
I0315 19:17:16.123622 140571517158592 shampoo_preconditioner_list.py:612] Rank 6: ShampooPreconditionerList Numel Breakdown: (165888, 2097314, 174080, 671744, 696320, 327698, 426002, 1310738, 1703954, 1048738, 655522, 1310738, 1212434, 327698, 303122, 671744, 665600, 2097314, 2048, 2, 2621440, 655360, 163840, 40960)
I0315 19:17:16.123630 139802597225664 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 21.block_0 (torch.Size([0])) for Parameter 21 (torch.Size([256, 128, 2, 2])), Block block_0 (torch.Size([256, 512])).
I0315 19:17:16.123629 139647719654592 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 15.block_0 (torch.Size([64, 576])) for Parameter 15 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 19:17:16.123657 140282512307392 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 19.block_0 (torch.Size([0])) for Parameter 19 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0315 19:17:16.123675 140066777728192 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 7.block_0 (torch.Size([0])) for Parameter 7 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 19:17:16.123677 140571517158592 shampoo_preconditioner_list.py:615] Rank 6: ShampooPreconditionerList Bytes Breakdown: (663552, 8389256, 696320, 2686976, 2785280, 1310792, 1704008)
I0315 19:17:16.123688 139802597225664 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 22.block_0 (torch.Size([0])) for Parameter 22 (torch.Size([128, 64, 2, 2])), Block block_0 (torch.Size([128, 256])).
I0315 19:17:16.123695 139647719654592 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 16.block_0 (torch.Size([0])) for Parameter 16 (torch.Size([32, 64, 3, 3])), Block block_0 (torch.Size([32, 576])).
I0315 19:17:16.123713 140571517158592 shampoo_preconditioner_list.py:618] Rank 6: ShampooPreconditionerList Total Elements: 19350298
I0315 19:17:16.123715 140282512307392 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 20.block_0 (torch.Size([0])) for Parameter 20 (torch.Size([512, 256, 2, 2])), Block block_0 (torch.Size([512, 1024])).
I0315 19:17:16.123736 140066777728192 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 8.block_0 (torch.Size([0])) for Parameter 8 (torch.Size([512, 256, 3, 3])), Block block_0 (torch.Size([512, 768, 3])).
I0315 19:17:16.123742 139802597225664 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 23.block_0 (torch.Size([0])) for Parameter 23 (torch.Size([64, 32, 2, 2])), Block block_0 (torch.Size([64, 128])).
I0315 19:17:16.123750 140571517158592 shampoo_preconditioner_list.py:621] Rank 6: ShampooPreconditionerList Total Bytes: 18236184
I0315 19:17:16.123773 140282512307392 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 21.block_0 (torch.Size([0])) for Parameter 21 (torch.Size([256, 128, 2, 2])), Block block_0 (torch.Size([256, 512])).
I0315 19:17:16.123775 139647719654592 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 17.block_0 (torch.Size([1024, 9])) for Parameter 17 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 19:17:16.123812 139802597225664 shampoo_preconditioner_list.py:375] Rank 1: AdaGradPreconditionerList Numel Breakdown: (0, 0, 0, 0, 0, 0, 0, 0, 1179648, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)
I0315 19:17:16.123831 140282512307392 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 22.block_0 (torch.Size([0])) for Parameter 22 (torch.Size([128, 64, 2, 2])), Block block_0 (torch.Size([128, 256])).
I0315 19:17:16.123836 139647719654592 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 18.block_0 (torch.Size([0])) for Parameter 18 (torch.Size([1, 32, 1, 1])), Block block_0 (torch.Size([32])).
I0315 19:17:16.123853 139802597225664 shampoo_preconditioner_list.py:378] Rank 1: AdaGradPreconditionerList Bytes Breakdown: (0, 0, 0, 0, 0, 0, 0, 0, 4718592, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)
I0315 19:17:16.123855 140066777728192 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 9.block_0 (torch.Size([512, 512, 9])) for Parameter 9 (torch.Size([512, 512, 3, 3])), Block block_0 (torch.Size([512, 512, 9])).
I0315 19:17:16.123886 139647719654592 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 19.block_0 (torch.Size([0])) for Parameter 19 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0315 19:17:16.123889 139802597225664 shampoo_preconditioner_list.py:381] Rank 1: AdaGradPreconditionerList Total Elements: 1179648
I0315 19:17:16.123890 140282512307392 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 23.block_0 (torch.Size([0])) for Parameter 23 (torch.Size([64, 32, 2, 2])), Block block_0 (torch.Size([64, 128])).
I0315 19:17:16.123878 140198727365824 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 4.block_0 (torch.Size([128, 576])) for Parameter 4 (torch.Size([128, 64, 3, 3])), Block block_0 (torch.Size([128, 576])).
I0315 19:17:16.123920 139802597225664 shampoo_preconditioner_list.py:384] Rank 1: AdaGradPreconditionerList Total Bytes: 4718592
I0315 19:17:16.123924 140066777728192 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 10.block_0 (torch.Size([0])) for Parameter 10 (torch.Size([256, 512, 3, 3])), Block block_0 (torch.Size([256, 512, 9])).
I0315 19:17:16.123961 140282512307392 shampoo_preconditioner_list.py:375] Rank 2: AdaGradPreconditionerList Numel Breakdown: (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1179648, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)
I0315 19:17:16.123979 140066777728192 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 11.block_0 (torch.Size([0])) for Parameter 11 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 19:17:16.123975 139647719654592 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 20.block_0 (torch.Size([512, 1024])) for Parameter 20 (torch.Size([512, 256, 2, 2])), Block block_0 (torch.Size([512, 1024])).
I0315 19:17:16.123979 140198727365824 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 5.block_0 (torch.Size([0])) for Parameter 5 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 19:17:16.124003 140282512307392 shampoo_preconditioner_list.py:378] Rank 2: AdaGradPreconditionerList Bytes Breakdown: (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4718592, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)
I0315 19:17:16.123997 140571517158592 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([288])) for Parameter 0 (torch.Size([32, 1, 3, 3])), Block block_0 (torch.Size([288])).
I0315 19:17:16.124032 140066777728192 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 12.block_0 (torch.Size([0])) for Parameter 12 (torch.Size([128, 256, 3, 3])), Block block_0 (torch.Size([128, 768, 3])).
I0315 19:17:16.124037 139647719654592 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 21.block_0 (torch.Size([0])) for Parameter 21 (torch.Size([256, 128, 2, 2])), Block block_0 (torch.Size([256, 512])).
I0315 19:17:16.124040 140282512307392 shampoo_preconditioner_list.py:381] Rank 2: AdaGradPreconditionerList Total Elements: 1179648
I0315 19:17:16.124038 140198727365824 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 6.block_0 (torch.Size([0])) for Parameter 6 (torch.Size([256, 128, 3, 3])), Block block_0 (torch.Size([256, 384, 3])).
I0315 19:17:16.124071 140282512307392 shampoo_preconditioner_list.py:384] Rank 2: AdaGradPreconditionerList Total Bytes: 4718592
I0315 19:17:16.124081 140571517158592 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 19:17:16.124089 139647719654592 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 22.block_0 (torch.Size([0])) for Parameter 22 (torch.Size([128, 64, 2, 2])), Block block_0 (torch.Size([128, 256])).
I0315 19:17:16.124092 140066777728192 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 13.block_0 (torch.Size([0])) for Parameter 13 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 19:17:16.124099 140198727365824 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 7.block_0 (torch.Size([0])) for Parameter 7 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 19:17:16.124069 139716304848064 logger_utils.py:262] Unable to record workload.train_mean information. Continuing without it.
I0315 19:17:16.124140 140571517158592 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([64, 32, 3, 3])), Block block_0 (torch.Size([64, 288])).
I0315 19:17:16.124142 139647719654592 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 23.block_0 (torch.Size([0])) for Parameter 23 (torch.Size([64, 32, 2, 2])), Block block_0 (torch.Size([64, 128])).
I0315 19:17:16.124140 139716304848064 logger_utils.py:262] Unable to record workload.train_stddev information. Continuing without it.
I0315 19:17:16.124144 140066777728192 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 14.block_0 (torch.Size([0])) for Parameter 14 (torch.Size([64, 128, 3, 3])), Block block_0 (torch.Size([64, 384, 3])).
I0315 19:17:16.124153 140198727365824 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 8.block_0 (torch.Size([0])) for Parameter 8 (torch.Size([512, 256, 3, 3])), Block block_0 (torch.Size([512, 768, 3])).
I0315 19:17:16.124193 140066777728192 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 15.block_0 (torch.Size([0])) for Parameter 15 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 19:17:16.124206 140198727365824 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 9.block_0 (torch.Size([0])) for Parameter 9 (torch.Size([512, 512, 3, 3])), Block block_0 (torch.Size([512, 512, 9])).
I0315 19:17:16.124205 140571517158592 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 19:17:16.124211 139647719654592 shampoo_preconditioner_list.py:375] Rank 5: AdaGradPreconditionerList Numel Breakdown: (0, 0, 0, 36864, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 36864, 0, 9216, 0, 0, 524288, 0, 0, 0)
I0315 19:17:16.124247 139647719654592 shampoo_preconditioner_list.py:378] Rank 5: AdaGradPreconditionerList Bytes Breakdown: (0, 0, 0, 147456, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 147456, 0, 36864, 0, 0, 2097152, 0, 0, 0)
I0315 19:17:16.124250 140066777728192 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 16.block_0 (torch.Size([0])) for Parameter 16 (torch.Size([32, 64, 3, 3])), Block block_0 (torch.Size([32, 576])).
I0315 19:17:16.124258 140198727365824 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 10.block_0 (torch.Size([0])) for Parameter 10 (torch.Size([256, 512, 3, 3])), Block block_0 (torch.Size([256, 512, 9])).
I0315 19:17:16.124259 140571517158592 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 4.block_0 (torch.Size([0])) for Parameter 4 (torch.Size([128, 64, 3, 3])), Block block_0 (torch.Size([128, 576])).
I0315 19:17:16.124287 139647719654592 shampoo_preconditioner_list.py:381] Rank 5: AdaGradPreconditionerList Total Elements: 607232
I0315 19:17:16.124303 140066777728192 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 17.block_0 (torch.Size([0])) for Parameter 17 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 19:17:16.124310 140198727365824 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 11.block_0 (torch.Size([0])) for Parameter 11 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 19:17:16.124321 139647719654592 shampoo_preconditioner_list.py:384] Rank 5: AdaGradPreconditionerList Total Bytes: 2428928
I0315 19:17:16.124352 140066777728192 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 18.block_0 (torch.Size([0])) for Parameter 18 (torch.Size([1, 32, 1, 1])), Block block_0 (torch.Size([32])).
I0315 19:17:16.124396 140198727365824 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 12.block_0 (torch.Size([128, 768, 3])) for Parameter 12 (torch.Size([128, 256, 3, 3])), Block block_0 (torch.Size([128, 768, 3])).
I0315 19:17:16.124407 140066777728192 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 19.block_0 (torch.Size([0])) for Parameter 19 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0315 19:17:16.124457 140066777728192 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 20.block_0 (torch.Size([0])) for Parameter 20 (torch.Size([512, 256, 2, 2])), Block block_0 (torch.Size([512, 1024])).
I0315 19:17:16.124490 140198727365824 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 13.block_0 (torch.Size([128, 384, 3])) for Parameter 13 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 19:17:16.124512 140066777728192 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 21.block_0 (torch.Size([0])) for Parameter 21 (torch.Size([256, 128, 2, 2])), Block block_0 (torch.Size([256, 512])).
I0315 19:17:16.124563 140066777728192 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 22.block_0 (torch.Size([0])) for Parameter 22 (torch.Size([128, 64, 2, 2])), Block block_0 (torch.Size([128, 256])).
I0315 19:17:16.124579 140198727365824 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 14.block_0 (torch.Size([64, 384, 3])) for Parameter 14 (torch.Size([64, 128, 3, 3])), Block block_0 (torch.Size([64, 384, 3])).
I0315 19:17:16.124618 140066777728192 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 23.block_0 (torch.Size([0])) for Parameter 23 (torch.Size([64, 32, 2, 2])), Block block_0 (torch.Size([64, 128])).
I0315 19:17:16.124652 140198727365824 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 15.block_0 (torch.Size([0])) for Parameter 15 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 19:17:16.124700 140066777728192 shampoo_preconditioner_list.py:375] Rank 0: AdaGradPreconditionerList Numel Breakdown: (0, 0, 0, 0, 0, 0, 0, 0, 0, 2359296, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)
I0315 19:17:16.124713 140198727365824 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 16.block_0 (torch.Size([0])) for Parameter 16 (torch.Size([32, 64, 3, 3])), Block block_0 (torch.Size([32, 576])).
I0315 19:17:16.124742 140066777728192 shampoo_preconditioner_list.py:378] Rank 0: AdaGradPreconditionerList Bytes Breakdown: (0, 0, 0, 0, 0, 0, 0, 0, 0, 9437184, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)
I0315 19:17:16.124740 140286885487808 logger_utils.py:262] Unable to record workload.train_mean information. Continuing without it.
I0315 19:17:16.124766 140198727365824 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 17.block_0 (torch.Size([0])) for Parameter 17 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 19:17:16.124779 140066777728192 shampoo_preconditioner_list.py:381] Rank 0: AdaGradPreconditionerList Total Elements: 2359296
I0315 19:17:16.124809 140066777728192 shampoo_preconditioner_list.py:384] Rank 0: AdaGradPreconditionerList Total Bytes: 9437184
I0315 19:17:16.124815 140286885487808 logger_utils.py:262] Unable to record workload.train_stddev information. Continuing without it.
I0315 19:17:16.124825 140198727365824 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 18.block_0 (torch.Size([0])) for Parameter 18 (torch.Size([1, 32, 1, 1])), Block block_0 (torch.Size([32])).
I0315 19:17:16.124815 140571517158592 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 5.block_0 (torch.Size([128, 384, 3])) for Parameter 5 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 19:17:16.124876 140198727365824 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 19.block_0 (torch.Size([0])) for Parameter 19 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0315 19:17:16.124935 140198727365824 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 20.block_0 (torch.Size([0])) for Parameter 20 (torch.Size([512, 256, 2, 2])), Block block_0 (torch.Size([512, 1024])).
I0315 19:17:16.124952 140571517158592 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 6.block_0 (torch.Size([256, 384, 3])) for Parameter 6 (torch.Size([256, 128, 3, 3])), Block block_0 (torch.Size([256, 384, 3])).
I0315 19:17:16.124993 140198727365824 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 21.block_0 (torch.Size([0])) for Parameter 21 (torch.Size([256, 128, 2, 2])), Block block_0 (torch.Size([256, 512])).
I0315 19:17:16.125043 140571517158592 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 7.block_0 (torch.Size([0])) for Parameter 7 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 19:17:16.125049 140198727365824 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 22.block_0 (torch.Size([0])) for Parameter 22 (torch.Size([128, 64, 2, 2])), Block block_0 (torch.Size([128, 256])).
I0315 19:17:16.125108 140571517158592 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 8.block_0 (torch.Size([0])) for Parameter 8 (torch.Size([512, 256, 3, 3])), Block block_0 (torch.Size([512, 768, 3])).
I0315 19:17:16.125135 140198727365824 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 23.block_0 (torch.Size([64, 128])) for Parameter 23 (torch.Size([64, 32, 2, 2])), Block block_0 (torch.Size([64, 128])).
I0315 19:17:16.125169 140571517158592 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 9.block_0 (torch.Size([0])) for Parameter 9 (torch.Size([512, 512, 3, 3])), Block block_0 (torch.Size([512, 512, 9])).
I0315 19:17:16.125194 140282512307392 logger_utils.py:262] Unable to record workload.train_mean information. Continuing without it.
I0315 19:17:16.125221 140198727365824 shampoo_preconditioner_list.py:375] Rank 7: AdaGradPreconditionerList Numel Breakdown: (0, 9216, 0, 0, 73728, 0, 0, 0, 0, 0, 0, 0, 294912, 147456, 73728, 0, 0, 0, 0, 0, 0, 0, 0, 8192)
I0315 19:17:16.125227 140571517158592 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 10.block_0 (torch.Size([0])) for Parameter 10 (torch.Size([256, 512, 3, 3])), Block block_0 (torch.Size([256, 512, 9])).
I0315 19:17:16.125260 140198727365824 shampoo_preconditioner_list.py:378] Rank 7: AdaGradPreconditionerList Bytes Breakdown: (0, 36864, 0, 0, 294912, 0, 0, 0, 0, 0, 0, 0, 1179648, 589824, 294912, 0, 0, 0, 0, 0, 0, 0, 0, 32768)
I0315 19:17:16.125273 140282512307392 logger_utils.py:262] Unable to record workload.train_stddev information. Continuing without it.
I0315 19:17:16.125278 140571517158592 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 11.block_0 (torch.Size([0])) for Parameter 11 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 19:17:16.125297 140198727365824 shampoo_preconditioner_list.py:381] Rank 7: AdaGradPreconditionerList Total Elements: 607232
I0315 19:17:16.125329 140571517158592 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 12.block_0 (torch.Size([0])) for Parameter 12 (torch.Size([128, 256, 3, 3])), Block block_0 (torch.Size([128, 768, 3])).
I0315 19:17:16.125333 140198727365824 shampoo_preconditioner_list.py:384] Rank 7: AdaGradPreconditionerList Total Bytes: 2428928
I0315 19:17:16.125385 140571517158592 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 13.block_0 (torch.Size([0])) for Parameter 13 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 19:17:16.125496 139647719654592 logger_utils.py:262] Unable to record workload.train_mean information. Continuing without it.
I0315 19:17:16.125510 139802597225664 logger_utils.py:262] Unable to record workload.train_mean information. Continuing without it.
I0315 19:17:16.125573 139647719654592 logger_utils.py:262] Unable to record workload.train_stddev information. Continuing without it.
I0315 19:17:16.125578 139802597225664 logger_utils.py:262] Unable to record workload.train_stddev information. Continuing without it.
I0315 19:17:16.125557 140571517158592 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 14.block_0 (torch.Size([0])) for Parameter 14 (torch.Size([64, 128, 3, 3])), Block block_0 (torch.Size([64, 384, 3])).
I0315 19:17:16.125641 140066777728192 submission.py:142] No large parameters detected! Continuing with only Shampoo....
I0315 19:17:16.125757 140571517158592 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 15.block_0 (torch.Size([0])) for Parameter 15 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 19:17:16.125857 140571517158592 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 16.block_0 (torch.Size([0])) for Parameter 16 (torch.Size([32, 64, 3, 3])), Block block_0 (torch.Size([32, 576])).
I0315 19:17:16.125900 140066777728192 submission_runner.py:279] Initializing metrics bundle.
I0315 19:17:16.125924 140571517158592 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 17.block_0 (torch.Size([0])) for Parameter 17 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 19:17:16.126036 140571517158592 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 18.block_0 (torch.Size([32])) for Parameter 18 (torch.Size([1, 32, 1, 1])), Block block_0 (torch.Size([32])).
I0315 19:17:16.126042 140066777728192 submission_runner.py:301] Initializing checkpoint and logger.
I0315 19:17:16.126139 140571517158592 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 19.block_0 (torch.Size([1])) for Parameter 19 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0315 19:17:16.126218 140571517158592 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 20.block_0 (torch.Size([0])) for Parameter 20 (torch.Size([512, 256, 2, 2])), Block block_0 (torch.Size([512, 1024])).
I0315 19:17:16.126310 140571517158592 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 21.block_0 (torch.Size([256, 512])) for Parameter 21 (torch.Size([256, 128, 2, 2])), Block block_0 (torch.Size([256, 512])).
I0315 19:17:16.126399 140571517158592 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 22.block_0 (torch.Size([128, 256])) for Parameter 22 (torch.Size([128, 64, 2, 2])), Block block_0 (torch.Size([128, 256])).
I0315 19:17:16.126482 140571517158592 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 23.block_0 (torch.Size([0])) for Parameter 23 (torch.Size([64, 32, 2, 2])), Block block_0 (torch.Size([64, 128])).
I0315 19:17:16.126477 140066777728192 submission_runner.py:321] Saving meta data to /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_2/fastmri_pytorch/trial_5/meta_data_0.json.
I0315 19:17:16.126570 140571517158592 shampoo_preconditioner_list.py:375] Rank 6: AdaGradPreconditionerList Numel Breakdown: (288, 0, 0, 0, 0, 147456, 294912, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 32, 1, 0, 131072, 32768, 0)
I0315 19:17:16.126627 140571517158592 shampoo_preconditioner_list.py:378] Rank 6: AdaGradPreconditionerList Bytes Breakdown: (1152, 0, 0, 0, 0, 589824, 1179648, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 128, 4, 0, 524288, 131072, 0)
I0315 19:17:16.126662 140571517158592 shampoo_preconditioner_list.py:381] Rank 6: AdaGradPreconditionerList Total Elements: 606529
I0315 19:17:16.126643 140066777728192 logger_utils.py:262] Unable to record workload.train_mean information. Continuing without it.
I0315 19:17:16.126694 140571517158592 shampoo_preconditioner_list.py:384] Rank 6: AdaGradPreconditionerList Total Bytes: 2426116
I0315 19:17:16.126698 140066777728192 logger_utils.py:262] Unable to record workload.train_stddev information. Continuing without it.
I0315 19:17:16.126967 140198727365824 logger_utils.py:262] Unable to record workload.train_mean information. Continuing without it.
I0315 19:17:16.127045 140198727365824 logger_utils.py:262] Unable to record workload.train_stddev information. Continuing without it.
I0315 19:17:16.128401 140571517158592 logger_utils.py:262] Unable to record workload.train_mean information. Continuing without it.
I0315 19:17:16.128486 140571517158592 logger_utils.py:262] Unable to record workload.train_stddev information. Continuing without it.
I0315 19:17:16.935092 140066777728192 submission_runner.py:325] Saving flags to /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_2/fastmri_pytorch/trial_5/flags_0.json.
I0315 19:17:17.031418 140066777728192 submission_runner.py:337] Starting training loop.
[rank7]:W0315 19:17:17.187000 51 site-packages/torch/_logging/_internal.py:1081] [0/0] Profiler function <class 'torch.autograd.profiler.record_function'> will be ignored
[rank6]:W0315 19:17:17.187000 50 site-packages/torch/_logging/_internal.py:1081] [0/0] Profiler function <class 'torch.autograd.profiler.record_function'> will be ignored
[rank4]:W0315 19:17:17.187000 48 site-packages/torch/_logging/_internal.py:1081] [0/0] Profiler function <class 'torch.autograd.profiler.record_function'> will be ignored
[rank1]:W0315 19:17:17.187000 45 site-packages/torch/_logging/_internal.py:1081] [0/0] Profiler function <class 'torch.autograd.profiler.record_function'> will be ignored
[rank2]:W0315 19:17:17.187000 46 site-packages/torch/_logging/_internal.py:1081] [0/0] Profiler function <class 'torch.autograd.profiler.record_function'> will be ignored
[rank3]:W0315 19:17:17.187000 47 site-packages/torch/_logging/_internal.py:1081] [0/0] Profiler function <class 'torch.autograd.profiler.record_function'> will be ignored
[rank5]:W0315 19:17:17.187000 49 site-packages/torch/_logging/_internal.py:1081] [0/0] Profiler function <class 'torch.autograd.profiler.record_function'> will be ignored
[rank0]:W0315 19:21:12.875000 44 site-packages/torch/_logging/_internal.py:1081] [0/0] Profiler function <class 'torch.autograd.profiler.record_function'> will be ignored
[rank1]:W0315 19:21:59.010000 45 site-packages/torch/_dynamo/convert_frame.py:844] [2/8] torch._dynamo hit config.cache_size_limit (8)
[rank1]:W0315 19:21:59.010000 45 site-packages/torch/_dynamo/convert_frame.py:844] [2/8]    function: 'forward' (/algorithmic-efficiency/algoperf/workloads/fastmri/fastmri_pytorch/models.py:141)
[rank1]:W0315 19:21:59.010000 45 site-packages/torch/_dynamo/convert_frame.py:844] [2/8]    last reason: 2/0: tensor 'L['x']' requires_grad mismatch. expected requires_grad=0
[rank1]:W0315 19:21:59.010000 45 site-packages/torch/_dynamo/convert_frame.py:844] [2/8] To log all recompilation reasons, use TORCH_LOGS="recompiles".
[rank1]:W0315 19:21:59.010000 45 site-packages/torch/_dynamo/convert_frame.py:844] [2/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.
[rank3]:W0315 19:21:59.030000 47 site-packages/torch/_dynamo/convert_frame.py:844] [2/8] torch._dynamo hit config.cache_size_limit (8)
[rank3]:W0315 19:21:59.030000 47 site-packages/torch/_dynamo/convert_frame.py:844] [2/8]    function: 'forward' (/algorithmic-efficiency/algoperf/workloads/fastmri/fastmri_pytorch/models.py:141)
[rank3]:W0315 19:21:59.030000 47 site-packages/torch/_dynamo/convert_frame.py:844] [2/8]    last reason: 2/0: tensor 'L['x']' requires_grad mismatch. expected requires_grad=0
[rank3]:W0315 19:21:59.030000 47 site-packages/torch/_dynamo/convert_frame.py:844] [2/8] To log all recompilation reasons, use TORCH_LOGS="recompiles".
[rank3]:W0315 19:21:59.030000 47 site-packages/torch/_dynamo/convert_frame.py:844] [2/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.
[rank7]:W0315 19:21:59.030000 51 site-packages/torch/_dynamo/convert_frame.py:844] [2/8] torch._dynamo hit config.cache_size_limit (8)
[rank7]:W0315 19:21:59.030000 51 site-packages/torch/_dynamo/convert_frame.py:844] [2/8]    function: 'forward' (/algorithmic-efficiency/algoperf/workloads/fastmri/fastmri_pytorch/models.py:141)
[rank7]:W0315 19:21:59.030000 51 site-packages/torch/_dynamo/convert_frame.py:844] [2/8]    last reason: 2/0: tensor 'L['x']' requires_grad mismatch. expected requires_grad=0
[rank7]:W0315 19:21:59.030000 51 site-packages/torch/_dynamo/convert_frame.py:844] [2/8] To log all recompilation reasons, use TORCH_LOGS="recompiles".
[rank7]:W0315 19:21:59.030000 51 site-packages/torch/_dynamo/convert_frame.py:844] [2/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.
[rank2]:W0315 19:21:59.030000 46 site-packages/torch/_dynamo/convert_frame.py:844] [2/8] torch._dynamo hit config.cache_size_limit (8)
[rank2]:W0315 19:21:59.030000 46 site-packages/torch/_dynamo/convert_frame.py:844] [2/8]    function: 'forward' (/algorithmic-efficiency/algoperf/workloads/fastmri/fastmri_pytorch/models.py:141)
[rank2]:W0315 19:21:59.030000 46 site-packages/torch/_dynamo/convert_frame.py:844] [2/8]    last reason: 2/0: tensor 'L['x']' requires_grad mismatch. expected requires_grad=0
[rank2]:W0315 19:21:59.030000 46 site-packages/torch/_dynamo/convert_frame.py:844] [2/8] To log all recompilation reasons, use TORCH_LOGS="recompiles".
[rank2]:W0315 19:21:59.030000 46 site-packages/torch/_dynamo/convert_frame.py:844] [2/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.
[rank4]:W0315 19:21:59.042000 48 site-packages/torch/_dynamo/convert_frame.py:844] [2/8] torch._dynamo hit config.cache_size_limit (8)
[rank4]:W0315 19:21:59.042000 48 site-packages/torch/_dynamo/convert_frame.py:844] [2/8]    function: 'forward' (/algorithmic-efficiency/algoperf/workloads/fastmri/fastmri_pytorch/models.py:141)
[rank4]:W0315 19:21:59.042000 48 site-packages/torch/_dynamo/convert_frame.py:844] [2/8]    last reason: 2/0: tensor 'L['x']' requires_grad mismatch. expected requires_grad=0
[rank4]:W0315 19:21:59.042000 48 site-packages/torch/_dynamo/convert_frame.py:844] [2/8] To log all recompilation reasons, use TORCH_LOGS="recompiles".
[rank4]:W0315 19:21:59.042000 48 site-packages/torch/_dynamo/convert_frame.py:844] [2/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.
[rank6]:W0315 19:21:59.045000 50 site-packages/torch/_dynamo/convert_frame.py:844] [2/8] torch._dynamo hit config.cache_size_limit (8)
[rank6]:W0315 19:21:59.045000 50 site-packages/torch/_dynamo/convert_frame.py:844] [2/8]    function: 'forward' (/algorithmic-efficiency/algoperf/workloads/fastmri/fastmri_pytorch/models.py:141)
[rank6]:W0315 19:21:59.045000 50 site-packages/torch/_dynamo/convert_frame.py:844] [2/8]    last reason: 2/0: tensor 'L['x']' requires_grad mismatch. expected requires_grad=0
[rank6]:W0315 19:21:59.045000 50 site-packages/torch/_dynamo/convert_frame.py:844] [2/8] To log all recompilation reasons, use TORCH_LOGS="recompiles".
[rank6]:W0315 19:21:59.045000 50 site-packages/torch/_dynamo/convert_frame.py:844] [2/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.
[rank5]:W0315 19:21:59.165000 49 site-packages/torch/_dynamo/convert_frame.py:844] [2/8] torch._dynamo hit config.cache_size_limit (8)
[rank5]:W0315 19:21:59.165000 49 site-packages/torch/_dynamo/convert_frame.py:844] [2/8]    function: 'forward' (/algorithmic-efficiency/algoperf/workloads/fastmri/fastmri_pytorch/models.py:141)
[rank5]:W0315 19:21:59.165000 49 site-packages/torch/_dynamo/convert_frame.py:844] [2/8]    last reason: 2/0: tensor 'L['x']' requires_grad mismatch. expected requires_grad=0
[rank5]:W0315 19:21:59.165000 49 site-packages/torch/_dynamo/convert_frame.py:844] [2/8] To log all recompilation reasons, use TORCH_LOGS="recompiles".
[rank5]:W0315 19:21:59.165000 49 site-packages/torch/_dynamo/convert_frame.py:844] [2/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.
[rank0]:W0315 19:21:59.356000 44 site-packages/torch/_dynamo/convert_frame.py:844] [2/8] torch._dynamo hit config.cache_size_limit (8)
[rank0]:W0315 19:21:59.356000 44 site-packages/torch/_dynamo/convert_frame.py:844] [2/8]    function: 'forward' (/algorithmic-efficiency/algoperf/workloads/fastmri/fastmri_pytorch/models.py:141)
[rank0]:W0315 19:21:59.356000 44 site-packages/torch/_dynamo/convert_frame.py:844] [2/8]    last reason: 2/0: tensor 'L['x']' requires_grad mismatch. expected requires_grad=0
[rank0]:W0315 19:21:59.356000 44 site-packages/torch/_dynamo/convert_frame.py:844] [2/8] To log all recompilation reasons, use TORCH_LOGS="recompiles".
[rank0]:W0315 19:21:59.356000 44 site-packages/torch/_dynamo/convert_frame.py:844] [2/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.
I0315 19:22:13.148092 140039368279808 logging_writer.py:48] [0] global_step=0, grad_norm=3.70903, loss=0.940758
I0315 19:22:13.472625 140066777728192 submission.py:265] 0) loss = 0.941, grad_norm = 3.709
I0315 19:22:14.121313 140066777728192 spec.py:321] Evaluating on the training split.
I0315 19:29:46.427087 140066777728192 spec.py:333] Evaluating on the validation split.
I0315 19:33:50.667374 140066777728192 spec.py:349] Evaluating on the test split.
I0315 19:37:53.884749 140066777728192 submission_runner.py:469] Time since start: 1236.85s, 	Step: 1, 	{'train/ssim': 0.22055355140141078, 'train/loss': 0.9768894740513393, 'validation/ssim': 0.21004864333651344, 'validation/loss': 0.9812056370243036, 'validation/num_examples': 3554, 'test/ssim': 0.23479067310545237, 'test/loss': 0.9799439260594108, 'test/num_examples': 3581, 'score': 296.44219613075256, 'total_duration': 1236.8534758090973, 'accumulated_submission_time': 296.44219613075256, 'accumulated_eval_time': 939.7636005878448, 'accumulated_logging_time': 0}
I0315 19:37:53.893353 140020497250048 logging_writer.py:48] [1] accumulated_eval_time=939.764, accumulated_logging_time=0, accumulated_submission_time=296.442, global_step=1, preemption_count=0, score=296.442, test/loss=0.979944, test/num_examples=3581, test/ssim=0.234791, total_duration=1236.85, train/loss=0.976889, train/ssim=0.220554, validation/loss=0.981206, validation/num_examples=3554, validation/ssim=0.210049
I0315 19:37:55.048835 140020488857344 logging_writer.py:48] [1] global_step=1, grad_norm=3.45223, loss=0.956741
I0315 19:37:55.051887 140066777728192 submission.py:265] 1) loss = 0.957, grad_norm = 3.452
I0315 19:37:55.155640 140020497250048 logging_writer.py:48] [2] global_step=2, grad_norm=3.14195, loss=0.981707
I0315 19:37:55.163002 140066777728192 submission.py:265] 2) loss = 0.982, grad_norm = 3.142
I0315 19:37:55.239165 140020488857344 logging_writer.py:48] [3] global_step=3, grad_norm=3.47471, loss=0.949906
I0315 19:37:55.243641 140066777728192 submission.py:265] 3) loss = 0.950, grad_norm = 3.475
I0315 19:37:55.350395 140020497250048 logging_writer.py:48] [4] global_step=4, grad_norm=3.27247, loss=0.987676
I0315 19:37:55.354656 140066777728192 submission.py:265] 4) loss = 0.988, grad_norm = 3.272
I0315 19:37:55.438537 140020488857344 logging_writer.py:48] [5] global_step=5, grad_norm=3.24804, loss=0.952647
I0315 19:37:55.446269 140066777728192 submission.py:265] 5) loss = 0.953, grad_norm = 3.248
I0315 19:37:55.535485 140020497250048 logging_writer.py:48] [6] global_step=6, grad_norm=3.60292, loss=0.941481
I0315 19:37:55.539502 140066777728192 submission.py:265] 6) loss = 0.941, grad_norm = 3.603
I0315 19:37:55.607311 140020488857344 logging_writer.py:48] [7] global_step=7, grad_norm=3.42563, loss=0.966402
I0315 19:37:55.615175 140066777728192 submission.py:265] 7) loss = 0.966, grad_norm = 3.426
I0315 19:37:55.704478 140020497250048 logging_writer.py:48] [8] global_step=8, grad_norm=3.43974, loss=1.03156
I0315 19:37:55.708788 140066777728192 submission.py:265] 8) loss = 1.032, grad_norm = 3.440
I0315 19:37:55.792679 140020488857344 logging_writer.py:48] [9] global_step=9, grad_norm=3.15152, loss=0.962938
I0315 19:37:55.798946 140066777728192 submission.py:265] 9) loss = 0.963, grad_norm = 3.152
I0315 19:37:55.890609 140020497250048 logging_writer.py:48] [10] global_step=10, grad_norm=3.71598, loss=0.926362
I0315 19:37:55.894924 140066777728192 submission.py:265] 10) loss = 0.926, grad_norm = 3.716
I0315 19:37:55.988659 140020488857344 logging_writer.py:48] [11] global_step=11, grad_norm=3.38798, loss=0.910432
I0315 19:37:55.993171 140066777728192 submission.py:265] 11) loss = 0.910, grad_norm = 3.388
I0315 19:37:56.085164 140020497250048 logging_writer.py:48] [12] global_step=12, grad_norm=3.54947, loss=0.939621
I0315 19:37:56.089739 140066777728192 submission.py:265] 12) loss = 0.940, grad_norm = 3.549
I0315 19:37:56.171706 140020488857344 logging_writer.py:48] [13] global_step=13, grad_norm=3.14735, loss=0.990842
I0315 19:37:56.176781 140066777728192 submission.py:265] 13) loss = 0.991, grad_norm = 3.147
I0315 19:37:56.257514 140020497250048 logging_writer.py:48] [14] global_step=14, grad_norm=2.98814, loss=0.865676
I0315 19:37:56.262816 140066777728192 submission.py:265] 14) loss = 0.866, grad_norm = 2.988
I0315 19:37:56.340759 140020488857344 logging_writer.py:48] [15] global_step=15, grad_norm=3.45573, loss=0.893435
I0315 19:37:56.345949 140066777728192 submission.py:265] 15) loss = 0.893, grad_norm = 3.456
I0315 19:37:56.424733 140020497250048 logging_writer.py:48] [16] global_step=16, grad_norm=3.24153, loss=0.931777
I0315 19:37:56.431246 140066777728192 submission.py:265] 16) loss = 0.932, grad_norm = 3.242
I0315 19:37:56.497284 140020488857344 logging_writer.py:48] [17] global_step=17, grad_norm=3.09261, loss=0.924899
I0315 19:37:56.502036 140066777728192 submission.py:265] 17) loss = 0.925, grad_norm = 3.093
I0315 19:37:56.600924 140020497250048 logging_writer.py:48] [18] global_step=18, grad_norm=3.03422, loss=0.900042
I0315 19:37:56.604997 140066777728192 submission.py:265] 18) loss = 0.900, grad_norm = 3.034
I0315 19:37:56.679813 140020488857344 logging_writer.py:48] [19] global_step=19, grad_norm=3.09858, loss=0.899225
I0315 19:37:56.687387 140066777728192 submission.py:265] 19) loss = 0.899, grad_norm = 3.099
I0315 19:37:56.757726 140020497250048 logging_writer.py:48] [20] global_step=20, grad_norm=2.9632, loss=0.925952
I0315 19:37:56.763045 140066777728192 submission.py:265] 20) loss = 0.926, grad_norm = 2.963
I0315 19:37:56.825577 140020488857344 logging_writer.py:48] [21] global_step=21, grad_norm=2.98894, loss=0.818852
I0315 19:37:56.832288 140066777728192 submission.py:265] 21) loss = 0.819, grad_norm = 2.989
I0315 19:37:56.910240 140020497250048 logging_writer.py:48] [22] global_step=22, grad_norm=3.19137, loss=0.816667
I0315 19:37:56.915275 140066777728192 submission.py:265] 22) loss = 0.817, grad_norm = 3.191
I0315 19:37:57.008754 140020488857344 logging_writer.py:48] [23] global_step=23, grad_norm=2.97522, loss=0.791542
I0315 19:37:57.014369 140066777728192 submission.py:265] 23) loss = 0.792, grad_norm = 2.975
I0315 19:37:57.103082 140020497250048 logging_writer.py:48] [24] global_step=24, grad_norm=2.99717, loss=0.805713
I0315 19:37:57.108928 140066777728192 submission.py:265] 24) loss = 0.806, grad_norm = 2.997
I0315 19:37:57.194248 140020488857344 logging_writer.py:48] [25] global_step=25, grad_norm=2.83916, loss=0.783369
I0315 19:37:57.198587 140066777728192 submission.py:265] 25) loss = 0.783, grad_norm = 2.839
I0315 19:37:57.269913 140020497250048 logging_writer.py:48] [26] global_step=26, grad_norm=2.73924, loss=0.829602
I0315 19:37:57.274480 140066777728192 submission.py:265] 26) loss = 0.830, grad_norm = 2.739
I0315 19:37:57.365900 140020488857344 logging_writer.py:48] [27] global_step=27, grad_norm=2.84965, loss=0.88121
I0315 19:37:57.371847 140066777728192 submission.py:265] 27) loss = 0.881, grad_norm = 2.850
I0315 19:37:57.449445 140020497250048 logging_writer.py:48] [28] global_step=28, grad_norm=2.94888, loss=0.800624
I0315 19:37:57.461231 140066777728192 submission.py:265] 28) loss = 0.801, grad_norm = 2.949
I0315 19:37:57.539231 140020488857344 logging_writer.py:48] [29] global_step=29, grad_norm=2.52557, loss=0.767318
I0315 19:37:57.550493 140066777728192 submission.py:265] 29) loss = 0.767, grad_norm = 2.526
I0315 19:37:57.623597 140020497250048 logging_writer.py:48] [30] global_step=30, grad_norm=2.7626, loss=0.726937
I0315 19:37:57.628729 140066777728192 submission.py:265] 30) loss = 0.727, grad_norm = 2.763
I0315 19:37:57.711845 140020488857344 logging_writer.py:48] [31] global_step=31, grad_norm=2.26284, loss=0.769921
I0315 19:37:57.717411 140066777728192 submission.py:265] 31) loss = 0.770, grad_norm = 2.263
I0315 19:37:57.802650 140020497250048 logging_writer.py:48] [32] global_step=32, grad_norm=2.74713, loss=0.755158
I0315 19:37:57.808651 140066777728192 submission.py:265] 32) loss = 0.755, grad_norm = 2.747
I0315 19:37:57.889440 140020488857344 logging_writer.py:48] [33] global_step=33, grad_norm=2.67678, loss=0.742896
I0315 19:37:57.893961 140066777728192 submission.py:265] 33) loss = 0.743, grad_norm = 2.677
I0315 19:37:57.974566 140020497250048 logging_writer.py:48] [34] global_step=34, grad_norm=2.70508, loss=0.732523
I0315 19:37:57.979866 140066777728192 submission.py:265] 34) loss = 0.733, grad_norm = 2.705
I0315 19:37:58.067220 140020488857344 logging_writer.py:48] [35] global_step=35, grad_norm=2.7422, loss=0.701232
I0315 19:37:58.071706 140066777728192 submission.py:265] 35) loss = 0.701, grad_norm = 2.742
I0315 19:37:58.157496 140020497250048 logging_writer.py:48] [36] global_step=36, grad_norm=2.71674, loss=0.666526
I0315 19:37:58.162062 140066777728192 submission.py:265] 36) loss = 0.667, grad_norm = 2.717
I0315 19:37:58.247604 140020488857344 logging_writer.py:48] [37] global_step=37, grad_norm=2.38232, loss=0.711583
I0315 19:37:58.253275 140066777728192 submission.py:265] 37) loss = 0.712, grad_norm = 2.382
I0315 19:37:58.335734 140020497250048 logging_writer.py:48] [38] global_step=38, grad_norm=2.44927, loss=0.596248
I0315 19:37:58.341501 140066777728192 submission.py:265] 38) loss = 0.596, grad_norm = 2.449
I0315 19:37:58.421697 140020488857344 logging_writer.py:48] [39] global_step=39, grad_norm=2.41831, loss=0.629456
I0315 19:37:58.428587 140066777728192 submission.py:265] 39) loss = 0.629, grad_norm = 2.418
I0315 19:37:58.497848 140020497250048 logging_writer.py:48] [40] global_step=40, grad_norm=2.59239, loss=0.60445
I0315 19:37:58.502703 140066777728192 submission.py:265] 40) loss = 0.604, grad_norm = 2.592
I0315 19:37:58.582858 140020488857344 logging_writer.py:48] [41] global_step=41, grad_norm=2.62822, loss=0.616194
I0315 19:37:58.587739 140066777728192 submission.py:265] 41) loss = 0.616, grad_norm = 2.628
I0315 19:37:58.659135 140020497250048 logging_writer.py:48] [42] global_step=42, grad_norm=2.29734, loss=0.579577
I0315 19:37:58.663198 140066777728192 submission.py:265] 42) loss = 0.580, grad_norm = 2.297
I0315 19:37:58.746027 140020488857344 logging_writer.py:48] [43] global_step=43, grad_norm=2.10038, loss=0.628432
I0315 19:37:58.754305 140066777728192 submission.py:265] 43) loss = 0.628, grad_norm = 2.100
I0315 19:37:58.827607 140020497250048 logging_writer.py:48] [44] global_step=44, grad_norm=1.87618, loss=0.70299
I0315 19:37:58.832520 140066777728192 submission.py:265] 44) loss = 0.703, grad_norm = 1.876
I0315 19:37:58.909505 140020488857344 logging_writer.py:48] [45] global_step=45, grad_norm=2.08422, loss=0.556835
I0315 19:37:58.916504 140066777728192 submission.py:265] 45) loss = 0.557, grad_norm = 2.084
I0315 19:37:58.983736 140020497250048 logging_writer.py:48] [46] global_step=46, grad_norm=1.96738, loss=0.564209
I0315 19:37:58.991966 140066777728192 submission.py:265] 46) loss = 0.564, grad_norm = 1.967
I0315 19:37:59.067545 140020488857344 logging_writer.py:48] [47] global_step=47, grad_norm=1.78586, loss=0.585177
I0315 19:37:59.072764 140066777728192 submission.py:265] 47) loss = 0.585, grad_norm = 1.786
I0315 19:37:59.140202 140020497250048 logging_writer.py:48] [48] global_step=48, grad_norm=1.81041, loss=0.535508
I0315 19:37:59.148786 140066777728192 submission.py:265] 48) loss = 0.536, grad_norm = 1.810
I0315 19:37:59.216675 140020488857344 logging_writer.py:48] [49] global_step=49, grad_norm=1.86172, loss=0.516583
I0315 19:37:59.221477 140066777728192 submission.py:265] 49) loss = 0.517, grad_norm = 1.862
I0315 19:37:59.295974 140020497250048 logging_writer.py:48] [50] global_step=50, grad_norm=1.69848, loss=0.529637
I0315 19:37:59.301432 140066777728192 submission.py:265] 50) loss = 0.530, grad_norm = 1.698
I0315 19:37:59.373407 140020488857344 logging_writer.py:48] [51] global_step=51, grad_norm=1.59216, loss=0.579325
I0315 19:37:59.378695 140066777728192 submission.py:265] 51) loss = 0.579, grad_norm = 1.592
I0315 19:37:59.447875 140020497250048 logging_writer.py:48] [52] global_step=52, grad_norm=1.5552, loss=0.468524
I0315 19:37:59.452708 140066777728192 submission.py:265] 52) loss = 0.469, grad_norm = 1.555
I0315 19:37:59.594621 140020488857344 logging_writer.py:48] [53] global_step=53, grad_norm=1.50645, loss=0.492487
I0315 19:37:59.600255 140066777728192 submission.py:265] 53) loss = 0.492, grad_norm = 1.506
I0315 19:37:59.847008 140020497250048 logging_writer.py:48] [54] global_step=54, grad_norm=1.42833, loss=0.503187
I0315 19:37:59.856744 140066777728192 submission.py:265] 54) loss = 0.503, grad_norm = 1.428
I0315 19:38:00.038120 140020488857344 logging_writer.py:48] [55] global_step=55, grad_norm=1.20722, loss=0.553259
I0315 19:38:00.043322 140066777728192 submission.py:265] 55) loss = 0.553, grad_norm = 1.207
I0315 19:38:00.436201 140020497250048 logging_writer.py:48] [56] global_step=56, grad_norm=1.21439, loss=0.430763
I0315 19:38:00.443960 140066777728192 submission.py:265] 56) loss = 0.431, grad_norm = 1.214
I0315 19:38:00.783647 140020488857344 logging_writer.py:48] [57] global_step=57, grad_norm=1.35156, loss=0.44102
I0315 19:38:00.788537 140066777728192 submission.py:265] 57) loss = 0.441, grad_norm = 1.352
I0315 19:38:01.117380 140020497250048 logging_writer.py:48] [58] global_step=58, grad_norm=1.15598, loss=0.508267
I0315 19:38:01.121986 140066777728192 submission.py:265] 58) loss = 0.508, grad_norm = 1.156
I0315 19:38:01.429508 140020488857344 logging_writer.py:48] [59] global_step=59, grad_norm=1.02574, loss=0.487268
I0315 19:38:01.434448 140066777728192 submission.py:265] 59) loss = 0.487, grad_norm = 1.026
I0315 19:38:01.807818 140020497250048 logging_writer.py:48] [60] global_step=60, grad_norm=0.982318, loss=0.439592
I0315 19:38:01.812345 140066777728192 submission.py:265] 60) loss = 0.440, grad_norm = 0.982
I0315 19:38:02.260615 140020488857344 logging_writer.py:48] [61] global_step=61, grad_norm=0.967331, loss=0.553099
I0315 19:38:02.267389 140066777728192 submission.py:265] 61) loss = 0.553, grad_norm = 0.967
I0315 19:38:02.558411 140020497250048 logging_writer.py:48] [62] global_step=62, grad_norm=0.967221, loss=0.40451
I0315 19:38:02.563668 140066777728192 submission.py:265] 62) loss = 0.405, grad_norm = 0.967
I0315 19:38:02.970233 140020488857344 logging_writer.py:48] [63] global_step=63, grad_norm=0.945149, loss=0.405781
I0315 19:38:02.976329 140066777728192 submission.py:265] 63) loss = 0.406, grad_norm = 0.945
I0315 19:38:03.303262 140020497250048 logging_writer.py:48] [64] global_step=64, grad_norm=0.927385, loss=0.544259
I0315 19:38:03.310258 140066777728192 submission.py:265] 64) loss = 0.544, grad_norm = 0.927
I0315 19:38:03.466102 140020488857344 logging_writer.py:48] [65] global_step=65, grad_norm=0.879228, loss=0.544433
I0315 19:38:03.470045 140066777728192 submission.py:265] 65) loss = 0.544, grad_norm = 0.879
I0315 19:38:03.543837 140020497250048 logging_writer.py:48] [66] global_step=66, grad_norm=0.915108, loss=0.420224
I0315 19:38:03.550750 140066777728192 submission.py:265] 66) loss = 0.420, grad_norm = 0.915
I0315 19:38:03.813259 140020488857344 logging_writer.py:48] [67] global_step=67, grad_norm=0.899168, loss=0.452178
I0315 19:38:03.818027 140066777728192 submission.py:265] 67) loss = 0.452, grad_norm = 0.899
I0315 19:38:03.963842 140020497250048 logging_writer.py:48] [68] global_step=68, grad_norm=0.884768, loss=0.415115
I0315 19:38:03.969250 140066777728192 submission.py:265] 68) loss = 0.415, grad_norm = 0.885
I0315 19:38:04.112291 140020488857344 logging_writer.py:48] [69] global_step=69, grad_norm=0.945491, loss=0.409766
I0315 19:38:04.118540 140066777728192 submission.py:265] 69) loss = 0.410, grad_norm = 0.945
I0315 19:38:04.212270 140020497250048 logging_writer.py:48] [70] global_step=70, grad_norm=0.970766, loss=0.495114
I0315 19:38:04.215763 140066777728192 submission.py:265] 70) loss = 0.495, grad_norm = 0.971
I0315 19:38:04.347603 140020488857344 logging_writer.py:48] [71] global_step=71, grad_norm=0.938202, loss=0.387411
I0315 19:38:04.354132 140066777728192 submission.py:265] 71) loss = 0.387, grad_norm = 0.938
I0315 19:38:04.484248 140020497250048 logging_writer.py:48] [72] global_step=72, grad_norm=0.907872, loss=0.470553
I0315 19:38:04.489515 140066777728192 submission.py:265] 72) loss = 0.471, grad_norm = 0.908
I0315 19:38:04.621930 140020488857344 logging_writer.py:48] [73] global_step=73, grad_norm=1.01353, loss=0.377895
I0315 19:38:04.626930 140066777728192 submission.py:265] 73) loss = 0.378, grad_norm = 1.014
I0315 19:38:04.711383 140020497250048 logging_writer.py:48] [74] global_step=74, grad_norm=0.973691, loss=0.478117
I0315 19:38:04.715703 140066777728192 submission.py:265] 74) loss = 0.478, grad_norm = 0.974
I0315 19:38:04.812267 140020488857344 logging_writer.py:48] [75] global_step=75, grad_norm=0.993006, loss=0.408668
I0315 19:38:04.819687 140066777728192 submission.py:265] 75) loss = 0.409, grad_norm = 0.993
I0315 19:38:04.953821 140020497250048 logging_writer.py:48] [76] global_step=76, grad_norm=0.982195, loss=0.488823
I0315 19:38:04.960364 140066777728192 submission.py:265] 76) loss = 0.489, grad_norm = 0.982
I0315 19:38:05.030231 140020488857344 logging_writer.py:48] [77] global_step=77, grad_norm=1.03349, loss=0.406325
I0315 19:38:05.035161 140066777728192 submission.py:265] 77) loss = 0.406, grad_norm = 1.033
I0315 19:38:05.148344 140020497250048 logging_writer.py:48] [78] global_step=78, grad_norm=1.03421, loss=0.450787
I0315 19:38:05.154499 140066777728192 submission.py:265] 78) loss = 0.451, grad_norm = 1.034
I0315 19:38:05.272066 140020488857344 logging_writer.py:48] [79] global_step=79, grad_norm=1.0699, loss=0.418011
I0315 19:38:05.276532 140066777728192 submission.py:265] 79) loss = 0.418, grad_norm = 1.070
I0315 19:38:05.392319 140020497250048 logging_writer.py:48] [80] global_step=80, grad_norm=1.15361, loss=0.456429
I0315 19:38:05.397459 140066777728192 submission.py:265] 80) loss = 0.456, grad_norm = 1.154
I0315 19:38:05.540441 140020488857344 logging_writer.py:48] [81] global_step=81, grad_norm=1.0316, loss=0.417323
I0315 19:38:05.545833 140066777728192 submission.py:265] 81) loss = 0.417, grad_norm = 1.032
I0315 19:38:05.654294 140020497250048 logging_writer.py:48] [82] global_step=82, grad_norm=0.984052, loss=0.426931
I0315 19:38:05.660583 140066777728192 submission.py:265] 82) loss = 0.427, grad_norm = 0.984
I0315 19:38:05.750758 140020488857344 logging_writer.py:48] [83] global_step=83, grad_norm=1.03323, loss=0.366984
I0315 19:38:05.756316 140066777728192 submission.py:265] 83) loss = 0.367, grad_norm = 1.033
I0315 19:38:05.841071 140020497250048 logging_writer.py:48] [84] global_step=84, grad_norm=1.05723, loss=0.41787
I0315 19:38:05.847201 140066777728192 submission.py:265] 84) loss = 0.418, grad_norm = 1.057
I0315 19:38:05.927009 140020488857344 logging_writer.py:48] [85] global_step=85, grad_norm=0.943251, loss=0.401294
I0315 19:38:05.932179 140066777728192 submission.py:265] 85) loss = 0.401, grad_norm = 0.943
I0315 19:38:06.005390 140020497250048 logging_writer.py:48] [86] global_step=86, grad_norm=0.940118, loss=0.404441
I0315 19:38:06.010245 140066777728192 submission.py:265] 86) loss = 0.404, grad_norm = 0.940
I0315 19:38:06.155649 140020488857344 logging_writer.py:48] [87] global_step=87, grad_norm=1.06195, loss=0.376547
I0315 19:38:06.164371 140066777728192 submission.py:265] 87) loss = 0.377, grad_norm = 1.062
I0315 19:38:06.333975 140020497250048 logging_writer.py:48] [88] global_step=88, grad_norm=0.912753, loss=0.383693
I0315 19:38:06.339292 140066777728192 submission.py:265] 88) loss = 0.384, grad_norm = 0.913
I0315 19:38:06.578800 140020488857344 logging_writer.py:48] [89] global_step=89, grad_norm=0.895074, loss=0.412764
I0315 19:38:06.584164 140066777728192 submission.py:265] 89) loss = 0.413, grad_norm = 0.895
I0315 19:38:06.808871 140020497250048 logging_writer.py:48] [90] global_step=90, grad_norm=1.00078, loss=0.481872
I0315 19:38:06.814083 140066777728192 submission.py:265] 90) loss = 0.482, grad_norm = 1.001
I0315 19:38:07.075845 140020488857344 logging_writer.py:48] [91] global_step=91, grad_norm=0.835311, loss=0.388705
I0315 19:38:07.084091 140066777728192 submission.py:265] 91) loss = 0.389, grad_norm = 0.835
I0315 19:38:07.321263 140020497250048 logging_writer.py:48] [92] global_step=92, grad_norm=0.912242, loss=0.521058
I0315 19:38:07.325723 140066777728192 submission.py:265] 92) loss = 0.521, grad_norm = 0.912
I0315 19:38:07.572137 140020488857344 logging_writer.py:48] [93] global_step=93, grad_norm=0.999007, loss=0.345446
I0315 19:38:07.579130 140066777728192 submission.py:265] 93) loss = 0.345, grad_norm = 0.999
I0315 19:38:07.814001 140020497250048 logging_writer.py:48] [94] global_step=94, grad_norm=0.817244, loss=0.510264
I0315 19:38:07.819995 140066777728192 submission.py:265] 94) loss = 0.510, grad_norm = 0.817
I0315 19:38:08.150995 140020488857344 logging_writer.py:48] [95] global_step=95, grad_norm=0.710431, loss=0.351338
I0315 19:38:08.155887 140066777728192 submission.py:265] 95) loss = 0.351, grad_norm = 0.710
I0315 19:38:08.350630 140020497250048 logging_writer.py:48] [96] global_step=96, grad_norm=0.759052, loss=0.414138
I0315 19:38:08.354347 140066777728192 submission.py:265] 96) loss = 0.414, grad_norm = 0.759
I0315 19:38:08.667654 140020488857344 logging_writer.py:48] [97] global_step=97, grad_norm=0.726864, loss=0.488041
I0315 19:38:08.673176 140066777728192 submission.py:265] 97) loss = 0.488, grad_norm = 0.727
I0315 19:38:08.986713 140020497250048 logging_writer.py:48] [98] global_step=98, grad_norm=0.667613, loss=0.363495
I0315 19:38:08.991617 140066777728192 submission.py:265] 98) loss = 0.363, grad_norm = 0.668
I0315 19:38:11.440048 140020488857344 logging_writer.py:48] [99] global_step=99, grad_norm=0.699712, loss=0.436224
I0315 19:38:11.449221 140066777728192 submission.py:265] 99) loss = 0.436, grad_norm = 0.700
I0315 19:38:11.528178 140020497250048 logging_writer.py:48] [100] global_step=100, grad_norm=0.661772, loss=0.37208
I0315 19:38:11.535002 140066777728192 submission.py:265] 100) loss = 0.372, grad_norm = 0.662
I0315 19:39:16.541214 140066777728192 spec.py:321] Evaluating on the training split.
I0315 19:39:18.532232 140066777728192 spec.py:333] Evaluating on the validation split.
I0315 19:39:20.926884 140066777728192 spec.py:349] Evaluating on the test split.
I0315 19:39:23.155134 140066777728192 submission_runner.py:469] Time since start: 1326.12s, 	Step: 247, 	{'train/ssim': 0.6871802466256278, 'train/loss': 0.32192070143563406, 'validation/ssim': 0.6651276098454206, 'validation/loss': 0.33824474568707796, 'validation/num_examples': 3554, 'test/ssim': 0.6840749408771991, 'test/loss': 0.33948019795230033, 'test/num_examples': 3581, 'score': 377.5201585292816, 'total_duration': 1326.123896598816, 'accumulated_submission_time': 377.5201585292816, 'accumulated_eval_time': 946.3776602745056, 'accumulated_logging_time': 0.017385482788085938}
I0315 19:39:23.170259 140020488857344 logging_writer.py:48] [247] accumulated_eval_time=946.378, accumulated_logging_time=0.0173855, accumulated_submission_time=377.52, global_step=247, preemption_count=0, score=377.52, test/loss=0.33948, test/num_examples=3581, test/ssim=0.684075, total_duration=1326.12, train/loss=0.321921, train/ssim=0.68718, validation/loss=0.338245, validation/num_examples=3554, validation/ssim=0.665128
I0315 19:40:45.954710 140066777728192 spec.py:321] Evaluating on the training split.
I0315 19:40:47.888543 140066777728192 spec.py:333] Evaluating on the validation split.
I0315 19:40:50.367501 140066777728192 spec.py:349] Evaluating on the test split.
I0315 19:40:52.522379 140066777728192 submission_runner.py:469] Time since start: 1415.49s, 	Step: 292, 	{'train/ssim': 0.6966355187552316, 'train/loss': 0.31523081234523226, 'validation/ssim': 0.6748152664691193, 'validation/loss': 0.3311230042843451, 'validation/num_examples': 3554, 'test/ssim': 0.6933599204132924, 'test/loss': 0.33267361050509636, 'test/num_examples': 3581, 'score': 458.9091157913208, 'total_duration': 1415.491170167923, 'accumulated_submission_time': 458.9091157913208, 'accumulated_eval_time': 952.9454824924469, 'accumulated_logging_time': 0.04237723350524902}
I0315 19:40:52.533118 140020497250048 logging_writer.py:48] [292] accumulated_eval_time=952.945, accumulated_logging_time=0.0423772, accumulated_submission_time=458.909, global_step=292, preemption_count=0, score=458.909, test/loss=0.332674, test/num_examples=3581, test/ssim=0.69336, total_duration=1415.49, train/loss=0.315231, train/ssim=0.696636, validation/loss=0.331123, validation/num_examples=3554, validation/ssim=0.674815
I0315 19:42:14.612713 140066777728192 spec.py:321] Evaluating on the training split.
I0315 19:42:16.552134 140066777728192 spec.py:333] Evaluating on the validation split.
I0315 19:42:18.848432 140066777728192 spec.py:349] Evaluating on the test split.
I0315 19:42:20.927249 140066777728192 submission_runner.py:469] Time since start: 1503.90s, 	Step: 335, 	{'train/ssim': 0.7022774560110909, 'train/loss': 0.3099607058933803, 'validation/ssim': 0.6809610979705966, 'validation/loss': 0.32542282903066966, 'validation/num_examples': 3554, 'test/ssim': 0.6992722686793145, 'test/loss': 0.32718586648718934, 'test/num_examples': 3581, 'score': 539.6586167812347, 'total_duration': 1503.8959980010986, 'accumulated_submission_time': 539.6586167812347, 'accumulated_eval_time': 959.2601292133331, 'accumulated_logging_time': 0.06106853485107422}
I0315 19:42:20.938278 140020488857344 logging_writer.py:48] [335] accumulated_eval_time=959.26, accumulated_logging_time=0.0610685, accumulated_submission_time=539.659, global_step=335, preemption_count=0, score=539.659, test/loss=0.327186, test/num_examples=3581, test/ssim=0.699272, total_duration=1503.9, train/loss=0.309961, train/ssim=0.702277, validation/loss=0.325423, validation/num_examples=3554, validation/ssim=0.680961
I0315 19:43:45.540879 140066777728192 spec.py:321] Evaluating on the training split.
I0315 19:43:47.465330 140066777728192 spec.py:333] Evaluating on the validation split.
I0315 19:43:49.840357 140066777728192 spec.py:349] Evaluating on the test split.
I0315 19:43:51.970850 140066777728192 submission_runner.py:469] Time since start: 1594.94s, 	Step: 379, 	{'train/ssim': 0.706977094922747, 'train/loss': 0.3055340903145926, 'validation/ssim': 0.6859107503209412, 'validation/loss': 0.32067139496210256, 'validation/num_examples': 3554, 'test/ssim': 0.7039155765672996, 'test/loss': 0.3227066598235479, 'test/num_examples': 3581, 'score': 622.8703331947327, 'total_duration': 1594.9396224021912, 'accumulated_submission_time': 622.8703331947327, 'accumulated_eval_time': 965.6902973651886, 'accumulated_logging_time': 0.08345198631286621}
I0315 19:43:51.980761 140020497250048 logging_writer.py:48] [379] accumulated_eval_time=965.69, accumulated_logging_time=0.083452, accumulated_submission_time=622.87, global_step=379, preemption_count=0, score=622.87, test/loss=0.322707, test/num_examples=3581, test/ssim=0.703916, total_duration=1594.94, train/loss=0.305534, train/ssim=0.706977, validation/loss=0.320671, validation/num_examples=3554, validation/ssim=0.685911
I0315 19:45:13.825758 140066777728192 spec.py:321] Evaluating on the training split.
I0315 19:45:15.916250 140066777728192 spec.py:333] Evaluating on the validation split.
I0315 19:45:18.080633 140066777728192 spec.py:349] Evaluating on the test split.
I0315 19:45:20.230866 140066777728192 submission_runner.py:469] Time since start: 1683.20s, 	Step: 426, 	{'train/ssim': 0.7133266585213798, 'train/loss': 0.30159030641828266, 'validation/ssim': 0.6924917621430079, 'validation/loss': 0.31652299648063803, 'validation/num_examples': 3554, 'test/ssim': 0.7100473855068417, 'test/loss': 0.3188162608973576, 'test/num_examples': 3581, 'score': 703.2782878875732, 'total_duration': 1683.1996495723724, 'accumulated_submission_time': 703.2782878875732, 'accumulated_eval_time': 972.0957870483398, 'accumulated_logging_time': 0.10141587257385254}
I0315 19:45:20.241585 140020488857344 logging_writer.py:48] [426] accumulated_eval_time=972.096, accumulated_logging_time=0.101416, accumulated_submission_time=703.278, global_step=426, preemption_count=0, score=703.278, test/loss=0.318816, test/num_examples=3581, test/ssim=0.710047, total_duration=1683.2, train/loss=0.30159, train/ssim=0.713327, validation/loss=0.316523, validation/num_examples=3554, validation/ssim=0.692492
I0315 19:46:43.991375 140066777728192 spec.py:321] Evaluating on the training split.
I0315 19:46:46.038096 140066777728192 spec.py:333] Evaluating on the validation split.
I0315 19:46:48.300457 140066777728192 spec.py:349] Evaluating on the test split.
I0315 19:46:50.481891 140066777728192 submission_runner.py:469] Time since start: 1773.45s, 	Step: 467, 	{'train/ssim': 0.7155896595546177, 'train/loss': 0.29866320746285574, 'validation/ssim': 0.6948496357537282, 'validation/loss': 0.31335253439126687, 'validation/num_examples': 3554, 'test/ssim': 0.7123186228096202, 'test/loss': 0.31580939741648634, 'test/num_examples': 3581, 'score': 785.6607389450073, 'total_duration': 1773.4506480693817, 'accumulated_submission_time': 785.6607389450073, 'accumulated_eval_time': 978.5864672660828, 'accumulated_logging_time': 0.12050628662109375}
I0315 19:46:50.492186 140020497250048 logging_writer.py:48] [467] accumulated_eval_time=978.586, accumulated_logging_time=0.120506, accumulated_submission_time=785.661, global_step=467, preemption_count=0, score=785.661, test/loss=0.315809, test/num_examples=3581, test/ssim=0.712319, total_duration=1773.45, train/loss=0.298663, train/ssim=0.71559, validation/loss=0.313353, validation/num_examples=3554, validation/ssim=0.69485
I0315 19:47:51.794781 140020488857344 logging_writer.py:48] [500] global_step=500, grad_norm=0.146187, loss=0.354031
I0315 19:47:51.798323 140066777728192 submission.py:265] 500) loss = 0.354, grad_norm = 0.146
I0315 19:48:12.863032 140066777728192 spec.py:321] Evaluating on the training split.
I0315 19:48:14.824526 140066777728192 spec.py:333] Evaluating on the validation split.
I0315 19:48:16.919153 140066777728192 spec.py:349] Evaluating on the test split.
I0315 19:48:18.991933 140066777728192 submission_runner.py:469] Time since start: 1861.96s, 	Step: 510, 	{'train/ssim': 0.7178807939801898, 'train/loss': 0.29615061623709543, 'validation/ssim': 0.697465320215778, 'validation/loss': 0.31065723288240366, 'validation/num_examples': 3554, 'test/ssim': 0.7147783004049149, 'test/loss': 0.3130527422834927, 'test/num_examples': 3581, 'score': 866.7468185424805, 'total_duration': 1861.9607026576996, 'accumulated_submission_time': 866.7468185424805, 'accumulated_eval_time': 984.7154245376587, 'accumulated_logging_time': 0.1385667324066162}
I0315 19:48:19.002958 140020497250048 logging_writer.py:48] [510] accumulated_eval_time=984.715, accumulated_logging_time=0.138567, accumulated_submission_time=866.747, global_step=510, preemption_count=0, score=866.747, test/loss=0.313053, test/num_examples=3581, test/ssim=0.714778, total_duration=1861.96, train/loss=0.296151, train/ssim=0.717881, validation/loss=0.310657, validation/num_examples=3554, validation/ssim=0.697465
I0315 19:49:39.925582 140066777728192 spec.py:321] Evaluating on the training split.
I0315 19:49:41.913059 140066777728192 spec.py:333] Evaluating on the validation split.
I0315 19:49:44.070851 140066777728192 spec.py:349] Evaluating on the test split.
I0315 19:49:46.244889 140066777728192 submission_runner.py:469] Time since start: 1949.21s, 	Step: 550, 	{'train/ssim': 0.7203436579023089, 'train/loss': 0.29445838928222656, 'validation/ssim': 0.6997764814953221, 'validation/loss': 0.30925198170195556, 'validation/num_examples': 3554, 'test/ssim': 0.7171714375785395, 'test/loss': 0.31148396321296423, 'test/num_examples': 3581, 'score': 946.4454352855682, 'total_duration': 1949.213633298874, 'accumulated_submission_time': 946.4454352855682, 'accumulated_eval_time': 991.0348687171936, 'accumulated_logging_time': 0.1583108901977539}
I0315 19:49:46.255948 140020488857344 logging_writer.py:48] [550] accumulated_eval_time=991.035, accumulated_logging_time=0.158311, accumulated_submission_time=946.445, global_step=550, preemption_count=0, score=946.445, test/loss=0.311484, test/num_examples=3581, test/ssim=0.717171, total_duration=1949.21, train/loss=0.294458, train/ssim=0.720344, validation/loss=0.309252, validation/num_examples=3554, validation/ssim=0.699776
I0315 19:51:07.452581 140066777728192 spec.py:321] Evaluating on the training split.
I0315 19:51:09.469454 140066777728192 spec.py:333] Evaluating on the validation split.
I0315 19:51:12.109714 140066777728192 spec.py:349] Evaluating on the test split.
I0315 19:51:14.337945 140066777728192 submission_runner.py:469] Time since start: 2037.31s, 	Step: 594, 	{'train/ssim': 0.7228171484810966, 'train/loss': 0.2924823420388358, 'validation/ssim': 0.7023131671620005, 'validation/loss': 0.30727354274013435, 'validation/num_examples': 3554, 'test/ssim': 0.7196080032855696, 'test/loss': 0.3095075217947152, 'test/num_examples': 3581, 'score': 1026.307953119278, 'total_duration': 2037.306670665741, 'accumulated_submission_time': 1026.307953119278, 'accumulated_eval_time': 997.9203455448151, 'accumulated_logging_time': 0.17879509925842285}
I0315 19:51:14.348649 140020497250048 logging_writer.py:48] [594] accumulated_eval_time=997.92, accumulated_logging_time=0.178795, accumulated_submission_time=1026.31, global_step=594, preemption_count=0, score=1026.31, test/loss=0.309508, test/num_examples=3581, test/ssim=0.719608, total_duration=2037.31, train/loss=0.292482, train/ssim=0.722817, validation/loss=0.307274, validation/num_examples=3554, validation/ssim=0.702313
I0315 19:52:35.173727 140066777728192 spec.py:321] Evaluating on the training split.
I0315 19:52:37.133015 140066777728192 spec.py:333] Evaluating on the validation split.
I0315 19:52:39.540113 140066777728192 spec.py:349] Evaluating on the test split.
I0315 19:52:41.707790 140066777728192 submission_runner.py:469] Time since start: 2124.68s, 	Step: 638, 	{'train/ssim': 0.722550528390067, 'train/loss': 0.2910958358219692, 'validation/ssim': 0.7024947956967501, 'validation/loss': 0.30546546639789673, 'validation/num_examples': 3554, 'test/ssim': 0.7197104046312134, 'test/loss': 0.3077844248507749, 'test/num_examples': 3581, 'score': 1105.8310816287994, 'total_duration': 2124.676556825638, 'accumulated_submission_time': 1105.8310816287994, 'accumulated_eval_time': 1004.4546413421631, 'accumulated_logging_time': 0.1983494758605957}
I0315 19:52:41.719138 140020488857344 logging_writer.py:48] [638] accumulated_eval_time=1004.45, accumulated_logging_time=0.198349, accumulated_submission_time=1105.83, global_step=638, preemption_count=0, score=1105.83, test/loss=0.307784, test/num_examples=3581, test/ssim=0.71971, total_duration=2124.68, train/loss=0.291096, train/ssim=0.722551, validation/loss=0.305465, validation/num_examples=3554, validation/ssim=0.702495
I0315 19:54:02.718326 140066777728192 spec.py:321] Evaluating on the training split.
I0315 19:54:04.651466 140066777728192 spec.py:333] Evaluating on the validation split.
I0315 19:54:06.728824 140066777728192 spec.py:349] Evaluating on the test split.
I0315 19:54:08.772868 140066777728192 submission_runner.py:469] Time since start: 2211.74s, 	Step: 680, 	{'train/ssim': 0.7251602581569127, 'train/loss': 0.2896979536328997, 'validation/ssim': 0.7048416094761185, 'validation/loss': 0.30409906201639, 'validation/num_examples': 3554, 'test/ssim': 0.7220813843898353, 'test/loss': 0.30640647225591666, 'test/num_examples': 3581, 'score': 1183.0385553836823, 'total_duration': 2211.741660118103, 'accumulated_submission_time': 1183.0385553836823, 'accumulated_eval_time': 1010.5094039440155, 'accumulated_logging_time': 2.58011794090271}
I0315 19:54:08.783123 140020497250048 logging_writer.py:48] [680] accumulated_eval_time=1010.51, accumulated_logging_time=2.58012, accumulated_submission_time=1183.04, global_step=680, preemption_count=0, score=1183.04, test/loss=0.306406, test/num_examples=3581, test/ssim=0.722081, total_duration=2211.74, train/loss=0.289698, train/ssim=0.72516, validation/loss=0.304099, validation/num_examples=3554, validation/ssim=0.704842
I0315 19:55:30.537107 140066777728192 spec.py:321] Evaluating on the training split.
I0315 19:55:32.585289 140066777728192 spec.py:333] Evaluating on the validation split.
I0315 19:55:34.634157 140066777728192 spec.py:349] Evaluating on the test split.
I0315 19:55:36.679233 140066777728192 submission_runner.py:469] Time since start: 2299.65s, 	Step: 727, 	{'train/ssim': 0.7255374363490513, 'train/loss': 0.2886474813733782, 'validation/ssim': 0.7050717364017656, 'validation/loss': 0.30295117517234105, 'validation/num_examples': 3554, 'test/ssim': 0.7223228661250349, 'test/loss': 0.30526969460128106, 'test/num_examples': 3581, 'score': 1263.3923892974854, 'total_duration': 2299.6480371952057, 'accumulated_submission_time': 1263.3923892974854, 'accumulated_eval_time': 1016.6518099308014, 'accumulated_logging_time': 2.5981640815734863}
I0315 19:55:36.690274 140020488857344 logging_writer.py:48] [727] accumulated_eval_time=1016.65, accumulated_logging_time=2.59816, accumulated_submission_time=1263.39, global_step=727, preemption_count=0, score=1263.39, test/loss=0.30527, test/num_examples=3581, test/ssim=0.722323, total_duration=2299.65, train/loss=0.288647, train/ssim=0.725537, validation/loss=0.302951, validation/num_examples=3554, validation/ssim=0.705072
I0315 19:56:58.531770 140066777728192 spec.py:321] Evaluating on the training split.
I0315 19:57:00.557029 140066777728192 spec.py:333] Evaluating on the validation split.
I0315 19:57:02.638624 140066777728192 spec.py:349] Evaluating on the test split.
I0315 19:57:04.680340 140066777728192 submission_runner.py:469] Time since start: 2387.65s, 	Step: 769, 	{'train/ssim': 0.7268263271876744, 'train/loss': 0.2876706463950021, 'validation/ssim': 0.7066573452667769, 'validation/loss': 0.30190361679841377, 'validation/num_examples': 3554, 'test/ssim': 0.7238372061313181, 'test/loss': 0.3040889089116169, 'test/num_examples': 3581, 'score': 1343.9352555274963, 'total_duration': 2387.6491277217865, 'accumulated_submission_time': 1343.9352555274963, 'accumulated_eval_time': 1022.8006691932678, 'accumulated_logging_time': 2.6172313690185547}
I0315 19:57:04.690816 140020497250048 logging_writer.py:48] [769] accumulated_eval_time=1022.8, accumulated_logging_time=2.61723, accumulated_submission_time=1343.94, global_step=769, preemption_count=0, score=1343.94, test/loss=0.304089, test/num_examples=3581, test/ssim=0.723837, total_duration=2387.65, train/loss=0.287671, train/ssim=0.726826, validation/loss=0.301904, validation/num_examples=3554, validation/ssim=0.706657
I0315 19:58:25.299272 140066777728192 spec.py:321] Evaluating on the training split.
I0315 19:58:27.233057 140066777728192 spec.py:333] Evaluating on the validation split.
I0315 19:58:29.285554 140066777728192 spec.py:349] Evaluating on the test split.
I0315 19:58:31.343419 140066777728192 submission_runner.py:469] Time since start: 2474.31s, 	Step: 813, 	{'train/ssim': 0.7286319732666016, 'train/loss': 0.2866887365068708, 'validation/ssim': 0.7077479408105304, 'validation/loss': 0.30132754384364446, 'validation/num_examples': 3554, 'test/ssim': 0.7249458950013963, 'test/loss': 0.30357195937543635, 'test/num_examples': 3581, 'score': 1423.253351688385, 'total_duration': 2474.312212228775, 'accumulated_submission_time': 1423.253351688385, 'accumulated_eval_time': 1028.8449442386627, 'accumulated_logging_time': 2.640709638595581}
I0315 19:58:31.355565 140020488857344 logging_writer.py:48] [813] accumulated_eval_time=1028.84, accumulated_logging_time=2.64071, accumulated_submission_time=1423.25, global_step=813, preemption_count=0, score=1423.25, test/loss=0.303572, test/num_examples=3581, test/ssim=0.724946, total_duration=2474.31, train/loss=0.286689, train/ssim=0.728632, validation/loss=0.301328, validation/num_examples=3554, validation/ssim=0.707748
I0315 19:59:52.397350 140066777728192 spec.py:321] Evaluating on the training split.
I0315 19:59:54.362275 140066777728192 spec.py:333] Evaluating on the validation split.
I0315 19:59:56.690668 140066777728192 spec.py:349] Evaluating on the test split.
I0315 19:59:58.930800 140066777728192 submission_runner.py:469] Time since start: 2561.90s, 	Step: 856, 	{'train/ssim': 0.7295635768345424, 'train/loss': 0.28571297441210064, 'validation/ssim': 0.7090743649321187, 'validation/loss': 0.3002114626015581, 'validation/num_examples': 3554, 'test/ssim': 0.7261402137583776, 'test/loss': 0.30243804514058575, 'test/num_examples': 3581, 'score': 1503.073911190033, 'total_duration': 2561.8995871543884, 'accumulated_submission_time': 1503.073911190033, 'accumulated_eval_time': 1035.3785729408264, 'accumulated_logging_time': 2.6632027626037598}
I0315 19:59:58.941841 140020497250048 logging_writer.py:48] [856] accumulated_eval_time=1035.38, accumulated_logging_time=2.6632, accumulated_submission_time=1503.07, global_step=856, preemption_count=0, score=1503.07, test/loss=0.302438, test/num_examples=3581, test/ssim=0.72614, total_duration=2561.9, train/loss=0.285713, train/ssim=0.729564, validation/loss=0.300211, validation/num_examples=3554, validation/ssim=0.709074
I0315 20:01:21.880748 140066777728192 spec.py:321] Evaluating on the training split.
I0315 20:01:23.872362 140066777728192 spec.py:333] Evaluating on the validation split.
I0315 20:01:26.242602 140066777728192 spec.py:349] Evaluating on the test split.
I0315 20:01:28.357336 140066777728192 submission_runner.py:469] Time since start: 2651.33s, 	Step: 899, 	{'train/ssim': 0.7312474931989398, 'train/loss': 0.28482076099940706, 'validation/ssim': 0.7103371091551772, 'validation/loss': 0.2995338246737831, 'validation/num_examples': 3554, 'test/ssim': 0.7274653635288676, 'test/loss': 0.30167715950938984, 'test/num_examples': 3581, 'score': 1584.7094411849976, 'total_duration': 2651.3256406784058, 'accumulated_submission_time': 1584.7094411849976, 'accumulated_eval_time': 1041.8547542095184, 'accumulated_logging_time': 2.6917123794555664}
I0315 20:01:28.368921 140020488857344 logging_writer.py:48] [899] accumulated_eval_time=1041.85, accumulated_logging_time=2.69171, accumulated_submission_time=1584.71, global_step=899, preemption_count=0, score=1584.71, test/loss=0.301677, test/num_examples=3581, test/ssim=0.727465, total_duration=2651.33, train/loss=0.284821, train/ssim=0.731247, validation/loss=0.299534, validation/num_examples=3554, validation/ssim=0.710337
I0315 20:02:49.675018 140066777728192 spec.py:321] Evaluating on the training split.
I0315 20:02:51.609048 140066777728192 spec.py:333] Evaluating on the validation split.
I0315 20:02:54.520821 140066777728192 spec.py:349] Evaluating on the test split.
I0315 20:02:56.728098 140066777728192 submission_runner.py:469] Time since start: 2739.70s, 	Step: 942, 	{'train/ssim': 0.7319624764578683, 'train/loss': 0.2842667443411691, 'validation/ssim': 0.7111894031153277, 'validation/loss': 0.29877997008212576, 'validation/num_examples': 3554, 'test/ssim': 0.7282542356796285, 'test/loss': 0.30096266809637673, 'test/num_examples': 3581, 'score': 1664.8065073490143, 'total_duration': 2739.6968879699707, 'accumulated_submission_time': 1664.8065073490143, 'accumulated_eval_time': 1048.9080023765564, 'accumulated_logging_time': 2.711026191711426}
I0315 20:02:56.742301 140020497250048 logging_writer.py:48] [942] accumulated_eval_time=1048.91, accumulated_logging_time=2.71103, accumulated_submission_time=1664.81, global_step=942, preemption_count=0, score=1664.81, test/loss=0.300963, test/num_examples=3581, test/ssim=0.728254, total_duration=2739.7, train/loss=0.284267, train/ssim=0.731962, validation/loss=0.29878, validation/num_examples=3554, validation/ssim=0.711189
I0315 20:03:47.603169 140020488857344 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.0512921, loss=0.293253
I0315 20:03:47.615999 140066777728192 submission.py:265] 1000) loss = 0.293, grad_norm = 0.051
I0315 20:04:17.401883 140066777728192 spec.py:321] Evaluating on the training split.
I0315 20:04:19.332844 140066777728192 spec.py:333] Evaluating on the validation split.
I0315 20:04:21.777832 140066777728192 spec.py:349] Evaluating on the test split.
I0315 20:04:23.998351 140066777728192 submission_runner.py:469] Time since start: 2826.97s, 	Step: 1196, 	{'train/ssim': 0.7357939311436245, 'train/loss': 0.28074799265180317, 'validation/ssim': 0.714151583163337, 'validation/loss': 0.2959299683235439, 'validation/num_examples': 3554, 'test/ssim': 0.7312075123045239, 'test/loss': 0.29801747040587473, 'test/num_examples': 3581, 'score': 1744.1340277194977, 'total_duration': 2826.9671466350555, 'accumulated_submission_time': 1744.1340277194977, 'accumulated_eval_time': 1055.5046236515045, 'accumulated_logging_time': 2.7344465255737305}
I0315 20:04:24.009292 140020497250048 logging_writer.py:48] [1196] accumulated_eval_time=1055.5, accumulated_logging_time=2.73445, accumulated_submission_time=1744.13, global_step=1196, preemption_count=0, score=1744.13, test/loss=0.298017, test/num_examples=3581, test/ssim=0.731208, total_duration=2826.97, train/loss=0.280748, train/ssim=0.735794, validation/loss=0.29593, validation/num_examples=3554, validation/ssim=0.714152
I0315 20:04:42.940506 140020488857344 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.0530374, loss=0.268111
I0315 20:04:42.943900 140066777728192 submission.py:265] 1500) loss = 0.268, grad_norm = 0.053
/usr/local/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
I0315 20:05:12.788917 140020497250048 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.0671597, loss=0.233171
I0315 20:05:12.792009 140066777728192 submission.py:265] 2000) loss = 0.233, grad_norm = 0.067
I0315 20:05:42.630908 140020488857344 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.0881256, loss=0.326614
I0315 20:05:42.634176 140066777728192 submission.py:265] 2500) loss = 0.327, grad_norm = 0.088
I0315 20:05:44.685473 140066777728192 spec.py:321] Evaluating on the training split.
I0315 20:05:46.577795 140066777728192 spec.py:333] Evaluating on the validation split.
I0315 20:05:48.744814 140066777728192 spec.py:349] Evaluating on the test split.
I0315 20:05:50.850355 140066777728192 submission_runner.py:469] Time since start: 2913.82s, 	Step: 2525, 	{'train/ssim': 0.7419353212629046, 'train/loss': 0.2741779770169939, 'validation/ssim': 0.720409112092009, 'validation/loss': 0.28936042850045723, 'validation/num_examples': 3554, 'test/ssim': 0.7375877569987433, 'test/loss': 0.2908925661259076, 'test/num_examples': 3581, 'score': 1823.0307388305664, 'total_duration': 2913.819164276123, 'accumulated_submission_time': 1823.0307388305664, 'accumulated_eval_time': 1061.6697239875793, 'accumulated_logging_time': 2.753291606903076}
I0315 20:05:50.861023 140020497250048 logging_writer.py:48] [2525] accumulated_eval_time=1061.67, accumulated_logging_time=2.75329, accumulated_submission_time=1823.03, global_step=2525, preemption_count=0, score=1823.03, test/loss=0.290893, test/num_examples=3581, test/ssim=0.737588, total_duration=2913.82, train/loss=0.274178, train/ssim=0.741935, validation/loss=0.28936, validation/num_examples=3554, validation/ssim=0.720409
I0315 20:06:19.980456 140020488857344 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.0791447, loss=0.334815
I0315 20:06:19.983704 140066777728192 submission.py:265] 3000) loss = 0.335, grad_norm = 0.079
I0315 20:06:49.851500 140020497250048 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.132351, loss=0.203783
I0315 20:06:49.854716 140066777728192 submission.py:265] 3500) loss = 0.204, grad_norm = 0.132
I0315 20:07:11.570452 140066777728192 spec.py:321] Evaluating on the training split.
I0315 20:07:13.476818 140066777728192 spec.py:333] Evaluating on the validation split.
I0315 20:07:15.856868 140066777728192 spec.py:349] Evaluating on the test split.
I0315 20:07:17.926808 140066777728192 submission_runner.py:469] Time since start: 3000.90s, 	Step: 3854, 	{'train/ssim': 0.7444802692958287, 'train/loss': 0.2722600357873099, 'validation/ssim': 0.7229349440111494, 'validation/loss': 0.2875359169740521, 'validation/num_examples': 3554, 'test/ssim': 0.7401205199752164, 'test/loss': 0.2889454065837929, 'test/num_examples': 3581, 'score': 1901.8422586917877, 'total_duration': 3000.8955886363983, 'accumulated_submission_time': 1901.8422586917877, 'accumulated_eval_time': 1068.026228427887, 'accumulated_logging_time': 2.7728962898254395}
I0315 20:07:17.937717 140020488857344 logging_writer.py:48] [3854] accumulated_eval_time=1068.03, accumulated_logging_time=2.7729, accumulated_submission_time=1901.84, global_step=3854, preemption_count=0, score=1901.84, test/loss=0.288945, test/num_examples=3581, test/ssim=0.740121, total_duration=3000.9, train/loss=0.27226, train/ssim=0.74448, validation/loss=0.287536, validation/num_examples=3554, validation/ssim=0.722935
I0315 20:07:27.529903 140020497250048 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.050195, loss=0.271033
I0315 20:07:27.533174 140066777728192 submission.py:265] 4000) loss = 0.271, grad_norm = 0.050
I0315 20:07:57.425993 140020488857344 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.0714221, loss=0.323362
I0315 20:07:57.429261 140066777728192 submission.py:265] 4500) loss = 0.323, grad_norm = 0.071
I0315 20:08:27.307367 140020497250048 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.427948, loss=0.272582
I0315 20:08:27.310458 140066777728192 submission.py:265] 5000) loss = 0.273, grad_norm = 0.428
I0315 20:08:38.637177 140066777728192 spec.py:321] Evaluating on the training split.
I0315 20:08:40.543450 140066777728192 spec.py:333] Evaluating on the validation split.
I0315 20:08:42.911386 140066777728192 spec.py:349] Evaluating on the test split.
I0315 20:08:45.194731 140066777728192 submission_runner.py:469] Time since start: 3088.16s, 	Step: 5180, 	{'train/ssim': 0.745368208203997, 'train/loss': 0.27128916127341135, 'validation/ssim': 0.7236617329285664, 'validation/loss': 0.2865570703784468, 'validation/num_examples': 3554, 'test/ssim': 0.7409040061697152, 'test/loss': 0.28793840320441216, 'test/num_examples': 3581, 'score': 1980.5941214561462, 'total_duration': 3088.1635172367096, 'accumulated_submission_time': 1980.5941214561462, 'accumulated_eval_time': 1074.583905696869, 'accumulated_logging_time': 2.7918646335601807}
I0315 20:08:45.205947 140020488857344 logging_writer.py:48] [5180] accumulated_eval_time=1074.58, accumulated_logging_time=2.79186, accumulated_submission_time=1980.59, global_step=5180, preemption_count=0, score=1980.59, test/loss=0.287938, test/num_examples=3581, test/ssim=0.740904, total_duration=3088.16, train/loss=0.271289, train/ssim=0.745368, validation/loss=0.286557, validation/num_examples=3554, validation/ssim=0.723662
I0315 20:08:45.881089 140020497250048 logging_writer.py:48] [5180] global_step=5180, preemption_count=0, score=1980.59
I0315 20:08:47.647327 140066777728192 submission_runner.py:646] Tuning trial 5/5
I0315 20:08:47.647531 140066777728192 submission_runner.py:647] Hyperparameters: Hyperparameters(learning_rate=0.001308209823469072, one_minus_beta1=0.02686663061, one_minus_beta2=0.00187670778, epsilon=1e-08, one_minus_momentum=0.0, use_momentum=False, weight_decay=0.16375311233774334, max_preconditioner_dim=1024, precondition_frequency=100, start_preconditioning_step=-1, inv_root_override=0, exponent_multiplier=1.0, grafting_type='ADAM', grafting_epsilon=1e-08, use_normalized_grafting=False, communication_dtype='FP32', communicate_params=True, use_cosine_decay=True, warmup_factor=0.1, label_smoothing=0.1, dropout_rate=0.0, use_nadam=True, step_hint_factor=1.0)
I0315 20:08:47.648420 140066777728192 submission_runner.py:648] Metrics: {'eval_results': [(1, {'train/ssim': 0.22055355140141078, 'train/loss': 0.9768894740513393, 'validation/ssim': 0.21004864333651344, 'validation/loss': 0.9812056370243036, 'validation/num_examples': 3554, 'test/ssim': 0.23479067310545237, 'test/loss': 0.9799439260594108, 'test/num_examples': 3581, 'score': 296.44219613075256, 'total_duration': 1236.8534758090973, 'accumulated_submission_time': 296.44219613075256, 'accumulated_eval_time': 939.7636005878448, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (247, {'train/ssim': 0.6871802466256278, 'train/loss': 0.32192070143563406, 'validation/ssim': 0.6651276098454206, 'validation/loss': 0.33824474568707796, 'validation/num_examples': 3554, 'test/ssim': 0.6840749408771991, 'test/loss': 0.33948019795230033, 'test/num_examples': 3581, 'score': 377.5201585292816, 'total_duration': 1326.123896598816, 'accumulated_submission_time': 377.5201585292816, 'accumulated_eval_time': 946.3776602745056, 'accumulated_logging_time': 0.017385482788085938, 'global_step': 247, 'preemption_count': 0}), (292, {'train/ssim': 0.6966355187552316, 'train/loss': 0.31523081234523226, 'validation/ssim': 0.6748152664691193, 'validation/loss': 0.3311230042843451, 'validation/num_examples': 3554, 'test/ssim': 0.6933599204132924, 'test/loss': 0.33267361050509636, 'test/num_examples': 3581, 'score': 458.9091157913208, 'total_duration': 1415.491170167923, 'accumulated_submission_time': 458.9091157913208, 'accumulated_eval_time': 952.9454824924469, 'accumulated_logging_time': 0.04237723350524902, 'global_step': 292, 'preemption_count': 0}), (335, {'train/ssim': 0.7022774560110909, 'train/loss': 0.3099607058933803, 'validation/ssim': 0.6809610979705966, 'validation/loss': 0.32542282903066966, 'validation/num_examples': 3554, 'test/ssim': 0.6992722686793145, 'test/loss': 0.32718586648718934, 'test/num_examples': 3581, 'score': 539.6586167812347, 'total_duration': 1503.8959980010986, 'accumulated_submission_time': 539.6586167812347, 'accumulated_eval_time': 959.2601292133331, 'accumulated_logging_time': 0.06106853485107422, 'global_step': 335, 'preemption_count': 0}), (379, {'train/ssim': 0.706977094922747, 'train/loss': 0.3055340903145926, 'validation/ssim': 0.6859107503209412, 'validation/loss': 0.32067139496210256, 'validation/num_examples': 3554, 'test/ssim': 0.7039155765672996, 'test/loss': 0.3227066598235479, 'test/num_examples': 3581, 'score': 622.8703331947327, 'total_duration': 1594.9396224021912, 'accumulated_submission_time': 622.8703331947327, 'accumulated_eval_time': 965.6902973651886, 'accumulated_logging_time': 0.08345198631286621, 'global_step': 379, 'preemption_count': 0}), (426, {'train/ssim': 0.7133266585213798, 'train/loss': 0.30159030641828266, 'validation/ssim': 0.6924917621430079, 'validation/loss': 0.31652299648063803, 'validation/num_examples': 3554, 'test/ssim': 0.7100473855068417, 'test/loss': 0.3188162608973576, 'test/num_examples': 3581, 'score': 703.2782878875732, 'total_duration': 1683.1996495723724, 'accumulated_submission_time': 703.2782878875732, 'accumulated_eval_time': 972.0957870483398, 'accumulated_logging_time': 0.10141587257385254, 'global_step': 426, 'preemption_count': 0}), (467, {'train/ssim': 0.7155896595546177, 'train/loss': 0.29866320746285574, 'validation/ssim': 0.6948496357537282, 'validation/loss': 0.31335253439126687, 'validation/num_examples': 3554, 'test/ssim': 0.7123186228096202, 'test/loss': 0.31580939741648634, 'test/num_examples': 3581, 'score': 785.6607389450073, 'total_duration': 1773.4506480693817, 'accumulated_submission_time': 785.6607389450073, 'accumulated_eval_time': 978.5864672660828, 'accumulated_logging_time': 0.12050628662109375, 'global_step': 467, 'preemption_count': 0}), (510, {'train/ssim': 0.7178807939801898, 'train/loss': 0.29615061623709543, 'validation/ssim': 0.697465320215778, 'validation/loss': 0.31065723288240366, 'validation/num_examples': 3554, 'test/ssim': 0.7147783004049149, 'test/loss': 0.3130527422834927, 'test/num_examples': 3581, 'score': 866.7468185424805, 'total_duration': 1861.9607026576996, 'accumulated_submission_time': 866.7468185424805, 'accumulated_eval_time': 984.7154245376587, 'accumulated_logging_time': 0.1385667324066162, 'global_step': 510, 'preemption_count': 0}), (550, {'train/ssim': 0.7203436579023089, 'train/loss': 0.29445838928222656, 'validation/ssim': 0.6997764814953221, 'validation/loss': 0.30925198170195556, 'validation/num_examples': 3554, 'test/ssim': 0.7171714375785395, 'test/loss': 0.31148396321296423, 'test/num_examples': 3581, 'score': 946.4454352855682, 'total_duration': 1949.213633298874, 'accumulated_submission_time': 946.4454352855682, 'accumulated_eval_time': 991.0348687171936, 'accumulated_logging_time': 0.1583108901977539, 'global_step': 550, 'preemption_count': 0}), (594, {'train/ssim': 0.7228171484810966, 'train/loss': 0.2924823420388358, 'validation/ssim': 0.7023131671620005, 'validation/loss': 0.30727354274013435, 'validation/num_examples': 3554, 'test/ssim': 0.7196080032855696, 'test/loss': 0.3095075217947152, 'test/num_examples': 3581, 'score': 1026.307953119278, 'total_duration': 2037.306670665741, 'accumulated_submission_time': 1026.307953119278, 'accumulated_eval_time': 997.9203455448151, 'accumulated_logging_time': 0.17879509925842285, 'global_step': 594, 'preemption_count': 0}), (638, {'train/ssim': 0.722550528390067, 'train/loss': 0.2910958358219692, 'validation/ssim': 0.7024947956967501, 'validation/loss': 0.30546546639789673, 'validation/num_examples': 3554, 'test/ssim': 0.7197104046312134, 'test/loss': 0.3077844248507749, 'test/num_examples': 3581, 'score': 1105.8310816287994, 'total_duration': 2124.676556825638, 'accumulated_submission_time': 1105.8310816287994, 'accumulated_eval_time': 1004.4546413421631, 'accumulated_logging_time': 0.1983494758605957, 'global_step': 638, 'preemption_count': 0}), (680, {'train/ssim': 0.7251602581569127, 'train/loss': 0.2896979536328997, 'validation/ssim': 0.7048416094761185, 'validation/loss': 0.30409906201639, 'validation/num_examples': 3554, 'test/ssim': 0.7220813843898353, 'test/loss': 0.30640647225591666, 'test/num_examples': 3581, 'score': 1183.0385553836823, 'total_duration': 2211.741660118103, 'accumulated_submission_time': 1183.0385553836823, 'accumulated_eval_time': 1010.5094039440155, 'accumulated_logging_time': 2.58011794090271, 'global_step': 680, 'preemption_count': 0}), (727, {'train/ssim': 0.7255374363490513, 'train/loss': 0.2886474813733782, 'validation/ssim': 0.7050717364017656, 'validation/loss': 0.30295117517234105, 'validation/num_examples': 3554, 'test/ssim': 0.7223228661250349, 'test/loss': 0.30526969460128106, 'test/num_examples': 3581, 'score': 1263.3923892974854, 'total_duration': 2299.6480371952057, 'accumulated_submission_time': 1263.3923892974854, 'accumulated_eval_time': 1016.6518099308014, 'accumulated_logging_time': 2.5981640815734863, 'global_step': 727, 'preemption_count': 0}), (769, {'train/ssim': 0.7268263271876744, 'train/loss': 0.2876706463950021, 'validation/ssim': 0.7066573452667769, 'validation/loss': 0.30190361679841377, 'validation/num_examples': 3554, 'test/ssim': 0.7238372061313181, 'test/loss': 0.3040889089116169, 'test/num_examples': 3581, 'score': 1343.9352555274963, 'total_duration': 2387.6491277217865, 'accumulated_submission_time': 1343.9352555274963, 'accumulated_eval_time': 1022.8006691932678, 'accumulated_logging_time': 2.6172313690185547, 'global_step': 769, 'preemption_count': 0}), (813, {'train/ssim': 0.7286319732666016, 'train/loss': 0.2866887365068708, 'validation/ssim': 0.7077479408105304, 'validation/loss': 0.30132754384364446, 'validation/num_examples': 3554, 'test/ssim': 0.7249458950013963, 'test/loss': 0.30357195937543635, 'test/num_examples': 3581, 'score': 1423.253351688385, 'total_duration': 2474.312212228775, 'accumulated_submission_time': 1423.253351688385, 'accumulated_eval_time': 1028.8449442386627, 'accumulated_logging_time': 2.640709638595581, 'global_step': 813, 'preemption_count': 0}), (856, {'train/ssim': 0.7295635768345424, 'train/loss': 0.28571297441210064, 'validation/ssim': 0.7090743649321187, 'validation/loss': 0.3002114626015581, 'validation/num_examples': 3554, 'test/ssim': 0.7261402137583776, 'test/loss': 0.30243804514058575, 'test/num_examples': 3581, 'score': 1503.073911190033, 'total_duration': 2561.8995871543884, 'accumulated_submission_time': 1503.073911190033, 'accumulated_eval_time': 1035.3785729408264, 'accumulated_logging_time': 2.6632027626037598, 'global_step': 856, 'preemption_count': 0}), (899, {'train/ssim': 0.7312474931989398, 'train/loss': 0.28482076099940706, 'validation/ssim': 0.7103371091551772, 'validation/loss': 0.2995338246737831, 'validation/num_examples': 3554, 'test/ssim': 0.7274653635288676, 'test/loss': 0.30167715950938984, 'test/num_examples': 3581, 'score': 1584.7094411849976, 'total_duration': 2651.3256406784058, 'accumulated_submission_time': 1584.7094411849976, 'accumulated_eval_time': 1041.8547542095184, 'accumulated_logging_time': 2.6917123794555664, 'global_step': 899, 'preemption_count': 0}), (942, {'train/ssim': 0.7319624764578683, 'train/loss': 0.2842667443411691, 'validation/ssim': 0.7111894031153277, 'validation/loss': 0.29877997008212576, 'validation/num_examples': 3554, 'test/ssim': 0.7282542356796285, 'test/loss': 0.30096266809637673, 'test/num_examples': 3581, 'score': 1664.8065073490143, 'total_duration': 2739.6968879699707, 'accumulated_submission_time': 1664.8065073490143, 'accumulated_eval_time': 1048.9080023765564, 'accumulated_logging_time': 2.711026191711426, 'global_step': 942, 'preemption_count': 0}), (1196, {'train/ssim': 0.7357939311436245, 'train/loss': 0.28074799265180317, 'validation/ssim': 0.714151583163337, 'validation/loss': 0.2959299683235439, 'validation/num_examples': 3554, 'test/ssim': 0.7312075123045239, 'test/loss': 0.29801747040587473, 'test/num_examples': 3581, 'score': 1744.1340277194977, 'total_duration': 2826.9671466350555, 'accumulated_submission_time': 1744.1340277194977, 'accumulated_eval_time': 1055.5046236515045, 'accumulated_logging_time': 2.7344465255737305, 'global_step': 1196, 'preemption_count': 0}), (2525, {'train/ssim': 0.7419353212629046, 'train/loss': 0.2741779770169939, 'validation/ssim': 0.720409112092009, 'validation/loss': 0.28936042850045723, 'validation/num_examples': 3554, 'test/ssim': 0.7375877569987433, 'test/loss': 0.2908925661259076, 'test/num_examples': 3581, 'score': 1823.0307388305664, 'total_duration': 2913.819164276123, 'accumulated_submission_time': 1823.0307388305664, 'accumulated_eval_time': 1061.6697239875793, 'accumulated_logging_time': 2.753291606903076, 'global_step': 2525, 'preemption_count': 0}), (3854, {'train/ssim': 0.7444802692958287, 'train/loss': 0.2722600357873099, 'validation/ssim': 0.7229349440111494, 'validation/loss': 0.2875359169740521, 'validation/num_examples': 3554, 'test/ssim': 0.7401205199752164, 'test/loss': 0.2889454065837929, 'test/num_examples': 3581, 'score': 1901.8422586917877, 'total_duration': 3000.8955886363983, 'accumulated_submission_time': 1901.8422586917877, 'accumulated_eval_time': 1068.026228427887, 'accumulated_logging_time': 2.7728962898254395, 'global_step': 3854, 'preemption_count': 0}), (5180, {'train/ssim': 0.745368208203997, 'train/loss': 0.27128916127341135, 'validation/ssim': 0.7236617329285664, 'validation/loss': 0.2865570703784468, 'validation/num_examples': 3554, 'test/ssim': 0.7409040061697152, 'test/loss': 0.28793840320441216, 'test/num_examples': 3581, 'score': 1980.5941214561462, 'total_duration': 3088.1635172367096, 'accumulated_submission_time': 1980.5941214561462, 'accumulated_eval_time': 1074.583905696869, 'accumulated_logging_time': 2.7918646335601807, 'global_step': 5180, 'preemption_count': 0})], 'global_step': 5180}
I0315 20:08:47.648508 140066777728192 submission_runner.py:649] Timing: 1980.5941214561462
I0315 20:08:47.648556 140066777728192 submission_runner.py:651] Total number of evals: 22
I0315 20:08:47.648595 140066777728192 submission_runner.py:652] ====================
I0315 20:08:47.648703 140066777728192 submission_runner.py:750] Final fastmri score: 4
