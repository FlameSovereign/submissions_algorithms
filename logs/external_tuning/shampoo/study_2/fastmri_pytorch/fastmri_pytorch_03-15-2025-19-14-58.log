torchrun --redirects 1:0,2:0,3:0,4:0,5:0,6:0,7:0 --standalone --nnodes=1 --nproc_per_node=8 submission_runner.py --framework=pytorch --workload=fastmri --submission_path=submissions_algorithms/leaderboard/external_tuning/shampoo_submission/submission.py --data_dir=/data/fastmri --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=submissions/rolling_leaderboard/external_tuning/shampoo/study_2 --overwrite=True --save_checkpoints=False --rng_seed=196739736 --torch_compile=true --tuning_ruleset=external --tuning_search_space=submissions_algorithms/leaderboard/external_tuning/shampoo_submission/tuning_search_space.json --num_tuning_trials=5 --hparam_start_index=3 --hparam_end_index=4 2>&1 | tee -a /logs/fastmri_pytorch_03-15-2025-19-14-58.log
W0315 19:15:18.339000 9 site-packages/torch/distributed/run.py:793] 
W0315 19:15:18.339000 9 site-packages/torch/distributed/run.py:793] *****************************************
W0315 19:15:18.339000 9 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0315 19:15:18.339000 9 site-packages/torch/distributed/run.py:793] *****************************************
2025-03-15 19:15:32.448842: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-03-15 19:15:32.448842: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-03-15 19:15:32.448846: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-03-15 19:15:32.448842: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-03-15 19:15:32.448854: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-03-15 19:15:32.448846: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-03-15 19:15:32.448856: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-03-15 19:15:32.448845: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1742066133.100035      46 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1742066133.099993      45 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1742066133.100019      47 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1742066133.100046      49 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1742066133.100041      44 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1742066133.100023      50 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1742066133.100031      48 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1742066133.100032      51 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1742066133.282738      49 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
E0000 00:00:1742066133.282758      45 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
E0000 00:00:1742066133.282769      51 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
E0000 00:00:1742066133.282774      48 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
E0000 00:00:1742066133.282778      44 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
E0000 00:00:1742066133.282777      50 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
E0000 00:00:1742066133.282787      46 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
E0000 00:00:1742066133.282793      47 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
[rank1]:[W315 19:16:18.207656603 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank2]:[W315 19:16:18.207666400 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank3]:[W315 19:16:18.207675445 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank7]:[W315 19:16:18.207675623 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 7]  using GPU 7 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank6]:[W315 19:16:18.207680555 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 6]  using GPU 6 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank4]:[W315 19:16:18.207682935 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 4]  using GPU 4 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank5]:[W315 19:16:18.207692253 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 5]  using GPU 5 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank0]:[W315 19:16:18.346747657 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
I0315 19:16:20.942080 140257611351232 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_2/fastmri_pytorch.
I0315 19:16:20.942080 140321624544448 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_2/fastmri_pytorch.
I0315 19:16:20.942080 139629662209216 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_2/fastmri_pytorch.
I0315 19:16:20.942080 140531948930240 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_2/fastmri_pytorch.
I0315 19:16:20.942080 139912558875840 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_2/fastmri_pytorch.
I0315 19:16:20.942081 140072725882048 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_2/fastmri_pytorch.
I0315 19:16:20.942117 140572191630528 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_2/fastmri_pytorch.
I0315 19:16:20.942212 140186397963456 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_2/fastmri_pytorch.
I0315 19:16:22.892719 140186397963456 submission_runner.py:606] Using RNG seed 196739736
I0315 19:16:22.893308 140321624544448 logger_utils.py:97] Saving hparams to /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_2/fastmri_pytorch/trial_4/hparams.json.
I0315 19:16:22.893293 140531948930240 logger_utils.py:97] Saving hparams to /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_2/fastmri_pytorch/trial_4/hparams.json.
I0315 19:16:22.893299 139912558875840 logger_utils.py:97] Saving hparams to /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_2/fastmri_pytorch/trial_4/hparams.json.
I0315 19:16:22.893298 140257611351232 logger_utils.py:97] Saving hparams to /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_2/fastmri_pytorch/trial_4/hparams.json.
I0315 19:16:22.893329 139629662209216 logger_utils.py:97] Saving hparams to /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_2/fastmri_pytorch/trial_4/hparams.json.
I0315 19:16:22.893295 140072725882048 logger_utils.py:97] Saving hparams to /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_2/fastmri_pytorch/trial_4/hparams.json.
I0315 19:16:22.894127 140186397963456 submission_runner.py:615] --- Tuning run 4/5 ---
I0315 19:16:22.894280 140186397963456 submission_runner.py:620] Creating tuning directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_2/fastmri_pytorch/trial_4.
I0315 19:16:22.893840 140572191630528 logger_utils.py:97] Saving hparams to /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_2/fastmri_pytorch/trial_4/hparams.json.
I0315 19:16:22.911474 140186397963456 logger_utils.py:97] Saving hparams to /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_2/fastmri_pytorch/trial_4/hparams.json.
I0315 19:16:23.330848 140186397963456 submission_runner.py:218] Initializing dataset.
I0315 19:16:23.331066 140186397963456 submission_runner.py:229] Initializing model.
I0315 19:16:24.162566 140186397963456 submission_runner.py:268] Performing `torch.compile`.
I0315 19:16:26.705875 140186397963456 submission_runner.py:272] Initializing optimizer.
W0315 19:16:26.766378 140572191630528 distributed_shampoo.py:341] start_preconditioning_step set to -1. Setting start_preconditioning_step equal to precondition frequency 100 by default.
W0315 19:16:26.766424 140257611351232 distributed_shampoo.py:341] start_preconditioning_step set to -1. Setting start_preconditioning_step equal to precondition frequency 100 by default.
W0315 19:16:26.766445 140321624544448 distributed_shampoo.py:341] start_preconditioning_step set to -1. Setting start_preconditioning_step equal to precondition frequency 100 by default.
W0315 19:16:26.766449 139629662209216 distributed_shampoo.py:341] start_preconditioning_step set to -1. Setting start_preconditioning_step equal to precondition frequency 100 by default.
W0315 19:16:26.766458 140072725882048 distributed_shampoo.py:341] start_preconditioning_step set to -1. Setting start_preconditioning_step equal to precondition frequency 100 by default.
W0315 19:16:26.766461 140531948930240 distributed_shampoo.py:341] start_preconditioning_step set to -1. Setting start_preconditioning_step equal to precondition frequency 100 by default.
W0315 19:16:26.766507 139912558875840 distributed_shampoo.py:341] start_preconditioning_step set to -1. Setting start_preconditioning_step equal to precondition frequency 100 by default.
W0315 19:16:26.766554 140186397963456 distributed_shampoo.py:341] start_preconditioning_step set to -1. Setting start_preconditioning_step equal to precondition frequency 100 by default.
I0315 19:16:26.814028 140257611351232 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 0.block_0 ([torch.Size([0])]) for Parameter 0 (torch.Size([32, 1, 3, 3])), Block block_0 (torch.Size([288])).
I0315 19:16:26.814013 140321624544448 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 0.block_0 ([torch.Size([0])]) for Parameter 0 (torch.Size([32, 1, 3, 3])), Block block_0 (torch.Size([288])).
I0315 19:16:26.814057 139912558875840 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 0.block_0 ([torch.Size([0])]) for Parameter 0 (torch.Size([32, 1, 3, 3])), Block block_0 (torch.Size([288])).
I0315 19:16:26.814044 140572191630528 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 0.block_0 ([torch.Size([0])]) for Parameter 0 (torch.Size([32, 1, 3, 3])), Block block_0 (torch.Size([288])).
I0315 19:16:26.814078 140072725882048 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 0.block_0 ([torch.Size([0])]) for Parameter 0 (torch.Size([32, 1, 3, 3])), Block block_0 (torch.Size([288])).
I0315 19:16:26.814181 140531948930240 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 0.block_0 ([torch.Size([288, 288])]) for Parameter 0 (torch.Size([32, 1, 3, 3])), Block block_0 (torch.Size([288])).
I0315 19:16:26.814216 139629662209216 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 0.block_0 ([torch.Size([0])]) for Parameter 0 (torch.Size([32, 1, 3, 3])), Block block_0 (torch.Size([288])).
I0315 19:16:26.816631 140257611351232 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 1.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 1 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 19:16:26.816707 139912558875840 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 1.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 1 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 19:16:26.816710 140572191630528 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 1.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 1 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 19:16:26.816777 140072725882048 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 1.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 1 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 19:16:26.816782 140531948930240 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 1.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 1 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 19:16:26.816812 140321624544448 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 1.block_0 ([torch.Size([1024, 1024]), torch.Size([9, 9])]) for Parameter 1 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 19:16:26.816874 140257611351232 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 2.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 2 (torch.Size([64, 32, 3, 3])), Block block_0 (torch.Size([64, 288])).
I0315 19:16:26.816870 139629662209216 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 1.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 1 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 19:16:26.816915 140572191630528 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 2.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 2 (torch.Size([64, 32, 3, 3])), Block block_0 (torch.Size([64, 288])).
I0315 19:16:26.816976 140531948930240 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 2.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 2 (torch.Size([64, 32, 3, 3])), Block block_0 (torch.Size([64, 288])).
I0315 19:16:26.816989 140072725882048 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 2.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 2 (torch.Size([64, 32, 3, 3])), Block block_0 (torch.Size([64, 288])).
I0315 19:16:26.817035 140321624544448 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 2.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 2 (torch.Size([64, 32, 3, 3])), Block block_0 (torch.Size([64, 288])).
I0315 19:16:26.817042 140257611351232 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 3.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 3 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 19:16:26.817046 139912558875840 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 2.block_0 ([torch.Size([64, 64]), torch.Size([288, 288])]) for Parameter 2 (torch.Size([64, 32, 3, 3])), Block block_0 (torch.Size([64, 288])).
I0315 19:16:26.817066 139629662209216 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 2.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 2 (torch.Size([64, 32, 3, 3])), Block block_0 (torch.Size([64, 288])).
I0315 19:16:26.817090 140572191630528 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 3.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 3 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 19:16:26.814119 140186397963456 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 0.block_0 ([torch.Size([0])]) for Parameter 0 (torch.Size([32, 1, 3, 3])), Block block_0 (torch.Size([288])).
I0315 19:16:26.817144 140531948930240 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 3.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 3 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 19:16:26.817182 140257611351232 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 4.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 4 (torch.Size([128, 64, 3, 3])), Block block_0 (torch.Size([128, 576])).
I0315 19:16:26.817219 139629662209216 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 3.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 3 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 19:16:26.817216 140321624544448 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 3.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 3 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 19:16:26.817231 139912558875840 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 3.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 3 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 19:16:26.817245 140572191630528 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 4.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 4 (torch.Size([128, 64, 3, 3])), Block block_0 (torch.Size([128, 576])).
I0315 19:16:26.817286 140531948930240 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 4.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 4 (torch.Size([128, 64, 3, 3])), Block block_0 (torch.Size([128, 576])).
I0315 19:16:26.817282 140072725882048 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 3.block_0 ([torch.Size([64, 64]), torch.Size([576, 576])]) for Parameter 3 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 19:16:26.817350 140257611351232 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 5.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 5 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 19:16:26.817375 139629662209216 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 4.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 4 (torch.Size([128, 64, 3, 3])), Block block_0 (torch.Size([128, 576])).
I0315 19:16:26.817379 139912558875840 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 4.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 4 (torch.Size([128, 64, 3, 3])), Block block_0 (torch.Size([128, 576])).
I0315 19:16:26.817425 140572191630528 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 5.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 5 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 19:16:26.817415 140186397963456 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 1.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 1 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 19:16:26.817488 140072725882048 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 4.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 4 (torch.Size([128, 64, 3, 3])), Block block_0 (torch.Size([128, 576])).
I0315 19:16:26.817476 140321624544448 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 4.block_0 ([torch.Size([128, 128]), torch.Size([576, 576])]) for Parameter 4 (torch.Size([128, 64, 3, 3])), Block block_0 (torch.Size([128, 576])).
I0315 19:16:26.817534 140257611351232 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 6.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 6 (torch.Size([256, 128, 3, 3])), Block block_0 (torch.Size([256, 384, 3])).
I0315 19:16:26.817578 139912558875840 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 5.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 5 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 19:16:26.817589 140186397963456 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 2.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 2 (torch.Size([64, 32, 3, 3])), Block block_0 (torch.Size([64, 288])).
I0315 19:16:26.817589 139629662209216 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 5.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 5 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 19:16:26.817597 140572191630528 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 6.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 6 (torch.Size([256, 128, 3, 3])), Block block_0 (torch.Size([256, 384, 3])).
I0315 19:16:26.817678 140072725882048 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 5.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 5 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 19:16:26.817682 140321624544448 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 5.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 5 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 19:16:26.817699 140257611351232 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 7.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 7 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 19:16:26.817713 140186397963456 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 3.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 3 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 19:16:26.817746 139912558875840 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 6.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 6 (torch.Size([256, 128, 3, 3])), Block block_0 (torch.Size([256, 384, 3])).
I0315 19:16:26.817748 139629662209216 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 6.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 6 (torch.Size([256, 128, 3, 3])), Block block_0 (torch.Size([256, 384, 3])).
I0315 19:16:26.817757 140572191630528 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 7.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 7 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 19:16:26.817844 140072725882048 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 6.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 6 (torch.Size([256, 128, 3, 3])), Block block_0 (torch.Size([256, 384, 3])).
I0315 19:16:26.817844 140321624544448 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 6.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 6 (torch.Size([256, 128, 3, 3])), Block block_0 (torch.Size([256, 384, 3])).
I0315 19:16:26.817852 140257611351232 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 8.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 8 (torch.Size([512, 256, 3, 3])), Block block_0 (torch.Size([512, 768, 3])).
I0315 19:16:26.817860 140186397963456 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 4.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 4 (torch.Size([128, 64, 3, 3])), Block block_0 (torch.Size([128, 576])).
I0315 19:16:26.817904 139629662209216 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 7.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 7 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 19:16:26.817911 140572191630528 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 8.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 8 (torch.Size([512, 256, 3, 3])), Block block_0 (torch.Size([512, 768, 3])).
I0315 19:16:26.817991 140257611351232 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 9.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 9 (torch.Size([512, 512, 3, 3])), Block block_0 (torch.Size([512, 512, 9])).
I0315 19:16:26.817999 140186397963456 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 5.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 5 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 19:16:26.818005 140321624544448 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 7.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 7 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 19:16:26.818020 140072725882048 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 7.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 7 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 19:16:26.818061 140572191630528 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 9.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 9 (torch.Size([512, 512, 3, 3])), Block block_0 (torch.Size([512, 512, 9])).
I0315 19:16:26.818096 140531948930240 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 5.block_0 ([torch.Size([128, 128]), torch.Size([384, 384]), torch.Size([3, 3])]) for Parameter 5 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 19:16:26.818134 140186397963456 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 6.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 6 (torch.Size([256, 128, 3, 3])), Block block_0 (torch.Size([256, 384, 3])).
I0315 19:16:26.818139 140257611351232 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 10.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 10 (torch.Size([256, 512, 3, 3])), Block block_0 (torch.Size([256, 512, 9])).
I0315 19:16:26.818172 140321624544448 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 8.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 8 (torch.Size([512, 256, 3, 3])), Block block_0 (torch.Size([512, 768, 3])).
I0315 19:16:26.818189 140072725882048 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 8.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 8 (torch.Size([512, 256, 3, 3])), Block block_0 (torch.Size([512, 768, 3])).
I0315 19:16:26.818276 140186397963456 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 7.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 7 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 19:16:26.818333 140321624544448 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 9.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 9 (torch.Size([512, 512, 3, 3])), Block block_0 (torch.Size([512, 512, 9])).
I0315 19:16:26.818337 140072725882048 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 9.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 9 (torch.Size([512, 512, 3, 3])), Block block_0 (torch.Size([512, 512, 9])).
I0315 19:16:26.818451 140186397963456 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 8.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 8 (torch.Size([512, 256, 3, 3])), Block block_0 (torch.Size([512, 768, 3])).
I0315 19:16:26.818452 140257611351232 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 11.block_0 ([torch.Size([256, 256]), torch.Size([768, 768]), torch.Size([3, 3])]) for Parameter 11 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 19:16:26.818498 140072725882048 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 10.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 10 (torch.Size([256, 512, 3, 3])), Block block_0 (torch.Size([256, 512, 9])).
I0315 19:16:26.818492 140321624544448 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 10.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 10 (torch.Size([256, 512, 3, 3])), Block block_0 (torch.Size([256, 512, 9])).
I0315 19:16:26.818513 139912558875840 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 7.block_0 ([torch.Size([256, 256]), torch.Size([768, 768]), torch.Size([3, 3])]) for Parameter 7 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 19:16:26.818635 140257611351232 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 12.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 12 (torch.Size([128, 256, 3, 3])), Block block_0 (torch.Size([128, 768, 3])).
I0315 19:16:26.818659 140321624544448 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 11.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 11 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 19:16:26.818663 140072725882048 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 11.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 11 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 19:16:26.818728 139912558875840 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 8.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 8 (torch.Size([512, 256, 3, 3])), Block block_0 (torch.Size([512, 768, 3])).
I0315 19:16:26.818791 140257611351232 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 13.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 13 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 19:16:26.818817 140072725882048 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 12.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 12 (torch.Size([128, 256, 3, 3])), Block block_0 (torch.Size([128, 768, 3])).
I0315 19:16:26.818892 139912558875840 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 9.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 9 (torch.Size([512, 512, 3, 3])), Block block_0 (torch.Size([512, 512, 9])).
I0315 19:16:26.818947 140257611351232 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 14.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 14 (torch.Size([64, 128, 3, 3])), Block block_0 (torch.Size([64, 384, 3])).
I0315 19:16:26.818964 140072725882048 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 13.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 13 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 19:16:26.818960 139629662209216 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 8.block_0 ([torch.Size([512, 512]), torch.Size([768, 768]), torch.Size([3, 3])]) for Parameter 8 (torch.Size([512, 256, 3, 3])), Block block_0 (torch.Size([512, 768, 3])).
I0315 19:16:26.819047 139912558875840 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 10.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 10 (torch.Size([256, 512, 3, 3])), Block block_0 (torch.Size([256, 512, 9])).
I0315 19:16:26.819076 140257611351232 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 15.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 15 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 19:16:26.819120 140072725882048 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 14.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 14 (torch.Size([64, 128, 3, 3])), Block block_0 (torch.Size([64, 384, 3])).
I0315 19:16:26.819169 139629662209216 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 9.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 9 (torch.Size([512, 512, 3, 3])), Block block_0 (torch.Size([512, 512, 9])).
I0315 19:16:26.819206 139912558875840 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 11.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 11 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 19:16:26.819288 140257611351232 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 16.block_0 ([torch.Size([32, 32]), torch.Size([576, 576])]) for Parameter 16 (torch.Size([32, 64, 3, 3])), Block block_0 (torch.Size([32, 576])).
I0315 19:16:26.819281 140531948930240 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 6.block_0 ([torch.Size([256, 256]), torch.Size([384, 384]), torch.Size([3, 3])]) for Parameter 6 (torch.Size([256, 128, 3, 3])), Block block_0 (torch.Size([256, 384, 3])).
I0315 19:16:26.819276 140572191630528 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 10.block_0 ([torch.Size([256, 256]), torch.Size([512, 512]), torch.Size([9, 9])]) for Parameter 10 (torch.Size([256, 512, 3, 3])), Block block_0 (torch.Size([256, 512, 9])).
I0315 19:16:26.819343 139629662209216 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 10.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 10 (torch.Size([256, 512, 3, 3])), Block block_0 (torch.Size([256, 512, 9])).
I0315 19:16:26.819359 139912558875840 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 12.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 12 (torch.Size([128, 256, 3, 3])), Block block_0 (torch.Size([128, 768, 3])).
I0315 19:16:26.819353 140072725882048 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 15.block_0 ([torch.Size([64, 64]), torch.Size([576, 576])]) for Parameter 15 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 19:16:26.819432 140257611351232 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 17.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 17 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 19:16:26.819473 140572191630528 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 11.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 11 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 19:16:26.819486 140531948930240 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 7.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 7 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 19:16:26.819504 140072725882048 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 16.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 16 (torch.Size([32, 64, 3, 3])), Block block_0 (torch.Size([32, 576])).
I0315 19:16:26.819515 139629662209216 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 11.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 11 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 19:16:26.819521 139912558875840 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 13.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 13 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 19:16:26.819556 140257611351232 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 18.block_0 ([torch.Size([0])]) for Parameter 18 (torch.Size([1, 32, 1, 1])), Block block_0 (torch.Size([32])).
I0315 19:16:26.819649 140572191630528 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 12.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 12 (torch.Size([128, 256, 3, 3])), Block block_0 (torch.Size([128, 768, 3])).
I0315 19:16:26.819656 140257611351232 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 19.block_0 ([torch.Size([0])]) for Parameter 19 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0315 19:16:26.819670 140531948930240 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 8.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 8 (torch.Size([512, 256, 3, 3])), Block block_0 (torch.Size([512, 768, 3])).
I0315 19:16:26.819683 139912558875840 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 14.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 14 (torch.Size([64, 128, 3, 3])), Block block_0 (torch.Size([64, 384, 3])).
I0315 19:16:26.819684 139629662209216 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 12.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 12 (torch.Size([128, 256, 3, 3])), Block block_0 (torch.Size([128, 768, 3])).
I0315 19:16:26.819786 140257611351232 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 20.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 20 (torch.Size([512, 256, 2, 2])), Block block_0 (torch.Size([512, 1024])).
I0315 19:16:26.819800 139912558875840 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 15.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 15 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 19:16:26.819819 140531948930240 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 9.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 9 (torch.Size([512, 512, 3, 3])), Block block_0 (torch.Size([512, 512, 9])).
I0315 19:16:26.819812 140572191630528 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 13.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 13 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 19:16:26.819821 139629662209216 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 13.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 13 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 19:16:26.819829 140321624544448 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 12.block_0 ([torch.Size([128, 128]), torch.Size([768, 768]), torch.Size([3, 3])]) for Parameter 12 (torch.Size([128, 256, 3, 3])), Block block_0 (torch.Size([128, 768, 3])).
I0315 19:16:26.819911 139912558875840 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 16.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 16 (torch.Size([32, 64, 3, 3])), Block block_0 (torch.Size([32, 576])).
I0315 19:16:26.819920 140257611351232 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 21.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 21 (torch.Size([256, 128, 2, 2])), Block block_0 (torch.Size([256, 512])).
I0315 19:16:26.819956 140572191630528 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 14.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 14 (torch.Size([64, 128, 3, 3])), Block block_0 (torch.Size([64, 384, 3])).
I0315 19:16:26.819963 139629662209216 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 14.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 14 (torch.Size([64, 128, 3, 3])), Block block_0 (torch.Size([64, 384, 3])).
I0315 19:16:26.819973 140531948930240 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 10.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 10 (torch.Size([256, 512, 3, 3])), Block block_0 (torch.Size([256, 512, 9])).
I0315 19:16:26.820033 139912558875840 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 17.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 17 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 19:16:26.820035 140257611351232 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 22.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 22 (torch.Size([128, 64, 2, 2])), Block block_0 (torch.Size([128, 256])).
I0315 19:16:26.820089 140572191630528 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 15.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 15 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 19:16:26.820095 139629662209216 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 15.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 15 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 19:16:26.820118 140531948930240 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 11.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 11 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 19:16:26.820137 139912558875840 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 18.block_0 ([torch.Size([0])]) for Parameter 18 (torch.Size([1, 32, 1, 1])), Block block_0 (torch.Size([32])).
I0315 19:16:26.820149 140257611351232 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 23.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 23 (torch.Size([64, 32, 2, 2])), Block block_0 (torch.Size([64, 128])).
I0315 19:16:26.820214 139629662209216 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 16.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 16 (torch.Size([32, 64, 3, 3])), Block block_0 (torch.Size([32, 576])).
I0315 19:16:26.820214 140572191630528 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 16.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 16 (torch.Size([32, 64, 3, 3])), Block block_0 (torch.Size([32, 576])).
I0315 19:16:26.820225 139912558875840 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 19.block_0 ([torch.Size([0])]) for Parameter 19 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0315 19:16:26.820251 140257611351232 shampoo_preconditioner_list.py:612] Rank 4: ShampooPreconditionerList Numel Breakdown: (165888, 2097314, 174080, 671744, 696320, 327698, 426002, 1310738, 1703954, 1048738, 655522, 1310738, 1212434, 327698, 303122, 671744, 665600, 2097314, 2048, 2, 2621440, 655360, 163840, 40960)
I0315 19:16:26.820267 140531948930240 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 12.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 12 (torch.Size([128, 256, 3, 3])), Block block_0 (torch.Size([128, 768, 3])).
I0315 19:16:26.820299 140257611351232 shampoo_preconditioner_list.py:615] Rank 4: ShampooPreconditionerList Bytes Breakdown: (663552, 8389256)
I0315 19:16:26.820279 140072725882048 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 17.block_0 ([torch.Size([1024, 1024]), torch.Size([9, 9])]) for Parameter 17 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 19:16:26.820339 140257611351232 shampoo_preconditioner_list.py:618] Rank 4: ShampooPreconditionerList Total Elements: 19350298
I0315 19:16:26.820337 140572191630528 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 17.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 17 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 19:16:26.820340 139912558875840 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 20.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 20 (torch.Size([512, 256, 2, 2])), Block block_0 (torch.Size([512, 1024])).
I0315 19:16:26.820342 139629662209216 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 17.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 17 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 19:16:26.820375 140257611351232 shampoo_preconditioner_list.py:621] Rank 4: ShampooPreconditionerList Total Bytes: 9052808
I0315 19:16:26.820340 140186397963456 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 9.block_0 ([torch.Size([512, 512]), torch.Size([512, 512]), torch.Size([9, 9])]) for Parameter 9 (torch.Size([512, 512, 3, 3])), Block block_0 (torch.Size([512, 512, 9])).
I0315 19:16:26.820402 140072725882048 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 18.block_0 ([torch.Size([0])]) for Parameter 18 (torch.Size([1, 32, 1, 1])), Block block_0 (torch.Size([32])).
I0315 19:16:26.820408 140531948930240 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 13.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 13 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 19:16:26.820427 140572191630528 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 18.block_0 ([torch.Size([0])]) for Parameter 18 (torch.Size([1, 32, 1, 1])), Block block_0 (torch.Size([32])).
I0315 19:16:26.820430 139629662209216 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 18.block_0 ([torch.Size([0])]) for Parameter 18 (torch.Size([1, 32, 1, 1])), Block block_0 (torch.Size([32])).
I0315 19:16:26.820473 139912558875840 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 21.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 21 (torch.Size([256, 128, 2, 2])), Block block_0 (torch.Size([256, 512])).
I0315 19:16:26.820511 140572191630528 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 19.block_0 ([torch.Size([0])]) for Parameter 19 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0315 19:16:26.820522 139629662209216 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 19.block_0 ([torch.Size([0])]) for Parameter 19 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0315 19:16:26.820522 140072725882048 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 19.block_0 ([torch.Size([0])]) for Parameter 19 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0315 19:16:26.820535 140186397963456 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 10.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 10 (torch.Size([256, 512, 3, 3])), Block block_0 (torch.Size([256, 512, 9])).
I0315 19:16:26.820571 140531948930240 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 14.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 14 (torch.Size([64, 128, 3, 3])), Block block_0 (torch.Size([64, 384, 3])).
I0315 19:16:26.820570 140257611351232 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([32, 1, 3, 3])), Block block_0 (torch.Size([288])).
I0315 19:16:26.820609 139912558875840 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 22.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 22 (torch.Size([128, 64, 2, 2])), Block block_0 (torch.Size([128, 256])).
I0315 19:16:26.820605 140321624544448 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 13.block_0 ([torch.Size([128, 128]), torch.Size([384, 384]), torch.Size([3, 3])]) for Parameter 13 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 19:16:26.820637 139629662209216 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 20.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 20 (torch.Size([512, 256, 2, 2])), Block block_0 (torch.Size([512, 1024])).
I0315 19:16:26.820649 140257611351232 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 19:16:26.820650 140572191630528 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 20.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 20 (torch.Size([512, 256, 2, 2])), Block block_0 (torch.Size([512, 1024])).
I0315 19:16:26.820699 140186397963456 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 11.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 11 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 19:16:26.820730 140257611351232 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([64, 32, 3, 3])), Block block_0 (torch.Size([64, 288])).
I0315 19:16:26.820738 140531948930240 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 15.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 15 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 19:16:26.820754 139912558875840 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 23.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 23 (torch.Size([64, 32, 2, 2])), Block block_0 (torch.Size([64, 128])).
I0315 19:16:26.820794 140257611351232 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 19:16:26.820791 139629662209216 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 21.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 21 (torch.Size([256, 128, 2, 2])), Block block_0 (torch.Size([256, 512])).
I0315 19:16:26.820803 140572191630528 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 21.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 21 (torch.Size([256, 128, 2, 2])), Block block_0 (torch.Size([256, 512])).
I0315 19:16:26.820853 140257611351232 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 4.block_0 (torch.Size([0])) for Parameter 4 (torch.Size([128, 64, 3, 3])), Block block_0 (torch.Size([128, 576])).
I0315 19:16:26.820853 140186397963456 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 12.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 12 (torch.Size([128, 256, 3, 3])), Block block_0 (torch.Size([128, 768, 3])).
I0315 19:16:26.820860 140531948930240 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 16.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 16 (torch.Size([32, 64, 3, 3])), Block block_0 (torch.Size([32, 576])).
I0315 19:16:26.820868 139912558875840 shampoo_preconditioner_list.py:612] Rank 3: ShampooPreconditionerList Numel Breakdown: (165888, 2097314, 174080, 671744, 696320, 327698, 426002, 1310738, 1703954, 1048738, 655522, 1310738, 1212434, 327698, 303122, 671744, 665600, 2097314, 2048, 2, 2621440, 655360, 163840, 40960)
I0315 19:16:26.820909 139912558875840 shampoo_preconditioner_list.py:615] Rank 3: ShampooPreconditionerList Bytes Breakdown: (663552, 8389256)
I0315 19:16:26.820907 139629662209216 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 22.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 22 (torch.Size([128, 64, 2, 2])), Block block_0 (torch.Size([128, 256])).
I0315 19:16:26.820915 140257611351232 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 5.block_0 (torch.Size([0])) for Parameter 5 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 19:16:26.820917 140572191630528 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 22.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 22 (torch.Size([128, 64, 2, 2])), Block block_0 (torch.Size([128, 256])).
I0315 19:16:26.820911 140321624544448 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 14.block_0 ([torch.Size([64, 64]), torch.Size([384, 384]), torch.Size([3, 3])]) for Parameter 14 (torch.Size([64, 128, 3, 3])), Block block_0 (torch.Size([64, 384, 3])).
I0315 19:16:26.820949 139912558875840 shampoo_preconditioner_list.py:618] Rank 3: ShampooPreconditionerList Total Elements: 19350298
I0315 19:16:26.820967 140257611351232 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 6.block_0 (torch.Size([0])) for Parameter 6 (torch.Size([256, 128, 3, 3])), Block block_0 (torch.Size([256, 384, 3])).
I0315 19:16:26.820977 140531948930240 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 17.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 17 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 19:16:26.820984 139912558875840 shampoo_preconditioner_list.py:621] Rank 3: ShampooPreconditionerList Total Bytes: 9052808
I0315 19:16:26.820994 140186397963456 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 13.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 13 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 19:16:26.821024 139629662209216 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 23.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 23 (torch.Size([64, 32, 2, 2])), Block block_0 (torch.Size([64, 128])).
I0315 19:16:26.821029 140572191630528 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 23.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 23 (torch.Size([64, 32, 2, 2])), Block block_0 (torch.Size([64, 128])).
I0315 19:16:26.821032 140257611351232 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 7.block_0 (torch.Size([0])) for Parameter 7 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 19:16:26.821060 140321624544448 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 15.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 15 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 19:16:26.821084 140257611351232 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 8.block_0 (torch.Size([0])) for Parameter 8 (torch.Size([512, 256, 3, 3])), Block block_0 (torch.Size([512, 768, 3])).
I0315 19:16:26.821135 140257611351232 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 9.block_0 (torch.Size([0])) for Parameter 9 (torch.Size([512, 512, 3, 3])), Block block_0 (torch.Size([512, 512, 9])).
I0315 19:16:26.821133 140572191630528 shampoo_preconditioner_list.py:612] Rank 2: ShampooPreconditionerList Numel Breakdown: (165888, 2097314, 174080, 671744, 696320, 327698, 426002, 1310738, 1703954, 1048738, 655522, 1310738, 1212434, 327698, 303122, 671744, 665600, 2097314, 2048, 2, 2621440, 655360, 163840, 40960)
I0315 19:16:26.821135 139629662209216 shampoo_preconditioner_list.py:612] Rank 1: ShampooPreconditionerList Numel Breakdown: (165888, 2097314, 174080, 671744, 696320, 327698, 426002, 1310738, 1703954, 1048738, 655522, 1310738, 1212434, 327698, 303122, 671744, 665600, 2097314, 2048, 2, 2621440, 655360, 163840, 40960)
I0315 19:16:26.821123 140072725882048 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 20.block_0 ([torch.Size([512, 512]), torch.Size([1024, 1024])]) for Parameter 20 (torch.Size([512, 256, 2, 2])), Block block_0 (torch.Size([512, 1024])).
I0315 19:16:26.821148 140186397963456 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 14.block_0 ([torch.Size([0]), torch.Size([0]), torch.Size([0])]) for Parameter 14 (torch.Size([64, 128, 3, 3])), Block block_0 (torch.Size([64, 384, 3])).
I0315 19:16:26.821142 140531948930240 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 18.block_0 ([torch.Size([32, 32])]) for Parameter 18 (torch.Size([1, 32, 1, 1])), Block block_0 (torch.Size([32])).
I0315 19:16:26.821172 140572191630528 shampoo_preconditioner_list.py:615] Rank 2: ShampooPreconditionerList Bytes Breakdown: (663552,)
I0315 19:16:26.821166 139912558875840 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([32, 1, 3, 3])), Block block_0 (torch.Size([288])).
I0315 19:16:26.821176 139629662209216 shampoo_preconditioner_list.py:615] Rank 1: ShampooPreconditionerList Bytes Breakdown: (663552,)
I0315 19:16:26.821192 140257611351232 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 10.block_0 (torch.Size([0])) for Parameter 10 (torch.Size([256, 512, 3, 3])), Block block_0 (torch.Size([256, 512, 9])).
I0315 19:16:26.821210 139629662209216 shampoo_preconditioner_list.py:618] Rank 1: ShampooPreconditionerList Total Elements: 19350298
I0315 19:16:26.821206 140321624544448 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 16.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 16 (torch.Size([32, 64, 3, 3])), Block block_0 (torch.Size([32, 576])).
I0315 19:16:26.821216 140572191630528 shampoo_preconditioner_list.py:618] Rank 2: ShampooPreconditionerList Total Elements: 19350298
I0315 19:16:26.821231 139912558875840 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 19:16:26.821247 139629662209216 shampoo_preconditioner_list.py:621] Rank 1: ShampooPreconditionerList Total Bytes: 663552
I0315 19:16:26.821249 140572191630528 shampoo_preconditioner_list.py:621] Rank 2: ShampooPreconditionerList Total Bytes: 663552
I0315 19:16:26.821275 140186397963456 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 15.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 15 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 19:16:26.821286 140531948930240 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 19.block_0 ([torch.Size([1, 1])]) for Parameter 19 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0315 19:16:26.821290 140257611351232 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 11.block_0 (torch.Size([256, 768, 3])) for Parameter 11 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 19:16:26.821293 140072725882048 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 21.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 21 (torch.Size([256, 128, 2, 2])), Block block_0 (torch.Size([256, 512])).
I0315 19:16:26.821333 140321624544448 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 17.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 17 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 19:16:26.821341 139912558875840 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([64, 288])) for Parameter 2 (torch.Size([64, 32, 3, 3])), Block block_0 (torch.Size([64, 288])).
I0315 19:16:26.821359 140257611351232 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 12.block_0 (torch.Size([0])) for Parameter 12 (torch.Size([128, 256, 3, 3])), Block block_0 (torch.Size([128, 768, 3])).
I0315 19:16:26.821410 140257611351232 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 13.block_0 (torch.Size([0])) for Parameter 13 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 19:16:26.821411 139912558875840 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 19:16:26.821411 140072725882048 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 22.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 22 (torch.Size([128, 64, 2, 2])), Block block_0 (torch.Size([128, 256])).
I0315 19:16:26.821419 140531948930240 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 20.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 20 (torch.Size([512, 256, 2, 2])), Block block_0 (torch.Size([512, 1024])).
I0315 19:16:26.821423 140186397963456 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 16.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 16 (torch.Size([32, 64, 3, 3])), Block block_0 (torch.Size([32, 576])).
I0315 19:16:26.821434 140321624544448 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 18.block_0 ([torch.Size([0])]) for Parameter 18 (torch.Size([1, 32, 1, 1])), Block block_0 (torch.Size([32])).
I0315 19:16:26.821434 139629662209216 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([32, 1, 3, 3])), Block block_0 (torch.Size([288])).
I0315 19:16:26.821439 140572191630528 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([32, 1, 3, 3])), Block block_0 (torch.Size([288])).
I0315 19:16:26.821465 139912558875840 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 4.block_0 (torch.Size([0])) for Parameter 4 (torch.Size([128, 64, 3, 3])), Block block_0 (torch.Size([128, 576])).
I0315 19:16:26.821471 140257611351232 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 14.block_0 (torch.Size([0])) for Parameter 14 (torch.Size([64, 128, 3, 3])), Block block_0 (torch.Size([64, 384, 3])).
I0315 19:16:26.821516 139629662209216 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 19:16:26.821527 140186397963456 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 17.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 17 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 19:16:26.821518 140572191630528 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 19:16:26.821530 139912558875840 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 5.block_0 (torch.Size([0])) for Parameter 5 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 19:16:26.821532 140257611351232 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 15.block_0 (torch.Size([0])) for Parameter 15 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 19:16:26.821543 140321624544448 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 19.block_0 ([torch.Size([0])]) for Parameter 19 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0315 19:16:26.821545 140072725882048 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 23.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 23 (torch.Size([64, 32, 2, 2])), Block block_0 (torch.Size([64, 128])).
I0315 19:16:26.821583 139629662209216 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([64, 32, 3, 3])), Block block_0 (torch.Size([64, 288])).
I0315 19:16:26.821585 139912558875840 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 6.block_0 (torch.Size([0])) for Parameter 6 (torch.Size([256, 128, 3, 3])), Block block_0 (torch.Size([256, 384, 3])).
I0315 19:16:26.821586 140572191630528 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([64, 32, 3, 3])), Block block_0 (torch.Size([64, 288])).
I0315 19:16:26.821609 140186397963456 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 18.block_0 ([torch.Size([0])]) for Parameter 18 (torch.Size([1, 32, 1, 1])), Block block_0 (torch.Size([32])).
I0315 19:16:26.821615 140257611351232 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 16.block_0 (torch.Size([32, 576])) for Parameter 16 (torch.Size([32, 64, 3, 3])), Block block_0 (torch.Size([32, 576])).
I0315 19:16:26.821638 140572191630528 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 19:16:26.821643 139629662209216 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 19:16:26.821657 140072725882048 shampoo_preconditioner_list.py:612] Rank 5: ShampooPreconditionerList Numel Breakdown: (165888, 2097314, 174080, 671744, 696320, 327698, 426002, 1310738, 1703954, 1048738, 655522, 1310738, 1212434, 327698, 303122, 671744, 665600, 2097314, 2048, 2, 2621440, 655360, 163840, 40960)
I0315 19:16:26.821667 140321624544448 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 20.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 20 (torch.Size([512, 256, 2, 2])), Block block_0 (torch.Size([512, 1024])).
I0315 19:16:26.821678 140257611351232 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 17.block_0 (torch.Size([0])) for Parameter 17 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 19:16:26.821678 139912558875840 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 7.block_0 (torch.Size([256, 768, 3])) for Parameter 7 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 19:16:26.821695 139629662209216 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 4.block_0 (torch.Size([0])) for Parameter 4 (torch.Size([128, 64, 3, 3])), Block block_0 (torch.Size([128, 576])).
I0315 19:16:26.821697 140186397963456 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 19.block_0 ([torch.Size([0])]) for Parameter 19 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0315 19:16:26.821700 140572191630528 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 4.block_0 (torch.Size([0])) for Parameter 4 (torch.Size([128, 64, 3, 3])), Block block_0 (torch.Size([128, 576])).
I0315 19:16:26.821711 140072725882048 shampoo_preconditioner_list.py:615] Rank 5: ShampooPreconditionerList Bytes Breakdown: (663552, 8389256, 696320, 2686976)
I0315 19:16:26.821731 140257611351232 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 18.block_0 (torch.Size([0])) for Parameter 18 (torch.Size([1, 32, 1, 1])), Block block_0 (torch.Size([32])).
I0315 19:16:26.821747 140072725882048 shampoo_preconditioner_list.py:618] Rank 5: ShampooPreconditionerList Total Elements: 19350298
I0315 19:16:26.821743 139912558875840 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 8.block_0 (torch.Size([0])) for Parameter 8 (torch.Size([512, 256, 3, 3])), Block block_0 (torch.Size([512, 768, 3])).
I0315 19:16:26.821754 140572191630528 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 5.block_0 (torch.Size([0])) for Parameter 5 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 19:16:26.821756 139629662209216 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 5.block_0 (torch.Size([0])) for Parameter 5 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 19:16:26.821777 140072725882048 shampoo_preconditioner_list.py:621] Rank 5: ShampooPreconditionerList Total Bytes: 12436104
I0315 19:16:26.821789 140257611351232 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 19.block_0 (torch.Size([0])) for Parameter 19 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0315 19:16:26.821798 139912558875840 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 9.block_0 (torch.Size([0])) for Parameter 9 (torch.Size([512, 512, 3, 3])), Block block_0 (torch.Size([512, 512, 9])).
I0315 19:16:26.821797 140321624544448 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 21.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 21 (torch.Size([256, 128, 2, 2])), Block block_0 (torch.Size([256, 512])).
I0315 19:16:26.821802 140186397963456 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 20.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 20 (torch.Size([512, 256, 2, 2])), Block block_0 (torch.Size([512, 1024])).
I0315 19:16:26.821805 140572191630528 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 6.block_0 (torch.Size([0])) for Parameter 6 (torch.Size([256, 128, 3, 3])), Block block_0 (torch.Size([256, 384, 3])).
I0315 19:16:26.821808 139629662209216 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 6.block_0 (torch.Size([0])) for Parameter 6 (torch.Size([256, 128, 3, 3])), Block block_0 (torch.Size([256, 384, 3])).
I0315 19:16:26.821842 140257611351232 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 20.block_0 (torch.Size([0])) for Parameter 20 (torch.Size([512, 256, 2, 2])), Block block_0 (torch.Size([512, 1024])).
I0315 19:16:26.821862 140572191630528 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 7.block_0 (torch.Size([0])) for Parameter 7 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 19:16:26.821862 139912558875840 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 10.block_0 (torch.Size([0])) for Parameter 10 (torch.Size([256, 512, 3, 3])), Block block_0 (torch.Size([256, 512, 9])).
I0315 19:16:26.821866 139629662209216 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 7.block_0 (torch.Size([0])) for Parameter 7 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 19:16:26.821900 140257611351232 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 21.block_0 (torch.Size([0])) for Parameter 21 (torch.Size([256, 128, 2, 2])), Block block_0 (torch.Size([256, 512])).
I0315 19:16:26.821914 139912558875840 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 11.block_0 (torch.Size([0])) for Parameter 11 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 19:16:26.821916 140321624544448 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 22.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 22 (torch.Size([128, 64, 2, 2])), Block block_0 (torch.Size([128, 256])).
I0315 19:16:26.821918 140186397963456 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 21.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 21 (torch.Size([256, 128, 2, 2])), Block block_0 (torch.Size([256, 512])).
I0315 19:16:26.821921 140572191630528 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 8.block_0 (torch.Size([0])) for Parameter 8 (torch.Size([512, 256, 3, 3])), Block block_0 (torch.Size([512, 768, 3])).
I0315 19:16:26.821957 140257611351232 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 22.block_0 (torch.Size([0])) for Parameter 22 (torch.Size([128, 64, 2, 2])), Block block_0 (torch.Size([128, 256])).
I0315 19:16:26.821966 139912558875840 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 12.block_0 (torch.Size([0])) for Parameter 12 (torch.Size([128, 256, 3, 3])), Block block_0 (torch.Size([128, 768, 3])).
I0315 19:16:26.821973 140572191630528 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 9.block_0 (torch.Size([0])) for Parameter 9 (torch.Size([512, 512, 3, 3])), Block block_0 (torch.Size([512, 512, 9])).
I0315 19:16:26.821970 139629662209216 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 8.block_0 (torch.Size([512, 768, 3])) for Parameter 8 (torch.Size([512, 256, 3, 3])), Block block_0 (torch.Size([512, 768, 3])).
I0315 19:16:26.821969 140072725882048 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([32, 1, 3, 3])), Block block_0 (torch.Size([288])).
I0315 19:16:26.821982 140531948930240 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 21.block_0 ([torch.Size([256, 256]), torch.Size([512, 512])]) for Parameter 21 (torch.Size([256, 128, 2, 2])), Block block_0 (torch.Size([256, 512])).
I0315 19:16:26.822014 140257611351232 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 23.block_0 (torch.Size([0])) for Parameter 23 (torch.Size([64, 32, 2, 2])), Block block_0 (torch.Size([64, 128])).
I0315 19:16:26.822015 139912558875840 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 13.block_0 (torch.Size([0])) for Parameter 13 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 19:16:26.822022 140186397963456 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 22.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 22 (torch.Size([128, 64, 2, 2])), Block block_0 (torch.Size([128, 256])).
I0315 19:16:26.822039 139629662209216 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 9.block_0 (torch.Size([0])) for Parameter 9 (torch.Size([512, 512, 3, 3])), Block block_0 (torch.Size([512, 512, 9])).
I0315 19:16:26.822047 140072725882048 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 19:16:26.822065 139912558875840 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 14.block_0 (torch.Size([0])) for Parameter 14 (torch.Size([64, 128, 3, 3])), Block block_0 (torch.Size([64, 384, 3])).
I0315 19:16:26.822077 140572191630528 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 10.block_0 (torch.Size([256, 512, 9])) for Parameter 10 (torch.Size([256, 512, 3, 3])), Block block_0 (torch.Size([256, 512, 9])).
I0315 19:16:26.822085 140257611351232 shampoo_preconditioner_list.py:375] Rank 4: AdaGradPreconditionerList Numel Breakdown: (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 589824, 0, 0, 0, 0, 18432, 0, 0, 0, 0, 0, 0, 0)
I0315 19:16:26.822102 139629662209216 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 10.block_0 (torch.Size([0])) for Parameter 10 (torch.Size([256, 512, 3, 3])), Block block_0 (torch.Size([256, 512, 9])).
I0315 19:16:26.822105 140072725882048 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([64, 32, 3, 3])), Block block_0 (torch.Size([64, 288])).
I0315 19:16:26.822115 139912558875840 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 15.block_0 (torch.Size([0])) for Parameter 15 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 19:16:26.822122 140257611351232 shampoo_preconditioner_list.py:378] Rank 4: AdaGradPreconditionerList Bytes Breakdown: (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2359296, 0, 0, 0, 0, 73728, 0, 0, 0, 0, 0, 0, 0)
I0315 19:16:26.822129 140186397963456 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 23.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 23 (torch.Size([64, 32, 2, 2])), Block block_0 (torch.Size([64, 128])).
I0315 19:16:26.822121 140321624544448 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 23.block_0 ([torch.Size([64, 64]), torch.Size([128, 128])]) for Parameter 23 (torch.Size([64, 32, 2, 2])), Block block_0 (torch.Size([64, 128])).
I0315 19:16:26.822152 140257611351232 shampoo_preconditioner_list.py:381] Rank 4: AdaGradPreconditionerList Total Elements: 608256
I0315 19:16:26.822155 139629662209216 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 11.block_0 (torch.Size([0])) for Parameter 11 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 19:16:26.822161 140572191630528 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 11.block_0 (torch.Size([0])) for Parameter 11 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 19:16:26.822164 139912558875840 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 16.block_0 (torch.Size([0])) for Parameter 16 (torch.Size([32, 64, 3, 3])), Block block_0 (torch.Size([32, 576])).
I0315 19:16:26.822187 140257611351232 shampoo_preconditioner_list.py:384] Rank 4: AdaGradPreconditionerList Total Bytes: 2433024
I0315 19:16:26.822206 139629662209216 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 12.block_0 (torch.Size([0])) for Parameter 12 (torch.Size([128, 256, 3, 3])), Block block_0 (torch.Size([128, 768, 3])).
I0315 19:16:26.822206 140072725882048 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([64, 576])) for Parameter 3 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 19:16:26.822213 139912558875840 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 17.block_0 (torch.Size([0])) for Parameter 17 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 19:16:26.822214 140572191630528 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 12.block_0 (torch.Size([0])) for Parameter 12 (torch.Size([128, 256, 3, 3])), Block block_0 (torch.Size([128, 768, 3])).
I0315 19:16:26.822213 140531948930240 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 22.block_0 ([torch.Size([128, 128]), torch.Size([256, 256])]) for Parameter 22 (torch.Size([128, 64, 2, 2])), Block block_0 (torch.Size([128, 256])).
I0315 19:16:26.822231 140186397963456 shampoo_preconditioner_list.py:612] Rank 0: ShampooPreconditionerList Numel Breakdown: (165888, 2097314, 174080, 671744, 696320, 327698, 426002, 1310738, 1703954, 1048738, 655522, 1310738, 1212434, 327698, 303122, 671744, 665600, 2097314, 2048, 2, 2621440, 655360, 163840, 40960)
I0315 19:16:26.822248 140321624544448 shampoo_preconditioner_list.py:612] Rank 7: ShampooPreconditionerList Numel Breakdown: (165888, 2097314, 174080, 671744, 696320, 327698, 426002, 1310738, 1703954, 1048738, 655522, 1310738, 1212434, 327698, 303122, 671744, 665600, 2097314, 2048, 2, 2621440, 655360, 163840, 40960)
I0315 19:16:26.822263 139629662209216 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 13.block_0 (torch.Size([0])) for Parameter 13 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 19:16:26.822268 140186397963456 shampoo_preconditioner_list.py:615] Rank 0: ShampooPreconditionerList Bytes Breakdown: (663552,)
I0315 19:16:26.822265 140572191630528 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 13.block_0 (torch.Size([0])) for Parameter 13 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 19:16:26.822264 139912558875840 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 18.block_0 (torch.Size([0])) for Parameter 18 (torch.Size([1, 32, 1, 1])), Block block_0 (torch.Size([32])).
I0315 19:16:26.822274 140072725882048 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 4.block_0 (torch.Size([0])) for Parameter 4 (torch.Size([128, 64, 3, 3])), Block block_0 (torch.Size([128, 576])).
I0315 19:16:26.822297 140186397963456 shampoo_preconditioner_list.py:618] Rank 0: ShampooPreconditionerList Total Elements: 19350298
I0315 19:16:26.822302 140321624544448 shampoo_preconditioner_list.py:615] Rank 7: ShampooPreconditionerList Bytes Breakdown: (663552, 8389256, 696320, 2686976, 2785280, 1310792)
I0315 19:16:26.822314 139629662209216 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 14.block_0 (torch.Size([0])) for Parameter 14 (torch.Size([64, 128, 3, 3])), Block block_0 (torch.Size([64, 384, 3])).
I0315 19:16:26.822314 139912558875840 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 19.block_0 (torch.Size([0])) for Parameter 19 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0315 19:16:26.822316 140572191630528 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 14.block_0 (torch.Size([0])) for Parameter 14 (torch.Size([64, 128, 3, 3])), Block block_0 (torch.Size([64, 384, 3])).
I0315 19:16:26.822330 140072725882048 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 5.block_0 (torch.Size([0])) for Parameter 5 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 19:16:26.822337 140321624544448 shampoo_preconditioner_list.py:618] Rank 7: ShampooPreconditionerList Total Elements: 19350298
I0315 19:16:26.822340 140186397963456 shampoo_preconditioner_list.py:621] Rank 0: ShampooPreconditionerList Total Bytes: 663552
I0315 19:16:26.822348 140531948930240 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 23.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 23 (torch.Size([64, 32, 2, 2])), Block block_0 (torch.Size([64, 128])).
I0315 19:16:26.822364 140572191630528 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 15.block_0 (torch.Size([0])) for Parameter 15 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 19:16:26.822363 139629662209216 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 15.block_0 (torch.Size([0])) for Parameter 15 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 19:16:26.822365 139912558875840 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 20.block_0 (torch.Size([0])) for Parameter 20 (torch.Size([512, 256, 2, 2])), Block block_0 (torch.Size([512, 1024])).
I0315 19:16:26.822367 140321624544448 shampoo_preconditioner_list.py:621] Rank 7: ShampooPreconditionerList Total Bytes: 16532176
I0315 19:16:26.822397 140072725882048 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 6.block_0 (torch.Size([0])) for Parameter 6 (torch.Size([256, 128, 3, 3])), Block block_0 (torch.Size([256, 384, 3])).
I0315 19:16:26.822411 140572191630528 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 16.block_0 (torch.Size([0])) for Parameter 16 (torch.Size([32, 64, 3, 3])), Block block_0 (torch.Size([32, 576])).
I0315 19:16:26.822412 139912558875840 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 21.block_0 (torch.Size([0])) for Parameter 21 (torch.Size([256, 128, 2, 2])), Block block_0 (torch.Size([256, 512])).
I0315 19:16:26.822413 139629662209216 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 16.block_0 (torch.Size([0])) for Parameter 16 (torch.Size([32, 64, 3, 3])), Block block_0 (torch.Size([32, 576])).
I0315 19:16:26.822459 140572191630528 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 17.block_0 (torch.Size([0])) for Parameter 17 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 19:16:26.822461 139912558875840 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 22.block_0 (torch.Size([0])) for Parameter 22 (torch.Size([128, 64, 2, 2])), Block block_0 (torch.Size([128, 256])).
I0315 19:16:26.822462 139629662209216 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 17.block_0 (torch.Size([0])) for Parameter 17 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 19:16:26.822472 140072725882048 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 7.block_0 (torch.Size([0])) for Parameter 7 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 19:16:26.822488 140531948930240 shampoo_preconditioner_list.py:612] Rank 6: ShampooPreconditionerList Numel Breakdown: (165888, 2097314, 174080, 671744, 696320, 327698, 426002, 1310738, 1703954, 1048738, 655522, 1310738, 1212434, 327698, 303122, 671744, 665600, 2097314, 2048, 2, 2621440, 655360, 163840, 40960)
I0315 19:16:26.822519 139629662209216 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 18.block_0 (torch.Size([0])) for Parameter 18 (torch.Size([1, 32, 1, 1])), Block block_0 (torch.Size([32])).
I0315 19:16:26.822518 139912558875840 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 23.block_0 (torch.Size([0])) for Parameter 23 (torch.Size([64, 32, 2, 2])), Block block_0 (torch.Size([64, 128])).
I0315 19:16:26.822515 140572191630528 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 18.block_0 (torch.Size([0])) for Parameter 18 (torch.Size([1, 32, 1, 1])), Block block_0 (torch.Size([32])).
I0315 19:16:26.822530 140072725882048 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 8.block_0 (torch.Size([0])) for Parameter 8 (torch.Size([512, 256, 3, 3])), Block block_0 (torch.Size([512, 768, 3])).
I0315 19:16:26.822540 140531948930240 shampoo_preconditioner_list.py:615] Rank 6: ShampooPreconditionerList Bytes Breakdown: (663552, 8389256, 696320, 2686976, 2785280, 1310792, 1704008)
I0315 19:16:26.822538 140186397963456 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([32, 1, 3, 3])), Block block_0 (torch.Size([288])).
I0315 19:16:26.822571 140572191630528 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 19.block_0 (torch.Size([0])) for Parameter 19 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0315 19:16:26.822577 140531948930240 shampoo_preconditioner_list.py:618] Rank 6: ShampooPreconditionerList Total Elements: 19350298
I0315 19:16:26.822561 140321624544448 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([32, 1, 3, 3])), Block block_0 (torch.Size([288])).
I0315 19:16:26.822576 139629662209216 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 19.block_0 (torch.Size([0])) for Parameter 19 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0315 19:16:26.822583 140072725882048 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 9.block_0 (torch.Size([0])) for Parameter 9 (torch.Size([512, 512, 3, 3])), Block block_0 (torch.Size([512, 512, 9])).
I0315 19:16:26.822590 139912558875840 shampoo_preconditioner_list.py:375] Rank 3: AdaGradPreconditionerList Numel Breakdown: (0, 0, 18432, 0, 0, 0, 0, 589824, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)
I0315 19:16:26.822598 140186397963456 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 19:16:26.822608 140531948930240 shampoo_preconditioner_list.py:621] Rank 6: ShampooPreconditionerList Total Bytes: 18236184
I0315 19:16:26.822626 139912558875840 shampoo_preconditioner_list.py:378] Rank 3: AdaGradPreconditionerList Bytes Breakdown: (0, 0, 73728, 0, 0, 0, 0, 2359296, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)
I0315 19:16:26.822626 139629662209216 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 20.block_0 (torch.Size([0])) for Parameter 20 (torch.Size([512, 256, 2, 2])), Block block_0 (torch.Size([512, 1024])).
I0315 19:16:26.822627 140572191630528 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 20.block_0 (torch.Size([0])) for Parameter 20 (torch.Size([512, 256, 2, 2])), Block block_0 (torch.Size([512, 1024])).
I0315 19:16:26.822634 140072725882048 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 10.block_0 (torch.Size([0])) for Parameter 10 (torch.Size([256, 512, 3, 3])), Block block_0 (torch.Size([256, 512, 9])).
I0315 19:16:26.822647 140186397963456 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([64, 32, 3, 3])), Block block_0 (torch.Size([64, 288])).
I0315 19:16:26.822665 139912558875840 shampoo_preconditioner_list.py:381] Rank 3: AdaGradPreconditionerList Total Elements: 608256
I0315 19:16:26.822675 139629662209216 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 21.block_0 (torch.Size([0])) for Parameter 21 (torch.Size([256, 128, 2, 2])), Block block_0 (torch.Size([256, 512])).
I0315 19:16:26.822676 140572191630528 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 21.block_0 (torch.Size([0])) for Parameter 21 (torch.Size([256, 128, 2, 2])), Block block_0 (torch.Size([256, 512])).
I0315 19:16:26.822686 140072725882048 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 11.block_0 (torch.Size([0])) for Parameter 11 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 19:16:26.822676 140321624544448 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([1024, 9])) for Parameter 1 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 19:16:26.822693 140186397963456 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 19:16:26.822700 139912558875840 shampoo_preconditioner_list.py:384] Rank 3: AdaGradPreconditionerList Total Bytes: 2433024
I0315 19:16:26.822724 140572191630528 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 22.block_0 (torch.Size([0])) for Parameter 22 (torch.Size([128, 64, 2, 2])), Block block_0 (torch.Size([128, 256])).
I0315 19:16:26.822723 139629662209216 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 22.block_0 (torch.Size([0])) for Parameter 22 (torch.Size([128, 64, 2, 2])), Block block_0 (torch.Size([128, 256])).
I0315 19:16:26.822738 140186397963456 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 4.block_0 (torch.Size([0])) for Parameter 4 (torch.Size([128, 64, 3, 3])), Block block_0 (torch.Size([128, 576])).
I0315 19:16:26.822738 140072725882048 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 12.block_0 (torch.Size([0])) for Parameter 12 (torch.Size([128, 256, 3, 3])), Block block_0 (torch.Size([128, 768, 3])).
I0315 19:16:26.822751 140321624544448 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([64, 32, 3, 3])), Block block_0 (torch.Size([64, 288])).
I0315 19:16:26.822772 139629662209216 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 23.block_0 (torch.Size([0])) for Parameter 23 (torch.Size([64, 32, 2, 2])), Block block_0 (torch.Size([64, 128])).
I0315 19:16:26.822779 140572191630528 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 23.block_0 (torch.Size([0])) for Parameter 23 (torch.Size([64, 32, 2, 2])), Block block_0 (torch.Size([64, 128])).
I0315 19:16:26.822785 140186397963456 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 5.block_0 (torch.Size([0])) for Parameter 5 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 19:16:26.822788 140072725882048 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 13.block_0 (torch.Size([0])) for Parameter 13 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 19:16:26.822820 140321624544448 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 19:16:26.822830 140186397963456 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 6.block_0 (torch.Size([0])) for Parameter 6 (torch.Size([256, 128, 3, 3])), Block block_0 (torch.Size([256, 384, 3])).
I0315 19:16:26.822838 140072725882048 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 14.block_0 (torch.Size([0])) for Parameter 14 (torch.Size([64, 128, 3, 3])), Block block_0 (torch.Size([64, 384, 3])).
I0315 19:16:26.822839 139629662209216 shampoo_preconditioner_list.py:375] Rank 1: AdaGradPreconditionerList Numel Breakdown: (0, 0, 0, 0, 0, 0, 0, 0, 1179648, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)
I0315 19:16:26.822848 140572191630528 shampoo_preconditioner_list.py:375] Rank 2: AdaGradPreconditionerList Numel Breakdown: (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1179648, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)
I0315 19:16:26.822836 140531948930240 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([288])) for Parameter 0 (torch.Size([32, 1, 3, 3])), Block block_0 (torch.Size([288])).
I0315 19:16:26.822874 139629662209216 shampoo_preconditioner_list.py:378] Rank 1: AdaGradPreconditionerList Bytes Breakdown: (0, 0, 0, 0, 0, 0, 0, 0, 4718592, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)
I0315 19:16:26.822883 140186397963456 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 7.block_0 (torch.Size([0])) for Parameter 7 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 19:16:26.822888 140572191630528 shampoo_preconditioner_list.py:378] Rank 2: AdaGradPreconditionerList Bytes Breakdown: (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4718592, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)
I0315 19:16:26.822905 139629662209216 shampoo_preconditioner_list.py:381] Rank 1: AdaGradPreconditionerList Total Elements: 1179648
I0315 19:16:26.822920 140572191630528 shampoo_preconditioner_list.py:381] Rank 2: AdaGradPreconditionerList Total Elements: 1179648
I0315 19:16:26.822928 140186397963456 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 8.block_0 (torch.Size([0])) for Parameter 8 (torch.Size([512, 256, 3, 3])), Block block_0 (torch.Size([512, 768, 3])).
I0315 19:16:26.822926 140531948930240 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 19:16:26.822927 140072725882048 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 15.block_0 (torch.Size([64, 576])) for Parameter 15 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 19:16:26.822941 139629662209216 shampoo_preconditioner_list.py:384] Rank 1: AdaGradPreconditionerList Total Bytes: 4718592
I0315 19:16:26.822949 140572191630528 shampoo_preconditioner_list.py:384] Rank 2: AdaGradPreconditionerList Total Bytes: 4718592
I0315 19:16:26.822985 140531948930240 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([64, 32, 3, 3])), Block block_0 (torch.Size([64, 288])).
I0315 19:16:26.822989 140072725882048 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 16.block_0 (torch.Size([0])) for Parameter 16 (torch.Size([32, 64, 3, 3])), Block block_0 (torch.Size([32, 576])).
I0315 19:16:26.823038 140186397963456 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 9.block_0 (torch.Size([512, 512, 9])) for Parameter 9 (torch.Size([512, 512, 3, 3])), Block block_0 (torch.Size([512, 512, 9])).
I0315 19:16:26.823052 140531948930240 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 19:16:26.823067 140072725882048 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 17.block_0 (torch.Size([1024, 9])) for Parameter 17 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 19:16:26.823107 140531948930240 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 4.block_0 (torch.Size([0])) for Parameter 4 (torch.Size([128, 64, 3, 3])), Block block_0 (torch.Size([128, 576])).
I0315 19:16:26.823112 140186397963456 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 10.block_0 (torch.Size([0])) for Parameter 10 (torch.Size([256, 512, 3, 3])), Block block_0 (torch.Size([256, 512, 9])).
I0315 19:16:26.823139 140072725882048 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 18.block_0 (torch.Size([0])) for Parameter 18 (torch.Size([1, 32, 1, 1])), Block block_0 (torch.Size([32])).
I0315 19:16:26.823160 140186397963456 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 11.block_0 (torch.Size([0])) for Parameter 11 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 19:16:26.823198 140072725882048 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 19.block_0 (torch.Size([0])) for Parameter 19 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0315 19:16:26.823205 140186397963456 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 12.block_0 (torch.Size([0])) for Parameter 12 (torch.Size([128, 256, 3, 3])), Block block_0 (torch.Size([128, 768, 3])).
I0315 19:16:26.823249 140186397963456 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 13.block_0 (torch.Size([0])) for Parameter 13 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 19:16:26.823286 140072725882048 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 20.block_0 (torch.Size([512, 1024])) for Parameter 20 (torch.Size([512, 256, 2, 2])), Block block_0 (torch.Size([512, 1024])).
I0315 19:16:26.823302 140186397963456 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 14.block_0 (torch.Size([0])) for Parameter 14 (torch.Size([64, 128, 3, 3])), Block block_0 (torch.Size([64, 384, 3])).
I0315 19:16:26.823355 140186397963456 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 15.block_0 (torch.Size([0])) for Parameter 15 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 19:16:26.823360 140072725882048 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 21.block_0 (torch.Size([0])) for Parameter 21 (torch.Size([256, 128, 2, 2])), Block block_0 (torch.Size([256, 512])).
I0315 19:16:26.823408 140186397963456 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 16.block_0 (torch.Size([0])) for Parameter 16 (torch.Size([32, 64, 3, 3])), Block block_0 (torch.Size([32, 576])).
I0315 19:16:26.823415 140072725882048 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 22.block_0 (torch.Size([0])) for Parameter 22 (torch.Size([128, 64, 2, 2])), Block block_0 (torch.Size([128, 256])).
I0315 19:16:26.823414 140321624544448 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 4.block_0 (torch.Size([128, 576])) for Parameter 4 (torch.Size([128, 64, 3, 3])), Block block_0 (torch.Size([128, 576])).
I0315 19:16:26.823458 140186397963456 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 17.block_0 (torch.Size([0])) for Parameter 17 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 19:16:26.823476 140072725882048 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 23.block_0 (torch.Size([0])) for Parameter 23 (torch.Size([64, 32, 2, 2])), Block block_0 (torch.Size([64, 128])).
I0315 19:16:26.823511 140186397963456 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 18.block_0 (torch.Size([0])) for Parameter 18 (torch.Size([1, 32, 1, 1])), Block block_0 (torch.Size([32])).
I0315 19:16:26.823529 140321624544448 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 5.block_0 (torch.Size([0])) for Parameter 5 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 19:16:26.823547 140072725882048 shampoo_preconditioner_list.py:375] Rank 5: AdaGradPreconditionerList Numel Breakdown: (0, 0, 0, 36864, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 36864, 0, 9216, 0, 0, 524288, 0, 0, 0)
I0315 19:16:26.823562 140186397963456 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 19.block_0 (torch.Size([0])) for Parameter 19 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0315 19:16:26.823584 140072725882048 shampoo_preconditioner_list.py:378] Rank 5: AdaGradPreconditionerList Bytes Breakdown: (0, 0, 0, 147456, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 147456, 0, 36864, 0, 0, 2097152, 0, 0, 0)
I0315 19:16:26.823601 140321624544448 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 6.block_0 (torch.Size([0])) for Parameter 6 (torch.Size([256, 128, 3, 3])), Block block_0 (torch.Size([256, 384, 3])).
I0315 19:16:26.823614 140186397963456 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 20.block_0 (torch.Size([0])) for Parameter 20 (torch.Size([512, 256, 2, 2])), Block block_0 (torch.Size([512, 1024])).
I0315 19:16:26.823623 140072725882048 shampoo_preconditioner_list.py:381] Rank 5: AdaGradPreconditionerList Total Elements: 607232
I0315 19:16:26.823654 140072725882048 shampoo_preconditioner_list.py:384] Rank 5: AdaGradPreconditionerList Total Bytes: 2428928
I0315 19:16:26.823658 140186397963456 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 21.block_0 (torch.Size([0])) for Parameter 21 (torch.Size([256, 128, 2, 2])), Block block_0 (torch.Size([256, 512])).
I0315 19:16:26.823669 140321624544448 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 7.block_0 (torch.Size([0])) for Parameter 7 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 19:16:26.823703 140186397963456 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 22.block_0 (torch.Size([0])) for Parameter 22 (torch.Size([128, 64, 2, 2])), Block block_0 (torch.Size([128, 256])).
I0315 19:16:26.823731 140321624544448 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 8.block_0 (torch.Size([0])) for Parameter 8 (torch.Size([512, 256, 3, 3])), Block block_0 (torch.Size([512, 768, 3])).
I0315 19:16:26.823753 140186397963456 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 23.block_0 (torch.Size([0])) for Parameter 23 (torch.Size([64, 32, 2, 2])), Block block_0 (torch.Size([64, 128])).
I0315 19:16:26.823792 140321624544448 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 9.block_0 (torch.Size([0])) for Parameter 9 (torch.Size([512, 512, 3, 3])), Block block_0 (torch.Size([512, 512, 9])).
I0315 19:16:26.823822 140186397963456 shampoo_preconditioner_list.py:375] Rank 0: AdaGradPreconditionerList Numel Breakdown: (0, 0, 0, 0, 0, 0, 0, 0, 0, 2359296, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)
I0315 19:16:26.823850 140321624544448 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 10.block_0 (torch.Size([0])) for Parameter 10 (torch.Size([256, 512, 3, 3])), Block block_0 (torch.Size([256, 512, 9])).
I0315 19:16:26.823860 140186397963456 shampoo_preconditioner_list.py:378] Rank 0: AdaGradPreconditionerList Bytes Breakdown: (0, 0, 0, 0, 0, 0, 0, 0, 0, 9437184, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)
I0315 19:16:26.823889 140186397963456 shampoo_preconditioner_list.py:381] Rank 0: AdaGradPreconditionerList Total Elements: 2359296
I0315 19:16:26.823909 140321624544448 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 11.block_0 (torch.Size([0])) for Parameter 11 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 19:16:26.823922 140186397963456 shampoo_preconditioner_list.py:384] Rank 0: AdaGradPreconditionerList Total Bytes: 9437184
I0315 19:16:26.823896 140531948930240 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 5.block_0 (torch.Size([128, 384, 3])) for Parameter 5 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 19:16:26.824007 140321624544448 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 12.block_0 (torch.Size([128, 768, 3])) for Parameter 12 (torch.Size([128, 256, 3, 3])), Block block_0 (torch.Size([128, 768, 3])).
I0315 19:16:26.824018 140531948930240 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 6.block_0 (torch.Size([256, 384, 3])) for Parameter 6 (torch.Size([256, 128, 3, 3])), Block block_0 (torch.Size([256, 384, 3])).
I0315 19:16:26.824117 140531948930240 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 7.block_0 (torch.Size([0])) for Parameter 7 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 19:16:26.824125 140321624544448 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 13.block_0 (torch.Size([128, 384, 3])) for Parameter 13 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 19:16:26.824111 139912558875840 logger_utils.py:262] Unable to record workload.train_mean information. Continuing without it.
I0315 19:16:26.824116 140257611351232 logger_utils.py:262] Unable to record workload.train_mean information. Continuing without it.
I0315 19:16:26.824177 139912558875840 logger_utils.py:262] Unable to record workload.train_stddev information. Continuing without it.
I0315 19:16:26.824184 140257611351232 logger_utils.py:262] Unable to record workload.train_stddev information. Continuing without it.
I0315 19:16:26.824191 140531948930240 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 8.block_0 (torch.Size([0])) for Parameter 8 (torch.Size([512, 256, 3, 3])), Block block_0 (torch.Size([512, 768, 3])).
I0315 19:16:26.824207 140321624544448 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 14.block_0 (torch.Size([64, 384, 3])) for Parameter 14 (torch.Size([64, 128, 3, 3])), Block block_0 (torch.Size([64, 384, 3])).
I0315 19:16:26.824247 140531948930240 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 9.block_0 (torch.Size([0])) for Parameter 9 (torch.Size([512, 512, 3, 3])), Block block_0 (torch.Size([512, 512, 9])).
I0315 19:16:26.824282 140321624544448 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 15.block_0 (torch.Size([0])) for Parameter 15 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 19:16:26.824308 140531948930240 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 10.block_0 (torch.Size([0])) for Parameter 10 (torch.Size([256, 512, 3, 3])), Block block_0 (torch.Size([256, 512, 9])).
I0315 19:16:26.824346 140321624544448 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 16.block_0 (torch.Size([0])) for Parameter 16 (torch.Size([32, 64, 3, 3])), Block block_0 (torch.Size([32, 576])).
I0315 19:16:26.824368 140531948930240 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 11.block_0 (torch.Size([0])) for Parameter 11 (torch.Size([256, 256, 3, 3])), Block block_0 (torch.Size([256, 768, 3])).
I0315 19:16:26.824407 140321624544448 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 17.block_0 (torch.Size([0])) for Parameter 17 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 19:16:26.824429 140531948930240 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 12.block_0 (torch.Size([0])) for Parameter 12 (torch.Size([128, 256, 3, 3])), Block block_0 (torch.Size([128, 768, 3])).
I0315 19:16:26.824466 140321624544448 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 18.block_0 (torch.Size([0])) for Parameter 18 (torch.Size([1, 32, 1, 1])), Block block_0 (torch.Size([32])).
I0315 19:16:26.824496 140531948930240 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 13.block_0 (torch.Size([0])) for Parameter 13 (torch.Size([128, 128, 3, 3])), Block block_0 (torch.Size([128, 384, 3])).
I0315 19:16:26.824533 140321624544448 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 19.block_0 (torch.Size([0])) for Parameter 19 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0315 19:16:26.824558 140531948930240 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 14.block_0 (torch.Size([0])) for Parameter 14 (torch.Size([64, 128, 3, 3])), Block block_0 (torch.Size([64, 384, 3])).
I0315 19:16:26.824596 140321624544448 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 20.block_0 (torch.Size([0])) for Parameter 20 (torch.Size([512, 256, 2, 2])), Block block_0 (torch.Size([512, 1024])).
I0315 19:16:26.824616 140531948930240 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 15.block_0 (torch.Size([0])) for Parameter 15 (torch.Size([64, 64, 3, 3])), Block block_0 (torch.Size([64, 576])).
I0315 19:16:26.824671 140321624544448 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 21.block_0 (torch.Size([0])) for Parameter 21 (torch.Size([256, 128, 2, 2])), Block block_0 (torch.Size([256, 512])).
I0315 19:16:26.824692 140531948930240 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 16.block_0 (torch.Size([0])) for Parameter 16 (torch.Size([32, 64, 3, 3])), Block block_0 (torch.Size([32, 576])).
I0315 19:16:26.824737 140321624544448 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 22.block_0 (torch.Size([0])) for Parameter 22 (torch.Size([128, 64, 2, 2])), Block block_0 (torch.Size([128, 256])).
I0315 19:16:26.824755 140531948930240 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 17.block_0 (torch.Size([0])) for Parameter 17 (torch.Size([32, 32, 3, 3])), Block block_0 (torch.Size([1024, 9])).
I0315 19:16:26.824832 140321624544448 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 23.block_0 (torch.Size([64, 128])) for Parameter 23 (torch.Size([64, 32, 2, 2])), Block block_0 (torch.Size([64, 128])).
I0315 19:16:26.824847 140531948930240 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 18.block_0 (torch.Size([32])) for Parameter 18 (torch.Size([1, 32, 1, 1])), Block block_0 (torch.Size([32])).
I0315 19:16:26.824925 140321624544448 shampoo_preconditioner_list.py:375] Rank 7: AdaGradPreconditionerList Numel Breakdown: (0, 9216, 0, 0, 73728, 0, 0, 0, 0, 0, 0, 0, 294912, 147456, 73728, 0, 0, 0, 0, 0, 0, 0, 0, 8192)
I0315 19:16:26.824928 140531948930240 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 19.block_0 (torch.Size([1])) for Parameter 19 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0315 19:16:26.824913 140572191630528 logger_utils.py:262] Unable to record workload.train_mean information. Continuing without it.
I0315 19:16:26.824971 140321624544448 shampoo_preconditioner_list.py:378] Rank 7: AdaGradPreconditionerList Bytes Breakdown: (0, 36864, 0, 0, 294912, 0, 0, 0, 0, 0, 0, 0, 1179648, 589824, 294912, 0, 0, 0, 0, 0, 0, 0, 0, 32768)
I0315 19:16:26.824982 140572191630528 logger_utils.py:262] Unable to record workload.train_stddev information. Continuing without it.
I0315 19:16:26.824994 140531948930240 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 20.block_0 (torch.Size([0])) for Parameter 20 (torch.Size([512, 256, 2, 2])), Block block_0 (torch.Size([512, 1024])).
I0315 19:16:26.824977 139629662209216 logger_utils.py:262] Unable to record workload.train_mean information. Continuing without it.
I0315 19:16:26.825009 140321624544448 shampoo_preconditioner_list.py:381] Rank 7: AdaGradPreconditionerList Total Elements: 607232
I0315 19:16:26.825005 140186397963456 submission.py:142] No large parameters detected! Continuing with only Shampoo....
I0315 19:16:26.825040 140321624544448 shampoo_preconditioner_list.py:384] Rank 7: AdaGradPreconditionerList Total Bytes: 2428928
I0315 19:16:26.825045 139629662209216 logger_utils.py:262] Unable to record workload.train_stddev information. Continuing without it.
I0315 19:16:26.825072 140531948930240 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 21.block_0 (torch.Size([256, 512])) for Parameter 21 (torch.Size([256, 128, 2, 2])), Block block_0 (torch.Size([256, 512])).
I0315 19:16:26.825161 140531948930240 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 22.block_0 (torch.Size([128, 256])) for Parameter 22 (torch.Size([128, 64, 2, 2])), Block block_0 (torch.Size([128, 256])).
I0315 19:16:26.825163 140072725882048 logger_utils.py:262] Unable to record workload.train_mean information. Continuing without it.
I0315 19:16:26.825221 140186397963456 submission_runner.py:279] Initializing metrics bundle.
I0315 19:16:26.825235 140531948930240 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 23.block_0 (torch.Size([0])) for Parameter 23 (torch.Size([64, 32, 2, 2])), Block block_0 (torch.Size([64, 128])).
I0315 19:16:26.825239 140072725882048 logger_utils.py:262] Unable to record workload.train_stddev information. Continuing without it.
I0315 19:16:26.825322 140531948930240 shampoo_preconditioner_list.py:375] Rank 6: AdaGradPreconditionerList Numel Breakdown: (288, 0, 0, 0, 0, 147456, 294912, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 32, 1, 0, 131072, 32768, 0)
I0315 19:16:26.825365 140531948930240 shampoo_preconditioner_list.py:378] Rank 6: AdaGradPreconditionerList Bytes Breakdown: (1152, 0, 0, 0, 0, 589824, 1179648, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 128, 4, 0, 524288, 131072, 0)
I0315 19:16:26.825379 140186397963456 submission_runner.py:301] Initializing checkpoint and logger.
I0315 19:16:26.825402 140531948930240 shampoo_preconditioner_list.py:381] Rank 6: AdaGradPreconditionerList Total Elements: 606529
I0315 19:16:26.825432 140531948930240 shampoo_preconditioner_list.py:384] Rank 6: AdaGradPreconditionerList Total Bytes: 2426116
I0315 19:16:26.825845 140186397963456 submission_runner.py:321] Saving meta data to /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_2/fastmri_pytorch/trial_4/meta_data_0.json.
I0315 19:16:26.826026 140186397963456 logger_utils.py:262] Unable to record workload.train_mean information. Continuing without it.
I0315 19:16:26.826076 140186397963456 logger_utils.py:262] Unable to record workload.train_stddev information. Continuing without it.
I0315 19:16:26.826999 140321624544448 logger_utils.py:262] Unable to record workload.train_mean information. Continuing without it.
I0315 19:16:26.827078 140321624544448 logger_utils.py:262] Unable to record workload.train_stddev information. Continuing without it.
I0315 19:16:26.827385 140531948930240 logger_utils.py:262] Unable to record workload.train_mean information. Continuing without it.
I0315 19:16:26.827462 140531948930240 logger_utils.py:262] Unable to record workload.train_stddev information. Continuing without it.
I0315 19:16:27.766345 140186397963456 submission_runner.py:325] Saving flags to /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_2/fastmri_pytorch/trial_4/flags_0.json.
I0315 19:16:27.933130 140186397963456 submission_runner.py:337] Starting training loop.
[rank4]:W0315 19:16:28.121000 48 site-packages/torch/_logging/_internal.py:1081] [0/0] Profiler function <class 'torch.autograd.profiler.record_function'> will be ignored
[rank5]:W0315 19:16:28.121000 49 site-packages/torch/_logging/_internal.py:1081] [0/0] Profiler function <class 'torch.autograd.profiler.record_function'> will be ignored
[rank6]:W0315 19:16:28.121000 50 site-packages/torch/_logging/_internal.py:1081] [0/0] Profiler function <class 'torch.autograd.profiler.record_function'> will be ignored
[rank1]:W0315 19:16:28.121000 45 site-packages/torch/_logging/_internal.py:1081] [0/0] Profiler function <class 'torch.autograd.profiler.record_function'> will be ignored
[rank7]:W0315 19:16:28.121000 51 site-packages/torch/_logging/_internal.py:1081] [0/0] Profiler function <class 'torch.autograd.profiler.record_function'> will be ignored
[rank3]:W0315 19:16:28.121000 47 site-packages/torch/_logging/_internal.py:1081] [0/0] Profiler function <class 'torch.autograd.profiler.record_function'> will be ignored
[rank2]:W0315 19:16:28.121000 46 site-packages/torch/_logging/_internal.py:1081] [0/0] Profiler function <class 'torch.autograd.profiler.record_function'> will be ignored
[rank0]:W0315 19:20:14.309000 44 site-packages/torch/_logging/_internal.py:1081] [0/0] Profiler function <class 'torch.autograd.profiler.record_function'> will be ignored
[rank3]:W0315 19:21:03.265000 47 site-packages/torch/_dynamo/convert_frame.py:844] [2/8] torch._dynamo hit config.cache_size_limit (8)
[rank3]:W0315 19:21:03.265000 47 site-packages/torch/_dynamo/convert_frame.py:844] [2/8]    function: 'forward' (/algorithmic-efficiency/algoperf/workloads/fastmri/fastmri_pytorch/models.py:141)
[rank3]:W0315 19:21:03.265000 47 site-packages/torch/_dynamo/convert_frame.py:844] [2/8]    last reason: 2/0: tensor 'L['x']' requires_grad mismatch. expected requires_grad=0
[rank3]:W0315 19:21:03.265000 47 site-packages/torch/_dynamo/convert_frame.py:844] [2/8] To log all recompilation reasons, use TORCH_LOGS="recompiles".
[rank3]:W0315 19:21:03.265000 47 site-packages/torch/_dynamo/convert_frame.py:844] [2/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.
[rank5]:W0315 19:21:03.269000 49 site-packages/torch/_dynamo/convert_frame.py:844] [2/8] torch._dynamo hit config.cache_size_limit (8)
[rank5]:W0315 19:21:03.269000 49 site-packages/torch/_dynamo/convert_frame.py:844] [2/8]    function: 'forward' (/algorithmic-efficiency/algoperf/workloads/fastmri/fastmri_pytorch/models.py:141)
[rank5]:W0315 19:21:03.269000 49 site-packages/torch/_dynamo/convert_frame.py:844] [2/8]    last reason: 2/0: tensor 'L['x']' requires_grad mismatch. expected requires_grad=0
[rank5]:W0315 19:21:03.269000 49 site-packages/torch/_dynamo/convert_frame.py:844] [2/8] To log all recompilation reasons, use TORCH_LOGS="recompiles".
[rank5]:W0315 19:21:03.269000 49 site-packages/torch/_dynamo/convert_frame.py:844] [2/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.
[rank7]:W0315 19:21:03.269000 51 site-packages/torch/_dynamo/convert_frame.py:844] [2/8] torch._dynamo hit config.cache_size_limit (8)
[rank7]:W0315 19:21:03.269000 51 site-packages/torch/_dynamo/convert_frame.py:844] [2/8]    function: 'forward' (/algorithmic-efficiency/algoperf/workloads/fastmri/fastmri_pytorch/models.py:141)
[rank7]:W0315 19:21:03.269000 51 site-packages/torch/_dynamo/convert_frame.py:844] [2/8]    last reason: 2/0: tensor 'L['x']' requires_grad mismatch. expected requires_grad=0
[rank7]:W0315 19:21:03.269000 51 site-packages/torch/_dynamo/convert_frame.py:844] [2/8] To log all recompilation reasons, use TORCH_LOGS="recompiles".
[rank7]:W0315 19:21:03.269000 51 site-packages/torch/_dynamo/convert_frame.py:844] [2/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.
[rank4]:W0315 19:21:03.285000 48 site-packages/torch/_dynamo/convert_frame.py:844] [2/8] torch._dynamo hit config.cache_size_limit (8)
[rank4]:W0315 19:21:03.285000 48 site-packages/torch/_dynamo/convert_frame.py:844] [2/8]    function: 'forward' (/algorithmic-efficiency/algoperf/workloads/fastmri/fastmri_pytorch/models.py:141)
[rank4]:W0315 19:21:03.285000 48 site-packages/torch/_dynamo/convert_frame.py:844] [2/8]    last reason: 2/0: tensor 'L['x']' requires_grad mismatch. expected requires_grad=0
[rank4]:W0315 19:21:03.285000 48 site-packages/torch/_dynamo/convert_frame.py:844] [2/8] To log all recompilation reasons, use TORCH_LOGS="recompiles".
[rank4]:W0315 19:21:03.285000 48 site-packages/torch/_dynamo/convert_frame.py:844] [2/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.
[rank2]:W0315 19:21:03.322000 46 site-packages/torch/_dynamo/convert_frame.py:844] [2/8] torch._dynamo hit config.cache_size_limit (8)
[rank2]:W0315 19:21:03.322000 46 site-packages/torch/_dynamo/convert_frame.py:844] [2/8]    function: 'forward' (/algorithmic-efficiency/algoperf/workloads/fastmri/fastmri_pytorch/models.py:141)
[rank2]:W0315 19:21:03.322000 46 site-packages/torch/_dynamo/convert_frame.py:844] [2/8]    last reason: 2/0: tensor 'L['x']' requires_grad mismatch. expected requires_grad=0
[rank2]:W0315 19:21:03.322000 46 site-packages/torch/_dynamo/convert_frame.py:844] [2/8] To log all recompilation reasons, use TORCH_LOGS="recompiles".
[rank2]:W0315 19:21:03.322000 46 site-packages/torch/_dynamo/convert_frame.py:844] [2/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.
[rank1]:W0315 19:21:03.458000 45 site-packages/torch/_dynamo/convert_frame.py:844] [2/8] torch._dynamo hit config.cache_size_limit (8)
[rank1]:W0315 19:21:03.458000 45 site-packages/torch/_dynamo/convert_frame.py:844] [2/8]    function: 'forward' (/algorithmic-efficiency/algoperf/workloads/fastmri/fastmri_pytorch/models.py:141)
[rank1]:W0315 19:21:03.458000 45 site-packages/torch/_dynamo/convert_frame.py:844] [2/8]    last reason: 2/0: tensor 'L['x']' requires_grad mismatch. expected requires_grad=0
[rank1]:W0315 19:21:03.458000 45 site-packages/torch/_dynamo/convert_frame.py:844] [2/8] To log all recompilation reasons, use TORCH_LOGS="recompiles".
[rank1]:W0315 19:21:03.458000 45 site-packages/torch/_dynamo/convert_frame.py:844] [2/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.
[rank6]:W0315 19:21:03.559000 50 site-packages/torch/_dynamo/convert_frame.py:844] [2/8] torch._dynamo hit config.cache_size_limit (8)
[rank6]:W0315 19:21:03.559000 50 site-packages/torch/_dynamo/convert_frame.py:844] [2/8]    function: 'forward' (/algorithmic-efficiency/algoperf/workloads/fastmri/fastmri_pytorch/models.py:141)
[rank6]:W0315 19:21:03.559000 50 site-packages/torch/_dynamo/convert_frame.py:844] [2/8]    last reason: 2/0: tensor 'L['x']' requires_grad mismatch. expected requires_grad=0
[rank6]:W0315 19:21:03.559000 50 site-packages/torch/_dynamo/convert_frame.py:844] [2/8] To log all recompilation reasons, use TORCH_LOGS="recompiles".
[rank6]:W0315 19:21:03.559000 50 site-packages/torch/_dynamo/convert_frame.py:844] [2/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.
[rank0]:W0315 19:21:03.581000 44 site-packages/torch/_dynamo/convert_frame.py:844] [2/8] torch._dynamo hit config.cache_size_limit (8)
[rank0]:W0315 19:21:03.581000 44 site-packages/torch/_dynamo/convert_frame.py:844] [2/8]    function: 'forward' (/algorithmic-efficiency/algoperf/workloads/fastmri/fastmri_pytorch/models.py:141)
[rank0]:W0315 19:21:03.581000 44 site-packages/torch/_dynamo/convert_frame.py:844] [2/8]    last reason: 2/0: tensor 'L['x']' requires_grad mismatch. expected requires_grad=0
[rank0]:W0315 19:21:03.581000 44 site-packages/torch/_dynamo/convert_frame.py:844] [2/8] To log all recompilation reasons, use TORCH_LOGS="recompiles".
[rank0]:W0315 19:21:03.581000 44 site-packages/torch/_dynamo/convert_frame.py:844] [2/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.
I0315 19:21:19.516736 140158303368960 logging_writer.py:48] [0] global_step=0, grad_norm=3.72486, loss=1.04869
I0315 19:21:19.838690 140186397963456 submission.py:265] 0) loss = 1.049, grad_norm = 3.725
I0315 19:21:20.665250 140186397963456 spec.py:321] Evaluating on the training split.
I0315 19:28:44.137606 140186397963456 spec.py:333] Evaluating on the validation split.
I0315 19:32:42.271973 140186397963456 spec.py:349] Evaluating on the test split.
I0315 19:36:45.840038 140186397963456 submission_runner.py:469] Time since start: 1217.91s, 	Step: 1, 	{'train/ssim': 0.18409654072352818, 'train/loss': 1.1149094445364816, 'validation/ssim': 0.1786466050027258, 'validation/loss': 1.121061188757386, 'validation/num_examples': 3554, 'test/ssim': 0.1992406347083566, 'test/loss': 1.1195749103340547, 'test/num_examples': 3581, 'score': 291.90677642822266, 'total_duration': 1217.9073350429535, 'accumulated_submission_time': 291.90677642822266, 'accumulated_eval_time': 925.1751244068146, 'accumulated_logging_time': 0}
I0315 19:36:45.848344 140135622489856 logging_writer.py:48] [1] accumulated_eval_time=925.175, accumulated_logging_time=0, accumulated_submission_time=291.907, global_step=1, preemption_count=0, score=291.907, test/loss=1.11957, test/num_examples=3581, test/ssim=0.199241, total_duration=1217.91, train/loss=1.11491, train/ssim=0.184097, validation/loss=1.12106, validation/num_examples=3554, validation/ssim=0.178647
I0315 19:36:47.033900 140135614097152 logging_writer.py:48] [1] global_step=1, grad_norm=3.6416, loss=1.17014
I0315 19:36:47.037014 140186397963456 submission.py:265] 1) loss = 1.170, grad_norm = 3.642
I0315 19:36:47.139022 140135622489856 logging_writer.py:48] [2] global_step=2, grad_norm=4.88022, loss=1.08254
I0315 19:36:47.146641 140186397963456 submission.py:265] 2) loss = 1.083, grad_norm = 4.880
I0315 19:36:47.236950 140135614097152 logging_writer.py:48] [3] global_step=3, grad_norm=4.52419, loss=1.07932
I0315 19:36:47.244317 140186397963456 submission.py:265] 3) loss = 1.079, grad_norm = 4.524
I0315 19:36:47.347244 140135622489856 logging_writer.py:48] [4] global_step=4, grad_norm=4.70326, loss=1.08922
I0315 19:36:47.352063 140186397963456 submission.py:265] 4) loss = 1.089, grad_norm = 4.703
I0315 19:36:47.458492 140135614097152 logging_writer.py:48] [5] global_step=5, grad_norm=5.72407, loss=1.11272
I0315 19:36:47.462926 140186397963456 submission.py:265] 5) loss = 1.113, grad_norm = 5.724
I0315 19:36:47.552092 140135622489856 logging_writer.py:48] [6] global_step=6, grad_norm=3.96418, loss=0.972603
I0315 19:36:47.561219 140186397963456 submission.py:265] 6) loss = 0.973, grad_norm = 3.964
I0315 19:36:47.643326 140135614097152 logging_writer.py:48] [7] global_step=7, grad_norm=4.13429, loss=1.01254
I0315 19:36:47.647683 140186397963456 submission.py:265] 7) loss = 1.013, grad_norm = 4.134
I0315 19:36:47.737536 140135622489856 logging_writer.py:48] [8] global_step=8, grad_norm=4.67314, loss=0.957081
I0315 19:36:47.742560 140186397963456 submission.py:265] 8) loss = 0.957, grad_norm = 4.673
I0315 19:36:47.843000 140135614097152 logging_writer.py:48] [9] global_step=9, grad_norm=3.48119, loss=0.951731
I0315 19:36:47.847895 140186397963456 submission.py:265] 9) loss = 0.952, grad_norm = 3.481
I0315 19:36:47.922446 140135622489856 logging_writer.py:48] [10] global_step=10, grad_norm=4.44667, loss=1.03514
I0315 19:36:47.927871 140186397963456 submission.py:265] 10) loss = 1.035, grad_norm = 4.447
I0315 19:36:48.019595 140135614097152 logging_writer.py:48] [11] global_step=11, grad_norm=3.91966, loss=0.972732
I0315 19:36:48.024231 140186397963456 submission.py:265] 11) loss = 0.973, grad_norm = 3.920
I0315 19:36:48.112935 140135622489856 logging_writer.py:48] [12] global_step=12, grad_norm=4.85002, loss=1.02165
I0315 19:36:48.118004 140186397963456 submission.py:265] 12) loss = 1.022, grad_norm = 4.850
I0315 19:36:48.203852 140135614097152 logging_writer.py:48] [13] global_step=13, grad_norm=3.11263, loss=0.935135
I0315 19:36:48.209564 140186397963456 submission.py:265] 13) loss = 0.935, grad_norm = 3.113
I0315 19:36:48.299371 140135622489856 logging_writer.py:48] [14] global_step=14, grad_norm=2.92111, loss=0.897752
I0315 19:36:48.305564 140186397963456 submission.py:265] 14) loss = 0.898, grad_norm = 2.921
I0315 19:36:48.397814 140135614097152 logging_writer.py:48] [15] global_step=15, grad_norm=3.44954, loss=0.858521
I0315 19:36:48.404062 140186397963456 submission.py:265] 15) loss = 0.859, grad_norm = 3.450
I0315 19:36:48.490056 140135622489856 logging_writer.py:48] [16] global_step=16, grad_norm=2.8416, loss=0.842523
I0315 19:36:48.496956 140186397963456 submission.py:265] 16) loss = 0.843, grad_norm = 2.842
I0315 19:36:48.571464 140135614097152 logging_writer.py:48] [17] global_step=17, grad_norm=2.35223, loss=0.935571
I0315 19:36:48.577159 140186397963456 submission.py:265] 17) loss = 0.936, grad_norm = 2.352
I0315 19:36:48.651040 140135622489856 logging_writer.py:48] [18] global_step=18, grad_norm=2.26858, loss=0.808577
I0315 19:36:48.657580 140186397963456 submission.py:265] 18) loss = 0.809, grad_norm = 2.269
I0315 19:36:48.736827 140135614097152 logging_writer.py:48] [19] global_step=19, grad_norm=3.02554, loss=0.879576
I0315 19:36:48.744110 140186397963456 submission.py:265] 19) loss = 0.880, grad_norm = 3.026
I0315 19:36:48.834752 140135622489856 logging_writer.py:48] [20] global_step=20, grad_norm=1.8748, loss=0.725059
I0315 19:36:48.839448 140186397963456 submission.py:265] 20) loss = 0.725, grad_norm = 1.875
I0315 19:36:48.922966 140135614097152 logging_writer.py:48] [21] global_step=21, grad_norm=2.8541, loss=0.896399
I0315 19:36:48.928002 140186397963456 submission.py:265] 21) loss = 0.896, grad_norm = 2.854
I0315 19:36:49.020117 140135622489856 logging_writer.py:48] [22] global_step=22, grad_norm=2.83117, loss=0.894223
I0315 19:36:49.026738 140186397963456 submission.py:265] 22) loss = 0.894, grad_norm = 2.831
I0315 19:36:49.104692 140135614097152 logging_writer.py:48] [23] global_step=23, grad_norm=2.07047, loss=0.794216
I0315 19:36:49.109180 140186397963456 submission.py:265] 23) loss = 0.794, grad_norm = 2.070
I0315 19:36:49.202943 140135622489856 logging_writer.py:48] [24] global_step=24, grad_norm=1.68447, loss=0.672866
I0315 19:36:49.207773 140186397963456 submission.py:265] 24) loss = 0.673, grad_norm = 1.684
I0315 19:36:49.290770 140135614097152 logging_writer.py:48] [25] global_step=25, grad_norm=1.56759, loss=0.678759
I0315 19:36:49.295277 140186397963456 submission.py:265] 25) loss = 0.679, grad_norm = 1.568
I0315 19:36:49.387046 140135622489856 logging_writer.py:48] [26] global_step=26, grad_norm=1.45368, loss=0.755824
I0315 19:36:49.394576 140186397963456 submission.py:265] 26) loss = 0.756, grad_norm = 1.454
I0315 19:36:49.474630 140135614097152 logging_writer.py:48] [27] global_step=27, grad_norm=1.60166, loss=0.671206
I0315 19:36:49.484769 140186397963456 submission.py:265] 27) loss = 0.671, grad_norm = 1.602
I0315 19:36:49.555463 140135622489856 logging_writer.py:48] [28] global_step=28, grad_norm=1.51081, loss=0.702584
I0315 19:36:49.559653 140186397963456 submission.py:265] 28) loss = 0.703, grad_norm = 1.511
I0315 19:36:49.654997 140135614097152 logging_writer.py:48] [29] global_step=29, grad_norm=1.66862, loss=0.67887
I0315 19:36:49.659127 140186397963456 submission.py:265] 29) loss = 0.679, grad_norm = 1.669
I0315 19:36:49.740964 140135622489856 logging_writer.py:48] [30] global_step=30, grad_norm=1.18608, loss=0.606717
I0315 19:36:49.745621 140186397963456 submission.py:265] 30) loss = 0.607, grad_norm = 1.186
I0315 19:36:49.828864 140135614097152 logging_writer.py:48] [31] global_step=31, grad_norm=1.59332, loss=0.664129
I0315 19:36:49.833726 140186397963456 submission.py:265] 31) loss = 0.664, grad_norm = 1.593
I0315 19:36:49.923962 140135622489856 logging_writer.py:48] [32] global_step=32, grad_norm=1.61364, loss=0.652436
I0315 19:36:49.928206 140186397963456 submission.py:265] 32) loss = 0.652, grad_norm = 1.614
I0315 19:36:50.027622 140135614097152 logging_writer.py:48] [33] global_step=33, grad_norm=1.65345, loss=0.679988
I0315 19:36:50.035951 140186397963456 submission.py:265] 33) loss = 0.680, grad_norm = 1.653
I0315 19:36:50.119325 140135622489856 logging_writer.py:48] [34] global_step=34, grad_norm=1.63697, loss=0.681786
I0315 19:36:50.123879 140186397963456 submission.py:265] 34) loss = 0.682, grad_norm = 1.637
I0315 19:36:50.215889 140135614097152 logging_writer.py:48] [35] global_step=35, grad_norm=1.65016, loss=0.68857
I0315 19:36:50.219889 140186397963456 submission.py:265] 35) loss = 0.689, grad_norm = 1.650
I0315 19:36:50.307216 140135622489856 logging_writer.py:48] [36] global_step=36, grad_norm=1.51905, loss=0.708869
I0315 19:36:50.312051 140186397963456 submission.py:265] 36) loss = 0.709, grad_norm = 1.519
I0315 19:36:50.396039 140135614097152 logging_writer.py:48] [37] global_step=37, grad_norm=1.6332, loss=0.603519
I0315 19:36:50.399937 140186397963456 submission.py:265] 37) loss = 0.604, grad_norm = 1.633
I0315 19:36:50.484614 140135622489856 logging_writer.py:48] [38] global_step=38, grad_norm=1.48599, loss=0.566676
I0315 19:36:50.489644 140186397963456 submission.py:265] 38) loss = 0.567, grad_norm = 1.486
I0315 19:36:50.569694 140135614097152 logging_writer.py:48] [39] global_step=39, grad_norm=1.36542, loss=0.608498
I0315 19:36:50.574882 140186397963456 submission.py:265] 39) loss = 0.608, grad_norm = 1.365
I0315 19:36:50.651124 140135622489856 logging_writer.py:48] [40] global_step=40, grad_norm=1.41194, loss=0.572986
I0315 19:36:50.656088 140186397963456 submission.py:265] 40) loss = 0.573, grad_norm = 1.412
I0315 19:36:50.736053 140135614097152 logging_writer.py:48] [41] global_step=41, grad_norm=1.72117, loss=0.539413
I0315 19:36:50.742307 140186397963456 submission.py:265] 41) loss = 0.539, grad_norm = 1.721
I0315 19:36:50.814249 140135622489856 logging_writer.py:48] [42] global_step=42, grad_norm=1.33094, loss=0.553231
I0315 19:36:50.819296 140186397963456 submission.py:265] 42) loss = 0.553, grad_norm = 1.331
I0315 19:36:50.895083 140135614097152 logging_writer.py:48] [43] global_step=43, grad_norm=1.80308, loss=0.554814
I0315 19:36:50.900414 140186397963456 submission.py:265] 43) loss = 0.555, grad_norm = 1.803
I0315 19:36:50.981646 140135622489856 logging_writer.py:48] [44] global_step=44, grad_norm=1.92909, loss=0.552009
I0315 19:36:50.986616 140186397963456 submission.py:265] 44) loss = 0.552, grad_norm = 1.929
I0315 19:36:51.065460 140135614097152 logging_writer.py:48] [45] global_step=45, grad_norm=1.73537, loss=0.495491
I0315 19:36:51.069920 140186397963456 submission.py:265] 45) loss = 0.495, grad_norm = 1.735
I0315 19:36:51.143379 140135622489856 logging_writer.py:48] [46] global_step=46, grad_norm=1.99315, loss=0.696637
I0315 19:36:51.148327 140186397963456 submission.py:265] 46) loss = 0.697, grad_norm = 1.993
I0315 19:36:51.234956 140135614097152 logging_writer.py:48] [47] global_step=47, grad_norm=1.20581, loss=0.511068
I0315 19:36:51.239632 140186397963456 submission.py:265] 47) loss = 0.511, grad_norm = 1.206
I0315 19:36:51.314042 140135622489856 logging_writer.py:48] [48] global_step=48, grad_norm=1.10071, loss=0.597717
I0315 19:36:51.318434 140186397963456 submission.py:265] 48) loss = 0.598, grad_norm = 1.101
I0315 19:36:51.397023 140135614097152 logging_writer.py:48] [49] global_step=49, grad_norm=1.1941, loss=0.444174
I0315 19:36:51.405474 140186397963456 submission.py:265] 49) loss = 0.444, grad_norm = 1.194
I0315 19:36:51.483749 140135622489856 logging_writer.py:48] [50] global_step=50, grad_norm=1.2298, loss=0.430357
I0315 19:36:51.495255 140186397963456 submission.py:265] 50) loss = 0.430, grad_norm = 1.230
I0315 19:36:51.570975 140135614097152 logging_writer.py:48] [51] global_step=51, grad_norm=1.30626, loss=0.440027
I0315 19:36:51.575790 140186397963456 submission.py:265] 51) loss = 0.440, grad_norm = 1.306
I0315 19:36:51.653079 140135622489856 logging_writer.py:48] [52] global_step=52, grad_norm=1.04756, loss=0.419315
I0315 19:36:51.659181 140186397963456 submission.py:265] 52) loss = 0.419, grad_norm = 1.048
I0315 19:36:51.739353 140135614097152 logging_writer.py:48] [53] global_step=53, grad_norm=1.30149, loss=0.410826
I0315 19:36:51.744883 140186397963456 submission.py:265] 53) loss = 0.411, grad_norm = 1.301
I0315 19:36:51.862710 140135622489856 logging_writer.py:48] [54] global_step=54, grad_norm=1.77676, loss=0.533014
I0315 19:36:51.868254 140186397963456 submission.py:265] 54) loss = 0.533, grad_norm = 1.777
I0315 19:36:52.080206 140135614097152 logging_writer.py:48] [55] global_step=55, grad_norm=1.20266, loss=0.507758
I0315 19:36:52.087229 140186397963456 submission.py:265] 55) loss = 0.508, grad_norm = 1.203
I0315 19:36:52.239716 140135622489856 logging_writer.py:48] [56] global_step=56, grad_norm=1.06537, loss=0.422398
I0315 19:36:52.244253 140186397963456 submission.py:265] 56) loss = 0.422, grad_norm = 1.065
I0315 19:36:52.730210 140135614097152 logging_writer.py:48] [57] global_step=57, grad_norm=1.34124, loss=0.461894
I0315 19:36:52.737345 140186397963456 submission.py:265] 57) loss = 0.462, grad_norm = 1.341
I0315 19:36:53.063650 140135622489856 logging_writer.py:48] [58] global_step=58, grad_norm=1.0161, loss=0.415572
I0315 19:36:53.070048 140186397963456 submission.py:265] 58) loss = 0.416, grad_norm = 1.016
I0315 19:36:53.243574 140135614097152 logging_writer.py:48] [59] global_step=59, grad_norm=0.980133, loss=0.44932
I0315 19:36:53.250720 140186397963456 submission.py:265] 59) loss = 0.449, grad_norm = 0.980
I0315 19:36:53.431086 140135622489856 logging_writer.py:48] [60] global_step=60, grad_norm=1.097, loss=0.471738
I0315 19:36:53.436982 140186397963456 submission.py:265] 60) loss = 0.472, grad_norm = 1.097
I0315 19:36:53.705711 140135614097152 logging_writer.py:48] [61] global_step=61, grad_norm=1.36852, loss=0.405568
I0315 19:36:53.710215 140186397963456 submission.py:265] 61) loss = 0.406, grad_norm = 1.369
I0315 19:36:53.972533 140135622489856 logging_writer.py:48] [62] global_step=62, grad_norm=1.34157, loss=0.460333
I0315 19:36:53.977365 140186397963456 submission.py:265] 62) loss = 0.460, grad_norm = 1.342
I0315 19:36:54.440084 140135614097152 logging_writer.py:48] [63] global_step=63, grad_norm=1.46973, loss=0.402738
I0315 19:36:54.445365 140186397963456 submission.py:265] 63) loss = 0.403, grad_norm = 1.470
I0315 19:36:54.841915 140135622489856 logging_writer.py:48] [64] global_step=64, grad_norm=1.2971, loss=0.445192
I0315 19:36:54.848258 140186397963456 submission.py:265] 64) loss = 0.445, grad_norm = 1.297
I0315 19:36:55.028720 140135614097152 logging_writer.py:48] [65] global_step=65, grad_norm=0.999463, loss=0.43885
I0315 19:36:55.033597 140186397963456 submission.py:265] 65) loss = 0.439, grad_norm = 0.999
I0315 19:36:55.582488 140135622489856 logging_writer.py:48] [66] global_step=66, grad_norm=1.65757, loss=0.445503
I0315 19:36:55.587579 140186397963456 submission.py:265] 66) loss = 0.446, grad_norm = 1.658
I0315 19:36:55.828611 140135614097152 logging_writer.py:48] [67] global_step=67, grad_norm=1.36147, loss=0.443758
I0315 19:36:55.833706 140186397963456 submission.py:265] 67) loss = 0.444, grad_norm = 1.361
I0315 19:36:55.945701 140135622489856 logging_writer.py:48] [68] global_step=68, grad_norm=1.58061, loss=0.39823
I0315 19:36:55.950852 140186397963456 submission.py:265] 68) loss = 0.398, grad_norm = 1.581
I0315 19:36:56.098239 140135614097152 logging_writer.py:48] [69] global_step=69, grad_norm=1.12805, loss=0.526487
I0315 19:36:56.103584 140186397963456 submission.py:265] 69) loss = 0.526, grad_norm = 1.128
I0315 19:36:56.189615 140135622489856 logging_writer.py:48] [70] global_step=70, grad_norm=0.673812, loss=0.437719
I0315 19:36:56.195280 140186397963456 submission.py:265] 70) loss = 0.438, grad_norm = 0.674
I0315 19:36:56.286164 140135614097152 logging_writer.py:48] [71] global_step=71, grad_norm=1.31305, loss=0.460619
I0315 19:36:56.296528 140186397963456 submission.py:265] 71) loss = 0.461, grad_norm = 1.313
I0315 19:36:56.379964 140135622489856 logging_writer.py:48] [72] global_step=72, grad_norm=1.28101, loss=0.386859
I0315 19:36:56.387140 140186397963456 submission.py:265] 72) loss = 0.387, grad_norm = 1.281
I0315 19:36:56.468879 140135614097152 logging_writer.py:48] [73] global_step=73, grad_norm=0.914272, loss=0.482641
I0315 19:36:56.474982 140186397963456 submission.py:265] 73) loss = 0.483, grad_norm = 0.914
I0315 19:36:56.560649 140135622489856 logging_writer.py:48] [74] global_step=74, grad_norm=0.627838, loss=0.426421
I0315 19:36:56.565804 140186397963456 submission.py:265] 74) loss = 0.426, grad_norm = 0.628
I0315 19:36:56.645853 140135614097152 logging_writer.py:48] [75] global_step=75, grad_norm=1.10027, loss=0.49242
I0315 19:36:56.650322 140186397963456 submission.py:265] 75) loss = 0.492, grad_norm = 1.100
I0315 19:36:56.735522 140135622489856 logging_writer.py:48] [76] global_step=76, grad_norm=1.06554, loss=0.486174
I0315 19:36:56.742952 140186397963456 submission.py:265] 76) loss = 0.486, grad_norm = 1.066
I0315 19:36:56.824976 140135614097152 logging_writer.py:48] [77] global_step=77, grad_norm=1.64519, loss=0.440179
I0315 19:36:56.830937 140186397963456 submission.py:265] 77) loss = 0.440, grad_norm = 1.645
I0315 19:36:56.910749 140135622489856 logging_writer.py:48] [78] global_step=78, grad_norm=0.61902, loss=0.49203
I0315 19:36:56.915715 140186397963456 submission.py:265] 78) loss = 0.492, grad_norm = 0.619
I0315 19:36:56.996845 140135614097152 logging_writer.py:48] [79] global_step=79, grad_norm=0.549993, loss=0.366976
I0315 19:36:57.002651 140186397963456 submission.py:265] 79) loss = 0.367, grad_norm = 0.550
I0315 19:36:57.126631 140135622489856 logging_writer.py:48] [80] global_step=80, grad_norm=1.11231, loss=0.396415
I0315 19:36:57.131503 140186397963456 submission.py:265] 80) loss = 0.396, grad_norm = 1.112
I0315 19:36:57.252511 140135614097152 logging_writer.py:48] [81] global_step=81, grad_norm=0.867832, loss=0.429169
I0315 19:36:57.258086 140186397963456 submission.py:265] 81) loss = 0.429, grad_norm = 0.868
I0315 19:36:57.393578 140135622489856 logging_writer.py:48] [82] global_step=82, grad_norm=0.602134, loss=0.391905
I0315 19:36:57.398904 140186397963456 submission.py:265] 82) loss = 0.392, grad_norm = 0.602
I0315 19:36:57.546275 140135614097152 logging_writer.py:48] [83] global_step=83, grad_norm=0.833034, loss=0.311031
I0315 19:36:57.556854 140186397963456 submission.py:265] 83) loss = 0.311, grad_norm = 0.833
I0315 19:36:57.755233 140135622489856 logging_writer.py:48] [84] global_step=84, grad_norm=0.547037, loss=0.402796
I0315 19:36:57.760463 140186397963456 submission.py:265] 84) loss = 0.403, grad_norm = 0.547
I0315 19:36:57.918084 140135614097152 logging_writer.py:48] [85] global_step=85, grad_norm=0.83268, loss=0.35383
I0315 19:36:57.923663 140186397963456 submission.py:265] 85) loss = 0.354, grad_norm = 0.833
I0315 19:36:58.066160 140135622489856 logging_writer.py:48] [86] global_step=86, grad_norm=0.792386, loss=0.350568
I0315 19:36:58.073335 140186397963456 submission.py:265] 86) loss = 0.351, grad_norm = 0.792
I0315 19:36:58.219489 140135614097152 logging_writer.py:48] [87] global_step=87, grad_norm=0.850623, loss=0.373072
I0315 19:36:58.226089 140186397963456 submission.py:265] 87) loss = 0.373, grad_norm = 0.851
I0315 19:36:58.441823 140135622489856 logging_writer.py:48] [88] global_step=88, grad_norm=0.342108, loss=0.487698
I0315 19:36:58.447799 140186397963456 submission.py:265] 88) loss = 0.488, grad_norm = 0.342
I0315 19:36:58.664221 140135614097152 logging_writer.py:48] [89] global_step=89, grad_norm=0.514793, loss=0.414895
I0315 19:36:58.669573 140186397963456 submission.py:265] 89) loss = 0.415, grad_norm = 0.515
I0315 19:36:58.890001 140135622489856 logging_writer.py:48] [90] global_step=90, grad_norm=1.33388, loss=0.486028
I0315 19:36:58.894605 140186397963456 submission.py:265] 90) loss = 0.486, grad_norm = 1.334
I0315 19:36:59.072153 140135614097152 logging_writer.py:48] [91] global_step=91, grad_norm=0.603886, loss=0.384388
I0315 19:36:59.079056 140186397963456 submission.py:265] 91) loss = 0.384, grad_norm = 0.604
I0315 19:36:59.216049 140135622489856 logging_writer.py:48] [92] global_step=92, grad_norm=0.745486, loss=0.458593
I0315 19:36:59.221855 140186397963456 submission.py:265] 92) loss = 0.459, grad_norm = 0.745
I0315 19:36:59.359350 140135614097152 logging_writer.py:48] [93] global_step=93, grad_norm=0.583906, loss=0.377103
I0315 19:36:59.363614 140186397963456 submission.py:265] 93) loss = 0.377, grad_norm = 0.584
I0315 19:36:59.546173 140135622489856 logging_writer.py:48] [94] global_step=94, grad_norm=0.90514, loss=0.372426
I0315 19:36:59.550809 140186397963456 submission.py:265] 94) loss = 0.372, grad_norm = 0.905
I0315 19:36:59.848099 140135614097152 logging_writer.py:48] [95] global_step=95, grad_norm=0.789083, loss=0.432269
I0315 19:36:59.855543 140186397963456 submission.py:265] 95) loss = 0.432, grad_norm = 0.789
I0315 19:37:00.054384 140135622489856 logging_writer.py:48] [96] global_step=96, grad_norm=0.547715, loss=0.429687
I0315 19:37:00.059691 140186397963456 submission.py:265] 96) loss = 0.430, grad_norm = 0.548
I0315 19:37:00.350360 140135614097152 logging_writer.py:48] [97] global_step=97, grad_norm=0.581652, loss=0.492284
I0315 19:37:00.354651 140186397963456 submission.py:265] 97) loss = 0.492, grad_norm = 0.582
I0315 19:37:00.588653 140135622489856 logging_writer.py:48] [98] global_step=98, grad_norm=0.512228, loss=0.403521
I0315 19:37:00.594352 140186397963456 submission.py:265] 98) loss = 0.404, grad_norm = 0.512
I0315 19:37:03.063886 140135614097152 logging_writer.py:48] [99] global_step=99, grad_norm=0.94444, loss=0.353844
I0315 19:37:03.073904 140186397963456 submission.py:265] 99) loss = 0.354, grad_norm = 0.944
I0315 19:37:03.165261 140135622489856 logging_writer.py:48] [100] global_step=100, grad_norm=0.511963, loss=0.455104
I0315 19:37:03.169934 140186397963456 submission.py:265] 100) loss = 0.455, grad_norm = 0.512
I0315 19:38:07.616178 140186397963456 spec.py:321] Evaluating on the training split.
I0315 19:38:09.666011 140186397963456 spec.py:333] Evaluating on the validation split.
I0315 19:38:11.920332 140186397963456 spec.py:349] Evaluating on the test split.
I0315 19:38:14.039009 140186397963456 submission_runner.py:469] Time since start: 1306.11s, 	Step: 246, 	{'train/ssim': 0.683241435459682, 'train/loss': 0.3313148702893938, 'validation/ssim': 0.6630600396340391, 'validation/loss': 0.35151733329742896, 'validation/num_examples': 3554, 'test/ssim': 0.6822898031712511, 'test/loss': 0.35207542711314926, 'test/num_examples': 3581, 'score': 371.9599258899689, 'total_duration': 1306.1063628196716, 'accumulated_submission_time': 371.9599258899689, 'accumulated_eval_time': 931.5982193946838, 'accumulated_logging_time': 0.016763925552368164}
I0315 19:38:14.048548 140135614097152 logging_writer.py:48] [246] accumulated_eval_time=931.598, accumulated_logging_time=0.0167639, accumulated_submission_time=371.96, global_step=246, preemption_count=0, score=371.96, test/loss=0.352075, test/num_examples=3581, test/ssim=0.68229, total_duration=1306.11, train/loss=0.331315, train/ssim=0.683241, validation/loss=0.351517, validation/num_examples=3554, validation/ssim=0.66306
I0315 19:39:36.189879 140186397963456 spec.py:321] Evaluating on the training split.
I0315 19:39:38.192977 140186397963456 spec.py:333] Evaluating on the validation split.
I0315 19:39:40.747520 140186397963456 spec.py:349] Evaluating on the test split.
I0315 19:39:42.778007 140186397963456 submission_runner.py:469] Time since start: 1394.85s, 	Step: 292, 	{'train/ssim': 0.6847618647984096, 'train/loss': 0.32754250935145784, 'validation/ssim': 0.6656360186145892, 'validation/loss': 0.34662091926570415, 'validation/num_examples': 3554, 'test/ssim': 0.6842660400595155, 'test/loss': 0.3475430426535011, 'test/num_examples': 3581, 'score': 452.77306604385376, 'total_duration': 1394.8453109264374, 'accumulated_submission_time': 452.77306604385376, 'accumulated_eval_time': 938.1863675117493, 'accumulated_logging_time': 0.03412342071533203}
I0315 19:39:42.791399 140135622489856 logging_writer.py:48] [292] accumulated_eval_time=938.186, accumulated_logging_time=0.0341234, accumulated_submission_time=452.773, global_step=292, preemption_count=0, score=452.773, test/loss=0.347543, test/num_examples=3581, test/ssim=0.684266, total_duration=1394.85, train/loss=0.327543, train/ssim=0.684762, validation/loss=0.346621, validation/num_examples=3554, validation/ssim=0.665636
I0315 19:41:04.714143 140186397963456 spec.py:321] Evaluating on the training split.
I0315 19:41:06.752219 140186397963456 spec.py:333] Evaluating on the validation split.
I0315 19:41:09.246971 140186397963456 spec.py:349] Evaluating on the test split.
I0315 19:41:11.313086 140186397963456 submission_runner.py:469] Time since start: 1483.38s, 	Step: 339, 	{'train/ssim': 0.6921582221984863, 'train/loss': 0.32070415360586985, 'validation/ssim': 0.672151907951428, 'validation/loss': 0.3399712815335713, 'validation/num_examples': 3554, 'test/ssim': 0.6912383308826096, 'test/loss': 0.34079965497155124, 'test/num_examples': 3581, 'score': 533.3589107990265, 'total_duration': 1483.3803660869598, 'accumulated_submission_time': 533.3589107990265, 'accumulated_eval_time': 944.7853229045868, 'accumulated_logging_time': 0.06541895866394043}
I0315 19:41:11.324263 140135614097152 logging_writer.py:48] [339] accumulated_eval_time=944.785, accumulated_logging_time=0.065419, accumulated_submission_time=533.359, global_step=339, preemption_count=0, score=533.359, test/loss=0.3408, test/num_examples=3581, test/ssim=0.691238, total_duration=1483.38, train/loss=0.320704, train/ssim=0.692158, validation/loss=0.339971, validation/num_examples=3554, validation/ssim=0.672152
/usr/local/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
I0315 19:42:34.112350 140186397963456 spec.py:321] Evaluating on the training split.
I0315 19:42:36.159376 140186397963456 spec.py:333] Evaluating on the validation split.
I0315 19:42:38.383789 140186397963456 spec.py:349] Evaluating on the test split.
I0315 19:42:40.573182 140186397963456 submission_runner.py:469] Time since start: 1572.64s, 	Step: 381, 	{'train/ssim': 0.6937106677464077, 'train/loss': 0.31738380023411344, 'validation/ssim': 0.674216043432576, 'validation/loss': 0.3358735796703538, 'validation/num_examples': 3554, 'test/ssim': 0.6930863956471656, 'test/loss': 0.33695558208688214, 'test/num_examples': 3581, 'score': 614.808497428894, 'total_duration': 1572.6405503749847, 'accumulated_submission_time': 614.808497428894, 'accumulated_eval_time': 951.2462854385376, 'accumulated_logging_time': 0.08534479141235352}
I0315 19:42:40.582778 140135622489856 logging_writer.py:48] [381] accumulated_eval_time=951.246, accumulated_logging_time=0.0853448, accumulated_submission_time=614.808, global_step=381, preemption_count=0, score=614.808, test/loss=0.336956, test/num_examples=3581, test/ssim=0.693086, total_duration=1572.64, train/loss=0.317384, train/ssim=0.693711, validation/loss=0.335874, validation/num_examples=3554, validation/ssim=0.674216
I0315 19:44:04.342730 140186397963456 spec.py:321] Evaluating on the training split.
I0315 19:44:06.345885 140186397963456 spec.py:333] Evaluating on the validation split.
I0315 19:44:08.866543 140186397963456 spec.py:349] Evaluating on the test split.
I0315 19:44:11.028413 140186397963456 submission_runner.py:469] Time since start: 1663.10s, 	Step: 426, 	{'train/ssim': 0.6991664341517857, 'train/loss': 0.31401896476745605, 'validation/ssim': 0.6800107768095808, 'validation/loss': 0.332486660881665, 'validation/num_examples': 3554, 'test/ssim': 0.6986218633281556, 'test/loss': 0.33361420981613027, 'test/num_examples': 3581, 'score': 697.245766878128, 'total_duration': 1663.0957493782043, 'accumulated_submission_time': 697.245766878128, 'accumulated_eval_time': 957.9321157932281, 'accumulated_logging_time': 0.10335135459899902}
I0315 19:44:11.039790 140135614097152 logging_writer.py:48] [426] accumulated_eval_time=957.932, accumulated_logging_time=0.103351, accumulated_submission_time=697.246, global_step=426, preemption_count=0, score=697.246, test/loss=0.333614, test/num_examples=3581, test/ssim=0.698622, total_duration=1663.1, train/loss=0.314019, train/ssim=0.699166, validation/loss=0.332487, validation/num_examples=3554, validation/ssim=0.680011
I0315 19:45:33.588363 140186397963456 spec.py:321] Evaluating on the training split.
I0315 19:45:35.600679 140186397963456 spec.py:333] Evaluating on the validation split.
I0315 19:45:37.723389 140186397963456 spec.py:349] Evaluating on the test split.
I0315 19:45:39.828870 140186397963456 submission_runner.py:469] Time since start: 1751.90s, 	Step: 471, 	{'train/ssim': 0.700164931161063, 'train/loss': 0.3100807326180594, 'validation/ssim': 0.6808697341463844, 'validation/loss': 0.32818387127620285, 'validation/num_examples': 3554, 'test/ssim': 0.6996092659173415, 'test/loss': 0.3294896581458566, 'test/num_examples': 3581, 'score': 778.2560493946075, 'total_duration': 1751.8961975574493, 'accumulated_submission_time': 778.2560493946075, 'accumulated_eval_time': 964.1728105545044, 'accumulated_logging_time': 0.12435078620910645}
I0315 19:45:39.840351 140135622489856 logging_writer.py:48] [471] accumulated_eval_time=964.173, accumulated_logging_time=0.124351, accumulated_submission_time=778.256, global_step=471, preemption_count=0, score=778.256, test/loss=0.32949, test/num_examples=3581, test/ssim=0.699609, total_duration=1751.9, train/loss=0.310081, train/ssim=0.700165, validation/loss=0.328184, validation/num_examples=3554, validation/ssim=0.68087
I0315 19:46:38.767989 140135614097152 logging_writer.py:48] [500] global_step=500, grad_norm=0.323739, loss=0.359082
I0315 19:46:38.771996 140186397963456 submission.py:265] 500) loss = 0.359, grad_norm = 0.324
I0315 19:47:02.700705 140186397963456 spec.py:321] Evaluating on the training split.
I0315 19:47:04.803246 140186397963456 spec.py:333] Evaluating on the validation split.
I0315 19:47:06.924045 140186397963456 spec.py:349] Evaluating on the test split.
I0315 19:47:09.013820 140186397963456 submission_runner.py:469] Time since start: 1841.08s, 	Step: 511, 	{'train/ssim': 0.7027715955461774, 'train/loss': 0.30712958744594027, 'validation/ssim': 0.6837593726918613, 'validation/loss': 0.3252596106499719, 'validation/num_examples': 3554, 'test/ssim': 0.702157164112678, 'test/loss': 0.326789896436924, 'test/num_examples': 3581, 'score': 859.5855665206909, 'total_duration': 1841.0811610221863, 'accumulated_submission_time': 859.5855665206909, 'accumulated_eval_time': 970.4861626625061, 'accumulated_logging_time': 0.14436769485473633}
I0315 19:47:09.030478 140135622489856 logging_writer.py:48] [511] accumulated_eval_time=970.486, accumulated_logging_time=0.144368, accumulated_submission_time=859.586, global_step=511, preemption_count=0, score=859.586, test/loss=0.32679, test/num_examples=3581, test/ssim=0.702157, total_duration=1841.08, train/loss=0.30713, train/ssim=0.702772, validation/loss=0.32526, validation/num_examples=3554, validation/ssim=0.683759
I0315 19:48:30.759833 140186397963456 spec.py:321] Evaluating on the training split.
I0315 19:48:32.774930 140186397963456 spec.py:333] Evaluating on the validation split.
I0315 19:48:35.229753 140186397963456 spec.py:349] Evaluating on the test split.
I0315 19:48:37.467002 140186397963456 submission_runner.py:469] Time since start: 1929.53s, 	Step: 552, 	{'train/ssim': 0.7054307120186942, 'train/loss': 0.305295603615897, 'validation/ssim': 0.686424929436902, 'validation/loss': 0.3232657497494021, 'validation/num_examples': 3554, 'test/ssim': 0.7046996081750908, 'test/loss': 0.324789968158772, 'test/num_examples': 3581, 'score': 939.7510685920715, 'total_duration': 1929.534300327301, 'accumulated_submission_time': 939.7510685920715, 'accumulated_eval_time': 977.1934859752655, 'accumulated_logging_time': 0.1838066577911377}
I0315 19:48:37.481269 140135614097152 logging_writer.py:48] [552] accumulated_eval_time=977.193, accumulated_logging_time=0.183807, accumulated_submission_time=939.751, global_step=552, preemption_count=0, score=939.751, test/loss=0.32479, test/num_examples=3581, test/ssim=0.7047, total_duration=1929.53, train/loss=0.305296, train/ssim=0.705431, validation/loss=0.323266, validation/num_examples=3554, validation/ssim=0.686425
I0315 19:50:01.009520 140186397963456 spec.py:321] Evaluating on the training split.
I0315 19:50:03.025700 140186397963456 spec.py:333] Evaluating on the validation split.
I0315 19:50:05.624169 140186397963456 spec.py:349] Evaluating on the test split.
I0315 19:50:07.894056 140186397963456 submission_runner.py:469] Time since start: 2019.96s, 	Step: 595, 	{'train/ssim': 0.7052392959594727, 'train/loss': 0.303933926991054, 'validation/ssim': 0.6867056155915869, 'validation/loss': 0.3218366271938309, 'validation/num_examples': 3554, 'test/ssim': 0.7044920784173415, 'test/loss': 0.3236474636645665, 'test/num_examples': 3581, 'score': 1021.8879618644714, 'total_duration': 2019.9614052772522, 'accumulated_submission_time': 1021.8879618644714, 'accumulated_eval_time': 984.0781018733978, 'accumulated_logging_time': 0.20573854446411133}
I0315 19:50:07.903795 140135622489856 logging_writer.py:48] [595] accumulated_eval_time=984.078, accumulated_logging_time=0.205739, accumulated_submission_time=1021.89, global_step=595, preemption_count=0, score=1021.89, test/loss=0.323647, test/num_examples=3581, test/ssim=0.704492, total_duration=2019.96, train/loss=0.303934, train/ssim=0.705239, validation/loss=0.321837, validation/num_examples=3554, validation/ssim=0.686706
I0315 19:51:28.769828 140186397963456 spec.py:321] Evaluating on the training split.
I0315 19:51:30.779029 140186397963456 spec.py:333] Evaluating on the validation split.
I0315 19:51:33.347191 140186397963456 spec.py:349] Evaluating on the test split.
I0315 19:51:35.556934 140186397963456 submission_runner.py:469] Time since start: 2107.62s, 	Step: 641, 	{'train/ssim': 0.7071234158107212, 'train/loss': 0.3007950782775879, 'validation/ssim': 0.687910656346722, 'validation/loss': 0.3186262191918437, 'validation/num_examples': 3554, 'test/ssim': 0.7060493697203993, 'test/loss': 0.32046153418214185, 'test/num_examples': 3581, 'score': 1101.4734392166138, 'total_duration': 2107.624259710312, 'accumulated_submission_time': 1101.4734392166138, 'accumulated_eval_time': 990.8655195236206, 'accumulated_logging_time': 0.22465085983276367}
I0315 19:51:35.567032 140135614097152 logging_writer.py:48] [641] accumulated_eval_time=990.866, accumulated_logging_time=0.224651, accumulated_submission_time=1101.47, global_step=641, preemption_count=0, score=1101.47, test/loss=0.320462, test/num_examples=3581, test/ssim=0.706049, total_duration=2107.62, train/loss=0.300795, train/ssim=0.707123, validation/loss=0.318626, validation/num_examples=3554, validation/ssim=0.687911
I0315 19:52:56.804204 140186397963456 spec.py:321] Evaluating on the training split.
I0315 19:52:58.699183 140186397963456 spec.py:333] Evaluating on the validation split.
I0315 19:53:00.757369 140186397963456 spec.py:349] Evaluating on the test split.
I0315 19:53:02.853694 140186397963456 submission_runner.py:469] Time since start: 2194.92s, 	Step: 684, 	{'train/ssim': 0.7102981294904437, 'train/loss': 0.29928711482456755, 'validation/ssim': 0.6917001255187817, 'validation/loss': 0.31687110638980726, 'validation/num_examples': 3554, 'test/ssim': 0.7092972376998394, 'test/loss': 0.3189118104885158, 'test/num_examples': 3581, 'score': 1179.050456047058, 'total_duration': 2194.921053647995, 'accumulated_submission_time': 1179.050456047058, 'accumulated_eval_time': 996.9152300357819, 'accumulated_logging_time': 2.405137538909912}
I0315 19:53:02.863796 140135622489856 logging_writer.py:48] [684] accumulated_eval_time=996.915, accumulated_logging_time=2.40514, accumulated_submission_time=1179.05, global_step=684, preemption_count=0, score=1179.05, test/loss=0.318912, test/num_examples=3581, test/ssim=0.709297, total_duration=2194.92, train/loss=0.299287, train/ssim=0.710298, validation/loss=0.316871, validation/num_examples=3554, validation/ssim=0.6917
I0315 19:54:25.503337 140186397963456 spec.py:321] Evaluating on the training split.
I0315 19:54:27.604123 140186397963456 spec.py:333] Evaluating on the validation split.
I0315 19:54:29.757647 140186397963456 spec.py:349] Evaluating on the test split.
I0315 19:54:31.870760 140186397963456 submission_runner.py:469] Time since start: 2283.94s, 	Step: 728, 	{'train/ssim': 0.7101012638636998, 'train/loss': 0.29810987200055805, 'validation/ssim': 0.6916155624604319, 'validation/loss': 0.31551088452272086, 'validation/num_examples': 3554, 'test/ssim': 0.7091854279749022, 'test/loss': 0.31762108993210697, 'test/num_examples': 3581, 'score': 1260.2919743061066, 'total_duration': 2283.938000202179, 'accumulated_submission_time': 1260.2919743061066, 'accumulated_eval_time': 1003.282835483551, 'accumulated_logging_time': 2.423387050628662}
I0315 19:54:31.882870 140135614097152 logging_writer.py:48] [728] accumulated_eval_time=1003.28, accumulated_logging_time=2.42339, accumulated_submission_time=1260.29, global_step=728, preemption_count=0, score=1260.29, test/loss=0.317621, test/num_examples=3581, test/ssim=0.709185, total_duration=2283.94, train/loss=0.29811, train/ssim=0.710101, validation/loss=0.315511, validation/num_examples=3554, validation/ssim=0.691616
I0315 19:55:53.871938 140186397963456 spec.py:321] Evaluating on the training split.
I0315 19:55:55.764156 140186397963456 spec.py:333] Evaluating on the validation split.
I0315 19:55:57.792005 140186397963456 spec.py:349] Evaluating on the test split.
I0315 19:55:59.782817 140186397963456 submission_runner.py:469] Time since start: 2371.85s, 	Step: 771, 	{'train/ssim': 0.7121325901576451, 'train/loss': 0.2968846729823521, 'validation/ssim': 0.693021534934229, 'validation/loss': 0.31457784005477984, 'validation/num_examples': 3554, 'test/ssim': 0.711100510397759, 'test/loss': 0.31620771956157495, 'test/num_examples': 3581, 'score': 1340.7943110466003, 'total_duration': 2371.850153684616, 'accumulated_submission_time': 1340.7943110466003, 'accumulated_eval_time': 1009.1938934326172, 'accumulated_logging_time': 2.445952892303467}
I0315 19:55:59.794733 140135622489856 logging_writer.py:48] [771] accumulated_eval_time=1009.19, accumulated_logging_time=2.44595, accumulated_submission_time=1340.79, global_step=771, preemption_count=0, score=1340.79, test/loss=0.316208, test/num_examples=3581, test/ssim=0.711101, total_duration=2371.85, train/loss=0.296885, train/ssim=0.712133, validation/loss=0.314578, validation/num_examples=3554, validation/ssim=0.693022
I0315 19:57:21.355888 140186397963456 spec.py:321] Evaluating on the training split.
I0315 19:57:23.283755 140186397963456 spec.py:333] Evaluating on the validation split.
I0315 19:57:25.339261 140186397963456 spec.py:349] Evaluating on the test split.
I0315 19:57:27.395430 140186397963456 submission_runner.py:469] Time since start: 2459.46s, 	Step: 816, 	{'train/ssim': 0.7137889862060547, 'train/loss': 0.2947451046534947, 'validation/ssim': 0.6948450332152153, 'validation/loss': 0.31222188989299027, 'validation/num_examples': 3554, 'test/ssim': 0.7124516354762985, 'test/loss': 0.31426311664426837, 'test/num_examples': 3581, 'score': 1420.809282541275, 'total_duration': 2459.462755203247, 'accumulated_submission_time': 1420.809282541275, 'accumulated_eval_time': 1015.2336835861206, 'accumulated_logging_time': 2.468169927597046}
I0315 19:57:27.407496 140135614097152 logging_writer.py:48] [816] accumulated_eval_time=1015.23, accumulated_logging_time=2.46817, accumulated_submission_time=1420.81, global_step=816, preemption_count=0, score=1420.81, test/loss=0.314263, test/num_examples=3581, test/ssim=0.712452, total_duration=2459.46, train/loss=0.294745, train/ssim=0.713789, validation/loss=0.312222, validation/num_examples=3554, validation/ssim=0.694845
I0315 19:58:48.950532 140186397963456 spec.py:321] Evaluating on the training split.
I0315 19:58:50.876711 140186397963456 spec.py:333] Evaluating on the validation split.
I0315 19:58:53.457073 140186397963456 spec.py:349] Evaluating on the test split.
I0315 19:58:55.653197 140186397963456 submission_runner.py:469] Time since start: 2547.72s, 	Step: 860, 	{'train/ssim': 0.7154627527509417, 'train/loss': 0.29365757533482145, 'validation/ssim': 0.6967026727147229, 'validation/loss': 0.31111758977560494, 'validation/num_examples': 3554, 'test/ssim': 0.713929364615331, 'test/loss': 0.3134351110897794, 'test/num_examples': 3581, 'score': 1500.8472037315369, 'total_duration': 2547.7205147743225, 'accumulated_submission_time': 1500.8472037315369, 'accumulated_eval_time': 1021.9365949630737, 'accumulated_logging_time': 2.488558053970337}
I0315 19:58:55.664792 140135622489856 logging_writer.py:48] [860] accumulated_eval_time=1021.94, accumulated_logging_time=2.48856, accumulated_submission_time=1500.85, global_step=860, preemption_count=0, score=1500.85, test/loss=0.313435, test/num_examples=3581, test/ssim=0.713929, total_duration=2547.72, train/loss=0.293658, train/ssim=0.715463, validation/loss=0.311118, validation/num_examples=3554, validation/ssim=0.696703
I0315 20:00:19.429274 140186397963456 spec.py:321] Evaluating on the training split.
I0315 20:00:21.361059 140186397963456 spec.py:333] Evaluating on the validation split.
I0315 20:00:23.757681 140186397963456 spec.py:349] Evaluating on the test split.
I0315 20:00:26.073254 140186397963456 submission_runner.py:469] Time since start: 2638.14s, 	Step: 904, 	{'train/ssim': 0.717120715550014, 'train/loss': 0.29315594264439176, 'validation/ssim': 0.699380869267023, 'validation/loss': 0.3099805910263963, 'validation/num_examples': 3554, 'test/ssim': 0.7166539767173974, 'test/loss': 0.31200200357572955, 'test/num_examples': 3581, 'score': 1583.2446143627167, 'total_duration': 2638.1406264305115, 'accumulated_submission_time': 1583.2446143627167, 'accumulated_eval_time': 1028.58087849617, 'accumulated_logging_time': 2.514470100402832}
I0315 20:00:26.083939 140135614097152 logging_writer.py:48] [904] accumulated_eval_time=1028.58, accumulated_logging_time=2.51447, accumulated_submission_time=1583.24, global_step=904, preemption_count=0, score=1583.24, test/loss=0.312002, test/num_examples=3581, test/ssim=0.716654, total_duration=2638.14, train/loss=0.293156, train/ssim=0.717121, validation/loss=0.309981, validation/num_examples=3554, validation/ssim=0.699381
I0315 20:01:47.527535 140186397963456 spec.py:321] Evaluating on the training split.
I0315 20:01:49.570272 140186397963456 spec.py:333] Evaluating on the validation split.
I0315 20:01:52.075144 140186397963456 spec.py:349] Evaluating on the test split.
I0315 20:01:54.083341 140186397963456 submission_runner.py:469] Time since start: 2726.15s, 	Step: 945, 	{'train/ssim': 0.7174336569649833, 'train/loss': 0.29124389375959125, 'validation/ssim': 0.698752176245076, 'validation/loss': 0.30859399043111635, 'validation/num_examples': 3554, 'test/ssim': 0.7158572642505585, 'test/loss': 0.3109803422141162, 'test/num_examples': 3581, 'score': 1663.2505435943604, 'total_duration': 2726.150632619858, 'accumulated_submission_time': 1663.2505435943604, 'accumulated_eval_time': 1035.136889219284, 'accumulated_logging_time': 2.533992052078247}
I0315 20:01:54.097602 140135622489856 logging_writer.py:48] [945] accumulated_eval_time=1035.14, accumulated_logging_time=2.53399, accumulated_submission_time=1663.25, global_step=945, preemption_count=0, score=1663.25, test/loss=0.31098, test/num_examples=3581, test/ssim=0.715857, total_duration=2726.15, train/loss=0.291244, train/ssim=0.717434, validation/loss=0.308594, validation/num_examples=3554, validation/ssim=0.698752
I0315 20:02:34.082946 140135614097152 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.210794, loss=0.321111
I0315 20:02:34.089802 140186397963456 submission.py:265] 1000) loss = 0.321, grad_norm = 0.211
I0315 20:03:14.829774 140186397963456 spec.py:321] Evaluating on the training split.
I0315 20:03:16.873659 140186397963456 spec.py:333] Evaluating on the validation split.
I0315 20:03:19.417452 140186397963456 spec.py:349] Evaluating on the test split.
I0315 20:03:21.712897 140186397963456 submission_runner.py:469] Time since start: 2813.78s, 	Step: 1306, 	{'train/ssim': 0.7231512750898089, 'train/loss': 0.2860053607395717, 'validation/ssim': 0.7043531221423045, 'validation/loss': 0.303184152924082, 'validation/num_examples': 3554, 'test/ssim': 0.721632577426871, 'test/loss': 0.30526171793188006, 'test/num_examples': 3581, 'score': 1742.3422038555145, 'total_duration': 2813.7800657749176, 'accumulated_submission_time': 1742.3422038555145, 'accumulated_eval_time': 1042.0200634002686, 'accumulated_logging_time': 2.5821340084075928}
I0315 20:03:21.723076 140135622489856 logging_writer.py:48] [1306] accumulated_eval_time=1042.02, accumulated_logging_time=2.58213, accumulated_submission_time=1742.34, global_step=1306, preemption_count=0, score=1742.34, test/loss=0.305262, test/num_examples=3581, test/ssim=0.721633, total_duration=2813.78, train/loss=0.286005, train/ssim=0.723151, validation/loss=0.303184, validation/num_examples=3554, validation/ssim=0.704353
I0315 20:03:34.761816 140135614097152 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.398471, loss=0.343713
I0315 20:03:34.764769 140186397963456 submission.py:265] 1500) loss = 0.344, grad_norm = 0.398
I0315 20:04:06.290500 140135622489856 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.218555, loss=0.342843
I0315 20:04:06.293496 140186397963456 submission.py:265] 2000) loss = 0.343, grad_norm = 0.219
I0315 20:04:37.835188 140135614097152 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.220061, loss=0.285266
I0315 20:04:37.838175 140186397963456 submission.py:265] 2500) loss = 0.285, grad_norm = 0.220
I0315 20:04:42.448494 140186397963456 spec.py:321] Evaluating on the training split.
I0315 20:04:44.439059 140186397963456 spec.py:333] Evaluating on the validation split.
I0315 20:04:46.549440 140186397963456 spec.py:349] Evaluating on the test split.
I0315 20:04:48.638255 140186397963456 submission_runner.py:469] Time since start: 2900.71s, 	Step: 2564, 	{'train/ssim': 0.7327795028686523, 'train/loss': 0.27849178654806955, 'validation/ssim': 0.7133378268764069, 'validation/loss': 0.29591660722293545, 'validation/num_examples': 3554, 'test/ssim': 0.7304306392461952, 'test/loss': 0.29781634925431094, 'test/num_examples': 3581, 'score': 1821.1868495941162, 'total_duration': 2900.7056114673615, 'accumulated_submission_time': 1821.1868495941162, 'accumulated_eval_time': 1048.2100443840027, 'accumulated_logging_time': 2.6013505458831787}
I0315 20:04:48.648699 140135622489856 logging_writer.py:48] [2564] accumulated_eval_time=1048.21, accumulated_logging_time=2.60135, accumulated_submission_time=1821.19, global_step=2564, preemption_count=0, score=1821.19, test/loss=0.297816, test/num_examples=3581, test/ssim=0.730431, total_duration=2900.71, train/loss=0.278492, train/ssim=0.73278, validation/loss=0.295917, validation/num_examples=3554, validation/ssim=0.713338
I0315 20:05:17.014051 140135614097152 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.224632, loss=0.250713
I0315 20:05:17.017003 140186397963456 submission.py:265] 3000) loss = 0.251, grad_norm = 0.225
I0315 20:05:48.543647 140135622489856 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.568129, loss=0.284952
I0315 20:05:48.546921 140186397963456 submission.py:265] 3500) loss = 0.285, grad_norm = 0.568
I0315 20:06:09.391055 140186397963456 spec.py:321] Evaluating on the training split.
I0315 20:06:11.399149 140186397963456 spec.py:333] Evaluating on the validation split.
I0315 20:06:13.488228 140186397963456 spec.py:349] Evaluating on the test split.
I0315 20:06:15.571987 140186397963456 submission_runner.py:469] Time since start: 2987.64s, 	Step: 3820, 	{'train/ssim': 0.7330936704363141, 'train/loss': 0.2768761089869908, 'validation/ssim': 0.7134974044430571, 'validation/loss': 0.2942110919254713, 'validation/num_examples': 3554, 'test/ssim': 0.7310885440301242, 'test/loss': 0.2957698222279915, 'test/num_examples': 3581, 'score': 1900.0178663730621, 'total_duration': 2987.639354944229, 'accumulated_submission_time': 1900.0178663730621, 'accumulated_eval_time': 1054.3911406993866, 'accumulated_logging_time': 2.619729518890381}
I0315 20:06:15.582273 140135614097152 logging_writer.py:48] [3820] accumulated_eval_time=1054.39, accumulated_logging_time=2.61973, accumulated_submission_time=1900.02, global_step=3820, preemption_count=0, score=1900.02, test/loss=0.29577, test/num_examples=3581, test/ssim=0.731089, total_duration=2987.64, train/loss=0.276876, train/ssim=0.733094, validation/loss=0.294211, validation/num_examples=3554, validation/ssim=0.713497
I0315 20:06:27.799329 140135622489856 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.206412, loss=0.330913
I0315 20:06:27.802200 140186397963456 submission.py:265] 4000) loss = 0.331, grad_norm = 0.206
I0315 20:06:59.258638 140135614097152 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.168451, loss=0.275799
I0315 20:06:59.262506 140186397963456 submission.py:265] 4500) loss = 0.276, grad_norm = 0.168
I0315 20:07:30.758545 140135622489856 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.272352, loss=0.230113
I0315 20:07:30.762223 140186397963456 submission.py:265] 5000) loss = 0.230, grad_norm = 0.272
I0315 20:07:36.306969 140186397963456 spec.py:321] Evaluating on the training split.
I0315 20:07:38.311259 140186397963456 spec.py:333] Evaluating on the validation split.
I0315 20:07:40.497496 140186397963456 spec.py:349] Evaluating on the test split.
I0315 20:07:42.610280 140186397963456 submission_runner.py:469] Time since start: 3074.68s, 	Step: 5079, 	{'train/ssim': 0.733149528503418, 'train/loss': 0.2756320408412388, 'validation/ssim': 0.7130767873786579, 'validation/loss': 0.29312956406953433, 'validation/num_examples': 3554, 'test/ssim': 0.7305879228043842, 'test/loss': 0.29478313549375174, 'test/num_examples': 3581, 'score': 1978.8389701843262, 'total_duration': 3074.677646636963, 'accumulated_submission_time': 1978.8389701843262, 'accumulated_eval_time': 1060.6946742534637, 'accumulated_logging_time': 2.63810396194458}
I0315 20:07:42.621322 140135614097152 logging_writer.py:48] [5079] accumulated_eval_time=1060.69, accumulated_logging_time=2.6381, accumulated_submission_time=1978.84, global_step=5079, preemption_count=0, score=1978.84, test/loss=0.294783, test/num_examples=3581, test/ssim=0.730588, total_duration=3074.68, train/loss=0.275632, train/ssim=0.73315, validation/loss=0.29313, validation/num_examples=3554, validation/ssim=0.713077
I0315 20:08:10.020997 140135622489856 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.16103, loss=0.256447
I0315 20:08:10.024109 140186397963456 submission.py:265] 5500) loss = 0.256, grad_norm = 0.161
I0315 20:08:41.518139 140135614097152 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.224894, loss=0.316469
I0315 20:08:41.521120 140186397963456 submission.py:265] 6000) loss = 0.316, grad_norm = 0.225
I0315 20:09:03.333003 140186397963456 spec.py:321] Evaluating on the training split.
I0315 20:09:05.314926 140186397963456 spec.py:333] Evaluating on the validation split.
I0315 20:09:07.548254 140186397963456 spec.py:349] Evaluating on the test split.
I0315 20:09:09.620447 140186397963456 submission_runner.py:469] Time since start: 3161.69s, 	Step: 6337, 	{'train/ssim': 0.7367469242640904, 'train/loss': 0.27573817116873606, 'validation/ssim': 0.7173668341349536, 'validation/loss': 0.29275765147984667, 'validation/num_examples': 3554, 'test/ssim': 0.7347619707309411, 'test/loss': 0.2942155988747033, 'test/num_examples': 3581, 'score': 2057.709547519684, 'total_duration': 3161.6878023147583, 'accumulated_submission_time': 2057.709547519684, 'accumulated_eval_time': 1066.9823379516602, 'accumulated_logging_time': 2.6574103832244873}
I0315 20:09:09.630992 140135622489856 logging_writer.py:48] [6337] accumulated_eval_time=1066.98, accumulated_logging_time=2.65741, accumulated_submission_time=2057.71, global_step=6337, preemption_count=0, score=2057.71, test/loss=0.294216, test/num_examples=3581, test/ssim=0.734762, total_duration=3161.69, train/loss=0.275738, train/ssim=0.736747, validation/loss=0.292758, validation/num_examples=3554, validation/ssim=0.717367
I0315 20:09:20.798096 140135614097152 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.608313, loss=0.280896
I0315 20:09:20.801527 140186397963456 submission.py:265] 6500) loss = 0.281, grad_norm = 0.608
I0315 20:09:52.271871 140135622489856 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.378622, loss=0.243954
I0315 20:09:52.274930 140186397963456 submission.py:265] 7000) loss = 0.244, grad_norm = 0.379
I0315 20:10:23.774135 140135614097152 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.33324, loss=0.356369
I0315 20:10:23.777868 140186397963456 submission.py:265] 7500) loss = 0.356, grad_norm = 0.333
I0315 20:10:30.386394 140186397963456 spec.py:321] Evaluating on the training split.
I0315 20:10:32.392036 140186397963456 spec.py:333] Evaluating on the validation split.
I0315 20:10:34.531807 140186397963456 spec.py:349] Evaluating on the test split.
I0315 20:10:36.635182 140186397963456 submission_runner.py:469] Time since start: 3248.70s, 	Step: 7595, 	{'train/ssim': 0.7369610922677177, 'train/loss': 0.274746264730181, 'validation/ssim': 0.7170300931828574, 'validation/loss': 0.2920796700790483, 'validation/num_examples': 3554, 'test/ssim': 0.7342255567578888, 'test/loss': 0.29366970834569606, 'test/num_examples': 3581, 'score': 2136.5854620933533, 'total_duration': 3248.7025434970856, 'accumulated_submission_time': 2136.5854620933533, 'accumulated_eval_time': 1073.2312784194946, 'accumulated_logging_time': 2.676490545272827}
I0315 20:10:36.645659 140135622489856 logging_writer.py:48] [7595] accumulated_eval_time=1073.23, accumulated_logging_time=2.67649, accumulated_submission_time=2136.59, global_step=7595, preemption_count=0, score=2136.59, test/loss=0.29367, test/num_examples=3581, test/ssim=0.734226, total_duration=3248.7, train/loss=0.274746, train/ssim=0.736961, validation/loss=0.29208, validation/num_examples=3554, validation/ssim=0.71703
I0315 20:11:03.067385 140135614097152 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.238058, loss=0.338207
I0315 20:11:03.070589 140186397963456 submission.py:265] 8000) loss = 0.338, grad_norm = 0.238
I0315 20:11:34.592961 140135622489856 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.260571, loss=0.331365
I0315 20:11:34.596105 140186397963456 submission.py:265] 8500) loss = 0.331, grad_norm = 0.261
I0315 20:11:57.357932 140186397963456 spec.py:321] Evaluating on the training split.
I0315 20:11:59.363020 140186397963456 spec.py:333] Evaluating on the validation split.
I0315 20:12:01.534251 140186397963456 spec.py:349] Evaluating on the test split.
I0315 20:12:03.617287 140186397963456 submission_runner.py:469] Time since start: 3335.68s, 	Step: 8852, 	{'train/ssim': 0.7400879178728376, 'train/loss': 0.2729393073490688, 'validation/ssim': 0.719876179348973, 'validation/loss': 0.29088448699968344, 'validation/num_examples': 3554, 'test/ssim': 0.737091703609327, 'test/loss': 0.292421768644408, 'test/num_examples': 3581, 'score': 2215.3724427223206, 'total_duration': 3335.684629678726, 'accumulated_submission_time': 2215.3724427223206, 'accumulated_eval_time': 1079.4908452033997, 'accumulated_logging_time': 2.6948652267456055}
I0315 20:12:03.628162 140135614097152 logging_writer.py:48] [8852] accumulated_eval_time=1079.49, accumulated_logging_time=2.69487, accumulated_submission_time=2215.37, global_step=8852, preemption_count=0, score=2215.37, test/loss=0.292422, test/num_examples=3581, test/ssim=0.737092, total_duration=3335.68, train/loss=0.272939, train/ssim=0.740088, validation/loss=0.290884, validation/num_examples=3554, validation/ssim=0.719876
I0315 20:12:13.802906 140135622489856 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.254122, loss=0.328456
I0315 20:12:13.806042 140186397963456 submission.py:265] 9000) loss = 0.328, grad_norm = 0.254
I0315 20:12:45.266390 140135614097152 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.245685, loss=0.346106
I0315 20:12:45.269931 140186397963456 submission.py:265] 9500) loss = 0.346, grad_norm = 0.246
I0315 20:13:16.758110 140135622489856 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.35043, loss=0.282661
I0315 20:13:16.761073 140186397963456 submission.py:265] 10000) loss = 0.283, grad_norm = 0.350
I0315 20:13:24.200938 140186397963456 spec.py:321] Evaluating on the training split.
I0315 20:13:26.199820 140186397963456 spec.py:333] Evaluating on the validation split.
I0315 20:13:28.352005 140186397963456 spec.py:349] Evaluating on the test split.
I0315 20:13:30.478157 140186397963456 submission_runner.py:469] Time since start: 3422.55s, 	Step: 10110, 	{'train/ssim': 0.7382880619594029, 'train/loss': 0.27315538270132883, 'validation/ssim': 0.7178178142146173, 'validation/loss': 0.29106223428926914, 'validation/num_examples': 3554, 'test/ssim': 0.7351747804166084, 'test/loss': 0.2924892635393396, 'test/num_examples': 3581, 'score': 2294.223144054413, 'total_duration': 3422.5454726219177, 'accumulated_submission_time': 2294.223144054413, 'accumulated_eval_time': 1085.7681047916412, 'accumulated_logging_time': 2.713733196258545}
I0315 20:13:30.490284 140135614097152 logging_writer.py:48] [10110] accumulated_eval_time=1085.77, accumulated_logging_time=2.71373, accumulated_submission_time=2294.22, global_step=10110, preemption_count=0, score=2294.22, test/loss=0.292489, test/num_examples=3581, test/ssim=0.735175, total_duration=3422.55, train/loss=0.273155, train/ssim=0.738288, validation/loss=0.291062, validation/num_examples=3554, validation/ssim=0.717818
I0315 20:13:55.922504 140135622489856 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.282132, loss=0.300264
I0315 20:13:55.926141 140186397963456 submission.py:265] 10500) loss = 0.300, grad_norm = 0.282
I0315 20:14:27.395100 140135614097152 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.928357, loss=0.267034
I0315 20:14:27.398248 140186397963456 submission.py:265] 11000) loss = 0.267, grad_norm = 0.928
I0315 20:14:51.267998 140186397963456 spec.py:321] Evaluating on the training split.
I0315 20:14:53.138752 140186397963456 spec.py:333] Evaluating on the validation split.
I0315 20:14:55.374430 140186397963456 spec.py:349] Evaluating on the test split.
I0315 20:14:57.458579 140186397963456 submission_runner.py:469] Time since start: 3509.53s, 	Step: 11369, 	{'train/ssim': 0.7401157106672015, 'train/loss': 0.2714069059916905, 'validation/ssim': 0.7199538729468556, 'validation/loss': 0.2892316948113042, 'validation/num_examples': 3554, 'test/ssim': 0.7372600999633482, 'test/loss': 0.29071193206113866, 'test/num_examples': 3581, 'score': 2373.0823266506195, 'total_duration': 3509.5259273052216, 'accumulated_submission_time': 2373.0823266506195, 'accumulated_eval_time': 1091.9589080810547, 'accumulated_logging_time': 2.734273910522461}
I0315 20:14:57.469846 140135622489856 logging_writer.py:48] [11369] accumulated_eval_time=1091.96, accumulated_logging_time=2.73427, accumulated_submission_time=2373.08, global_step=11369, preemption_count=0, score=2373.08, test/loss=0.290712, test/num_examples=3581, test/ssim=0.73726, total_duration=3509.53, train/loss=0.271407, train/ssim=0.740116, validation/loss=0.289232, validation/num_examples=3554, validation/ssim=0.719954
I0315 20:15:06.628010 140135614097152 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.268343, loss=0.247807
I0315 20:15:06.631444 140186397963456 submission.py:265] 11500) loss = 0.248, grad_norm = 0.268
I0315 20:15:38.074840 140135622489856 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.312322, loss=0.218611
I0315 20:15:38.078333 140186397963456 submission.py:265] 12000) loss = 0.219, grad_norm = 0.312
I0315 20:16:09.483583 140135614097152 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.261283, loss=0.279381
I0315 20:16:09.486614 140186397963456 submission.py:265] 12500) loss = 0.279, grad_norm = 0.261
I0315 20:16:18.179111 140186397963456 spec.py:321] Evaluating on the training split.
I0315 20:16:20.263021 140186397963456 spec.py:333] Evaluating on the validation split.
I0315 20:16:22.531942 140186397963456 spec.py:349] Evaluating on the test split.
I0315 20:16:24.604142 140186397963456 submission_runner.py:469] Time since start: 3596.67s, 	Step: 12628, 	{'train/ssim': 0.7395613534109933, 'train/loss': 0.2722933122089931, 'validation/ssim': 0.7191644345499789, 'validation/loss': 0.2902645868871342, 'validation/num_examples': 3554, 'test/ssim': 0.736467955332833, 'test/loss': 0.29177170417655685, 'test/num_examples': 3581, 'score': 2451.9636318683624, 'total_duration': 3596.6715099811554, 'accumulated_submission_time': 2451.9636318683624, 'accumulated_eval_time': 1098.3841705322266, 'accumulated_logging_time': 2.753032684326172}
I0315 20:16:24.615018 140135622489856 logging_writer.py:48] [12628] accumulated_eval_time=1098.38, accumulated_logging_time=2.75303, accumulated_submission_time=2451.96, global_step=12628, preemption_count=0, score=2451.96, test/loss=0.291772, test/num_examples=3581, test/ssim=0.736468, total_duration=3596.67, train/loss=0.272293, train/ssim=0.739561, validation/loss=0.290265, validation/num_examples=3554, validation/ssim=0.719164
I0315 20:16:48.944167 140135614097152 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.346574, loss=0.249415
I0315 20:16:48.947137 140186397963456 submission.py:265] 13000) loss = 0.249, grad_norm = 0.347
I0315 20:17:20.423657 140135622489856 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.373068, loss=0.265416
I0315 20:17:20.427242 140186397963456 submission.py:265] 13500) loss = 0.265, grad_norm = 0.373
I0315 20:17:45.308004 140186397963456 spec.py:321] Evaluating on the training split.
I0315 20:17:47.291413 140186397963456 spec.py:333] Evaluating on the validation split.
I0315 20:17:49.440336 140186397963456 spec.py:349] Evaluating on the test split.
I0315 20:17:51.512524 140186397963456 submission_runner.py:469] Time since start: 3683.58s, 	Step: 13887, 	{'train/ssim': 0.7403653008597237, 'train/loss': 0.2718982015337263, 'validation/ssim': 0.7200911934615926, 'validation/loss': 0.2897718061405283, 'validation/num_examples': 3554, 'test/ssim': 0.7373802272409942, 'test/loss': 0.2912192345997103, 'test/num_examples': 3581, 'score': 2530.7891280651093, 'total_duration': 3683.579870700836, 'accumulated_submission_time': 2530.7891280651093, 'accumulated_eval_time': 1104.5889275074005, 'accumulated_logging_time': 2.771928071975708}
I0315 20:17:51.523205 140135614097152 logging_writer.py:48] [13887] accumulated_eval_time=1104.59, accumulated_logging_time=2.77193, accumulated_submission_time=2530.79, global_step=13887, preemption_count=0, score=2530.79, test/loss=0.291219, test/num_examples=3581, test/ssim=0.73738, total_duration=3683.58, train/loss=0.271898, train/ssim=0.740365, validation/loss=0.289772, validation/num_examples=3554, validation/ssim=0.720091
I0315 20:17:59.506925 140135622489856 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.190991, loss=0.366612
I0315 20:17:59.510121 140186397963456 submission.py:265] 14000) loss = 0.367, grad_norm = 0.191
I0315 20:18:30.961554 140135614097152 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.48907, loss=0.325952
I0315 20:18:30.965422 140186397963456 submission.py:265] 14500) loss = 0.326, grad_norm = 0.489
I0315 20:19:02.506271 140135622489856 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.358501, loss=0.264425
I0315 20:19:02.509374 140186397963456 submission.py:265] 15000) loss = 0.264, grad_norm = 0.359
I0315 20:19:12.225885 140186397963456 spec.py:321] Evaluating on the training split.
I0315 20:19:14.206215 140186397963456 spec.py:333] Evaluating on the validation split.
I0315 20:19:16.341831 140186397963456 spec.py:349] Evaluating on the test split.
I0315 20:19:18.414898 140186397963456 submission_runner.py:469] Time since start: 3770.48s, 	Step: 15145, 	{'train/ssim': 0.7391048840114048, 'train/loss': 0.2720621313367571, 'validation/ssim': 0.7185385580068233, 'validation/loss': 0.2901387383713773, 'validation/num_examples': 3554, 'test/ssim': 0.7358353440903379, 'test/loss': 0.2916358621784243, 'test/num_examples': 3581, 'score': 2609.6232430934906, 'total_duration': 3770.482263326645, 'accumulated_submission_time': 2609.6232430934906, 'accumulated_eval_time': 1110.7782111167908, 'accumulated_logging_time': 2.790510654449463}
I0315 20:19:18.425987 140135614097152 logging_writer.py:48] [15145] accumulated_eval_time=1110.78, accumulated_logging_time=2.79051, accumulated_submission_time=2609.62, global_step=15145, preemption_count=0, score=2609.62, test/loss=0.291636, test/num_examples=3581, test/ssim=0.735835, total_duration=3770.48, train/loss=0.272062, train/ssim=0.739105, validation/loss=0.290139, validation/num_examples=3554, validation/ssim=0.718539
I0315 20:19:41.636395 140135622489856 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.180096, loss=0.29435
I0315 20:19:41.639422 140186397963456 submission.py:265] 15500) loss = 0.294, grad_norm = 0.180
I0315 20:20:13.138716 140135614097152 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.189887, loss=0.271245
I0315 20:20:13.141741 140186397963456 submission.py:265] 16000) loss = 0.271, grad_norm = 0.190
I0315 20:20:39.063437 140186397963456 spec.py:321] Evaluating on the training split.
I0315 20:20:41.040793 140186397963456 spec.py:333] Evaluating on the validation split.
I0315 20:20:43.167658 140186397963456 spec.py:349] Evaluating on the test split.
I0315 20:20:45.246973 140186397963456 submission_runner.py:469] Time since start: 3857.31s, 	Step: 16403, 	{'train/ssim': 0.7399223872593471, 'train/loss': 0.2718940121786935, 'validation/ssim': 0.719499732915377, 'validation/loss': 0.28986540253939225, 'validation/num_examples': 3554, 'test/ssim': 0.7367403210957135, 'test/loss': 0.2913569855443661, 'test/num_examples': 3581, 'score': 2688.4423418045044, 'total_duration': 3857.314350128174, 'accumulated_submission_time': 2688.4423418045044, 'accumulated_eval_time': 1116.9619691371918, 'accumulated_logging_time': 2.810589075088501}
I0315 20:20:45.257834 140135622489856 logging_writer.py:48] [16403] accumulated_eval_time=1116.96, accumulated_logging_time=2.81059, accumulated_submission_time=2688.44, global_step=16403, preemption_count=0, score=2688.44, test/loss=0.291357, test/num_examples=3581, test/ssim=0.73674, total_duration=3857.31, train/loss=0.271894, train/ssim=0.739922, validation/loss=0.289865, validation/num_examples=3554, validation/ssim=0.7195
I0315 20:20:52.167274 140135614097152 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.487868, loss=0.200849
I0315 20:20:52.170210 140186397963456 submission.py:265] 16500) loss = 0.201, grad_norm = 0.488
I0315 20:21:23.624394 140135622489856 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.368049, loss=0.21799
I0315 20:21:23.627359 140186397963456 submission.py:265] 17000) loss = 0.218, grad_norm = 0.368
I0315 20:21:55.111387 140135614097152 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.216746, loss=0.256133
I0315 20:21:55.114808 140186397963456 submission.py:265] 17500) loss = 0.256, grad_norm = 0.217
I0315 20:22:05.820255 140186397963456 spec.py:321] Evaluating on the training split.
I0315 20:22:07.823839 140186397963456 spec.py:333] Evaluating on the validation split.
I0315 20:22:09.957508 140186397963456 spec.py:349] Evaluating on the test split.
I0315 20:22:12.042546 140186397963456 submission_runner.py:469] Time since start: 3944.11s, 	Step: 17663, 	{'train/ssim': 0.73988219669887, 'train/loss': 0.27192308221544537, 'validation/ssim': 0.719404590887908, 'validation/loss': 0.28993935228132034, 'validation/num_examples': 3554, 'test/ssim': 0.7366789621003211, 'test/loss': 0.29140760671556476, 'test/num_examples': 3581, 'score': 2767.3195033073425, 'total_duration': 3944.1099047660828, 'accumulated_submission_time': 2767.3195033073425, 'accumulated_eval_time': 1123.184329509735, 'accumulated_logging_time': 2.82999849319458}
I0315 20:22:12.053776 140135622489856 logging_writer.py:48] [17663] accumulated_eval_time=1123.18, accumulated_logging_time=2.83, accumulated_submission_time=2767.32, global_step=17663, preemption_count=0, score=2767.32, test/loss=0.291408, test/num_examples=3581, test/ssim=0.736679, total_duration=3944.11, train/loss=0.271923, train/ssim=0.739882, validation/loss=0.289939, validation/num_examples=3554, validation/ssim=0.719405
I0315 20:22:33.955000 140135614097152 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.213441, loss=0.283748
I0315 20:22:33.958215 140186397963456 submission.py:265] 18000) loss = 0.284, grad_norm = 0.213
I0315 20:23:05.397490 140135622489856 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.341437, loss=0.260052
I0315 20:23:05.400333 140186397963456 submission.py:265] 18500) loss = 0.260, grad_norm = 0.341
I0315 20:23:32.641576 140186397963456 spec.py:321] Evaluating on the training split.
I0315 20:23:34.640042 140186397963456 spec.py:333] Evaluating on the validation split.
I0315 20:23:36.795053 140186397963456 spec.py:349] Evaluating on the test split.
I0315 20:23:38.880790 140186397963456 submission_runner.py:469] Time since start: 4030.95s, 	Step: 18926, 	{'train/ssim': 0.740135942186628, 'train/loss': 0.2717715161187308, 'validation/ssim': 0.7196650808288196, 'validation/loss': 0.2897735578529474, 'validation/num_examples': 3554, 'test/ssim': 0.7369563047594946, 'test/loss': 0.2912311996038118, 'test/num_examples': 3581, 'score': 2846.3506274223328, 'total_duration': 4030.9481229782104, 'accumulated_submission_time': 2846.3506274223328, 'accumulated_eval_time': 1129.4235906600952, 'accumulated_logging_time': 2.8493332862854004}
I0315 20:23:38.892162 140135614097152 logging_writer.py:48] [18926] accumulated_eval_time=1129.42, accumulated_logging_time=2.84933, accumulated_submission_time=2846.35, global_step=18926, preemption_count=0, score=2846.35, test/loss=0.291231, test/num_examples=3581, test/ssim=0.736956, total_duration=4030.95, train/loss=0.271772, train/ssim=0.740136, validation/loss=0.289774, validation/num_examples=3554, validation/ssim=0.719665
I0315 20:23:44.206981 140135622489856 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.245556, loss=0.310916
I0315 20:23:44.209960 140186397963456 submission.py:265] 19000) loss = 0.311, grad_norm = 0.246
I0315 20:24:15.637391 140135614097152 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.187887, loss=0.214289
I0315 20:24:15.640647 140186397963456 submission.py:265] 19500) loss = 0.214, grad_norm = 0.188
I0315 20:24:47.141301 140135622489856 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.327753, loss=0.31215
I0315 20:24:47.144286 140186397963456 submission.py:265] 20000) loss = 0.312, grad_norm = 0.328
I0315 20:24:59.455524 140186397963456 spec.py:321] Evaluating on the training split.
I0315 20:25:01.465577 140186397963456 spec.py:333] Evaluating on the validation split.
I0315 20:25:03.552049 140186397963456 spec.py:349] Evaluating on the test split.
I0315 20:25:05.624507 140186397963456 submission_runner.py:469] Time since start: 4117.69s, 	Step: 20189, 	{'train/ssim': 0.7400953429085868, 'train/loss': 0.2717846802302769, 'validation/ssim': 0.7196287413829487, 'validation/loss': 0.28981508374147086, 'validation/num_examples': 3554, 'test/ssim': 0.7369218073687518, 'test/loss': 0.2912691740042935, 'test/num_examples': 3581, 'score': 2925.382669687271, 'total_duration': 4117.691866874695, 'accumulated_submission_time': 2925.382669687271, 'accumulated_eval_time': 1135.5926797389984, 'accumulated_logging_time': 2.868069648742676}
I0315 20:25:05.635861 140135614097152 logging_writer.py:48] [20189] accumulated_eval_time=1135.59, accumulated_logging_time=2.86807, accumulated_submission_time=2925.38, global_step=20189, preemption_count=0, score=2925.38, test/loss=0.291269, test/num_examples=3581, test/ssim=0.736922, total_duration=4117.69, train/loss=0.271785, train/ssim=0.740095, validation/loss=0.289815, validation/num_examples=3554, validation/ssim=0.719629
I0315 20:25:25.938750 140135622489856 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.222949, loss=0.279409
I0315 20:25:25.942191 140186397963456 submission.py:265] 20500) loss = 0.279, grad_norm = 0.223
I0315 20:25:57.398221 140135614097152 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.24884, loss=0.248656
I0315 20:25:57.401229 140186397963456 submission.py:265] 21000) loss = 0.249, grad_norm = 0.249
I0315 20:26:26.236324 140186397963456 spec.py:321] Evaluating on the training split.
I0315 20:26:28.252870 140186397963456 spec.py:333] Evaluating on the validation split.
I0315 20:26:30.385268 140186397963456 spec.py:349] Evaluating on the test split.
I0315 20:26:32.472805 140186397963456 submission_runner.py:469] Time since start: 4204.54s, 	Step: 21450, 	{'train/ssim': 0.7408422742571149, 'train/loss': 0.27163309710366385, 'validation/ssim': 0.7205015063352912, 'validation/loss': 0.2895551777046989, 'validation/num_examples': 3554, 'test/ssim': 0.7377738111081052, 'test/loss': 0.2909836501457344, 'test/num_examples': 3581, 'score': 3004.4421796798706, 'total_duration': 4204.540153980255, 'accumulated_submission_time': 3004.4421796798706, 'accumulated_eval_time': 1141.8292672634125, 'accumulated_logging_time': 2.8871848583221436}
I0315 20:26:32.484225 140135622489856 logging_writer.py:48] [21450] accumulated_eval_time=1141.83, accumulated_logging_time=2.88718, accumulated_submission_time=3004.44, global_step=21450, preemption_count=0, score=3004.44, test/loss=0.290984, test/num_examples=3581, test/ssim=0.737774, total_duration=4204.54, train/loss=0.271633, train/ssim=0.740842, validation/loss=0.289555, validation/num_examples=3554, validation/ssim=0.720502
I0315 20:26:36.328540 140135614097152 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.200561, loss=0.252482
I0315 20:26:36.331859 140186397963456 submission.py:265] 21500) loss = 0.252, grad_norm = 0.201
I0315 20:27:07.764868 140135622489856 logging_writer.py:48] [22000] global_step=22000, grad_norm=0.20239, loss=0.379346
I0315 20:27:07.767813 140186397963456 submission.py:265] 22000) loss = 0.379, grad_norm = 0.202
I0315 20:27:39.199639 140135614097152 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.337078, loss=0.305577
I0315 20:27:39.202749 140186397963456 submission.py:265] 22500) loss = 0.306, grad_norm = 0.337
I0315 20:27:53.254937 140186397963456 spec.py:321] Evaluating on the training split.
I0315 20:27:55.249683 140186397963456 spec.py:333] Evaluating on the validation split.
I0315 20:27:57.360843 140186397963456 spec.py:349] Evaluating on the test split.
I0315 20:27:59.434713 140186397963456 submission_runner.py:469] Time since start: 4291.50s, 	Step: 22713, 	{'train/ssim': 0.7403130531311035, 'train/loss': 0.27126799310956684, 'validation/ssim': 0.7198815375281373, 'validation/loss': 0.2892965768654157, 'validation/num_examples': 3554, 'test/ssim': 0.7371879690554315, 'test/loss': 0.29075300849972074, 'test/num_examples': 3581, 'score': 3083.4940004348755, 'total_duration': 4291.502067804337, 'accumulated_submission_time': 3083.4940004348755, 'accumulated_eval_time': 1148.0091652870178, 'accumulated_logging_time': 2.906733989715576}
I0315 20:27:59.445559 140135622489856 logging_writer.py:48] [22713] accumulated_eval_time=1148.01, accumulated_logging_time=2.90673, accumulated_submission_time=3083.49, global_step=22713, preemption_count=0, score=3083.49, test/loss=0.290753, test/num_examples=3581, test/ssim=0.737188, total_duration=4291.5, train/loss=0.271268, train/ssim=0.740313, validation/loss=0.289297, validation/num_examples=3554, validation/ssim=0.719882
I0315 20:28:18.423437 140135614097152 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.138503, loss=0.341124
I0315 20:28:18.426579 140186397963456 submission.py:265] 23000) loss = 0.341, grad_norm = 0.139
I0315 20:28:49.787805 140135622489856 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.23904, loss=0.281125
I0315 20:28:49.790941 140186397963456 submission.py:265] 23500) loss = 0.281, grad_norm = 0.239
I0315 20:29:20.064579 140186397963456 spec.py:321] Evaluating on the training split.
I0315 20:29:22.053095 140186397963456 spec.py:333] Evaluating on the validation split.
I0315 20:29:24.156019 140186397963456 spec.py:349] Evaluating on the test split.
I0315 20:29:26.228720 140186397963456 submission_runner.py:469] Time since start: 4378.30s, 	Step: 23974, 	{'train/ssim': 0.7401350566319057, 'train/loss': 0.2710800681795393, 'validation/ssim': 0.7195230890809651, 'validation/loss': 0.28917302961891533, 'validation/num_examples': 3554, 'test/ssim': 0.7369552139329097, 'test/loss': 0.2906009745444708, 'test/num_examples': 3581, 'score': 3162.3393557071686, 'total_duration': 4378.296018362045, 'accumulated_submission_time': 3162.3393557071686, 'accumulated_eval_time': 1154.1733121871948, 'accumulated_logging_time': 2.9260504245758057}
I0315 20:29:26.239974 140135614097152 logging_writer.py:48] [23974] accumulated_eval_time=1154.17, accumulated_logging_time=2.92605, accumulated_submission_time=3162.34, global_step=23974, preemption_count=0, score=3162.34, test/loss=0.290601, test/num_examples=3581, test/ssim=0.736955, total_duration=4378.3, train/loss=0.27108, train/ssim=0.740135, validation/loss=0.289173, validation/num_examples=3554, validation/ssim=0.719523
I0315 20:29:28.603750 140135622489856 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.288293, loss=0.331582
I0315 20:29:28.606791 140186397963456 submission.py:265] 24000) loss = 0.332, grad_norm = 0.288
I0315 20:29:59.996058 140135614097152 logging_writer.py:48] [24500] global_step=24500, grad_norm=0.399991, loss=0.336941
I0315 20:29:59.999165 140186397963456 submission.py:265] 24500) loss = 0.337, grad_norm = 0.400
I0315 20:30:31.379641 140135622489856 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.354815, loss=0.248818
I0315 20:30:31.382827 140186397963456 submission.py:265] 25000) loss = 0.249, grad_norm = 0.355
I0315 20:30:46.960180 140186397963456 spec.py:321] Evaluating on the training split.
I0315 20:30:48.946895 140186397963456 spec.py:333] Evaluating on the validation split.
I0315 20:30:51.156862 140186397963456 spec.py:349] Evaluating on the test split.
I0315 20:30:53.223184 140186397963456 submission_runner.py:469] Time since start: 4465.29s, 	Step: 25238, 	{'train/ssim': 0.7400530406406948, 'train/loss': 0.2712017297744751, 'validation/ssim': 0.7193604202571047, 'validation/loss': 0.28941782284265966, 'validation/num_examples': 3554, 'test/ssim': 0.7367063009416015, 'test/loss': 0.29093149499965093, 'test/num_examples': 3581, 'score': 3241.3589446544647, 'total_duration': 4465.290505170822, 'accumulated_submission_time': 3241.3589446544647, 'accumulated_eval_time': 1160.436423778534, 'accumulated_logging_time': 2.9453465938568115}
I0315 20:30:53.234254 140135614097152 logging_writer.py:48] [25238] accumulated_eval_time=1160.44, accumulated_logging_time=2.94535, accumulated_submission_time=3241.36, global_step=25238, preemption_count=0, score=3241.36, test/loss=0.290931, test/num_examples=3581, test/ssim=0.736706, total_duration=4465.29, train/loss=0.271202, train/ssim=0.740053, validation/loss=0.289418, validation/num_examples=3554, validation/ssim=0.71936
I0315 20:31:10.592330 140135622489856 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.474587, loss=0.242302
I0315 20:31:10.595926 140186397963456 submission.py:265] 25500) loss = 0.242, grad_norm = 0.475
I0315 20:31:42.035584 140135614097152 logging_writer.py:48] [26000] global_step=26000, grad_norm=0.208387, loss=0.294932
I0315 20:31:42.039087 140186397963456 submission.py:265] 26000) loss = 0.295, grad_norm = 0.208
I0315 20:32:13.797405 140186397963456 spec.py:321] Evaluating on the training split.
I0315 20:32:15.787024 140186397963456 spec.py:333] Evaluating on the validation split.
I0315 20:32:17.975823 140186397963456 spec.py:349] Evaluating on the test split.
I0315 20:32:20.051691 140186397963456 submission_runner.py:469] Time since start: 4552.12s, 	Step: 26498, 	{'train/ssim': 0.7395928246634347, 'train/loss': 0.272850615637643, 'validation/ssim': 0.7189854851048115, 'validation/loss': 0.2912730580310038, 'validation/num_examples': 3554, 'test/ssim': 0.736099324123848, 'test/loss': 0.2927947972480627, 'test/num_examples': 3581, 'score': 3320.170478105545, 'total_duration': 4552.119049072266, 'accumulated_submission_time': 3320.170478105545, 'accumulated_eval_time': 1166.690768480301, 'accumulated_logging_time': 2.9652259349823}
I0315 20:32:20.063176 140135622489856 logging_writer.py:48] [26498] accumulated_eval_time=1166.69, accumulated_logging_time=2.96523, accumulated_submission_time=3320.17, global_step=26498, preemption_count=0, score=3320.17, test/loss=0.292795, test/num_examples=3581, test/ssim=0.736099, total_duration=4552.12, train/loss=0.272851, train/ssim=0.739593, validation/loss=0.291273, validation/num_examples=3554, validation/ssim=0.718985
I0315 20:32:20.915234 140135614097152 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.538773, loss=0.253715
I0315 20:32:20.918728 140186397963456 submission.py:265] 26500) loss = 0.254, grad_norm = 0.539
I0315 20:32:52.372739 140135622489856 logging_writer.py:48] [27000] global_step=27000, grad_norm=0.487423, loss=0.328926
I0315 20:32:52.376368 140186397963456 submission.py:265] 27000) loss = 0.329, grad_norm = 0.487
I0315 20:33:23.800468 140135614097152 logging_writer.py:48] [27500] global_step=27500, grad_norm=0.213039, loss=0.258065
I0315 20:33:23.803970 140186397963456 submission.py:265] 27500) loss = 0.258, grad_norm = 0.213
I0315 20:33:40.742868 140186397963456 spec.py:321] Evaluating on the training split.
I0315 20:33:42.724963 140186397963456 spec.py:333] Evaluating on the validation split.
I0315 20:33:44.976377 140186397963456 spec.py:349] Evaluating on the test split.
I0315 20:33:47.052066 140186397963456 submission_runner.py:469] Time since start: 4639.12s, 	Step: 27761, 	{'train/ssim': 0.7404659816196987, 'train/loss': 0.2712956326348441, 'validation/ssim': 0.7201540490248663, 'validation/loss': 0.28919219541361846, 'validation/num_examples': 3554, 'test/ssim': 0.7374921051425929, 'test/loss': 0.2906358469068521, 'test/num_examples': 3581, 'score': 3399.188649892807, 'total_duration': 4639.119434833527, 'accumulated_submission_time': 3399.188649892807, 'accumulated_eval_time': 1173.0001981258392, 'accumulated_logging_time': 2.985692024230957}
I0315 20:33:47.063217 140135622489856 logging_writer.py:48] [27761] accumulated_eval_time=1173, accumulated_logging_time=2.98569, accumulated_submission_time=3399.19, global_step=27761, preemption_count=0, score=3399.19, test/loss=0.290636, test/num_examples=3581, test/ssim=0.737492, total_duration=4639.12, train/loss=0.271296, train/ssim=0.740466, validation/loss=0.289192, validation/num_examples=3554, validation/ssim=0.720154
I0315 20:34:02.958666 140135614097152 logging_writer.py:48] [28000] global_step=28000, grad_norm=0.185161, loss=0.360087
I0315 20:34:02.961603 140186397963456 submission.py:265] 28000) loss = 0.360, grad_norm = 0.185
I0315 20:34:34.406085 140135622489856 logging_writer.py:48] [28500] global_step=28500, grad_norm=0.229087, loss=0.356668
I0315 20:34:34.409262 140186397963456 submission.py:265] 28500) loss = 0.357, grad_norm = 0.229
I0315 20:35:05.818389 140135614097152 logging_writer.py:48] [29000] global_step=29000, grad_norm=0.414397, loss=0.281312
I0315 20:35:05.821416 140186397963456 submission.py:265] 29000) loss = 0.281, grad_norm = 0.414
I0315 20:35:07.779485 140186397963456 spec.py:321] Evaluating on the training split.
I0315 20:35:09.767609 140186397963456 spec.py:333] Evaluating on the validation split.
I0315 20:35:12.018399 140186397963456 spec.py:349] Evaluating on the test split.
I0315 20:35:14.175753 140186397963456 submission_runner.py:469] Time since start: 4726.24s, 	Step: 29021, 	{'train/ssim': 0.7401444571358817, 'train/loss': 0.27202194077628, 'validation/ssim': 0.7198783088817178, 'validation/loss': 0.2898518697022721, 'validation/num_examples': 3554, 'test/ssim': 0.7371833330424462, 'test/loss': 0.29125690220521505, 'test/num_examples': 3581, 'score': 3478.00851893425, 'total_duration': 4726.243095636368, 'accumulated_submission_time': 3478.00851893425, 'accumulated_eval_time': 1179.396704673767, 'accumulated_logging_time': 3.0045039653778076}
I0315 20:35:14.189848 140135622489856 logging_writer.py:48] [29021] accumulated_eval_time=1179.4, accumulated_logging_time=3.0045, accumulated_submission_time=3478.01, global_step=29021, preemption_count=0, score=3478.01, test/loss=0.291257, test/num_examples=3581, test/ssim=0.737183, total_duration=4726.24, train/loss=0.272022, train/ssim=0.740144, validation/loss=0.289852, validation/num_examples=3554, validation/ssim=0.719878
I0315 20:35:45.140497 140135614097152 logging_writer.py:48] [29500] global_step=29500, grad_norm=0.643309, loss=0.255527
I0315 20:35:45.143537 140186397963456 submission.py:265] 29500) loss = 0.256, grad_norm = 0.643
I0315 20:36:16.636765 140135622489856 logging_writer.py:48] [30000] global_step=30000, grad_norm=0.2653, loss=0.294192
I0315 20:36:16.639899 140186397963456 submission.py:265] 30000) loss = 0.294, grad_norm = 0.265
I0315 20:36:34.922628 140186397963456 spec.py:321] Evaluating on the training split.
I0315 20:36:36.791685 140186397963456 spec.py:333] Evaluating on the validation split.
I0315 20:36:38.802768 140186397963456 spec.py:349] Evaluating on the test split.
I0315 20:36:40.783638 140186397963456 submission_runner.py:469] Time since start: 4812.85s, 	Step: 30281, 	{'train/ssim': 0.7407717704772949, 'train/loss': 0.2711109774453299, 'validation/ssim': 0.7206109368405318, 'validation/loss': 0.2890636678082794, 'validation/num_examples': 3554, 'test/ssim': 0.7379377759791259, 'test/loss': 0.29052223050038395, 'test/num_examples': 3581, 'score': 3556.8466629981995, 'total_duration': 4812.850958824158, 'accumulated_submission_time': 3556.8466629981995, 'accumulated_eval_time': 1185.2578961849213, 'accumulated_logging_time': 3.0273828506469727}
I0315 20:36:40.798294 140135614097152 logging_writer.py:48] [30281] accumulated_eval_time=1185.26, accumulated_logging_time=3.02738, accumulated_submission_time=3556.85, global_step=30281, preemption_count=0, score=3556.85, test/loss=0.290522, test/num_examples=3581, test/ssim=0.737938, total_duration=4812.85, train/loss=0.271111, train/ssim=0.740772, validation/loss=0.289064, validation/num_examples=3554, validation/ssim=0.720611
I0315 20:36:55.486657 140135622489856 logging_writer.py:48] [30500] global_step=30500, grad_norm=0.093853, loss=0.305773
I0315 20:36:55.489811 140186397963456 submission.py:265] 30500) loss = 0.306, grad_norm = 0.094
I0315 20:37:26.873528 140135614097152 logging_writer.py:48] [31000] global_step=31000, grad_norm=0.339684, loss=0.313614
I0315 20:37:26.877083 140186397963456 submission.py:265] 31000) loss = 0.314, grad_norm = 0.340
I0315 20:37:58.303773 140135622489856 logging_writer.py:48] [31500] global_step=31500, grad_norm=0.223781, loss=0.270127
I0315 20:37:58.306729 140186397963456 submission.py:265] 31500) loss = 0.270, grad_norm = 0.224
I0315 20:38:01.369764 140186397963456 spec.py:321] Evaluating on the training split.
I0315 20:38:03.487906 140186397963456 spec.py:333] Evaluating on the validation split.
I0315 20:38:05.696477 140186397963456 spec.py:349] Evaluating on the test split.
I0315 20:38:07.888820 140186397963456 submission_runner.py:469] Time since start: 4899.96s, 	Step: 31541, 	{'train/ssim': 0.7397125789097377, 'train/loss': 0.27212623187473844, 'validation/ssim': 0.7193740217888295, 'validation/loss': 0.29003425387767306, 'validation/num_examples': 3554, 'test/ssim': 0.7366589863384878, 'test/loss': 0.2915125646860165, 'test/num_examples': 3581, 'score': 3635.6410682201385, 'total_duration': 4899.956174850464, 'accumulated_submission_time': 3635.6410682201385, 'accumulated_eval_time': 1191.7770550251007, 'accumulated_logging_time': 3.050157308578491}
I0315 20:38:07.900276 140135614097152 logging_writer.py:48] [31541] accumulated_eval_time=1191.78, accumulated_logging_time=3.05016, accumulated_submission_time=3635.64, global_step=31541, preemption_count=0, score=3635.64, test/loss=0.291513, test/num_examples=3581, test/ssim=0.736659, total_duration=4899.96, train/loss=0.272126, train/ssim=0.739713, validation/loss=0.290034, validation/num_examples=3554, validation/ssim=0.719374
I0315 20:38:37.447265 140135622489856 logging_writer.py:48] [32000] global_step=32000, grad_norm=0.260778, loss=0.21199
I0315 20:38:37.450609 140186397963456 submission.py:265] 32000) loss = 0.212, grad_norm = 0.261
I0315 20:39:08.881536 140135614097152 logging_writer.py:48] [32500] global_step=32500, grad_norm=0.321237, loss=0.355744
I0315 20:39:08.884428 140186397963456 submission.py:265] 32500) loss = 0.356, grad_norm = 0.321
I0315 20:39:28.623832 140186397963456 spec.py:321] Evaluating on the training split.
I0315 20:39:30.609209 140186397963456 spec.py:333] Evaluating on the validation split.
I0315 20:39:32.739052 140186397963456 spec.py:349] Evaluating on the test split.
I0315 20:39:34.826965 140186397963456 submission_runner.py:469] Time since start: 4986.89s, 	Step: 32804, 	{'train/ssim': 0.7418554169791085, 'train/loss': 0.26988162313188824, 'validation/ssim': 0.721446812790166, 'validation/loss': 0.28802124435605125, 'validation/num_examples': 3554, 'test/ssim': 0.7389320644111281, 'test/loss': 0.2894319152405927, 'test/num_examples': 3581, 'score': 3714.6501758098602, 'total_duration': 4986.894310474396, 'accumulated_submission_time': 3714.6501758098602, 'accumulated_eval_time': 1197.980364561081, 'accumulated_logging_time': 3.0693719387054443}
I0315 20:39:34.838673 140135622489856 logging_writer.py:48] [32804] accumulated_eval_time=1197.98, accumulated_logging_time=3.06937, accumulated_submission_time=3714.65, global_step=32804, preemption_count=0, score=3714.65, test/loss=0.289432, test/num_examples=3581, test/ssim=0.738932, total_duration=4986.89, train/loss=0.269882, train/ssim=0.741855, validation/loss=0.288021, validation/num_examples=3554, validation/ssim=0.721447
I0315 20:39:48.021049 140135614097152 logging_writer.py:48] [33000] global_step=33000, grad_norm=0.241186, loss=0.285597
I0315 20:39:48.024581 140186397963456 submission.py:265] 33000) loss = 0.286, grad_norm = 0.241
I0315 20:40:19.394642 140135622489856 logging_writer.py:48] [33500] global_step=33500, grad_norm=0.371743, loss=0.258394
I0315 20:40:19.397715 140186397963456 submission.py:265] 33500) loss = 0.258, grad_norm = 0.372
I0315 20:40:50.798962 140135614097152 logging_writer.py:48] [34000] global_step=34000, grad_norm=0.203106, loss=0.33112
I0315 20:40:50.801651 140186397963456 submission.py:265] 34000) loss = 0.331, grad_norm = 0.203
I0315 20:40:55.508910 140186397963456 spec.py:321] Evaluating on the training split.
I0315 20:40:57.496188 140186397963456 spec.py:333] Evaluating on the validation split.
I0315 20:40:59.620455 140186397963456 spec.py:349] Evaluating on the test split.
I0315 20:41:01.699643 140186397963456 submission_runner.py:469] Time since start: 5073.77s, 	Step: 34067, 	{'train/ssim': 0.7407678876604352, 'train/loss': 0.27108287811279297, 'validation/ssim': 0.7208530853219963, 'validation/loss': 0.2887741887441967, 'validation/num_examples': 3554, 'test/ssim': 0.738248593379119, 'test/loss': 0.2901182496923869, 'test/num_examples': 3581, 'score': 3793.4982719421387, 'total_duration': 5073.766982555389, 'accumulated_submission_time': 3793.4982719421387, 'accumulated_eval_time': 1204.1712939739227, 'accumulated_logging_time': 3.0891032218933105}
I0315 20:41:01.711715 140135622489856 logging_writer.py:48] [34067] accumulated_eval_time=1204.17, accumulated_logging_time=3.0891, accumulated_submission_time=3793.5, global_step=34067, preemption_count=0, score=3793.5, test/loss=0.290118, test/num_examples=3581, test/ssim=0.738249, total_duration=5073.77, train/loss=0.271083, train/ssim=0.740768, validation/loss=0.288774, validation/num_examples=3554, validation/ssim=0.720853
I0315 20:41:29.751281 140135614097152 logging_writer.py:48] [34500] global_step=34500, grad_norm=0.0943496, loss=0.289471
I0315 20:41:29.754719 140186397963456 submission.py:265] 34500) loss = 0.289, grad_norm = 0.094
I0315 20:42:01.169025 140135622489856 logging_writer.py:48] [35000] global_step=35000, grad_norm=0.262071, loss=0.32012
I0315 20:42:01.172404 140186397963456 submission.py:265] 35000) loss = 0.320, grad_norm = 0.262
I0315 20:42:22.305994 140186397963456 spec.py:321] Evaluating on the training split.
I0315 20:42:24.324262 140186397963456 spec.py:333] Evaluating on the validation split.
I0315 20:42:26.449348 140186397963456 spec.py:349] Evaluating on the test split.
I0315 20:42:28.536739 140186397963456 submission_runner.py:469] Time since start: 5160.60s, 	Step: 35329, 	{'train/ssim': 0.7404485430036273, 'train/loss': 0.2722528321402414, 'validation/ssim': 0.7201015663468978, 'validation/loss': 0.29008529396894345, 'validation/num_examples': 3554, 'test/ssim': 0.7375035588217328, 'test/loss': 0.2914833850748743, 'test/num_examples': 3581, 'score': 3872.440690755844, 'total_duration': 5160.604091644287, 'accumulated_submission_time': 3872.440690755844, 'accumulated_eval_time': 1210.4021348953247, 'accumulated_logging_time': 3.1091811656951904}
I0315 20:42:28.549127 140135614097152 logging_writer.py:48] [35329] accumulated_eval_time=1210.4, accumulated_logging_time=3.10918, accumulated_submission_time=3872.44, global_step=35329, preemption_count=0, score=3872.44, test/loss=0.291483, test/num_examples=3581, test/ssim=0.737504, total_duration=5160.6, train/loss=0.272253, train/ssim=0.740449, validation/loss=0.290085, validation/num_examples=3554, validation/ssim=0.720102
I0315 20:42:39.951588 140135622489856 logging_writer.py:48] [35500] global_step=35500, grad_norm=0.0929571, loss=0.235766
I0315 20:42:39.954505 140186397963456 submission.py:265] 35500) loss = 0.236, grad_norm = 0.093
I0315 20:43:11.334980 140135614097152 logging_writer.py:48] [36000] global_step=36000, grad_norm=0.34201, loss=0.272298
I0315 20:43:11.338047 140186397963456 submission.py:265] 36000) loss = 0.272, grad_norm = 0.342
I0315 20:43:42.744293 140135622489856 logging_writer.py:48] [36500] global_step=36500, grad_norm=0.18057, loss=0.322437
I0315 20:43:42.747571 140186397963456 submission.py:265] 36500) loss = 0.322, grad_norm = 0.181
I0315 20:43:49.242782 140186397963456 spec.py:321] Evaluating on the training split.
I0315 20:43:51.240783 140186397963456 spec.py:333] Evaluating on the validation split.
I0315 20:43:53.336762 140186397963456 spec.py:349] Evaluating on the test split.
I0315 20:43:55.419594 140186397963456 submission_runner.py:469] Time since start: 5247.49s, 	Step: 36595, 	{'train/ssim': 0.7422323908124652, 'train/loss': 0.27098608016967773, 'validation/ssim': 0.7221174095209623, 'validation/loss': 0.28886373216138506, 'validation/num_examples': 3554, 'test/ssim': 0.739335942954133, 'test/loss': 0.2902255256693312, 'test/num_examples': 3581, 'score': 3951.5058710575104, 'total_duration': 5247.4869701862335, 'accumulated_submission_time': 3951.5058710575104, 'accumulated_eval_time': 1216.5791521072388, 'accumulated_logging_time': 3.1301071643829346}
I0315 20:43:55.432193 140135614097152 logging_writer.py:48] [36595] accumulated_eval_time=1216.58, accumulated_logging_time=3.13011, accumulated_submission_time=3951.51, global_step=36595, preemption_count=0, score=3951.51, test/loss=0.290226, test/num_examples=3581, test/ssim=0.739336, total_duration=5247.49, train/loss=0.270986, train/ssim=0.742232, validation/loss=0.288864, validation/num_examples=3554, validation/ssim=0.722117
I0315 20:44:21.702447 140135622489856 logging_writer.py:48] [37000] global_step=37000, grad_norm=0.174784, loss=0.2307
I0315 20:44:21.705505 140186397963456 submission.py:265] 37000) loss = 0.231, grad_norm = 0.175
I0315 20:44:53.142701 140135614097152 logging_writer.py:48] [37500] global_step=37500, grad_norm=0.385818, loss=0.285231
I0315 20:44:53.146114 140186397963456 submission.py:265] 37500) loss = 0.285, grad_norm = 0.386
I0315 20:45:16.168651 140186397963456 spec.py:321] Evaluating on the training split.
I0315 20:45:18.154312 140186397963456 spec.py:333] Evaluating on the validation split.
I0315 20:45:20.247074 140186397963456 spec.py:349] Evaluating on the test split.
I0315 20:45:22.313027 140186397963456 submission_runner.py:469] Time since start: 5334.38s, 	Step: 37857, 	{'train/ssim': 0.7394915989467076, 'train/loss': 0.2727263314383371, 'validation/ssim': 0.7190967016697735, 'validation/loss': 0.2908805370599149, 'validation/num_examples': 3554, 'test/ssim': 0.7362885825363027, 'test/loss': 0.29230532290648564, 'test/num_examples': 3581, 'score': 4030.429238796234, 'total_duration': 5334.380391359329, 'accumulated_submission_time': 4030.429238796234, 'accumulated_eval_time': 1222.723665714264, 'accumulated_logging_time': 3.1515796184539795}
I0315 20:45:22.324519 140135622489856 logging_writer.py:48] [37857] accumulated_eval_time=1222.72, accumulated_logging_time=3.15158, accumulated_submission_time=4030.43, global_step=37857, preemption_count=0, score=4030.43, test/loss=0.292305, test/num_examples=3581, test/ssim=0.736289, total_duration=5334.38, train/loss=0.272726, train/ssim=0.739492, validation/loss=0.290881, validation/num_examples=3554, validation/ssim=0.719097
I0315 20:45:32.147023 140135614097152 logging_writer.py:48] [38000] global_step=38000, grad_norm=0.333445, loss=0.267722
I0315 20:45:32.149969 140186397963456 submission.py:265] 38000) loss = 0.268, grad_norm = 0.333
I0315 20:46:03.526554 140135622489856 logging_writer.py:48] [38500] global_step=38500, grad_norm=0.179601, loss=0.266221
I0315 20:46:03.530314 140186397963456 submission.py:265] 38500) loss = 0.266, grad_norm = 0.180
I0315 20:46:34.914332 140135614097152 logging_writer.py:48] [39000] global_step=39000, grad_norm=0.301714, loss=0.341389
I0315 20:46:34.917783 140186397963456 submission.py:265] 39000) loss = 0.341, grad_norm = 0.302
I0315 20:46:43.036006 140186397963456 spec.py:321] Evaluating on the training split.
I0315 20:46:45.038099 140186397963456 spec.py:333] Evaluating on the validation split.
I0315 20:46:47.167321 140186397963456 spec.py:349] Evaluating on the test split.
I0315 20:46:49.257195 140186397963456 submission_runner.py:469] Time since start: 5421.32s, 	Step: 39119, 	{'train/ssim': 0.7407628468104771, 'train/loss': 0.2713686738695417, 'validation/ssim': 0.7204628312728616, 'validation/loss': 0.2891524555848164, 'validation/num_examples': 3554, 'test/ssim': 0.737906755598122, 'test/loss': 0.2905214464687762, 'test/num_examples': 3581, 'score': 4109.2731120586395, 'total_duration': 5421.324540138245, 'accumulated_submission_time': 4109.2731120586395, 'accumulated_eval_time': 1228.9450013637543, 'accumulated_logging_time': 3.1711127758026123}
I0315 20:46:49.269337 140135622489856 logging_writer.py:48] [39119] accumulated_eval_time=1228.95, accumulated_logging_time=3.17111, accumulated_submission_time=4109.27, global_step=39119, preemption_count=0, score=4109.27, test/loss=0.290521, test/num_examples=3581, test/ssim=0.737907, total_duration=5421.32, train/loss=0.271369, train/ssim=0.740763, validation/loss=0.289152, validation/num_examples=3554, validation/ssim=0.720463
I0315 20:47:14.121494 140135614097152 logging_writer.py:48] [39500] global_step=39500, grad_norm=0.452892, loss=0.262909
I0315 20:47:14.124541 140186397963456 submission.py:265] 39500) loss = 0.263, grad_norm = 0.453
I0315 20:47:45.576218 140135622489856 logging_writer.py:48] [40000] global_step=40000, grad_norm=0.552689, loss=0.294563
I0315 20:47:45.579138 140186397963456 submission.py:265] 40000) loss = 0.295, grad_norm = 0.553
I0315 20:48:09.948955 140186397963456 spec.py:321] Evaluating on the training split.
I0315 20:48:11.933429 140186397963456 spec.py:333] Evaluating on the validation split.
I0315 20:48:14.027599 140186397963456 spec.py:349] Evaluating on the test split.
I0315 20:48:16.104474 140186397963456 submission_runner.py:469] Time since start: 5508.17s, 	Step: 40380, 	{'train/ssim': 0.7396753856113979, 'train/loss': 0.2711759465081351, 'validation/ssim': 0.7193080062737408, 'validation/loss': 0.28906655298167555, 'validation/num_examples': 3554, 'test/ssim': 0.736995983576515, 'test/loss': 0.290319473108943, 'test/num_examples': 3581, 'score': 4188.126078367233, 'total_duration': 5508.1718373298645, 'accumulated_submission_time': 4188.126078367233, 'accumulated_eval_time': 1235.1007196903229, 'accumulated_logging_time': 3.1906673908233643}
I0315 20:48:16.117833 140135614097152 logging_writer.py:48] [40380] accumulated_eval_time=1235.1, accumulated_logging_time=3.19067, accumulated_submission_time=4188.13, global_step=40380, preemption_count=0, score=4188.13, test/loss=0.290319, test/num_examples=3581, test/ssim=0.736996, total_duration=5508.17, train/loss=0.271176, train/ssim=0.739675, validation/loss=0.289067, validation/num_examples=3554, validation/ssim=0.719308
I0315 20:48:24.502560 140135622489856 logging_writer.py:48] [40500] global_step=40500, grad_norm=0.280424, loss=0.343871
I0315 20:48:24.506234 140186397963456 submission.py:265] 40500) loss = 0.344, grad_norm = 0.280
I0315 20:48:55.887899 140135614097152 logging_writer.py:48] [41000] global_step=41000, grad_norm=0.144988, loss=0.2209
I0315 20:48:55.890828 140186397963456 submission.py:265] 41000) loss = 0.221, grad_norm = 0.145
I0315 20:49:27.251255 140135622489856 logging_writer.py:48] [41500] global_step=41500, grad_norm=0.270868, loss=0.308575
I0315 20:49:27.254730 140186397963456 submission.py:265] 41500) loss = 0.309, grad_norm = 0.271
I0315 20:49:36.825813 140186397963456 spec.py:321] Evaluating on the training split.
I0315 20:49:38.805964 140186397963456 spec.py:333] Evaluating on the validation split.
I0315 20:49:40.916213 140186397963456 spec.py:349] Evaluating on the test split.
I0315 20:49:42.980080 140186397963456 submission_runner.py:469] Time since start: 5595.05s, 	Step: 41643, 	{'train/ssim': 0.7431534358433315, 'train/loss': 0.27019127777644564, 'validation/ssim': 0.722939203076639, 'validation/loss': 0.28824881240767447, 'validation/num_examples': 3554, 'test/ssim': 0.7402101041084892, 'test/loss': 0.2896101631222075, 'test/num_examples': 3581, 'score': 4267.058005809784, 'total_duration': 5595.047441482544, 'accumulated_submission_time': 4267.058005809784, 'accumulated_eval_time': 1241.2552149295807, 'accumulated_logging_time': 3.211714506149292}
I0315 20:49:42.992227 140135614097152 logging_writer.py:48] [41643] accumulated_eval_time=1241.26, accumulated_logging_time=3.21171, accumulated_submission_time=4267.06, global_step=41643, preemption_count=0, score=4267.06, test/loss=0.28961, test/num_examples=3581, test/ssim=0.74021, total_duration=5595.05, train/loss=0.270191, train/ssim=0.743153, validation/loss=0.288249, validation/num_examples=3554, validation/ssim=0.722939
I0315 20:50:06.221231 140135622489856 logging_writer.py:48] [42000] global_step=42000, grad_norm=0.421713, loss=0.271654
I0315 20:50:06.224127 140186397963456 submission.py:265] 42000) loss = 0.272, grad_norm = 0.422
I0315 20:50:37.629614 140135614097152 logging_writer.py:48] [42500] global_step=42500, grad_norm=0.342458, loss=0.319151
I0315 20:50:37.632699 140186397963456 submission.py:265] 42500) loss = 0.319, grad_norm = 0.342
I0315 20:51:03.580669 140186397963456 spec.py:321] Evaluating on the training split.
I0315 20:51:05.571270 140186397963456 spec.py:333] Evaluating on the validation split.
I0315 20:51:07.683227 140186397963456 spec.py:349] Evaluating on the test split.
I0315 20:51:09.753373 140186397963456 submission_runner.py:469] Time since start: 5681.82s, 	Step: 42906, 	{'train/ssim': 0.7432664462498256, 'train/loss': 0.2701637234006609, 'validation/ssim': 0.7229615975177617, 'validation/loss': 0.28816253198420794, 'validation/num_examples': 3554, 'test/ssim': 0.7403097102110094, 'test/loss': 0.2895042506784941, 'test/num_examples': 3581, 'score': 4345.948639631271, 'total_duration': 5681.8207404613495, 'accumulated_submission_time': 4345.948639631271, 'accumulated_eval_time': 1247.42800283432, 'accumulated_logging_time': 3.232004404067993}
I0315 20:51:09.766161 140135622489856 logging_writer.py:48] [42906] accumulated_eval_time=1247.43, accumulated_logging_time=3.232, accumulated_submission_time=4345.95, global_step=42906, preemption_count=0, score=4345.95, test/loss=0.289504, test/num_examples=3581, test/ssim=0.74031, total_duration=5681.82, train/loss=0.270164, train/ssim=0.743266, validation/loss=0.288163, validation/num_examples=3554, validation/ssim=0.722962
I0315 20:51:16.381237 140135614097152 logging_writer.py:48] [43000] global_step=43000, grad_norm=0.274834, loss=0.388873
I0315 20:51:16.384214 140186397963456 submission.py:265] 43000) loss = 0.389, grad_norm = 0.275
I0315 20:51:47.812069 140135622489856 logging_writer.py:48] [43500] global_step=43500, grad_norm=0.187245, loss=0.326033
I0315 20:51:47.815021 140186397963456 submission.py:265] 43500) loss = 0.326, grad_norm = 0.187
I0315 20:52:19.186074 140135614097152 logging_writer.py:48] [44000] global_step=44000, grad_norm=0.56239, loss=0.334584
I0315 20:52:19.189613 140186397963456 submission.py:265] 44000) loss = 0.335, grad_norm = 0.562
I0315 20:52:30.323274 140186397963456 spec.py:321] Evaluating on the training split.
I0315 20:52:32.330334 140186397963456 spec.py:333] Evaluating on the validation split.
I0315 20:52:34.418232 140186397963456 spec.py:349] Evaluating on the test split.
I0315 20:52:36.496218 140186397963456 submission_runner.py:469] Time since start: 5768.56s, 	Step: 44171, 	{'train/ssim': 0.7426543235778809, 'train/loss': 0.26990229742867605, 'validation/ssim': 0.7220100398538618, 'validation/loss': 0.2882151863986881, 'validation/num_examples': 3554, 'test/ssim': 0.739357009542551, 'test/loss': 0.2895896419470818, 'test/num_examples': 3581, 'score': 4425.003296375275, 'total_duration': 5768.563581228256, 'accumulated_submission_time': 4425.003296375275, 'accumulated_eval_time': 1253.6011283397675, 'accumulated_logging_time': 3.2529613971710205}
I0315 20:52:36.508886 140135622489856 logging_writer.py:48] [44171] accumulated_eval_time=1253.6, accumulated_logging_time=3.25296, accumulated_submission_time=4425, global_step=44171, preemption_count=0, score=4425, test/loss=0.28959, test/num_examples=3581, test/ssim=0.739357, total_duration=5768.56, train/loss=0.269902, train/ssim=0.742654, validation/loss=0.288215, validation/num_examples=3554, validation/ssim=0.72201
I0315 20:52:57.920348 140135614097152 logging_writer.py:48] [44500] global_step=44500, grad_norm=0.394913, loss=0.207964
I0315 20:52:57.924173 140186397963456 submission.py:265] 44500) loss = 0.208, grad_norm = 0.395
I0315 20:53:29.406641 140135622489856 logging_writer.py:48] [45000] global_step=45000, grad_norm=0.528179, loss=0.2764
I0315 20:53:29.409744 140186397963456 submission.py:265] 45000) loss = 0.276, grad_norm = 0.528
I0315 20:53:56.543341 140135614097152 logging_writer.py:48] [45433] global_step=45433, preemption_count=0, score=4504.07
I0315 20:53:57.450710 140186397963456 submission_runner.py:646] Tuning trial 4/5
I0315 20:53:57.450875 140186397963456 submission_runner.py:647] Hyperparameters: Hyperparameters(learning_rate=0.0012, one_minus_beta1=0.016610699316537858, one_minus_beta2=0.005888216674053163, epsilon=1e-08, one_minus_momentum=0.5, use_momentum=True, weight_decay=0.00040349948255455174, max_preconditioner_dim=1024, precondition_frequency=100, start_preconditioning_step=-1, inv_root_override=2, exponent_multiplier=1.0, grafting_type='ADAM', grafting_epsilon=1e-08, use_normalized_grafting=False, communication_dtype='FP32', communicate_params=True, use_cosine_decay=True, warmup_factor=0.02, label_smoothing=0.1, dropout_rate=0.1, use_nadam=True, step_hint_factor=1.0)
I0315 20:53:57.452165 140186397963456 submission_runner.py:648] Metrics: {'eval_results': [(1, {'train/ssim': 0.18409654072352818, 'train/loss': 1.1149094445364816, 'validation/ssim': 0.1786466050027258, 'validation/loss': 1.121061188757386, 'validation/num_examples': 3554, 'test/ssim': 0.1992406347083566, 'test/loss': 1.1195749103340547, 'test/num_examples': 3581, 'score': 291.90677642822266, 'total_duration': 1217.9073350429535, 'accumulated_submission_time': 291.90677642822266, 'accumulated_eval_time': 925.1751244068146, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (246, {'train/ssim': 0.683241435459682, 'train/loss': 0.3313148702893938, 'validation/ssim': 0.6630600396340391, 'validation/loss': 0.35151733329742896, 'validation/num_examples': 3554, 'test/ssim': 0.6822898031712511, 'test/loss': 0.35207542711314926, 'test/num_examples': 3581, 'score': 371.9599258899689, 'total_duration': 1306.1063628196716, 'accumulated_submission_time': 371.9599258899689, 'accumulated_eval_time': 931.5982193946838, 'accumulated_logging_time': 0.016763925552368164, 'global_step': 246, 'preemption_count': 0}), (292, {'train/ssim': 0.6847618647984096, 'train/loss': 0.32754250935145784, 'validation/ssim': 0.6656360186145892, 'validation/loss': 0.34662091926570415, 'validation/num_examples': 3554, 'test/ssim': 0.6842660400595155, 'test/loss': 0.3475430426535011, 'test/num_examples': 3581, 'score': 452.77306604385376, 'total_duration': 1394.8453109264374, 'accumulated_submission_time': 452.77306604385376, 'accumulated_eval_time': 938.1863675117493, 'accumulated_logging_time': 0.03412342071533203, 'global_step': 292, 'preemption_count': 0}), (339, {'train/ssim': 0.6921582221984863, 'train/loss': 0.32070415360586985, 'validation/ssim': 0.672151907951428, 'validation/loss': 0.3399712815335713, 'validation/num_examples': 3554, 'test/ssim': 0.6912383308826096, 'test/loss': 0.34079965497155124, 'test/num_examples': 3581, 'score': 533.3589107990265, 'total_duration': 1483.3803660869598, 'accumulated_submission_time': 533.3589107990265, 'accumulated_eval_time': 944.7853229045868, 'accumulated_logging_time': 0.06541895866394043, 'global_step': 339, 'preemption_count': 0}), (381, {'train/ssim': 0.6937106677464077, 'train/loss': 0.31738380023411344, 'validation/ssim': 0.674216043432576, 'validation/loss': 0.3358735796703538, 'validation/num_examples': 3554, 'test/ssim': 0.6930863956471656, 'test/loss': 0.33695558208688214, 'test/num_examples': 3581, 'score': 614.808497428894, 'total_duration': 1572.6405503749847, 'accumulated_submission_time': 614.808497428894, 'accumulated_eval_time': 951.2462854385376, 'accumulated_logging_time': 0.08534479141235352, 'global_step': 381, 'preemption_count': 0}), (426, {'train/ssim': 0.6991664341517857, 'train/loss': 0.31401896476745605, 'validation/ssim': 0.6800107768095808, 'validation/loss': 0.332486660881665, 'validation/num_examples': 3554, 'test/ssim': 0.6986218633281556, 'test/loss': 0.33361420981613027, 'test/num_examples': 3581, 'score': 697.245766878128, 'total_duration': 1663.0957493782043, 'accumulated_submission_time': 697.245766878128, 'accumulated_eval_time': 957.9321157932281, 'accumulated_logging_time': 0.10335135459899902, 'global_step': 426, 'preemption_count': 0}), (471, {'train/ssim': 0.700164931161063, 'train/loss': 0.3100807326180594, 'validation/ssim': 0.6808697341463844, 'validation/loss': 0.32818387127620285, 'validation/num_examples': 3554, 'test/ssim': 0.6996092659173415, 'test/loss': 0.3294896581458566, 'test/num_examples': 3581, 'score': 778.2560493946075, 'total_duration': 1751.8961975574493, 'accumulated_submission_time': 778.2560493946075, 'accumulated_eval_time': 964.1728105545044, 'accumulated_logging_time': 0.12435078620910645, 'global_step': 471, 'preemption_count': 0}), (511, {'train/ssim': 0.7027715955461774, 'train/loss': 0.30712958744594027, 'validation/ssim': 0.6837593726918613, 'validation/loss': 0.3252596106499719, 'validation/num_examples': 3554, 'test/ssim': 0.702157164112678, 'test/loss': 0.326789896436924, 'test/num_examples': 3581, 'score': 859.5855665206909, 'total_duration': 1841.0811610221863, 'accumulated_submission_time': 859.5855665206909, 'accumulated_eval_time': 970.4861626625061, 'accumulated_logging_time': 0.14436769485473633, 'global_step': 511, 'preemption_count': 0}), (552, {'train/ssim': 0.7054307120186942, 'train/loss': 0.305295603615897, 'validation/ssim': 0.686424929436902, 'validation/loss': 0.3232657497494021, 'validation/num_examples': 3554, 'test/ssim': 0.7046996081750908, 'test/loss': 0.324789968158772, 'test/num_examples': 3581, 'score': 939.7510685920715, 'total_duration': 1929.534300327301, 'accumulated_submission_time': 939.7510685920715, 'accumulated_eval_time': 977.1934859752655, 'accumulated_logging_time': 0.1838066577911377, 'global_step': 552, 'preemption_count': 0}), (595, {'train/ssim': 0.7052392959594727, 'train/loss': 0.303933926991054, 'validation/ssim': 0.6867056155915869, 'validation/loss': 0.3218366271938309, 'validation/num_examples': 3554, 'test/ssim': 0.7044920784173415, 'test/loss': 0.3236474636645665, 'test/num_examples': 3581, 'score': 1021.8879618644714, 'total_duration': 2019.9614052772522, 'accumulated_submission_time': 1021.8879618644714, 'accumulated_eval_time': 984.0781018733978, 'accumulated_logging_time': 0.20573854446411133, 'global_step': 595, 'preemption_count': 0}), (641, {'train/ssim': 0.7071234158107212, 'train/loss': 0.3007950782775879, 'validation/ssim': 0.687910656346722, 'validation/loss': 0.3186262191918437, 'validation/num_examples': 3554, 'test/ssim': 0.7060493697203993, 'test/loss': 0.32046153418214185, 'test/num_examples': 3581, 'score': 1101.4734392166138, 'total_duration': 2107.624259710312, 'accumulated_submission_time': 1101.4734392166138, 'accumulated_eval_time': 990.8655195236206, 'accumulated_logging_time': 0.22465085983276367, 'global_step': 641, 'preemption_count': 0}), (684, {'train/ssim': 0.7102981294904437, 'train/loss': 0.29928711482456755, 'validation/ssim': 0.6917001255187817, 'validation/loss': 0.31687110638980726, 'validation/num_examples': 3554, 'test/ssim': 0.7092972376998394, 'test/loss': 0.3189118104885158, 'test/num_examples': 3581, 'score': 1179.050456047058, 'total_duration': 2194.921053647995, 'accumulated_submission_time': 1179.050456047058, 'accumulated_eval_time': 996.9152300357819, 'accumulated_logging_time': 2.405137538909912, 'global_step': 684, 'preemption_count': 0}), (728, {'train/ssim': 0.7101012638636998, 'train/loss': 0.29810987200055805, 'validation/ssim': 0.6916155624604319, 'validation/loss': 0.31551088452272086, 'validation/num_examples': 3554, 'test/ssim': 0.7091854279749022, 'test/loss': 0.31762108993210697, 'test/num_examples': 3581, 'score': 1260.2919743061066, 'total_duration': 2283.938000202179, 'accumulated_submission_time': 1260.2919743061066, 'accumulated_eval_time': 1003.282835483551, 'accumulated_logging_time': 2.423387050628662, 'global_step': 728, 'preemption_count': 0}), (771, {'train/ssim': 0.7121325901576451, 'train/loss': 0.2968846729823521, 'validation/ssim': 0.693021534934229, 'validation/loss': 0.31457784005477984, 'validation/num_examples': 3554, 'test/ssim': 0.711100510397759, 'test/loss': 0.31620771956157495, 'test/num_examples': 3581, 'score': 1340.7943110466003, 'total_duration': 2371.850153684616, 'accumulated_submission_time': 1340.7943110466003, 'accumulated_eval_time': 1009.1938934326172, 'accumulated_logging_time': 2.445952892303467, 'global_step': 771, 'preemption_count': 0}), (816, {'train/ssim': 0.7137889862060547, 'train/loss': 0.2947451046534947, 'validation/ssim': 0.6948450332152153, 'validation/loss': 0.31222188989299027, 'validation/num_examples': 3554, 'test/ssim': 0.7124516354762985, 'test/loss': 0.31426311664426837, 'test/num_examples': 3581, 'score': 1420.809282541275, 'total_duration': 2459.462755203247, 'accumulated_submission_time': 1420.809282541275, 'accumulated_eval_time': 1015.2336835861206, 'accumulated_logging_time': 2.468169927597046, 'global_step': 816, 'preemption_count': 0}), (860, {'train/ssim': 0.7154627527509417, 'train/loss': 0.29365757533482145, 'validation/ssim': 0.6967026727147229, 'validation/loss': 0.31111758977560494, 'validation/num_examples': 3554, 'test/ssim': 0.713929364615331, 'test/loss': 0.3134351110897794, 'test/num_examples': 3581, 'score': 1500.8472037315369, 'total_duration': 2547.7205147743225, 'accumulated_submission_time': 1500.8472037315369, 'accumulated_eval_time': 1021.9365949630737, 'accumulated_logging_time': 2.488558053970337, 'global_step': 860, 'preemption_count': 0}), (904, {'train/ssim': 0.717120715550014, 'train/loss': 0.29315594264439176, 'validation/ssim': 0.699380869267023, 'validation/loss': 0.3099805910263963, 'validation/num_examples': 3554, 'test/ssim': 0.7166539767173974, 'test/loss': 0.31200200357572955, 'test/num_examples': 3581, 'score': 1583.2446143627167, 'total_duration': 2638.1406264305115, 'accumulated_submission_time': 1583.2446143627167, 'accumulated_eval_time': 1028.58087849617, 'accumulated_logging_time': 2.514470100402832, 'global_step': 904, 'preemption_count': 0}), (945, {'train/ssim': 0.7174336569649833, 'train/loss': 0.29124389375959125, 'validation/ssim': 0.698752176245076, 'validation/loss': 0.30859399043111635, 'validation/num_examples': 3554, 'test/ssim': 0.7158572642505585, 'test/loss': 0.3109803422141162, 'test/num_examples': 3581, 'score': 1663.2505435943604, 'total_duration': 2726.150632619858, 'accumulated_submission_time': 1663.2505435943604, 'accumulated_eval_time': 1035.136889219284, 'accumulated_logging_time': 2.533992052078247, 'global_step': 945, 'preemption_count': 0}), (1306, {'train/ssim': 0.7231512750898089, 'train/loss': 0.2860053607395717, 'validation/ssim': 0.7043531221423045, 'validation/loss': 0.303184152924082, 'validation/num_examples': 3554, 'test/ssim': 0.721632577426871, 'test/loss': 0.30526171793188006, 'test/num_examples': 3581, 'score': 1742.3422038555145, 'total_duration': 2813.7800657749176, 'accumulated_submission_time': 1742.3422038555145, 'accumulated_eval_time': 1042.0200634002686, 'accumulated_logging_time': 2.5821340084075928, 'global_step': 1306, 'preemption_count': 0}), (2564, {'train/ssim': 0.7327795028686523, 'train/loss': 0.27849178654806955, 'validation/ssim': 0.7133378268764069, 'validation/loss': 0.29591660722293545, 'validation/num_examples': 3554, 'test/ssim': 0.7304306392461952, 'test/loss': 0.29781634925431094, 'test/num_examples': 3581, 'score': 1821.1868495941162, 'total_duration': 2900.7056114673615, 'accumulated_submission_time': 1821.1868495941162, 'accumulated_eval_time': 1048.2100443840027, 'accumulated_logging_time': 2.6013505458831787, 'global_step': 2564, 'preemption_count': 0}), (3820, {'train/ssim': 0.7330936704363141, 'train/loss': 0.2768761089869908, 'validation/ssim': 0.7134974044430571, 'validation/loss': 0.2942110919254713, 'validation/num_examples': 3554, 'test/ssim': 0.7310885440301242, 'test/loss': 0.2957698222279915, 'test/num_examples': 3581, 'score': 1900.0178663730621, 'total_duration': 2987.639354944229, 'accumulated_submission_time': 1900.0178663730621, 'accumulated_eval_time': 1054.3911406993866, 'accumulated_logging_time': 2.619729518890381, 'global_step': 3820, 'preemption_count': 0}), (5079, {'train/ssim': 0.733149528503418, 'train/loss': 0.2756320408412388, 'validation/ssim': 0.7130767873786579, 'validation/loss': 0.29312956406953433, 'validation/num_examples': 3554, 'test/ssim': 0.7305879228043842, 'test/loss': 0.29478313549375174, 'test/num_examples': 3581, 'score': 1978.8389701843262, 'total_duration': 3074.677646636963, 'accumulated_submission_time': 1978.8389701843262, 'accumulated_eval_time': 1060.6946742534637, 'accumulated_logging_time': 2.63810396194458, 'global_step': 5079, 'preemption_count': 0}), (6337, {'train/ssim': 0.7367469242640904, 'train/loss': 0.27573817116873606, 'validation/ssim': 0.7173668341349536, 'validation/loss': 0.29275765147984667, 'validation/num_examples': 3554, 'test/ssim': 0.7347619707309411, 'test/loss': 0.2942155988747033, 'test/num_examples': 3581, 'score': 2057.709547519684, 'total_duration': 3161.6878023147583, 'accumulated_submission_time': 2057.709547519684, 'accumulated_eval_time': 1066.9823379516602, 'accumulated_logging_time': 2.6574103832244873, 'global_step': 6337, 'preemption_count': 0}), (7595, {'train/ssim': 0.7369610922677177, 'train/loss': 0.274746264730181, 'validation/ssim': 0.7170300931828574, 'validation/loss': 0.2920796700790483, 'validation/num_examples': 3554, 'test/ssim': 0.7342255567578888, 'test/loss': 0.29366970834569606, 'test/num_examples': 3581, 'score': 2136.5854620933533, 'total_duration': 3248.7025434970856, 'accumulated_submission_time': 2136.5854620933533, 'accumulated_eval_time': 1073.2312784194946, 'accumulated_logging_time': 2.676490545272827, 'global_step': 7595, 'preemption_count': 0}), (8852, {'train/ssim': 0.7400879178728376, 'train/loss': 0.2729393073490688, 'validation/ssim': 0.719876179348973, 'validation/loss': 0.29088448699968344, 'validation/num_examples': 3554, 'test/ssim': 0.737091703609327, 'test/loss': 0.292421768644408, 'test/num_examples': 3581, 'score': 2215.3724427223206, 'total_duration': 3335.684629678726, 'accumulated_submission_time': 2215.3724427223206, 'accumulated_eval_time': 1079.4908452033997, 'accumulated_logging_time': 2.6948652267456055, 'global_step': 8852, 'preemption_count': 0}), (10110, {'train/ssim': 0.7382880619594029, 'train/loss': 0.27315538270132883, 'validation/ssim': 0.7178178142146173, 'validation/loss': 0.29106223428926914, 'validation/num_examples': 3554, 'test/ssim': 0.7351747804166084, 'test/loss': 0.2924892635393396, 'test/num_examples': 3581, 'score': 2294.223144054413, 'total_duration': 3422.5454726219177, 'accumulated_submission_time': 2294.223144054413, 'accumulated_eval_time': 1085.7681047916412, 'accumulated_logging_time': 2.713733196258545, 'global_step': 10110, 'preemption_count': 0}), (11369, {'train/ssim': 0.7401157106672015, 'train/loss': 0.2714069059916905, 'validation/ssim': 0.7199538729468556, 'validation/loss': 0.2892316948113042, 'validation/num_examples': 3554, 'test/ssim': 0.7372600999633482, 'test/loss': 0.29071193206113866, 'test/num_examples': 3581, 'score': 2373.0823266506195, 'total_duration': 3509.5259273052216, 'accumulated_submission_time': 2373.0823266506195, 'accumulated_eval_time': 1091.9589080810547, 'accumulated_logging_time': 2.734273910522461, 'global_step': 11369, 'preemption_count': 0}), (12628, {'train/ssim': 0.7395613534109933, 'train/loss': 0.2722933122089931, 'validation/ssim': 0.7191644345499789, 'validation/loss': 0.2902645868871342, 'validation/num_examples': 3554, 'test/ssim': 0.736467955332833, 'test/loss': 0.29177170417655685, 'test/num_examples': 3581, 'score': 2451.9636318683624, 'total_duration': 3596.6715099811554, 'accumulated_submission_time': 2451.9636318683624, 'accumulated_eval_time': 1098.3841705322266, 'accumulated_logging_time': 2.753032684326172, 'global_step': 12628, 'preemption_count': 0}), (13887, {'train/ssim': 0.7403653008597237, 'train/loss': 0.2718982015337263, 'validation/ssim': 0.7200911934615926, 'validation/loss': 0.2897718061405283, 'validation/num_examples': 3554, 'test/ssim': 0.7373802272409942, 'test/loss': 0.2912192345997103, 'test/num_examples': 3581, 'score': 2530.7891280651093, 'total_duration': 3683.579870700836, 'accumulated_submission_time': 2530.7891280651093, 'accumulated_eval_time': 1104.5889275074005, 'accumulated_logging_time': 2.771928071975708, 'global_step': 13887, 'preemption_count': 0}), (15145, {'train/ssim': 0.7391048840114048, 'train/loss': 0.2720621313367571, 'validation/ssim': 0.7185385580068233, 'validation/loss': 0.2901387383713773, 'validation/num_examples': 3554, 'test/ssim': 0.7358353440903379, 'test/loss': 0.2916358621784243, 'test/num_examples': 3581, 'score': 2609.6232430934906, 'total_duration': 3770.482263326645, 'accumulated_submission_time': 2609.6232430934906, 'accumulated_eval_time': 1110.7782111167908, 'accumulated_logging_time': 2.790510654449463, 'global_step': 15145, 'preemption_count': 0}), (16403, {'train/ssim': 0.7399223872593471, 'train/loss': 0.2718940121786935, 'validation/ssim': 0.719499732915377, 'validation/loss': 0.28986540253939225, 'validation/num_examples': 3554, 'test/ssim': 0.7367403210957135, 'test/loss': 0.2913569855443661, 'test/num_examples': 3581, 'score': 2688.4423418045044, 'total_duration': 3857.314350128174, 'accumulated_submission_time': 2688.4423418045044, 'accumulated_eval_time': 1116.9619691371918, 'accumulated_logging_time': 2.810589075088501, 'global_step': 16403, 'preemption_count': 0}), (17663, {'train/ssim': 0.73988219669887, 'train/loss': 0.27192308221544537, 'validation/ssim': 0.719404590887908, 'validation/loss': 0.28993935228132034, 'validation/num_examples': 3554, 'test/ssim': 0.7366789621003211, 'test/loss': 0.29140760671556476, 'test/num_examples': 3581, 'score': 2767.3195033073425, 'total_duration': 3944.1099047660828, 'accumulated_submission_time': 2767.3195033073425, 'accumulated_eval_time': 1123.184329509735, 'accumulated_logging_time': 2.82999849319458, 'global_step': 17663, 'preemption_count': 0}), (18926, {'train/ssim': 0.740135942186628, 'train/loss': 0.2717715161187308, 'validation/ssim': 0.7196650808288196, 'validation/loss': 0.2897735578529474, 'validation/num_examples': 3554, 'test/ssim': 0.7369563047594946, 'test/loss': 0.2912311996038118, 'test/num_examples': 3581, 'score': 2846.3506274223328, 'total_duration': 4030.9481229782104, 'accumulated_submission_time': 2846.3506274223328, 'accumulated_eval_time': 1129.4235906600952, 'accumulated_logging_time': 2.8493332862854004, 'global_step': 18926, 'preemption_count': 0}), (20189, {'train/ssim': 0.7400953429085868, 'train/loss': 0.2717846802302769, 'validation/ssim': 0.7196287413829487, 'validation/loss': 0.28981508374147086, 'validation/num_examples': 3554, 'test/ssim': 0.7369218073687518, 'test/loss': 0.2912691740042935, 'test/num_examples': 3581, 'score': 2925.382669687271, 'total_duration': 4117.691866874695, 'accumulated_submission_time': 2925.382669687271, 'accumulated_eval_time': 1135.5926797389984, 'accumulated_logging_time': 2.868069648742676, 'global_step': 20189, 'preemption_count': 0}), (21450, {'train/ssim': 0.7408422742571149, 'train/loss': 0.27163309710366385, 'validation/ssim': 0.7205015063352912, 'validation/loss': 0.2895551777046989, 'validation/num_examples': 3554, 'test/ssim': 0.7377738111081052, 'test/loss': 0.2909836501457344, 'test/num_examples': 3581, 'score': 3004.4421796798706, 'total_duration': 4204.540153980255, 'accumulated_submission_time': 3004.4421796798706, 'accumulated_eval_time': 1141.8292672634125, 'accumulated_logging_time': 2.8871848583221436, 'global_step': 21450, 'preemption_count': 0}), (22713, {'train/ssim': 0.7403130531311035, 'train/loss': 0.27126799310956684, 'validation/ssim': 0.7198815375281373, 'validation/loss': 0.2892965768654157, 'validation/num_examples': 3554, 'test/ssim': 0.7371879690554315, 'test/loss': 0.29075300849972074, 'test/num_examples': 3581, 'score': 3083.4940004348755, 'total_duration': 4291.502067804337, 'accumulated_submission_time': 3083.4940004348755, 'accumulated_eval_time': 1148.0091652870178, 'accumulated_logging_time': 2.906733989715576, 'global_step': 22713, 'preemption_count': 0}), (23974, {'train/ssim': 0.7401350566319057, 'train/loss': 0.2710800681795393, 'validation/ssim': 0.7195230890809651, 'validation/loss': 0.28917302961891533, 'validation/num_examples': 3554, 'test/ssim': 0.7369552139329097, 'test/loss': 0.2906009745444708, 'test/num_examples': 3581, 'score': 3162.3393557071686, 'total_duration': 4378.296018362045, 'accumulated_submission_time': 3162.3393557071686, 'accumulated_eval_time': 1154.1733121871948, 'accumulated_logging_time': 2.9260504245758057, 'global_step': 23974, 'preemption_count': 0}), (25238, {'train/ssim': 0.7400530406406948, 'train/loss': 0.2712017297744751, 'validation/ssim': 0.7193604202571047, 'validation/loss': 0.28941782284265966, 'validation/num_examples': 3554, 'test/ssim': 0.7367063009416015, 'test/loss': 0.29093149499965093, 'test/num_examples': 3581, 'score': 3241.3589446544647, 'total_duration': 4465.290505170822, 'accumulated_submission_time': 3241.3589446544647, 'accumulated_eval_time': 1160.436423778534, 'accumulated_logging_time': 2.9453465938568115, 'global_step': 25238, 'preemption_count': 0}), (26498, {'train/ssim': 0.7395928246634347, 'train/loss': 0.272850615637643, 'validation/ssim': 0.7189854851048115, 'validation/loss': 0.2912730580310038, 'validation/num_examples': 3554, 'test/ssim': 0.736099324123848, 'test/loss': 0.2927947972480627, 'test/num_examples': 3581, 'score': 3320.170478105545, 'total_duration': 4552.119049072266, 'accumulated_submission_time': 3320.170478105545, 'accumulated_eval_time': 1166.690768480301, 'accumulated_logging_time': 2.9652259349823, 'global_step': 26498, 'preemption_count': 0}), (27761, {'train/ssim': 0.7404659816196987, 'train/loss': 0.2712956326348441, 'validation/ssim': 0.7201540490248663, 'validation/loss': 0.28919219541361846, 'validation/num_examples': 3554, 'test/ssim': 0.7374921051425929, 'test/loss': 0.2906358469068521, 'test/num_examples': 3581, 'score': 3399.188649892807, 'total_duration': 4639.119434833527, 'accumulated_submission_time': 3399.188649892807, 'accumulated_eval_time': 1173.0001981258392, 'accumulated_logging_time': 2.985692024230957, 'global_step': 27761, 'preemption_count': 0}), (29021, {'train/ssim': 0.7401444571358817, 'train/loss': 0.27202194077628, 'validation/ssim': 0.7198783088817178, 'validation/loss': 0.2898518697022721, 'validation/num_examples': 3554, 'test/ssim': 0.7371833330424462, 'test/loss': 0.29125690220521505, 'test/num_examples': 3581, 'score': 3478.00851893425, 'total_duration': 4726.243095636368, 'accumulated_submission_time': 3478.00851893425, 'accumulated_eval_time': 1179.396704673767, 'accumulated_logging_time': 3.0045039653778076, 'global_step': 29021, 'preemption_count': 0}), (30281, {'train/ssim': 0.7407717704772949, 'train/loss': 0.2711109774453299, 'validation/ssim': 0.7206109368405318, 'validation/loss': 0.2890636678082794, 'validation/num_examples': 3554, 'test/ssim': 0.7379377759791259, 'test/loss': 0.29052223050038395, 'test/num_examples': 3581, 'score': 3556.8466629981995, 'total_duration': 4812.850958824158, 'accumulated_submission_time': 3556.8466629981995, 'accumulated_eval_time': 1185.2578961849213, 'accumulated_logging_time': 3.0273828506469727, 'global_step': 30281, 'preemption_count': 0}), (31541, {'train/ssim': 0.7397125789097377, 'train/loss': 0.27212623187473844, 'validation/ssim': 0.7193740217888295, 'validation/loss': 0.29003425387767306, 'validation/num_examples': 3554, 'test/ssim': 0.7366589863384878, 'test/loss': 0.2915125646860165, 'test/num_examples': 3581, 'score': 3635.6410682201385, 'total_duration': 4899.956174850464, 'accumulated_submission_time': 3635.6410682201385, 'accumulated_eval_time': 1191.7770550251007, 'accumulated_logging_time': 3.050157308578491, 'global_step': 31541, 'preemption_count': 0}), (32804, {'train/ssim': 0.7418554169791085, 'train/loss': 0.26988162313188824, 'validation/ssim': 0.721446812790166, 'validation/loss': 0.28802124435605125, 'validation/num_examples': 3554, 'test/ssim': 0.7389320644111281, 'test/loss': 0.2894319152405927, 'test/num_examples': 3581, 'score': 3714.6501758098602, 'total_duration': 4986.894310474396, 'accumulated_submission_time': 3714.6501758098602, 'accumulated_eval_time': 1197.980364561081, 'accumulated_logging_time': 3.0693719387054443, 'global_step': 32804, 'preemption_count': 0}), (34067, {'train/ssim': 0.7407678876604352, 'train/loss': 0.27108287811279297, 'validation/ssim': 0.7208530853219963, 'validation/loss': 0.2887741887441967, 'validation/num_examples': 3554, 'test/ssim': 0.738248593379119, 'test/loss': 0.2901182496923869, 'test/num_examples': 3581, 'score': 3793.4982719421387, 'total_duration': 5073.766982555389, 'accumulated_submission_time': 3793.4982719421387, 'accumulated_eval_time': 1204.1712939739227, 'accumulated_logging_time': 3.0891032218933105, 'global_step': 34067, 'preemption_count': 0}), (35329, {'train/ssim': 0.7404485430036273, 'train/loss': 0.2722528321402414, 'validation/ssim': 0.7201015663468978, 'validation/loss': 0.29008529396894345, 'validation/num_examples': 3554, 'test/ssim': 0.7375035588217328, 'test/loss': 0.2914833850748743, 'test/num_examples': 3581, 'score': 3872.440690755844, 'total_duration': 5160.604091644287, 'accumulated_submission_time': 3872.440690755844, 'accumulated_eval_time': 1210.4021348953247, 'accumulated_logging_time': 3.1091811656951904, 'global_step': 35329, 'preemption_count': 0}), (36595, {'train/ssim': 0.7422323908124652, 'train/loss': 0.27098608016967773, 'validation/ssim': 0.7221174095209623, 'validation/loss': 0.28886373216138506, 'validation/num_examples': 3554, 'test/ssim': 0.739335942954133, 'test/loss': 0.2902255256693312, 'test/num_examples': 3581, 'score': 3951.5058710575104, 'total_duration': 5247.4869701862335, 'accumulated_submission_time': 3951.5058710575104, 'accumulated_eval_time': 1216.5791521072388, 'accumulated_logging_time': 3.1301071643829346, 'global_step': 36595, 'preemption_count': 0}), (37857, {'train/ssim': 0.7394915989467076, 'train/loss': 0.2727263314383371, 'validation/ssim': 0.7190967016697735, 'validation/loss': 0.2908805370599149, 'validation/num_examples': 3554, 'test/ssim': 0.7362885825363027, 'test/loss': 0.29230532290648564, 'test/num_examples': 3581, 'score': 4030.429238796234, 'total_duration': 5334.380391359329, 'accumulated_submission_time': 4030.429238796234, 'accumulated_eval_time': 1222.723665714264, 'accumulated_logging_time': 3.1515796184539795, 'global_step': 37857, 'preemption_count': 0}), (39119, {'train/ssim': 0.7407628468104771, 'train/loss': 0.2713686738695417, 'validation/ssim': 0.7204628312728616, 'validation/loss': 0.2891524555848164, 'validation/num_examples': 3554, 'test/ssim': 0.737906755598122, 'test/loss': 0.2905214464687762, 'test/num_examples': 3581, 'score': 4109.2731120586395, 'total_duration': 5421.324540138245, 'accumulated_submission_time': 4109.2731120586395, 'accumulated_eval_time': 1228.9450013637543, 'accumulated_logging_time': 3.1711127758026123, 'global_step': 39119, 'preemption_count': 0}), (40380, {'train/ssim': 0.7396753856113979, 'train/loss': 0.2711759465081351, 'validation/ssim': 0.7193080062737408, 'validation/loss': 0.28906655298167555, 'validation/num_examples': 3554, 'test/ssim': 0.736995983576515, 'test/loss': 0.290319473108943, 'test/num_examples': 3581, 'score': 4188.126078367233, 'total_duration': 5508.1718373298645, 'accumulated_submission_time': 4188.126078367233, 'accumulated_eval_time': 1235.1007196903229, 'accumulated_logging_time': 3.1906673908233643, 'global_step': 40380, 'preemption_count': 0}), (41643, {'train/ssim': 0.7431534358433315, 'train/loss': 0.27019127777644564, 'validation/ssim': 0.722939203076639, 'validation/loss': 0.28824881240767447, 'validation/num_examples': 3554, 'test/ssim': 0.7402101041084892, 'test/loss': 0.2896101631222075, 'test/num_examples': 3581, 'score': 4267.058005809784, 'total_duration': 5595.047441482544, 'accumulated_submission_time': 4267.058005809784, 'accumulated_eval_time': 1241.2552149295807, 'accumulated_logging_time': 3.211714506149292, 'global_step': 41643, 'preemption_count': 0}), (42906, {'train/ssim': 0.7432664462498256, 'train/loss': 0.2701637234006609, 'validation/ssim': 0.7229615975177617, 'validation/loss': 0.28816253198420794, 'validation/num_examples': 3554, 'test/ssim': 0.7403097102110094, 'test/loss': 0.2895042506784941, 'test/num_examples': 3581, 'score': 4345.948639631271, 'total_duration': 5681.8207404613495, 'accumulated_submission_time': 4345.948639631271, 'accumulated_eval_time': 1247.42800283432, 'accumulated_logging_time': 3.232004404067993, 'global_step': 42906, 'preemption_count': 0}), (44171, {'train/ssim': 0.7426543235778809, 'train/loss': 0.26990229742867605, 'validation/ssim': 0.7220100398538618, 'validation/loss': 0.2882151863986881, 'validation/num_examples': 3554, 'test/ssim': 0.739357009542551, 'test/loss': 0.2895896419470818, 'test/num_examples': 3581, 'score': 4425.003296375275, 'total_duration': 5768.563581228256, 'accumulated_submission_time': 4425.003296375275, 'accumulated_eval_time': 1253.6011283397675, 'accumulated_logging_time': 3.2529613971710205, 'global_step': 44171, 'preemption_count': 0})], 'global_step': 45433}
I0315 20:53:57.452262 140186397963456 submission_runner.py:649] Timing: 4504.068758487701
I0315 20:53:57.452296 140186397963456 submission_runner.py:651] Total number of evals: 53
I0315 20:53:57.452323 140186397963456 submission_runner.py:652] ====================
I0315 20:53:57.452463 140186397963456 submission_runner.py:750] Final fastmri score: 3
