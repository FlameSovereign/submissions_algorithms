torchrun --redirects 1:0,2:0,3:0,4:0,5:0,6:0,7:0 --standalone --nnodes=1 --nproc_per_node=8 submission_runner.py --framework=pytorch --workload=criteo1tb --submission_path=submissions_algorithms/leaderboard/external_tuning/shampoo_submission/submission.py --data_dir=/data/criteo1tb --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=submissions/rolling_leaderboard/external_tuning/shampoo/study_2 --overwrite=True --save_checkpoints=False --rng_seed=-640949678 --torch_compile=true --tuning_ruleset=external --tuning_search_space=submissions_algorithms/leaderboard/external_tuning/shampoo_submission/tuning_search_space.json --num_tuning_trials=5 --hparam_start_index=1 --hparam_end_index=2 2>&1 | tee -a /logs/criteo1tb_pytorch_03-16-2025-14-52-27.log
W0316 14:52:28.802000 9 site-packages/torch/distributed/run.py:793] 
W0316 14:52:28.802000 9 site-packages/torch/distributed/run.py:793] *****************************************
W0316 14:52:28.802000 9 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0316 14:52:28.802000 9 site-packages/torch/distributed/run.py:793] *****************************************
2025-03-16 14:52:29.893752: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-03-16 14:52:29.893752: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-03-16 14:52:29.893752: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-03-16 14:52:29.893752: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-03-16 14:52:29.893753: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-03-16 14:52:29.893753: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-03-16 14:52:29.893752: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-03-16 14:52:29.893752: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1742136749.916666      51 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1742136749.916667      48 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1742136749.916668      49 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1742136749.916667      50 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1742136749.916667      47 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1742136749.916667      46 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1742136749.916667      44 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1742136749.916672      45 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1742136749.923599      44 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
E0000 00:00:1742136749.923600      46 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
E0000 00:00:1742136749.923599      45 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
E0000 00:00:1742136749.923601      49 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
E0000 00:00:1742136749.923599      50 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
E0000 00:00:1742136749.923598      51 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
E0000 00:00:1742136749.923601      48 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
E0000 00:00:1742136749.923620      47 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
[rank2]:[W316 14:52:37.022863900 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank1]:[W316 14:52:37.025102925 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank4]:[W316 14:52:37.039140909 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 4]  using GPU 4 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank7]:[W316 14:52:37.089695470 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 7]  using GPU 7 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank0]:[W316 14:52:37.107876861 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank6]:[W316 14:52:37.112621354 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 6]  using GPU 6 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank3]:[W316 14:52:37.115399870 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank5]:[W316 14:52:37.115609631 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 5]  using GPU 5 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
I0316 14:52:38.915008 140693587801280 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_2/criteo1tb_pytorch.
I0316 14:52:38.915008 140547343606976 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_2/criteo1tb_pytorch.
I0316 14:52:38.915009 140344165901504 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_2/criteo1tb_pytorch.
I0316 14:52:38.915008 140610183144640 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_2/criteo1tb_pytorch.
I0316 14:52:38.915019 139868253091008 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_2/criteo1tb_pytorch.
I0316 14:52:38.915039 140456020972736 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_2/criteo1tb_pytorch.
I0316 14:52:38.915092 139745737082048 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_2/criteo1tb_pytorch.
I0316 14:52:38.915162 140292705682624 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_2/criteo1tb_pytorch.
I0316 14:52:40.094955 140456020972736 logger_utils.py:97] Saving hparams to /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_2/criteo1tb_pytorch/trial_2/hparams.json.
I0316 14:52:40.094964 140693587801280 logger_utils.py:97] Saving hparams to /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_2/criteo1tb_pytorch/trial_2/hparams.json.
I0316 14:52:40.094961 140344165901504 logger_utils.py:97] Saving hparams to /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_2/criteo1tb_pytorch/trial_2/hparams.json.
I0316 14:52:40.094979 140610183144640 logger_utils.py:97] Saving hparams to /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_2/criteo1tb_pytorch/trial_2/hparams.json.
I0316 14:52:40.095565 140547343606976 submission_runner.py:606] Using RNG seed -640949678
I0316 14:52:40.096701 140547343606976 submission_runner.py:615] --- Tuning run 2/5 ---
I0316 14:52:40.096820 140547343606976 submission_runner.py:620] Creating tuning directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_2/criteo1tb_pytorch/trial_2.
I0316 14:52:40.097037 140547343606976 logger_utils.py:97] Saving hparams to /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_2/criteo1tb_pytorch/trial_2/hparams.json.
I0316 14:52:40.097833 139868253091008 logger_utils.py:91] Loading hparams from /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_2/criteo1tb_pytorch/trial_2/hparams.json.
I0316 14:52:40.169425 140292705682624 logger_utils.py:91] Loading hparams from /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_2/criteo1tb_pytorch/trial_2/hparams.json.
I0316 14:52:40.170453 139745737082048 logger_utils.py:91] Loading hparams from /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_2/criteo1tb_pytorch/trial_2/hparams.json.
I0316 14:52:40.425707 140547343606976 submission_runner.py:218] Initializing dataset.
I0316 14:52:40.425891 140547343606976 submission_runner.py:229] Initializing model.
W0316 14:52:46.951922 140547343606976 submission_runner.py:254] These workloads cannot be fully compiled under current PyTorch version. Proceeding without `torch.compile`.
I0316 14:52:46.952135 140547343606976 submission_runner.py:272] Initializing optimizer.
W0316 14:52:46.953431 140547343606976 distributed_shampoo.py:341] start_preconditioning_step set to -1. Setting start_preconditioning_step equal to precondition frequency 100 by default.
W0316 14:52:46.953542 140547343606976 distributed_shampoo.py:359] Nesterov flag is enabled but momentum parameter is zero! Continuing without using momentum or Nesterov acceleration...
W0316 14:52:46.953563 139868253091008 submission_runner.py:254] These workloads cannot be fully compiled under current PyTorch version. Proceeding without `torch.compile`.
W0316 14:52:46.953672 140344165901504 submission_runner.py:254] These workloads cannot be fully compiled under current PyTorch version. Proceeding without `torch.compile`.
W0316 14:52:46.953735 140292705682624 submission_runner.py:254] These workloads cannot be fully compiled under current PyTorch version. Proceeding without `torch.compile`.
W0316 14:52:46.953863 139745737082048 submission_runner.py:254] These workloads cannot be fully compiled under current PyTorch version. Proceeding without `torch.compile`.
W0316 14:52:46.953905 140693587801280 submission_runner.py:254] These workloads cannot be fully compiled under current PyTorch version. Proceeding without `torch.compile`.
W0316 14:52:46.953981 140456020972736 submission_runner.py:254] These workloads cannot be fully compiled under current PyTorch version. Proceeding without `torch.compile`.
W0316 14:52:46.954077 140610183144640 submission_runner.py:254] These workloads cannot be fully compiled under current PyTorch version. Proceeding without `torch.compile`.
W0316 14:52:46.954723 139868253091008 distributed_shampoo.py:341] start_preconditioning_step set to -1. Setting start_preconditioning_step equal to precondition frequency 100 by default.
W0316 14:52:46.954838 139868253091008 distributed_shampoo.py:359] Nesterov flag is enabled but momentum parameter is zero! Continuing without using momentum or Nesterov acceleration...
W0316 14:52:46.954820 140344165901504 distributed_shampoo.py:341] start_preconditioning_step set to -1. Setting start_preconditioning_step equal to precondition frequency 100 by default.
W0316 14:52:46.954948 140344165901504 distributed_shampoo.py:359] Nesterov flag is enabled but momentum parameter is zero! Continuing without using momentum or Nesterov acceleration...
W0316 14:52:46.954974 140292705682624 distributed_shampoo.py:341] start_preconditioning_step set to -1. Setting start_preconditioning_step equal to precondition frequency 100 by default.
W0316 14:52:46.955125 140292705682624 distributed_shampoo.py:359] Nesterov flag is enabled but momentum parameter is zero! Continuing without using momentum or Nesterov acceleration...
W0316 14:52:46.955071 140693587801280 distributed_shampoo.py:341] start_preconditioning_step set to -1. Setting start_preconditioning_step equal to precondition frequency 100 by default.
W0316 14:52:46.955094 139745737082048 distributed_shampoo.py:341] start_preconditioning_step set to -1. Setting start_preconditioning_step equal to precondition frequency 100 by default.
W0316 14:52:46.955121 140456020972736 distributed_shampoo.py:341] start_preconditioning_step set to -1. Setting start_preconditioning_step equal to precondition frequency 100 by default.
W0316 14:52:46.955198 140693587801280 distributed_shampoo.py:359] Nesterov flag is enabled but momentum parameter is zero! Continuing without using momentum or Nesterov acceleration...
W0316 14:52:46.955240 140456020972736 distributed_shampoo.py:359] Nesterov flag is enabled but momentum parameter is zero! Continuing without using momentum or Nesterov acceleration...
W0316 14:52:46.955245 139745737082048 distributed_shampoo.py:359] Nesterov flag is enabled but momentum parameter is zero! Continuing without using momentum or Nesterov acceleration...
W0316 14:52:46.955213 140610183144640 distributed_shampoo.py:341] start_preconditioning_step set to -1. Setting start_preconditioning_step equal to precondition frequency 100 by default.
W0316 14:52:46.955335 140610183144640 distributed_shampoo.py:359] Nesterov flag is enabled but momentum parameter is zero! Continuing without using momentum or Nesterov acceleration...
I0316 14:52:46.956033 140547343606976 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 0.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 0 (torch.Size([512, 13])), Block block_0 (torch.Size([512, 13])).
I0316 14:52:46.956213 140547343606976 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 1.block_0 ([torch.Size([0])]) for Parameter 1 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 14:52:46.956381 140547343606976 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 2.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 2 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 14:52:46.956494 140547343606976 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 3.block_0 ([torch.Size([0])]) for Parameter 3 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 14:52:46.956647 140547343606976 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 4.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 4 (torch.Size([128, 256])), Block block_0 (torch.Size([128, 256])).
I0316 14:52:46.956752 140547343606976 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 5.block_0 ([torch.Size([0])]) for Parameter 5 (torch.Size([128])), Block block_0 (torch.Size([128])).
I0316 14:52:46.956904 140547343606976 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 6.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 6 (torch.Size([1024, 506])), Block block_0 (torch.Size([1024, 506])).
I0316 14:52:46.956891 139868253091008 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 0.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 0 (torch.Size([512, 13])), Block block_0 (torch.Size([512, 13])).
I0316 14:52:46.957024 140547343606976 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 7.block_0 ([torch.Size([0])]) for Parameter 7 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 14:52:46.957050 139868253091008 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 1.block_0 ([torch.Size([0])]) for Parameter 1 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 14:52:46.957211 139868253091008 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 2.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 2 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 14:52:46.957342 139868253091008 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 3.block_0 ([torch.Size([0])]) for Parameter 3 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 14:52:46.957463 139868253091008 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 4.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 4 (torch.Size([128, 256])), Block block_0 (torch.Size([128, 256])).
I0316 14:52:46.957563 139868253091008 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 5.block_0 ([torch.Size([0])]) for Parameter 5 (torch.Size([128])), Block block_0 (torch.Size([128])).
I0316 14:52:46.957811 140547343606976 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 8.block_0 ([torch.Size([1024, 1024]), torch.Size([1024, 1024])]) for Parameter 8 (torch.Size([1024, 1024])), Block block_0 (torch.Size([1024, 1024])).
I0316 14:52:46.957858 140344165901504 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 0.block_0 ([torch.Size([512, 512]), torch.Size([13, 13])]) for Parameter 0 (torch.Size([512, 13])), Block block_0 (torch.Size([512, 13])).
I0316 14:52:46.957958 140547343606976 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 9.block_0 ([torch.Size([0])]) for Parameter 9 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 14:52:46.957978 140292705682624 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 0.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 0 (torch.Size([512, 13])), Block block_0 (torch.Size([512, 13])).
I0316 14:52:46.958038 140344165901504 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 1.block_0 ([torch.Size([0])]) for Parameter 1 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 14:52:46.958041 140693587801280 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 0.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 0 (torch.Size([512, 13])), Block block_0 (torch.Size([512, 13])).
I0316 14:52:46.958116 140547343606976 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 10.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 10 (torch.Size([512, 1024])), Block block_0 (torch.Size([512, 1024])).
I0316 14:52:46.958109 139868253091008 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 6.block_0 ([torch.Size([1024, 1024]), torch.Size([506, 506])]) for Parameter 6 (torch.Size([1024, 506])), Block block_0 (torch.Size([1024, 506])).
I0316 14:52:46.958111 140456020972736 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 0.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 0 (torch.Size([512, 13])), Block block_0 (torch.Size([512, 13])).
I0316 14:52:46.958176 140344165901504 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 2.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 2 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 14:52:46.958146 140610183144640 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 0.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 0 (torch.Size([512, 13])), Block block_0 (torch.Size([512, 13])).
I0316 14:52:46.958184 140292705682624 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 1.block_0 ([torch.Size([0])]) for Parameter 1 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 14:52:46.958190 140693587801280 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 1.block_0 ([torch.Size([0])]) for Parameter 1 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 14:52:46.958233 140547343606976 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 11.block_0 ([torch.Size([0])]) for Parameter 11 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 14:52:46.958234 139868253091008 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 7.block_0 ([torch.Size([0])]) for Parameter 7 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 14:52:46.958278 140456020972736 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 1.block_0 ([torch.Size([0])]) for Parameter 1 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 14:52:46.958294 140344165901504 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 3.block_0 ([torch.Size([0])]) for Parameter 3 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 14:52:46.958303 140610183144640 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 1.block_0 ([torch.Size([0])]) for Parameter 1 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 14:52:46.958321 140292705682624 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 2.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 2 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 14:52:46.958345 139868253091008 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 8.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 8 (torch.Size([1024, 1024])), Block block_0 (torch.Size([1024, 1024])).
I0316 14:52:46.958337 140693587801280 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 2.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 2 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 14:52:46.958374 140547343606976 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 12.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 12 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 14:52:46.958355 139745737082048 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 0.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 0 (torch.Size([512, 13])), Block block_0 (torch.Size([512, 13])).
I0316 14:52:46.958414 140344165901504 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 4.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 4 (torch.Size([128, 256])), Block block_0 (torch.Size([128, 256])).
I0316 14:52:46.958421 140292705682624 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 3.block_0 ([torch.Size([0])]) for Parameter 3 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 14:52:46.958422 140456020972736 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 2.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 2 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 14:52:46.958439 139868253091008 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 9.block_0 ([torch.Size([0])]) for Parameter 9 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 14:52:46.958439 140693587801280 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 3.block_0 ([torch.Size([0])]) for Parameter 3 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 14:52:46.958478 140547343606976 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 13.block_0 ([torch.Size([0])]) for Parameter 13 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 14:52:46.958505 140344165901504 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 5.block_0 ([torch.Size([0])]) for Parameter 5 (torch.Size([128])), Block block_0 (torch.Size([128])).
I0316 14:52:46.958522 140456020972736 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 3.block_0 ([torch.Size([0])]) for Parameter 3 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 14:52:46.958542 139868253091008 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 10.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 10 (torch.Size([512, 1024])), Block block_0 (torch.Size([512, 1024])).
I0316 14:52:46.958556 140693587801280 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 4.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 4 (torch.Size([128, 256])), Block block_0 (torch.Size([128, 256])).
I0316 14:52:46.958575 140547343606976 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 14.block_0 ([torch.Size([0])]) for Parameter 14 (torch.Size([1, 256])), Block block_0 (torch.Size([256])).
I0316 14:52:46.958619 139868253091008 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 11.block_0 ([torch.Size([0])]) for Parameter 11 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 14:52:46.958625 140344165901504 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 6.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 6 (torch.Size([1024, 506])), Block block_0 (torch.Size([1024, 506])).
I0316 14:52:46.958636 140456020972736 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 4.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 4 (torch.Size([128, 256])), Block block_0 (torch.Size([128, 256])).
I0316 14:52:46.958631 140292705682624 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 4.block_0 ([torch.Size([128, 128]), torch.Size([256, 256])]) for Parameter 4 (torch.Size([128, 256])), Block block_0 (torch.Size([128, 256])).
I0316 14:52:46.958661 140693587801280 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 5.block_0 ([torch.Size([0])]) for Parameter 5 (torch.Size([128])), Block block_0 (torch.Size([128])).
I0316 14:52:46.958668 140547343606976 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 15.block_0 ([torch.Size([0])]) for Parameter 15 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0316 14:52:46.958713 140344165901504 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 7.block_0 ([torch.Size([0])]) for Parameter 7 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 14:52:46.958731 139868253091008 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 12.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 12 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 14:52:46.958732 140456020972736 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 5.block_0 ([torch.Size([0])]) for Parameter 5 (torch.Size([128])), Block block_0 (torch.Size([128])).
I0316 14:52:46.958740 140292705682624 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 5.block_0 ([torch.Size([0])]) for Parameter 5 (torch.Size([128])), Block block_0 (torch.Size([128])).
I0316 14:52:46.958758 140547343606976 shampoo_preconditioner_list.py:612] Rank 0: ShampooPreconditionerList Numel Breakdown: (524626, 524288, 655360, 131072, 163840, 32768, 2609224, 2097152, 4194304, 2097152, 2621440, 524288, 655360, 131072, 131072, 2)
I0316 14:52:46.958784 140693587801280 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 6.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 6 (torch.Size([1024, 506])), Block block_0 (torch.Size([1024, 506])).
I0316 14:52:46.958794 140547343606976 shampoo_preconditioner_list.py:615] Rank 0: ShampooPreconditionerList Bytes Breakdown: (2098504,)
I0316 14:52:46.958812 139868253091008 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 13.block_0 ([torch.Size([0])]) for Parameter 13 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 14:52:46.958826 140547343606976 shampoo_preconditioner_list.py:618] Rank 0: ShampooPreconditionerList Total Elements: 17093020
I0316 14:52:46.958829 140344165901504 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 8.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 8 (torch.Size([1024, 1024])), Block block_0 (torch.Size([1024, 1024])).
I0316 14:52:46.958839 140456020972736 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 6.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 6 (torch.Size([1024, 506])), Block block_0 (torch.Size([1024, 506])).
I0316 14:52:46.958855 140547343606976 shampoo_preconditioner_list.py:621] Rank 0: ShampooPreconditionerList Total Bytes: 2098504
I0316 14:52:46.958860 140292705682624 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 6.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 6 (torch.Size([1024, 506])), Block block_0 (torch.Size([1024, 506])).
I0316 14:52:46.958884 140693587801280 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 7.block_0 ([torch.Size([0])]) for Parameter 7 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 14:52:46.958895 139868253091008 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 14.block_0 ([torch.Size([0])]) for Parameter 14 (torch.Size([1, 256])), Block block_0 (torch.Size([256])).
I0316 14:52:46.958909 140344165901504 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 9.block_0 ([torch.Size([0])]) for Parameter 9 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 14:52:46.958931 140456020972736 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 7.block_0 ([torch.Size([0])]) for Parameter 7 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 14:52:46.958947 140292705682624 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 7.block_0 ([torch.Size([0])]) for Parameter 7 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 14:52:46.958970 139868253091008 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 15.block_0 ([torch.Size([0])]) for Parameter 15 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0316 14:52:46.958983 140547343606976 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([512, 13])), Block block_0 (torch.Size([512, 13])).
I0316 14:52:46.958995 140693587801280 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 8.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 8 (torch.Size([1024, 1024])), Block block_0 (torch.Size([1024, 1024])).
I0316 14:52:46.958973 140610183144640 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 2.block_0 ([torch.Size([256, 256]), torch.Size([512, 512])]) for Parameter 2 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 14:52:46.959019 140344165901504 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 10.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 10 (torch.Size([512, 1024])), Block block_0 (torch.Size([512, 1024])).
I0316 14:52:46.959043 140456020972736 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 8.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 8 (torch.Size([1024, 1024])), Block block_0 (torch.Size([1024, 1024])).
I0316 14:52:46.959043 140547343606976 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 14:52:46.959061 139868253091008 shampoo_preconditioner_list.py:612] Rank 2: ShampooPreconditionerList Numel Breakdown: (524626, 524288, 655360, 131072, 163840, 32768, 2609224, 2097152, 4194304, 2097152, 2621440, 524288, 655360, 131072, 131072, 2)
I0316 14:52:46.959084 140693587801280 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 9.block_0 ([torch.Size([0])]) for Parameter 9 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 14:52:46.959083 140292705682624 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 8.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 8 (torch.Size([1024, 1024])), Block block_0 (torch.Size([1024, 1024])).
I0316 14:52:46.959100 139868253091008 shampoo_preconditioner_list.py:615] Rank 2: ShampooPreconditionerList Bytes Breakdown: (2098504,)
I0316 14:52:46.959105 140344165901504 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 11.block_0 ([torch.Size([0])]) for Parameter 11 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 14:52:46.959097 140547343606976 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 14:52:46.959122 140456020972736 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 9.block_0 ([torch.Size([0])]) for Parameter 9 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 14:52:46.959116 140610183144640 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 3.block_0 ([torch.Size([0])]) for Parameter 3 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 14:52:46.959130 139868253091008 shampoo_preconditioner_list.py:618] Rank 2: ShampooPreconditionerList Total Elements: 17093020
I0316 14:52:46.959095 139745737082048 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 1.block_0 ([torch.Size([512, 512])]) for Parameter 1 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 14:52:46.959156 139868253091008 shampoo_preconditioner_list.py:621] Rank 2: ShampooPreconditionerList Total Bytes: 2098504
I0316 14:52:46.959160 140547343606976 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 14:52:46.959179 140292705682624 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 9.block_0 ([torch.Size([0])]) for Parameter 9 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 14:52:46.959221 140547343606976 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 4.block_0 (torch.Size([0])) for Parameter 4 (torch.Size([128, 256])), Block block_0 (torch.Size([128, 256])).
I0316 14:52:46.959225 140344165901504 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 12.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 12 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 14:52:46.959230 140610183144640 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 4.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 4 (torch.Size([128, 256])), Block block_0 (torch.Size([128, 256])).
I0316 14:52:46.959233 140456020972736 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 10.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 10 (torch.Size([512, 1024])), Block block_0 (torch.Size([512, 1024])).
I0316 14:52:46.959286 140547343606976 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 5.block_0 (torch.Size([0])) for Parameter 5 (torch.Size([128])), Block block_0 (torch.Size([128])).
I0316 14:52:46.959282 139868253091008 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([512, 13])), Block block_0 (torch.Size([512, 13])).
I0316 14:52:46.959292 140292705682624 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 10.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 10 (torch.Size([512, 1024])), Block block_0 (torch.Size([512, 1024])).
I0316 14:52:46.959303 140344165901504 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 13.block_0 ([torch.Size([0])]) for Parameter 13 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 14:52:46.959313 140456020972736 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 11.block_0 ([torch.Size([0])]) for Parameter 11 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 14:52:46.959298 139745737082048 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 2.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 2 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 14:52:46.959327 140610183144640 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 5.block_0 ([torch.Size([0])]) for Parameter 5 (torch.Size([128])), Block block_0 (torch.Size([128])).
I0316 14:52:46.959337 139868253091008 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 14:52:46.959346 140547343606976 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 6.block_0 (torch.Size([0])) for Parameter 6 (torch.Size([1024, 506])), Block block_0 (torch.Size([1024, 506])).
I0316 14:52:46.959367 140292705682624 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 11.block_0 ([torch.Size([0])]) for Parameter 11 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 14:52:46.959379 140344165901504 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 14.block_0 ([torch.Size([0])]) for Parameter 14 (torch.Size([1, 256])), Block block_0 (torch.Size([256])).
I0316 14:52:46.959382 139868253091008 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 14:52:46.959402 140547343606976 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 7.block_0 (torch.Size([0])) for Parameter 7 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 14:52:46.959426 139868253091008 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 14:52:46.959450 140344165901504 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 15.block_0 ([torch.Size([0])]) for Parameter 15 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0316 14:52:46.959453 140610183144640 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 6.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 6 (torch.Size([1024, 506])), Block block_0 (torch.Size([1024, 506])).
I0316 14:52:46.959475 140292705682624 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 12.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 12 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 14:52:46.959466 139745737082048 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 3.block_0 ([torch.Size([256, 256])]) for Parameter 3 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 14:52:46.959479 139868253091008 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 4.block_0 (torch.Size([0])) for Parameter 4 (torch.Size([128, 256])), Block block_0 (torch.Size([128, 256])).
I0316 14:52:46.959509 140547343606976 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 8.block_0 (torch.Size([1024, 1024])) for Parameter 8 (torch.Size([1024, 1024])), Block block_0 (torch.Size([1024, 1024])).
I0316 14:52:46.959526 139868253091008 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 5.block_0 (torch.Size([0])) for Parameter 5 (torch.Size([128])), Block block_0 (torch.Size([128])).
I0316 14:52:46.959542 140344165901504 shampoo_preconditioner_list.py:612] Rank 6: ShampooPreconditionerList Numel Breakdown: (524626, 524288, 655360, 131072, 163840, 32768, 2609224, 2097152, 4194304, 2097152, 2621440, 524288, 655360, 131072, 131072, 2)
I0316 14:52:46.959562 140292705682624 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 13.block_0 ([torch.Size([0])]) for Parameter 13 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 14:52:46.959560 140610183144640 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 7.block_0 ([torch.Size([0])]) for Parameter 7 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 14:52:46.959582 140344165901504 shampoo_preconditioner_list.py:615] Rank 6: ShampooPreconditionerList Bytes Breakdown: (2098504,)
I0316 14:52:46.959587 140547343606976 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 9.block_0 (torch.Size([0])) for Parameter 9 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 14:52:46.959611 140344165901504 shampoo_preconditioner_list.py:618] Rank 6: ShampooPreconditionerList Total Elements: 17093020
I0316 14:52:46.959605 139745737082048 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 4.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 4 (torch.Size([128, 256])), Block block_0 (torch.Size([128, 256])).
I0316 14:52:46.959612 139868253091008 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 6.block_0 (torch.Size([1024, 506])) for Parameter 6 (torch.Size([1024, 506])), Block block_0 (torch.Size([1024, 506])).
I0316 14:52:46.959641 140344165901504 shampoo_preconditioner_list.py:621] Rank 6: ShampooPreconditionerList Total Bytes: 2098504
I0316 14:52:46.959646 140547343606976 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 10.block_0 (torch.Size([0])) for Parameter 10 (torch.Size([512, 1024])), Block block_0 (torch.Size([512, 1024])).
I0316 14:52:46.959646 140292705682624 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 14.block_0 ([torch.Size([0])]) for Parameter 14 (torch.Size([1, 256])), Block block_0 (torch.Size([256])).
I0316 14:52:46.959646 140693587801280 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 10.block_0 ([torch.Size([512, 512]), torch.Size([1024, 1024])]) for Parameter 10 (torch.Size([512, 1024])), Block block_0 (torch.Size([512, 1024])).
I0316 14:52:46.959670 140610183144640 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 8.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 8 (torch.Size([1024, 1024])), Block block_0 (torch.Size([1024, 1024])).
I0316 14:52:46.959677 139868253091008 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 7.block_0 (torch.Size([0])) for Parameter 7 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 14:52:46.959697 140547343606976 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 11.block_0 (torch.Size([0])) for Parameter 11 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 14:52:46.959723 139868253091008 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 8.block_0 (torch.Size([0])) for Parameter 8 (torch.Size([1024, 1024])), Block block_0 (torch.Size([1024, 1024])).
I0316 14:52:46.959723 140292705682624 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 15.block_0 ([torch.Size([0])]) for Parameter 15 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0316 14:52:46.959748 140610183144640 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 9.block_0 ([torch.Size([0])]) for Parameter 9 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 14:52:46.959751 139745737082048 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 5.block_0 ([torch.Size([128, 128])]) for Parameter 5 (torch.Size([128])), Block block_0 (torch.Size([128])).
I0316 14:52:46.959761 140547343606976 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 12.block_0 (torch.Size([0])) for Parameter 12 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 14:52:46.959770 140693587801280 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 11.block_0 ([torch.Size([0])]) for Parameter 11 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 14:52:46.959778 139868253091008 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 9.block_0 (torch.Size([0])) for Parameter 9 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 14:52:46.959807 140292705682624 shampoo_preconditioner_list.py:612] Rank 5: ShampooPreconditionerList Numel Breakdown: (524626, 524288, 655360, 131072, 163840, 32768, 2609224, 2097152, 4194304, 2097152, 2621440, 524288, 655360, 131072, 131072, 2)
I0316 14:52:46.959831 140547343606976 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 13.block_0 (torch.Size([0])) for Parameter 13 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 14:52:46.959841 139868253091008 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 10.block_0 (torch.Size([0])) for Parameter 10 (torch.Size([512, 1024])), Block block_0 (torch.Size([512, 1024])).
I0316 14:52:46.959829 140344165901504 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([512, 13])) for Parameter 0 (torch.Size([512, 13])), Block block_0 (torch.Size([512, 13])).
I0316 14:52:46.959847 140292705682624 shampoo_preconditioner_list.py:615] Rank 5: ShampooPreconditionerList Bytes Breakdown: (2098504,)
I0316 14:52:46.959876 140292705682624 shampoo_preconditioner_list.py:618] Rank 5: ShampooPreconditionerList Total Elements: 17093020
I0316 14:52:46.959860 140456020972736 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 12.block_0 ([torch.Size([256, 256]), torch.Size([512, 512])]) for Parameter 12 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 14:52:46.959887 139868253091008 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 11.block_0 (torch.Size([0])) for Parameter 11 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 14:52:46.959885 140610183144640 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 10.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 10 (torch.Size([512, 1024])), Block block_0 (torch.Size([512, 1024])).
I0316 14:52:46.959886 139745737082048 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 6.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 6 (torch.Size([1024, 506])), Block block_0 (torch.Size([1024, 506])).
I0316 14:52:46.959891 140547343606976 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 14.block_0 (torch.Size([0])) for Parameter 14 (torch.Size([1, 256])), Block block_0 (torch.Size([256])).
I0316 14:52:46.959903 140344165901504 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 14:52:46.959908 140292705682624 shampoo_preconditioner_list.py:621] Rank 5: ShampooPreconditionerList Total Bytes: 2098504
I0316 14:52:46.959900 140693587801280 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 12.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 12 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 14:52:46.959932 139868253091008 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 12.block_0 (torch.Size([0])) for Parameter 12 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 14:52:46.959944 140547343606976 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 15.block_0 (torch.Size([0])) for Parameter 15 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0316 14:52:46.959963 140344165901504 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 14:52:46.959968 140456020972736 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 13.block_0 ([torch.Size([0])]) for Parameter 13 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 14:52:46.959970 140610183144640 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 11.block_0 ([torch.Size([0])]) for Parameter 11 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 14:52:46.959983 139868253091008 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 13.block_0 (torch.Size([0])) for Parameter 13 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 14:52:46.959988 140693587801280 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 13.block_0 ([torch.Size([0])]) for Parameter 13 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 14:52:46.960004 140547343606976 shampoo_preconditioner_list.py:375] Rank 0: AdaGradPreconditionerList Numel Breakdown: (0, 0, 0, 0, 0, 0, 0, 0, 1048576, 0, 0, 0, 0, 0, 0, 0)
I0316 14:52:46.960018 140344165901504 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 14:52:46.960028 139868253091008 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 14.block_0 (torch.Size([0])) for Parameter 14 (torch.Size([1, 256])), Block block_0 (torch.Size([256])).
I0316 14:52:46.960035 140292705682624 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([512, 13])), Block block_0 (torch.Size([512, 13])).
I0316 14:52:46.960029 139745737082048 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 7.block_0 ([torch.Size([1024, 1024])]) for Parameter 7 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 14:52:46.960046 140547343606976 shampoo_preconditioner_list.py:378] Rank 0: AdaGradPreconditionerList Bytes Breakdown: (0, 0, 0, 0, 0, 0, 0, 0, 4194304, 0, 0, 0, 0, 0, 0, 0)
I0316 14:52:46.960053 140456020972736 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 14.block_0 ([torch.Size([0])]) for Parameter 14 (torch.Size([1, 256])), Block block_0 (torch.Size([256])).
I0316 14:52:46.960067 140693587801280 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 14.block_0 ([torch.Size([0])]) for Parameter 14 (torch.Size([1, 256])), Block block_0 (torch.Size([256])).
I0316 14:52:46.960072 139868253091008 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 15.block_0 (torch.Size([0])) for Parameter 15 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0316 14:52:46.960073 140344165901504 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 4.block_0 (torch.Size([0])) for Parameter 4 (torch.Size([128, 256])), Block block_0 (torch.Size([128, 256])).
I0316 14:52:46.960078 140547343606976 shampoo_preconditioner_list.py:381] Rank 0: AdaGradPreconditionerList Total Elements: 1048576
I0316 14:52:46.960080 140610183144640 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 12.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 12 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 14:52:46.960108 140547343606976 shampoo_preconditioner_list.py:384] Rank 0: AdaGradPreconditionerList Total Bytes: 4194304
I0316 14:52:46.960119 140344165901504 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 5.block_0 (torch.Size([0])) for Parameter 5 (torch.Size([128])), Block block_0 (torch.Size([128])).
I0316 14:52:46.960105 140292705682624 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 14:52:46.960128 139868253091008 shampoo_preconditioner_list.py:375] Rank 2: AdaGradPreconditionerList Numel Breakdown: (0, 0, 0, 0, 0, 0, 518144, 0, 0, 0, 0, 0, 0, 0, 0, 0)
I0316 14:52:46.960133 140456020972736 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 15.block_0 ([torch.Size([0])]) for Parameter 15 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0316 14:52:46.960158 140610183144640 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 13.block_0 ([torch.Size([0])]) for Parameter 13 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 14:52:46.960166 139868253091008 shampoo_preconditioner_list.py:378] Rank 2: AdaGradPreconditionerList Bytes Breakdown: (0, 0, 0, 0, 0, 0, 2072576, 0, 0, 0, 0, 0, 0, 0, 0, 0)
I0316 14:52:46.960166 140344165901504 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 6.block_0 (torch.Size([0])) for Parameter 6 (torch.Size([1024, 506])), Block block_0 (torch.Size([1024, 506])).
I0316 14:52:46.960163 140693587801280 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 15.block_0 ([torch.Size([0])]) for Parameter 15 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0316 14:52:46.960174 140292705682624 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 14:52:46.960183 139745737082048 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 8.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 8 (torch.Size([1024, 1024])), Block block_0 (torch.Size([1024, 1024])).
I0316 14:52:46.960194 139868253091008 shampoo_preconditioner_list.py:381] Rank 2: AdaGradPreconditionerList Total Elements: 518144
I0316 14:52:46.960222 140292705682624 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 14:52:46.960222 139868253091008 shampoo_preconditioner_list.py:384] Rank 2: AdaGradPreconditionerList Total Bytes: 2072576
I0316 14:52:46.960229 140344165901504 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 7.block_0 (torch.Size([0])) for Parameter 7 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 14:52:46.960219 140456020972736 shampoo_preconditioner_list.py:612] Rank 4: ShampooPreconditionerList Numel Breakdown: (524626, 524288, 655360, 131072, 163840, 32768, 2609224, 2097152, 4194304, 2097152, 2621440, 524288, 655360, 131072, 131072, 2)
I0316 14:52:46.960250 140610183144640 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 14.block_0 ([torch.Size([0])]) for Parameter 14 (torch.Size([1, 256])), Block block_0 (torch.Size([256])).
I0316 14:52:46.960265 140456020972736 shampoo_preconditioner_list.py:615] Rank 4: ShampooPreconditionerList Bytes Breakdown: (2098504,)
I0316 14:52:46.960270 140693587801280 shampoo_preconditioner_list.py:612] Rank 1: ShampooPreconditionerList Numel Breakdown: (524626, 524288, 655360, 131072, 163840, 32768, 2609224, 2097152, 4194304, 2097152, 2621440, 524288, 655360, 131072, 131072, 2)
I0316 14:52:46.960279 140344165901504 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 8.block_0 (torch.Size([0])) for Parameter 8 (torch.Size([1024, 1024])), Block block_0 (torch.Size([1024, 1024])).
I0316 14:52:46.960295 140456020972736 shampoo_preconditioner_list.py:618] Rank 4: ShampooPreconditionerList Total Elements: 17093020
I0316 14:52:46.960309 140693587801280 shampoo_preconditioner_list.py:615] Rank 1: ShampooPreconditionerList Bytes Breakdown: (2098504,)
I0316 14:52:46.960326 140344165901504 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 9.block_0 (torch.Size([0])) for Parameter 9 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 14:52:46.960322 140292705682624 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 4.block_0 (torch.Size([128, 256])) for Parameter 4 (torch.Size([128, 256])), Block block_0 (torch.Size([128, 256])).
I0316 14:52:46.960333 140456020972736 shampoo_preconditioner_list.py:621] Rank 4: ShampooPreconditionerList Total Bytes: 2098504
I0316 14:52:46.960340 140693587801280 shampoo_preconditioner_list.py:618] Rank 1: ShampooPreconditionerList Total Elements: 17093020
I0316 14:52:46.960347 140610183144640 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 15.block_0 ([torch.Size([0])]) for Parameter 15 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0316 14:52:46.960368 140693587801280 shampoo_preconditioner_list.py:621] Rank 1: ShampooPreconditionerList Total Bytes: 2098504
I0316 14:52:46.960378 140344165901504 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 10.block_0 (torch.Size([0])) for Parameter 10 (torch.Size([512, 1024])), Block block_0 (torch.Size([512, 1024])).
I0316 14:52:46.960385 140292705682624 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 5.block_0 (torch.Size([0])) for Parameter 5 (torch.Size([128])), Block block_0 (torch.Size([128])).
I0316 14:52:46.960425 140344165901504 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 11.block_0 (torch.Size([0])) for Parameter 11 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 14:52:46.960434 140292705682624 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 6.block_0 (torch.Size([0])) for Parameter 6 (torch.Size([1024, 506])), Block block_0 (torch.Size([1024, 506])).
I0316 14:52:46.960434 140610183144640 shampoo_preconditioner_list.py:612] Rank 3: ShampooPreconditionerList Numel Breakdown: (524626, 524288, 655360, 131072, 163840, 32768, 2609224, 2097152, 4194304, 2097152, 2621440, 524288, 655360, 131072, 131072, 2)
I0316 14:52:46.960459 140456020972736 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([512, 13])), Block block_0 (torch.Size([512, 13])).
I0316 14:52:46.960470 140610183144640 shampoo_preconditioner_list.py:615] Rank 3: ShampooPreconditionerList Bytes Breakdown: (2098504,)
I0316 14:52:46.960476 140344165901504 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 12.block_0 (torch.Size([0])) for Parameter 12 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 14:52:46.960489 140292705682624 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 7.block_0 (torch.Size([0])) for Parameter 7 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 14:52:46.960500 140610183144640 shampoo_preconditioner_list.py:618] Rank 3: ShampooPreconditionerList Total Elements: 17093020
I0316 14:52:46.960521 140344165901504 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 13.block_0 (torch.Size([0])) for Parameter 13 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 14:52:46.960514 140693587801280 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([512, 13])), Block block_0 (torch.Size([512, 13])).
I0316 14:52:46.960522 140456020972736 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 14:52:46.960529 140610183144640 shampoo_preconditioner_list.py:621] Rank 3: ShampooPreconditionerList Total Bytes: 2098504
I0316 14:52:46.960548 140292705682624 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 8.block_0 (torch.Size([0])) for Parameter 8 (torch.Size([1024, 1024])), Block block_0 (torch.Size([1024, 1024])).
I0316 14:52:46.960533 140547343606976 submission.py:105] Large parameters (embedding tables) detected (dim >= 1_000_000). Instantiating Row-Wise AdaGrad...
I0316 14:52:46.960565 140344165901504 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 14.block_0 (torch.Size([0])) for Parameter 14 (torch.Size([1, 256])), Block block_0 (torch.Size([256])).
I0316 14:52:46.960572 140456020972736 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 14:52:46.960573 140693587801280 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([512])), Block block_0 (torch.Size([512])).
W0316 14:52:46.960572 139868253091008 distributed_shampoo.py:359] Nesterov flag is enabled but momentum parameter is zero! Continuing without using momentum or Nesterov acceleration...
I0316 14:52:46.960596 140292705682624 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 9.block_0 (torch.Size([0])) for Parameter 9 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 14:52:46.960609 140344165901504 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 15.block_0 (torch.Size([0])) for Parameter 15 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0316 14:52:46.960619 140456020972736 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 14:52:46.960622 140693587801280 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
W0316 14:52:46.960624 140547343606976 distributed_shampoo.py:359] Nesterov flag is enabled but momentum parameter is zero! Continuing without using momentum or Nesterov acceleration...
I0316 14:52:46.960641 140292705682624 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 10.block_0 (torch.Size([0])) for Parameter 10 (torch.Size([512, 1024])), Block block_0 (torch.Size([512, 1024])).
I0316 14:52:46.960655 140610183144640 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([512, 13])), Block block_0 (torch.Size([512, 13])).
I0316 14:52:46.960667 140344165901504 shampoo_preconditioner_list.py:375] Rank 6: AdaGradPreconditionerList Numel Breakdown: (6656, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)
I0316 14:52:46.960687 140292705682624 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 11.block_0 (torch.Size([0])) for Parameter 11 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 14:52:46.960676 140456020972736 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 4.block_0 (torch.Size([0])) for Parameter 4 (torch.Size([128, 256])), Block block_0 (torch.Size([128, 256])).
I0316 14:52:46.960675 140693587801280 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 14:52:46.960719 140344165901504 shampoo_preconditioner_list.py:378] Rank 6: AdaGradPreconditionerList Bytes Breakdown: (26624, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)
I0316 14:52:46.960698 139745737082048 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 9.block_0 ([torch.Size([1024, 1024])]) for Parameter 9 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 14:52:46.960733 140292705682624 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 12.block_0 (torch.Size([0])) for Parameter 12 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 14:52:46.960743 140456020972736 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 5.block_0 (torch.Size([0])) for Parameter 5 (torch.Size([128])), Block block_0 (torch.Size([128])).
I0316 14:52:46.960745 140693587801280 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 4.block_0 (torch.Size([0])) for Parameter 4 (torch.Size([128, 256])), Block block_0 (torch.Size([128, 256])).
I0316 14:52:46.960747 140610183144640 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 14:52:46.960757 140344165901504 shampoo_preconditioner_list.py:381] Rank 6: AdaGradPreconditionerList Total Elements: 6656
I0316 14:52:46.960777 140292705682624 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 13.block_0 (torch.Size([0])) for Parameter 13 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 14:52:46.960788 140344165901504 shampoo_preconditioner_list.py:384] Rank 6: AdaGradPreconditionerList Total Bytes: 26624
I0316 14:52:46.960796 140693587801280 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 5.block_0 (torch.Size([0])) for Parameter 5 (torch.Size([128])), Block block_0 (torch.Size([128])).
I0316 14:52:46.960802 140456020972736 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 6.block_0 (torch.Size([0])) for Parameter 6 (torch.Size([1024, 506])), Block block_0 (torch.Size([1024, 506])).
I0316 14:52:46.960830 140292705682624 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 14.block_0 (torch.Size([0])) for Parameter 14 (torch.Size([1, 256])), Block block_0 (torch.Size([256])).
I0316 14:52:46.960847 140693587801280 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 6.block_0 (torch.Size([0])) for Parameter 6 (torch.Size([1024, 506])), Block block_0 (torch.Size([1024, 506])).
I0316 14:52:46.960848 139745737082048 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 10.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 10 (torch.Size([512, 1024])), Block block_0 (torch.Size([512, 1024])).
I0316 14:52:46.960861 140456020972736 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 7.block_0 (torch.Size([0])) for Parameter 7 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 14:52:46.960883 140292705682624 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 15.block_0 (torch.Size([0])) for Parameter 15 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0316 14:52:46.960905 140693587801280 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 7.block_0 (torch.Size([0])) for Parameter 7 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 14:52:46.960921 140456020972736 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 8.block_0 (torch.Size([0])) for Parameter 8 (torch.Size([1024, 1024])), Block block_0 (torch.Size([1024, 1024])).
I0316 14:52:46.960947 140292705682624 shampoo_preconditioner_list.py:375] Rank 5: AdaGradPreconditionerList Numel Breakdown: (0, 0, 0, 0, 32768, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)
I0316 14:52:46.960968 140693587801280 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 8.block_0 (torch.Size([0])) for Parameter 8 (torch.Size([1024, 1024])), Block block_0 (torch.Size([1024, 1024])).
I0316 14:52:46.960972 140456020972736 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 9.block_0 (torch.Size([0])) for Parameter 9 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 14:52:46.960986 140292705682624 shampoo_preconditioner_list.py:378] Rank 5: AdaGradPreconditionerList Bytes Breakdown: (0, 0, 0, 0, 131072, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)
I0316 14:52:46.961015 140292705682624 shampoo_preconditioner_list.py:381] Rank 5: AdaGradPreconditionerList Total Elements: 32768
I0316 14:52:46.961021 140456020972736 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 10.block_0 (torch.Size([0])) for Parameter 10 (torch.Size([512, 1024])), Block block_0 (torch.Size([512, 1024])).
I0316 14:52:46.961027 140693587801280 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 9.block_0 (torch.Size([0])) for Parameter 9 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 14:52:46.961048 140292705682624 shampoo_preconditioner_list.py:384] Rank 5: AdaGradPreconditionerList Total Bytes: 131072
I0316 14:52:46.961071 140456020972736 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 11.block_0 (torch.Size([0])) for Parameter 11 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 14:52:46.961126 140693587801280 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 10.block_0 (torch.Size([512, 1024])) for Parameter 10 (torch.Size([512, 1024])), Block block_0 (torch.Size([512, 1024])).
W0316 14:52:46.961184 140344165901504 distributed_shampoo.py:359] Nesterov flag is enabled but momentum parameter is zero! Continuing without using momentum or Nesterov acceleration...
I0316 14:52:46.961220 140693587801280 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 11.block_0 (torch.Size([0])) for Parameter 11 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 14:52:46.961285 140693587801280 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 12.block_0 (torch.Size([0])) for Parameter 12 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 14:52:46.961308 140610183144640 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([256, 512])) for Parameter 2 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 14:52:46.961343 140693587801280 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 13.block_0 (torch.Size([0])) for Parameter 13 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 14:52:46.961395 140693587801280 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 14.block_0 (torch.Size([0])) for Parameter 14 (torch.Size([1, 256])), Block block_0 (torch.Size([256])).
I0316 14:52:46.961430 140610183144640 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 14:52:46.961451 140693587801280 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 15.block_0 (torch.Size([0])) for Parameter 15 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0316 14:52:46.961498 140610183144640 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 4.block_0 (torch.Size([0])) for Parameter 4 (torch.Size([128, 256])), Block block_0 (torch.Size([128, 256])).
I0316 14:52:46.961515 140693587801280 shampoo_preconditioner_list.py:375] Rank 1: AdaGradPreconditionerList Numel Breakdown: (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 524288, 0, 0, 0, 0, 0)
I0316 14:52:46.961550 140693587801280 shampoo_preconditioner_list.py:378] Rank 1: AdaGradPreconditionerList Bytes Breakdown: (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2097152, 0, 0, 0, 0, 0)
I0316 14:52:46.961559 140610183144640 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 5.block_0 (torch.Size([0])) for Parameter 5 (torch.Size([128])), Block block_0 (torch.Size([128])).
I0316 14:52:46.961587 140693587801280 shampoo_preconditioner_list.py:381] Rank 1: AdaGradPreconditionerList Total Elements: 524288
I0316 14:52:46.961614 140610183144640 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 6.block_0 (torch.Size([0])) for Parameter 6 (torch.Size([1024, 506])), Block block_0 (torch.Size([1024, 506])).
I0316 14:52:46.961622 140693587801280 shampoo_preconditioner_list.py:384] Rank 1: AdaGradPreconditionerList Total Bytes: 2097152
I0316 14:52:46.961686 140610183144640 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 7.block_0 (torch.Size([0])) for Parameter 7 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 14:52:46.961750 140610183144640 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 8.block_0 (torch.Size([0])) for Parameter 8 (torch.Size([1024, 1024])), Block block_0 (torch.Size([1024, 1024])).
I0316 14:52:46.961810 140610183144640 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 9.block_0 (torch.Size([0])) for Parameter 9 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 14:52:46.961869 140610183144640 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 10.block_0 (torch.Size([0])) for Parameter 10 (torch.Size([512, 1024])), Block block_0 (torch.Size([512, 1024])).
I0316 14:52:46.961865 139745737082048 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 11.block_0 ([torch.Size([512, 512])]) for Parameter 11 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 14:52:46.961946 140610183144640 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 11.block_0 (torch.Size([0])) for Parameter 11 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 14:52:46.961997 140610183144640 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 12.block_0 (torch.Size([0])) for Parameter 12 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 14:52:46.962023 139745737082048 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 12.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 12 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
W0316 14:52:46.962013 140693587801280 distributed_shampoo.py:359] Nesterov flag is enabled but momentum parameter is zero! Continuing without using momentum or Nesterov acceleration...
I0316 14:52:46.962048 140610183144640 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 13.block_0 (torch.Size([0])) for Parameter 13 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 14:52:46.962094 140610183144640 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 14.block_0 (torch.Size([0])) for Parameter 14 (torch.Size([1, 256])), Block block_0 (torch.Size([256])).
I0316 14:52:46.962145 140610183144640 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 15.block_0 (torch.Size([0])) for Parameter 15 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0316 14:52:46.962207 140610183144640 shampoo_preconditioner_list.py:375] Rank 3: AdaGradPreconditionerList Numel Breakdown: (0, 0, 131072, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)
I0316 14:52:46.962244 140610183144640 shampoo_preconditioner_list.py:378] Rank 3: AdaGradPreconditionerList Bytes Breakdown: (0, 0, 524288, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)
I0316 14:52:46.962277 140610183144640 shampoo_preconditioner_list.py:381] Rank 3: AdaGradPreconditionerList Total Elements: 131072
I0316 14:52:46.962315 140610183144640 shampoo_preconditioner_list.py:384] Rank 3: AdaGradPreconditionerList Total Bytes: 524288
I0316 14:52:46.962327 140456020972736 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 12.block_0 (torch.Size([256, 512])) for Parameter 12 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 14:52:46.962431 140456020972736 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 13.block_0 (torch.Size([0])) for Parameter 13 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 14:52:46.962500 140456020972736 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 14.block_0 (torch.Size([0])) for Parameter 14 (torch.Size([1, 256])), Block block_0 (torch.Size([256])).
I0316 14:52:46.962550 140456020972736 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 15.block_0 (torch.Size([0])) for Parameter 15 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0316 14:52:46.962611 140456020972736 shampoo_preconditioner_list.py:375] Rank 4: AdaGradPreconditionerList Numel Breakdown: (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 131072, 0, 0, 0)
I0316 14:52:46.962646 140456020972736 shampoo_preconditioner_list.py:378] Rank 4: AdaGradPreconditionerList Bytes Breakdown: (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 524288, 0, 0, 0)
I0316 14:52:46.962677 140456020972736 shampoo_preconditioner_list.py:381] Rank 4: AdaGradPreconditionerList Total Elements: 131072
W0316 14:52:46.962676 140610183144640 distributed_shampoo.py:359] Nesterov flag is enabled but momentum parameter is zero! Continuing without using momentum or Nesterov acceleration...
I0316 14:52:46.962707 140456020972736 shampoo_preconditioner_list.py:384] Rank 4: AdaGradPreconditionerList Total Bytes: 524288
W0316 14:52:46.962723 140292705682624 distributed_shampoo.py:359] Nesterov flag is enabled but momentum parameter is zero! Continuing without using momentum or Nesterov acceleration...
I0316 14:52:46.962773 139868253091008 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 14:52:46.962867 139868253091008 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_1 (torch.Size([0])) for Parameter 0 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
W0316 14:52:46.963035 140456020972736 distributed_shampoo.py:359] Nesterov flag is enabled but momentum parameter is zero! Continuing without using momentum or Nesterov acceleration...
I0316 14:52:46.963193 139745737082048 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 13.block_0 ([torch.Size([256, 256])]) for Parameter 13 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 14:52:46.963264 140344165901504 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 14:52:46.963357 140344165901504 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_1 (torch.Size([0])) for Parameter 0 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 14:52:46.963387 139745737082048 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 14.block_0 ([torch.Size([256, 256])]) for Parameter 14 (torch.Size([1, 256])), Block block_0 (torch.Size([256])).
I0316 14:52:46.963418 140344165901504 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 14:52:46.963467 140344165901504 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_1 (torch.Size([0])) for Parameter 1 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 14:52:46.963519 140344165901504 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 14:52:46.963551 139745737082048 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 15.block_0 ([torch.Size([1, 1])]) for Parameter 15 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0316 14:52:46.963569 140344165901504 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_1 (torch.Size([0])) for Parameter 2 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 14:52:46.963683 139745737082048 shampoo_preconditioner_list.py:612] Rank 7: ShampooPreconditionerList Numel Breakdown: (524626, 524288, 655360, 131072, 163840, 32768, 2609224, 2097152, 4194304, 2097152, 2621440, 524288, 655360, 131072, 131072, 2)
I0316 14:52:46.963732 139745737082048 shampoo_preconditioner_list.py:615] Rank 7: ShampooPreconditionerList Bytes Breakdown: (2098504, 2097152, 2621440, 524288, 655360, 131072, 10436896, 8388608, 16777216)
I0316 14:52:46.963739 139868253091008 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([524288, 128])) for Parameter 1 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 14:52:46.963770 139745737082048 shampoo_preconditioner_list.py:618] Rank 7: ShampooPreconditionerList Total Elements: 17093020
I0316 14:52:46.963801 139745737082048 shampoo_preconditioner_list.py:621] Rank 7: ShampooPreconditionerList Total Bytes: 43730536
I0316 14:52:46.963804 140693587801280 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 14:52:46.963842 139868253091008 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_1 (torch.Size([0])) for Parameter 1 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 14:52:46.963895 139868253091008 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 14:52:46.963948 139868253091008 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_1 (torch.Size([0])) for Parameter 2 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 14:52:46.963943 139745737082048 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([512, 13])), Block block_0 (torch.Size([512, 13])).
I0316 14:52:46.963999 139868253091008 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 14:52:46.964040 139745737082048 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([512])) for Parameter 1 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 14:52:46.964049 139868253091008 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_1 (torch.Size([0])) for Parameter 3 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 14:52:46.964092 139868253091008 shampoo_preconditioner_list.py:375] Rank 2: AdaGradPreconditionerList Numel Breakdown: (0, 0, 67108864, 0, 0, 0, 0, 0)
I0316 14:52:46.964127 139868253091008 shampoo_preconditioner_list.py:378] Rank 2: AdaGradPreconditionerList Bytes Breakdown: (0, 0, 268435456, 0, 0, 0, 0, 0)
I0316 14:52:46.964125 139745737082048 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 14:52:46.964157 139868253091008 shampoo_preconditioner_list.py:381] Rank 2: AdaGradPreconditionerList Total Elements: 67108864
I0316 14:52:46.964187 139868253091008 shampoo_preconditioner_list.py:384] Rank 2: AdaGradPreconditionerList Total Bytes: 268435456
I0316 14:52:46.964153 140547343606976 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([524288, 128])) for Parameter 0 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 14:52:46.964207 139745737082048 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([256])) for Parameter 3 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 14:52:46.964280 139745737082048 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 4.block_0 (torch.Size([0])) for Parameter 4 (torch.Size([128, 256])), Block block_0 (torch.Size([128, 256])).
I0316 14:52:46.964276 140547343606976 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_1 (torch.Size([0])) for Parameter 0 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 14:52:46.964342 140547343606976 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 14:52:46.964363 139745737082048 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 5.block_0 (torch.Size([128])) for Parameter 5 (torch.Size([128])), Block block_0 (torch.Size([128])).
I0316 14:52:46.964409 140547343606976 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_1 (torch.Size([0])) for Parameter 1 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 14:52:46.964434 139745737082048 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 6.block_0 (torch.Size([0])) for Parameter 6 (torch.Size([1024, 506])), Block block_0 (torch.Size([1024, 506])).
I0316 14:52:46.964469 140547343606976 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 14:52:46.964527 140547343606976 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_1 (torch.Size([0])) for Parameter 2 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 14:52:46.964524 139745737082048 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 7.block_0 (torch.Size([1024])) for Parameter 7 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 14:52:46.964585 140547343606976 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 14:52:46.964585 139745737082048 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 8.block_0 (torch.Size([0])) for Parameter 8 (torch.Size([1024, 1024])), Block block_0 (torch.Size([1024, 1024])).
I0316 14:52:46.964641 140547343606976 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_1 (torch.Size([0])) for Parameter 3 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 14:52:46.964656 139745737082048 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 9.block_0 (torch.Size([1024])) for Parameter 9 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 14:52:46.964691 140547343606976 shampoo_preconditioner_list.py:375] Rank 0: AdaGradPreconditionerList Numel Breakdown: (67108864, 0, 0, 0, 0, 0, 0, 0)
I0316 14:52:46.964722 139745737082048 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 10.block_0 (torch.Size([0])) for Parameter 10 (torch.Size([512, 1024])), Block block_0 (torch.Size([512, 1024])).
I0316 14:52:46.964728 140547343606976 shampoo_preconditioner_list.py:378] Rank 0: AdaGradPreconditionerList Bytes Breakdown: (268435456, 0, 0, 0, 0, 0, 0, 0)
I0316 14:52:46.964759 140547343606976 shampoo_preconditioner_list.py:381] Rank 0: AdaGradPreconditionerList Total Elements: 67108864
I0316 14:52:46.964755 140610183144640 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 14:52:46.964793 140547343606976 shampoo_preconditioner_list.py:384] Rank 0: AdaGradPreconditionerList Total Bytes: 268435456
I0316 14:52:46.964803 139745737082048 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 11.block_0 (torch.Size([512])) for Parameter 11 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 14:52:46.964851 140610183144640 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_1 (torch.Size([0])) for Parameter 0 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 14:52:46.964870 139745737082048 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 12.block_0 (torch.Size([0])) for Parameter 12 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 14:52:46.964913 140610183144640 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 14:52:46.964941 139745737082048 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 13.block_0 (torch.Size([256])) for Parameter 13 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 14:52:46.964938 140344165901504 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([524288, 128])) for Parameter 3 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 14:52:46.964985 140292705682624 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 14:52:46.965020 139745737082048 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 14.block_0 (torch.Size([256])) for Parameter 14 (torch.Size([1, 256])), Block block_0 (torch.Size([256])).
I0316 14:52:46.965036 140344165901504 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_1 (torch.Size([0])) for Parameter 3 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 14:52:46.965042 140693587801280 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_1 (torch.Size([524288, 128])) for Parameter 0 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 14:52:46.965094 140344165901504 shampoo_preconditioner_list.py:375] Rank 6: AdaGradPreconditionerList Numel Breakdown: (0, 0, 0, 0, 0, 0, 67108864, 0)
I0316 14:52:46.965086 139745737082048 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 15.block_0 (torch.Size([1])) for Parameter 15 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0316 14:52:46.965087 140292705682624 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_1 (torch.Size([0])) for Parameter 0 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 14:52:46.965131 140344165901504 shampoo_preconditioner_list.py:378] Rank 6: AdaGradPreconditionerList Bytes Breakdown: (0, 0, 0, 0, 0, 0, 268435456, 0)
I0316 14:52:46.965155 140292705682624 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 14:52:46.965163 140344165901504 shampoo_preconditioner_list.py:381] Rank 6: AdaGradPreconditionerList Total Elements: 67108864
I0316 14:52:46.965156 140693587801280 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 14:52:46.965161 139745737082048 shampoo_preconditioner_list.py:375] Rank 7: AdaGradPreconditionerList Numel Breakdown: (0, 512, 0, 256, 0, 128, 0, 1024, 0, 1024, 0, 512, 0, 256, 256, 1)
I0316 14:52:46.965155 140456020972736 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 14:52:46.965201 140344165901504 shampoo_preconditioner_list.py:384] Rank 6: AdaGradPreconditionerList Total Bytes: 268435456
I0316 14:52:46.965211 139745737082048 shampoo_preconditioner_list.py:378] Rank 7: AdaGradPreconditionerList Bytes Breakdown: (0, 2048, 0, 1024, 0, 512, 0, 4096, 0, 4096, 0, 2048, 0, 1024, 1024, 4)
I0316 14:52:46.965241 139745737082048 shampoo_preconditioner_list.py:381] Rank 7: AdaGradPreconditionerList Total Elements: 3969
I0316 14:52:46.965229 140292705682624 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_1 (torch.Size([0])) for Parameter 1 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 14:52:46.965228 140693587801280 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_1 (torch.Size([0])) for Parameter 1 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 14:52:46.965279 139745737082048 shampoo_preconditioner_list.py:384] Rank 7: AdaGradPreconditionerList Total Bytes: 15876
I0316 14:52:46.965288 140292705682624 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 14:52:46.965283 140456020972736 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_1 (torch.Size([0])) for Parameter 0 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 14:52:46.965307 140693587801280 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 14:52:46.965343 140456020972736 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 14:52:46.965363 140693587801280 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_1 (torch.Size([0])) for Parameter 2 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 14:52:46.965403 140456020972736 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_1 (torch.Size([0])) for Parameter 1 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 14:52:46.965411 140693587801280 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 14:52:46.965457 140693587801280 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_1 (torch.Size([0])) for Parameter 3 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 14:52:46.965507 140693587801280 shampoo_preconditioner_list.py:375] Rank 1: AdaGradPreconditionerList Numel Breakdown: (0, 67108864, 0, 0, 0, 0, 0, 0)
I0316 14:52:46.965543 140693587801280 shampoo_preconditioner_list.py:378] Rank 1: AdaGradPreconditionerList Bytes Breakdown: (0, 268435456, 0, 0, 0, 0, 0, 0)
I0316 14:52:46.965575 140693587801280 shampoo_preconditioner_list.py:381] Rank 1: AdaGradPreconditionerList Total Elements: 67108864
I0316 14:52:46.965607 140693587801280 shampoo_preconditioner_list.py:384] Rank 1: AdaGradPreconditionerList Total Bytes: 268435456
I0316 14:52:46.965732 140610183144640 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_1 (torch.Size([524288, 128])) for Parameter 1 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
W0316 14:52:46.965816 139745737082048 distributed_shampoo.py:359] Nesterov flag is enabled but momentum parameter is zero! Continuing without using momentum or Nesterov acceleration...
I0316 14:52:46.965842 140610183144640 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 14:52:46.965899 140610183144640 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_1 (torch.Size([0])) for Parameter 2 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 14:52:46.965957 140610183144640 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 14:52:46.965955 140547343606976 submission_runner.py:279] Initializing metrics bundle.
I0316 14:52:46.966021 140610183144640 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_1 (torch.Size([0])) for Parameter 3 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 14:52:46.966073 140610183144640 shampoo_preconditioner_list.py:375] Rank 3: AdaGradPreconditionerList Numel Breakdown: (0, 0, 0, 67108864, 0, 0, 0, 0)
I0316 14:52:46.966069 139868253091008 logger_utils.py:262] Unable to record workload.train_mean information. Continuing without it.
I0316 14:52:46.966109 140610183144640 shampoo_preconditioner_list.py:378] Rank 3: AdaGradPreconditionerList Bytes Breakdown: (0, 0, 0, 268435456, 0, 0, 0, 0)
I0316 14:52:46.966123 140547343606976 submission_runner.py:301] Initializing checkpoint and logger.
I0316 14:52:46.966138 140610183144640 shampoo_preconditioner_list.py:381] Rank 3: AdaGradPreconditionerList Total Elements: 67108864
I0316 14:52:46.966144 139868253091008 logger_utils.py:262] Unable to record workload.train_stddev information. Continuing without it.
I0316 14:52:46.966165 140610183144640 shampoo_preconditioner_list.py:384] Rank 3: AdaGradPreconditionerList Total Bytes: 268435456
I0316 14:52:46.966222 140292705682624 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_1 (torch.Size([524288, 128])) for Parameter 2 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 14:52:46.966328 140292705682624 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 14:52:46.966390 140292705682624 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_1 (torch.Size([0])) for Parameter 3 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 14:52:46.966444 140292705682624 shampoo_preconditioner_list.py:375] Rank 5: AdaGradPreconditionerList Numel Breakdown: (0, 0, 0, 0, 0, 67108864, 0, 0)
I0316 14:52:46.966482 140292705682624 shampoo_preconditioner_list.py:378] Rank 5: AdaGradPreconditionerList Bytes Breakdown: (0, 0, 0, 0, 0, 268435456, 0, 0)
I0316 14:52:46.966485 140456020972736 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([524288, 128])) for Parameter 2 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 14:52:46.966516 140292705682624 shampoo_preconditioner_list.py:381] Rank 5: AdaGradPreconditionerList Total Elements: 67108864
I0316 14:52:46.966543 140292705682624 shampoo_preconditioner_list.py:384] Rank 5: AdaGradPreconditionerList Total Bytes: 268435456
I0316 14:52:46.966590 140456020972736 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_1 (torch.Size([0])) for Parameter 2 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 14:52:46.966621 140547343606976 submission_runner.py:321] Saving meta data to /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_2/criteo1tb_pytorch/trial_2/meta_data_0.json.
I0316 14:52:46.966652 140456020972736 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 14:52:46.966701 140456020972736 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_1 (torch.Size([0])) for Parameter 3 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 14:52:46.966751 140456020972736 shampoo_preconditioner_list.py:375] Rank 4: AdaGradPreconditionerList Numel Breakdown: (0, 0, 0, 0, 67108864, 0, 0, 0)
I0316 14:52:46.966788 140456020972736 shampoo_preconditioner_list.py:378] Rank 4: AdaGradPreconditionerList Bytes Breakdown: (0, 0, 0, 0, 268435456, 0, 0, 0)
I0316 14:52:46.966796 140547343606976 logger_utils.py:262] Unable to record workload.train_mean information. Continuing without it.
I0316 14:52:46.966821 140456020972736 shampoo_preconditioner_list.py:381] Rank 4: AdaGradPreconditionerList Total Elements: 67108864
I0316 14:52:46.966843 140547343606976 logger_utils.py:262] Unable to record workload.train_stddev information. Continuing without it.
I0316 14:52:46.966848 140456020972736 shampoo_preconditioner_list.py:384] Rank 4: AdaGradPreconditionerList Total Bytes: 268435456
I0316 14:52:46.967139 140344165901504 logger_utils.py:262] Unable to record workload.train_mean information. Continuing without it.
I0316 14:52:46.967213 140344165901504 logger_utils.py:262] Unable to record workload.train_stddev information. Continuing without it.
I0316 14:52:46.967499 140693587801280 logger_utils.py:262] Unable to record workload.train_mean information. Continuing without it.
I0316 14:52:46.967584 140693587801280 logger_utils.py:262] Unable to record workload.train_stddev information. Continuing without it.
I0316 14:52:46.967558 139745737082048 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 14:52:46.967648 139745737082048 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_1 (torch.Size([0])) for Parameter 0 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 14:52:46.967723 139745737082048 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 14:52:46.967786 139745737082048 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_1 (torch.Size([0])) for Parameter 1 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 14:52:46.967844 139745737082048 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 14:52:46.967913 139745737082048 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_1 (torch.Size([0])) for Parameter 2 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 14:52:46.967968 139745737082048 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 14:52:46.968006 140610183144640 logger_utils.py:262] Unable to record workload.train_mean information. Continuing without it.
I0316 14:52:46.968077 140610183144640 logger_utils.py:262] Unable to record workload.train_stddev information. Continuing without it.
I0316 14:52:46.968267 140292705682624 logger_utils.py:262] Unable to record workload.train_mean information. Continuing without it.
I0316 14:52:46.968336 140292705682624 logger_utils.py:262] Unable to record workload.train_stddev information. Continuing without it.
I0316 14:52:46.968426 139745737082048 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_1 (torch.Size([524288, 128])) for Parameter 3 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 14:52:46.968451 140456020972736 logger_utils.py:262] Unable to record workload.train_mean information. Continuing without it.
I0316 14:52:46.968512 140456020972736 logger_utils.py:262] Unable to record workload.train_stddev information. Continuing without it.
I0316 14:52:46.968521 139745737082048 shampoo_preconditioner_list.py:375] Rank 7: AdaGradPreconditionerList Numel Breakdown: (0, 0, 0, 0, 0, 0, 0, 67108864)
I0316 14:52:46.968554 139745737082048 shampoo_preconditioner_list.py:378] Rank 7: AdaGradPreconditionerList Bytes Breakdown: (0, 0, 0, 0, 0, 0, 0, 268435456)
I0316 14:52:46.968589 139745737082048 shampoo_preconditioner_list.py:381] Rank 7: AdaGradPreconditionerList Total Elements: 67108864
I0316 14:52:46.968621 139745737082048 shampoo_preconditioner_list.py:384] Rank 7: AdaGradPreconditionerList Total Bytes: 268435456
I0316 14:52:46.970036 139745737082048 logger_utils.py:262] Unable to record workload.train_mean information. Continuing without it.
I0316 14:52:46.970121 139745737082048 logger_utils.py:262] Unable to record workload.train_stddev information. Continuing without it.
I0316 14:52:47.264360 140547343606976 submission_runner.py:325] Saving flags to /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_2/criteo1tb_pytorch/trial_2/flags_0.json.
I0316 14:52:47.297065 140547343606976 submission_runner.py:337] Starting training loop.
I0316 14:52:51.750529 140519729514240 logging_writer.py:48] [0] global_step=0, grad_norm=9.12113, loss=1.26601
I0316 14:52:51.767628 140547343606976 submission.py:265] 0) loss = 1.266, grad_norm = 9.121
I0316 14:52:52.172693 140547343606976 spec.py:321] Evaluating on the training split.
I0316 14:57:57.994124 140547343606976 spec.py:333] Evaluating on the validation split.
I0316 15:02:57.824193 140547343606976 spec.py:349] Evaluating on the test split.
I0316 15:08:35.158207 140547343606976 submission_runner.py:469] Time since start: 947.86s, 	Step: 1, 	{'train/loss': 1.2654326404953318, 'validation/loss': 1.268143453003206, 'validation/num_examples': 83274637, 'test/loss': 1.2667239189523798, 'test/num_examples': 95000000, 'score': 4.471369504928589, 'total_duration': 947.8612711429596, 'accumulated_submission_time': 4.471369504928589, 'accumulated_eval_time': 942.9857084751129, 'accumulated_logging_time': 0}
I0316 15:08:35.166743 140504872244992 logging_writer.py:48] [1] accumulated_eval_time=942.986, accumulated_logging_time=0, accumulated_submission_time=4.47137, global_step=1, preemption_count=0, score=4.47137, test/loss=1.26672, test/num_examples=95000000, total_duration=947.861, train/loss=1.26543, validation/loss=1.26814, validation/num_examples=83274637
I0316 15:08:35.819144 140504863852288 logging_writer.py:48] [1] global_step=1, grad_norm=9.11629, loss=1.26543
I0316 15:08:35.822375 140547343606976 submission.py:265] 1) loss = 1.265, grad_norm = 9.116
I0316 15:08:36.018100 140504872244992 logging_writer.py:48] [2] global_step=2, grad_norm=9.0926, loss=1.24088
I0316 15:08:36.021237 140547343606976 submission.py:265] 2) loss = 1.241, grad_norm = 9.093
I0316 15:08:36.212984 140504863852288 logging_writer.py:48] [3] global_step=3, grad_norm=9.03176, loss=1.19769
I0316 15:08:36.217108 140547343606976 submission.py:265] 3) loss = 1.198, grad_norm = 9.032
I0316 15:08:36.406095 140504872244992 logging_writer.py:48] [4] global_step=4, grad_norm=8.90542, loss=1.13707
I0316 15:08:36.409277 140547343606976 submission.py:265] 4) loss = 1.137, grad_norm = 8.905
I0316 15:08:36.614675 140504863852288 logging_writer.py:48] [5] global_step=5, grad_norm=8.44615, loss=1.06152
I0316 15:08:36.617796 140547343606976 submission.py:265] 5) loss = 1.062, grad_norm = 8.446
I0316 15:08:36.809164 140504872244992 logging_writer.py:48] [6] global_step=6, grad_norm=8.06983, loss=0.977546
I0316 15:08:36.812047 140547343606976 submission.py:265] 6) loss = 0.978, grad_norm = 8.070
I0316 15:08:37.002370 140504863852288 logging_writer.py:48] [7] global_step=7, grad_norm=7.41959, loss=0.888133
I0316 15:08:37.005253 140547343606976 submission.py:265] 7) loss = 0.888, grad_norm = 7.420
I0316 15:08:37.195980 140504872244992 logging_writer.py:48] [8] global_step=8, grad_norm=7.09757, loss=0.794189
I0316 15:08:37.198930 140547343606976 submission.py:265] 8) loss = 0.794, grad_norm = 7.098
I0316 15:08:37.396394 140504863852288 logging_writer.py:48] [9] global_step=9, grad_norm=6.71509, loss=0.696847
I0316 15:08:37.400317 140547343606976 submission.py:265] 9) loss = 0.697, grad_norm = 6.715
I0316 15:08:37.591456 140504872244992 logging_writer.py:48] [10] global_step=10, grad_norm=6.20911, loss=0.599956
I0316 15:08:37.595314 140547343606976 submission.py:265] 10) loss = 0.600, grad_norm = 6.209
I0316 15:08:37.788889 140504863852288 logging_writer.py:48] [11] global_step=11, grad_norm=5.51931, loss=0.506901
I0316 15:08:37.792535 140547343606976 submission.py:265] 11) loss = 0.507, grad_norm = 5.519
I0316 15:08:38.000756 140504872244992 logging_writer.py:48] [12] global_step=12, grad_norm=4.65733, loss=0.420439
I0316 15:08:38.004116 140547343606976 submission.py:265] 12) loss = 0.420, grad_norm = 4.657
I0316 15:08:38.195482 140504863852288 logging_writer.py:48] [13] global_step=13, grad_norm=3.71787, loss=0.348887
I0316 15:08:38.199222 140547343606976 submission.py:265] 13) loss = 0.349, grad_norm = 3.718
I0316 15:08:38.391438 140504872244992 logging_writer.py:48] [14] global_step=14, grad_norm=2.82829, loss=0.290682
I0316 15:08:38.394846 140547343606976 submission.py:265] 14) loss = 0.291, grad_norm = 2.828
I0316 15:08:38.586250 140504863852288 logging_writer.py:48] [15] global_step=15, grad_norm=1.93008, loss=0.247189
I0316 15:08:38.591289 140547343606976 submission.py:265] 15) loss = 0.247, grad_norm = 1.930
I0316 15:08:38.782858 140504872244992 logging_writer.py:48] [16] global_step=16, grad_norm=1.16477, loss=0.216043
I0316 15:08:38.786433 140547343606976 submission.py:265] 16) loss = 0.216, grad_norm = 1.165
I0316 15:08:38.977221 140504863852288 logging_writer.py:48] [17] global_step=17, grad_norm=0.552312, loss=0.200415
I0316 15:08:38.980535 140547343606976 submission.py:265] 17) loss = 0.200, grad_norm = 0.552
I0316 15:08:39.171329 140504872244992 logging_writer.py:48] [18] global_step=18, grad_norm=0.259135, loss=0.196471
I0316 15:08:39.174581 140547343606976 submission.py:265] 18) loss = 0.196, grad_norm = 0.259
I0316 15:08:39.365286 140504863852288 logging_writer.py:48] [19] global_step=19, grad_norm=0.464942, loss=0.200331
I0316 15:08:39.368251 140547343606976 submission.py:265] 19) loss = 0.200, grad_norm = 0.465
I0316 15:08:39.559361 140504872244992 logging_writer.py:48] [20] global_step=20, grad_norm=0.75617, loss=0.208335
I0316 15:08:39.562480 140547343606976 submission.py:265] 20) loss = 0.208, grad_norm = 0.756
I0316 15:08:39.754267 140504863852288 logging_writer.py:48] [21] global_step=21, grad_norm=1.05865, loss=0.223907
I0316 15:08:39.757258 140547343606976 submission.py:265] 21) loss = 0.224, grad_norm = 1.059
I0316 15:08:39.947661 140504872244992 logging_writer.py:48] [22] global_step=22, grad_norm=1.27148, loss=0.237106
I0316 15:08:39.950492 140547343606976 submission.py:265] 22) loss = 0.237, grad_norm = 1.271
I0316 15:08:40.142652 140504863852288 logging_writer.py:48] [23] global_step=23, grad_norm=1.47815, loss=0.257668
I0316 15:08:40.145885 140547343606976 submission.py:265] 23) loss = 0.258, grad_norm = 1.478
I0316 15:08:40.338304 140504872244992 logging_writer.py:48] [24] global_step=24, grad_norm=1.67102, loss=0.277814
I0316 15:08:40.341463 140547343606976 submission.py:265] 24) loss = 0.278, grad_norm = 1.671
I0316 15:08:40.533326 140504863852288 logging_writer.py:48] [25] global_step=25, grad_norm=1.8618, loss=0.301357
I0316 15:08:40.536309 140547343606976 submission.py:265] 25) loss = 0.301, grad_norm = 1.862
I0316 15:08:40.728035 140504872244992 logging_writer.py:48] [26] global_step=26, grad_norm=2.02134, loss=0.324313
I0316 15:08:40.731322 140547343606976 submission.py:265] 26) loss = 0.324, grad_norm = 2.021
I0316 15:08:40.926469 140504863852288 logging_writer.py:48] [27] global_step=27, grad_norm=2.14774, loss=0.341902
I0316 15:08:40.929816 140547343606976 submission.py:265] 27) loss = 0.342, grad_norm = 2.148
I0316 15:08:41.120034 140504872244992 logging_writer.py:48] [28] global_step=28, grad_norm=2.31321, loss=0.367739
I0316 15:08:41.123782 140547343606976 submission.py:265] 28) loss = 0.368, grad_norm = 2.313
I0316 15:08:41.312605 140504863852288 logging_writer.py:48] [29] global_step=29, grad_norm=2.36234, loss=0.376786
I0316 15:08:41.316224 140547343606976 submission.py:265] 29) loss = 0.377, grad_norm = 2.362
I0316 15:08:41.506486 140504872244992 logging_writer.py:48] [30] global_step=30, grad_norm=2.47819, loss=0.396188
I0316 15:08:41.509939 140547343606976 submission.py:265] 30) loss = 0.396, grad_norm = 2.478
I0316 15:08:42.526008 140504863852288 logging_writer.py:48] [31] global_step=31, grad_norm=2.56156, loss=0.410609
I0316 15:08:42.529577 140547343606976 submission.py:265] 31) loss = 0.411, grad_norm = 2.562
I0316 15:08:43.441603 140504872244992 logging_writer.py:48] [32] global_step=32, grad_norm=2.58116, loss=0.415535
I0316 15:08:43.445265 140547343606976 submission.py:265] 32) loss = 0.416, grad_norm = 2.581
I0316 15:08:44.775703 140504863852288 logging_writer.py:48] [33] global_step=33, grad_norm=2.59687, loss=0.419007
I0316 15:08:44.779358 140547343606976 submission.py:265] 33) loss = 0.419, grad_norm = 2.597
I0316 15:08:45.690464 140504872244992 logging_writer.py:48] [34] global_step=34, grad_norm=2.60275, loss=0.419851
I0316 15:08:45.694087 140547343606976 submission.py:265] 34) loss = 0.420, grad_norm = 2.603
I0316 15:08:47.297362 140504863852288 logging_writer.py:48] [35] global_step=35, grad_norm=2.62786, loss=0.421795
I0316 15:08:47.300574 140547343606976 submission.py:265] 35) loss = 0.422, grad_norm = 2.628
I0316 15:08:48.072200 140504872244992 logging_writer.py:48] [36] global_step=36, grad_norm=2.59263, loss=0.415092
I0316 15:08:48.076043 140547343606976 submission.py:265] 36) loss = 0.415, grad_norm = 2.593
I0316 15:08:49.535873 140504863852288 logging_writer.py:48] [37] global_step=37, grad_norm=2.5559, loss=0.40639
I0316 15:08:49.539155 140547343606976 submission.py:265] 37) loss = 0.406, grad_norm = 2.556
I0316 15:08:50.598726 140504872244992 logging_writer.py:48] [38] global_step=38, grad_norm=2.53098, loss=0.395161
I0316 15:08:50.601724 140547343606976 submission.py:265] 38) loss = 0.395, grad_norm = 2.531
I0316 15:08:51.643548 140504863852288 logging_writer.py:48] [39] global_step=39, grad_norm=2.50679, loss=0.387371
I0316 15:08:51.646756 140547343606976 submission.py:265] 39) loss = 0.387, grad_norm = 2.507
I0316 15:08:52.759122 140504872244992 logging_writer.py:48] [40] global_step=40, grad_norm=2.35187, loss=0.359987
I0316 15:08:52.762284 140547343606976 submission.py:265] 40) loss = 0.360, grad_norm = 2.352
I0316 15:08:53.809834 140504863852288 logging_writer.py:48] [41] global_step=41, grad_norm=2.25516, loss=0.341833
I0316 15:08:53.813045 140547343606976 submission.py:265] 41) loss = 0.342, grad_norm = 2.255
I0316 15:08:54.775792 140504872244992 logging_writer.py:48] [42] global_step=42, grad_norm=2.13221, loss=0.320296
I0316 15:08:54.778927 140547343606976 submission.py:265] 42) loss = 0.320, grad_norm = 2.132
I0316 15:08:55.979403 140504863852288 logging_writer.py:48] [43] global_step=43, grad_norm=1.95661, loss=0.293302
I0316 15:08:55.982426 140547343606976 submission.py:265] 43) loss = 0.293, grad_norm = 1.957
I0316 15:08:57.100464 140504872244992 logging_writer.py:48] [44] global_step=44, grad_norm=1.83012, loss=0.274279
I0316 15:08:57.103888 140547343606976 submission.py:265] 44) loss = 0.274, grad_norm = 1.830
I0316 15:08:58.559774 140504863852288 logging_writer.py:48] [45] global_step=45, grad_norm=1.57557, loss=0.242505
I0316 15:08:58.562903 140547343606976 submission.py:265] 45) loss = 0.243, grad_norm = 1.576
I0316 15:08:59.367013 140504872244992 logging_writer.py:48] [46] global_step=46, grad_norm=1.41142, loss=0.223447
I0316 15:08:59.370230 140547343606976 submission.py:265] 46) loss = 0.223, grad_norm = 1.411
I0316 15:09:01.222342 140504863852288 logging_writer.py:48] [47] global_step=47, grad_norm=1.19216, loss=0.203492
I0316 15:09:01.225548 140547343606976 submission.py:265] 47) loss = 0.203, grad_norm = 1.192
I0316 15:09:02.105036 140504872244992 logging_writer.py:48] [48] global_step=48, grad_norm=0.975386, loss=0.18917
I0316 15:09:02.108694 140547343606976 submission.py:265] 48) loss = 0.189, grad_norm = 0.975
I0316 15:09:03.884087 140504863852288 logging_writer.py:48] [49] global_step=49, grad_norm=0.683203, loss=0.174171
I0316 15:09:03.887373 140547343606976 submission.py:265] 49) loss = 0.174, grad_norm = 0.683
I0316 15:09:04.855475 140504872244992 logging_writer.py:48] [50] global_step=50, grad_norm=0.376523, loss=0.164092
I0316 15:09:04.858717 140547343606976 submission.py:265] 50) loss = 0.164, grad_norm = 0.377
I0316 15:09:06.630229 140504863852288 logging_writer.py:48] [51] global_step=51, grad_norm=0.194721, loss=0.160363
I0316 15:09:06.633516 140547343606976 submission.py:265] 51) loss = 0.160, grad_norm = 0.195
I0316 15:09:07.693261 140504872244992 logging_writer.py:48] [52] global_step=52, grad_norm=0.437103, loss=0.163977
I0316 15:09:07.696363 140547343606976 submission.py:265] 52) loss = 0.164, grad_norm = 0.437
I0316 15:09:09.180203 140504863852288 logging_writer.py:48] [53] global_step=53, grad_norm=0.793806, loss=0.167257
I0316 15:09:09.183491 140547343606976 submission.py:265] 53) loss = 0.167, grad_norm = 0.794
I0316 15:09:10.117848 140504872244992 logging_writer.py:48] [54] global_step=54, grad_norm=1.09979, loss=0.16948
I0316 15:09:10.121042 140547343606976 submission.py:265] 54) loss = 0.169, grad_norm = 1.100
I0316 15:09:11.614143 140504863852288 logging_writer.py:48] [55] global_step=55, grad_norm=1.2465, loss=0.173343
I0316 15:09:11.617646 140547343606976 submission.py:265] 55) loss = 0.173, grad_norm = 1.247
I0316 15:09:12.630346 140504872244992 logging_writer.py:48] [56] global_step=56, grad_norm=1.28164, loss=0.171541
I0316 15:09:12.633539 140547343606976 submission.py:265] 56) loss = 0.172, grad_norm = 1.282
I0316 15:09:13.919688 140504863852288 logging_writer.py:48] [57] global_step=57, grad_norm=1.14948, loss=0.170823
I0316 15:09:13.922653 140547343606976 submission.py:265] 57) loss = 0.171, grad_norm = 1.149
I0316 15:09:15.467673 140504872244992 logging_writer.py:48] [58] global_step=58, grad_norm=0.924907, loss=0.167584
I0316 15:09:15.470867 140547343606976 submission.py:265] 58) loss = 0.168, grad_norm = 0.925
I0316 15:09:15.929633 140504863852288 logging_writer.py:48] [59] global_step=59, grad_norm=0.660701, loss=0.163309
I0316 15:09:15.932848 140547343606976 submission.py:265] 59) loss = 0.163, grad_norm = 0.661
I0316 15:09:18.207082 140504872244992 logging_writer.py:48] [60] global_step=60, grad_norm=0.420345, loss=0.155913
I0316 15:09:18.210127 140547343606976 submission.py:265] 60) loss = 0.156, grad_norm = 0.420
I0316 15:09:18.990333 140504863852288 logging_writer.py:48] [61] global_step=61, grad_norm=0.16733, loss=0.15498
I0316 15:09:18.994090 140547343606976 submission.py:265] 61) loss = 0.155, grad_norm = 0.167
I0316 15:09:20.921184 140504872244992 logging_writer.py:48] [62] global_step=62, grad_norm=0.115138, loss=0.15053
I0316 15:09:20.924408 140547343606976 submission.py:265] 62) loss = 0.151, grad_norm = 0.115
I0316 15:09:21.697727 140504863852288 logging_writer.py:48] [63] global_step=63, grad_norm=0.262809, loss=0.150573
I0316 15:09:21.700725 140547343606976 submission.py:265] 63) loss = 0.151, grad_norm = 0.263
I0316 15:09:23.201382 140504872244992 logging_writer.py:48] [64] global_step=64, grad_norm=0.380099, loss=0.150476
I0316 15:09:23.204439 140547343606976 submission.py:265] 64) loss = 0.150, grad_norm = 0.380
I0316 15:09:24.103391 140504863852288 logging_writer.py:48] [65] global_step=65, grad_norm=0.476579, loss=0.152462
I0316 15:09:24.107077 140547343606976 submission.py:265] 65) loss = 0.152, grad_norm = 0.477
I0316 15:09:26.067423 140504872244992 logging_writer.py:48] [66] global_step=66, grad_norm=0.550706, loss=0.155174
I0316 15:09:26.071189 140547343606976 submission.py:265] 66) loss = 0.155, grad_norm = 0.551
I0316 15:09:26.904665 140504863852288 logging_writer.py:48] [67] global_step=67, grad_norm=0.557674, loss=0.154852
I0316 15:09:26.908062 140547343606976 submission.py:265] 67) loss = 0.155, grad_norm = 0.558
I0316 15:09:28.731857 140504872244992 logging_writer.py:48] [68] global_step=68, grad_norm=0.508051, loss=0.150597
I0316 15:09:28.735414 140547343606976 submission.py:265] 68) loss = 0.151, grad_norm = 0.508
I0316 15:09:29.489967 140504863852288 logging_writer.py:48] [69] global_step=69, grad_norm=0.463656, loss=0.149284
I0316 15:09:29.494076 140547343606976 submission.py:265] 69) loss = 0.149, grad_norm = 0.464
I0316 15:09:31.375779 140504872244992 logging_writer.py:48] [70] global_step=70, grad_norm=0.410636, loss=0.148637
I0316 15:09:31.379545 140547343606976 submission.py:265] 70) loss = 0.149, grad_norm = 0.411
I0316 15:09:31.979860 140504863852288 logging_writer.py:48] [71] global_step=71, grad_norm=0.298888, loss=0.143523
I0316 15:09:31.983421 140547343606976 submission.py:265] 71) loss = 0.144, grad_norm = 0.299
I0316 15:09:34.164198 140504872244992 logging_writer.py:48] [72] global_step=72, grad_norm=0.191639, loss=0.139962
I0316 15:09:34.167916 140547343606976 submission.py:265] 72) loss = 0.140, grad_norm = 0.192
I0316 15:09:35.191092 140504863852288 logging_writer.py:48] [73] global_step=73, grad_norm=0.117346, loss=0.140817
I0316 15:09:35.194707 140547343606976 submission.py:265] 73) loss = 0.141, grad_norm = 0.117
I0316 15:09:36.655082 140504872244992 logging_writer.py:48] [74] global_step=74, grad_norm=0.0415958, loss=0.138337
I0316 15:09:36.658792 140547343606976 submission.py:265] 74) loss = 0.138, grad_norm = 0.042
I0316 15:09:37.503150 140504863852288 logging_writer.py:48] [75] global_step=75, grad_norm=0.0617178, loss=0.141947
I0316 15:09:37.506872 140547343606976 submission.py:265] 75) loss = 0.142, grad_norm = 0.062
I0316 15:09:39.375165 140504872244992 logging_writer.py:48] [76] global_step=76, grad_norm=0.0915646, loss=0.145922
I0316 15:09:39.378716 140547343606976 submission.py:265] 76) loss = 0.146, grad_norm = 0.092
I0316 15:09:40.117223 140504863852288 logging_writer.py:48] [77] global_step=77, grad_norm=0.115988, loss=0.147707
I0316 15:09:40.120948 140547343606976 submission.py:265] 77) loss = 0.148, grad_norm = 0.116
I0316 15:09:41.714489 140504872244992 logging_writer.py:48] [78] global_step=78, grad_norm=0.141606, loss=0.146462
I0316 15:09:41.718112 140547343606976 submission.py:265] 78) loss = 0.146, grad_norm = 0.142
I0316 15:09:42.703681 140504863852288 logging_writer.py:48] [79] global_step=79, grad_norm=0.118234, loss=0.145897
I0316 15:09:42.707441 140547343606976 submission.py:265] 79) loss = 0.146, grad_norm = 0.118
I0316 15:09:44.288023 140504872244992 logging_writer.py:48] [80] global_step=80, grad_norm=0.0676106, loss=0.148934
I0316 15:09:44.291718 140547343606976 submission.py:265] 80) loss = 0.149, grad_norm = 0.068
I0316 15:09:45.212505 140504863852288 logging_writer.py:48] [81] global_step=81, grad_norm=0.0507063, loss=0.147648
I0316 15:09:45.216074 140547343606976 submission.py:265] 81) loss = 0.148, grad_norm = 0.051
I0316 15:09:47.102284 140504872244992 logging_writer.py:48] [82] global_step=82, grad_norm=0.0497255, loss=0.146797
I0316 15:09:47.105927 140547343606976 submission.py:265] 82) loss = 0.147, grad_norm = 0.050
I0316 15:09:47.837400 140504863852288 logging_writer.py:48] [83] global_step=83, grad_norm=0.0731009, loss=0.148505
I0316 15:09:47.841206 140547343606976 submission.py:265] 83) loss = 0.149, grad_norm = 0.073
I0316 15:09:49.389652 140504872244992 logging_writer.py:48] [84] global_step=84, grad_norm=0.0667798, loss=0.146466
I0316 15:09:49.393421 140547343606976 submission.py:265] 84) loss = 0.146, grad_norm = 0.067
I0316 15:09:50.483493 140504863852288 logging_writer.py:48] [85] global_step=85, grad_norm=0.06845, loss=0.147514
I0316 15:09:50.487166 140547343606976 submission.py:265] 85) loss = 0.148, grad_norm = 0.068
I0316 15:09:51.504937 140504872244992 logging_writer.py:48] [86] global_step=86, grad_norm=0.0509161, loss=0.146477
I0316 15:09:51.508495 140547343606976 submission.py:265] 86) loss = 0.146, grad_norm = 0.051
I0316 15:09:52.814175 140504863852288 logging_writer.py:48] [87] global_step=87, grad_norm=0.0425298, loss=0.145392
I0316 15:09:52.817698 140547343606976 submission.py:265] 87) loss = 0.145, grad_norm = 0.043
I0316 15:09:53.735347 140504872244992 logging_writer.py:48] [88] global_step=88, grad_norm=0.043571, loss=0.146701
I0316 15:09:53.738921 140547343606976 submission.py:265] 88) loss = 0.147, grad_norm = 0.044
I0316 15:09:55.721656 140504863852288 logging_writer.py:48] [89] global_step=89, grad_norm=0.0436494, loss=0.149025
I0316 15:09:55.725328 140547343606976 submission.py:265] 89) loss = 0.149, grad_norm = 0.044
I0316 15:09:56.822781 140504872244992 logging_writer.py:48] [90] global_step=90, grad_norm=0.0459132, loss=0.149257
I0316 15:09:56.826321 140547343606976 submission.py:265] 90) loss = 0.149, grad_norm = 0.046
I0316 15:09:58.233122 140504863852288 logging_writer.py:48] [91] global_step=91, grad_norm=0.0564249, loss=0.146408
I0316 15:09:58.236838 140547343606976 submission.py:265] 91) loss = 0.146, grad_norm = 0.056
I0316 15:09:59.772854 140504872244992 logging_writer.py:48] [92] global_step=92, grad_norm=0.0524471, loss=0.145525
I0316 15:09:59.776371 140547343606976 submission.py:265] 92) loss = 0.146, grad_norm = 0.052
I0316 15:10:00.314169 140504863852288 logging_writer.py:48] [93] global_step=93, grad_norm=0.04284, loss=0.144107
I0316 15:10:00.317745 140547343606976 submission.py:265] 93) loss = 0.144, grad_norm = 0.043
I0316 15:10:02.001477 140504872244992 logging_writer.py:48] [94] global_step=94, grad_norm=0.0293991, loss=0.144906
I0316 15:10:02.004977 140547343606976 submission.py:265] 94) loss = 0.145, grad_norm = 0.029
I0316 15:10:02.945141 140504863852288 logging_writer.py:48] [95] global_step=95, grad_norm=0.0333043, loss=0.144627
I0316 15:10:02.948530 140547343606976 submission.py:265] 95) loss = 0.145, grad_norm = 0.033
I0316 15:10:04.362742 140504872244992 logging_writer.py:48] [96] global_step=96, grad_norm=0.0232431, loss=0.141017
I0316 15:10:04.366330 140547343606976 submission.py:265] 96) loss = 0.141, grad_norm = 0.023
I0316 15:10:05.283932 140504863852288 logging_writer.py:48] [97] global_step=97, grad_norm=0.0436656, loss=0.144753
I0316 15:10:05.287610 140547343606976 submission.py:265] 97) loss = 0.145, grad_norm = 0.044
I0316 15:10:06.930866 140504872244992 logging_writer.py:48] [98] global_step=98, grad_norm=0.0242137, loss=0.142615
I0316 15:10:06.934677 140547343606976 submission.py:265] 98) loss = 0.143, grad_norm = 0.024
I0316 15:10:08.098258 140504863852288 logging_writer.py:48] [99] global_step=99, grad_norm=0.019712, loss=0.141636
I0316 15:10:08.101948 140547343606976 submission.py:265] 99) loss = 0.142, grad_norm = 0.020
I0316 15:10:09.049582 140504872244992 logging_writer.py:48] [100] global_step=100, grad_norm=0.0238907, loss=0.142549
I0316 15:10:09.053348 140547343606976 submission.py:265] 100) loss = 0.143, grad_norm = 0.024
I0316 15:10:35.671992 140547343606976 spec.py:321] Evaluating on the training split.
I0316 15:15:47.610074 140547343606976 spec.py:333] Evaluating on the validation split.
I0316 15:20:11.982863 140547343606976 spec.py:349] Evaluating on the test split.
I0316 15:25:14.233716 140547343606976 submission_runner.py:469] Time since start: 1946.94s, 	Step: 123, 	{'train/loss': 0.13341089352456362, 'validation/loss': 0.13326975551597872, 'validation/num_examples': 83274637, 'test/loss': 0.13571372937228554, 'test/num_examples': 95000000, 'score': 124.07613039016724, 'total_duration': 1946.936799287796, 'accumulated_submission_time': 124.07613039016724, 'accumulated_eval_time': 1821.5475096702576, 'accumulated_logging_time': 0.016313552856445312}
I0316 15:25:14.243494 140504863852288 logging_writer.py:48] [123] accumulated_eval_time=1821.55, accumulated_logging_time=0.0163136, accumulated_submission_time=124.076, global_step=123, preemption_count=0, score=124.076, test/loss=0.135714, test/num_examples=95000000, total_duration=1946.94, train/loss=0.133411, validation/loss=0.13327, validation/num_examples=83274637
I0316 15:27:15.700552 140547343606976 spec.py:321] Evaluating on the training split.
I0316 15:32:23.141344 140547343606976 spec.py:333] Evaluating on the validation split.
I0316 15:36:45.088455 140547343606976 spec.py:349] Evaluating on the test split.
I0316 15:41:45.316007 140547343606976 submission_runner.py:469] Time since start: 2938.02s, 	Step: 243, 	{'train/loss': 0.1269283348552171, 'validation/loss': 0.12901506017080552, 'validation/num_examples': 83274637, 'test/loss': 0.13145769815701935, 'test/num_examples': 95000000, 'score': 244.66989922523499, 'total_duration': 2938.0191299915314, 'accumulated_submission_time': 244.66989922523499, 'accumulated_eval_time': 2691.1630423069, 'accumulated_logging_time': 0.033066511154174805}
I0316 15:41:45.325216 140504872244992 logging_writer.py:48] [243] accumulated_eval_time=2691.16, accumulated_logging_time=0.0330665, accumulated_submission_time=244.67, global_step=243, preemption_count=0, score=244.67, test/loss=0.131458, test/num_examples=95000000, total_duration=2938.02, train/loss=0.126928, validation/loss=0.129015, validation/num_examples=83274637
/usr/local/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
I0316 15:43:46.472352 140547343606976 spec.py:321] Evaluating on the training split.
I0316 15:48:51.093572 140547343606976 spec.py:333] Evaluating on the validation split.
I0316 15:53:09.996301 140547343606976 spec.py:349] Evaluating on the test split.
I0316 15:58:06.711014 140547343606976 submission_runner.py:469] Time since start: 3919.41s, 	Step: 367, 	{'train/loss': 0.1277643667687718, 'validation/loss': 0.1276116532155123, 'validation/num_examples': 83274637, 'test/loss': 0.12997279920413368, 'test/num_examples': 95000000, 'score': 364.9590401649475, 'total_duration': 3919.4140877723694, 'accumulated_submission_time': 364.9590401649475, 'accumulated_eval_time': 3551.4018607139587, 'accumulated_logging_time': 0.0490565299987793}
I0316 15:58:06.721670 140504863852288 logging_writer.py:48] [367] accumulated_eval_time=3551.4, accumulated_logging_time=0.0490565, accumulated_submission_time=364.959, global_step=367, preemption_count=0, score=364.959, test/loss=0.129973, test/num_examples=95000000, total_duration=3919.41, train/loss=0.127764, validation/loss=0.127612, validation/num_examples=83274637
I0316 16:00:07.366732 140547343606976 spec.py:321] Evaluating on the training split.
I0316 16:05:12.033283 140547343606976 spec.py:333] Evaluating on the validation split.
I0316 16:09:26.730553 140547343606976 spec.py:349] Evaluating on the test split.
I0316 16:14:17.461935 140547343606976 submission_runner.py:469] Time since start: 4890.17s, 	Step: 490, 	{'train/loss': 0.1287535887553089, 'validation/loss': 0.12709300201926577, 'validation/num_examples': 83274637, 'test/loss': 0.12948077687434145, 'test/num_examples': 95000000, 'score': 484.687130689621, 'total_duration': 4890.165053129196, 'accumulated_submission_time': 484.687130689621, 'accumulated_eval_time': 4401.497174024582, 'accumulated_logging_time': 0.06656885147094727}
I0316 16:14:17.471702 140504872244992 logging_writer.py:48] [490] accumulated_eval_time=4401.5, accumulated_logging_time=0.0665689, accumulated_submission_time=484.687, global_step=490, preemption_count=0, score=484.687, test/loss=0.129481, test/num_examples=95000000, total_duration=4890.17, train/loss=0.128754, validation/loss=0.127093, validation/num_examples=83274637
I0316 16:14:20.094050 140504863852288 logging_writer.py:48] [500] global_step=500, grad_norm=0.0495851, loss=0.125322
I0316 16:14:20.098363 140547343606976 submission.py:265] 500) loss = 0.125, grad_norm = 0.050
I0316 16:16:18.828152 140547343606976 spec.py:321] Evaluating on the training split.
I0316 16:21:33.336393 140547343606976 spec.py:333] Evaluating on the validation split.
I0316 16:25:49.070355 140547343606976 spec.py:349] Evaluating on the test split.
I0316 16:30:43.337143 140547343606976 submission_runner.py:469] Time since start: 5876.04s, 	Step: 619, 	{'train/loss': 0.12722629356686668, 'validation/loss': 0.1269996693307299, 'validation/num_examples': 83274637, 'test/loss': 0.12964734526539853, 'test/num_examples': 95000000, 'score': 605.1324234008789, 'total_duration': 5876.039811134338, 'accumulated_submission_time': 605.1324234008789, 'accumulated_eval_time': 5266.00591635704, 'accumulated_logging_time': 0.10360264778137207}
I0316 16:30:43.346519 140504872244992 logging_writer.py:48] [619] accumulated_eval_time=5266.01, accumulated_logging_time=0.103603, accumulated_submission_time=605.132, global_step=619, preemption_count=0, score=605.132, test/loss=0.129647, test/num_examples=95000000, total_duration=5876.04, train/loss=0.127226, validation/loss=0.127, validation/num_examples=83274637
I0316 16:32:43.959937 140547343606976 spec.py:321] Evaluating on the training split.
I0316 16:37:41.064181 140547343606976 spec.py:333] Evaluating on the validation split.
I0316 16:41:57.392337 140547343606976 spec.py:349] Evaluating on the test split.
I0316 16:46:50.469460 140547343606976 submission_runner.py:469] Time since start: 6843.17s, 	Step: 739, 	{'train/loss': 0.12406153249526486, 'validation/loss': 0.1266881260307427, 'validation/num_examples': 83274637, 'test/loss': 0.12918211434719687, 'test/num_examples': 95000000, 'score': 724.8710491657257, 'total_duration': 6843.17255115509, 'accumulated_submission_time': 724.8710491657257, 'accumulated_eval_time': 6112.515592813492, 'accumulated_logging_time': 0.12002348899841309}
I0316 16:46:50.480009 140504863852288 logging_writer.py:48] [739] accumulated_eval_time=6112.52, accumulated_logging_time=0.120023, accumulated_submission_time=724.871, global_step=739, preemption_count=0, score=724.871, test/loss=0.129182, test/num_examples=95000000, total_duration=6843.17, train/loss=0.124062, validation/loss=0.126688, validation/num_examples=83274637
I0316 16:48:51.860400 140547343606976 spec.py:321] Evaluating on the training split.
I0316 16:53:51.022573 140547343606976 spec.py:333] Evaluating on the validation split.
I0316 16:58:07.335149 140547343606976 spec.py:349] Evaluating on the test split.
I0316 17:02:58.686738 140547343606976 submission_runner.py:469] Time since start: 7811.39s, 	Step: 866, 	{'train/loss': 0.12563989459042818, 'validation/loss': 0.12720733865937903, 'validation/num_examples': 83274637, 'test/loss': 0.12965291715838784, 'test/num_examples': 95000000, 'score': 845.3385343551636, 'total_duration': 7811.389776229858, 'accumulated_submission_time': 845.3385343551636, 'accumulated_eval_time': 6959.3419642448425, 'accumulated_logging_time': 0.1382277011871338}
I0316 17:02:58.717712 140504872244992 logging_writer.py:48] [866] accumulated_eval_time=6959.34, accumulated_logging_time=0.138228, accumulated_submission_time=845.339, global_step=866, preemption_count=0, score=845.339, test/loss=0.129653, test/num_examples=95000000, total_duration=7811.39, train/loss=0.12564, validation/loss=0.127207, validation/num_examples=83274637
I0316 17:04:59.286807 140547343606976 spec.py:321] Evaluating on the training split.
I0316 17:10:10.745771 140547343606976 spec.py:333] Evaluating on the validation split.
I0316 17:14:25.970322 140547343606976 spec.py:349] Evaluating on the test split.
I0316 17:19:21.040019 140547343606976 submission_runner.py:469] Time since start: 8793.74s, 	Step: 987, 	{'train/loss': 0.12606355280106213, 'validation/loss': 0.12634416380811228, 'validation/num_examples': 83274637, 'test/loss': 0.12862187480693615, 'test/num_examples': 95000000, 'score': 965.0354306697845, 'total_duration': 8793.743139266968, 'accumulated_submission_time': 965.0354306697845, 'accumulated_eval_time': 7821.095346450806, 'accumulated_logging_time': 0.17729401588439941}
I0316 17:19:21.050509 140504863852288 logging_writer.py:48] [987] accumulated_eval_time=7821.1, accumulated_logging_time=0.177294, accumulated_submission_time=965.035, global_step=987, preemption_count=0, score=965.035, test/loss=0.128622, test/num_examples=95000000, total_duration=8793.74, train/loss=0.126064, validation/loss=0.126344, validation/num_examples=83274637
I0316 17:19:24.183729 140504872244992 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.0489419, loss=0.125201
I0316 17:19:24.187472 140547343606976 submission.py:265] 1000) loss = 0.125, grad_norm = 0.049
I0316 17:21:21.568403 140547343606976 spec.py:321] Evaluating on the training split.
I0316 17:26:11.604580 140547343606976 spec.py:333] Evaluating on the validation split.
I0316 17:30:12.943839 140547343606976 spec.py:349] Evaluating on the test split.
I0316 17:34:55.762137 140547343606976 submission_runner.py:469] Time since start: 9728.47s, 	Step: 1112, 	{'train/loss': 0.12306550925835005, 'validation/loss': 0.12612130971834223, 'validation/num_examples': 83274637, 'test/loss': 0.12866027056920906, 'test/num_examples': 95000000, 'score': 1084.6656374931335, 'total_duration': 9728.465208768845, 'accumulated_submission_time': 1084.6656374931335, 'accumulated_eval_time': 8635.289261102676, 'accumulated_logging_time': 0.1951770782470703}
I0316 17:34:55.771969 140504863852288 logging_writer.py:48] [1112] accumulated_eval_time=8635.29, accumulated_logging_time=0.195177, accumulated_submission_time=1084.67, global_step=1112, preemption_count=0, score=1084.67, test/loss=0.12866, test/num_examples=95000000, total_duration=9728.47, train/loss=0.123066, validation/loss=0.126121, validation/num_examples=83274637
I0316 17:36:56.804754 140547343606976 spec.py:321] Evaluating on the training split.
I0316 17:41:11.120694 140547343606976 spec.py:333] Evaluating on the validation split.
I0316 17:44:52.392451 140547343606976 spec.py:349] Evaluating on the test split.
I0316 17:49:16.406016 140547343606976 submission_runner.py:469] Time since start: 10589.11s, 	Step: 1236, 	{'train/loss': 0.12568337635088403, 'validation/loss': 0.12681705435823598, 'validation/num_examples': 83274637, 'test/loss': 0.1293259257223029, 'test/num_examples': 95000000, 'score': 1204.7923319339752, 'total_duration': 10589.109101057053, 'accumulated_submission_time': 1204.7923319339752, 'accumulated_eval_time': 9374.890599489212, 'accumulated_logging_time': 0.2192680835723877}
I0316 17:49:16.417699 140504872244992 logging_writer.py:48] [1236] accumulated_eval_time=9374.89, accumulated_logging_time=0.219268, accumulated_submission_time=1204.79, global_step=1236, preemption_count=0, score=1204.79, test/loss=0.129326, test/num_examples=95000000, total_duration=10589.1, train/loss=0.125683, validation/loss=0.126817, validation/num_examples=83274637
I0316 17:51:17.631389 140547343606976 spec.py:321] Evaluating on the training split.
I0316 17:54:30.572775 140547343606976 spec.py:333] Evaluating on the validation split.
I0316 17:57:24.405319 140547343606976 spec.py:349] Evaluating on the test split.
I0316 18:01:06.125357 140547343606976 submission_runner.py:469] Time since start: 11298.83s, 	Step: 1359, 	{'train/loss': 0.12683596488313, 'validation/loss': 0.12613078720027524, 'validation/num_examples': 83274637, 'test/loss': 0.12857943969485633, 'test/num_examples': 95000000, 'score': 1325.1372377872467, 'total_duration': 11298.828404426575, 'accumulated_submission_time': 1325.1372377872467, 'accumulated_eval_time': 9963.38474059105, 'accumulated_logging_time': 0.23853278160095215}
I0316 18:01:06.135666 140504863852288 logging_writer.py:48] [1359] accumulated_eval_time=9963.38, accumulated_logging_time=0.238533, accumulated_submission_time=1325.14, global_step=1359, preemption_count=0, score=1325.14, test/loss=0.128579, test/num_examples=95000000, total_duration=11298.8, train/loss=0.126836, validation/loss=0.126131, validation/num_examples=83274637
I0316 18:03:06.678048 140547343606976 spec.py:321] Evaluating on the training split.
I0316 18:05:13.612488 140547343606976 spec.py:333] Evaluating on the validation split.
I0316 18:07:18.310863 140547343606976 spec.py:349] Evaluating on the test split.
I0316 18:10:06.055292 140547343606976 submission_runner.py:469] Time since start: 11838.76s, 	Step: 1480, 	{'train/loss': 0.12360922061181721, 'validation/loss': 0.12607019025036834, 'validation/num_examples': 83274637, 'test/loss': 0.1287034720681843, 'test/num_examples': 95000000, 'score': 1444.8060755729675, 'total_duration': 11838.758248090744, 'accumulated_submission_time': 1444.8060755729675, 'accumulated_eval_time': 10382.76194190979, 'accumulated_logging_time': 0.2553706169128418}
I0316 18:10:06.065773 140504872244992 logging_writer.py:48] [1480] accumulated_eval_time=10382.8, accumulated_logging_time=0.255371, accumulated_submission_time=1444.81, global_step=1480, preemption_count=0, score=1444.81, test/loss=0.128703, test/num_examples=95000000, total_duration=11838.8, train/loss=0.123609, validation/loss=0.12607, validation/num_examples=83274637
I0316 18:10:10.534160 140504863852288 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.0115255, loss=0.125972
I0316 18:10:10.537729 140547343606976 submission.py:265] 1500) loss = 0.126, grad_norm = 0.012
I0316 18:12:06.998665 140547343606976 spec.py:321] Evaluating on the training split.
I0316 18:14:11.360930 140547343606976 spec.py:333] Evaluating on the validation split.
I0316 18:16:15.394558 140547343606976 spec.py:349] Evaluating on the test split.
I0316 18:18:38.768178 140547343606976 submission_runner.py:469] Time since start: 12351.47s, 	Step: 1603, 	{'train/loss': 0.12548421004431817, 'validation/loss': 0.12641472428031095, 'validation/num_examples': 83274637, 'test/loss': 0.12864747096372905, 'test/num_examples': 95000000, 'score': 1564.8674092292786, 'total_duration': 12351.471280813217, 'accumulated_submission_time': 1564.8674092292786, 'accumulated_eval_time': 10774.531562805176, 'accumulated_logging_time': 0.2729027271270752}
I0316 18:18:38.779419 140504872244992 logging_writer.py:48] [1603] accumulated_eval_time=10774.5, accumulated_logging_time=0.272903, accumulated_submission_time=1564.87, global_step=1603, preemption_count=0, score=1564.87, test/loss=0.128647, test/num_examples=95000000, total_duration=12351.5, train/loss=0.125484, validation/loss=0.126415, validation/num_examples=83274637
I0316 18:20:40.665018 140547343606976 spec.py:321] Evaluating on the training split.
I0316 18:22:44.378763 140547343606976 spec.py:333] Evaluating on the validation split.
I0316 18:24:48.657237 140547343606976 spec.py:349] Evaluating on the test split.
I0316 18:27:10.707901 140547343606976 submission_runner.py:469] Time since start: 12863.41s, 	Step: 1724, 	{'train/loss': 0.12395863624309053, 'validation/loss': 0.1257001094930213, 'validation/num_examples': 83274637, 'test/loss': 0.12816850327309057, 'test/num_examples': 95000000, 'score': 1685.808421611786, 'total_duration': 12863.411012172699, 'accumulated_submission_time': 1685.808421611786, 'accumulated_eval_time': 11164.574544668198, 'accumulated_logging_time': 0.31430673599243164}
I0316 18:27:10.719029 140504863852288 logging_writer.py:48] [1724] accumulated_eval_time=11164.6, accumulated_logging_time=0.314307, accumulated_submission_time=1685.81, global_step=1724, preemption_count=0, score=1685.81, test/loss=0.128169, test/num_examples=95000000, total_duration=12863.4, train/loss=0.123959, validation/loss=0.1257, validation/num_examples=83274637
I0316 18:29:11.794916 140547343606976 spec.py:321] Evaluating on the training split.
I0316 18:31:15.552419 140547343606976 spec.py:333] Evaluating on the validation split.
I0316 18:33:19.351554 140547343606976 spec.py:349] Evaluating on the test split.
I0316 18:35:41.953849 140547343606976 submission_runner.py:469] Time since start: 13374.66s, 	Step: 1845, 	{'train/loss': 0.1239956716011778, 'validation/loss': 0.12575150284966533, 'validation/num_examples': 83274637, 'test/loss': 0.12821377153966804, 'test/num_examples': 95000000, 'score': 1805.9931104183197, 'total_duration': 13374.65697145462, 'accumulated_submission_time': 1805.9931104183197, 'accumulated_eval_time': 11554.733649015427, 'accumulated_logging_time': 0.33339762687683105}
I0316 18:35:41.965363 140504872244992 logging_writer.py:48] [1845] accumulated_eval_time=11554.7, accumulated_logging_time=0.333398, accumulated_submission_time=1805.99, global_step=1845, preemption_count=0, score=1805.99, test/loss=0.128214, test/num_examples=95000000, total_duration=13374.7, train/loss=0.123996, validation/loss=0.125752, validation/num_examples=83274637
I0316 18:37:42.726852 140547343606976 spec.py:321] Evaluating on the training split.
I0316 18:39:46.051439 140547343606976 spec.py:333] Evaluating on the validation split.
I0316 18:41:50.383095 140547343606976 spec.py:349] Evaluating on the test split.
I0316 18:44:13.184505 140547343606976 submission_runner.py:469] Time since start: 13885.89s, 	Step: 1965, 	{'train/loss': 0.12699316400054717, 'validation/loss': 0.12514760503964595, 'validation/num_examples': 83274637, 'test/loss': 0.1275636140686035, 'test/num_examples': 95000000, 'score': 1925.828624010086, 'total_duration': 13885.887619018555, 'accumulated_submission_time': 1925.828624010086, 'accumulated_eval_time': 11945.19139456749, 'accumulated_logging_time': 0.35192060470581055}
I0316 18:44:13.210798 140504863852288 logging_writer.py:48] [1965] accumulated_eval_time=11945.2, accumulated_logging_time=0.351921, accumulated_submission_time=1925.83, global_step=1965, preemption_count=0, score=1925.83, test/loss=0.127564, test/num_examples=95000000, total_duration=13885.9, train/loss=0.126993, validation/loss=0.125148, validation/num_examples=83274637
I0316 18:44:28.800400 140504872244992 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.045576, loss=0.119802
I0316 18:44:28.803952 140547343606976 submission.py:265] 2000) loss = 0.120, grad_norm = 0.046
I0316 18:46:14.725948 140547343606976 spec.py:321] Evaluating on the training split.
I0316 18:48:18.342901 140547343606976 spec.py:333] Evaluating on the validation split.
I0316 18:50:22.657064 140547343606976 spec.py:349] Evaluating on the test split.
I0316 18:52:45.332803 140547343606976 submission_runner.py:469] Time since start: 14398.04s, 	Step: 2083, 	{'train/loss': 0.12737041671049293, 'validation/loss': 0.12563726118326193, 'validation/num_examples': 83274637, 'test/loss': 0.12811624996727392, 'test/num_examples': 95000000, 'score': 2046.4323828220367, 'total_duration': 14398.035952806473, 'accumulated_submission_time': 2046.4323828220367, 'accumulated_eval_time': 12335.79840350151, 'accumulated_logging_time': 0.3870232105255127}
I0316 18:52:45.343507 140504863852288 logging_writer.py:48] [2083] accumulated_eval_time=12335.8, accumulated_logging_time=0.387023, accumulated_submission_time=2046.43, global_step=2083, preemption_count=0, score=2046.43, test/loss=0.128116, test/num_examples=95000000, total_duration=14398, train/loss=0.12737, validation/loss=0.125637, validation/num_examples=83274637
I0316 18:54:46.358589 140547343606976 spec.py:321] Evaluating on the training split.
I0316 18:56:49.978431 140547343606976 spec.py:333] Evaluating on the validation split.
I0316 18:58:53.612798 140547343606976 spec.py:349] Evaluating on the test split.
I0316 19:01:16.726825 140547343606976 submission_runner.py:469] Time since start: 14909.43s, 	Step: 2204, 	{'train/loss': 0.12286555980834776, 'validation/loss': 0.12588682705844823, 'validation/num_examples': 83274637, 'test/loss': 0.12843282979254472, 'test/num_examples': 95000000, 'score': 2166.552446126938, 'total_duration': 14909.4299659729, 'accumulated_submission_time': 2166.552446126938, 'accumulated_eval_time': 12726.166886806488, 'accumulated_logging_time': 0.41201138496398926}
I0316 19:01:16.738659 140504872244992 logging_writer.py:48] [2204] accumulated_eval_time=12726.2, accumulated_logging_time=0.412011, accumulated_submission_time=2166.55, global_step=2204, preemption_count=0, score=2166.55, test/loss=0.128433, test/num_examples=95000000, total_duration=14909.4, train/loss=0.122866, validation/loss=0.125887, validation/num_examples=83274637
I0316 19:03:17.289745 140547343606976 spec.py:321] Evaluating on the training split.
I0316 19:05:21.019798 140547343606976 spec.py:333] Evaluating on the validation split.
I0316 19:07:24.844343 140547343606976 spec.py:349] Evaluating on the test split.
I0316 19:09:47.306656 140547343606976 submission_runner.py:469] Time since start: 15420.01s, 	Step: 2328, 	{'train/loss': 0.12363505133954629, 'validation/loss': 0.12519407248759445, 'validation/num_examples': 83274637, 'test/loss': 0.12766449967338162, 'test/num_examples': 95000000, 'score': 2286.2150740623474, 'total_duration': 15420.009753465652, 'accumulated_submission_time': 2286.2150740623474, 'accumulated_eval_time': 13116.183973550797, 'accumulated_logging_time': 0.4317514896392822}
I0316 19:09:47.317686 140504863852288 logging_writer.py:48] [2328] accumulated_eval_time=13116.2, accumulated_logging_time=0.431751, accumulated_submission_time=2286.22, global_step=2328, preemption_count=0, score=2286.22, test/loss=0.127664, test/num_examples=95000000, total_duration=15420, train/loss=0.123635, validation/loss=0.125194, validation/num_examples=83274637
I0316 19:11:48.732094 140547343606976 spec.py:321] Evaluating on the training split.
I0316 19:13:52.277704 140547343606976 spec.py:333] Evaluating on the validation split.
I0316 19:15:56.230822 140547343606976 spec.py:349] Evaluating on the test split.
I0316 19:18:19.202524 140547343606976 submission_runner.py:469] Time since start: 15931.91s, 	Step: 2451, 	{'train/loss': 0.12398902659307921, 'validation/loss': 0.12512786840125564, 'validation/num_examples': 83274637, 'test/loss': 0.12757607830657958, 'test/num_examples': 95000000, 'score': 2406.7126801013947, 'total_duration': 15931.905608654022, 'accumulated_submission_time': 2406.7126801013947, 'accumulated_eval_time': 13506.654529094696, 'accumulated_logging_time': 0.4497056007385254}
I0316 19:18:19.214075 140504872244992 logging_writer.py:48] [2451] accumulated_eval_time=13506.7, accumulated_logging_time=0.449706, accumulated_submission_time=2406.71, global_step=2451, preemption_count=0, score=2406.71, test/loss=0.127576, test/num_examples=95000000, total_duration=15931.9, train/loss=0.123989, validation/loss=0.125128, validation/num_examples=83274637
I0316 19:18:49.151306 140504863852288 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.00671211, loss=0.117756
I0316 19:18:49.155174 140547343606976 submission.py:265] 2500) loss = 0.118, grad_norm = 0.007
I0316 19:20:19.748602 140547343606976 spec.py:321] Evaluating on the training split.
I0316 19:22:23.257480 140547343606976 spec.py:333] Evaluating on the validation split.
I0316 19:24:26.669770 140547343606976 spec.py:349] Evaluating on the test split.
I0316 19:26:49.635798 140547343606976 submission_runner.py:469] Time since start: 16442.34s, 	Step: 2574, 	{'train/loss': 0.12235326196417724, 'validation/loss': 0.1251100243813887, 'validation/num_examples': 83274637, 'test/loss': 0.12749482891074732, 'test/num_examples': 95000000, 'score': 2526.345806837082, 'total_duration': 16442.338914632797, 'accumulated_submission_time': 2526.345806837082, 'accumulated_eval_time': 13896.541800737381, 'accumulated_logging_time': 0.5222728252410889}
I0316 19:26:49.647338 140504872244992 logging_writer.py:48] [2574] accumulated_eval_time=13896.5, accumulated_logging_time=0.522273, accumulated_submission_time=2526.35, global_step=2574, preemption_count=0, score=2526.35, test/loss=0.127495, test/num_examples=95000000, total_duration=16442.3, train/loss=0.122353, validation/loss=0.12511, validation/num_examples=83274637
I0316 19:28:50.733557 140547343606976 spec.py:321] Evaluating on the training split.
I0316 19:30:54.380488 140547343606976 spec.py:333] Evaluating on the validation split.
I0316 19:32:58.523930 140547343606976 spec.py:349] Evaluating on the test split.
I0316 19:35:21.205699 140547343606976 submission_runner.py:469] Time since start: 16953.91s, 	Step: 2695, 	{'train/loss': 0.12528469822518476, 'validation/loss': 0.12507892899566195, 'validation/num_examples': 83274637, 'test/loss': 0.1276123143187272, 'test/num_examples': 95000000, 'score': 2646.5343613624573, 'total_duration': 16953.908809661865, 'accumulated_submission_time': 2646.5343613624573, 'accumulated_eval_time': 14287.014113903046, 'accumulated_logging_time': 0.5414602756500244}
I0316 19:35:21.217038 140504863852288 logging_writer.py:48] [2695] accumulated_eval_time=14287, accumulated_logging_time=0.54146, accumulated_submission_time=2646.53, global_step=2695, preemption_count=0, score=2646.53, test/loss=0.127612, test/num_examples=95000000, total_duration=16953.9, train/loss=0.125285, validation/loss=0.125079, validation/num_examples=83274637
I0316 19:37:22.157700 140547343606976 spec.py:321] Evaluating on the training split.
I0316 19:39:25.995407 140547343606976 spec.py:333] Evaluating on the validation split.
I0316 19:41:29.248734 140547343606976 spec.py:349] Evaluating on the test split.
I0316 19:43:51.953684 140547343606976 submission_runner.py:469] Time since start: 17464.66s, 	Step: 2819, 	{'train/loss': 0.1245684371476056, 'validation/loss': 0.12516579796142246, 'validation/num_examples': 83274637, 'test/loss': 0.12747068378858065, 'test/num_examples': 95000000, 'score': 2766.590095281601, 'total_duration': 17464.656788110733, 'accumulated_submission_time': 2766.590095281601, 'accumulated_eval_time': 14676.810285806656, 'accumulated_logging_time': 0.5613434314727783}
I0316 19:43:51.964923 140504872244992 logging_writer.py:48] [2819] accumulated_eval_time=14676.8, accumulated_logging_time=0.561343, accumulated_submission_time=2766.59, global_step=2819, preemption_count=0, score=2766.59, test/loss=0.127471, test/num_examples=95000000, total_duration=17464.7, train/loss=0.124568, validation/loss=0.125166, validation/num_examples=83274637
I0316 19:45:53.128483 140547343606976 spec.py:321] Evaluating on the training split.
I0316 19:47:56.270141 140547343606976 spec.py:333] Evaluating on the validation split.
I0316 19:49:58.818089 140547343606976 spec.py:349] Evaluating on the test split.
I0316 19:52:21.002846 140547343606976 submission_runner.py:469] Time since start: 17973.71s, 	Step: 2939, 	{'train/loss': 0.12316614993066094, 'validation/loss': 0.1247863788215217, 'validation/num_examples': 83274637, 'test/loss': 0.12726495481475028, 'test/num_examples': 95000000, 'score': 2886.8806285858154, 'total_duration': 17973.705956697464, 'accumulated_submission_time': 2886.8806285858154, 'accumulated_eval_time': 15064.684843540192, 'accumulated_logging_time': 0.5814967155456543}
I0316 19:52:21.015144 140504863852288 logging_writer.py:48] [2939] accumulated_eval_time=15064.7, accumulated_logging_time=0.581497, accumulated_submission_time=2886.88, global_step=2939, preemption_count=0, score=2886.88, test/loss=0.127265, test/num_examples=95000000, total_duration=17973.7, train/loss=0.123166, validation/loss=0.124786, validation/num_examples=83274637
I0316 19:53:12.999423 140504872244992 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.0082126, loss=0.123695
I0316 19:53:13.002833 140547343606976 submission.py:265] 3000) loss = 0.124, grad_norm = 0.008
I0316 19:54:21.751166 140547343606976 spec.py:321] Evaluating on the training split.
I0316 19:56:25.388373 140547343606976 spec.py:333] Evaluating on the validation split.
I0316 19:58:28.649262 140547343606976 spec.py:349] Evaluating on the test split.
I0316 20:00:50.852253 140547343606976 submission_runner.py:469] Time since start: 18483.56s, 	Step: 3057, 	{'train/loss': 0.12297670783870987, 'validation/loss': 0.12524698528305567, 'validation/num_examples': 83274637, 'test/loss': 0.12784091562291194, 'test/num_examples': 95000000, 'score': 3006.7930765151978, 'total_duration': 18483.55531978607, 'accumulated_submission_time': 3006.7930765151978, 'accumulated_eval_time': 15453.785985708237, 'accumulated_logging_time': 0.6009373664855957}
I0316 20:00:50.862936 140504863852288 logging_writer.py:48] [3057] accumulated_eval_time=15453.8, accumulated_logging_time=0.600937, accumulated_submission_time=3006.79, global_step=3057, preemption_count=0, score=3006.79, test/loss=0.127841, test/num_examples=95000000, total_duration=18483.6, train/loss=0.122977, validation/loss=0.125247, validation/num_examples=83274637
I0316 20:02:52.682260 140547343606976 spec.py:321] Evaluating on the training split.
I0316 20:04:56.898401 140547343606976 spec.py:333] Evaluating on the validation split.
I0316 20:07:01.664911 140547343606976 spec.py:349] Evaluating on the test split.
I0316 20:09:24.791235 140547343606976 submission_runner.py:469] Time since start: 18997.49s, 	Step: 3175, 	{'train/loss': 0.12450858660600383, 'validation/loss': 0.1248025579606663, 'validation/num_examples': 83274637, 'test/loss': 0.1272420657250254, 'test/num_examples': 95000000, 'score': 3127.7385132312775, 'total_duration': 18997.494317770004, 'accumulated_submission_time': 3127.7385132312775, 'accumulated_eval_time': 15845.895000696182, 'accumulated_logging_time': 0.6293764114379883}
I0316 20:09:24.802201 140504872244992 logging_writer.py:48] [3175] accumulated_eval_time=15845.9, accumulated_logging_time=0.629376, accumulated_submission_time=3127.74, global_step=3175, preemption_count=0, score=3127.74, test/loss=0.127242, test/num_examples=95000000, total_duration=18997.5, train/loss=0.124509, validation/loss=0.124803, validation/num_examples=83274637
I0316 20:11:26.443598 140547343606976 spec.py:321] Evaluating on the training split.
I0316 20:13:29.781029 140547343606976 spec.py:333] Evaluating on the validation split.
I0316 20:15:33.330488 140547343606976 spec.py:349] Evaluating on the test split.
I0316 20:17:55.784262 140547343606976 submission_runner.py:469] Time since start: 19508.49s, 	Step: 3298, 	{'train/loss': 0.12479026925953637, 'validation/loss': 0.12500996992148292, 'validation/num_examples': 83274637, 'test/loss': 0.12734507960951955, 'test/num_examples': 95000000, 'score': 3248.4884819984436, 'total_duration': 19508.487350702286, 'accumulated_submission_time': 3248.4884819984436, 'accumulated_eval_time': 16235.235828399658, 'accumulated_logging_time': 0.6473820209503174}
I0316 20:17:55.796487 140504863852288 logging_writer.py:48] [3298] accumulated_eval_time=16235.2, accumulated_logging_time=0.647382, accumulated_submission_time=3248.49, global_step=3298, preemption_count=0, score=3248.49, test/loss=0.127345, test/num_examples=95000000, total_duration=19508.5, train/loss=0.12479, validation/loss=0.12501, validation/num_examples=83274637
I0316 20:19:57.056135 140547343606976 spec.py:321] Evaluating on the training split.
I0316 20:22:01.200490 140547343606976 spec.py:333] Evaluating on the validation split.
I0316 20:24:05.107244 140547343606976 spec.py:349] Evaluating on the test split.
I0316 20:26:27.501780 140547343606976 submission_runner.py:469] Time since start: 20020.20s, 	Step: 3416, 	{'train/loss': 0.12294937778121737, 'validation/loss': 0.12507050685223203, 'validation/num_examples': 83274637, 'test/loss': 0.12756497858312507, 'test/num_examples': 95000000, 'score': 3368.837310552597, 'total_duration': 20020.204884767532, 'accumulated_submission_time': 3368.837310552597, 'accumulated_eval_time': 16625.681668281555, 'accumulated_logging_time': 0.6676650047302246}
I0316 20:26:27.513734 140504872244992 logging_writer.py:48] [3416] accumulated_eval_time=16625.7, accumulated_logging_time=0.667665, accumulated_submission_time=3368.84, global_step=3416, preemption_count=0, score=3368.84, test/loss=0.127565, test/num_examples=95000000, total_duration=20020.2, train/loss=0.122949, validation/loss=0.125071, validation/num_examples=83274637
I0316 20:27:44.290511 140504863852288 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.0230824, loss=0.128759
I0316 20:27:44.294404 140547343606976 submission.py:265] 3500) loss = 0.129, grad_norm = 0.023
I0316 20:28:29.512061 140547343606976 spec.py:321] Evaluating on the training split.
I0316 20:30:33.377541 140547343606976 spec.py:333] Evaluating on the validation split.
I0316 20:32:37.326388 140547343606976 spec.py:349] Evaluating on the test split.
I0316 20:34:59.788952 140547343606976 submission_runner.py:469] Time since start: 20532.49s, 	Step: 3536, 	{'train/loss': 0.12578364714127954, 'validation/loss': 0.12473236696724861, 'validation/num_examples': 83274637, 'test/loss': 0.1271822725105687, 'test/num_examples': 95000000, 'score': 3489.975656270981, 'total_duration': 20532.49209332466, 'accumulated_submission_time': 3489.975656270981, 'accumulated_eval_time': 17015.958689451218, 'accumulated_logging_time': 0.6872437000274658}
I0316 20:34:59.800641 140504872244992 logging_writer.py:48] [3536] accumulated_eval_time=17016, accumulated_logging_time=0.687244, accumulated_submission_time=3489.98, global_step=3536, preemption_count=0, score=3489.98, test/loss=0.127182, test/num_examples=95000000, total_duration=20532.5, train/loss=0.125784, validation/loss=0.124732, validation/num_examples=83274637
I0316 20:37:00.750787 140547343606976 spec.py:321] Evaluating on the training split.
I0316 20:39:04.442252 140547343606976 spec.py:333] Evaluating on the validation split.
I0316 20:41:08.364400 140547343606976 spec.py:349] Evaluating on the test split.
I0316 20:43:30.286900 140547343606976 submission_runner.py:469] Time since start: 21042.99s, 	Step: 3657, 	{'train/loss': 0.12345025228410109, 'validation/loss': 0.12471244156769891, 'validation/num_examples': 83274637, 'test/loss': 0.12710413816006308, 'test/num_examples': 95000000, 'score': 3610.0101730823517, 'total_duration': 21042.989954948425, 'accumulated_submission_time': 3610.0101730823517, 'accumulated_eval_time': 17405.494908571243, 'accumulated_logging_time': 0.7360546588897705}
I0316 20:43:30.299297 140504863852288 logging_writer.py:48] [3657] accumulated_eval_time=17405.5, accumulated_logging_time=0.736055, accumulated_submission_time=3610.01, global_step=3657, preemption_count=0, score=3610.01, test/loss=0.127104, test/num_examples=95000000, total_duration=21043, train/loss=0.12345, validation/loss=0.124712, validation/num_examples=83274637
I0316 20:45:30.826166 140547343606976 spec.py:321] Evaluating on the training split.
I0316 20:47:34.596083 140547343606976 spec.py:333] Evaluating on the validation split.
I0316 20:49:38.360560 140547343606976 spec.py:349] Evaluating on the test split.
I0316 20:52:01.410815 140547343606976 submission_runner.py:469] Time since start: 21554.11s, 	Step: 3777, 	{'train/loss': 0.12337209427709644, 'validation/loss': 0.12460818865511962, 'validation/num_examples': 83274637, 'test/loss': 0.1269813379375257, 'test/num_examples': 95000000, 'score': 3729.6726961135864, 'total_duration': 21554.1139087677, 'accumulated_submission_time': 3729.6726961135864, 'accumulated_eval_time': 17796.079731464386, 'accumulated_logging_time': 0.755990743637085}
I0316 20:52:01.422798 140504872244992 logging_writer.py:48] [3777] accumulated_eval_time=17796.1, accumulated_logging_time=0.755991, accumulated_submission_time=3729.67, global_step=3777, preemption_count=0, score=3729.67, test/loss=0.126981, test/num_examples=95000000, total_duration=21554.1, train/loss=0.123372, validation/loss=0.124608, validation/num_examples=83274637
I0316 20:54:02.506637 140547343606976 spec.py:321] Evaluating on the training split.
I0316 20:56:06.225304 140547343606976 spec.py:333] Evaluating on the validation split.
I0316 20:58:09.981528 140547343606976 spec.py:349] Evaluating on the test split.
I0316 21:00:32.850821 140547343606976 submission_runner.py:469] Time since start: 22065.55s, 	Step: 3898, 	{'train/loss': 0.12300143427135449, 'validation/loss': 0.12466244061566206, 'validation/num_examples': 83274637, 'test/loss': 0.12707269502475135, 'test/num_examples': 95000000, 'score': 3849.871850967407, 'total_duration': 22065.553964853287, 'accumulated_submission_time': 3849.871850967407, 'accumulated_eval_time': 18186.42405295372, 'accumulated_logging_time': 0.774956464767456}
I0316 21:00:32.862530 140504863852288 logging_writer.py:48] [3898] accumulated_eval_time=18186.4, accumulated_logging_time=0.774956, accumulated_submission_time=3849.87, global_step=3898, preemption_count=0, score=3849.87, test/loss=0.127073, test/num_examples=95000000, total_duration=22065.6, train/loss=0.123001, validation/loss=0.124662, validation/num_examples=83274637
I0316 21:02:07.996391 140504872244992 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.00787242, loss=0.12805
I0316 21:02:07.999972 140547343606976 submission.py:265] 4000) loss = 0.128, grad_norm = 0.008
I0316 21:02:34.074987 140547343606976 spec.py:321] Evaluating on the training split.
I0316 21:04:37.150675 140547343606976 spec.py:333] Evaluating on the validation split.
I0316 21:06:40.259436 140547343606976 spec.py:349] Evaluating on the test split.
I0316 21:09:03.071717 140547343606976 submission_runner.py:469] Time since start: 22575.77s, 	Step: 4022, 	{'train/loss': 0.12409947391561123, 'validation/loss': 0.12464322060803996, 'validation/num_examples': 83274637, 'test/loss': 0.1269956378360949, 'test/num_examples': 95000000, 'score': 3970.2064530849457, 'total_duration': 22575.774784088135, 'accumulated_submission_time': 3970.2064530849457, 'accumulated_eval_time': 18575.420905828476, 'accumulated_logging_time': 0.7943100929260254}
I0316 21:09:03.084401 140504863852288 logging_writer.py:48] [4022] accumulated_eval_time=18575.4, accumulated_logging_time=0.79431, accumulated_submission_time=3970.21, global_step=4022, preemption_count=0, score=3970.21, test/loss=0.126996, test/num_examples=95000000, total_duration=22575.8, train/loss=0.124099, validation/loss=0.124643, validation/num_examples=83274637
I0316 21:11:03.538243 140547343606976 spec.py:321] Evaluating on the training split.
I0316 21:13:07.387023 140547343606976 spec.py:333] Evaluating on the validation split.
I0316 21:15:11.352717 140547343606976 spec.py:349] Evaluating on the test split.
I0316 21:17:33.540308 140547343606976 submission_runner.py:469] Time since start: 23086.24s, 	Step: 4142, 	{'train/loss': 0.12190909869923085, 'validation/loss': 0.12461322014707922, 'validation/num_examples': 83274637, 'test/loss': 0.12698302995593422, 'test/num_examples': 95000000, 'score': 4089.7952704429626, 'total_duration': 23086.243463277817, 'accumulated_submission_time': 4089.7952704429626, 'accumulated_eval_time': 18965.42326927185, 'accumulated_logging_time': 0.8256714344024658}
I0316 21:17:33.551474 140504872244992 logging_writer.py:48] [4142] accumulated_eval_time=18965.4, accumulated_logging_time=0.825671, accumulated_submission_time=4089.8, global_step=4142, preemption_count=0, score=4089.8, test/loss=0.126983, test/num_examples=95000000, total_duration=23086.2, train/loss=0.121909, validation/loss=0.124613, validation/num_examples=83274637
I0316 21:19:35.218000 140547343606976 spec.py:321] Evaluating on the training split.
I0316 21:21:39.135551 140547343606976 spec.py:333] Evaluating on the validation split.
I0316 21:23:42.287015 140547343606976 spec.py:349] Evaluating on the test split.
I0316 21:26:05.036802 140547343606976 submission_runner.py:469] Time since start: 23597.74s, 	Step: 4265, 	{'train/loss': 0.12459496570564733, 'validation/loss': 0.1243746373516105, 'validation/num_examples': 83274637, 'test/loss': 0.12672727944464432, 'test/num_examples': 95000000, 'score': 4210.560853004456, 'total_duration': 23597.73993253708, 'accumulated_submission_time': 4210.560853004456, 'accumulated_eval_time': 19355.242249250412, 'accumulated_logging_time': 0.8442246913909912}
I0316 21:26:05.048171 140504863852288 logging_writer.py:48] [4265] accumulated_eval_time=19355.2, accumulated_logging_time=0.844225, accumulated_submission_time=4210.56, global_step=4265, preemption_count=0, score=4210.56, test/loss=0.126727, test/num_examples=95000000, total_duration=23597.7, train/loss=0.124595, validation/loss=0.124375, validation/num_examples=83274637
I0316 21:28:06.021127 140547343606976 spec.py:321] Evaluating on the training split.
I0316 21:30:09.525211 140547343606976 spec.py:333] Evaluating on the validation split.
I0316 21:32:13.315227 140547343606976 spec.py:349] Evaluating on the test split.
I0316 21:34:36.338970 140547343606976 submission_runner.py:469] Time since start: 24109.04s, 	Step: 4381, 	{'train/loss': 0.12466743180299987, 'validation/loss': 0.12458743107409626, 'validation/num_examples': 83274637, 'test/loss': 0.12704688702982853, 'test/num_examples': 95000000, 'score': 4330.669893980026, 'total_duration': 24109.042078733444, 'accumulated_submission_time': 4330.669893980026, 'accumulated_eval_time': 19745.560178279877, 'accumulated_logging_time': 0.8625025749206543}
I0316 21:34:36.351010 140504872244992 logging_writer.py:48] [4381] accumulated_eval_time=19745.6, accumulated_logging_time=0.862503, accumulated_submission_time=4330.67, global_step=4381, preemption_count=0, score=4330.67, test/loss=0.127047, test/num_examples=95000000, total_duration=24109, train/loss=0.124667, validation/loss=0.124587, validation/num_examples=83274637
I0316 21:36:32.430551 140504863852288 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.00490997, loss=0.125761
I0316 21:36:32.434282 140547343606976 submission.py:265] 4500) loss = 0.126, grad_norm = 0.005
I0316 21:36:37.359853 140547343606976 spec.py:321] Evaluating on the training split.
I0316 21:38:41.114440 140547343606976 spec.py:333] Evaluating on the validation split.
I0316 21:40:44.763811 140547343606976 spec.py:349] Evaluating on the test split.
I0316 21:43:04.935518 140547343606976 submission_runner.py:469] Time since start: 24617.64s, 	Step: 4505, 	{'train/loss': 0.12409469642653144, 'validation/loss': 0.12443356424487768, 'validation/num_examples': 83274637, 'test/loss': 0.1267989576599924, 'test/num_examples': 95000000, 'score': 4450.793030977249, 'total_duration': 24617.638649463654, 'accumulated_submission_time': 4450.793030977249, 'accumulated_eval_time': 20133.1359937191, 'accumulated_logging_time': 0.8813877105712891}
I0316 21:43:04.947377 140504872244992 logging_writer.py:48] [4505] accumulated_eval_time=20133.1, accumulated_logging_time=0.881388, accumulated_submission_time=4450.79, global_step=4505, preemption_count=0, score=4450.79, test/loss=0.126799, test/num_examples=95000000, total_duration=24617.6, train/loss=0.124095, validation/loss=0.124434, validation/num_examples=83274637
I0316 21:45:06.086309 140547343606976 spec.py:321] Evaluating on the training split.
I0316 21:47:11.255189 140547343606976 spec.py:333] Evaluating on the validation split.
I0316 21:49:14.998924 140547343606976 spec.py:349] Evaluating on the test split.
I0316 21:51:38.382047 140547343606976 submission_runner.py:469] Time since start: 25131.09s, 	Step: 4627, 	{'train/loss': 0.12185764490026148, 'validation/loss': 0.12445862783719847, 'validation/num_examples': 83274637, 'test/loss': 0.12687804209345768, 'test/num_examples': 95000000, 'score': 4571.081116437912, 'total_duration': 25131.08515906334, 'accumulated_submission_time': 4571.081116437912, 'accumulated_eval_time': 20525.43200445175, 'accumulated_logging_time': 0.9007058143615723}
I0316 21:51:38.423994 140504863852288 logging_writer.py:48] [4627] accumulated_eval_time=20525.4, accumulated_logging_time=0.900706, accumulated_submission_time=4571.08, global_step=4627, preemption_count=0, score=4571.08, test/loss=0.126878, test/num_examples=95000000, total_duration=25131.1, train/loss=0.121858, validation/loss=0.124459, validation/num_examples=83274637
I0316 21:53:38.979582 140547343606976 spec.py:321] Evaluating on the training split.
I0316 21:55:43.888217 140547343606976 spec.py:333] Evaluating on the validation split.
I0316 21:57:48.974342 140547343606976 spec.py:349] Evaluating on the test split.
I0316 22:00:13.079094 140547343606976 submission_runner.py:469] Time since start: 25645.78s, 	Step: 4749, 	{'train/loss': 0.12339741509591348, 'validation/loss': 0.12446318105601947, 'validation/num_examples': 83274637, 'test/loss': 0.12687660422624286, 'test/num_examples': 95000000, 'score': 4690.717064142227, 'total_duration': 25645.78223156929, 'accumulated_submission_time': 4690.717064142227, 'accumulated_eval_time': 20919.531650066376, 'accumulated_logging_time': 0.978297233581543}
I0316 22:00:13.090848 140504872244992 logging_writer.py:48] [4749] accumulated_eval_time=20919.5, accumulated_logging_time=0.978297, accumulated_submission_time=4690.72, global_step=4749, preemption_count=0, score=4690.72, test/loss=0.126877, test/num_examples=95000000, total_duration=25645.8, train/loss=0.123397, validation/loss=0.124463, validation/num_examples=83274637
I0316 22:02:13.504578 140547343606976 spec.py:321] Evaluating on the training split.
I0316 22:04:17.229163 140547343606976 spec.py:333] Evaluating on the validation split.
I0316 22:06:21.303120 140547343606976 spec.py:349] Evaluating on the test split.
I0316 22:08:44.038128 140547343606976 submission_runner.py:469] Time since start: 26156.74s, 	Step: 4871, 	{'train/loss': 0.12138268558138818, 'validation/loss': 0.12437777469984829, 'validation/num_examples': 83274637, 'test/loss': 0.12672705537506906, 'test/num_examples': 95000000, 'score': 4810.248945713043, 'total_duration': 26156.7412481308, 'accumulated_submission_time': 4810.248945713043, 'accumulated_eval_time': 21310.065472364426, 'accumulated_logging_time': 0.9972858428955078}
I0316 22:08:44.049696 140504863852288 logging_writer.py:48] [4871] accumulated_eval_time=21310.1, accumulated_logging_time=0.997286, accumulated_submission_time=4810.25, global_step=4871, preemption_count=0, score=4810.25, test/loss=0.126727, test/num_examples=95000000, total_duration=26156.7, train/loss=0.121383, validation/loss=0.124378, validation/num_examples=83274637
I0316 22:10:45.105910 140547343606976 spec.py:321] Evaluating on the training split.
I0316 22:12:48.474177 140547343606976 spec.py:333] Evaluating on the validation split.
I0316 22:14:50.766405 140547343606976 spec.py:349] Evaluating on the test split.
I0316 22:17:13.012661 140547343606976 submission_runner.py:469] Time since start: 26665.72s, 	Step: 4995, 	{'train/loss': 0.12405936316955858, 'validation/loss': 0.12425519503337303, 'validation/num_examples': 83274637, 'test/loss': 0.12659040654425371, 'test/num_examples': 95000000, 'score': 4930.463898658752, 'total_duration': 26665.71575832367, 'accumulated_submission_time': 4930.463898658752, 'accumulated_eval_time': 21697.972408771515, 'accumulated_logging_time': 1.0223040580749512}
I0316 22:17:13.024203 140504872244992 logging_writer.py:48] [4995] accumulated_eval_time=21698, accumulated_logging_time=1.0223, accumulated_submission_time=4930.46, global_step=4995, preemption_count=0, score=4930.46, test/loss=0.12659, test/num_examples=95000000, total_duration=26665.7, train/loss=0.124059, validation/loss=0.124255, validation/num_examples=83274637
I0316 22:17:14.659558 140504863852288 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.0086383, loss=0.120614
I0316 22:17:14.662730 140547343606976 submission.py:265] 5000) loss = 0.121, grad_norm = 0.009
I0316 22:19:13.860592 140547343606976 spec.py:321] Evaluating on the training split.
I0316 22:21:16.655712 140547343606976 spec.py:333] Evaluating on the validation split.
I0316 22:23:20.079026 140547343606976 spec.py:349] Evaluating on the test split.
I0316 22:25:42.478284 140547343606976 submission_runner.py:469] Time since start: 27175.18s, 	Step: 5118, 	{'train/loss': 0.12156134794869104, 'validation/loss': 0.12428236397689668, 'validation/num_examples': 83274637, 'test/loss': 0.12658220535551373, 'test/num_examples': 95000000, 'score': 5050.426526784897, 'total_duration': 27175.18135023117, 'accumulated_submission_time': 5050.426526784897, 'accumulated_eval_time': 22086.59019947052, 'accumulated_logging_time': 1.0405488014221191}
I0316 22:25:42.490361 140504872244992 logging_writer.py:48] [5118] accumulated_eval_time=22086.6, accumulated_logging_time=1.04055, accumulated_submission_time=5050.43, global_step=5118, preemption_count=0, score=5050.43, test/loss=0.126582, test/num_examples=95000000, total_duration=27175.2, train/loss=0.121561, validation/loss=0.124282, validation/num_examples=83274637
I0316 22:27:43.265652 140547343606976 spec.py:321] Evaluating on the training split.
I0316 22:29:47.558066 140547343606976 spec.py:333] Evaluating on the validation split.
I0316 22:31:52.186576 140547343606976 spec.py:349] Evaluating on the test split.
I0316 22:34:15.592356 140547343606976 submission_runner.py:469] Time since start: 27688.30s, 	Step: 5242, 	{'train/loss': 0.12281366401157222, 'validation/loss': 0.1243227352565676, 'validation/num_examples': 83274637, 'test/loss': 0.12662885685039318, 'test/num_examples': 95000000, 'score': 5170.3124215602875, 'total_duration': 27688.295424938202, 'accumulated_submission_time': 5170.3124215602875, 'accumulated_eval_time': 22478.917066335678, 'accumulated_logging_time': 1.0606460571289062}
I0316 22:34:15.603970 140504863852288 logging_writer.py:48] [5242] accumulated_eval_time=22478.9, accumulated_logging_time=1.06065, accumulated_submission_time=5170.31, global_step=5242, preemption_count=0, score=5170.31, test/loss=0.126629, test/num_examples=95000000, total_duration=27688.3, train/loss=0.122814, validation/loss=0.124323, validation/num_examples=83274637
I0316 22:36:16.789666 140547343606976 spec.py:321] Evaluating on the training split.
I0316 22:38:21.765409 140547343606976 spec.py:333] Evaluating on the validation split.
I0316 22:40:27.081386 140547343606976 spec.py:349] Evaluating on the test split.
I0316 22:42:49.536565 140547343606976 submission_runner.py:469] Time since start: 28202.24s, 	Step: 5367, 	{'train/loss': 0.1240012564713503, 'validation/loss': 0.12419587596007815, 'validation/num_examples': 83274637, 'test/loss': 0.1265525299267016, 'test/num_examples': 95000000, 'score': 5290.612094640732, 'total_duration': 28202.2397043705, 'accumulated_submission_time': 5290.612094640732, 'accumulated_eval_time': 22871.664244651794, 'accumulated_logging_time': 1.102116346359253}
I0316 22:42:49.548326 140504872244992 logging_writer.py:48] [5367] accumulated_eval_time=22871.7, accumulated_logging_time=1.10212, accumulated_submission_time=5290.61, global_step=5367, preemption_count=0, score=5290.61, test/loss=0.126553, test/num_examples=95000000, total_duration=28202.2, train/loss=0.124001, validation/loss=0.124196, validation/num_examples=83274637
I0316 22:44:50.863420 140547343606976 spec.py:321] Evaluating on the training split.
I0316 22:46:55.519823 140547343606976 spec.py:333] Evaluating on the validation split.
I0316 22:49:00.372356 140547343606976 spec.py:349] Evaluating on the test split.
I0316 22:51:24.081768 140547343606976 submission_runner.py:469] Time since start: 28716.78s, 	Step: 5491, 	{'train/loss': 0.1255880333273429, 'validation/loss': 0.12416732956113652, 'validation/num_examples': 83274637, 'test/loss': 0.12650941533018414, 'test/num_examples': 95000000, 'score': 5411.111576080322, 'total_duration': 28716.784876823425, 'accumulated_submission_time': 5411.111576080322, 'accumulated_eval_time': 23264.8827586174, 'accumulated_logging_time': 1.1208200454711914}
I0316 22:51:24.093287 140504863852288 logging_writer.py:48] [5491] accumulated_eval_time=23264.9, accumulated_logging_time=1.12082, accumulated_submission_time=5411.11, global_step=5491, preemption_count=0, score=5411.11, test/loss=0.126509, test/num_examples=95000000, total_duration=28716.8, train/loss=0.125588, validation/loss=0.124167, validation/num_examples=83274637
I0316 22:51:26.477583 140504872244992 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.00629769, loss=0.118483
I0316 22:51:26.480866 140547343606976 submission.py:265] 5500) loss = 0.118, grad_norm = 0.006
I0316 22:53:25.357712 140547343606976 spec.py:321] Evaluating on the training split.
I0316 22:55:27.990036 140547343606976 spec.py:333] Evaluating on the validation split.
I0316 22:57:31.654926 140547343606976 spec.py:349] Evaluating on the test split.
I0316 22:59:53.369205 140547343606976 submission_runner.py:469] Time since start: 29226.07s, 	Step: 5611, 	{'train/loss': 0.1250117608231897, 'validation/loss': 0.12416414177612817, 'validation/num_examples': 83274637, 'test/loss': 0.12658224458353143, 'test/num_examples': 95000000, 'score': 5531.519562482834, 'total_duration': 29226.07232069969, 'accumulated_submission_time': 5531.519562482834, 'accumulated_eval_time': 23652.894488811493, 'accumulated_logging_time': 1.1391315460205078}
I0316 22:59:53.381389 140504863852288 logging_writer.py:48] [5611] accumulated_eval_time=23652.9, accumulated_logging_time=1.13913, accumulated_submission_time=5531.52, global_step=5611, preemption_count=0, score=5531.52, test/loss=0.126582, test/num_examples=95000000, total_duration=29226.1, train/loss=0.125012, validation/loss=0.124164, validation/num_examples=83274637
I0316 23:01:53.799297 140547343606976 spec.py:321] Evaluating on the training split.
I0316 23:03:57.322299 140547343606976 spec.py:333] Evaluating on the validation split.
I0316 23:06:01.040305 140547343606976 spec.py:349] Evaluating on the test split.
I0316 23:08:23.023438 140547343606976 submission_runner.py:469] Time since start: 29735.73s, 	Step: 5733, 	{'train/loss': 0.1218625434886678, 'validation/loss': 0.12413444761060512, 'validation/num_examples': 83274637, 'test/loss': 0.12647722494623284, 'test/num_examples': 95000000, 'score': 5651.106167078018, 'total_duration': 29735.726541519165, 'accumulated_submission_time': 5651.106167078018, 'accumulated_eval_time': 24042.11881875992, 'accumulated_logging_time': 1.1583502292633057}
I0316 23:08:23.035620 140504872244992 logging_writer.py:48] [5733] accumulated_eval_time=24042.1, accumulated_logging_time=1.15835, accumulated_submission_time=5651.11, global_step=5733, preemption_count=0, score=5651.11, test/loss=0.126477, test/num_examples=95000000, total_duration=29735.7, train/loss=0.121863, validation/loss=0.124134, validation/num_examples=83274637
I0316 23:10:23.847003 140547343606976 spec.py:321] Evaluating on the training split.
I0316 23:12:26.266002 140547343606976 spec.py:333] Evaluating on the validation split.
I0316 23:14:30.127250 140547343606976 spec.py:349] Evaluating on the test split.
I0316 23:16:52.926449 140547343606976 submission_runner.py:469] Time since start: 30245.63s, 	Step: 5856, 	{'train/loss': 0.12244205650434296, 'validation/loss': 0.12411334455122612, 'validation/num_examples': 83274637, 'test/loss': 0.1264304101319564, 'test/num_examples': 95000000, 'score': 5771.035926580429, 'total_duration': 30245.62953567505, 'accumulated_submission_time': 5771.035926580429, 'accumulated_eval_time': 24431.19833636284, 'accumulated_logging_time': 1.206749439239502}
I0316 23:16:52.937835 140504863852288 logging_writer.py:48] [5856] accumulated_eval_time=24431.2, accumulated_logging_time=1.20675, accumulated_submission_time=5771.04, global_step=5856, preemption_count=0, score=5771.04, test/loss=0.12643, test/num_examples=95000000, total_duration=30245.6, train/loss=0.122442, validation/loss=0.124113, validation/num_examples=83274637
I0316 23:18:53.515689 140547343606976 spec.py:321] Evaluating on the training split.
I0316 23:20:57.161213 140547343606976 spec.py:333] Evaluating on the validation split.
I0316 23:23:00.816903 140547343606976 spec.py:349] Evaluating on the test split.
I0316 23:25:23.341650 140547343606976 submission_runner.py:469] Time since start: 30756.04s, 	Step: 5978, 	{'train/loss': 0.12180339718271782, 'validation/loss': 0.12411103954708265, 'validation/num_examples': 83274637, 'test/loss': 0.12646240563422756, 'test/num_examples': 95000000, 'score': 5890.758489370346, 'total_duration': 30756.044761657715, 'accumulated_submission_time': 5890.758489370346, 'accumulated_eval_time': 24821.024425268173, 'accumulated_logging_time': 1.2256195545196533}
I0316 23:25:23.417160 140504872244992 logging_writer.py:48] [5978] accumulated_eval_time=24821, accumulated_logging_time=1.22562, accumulated_submission_time=5890.76, global_step=5978, preemption_count=0, score=5890.76, test/loss=0.126462, test/num_examples=95000000, total_duration=30756, train/loss=0.121803, validation/loss=0.124111, validation/num_examples=83274637
I0316 23:25:28.309237 140504863852288 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.00529787, loss=0.117853
I0316 23:25:28.312965 140547343606976 submission.py:265] 6000) loss = 0.118, grad_norm = 0.005
I0316 23:27:24.144167 140547343606976 spec.py:321] Evaluating on the training split.
I0316 23:29:27.779161 140547343606976 spec.py:333] Evaluating on the validation split.
I0316 23:31:30.073848 140547343606976 spec.py:349] Evaluating on the test split.
I0316 23:33:52.517003 140547343606976 submission_runner.py:469] Time since start: 31265.22s, 	Step: 6102, 	{'train/loss': 0.12303583072042215, 'validation/loss': 0.12409796456324053, 'validation/num_examples': 83274637, 'test/loss': 0.12644789869995118, 'test/num_examples': 95000000, 'score': 6010.606308698654, 'total_duration': 31265.220098733902, 'accumulated_submission_time': 6010.606308698654, 'accumulated_eval_time': 25209.397354602814, 'accumulated_logging_time': 1.3086576461791992}
I0316 23:33:52.529234 140504872244992 logging_writer.py:48] [6102] accumulated_eval_time=25209.4, accumulated_logging_time=1.30866, accumulated_submission_time=6010.61, global_step=6102, preemption_count=0, score=6010.61, test/loss=0.126448, test/num_examples=95000000, total_duration=31265.2, train/loss=0.123036, validation/loss=0.124098, validation/num_examples=83274637
I0316 23:35:53.127244 140547343606976 spec.py:321] Evaluating on the training split.
I0316 23:37:56.759917 140547343606976 spec.py:333] Evaluating on the validation split.
I0316 23:40:00.650125 140547343606976 spec.py:349] Evaluating on the test split.
I0316 23:42:22.672620 140547343606976 submission_runner.py:469] Time since start: 31775.38s, 	Step: 6228, 	{'train/loss': 0.12120614627877498, 'validation/loss': 0.12415118430911123, 'validation/num_examples': 83274637, 'test/loss': 0.12653906890820954, 'test/num_examples': 95000000, 'score': 6130.327610254288, 'total_duration': 31775.37571954727, 'accumulated_submission_time': 6130.327610254288, 'accumulated_eval_time': 25598.942897319794, 'accumulated_logging_time': 1.3277554512023926}
I0316 23:42:22.684575 140504863852288 logging_writer.py:48] [6228] accumulated_eval_time=25598.9, accumulated_logging_time=1.32776, accumulated_submission_time=6130.33, global_step=6228, preemption_count=0, score=6130.33, test/loss=0.126539, test/num_examples=95000000, total_duration=31775.4, train/loss=0.121206, validation/loss=0.124151, validation/num_examples=83274637
I0316 23:44:24.757810 140547343606976 spec.py:321] Evaluating on the training split.
I0316 23:46:28.522448 140547343606976 spec.py:333] Evaluating on the validation split.
I0316 23:48:32.018034 140547343606976 spec.py:349] Evaluating on the test split.
I0316 23:50:54.173783 140547343606976 submission_runner.py:469] Time since start: 32286.88s, 	Step: 6353, 	{'train/loss': 0.1222796146974562, 'validation/loss': 0.12408676659261551, 'validation/num_examples': 83274637, 'test/loss': 0.12643092393614117, 'test/num_examples': 95000000, 'score': 6251.561593294144, 'total_duration': 32286.87687563896, 'accumulated_submission_time': 6251.561593294144, 'accumulated_eval_time': 25988.358961105347, 'accumulated_logging_time': 1.347278118133545}
I0316 23:50:54.185295 140504872244992 logging_writer.py:48] [6353] accumulated_eval_time=25988.4, accumulated_logging_time=1.34728, accumulated_submission_time=6251.56, global_step=6353, preemption_count=0, score=6251.56, test/loss=0.126431, test/num_examples=95000000, total_duration=32286.9, train/loss=0.12228, validation/loss=0.124087, validation/num_examples=83274637
I0316 23:52:54.790541 140547343606976 spec.py:321] Evaluating on the training split.
I0316 23:54:58.593374 140547343606976 spec.py:333] Evaluating on the validation split.
I0316 23:57:02.069568 140547343606976 spec.py:349] Evaluating on the test split.
I0316 23:59:24.196705 140547343606976 submission_runner.py:469] Time since start: 32796.90s, 	Step: 6476, 	{'train/loss': 0.12382189131686518, 'validation/loss': 0.12408259335786924, 'validation/num_examples': 83274637, 'test/loss': 0.12642106500549316, 'test/num_examples': 95000000, 'score': 6371.3185613155365, 'total_duration': 32796.89978790283, 'accumulated_submission_time': 6371.3185613155365, 'accumulated_eval_time': 26377.765162706375, 'accumulated_logging_time': 1.3853793144226074}
I0316 23:59:24.208998 140504863852288 logging_writer.py:48] [6476] accumulated_eval_time=26377.8, accumulated_logging_time=1.38538, accumulated_submission_time=6371.32, global_step=6476, preemption_count=0, score=6371.32, test/loss=0.126421, test/num_examples=95000000, total_duration=32796.9, train/loss=0.123822, validation/loss=0.124083, validation/num_examples=83274637
I0316 23:59:29.435191 140504872244992 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.00679199, loss=0.122046
I0316 23:59:29.438554 140547343606976 submission.py:265] 6500) loss = 0.122, grad_norm = 0.007
I0317 00:01:25.320228 140547343606976 spec.py:321] Evaluating on the training split.
I0317 00:03:28.627529 140547343606976 spec.py:333] Evaluating on the validation split.
I0317 00:05:30.704017 140547343606976 spec.py:349] Evaluating on the test split.
I0317 00:07:50.808781 140547343606976 submission_runner.py:469] Time since start: 33303.51s, 	Step: 6600, 	{'train/loss': 0.12279644606408878, 'validation/loss': 0.12407576518216126, 'validation/num_examples': 83274637, 'test/loss': 0.12639773563445242, 'test/num_examples': 95000000, 'score': 6491.561371088028, 'total_duration': 33303.51187944412, 'accumulated_submission_time': 6491.561371088028, 'accumulated_eval_time': 26763.253753185272, 'accumulated_logging_time': 1.404968500137329}
I0317 00:07:50.821073 140504863852288 logging_writer.py:48] [6600] accumulated_eval_time=26763.3, accumulated_logging_time=1.40497, accumulated_submission_time=6491.56, global_step=6600, preemption_count=0, score=6491.56, test/loss=0.126398, test/num_examples=95000000, total_duration=33303.5, train/loss=0.122796, validation/loss=0.124076, validation/num_examples=83274637
I0317 00:09:51.621989 140547343606976 spec.py:321] Evaluating on the training split.
I0317 00:11:55.098382 140547343606976 spec.py:333] Evaluating on the validation split.
I0317 00:13:58.654585 140547343606976 spec.py:349] Evaluating on the test split.
I0317 00:16:20.556360 140547343606976 submission_runner.py:469] Time since start: 33813.26s, 	Step: 6722, 	{'train/loss': 0.12109561285741309, 'validation/loss': 0.12412481251829917, 'validation/num_examples': 83274637, 'test/loss': 0.12650794251692923, 'test/num_examples': 95000000, 'score': 6611.492800712585, 'total_duration': 33813.259452581406, 'accumulated_submission_time': 6611.492800712585, 'accumulated_eval_time': 27152.18819618225, 'accumulated_logging_time': 1.4246456623077393}
I0317 00:16:20.568128 140504872244992 logging_writer.py:48] [6722] accumulated_eval_time=27152.2, accumulated_logging_time=1.42465, accumulated_submission_time=6611.49, global_step=6722, preemption_count=0, score=6611.49, test/loss=0.126508, test/num_examples=95000000, total_duration=33813.3, train/loss=0.121096, validation/loss=0.124125, validation/num_examples=83274637
I0317 00:18:21.520332 140547343606976 spec.py:321] Evaluating on the training split.
I0317 00:20:25.795053 140547343606976 spec.py:333] Evaluating on the validation split.
I0317 00:22:29.735116 140547343606976 spec.py:349] Evaluating on the test split.
I0317 00:24:52.518842 140547343606976 submission_runner.py:469] Time since start: 34325.22s, 	Step: 6845, 	{'train/loss': 0.12436573623102243, 'validation/loss': 0.12413831232677495, 'validation/num_examples': 83274637, 'test/loss': 0.12650902922732704, 'test/num_examples': 95000000, 'score': 6731.540198326111, 'total_duration': 34325.22195005417, 'accumulated_submission_time': 6731.540198326111, 'accumulated_eval_time': 27543.18686771393, 'accumulated_logging_time': 1.4530954360961914}
I0317 00:24:52.531437 140504863852288 logging_writer.py:48] [6845] accumulated_eval_time=27543.2, accumulated_logging_time=1.4531, accumulated_submission_time=6731.54, global_step=6845, preemption_count=0, score=6731.54, test/loss=0.126509, test/num_examples=95000000, total_duration=34325.2, train/loss=0.124366, validation/loss=0.124138, validation/num_examples=83274637
I0317 00:26:53.294795 140547343606976 spec.py:321] Evaluating on the training split.
I0317 00:28:56.906176 140547343606976 spec.py:333] Evaluating on the validation split.
I0317 00:31:00.675729 140547343606976 spec.py:349] Evaluating on the test split.
I0317 00:33:22.953129 140547343606976 submission_runner.py:469] Time since start: 34835.66s, 	Step: 6968, 	{'train/loss': 0.12409871071696167, 'validation/loss': 0.12414793238848974, 'validation/num_examples': 83274637, 'test/loss': 0.1265248481440494, 'test/num_examples': 95000000, 'score': 6851.418256521225, 'total_duration': 34835.656227350235, 'accumulated_submission_time': 6851.418256521225, 'accumulated_eval_time': 27932.84537243843, 'accumulated_logging_time': 1.4725987911224365}
I0317 00:33:22.965321 140504872244992 logging_writer.py:48] [6968] accumulated_eval_time=27932.8, accumulated_logging_time=1.4726, accumulated_submission_time=6851.42, global_step=6968, preemption_count=0, score=6851.42, test/loss=0.126525, test/num_examples=95000000, total_duration=34835.7, train/loss=0.124099, validation/loss=0.124148, validation/num_examples=83274637
I0317 00:33:33.568639 140504863852288 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.00761356, loss=0.12569
I0317 00:33:33.572426 140547343606976 submission.py:265] 7000) loss = 0.126, grad_norm = 0.008
I0317 00:35:23.665756 140547343606976 spec.py:321] Evaluating on the training split.
I0317 00:37:27.120712 140547343606976 spec.py:333] Evaluating on the validation split.
I0317 00:39:30.518128 140547343606976 spec.py:349] Evaluating on the test split.
I0317 00:41:52.923681 140547343606976 submission_runner.py:469] Time since start: 35345.63s, 	Step: 7087, 	{'train/loss': 0.12132644572234969, 'validation/loss': 0.12410020596040247, 'validation/num_examples': 83274637, 'test/loss': 0.12642837091016268, 'test/num_examples': 95000000, 'score': 6971.287824630737, 'total_duration': 35345.626782655716, 'accumulated_submission_time': 6971.287824630737, 'accumulated_eval_time': 28322.10343337059, 'accumulated_logging_time': 1.4918618202209473}
I0317 00:41:52.936030 140504872244992 logging_writer.py:48] [7087] accumulated_eval_time=28322.1, accumulated_logging_time=1.49186, accumulated_submission_time=6971.29, global_step=7087, preemption_count=0, score=6971.29, test/loss=0.126428, test/num_examples=95000000, total_duration=35345.6, train/loss=0.121326, validation/loss=0.1241, validation/num_examples=83274637
I0317 00:43:54.108728 140547343606976 spec.py:321] Evaluating on the training split.
I0317 00:45:57.811390 140547343606976 spec.py:333] Evaluating on the validation split.
I0317 00:48:01.399856 140547343606976 spec.py:349] Evaluating on the test split.
I0317 00:50:23.940407 140547343606976 submission_runner.py:469] Time since start: 35856.64s, 	Step: 7210, 	{'train/loss': 0.12061052252472541, 'validation/loss': 0.12412765872184647, 'validation/num_examples': 83274637, 'test/loss': 0.1264587066840724, 'test/num_examples': 95000000, 'score': 7091.614373683929, 'total_duration': 35856.643523693085, 'accumulated_submission_time': 7091.614373683929, 'accumulated_eval_time': 28711.935252189636, 'accumulated_logging_time': 1.5113720893859863}
I0317 00:50:23.952828 140504863852288 logging_writer.py:48] [7210] accumulated_eval_time=28711.9, accumulated_logging_time=1.51137, accumulated_submission_time=7091.61, global_step=7210, preemption_count=0, score=7091.61, test/loss=0.126459, test/num_examples=95000000, total_duration=35856.6, train/loss=0.120611, validation/loss=0.124128, validation/num_examples=83274637
I0317 00:52:25.561542 140547343606976 spec.py:321] Evaluating on the training split.
I0317 00:54:30.279614 140547343606976 spec.py:333] Evaluating on the validation split.
I0317 00:56:34.923124 140547343606976 spec.py:349] Evaluating on the test split.
I0317 00:58:58.702570 140547343606976 submission_runner.py:469] Time since start: 36371.41s, 	Step: 7335, 	{'train/loss': 0.12295499959785236, 'validation/loss': 0.12414347056952652, 'validation/num_examples': 83274637, 'test/loss': 0.12651339044004742, 'test/num_examples': 95000000, 'score': 7212.341063261032, 'total_duration': 36371.40566778183, 'accumulated_submission_time': 7212.341063261032, 'accumulated_eval_time': 29105.076414585114, 'accumulated_logging_time': 1.5305519104003906}
I0317 00:58:58.715677 140504872244992 logging_writer.py:48] [7335] accumulated_eval_time=29105.1, accumulated_logging_time=1.53055, accumulated_submission_time=7212.34, global_step=7335, preemption_count=0, score=7212.34, test/loss=0.126513, test/num_examples=95000000, total_duration=36371.4, train/loss=0.122955, validation/loss=0.124143, validation/num_examples=83274637
I0317 01:01:00.422472 140547343606976 spec.py:321] Evaluating on the training split.
I0317 01:03:04.213286 140547343606976 spec.py:333] Evaluating on the validation split.
I0317 01:05:07.998537 140547343606976 spec.py:349] Evaluating on the test split.
I0317 01:07:29.948097 140547343606976 submission_runner.py:469] Time since start: 36882.65s, 	Step: 7461, 	{'train/loss': 0.12517155237863628, 'validation/loss': 0.12409859664797958, 'validation/num_examples': 83274637, 'test/loss': 0.12642165964829294, 'test/num_examples': 95000000, 'score': 7333.16911482811, 'total_duration': 36882.65119767189, 'accumulated_submission_time': 7333.16911482811, 'accumulated_eval_time': 29494.60210800171, 'accumulated_logging_time': 1.5774853229522705}
I0317 01:07:30.020160 140504863852288 logging_writer.py:48] [7461] accumulated_eval_time=29494.6, accumulated_logging_time=1.57749, accumulated_submission_time=7333.17, global_step=7461, preemption_count=0, score=7333.17, test/loss=0.126422, test/num_examples=95000000, total_duration=36882.7, train/loss=0.125172, validation/loss=0.124099, validation/num_examples=83274637
I0317 01:07:48.415629 140504872244992 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.0091934, loss=0.135763
I0317 01:07:48.419310 140547343606976 submission.py:265] 7500) loss = 0.136, grad_norm = 0.009
I0317 01:09:30.777388 140547343606976 spec.py:321] Evaluating on the training split.
I0317 01:11:34.686642 140547343606976 spec.py:333] Evaluating on the validation split.
I0317 01:13:38.676954 140547343606976 spec.py:349] Evaluating on the test split.
I0317 01:16:01.603812 140547343606976 submission_runner.py:469] Time since start: 37394.31s, 	Step: 7585, 	{'train/loss': 0.12223967814120128, 'validation/loss': 0.12407354944749928, 'validation/num_examples': 83274637, 'test/loss': 0.1264031073473077, 'test/num_examples': 95000000, 'score': 7453.064138650894, 'total_duration': 37394.306941986084, 'accumulated_submission_time': 7453.064138650894, 'accumulated_eval_time': 29885.42869091034, 'accumulated_logging_time': 1.6574649810791016}
I0317 01:16:01.616117 140504863852288 logging_writer.py:48] [7585] accumulated_eval_time=29885.4, accumulated_logging_time=1.65746, accumulated_submission_time=7453.06, global_step=7585, preemption_count=0, score=7453.06, test/loss=0.126403, test/num_examples=95000000, total_duration=37394.3, train/loss=0.12224, validation/loss=0.124074, validation/num_examples=83274637
I0317 01:18:03.112266 140547343606976 spec.py:321] Evaluating on the training split.
I0317 01:20:06.851131 140547343606976 spec.py:333] Evaluating on the validation split.
I0317 01:22:10.741840 140547343606976 spec.py:349] Evaluating on the test split.
I0317 01:24:32.361019 140547343606976 submission_runner.py:469] Time since start: 37905.06s, 	Step: 7709, 	{'train/loss': 0.12293205525874124, 'validation/loss': 0.12419659054882172, 'validation/num_examples': 83274637, 'test/loss': 0.12654806381117167, 'test/num_examples': 95000000, 'score': 7573.724761009216, 'total_duration': 37905.06407427788, 'accumulated_submission_time': 7573.724761009216, 'accumulated_eval_time': 30274.67745780945, 'accumulated_logging_time': 1.6773288249969482}
I0317 01:24:32.374134 140504872244992 logging_writer.py:48] [7709] accumulated_eval_time=30274.7, accumulated_logging_time=1.67733, accumulated_submission_time=7573.72, global_step=7709, preemption_count=0, score=7573.72, test/loss=0.126548, test/num_examples=95000000, total_duration=37905.1, train/loss=0.122932, validation/loss=0.124197, validation/num_examples=83274637
I0317 01:26:32.831097 140547343606976 spec.py:321] Evaluating on the training split.
I0317 01:28:36.476504 140547343606976 spec.py:333] Evaluating on the validation split.
I0317 01:30:40.112196 140547343606976 spec.py:349] Evaluating on the test split.
I0317 01:33:02.654687 140547343606976 submission_runner.py:469] Time since start: 38415.36s, 	Step: 7832, 	{'train/loss': 0.1246797614828919, 'validation/loss': 0.12413484338706857, 'validation/num_examples': 83274637, 'test/loss': 0.12639633949440404, 'test/num_examples': 95000000, 'score': 7693.310275793076, 'total_duration': 38415.35781812668, 'accumulated_submission_time': 7693.310275793076, 'accumulated_eval_time': 30664.501200199127, 'accumulated_logging_time': 1.6981863975524902}
I0317 01:33:02.667245 140504863852288 logging_writer.py:48] [7832] accumulated_eval_time=30664.5, accumulated_logging_time=1.69819, accumulated_submission_time=7693.31, global_step=7832, preemption_count=0, score=7693.31, test/loss=0.126396, test/num_examples=95000000, total_duration=38415.4, train/loss=0.12468, validation/loss=0.124135, validation/num_examples=83274637
I0317 01:35:03.095077 140504872244992 logging_writer.py:48] [7953] global_step=7953, preemption_count=0, score=7813.27
I0317 01:35:13.030457 140547343606976 submission_runner.py:646] Tuning trial 2/5
I0317 01:35:13.030671 140547343606976 submission_runner.py:647] Hyperparameters: Hyperparameters(learning_rate=0.0014381744028656841, one_minus_beta1=0.025337537053408913, one_minus_beta2=0.02508024059481679, epsilon=1e-08, one_minus_momentum=0.0, use_momentum=False, weight_decay=0.00019716633625688372, max_preconditioner_dim=1024, precondition_frequency=100, start_preconditioning_step=-1, inv_root_override=0, exponent_multiplier=1.0, grafting_type='ADAM', grafting_epsilon=1e-08, use_normalized_grafting=False, communication_dtype='FP32', communicate_params=True, use_cosine_decay=True, warmup_factor=0.05, label_smoothing=0.2, dropout_rate=0.0, use_nadam=True, step_hint_factor=0.6)
I0317 01:35:13.031539 140547343606976 submission_runner.py:648] Metrics: {'eval_results': [(1, {'train/loss': 1.2654326404953318, 'validation/loss': 1.268143453003206, 'validation/num_examples': 83274637, 'test/loss': 1.2667239189523798, 'test/num_examples': 95000000, 'score': 4.471369504928589, 'total_duration': 947.8612711429596, 'accumulated_submission_time': 4.471369504928589, 'accumulated_eval_time': 942.9857084751129, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (123, {'train/loss': 0.13341089352456362, 'validation/loss': 0.13326975551597872, 'validation/num_examples': 83274637, 'test/loss': 0.13571372937228554, 'test/num_examples': 95000000, 'score': 124.07613039016724, 'total_duration': 1946.936799287796, 'accumulated_submission_time': 124.07613039016724, 'accumulated_eval_time': 1821.5475096702576, 'accumulated_logging_time': 0.016313552856445312, 'global_step': 123, 'preemption_count': 0}), (243, {'train/loss': 0.1269283348552171, 'validation/loss': 0.12901506017080552, 'validation/num_examples': 83274637, 'test/loss': 0.13145769815701935, 'test/num_examples': 95000000, 'score': 244.66989922523499, 'total_duration': 2938.0191299915314, 'accumulated_submission_time': 244.66989922523499, 'accumulated_eval_time': 2691.1630423069, 'accumulated_logging_time': 0.033066511154174805, 'global_step': 243, 'preemption_count': 0}), (367, {'train/loss': 0.1277643667687718, 'validation/loss': 0.1276116532155123, 'validation/num_examples': 83274637, 'test/loss': 0.12997279920413368, 'test/num_examples': 95000000, 'score': 364.9590401649475, 'total_duration': 3919.4140877723694, 'accumulated_submission_time': 364.9590401649475, 'accumulated_eval_time': 3551.4018607139587, 'accumulated_logging_time': 0.0490565299987793, 'global_step': 367, 'preemption_count': 0}), (490, {'train/loss': 0.1287535887553089, 'validation/loss': 0.12709300201926577, 'validation/num_examples': 83274637, 'test/loss': 0.12948077687434145, 'test/num_examples': 95000000, 'score': 484.687130689621, 'total_duration': 4890.165053129196, 'accumulated_submission_time': 484.687130689621, 'accumulated_eval_time': 4401.497174024582, 'accumulated_logging_time': 0.06656885147094727, 'global_step': 490, 'preemption_count': 0}), (619, {'train/loss': 0.12722629356686668, 'validation/loss': 0.1269996693307299, 'validation/num_examples': 83274637, 'test/loss': 0.12964734526539853, 'test/num_examples': 95000000, 'score': 605.1324234008789, 'total_duration': 5876.039811134338, 'accumulated_submission_time': 605.1324234008789, 'accumulated_eval_time': 5266.00591635704, 'accumulated_logging_time': 0.10360264778137207, 'global_step': 619, 'preemption_count': 0}), (739, {'train/loss': 0.12406153249526486, 'validation/loss': 0.1266881260307427, 'validation/num_examples': 83274637, 'test/loss': 0.12918211434719687, 'test/num_examples': 95000000, 'score': 724.8710491657257, 'total_duration': 6843.17255115509, 'accumulated_submission_time': 724.8710491657257, 'accumulated_eval_time': 6112.515592813492, 'accumulated_logging_time': 0.12002348899841309, 'global_step': 739, 'preemption_count': 0}), (866, {'train/loss': 0.12563989459042818, 'validation/loss': 0.12720733865937903, 'validation/num_examples': 83274637, 'test/loss': 0.12965291715838784, 'test/num_examples': 95000000, 'score': 845.3385343551636, 'total_duration': 7811.389776229858, 'accumulated_submission_time': 845.3385343551636, 'accumulated_eval_time': 6959.3419642448425, 'accumulated_logging_time': 0.1382277011871338, 'global_step': 866, 'preemption_count': 0}), (987, {'train/loss': 0.12606355280106213, 'validation/loss': 0.12634416380811228, 'validation/num_examples': 83274637, 'test/loss': 0.12862187480693615, 'test/num_examples': 95000000, 'score': 965.0354306697845, 'total_duration': 8793.743139266968, 'accumulated_submission_time': 965.0354306697845, 'accumulated_eval_time': 7821.095346450806, 'accumulated_logging_time': 0.17729401588439941, 'global_step': 987, 'preemption_count': 0}), (1112, {'train/loss': 0.12306550925835005, 'validation/loss': 0.12612130971834223, 'validation/num_examples': 83274637, 'test/loss': 0.12866027056920906, 'test/num_examples': 95000000, 'score': 1084.6656374931335, 'total_duration': 9728.465208768845, 'accumulated_submission_time': 1084.6656374931335, 'accumulated_eval_time': 8635.289261102676, 'accumulated_logging_time': 0.1951770782470703, 'global_step': 1112, 'preemption_count': 0}), (1236, {'train/loss': 0.12568337635088403, 'validation/loss': 0.12681705435823598, 'validation/num_examples': 83274637, 'test/loss': 0.1293259257223029, 'test/num_examples': 95000000, 'score': 1204.7923319339752, 'total_duration': 10589.109101057053, 'accumulated_submission_time': 1204.7923319339752, 'accumulated_eval_time': 9374.890599489212, 'accumulated_logging_time': 0.2192680835723877, 'global_step': 1236, 'preemption_count': 0}), (1359, {'train/loss': 0.12683596488313, 'validation/loss': 0.12613078720027524, 'validation/num_examples': 83274637, 'test/loss': 0.12857943969485633, 'test/num_examples': 95000000, 'score': 1325.1372377872467, 'total_duration': 11298.828404426575, 'accumulated_submission_time': 1325.1372377872467, 'accumulated_eval_time': 9963.38474059105, 'accumulated_logging_time': 0.23853278160095215, 'global_step': 1359, 'preemption_count': 0}), (1480, {'train/loss': 0.12360922061181721, 'validation/loss': 0.12607019025036834, 'validation/num_examples': 83274637, 'test/loss': 0.1287034720681843, 'test/num_examples': 95000000, 'score': 1444.8060755729675, 'total_duration': 11838.758248090744, 'accumulated_submission_time': 1444.8060755729675, 'accumulated_eval_time': 10382.76194190979, 'accumulated_logging_time': 0.2553706169128418, 'global_step': 1480, 'preemption_count': 0}), (1603, {'train/loss': 0.12548421004431817, 'validation/loss': 0.12641472428031095, 'validation/num_examples': 83274637, 'test/loss': 0.12864747096372905, 'test/num_examples': 95000000, 'score': 1564.8674092292786, 'total_duration': 12351.471280813217, 'accumulated_submission_time': 1564.8674092292786, 'accumulated_eval_time': 10774.531562805176, 'accumulated_logging_time': 0.2729027271270752, 'global_step': 1603, 'preemption_count': 0}), (1724, {'train/loss': 0.12395863624309053, 'validation/loss': 0.1257001094930213, 'validation/num_examples': 83274637, 'test/loss': 0.12816850327309057, 'test/num_examples': 95000000, 'score': 1685.808421611786, 'total_duration': 12863.411012172699, 'accumulated_submission_time': 1685.808421611786, 'accumulated_eval_time': 11164.574544668198, 'accumulated_logging_time': 0.31430673599243164, 'global_step': 1724, 'preemption_count': 0}), (1845, {'train/loss': 0.1239956716011778, 'validation/loss': 0.12575150284966533, 'validation/num_examples': 83274637, 'test/loss': 0.12821377153966804, 'test/num_examples': 95000000, 'score': 1805.9931104183197, 'total_duration': 13374.65697145462, 'accumulated_submission_time': 1805.9931104183197, 'accumulated_eval_time': 11554.733649015427, 'accumulated_logging_time': 0.33339762687683105, 'global_step': 1845, 'preemption_count': 0}), (1965, {'train/loss': 0.12699316400054717, 'validation/loss': 0.12514760503964595, 'validation/num_examples': 83274637, 'test/loss': 0.1275636140686035, 'test/num_examples': 95000000, 'score': 1925.828624010086, 'total_duration': 13885.887619018555, 'accumulated_submission_time': 1925.828624010086, 'accumulated_eval_time': 11945.19139456749, 'accumulated_logging_time': 0.35192060470581055, 'global_step': 1965, 'preemption_count': 0}), (2083, {'train/loss': 0.12737041671049293, 'validation/loss': 0.12563726118326193, 'validation/num_examples': 83274637, 'test/loss': 0.12811624996727392, 'test/num_examples': 95000000, 'score': 2046.4323828220367, 'total_duration': 14398.035952806473, 'accumulated_submission_time': 2046.4323828220367, 'accumulated_eval_time': 12335.79840350151, 'accumulated_logging_time': 0.3870232105255127, 'global_step': 2083, 'preemption_count': 0}), (2204, {'train/loss': 0.12286555980834776, 'validation/loss': 0.12588682705844823, 'validation/num_examples': 83274637, 'test/loss': 0.12843282979254472, 'test/num_examples': 95000000, 'score': 2166.552446126938, 'total_duration': 14909.4299659729, 'accumulated_submission_time': 2166.552446126938, 'accumulated_eval_time': 12726.166886806488, 'accumulated_logging_time': 0.41201138496398926, 'global_step': 2204, 'preemption_count': 0}), (2328, {'train/loss': 0.12363505133954629, 'validation/loss': 0.12519407248759445, 'validation/num_examples': 83274637, 'test/loss': 0.12766449967338162, 'test/num_examples': 95000000, 'score': 2286.2150740623474, 'total_duration': 15420.009753465652, 'accumulated_submission_time': 2286.2150740623474, 'accumulated_eval_time': 13116.183973550797, 'accumulated_logging_time': 0.4317514896392822, 'global_step': 2328, 'preemption_count': 0}), (2451, {'train/loss': 0.12398902659307921, 'validation/loss': 0.12512786840125564, 'validation/num_examples': 83274637, 'test/loss': 0.12757607830657958, 'test/num_examples': 95000000, 'score': 2406.7126801013947, 'total_duration': 15931.905608654022, 'accumulated_submission_time': 2406.7126801013947, 'accumulated_eval_time': 13506.654529094696, 'accumulated_logging_time': 0.4497056007385254, 'global_step': 2451, 'preemption_count': 0}), (2574, {'train/loss': 0.12235326196417724, 'validation/loss': 0.1251100243813887, 'validation/num_examples': 83274637, 'test/loss': 0.12749482891074732, 'test/num_examples': 95000000, 'score': 2526.345806837082, 'total_duration': 16442.338914632797, 'accumulated_submission_time': 2526.345806837082, 'accumulated_eval_time': 13896.541800737381, 'accumulated_logging_time': 0.5222728252410889, 'global_step': 2574, 'preemption_count': 0}), (2695, {'train/loss': 0.12528469822518476, 'validation/loss': 0.12507892899566195, 'validation/num_examples': 83274637, 'test/loss': 0.1276123143187272, 'test/num_examples': 95000000, 'score': 2646.5343613624573, 'total_duration': 16953.908809661865, 'accumulated_submission_time': 2646.5343613624573, 'accumulated_eval_time': 14287.014113903046, 'accumulated_logging_time': 0.5414602756500244, 'global_step': 2695, 'preemption_count': 0}), (2819, {'train/loss': 0.1245684371476056, 'validation/loss': 0.12516579796142246, 'validation/num_examples': 83274637, 'test/loss': 0.12747068378858065, 'test/num_examples': 95000000, 'score': 2766.590095281601, 'total_duration': 17464.656788110733, 'accumulated_submission_time': 2766.590095281601, 'accumulated_eval_time': 14676.810285806656, 'accumulated_logging_time': 0.5613434314727783, 'global_step': 2819, 'preemption_count': 0}), (2939, {'train/loss': 0.12316614993066094, 'validation/loss': 0.1247863788215217, 'validation/num_examples': 83274637, 'test/loss': 0.12726495481475028, 'test/num_examples': 95000000, 'score': 2886.8806285858154, 'total_duration': 17973.705956697464, 'accumulated_submission_time': 2886.8806285858154, 'accumulated_eval_time': 15064.684843540192, 'accumulated_logging_time': 0.5814967155456543, 'global_step': 2939, 'preemption_count': 0}), (3057, {'train/loss': 0.12297670783870987, 'validation/loss': 0.12524698528305567, 'validation/num_examples': 83274637, 'test/loss': 0.12784091562291194, 'test/num_examples': 95000000, 'score': 3006.7930765151978, 'total_duration': 18483.55531978607, 'accumulated_submission_time': 3006.7930765151978, 'accumulated_eval_time': 15453.785985708237, 'accumulated_logging_time': 0.6009373664855957, 'global_step': 3057, 'preemption_count': 0}), (3175, {'train/loss': 0.12450858660600383, 'validation/loss': 0.1248025579606663, 'validation/num_examples': 83274637, 'test/loss': 0.1272420657250254, 'test/num_examples': 95000000, 'score': 3127.7385132312775, 'total_duration': 18997.494317770004, 'accumulated_submission_time': 3127.7385132312775, 'accumulated_eval_time': 15845.895000696182, 'accumulated_logging_time': 0.6293764114379883, 'global_step': 3175, 'preemption_count': 0}), (3298, {'train/loss': 0.12479026925953637, 'validation/loss': 0.12500996992148292, 'validation/num_examples': 83274637, 'test/loss': 0.12734507960951955, 'test/num_examples': 95000000, 'score': 3248.4884819984436, 'total_duration': 19508.487350702286, 'accumulated_submission_time': 3248.4884819984436, 'accumulated_eval_time': 16235.235828399658, 'accumulated_logging_time': 0.6473820209503174, 'global_step': 3298, 'preemption_count': 0}), (3416, {'train/loss': 0.12294937778121737, 'validation/loss': 0.12507050685223203, 'validation/num_examples': 83274637, 'test/loss': 0.12756497858312507, 'test/num_examples': 95000000, 'score': 3368.837310552597, 'total_duration': 20020.204884767532, 'accumulated_submission_time': 3368.837310552597, 'accumulated_eval_time': 16625.681668281555, 'accumulated_logging_time': 0.6676650047302246, 'global_step': 3416, 'preemption_count': 0}), (3536, {'train/loss': 0.12578364714127954, 'validation/loss': 0.12473236696724861, 'validation/num_examples': 83274637, 'test/loss': 0.1271822725105687, 'test/num_examples': 95000000, 'score': 3489.975656270981, 'total_duration': 20532.49209332466, 'accumulated_submission_time': 3489.975656270981, 'accumulated_eval_time': 17015.958689451218, 'accumulated_logging_time': 0.6872437000274658, 'global_step': 3536, 'preemption_count': 0}), (3657, {'train/loss': 0.12345025228410109, 'validation/loss': 0.12471244156769891, 'validation/num_examples': 83274637, 'test/loss': 0.12710413816006308, 'test/num_examples': 95000000, 'score': 3610.0101730823517, 'total_duration': 21042.989954948425, 'accumulated_submission_time': 3610.0101730823517, 'accumulated_eval_time': 17405.494908571243, 'accumulated_logging_time': 0.7360546588897705, 'global_step': 3657, 'preemption_count': 0}), (3777, {'train/loss': 0.12337209427709644, 'validation/loss': 0.12460818865511962, 'validation/num_examples': 83274637, 'test/loss': 0.1269813379375257, 'test/num_examples': 95000000, 'score': 3729.6726961135864, 'total_duration': 21554.1139087677, 'accumulated_submission_time': 3729.6726961135864, 'accumulated_eval_time': 17796.079731464386, 'accumulated_logging_time': 0.755990743637085, 'global_step': 3777, 'preemption_count': 0}), (3898, {'train/loss': 0.12300143427135449, 'validation/loss': 0.12466244061566206, 'validation/num_examples': 83274637, 'test/loss': 0.12707269502475135, 'test/num_examples': 95000000, 'score': 3849.871850967407, 'total_duration': 22065.553964853287, 'accumulated_submission_time': 3849.871850967407, 'accumulated_eval_time': 18186.42405295372, 'accumulated_logging_time': 0.774956464767456, 'global_step': 3898, 'preemption_count': 0}), (4022, {'train/loss': 0.12409947391561123, 'validation/loss': 0.12464322060803996, 'validation/num_examples': 83274637, 'test/loss': 0.1269956378360949, 'test/num_examples': 95000000, 'score': 3970.2064530849457, 'total_duration': 22575.774784088135, 'accumulated_submission_time': 3970.2064530849457, 'accumulated_eval_time': 18575.420905828476, 'accumulated_logging_time': 0.7943100929260254, 'global_step': 4022, 'preemption_count': 0}), (4142, {'train/loss': 0.12190909869923085, 'validation/loss': 0.12461322014707922, 'validation/num_examples': 83274637, 'test/loss': 0.12698302995593422, 'test/num_examples': 95000000, 'score': 4089.7952704429626, 'total_duration': 23086.243463277817, 'accumulated_submission_time': 4089.7952704429626, 'accumulated_eval_time': 18965.42326927185, 'accumulated_logging_time': 0.8256714344024658, 'global_step': 4142, 'preemption_count': 0}), (4265, {'train/loss': 0.12459496570564733, 'validation/loss': 0.1243746373516105, 'validation/num_examples': 83274637, 'test/loss': 0.12672727944464432, 'test/num_examples': 95000000, 'score': 4210.560853004456, 'total_duration': 23597.73993253708, 'accumulated_submission_time': 4210.560853004456, 'accumulated_eval_time': 19355.242249250412, 'accumulated_logging_time': 0.8442246913909912, 'global_step': 4265, 'preemption_count': 0}), (4381, {'train/loss': 0.12466743180299987, 'validation/loss': 0.12458743107409626, 'validation/num_examples': 83274637, 'test/loss': 0.12704688702982853, 'test/num_examples': 95000000, 'score': 4330.669893980026, 'total_duration': 24109.042078733444, 'accumulated_submission_time': 4330.669893980026, 'accumulated_eval_time': 19745.560178279877, 'accumulated_logging_time': 0.8625025749206543, 'global_step': 4381, 'preemption_count': 0}), (4505, {'train/loss': 0.12409469642653144, 'validation/loss': 0.12443356424487768, 'validation/num_examples': 83274637, 'test/loss': 0.1267989576599924, 'test/num_examples': 95000000, 'score': 4450.793030977249, 'total_duration': 24617.638649463654, 'accumulated_submission_time': 4450.793030977249, 'accumulated_eval_time': 20133.1359937191, 'accumulated_logging_time': 0.8813877105712891, 'global_step': 4505, 'preemption_count': 0}), (4627, {'train/loss': 0.12185764490026148, 'validation/loss': 0.12445862783719847, 'validation/num_examples': 83274637, 'test/loss': 0.12687804209345768, 'test/num_examples': 95000000, 'score': 4571.081116437912, 'total_duration': 25131.08515906334, 'accumulated_submission_time': 4571.081116437912, 'accumulated_eval_time': 20525.43200445175, 'accumulated_logging_time': 0.9007058143615723, 'global_step': 4627, 'preemption_count': 0}), (4749, {'train/loss': 0.12339741509591348, 'validation/loss': 0.12446318105601947, 'validation/num_examples': 83274637, 'test/loss': 0.12687660422624286, 'test/num_examples': 95000000, 'score': 4690.717064142227, 'total_duration': 25645.78223156929, 'accumulated_submission_time': 4690.717064142227, 'accumulated_eval_time': 20919.531650066376, 'accumulated_logging_time': 0.978297233581543, 'global_step': 4749, 'preemption_count': 0}), (4871, {'train/loss': 0.12138268558138818, 'validation/loss': 0.12437777469984829, 'validation/num_examples': 83274637, 'test/loss': 0.12672705537506906, 'test/num_examples': 95000000, 'score': 4810.248945713043, 'total_duration': 26156.7412481308, 'accumulated_submission_time': 4810.248945713043, 'accumulated_eval_time': 21310.065472364426, 'accumulated_logging_time': 0.9972858428955078, 'global_step': 4871, 'preemption_count': 0}), (4995, {'train/loss': 0.12405936316955858, 'validation/loss': 0.12425519503337303, 'validation/num_examples': 83274637, 'test/loss': 0.12659040654425371, 'test/num_examples': 95000000, 'score': 4930.463898658752, 'total_duration': 26665.71575832367, 'accumulated_submission_time': 4930.463898658752, 'accumulated_eval_time': 21697.972408771515, 'accumulated_logging_time': 1.0223040580749512, 'global_step': 4995, 'preemption_count': 0}), (5118, {'train/loss': 0.12156134794869104, 'validation/loss': 0.12428236397689668, 'validation/num_examples': 83274637, 'test/loss': 0.12658220535551373, 'test/num_examples': 95000000, 'score': 5050.426526784897, 'total_duration': 27175.18135023117, 'accumulated_submission_time': 5050.426526784897, 'accumulated_eval_time': 22086.59019947052, 'accumulated_logging_time': 1.0405488014221191, 'global_step': 5118, 'preemption_count': 0}), (5242, {'train/loss': 0.12281366401157222, 'validation/loss': 0.1243227352565676, 'validation/num_examples': 83274637, 'test/loss': 0.12662885685039318, 'test/num_examples': 95000000, 'score': 5170.3124215602875, 'total_duration': 27688.295424938202, 'accumulated_submission_time': 5170.3124215602875, 'accumulated_eval_time': 22478.917066335678, 'accumulated_logging_time': 1.0606460571289062, 'global_step': 5242, 'preemption_count': 0}), (5367, {'train/loss': 0.1240012564713503, 'validation/loss': 0.12419587596007815, 'validation/num_examples': 83274637, 'test/loss': 0.1265525299267016, 'test/num_examples': 95000000, 'score': 5290.612094640732, 'total_duration': 28202.2397043705, 'accumulated_submission_time': 5290.612094640732, 'accumulated_eval_time': 22871.664244651794, 'accumulated_logging_time': 1.102116346359253, 'global_step': 5367, 'preemption_count': 0}), (5491, {'train/loss': 0.1255880333273429, 'validation/loss': 0.12416732956113652, 'validation/num_examples': 83274637, 'test/loss': 0.12650941533018414, 'test/num_examples': 95000000, 'score': 5411.111576080322, 'total_duration': 28716.784876823425, 'accumulated_submission_time': 5411.111576080322, 'accumulated_eval_time': 23264.8827586174, 'accumulated_logging_time': 1.1208200454711914, 'global_step': 5491, 'preemption_count': 0}), (5611, {'train/loss': 0.1250117608231897, 'validation/loss': 0.12416414177612817, 'validation/num_examples': 83274637, 'test/loss': 0.12658224458353143, 'test/num_examples': 95000000, 'score': 5531.519562482834, 'total_duration': 29226.07232069969, 'accumulated_submission_time': 5531.519562482834, 'accumulated_eval_time': 23652.894488811493, 'accumulated_logging_time': 1.1391315460205078, 'global_step': 5611, 'preemption_count': 0}), (5733, {'train/loss': 0.1218625434886678, 'validation/loss': 0.12413444761060512, 'validation/num_examples': 83274637, 'test/loss': 0.12647722494623284, 'test/num_examples': 95000000, 'score': 5651.106167078018, 'total_duration': 29735.726541519165, 'accumulated_submission_time': 5651.106167078018, 'accumulated_eval_time': 24042.11881875992, 'accumulated_logging_time': 1.1583502292633057, 'global_step': 5733, 'preemption_count': 0}), (5856, {'train/loss': 0.12244205650434296, 'validation/loss': 0.12411334455122612, 'validation/num_examples': 83274637, 'test/loss': 0.1264304101319564, 'test/num_examples': 95000000, 'score': 5771.035926580429, 'total_duration': 30245.62953567505, 'accumulated_submission_time': 5771.035926580429, 'accumulated_eval_time': 24431.19833636284, 'accumulated_logging_time': 1.206749439239502, 'global_step': 5856, 'preemption_count': 0}), (5978, {'train/loss': 0.12180339718271782, 'validation/loss': 0.12411103954708265, 'validation/num_examples': 83274637, 'test/loss': 0.12646240563422756, 'test/num_examples': 95000000, 'score': 5890.758489370346, 'total_duration': 30756.044761657715, 'accumulated_submission_time': 5890.758489370346, 'accumulated_eval_time': 24821.024425268173, 'accumulated_logging_time': 1.2256195545196533, 'global_step': 5978, 'preemption_count': 0}), (6102, {'train/loss': 0.12303583072042215, 'validation/loss': 0.12409796456324053, 'validation/num_examples': 83274637, 'test/loss': 0.12644789869995118, 'test/num_examples': 95000000, 'score': 6010.606308698654, 'total_duration': 31265.220098733902, 'accumulated_submission_time': 6010.606308698654, 'accumulated_eval_time': 25209.397354602814, 'accumulated_logging_time': 1.3086576461791992, 'global_step': 6102, 'preemption_count': 0}), (6228, {'train/loss': 0.12120614627877498, 'validation/loss': 0.12415118430911123, 'validation/num_examples': 83274637, 'test/loss': 0.12653906890820954, 'test/num_examples': 95000000, 'score': 6130.327610254288, 'total_duration': 31775.37571954727, 'accumulated_submission_time': 6130.327610254288, 'accumulated_eval_time': 25598.942897319794, 'accumulated_logging_time': 1.3277554512023926, 'global_step': 6228, 'preemption_count': 0}), (6353, {'train/loss': 0.1222796146974562, 'validation/loss': 0.12408676659261551, 'validation/num_examples': 83274637, 'test/loss': 0.12643092393614117, 'test/num_examples': 95000000, 'score': 6251.561593294144, 'total_duration': 32286.87687563896, 'accumulated_submission_time': 6251.561593294144, 'accumulated_eval_time': 25988.358961105347, 'accumulated_logging_time': 1.347278118133545, 'global_step': 6353, 'preemption_count': 0}), (6476, {'train/loss': 0.12382189131686518, 'validation/loss': 0.12408259335786924, 'validation/num_examples': 83274637, 'test/loss': 0.12642106500549316, 'test/num_examples': 95000000, 'score': 6371.3185613155365, 'total_duration': 32796.89978790283, 'accumulated_submission_time': 6371.3185613155365, 'accumulated_eval_time': 26377.765162706375, 'accumulated_logging_time': 1.3853793144226074, 'global_step': 6476, 'preemption_count': 0}), (6600, {'train/loss': 0.12279644606408878, 'validation/loss': 0.12407576518216126, 'validation/num_examples': 83274637, 'test/loss': 0.12639773563445242, 'test/num_examples': 95000000, 'score': 6491.561371088028, 'total_duration': 33303.51187944412, 'accumulated_submission_time': 6491.561371088028, 'accumulated_eval_time': 26763.253753185272, 'accumulated_logging_time': 1.404968500137329, 'global_step': 6600, 'preemption_count': 0}), (6722, {'train/loss': 0.12109561285741309, 'validation/loss': 0.12412481251829917, 'validation/num_examples': 83274637, 'test/loss': 0.12650794251692923, 'test/num_examples': 95000000, 'score': 6611.492800712585, 'total_duration': 33813.259452581406, 'accumulated_submission_time': 6611.492800712585, 'accumulated_eval_time': 27152.18819618225, 'accumulated_logging_time': 1.4246456623077393, 'global_step': 6722, 'preemption_count': 0}), (6845, {'train/loss': 0.12436573623102243, 'validation/loss': 0.12413831232677495, 'validation/num_examples': 83274637, 'test/loss': 0.12650902922732704, 'test/num_examples': 95000000, 'score': 6731.540198326111, 'total_duration': 34325.22195005417, 'accumulated_submission_time': 6731.540198326111, 'accumulated_eval_time': 27543.18686771393, 'accumulated_logging_time': 1.4530954360961914, 'global_step': 6845, 'preemption_count': 0}), (6968, {'train/loss': 0.12409871071696167, 'validation/loss': 0.12414793238848974, 'validation/num_examples': 83274637, 'test/loss': 0.1265248481440494, 'test/num_examples': 95000000, 'score': 6851.418256521225, 'total_duration': 34835.656227350235, 'accumulated_submission_time': 6851.418256521225, 'accumulated_eval_time': 27932.84537243843, 'accumulated_logging_time': 1.4725987911224365, 'global_step': 6968, 'preemption_count': 0}), (7087, {'train/loss': 0.12132644572234969, 'validation/loss': 0.12410020596040247, 'validation/num_examples': 83274637, 'test/loss': 0.12642837091016268, 'test/num_examples': 95000000, 'score': 6971.287824630737, 'total_duration': 35345.626782655716, 'accumulated_submission_time': 6971.287824630737, 'accumulated_eval_time': 28322.10343337059, 'accumulated_logging_time': 1.4918618202209473, 'global_step': 7087, 'preemption_count': 0}), (7210, {'train/loss': 0.12061052252472541, 'validation/loss': 0.12412765872184647, 'validation/num_examples': 83274637, 'test/loss': 0.1264587066840724, 'test/num_examples': 95000000, 'score': 7091.614373683929, 'total_duration': 35856.643523693085, 'accumulated_submission_time': 7091.614373683929, 'accumulated_eval_time': 28711.935252189636, 'accumulated_logging_time': 1.5113720893859863, 'global_step': 7210, 'preemption_count': 0}), (7335, {'train/loss': 0.12295499959785236, 'validation/loss': 0.12414347056952652, 'validation/num_examples': 83274637, 'test/loss': 0.12651339044004742, 'test/num_examples': 95000000, 'score': 7212.341063261032, 'total_duration': 36371.40566778183, 'accumulated_submission_time': 7212.341063261032, 'accumulated_eval_time': 29105.076414585114, 'accumulated_logging_time': 1.5305519104003906, 'global_step': 7335, 'preemption_count': 0}), (7461, {'train/loss': 0.12517155237863628, 'validation/loss': 0.12409859664797958, 'validation/num_examples': 83274637, 'test/loss': 0.12642165964829294, 'test/num_examples': 95000000, 'score': 7333.16911482811, 'total_duration': 36882.65119767189, 'accumulated_submission_time': 7333.16911482811, 'accumulated_eval_time': 29494.60210800171, 'accumulated_logging_time': 1.5774853229522705, 'global_step': 7461, 'preemption_count': 0}), (7585, {'train/loss': 0.12223967814120128, 'validation/loss': 0.12407354944749928, 'validation/num_examples': 83274637, 'test/loss': 0.1264031073473077, 'test/num_examples': 95000000, 'score': 7453.064138650894, 'total_duration': 37394.306941986084, 'accumulated_submission_time': 7453.064138650894, 'accumulated_eval_time': 29885.42869091034, 'accumulated_logging_time': 1.6574649810791016, 'global_step': 7585, 'preemption_count': 0}), (7709, {'train/loss': 0.12293205525874124, 'validation/loss': 0.12419659054882172, 'validation/num_examples': 83274637, 'test/loss': 0.12654806381117167, 'test/num_examples': 95000000, 'score': 7573.724761009216, 'total_duration': 37905.06407427788, 'accumulated_submission_time': 7573.724761009216, 'accumulated_eval_time': 30274.67745780945, 'accumulated_logging_time': 1.6773288249969482, 'global_step': 7709, 'preemption_count': 0}), (7832, {'train/loss': 0.1246797614828919, 'validation/loss': 0.12413484338706857, 'validation/num_examples': 83274637, 'test/loss': 0.12639633949440404, 'test/num_examples': 95000000, 'score': 7693.310275793076, 'total_duration': 38415.35781812668, 'accumulated_submission_time': 7693.310275793076, 'accumulated_eval_time': 30664.501200199127, 'accumulated_logging_time': 1.6981863975524902, 'global_step': 7832, 'preemption_count': 0})], 'global_step': 7953}
I0317 01:35:13.031641 140547343606976 submission_runner.py:649] Timing: 7813.274732589722
I0317 01:35:13.031682 140547343606976 submission_runner.py:651] Total number of evals: 65
I0317 01:35:13.031714 140547343606976 submission_runner.py:652] ====================
I0317 01:35:13.031834 140547343606976 submission_runner.py:750] Final criteo1tb score: 1
