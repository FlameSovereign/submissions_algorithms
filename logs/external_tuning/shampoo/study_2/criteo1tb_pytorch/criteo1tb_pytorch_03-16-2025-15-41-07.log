torchrun --redirects 1:0,2:0,3:0,4:0,5:0,6:0,7:0 --standalone --nnodes=1 --nproc_per_node=8 submission_runner.py --framework=pytorch --workload=criteo1tb --submission_path=submissions_algorithms/leaderboard/external_tuning/shampoo_submission/submission.py --data_dir=/data/criteo1tb --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=submissions/rolling_leaderboard/external_tuning/shampoo/study_2 --overwrite=True --save_checkpoints=False --rng_seed=723912775 --torch_compile=true --tuning_ruleset=external --tuning_search_space=submissions_algorithms/leaderboard/external_tuning/shampoo_submission/tuning_search_space.json --num_tuning_trials=5 --hparam_start_index=3 --hparam_end_index=4 2>&1 | tee -a /logs/criteo1tb_pytorch_03-16-2025-15-41-07.log
W0316 15:41:09.037000 9 site-packages/torch/distributed/run.py:793] 
W0316 15:41:09.037000 9 site-packages/torch/distributed/run.py:793] *****************************************
W0316 15:41:09.037000 9 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0316 15:41:09.037000 9 site-packages/torch/distributed/run.py:793] *****************************************
2025-03-16 15:41:10.080660: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-03-16 15:41:10.080642: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-03-16 15:41:10.080644: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-03-16 15:41:10.080642: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-03-16 15:41:10.080642: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-03-16 15:41:10.080642: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-03-16 15:41:10.080643: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-03-16 15:41:10.080642: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1742139670.103397      47 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1742139670.103386      49 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1742139670.103396      44 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1742139670.103388      51 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1742139670.103400      48 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1742139670.103396      50 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1742139670.103397      46 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1742139670.103396      45 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1742139670.110482      47 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
E0000 00:00:1742139670.110481      45 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
E0000 00:00:1742139670.110482      50 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
E0000 00:00:1742139670.110481      51 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
E0000 00:00:1742139670.110482      44 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
E0000 00:00:1742139670.110482      49 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
E0000 00:00:1742139670.110490      48 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
E0000 00:00:1742139670.110516      46 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
[rank1]:[W316 15:41:17.004945394 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank2]:[W316 15:41:17.004962993 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank3]:[W316 15:41:17.048877699 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank7]:[W316 15:41:17.085395768 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 7]  using GPU 7 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank0]:[W316 15:41:17.139844963 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank6]:[W316 15:41:17.209952531 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 6]  using GPU 6 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank4]:[W316 15:41:17.223700985 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 4]  using GPU 4 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank5]:[W316 15:41:17.238649814 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 5]  using GPU 5 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
I0316 15:41:18.996782 140688500745408 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_2/criteo1tb_pytorch.
I0316 15:41:18.996784 140575059936448 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_2/criteo1tb_pytorch.
I0316 15:41:18.996794 140125047432384 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_2/criteo1tb_pytorch.
I0316 15:41:18.996784 140476660065472 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_2/criteo1tb_pytorch.
I0316 15:41:18.996789 139901308867776 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_2/criteo1tb_pytorch.
I0316 15:41:18.996799 140305644491968 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_2/criteo1tb_pytorch.
I0316 15:41:18.996881 140465193141440 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_2/criteo1tb_pytorch.
I0316 15:41:18.996876 140556456256704 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_2/criteo1tb_pytorch.
I0316 15:41:20.200490 140476660065472 logger_utils.py:97] Saving hparams to /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_2/criteo1tb_pytorch/trial_4/hparams.json.
I0316 15:41:20.200492 140575059936448 logger_utils.py:97] Saving hparams to /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_2/criteo1tb_pytorch/trial_4/hparams.json.
I0316 15:41:20.200711 140465193141440 logger_utils.py:97] Saving hparams to /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_2/criteo1tb_pytorch/trial_4/hparams.json.
I0316 15:41:20.200905 140688500745408 logger_utils.py:97] Saving hparams to /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_2/criteo1tb_pytorch/trial_4/hparams.json.
I0316 15:41:20.201041 140125047432384 submission_runner.py:606] Using RNG seed 723912775
I0316 15:41:20.201071 140305644491968 logger_utils.py:97] Saving hparams to /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_2/criteo1tb_pytorch/trial_4/hparams.json.
I0316 15:41:20.201064 139901308867776 logger_utils.py:97] Saving hparams to /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_2/criteo1tb_pytorch/trial_4/hparams.json.
I0316 15:41:20.202256 140125047432384 submission_runner.py:615] --- Tuning run 4/5 ---
I0316 15:41:20.202395 140125047432384 submission_runner.py:620] Creating tuning directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_2/criteo1tb_pytorch/trial_4.
I0316 15:41:20.202622 140125047432384 logger_utils.py:97] Saving hparams to /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_2/criteo1tb_pytorch/trial_4/hparams.json.
I0316 15:41:20.231342 140556456256704 logger_utils.py:91] Loading hparams from /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_2/criteo1tb_pytorch/trial_4/hparams.json.
I0316 15:41:20.537682 140125047432384 submission_runner.py:218] Initializing dataset.
I0316 15:41:20.537857 140125047432384 submission_runner.py:229] Initializing model.
W0316 15:41:26.527139 140125047432384 submission_runner.py:254] These workloads cannot be fully compiled under current PyTorch version. Proceeding without `torch.compile`.
I0316 15:41:26.527309 140125047432384 submission_runner.py:272] Initializing optimizer.
W0316 15:41:26.528381 140125047432384 distributed_shampoo.py:341] start_preconditioning_step set to -1. Setting start_preconditioning_step equal to precondition frequency 100 by default.
W0316 15:41:26.528636 140305644491968 submission_runner.py:254] These workloads cannot be fully compiled under current PyTorch version. Proceeding without `torch.compile`.
W0316 15:41:26.528973 140465193141440 submission_runner.py:254] These workloads cannot be fully compiled under current PyTorch version. Proceeding without `torch.compile`.
W0316 15:41:26.529130 140575059936448 submission_runner.py:254] These workloads cannot be fully compiled under current PyTorch version. Proceeding without `torch.compile`.
W0316 15:41:26.529199 140476660065472 submission_runner.py:254] These workloads cannot be fully compiled under current PyTorch version. Proceeding without `torch.compile`.
W0316 15:41:26.529348 140556456256704 submission_runner.py:254] These workloads cannot be fully compiled under current PyTorch version. Proceeding without `torch.compile`.
W0316 15:41:26.529386 140688500745408 submission_runner.py:254] These workloads cannot be fully compiled under current PyTorch version. Proceeding without `torch.compile`.
W0316 15:41:26.529453 139901308867776 submission_runner.py:254] These workloads cannot be fully compiled under current PyTorch version. Proceeding without `torch.compile`.
W0316 15:41:26.529711 140305644491968 distributed_shampoo.py:341] start_preconditioning_step set to -1. Setting start_preconditioning_step equal to precondition frequency 100 by default.
W0316 15:41:26.530087 140465193141440 distributed_shampoo.py:341] start_preconditioning_step set to -1. Setting start_preconditioning_step equal to precondition frequency 100 by default.
W0316 15:41:26.530259 140575059936448 distributed_shampoo.py:341] start_preconditioning_step set to -1. Setting start_preconditioning_step equal to precondition frequency 100 by default.
W0316 15:41:26.530377 140476660065472 distributed_shampoo.py:341] start_preconditioning_step set to -1. Setting start_preconditioning_step equal to precondition frequency 100 by default.
W0316 15:41:26.530521 140688500745408 distributed_shampoo.py:341] start_preconditioning_step set to -1. Setting start_preconditioning_step equal to precondition frequency 100 by default.
W0316 15:41:26.530606 140556456256704 distributed_shampoo.py:341] start_preconditioning_step set to -1. Setting start_preconditioning_step equal to precondition frequency 100 by default.
W0316 15:41:26.530612 139901308867776 distributed_shampoo.py:341] start_preconditioning_step set to -1. Setting start_preconditioning_step equal to precondition frequency 100 by default.
I0316 15:41:26.530673 140125047432384 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 0.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 0 (torch.Size([512, 13])), Block block_0 (torch.Size([512, 13])).
I0316 15:41:26.530848 140125047432384 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 1.block_0 ([torch.Size([0])]) for Parameter 1 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 15:41:26.531026 140125047432384 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 2.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 2 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 15:41:26.531143 140125047432384 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 3.block_0 ([torch.Size([0])]) for Parameter 3 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 15:41:26.531293 140125047432384 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 4.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 4 (torch.Size([128, 256])), Block block_0 (torch.Size([128, 256])).
I0316 15:41:26.531391 140125047432384 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 5.block_0 ([torch.Size([0])]) for Parameter 5 (torch.Size([128])), Block block_0 (torch.Size([128])).
I0316 15:41:26.531510 140125047432384 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 6.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 6 (torch.Size([1024, 506])), Block block_0 (torch.Size([1024, 506])).
I0316 15:41:26.531607 140125047432384 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 7.block_0 ([torch.Size([0])]) for Parameter 7 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 15:41:26.531910 140305644491968 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 0.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 0 (torch.Size([512, 13])), Block block_0 (torch.Size([512, 13])).
I0316 15:41:26.532073 140305644491968 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 1.block_0 ([torch.Size([0])]) for Parameter 1 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 15:41:26.532222 140305644491968 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 2.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 2 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 15:41:26.532258 140465193141440 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 0.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 0 (torch.Size([512, 13])), Block block_0 (torch.Size([512, 13])).
I0316 15:41:26.532334 140305644491968 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 3.block_0 ([torch.Size([0])]) for Parameter 3 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 15:41:26.532431 140465193141440 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 1.block_0 ([torch.Size([0])]) for Parameter 1 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 15:41:26.532463 140305644491968 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 4.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 4 (torch.Size([128, 256])), Block block_0 (torch.Size([128, 256])).
I0316 15:41:26.532560 140305644491968 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 5.block_0 ([torch.Size([0])]) for Parameter 5 (torch.Size([128])), Block block_0 (torch.Size([128])).
I0316 15:41:26.532639 140575059936448 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 0.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 0 (torch.Size([512, 13])), Block block_0 (torch.Size([512, 13])).
I0316 15:41:26.532837 140125047432384 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 8.block_0 ([torch.Size([1024, 1024]), torch.Size([1024, 1024])]) for Parameter 8 (torch.Size([1024, 1024])), Block block_0 (torch.Size([1024, 1024])).
I0316 15:41:26.532975 140125047432384 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 9.block_0 ([torch.Size([0])]) for Parameter 9 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 15:41:26.533091 140125047432384 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 10.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 10 (torch.Size([512, 1024])), Block block_0 (torch.Size([512, 1024])).
I0316 15:41:26.533096 140476660065472 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 0.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 0 (torch.Size([512, 13])), Block block_0 (torch.Size([512, 13])).
I0316 15:41:26.533178 140125047432384 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 11.block_0 ([torch.Size([0])]) for Parameter 11 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 15:41:26.533193 140465193141440 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 2.block_0 ([torch.Size([256, 256]), torch.Size([512, 512])]) for Parameter 2 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 15:41:26.533274 140476660065472 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 1.block_0 ([torch.Size([0])]) for Parameter 1 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 15:41:26.533294 140125047432384 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 12.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 12 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 15:41:26.533350 140465193141440 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 3.block_0 ([torch.Size([0])]) for Parameter 3 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 15:41:26.533373 140125047432384 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 13.block_0 ([torch.Size([0])]) for Parameter 13 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 15:41:26.533387 140688500745408 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 0.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 0 (torch.Size([512, 13])), Block block_0 (torch.Size([512, 13])).
I0316 15:41:26.533413 140305644491968 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 6.block_0 ([torch.Size([1024, 1024]), torch.Size([506, 506])]) for Parameter 6 (torch.Size([1024, 506])), Block block_0 (torch.Size([1024, 506])).
I0316 15:41:26.533445 140125047432384 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 14.block_0 ([torch.Size([0])]) for Parameter 14 (torch.Size([1, 256])), Block block_0 (torch.Size([256])).
I0316 15:41:26.533432 140476660065472 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 2.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 2 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 15:41:26.533494 140465193141440 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 4.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 4 (torch.Size([128, 256])), Block block_0 (torch.Size([128, 256])).
I0316 15:41:26.533466 140575059936448 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 1.block_0 ([torch.Size([512, 512])]) for Parameter 1 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 15:41:26.533521 140125047432384 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 15.block_0 ([torch.Size([0])]) for Parameter 15 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0316 15:41:26.533536 140305644491968 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 7.block_0 ([torch.Size([0])]) for Parameter 7 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 15:41:26.533536 140476660065472 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 3.block_0 ([torch.Size([0])]) for Parameter 3 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 15:41:26.533558 140688500745408 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 1.block_0 ([torch.Size([0])]) for Parameter 1 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 15:41:26.533531 139901308867776 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 0.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 0 (torch.Size([512, 13])), Block block_0 (torch.Size([512, 13])).
I0316 15:41:26.533587 140465193141440 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 5.block_0 ([torch.Size([0])]) for Parameter 5 (torch.Size([128])), Block block_0 (torch.Size([128])).
I0316 15:41:26.533624 140125047432384 shampoo_preconditioner_list.py:612] Rank 0: ShampooPreconditionerList Numel Breakdown: (524626, 524288, 655360, 131072, 163840, 32768, 2609224, 2097152, 4194304, 2097152, 2621440, 524288, 655360, 131072, 131072, 2)
I0316 15:41:26.533664 140125047432384 shampoo_preconditioner_list.py:615] Rank 0: ShampooPreconditionerList Bytes Breakdown: (2098504,)
I0316 15:41:26.533657 140575059936448 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 2.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 2 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 15:41:26.533678 140305644491968 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 8.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 8 (torch.Size([1024, 1024])), Block block_0 (torch.Size([1024, 1024])).
I0316 15:41:26.533681 140476660065472 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 4.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 4 (torch.Size([128, 256])), Block block_0 (torch.Size([128, 256])).
I0316 15:41:26.533704 140125047432384 shampoo_preconditioner_list.py:618] Rank 0: ShampooPreconditionerList Total Elements: 17093020
I0316 15:41:26.533728 140465193141440 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 6.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 6 (torch.Size([1024, 506])), Block block_0 (torch.Size([1024, 506])).
I0316 15:41:26.533712 139901308867776 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 1.block_0 ([torch.Size([0])]) for Parameter 1 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 15:41:26.533736 140125047432384 shampoo_preconditioner_list.py:621] Rank 0: ShampooPreconditionerList Total Bytes: 2098504
I0316 15:41:26.533739 140688500745408 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 2.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 2 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 15:41:26.533770 140305644491968 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 9.block_0 ([torch.Size([0])]) for Parameter 9 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 15:41:26.533778 140476660065472 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 5.block_0 ([torch.Size([0])]) for Parameter 5 (torch.Size([128])), Block block_0 (torch.Size([128])).
I0316 15:41:26.533833 140465193141440 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 7.block_0 ([torch.Size([0])]) for Parameter 7 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 15:41:26.533837 140575059936448 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 3.block_0 ([torch.Size([256, 256])]) for Parameter 3 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 15:41:26.533859 140688500745408 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 3.block_0 ([torch.Size([0])]) for Parameter 3 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 15:41:26.533874 140305644491968 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 10.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 10 (torch.Size([512, 1024])), Block block_0 (torch.Size([512, 1024])).
I0316 15:41:26.533867 139901308867776 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 2.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 2 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 15:41:26.533878 140125047432384 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([512, 13])), Block block_0 (torch.Size([512, 13])).
I0316 15:41:26.533890 140476660065472 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 6.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 6 (torch.Size([1024, 506])), Block block_0 (torch.Size([1024, 506])).
I0316 15:41:26.533935 140125047432384 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 15:41:26.533950 140465193141440 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 8.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 8 (torch.Size([1024, 1024])), Block block_0 (torch.Size([1024, 1024])).
I0316 15:41:26.533960 140305644491968 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 11.block_0 ([torch.Size([0])]) for Parameter 11 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 15:41:26.533969 139901308867776 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 3.block_0 ([torch.Size([0])]) for Parameter 3 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 15:41:26.533988 140476660065472 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 7.block_0 ([torch.Size([0])]) for Parameter 7 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 15:41:26.533993 140125047432384 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 15:41:26.533990 140688500745408 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 4.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 4 (torch.Size([128, 256])), Block block_0 (torch.Size([128, 256])).
I0316 15:41:26.533995 140575059936448 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 4.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 4 (torch.Size([128, 256])), Block block_0 (torch.Size([128, 256])).
I0316 15:41:26.534031 140465193141440 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 9.block_0 ([torch.Size([0])]) for Parameter 9 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 15:41:26.534039 140125047432384 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 15:41:26.534075 140688500745408 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 5.block_0 ([torch.Size([0])]) for Parameter 5 (torch.Size([128])), Block block_0 (torch.Size([128])).
I0316 15:41:26.534085 140125047432384 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 4.block_0 (torch.Size([0])) for Parameter 4 (torch.Size([128, 256])), Block block_0 (torch.Size([128, 256])).
I0316 15:41:26.534082 140305644491968 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 12.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 12 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 15:41:26.534118 140476660065472 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 8.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 8 (torch.Size([1024, 1024])), Block block_0 (torch.Size([1024, 1024])).
I0316 15:41:26.534128 140125047432384 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 5.block_0 (torch.Size([0])) for Parameter 5 (torch.Size([128])), Block block_0 (torch.Size([128])).
I0316 15:41:26.534136 140575059936448 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 5.block_0 ([torch.Size([128, 128])]) for Parameter 5 (torch.Size([128])), Block block_0 (torch.Size([128])).
I0316 15:41:26.534145 140465193141440 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 10.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 10 (torch.Size([512, 1024])), Block block_0 (torch.Size([512, 1024])).
I0316 15:41:26.534158 140305644491968 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 13.block_0 ([torch.Size([0])]) for Parameter 13 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 15:41:26.534178 140125047432384 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 6.block_0 (torch.Size([0])) for Parameter 6 (torch.Size([1024, 506])), Block block_0 (torch.Size([1024, 506])).
I0316 15:41:26.534191 140688500745408 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 6.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 6 (torch.Size([1024, 506])), Block block_0 (torch.Size([1024, 506])).
I0316 15:41:26.534151 140556456256704 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 0.block_0 ([torch.Size([512, 512]), torch.Size([13, 13])]) for Parameter 0 (torch.Size([512, 13])), Block block_0 (torch.Size([512, 13])).
I0316 15:41:26.534200 139901308867776 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 4.block_0 ([torch.Size([128, 128]), torch.Size([256, 256])]) for Parameter 4 (torch.Size([128, 256])), Block block_0 (torch.Size([128, 256])).
I0316 15:41:26.534218 140476660065472 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 9.block_0 ([torch.Size([0])]) for Parameter 9 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 15:41:26.534225 140465193141440 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 11.block_0 ([torch.Size([0])]) for Parameter 11 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 15:41:26.534234 140305644491968 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 14.block_0 ([torch.Size([0])]) for Parameter 14 (torch.Size([1, 256])), Block block_0 (torch.Size([256])).
I0316 15:41:26.534237 140125047432384 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 7.block_0 (torch.Size([0])) for Parameter 7 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 15:41:26.534272 140575059936448 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 6.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 6 (torch.Size([1024, 506])), Block block_0 (torch.Size([1024, 506])).
I0316 15:41:26.534298 140688500745408 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 7.block_0 ([torch.Size([0])]) for Parameter 7 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 15:41:26.534304 139901308867776 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 5.block_0 ([torch.Size([0])]) for Parameter 5 (torch.Size([128])), Block block_0 (torch.Size([128])).
I0316 15:41:26.534315 140305644491968 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 15.block_0 ([torch.Size([0])]) for Parameter 15 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0316 15:41:26.534328 140465193141440 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 12.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 12 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 15:41:26.534330 140125047432384 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 8.block_0 (torch.Size([1024, 1024])) for Parameter 8 (torch.Size([1024, 1024])), Block block_0 (torch.Size([1024, 1024])).
I0316 15:41:26.534356 140556456256704 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 1.block_0 ([torch.Size([0])]) for Parameter 1 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 15:41:26.534389 140125047432384 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 9.block_0 (torch.Size([0])) for Parameter 9 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 15:41:26.534394 140305644491968 shampoo_preconditioner_list.py:612] Rank 2: ShampooPreconditionerList Numel Breakdown: (524626, 524288, 655360, 131072, 163840, 32768, 2609224, 2097152, 4194304, 2097152, 2621440, 524288, 655360, 131072, 131072, 2)
I0316 15:41:26.534413 139901308867776 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 6.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 6 (torch.Size([1024, 506])), Block block_0 (torch.Size([1024, 506])).
I0316 15:41:26.534414 140465193141440 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 13.block_0 ([torch.Size([0])]) for Parameter 13 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 15:41:26.534419 140688500745408 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 8.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 8 (torch.Size([1024, 1024])), Block block_0 (torch.Size([1024, 1024])).
I0316 15:41:26.534428 140305644491968 shampoo_preconditioner_list.py:615] Rank 2: ShampooPreconditionerList Bytes Breakdown: (2098504,)
I0316 15:41:26.534435 140125047432384 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 10.block_0 (torch.Size([0])) for Parameter 10 (torch.Size([512, 1024])), Block block_0 (torch.Size([512, 1024])).
I0316 15:41:26.534431 140575059936448 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 7.block_0 ([torch.Size([1024, 1024])]) for Parameter 7 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 15:41:26.534456 140305644491968 shampoo_preconditioner_list.py:618] Rank 2: ShampooPreconditionerList Total Elements: 17093020
I0316 15:41:26.534482 140305644491968 shampoo_preconditioner_list.py:621] Rank 2: ShampooPreconditionerList Total Bytes: 2098504
I0316 15:41:26.534480 140125047432384 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 11.block_0 (torch.Size([0])) for Parameter 11 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 15:41:26.534491 140465193141440 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 14.block_0 ([torch.Size([0])]) for Parameter 14 (torch.Size([1, 256])), Block block_0 (torch.Size([256])).
I0316 15:41:26.534501 140688500745408 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 9.block_0 ([torch.Size([0])]) for Parameter 9 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 15:41:26.534523 140125047432384 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 12.block_0 (torch.Size([0])) for Parameter 12 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 15:41:26.534518 139901308867776 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 7.block_0 ([torch.Size([0])]) for Parameter 7 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 15:41:26.534520 140556456256704 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 2.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 2 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 15:41:26.534563 140465193141440 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 15.block_0 ([torch.Size([0])]) for Parameter 15 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0316 15:41:26.534562 140575059936448 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 8.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 8 (torch.Size([1024, 1024])), Block block_0 (torch.Size([1024, 1024])).
I0316 15:41:26.534576 140125047432384 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 13.block_0 (torch.Size([0])) for Parameter 13 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 15:41:26.534603 140688500745408 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 10.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 10 (torch.Size([512, 1024])), Block block_0 (torch.Size([512, 1024])).
I0316 15:41:26.534612 140305644491968 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([512, 13])), Block block_0 (torch.Size([512, 13])).
I0316 15:41:26.534626 140125047432384 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 14.block_0 (torch.Size([0])) for Parameter 14 (torch.Size([1, 256])), Block block_0 (torch.Size([256])).
I0316 15:41:26.534628 139901308867776 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 8.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 8 (torch.Size([1024, 1024])), Block block_0 (torch.Size([1024, 1024])).
I0316 15:41:26.534646 140465193141440 shampoo_preconditioner_list.py:612] Rank 3: ShampooPreconditionerList Numel Breakdown: (524626, 524288, 655360, 131072, 163840, 32768, 2609224, 2097152, 4194304, 2097152, 2621440, 524288, 655360, 131072, 131072, 2)
I0316 15:41:26.534651 140556456256704 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 3.block_0 ([torch.Size([0])]) for Parameter 3 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 15:41:26.534670 140125047432384 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 15.block_0 (torch.Size([0])) for Parameter 15 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0316 15:41:26.534666 140305644491968 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 15:41:26.534684 140688500745408 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 11.block_0 ([torch.Size([0])]) for Parameter 11 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 15:41:26.534692 140465193141440 shampoo_preconditioner_list.py:615] Rank 3: ShampooPreconditionerList Bytes Breakdown: (2098504,)
I0316 15:41:26.534722 140465193141440 shampoo_preconditioner_list.py:618] Rank 3: ShampooPreconditionerList Total Elements: 17093020
I0316 15:41:26.534717 139901308867776 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 9.block_0 ([torch.Size([0])]) for Parameter 9 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 15:41:26.534722 140305644491968 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 15:41:26.534726 140125047432384 shampoo_preconditioner_list.py:375] Rank 0: AdaGradPreconditionerList Numel Breakdown: (0, 0, 0, 0, 0, 0, 0, 0, 1048576, 0, 0, 0, 0, 0, 0, 0)
I0316 15:41:26.534753 140465193141440 shampoo_preconditioner_list.py:621] Rank 3: ShampooPreconditionerList Total Bytes: 2098504
I0316 15:41:26.534765 140125047432384 shampoo_preconditioner_list.py:378] Rank 0: AdaGradPreconditionerList Bytes Breakdown: (0, 0, 0, 0, 0, 0, 0, 0, 4194304, 0, 0, 0, 0, 0, 0, 0)
I0316 15:41:26.534766 140305644491968 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 15:41:26.534798 140125047432384 shampoo_preconditioner_list.py:381] Rank 0: AdaGradPreconditionerList Total Elements: 1048576
I0316 15:41:26.534809 140305644491968 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 4.block_0 (torch.Size([0])) for Parameter 4 (torch.Size([128, 256])), Block block_0 (torch.Size([128, 256])).
I0316 15:41:26.534808 140556456256704 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 4.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 4 (torch.Size([128, 256])), Block block_0 (torch.Size([128, 256])).
I0316 15:41:26.534820 139901308867776 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 10.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 10 (torch.Size([512, 1024])), Block block_0 (torch.Size([512, 1024])).
I0316 15:41:26.534829 140125047432384 shampoo_preconditioner_list.py:384] Rank 0: AdaGradPreconditionerList Total Bytes: 4194304
I0316 15:41:26.534806 140476660065472 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 10.block_0 ([torch.Size([512, 512]), torch.Size([1024, 1024])]) for Parameter 10 (torch.Size([512, 1024])), Block block_0 (torch.Size([512, 1024])).
I0316 15:41:26.534852 140305644491968 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 5.block_0 (torch.Size([0])) for Parameter 5 (torch.Size([128])), Block block_0 (torch.Size([128])).
I0316 15:41:26.534883 140465193141440 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([512, 13])), Block block_0 (torch.Size([512, 13])).
I0316 15:41:26.534901 139901308867776 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 11.block_0 ([torch.Size([0])]) for Parameter 11 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 15:41:26.534902 140556456256704 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 5.block_0 ([torch.Size([0])]) for Parameter 5 (torch.Size([128])), Block block_0 (torch.Size([128])).
I0316 15:41:26.534940 140305644491968 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 6.block_0 (torch.Size([1024, 506])) for Parameter 6 (torch.Size([1024, 506])), Block block_0 (torch.Size([1024, 506])).
I0316 15:41:26.534937 140476660065472 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 11.block_0 ([torch.Size([0])]) for Parameter 11 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 15:41:26.534947 140465193141440 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 15:41:26.535014 139901308867776 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 12.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 12 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 15:41:26.535016 140305644491968 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 7.block_0 (torch.Size([0])) for Parameter 7 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 15:41:26.535035 140556456256704 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 6.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 6 (torch.Size([1024, 506])), Block block_0 (torch.Size([1024, 506])).
I0316 15:41:26.535054 140476660065472 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 12.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 12 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 15:41:26.535065 140305644491968 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 8.block_0 (torch.Size([0])) for Parameter 8 (torch.Size([1024, 1024])), Block block_0 (torch.Size([1024, 1024])).
I0316 15:41:26.535098 139901308867776 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 13.block_0 ([torch.Size([0])]) for Parameter 13 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 15:41:26.535109 140305644491968 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 9.block_0 (torch.Size([0])) for Parameter 9 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 15:41:26.535087 140575059936448 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 9.block_0 ([torch.Size([1024, 1024])]) for Parameter 9 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 15:41:26.535136 140476660065472 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 13.block_0 ([torch.Size([0])]) for Parameter 13 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 15:41:26.535143 140556456256704 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 7.block_0 ([torch.Size([0])]) for Parameter 7 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 15:41:26.535152 140305644491968 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 10.block_0 (torch.Size([0])) for Parameter 10 (torch.Size([512, 1024])), Block block_0 (torch.Size([512, 1024])).
I0316 15:41:26.535183 139901308867776 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 14.block_0 ([torch.Size([0])]) for Parameter 14 (torch.Size([1, 256])), Block block_0 (torch.Size([256])).
I0316 15:41:26.535195 140305644491968 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 11.block_0 (torch.Size([0])) for Parameter 11 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 15:41:26.535222 140476660065472 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 14.block_0 ([torch.Size([0])]) for Parameter 14 (torch.Size([1, 256])), Block block_0 (torch.Size([256])).
I0316 15:41:26.535237 140305644491968 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 12.block_0 (torch.Size([0])) for Parameter 12 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 15:41:26.535232 140575059936448 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 10.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 10 (torch.Size([512, 1024])), Block block_0 (torch.Size([512, 1024])).
I0316 15:41:26.535257 140556456256704 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 8.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 8 (torch.Size([1024, 1024])), Block block_0 (torch.Size([1024, 1024])).
I0316 15:41:26.535262 139901308867776 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 15.block_0 ([torch.Size([0])]) for Parameter 15 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0316 15:41:26.535277 140305644491968 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 13.block_0 (torch.Size([0])) for Parameter 13 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 15:41:26.535311 140476660065472 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 15.block_0 ([torch.Size([0])]) for Parameter 15 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0316 15:41:26.535318 140305644491968 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 14.block_0 (torch.Size([0])) for Parameter 14 (torch.Size([1, 256])), Block block_0 (torch.Size([256])).
I0316 15:41:26.535354 140556456256704 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 9.block_0 ([torch.Size([0])]) for Parameter 9 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 15:41:26.535347 139901308867776 shampoo_preconditioner_list.py:612] Rank 5: ShampooPreconditionerList Numel Breakdown: (524626, 524288, 655360, 131072, 163840, 32768, 2609224, 2097152, 4194304, 2097152, 2621440, 524288, 655360, 131072, 131072, 2)
I0316 15:41:26.535333 140125047432384 submission.py:105] Large parameters (embedding tables) detected (dim >= 1_000_000). Instantiating Row-Wise AdaGrad...
I0316 15:41:26.535376 140305644491968 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 15.block_0 (torch.Size([0])) for Parameter 15 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0316 15:41:26.535400 139901308867776 shampoo_preconditioner_list.py:615] Rank 5: ShampooPreconditionerList Bytes Breakdown: (2098504,)
I0316 15:41:26.535421 140476660065472 shampoo_preconditioner_list.py:612] Rank 1: ShampooPreconditionerList Numel Breakdown: (524626, 524288, 655360, 131072, 163840, 32768, 2609224, 2097152, 4194304, 2097152, 2621440, 524288, 655360, 131072, 131072, 2)
I0316 15:41:26.535432 140305644491968 shampoo_preconditioner_list.py:375] Rank 2: AdaGradPreconditionerList Numel Breakdown: (0, 0, 0, 0, 0, 0, 518144, 0, 0, 0, 0, 0, 0, 0, 0, 0)
I0316 15:41:26.535436 139901308867776 shampoo_preconditioner_list.py:618] Rank 5: ShampooPreconditionerList Total Elements: 17093020
I0316 15:41:26.535427 140688500745408 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 12.block_0 ([torch.Size([256, 256]), torch.Size([512, 512])]) for Parameter 12 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 15:41:26.535464 140305644491968 shampoo_preconditioner_list.py:378] Rank 2: AdaGradPreconditionerList Bytes Breakdown: (0, 0, 0, 0, 0, 0, 2072576, 0, 0, 0, 0, 0, 0, 0, 0, 0)
I0316 15:41:26.535464 139901308867776 shampoo_preconditioner_list.py:621] Rank 5: ShampooPreconditionerList Total Bytes: 2098504
I0316 15:41:26.535464 140476660065472 shampoo_preconditioner_list.py:615] Rank 1: ShampooPreconditionerList Bytes Breakdown: (2098504,)
I0316 15:41:26.535447 140465193141440 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([256, 512])) for Parameter 2 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 15:41:26.535479 140556456256704 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 10.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 10 (torch.Size([512, 1024])), Block block_0 (torch.Size([512, 1024])).
I0316 15:41:26.535490 140305644491968 shampoo_preconditioner_list.py:381] Rank 2: AdaGradPreconditionerList Total Elements: 518144
I0316 15:41:26.535495 140476660065472 shampoo_preconditioner_list.py:618] Rank 1: ShampooPreconditionerList Total Elements: 17093020
I0316 15:41:26.535517 140305644491968 shampoo_preconditioner_list.py:384] Rank 2: AdaGradPreconditionerList Total Bytes: 2072576
I0316 15:41:26.535524 140476660065472 shampoo_preconditioner_list.py:621] Rank 1: ShampooPreconditionerList Total Bytes: 2098504
I0316 15:41:26.535542 140465193141440 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 15:41:26.535560 140688500745408 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 13.block_0 ([torch.Size([0])]) for Parameter 13 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 15:41:26.535581 140556456256704 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 11.block_0 ([torch.Size([0])]) for Parameter 11 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 15:41:26.535589 139901308867776 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([512, 13])), Block block_0 (torch.Size([512, 13])).
I0316 15:41:26.535608 140465193141440 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 4.block_0 (torch.Size([0])) for Parameter 4 (torch.Size([128, 256])), Block block_0 (torch.Size([128, 256])).
I0316 15:41:26.535645 139901308867776 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 15:41:26.535645 140688500745408 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 14.block_0 ([torch.Size([0])]) for Parameter 14 (torch.Size([1, 256])), Block block_0 (torch.Size([256])).
I0316 15:41:26.535657 140465193141440 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 5.block_0 (torch.Size([0])) for Parameter 5 (torch.Size([128])), Block block_0 (torch.Size([128])).
I0316 15:41:26.535659 140476660065472 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([512, 13])), Block block_0 (torch.Size([512, 13])).
I0316 15:41:26.535709 139901308867776 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 15:41:26.535709 140556456256704 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 12.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 12 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 15:41:26.535714 140465193141440 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 6.block_0 (torch.Size([0])) for Parameter 6 (torch.Size([1024, 506])), Block block_0 (torch.Size([1024, 506])).
I0316 15:41:26.535738 140476660065472 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 15:41:26.535738 140688500745408 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 15.block_0 ([torch.Size([0])]) for Parameter 15 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0316 15:41:26.535766 139901308867776 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 15:41:26.535768 140465193141440 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 7.block_0 (torch.Size([0])) for Parameter 7 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 15:41:26.535788 140556456256704 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 13.block_0 ([torch.Size([0])]) for Parameter 13 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 15:41:26.535796 140476660065472 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 15:41:26.535784 140575059936448 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 11.block_0 ([torch.Size([512, 512])]) for Parameter 11 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 15:41:26.535817 140465193141440 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 8.block_0 (torch.Size([0])) for Parameter 8 (torch.Size([1024, 1024])), Block block_0 (torch.Size([1024, 1024])).
I0316 15:41:26.535832 140688500745408 shampoo_preconditioner_list.py:612] Rank 4: ShampooPreconditionerList Numel Breakdown: (524626, 524288, 655360, 131072, 163840, 32768, 2609224, 2097152, 4194304, 2097152, 2621440, 524288, 655360, 131072, 131072, 2)
I0316 15:41:26.535845 140476660065472 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 15:41:26.535864 140465193141440 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 9.block_0 (torch.Size([0])) for Parameter 9 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 15:41:26.535866 139901308867776 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 4.block_0 (torch.Size([128, 256])) for Parameter 4 (torch.Size([128, 256])), Block block_0 (torch.Size([128, 256])).
I0316 15:41:26.535877 140688500745408 shampoo_preconditioner_list.py:615] Rank 4: ShampooPreconditionerList Bytes Breakdown: (2098504,)
I0316 15:41:26.535881 140556456256704 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 14.block_0 ([torch.Size([0])]) for Parameter 14 (torch.Size([1, 256])), Block block_0 (torch.Size([256])).
I0316 15:41:26.535894 140476660065472 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 4.block_0 (torch.Size([0])) for Parameter 4 (torch.Size([128, 256])), Block block_0 (torch.Size([128, 256])).
I0316 15:41:26.535908 140688500745408 shampoo_preconditioner_list.py:618] Rank 4: ShampooPreconditionerList Total Elements: 17093020
I0316 15:41:26.535918 140465193141440 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 10.block_0 (torch.Size([0])) for Parameter 10 (torch.Size([512, 1024])), Block block_0 (torch.Size([512, 1024])).
I0316 15:41:26.535930 139901308867776 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 5.block_0 (torch.Size([0])) for Parameter 5 (torch.Size([128])), Block block_0 (torch.Size([128])).
I0316 15:41:26.535942 140688500745408 shampoo_preconditioner_list.py:621] Rank 4: ShampooPreconditionerList Total Bytes: 2098504
I0316 15:41:26.535942 140476660065472 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 5.block_0 (torch.Size([0])) for Parameter 5 (torch.Size([128])), Block block_0 (torch.Size([128])).
I0316 15:41:26.535937 140575059936448 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 12.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 12 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 15:41:26.535959 140556456256704 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 15.block_0 ([torch.Size([0])]) for Parameter 15 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0316 15:41:26.535964 140465193141440 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 11.block_0 (torch.Size([0])) for Parameter 11 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 15:41:26.535989 139901308867776 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 6.block_0 (torch.Size([0])) for Parameter 6 (torch.Size([1024, 506])), Block block_0 (torch.Size([1024, 506])).
I0316 15:41:26.535990 140476660065472 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 6.block_0 (torch.Size([0])) for Parameter 6 (torch.Size([1024, 506])), Block block_0 (torch.Size([1024, 506])).
I0316 15:41:26.536010 140465193141440 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 12.block_0 (torch.Size([0])) for Parameter 12 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 15:41:26.536043 140476660065472 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 7.block_0 (torch.Size([0])) for Parameter 7 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 15:41:26.536048 139901308867776 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 7.block_0 (torch.Size([0])) for Parameter 7 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 15:41:26.536060 140465193141440 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 13.block_0 (torch.Size([0])) for Parameter 13 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 15:41:26.536058 140556456256704 shampoo_preconditioner_list.py:612] Rank 6: ShampooPreconditionerList Numel Breakdown: (524626, 524288, 655360, 131072, 163840, 32768, 2609224, 2097152, 4194304, 2097152, 2621440, 524288, 655360, 131072, 131072, 2)
I0316 15:41:26.536077 140688500745408 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([512, 13])), Block block_0 (torch.Size([512, 13])).
I0316 15:41:26.536094 140556456256704 shampoo_preconditioner_list.py:615] Rank 6: ShampooPreconditionerList Bytes Breakdown: (2098504,)
I0316 15:41:26.536097 139901308867776 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 8.block_0 (torch.Size([0])) for Parameter 8 (torch.Size([1024, 1024])), Block block_0 (torch.Size([1024, 1024])).
I0316 15:41:26.536100 140476660065472 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 8.block_0 (torch.Size([0])) for Parameter 8 (torch.Size([1024, 1024])), Block block_0 (torch.Size([1024, 1024])).
I0316 15:41:26.536104 140465193141440 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 14.block_0 (torch.Size([0])) for Parameter 14 (torch.Size([1, 256])), Block block_0 (torch.Size([256])).
I0316 15:41:26.536124 140556456256704 shampoo_preconditioner_list.py:618] Rank 6: ShampooPreconditionerList Total Elements: 17093020
I0316 15:41:26.536142 139901308867776 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 9.block_0 (torch.Size([0])) for Parameter 9 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 15:41:26.536143 140688500745408 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 15:41:26.536148 140476660065472 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 9.block_0 (torch.Size([0])) for Parameter 9 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 15:41:26.536160 140556456256704 shampoo_preconditioner_list.py:621] Rank 6: ShampooPreconditionerList Total Bytes: 2098504
I0316 15:41:26.536158 140465193141440 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 15.block_0 (torch.Size([0])) for Parameter 15 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0316 15:41:26.536199 139901308867776 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 10.block_0 (torch.Size([0])) for Parameter 10 (torch.Size([512, 1024])), Block block_0 (torch.Size([512, 1024])).
I0316 15:41:26.536205 140688500745408 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 15:41:26.536226 140465193141440 shampoo_preconditioner_list.py:375] Rank 3: AdaGradPreconditionerList Numel Breakdown: (0, 0, 131072, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)
I0316 15:41:26.536248 139901308867776 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 11.block_0 (torch.Size([0])) for Parameter 11 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 15:41:26.536249 140688500745408 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 15:41:26.536262 140465193141440 shampoo_preconditioner_list.py:378] Rank 3: AdaGradPreconditionerList Bytes Breakdown: (0, 0, 524288, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)
I0316 15:41:26.536260 140476660065472 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 10.block_0 (torch.Size([512, 1024])) for Parameter 10 (torch.Size([512, 1024])), Block block_0 (torch.Size([512, 1024])).
I0316 15:41:26.536288 140465193141440 shampoo_preconditioner_list.py:381] Rank 3: AdaGradPreconditionerList Total Elements: 131072
I0316 15:41:26.536293 140688500745408 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 4.block_0 (torch.Size([0])) for Parameter 4 (torch.Size([128, 256])), Block block_0 (torch.Size([128, 256])).
I0316 15:41:26.536297 139901308867776 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 12.block_0 (torch.Size([0])) for Parameter 12 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 15:41:26.536318 140465193141440 shampoo_preconditioner_list.py:384] Rank 3: AdaGradPreconditionerList Total Bytes: 524288
I0316 15:41:26.536324 140476660065472 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 11.block_0 (torch.Size([0])) for Parameter 11 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 15:41:26.536337 140688500745408 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 5.block_0 (torch.Size([0])) for Parameter 5 (torch.Size([128])), Block block_0 (torch.Size([128])).
I0316 15:41:26.536339 139901308867776 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 13.block_0 (torch.Size([0])) for Parameter 13 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 15:41:26.536370 140476660065472 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 12.block_0 (torch.Size([0])) for Parameter 12 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 15:41:26.536353 140556456256704 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([512, 13])) for Parameter 0 (torch.Size([512, 13])), Block block_0 (torch.Size([512, 13])).
I0316 15:41:26.536380 139901308867776 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 14.block_0 (torch.Size([0])) for Parameter 14 (torch.Size([1, 256])), Block block_0 (torch.Size([256])).
I0316 15:41:26.536381 140688500745408 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 6.block_0 (torch.Size([0])) for Parameter 6 (torch.Size([1024, 506])), Block block_0 (torch.Size([1024, 506])).
I0316 15:41:26.536421 139901308867776 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 15.block_0 (torch.Size([0])) for Parameter 15 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0316 15:41:26.536423 140476660065472 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 13.block_0 (torch.Size([0])) for Parameter 13 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 15:41:26.536430 140688500745408 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 7.block_0 (torch.Size([0])) for Parameter 7 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 15:41:26.536429 140556456256704 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 15:41:26.536468 140476660065472 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 14.block_0 (torch.Size([0])) for Parameter 14 (torch.Size([1, 256])), Block block_0 (torch.Size([256])).
I0316 15:41:26.536476 140688500745408 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 8.block_0 (torch.Size([0])) for Parameter 8 (torch.Size([1024, 1024])), Block block_0 (torch.Size([1024, 1024])).
I0316 15:41:26.536479 139901308867776 shampoo_preconditioner_list.py:375] Rank 5: AdaGradPreconditionerList Numel Breakdown: (0, 0, 0, 0, 32768, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)
I0316 15:41:26.536481 140556456256704 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 15:41:26.536509 139901308867776 shampoo_preconditioner_list.py:378] Rank 5: AdaGradPreconditionerList Bytes Breakdown: (0, 0, 0, 0, 131072, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)
I0316 15:41:26.536516 140476660065472 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 15.block_0 (torch.Size([0])) for Parameter 15 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0316 15:41:26.536518 140688500745408 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 9.block_0 (torch.Size([0])) for Parameter 9 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 15:41:26.536540 139901308867776 shampoo_preconditioner_list.py:381] Rank 5: AdaGradPreconditionerList Total Elements: 32768
I0316 15:41:26.536528 140556456256704 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 15:41:26.536562 140688500745408 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 10.block_0 (torch.Size([0])) for Parameter 10 (torch.Size([512, 1024])), Block block_0 (torch.Size([512, 1024])).
I0316 15:41:26.536571 139901308867776 shampoo_preconditioner_list.py:384] Rank 5: AdaGradPreconditionerList Total Bytes: 131072
I0316 15:41:26.536572 140476660065472 shampoo_preconditioner_list.py:375] Rank 1: AdaGradPreconditionerList Numel Breakdown: (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 524288, 0, 0, 0, 0, 0)
I0316 15:41:26.536586 140556456256704 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 4.block_0 (torch.Size([0])) for Parameter 4 (torch.Size([128, 256])), Block block_0 (torch.Size([128, 256])).
I0316 15:41:26.536603 140476660065472 shampoo_preconditioner_list.py:378] Rank 1: AdaGradPreconditionerList Bytes Breakdown: (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2097152, 0, 0, 0, 0, 0)
I0316 15:41:26.536612 140688500745408 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 11.block_0 (torch.Size([0])) for Parameter 11 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 15:41:26.536631 140476660065472 shampoo_preconditioner_list.py:381] Rank 1: AdaGradPreconditionerList Total Elements: 524288
I0316 15:41:26.536646 140556456256704 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 5.block_0 (torch.Size([0])) for Parameter 5 (torch.Size([128])), Block block_0 (torch.Size([128])).
I0316 15:41:26.536662 140476660065472 shampoo_preconditioner_list.py:384] Rank 1: AdaGradPreconditionerList Total Bytes: 2097152
I0316 15:41:26.536703 140556456256704 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 6.block_0 (torch.Size([0])) for Parameter 6 (torch.Size([1024, 506])), Block block_0 (torch.Size([1024, 506])).
I0316 15:41:26.536710 140575059936448 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 13.block_0 ([torch.Size([256, 256])]) for Parameter 13 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 15:41:26.536765 140556456256704 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 7.block_0 (torch.Size([0])) for Parameter 7 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 15:41:26.536823 140556456256704 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 8.block_0 (torch.Size([0])) for Parameter 8 (torch.Size([1024, 1024])), Block block_0 (torch.Size([1024, 1024])).
I0316 15:41:26.536877 140556456256704 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 9.block_0 (torch.Size([0])) for Parameter 9 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 15:41:26.536886 140575059936448 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 14.block_0 ([torch.Size([256, 256])]) for Parameter 14 (torch.Size([1, 256])), Block block_0 (torch.Size([256])).
I0316 15:41:26.536932 140556456256704 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 10.block_0 (torch.Size([0])) for Parameter 10 (torch.Size([512, 1024])), Block block_0 (torch.Size([512, 1024])).
I0316 15:41:26.536987 140556456256704 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 11.block_0 (torch.Size([0])) for Parameter 11 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 15:41:26.537029 140575059936448 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 15.block_0 ([torch.Size([1, 1])]) for Parameter 15 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0316 15:41:26.537041 140556456256704 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 12.block_0 (torch.Size([0])) for Parameter 12 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 15:41:26.537086 140556456256704 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 13.block_0 (torch.Size([0])) for Parameter 13 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 15:41:26.537112 140688500745408 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 12.block_0 (torch.Size([256, 512])) for Parameter 12 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 15:41:26.537139 140556456256704 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 14.block_0 (torch.Size([0])) for Parameter 14 (torch.Size([1, 256])), Block block_0 (torch.Size([256])).
I0316 15:41:26.537147 140575059936448 shampoo_preconditioner_list.py:612] Rank 7: ShampooPreconditionerList Numel Breakdown: (524626, 524288, 655360, 131072, 163840, 32768, 2609224, 2097152, 4194304, 2097152, 2621440, 524288, 655360, 131072, 131072, 2)
I0316 15:41:26.537186 140575059936448 shampoo_preconditioner_list.py:615] Rank 7: ShampooPreconditionerList Bytes Breakdown: (2098504, 2097152, 2621440, 524288, 655360, 131072, 10436896, 8388608, 16777216)
I0316 15:41:26.537184 140556456256704 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 15.block_0 (torch.Size([0])) for Parameter 15 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0316 15:41:26.537210 140688500745408 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 13.block_0 (torch.Size([0])) for Parameter 13 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 15:41:26.537224 140575059936448 shampoo_preconditioner_list.py:618] Rank 7: ShampooPreconditionerList Total Elements: 17093020
I0316 15:41:26.537243 140556456256704 shampoo_preconditioner_list.py:375] Rank 6: AdaGradPreconditionerList Numel Breakdown: (6656, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)
I0316 15:41:26.537252 140575059936448 shampoo_preconditioner_list.py:621] Rank 7: ShampooPreconditionerList Total Bytes: 43730536
I0316 15:41:26.537265 140688500745408 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 14.block_0 (torch.Size([0])) for Parameter 14 (torch.Size([1, 256])), Block block_0 (torch.Size([256])).
I0316 15:41:26.537283 140556456256704 shampoo_preconditioner_list.py:378] Rank 6: AdaGradPreconditionerList Bytes Breakdown: (26624, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)
I0316 15:41:26.537309 140688500745408 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 15.block_0 (torch.Size([0])) for Parameter 15 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0316 15:41:26.537318 140556456256704 shampoo_preconditioner_list.py:381] Rank 6: AdaGradPreconditionerList Total Elements: 6656
I0316 15:41:26.537346 140556456256704 shampoo_preconditioner_list.py:384] Rank 6: AdaGradPreconditionerList Total Bytes: 26624
I0316 15:41:26.537369 140688500745408 shampoo_preconditioner_list.py:375] Rank 4: AdaGradPreconditionerList Numel Breakdown: (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 131072, 0, 0, 0)
I0316 15:41:26.537372 140575059936448 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([512, 13])), Block block_0 (torch.Size([512, 13])).
I0316 15:41:26.537404 140688500745408 shampoo_preconditioner_list.py:378] Rank 4: AdaGradPreconditionerList Bytes Breakdown: (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 524288, 0, 0, 0)
I0316 15:41:26.537435 140688500745408 shampoo_preconditioner_list.py:381] Rank 4: AdaGradPreconditionerList Total Elements: 131072
I0316 15:41:26.537465 140688500745408 shampoo_preconditioner_list.py:384] Rank 4: AdaGradPreconditionerList Total Bytes: 524288
I0316 15:41:26.537463 140575059936448 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([512])) for Parameter 1 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 15:41:26.537522 140575059936448 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 15:41:26.537618 140575059936448 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([256])) for Parameter 3 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 15:41:26.537625 140125047432384 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([524288, 128])) for Parameter 0 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 15:41:26.537685 140575059936448 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 4.block_0 (torch.Size([0])) for Parameter 4 (torch.Size([128, 256])), Block block_0 (torch.Size([128, 256])).
I0316 15:41:26.537750 140125047432384 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_1 (torch.Size([0])) for Parameter 0 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 15:41:26.537771 140575059936448 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 5.block_0 (torch.Size([128])) for Parameter 5 (torch.Size([128])), Block block_0 (torch.Size([128])).
I0316 15:41:26.537822 140125047432384 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 15:41:26.537829 140575059936448 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 6.block_0 (torch.Size([0])) for Parameter 6 (torch.Size([1024, 506])), Block block_0 (torch.Size([1024, 506])).
I0316 15:41:26.537880 140125047432384 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_1 (torch.Size([0])) for Parameter 1 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 15:41:26.537902 140575059936448 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 7.block_0 (torch.Size([1024])) for Parameter 7 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 15:41:26.537934 140125047432384 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 15:41:26.537967 140575059936448 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 8.block_0 (torch.Size([0])) for Parameter 8 (torch.Size([1024, 1024])), Block block_0 (torch.Size([1024, 1024])).
I0316 15:41:26.537986 140125047432384 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_1 (torch.Size([0])) for Parameter 2 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 15:41:26.538038 140125047432384 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 15:41:26.538042 140575059936448 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 9.block_0 (torch.Size([1024])) for Parameter 9 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 15:41:26.538089 140125047432384 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_1 (torch.Size([0])) for Parameter 3 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 15:41:26.538098 140575059936448 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 10.block_0 (torch.Size([0])) for Parameter 10 (torch.Size([512, 1024])), Block block_0 (torch.Size([512, 1024])).
I0316 15:41:26.538137 140125047432384 shampoo_preconditioner_list.py:375] Rank 0: AdaGradPreconditionerList Numel Breakdown: (67108864, 0, 0, 0, 0, 0, 0, 0)
I0316 15:41:26.538162 140575059936448 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 11.block_0 (torch.Size([512])) for Parameter 11 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 15:41:26.538170 140125047432384 shampoo_preconditioner_list.py:378] Rank 0: AdaGradPreconditionerList Bytes Breakdown: (268435456, 0, 0, 0, 0, 0, 0, 0)
I0316 15:41:26.538197 140125047432384 shampoo_preconditioner_list.py:381] Rank 0: AdaGradPreconditionerList Total Elements: 67108864
I0316 15:41:26.538223 140575059936448 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 12.block_0 (torch.Size([0])) for Parameter 12 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 15:41:26.538226 140125047432384 shampoo_preconditioner_list.py:384] Rank 0: AdaGradPreconditionerList Total Bytes: 268435456
I0316 15:41:26.538295 140575059936448 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 13.block_0 (torch.Size([256])) for Parameter 13 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 15:41:26.538367 140575059936448 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 14.block_0 (torch.Size([256])) for Parameter 14 (torch.Size([1, 256])), Block block_0 (torch.Size([256])).
I0316 15:41:26.538455 140575059936448 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 15.block_0 (torch.Size([1])) for Parameter 15 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0316 15:41:26.538425 140305644491968 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 15:41:26.538499 140465193141440 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 15:41:26.538550 140305644491968 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_1 (torch.Size([0])) for Parameter 0 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 15:41:26.538557 140575059936448 shampoo_preconditioner_list.py:375] Rank 7: AdaGradPreconditionerList Numel Breakdown: (0, 512, 0, 256, 0, 128, 0, 1024, 0, 1024, 0, 512, 0, 256, 256, 1)
I0316 15:41:26.538613 140575059936448 shampoo_preconditioner_list.py:378] Rank 7: AdaGradPreconditionerList Bytes Breakdown: (0, 2048, 0, 1024, 0, 512, 0, 4096, 0, 4096, 0, 2048, 0, 1024, 1024, 4)
I0316 15:41:26.538650 140575059936448 shampoo_preconditioner_list.py:381] Rank 7: AdaGradPreconditionerList Total Elements: 3969
I0316 15:41:26.538641 140465193141440 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_1 (torch.Size([0])) for Parameter 0 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 15:41:26.538685 140575059936448 shampoo_preconditioner_list.py:384] Rank 7: AdaGradPreconditionerList Total Bytes: 15876
I0316 15:41:26.538713 140465193141440 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 15:41:26.539386 139901308867776 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 15:41:26.539499 139901308867776 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_1 (torch.Size([0])) for Parameter 0 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 15:41:26.539571 139901308867776 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 15:41:26.539633 139901308867776 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_1 (torch.Size([0])) for Parameter 1 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 15:41:26.539716 139901308867776 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 15:41:26.539912 140556456256704 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 15:41:26.540081 140556456256704 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_1 (torch.Size([0])) for Parameter 0 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 15:41:26.540154 140556456256704 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 15:41:26.540163 140305644491968 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([524288, 128])) for Parameter 1 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 15:41:26.540216 140556456256704 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_1 (torch.Size([0])) for Parameter 1 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 15:41:26.540275 140556456256704 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 15:41:26.540271 140305644491968 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_1 (torch.Size([0])) for Parameter 1 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 15:41:26.540332 140556456256704 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_1 (torch.Size([0])) for Parameter 2 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 15:41:26.540333 140305644491968 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 15:41:26.540341 140688500745408 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 15:41:26.540355 140465193141440 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_1 (torch.Size([524288, 128])) for Parameter 1 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 15:41:26.540388 140305644491968 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_1 (torch.Size([0])) for Parameter 2 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 15:41:26.540440 140305644491968 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 15:41:26.540452 140688500745408 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_1 (torch.Size([0])) for Parameter 0 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 15:41:26.540460 140465193141440 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 15:41:26.540444 140476660065472 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 15:41:26.540505 140305644491968 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_1 (torch.Size([0])) for Parameter 3 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 15:41:26.540516 140688500745408 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 15:41:26.540519 140465193141440 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_1 (torch.Size([0])) for Parameter 2 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 15:41:26.540561 140305644491968 shampoo_preconditioner_list.py:375] Rank 2: AdaGradPreconditionerList Numel Breakdown: (0, 0, 67108864, 0, 0, 0, 0, 0)
I0316 15:41:26.540572 140688500745408 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_1 (torch.Size([0])) for Parameter 1 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 15:41:26.540573 140465193141440 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 15:41:26.540601 140305644491968 shampoo_preconditioner_list.py:378] Rank 2: AdaGradPreconditionerList Bytes Breakdown: (0, 0, 268435456, 0, 0, 0, 0, 0)
I0316 15:41:26.540625 140465193141440 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_1 (torch.Size([0])) for Parameter 3 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 15:41:26.540637 140305644491968 shampoo_preconditioner_list.py:381] Rank 2: AdaGradPreconditionerList Total Elements: 67108864
I0316 15:41:26.540675 140305644491968 shampoo_preconditioner_list.py:384] Rank 2: AdaGradPreconditionerList Total Bytes: 268435456
I0316 15:41:26.540691 140465193141440 shampoo_preconditioner_list.py:375] Rank 3: AdaGradPreconditionerList Numel Breakdown: (0, 0, 0, 67108864, 0, 0, 0, 0)
I0316 15:41:26.540734 140465193141440 shampoo_preconditioner_list.py:378] Rank 3: AdaGradPreconditionerList Bytes Breakdown: (0, 0, 0, 268435456, 0, 0, 0, 0)
I0316 15:41:26.540769 140465193141440 shampoo_preconditioner_list.py:381] Rank 3: AdaGradPreconditionerList Total Elements: 67108864
I0316 15:41:26.540804 140465193141440 shampoo_preconditioner_list.py:384] Rank 3: AdaGradPreconditionerList Total Bytes: 268435456
I0316 15:41:26.540856 139901308867776 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_1 (torch.Size([524288, 128])) for Parameter 2 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 15:41:26.540971 139901308867776 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 15:41:26.541038 139901308867776 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_1 (torch.Size([0])) for Parameter 3 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 15:41:26.541095 139901308867776 shampoo_preconditioner_list.py:375] Rank 5: AdaGradPreconditionerList Numel Breakdown: (0, 0, 0, 0, 0, 67108864, 0, 0)
I0316 15:41:26.541135 139901308867776 shampoo_preconditioner_list.py:378] Rank 5: AdaGradPreconditionerList Bytes Breakdown: (0, 0, 0, 0, 0, 268435456, 0, 0)
I0316 15:41:26.541182 139901308867776 shampoo_preconditioner_list.py:381] Rank 5: AdaGradPreconditionerList Total Elements: 67108864
I0316 15:41:26.541226 139901308867776 shampoo_preconditioner_list.py:384] Rank 5: AdaGradPreconditionerList Total Bytes: 268435456
I0316 15:41:26.541448 140575059936448 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 15:41:26.541454 140125047432384 submission_runner.py:279] Initializing metrics bundle.
I0316 15:41:26.541547 140575059936448 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_1 (torch.Size([0])) for Parameter 0 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 15:41:26.541595 140125047432384 submission_runner.py:301] Initializing checkpoint and logger.
I0316 15:41:26.541627 140575059936448 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 15:41:26.541678 140575059936448 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_1 (torch.Size([0])) for Parameter 1 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 15:41:26.541659 140556456256704 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([524288, 128])) for Parameter 3 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 15:41:26.541739 140575059936448 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 15:41:26.541760 140556456256704 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_1 (torch.Size([0])) for Parameter 3 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 15:41:26.541790 140575059936448 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_1 (torch.Size([0])) for Parameter 2 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 15:41:26.541821 140556456256704 shampoo_preconditioner_list.py:375] Rank 6: AdaGradPreconditionerList Numel Breakdown: (0, 0, 0, 0, 0, 0, 67108864, 0)
I0316 15:41:26.541841 140575059936448 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 15:41:26.541860 140556456256704 shampoo_preconditioner_list.py:378] Rank 6: AdaGradPreconditionerList Bytes Breakdown: (0, 0, 0, 0, 0, 0, 268435456, 0)
I0316 15:41:26.541895 140556456256704 shampoo_preconditioner_list.py:381] Rank 6: AdaGradPreconditionerList Total Elements: 67108864
I0316 15:41:26.541883 140476660065472 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_1 (torch.Size([524288, 128])) for Parameter 0 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 15:41:26.541930 140556456256704 shampoo_preconditioner_list.py:384] Rank 6: AdaGradPreconditionerList Total Bytes: 268435456
I0316 15:41:26.541989 140476660065472 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 15:41:26.542060 140476660065472 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_1 (torch.Size([0])) for Parameter 1 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 15:41:26.542050 140125047432384 submission_runner.py:321] Saving meta data to /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_2/criteo1tb_pytorch/trial_4/meta_data_0.json.
I0316 15:41:26.542061 140688500745408 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([524288, 128])) for Parameter 2 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 15:41:26.542116 140476660065472 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 15:41:26.542168 140476660065472 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_1 (torch.Size([0])) for Parameter 2 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 15:41:26.542168 140688500745408 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_1 (torch.Size([0])) for Parameter 2 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 15:41:26.542200 140125047432384 logger_utils.py:262] Unable to record workload.train_mean information. Continuing without it.
I0316 15:41:26.542220 140476660065472 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 15:41:26.542221 140688500745408 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 15:41:26.542248 140125047432384 logger_utils.py:262] Unable to record workload.train_stddev information. Continuing without it.
I0316 15:41:26.542266 140476660065472 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_1 (torch.Size([0])) for Parameter 3 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 15:41:26.542267 140688500745408 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_1 (torch.Size([0])) for Parameter 3 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 15:41:26.542310 140688500745408 shampoo_preconditioner_list.py:375] Rank 4: AdaGradPreconditionerList Numel Breakdown: (0, 0, 0, 0, 67108864, 0, 0, 0)
I0316 15:41:26.542314 140476660065472 shampoo_preconditioner_list.py:375] Rank 1: AdaGradPreconditionerList Numel Breakdown: (0, 67108864, 0, 0, 0, 0, 0, 0)
I0316 15:41:26.542343 140476660065472 shampoo_preconditioner_list.py:378] Rank 1: AdaGradPreconditionerList Bytes Breakdown: (0, 268435456, 0, 0, 0, 0, 0, 0)
I0316 15:41:26.542346 140688500745408 shampoo_preconditioner_list.py:378] Rank 4: AdaGradPreconditionerList Bytes Breakdown: (0, 0, 0, 0, 268435456, 0, 0, 0)
I0316 15:41:26.542373 140688500745408 shampoo_preconditioner_list.py:381] Rank 4: AdaGradPreconditionerList Total Elements: 67108864
I0316 15:41:26.542374 140476660065472 shampoo_preconditioner_list.py:381] Rank 1: AdaGradPreconditionerList Total Elements: 67108864
I0316 15:41:26.542400 140476660065472 shampoo_preconditioner_list.py:384] Rank 1: AdaGradPreconditionerList Total Bytes: 268435456
I0316 15:41:26.542404 140688500745408 shampoo_preconditioner_list.py:384] Rank 4: AdaGradPreconditionerList Total Bytes: 268435456
I0316 15:41:26.542855 140575059936448 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_1 (torch.Size([524288, 128])) for Parameter 3 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 15:41:26.542949 140575059936448 shampoo_preconditioner_list.py:375] Rank 7: AdaGradPreconditionerList Numel Breakdown: (0, 0, 0, 0, 0, 0, 0, 67108864)
I0316 15:41:26.542985 140575059936448 shampoo_preconditioner_list.py:378] Rank 7: AdaGradPreconditionerList Bytes Breakdown: (0, 0, 0, 0, 0, 0, 0, 268435456)
I0316 15:41:26.543016 140575059936448 shampoo_preconditioner_list.py:381] Rank 7: AdaGradPreconditionerList Total Elements: 67108864
I0316 15:41:26.543047 140575059936448 shampoo_preconditioner_list.py:384] Rank 7: AdaGradPreconditionerList Total Bytes: 268435456
I0316 15:41:26.544290 140305644491968 logger_utils.py:262] Unable to record workload.train_mean information. Continuing without it.
I0316 15:41:26.544370 140305644491968 logger_utils.py:262] Unable to record workload.train_stddev information. Continuing without it.
I0316 15:41:26.544495 140465193141440 logger_utils.py:262] Unable to record workload.train_mean information. Continuing without it.
I0316 15:41:26.544570 140465193141440 logger_utils.py:262] Unable to record workload.train_stddev information. Continuing without it.
I0316 15:41:26.544821 139901308867776 logger_utils.py:262] Unable to record workload.train_mean information. Continuing without it.
I0316 15:41:26.544894 139901308867776 logger_utils.py:262] Unable to record workload.train_stddev information. Continuing without it.
I0316 15:41:26.545381 140556456256704 logger_utils.py:262] Unable to record workload.train_mean information. Continuing without it.
I0316 15:41:26.545457 140556456256704 logger_utils.py:262] Unable to record workload.train_stddev information. Continuing without it.
I0316 15:41:26.545652 140476660065472 logger_utils.py:262] Unable to record workload.train_mean information. Continuing without it.
I0316 15:41:26.545696 140688500745408 logger_utils.py:262] Unable to record workload.train_mean information. Continuing without it.
I0316 15:41:26.545735 140476660065472 logger_utils.py:262] Unable to record workload.train_stddev information. Continuing without it.
I0316 15:41:26.545776 140688500745408 logger_utils.py:262] Unable to record workload.train_stddev information. Continuing without it.
I0316 15:41:26.545756 140575059936448 logger_utils.py:262] Unable to record workload.train_mean information. Continuing without it.
I0316 15:41:26.545817 140575059936448 logger_utils.py:262] Unable to record workload.train_stddev information. Continuing without it.
I0316 15:41:26.789609 140125047432384 submission_runner.py:325] Saving flags to /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_2/criteo1tb_pytorch/trial_4/flags_0.json.
I0316 15:41:26.870821 140125047432384 submission_runner.py:337] Starting training loop.
I0316 15:41:31.820744 140093209687808 logging_writer.py:48] [0] global_step=0, grad_norm=7.64736, loss=1.20848
I0316 15:41:31.841364 140125047432384 submission.py:265] 0) loss = 1.208, grad_norm = 7.647
I0316 15:41:32.211596 140125047432384 spec.py:321] Evaluating on the training split.
I0316 15:47:00.508201 140125047432384 spec.py:333] Evaluating on the validation split.
I0316 15:52:08.054567 140125047432384 spec.py:349] Evaluating on the test split.
I0316 15:58:14.908908 140125047432384 submission_runner.py:469] Time since start: 1008.04s, 	Step: 1, 	{'train/loss': 1.2065096837157883, 'validation/loss': 1.2110276654353231, 'validation/num_examples': 83274637, 'test/loss': 1.2072974342477898, 'test/num_examples': 95000000, 'score': 4.9714789390563965, 'total_duration': 1008.0382068157196, 'accumulated_submission_time': 4.9714789390563965, 'accumulated_eval_time': 1002.6973347663879, 'accumulated_logging_time': 0}
I0316 15:58:14.917076 140082530985728 logging_writer.py:48] [1] accumulated_eval_time=1002.7, accumulated_logging_time=0, accumulated_submission_time=4.97148, global_step=1, preemption_count=0, score=4.97148, test/loss=1.2073, test/num_examples=95000000, total_duration=1008.04, train/loss=1.20651, validation/loss=1.21103, validation/num_examples=83274637
I0316 15:58:15.573908 140082522593024 logging_writer.py:48] [1] global_step=1, grad_norm=7.6357, loss=1.20928
I0316 15:58:15.576961 140125047432384 submission.py:265] 1) loss = 1.209, grad_norm = 7.636
I0316 15:58:15.772765 140082530985728 logging_writer.py:48] [2] global_step=2, grad_norm=7.59589, loss=1.186
I0316 15:58:15.776098 140125047432384 submission.py:265] 2) loss = 1.186, grad_norm = 7.596
I0316 15:58:15.971376 140082522593024 logging_writer.py:48] [3] global_step=3, grad_norm=7.47644, loss=1.14326
I0316 15:58:15.974619 140125047432384 submission.py:265] 3) loss = 1.143, grad_norm = 7.476
I0316 15:58:16.170736 140082530985728 logging_writer.py:48] [4] global_step=4, grad_norm=7.3442, loss=1.08373
I0316 15:58:16.173902 140125047432384 submission.py:265] 4) loss = 1.084, grad_norm = 7.344
I0316 15:58:16.370357 140082522593024 logging_writer.py:48] [5] global_step=5, grad_norm=7.10207, loss=1.00768
I0316 15:58:16.374248 140125047432384 submission.py:265] 5) loss = 1.008, grad_norm = 7.102
I0316 15:58:16.568892 140082530985728 logging_writer.py:48] [6] global_step=6, grad_norm=6.81858, loss=0.921983
I0316 15:58:16.572767 140125047432384 submission.py:265] 6) loss = 0.922, grad_norm = 6.819
I0316 15:58:16.767769 140082522593024 logging_writer.py:48] [7] global_step=7, grad_norm=6.64301, loss=0.827357
I0316 15:58:16.770897 140125047432384 submission.py:265] 7) loss = 0.827, grad_norm = 6.643
I0316 15:58:16.964290 140082530985728 logging_writer.py:48] [8] global_step=8, grad_norm=6.35876, loss=0.723752
I0316 15:58:16.967098 140125047432384 submission.py:265] 8) loss = 0.724, grad_norm = 6.359
I0316 15:58:17.161491 140082522593024 logging_writer.py:48] [9] global_step=9, grad_norm=5.87088, loss=0.615651
I0316 15:58:17.164378 140125047432384 submission.py:265] 9) loss = 0.616, grad_norm = 5.871
I0316 15:58:17.358692 140082530985728 logging_writer.py:48] [10] global_step=10, grad_norm=5.19562, loss=0.509918
I0316 15:58:17.361493 140125047432384 submission.py:265] 10) loss = 0.510, grad_norm = 5.196
I0316 15:58:17.576528 140082522593024 logging_writer.py:48] [11] global_step=11, grad_norm=4.34379, loss=0.413873
I0316 15:58:17.580316 140125047432384 submission.py:265] 11) loss = 0.414, grad_norm = 4.344
I0316 15:58:17.776409 140082530985728 logging_writer.py:48] [12] global_step=12, grad_norm=3.37855, loss=0.330605
I0316 15:58:17.780283 140125047432384 submission.py:265] 12) loss = 0.331, grad_norm = 3.379
I0316 15:58:17.976921 140082522593024 logging_writer.py:48] [13] global_step=13, grad_norm=2.38028, loss=0.266643
I0316 15:58:17.980452 140125047432384 submission.py:265] 13) loss = 0.267, grad_norm = 2.380
I0316 15:58:18.175329 140082530985728 logging_writer.py:48] [14] global_step=14, grad_norm=1.45976, loss=0.222778
I0316 15:58:18.178751 140125047432384 submission.py:265] 14) loss = 0.223, grad_norm = 1.460
I0316 15:58:18.378548 140082522593024 logging_writer.py:48] [15] global_step=15, grad_norm=0.698288, loss=0.195878
I0316 15:58:18.381826 140125047432384 submission.py:265] 15) loss = 0.196, grad_norm = 0.698
I0316 15:58:18.578274 140082530985728 logging_writer.py:48] [16] global_step=16, grad_norm=0.237261, loss=0.184144
I0316 15:58:18.581790 140125047432384 submission.py:265] 16) loss = 0.184, grad_norm = 0.237
I0316 15:58:18.778442 140082522593024 logging_writer.py:48] [17] global_step=17, grad_norm=0.554498, loss=0.190324
I0316 15:58:18.781831 140125047432384 submission.py:265] 17) loss = 0.190, grad_norm = 0.554
I0316 15:58:18.977786 140082530985728 logging_writer.py:48] [18] global_step=18, grad_norm=0.943169, loss=0.205031
I0316 15:58:18.981225 140125047432384 submission.py:265] 18) loss = 0.205, grad_norm = 0.943
I0316 15:58:19.179469 140082522593024 logging_writer.py:48] [19] global_step=19, grad_norm=1.16739, loss=0.217263
I0316 15:58:19.182744 140125047432384 submission.py:265] 19) loss = 0.217, grad_norm = 1.167
I0316 15:58:19.380551 140082530985728 logging_writer.py:48] [20] global_step=20, grad_norm=1.44457, loss=0.242007
I0316 15:58:19.383743 140125047432384 submission.py:265] 20) loss = 0.242, grad_norm = 1.445
I0316 15:58:19.595673 140082522593024 logging_writer.py:48] [21] global_step=21, grad_norm=1.72898, loss=0.275945
I0316 15:58:19.701812 140125047432384 submission.py:265] 21) loss = 0.276, grad_norm = 1.729
I0316 15:58:19.898494 140082530985728 logging_writer.py:48] [22] global_step=22, grad_norm=1.91117, loss=0.30002
I0316 15:58:19.902034 140125047432384 submission.py:265] 22) loss = 0.300, grad_norm = 1.911
I0316 15:58:20.098569 140082522593024 logging_writer.py:48] [23] global_step=23, grad_norm=2.12927, loss=0.33218
I0316 15:58:20.101767 140125047432384 submission.py:265] 23) loss = 0.332, grad_norm = 2.129
I0316 15:58:20.299420 140082530985728 logging_writer.py:48] [24] global_step=24, grad_norm=2.3367, loss=0.366277
I0316 15:58:20.302603 140125047432384 submission.py:265] 24) loss = 0.366, grad_norm = 2.337
I0316 15:58:20.504307 140082522593024 logging_writer.py:48] [25] global_step=25, grad_norm=2.51522, loss=0.396977
I0316 15:58:20.507529 140125047432384 submission.py:265] 25) loss = 0.397, grad_norm = 2.515
I0316 15:58:20.703049 140082530985728 logging_writer.py:48] [26] global_step=26, grad_norm=2.62406, loss=0.416956
I0316 15:58:20.706183 140125047432384 submission.py:265] 26) loss = 0.417, grad_norm = 2.624
I0316 15:58:20.902598 140082522593024 logging_writer.py:48] [27] global_step=27, grad_norm=2.63676, loss=0.42223
I0316 15:58:20.905975 140125047432384 submission.py:265] 27) loss = 0.422, grad_norm = 2.637
I0316 15:58:21.101129 140082530985728 logging_writer.py:48] [28] global_step=28, grad_norm=2.77652, loss=0.445853
I0316 15:58:21.104816 140125047432384 submission.py:265] 28) loss = 0.446, grad_norm = 2.777
I0316 15:58:21.300033 140082522593024 logging_writer.py:48] [29] global_step=29, grad_norm=2.80115, loss=0.451889
I0316 15:58:21.303489 140125047432384 submission.py:265] 29) loss = 0.452, grad_norm = 2.801
I0316 15:58:21.498586 140082530985728 logging_writer.py:48] [30] global_step=30, grad_norm=2.82998, loss=0.456194
I0316 15:58:21.502233 140125047432384 submission.py:265] 30) loss = 0.456, grad_norm = 2.830
I0316 15:58:22.641193 140082522593024 logging_writer.py:48] [31] global_step=31, grad_norm=2.80821, loss=0.453683
I0316 15:58:22.644658 140125047432384 submission.py:265] 31) loss = 0.454, grad_norm = 2.808
I0316 15:58:23.198016 140082530985728 logging_writer.py:48] [32] global_step=32, grad_norm=2.78214, loss=0.448577
I0316 15:58:23.201423 140125047432384 submission.py:265] 32) loss = 0.449, grad_norm = 2.782
I0316 15:58:25.118549 140082522593024 logging_writer.py:48] [33] global_step=33, grad_norm=2.68714, loss=0.432789
I0316 15:58:25.122078 140125047432384 submission.py:265] 33) loss = 0.433, grad_norm = 2.687
I0316 15:58:26.128416 140082530985728 logging_writer.py:48] [34] global_step=34, grad_norm=2.63028, loss=0.420007
I0316 15:58:26.131951 140125047432384 submission.py:265] 34) loss = 0.420, grad_norm = 2.630
I0316 15:58:27.588951 140082522593024 logging_writer.py:48] [35] global_step=35, grad_norm=2.47153, loss=0.392941
I0316 15:58:27.592450 140125047432384 submission.py:265] 35) loss = 0.393, grad_norm = 2.472
I0316 15:58:28.468964 140082530985728 logging_writer.py:48] [36] global_step=36, grad_norm=2.4507, loss=0.385861
I0316 15:58:28.472471 140125047432384 submission.py:265] 36) loss = 0.386, grad_norm = 2.451
I0316 15:58:29.737237 140082522593024 logging_writer.py:48] [37] global_step=37, grad_norm=2.30468, loss=0.360405
I0316 15:58:29.740840 140125047432384 submission.py:265] 37) loss = 0.360, grad_norm = 2.305
I0316 15:58:30.749885 140082530985728 logging_writer.py:48] [38] global_step=38, grad_norm=2.50431, loss=0.3857
I0316 15:58:30.753415 140125047432384 submission.py:265] 38) loss = 0.386, grad_norm = 2.504
I0316 15:58:32.163861 140082522593024 logging_writer.py:48] [39] global_step=39, grad_norm=2.45834, loss=0.374101
I0316 15:58:32.167442 140125047432384 submission.py:265] 39) loss = 0.374, grad_norm = 2.458
I0316 15:58:33.317357 140082530985728 logging_writer.py:48] [40] global_step=40, grad_norm=2.25107, loss=0.339516
I0316 15:58:33.321079 140125047432384 submission.py:265] 40) loss = 0.340, grad_norm = 2.251
I0316 15:58:34.218923 140082522593024 logging_writer.py:48] [41] global_step=41, grad_norm=2.04403, loss=0.307825
I0316 15:58:34.222381 140125047432384 submission.py:265] 41) loss = 0.308, grad_norm = 2.044
I0316 15:58:35.571070 140082530985728 logging_writer.py:48] [42] global_step=42, grad_norm=1.84054, loss=0.278749
I0316 15:58:35.574771 140125047432384 submission.py:265] 42) loss = 0.279, grad_norm = 1.841
I0316 15:58:36.696049 140082522593024 logging_writer.py:48] [43] global_step=43, grad_norm=1.61483, loss=0.250397
I0316 15:58:36.699621 140125047432384 submission.py:265] 43) loss = 0.250, grad_norm = 1.615
I0316 15:58:37.999764 140082530985728 logging_writer.py:48] [44] global_step=44, grad_norm=1.34088, loss=0.223028
I0316 15:58:38.003358 140125047432384 submission.py:265] 44) loss = 0.223, grad_norm = 1.341
I0316 15:58:39.164306 140082522593024 logging_writer.py:48] [45] global_step=45, grad_norm=1.05013, loss=0.204676
I0316 15:58:39.167707 140125047432384 submission.py:265] 45) loss = 0.205, grad_norm = 1.050
I0316 15:58:40.505446 140082530985728 logging_writer.py:48] [46] global_step=46, grad_norm=0.615179, loss=0.183602
I0316 15:58:40.508855 140125047432384 submission.py:265] 46) loss = 0.184, grad_norm = 0.615
I0316 15:58:41.818354 140082522593024 logging_writer.py:48] [47] global_step=47, grad_norm=0.219838, loss=0.178198
I0316 15:58:41.822000 140125047432384 submission.py:265] 47) loss = 0.178, grad_norm = 0.220
I0316 15:58:42.850771 140082530985728 logging_writer.py:48] [48] global_step=48, grad_norm=0.561444, loss=0.179798
I0316 15:58:42.854189 140125047432384 submission.py:265] 48) loss = 0.180, grad_norm = 0.561
I0316 15:58:44.241677 140082522593024 logging_writer.py:48] [49] global_step=49, grad_norm=1.26664, loss=0.192798
I0316 15:58:44.245093 140125047432384 submission.py:265] 49) loss = 0.193, grad_norm = 1.267
I0316 15:58:45.444224 140082530985728 logging_writer.py:48] [50] global_step=50, grad_norm=1.93505, loss=0.212266
I0316 15:58:45.447618 140125047432384 submission.py:265] 50) loss = 0.212, grad_norm = 1.935
I0316 15:58:46.776801 140082522593024 logging_writer.py:48] [51] global_step=51, grad_norm=2.44877, loss=0.227011
I0316 15:58:46.780424 140125047432384 submission.py:265] 51) loss = 0.227, grad_norm = 2.449
I0316 15:58:47.842298 140082530985728 logging_writer.py:48] [52] global_step=52, grad_norm=2.6263, loss=0.23731
I0316 15:58:47.845753 140125047432384 submission.py:265] 52) loss = 0.237, grad_norm = 2.626
I0316 15:58:49.420955 140082522593024 logging_writer.py:48] [53] global_step=53, grad_norm=2.58203, loss=0.233506
I0316 15:58:49.424420 140125047432384 submission.py:265] 53) loss = 0.234, grad_norm = 2.582
I0316 15:58:50.655862 140082530985728 logging_writer.py:48] [54] global_step=54, grad_norm=2.30495, loss=0.222177
I0316 15:58:50.659346 140125047432384 submission.py:265] 54) loss = 0.222, grad_norm = 2.305
I0316 15:58:51.789062 140082522593024 logging_writer.py:48] [55] global_step=55, grad_norm=1.88334, loss=0.203139
I0316 15:58:51.792630 140125047432384 submission.py:265] 55) loss = 0.203, grad_norm = 1.883
I0316 15:58:53.423179 140082530985728 logging_writer.py:48] [56] global_step=56, grad_norm=1.39407, loss=0.183544
I0316 15:58:53.426638 140125047432384 submission.py:265] 56) loss = 0.184, grad_norm = 1.394
I0316 15:58:54.469114 140082522593024 logging_writer.py:48] [57] global_step=57, grad_norm=0.962909, loss=0.164285
I0316 15:58:54.472833 140125047432384 submission.py:265] 57) loss = 0.164, grad_norm = 0.963
I0316 15:58:56.084160 140082530985728 logging_writer.py:48] [58] global_step=58, grad_norm=0.537609, loss=0.148359
I0316 15:58:56.087677 140125047432384 submission.py:265] 58) loss = 0.148, grad_norm = 0.538
I0316 15:58:56.691442 140082522593024 logging_writer.py:48] [59] global_step=59, grad_norm=0.133104, loss=0.146006
I0316 15:58:56.694987 140125047432384 submission.py:265] 59) loss = 0.146, grad_norm = 0.133
I0316 15:58:58.362608 140082530985728 logging_writer.py:48] [60] global_step=60, grad_norm=0.205231, loss=0.142444
I0316 15:58:58.365869 140125047432384 submission.py:265] 60) loss = 0.142, grad_norm = 0.205
I0316 15:58:59.549757 140082522593024 logging_writer.py:48] [61] global_step=61, grad_norm=0.431636, loss=0.143469
I0316 15:58:59.553066 140125047432384 submission.py:265] 61) loss = 0.143, grad_norm = 0.432
I0316 15:59:01.309307 140082530985728 logging_writer.py:48] [62] global_step=62, grad_norm=0.601857, loss=0.149226
I0316 15:59:01.312694 140125047432384 submission.py:265] 62) loss = 0.149, grad_norm = 0.602
I0316 15:59:02.028019 140082522593024 logging_writer.py:48] [63] global_step=63, grad_norm=0.756988, loss=0.158627
I0316 15:59:02.031143 140125047432384 submission.py:265] 63) loss = 0.159, grad_norm = 0.757
I0316 15:59:04.044738 140082530985728 logging_writer.py:48] [64] global_step=64, grad_norm=0.84539, loss=0.164465
I0316 15:59:04.047819 140125047432384 submission.py:265] 64) loss = 0.164, grad_norm = 0.845
I0316 15:59:05.030230 140082522593024 logging_writer.py:48] [65] global_step=65, grad_norm=0.879545, loss=0.167224
I0316 15:59:05.033435 140125047432384 submission.py:265] 65) loss = 0.167, grad_norm = 0.880
I0316 15:59:06.566089 140082530985728 logging_writer.py:48] [66] global_step=66, grad_norm=0.910444, loss=0.170171
I0316 15:59:06.569180 140125047432384 submission.py:265] 66) loss = 0.170, grad_norm = 0.910
I0316 15:59:07.575446 140082522593024 logging_writer.py:48] [67] global_step=67, grad_norm=0.939022, loss=0.174498
I0316 15:59:07.578543 140125047432384 submission.py:265] 67) loss = 0.174, grad_norm = 0.939
I0316 15:59:09.122971 140082530985728 logging_writer.py:48] [68] global_step=68, grad_norm=0.908357, loss=0.171607
I0316 15:59:09.126003 140125047432384 submission.py:265] 68) loss = 0.172, grad_norm = 0.908
I0316 15:59:10.191089 140082522593024 logging_writer.py:48] [69] global_step=69, grad_norm=0.870237, loss=0.168096
I0316 15:59:10.194347 140125047432384 submission.py:265] 69) loss = 0.168, grad_norm = 0.870
I0316 15:59:11.611468 140082530985728 logging_writer.py:48] [70] global_step=70, grad_norm=0.82009, loss=0.164756
I0316 15:59:11.614672 140125047432384 submission.py:265] 70) loss = 0.165, grad_norm = 0.820
I0316 15:59:12.586135 140082522593024 logging_writer.py:48] [71] global_step=71, grad_norm=0.74601, loss=0.159118
I0316 15:59:12.589106 140125047432384 submission.py:265] 71) loss = 0.159, grad_norm = 0.746
I0316 15:59:13.820437 140082530985728 logging_writer.py:48] [72] global_step=72, grad_norm=0.667533, loss=0.153478
I0316 15:59:13.823674 140125047432384 submission.py:265] 72) loss = 0.153, grad_norm = 0.668
I0316 15:59:14.912057 140082522593024 logging_writer.py:48] [73] global_step=73, grad_norm=0.573277, loss=0.147462
I0316 15:59:14.915289 140125047432384 submission.py:265] 73) loss = 0.147, grad_norm = 0.573
I0316 15:59:15.956351 140082530985728 logging_writer.py:48] [74] global_step=74, grad_norm=0.475691, loss=0.143311
I0316 15:59:15.959309 140125047432384 submission.py:265] 74) loss = 0.143, grad_norm = 0.476
I0316 15:59:17.383386 140082522593024 logging_writer.py:48] [75] global_step=75, grad_norm=0.351122, loss=0.138159
I0316 15:59:17.386465 140125047432384 submission.py:265] 75) loss = 0.138, grad_norm = 0.351
I0316 15:59:18.474279 140082530985728 logging_writer.py:48] [76] global_step=76, grad_norm=0.272652, loss=0.142012
I0316 15:59:18.477256 140125047432384 submission.py:265] 76) loss = 0.142, grad_norm = 0.273
I0316 15:59:19.912979 140082522593024 logging_writer.py:48] [77] global_step=77, grad_norm=0.115347, loss=0.136658
I0316 15:59:19.916002 140125047432384 submission.py:265] 77) loss = 0.137, grad_norm = 0.115
I0316 15:59:21.399827 140082530985728 logging_writer.py:48] [78] global_step=78, grad_norm=0.0654264, loss=0.134954
I0316 15:59:21.402767 140125047432384 submission.py:265] 78) loss = 0.135, grad_norm = 0.065
I0316 15:59:22.251858 140082522593024 logging_writer.py:48] [79] global_step=79, grad_norm=0.18927, loss=0.137273
I0316 15:59:22.255249 140125047432384 submission.py:265] 79) loss = 0.137, grad_norm = 0.189
I0316 15:59:23.884771 140082530985728 logging_writer.py:48] [80] global_step=80, grad_norm=0.312699, loss=0.139875
I0316 15:59:23.887863 140125047432384 submission.py:265] 80) loss = 0.140, grad_norm = 0.313
I0316 15:59:25.082446 140082522593024 logging_writer.py:48] [81] global_step=81, grad_norm=0.419268, loss=0.142685
I0316 15:59:25.085673 140125047432384 submission.py:265] 81) loss = 0.143, grad_norm = 0.419
I0316 15:59:26.283338 140082530985728 logging_writer.py:48] [82] global_step=82, grad_norm=0.52874, loss=0.142455
I0316 15:59:26.286421 140125047432384 submission.py:265] 82) loss = 0.142, grad_norm = 0.529
I0316 15:59:27.759295 140082522593024 logging_writer.py:48] [83] global_step=83, grad_norm=0.566932, loss=0.146473
I0316 15:59:27.762442 140125047432384 submission.py:265] 83) loss = 0.146, grad_norm = 0.567
I0316 15:59:28.760433 140082530985728 logging_writer.py:48] [84] global_step=84, grad_norm=0.58922, loss=0.14765
I0316 15:59:28.764015 140125047432384 submission.py:265] 84) loss = 0.148, grad_norm = 0.589
I0316 15:59:30.419231 140082522593024 logging_writer.py:48] [85] global_step=85, grad_norm=0.593714, loss=0.147282
I0316 15:59:30.422726 140125047432384 submission.py:265] 85) loss = 0.147, grad_norm = 0.594
I0316 15:59:31.538754 140082530985728 logging_writer.py:48] [86] global_step=86, grad_norm=0.564624, loss=0.145949
I0316 15:59:31.542191 140125047432384 submission.py:265] 86) loss = 0.146, grad_norm = 0.565
I0316 15:59:33.432506 140082522593024 logging_writer.py:48] [87] global_step=87, grad_norm=0.518992, loss=0.143497
I0316 15:59:33.435759 140125047432384 submission.py:265] 87) loss = 0.143, grad_norm = 0.519
I0316 15:59:36.059922 140082530985728 logging_writer.py:48] [88] global_step=88, grad_norm=0.442227, loss=0.142832
I0316 15:59:36.062969 140125047432384 submission.py:265] 88) loss = 0.143, grad_norm = 0.442
I0316 15:59:36.863157 140082522593024 logging_writer.py:48] [89] global_step=89, grad_norm=0.364753, loss=0.139495
I0316 15:59:36.866515 140125047432384 submission.py:265] 89) loss = 0.139, grad_norm = 0.365
I0316 15:59:38.476507 140082530985728 logging_writer.py:48] [90] global_step=90, grad_norm=0.27784, loss=0.136056
I0316 15:59:38.479876 140125047432384 submission.py:265] 90) loss = 0.136, grad_norm = 0.278
I0316 15:59:39.351836 140082522593024 logging_writer.py:48] [91] global_step=91, grad_norm=0.172622, loss=0.13582
I0316 15:59:39.355031 140125047432384 submission.py:265] 91) loss = 0.136, grad_norm = 0.173
I0316 15:59:41.482153 140082530985728 logging_writer.py:48] [92] global_step=92, grad_norm=0.0801907, loss=0.133722
I0316 15:59:41.485627 140125047432384 submission.py:265] 92) loss = 0.134, grad_norm = 0.080
I0316 15:59:42.129413 140082522593024 logging_writer.py:48] [93] global_step=93, grad_norm=0.0339828, loss=0.133328
I0316 15:59:42.132774 140125047432384 submission.py:265] 93) loss = 0.133, grad_norm = 0.034
I0316 15:59:43.843269 140082530985728 logging_writer.py:48] [94] global_step=94, grad_norm=0.111971, loss=0.134222
I0316 15:59:43.846460 140125047432384 submission.py:265] 94) loss = 0.134, grad_norm = 0.112
I0316 15:59:44.933774 140082522593024 logging_writer.py:48] [95] global_step=95, grad_norm=0.205664, loss=0.138644
I0316 15:59:44.937026 140125047432384 submission.py:265] 95) loss = 0.139, grad_norm = 0.206
I0316 15:59:46.066539 140082530985728 logging_writer.py:48] [96] global_step=96, grad_norm=0.290064, loss=0.144697
I0316 15:59:46.069569 140125047432384 submission.py:265] 96) loss = 0.145, grad_norm = 0.290
I0316 15:59:47.331916 140082522593024 logging_writer.py:48] [97] global_step=97, grad_norm=0.333768, loss=0.146355
I0316 15:59:47.335183 140125047432384 submission.py:265] 97) loss = 0.146, grad_norm = 0.334
I0316 15:59:48.679451 140082530985728 logging_writer.py:48] [98] global_step=98, grad_norm=0.361321, loss=0.14728
I0316 15:59:48.682433 140125047432384 submission.py:265] 98) loss = 0.147, grad_norm = 0.361
I0316 15:59:50.203714 140082522593024 logging_writer.py:48] [99] global_step=99, grad_norm=0.366742, loss=0.14636
I0316 15:59:50.207097 140125047432384 submission.py:265] 99) loss = 0.146, grad_norm = 0.367
I0316 15:59:51.169359 140082530985728 logging_writer.py:48] [100] global_step=100, grad_norm=0.388872, loss=0.150311
I0316 15:59:51.172307 140125047432384 submission.py:265] 100) loss = 0.150, grad_norm = 0.389
I0316 16:00:16.004841 140125047432384 spec.py:321] Evaluating on the training split.
I0316 16:05:58.059621 140125047432384 spec.py:333] Evaluating on the validation split.
I0316 16:10:30.791713 140125047432384 spec.py:349] Evaluating on the test split.
I0316 16:16:14.382205 140125047432384 submission_runner.py:469] Time since start: 2087.51s, 	Step: 121, 	{'train/loss': 0.1398335201562238, 'validation/loss': 0.13855227911752868, 'validation/num_examples': 83274637, 'test/loss': 0.14231451538041767, 'test/num_examples': 95000000, 'score': 125.14563584327698, 'total_duration': 2087.511488199234, 'accumulated_submission_time': 125.14563584327698, 'accumulated_eval_time': 1961.0748245716095, 'accumulated_logging_time': 0.015397787094116211}
I0316 16:16:14.392338 140082522593024 logging_writer.py:48] [121] accumulated_eval_time=1961.07, accumulated_logging_time=0.0153978, accumulated_submission_time=125.146, global_step=121, preemption_count=0, score=125.146, test/loss=0.142315, test/num_examples=95000000, total_duration=2087.51, train/loss=0.139834, validation/loss=0.138552, validation/num_examples=83274637
/usr/local/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
I0316 16:18:15.532632 140125047432384 spec.py:321] Evaluating on the training split.
I0316 16:23:30.203638 140125047432384 spec.py:333] Evaluating on the validation split.
I0316 16:27:59.635703 140125047432384 spec.py:349] Evaluating on the test split.
I0316 16:33:22.366057 140125047432384 submission_runner.py:469] Time since start: 3115.50s, 	Step: 240, 	{'train/loss': 0.12788620280307472, 'validation/loss': 0.12861760013689474, 'validation/num_examples': 83274637, 'test/loss': 0.13154781391457004, 'test/num_examples': 95000000, 'score': 245.38731360435486, 'total_duration': 3115.4954023361206, 'accumulated_submission_time': 245.38731360435486, 'accumulated_eval_time': 2867.90834069252, 'accumulated_logging_time': 0.03335976600646973}
I0316 16:33:22.375933 140082530985728 logging_writer.py:48] [240] accumulated_eval_time=2867.91, accumulated_logging_time=0.0333598, accumulated_submission_time=245.387, global_step=240, preemption_count=0, score=245.387, test/loss=0.131548, test/num_examples=95000000, total_duration=3115.5, train/loss=0.127886, validation/loss=0.128618, validation/num_examples=83274637
I0316 16:35:23.664765 140125047432384 spec.py:321] Evaluating on the training split.
I0316 16:40:25.904976 140125047432384 spec.py:333] Evaluating on the validation split.
I0316 16:44:53.387151 140125047432384 spec.py:349] Evaluating on the test split.
I0316 16:50:16.739584 140125047432384 submission_runner.py:469] Time since start: 4129.87s, 	Step: 359, 	{'train/loss': 0.12783539908361508, 'validation/loss': 0.12767009365256995, 'validation/num_examples': 83274637, 'test/loss': 0.130446985331445, 'test/num_examples': 95000000, 'score': 365.8275294303894, 'total_duration': 4129.868925333023, 'accumulated_submission_time': 365.8275294303894, 'accumulated_eval_time': 3760.9832651615143, 'accumulated_logging_time': 0.05065274238586426}
I0316 16:50:16.749405 140082522593024 logging_writer.py:48] [359] accumulated_eval_time=3760.98, accumulated_logging_time=0.0506527, accumulated_submission_time=365.828, global_step=359, preemption_count=0, score=365.828, test/loss=0.130447, test/num_examples=95000000, total_duration=4129.87, train/loss=0.127835, validation/loss=0.12767, validation/num_examples=83274637
I0316 16:52:17.778444 140125047432384 spec.py:321] Evaluating on the training split.
I0316 16:57:18.382457 140125047432384 spec.py:333] Evaluating on the validation split.
I0316 17:01:35.709353 140125047432384 spec.py:349] Evaluating on the test split.
I0316 17:06:35.724277 140125047432384 submission_runner.py:469] Time since start: 5108.85s, 	Step: 478, 	{'train/loss': 0.1265810259673565, 'validation/loss': 0.12738667729795292, 'validation/num_examples': 83274637, 'test/loss': 0.13017763444390548, 'test/num_examples': 95000000, 'score': 485.9496235847473, 'total_duration': 5108.853622198105, 'accumulated_submission_time': 485.9496235847473, 'accumulated_eval_time': 4618.929227590561, 'accumulated_logging_time': 0.06751251220703125}
I0316 17:06:35.733984 140082530985728 logging_writer.py:48] [478] accumulated_eval_time=4618.93, accumulated_logging_time=0.0675125, accumulated_submission_time=485.95, global_step=478, preemption_count=0, score=485.95, test/loss=0.130178, test/num_examples=95000000, total_duration=5108.85, train/loss=0.126581, validation/loss=0.127387, validation/num_examples=83274637
I0316 17:06:40.721205 140082522593024 logging_writer.py:48] [500] global_step=500, grad_norm=0.0185029, loss=0.121783
I0316 17:06:40.724662 140125047432384 submission.py:265] 500) loss = 0.122, grad_norm = 0.019
I0316 17:08:36.481073 140125047432384 spec.py:321] Evaluating on the training split.
I0316 17:13:37.495843 140125047432384 spec.py:333] Evaluating on the validation split.
I0316 17:17:53.623830 140125047432384 spec.py:349] Evaluating on the test split.
I0316 17:23:00.475043 140125047432384 submission_runner.py:469] Time since start: 6093.60s, 	Step: 593, 	{'train/loss': 0.1262374924362613, 'validation/loss': 0.12679798651319518, 'validation/num_examples': 83274637, 'test/loss': 0.12979993873700593, 'test/num_examples': 95000000, 'score': 605.7171161174774, 'total_duration': 6093.6043701171875, 'accumulated_submission_time': 605.7171161174774, 'accumulated_eval_time': 5482.923253774643, 'accumulated_logging_time': 0.15216279029846191}
I0316 17:23:00.484376 140082530985728 logging_writer.py:48] [593] accumulated_eval_time=5482.92, accumulated_logging_time=0.152163, accumulated_submission_time=605.717, global_step=593, preemption_count=0, score=605.717, test/loss=0.1298, test/num_examples=95000000, total_duration=6093.6, train/loss=0.126237, validation/loss=0.126798, validation/num_examples=83274637
I0316 17:25:01.950297 140125047432384 spec.py:321] Evaluating on the training split.
I0316 17:30:05.392933 140125047432384 spec.py:333] Evaluating on the validation split.
I0316 17:34:21.983291 140125047432384 spec.py:349] Evaluating on the test split.
I0316 17:39:24.204612 140125047432384 submission_runner.py:469] Time since start: 7077.33s, 	Step: 713, 	{'train/loss': 0.12626206336277107, 'validation/loss': 0.1265042595853662, 'validation/num_examples': 83274637, 'test/loss': 0.12907553347175998, 'test/num_examples': 95000000, 'score': 726.267671585083, 'total_duration': 7077.333902359009, 'accumulated_submission_time': 726.267671585083, 'accumulated_eval_time': 6345.17764043808, 'accumulated_logging_time': 0.16869831085205078}
I0316 17:39:24.214615 140082522593024 logging_writer.py:48] [713] accumulated_eval_time=6345.18, accumulated_logging_time=0.168698, accumulated_submission_time=726.268, global_step=713, preemption_count=0, score=726.268, test/loss=0.129076, test/num_examples=95000000, total_duration=7077.33, train/loss=0.126262, validation/loss=0.126504, validation/num_examples=83274637
I0316 17:41:25.605955 140125047432384 spec.py:321] Evaluating on the training split.
I0316 17:46:30.806172 140125047432384 spec.py:333] Evaluating on the validation split.
I0316 17:50:55.869991 140125047432384 spec.py:349] Evaluating on the test split.
I0316 17:56:31.510408 140125047432384 submission_runner.py:469] Time since start: 8104.64s, 	Step: 839, 	{'train/loss': 0.12584306294757733, 'validation/loss': 0.12634487004401812, 'validation/num_examples': 83274637, 'test/loss': 0.12899712999074836, 'test/num_examples': 95000000, 'score': 846.7271239757538, 'total_duration': 8104.63975071907, 'accumulated_submission_time': 846.7271239757538, 'accumulated_eval_time': 7251.082218885422, 'accumulated_logging_time': 0.18576335906982422}
I0316 17:56:31.521117 140082530985728 logging_writer.py:48] [839] accumulated_eval_time=7251.08, accumulated_logging_time=0.185763, accumulated_submission_time=846.727, global_step=839, preemption_count=0, score=846.727, test/loss=0.128997, test/num_examples=95000000, total_duration=8104.64, train/loss=0.125843, validation/loss=0.126345, validation/num_examples=83274637
I0316 17:58:33.220323 140125047432384 spec.py:321] Evaluating on the training split.
I0316 18:03:27.614478 140125047432384 spec.py:333] Evaluating on the validation split.
I0316 18:07:37.860215 140125047432384 spec.py:349] Evaluating on the test split.
I0316 18:12:32.921054 140125047432384 submission_runner.py:469] Time since start: 9066.05s, 	Step: 961, 	{'train/loss': 0.12651580439648896, 'validation/loss': 0.12609595182342126, 'validation/num_examples': 83274637, 'test/loss': 0.128661724803001, 'test/num_examples': 95000000, 'score': 967.5486996173859, 'total_duration': 9066.050383329391, 'accumulated_submission_time': 967.5486996173859, 'accumulated_eval_time': 8090.783136606216, 'accumulated_logging_time': 0.2034168243408203}
I0316 18:12:32.930613 140082522593024 logging_writer.py:48] [961] accumulated_eval_time=8090.78, accumulated_logging_time=0.203417, accumulated_submission_time=967.549, global_step=961, preemption_count=0, score=967.549, test/loss=0.128662, test/num_examples=95000000, total_duration=9066.05, train/loss=0.126516, validation/loss=0.126096, validation/num_examples=83274637
I0316 18:12:50.559767 140082530985728 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.0363693, loss=0.124048
I0316 18:12:50.562825 140125047432384 submission.py:265] 1000) loss = 0.124, grad_norm = 0.036
I0316 18:14:33.788796 140125047432384 spec.py:321] Evaluating on the training split.
I0316 18:19:07.587237 140125047432384 spec.py:333] Evaluating on the validation split.
I0316 18:23:17.196087 140125047432384 spec.py:349] Evaluating on the test split.
I0316 18:28:18.994688 140125047432384 submission_runner.py:469] Time since start: 10012.12s, 	Step: 1083, 	{'train/loss': 0.12385800336856785, 'validation/loss': 0.1260711407536623, 'validation/num_examples': 83274637, 'test/loss': 0.12848827132158783, 'test/num_examples': 95000000, 'score': 1087.4823427200317, 'total_duration': 10012.124031066895, 'accumulated_submission_time': 1087.4823427200317, 'accumulated_eval_time': 8915.989181995392, 'accumulated_logging_time': 0.2786734104156494}
I0316 18:28:19.004863 140082522593024 logging_writer.py:48] [1083] accumulated_eval_time=8915.99, accumulated_logging_time=0.278673, accumulated_submission_time=1087.48, global_step=1083, preemption_count=0, score=1087.48, test/loss=0.128488, test/num_examples=95000000, total_duration=10012.1, train/loss=0.123858, validation/loss=0.126071, validation/num_examples=83274637
I0316 18:30:19.478375 140125047432384 spec.py:321] Evaluating on the training split.
I0316 18:34:28.880169 140125047432384 spec.py:333] Evaluating on the validation split.
I0316 18:38:15.203447 140125047432384 spec.py:349] Evaluating on the test split.
I0316 18:42:46.036494 140125047432384 submission_runner.py:469] Time since start: 10879.17s, 	Step: 1207, 	{'train/loss': 0.12492642857455166, 'validation/loss': 0.12569777573342475, 'validation/num_examples': 83274637, 'test/loss': 0.12820489671510396, 'test/num_examples': 95000000, 'score': 1207.085381269455, 'total_duration': 10879.165850400925, 'accumulated_submission_time': 1207.085381269455, 'accumulated_eval_time': 9662.547525167465, 'accumulated_logging_time': 0.29537510871887207}
I0316 18:42:46.046558 140082530985728 logging_writer.py:48] [1207] accumulated_eval_time=9662.55, accumulated_logging_time=0.295375, accumulated_submission_time=1207.09, global_step=1207, preemption_count=0, score=1207.09, test/loss=0.128205, test/num_examples=95000000, total_duration=10879.2, train/loss=0.124926, validation/loss=0.125698, validation/num_examples=83274637
I0316 18:44:47.287873 140125047432384 spec.py:321] Evaluating on the training split.
I0316 18:47:46.484356 140125047432384 spec.py:333] Evaluating on the validation split.
I0316 18:50:42.465442 140125047432384 spec.py:349] Evaluating on the test split.
I0316 18:54:30.911889 140125047432384 submission_runner.py:469] Time since start: 11584.04s, 	Step: 1327, 	{'train/loss': 0.1235392713216072, 'validation/loss': 0.12595282278939726, 'validation/num_examples': 83274637, 'test/loss': 0.12830045721491765, 'test/num_examples': 95000000, 'score': 1327.4393424987793, 'total_duration': 11584.041192770004, 'accumulated_submission_time': 1327.4393424987793, 'accumulated_eval_time': 10246.171612501144, 'accumulated_logging_time': 0.3119082450866699}
I0316 18:54:30.921573 140082522593024 logging_writer.py:48] [1327] accumulated_eval_time=10246.2, accumulated_logging_time=0.311908, accumulated_submission_time=1327.44, global_step=1327, preemption_count=0, score=1327.44, test/loss=0.1283, test/num_examples=95000000, total_duration=11584, train/loss=0.123539, validation/loss=0.125953, validation/num_examples=83274637
I0316 18:56:32.833832 140125047432384 spec.py:321] Evaluating on the training split.
I0316 18:58:37.146475 140125047432384 spec.py:333] Evaluating on the validation split.
I0316 19:00:41.409859 140125047432384 spec.py:349] Evaluating on the test split.
I0316 19:03:35.376661 140125047432384 submission_runner.py:469] Time since start: 12128.51s, 	Step: 1448, 	{'train/loss': 0.12544466136866228, 'validation/loss': 0.12537812173080837, 'validation/num_examples': 83274637, 'test/loss': 0.12780259701240942, 'test/num_examples': 95000000, 'score': 1448.440856218338, 'total_duration': 12128.506041288376, 'accumulated_submission_time': 1448.440856218338, 'accumulated_eval_time': 10668.714637517929, 'accumulated_logging_time': 0.32816600799560547}
I0316 19:03:35.387214 140082530985728 logging_writer.py:48] [1448] accumulated_eval_time=10668.7, accumulated_logging_time=0.328166, accumulated_submission_time=1448.44, global_step=1448, preemption_count=0, score=1448.44, test/loss=0.127803, test/num_examples=95000000, total_duration=12128.5, train/loss=0.125445, validation/loss=0.125378, validation/num_examples=83274637
I0316 19:04:07.354649 140082522593024 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.0140447, loss=0.133748
I0316 19:04:07.358792 140125047432384 submission.py:265] 1500) loss = 0.134, grad_norm = 0.014
I0316 19:05:36.313811 140125047432384 spec.py:321] Evaluating on the training split.
I0316 19:07:42.457732 140125047432384 spec.py:333] Evaluating on the validation split.
I0316 19:09:46.207456 140125047432384 spec.py:349] Evaluating on the test split.
I0316 19:12:10.183815 140125047432384 submission_runner.py:469] Time since start: 12643.31s, 	Step: 1570, 	{'train/loss': 0.12359536651792358, 'validation/loss': 0.12548711997275785, 'validation/num_examples': 83274637, 'test/loss': 0.12787936339657432, 'test/num_examples': 95000000, 'score': 1568.474135875702, 'total_duration': 12643.31314921379, 'accumulated_submission_time': 1568.474135875702, 'accumulated_eval_time': 11062.584811925888, 'accumulated_logging_time': 0.34617066383361816}
I0316 19:12:10.221785 140082530985728 logging_writer.py:48] [1570] accumulated_eval_time=11062.6, accumulated_logging_time=0.346171, accumulated_submission_time=1568.47, global_step=1570, preemption_count=0, score=1568.47, test/loss=0.127879, test/num_examples=95000000, total_duration=12643.3, train/loss=0.123595, validation/loss=0.125487, validation/num_examples=83274637
I0316 19:14:12.373547 140125047432384 spec.py:321] Evaluating on the training split.
I0316 19:16:16.973203 140125047432384 spec.py:333] Evaluating on the validation split.
I0316 19:18:20.704184 140125047432384 spec.py:349] Evaluating on the test split.
I0316 19:20:44.155435 140125047432384 submission_runner.py:469] Time since start: 13157.28s, 	Step: 1688, 	{'train/loss': 0.12383909421111233, 'validation/loss': 0.125504837567578, 'validation/num_examples': 83274637, 'test/loss': 0.12783957995814274, 'test/num_examples': 95000000, 'score': 1689.696527004242, 'total_duration': 13157.284777879715, 'accumulated_submission_time': 1689.696527004242, 'accumulated_eval_time': 11454.36686038971, 'accumulated_logging_time': 0.3975396156311035}
I0316 19:20:44.166965 140082522593024 logging_writer.py:48] [1688] accumulated_eval_time=11454.4, accumulated_logging_time=0.39754, accumulated_submission_time=1689.7, global_step=1688, preemption_count=0, score=1689.7, test/loss=0.12784, test/num_examples=95000000, total_duration=13157.3, train/loss=0.123839, validation/loss=0.125505, validation/num_examples=83274637
I0316 19:22:45.635741 140125047432384 spec.py:321] Evaluating on the training split.
I0316 19:24:50.423428 140125047432384 spec.py:333] Evaluating on the validation split.
I0316 19:26:55.089453 140125047432384 spec.py:349] Evaluating on the test split.
I0316 19:29:19.047082 140125047432384 submission_runner.py:469] Time since start: 13672.18s, 	Step: 1808, 	{'train/loss': 0.12356003743151876, 'validation/loss': 0.1251110461609913, 'validation/num_examples': 83274637, 'test/loss': 0.12751403249343068, 'test/num_examples': 95000000, 'score': 1810.2850329875946, 'total_duration': 13672.176440000534, 'accumulated_submission_time': 1810.2850329875946, 'accumulated_eval_time': 11847.77844619751, 'accumulated_logging_time': 0.41586995124816895}
I0316 19:29:19.058271 140082530985728 logging_writer.py:48] [1808] accumulated_eval_time=11847.8, accumulated_logging_time=0.41587, accumulated_submission_time=1810.29, global_step=1808, preemption_count=0, score=1810.29, test/loss=0.127514, test/num_examples=95000000, total_duration=13672.2, train/loss=0.12356, validation/loss=0.125111, validation/num_examples=83274637
I0316 19:31:20.389370 140125047432384 spec.py:321] Evaluating on the training split.
I0316 19:33:24.581424 140125047432384 spec.py:333] Evaluating on the validation split.
I0316 19:35:29.330669 140125047432384 spec.py:349] Evaluating on the test split.
I0316 19:37:52.041366 140125047432384 submission_runner.py:469] Time since start: 14185.17s, 	Step: 1929, 	{'train/loss': 0.12372115941218205, 'validation/loss': 0.1251081976454823, 'validation/num_examples': 83274637, 'test/loss': 0.12755520625634445, 'test/num_examples': 95000000, 'score': 1930.7281482219696, 'total_duration': 14185.170724868774, 'accumulated_submission_time': 1930.7281482219696, 'accumulated_eval_time': 12239.430521726608, 'accumulated_logging_time': 0.4337458610534668}
I0316 19:37:52.100355 140082522593024 logging_writer.py:48] [1929] accumulated_eval_time=12239.4, accumulated_logging_time=0.433746, accumulated_submission_time=1930.73, global_step=1929, preemption_count=0, score=1930.73, test/loss=0.127555, test/num_examples=95000000, total_duration=14185.2, train/loss=0.123721, validation/loss=0.125108, validation/num_examples=83274637
I0316 19:38:53.096288 140082530985728 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.024008, loss=0.115563
I0316 19:38:53.099648 140125047432384 submission.py:265] 2000) loss = 0.116, grad_norm = 0.024
I0316 19:39:52.454778 140125047432384 spec.py:321] Evaluating on the training split.
I0316 19:41:57.082311 140125047432384 spec.py:333] Evaluating on the validation split.
I0316 19:44:01.393836 140125047432384 spec.py:349] Evaluating on the test split.
I0316 19:46:24.956205 140125047432384 submission_runner.py:469] Time since start: 14698.09s, 	Step: 2046, 	{'train/loss': 0.12331670101579208, 'validation/loss': 0.1252169392266097, 'validation/num_examples': 83274637, 'test/loss': 0.127724946681374, 'test/num_examples': 95000000, 'score': 2050.1818478107452, 'total_duration': 14698.08552479744, 'accumulated_submission_time': 2050.1818478107452, 'accumulated_eval_time': 12631.932119846344, 'accumulated_logging_time': 0.5004451274871826}
I0316 19:46:24.967975 140082522593024 logging_writer.py:48] [2046] accumulated_eval_time=12631.9, accumulated_logging_time=0.500445, accumulated_submission_time=2050.18, global_step=2046, preemption_count=0, score=2050.18, test/loss=0.127725, test/num_examples=95000000, total_duration=14698.1, train/loss=0.123317, validation/loss=0.125217, validation/num_examples=83274637
I0316 19:48:25.724594 140125047432384 spec.py:321] Evaluating on the training split.
I0316 19:50:30.074011 140125047432384 spec.py:333] Evaluating on the validation split.
I0316 19:52:34.181189 140125047432384 spec.py:349] Evaluating on the test split.
I0316 19:54:58.234040 140125047432384 submission_runner.py:469] Time since start: 15211.36s, 	Step: 2166, 	{'train/loss': 0.12606750521895893, 'validation/loss': 0.12490935073714989, 'validation/num_examples': 83274637, 'test/loss': 0.12725140277637684, 'test/num_examples': 95000000, 'score': 2170.035364627838, 'total_duration': 15211.363380670547, 'accumulated_submission_time': 2170.035364627838, 'accumulated_eval_time': 13024.441817045212, 'accumulated_logging_time': 0.525820255279541}
I0316 19:54:58.244972 140082530985728 logging_writer.py:48] [2166] accumulated_eval_time=13024.4, accumulated_logging_time=0.52582, accumulated_submission_time=2170.04, global_step=2166, preemption_count=0, score=2170.04, test/loss=0.127251, test/num_examples=95000000, total_duration=15211.4, train/loss=0.126068, validation/loss=0.124909, validation/num_examples=83274637
I0316 19:56:59.550820 140125047432384 spec.py:321] Evaluating on the training split.
I0316 19:59:04.333975 140125047432384 spec.py:333] Evaluating on the validation split.
I0316 20:01:09.049563 140125047432384 spec.py:349] Evaluating on the test split.
I0316 20:03:33.109178 140125047432384 submission_runner.py:469] Time since start: 15726.24s, 	Step: 2287, 	{'train/loss': 0.12430699084547006, 'validation/loss': 0.12505820183940403, 'validation/num_examples': 83274637, 'test/loss': 0.12744297551028602, 'test/num_examples': 95000000, 'score': 2290.472546815872, 'total_duration': 15726.23852801323, 'accumulated_submission_time': 2290.472546815872, 'accumulated_eval_time': 13418.00026679039, 'accumulated_logging_time': 0.5434913635253906}
I0316 20:03:33.120169 140082522593024 logging_writer.py:48] [2287] accumulated_eval_time=13418, accumulated_logging_time=0.543491, accumulated_submission_time=2290.47, global_step=2287, preemption_count=0, score=2290.47, test/loss=0.127443, test/num_examples=95000000, total_duration=15726.2, train/loss=0.124307, validation/loss=0.125058, validation/num_examples=83274637
I0316 20:05:34.654401 140125047432384 spec.py:321] Evaluating on the training split.
I0316 20:07:39.144861 140125047432384 spec.py:333] Evaluating on the validation split.
I0316 20:09:42.832932 140125047432384 spec.py:349] Evaluating on the test split.
I0316 20:12:06.551115 140125047432384 submission_runner.py:469] Time since start: 16239.68s, 	Step: 2407, 	{'train/loss': 0.1240767633969145, 'validation/loss': 0.12485706137748302, 'validation/num_examples': 83274637, 'test/loss': 0.12727261800513018, 'test/num_examples': 95000000, 'score': 2411.14200258255, 'total_duration': 16239.680477380753, 'accumulated_submission_time': 2411.14200258255, 'accumulated_eval_time': 13809.897166490555, 'accumulated_logging_time': 0.5613570213317871}
I0316 20:12:06.562124 140082530985728 logging_writer.py:48] [2407] accumulated_eval_time=13809.9, accumulated_logging_time=0.561357, accumulated_submission_time=2411.14, global_step=2407, preemption_count=0, score=2411.14, test/loss=0.127273, test/num_examples=95000000, total_duration=16239.7, train/loss=0.124077, validation/loss=0.124857, validation/num_examples=83274637
I0316 20:13:32.258423 140082522593024 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.0146312, loss=0.126521
I0316 20:13:32.291059 140125047432384 submission.py:265] 2500) loss = 0.127, grad_norm = 0.015
I0316 20:14:07.315333 140125047432384 spec.py:321] Evaluating on the training split.
I0316 20:16:11.931530 140125047432384 spec.py:333] Evaluating on the validation split.
I0316 20:18:16.469036 140125047432384 spec.py:349] Evaluating on the test split.
I0316 20:20:40.428498 140125047432384 submission_runner.py:469] Time since start: 16753.56s, 	Step: 2528, 	{'train/loss': 0.1244476158688123, 'validation/loss': 0.12498529533130753, 'validation/num_examples': 83274637, 'test/loss': 0.12741465456101267, 'test/num_examples': 95000000, 'score': 2530.9913654327393, 'total_duration': 16753.557775259018, 'accumulated_submission_time': 2530.9913654327393, 'accumulated_eval_time': 14203.01037979126, 'accumulated_logging_time': 0.5801191329956055}
I0316 20:20:40.440654 140082530985728 logging_writer.py:48] [2528] accumulated_eval_time=14203, accumulated_logging_time=0.580119, accumulated_submission_time=2530.99, global_step=2528, preemption_count=0, score=2530.99, test/loss=0.127415, test/num_examples=95000000, total_duration=16753.6, train/loss=0.124448, validation/loss=0.124985, validation/num_examples=83274637
I0316 20:22:41.112114 140125047432384 spec.py:321] Evaluating on the training split.
I0316 20:24:45.883462 140125047432384 spec.py:333] Evaluating on the validation split.
I0316 20:26:50.377046 140125047432384 spec.py:349] Evaluating on the test split.
I0316 20:29:13.769820 140125047432384 submission_runner.py:469] Time since start: 17266.90s, 	Step: 2645, 	{'train/loss': 0.12409423905086361, 'validation/loss': 0.12489193480912575, 'validation/num_examples': 83274637, 'test/loss': 0.1272521022617541, 'test/num_examples': 95000000, 'score': 2650.7496197223663, 'total_duration': 17266.89911556244, 'accumulated_submission_time': 2650.7496197223663, 'accumulated_eval_time': 14595.668315887451, 'accumulated_logging_time': 0.6001720428466797}
I0316 20:29:13.780853 140082522593024 logging_writer.py:48] [2645] accumulated_eval_time=14595.7, accumulated_logging_time=0.600172, accumulated_submission_time=2650.75, global_step=2645, preemption_count=0, score=2650.75, test/loss=0.127252, test/num_examples=95000000, total_duration=17266.9, train/loss=0.124094, validation/loss=0.124892, validation/num_examples=83274637
I0316 20:31:14.453499 140125047432384 spec.py:321] Evaluating on the training split.
I0316 20:33:19.255961 140125047432384 spec.py:333] Evaluating on the validation split.
I0316 20:35:23.274136 140125047432384 spec.py:349] Evaluating on the test split.
I0316 20:37:47.421206 140125047432384 submission_runner.py:469] Time since start: 17780.55s, 	Step: 2764, 	{'train/loss': 0.12351750629526581, 'validation/loss': 0.1248726980479582, 'validation/num_examples': 83274637, 'test/loss': 0.12734072171325683, 'test/num_examples': 95000000, 'score': 2770.5040216445923, 'total_duration': 17780.550576925278, 'accumulated_submission_time': 2770.5040216445923, 'accumulated_eval_time': 14988.636237621307, 'accumulated_logging_time': 0.6231412887573242}
I0316 20:37:47.432834 140082530985728 logging_writer.py:48] [2764] accumulated_eval_time=14988.6, accumulated_logging_time=0.623141, accumulated_submission_time=2770.5, global_step=2764, preemption_count=0, score=2770.5, test/loss=0.127341, test/num_examples=95000000, total_duration=17780.6, train/loss=0.123518, validation/loss=0.124873, validation/num_examples=83274637
I0316 20:39:48.563876 140125047432384 spec.py:321] Evaluating on the training split.
I0316 20:41:53.814562 140125047432384 spec.py:333] Evaluating on the validation split.
I0316 20:43:58.861872 140125047432384 spec.py:349] Evaluating on the test split.
I0316 20:46:22.552965 140125047432384 submission_runner.py:469] Time since start: 18295.68s, 	Step: 2884, 	{'train/loss': 0.12423976621791734, 'validation/loss': 0.12496345321230495, 'validation/num_examples': 83274637, 'test/loss': 0.12735412327599777, 'test/num_examples': 95000000, 'score': 2890.734165906906, 'total_duration': 18295.68229651451, 'accumulated_submission_time': 2890.734165906906, 'accumulated_eval_time': 15382.625467538834, 'accumulated_logging_time': 0.6421463489532471}
I0316 20:46:22.564574 140082522593024 logging_writer.py:48] [2884] accumulated_eval_time=15382.6, accumulated_logging_time=0.642146, accumulated_submission_time=2890.73, global_step=2884, preemption_count=0, score=2890.73, test/loss=0.127354, test/num_examples=95000000, total_duration=18295.7, train/loss=0.12424, validation/loss=0.124963, validation/num_examples=83274637
I0316 20:48:20.859287 140082530985728 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.012828, loss=0.12955
I0316 20:48:20.862902 140125047432384 submission.py:265] 3000) loss = 0.130, grad_norm = 0.013
I0316 20:48:23.563170 140125047432384 spec.py:321] Evaluating on the training split.
I0316 20:50:28.977498 140125047432384 spec.py:333] Evaluating on the validation split.
I0316 20:52:33.966375 140125047432384 spec.py:349] Evaluating on the test split.
I0316 20:54:57.740874 140125047432384 submission_runner.py:469] Time since start: 18810.87s, 	Step: 3003, 	{'train/loss': 0.12459507288785768, 'validation/loss': 0.12472201906838395, 'validation/num_examples': 83274637, 'test/loss': 0.12716901240668047, 'test/num_examples': 95000000, 'score': 3010.8467679023743, 'total_duration': 18810.870223760605, 'accumulated_submission_time': 3010.8467679023743, 'accumulated_eval_time': 15776.80332660675, 'accumulated_logging_time': 0.6605775356292725}
I0316 20:54:57.752333 140082522593024 logging_writer.py:48] [3003] accumulated_eval_time=15776.8, accumulated_logging_time=0.660578, accumulated_submission_time=3010.85, global_step=3003, preemption_count=0, score=3010.85, test/loss=0.127169, test/num_examples=95000000, total_duration=18810.9, train/loss=0.124595, validation/loss=0.124722, validation/num_examples=83274637
I0316 20:56:58.881906 140125047432384 spec.py:321] Evaluating on the training split.
I0316 20:59:04.418475 140125047432384 spec.py:333] Evaluating on the validation split.
I0316 21:01:09.131351 140125047432384 spec.py:349] Evaluating on the test split.
I0316 21:03:33.138139 140125047432384 submission_runner.py:469] Time since start: 19326.27s, 	Step: 3121, 	{'train/loss': 0.1235495323890858, 'validation/loss': 0.12468437684359913, 'validation/num_examples': 83274637, 'test/loss': 0.12714822296969766, 'test/num_examples': 95000000, 'score': 3131.1110875606537, 'total_duration': 19326.26750612259, 'accumulated_submission_time': 3131.1110875606537, 'accumulated_eval_time': 16171.059642791748, 'accumulated_logging_time': 0.6791493892669678}
I0316 21:03:33.149734 140082530985728 logging_writer.py:48] [3121] accumulated_eval_time=16171.1, accumulated_logging_time=0.679149, accumulated_submission_time=3131.11, global_step=3121, preemption_count=0, score=3131.11, test/loss=0.127148, test/num_examples=95000000, total_duration=19326.3, train/loss=0.12355, validation/loss=0.124684, validation/num_examples=83274637
I0316 21:05:34.648347 140125047432384 spec.py:321] Evaluating on the training split.
I0316 21:07:39.751885 140125047432384 spec.py:333] Evaluating on the validation split.
I0316 21:09:44.697061 140125047432384 spec.py:349] Evaluating on the test split.
I0316 21:12:08.717665 140125047432384 submission_runner.py:469] Time since start: 19841.85s, 	Step: 3242, 	{'train/loss': 0.12485701749951436, 'validation/loss': 0.12474015643028985, 'validation/num_examples': 83274637, 'test/loss': 0.1271577455314636, 'test/num_examples': 95000000, 'score': 3251.654002904892, 'total_duration': 19841.84696173668, 'accumulated_submission_time': 3251.654002904892, 'accumulated_eval_time': 16565.129110336304, 'accumulated_logging_time': 0.7469582557678223}
I0316 21:12:08.729094 140082522593024 logging_writer.py:48] [3242] accumulated_eval_time=16565.1, accumulated_logging_time=0.746958, accumulated_submission_time=3251.65, global_step=3242, preemption_count=0, score=3251.65, test/loss=0.127158, test/num_examples=95000000, total_duration=19841.8, train/loss=0.124857, validation/loss=0.12474, validation/num_examples=83274637
I0316 21:14:09.487081 140125047432384 spec.py:321] Evaluating on the training split.
I0316 21:16:14.306476 140125047432384 spec.py:333] Evaluating on the validation split.
I0316 21:18:19.020149 140125047432384 spec.py:349] Evaluating on the test split.
I0316 21:20:42.225749 140125047432384 submission_runner.py:469] Time since start: 20355.36s, 	Step: 3360, 	{'train/loss': 0.12456685213488275, 'validation/loss': 0.12458642418710922, 'validation/num_examples': 83274637, 'test/loss': 0.12690310255524484, 'test/num_examples': 95000000, 'score': 3371.528143644333, 'total_duration': 20355.35507273674, 'accumulated_submission_time': 3371.528143644333, 'accumulated_eval_time': 16957.867936611176, 'accumulated_logging_time': 0.7656712532043457}
I0316 21:20:42.237292 140082530985728 logging_writer.py:48] [3360] accumulated_eval_time=16957.9, accumulated_logging_time=0.765671, accumulated_submission_time=3371.53, global_step=3360, preemption_count=0, score=3371.53, test/loss=0.126903, test/num_examples=95000000, total_duration=20355.4, train/loss=0.124567, validation/loss=0.124586, validation/num_examples=83274637
I0316 21:22:43.585233 140125047432384 spec.py:321] Evaluating on the training split.
I0316 21:24:48.468997 140125047432384 spec.py:333] Evaluating on the validation split.
I0316 21:26:54.233052 140125047432384 spec.py:349] Evaluating on the test split.
I0316 21:29:16.728487 140125047432384 submission_runner.py:469] Time since start: 20869.86s, 	Step: 3480, 	{'train/loss': 0.12011627089179072, 'validation/loss': 0.12478746094730993, 'validation/num_examples': 83274637, 'test/loss': 0.12710570001642327, 'test/num_examples': 95000000, 'score': 3492.01784157753, 'total_duration': 20869.85787153244, 'accumulated_submission_time': 3492.01784157753, 'accumulated_eval_time': 17351.011322259903, 'accumulated_logging_time': 0.7851085662841797}
I0316 21:29:16.739249 140082522593024 logging_writer.py:48] [3480] accumulated_eval_time=17351, accumulated_logging_time=0.785109, accumulated_submission_time=3492.02, global_step=3480, preemption_count=0, score=3492.02, test/loss=0.127106, test/num_examples=95000000, total_duration=20869.9, train/loss=0.120116, validation/loss=0.124787, validation/num_examples=83274637
I0316 21:29:21.281859 140082530985728 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.0124059, loss=0.118607
I0316 21:29:21.285542 140125047432384 submission.py:265] 3500) loss = 0.119, grad_norm = 0.012
I0316 21:31:18.248101 140125047432384 spec.py:321] Evaluating on the training split.
I0316 21:33:22.767867 140125047432384 spec.py:333] Evaluating on the validation split.
I0316 21:35:27.008148 140125047432384 spec.py:349] Evaluating on the test split.
I0316 21:37:49.381694 140125047432384 submission_runner.py:469] Time since start: 21382.51s, 	Step: 3598, 	{'train/loss': 0.1230597756653709, 'validation/loss': 0.12478527678569838, 'validation/num_examples': 83274637, 'test/loss': 0.1271509694898505, 'test/num_examples': 95000000, 'score': 3612.682875394821, 'total_duration': 21382.51104402542, 'accumulated_submission_time': 3612.682875394821, 'accumulated_eval_time': 17742.145010709763, 'accumulated_logging_time': 0.8029608726501465}
I0316 21:37:49.393103 140082522593024 logging_writer.py:48] [3598] accumulated_eval_time=17742.1, accumulated_logging_time=0.802961, accumulated_submission_time=3612.68, global_step=3598, preemption_count=0, score=3612.68, test/loss=0.127151, test/num_examples=95000000, total_duration=21382.5, train/loss=0.12306, validation/loss=0.124785, validation/num_examples=83274637
I0316 21:39:50.013918 140125047432384 spec.py:321] Evaluating on the training split.
I0316 21:41:54.504091 140125047432384 spec.py:333] Evaluating on the validation split.
I0316 21:43:58.886637 140125047432384 spec.py:349] Evaluating on the test split.
I0316 21:46:21.272033 140125047432384 submission_runner.py:469] Time since start: 21894.40s, 	Step: 3711, 	{'train/loss': 0.12492972502053244, 'validation/loss': 0.12477925005057326, 'validation/num_examples': 83274637, 'test/loss': 0.12718889769343325, 'test/num_examples': 95000000, 'score': 3732.3523695468903, 'total_duration': 21894.40137076378, 'accumulated_submission_time': 3732.3523695468903, 'accumulated_eval_time': 18133.40328001976, 'accumulated_logging_time': 0.8749756813049316}
I0316 21:46:21.283412 140082530985728 logging_writer.py:48] [3711] accumulated_eval_time=18133.4, accumulated_logging_time=0.874976, accumulated_submission_time=3732.35, global_step=3711, preemption_count=0, score=3732.35, test/loss=0.127189, test/num_examples=95000000, total_duration=21894.4, train/loss=0.12493, validation/loss=0.124779, validation/num_examples=83274637
I0316 21:48:22.263561 140125047432384 spec.py:321] Evaluating on the training split.
I0316 21:50:27.154915 140125047432384 spec.py:333] Evaluating on the validation split.
I0316 21:52:31.517004 140125047432384 spec.py:349] Evaluating on the test split.
I0316 21:54:54.800350 140125047432384 submission_runner.py:469] Time since start: 22407.93s, 	Step: 3829, 	{'train/loss': 0.12394620809173085, 'validation/loss': 0.12470025538686196, 'validation/num_examples': 83274637, 'test/loss': 0.12697247383989033, 'test/num_examples': 95000000, 'score': 3852.4891498088837, 'total_duration': 22407.929710388184, 'accumulated_submission_time': 3852.4891498088837, 'accumulated_eval_time': 18525.940205812454, 'accumulated_logging_time': 0.8935642242431641}
I0316 21:54:54.811411 140082522593024 logging_writer.py:48] [3829] accumulated_eval_time=18525.9, accumulated_logging_time=0.893564, accumulated_submission_time=3852.49, global_step=3829, preemption_count=0, score=3852.49, test/loss=0.126972, test/num_examples=95000000, total_duration=22407.9, train/loss=0.123946, validation/loss=0.1247, validation/num_examples=83274637
I0316 21:56:56.107511 140125047432384 spec.py:321] Evaluating on the training split.
I0316 21:59:00.826716 140125047432384 spec.py:333] Evaluating on the validation split.
I0316 22:01:04.352442 140125047432384 spec.py:349] Evaluating on the test split.
I0316 22:03:27.670836 140125047432384 submission_runner.py:469] Time since start: 22920.80s, 	Step: 3947, 	{'train/loss': 0.12408631839861824, 'validation/loss': 0.12451185750737204, 'validation/num_examples': 83274637, 'test/loss': 0.12690452366967453, 'test/num_examples': 95000000, 'score': 3972.8986468315125, 'total_duration': 22920.800115823746, 'accumulated_submission_time': 3972.8986468315125, 'accumulated_eval_time': 18917.503640413284, 'accumulated_logging_time': 0.9115588665008545}
I0316 22:03:27.682417 140082530985728 logging_writer.py:48] [3947] accumulated_eval_time=18917.5, accumulated_logging_time=0.911559, accumulated_submission_time=3972.9, global_step=3947, preemption_count=0, score=3972.9, test/loss=0.126905, test/num_examples=95000000, total_duration=22920.8, train/loss=0.124086, validation/loss=0.124512, validation/num_examples=83274637
I0316 22:04:05.405374 140082522593024 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.00811151, loss=0.12696
I0316 22:04:05.408905 140125047432384 submission.py:265] 4000) loss = 0.127, grad_norm = 0.008
I0316 22:05:29.068996 140125047432384 spec.py:321] Evaluating on the training split.
I0316 22:07:33.541661 140125047432384 spec.py:333] Evaluating on the validation split.
I0316 22:09:37.997064 140125047432384 spec.py:349] Evaluating on the test split.
I0316 22:12:00.946082 140125047432384 submission_runner.py:469] Time since start: 23434.08s, 	Step: 4067, 	{'train/loss': 0.12267989675411245, 'validation/loss': 0.12447034559370707, 'validation/num_examples': 83274637, 'test/loss': 0.1268672549417596, 'test/num_examples': 95000000, 'score': 4093.4268414974213, 'total_duration': 23434.075429677963, 'accumulated_submission_time': 4093.4268414974213, 'accumulated_eval_time': 19309.380796909332, 'accumulated_logging_time': 0.9294993877410889}
I0316 22:12:00.957566 140082530985728 logging_writer.py:48] [4067] accumulated_eval_time=19309.4, accumulated_logging_time=0.929499, accumulated_submission_time=4093.43, global_step=4067, preemption_count=0, score=4093.43, test/loss=0.126867, test/num_examples=95000000, total_duration=23434.1, train/loss=0.12268, validation/loss=0.12447, validation/num_examples=83274637
I0316 22:14:01.701066 140125047432384 spec.py:321] Evaluating on the training split.
I0316 22:16:06.043651 140125047432384 spec.py:333] Evaluating on the validation split.
I0316 22:18:10.390318 140125047432384 spec.py:349] Evaluating on the test split.
I0316 22:20:33.606807 140125047432384 submission_runner.py:469] Time since start: 23946.74s, 	Step: 4186, 	{'train/loss': 0.12334055098539727, 'validation/loss': 0.12445278047333339, 'validation/num_examples': 83274637, 'test/loss': 0.1267980420651486, 'test/num_examples': 95000000, 'score': 4213.290420770645, 'total_duration': 23946.736095905304, 'accumulated_submission_time': 4213.290420770645, 'accumulated_eval_time': 19701.286665439606, 'accumulated_logging_time': 0.9862127304077148}
I0316 22:20:33.618442 140082522593024 logging_writer.py:48] [4186] accumulated_eval_time=19701.3, accumulated_logging_time=0.986213, accumulated_submission_time=4213.29, global_step=4186, preemption_count=0, score=4213.29, test/loss=0.126798, test/num_examples=95000000, total_duration=23946.7, train/loss=0.123341, validation/loss=0.124453, validation/num_examples=83274637
I0316 22:22:34.679171 140125047432384 spec.py:321] Evaluating on the training split.
I0316 22:24:39.139040 140125047432384 spec.py:333] Evaluating on the validation split.
I0316 22:26:43.444505 140125047432384 spec.py:349] Evaluating on the test split.
I0316 22:29:05.184887 140125047432384 submission_runner.py:469] Time since start: 24458.31s, 	Step: 4308, 	{'train/loss': 0.12420711200601858, 'validation/loss': 0.12442421007969248, 'validation/num_examples': 83274637, 'test/loss': 0.12669958235666376, 'test/num_examples': 95000000, 'score': 4333.509029388428, 'total_duration': 24458.314195394516, 'accumulated_submission_time': 4333.509029388428, 'accumulated_eval_time': 20091.79252767563, 'accumulated_logging_time': 1.004957675933838}
I0316 22:29:05.196646 140082530985728 logging_writer.py:48] [4308] accumulated_eval_time=20091.8, accumulated_logging_time=1.00496, accumulated_submission_time=4333.51, global_step=4308, preemption_count=0, score=4333.51, test/loss=0.1267, test/num_examples=95000000, total_duration=24458.3, train/loss=0.124207, validation/loss=0.124424, validation/num_examples=83274637
I0316 22:31:05.740700 140125047432384 spec.py:321] Evaluating on the training split.
I0316 22:33:10.164377 140125047432384 spec.py:333] Evaluating on the validation split.
I0316 22:35:14.556557 140125047432384 spec.py:349] Evaluating on the test split.
I0316 22:37:37.568865 140125047432384 submission_runner.py:469] Time since start: 24970.70s, 	Step: 4427, 	{'train/loss': 0.12456859660888402, 'validation/loss': 0.1244903592250353, 'validation/num_examples': 83274637, 'test/loss': 0.12702604162734182, 'test/num_examples': 95000000, 'score': 4453.187712430954, 'total_duration': 24970.698207378387, 'accumulated_submission_time': 4453.187712430954, 'accumulated_eval_time': 20483.620792150497, 'accumulated_logging_time': 1.0239145755767822}
I0316 22:37:37.580992 140082522593024 logging_writer.py:48] [4427] accumulated_eval_time=20483.6, accumulated_logging_time=1.02391, accumulated_submission_time=4453.19, global_step=4427, preemption_count=0, score=4453.19, test/loss=0.127026, test/num_examples=95000000, total_duration=24970.7, train/loss=0.124569, validation/loss=0.12449, validation/num_examples=83274637
I0316 22:38:39.457523 140082530985728 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.0376143, loss=0.121915
I0316 22:38:39.461257 140125047432384 submission.py:265] 4500) loss = 0.122, grad_norm = 0.038
I0316 22:39:38.188407 140125047432384 spec.py:321] Evaluating on the training split.
I0316 22:41:42.517789 140125047432384 spec.py:333] Evaluating on the validation split.
I0316 22:43:46.988710 140125047432384 spec.py:349] Evaluating on the test split.
I0316 22:46:10.324320 140125047432384 submission_runner.py:469] Time since start: 25483.45s, 	Step: 4546, 	{'train/loss': 0.122877552610927, 'validation/loss': 0.12439467287346427, 'validation/num_examples': 83274637, 'test/loss': 0.12668399165745786, 'test/num_examples': 95000000, 'score': 4572.938852548599, 'total_duration': 25483.453661441803, 'accumulated_submission_time': 4572.938852548599, 'accumulated_eval_time': 20875.75681233406, 'accumulated_logging_time': 1.0431177616119385}
I0316 22:46:10.336822 140082522593024 logging_writer.py:48] [4546] accumulated_eval_time=20875.8, accumulated_logging_time=1.04312, accumulated_submission_time=4572.94, global_step=4546, preemption_count=0, score=4572.94, test/loss=0.126684, test/num_examples=95000000, total_duration=25483.5, train/loss=0.122878, validation/loss=0.124395, validation/num_examples=83274637
I0316 22:48:11.257097 140125047432384 spec.py:321] Evaluating on the training split.
I0316 22:50:16.160066 140125047432384 spec.py:333] Evaluating on the validation split.
I0316 22:52:20.556942 140125047432384 spec.py:349] Evaluating on the test split.
I0316 22:54:43.359810 140125047432384 submission_runner.py:469] Time since start: 25996.49s, 	Step: 4667, 	{'train/loss': 0.124621711375264, 'validation/loss': 0.12438899195453344, 'validation/num_examples': 83274637, 'test/loss': 0.12664475659914518, 'test/num_examples': 95000000, 'score': 4692.992021560669, 'total_duration': 25996.48916053772, 'accumulated_submission_time': 4692.992021560669, 'accumulated_eval_time': 21267.85973882675, 'accumulated_logging_time': 1.06290602684021}
I0316 22:54:43.435702 140082530985728 logging_writer.py:48] [4667] accumulated_eval_time=21267.9, accumulated_logging_time=1.06291, accumulated_submission_time=4692.99, global_step=4667, preemption_count=0, score=4692.99, test/loss=0.126645, test/num_examples=95000000, total_duration=25996.5, train/loss=0.124622, validation/loss=0.124389, validation/num_examples=83274637
I0316 22:56:44.889866 140125047432384 spec.py:321] Evaluating on the training split.
I0316 22:58:49.290802 140125047432384 spec.py:333] Evaluating on the validation split.
I0316 23:00:53.684327 140125047432384 spec.py:349] Evaluating on the test split.
I0316 23:03:16.513284 140125047432384 submission_runner.py:469] Time since start: 26509.64s, 	Step: 4788, 	{'train/loss': 0.12287733971558844, 'validation/loss': 0.12434669346756899, 'validation/num_examples': 83274637, 'test/loss': 0.12671939304147017, 'test/num_examples': 95000000, 'score': 4813.578488111496, 'total_duration': 26509.642488479614, 'accumulated_submission_time': 4813.578488111496, 'accumulated_eval_time': 21659.48313140869, 'accumulated_logging_time': 1.166368007659912}
I0316 23:03:16.524812 140082522593024 logging_writer.py:48] [4788] accumulated_eval_time=21659.5, accumulated_logging_time=1.16637, accumulated_submission_time=4813.58, global_step=4788, preemption_count=0, score=4813.58, test/loss=0.126719, test/num_examples=95000000, total_duration=26509.6, train/loss=0.122877, validation/loss=0.124347, validation/num_examples=83274637
I0316 23:05:17.893359 140125047432384 spec.py:321] Evaluating on the training split.
I0316 23:07:22.527986 140125047432384 spec.py:333] Evaluating on the validation split.
I0316 23:09:26.630689 140125047432384 spec.py:349] Evaluating on the test split.
I0316 23:11:49.852639 140125047432384 submission_runner.py:469] Time since start: 27022.98s, 	Step: 4910, 	{'train/loss': 0.12203624365267168, 'validation/loss': 0.12434088757993943, 'validation/num_examples': 83274637, 'test/loss': 0.1267937087255779, 'test/num_examples': 95000000, 'score': 4934.040390253067, 'total_duration': 27022.9820022583, 'accumulated_submission_time': 4934.040390253067, 'accumulated_eval_time': 22051.442607164383, 'accumulated_logging_time': 1.1932249069213867}
I0316 23:11:49.864265 140082530985728 logging_writer.py:48] [4910] accumulated_eval_time=22051.4, accumulated_logging_time=1.19322, accumulated_submission_time=4934.04, global_step=4910, preemption_count=0, score=4934.04, test/loss=0.126794, test/num_examples=95000000, total_duration=27023, train/loss=0.122036, validation/loss=0.124341, validation/num_examples=83274637
I0316 23:13:13.833907 140082522593024 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.0131783, loss=0.121559
I0316 23:13:13.837010 140125047432384 submission.py:265] 5000) loss = 0.122, grad_norm = 0.013
I0316 23:13:51.253813 140125047432384 spec.py:321] Evaluating on the training split.
I0316 23:15:55.972486 140125047432384 spec.py:333] Evaluating on the validation split.
I0316 23:17:59.367368 140125047432384 spec.py:349] Evaluating on the test split.
I0316 23:20:21.307467 140125047432384 submission_runner.py:469] Time since start: 27534.44s, 	Step: 5032, 	{'train/loss': 0.1220103436409009, 'validation/loss': 0.12433951502489995, 'validation/num_examples': 83274637, 'test/loss': 0.12675460836434616, 'test/num_examples': 95000000, 'score': 5054.550198793411, 'total_duration': 27534.43678355217, 'accumulated_submission_time': 5054.550198793411, 'accumulated_eval_time': 22441.496423006058, 'accumulated_logging_time': 1.2126541137695312}
I0316 23:20:21.319561 140082530985728 logging_writer.py:48] [5032] accumulated_eval_time=22441.5, accumulated_logging_time=1.21265, accumulated_submission_time=5054.55, global_step=5032, preemption_count=0, score=5054.55, test/loss=0.126755, test/num_examples=95000000, total_duration=27534.4, train/loss=0.12201, validation/loss=0.12434, validation/num_examples=83274637
I0316 23:22:21.996466 140125047432384 spec.py:321] Evaluating on the training split.
I0316 23:24:26.555567 140125047432384 spec.py:333] Evaluating on the validation split.
I0316 23:26:29.604429 140125047432384 spec.py:349] Evaluating on the test split.
I0316 23:28:52.579108 140125047432384 submission_runner.py:469] Time since start: 28045.71s, 	Step: 5153, 	{'train/loss': 0.12297991565825352, 'validation/loss': 0.12440757289068381, 'validation/num_examples': 83274637, 'test/loss': 0.12680271123010736, 'test/num_examples': 95000000, 'score': 5174.315847396851, 'total_duration': 28045.708434820175, 'accumulated_submission_time': 5174.315847396851, 'accumulated_eval_time': 22832.079281568527, 'accumulated_logging_time': 1.2540297508239746}
I0316 23:28:52.591690 140082522593024 logging_writer.py:48] [5153] accumulated_eval_time=22832.1, accumulated_logging_time=1.25403, accumulated_submission_time=5174.32, global_step=5153, preemption_count=0, score=5174.32, test/loss=0.126803, test/num_examples=95000000, total_duration=28045.7, train/loss=0.12298, validation/loss=0.124408, validation/num_examples=83274637
I0316 23:30:54.003393 140125047432384 spec.py:321] Evaluating on the training split.
I0316 23:32:58.265745 140125047432384 spec.py:333] Evaluating on the validation split.
I0316 23:35:02.439713 140125047432384 spec.py:349] Evaluating on the test split.
I0316 23:37:26.446422 140125047432384 submission_runner.py:469] Time since start: 28559.58s, 	Step: 5277, 	{'train/loss': 0.12210302423371013, 'validation/loss': 0.12422237176777717, 'validation/num_examples': 83274637, 'test/loss': 0.12658413014646833, 'test/num_examples': 95000000, 'score': 5294.875488758087, 'total_duration': 28559.5757958889, 'accumulated_submission_time': 5294.875488758087, 'accumulated_eval_time': 23224.52239561081, 'accumulated_logging_time': 1.2741541862487793}
I0316 23:37:26.457652 140082530985728 logging_writer.py:48] [5277] accumulated_eval_time=23224.5, accumulated_logging_time=1.27415, accumulated_submission_time=5294.88, global_step=5277, preemption_count=0, score=5294.88, test/loss=0.126584, test/num_examples=95000000, total_duration=28559.6, train/loss=0.122103, validation/loss=0.124222, validation/num_examples=83274637
I0316 23:39:27.240996 140125047432384 spec.py:321] Evaluating on the training split.
I0316 23:41:31.740955 140125047432384 spec.py:333] Evaluating on the validation split.
I0316 23:43:34.876663 140125047432384 spec.py:349] Evaluating on the test split.
I0316 23:45:55.857972 140125047432384 submission_runner.py:469] Time since start: 29068.99s, 	Step: 5399, 	{'train/loss': 0.1221285331308125, 'validation/loss': 0.12421721825820259, 'validation/num_examples': 83274637, 'test/loss': 0.12654207792776007, 'test/num_examples': 95000000, 'score': 5414.821226358414, 'total_duration': 29068.98725795746, 'accumulated_submission_time': 5414.821226358414, 'accumulated_eval_time': 23613.139500141144, 'accumulated_logging_time': 1.2927000522613525}
I0316 23:45:55.869770 140082522593024 logging_writer.py:48] [5399] accumulated_eval_time=23613.1, accumulated_logging_time=1.2927, accumulated_submission_time=5414.82, global_step=5399, preemption_count=0, score=5414.82, test/loss=0.126542, test/num_examples=95000000, total_duration=29069, train/loss=0.122129, validation/loss=0.124217, validation/num_examples=83274637
I0316 23:47:33.545219 140082530985728 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.0121031, loss=0.119579
I0316 23:47:33.548535 140125047432384 submission.py:265] 5500) loss = 0.120, grad_norm = 0.012
I0316 23:47:57.860730 140125047432384 spec.py:321] Evaluating on the training split.
I0316 23:50:02.633201 140125047432384 spec.py:333] Evaluating on the validation split.
I0316 23:52:07.159422 140125047432384 spec.py:349] Evaluating on the test split.
I0316 23:54:30.740716 140125047432384 submission_runner.py:469] Time since start: 29583.87s, 	Step: 5519, 	{'train/loss': 0.12325286575723693, 'validation/loss': 0.1241297274879848, 'validation/num_examples': 83274637, 'test/loss': 0.126475871347327, 'test/num_examples': 95000000, 'score': 5535.904304981232, 'total_duration': 29583.86993265152, 'accumulated_submission_time': 5535.904304981232, 'accumulated_eval_time': 24006.0195748806, 'accumulated_logging_time': 1.311828374862671}
I0316 23:54:30.771746 140082522593024 logging_writer.py:48] [5519] accumulated_eval_time=24006, accumulated_logging_time=1.31183, accumulated_submission_time=5535.9, global_step=5519, preemption_count=0, score=5535.9, test/loss=0.126476, test/num_examples=95000000, total_duration=29583.9, train/loss=0.123253, validation/loss=0.12413, validation/num_examples=83274637
I0316 23:56:31.354582 140125047432384 spec.py:321] Evaluating on the training split.
I0316 23:58:35.963213 140125047432384 spec.py:333] Evaluating on the validation split.
I0317 00:00:40.356807 140125047432384 spec.py:349] Evaluating on the test split.
I0317 00:03:01.565723 140125047432384 submission_runner.py:469] Time since start: 30094.69s, 	Step: 5640, 	{'train/loss': 0.1248612920755948, 'validation/loss': 0.12407719954156675, 'validation/num_examples': 83274637, 'test/loss': 0.126332503791006, 'test/num_examples': 95000000, 'score': 5655.649990081787, 'total_duration': 30094.69480919838, 'accumulated_submission_time': 5655.649990081787, 'accumulated_eval_time': 24396.230626106262, 'accumulated_logging_time': 1.3576281070709229}
I0317 00:03:01.578739 140082530985728 logging_writer.py:48] [5640] accumulated_eval_time=24396.2, accumulated_logging_time=1.35763, accumulated_submission_time=5655.65, global_step=5640, preemption_count=0, score=5655.65, test/loss=0.126333, test/num_examples=95000000, total_duration=30094.7, train/loss=0.124861, validation/loss=0.124077, validation/num_examples=83274637
I0317 00:05:02.386319 140125047432384 spec.py:321] Evaluating on the training split.
I0317 00:07:06.605887 140125047432384 spec.py:333] Evaluating on the validation split.
I0317 00:09:10.427528 140125047432384 spec.py:349] Evaluating on the test split.
I0317 00:11:33.421042 140125047432384 submission_runner.py:469] Time since start: 30606.55s, 	Step: 5761, 	{'train/loss': 0.12331755966950884, 'validation/loss': 0.12420114707649188, 'validation/num_examples': 83274637, 'test/loss': 0.1265980900042082, 'test/num_examples': 95000000, 'score': 5775.591628074646, 'total_duration': 30606.550399065018, 'accumulated_submission_time': 5775.591628074646, 'accumulated_eval_time': 24787.26549100876, 'accumulated_logging_time': 1.378502368927002}
I0317 00:11:33.433357 140082522593024 logging_writer.py:48] [5761] accumulated_eval_time=24787.3, accumulated_logging_time=1.3785, accumulated_submission_time=5775.59, global_step=5761, preemption_count=0, score=5775.59, test/loss=0.126598, test/num_examples=95000000, total_duration=30606.6, train/loss=0.123318, validation/loss=0.124201, validation/num_examples=83274637
I0317 00:13:34.345130 140125047432384 spec.py:321] Evaluating on the training split.
I0317 00:15:39.451661 140125047432384 spec.py:333] Evaluating on the validation split.
I0317 00:17:44.057175 140125047432384 spec.py:349] Evaluating on the test split.
I0317 00:20:06.075078 140125047432384 submission_runner.py:469] Time since start: 31119.20s, 	Step: 5881, 	{'train/loss': 0.12176472691771617, 'validation/loss': 0.12422807384333344, 'validation/num_examples': 83274637, 'test/loss': 0.12666772808026766, 'test/num_examples': 95000000, 'score': 5895.636150598526, 'total_duration': 31119.20441532135, 'accumulated_submission_time': 5895.636150598526, 'accumulated_eval_time': 25178.99560689926, 'accumulated_logging_time': 1.3982841968536377}
I0317 00:20:06.087474 140082530985728 logging_writer.py:48] [5881] accumulated_eval_time=25179, accumulated_logging_time=1.39828, accumulated_submission_time=5895.64, global_step=5881, preemption_count=0, score=5895.64, test/loss=0.126668, test/num_examples=95000000, total_duration=31119.2, train/loss=0.121765, validation/loss=0.124228, validation/num_examples=83274637
I0317 00:22:07.329070 140125047432384 spec.py:321] Evaluating on the training split.
I0317 00:24:11.703994 140125047432384 spec.py:333] Evaluating on the validation split.
I0317 00:26:15.721786 140125047432384 spec.py:349] Evaluating on the test split.
I0317 00:28:38.875598 140125047432384 submission_runner.py:469] Time since start: 31632.00s, 	Step: 6000, 	{'train/loss': 0.12259313634410977, 'validation/loss': 0.12407115590988162, 'validation/num_examples': 83274637, 'test/loss': 0.12637859495873702, 'test/num_examples': 95000000, 'score': 6016.020977973938, 'total_duration': 31632.004937171936, 'accumulated_submission_time': 6016.020977973938, 'accumulated_eval_time': 25570.542217731476, 'accumulated_logging_time': 1.4183399677276611}
I0317 00:28:38.887593 140082522593024 logging_writer.py:48] [6000] accumulated_eval_time=25570.5, accumulated_logging_time=1.41834, accumulated_submission_time=6016.02, global_step=6000, preemption_count=0, score=6016.02, test/loss=0.126379, test/num_examples=95000000, total_duration=31632, train/loss=0.122593, validation/loss=0.124071, validation/num_examples=83274637
I0317 00:28:39.506807 140082530985728 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.00533937, loss=0.116509
I0317 00:28:39.510431 140125047432384 submission.py:265] 6000) loss = 0.117, grad_norm = 0.005
I0317 00:30:39.774590 140125047432384 spec.py:321] Evaluating on the training split.
I0317 00:32:44.960226 140125047432384 spec.py:333] Evaluating on the validation split.
I0317 00:34:48.939395 140125047432384 spec.py:349] Evaluating on the test split.
I0317 00:37:12.555266 140125047432384 submission_runner.py:469] Time since start: 32145.68s, 	Step: 6121, 	{'train/loss': 0.12173886067761112, 'validation/loss': 0.12413500560638548, 'validation/num_examples': 83274637, 'test/loss': 0.12645006213651958, 'test/num_examples': 95000000, 'score': 6136.026384353638, 'total_duration': 32145.68463897705, 'accumulated_submission_time': 6136.026384353638, 'accumulated_eval_time': 25963.323083162308, 'accumulated_logging_time': 1.4368350505828857}
I0317 00:37:12.566908 140082522593024 logging_writer.py:48] [6121] accumulated_eval_time=25963.3, accumulated_logging_time=1.43684, accumulated_submission_time=6136.03, global_step=6121, preemption_count=0, score=6136.03, test/loss=0.12645, test/num_examples=95000000, total_duration=32145.7, train/loss=0.121739, validation/loss=0.124135, validation/num_examples=83274637
I0317 00:39:14.667231 140125047432384 spec.py:321] Evaluating on the training split.
I0317 00:41:18.554920 140125047432384 spec.py:333] Evaluating on the validation split.
I0317 00:43:22.761045 140125047432384 spec.py:349] Evaluating on the test split.
I0317 00:45:45.800680 140125047432384 submission_runner.py:469] Time since start: 32658.93s, 	Step: 6238, 	{'train/loss': 0.1225450244361541, 'validation/loss': 0.12408335139431446, 'validation/num_examples': 83274637, 'test/loss': 0.1264337816661634, 'test/num_examples': 95000000, 'score': 6257.196619272232, 'total_duration': 32658.93003511429, 'accumulated_submission_time': 6257.196619272232, 'accumulated_eval_time': 26354.456582069397, 'accumulated_logging_time': 1.4893195629119873}
I0317 00:45:45.812958 140082530985728 logging_writer.py:48] [6238] accumulated_eval_time=26354.5, accumulated_logging_time=1.48932, accumulated_submission_time=6257.2, global_step=6238, preemption_count=0, score=6257.2, test/loss=0.126434, test/num_examples=95000000, total_duration=32658.9, train/loss=0.122545, validation/loss=0.124083, validation/num_examples=83274637
I0317 00:47:46.728713 140125047432384 spec.py:321] Evaluating on the training split.
I0317 00:49:51.170988 140125047432384 spec.py:333] Evaluating on the validation split.
I0317 00:51:55.578138 140125047432384 spec.py:349] Evaluating on the test split.
I0317 00:54:16.643296 140125047432384 submission_runner.py:469] Time since start: 33169.77s, 	Step: 6359, 	{'train/loss': 0.12166632154307667, 'validation/loss': 0.12394725760526507, 'validation/num_examples': 83274637, 'test/loss': 0.12627319063744796, 'test/num_examples': 95000000, 'score': 6377.239097118378, 'total_duration': 33169.77266407013, 'accumulated_submission_time': 6377.239097118378, 'accumulated_eval_time': 26744.37144446373, 'accumulated_logging_time': 1.5083825588226318}
I0317 00:54:16.656170 140082522593024 logging_writer.py:48] [6359] accumulated_eval_time=26744.4, accumulated_logging_time=1.50838, accumulated_submission_time=6377.24, global_step=6359, preemption_count=0, score=6377.24, test/loss=0.126273, test/num_examples=95000000, total_duration=33169.8, train/loss=0.121666, validation/loss=0.123947, validation/num_examples=83274637
I0317 00:56:17.987705 140125047432384 spec.py:321] Evaluating on the training split.
I0317 00:58:22.206813 140125047432384 spec.py:333] Evaluating on the validation split.
I0317 01:00:26.528043 140125047432384 spec.py:349] Evaluating on the test split.
I0317 01:02:48.302430 140125047432384 submission_runner.py:469] Time since start: 33681.43s, 	Step: 6481, 	{'train/loss': 0.1236599762155395, 'validation/loss': 0.12406744893876209, 'validation/num_examples': 83274637, 'test/loss': 0.1263630222730938, 'test/num_examples': 95000000, 'score': 6497.68567943573, 'total_duration': 33681.43177866936, 'accumulated_submission_time': 6497.68567943573, 'accumulated_eval_time': 27134.686346530914, 'accumulated_logging_time': 1.5289485454559326}
I0317 01:02:48.314738 140082530985728 logging_writer.py:48] [6481] accumulated_eval_time=27134.7, accumulated_logging_time=1.52895, accumulated_submission_time=6497.69, global_step=6481, preemption_count=0, score=6497.69, test/loss=0.126363, test/num_examples=95000000, total_duration=33681.4, train/loss=0.12366, validation/loss=0.124067, validation/num_examples=83274637
I0317 01:02:52.638010 140082522593024 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.00929371, loss=0.126912
I0317 01:02:52.641714 140125047432384 submission.py:265] 6500) loss = 0.127, grad_norm = 0.009
I0317 01:04:49.632516 140125047432384 spec.py:321] Evaluating on the training split.
I0317 01:06:53.850160 140125047432384 spec.py:333] Evaluating on the validation split.
I0317 01:08:58.184557 140125047432384 spec.py:349] Evaluating on the test split.
I0317 01:11:20.742164 140125047432384 submission_runner.py:469] Time since start: 34193.87s, 	Step: 6603, 	{'train/loss': 0.12141429111806736, 'validation/loss': 0.1240808910040362, 'validation/num_examples': 83274637, 'test/loss': 0.12647164308427508, 'test/num_examples': 95000000, 'score': 6618.16734623909, 'total_duration': 34193.87152814865, 'accumulated_submission_time': 6618.16734623909, 'accumulated_eval_time': 27525.796075344086, 'accumulated_logging_time': 1.548236608505249}
I0317 01:11:20.754941 140082530985728 logging_writer.py:48] [6603] accumulated_eval_time=27525.8, accumulated_logging_time=1.54824, accumulated_submission_time=6618.17, global_step=6603, preemption_count=0, score=6618.17, test/loss=0.126472, test/num_examples=95000000, total_duration=34193.9, train/loss=0.121414, validation/loss=0.124081, validation/num_examples=83274637
I0317 01:13:21.770421 140125047432384 spec.py:321] Evaluating on the training split.
I0317 01:15:26.355159 140125047432384 spec.py:333] Evaluating on the validation split.
I0317 01:17:30.496805 140125047432384 spec.py:349] Evaluating on the test split.
I0317 01:19:53.663905 140125047432384 submission_runner.py:469] Time since start: 34706.79s, 	Step: 6722, 	{'train/loss': 0.12161881118297713, 'validation/loss': 0.12405593802659973, 'validation/num_examples': 83274637, 'test/loss': 0.12637740015535856, 'test/num_examples': 95000000, 'score': 6738.3079226017, 'total_duration': 34706.79324173927, 'accumulated_submission_time': 6738.3079226017, 'accumulated_eval_time': 27917.689661502838, 'accumulated_logging_time': 1.5685052871704102}
I0317 01:19:53.676499 140082522593024 logging_writer.py:48] [6722] accumulated_eval_time=27917.7, accumulated_logging_time=1.56851, accumulated_submission_time=6738.31, global_step=6722, preemption_count=0, score=6738.31, test/loss=0.126377, test/num_examples=95000000, total_duration=34706.8, train/loss=0.121619, validation/loss=0.124056, validation/num_examples=83274637
I0317 01:21:54.838414 140125047432384 spec.py:321] Evaluating on the training split.
I0317 01:23:59.253990 140125047432384 spec.py:333] Evaluating on the validation split.
I0317 01:26:02.947810 140125047432384 spec.py:349] Evaluating on the test split.
I0317 01:28:24.477337 140125047432384 submission_runner.py:469] Time since start: 35217.61s, 	Step: 6842, 	{'train/loss': 0.12353540886204141, 'validation/loss': 0.12398070586149186, 'validation/num_examples': 83274637, 'test/loss': 0.12629903492403532, 'test/num_examples': 95000000, 'score': 6858.5896282196045, 'total_duration': 35217.606699705124, 'accumulated_submission_time': 6858.5896282196045, 'accumulated_eval_time': 28307.328708171844, 'accumulated_logging_time': 1.617877721786499}
I0317 01:28:24.490236 140082530985728 logging_writer.py:48] [6842] accumulated_eval_time=28307.3, accumulated_logging_time=1.61788, accumulated_submission_time=6858.59, global_step=6842, preemption_count=0, score=6858.59, test/loss=0.126299, test/num_examples=95000000, total_duration=35217.6, train/loss=0.123535, validation/loss=0.123981, validation/num_examples=83274637
I0317 01:30:25.815064 140125047432384 spec.py:321] Evaluating on the training split.
I0317 01:32:30.496423 140125047432384 spec.py:333] Evaluating on the validation split.
I0317 01:34:35.014095 140125047432384 spec.py:349] Evaluating on the test split.
I0317 01:36:58.190253 140125047432384 submission_runner.py:469] Time since start: 35731.32s, 	Step: 6962, 	{'train/loss': 0.1220518383616609, 'validation/loss': 0.12396360053138962, 'validation/num_examples': 83274637, 'test/loss': 0.1262528602068851, 'test/num_examples': 95000000, 'score': 6979.075412988663, 'total_duration': 35731.3196079731, 'accumulated_submission_time': 6979.075412988663, 'accumulated_eval_time': 28699.703987836838, 'accumulated_logging_time': 1.6376278400421143}
I0317 01:36:58.202677 140082522593024 logging_writer.py:48] [6962] accumulated_eval_time=28699.7, accumulated_logging_time=1.63763, accumulated_submission_time=6979.08, global_step=6962, preemption_count=0, score=6979.08, test/loss=0.126253, test/num_examples=95000000, total_duration=35731.3, train/loss=0.122052, validation/loss=0.123964, validation/num_examples=83274637
I0317 01:37:16.478160 140082530985728 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.0252376, loss=0.126127
I0317 01:37:16.481996 140125047432384 submission.py:265] 7000) loss = 0.126, grad_norm = 0.025
I0317 01:38:58.862925 140125047432384 spec.py:321] Evaluating on the training split.
I0317 01:41:03.703989 140125047432384 spec.py:333] Evaluating on the validation split.
I0317 01:43:08.198649 140125047432384 spec.py:349] Evaluating on the test split.
I0317 01:45:31.211346 140125047432384 submission_runner.py:469] Time since start: 36244.34s, 	Step: 7080, 	{'train/loss': 0.12334785046788438, 'validation/loss': 0.1239673295513493, 'validation/num_examples': 83274637, 'test/loss': 0.12627200335516678, 'test/num_examples': 95000000, 'score': 7098.895557641983, 'total_duration': 36244.340718984604, 'accumulated_submission_time': 7098.895557641983, 'accumulated_eval_time': 29092.052516222, 'accumulated_logging_time': 1.6571931838989258}
I0317 01:45:31.223715 140082522593024 logging_writer.py:48] [7080] accumulated_eval_time=29092.1, accumulated_logging_time=1.65719, accumulated_submission_time=7098.9, global_step=7080, preemption_count=0, score=7098.9, test/loss=0.126272, test/num_examples=95000000, total_duration=36244.3, train/loss=0.123348, validation/loss=0.123967, validation/num_examples=83274637
I0317 01:47:32.196126 140125047432384 spec.py:321] Evaluating on the training split.
I0317 01:49:36.832263 140125047432384 spec.py:333] Evaluating on the validation split.
I0317 01:51:40.896452 140125047432384 spec.py:349] Evaluating on the test split.
I0317 01:54:02.221565 140125047432384 submission_runner.py:469] Time since start: 36755.35s, 	Step: 7199, 	{'train/loss': 0.12219998267285574, 'validation/loss': 0.12394473029338424, 'validation/num_examples': 83274637, 'test/loss': 0.12618354458353143, 'test/num_examples': 95000000, 'score': 7218.929492473602, 'total_duration': 36755.35090994835, 'accumulated_submission_time': 7218.929492473602, 'accumulated_eval_time': 29482.07812690735, 'accumulated_logging_time': 1.724928379058838}
I0317 01:54:02.234167 140082530985728 logging_writer.py:48] [7199] accumulated_eval_time=29482.1, accumulated_logging_time=1.72493, accumulated_submission_time=7218.93, global_step=7199, preemption_count=0, score=7218.93, test/loss=0.126184, test/num_examples=95000000, total_duration=36755.4, train/loss=0.1222, validation/loss=0.123945, validation/num_examples=83274637
I0317 01:56:04.514916 140125047432384 spec.py:321] Evaluating on the training split.
I0317 01:58:08.793374 140125047432384 spec.py:333] Evaluating on the validation split.
I0317 02:00:12.946397 140125047432384 spec.py:349] Evaluating on the test split.
I0317 02:02:35.562548 140125047432384 submission_runner.py:469] Time since start: 37268.69s, 	Step: 7319, 	{'train/loss': 0.12171445850480972, 'validation/loss': 0.12387481774393874, 'validation/num_examples': 83274637, 'test/loss': 0.12620564965053357, 'test/num_examples': 95000000, 'score': 7340.330881357193, 'total_duration': 37268.691870212555, 'accumulated_submission_time': 7340.330881357193, 'accumulated_eval_time': 29873.125950098038, 'accumulated_logging_time': 1.7447876930236816}
I0317 02:02:35.574894 140082522593024 logging_writer.py:48] [7319] accumulated_eval_time=29873.1, accumulated_logging_time=1.74479, accumulated_submission_time=7340.33, global_step=7319, preemption_count=0, score=7340.33, test/loss=0.126206, test/num_examples=95000000, total_duration=37268.7, train/loss=0.121714, validation/loss=0.123875, validation/num_examples=83274637
I0317 02:04:37.453382 140125047432384 spec.py:321] Evaluating on the training split.
I0317 02:06:41.878891 140125047432384 spec.py:333] Evaluating on the validation split.
I0317 02:08:44.958325 140125047432384 spec.py:349] Evaluating on the test split.
I0317 02:11:06.779884 140125047432384 submission_runner.py:469] Time since start: 37779.91s, 	Step: 7437, 	{'train/loss': 0.12288534987488654, 'validation/loss': 0.12406801430675478, 'validation/num_examples': 83274637, 'test/loss': 0.12640646192879929, 'test/num_examples': 95000000, 'score': 7461.327194690704, 'total_duration': 37779.90923547745, 'accumulated_submission_time': 7461.327194690704, 'accumulated_eval_time': 30262.452760219574, 'accumulated_logging_time': 1.7643799781799316}
I0317 02:11:06.792057 140082530985728 logging_writer.py:48] [7437] accumulated_eval_time=30262.5, accumulated_logging_time=1.76438, accumulated_submission_time=7461.33, global_step=7437, preemption_count=0, score=7461.33, test/loss=0.126406, test/num_examples=95000000, total_duration=37779.9, train/loss=0.122885, validation/loss=0.124068, validation/num_examples=83274637
I0317 02:11:56.835952 140082522593024 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.0104229, loss=0.12865
I0317 02:11:56.839515 140125047432384 submission.py:265] 7500) loss = 0.129, grad_norm = 0.010
I0317 02:13:07.996648 140125047432384 spec.py:321] Evaluating on the training split.
I0317 02:15:12.743407 140125047432384 spec.py:333] Evaluating on the validation split.
I0317 02:17:16.177411 140125047432384 spec.py:349] Evaluating on the test split.
I0317 02:19:38.906712 140125047432384 submission_runner.py:469] Time since start: 38292.02s, 	Step: 7555, 	{'train/loss': 0.12036232251866137, 'validation/loss': 0.12390697497757529, 'validation/num_examples': 83274637, 'test/loss': 0.12621231034228675, 'test/num_examples': 95000000, 'score': 7581.657673120499, 'total_duration': 38292.024292469025, 'accumulated_submission_time': 7581.657673120499, 'accumulated_eval_time': 30653.3511428833, 'accumulated_logging_time': 1.783919095993042}
I0317 02:19:38.918905 140082530985728 logging_writer.py:48] [7555] accumulated_eval_time=30653.4, accumulated_logging_time=1.78392, accumulated_submission_time=7581.66, global_step=7555, preemption_count=0, score=7581.66, test/loss=0.126212, test/num_examples=95000000, total_duration=38292, train/loss=0.120362, validation/loss=0.123907, validation/num_examples=83274637
I0317 02:21:40.117764 140125047432384 spec.py:321] Evaluating on the training split.
I0317 02:23:44.665938 140125047432384 spec.py:333] Evaluating on the validation split.
I0317 02:25:49.108274 140125047432384 spec.py:349] Evaluating on the test split.
I0317 02:28:12.284326 140125047432384 submission_runner.py:469] Time since start: 38805.41s, 	Step: 7673, 	{'train/loss': 0.12212368606732832, 'validation/loss': 0.12388197740955759, 'validation/num_examples': 83274637, 'test/loss': 0.12614466253079867, 'test/num_examples': 95000000, 'score': 7701.98690867424, 'total_duration': 38805.413675546646, 'accumulated_submission_time': 7701.98690867424, 'accumulated_eval_time': 31045.517868995667, 'accumulated_logging_time': 1.8035485744476318}
I0317 02:28:12.297501 140082522593024 logging_writer.py:48] [7673] accumulated_eval_time=31045.5, accumulated_logging_time=1.80355, accumulated_submission_time=7701.99, global_step=7673, preemption_count=0, score=7701.99, test/loss=0.126145, test/num_examples=95000000, total_duration=38805.4, train/loss=0.122124, validation/loss=0.123882, validation/num_examples=83274637
I0317 02:30:12.580209 140082530985728 logging_writer.py:48] [7793] global_step=7793, preemption_count=0, score=7821.72
I0317 02:30:21.994336 140125047432384 submission_runner.py:646] Tuning trial 4/5
I0317 02:30:21.994563 140125047432384 submission_runner.py:647] Hyperparameters: Hyperparameters(learning_rate=0.0012, one_minus_beta1=0.016610699316537858, one_minus_beta2=0.005888216674053163, epsilon=1e-08, one_minus_momentum=0.5, use_momentum=True, weight_decay=0.00040349948255455174, max_preconditioner_dim=1024, precondition_frequency=100, start_preconditioning_step=-1, inv_root_override=2, exponent_multiplier=1.0, grafting_type='ADAM', grafting_epsilon=1e-08, use_normalized_grafting=False, communication_dtype='FP32', communicate_params=True, use_cosine_decay=True, warmup_factor=0.02, label_smoothing=0.1, dropout_rate=0.1, use_nadam=True, step_hint_factor=1.0)
I0317 02:30:21.995557 140125047432384 submission_runner.py:648] Metrics: {'eval_results': [(1, {'train/loss': 1.2065096837157883, 'validation/loss': 1.2110276654353231, 'validation/num_examples': 83274637, 'test/loss': 1.2072974342477898, 'test/num_examples': 95000000, 'score': 4.9714789390563965, 'total_duration': 1008.0382068157196, 'accumulated_submission_time': 4.9714789390563965, 'accumulated_eval_time': 1002.6973347663879, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (121, {'train/loss': 0.1398335201562238, 'validation/loss': 0.13855227911752868, 'validation/num_examples': 83274637, 'test/loss': 0.14231451538041767, 'test/num_examples': 95000000, 'score': 125.14563584327698, 'total_duration': 2087.511488199234, 'accumulated_submission_time': 125.14563584327698, 'accumulated_eval_time': 1961.0748245716095, 'accumulated_logging_time': 0.015397787094116211, 'global_step': 121, 'preemption_count': 0}), (240, {'train/loss': 0.12788620280307472, 'validation/loss': 0.12861760013689474, 'validation/num_examples': 83274637, 'test/loss': 0.13154781391457004, 'test/num_examples': 95000000, 'score': 245.38731360435486, 'total_duration': 3115.4954023361206, 'accumulated_submission_time': 245.38731360435486, 'accumulated_eval_time': 2867.90834069252, 'accumulated_logging_time': 0.03335976600646973, 'global_step': 240, 'preemption_count': 0}), (359, {'train/loss': 0.12783539908361508, 'validation/loss': 0.12767009365256995, 'validation/num_examples': 83274637, 'test/loss': 0.130446985331445, 'test/num_examples': 95000000, 'score': 365.8275294303894, 'total_duration': 4129.868925333023, 'accumulated_submission_time': 365.8275294303894, 'accumulated_eval_time': 3760.9832651615143, 'accumulated_logging_time': 0.05065274238586426, 'global_step': 359, 'preemption_count': 0}), (478, {'train/loss': 0.1265810259673565, 'validation/loss': 0.12738667729795292, 'validation/num_examples': 83274637, 'test/loss': 0.13017763444390548, 'test/num_examples': 95000000, 'score': 485.9496235847473, 'total_duration': 5108.853622198105, 'accumulated_submission_time': 485.9496235847473, 'accumulated_eval_time': 4618.929227590561, 'accumulated_logging_time': 0.06751251220703125, 'global_step': 478, 'preemption_count': 0}), (593, {'train/loss': 0.1262374924362613, 'validation/loss': 0.12679798651319518, 'validation/num_examples': 83274637, 'test/loss': 0.12979993873700593, 'test/num_examples': 95000000, 'score': 605.7171161174774, 'total_duration': 6093.6043701171875, 'accumulated_submission_time': 605.7171161174774, 'accumulated_eval_time': 5482.923253774643, 'accumulated_logging_time': 0.15216279029846191, 'global_step': 593, 'preemption_count': 0}), (713, {'train/loss': 0.12626206336277107, 'validation/loss': 0.1265042595853662, 'validation/num_examples': 83274637, 'test/loss': 0.12907553347175998, 'test/num_examples': 95000000, 'score': 726.267671585083, 'total_duration': 7077.333902359009, 'accumulated_submission_time': 726.267671585083, 'accumulated_eval_time': 6345.17764043808, 'accumulated_logging_time': 0.16869831085205078, 'global_step': 713, 'preemption_count': 0}), (839, {'train/loss': 0.12584306294757733, 'validation/loss': 0.12634487004401812, 'validation/num_examples': 83274637, 'test/loss': 0.12899712999074836, 'test/num_examples': 95000000, 'score': 846.7271239757538, 'total_duration': 8104.63975071907, 'accumulated_submission_time': 846.7271239757538, 'accumulated_eval_time': 7251.082218885422, 'accumulated_logging_time': 0.18576335906982422, 'global_step': 839, 'preemption_count': 0}), (961, {'train/loss': 0.12651580439648896, 'validation/loss': 0.12609595182342126, 'validation/num_examples': 83274637, 'test/loss': 0.128661724803001, 'test/num_examples': 95000000, 'score': 967.5486996173859, 'total_duration': 9066.050383329391, 'accumulated_submission_time': 967.5486996173859, 'accumulated_eval_time': 8090.783136606216, 'accumulated_logging_time': 0.2034168243408203, 'global_step': 961, 'preemption_count': 0}), (1083, {'train/loss': 0.12385800336856785, 'validation/loss': 0.1260711407536623, 'validation/num_examples': 83274637, 'test/loss': 0.12848827132158783, 'test/num_examples': 95000000, 'score': 1087.4823427200317, 'total_duration': 10012.124031066895, 'accumulated_submission_time': 1087.4823427200317, 'accumulated_eval_time': 8915.989181995392, 'accumulated_logging_time': 0.2786734104156494, 'global_step': 1083, 'preemption_count': 0}), (1207, {'train/loss': 0.12492642857455166, 'validation/loss': 0.12569777573342475, 'validation/num_examples': 83274637, 'test/loss': 0.12820489671510396, 'test/num_examples': 95000000, 'score': 1207.085381269455, 'total_duration': 10879.165850400925, 'accumulated_submission_time': 1207.085381269455, 'accumulated_eval_time': 9662.547525167465, 'accumulated_logging_time': 0.29537510871887207, 'global_step': 1207, 'preemption_count': 0}), (1327, {'train/loss': 0.1235392713216072, 'validation/loss': 0.12595282278939726, 'validation/num_examples': 83274637, 'test/loss': 0.12830045721491765, 'test/num_examples': 95000000, 'score': 1327.4393424987793, 'total_duration': 11584.041192770004, 'accumulated_submission_time': 1327.4393424987793, 'accumulated_eval_time': 10246.171612501144, 'accumulated_logging_time': 0.3119082450866699, 'global_step': 1327, 'preemption_count': 0}), (1448, {'train/loss': 0.12544466136866228, 'validation/loss': 0.12537812173080837, 'validation/num_examples': 83274637, 'test/loss': 0.12780259701240942, 'test/num_examples': 95000000, 'score': 1448.440856218338, 'total_duration': 12128.506041288376, 'accumulated_submission_time': 1448.440856218338, 'accumulated_eval_time': 10668.714637517929, 'accumulated_logging_time': 0.32816600799560547, 'global_step': 1448, 'preemption_count': 0}), (1570, {'train/loss': 0.12359536651792358, 'validation/loss': 0.12548711997275785, 'validation/num_examples': 83274637, 'test/loss': 0.12787936339657432, 'test/num_examples': 95000000, 'score': 1568.474135875702, 'total_duration': 12643.31314921379, 'accumulated_submission_time': 1568.474135875702, 'accumulated_eval_time': 11062.584811925888, 'accumulated_logging_time': 0.34617066383361816, 'global_step': 1570, 'preemption_count': 0}), (1688, {'train/loss': 0.12383909421111233, 'validation/loss': 0.125504837567578, 'validation/num_examples': 83274637, 'test/loss': 0.12783957995814274, 'test/num_examples': 95000000, 'score': 1689.696527004242, 'total_duration': 13157.284777879715, 'accumulated_submission_time': 1689.696527004242, 'accumulated_eval_time': 11454.36686038971, 'accumulated_logging_time': 0.3975396156311035, 'global_step': 1688, 'preemption_count': 0}), (1808, {'train/loss': 0.12356003743151876, 'validation/loss': 0.1251110461609913, 'validation/num_examples': 83274637, 'test/loss': 0.12751403249343068, 'test/num_examples': 95000000, 'score': 1810.2850329875946, 'total_duration': 13672.176440000534, 'accumulated_submission_time': 1810.2850329875946, 'accumulated_eval_time': 11847.77844619751, 'accumulated_logging_time': 0.41586995124816895, 'global_step': 1808, 'preemption_count': 0}), (1929, {'train/loss': 0.12372115941218205, 'validation/loss': 0.1251081976454823, 'validation/num_examples': 83274637, 'test/loss': 0.12755520625634445, 'test/num_examples': 95000000, 'score': 1930.7281482219696, 'total_duration': 14185.170724868774, 'accumulated_submission_time': 1930.7281482219696, 'accumulated_eval_time': 12239.430521726608, 'accumulated_logging_time': 0.4337458610534668, 'global_step': 1929, 'preemption_count': 0}), (2046, {'train/loss': 0.12331670101579208, 'validation/loss': 0.1252169392266097, 'validation/num_examples': 83274637, 'test/loss': 0.127724946681374, 'test/num_examples': 95000000, 'score': 2050.1818478107452, 'total_duration': 14698.08552479744, 'accumulated_submission_time': 2050.1818478107452, 'accumulated_eval_time': 12631.932119846344, 'accumulated_logging_time': 0.5004451274871826, 'global_step': 2046, 'preemption_count': 0}), (2166, {'train/loss': 0.12606750521895893, 'validation/loss': 0.12490935073714989, 'validation/num_examples': 83274637, 'test/loss': 0.12725140277637684, 'test/num_examples': 95000000, 'score': 2170.035364627838, 'total_duration': 15211.363380670547, 'accumulated_submission_time': 2170.035364627838, 'accumulated_eval_time': 13024.441817045212, 'accumulated_logging_time': 0.525820255279541, 'global_step': 2166, 'preemption_count': 0}), (2287, {'train/loss': 0.12430699084547006, 'validation/loss': 0.12505820183940403, 'validation/num_examples': 83274637, 'test/loss': 0.12744297551028602, 'test/num_examples': 95000000, 'score': 2290.472546815872, 'total_duration': 15726.23852801323, 'accumulated_submission_time': 2290.472546815872, 'accumulated_eval_time': 13418.00026679039, 'accumulated_logging_time': 0.5434913635253906, 'global_step': 2287, 'preemption_count': 0}), (2407, {'train/loss': 0.1240767633969145, 'validation/loss': 0.12485706137748302, 'validation/num_examples': 83274637, 'test/loss': 0.12727261800513018, 'test/num_examples': 95000000, 'score': 2411.14200258255, 'total_duration': 16239.680477380753, 'accumulated_submission_time': 2411.14200258255, 'accumulated_eval_time': 13809.897166490555, 'accumulated_logging_time': 0.5613570213317871, 'global_step': 2407, 'preemption_count': 0}), (2528, {'train/loss': 0.1244476158688123, 'validation/loss': 0.12498529533130753, 'validation/num_examples': 83274637, 'test/loss': 0.12741465456101267, 'test/num_examples': 95000000, 'score': 2530.9913654327393, 'total_duration': 16753.557775259018, 'accumulated_submission_time': 2530.9913654327393, 'accumulated_eval_time': 14203.01037979126, 'accumulated_logging_time': 0.5801191329956055, 'global_step': 2528, 'preemption_count': 0}), (2645, {'train/loss': 0.12409423905086361, 'validation/loss': 0.12489193480912575, 'validation/num_examples': 83274637, 'test/loss': 0.1272521022617541, 'test/num_examples': 95000000, 'score': 2650.7496197223663, 'total_duration': 17266.89911556244, 'accumulated_submission_time': 2650.7496197223663, 'accumulated_eval_time': 14595.668315887451, 'accumulated_logging_time': 0.6001720428466797, 'global_step': 2645, 'preemption_count': 0}), (2764, {'train/loss': 0.12351750629526581, 'validation/loss': 0.1248726980479582, 'validation/num_examples': 83274637, 'test/loss': 0.12734072171325683, 'test/num_examples': 95000000, 'score': 2770.5040216445923, 'total_duration': 17780.550576925278, 'accumulated_submission_time': 2770.5040216445923, 'accumulated_eval_time': 14988.636237621307, 'accumulated_logging_time': 0.6231412887573242, 'global_step': 2764, 'preemption_count': 0}), (2884, {'train/loss': 0.12423976621791734, 'validation/loss': 0.12496345321230495, 'validation/num_examples': 83274637, 'test/loss': 0.12735412327599777, 'test/num_examples': 95000000, 'score': 2890.734165906906, 'total_duration': 18295.68229651451, 'accumulated_submission_time': 2890.734165906906, 'accumulated_eval_time': 15382.625467538834, 'accumulated_logging_time': 0.6421463489532471, 'global_step': 2884, 'preemption_count': 0}), (3003, {'train/loss': 0.12459507288785768, 'validation/loss': 0.12472201906838395, 'validation/num_examples': 83274637, 'test/loss': 0.12716901240668047, 'test/num_examples': 95000000, 'score': 3010.8467679023743, 'total_duration': 18810.870223760605, 'accumulated_submission_time': 3010.8467679023743, 'accumulated_eval_time': 15776.80332660675, 'accumulated_logging_time': 0.6605775356292725, 'global_step': 3003, 'preemption_count': 0}), (3121, {'train/loss': 0.1235495323890858, 'validation/loss': 0.12468437684359913, 'validation/num_examples': 83274637, 'test/loss': 0.12714822296969766, 'test/num_examples': 95000000, 'score': 3131.1110875606537, 'total_duration': 19326.26750612259, 'accumulated_submission_time': 3131.1110875606537, 'accumulated_eval_time': 16171.059642791748, 'accumulated_logging_time': 0.6791493892669678, 'global_step': 3121, 'preemption_count': 0}), (3242, {'train/loss': 0.12485701749951436, 'validation/loss': 0.12474015643028985, 'validation/num_examples': 83274637, 'test/loss': 0.1271577455314636, 'test/num_examples': 95000000, 'score': 3251.654002904892, 'total_duration': 19841.84696173668, 'accumulated_submission_time': 3251.654002904892, 'accumulated_eval_time': 16565.129110336304, 'accumulated_logging_time': 0.7469582557678223, 'global_step': 3242, 'preemption_count': 0}), (3360, {'train/loss': 0.12456685213488275, 'validation/loss': 0.12458642418710922, 'validation/num_examples': 83274637, 'test/loss': 0.12690310255524484, 'test/num_examples': 95000000, 'score': 3371.528143644333, 'total_duration': 20355.35507273674, 'accumulated_submission_time': 3371.528143644333, 'accumulated_eval_time': 16957.867936611176, 'accumulated_logging_time': 0.7656712532043457, 'global_step': 3360, 'preemption_count': 0}), (3480, {'train/loss': 0.12011627089179072, 'validation/loss': 0.12478746094730993, 'validation/num_examples': 83274637, 'test/loss': 0.12710570001642327, 'test/num_examples': 95000000, 'score': 3492.01784157753, 'total_duration': 20869.85787153244, 'accumulated_submission_time': 3492.01784157753, 'accumulated_eval_time': 17351.011322259903, 'accumulated_logging_time': 0.7851085662841797, 'global_step': 3480, 'preemption_count': 0}), (3598, {'train/loss': 0.1230597756653709, 'validation/loss': 0.12478527678569838, 'validation/num_examples': 83274637, 'test/loss': 0.1271509694898505, 'test/num_examples': 95000000, 'score': 3612.682875394821, 'total_duration': 21382.51104402542, 'accumulated_submission_time': 3612.682875394821, 'accumulated_eval_time': 17742.145010709763, 'accumulated_logging_time': 0.8029608726501465, 'global_step': 3598, 'preemption_count': 0}), (3711, {'train/loss': 0.12492972502053244, 'validation/loss': 0.12477925005057326, 'validation/num_examples': 83274637, 'test/loss': 0.12718889769343325, 'test/num_examples': 95000000, 'score': 3732.3523695468903, 'total_duration': 21894.40137076378, 'accumulated_submission_time': 3732.3523695468903, 'accumulated_eval_time': 18133.40328001976, 'accumulated_logging_time': 0.8749756813049316, 'global_step': 3711, 'preemption_count': 0}), (3829, {'train/loss': 0.12394620809173085, 'validation/loss': 0.12470025538686196, 'validation/num_examples': 83274637, 'test/loss': 0.12697247383989033, 'test/num_examples': 95000000, 'score': 3852.4891498088837, 'total_duration': 22407.929710388184, 'accumulated_submission_time': 3852.4891498088837, 'accumulated_eval_time': 18525.940205812454, 'accumulated_logging_time': 0.8935642242431641, 'global_step': 3829, 'preemption_count': 0}), (3947, {'train/loss': 0.12408631839861824, 'validation/loss': 0.12451185750737204, 'validation/num_examples': 83274637, 'test/loss': 0.12690452366967453, 'test/num_examples': 95000000, 'score': 3972.8986468315125, 'total_duration': 22920.800115823746, 'accumulated_submission_time': 3972.8986468315125, 'accumulated_eval_time': 18917.503640413284, 'accumulated_logging_time': 0.9115588665008545, 'global_step': 3947, 'preemption_count': 0}), (4067, {'train/loss': 0.12267989675411245, 'validation/loss': 0.12447034559370707, 'validation/num_examples': 83274637, 'test/loss': 0.1268672549417596, 'test/num_examples': 95000000, 'score': 4093.4268414974213, 'total_duration': 23434.075429677963, 'accumulated_submission_time': 4093.4268414974213, 'accumulated_eval_time': 19309.380796909332, 'accumulated_logging_time': 0.9294993877410889, 'global_step': 4067, 'preemption_count': 0}), (4186, {'train/loss': 0.12334055098539727, 'validation/loss': 0.12445278047333339, 'validation/num_examples': 83274637, 'test/loss': 0.1267980420651486, 'test/num_examples': 95000000, 'score': 4213.290420770645, 'total_duration': 23946.736095905304, 'accumulated_submission_time': 4213.290420770645, 'accumulated_eval_time': 19701.286665439606, 'accumulated_logging_time': 0.9862127304077148, 'global_step': 4186, 'preemption_count': 0}), (4308, {'train/loss': 0.12420711200601858, 'validation/loss': 0.12442421007969248, 'validation/num_examples': 83274637, 'test/loss': 0.12669958235666376, 'test/num_examples': 95000000, 'score': 4333.509029388428, 'total_duration': 24458.314195394516, 'accumulated_submission_time': 4333.509029388428, 'accumulated_eval_time': 20091.79252767563, 'accumulated_logging_time': 1.004957675933838, 'global_step': 4308, 'preemption_count': 0}), (4427, {'train/loss': 0.12456859660888402, 'validation/loss': 0.1244903592250353, 'validation/num_examples': 83274637, 'test/loss': 0.12702604162734182, 'test/num_examples': 95000000, 'score': 4453.187712430954, 'total_duration': 24970.698207378387, 'accumulated_submission_time': 4453.187712430954, 'accumulated_eval_time': 20483.620792150497, 'accumulated_logging_time': 1.0239145755767822, 'global_step': 4427, 'preemption_count': 0}), (4546, {'train/loss': 0.122877552610927, 'validation/loss': 0.12439467287346427, 'validation/num_examples': 83274637, 'test/loss': 0.12668399165745786, 'test/num_examples': 95000000, 'score': 4572.938852548599, 'total_duration': 25483.453661441803, 'accumulated_submission_time': 4572.938852548599, 'accumulated_eval_time': 20875.75681233406, 'accumulated_logging_time': 1.0431177616119385, 'global_step': 4546, 'preemption_count': 0}), (4667, {'train/loss': 0.124621711375264, 'validation/loss': 0.12438899195453344, 'validation/num_examples': 83274637, 'test/loss': 0.12664475659914518, 'test/num_examples': 95000000, 'score': 4692.992021560669, 'total_duration': 25996.48916053772, 'accumulated_submission_time': 4692.992021560669, 'accumulated_eval_time': 21267.85973882675, 'accumulated_logging_time': 1.06290602684021, 'global_step': 4667, 'preemption_count': 0}), (4788, {'train/loss': 0.12287733971558844, 'validation/loss': 0.12434669346756899, 'validation/num_examples': 83274637, 'test/loss': 0.12671939304147017, 'test/num_examples': 95000000, 'score': 4813.578488111496, 'total_duration': 26509.642488479614, 'accumulated_submission_time': 4813.578488111496, 'accumulated_eval_time': 21659.48313140869, 'accumulated_logging_time': 1.166368007659912, 'global_step': 4788, 'preemption_count': 0}), (4910, {'train/loss': 0.12203624365267168, 'validation/loss': 0.12434088757993943, 'validation/num_examples': 83274637, 'test/loss': 0.1267937087255779, 'test/num_examples': 95000000, 'score': 4934.040390253067, 'total_duration': 27022.9820022583, 'accumulated_submission_time': 4934.040390253067, 'accumulated_eval_time': 22051.442607164383, 'accumulated_logging_time': 1.1932249069213867, 'global_step': 4910, 'preemption_count': 0}), (5032, {'train/loss': 0.1220103436409009, 'validation/loss': 0.12433951502489995, 'validation/num_examples': 83274637, 'test/loss': 0.12675460836434616, 'test/num_examples': 95000000, 'score': 5054.550198793411, 'total_duration': 27534.43678355217, 'accumulated_submission_time': 5054.550198793411, 'accumulated_eval_time': 22441.496423006058, 'accumulated_logging_time': 1.2126541137695312, 'global_step': 5032, 'preemption_count': 0}), (5153, {'train/loss': 0.12297991565825352, 'validation/loss': 0.12440757289068381, 'validation/num_examples': 83274637, 'test/loss': 0.12680271123010736, 'test/num_examples': 95000000, 'score': 5174.315847396851, 'total_duration': 28045.708434820175, 'accumulated_submission_time': 5174.315847396851, 'accumulated_eval_time': 22832.079281568527, 'accumulated_logging_time': 1.2540297508239746, 'global_step': 5153, 'preemption_count': 0}), (5277, {'train/loss': 0.12210302423371013, 'validation/loss': 0.12422237176777717, 'validation/num_examples': 83274637, 'test/loss': 0.12658413014646833, 'test/num_examples': 95000000, 'score': 5294.875488758087, 'total_duration': 28559.5757958889, 'accumulated_submission_time': 5294.875488758087, 'accumulated_eval_time': 23224.52239561081, 'accumulated_logging_time': 1.2741541862487793, 'global_step': 5277, 'preemption_count': 0}), (5399, {'train/loss': 0.1221285331308125, 'validation/loss': 0.12421721825820259, 'validation/num_examples': 83274637, 'test/loss': 0.12654207792776007, 'test/num_examples': 95000000, 'score': 5414.821226358414, 'total_duration': 29068.98725795746, 'accumulated_submission_time': 5414.821226358414, 'accumulated_eval_time': 23613.139500141144, 'accumulated_logging_time': 1.2927000522613525, 'global_step': 5399, 'preemption_count': 0}), (5519, {'train/loss': 0.12325286575723693, 'validation/loss': 0.1241297274879848, 'validation/num_examples': 83274637, 'test/loss': 0.126475871347327, 'test/num_examples': 95000000, 'score': 5535.904304981232, 'total_duration': 29583.86993265152, 'accumulated_submission_time': 5535.904304981232, 'accumulated_eval_time': 24006.0195748806, 'accumulated_logging_time': 1.311828374862671, 'global_step': 5519, 'preemption_count': 0}), (5640, {'train/loss': 0.1248612920755948, 'validation/loss': 0.12407719954156675, 'validation/num_examples': 83274637, 'test/loss': 0.126332503791006, 'test/num_examples': 95000000, 'score': 5655.649990081787, 'total_duration': 30094.69480919838, 'accumulated_submission_time': 5655.649990081787, 'accumulated_eval_time': 24396.230626106262, 'accumulated_logging_time': 1.3576281070709229, 'global_step': 5640, 'preemption_count': 0}), (5761, {'train/loss': 0.12331755966950884, 'validation/loss': 0.12420114707649188, 'validation/num_examples': 83274637, 'test/loss': 0.1265980900042082, 'test/num_examples': 95000000, 'score': 5775.591628074646, 'total_duration': 30606.550399065018, 'accumulated_submission_time': 5775.591628074646, 'accumulated_eval_time': 24787.26549100876, 'accumulated_logging_time': 1.378502368927002, 'global_step': 5761, 'preemption_count': 0}), (5881, {'train/loss': 0.12176472691771617, 'validation/loss': 0.12422807384333344, 'validation/num_examples': 83274637, 'test/loss': 0.12666772808026766, 'test/num_examples': 95000000, 'score': 5895.636150598526, 'total_duration': 31119.20441532135, 'accumulated_submission_time': 5895.636150598526, 'accumulated_eval_time': 25178.99560689926, 'accumulated_logging_time': 1.3982841968536377, 'global_step': 5881, 'preemption_count': 0}), (6000, {'train/loss': 0.12259313634410977, 'validation/loss': 0.12407115590988162, 'validation/num_examples': 83274637, 'test/loss': 0.12637859495873702, 'test/num_examples': 95000000, 'score': 6016.020977973938, 'total_duration': 31632.004937171936, 'accumulated_submission_time': 6016.020977973938, 'accumulated_eval_time': 25570.542217731476, 'accumulated_logging_time': 1.4183399677276611, 'global_step': 6000, 'preemption_count': 0}), (6121, {'train/loss': 0.12173886067761112, 'validation/loss': 0.12413500560638548, 'validation/num_examples': 83274637, 'test/loss': 0.12645006213651958, 'test/num_examples': 95000000, 'score': 6136.026384353638, 'total_duration': 32145.68463897705, 'accumulated_submission_time': 6136.026384353638, 'accumulated_eval_time': 25963.323083162308, 'accumulated_logging_time': 1.4368350505828857, 'global_step': 6121, 'preemption_count': 0}), (6238, {'train/loss': 0.1225450244361541, 'validation/loss': 0.12408335139431446, 'validation/num_examples': 83274637, 'test/loss': 0.1264337816661634, 'test/num_examples': 95000000, 'score': 6257.196619272232, 'total_duration': 32658.93003511429, 'accumulated_submission_time': 6257.196619272232, 'accumulated_eval_time': 26354.456582069397, 'accumulated_logging_time': 1.4893195629119873, 'global_step': 6238, 'preemption_count': 0}), (6359, {'train/loss': 0.12166632154307667, 'validation/loss': 0.12394725760526507, 'validation/num_examples': 83274637, 'test/loss': 0.12627319063744796, 'test/num_examples': 95000000, 'score': 6377.239097118378, 'total_duration': 33169.77266407013, 'accumulated_submission_time': 6377.239097118378, 'accumulated_eval_time': 26744.37144446373, 'accumulated_logging_time': 1.5083825588226318, 'global_step': 6359, 'preemption_count': 0}), (6481, {'train/loss': 0.1236599762155395, 'validation/loss': 0.12406744893876209, 'validation/num_examples': 83274637, 'test/loss': 0.1263630222730938, 'test/num_examples': 95000000, 'score': 6497.68567943573, 'total_duration': 33681.43177866936, 'accumulated_submission_time': 6497.68567943573, 'accumulated_eval_time': 27134.686346530914, 'accumulated_logging_time': 1.5289485454559326, 'global_step': 6481, 'preemption_count': 0}), (6603, {'train/loss': 0.12141429111806736, 'validation/loss': 0.1240808910040362, 'validation/num_examples': 83274637, 'test/loss': 0.12647164308427508, 'test/num_examples': 95000000, 'score': 6618.16734623909, 'total_duration': 34193.87152814865, 'accumulated_submission_time': 6618.16734623909, 'accumulated_eval_time': 27525.796075344086, 'accumulated_logging_time': 1.548236608505249, 'global_step': 6603, 'preemption_count': 0}), (6722, {'train/loss': 0.12161881118297713, 'validation/loss': 0.12405593802659973, 'validation/num_examples': 83274637, 'test/loss': 0.12637740015535856, 'test/num_examples': 95000000, 'score': 6738.3079226017, 'total_duration': 34706.79324173927, 'accumulated_submission_time': 6738.3079226017, 'accumulated_eval_time': 27917.689661502838, 'accumulated_logging_time': 1.5685052871704102, 'global_step': 6722, 'preemption_count': 0}), (6842, {'train/loss': 0.12353540886204141, 'validation/loss': 0.12398070586149186, 'validation/num_examples': 83274637, 'test/loss': 0.12629903492403532, 'test/num_examples': 95000000, 'score': 6858.5896282196045, 'total_duration': 35217.606699705124, 'accumulated_submission_time': 6858.5896282196045, 'accumulated_eval_time': 28307.328708171844, 'accumulated_logging_time': 1.617877721786499, 'global_step': 6842, 'preemption_count': 0}), (6962, {'train/loss': 0.1220518383616609, 'validation/loss': 0.12396360053138962, 'validation/num_examples': 83274637, 'test/loss': 0.1262528602068851, 'test/num_examples': 95000000, 'score': 6979.075412988663, 'total_duration': 35731.3196079731, 'accumulated_submission_time': 6979.075412988663, 'accumulated_eval_time': 28699.703987836838, 'accumulated_logging_time': 1.6376278400421143, 'global_step': 6962, 'preemption_count': 0}), (7080, {'train/loss': 0.12334785046788438, 'validation/loss': 0.1239673295513493, 'validation/num_examples': 83274637, 'test/loss': 0.12627200335516678, 'test/num_examples': 95000000, 'score': 7098.895557641983, 'total_duration': 36244.340718984604, 'accumulated_submission_time': 7098.895557641983, 'accumulated_eval_time': 29092.052516222, 'accumulated_logging_time': 1.6571931838989258, 'global_step': 7080, 'preemption_count': 0}), (7199, {'train/loss': 0.12219998267285574, 'validation/loss': 0.12394473029338424, 'validation/num_examples': 83274637, 'test/loss': 0.12618354458353143, 'test/num_examples': 95000000, 'score': 7218.929492473602, 'total_duration': 36755.35090994835, 'accumulated_submission_time': 7218.929492473602, 'accumulated_eval_time': 29482.07812690735, 'accumulated_logging_time': 1.724928379058838, 'global_step': 7199, 'preemption_count': 0}), (7319, {'train/loss': 0.12171445850480972, 'validation/loss': 0.12387481774393874, 'validation/num_examples': 83274637, 'test/loss': 0.12620564965053357, 'test/num_examples': 95000000, 'score': 7340.330881357193, 'total_duration': 37268.691870212555, 'accumulated_submission_time': 7340.330881357193, 'accumulated_eval_time': 29873.125950098038, 'accumulated_logging_time': 1.7447876930236816, 'global_step': 7319, 'preemption_count': 0}), (7437, {'train/loss': 0.12288534987488654, 'validation/loss': 0.12406801430675478, 'validation/num_examples': 83274637, 'test/loss': 0.12640646192879929, 'test/num_examples': 95000000, 'score': 7461.327194690704, 'total_duration': 37779.90923547745, 'accumulated_submission_time': 7461.327194690704, 'accumulated_eval_time': 30262.452760219574, 'accumulated_logging_time': 1.7643799781799316, 'global_step': 7437, 'preemption_count': 0}), (7555, {'train/loss': 0.12036232251866137, 'validation/loss': 0.12390697497757529, 'validation/num_examples': 83274637, 'test/loss': 0.12621231034228675, 'test/num_examples': 95000000, 'score': 7581.657673120499, 'total_duration': 38292.024292469025, 'accumulated_submission_time': 7581.657673120499, 'accumulated_eval_time': 30653.3511428833, 'accumulated_logging_time': 1.783919095993042, 'global_step': 7555, 'preemption_count': 0}), (7673, {'train/loss': 0.12212368606732832, 'validation/loss': 0.12388197740955759, 'validation/num_examples': 83274637, 'test/loss': 0.12614466253079867, 'test/num_examples': 95000000, 'score': 7701.98690867424, 'total_duration': 38805.413675546646, 'accumulated_submission_time': 7701.98690867424, 'accumulated_eval_time': 31045.517868995667, 'accumulated_logging_time': 1.8035485744476318, 'global_step': 7673, 'preemption_count': 0})], 'global_step': 7793}
I0317 02:30:21.995667 140125047432384 submission_runner.py:649] Timing: 7821.718091726303
I0317 02:30:21.995705 140125047432384 submission_runner.py:651] Total number of evals: 65
I0317 02:30:21.995734 140125047432384 submission_runner.py:652] ====================
I0317 02:30:21.995864 140125047432384 submission_runner.py:750] Final criteo1tb score: 3
