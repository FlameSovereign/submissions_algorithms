torchrun --redirects 1:0,2:0,3:0,4:0,5:0,6:0,7:0 --standalone --nnodes=1 --nproc_per_node=8 submission_runner.py --framework=pytorch --workload=criteo1tb --submission_path=submissions_algorithms/leaderboard/external_tuning/shampoo_submission/submission.py --data_dir=/data/criteo1tb --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=submissions/rolling_leaderboard/external_tuning/shampoo/study_2 --overwrite=True --save_checkpoints=False --rng_seed=-164261234 --torch_compile=true --tuning_ruleset=external --tuning_search_space=submissions_algorithms/leaderboard/external_tuning/shampoo_submission/tuning_search_space.json --num_tuning_trials=5 --hparam_start_index=0 --hparam_end_index=1 2>&1 | tee -a /logs/criteo1tb_pytorch_03-16-2025-14-43-06.log
W0316 14:43:07.885000 9 site-packages/torch/distributed/run.py:793] 
W0316 14:43:07.885000 9 site-packages/torch/distributed/run.py:793] *****************************************
W0316 14:43:07.885000 9 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0316 14:43:07.885000 9 site-packages/torch/distributed/run.py:793] *****************************************
2025-03-16 14:43:09.084135: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-03-16 14:43:09.084152: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-03-16 14:43:09.084135: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-03-16 14:43:09.084135: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-03-16 14:43:09.084135: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-03-16 14:43:09.084139: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-03-16 14:43:09.084135: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-03-16 14:43:09.084185: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1742136189.106372      46 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1742136189.106382      44 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1742136189.106371      48 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1742136189.106371      47 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1742136189.106383      50 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1742136189.106371      45 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1742136189.106372      49 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1742136189.106373      51 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1742136189.113127      46 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
E0000 00:00:1742136189.113127      49 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
E0000 00:00:1742136189.113127      51 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
E0000 00:00:1742136189.113127      47 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
E0000 00:00:1742136189.113128      50 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
E0000 00:00:1742136189.113128      44 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
E0000 00:00:1742136189.113128      45 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
E0000 00:00:1742136189.113141      48 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
[rank4]:[W316 14:43:15.441897890 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 4]  using GPU 4 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank1]:[W316 14:43:15.486664925 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank2]:[W316 14:43:15.577080968 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank5]:[W316 14:43:15.580922699 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 5]  using GPU 5 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank0]:[W316 14:43:16.707457717 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank6]:[W316 14:43:16.774815626 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 6]  using GPU 6 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank3]:[W316 14:43:16.775626908 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank7]:[W316 14:43:16.779724547 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 7]  using GPU 7 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
I0316 14:43:17.785888 140461126632640 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_2/criteo1tb_pytorch.
I0316 14:43:17.785888 139906580186304 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_2/criteo1tb_pytorch.
I0316 14:43:17.785888 140472875287744 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_2/criteo1tb_pytorch.
I0316 14:43:17.785887 139748225434816 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_2/criteo1tb_pytorch.
I0316 14:43:17.785889 140249194808512 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_2/criteo1tb_pytorch.
I0316 14:43:17.785892 139780749235392 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_2/criteo1tb_pytorch.
I0316 14:43:17.785963 140370416096448 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_2/criteo1tb_pytorch.
I0316 14:43:17.786052 139986877789376 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_2/criteo1tb_pytorch.
I0316 14:43:18.914022 139748225434816 logger_utils.py:97] Saving hparams to /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_2/criteo1tb_pytorch/trial_1/hparams.json.
I0316 14:43:18.914016 139906580186304 logger_utils.py:97] Saving hparams to /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_2/criteo1tb_pytorch/trial_1/hparams.json.
I0316 14:43:18.914026 140249194808512 logger_utils.py:97] Saving hparams to /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_2/criteo1tb_pytorch/trial_1/hparams.json.
I0316 14:43:18.914037 140461126632640 logger_utils.py:97] Saving hparams to /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_2/criteo1tb_pytorch/trial_1/hparams.json.
I0316 14:43:18.914046 139780749235392 logger_utils.py:97] Saving hparams to /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_2/criteo1tb_pytorch/trial_1/hparams.json.
I0316 14:43:18.914204 140472875287744 logger_utils.py:97] Saving hparams to /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_2/criteo1tb_pytorch/trial_1/hparams.json.
I0316 14:43:19.066057 140370416096448 logger_utils.py:97] Saving hparams to /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_2/criteo1tb_pytorch/trial_1/hparams.json.
I0316 14:43:19.067698 139986877789376 submission_runner.py:606] Using RNG seed -164261234
I0316 14:43:19.069043 139986877789376 submission_runner.py:615] --- Tuning run 1/5 ---
I0316 14:43:19.069193 139986877789376 submission_runner.py:620] Creating tuning directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_2/criteo1tb_pytorch/trial_1.
I0316 14:43:19.069431 139986877789376 logger_utils.py:97] Saving hparams to /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_2/criteo1tb_pytorch/trial_1/hparams.json.
I0316 14:43:19.493851 139986877789376 submission_runner.py:218] Initializing dataset.
I0316 14:43:19.494077 139986877789376 submission_runner.py:229] Initializing model.
W0316 14:43:25.259836 139986877789376 submission_runner.py:254] These workloads cannot be fully compiled under current PyTorch version. Proceeding without `torch.compile`.
I0316 14:43:25.260035 139986877789376 submission_runner.py:272] Initializing optimizer.
W0316 14:43:25.261291 139986877789376 distributed_shampoo.py:341] start_preconditioning_step set to -1. Setting start_preconditioning_step equal to precondition frequency 100 by default.
W0316 14:43:25.261412 139986877789376 distributed_shampoo.py:359] Nesterov flag is enabled but momentum parameter is zero! Continuing without using momentum or Nesterov acceleration...
W0316 14:43:25.261496 140461126632640 submission_runner.py:254] These workloads cannot be fully compiled under current PyTorch version. Proceeding without `torch.compile`.
W0316 14:43:25.261595 139906580186304 submission_runner.py:254] These workloads cannot be fully compiled under current PyTorch version. Proceeding without `torch.compile`.
W0316 14:43:25.261656 140472875287744 submission_runner.py:254] These workloads cannot be fully compiled under current PyTorch version. Proceeding without `torch.compile`.
W0316 14:43:25.261805 139780749235392 submission_runner.py:254] These workloads cannot be fully compiled under current PyTorch version. Proceeding without `torch.compile`.
W0316 14:43:25.261805 140370416096448 submission_runner.py:254] These workloads cannot be fully compiled under current PyTorch version. Proceeding without `torch.compile`.
W0316 14:43:25.261899 139748225434816 submission_runner.py:254] These workloads cannot be fully compiled under current PyTorch version. Proceeding without `torch.compile`.
W0316 14:43:25.261998 140249194808512 submission_runner.py:254] These workloads cannot be fully compiled under current PyTorch version. Proceeding without `torch.compile`.
W0316 14:43:25.262558 140461126632640 distributed_shampoo.py:341] start_preconditioning_step set to -1. Setting start_preconditioning_step equal to precondition frequency 100 by default.
W0316 14:43:25.262668 140461126632640 distributed_shampoo.py:359] Nesterov flag is enabled but momentum parameter is zero! Continuing without using momentum or Nesterov acceleration...
W0316 14:43:25.262644 139906580186304 distributed_shampoo.py:341] start_preconditioning_step set to -1. Setting start_preconditioning_step equal to precondition frequency 100 by default.
W0316 14:43:25.262744 139906580186304 distributed_shampoo.py:359] Nesterov flag is enabled but momentum parameter is zero! Continuing without using momentum or Nesterov acceleration...
W0316 14:43:25.262693 140472875287744 distributed_shampoo.py:341] start_preconditioning_step set to -1. Setting start_preconditioning_step equal to precondition frequency 100 by default.
W0316 14:43:25.262796 140472875287744 distributed_shampoo.py:359] Nesterov flag is enabled but momentum parameter is zero! Continuing without using momentum or Nesterov acceleration...
W0316 14:43:25.262847 139780749235392 distributed_shampoo.py:341] start_preconditioning_step set to -1. Setting start_preconditioning_step equal to precondition frequency 100 by default.
W0316 14:43:25.262963 139780749235392 distributed_shampoo.py:359] Nesterov flag is enabled but momentum parameter is zero! Continuing without using momentum or Nesterov acceleration...
W0316 14:43:25.262949 139748225434816 distributed_shampoo.py:341] start_preconditioning_step set to -1. Setting start_preconditioning_step equal to precondition frequency 100 by default.
W0316 14:43:25.263060 139748225434816 distributed_shampoo.py:359] Nesterov flag is enabled but momentum parameter is zero! Continuing without using momentum or Nesterov acceleration...
W0316 14:43:25.263021 140249194808512 distributed_shampoo.py:341] start_preconditioning_step set to -1. Setting start_preconditioning_step equal to precondition frequency 100 by default.
W0316 14:43:25.263054 140370416096448 distributed_shampoo.py:341] start_preconditioning_step set to -1. Setting start_preconditioning_step equal to precondition frequency 100 by default.
W0316 14:43:25.263123 140249194808512 distributed_shampoo.py:359] Nesterov flag is enabled but momentum parameter is zero! Continuing without using momentum or Nesterov acceleration...
W0316 14:43:25.263197 140370416096448 distributed_shampoo.py:359] Nesterov flag is enabled but momentum parameter is zero! Continuing without using momentum or Nesterov acceleration...
I0316 14:43:25.263846 139986877789376 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 0.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 0 (torch.Size([512, 13])), Block block_0 (torch.Size([512, 13])).
I0316 14:43:25.264023 139986877789376 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 1.block_0 ([torch.Size([0])]) for Parameter 1 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 14:43:25.264192 139986877789376 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 2.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 2 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 14:43:25.264305 139986877789376 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 3.block_0 ([torch.Size([0])]) for Parameter 3 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 14:43:25.264432 139986877789376 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 4.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 4 (torch.Size([128, 256])), Block block_0 (torch.Size([128, 256])).
I0316 14:43:25.264537 139986877789376 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 5.block_0 ([torch.Size([0])]) for Parameter 5 (torch.Size([128])), Block block_0 (torch.Size([128])).
I0316 14:43:25.264665 139986877789376 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 6.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 6 (torch.Size([1024, 506])), Block block_0 (torch.Size([1024, 506])).
I0316 14:43:25.264771 139986877789376 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 7.block_0 ([torch.Size([0])]) for Parameter 7 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 14:43:25.265150 139906580186304 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 0.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 0 (torch.Size([512, 13])), Block block_0 (torch.Size([512, 13])).
I0316 14:43:25.265254 140472875287744 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 0.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 0 (torch.Size([512, 13])), Block block_0 (torch.Size([512, 13])).
I0316 14:43:25.265347 139906580186304 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 1.block_0 ([torch.Size([0])]) for Parameter 1 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 14:43:25.265433 140472875287744 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 1.block_0 ([torch.Size([0])]) for Parameter 1 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 14:43:25.265507 139906580186304 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 2.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 2 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 14:43:25.265542 140461126632640 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 0.block_0 ([torch.Size([512, 512]), torch.Size([13, 13])]) for Parameter 0 (torch.Size([512, 13])), Block block_0 (torch.Size([512, 13])).
I0316 14:43:25.265560 139986877789376 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 8.block_0 ([torch.Size([1024, 1024]), torch.Size([1024, 1024])]) for Parameter 8 (torch.Size([1024, 1024])), Block block_0 (torch.Size([1024, 1024])).
I0316 14:43:25.265593 140472875287744 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 2.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 2 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 14:43:25.265627 139906580186304 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 3.block_0 ([torch.Size([0])]) for Parameter 3 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 14:43:25.265708 139986877789376 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 9.block_0 ([torch.Size([0])]) for Parameter 9 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 14:43:25.265709 140472875287744 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 3.block_0 ([torch.Size([0])]) for Parameter 3 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 14:43:25.265707 140461126632640 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 1.block_0 ([torch.Size([0])]) for Parameter 1 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 14:43:25.265702 139780749235392 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 0.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 0 (torch.Size([512, 13])), Block block_0 (torch.Size([512, 13])).
I0316 14:43:25.265763 139906580186304 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 4.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 4 (torch.Size([128, 256])), Block block_0 (torch.Size([128, 256])).
I0316 14:43:25.265755 139748225434816 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 0.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 0 (torch.Size([512, 13])), Block block_0 (torch.Size([512, 13])).
I0316 14:43:25.265824 140472875287744 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 4.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 4 (torch.Size([128, 256])), Block block_0 (torch.Size([128, 256])).
I0316 14:43:25.265846 139986877789376 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 10.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 10 (torch.Size([512, 1024])), Block block_0 (torch.Size([512, 1024])).
I0316 14:43:25.265856 139906580186304 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 5.block_0 ([torch.Size([0])]) for Parameter 5 (torch.Size([128])), Block block_0 (torch.Size([128])).
I0316 14:43:25.265861 140461126632640 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 2.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 2 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 14:43:25.265877 139780749235392 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 1.block_0 ([torch.Size([0])]) for Parameter 1 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 14:43:25.265871 140249194808512 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 0.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 0 (torch.Size([512, 13])), Block block_0 (torch.Size([512, 13])).
I0316 14:43:25.265932 140472875287744 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 5.block_0 ([torch.Size([0])]) for Parameter 5 (torch.Size([128])), Block block_0 (torch.Size([128])).
I0316 14:43:25.265946 139986877789376 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 11.block_0 ([torch.Size([0])]) for Parameter 11 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 14:43:25.265936 139748225434816 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 1.block_0 ([torch.Size([0])]) for Parameter 1 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 14:43:25.265974 140461126632640 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 3.block_0 ([torch.Size([0])]) for Parameter 3 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 14:43:25.265980 139906580186304 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 6.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 6 (torch.Size([1024, 506])), Block block_0 (torch.Size([1024, 506])).
I0316 14:43:25.266047 139780749235392 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 2.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 2 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 14:43:25.266050 140249194808512 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 1.block_0 ([torch.Size([0])]) for Parameter 1 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 14:43:25.266069 139986877789376 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 12.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 12 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 14:43:25.266072 139906580186304 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 7.block_0 ([torch.Size([0])]) for Parameter 7 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 14:43:25.266079 139748225434816 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 2.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 2 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 14:43:25.266047 140370416096448 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 0.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 0 (torch.Size([512, 13])), Block block_0 (torch.Size([512, 13])).
I0316 14:43:25.266098 140461126632640 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 4.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 4 (torch.Size([128, 256])), Block block_0 (torch.Size([128, 256])).
I0316 14:43:25.266160 139986877789376 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 13.block_0 ([torch.Size([0])]) for Parameter 13 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 14:43:25.266160 139780749235392 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 3.block_0 ([torch.Size([0])]) for Parameter 3 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 14:43:25.266183 139906580186304 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 8.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 8 (torch.Size([1024, 1024])), Block block_0 (torch.Size([1024, 1024])).
I0316 14:43:25.266185 140461126632640 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 5.block_0 ([torch.Size([0])]) for Parameter 5 (torch.Size([128])), Block block_0 (torch.Size([128])).
I0316 14:43:25.266191 139748225434816 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 3.block_0 ([torch.Size([0])]) for Parameter 3 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 14:43:25.266256 139986877789376 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 14.block_0 ([torch.Size([0])]) for Parameter 14 (torch.Size([1, 256])), Block block_0 (torch.Size([256])).
I0316 14:43:25.266278 139906580186304 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 9.block_0 ([torch.Size([0])]) for Parameter 9 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 14:43:25.266280 139780749235392 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 4.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 4 (torch.Size([128, 256])), Block block_0 (torch.Size([128, 256])).
I0316 14:43:25.266301 140461126632640 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 6.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 6 (torch.Size([1024, 506])), Block block_0 (torch.Size([1024, 506])).
I0316 14:43:25.266344 139986877789376 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 15.block_0 ([torch.Size([0])]) for Parameter 15 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0316 14:43:25.266370 139780749235392 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 5.block_0 ([torch.Size([0])]) for Parameter 5 (torch.Size([128])), Block block_0 (torch.Size([128])).
I0316 14:43:25.266388 139906580186304 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 10.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 10 (torch.Size([512, 1024])), Block block_0 (torch.Size([512, 1024])).
I0316 14:43:25.266398 140461126632640 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 7.block_0 ([torch.Size([0])]) for Parameter 7 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 14:43:25.266421 139748225434816 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 4.block_0 ([torch.Size([128, 128]), torch.Size([256, 256])]) for Parameter 4 (torch.Size([128, 256])), Block block_0 (torch.Size([128, 256])).
I0316 14:43:25.266447 139986877789376 shampoo_preconditioner_list.py:612] Rank 0: ShampooPreconditionerList Numel Breakdown: (524626, 524288, 655360, 131072, 163840, 32768, 2609224, 2097152, 4194304, 2097152, 2621440, 524288, 655360, 131072, 131072, 2)
I0316 14:43:25.266481 139906580186304 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 11.block_0 ([torch.Size([0])]) for Parameter 11 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 14:43:25.266485 139780749235392 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 6.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 6 (torch.Size([1024, 506])), Block block_0 (torch.Size([1024, 506])).
I0316 14:43:25.266485 139986877789376 shampoo_preconditioner_list.py:615] Rank 0: ShampooPreconditionerList Bytes Breakdown: (2098504,)
I0316 14:43:25.266502 140461126632640 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 8.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 8 (torch.Size([1024, 1024])), Block block_0 (torch.Size([1024, 1024])).
I0316 14:43:25.266524 139986877789376 shampoo_preconditioner_list.py:618] Rank 0: ShampooPreconditionerList Total Elements: 17093020
I0316 14:43:25.266537 139748225434816 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 5.block_0 ([torch.Size([0])]) for Parameter 5 (torch.Size([128])), Block block_0 (torch.Size([128])).
I0316 14:43:25.266560 139986877789376 shampoo_preconditioner_list.py:621] Rank 0: ShampooPreconditionerList Total Bytes: 2098504
I0316 14:43:25.266573 139780749235392 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 7.block_0 ([torch.Size([0])]) for Parameter 7 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 14:43:25.266582 140461126632640 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 9.block_0 ([torch.Size([0])]) for Parameter 9 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 14:43:25.266578 140472875287744 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 6.block_0 ([torch.Size([1024, 1024]), torch.Size([506, 506])]) for Parameter 6 (torch.Size([1024, 506])), Block block_0 (torch.Size([1024, 506])).
I0316 14:43:25.266657 139748225434816 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 6.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 6 (torch.Size([1024, 506])), Block block_0 (torch.Size([1024, 506])).
I0316 14:43:25.266683 139780749235392 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 8.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 8 (torch.Size([1024, 1024])), Block block_0 (torch.Size([1024, 1024])).
I0316 14:43:25.266687 140461126632640 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 10.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 10 (torch.Size([512, 1024])), Block block_0 (torch.Size([512, 1024])).
I0316 14:43:25.266702 139986877789376 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([512, 13])), Block block_0 (torch.Size([512, 13])).
I0316 14:43:25.266716 140472875287744 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 7.block_0 ([torch.Size([0])]) for Parameter 7 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 14:43:25.266765 139748225434816 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 7.block_0 ([torch.Size([0])]) for Parameter 7 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 14:43:25.266769 140461126632640 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 11.block_0 ([torch.Size([0])]) for Parameter 11 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 14:43:25.266770 139986877789376 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 14:43:25.266778 139780749235392 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 9.block_0 ([torch.Size([0])]) for Parameter 9 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 14:43:25.266821 139986877789376 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 14:43:25.266829 140472875287744 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 8.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 8 (torch.Size([1024, 1024])), Block block_0 (torch.Size([1024, 1024])).
I0316 14:43:25.266869 140461126632640 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 12.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 12 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 14:43:25.266879 139986877789376 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 14:43:25.266879 139748225434816 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 8.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 8 (torch.Size([1024, 1024])), Block block_0 (torch.Size([1024, 1024])).
I0316 14:43:25.266911 140472875287744 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 9.block_0 ([torch.Size([0])]) for Parameter 9 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 14:43:25.266928 139986877789376 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 4.block_0 (torch.Size([0])) for Parameter 4 (torch.Size([128, 256])), Block block_0 (torch.Size([128, 256])).
I0316 14:43:25.266906 140370416096448 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 1.block_0 ([torch.Size([512, 512])]) for Parameter 1 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 14:43:25.266949 140461126632640 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 13.block_0 ([torch.Size([0])]) for Parameter 13 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 14:43:25.266961 139748225434816 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 9.block_0 ([torch.Size([0])]) for Parameter 9 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 14:43:25.266974 139986877789376 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 5.block_0 (torch.Size([0])) for Parameter 5 (torch.Size([128])), Block block_0 (torch.Size([128])).
I0316 14:43:25.266981 140249194808512 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 2.block_0 ([torch.Size([256, 256]), torch.Size([512, 512])]) for Parameter 2 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 14:43:25.267021 139986877789376 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 6.block_0 (torch.Size([0])) for Parameter 6 (torch.Size([1024, 506])), Block block_0 (torch.Size([1024, 506])).
I0316 14:43:25.267027 140461126632640 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 14.block_0 ([torch.Size([0])]) for Parameter 14 (torch.Size([1, 256])), Block block_0 (torch.Size([256])).
I0316 14:43:25.267029 140472875287744 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 10.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 10 (torch.Size([512, 1024])), Block block_0 (torch.Size([512, 1024])).
I0316 14:43:25.267061 139748225434816 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 10.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 10 (torch.Size([512, 1024])), Block block_0 (torch.Size([512, 1024])).
I0316 14:43:25.267075 139986877789376 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 7.block_0 (torch.Size([0])) for Parameter 7 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 14:43:25.267098 140461126632640 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 15.block_0 ([torch.Size([0])]) for Parameter 15 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0316 14:43:25.267109 140472875287744 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 11.block_0 ([torch.Size([0])]) for Parameter 11 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 14:43:25.267105 140370416096448 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 2.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 2 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 14:43:25.267128 140249194808512 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 3.block_0 ([torch.Size([0])]) for Parameter 3 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 14:43:25.267143 139748225434816 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 11.block_0 ([torch.Size([0])]) for Parameter 11 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 14:43:25.267186 140461126632640 shampoo_preconditioner_list.py:612] Rank 6: ShampooPreconditionerList Numel Breakdown: (524626, 524288, 655360, 131072, 163840, 32768, 2609224, 2097152, 4194304, 2097152, 2621440, 524288, 655360, 131072, 131072, 2)
I0316 14:43:25.267180 139986877789376 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 8.block_0 (torch.Size([1024, 1024])) for Parameter 8 (torch.Size([1024, 1024])), Block block_0 (torch.Size([1024, 1024])).
I0316 14:43:25.267213 140472875287744 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 12.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 12 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 14:43:25.267227 140461126632640 shampoo_preconditioner_list.py:615] Rank 6: ShampooPreconditionerList Bytes Breakdown: (2098504,)
I0316 14:43:25.267249 139986877789376 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 9.block_0 (torch.Size([0])) for Parameter 9 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 14:43:25.267255 140461126632640 shampoo_preconditioner_list.py:618] Rank 6: ShampooPreconditionerList Total Elements: 17093020
I0316 14:43:25.267236 139906580186304 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 12.block_0 ([torch.Size([256, 256]), torch.Size([512, 512])]) for Parameter 12 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 14:43:25.267260 140249194808512 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 4.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 4 (torch.Size([128, 256])), Block block_0 (torch.Size([128, 256])).
I0316 14:43:25.267264 139748225434816 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 12.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 12 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 14:43:25.267288 140461126632640 shampoo_preconditioner_list.py:621] Rank 6: ShampooPreconditionerList Total Bytes: 2098504
I0316 14:43:25.267304 140472875287744 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 13.block_0 ([torch.Size([0])]) for Parameter 13 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 14:43:25.267310 139986877789376 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 10.block_0 (torch.Size([0])) for Parameter 10 (torch.Size([512, 1024])), Block block_0 (torch.Size([512, 1024])).
I0316 14:43:25.267295 140370416096448 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 3.block_0 ([torch.Size([256, 256])]) for Parameter 3 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 14:43:25.267340 139748225434816 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 13.block_0 ([torch.Size([0])]) for Parameter 13 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 14:43:25.267348 140249194808512 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 5.block_0 ([torch.Size([0])]) for Parameter 5 (torch.Size([128])), Block block_0 (torch.Size([128])).
I0316 14:43:25.267358 139986877789376 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 11.block_0 (torch.Size([0])) for Parameter 11 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 14:43:25.267360 139906580186304 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 13.block_0 ([torch.Size([0])]) for Parameter 13 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 14:43:25.267395 140472875287744 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 14.block_0 ([torch.Size([0])]) for Parameter 14 (torch.Size([1, 256])), Block block_0 (torch.Size([256])).
I0316 14:43:25.267404 139986877789376 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 12.block_0 (torch.Size([0])) for Parameter 12 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 14:43:25.267396 139780749235392 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 10.block_0 ([torch.Size([512, 512]), torch.Size([1024, 1024])]) for Parameter 10 (torch.Size([512, 1024])), Block block_0 (torch.Size([512, 1024])).
I0316 14:43:25.267429 139748225434816 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 14.block_0 ([torch.Size([0])]) for Parameter 14 (torch.Size([1, 256])), Block block_0 (torch.Size([256])).
I0316 14:43:25.267442 139906580186304 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 14.block_0 ([torch.Size([0])]) for Parameter 14 (torch.Size([1, 256])), Block block_0 (torch.Size([256])).
I0316 14:43:25.267449 139986877789376 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 13.block_0 (torch.Size([0])) for Parameter 13 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 14:43:25.267443 140370416096448 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 4.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 4 (torch.Size([128, 256])), Block block_0 (torch.Size([128, 256])).
I0316 14:43:25.267461 140249194808512 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 6.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 6 (torch.Size([1024, 506])), Block block_0 (torch.Size([1024, 506])).
I0316 14:43:25.267454 140461126632640 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([512, 13])) for Parameter 0 (torch.Size([512, 13])), Block block_0 (torch.Size([512, 13])).
I0316 14:43:25.267477 140472875287744 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 15.block_0 ([torch.Size([0])]) for Parameter 15 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0316 14:43:25.267495 139986877789376 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 14.block_0 (torch.Size([0])) for Parameter 14 (torch.Size([1, 256])), Block block_0 (torch.Size([256])).
I0316 14:43:25.267502 139748225434816 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 15.block_0 ([torch.Size([0])]) for Parameter 15 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0316 14:43:25.267521 139906580186304 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 15.block_0 ([torch.Size([0])]) for Parameter 15 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0316 14:43:25.267523 140461126632640 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 14:43:25.267522 139780749235392 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 11.block_0 ([torch.Size([0])]) for Parameter 11 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 14:43:25.267550 139986877789376 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 15.block_0 (torch.Size([0])) for Parameter 15 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0316 14:43:25.267558 140249194808512 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 7.block_0 ([torch.Size([0])]) for Parameter 7 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 14:43:25.267561 140472875287744 shampoo_preconditioner_list.py:612] Rank 2: ShampooPreconditionerList Numel Breakdown: (524626, 524288, 655360, 131072, 163840, 32768, 2609224, 2097152, 4194304, 2097152, 2621440, 524288, 655360, 131072, 131072, 2)
I0316 14:43:25.267570 140461126632640 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 14:43:25.267587 139748225434816 shampoo_preconditioner_list.py:612] Rank 5: ShampooPreconditionerList Numel Breakdown: (524626, 524288, 655360, 131072, 163840, 32768, 2609224, 2097152, 4194304, 2097152, 2621440, 524288, 655360, 131072, 131072, 2)
I0316 14:43:25.267582 140370416096448 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 5.block_0 ([torch.Size([128, 128])]) for Parameter 5 (torch.Size([128])), Block block_0 (torch.Size([128])).
I0316 14:43:25.267596 140472875287744 shampoo_preconditioner_list.py:615] Rank 2: ShampooPreconditionerList Bytes Breakdown: (2098504,)
I0316 14:43:25.267615 140461126632640 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 14:43:25.267615 139986877789376 shampoo_preconditioner_list.py:375] Rank 0: AdaGradPreconditionerList Numel Breakdown: (0, 0, 0, 0, 0, 0, 0, 0, 1048576, 0, 0, 0, 0, 0, 0, 0)
I0316 14:43:25.267626 140472875287744 shampoo_preconditioner_list.py:618] Rank 2: ShampooPreconditionerList Total Elements: 17093020
I0316 14:43:25.267619 139906580186304 shampoo_preconditioner_list.py:612] Rank 4: ShampooPreconditionerList Numel Breakdown: (524626, 524288, 655360, 131072, 163840, 32768, 2609224, 2097152, 4194304, 2097152, 2621440, 524288, 655360, 131072, 131072, 2)
I0316 14:43:25.267627 139748225434816 shampoo_preconditioner_list.py:615] Rank 5: ShampooPreconditionerList Bytes Breakdown: (2098504,)
I0316 14:43:25.267647 139986877789376 shampoo_preconditioner_list.py:378] Rank 0: AdaGradPreconditionerList Bytes Breakdown: (0, 0, 0, 0, 0, 0, 0, 0, 4194304, 0, 0, 0, 0, 0, 0, 0)
I0316 14:43:25.267642 139780749235392 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 12.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 12 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 14:43:25.267653 140472875287744 shampoo_preconditioner_list.py:621] Rank 2: ShampooPreconditionerList Total Bytes: 2098504
I0316 14:43:25.267656 139748225434816 shampoo_preconditioner_list.py:618] Rank 5: ShampooPreconditionerList Total Elements: 17093020
I0316 14:43:25.267656 139906580186304 shampoo_preconditioner_list.py:615] Rank 4: ShampooPreconditionerList Bytes Breakdown: (2098504,)
I0316 14:43:25.267663 140249194808512 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 8.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 8 (torch.Size([1024, 1024])), Block block_0 (torch.Size([1024, 1024])).
I0316 14:43:25.267669 140461126632640 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 4.block_0 (torch.Size([0])) for Parameter 4 (torch.Size([128, 256])), Block block_0 (torch.Size([128, 256])).
I0316 14:43:25.267682 139986877789376 shampoo_preconditioner_list.py:381] Rank 0: AdaGradPreconditionerList Total Elements: 1048576
I0316 14:43:25.267682 139748225434816 shampoo_preconditioner_list.py:621] Rank 5: ShampooPreconditionerList Total Bytes: 2098504
I0316 14:43:25.267688 139906580186304 shampoo_preconditioner_list.py:618] Rank 4: ShampooPreconditionerList Total Elements: 17093020
I0316 14:43:25.267718 139986877789376 shampoo_preconditioner_list.py:384] Rank 0: AdaGradPreconditionerList Total Bytes: 4194304
I0316 14:43:25.267711 140370416096448 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 6.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 6 (torch.Size([1024, 506])), Block block_0 (torch.Size([1024, 506])).
I0316 14:43:25.267733 139906580186304 shampoo_preconditioner_list.py:621] Rank 4: ShampooPreconditionerList Total Bytes: 2098504
I0316 14:43:25.267732 140461126632640 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 5.block_0 (torch.Size([0])) for Parameter 5 (torch.Size([128])), Block block_0 (torch.Size([128])).
I0316 14:43:25.267739 139780749235392 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 13.block_0 ([torch.Size([0])]) for Parameter 13 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 14:43:25.267766 140249194808512 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 9.block_0 ([torch.Size([0])]) for Parameter 9 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 14:43:25.267779 140461126632640 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 6.block_0 (torch.Size([0])) for Parameter 6 (torch.Size([1024, 506])), Block block_0 (torch.Size([1024, 506])).
I0316 14:43:25.267791 140472875287744 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([512, 13])), Block block_0 (torch.Size([512, 13])).
I0316 14:43:25.267824 139780749235392 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 14.block_0 ([torch.Size([0])]) for Parameter 14 (torch.Size([1, 256])), Block block_0 (torch.Size([256])).
I0316 14:43:25.267825 139748225434816 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([512, 13])), Block block_0 (torch.Size([512, 13])).
I0316 14:43:25.267831 140461126632640 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 7.block_0 (torch.Size([0])) for Parameter 7 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 14:43:25.267863 140472875287744 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 14:43:25.267859 140370416096448 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 7.block_0 ([torch.Size([1024, 1024])]) for Parameter 7 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 14:43:25.267864 139906580186304 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([512, 13])), Block block_0 (torch.Size([512, 13])).
I0316 14:43:25.267886 140461126632640 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 8.block_0 (torch.Size([0])) for Parameter 8 (torch.Size([1024, 1024])), Block block_0 (torch.Size([1024, 1024])).
I0316 14:43:25.267887 139748225434816 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 14:43:25.267894 140249194808512 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 10.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 10 (torch.Size([512, 1024])), Block block_0 (torch.Size([512, 1024])).
I0316 14:43:25.267916 139780749235392 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 15.block_0 ([torch.Size([0])]) for Parameter 15 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0316 14:43:25.267939 140472875287744 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 14:43:25.267944 139906580186304 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 14:43:25.267956 139748225434816 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 14:43:25.267959 140461126632640 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 9.block_0 (torch.Size([0])) for Parameter 9 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 14:43:25.267983 140370416096448 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 8.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 8 (torch.Size([1024, 1024])), Block block_0 (torch.Size([1024, 1024])).
I0316 14:43:25.267993 140472875287744 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 14:43:25.268000 139906580186304 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 14:43:25.268006 139748225434816 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 14:43:25.268010 140461126632640 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 10.block_0 (torch.Size([0])) for Parameter 10 (torch.Size([512, 1024])), Block block_0 (torch.Size([512, 1024])).
I0316 14:43:25.268006 140249194808512 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 11.block_0 ([torch.Size([0])]) for Parameter 11 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 14:43:25.268039 139780749235392 shampoo_preconditioner_list.py:612] Rank 1: ShampooPreconditionerList Numel Breakdown: (524626, 524288, 655360, 131072, 163840, 32768, 2609224, 2097152, 4194304, 2097152, 2621440, 524288, 655360, 131072, 131072, 2)
I0316 14:43:25.268056 140472875287744 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 4.block_0 (torch.Size([0])) for Parameter 4 (torch.Size([128, 256])), Block block_0 (torch.Size([128, 256])).
I0316 14:43:25.268058 139906580186304 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 14:43:25.268060 140461126632640 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 11.block_0 (torch.Size([0])) for Parameter 11 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 14:43:25.268086 139780749235392 shampoo_preconditioner_list.py:615] Rank 1: ShampooPreconditionerList Bytes Breakdown: (2098504,)
I0316 14:43:25.268107 140472875287744 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 5.block_0 (torch.Size([0])) for Parameter 5 (torch.Size([128])), Block block_0 (torch.Size([128])).
I0316 14:43:25.268109 140461126632640 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 12.block_0 (torch.Size([0])) for Parameter 12 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 14:43:25.268103 139748225434816 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 4.block_0 (torch.Size([128, 256])) for Parameter 4 (torch.Size([128, 256])), Block block_0 (torch.Size([128, 256])).
I0316 14:43:25.268110 139906580186304 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 4.block_0 (torch.Size([0])) for Parameter 4 (torch.Size([128, 256])), Block block_0 (torch.Size([128, 256])).
I0316 14:43:25.268094 139986877789376 submission.py:105] Large parameters (embedding tables) detected (dim >= 1_000_000). Instantiating Row-Wise AdaGrad...
I0316 14:43:25.268126 139780749235392 shampoo_preconditioner_list.py:618] Rank 1: ShampooPreconditionerList Total Elements: 17093020
I0316 14:43:25.268132 140249194808512 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 12.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 12 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 14:43:25.268158 139780749235392 shampoo_preconditioner_list.py:621] Rank 1: ShampooPreconditionerList Total Bytes: 2098504
I0316 14:43:25.268157 140461126632640 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 13.block_0 (torch.Size([0])) for Parameter 13 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 14:43:25.268162 139906580186304 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 5.block_0 (torch.Size([0])) for Parameter 5 (torch.Size([128])), Block block_0 (torch.Size([128])).
I0316 14:43:25.268168 139748225434816 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 5.block_0 (torch.Size([0])) for Parameter 5 (torch.Size([128])), Block block_0 (torch.Size([128])).
W0316 14:43:25.268194 139986877789376 distributed_shampoo.py:359] Nesterov flag is enabled but momentum parameter is zero! Continuing without using momentum or Nesterov acceleration...
I0316 14:43:25.268205 140461126632640 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 14.block_0 (torch.Size([0])) for Parameter 14 (torch.Size([1, 256])), Block block_0 (torch.Size([256])).
I0316 14:43:25.268215 139906580186304 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 6.block_0 (torch.Size([0])) for Parameter 6 (torch.Size([1024, 506])), Block block_0 (torch.Size([1024, 506])).
I0316 14:43:25.268212 140472875287744 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 6.block_0 (torch.Size([1024, 506])) for Parameter 6 (torch.Size([1024, 506])), Block block_0 (torch.Size([1024, 506])).
I0316 14:43:25.268218 140249194808512 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 13.block_0 ([torch.Size([0])]) for Parameter 13 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 14:43:25.268222 139748225434816 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 6.block_0 (torch.Size([0])) for Parameter 6 (torch.Size([1024, 506])), Block block_0 (torch.Size([1024, 506])).
I0316 14:43:25.268254 140461126632640 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 15.block_0 (torch.Size([0])) for Parameter 15 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0316 14:43:25.268282 139906580186304 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 7.block_0 (torch.Size([0])) for Parameter 7 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 14:43:25.268296 139748225434816 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 7.block_0 (torch.Size([0])) for Parameter 7 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 14:43:25.268303 140472875287744 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 7.block_0 (torch.Size([0])) for Parameter 7 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 14:43:25.268311 139780749235392 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([512, 13])), Block block_0 (torch.Size([512, 13])).
I0316 14:43:25.268320 140249194808512 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 14.block_0 ([torch.Size([0])]) for Parameter 14 (torch.Size([1, 256])), Block block_0 (torch.Size([256])).
I0316 14:43:25.268332 140461126632640 shampoo_preconditioner_list.py:375] Rank 6: AdaGradPreconditionerList Numel Breakdown: (6656, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)
I0316 14:43:25.268345 139906580186304 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 8.block_0 (torch.Size([0])) for Parameter 8 (torch.Size([1024, 1024])), Block block_0 (torch.Size([1024, 1024])).
I0316 14:43:25.268360 140472875287744 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 8.block_0 (torch.Size([0])) for Parameter 8 (torch.Size([1024, 1024])), Block block_0 (torch.Size([1024, 1024])).
I0316 14:43:25.268361 139748225434816 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 8.block_0 (torch.Size([0])) for Parameter 8 (torch.Size([1024, 1024])), Block block_0 (torch.Size([1024, 1024])).
I0316 14:43:25.268367 140461126632640 shampoo_preconditioner_list.py:378] Rank 6: AdaGradPreconditionerList Bytes Breakdown: (26624, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)
I0316 14:43:25.268379 139780749235392 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 14:43:25.268398 140461126632640 shampoo_preconditioner_list.py:381] Rank 6: AdaGradPreconditionerList Total Elements: 6656
I0316 14:43:25.268405 140249194808512 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 15.block_0 ([torch.Size([0])]) for Parameter 15 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0316 14:43:25.268410 139906580186304 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 9.block_0 (torch.Size([0])) for Parameter 9 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 14:43:25.268413 139748225434816 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 9.block_0 (torch.Size([0])) for Parameter 9 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 14:43:25.268414 140472875287744 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 9.block_0 (torch.Size([0])) for Parameter 9 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 14:43:25.268434 140461126632640 shampoo_preconditioner_list.py:384] Rank 6: AdaGradPreconditionerList Total Bytes: 26624
I0316 14:43:25.268443 139780749235392 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 14:43:25.268463 139748225434816 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 10.block_0 (torch.Size([0])) for Parameter 10 (torch.Size([512, 1024])), Block block_0 (torch.Size([512, 1024])).
I0316 14:43:25.268463 139906580186304 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 10.block_0 (torch.Size([0])) for Parameter 10 (torch.Size([512, 1024])), Block block_0 (torch.Size([512, 1024])).
I0316 14:43:25.268464 140472875287744 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 10.block_0 (torch.Size([0])) for Parameter 10 (torch.Size([512, 1024])), Block block_0 (torch.Size([512, 1024])).
I0316 14:43:25.268498 139780749235392 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 14:43:25.268503 140249194808512 shampoo_preconditioner_list.py:612] Rank 3: ShampooPreconditionerList Numel Breakdown: (524626, 524288, 655360, 131072, 163840, 32768, 2609224, 2097152, 4194304, 2097152, 2621440, 524288, 655360, 131072, 131072, 2)
I0316 14:43:25.268514 140472875287744 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 11.block_0 (torch.Size([0])) for Parameter 11 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 14:43:25.268514 139906580186304 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 11.block_0 (torch.Size([0])) for Parameter 11 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 14:43:25.268521 139748225434816 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 11.block_0 (torch.Size([0])) for Parameter 11 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 14:43:25.268543 140249194808512 shampoo_preconditioner_list.py:615] Rank 3: ShampooPreconditionerList Bytes Breakdown: (2098504,)
I0316 14:43:25.268528 140370416096448 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 9.block_0 ([torch.Size([1024, 1024])]) for Parameter 9 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 14:43:25.268559 139780749235392 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 4.block_0 (torch.Size([0])) for Parameter 4 (torch.Size([128, 256])), Block block_0 (torch.Size([128, 256])).
I0316 14:43:25.268565 140472875287744 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 12.block_0 (torch.Size([0])) for Parameter 12 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 14:43:25.268570 139748225434816 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 12.block_0 (torch.Size([0])) for Parameter 12 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 14:43:25.268587 140249194808512 shampoo_preconditioner_list.py:618] Rank 3: ShampooPreconditionerList Total Elements: 17093020
I0316 14:43:25.268612 139780749235392 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 5.block_0 (torch.Size([0])) for Parameter 5 (torch.Size([128])), Block block_0 (torch.Size([128])).
I0316 14:43:25.268615 140472875287744 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 13.block_0 (torch.Size([0])) for Parameter 13 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 14:43:25.268618 139748225434816 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 13.block_0 (torch.Size([0])) for Parameter 13 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 14:43:25.268624 140249194808512 shampoo_preconditioner_list.py:621] Rank 3: ShampooPreconditionerList Total Bytes: 2098504
I0316 14:43:25.268665 140472875287744 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 14.block_0 (torch.Size([0])) for Parameter 14 (torch.Size([1, 256])), Block block_0 (torch.Size([256])).
I0316 14:43:25.268667 139748225434816 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 14.block_0 (torch.Size([0])) for Parameter 14 (torch.Size([1, 256])), Block block_0 (torch.Size([256])).
I0316 14:43:25.268672 139780749235392 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 6.block_0 (torch.Size([0])) for Parameter 6 (torch.Size([1024, 506])), Block block_0 (torch.Size([1024, 506])).
I0316 14:43:25.268673 140370416096448 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 10.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 10 (torch.Size([512, 1024])), Block block_0 (torch.Size([512, 1024])).
I0316 14:43:25.268714 139748225434816 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 15.block_0 (torch.Size([0])) for Parameter 15 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0316 14:43:25.268725 140472875287744 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 15.block_0 (torch.Size([0])) for Parameter 15 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0316 14:43:25.268733 139780749235392 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 7.block_0 (torch.Size([0])) for Parameter 7 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 14:43:25.268758 140249194808512 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([512, 13])), Block block_0 (torch.Size([512, 13])).
I0316 14:43:25.268782 139748225434816 shampoo_preconditioner_list.py:375] Rank 5: AdaGradPreconditionerList Numel Breakdown: (0, 0, 0, 0, 32768, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)
I0316 14:43:25.268789 140472875287744 shampoo_preconditioner_list.py:375] Rank 2: AdaGradPreconditionerList Numel Breakdown: (0, 0, 0, 0, 0, 0, 518144, 0, 0, 0, 0, 0, 0, 0, 0, 0)
I0316 14:43:25.268786 139780749235392 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 8.block_0 (torch.Size([0])) for Parameter 8 (torch.Size([1024, 1024])), Block block_0 (torch.Size([1024, 1024])).
I0316 14:43:25.268817 139748225434816 shampoo_preconditioner_list.py:378] Rank 5: AdaGradPreconditionerList Bytes Breakdown: (0, 0, 0, 0, 131072, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)
I0316 14:43:25.268824 140472875287744 shampoo_preconditioner_list.py:378] Rank 2: AdaGradPreconditionerList Bytes Breakdown: (0, 0, 0, 0, 0, 0, 2072576, 0, 0, 0, 0, 0, 0, 0, 0, 0)
I0316 14:43:25.268829 140249194808512 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([512])), Block block_0 (torch.Size([512])).
W0316 14:43:25.268813 140461126632640 distributed_shampoo.py:359] Nesterov flag is enabled but momentum parameter is zero! Continuing without using momentum or Nesterov acceleration...
I0316 14:43:25.268839 139780749235392 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 9.block_0 (torch.Size([0])) for Parameter 9 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 14:43:25.268852 139748225434816 shampoo_preconditioner_list.py:381] Rank 5: AdaGradPreconditionerList Total Elements: 32768
I0316 14:43:25.268855 140472875287744 shampoo_preconditioner_list.py:381] Rank 2: AdaGradPreconditionerList Total Elements: 518144
I0316 14:43:25.268886 140472875287744 shampoo_preconditioner_list.py:384] Rank 2: AdaGradPreconditionerList Total Bytes: 2072576
I0316 14:43:25.268887 139748225434816 shampoo_preconditioner_list.py:384] Rank 5: AdaGradPreconditionerList Total Bytes: 131072
I0316 14:43:25.268938 139780749235392 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 10.block_0 (torch.Size([512, 1024])) for Parameter 10 (torch.Size([512, 1024])), Block block_0 (torch.Size([512, 1024])).
I0316 14:43:25.269016 139780749235392 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 11.block_0 (torch.Size([0])) for Parameter 11 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 14:43:25.269092 139780749235392 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 12.block_0 (torch.Size([0])) for Parameter 12 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 14:43:25.269137 139906580186304 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 12.block_0 (torch.Size([256, 512])) for Parameter 12 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 14:43:25.269163 139780749235392 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 13.block_0 (torch.Size([0])) for Parameter 13 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 14:43:25.269213 139780749235392 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 14.block_0 (torch.Size([0])) for Parameter 14 (torch.Size([1, 256])), Block block_0 (torch.Size([256])).
I0316 14:43:25.269233 139906580186304 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 13.block_0 (torch.Size([0])) for Parameter 13 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 14:43:25.269270 139780749235392 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 15.block_0 (torch.Size([0])) for Parameter 15 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0316 14:43:25.269297 139906580186304 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 14.block_0 (torch.Size([0])) for Parameter 14 (torch.Size([1, 256])), Block block_0 (torch.Size([256])).
W0316 14:43:25.269306 140472875287744 distributed_shampoo.py:359] Nesterov flag is enabled but momentum parameter is zero! Continuing without using momentum or Nesterov acceleration...
I0316 14:43:25.269338 139780749235392 shampoo_preconditioner_list.py:375] Rank 1: AdaGradPreconditionerList Numel Breakdown: (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 524288, 0, 0, 0, 0, 0)
I0316 14:43:25.269359 139906580186304 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 15.block_0 (torch.Size([0])) for Parameter 15 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0316 14:43:25.269378 139780749235392 shampoo_preconditioner_list.py:378] Rank 1: AdaGradPreconditionerList Bytes Breakdown: (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2097152, 0, 0, 0, 0, 0)
I0316 14:43:25.269414 139780749235392 shampoo_preconditioner_list.py:381] Rank 1: AdaGradPreconditionerList Total Elements: 524288
I0316 14:43:25.269421 139906580186304 shampoo_preconditioner_list.py:375] Rank 4: AdaGradPreconditionerList Numel Breakdown: (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 131072, 0, 0, 0)
I0316 14:43:25.269448 139780749235392 shampoo_preconditioner_list.py:384] Rank 1: AdaGradPreconditionerList Total Bytes: 2097152
I0316 14:43:25.269455 139906580186304 shampoo_preconditioner_list.py:378] Rank 4: AdaGradPreconditionerList Bytes Breakdown: (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 524288, 0, 0, 0)
I0316 14:43:25.269484 139906580186304 shampoo_preconditioner_list.py:381] Rank 4: AdaGradPreconditionerList Total Elements: 131072
I0316 14:43:25.269520 139906580186304 shampoo_preconditioner_list.py:384] Rank 4: AdaGradPreconditionerList Total Bytes: 524288
W0316 14:43:25.269828 139780749235392 distributed_shampoo.py:359] Nesterov flag is enabled but momentum parameter is zero! Continuing without using momentum or Nesterov acceleration...
I0316 14:43:25.269894 140249194808512 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([256, 512])) for Parameter 2 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
W0316 14:43:25.269903 139906580186304 distributed_shampoo.py:359] Nesterov flag is enabled but momentum parameter is zero! Continuing without using momentum or Nesterov acceleration...
I0316 14:43:25.269917 140370416096448 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 11.block_0 ([torch.Size([512, 512])]) for Parameter 11 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 14:43:25.270003 140249194808512 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 14:43:25.270070 140249194808512 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 4.block_0 (torch.Size([0])) for Parameter 4 (torch.Size([128, 256])), Block block_0 (torch.Size([128, 256])).
I0316 14:43:25.270071 140370416096448 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 12.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 12 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 14:43:25.270132 140249194808512 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 5.block_0 (torch.Size([0])) for Parameter 5 (torch.Size([128])), Block block_0 (torch.Size([128])).
I0316 14:43:25.270199 140249194808512 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 6.block_0 (torch.Size([0])) for Parameter 6 (torch.Size([1024, 506])), Block block_0 (torch.Size([1024, 506])).
W0316 14:43:25.270346 139748225434816 distributed_shampoo.py:359] Nesterov flag is enabled but momentum parameter is zero! Continuing without using momentum or Nesterov acceleration...
I0316 14:43:25.270491 140249194808512 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 7.block_0 (torch.Size([0])) for Parameter 7 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 14:43:25.270622 140249194808512 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 8.block_0 (torch.Size([0])) for Parameter 8 (torch.Size([1024, 1024])), Block block_0 (torch.Size([1024, 1024])).
I0316 14:43:25.270694 140249194808512 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 9.block_0 (torch.Size([0])) for Parameter 9 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 14:43:25.270765 140249194808512 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 10.block_0 (torch.Size([0])) for Parameter 10 (torch.Size([512, 1024])), Block block_0 (torch.Size([512, 1024])).
I0316 14:43:25.270827 140249194808512 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 11.block_0 (torch.Size([0])) for Parameter 11 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 14:43:25.270814 140461126632640 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 14:43:25.270885 140249194808512 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 12.block_0 (torch.Size([0])) for Parameter 12 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 14:43:25.270905 140461126632640 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_1 (torch.Size([0])) for Parameter 0 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 14:43:25.270941 140249194808512 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 13.block_0 (torch.Size([0])) for Parameter 13 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 14:43:25.270978 140461126632640 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 14:43:25.270998 140249194808512 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 14.block_0 (torch.Size([0])) for Parameter 14 (torch.Size([1, 256])), Block block_0 (torch.Size([256])).
I0316 14:43:25.271041 140461126632640 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_1 (torch.Size([0])) for Parameter 1 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 14:43:25.271048 140249194808512 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 15.block_0 (torch.Size([0])) for Parameter 15 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0316 14:43:25.271095 140461126632640 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 14:43:25.271109 140249194808512 shampoo_preconditioner_list.py:375] Rank 3: AdaGradPreconditionerList Numel Breakdown: (0, 0, 131072, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)
I0316 14:43:25.271145 140249194808512 shampoo_preconditioner_list.py:378] Rank 3: AdaGradPreconditionerList Bytes Breakdown: (0, 0, 524288, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)
I0316 14:43:25.271153 140461126632640 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_1 (torch.Size([0])) for Parameter 2 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 14:43:25.271182 140249194808512 shampoo_preconditioner_list.py:381] Rank 3: AdaGradPreconditionerList Total Elements: 131072
I0316 14:43:25.271216 140249194808512 shampoo_preconditioner_list.py:384] Rank 3: AdaGradPreconditionerList Total Bytes: 524288
I0316 14:43:25.271347 140472875287744 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 14:43:25.271371 140370416096448 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 13.block_0 ([torch.Size([256, 256])]) for Parameter 13 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 14:43:25.271464 140472875287744 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_1 (torch.Size([0])) for Parameter 0 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 14:43:25.271549 140370416096448 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 14.block_0 ([torch.Size([256, 256])]) for Parameter 14 (torch.Size([1, 256])), Block block_0 (torch.Size([256])).
W0316 14:43:25.271618 140249194808512 distributed_shampoo.py:359] Nesterov flag is enabled but momentum parameter is zero! Continuing without using momentum or Nesterov acceleration...
I0316 14:43:25.271705 140370416096448 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 15.block_0 ([torch.Size([1, 1])]) for Parameter 15 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0316 14:43:25.271758 139986877789376 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([524288, 128])) for Parameter 0 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 14:43:25.271834 140370416096448 shampoo_preconditioner_list.py:612] Rank 7: ShampooPreconditionerList Numel Breakdown: (524626, 524288, 655360, 131072, 163840, 32768, 2609224, 2097152, 4194304, 2097152, 2621440, 524288, 655360, 131072, 131072, 2)
I0316 14:43:25.271882 140370416096448 shampoo_preconditioner_list.py:615] Rank 7: ShampooPreconditionerList Bytes Breakdown: (2098504, 2097152, 2621440, 524288, 655360, 131072, 10436896, 8388608, 16777216)
I0316 14:43:25.271892 139986877789376 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_1 (torch.Size([0])) for Parameter 0 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 14:43:25.271921 140370416096448 shampoo_preconditioner_list.py:618] Rank 7: ShampooPreconditionerList Total Elements: 17093020
I0316 14:43:25.271956 140370416096448 shampoo_preconditioner_list.py:621] Rank 7: ShampooPreconditionerList Total Bytes: 43730536
I0316 14:43:25.271960 139986877789376 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 14:43:25.271981 139780749235392 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 14:43:25.272018 139986877789376 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_1 (torch.Size([0])) for Parameter 1 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 14:43:25.272078 139986877789376 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 14:43:25.272091 140370416096448 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([512, 13])), Block block_0 (torch.Size([512, 13])).
I0316 14:43:25.272154 139986877789376 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_1 (torch.Size([0])) for Parameter 2 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 14:43:25.272169 139906580186304 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 14:43:25.272201 140370416096448 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([512])) for Parameter 1 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 14:43:25.272215 139986877789376 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 14:43:25.272282 139986877789376 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_1 (torch.Size([0])) for Parameter 3 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 14:43:25.272278 139906580186304 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_1 (torch.Size([0])) for Parameter 0 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 14:43:25.272285 140370416096448 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 14:43:25.272292 140461126632640 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([524288, 128])) for Parameter 3 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 14:43:25.272342 139906580186304 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 14:43:25.272343 139986877789376 shampoo_preconditioner_list.py:375] Rank 0: AdaGradPreconditionerList Numel Breakdown: (67108864, 0, 0, 0, 0, 0, 0, 0)
I0316 14:43:25.272363 140370416096448 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([256])) for Parameter 3 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 14:43:25.272388 139986877789376 shampoo_preconditioner_list.py:378] Rank 0: AdaGradPreconditionerList Bytes Breakdown: (268435456, 0, 0, 0, 0, 0, 0, 0)
I0316 14:43:25.272396 140461126632640 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_1 (torch.Size([0])) for Parameter 3 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 14:43:25.272401 139906580186304 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_1 (torch.Size([0])) for Parameter 1 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 14:43:25.272424 140370416096448 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 4.block_0 (torch.Size([0])) for Parameter 4 (torch.Size([128, 256])), Block block_0 (torch.Size([128, 256])).
I0316 14:43:25.272427 139986877789376 shampoo_preconditioner_list.py:381] Rank 0: AdaGradPreconditionerList Total Elements: 67108864
I0316 14:43:25.272447 140461126632640 shampoo_preconditioner_list.py:375] Rank 6: AdaGradPreconditionerList Numel Breakdown: (0, 0, 0, 0, 0, 0, 67108864, 0)
I0316 14:43:25.272458 139986877789376 shampoo_preconditioner_list.py:384] Rank 0: AdaGradPreconditionerList Total Bytes: 268435456
I0316 14:43:25.272442 140472875287744 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([524288, 128])) for Parameter 1 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 14:43:25.272483 140461126632640 shampoo_preconditioner_list.py:378] Rank 6: AdaGradPreconditionerList Bytes Breakdown: (0, 0, 0, 0, 0, 0, 268435456, 0)
I0316 14:43:25.272469 139748225434816 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 14:43:25.272512 140461126632640 shampoo_preconditioner_list.py:381] Rank 6: AdaGradPreconditionerList Total Elements: 67108864
I0316 14:43:25.272518 140370416096448 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 5.block_0 (torch.Size([128])) for Parameter 5 (torch.Size([128])), Block block_0 (torch.Size([128])).
I0316 14:43:25.272547 140461126632640 shampoo_preconditioner_list.py:384] Rank 6: AdaGradPreconditionerList Total Bytes: 268435456
I0316 14:43:25.272552 140472875287744 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_1 (torch.Size([0])) for Parameter 1 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 14:43:25.272556 139748225434816 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_1 (torch.Size([0])) for Parameter 0 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 14:43:25.272583 140370416096448 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 6.block_0 (torch.Size([0])) for Parameter 6 (torch.Size([1024, 506])), Block block_0 (torch.Size([1024, 506])).
I0316 14:43:25.272609 140472875287744 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 14:43:25.272613 139748225434816 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 14:43:25.272660 140472875287744 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_1 (torch.Size([0])) for Parameter 2 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 14:43:25.272663 139748225434816 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_1 (torch.Size([0])) for Parameter 1 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 14:43:25.272686 140370416096448 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 7.block_0 (torch.Size([1024])) for Parameter 7 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 14:43:25.272711 139748225434816 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 14:43:25.272716 140472875287744 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 14:43:25.272753 140370416096448 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 8.block_0 (torch.Size([0])) for Parameter 8 (torch.Size([1024, 1024])), Block block_0 (torch.Size([1024, 1024])).
I0316 14:43:25.272764 140472875287744 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_1 (torch.Size([0])) for Parameter 3 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 14:43:25.272809 140472875287744 shampoo_preconditioner_list.py:375] Rank 2: AdaGradPreconditionerList Numel Breakdown: (0, 0, 67108864, 0, 0, 0, 0, 0)
I0316 14:43:25.272833 140370416096448 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 9.block_0 (torch.Size([1024])) for Parameter 9 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 14:43:25.272856 140472875287744 shampoo_preconditioner_list.py:378] Rank 2: AdaGradPreconditionerList Bytes Breakdown: (0, 0, 268435456, 0, 0, 0, 0, 0)
I0316 14:43:25.272889 140472875287744 shampoo_preconditioner_list.py:381] Rank 2: AdaGradPreconditionerList Total Elements: 67108864
I0316 14:43:25.272890 140370416096448 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 10.block_0 (torch.Size([0])) for Parameter 10 (torch.Size([512, 1024])), Block block_0 (torch.Size([512, 1024])).
I0316 14:43:25.272921 140472875287744 shampoo_preconditioner_list.py:384] Rank 2: AdaGradPreconditionerList Total Bytes: 268435456
I0316 14:43:25.272956 140370416096448 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 11.block_0 (torch.Size([512])) for Parameter 11 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 14:43:25.272976 139780749235392 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_1 (torch.Size([524288, 128])) for Parameter 0 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 14:43:25.273021 140370416096448 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 12.block_0 (torch.Size([0])) for Parameter 12 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 14:43:25.273103 139780749235392 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 14:43:25.273106 140370416096448 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 13.block_0 (torch.Size([256])) for Parameter 13 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 14:43:25.273172 139780749235392 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_1 (torch.Size([0])) for Parameter 1 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 14:43:25.273187 140370416096448 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 14.block_0 (torch.Size([256])) for Parameter 14 (torch.Size([1, 256])), Block block_0 (torch.Size([256])).
I0316 14:43:25.273231 139780749235392 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 14:43:25.273277 140370416096448 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 15.block_0 (torch.Size([1])) for Parameter 15 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0316 14:43:25.273248 140249194808512 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 14:43:25.273299 139780749235392 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_1 (torch.Size([0])) for Parameter 2 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 14:43:25.273350 139780749235392 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 14:43:25.273346 140249194808512 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_1 (torch.Size([0])) for Parameter 0 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 14:43:25.273335 139906580186304 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([524288, 128])) for Parameter 2 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 14:43:25.273361 140370416096448 shampoo_preconditioner_list.py:375] Rank 7: AdaGradPreconditionerList Numel Breakdown: (0, 512, 0, 256, 0, 128, 0, 1024, 0, 1024, 0, 512, 0, 256, 256, 1)
I0316 14:43:25.273396 140370416096448 shampoo_preconditioner_list.py:378] Rank 7: AdaGradPreconditionerList Bytes Breakdown: (0, 2048, 0, 1024, 0, 512, 0, 4096, 0, 4096, 0, 2048, 0, 1024, 1024, 4)
I0316 14:43:25.273404 139780749235392 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_1 (torch.Size([0])) for Parameter 3 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 14:43:25.273404 140249194808512 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 14:43:25.273431 140370416096448 shampoo_preconditioner_list.py:381] Rank 7: AdaGradPreconditionerList Total Elements: 3969
I0316 14:43:25.273430 139906580186304 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_1 (torch.Size([0])) for Parameter 2 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 14:43:25.273450 139780749235392 shampoo_preconditioner_list.py:375] Rank 1: AdaGradPreconditionerList Numel Breakdown: (0, 67108864, 0, 0, 0, 0, 0, 0)
I0316 14:43:25.273465 140370416096448 shampoo_preconditioner_list.py:384] Rank 7: AdaGradPreconditionerList Total Bytes: 15876
I0316 14:43:25.273484 139906580186304 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 14:43:25.273497 139780749235392 shampoo_preconditioner_list.py:378] Rank 1: AdaGradPreconditionerList Bytes Breakdown: (0, 268435456, 0, 0, 0, 0, 0, 0)
I0316 14:43:25.273525 139780749235392 shampoo_preconditioner_list.py:381] Rank 1: AdaGradPreconditionerList Total Elements: 67108864
I0316 14:43:25.273544 139906580186304 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_1 (torch.Size([0])) for Parameter 3 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 14:43:25.273556 139780749235392 shampoo_preconditioner_list.py:384] Rank 1: AdaGradPreconditionerList Total Bytes: 268435456
I0316 14:43:25.273602 139906580186304 shampoo_preconditioner_list.py:375] Rank 4: AdaGradPreconditionerList Numel Breakdown: (0, 0, 0, 0, 67108864, 0, 0, 0)
I0316 14:43:25.273640 139906580186304 shampoo_preconditioner_list.py:378] Rank 4: AdaGradPreconditionerList Bytes Breakdown: (0, 0, 0, 0, 268435456, 0, 0, 0)
I0316 14:43:25.273673 139906580186304 shampoo_preconditioner_list.py:381] Rank 4: AdaGradPreconditionerList Total Elements: 67108864
I0316 14:43:25.273705 139906580186304 shampoo_preconditioner_list.py:384] Rank 4: AdaGradPreconditionerList Total Bytes: 268435456
W0316 14:43:25.273919 140370416096448 distributed_shampoo.py:359] Nesterov flag is enabled but momentum parameter is zero! Continuing without using momentum or Nesterov acceleration...
I0316 14:43:25.273983 139748225434816 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_1 (torch.Size([524288, 128])) for Parameter 2 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 14:43:25.274094 139748225434816 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 14:43:25.274157 139748225434816 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_1 (torch.Size([0])) for Parameter 3 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 14:43:25.274210 139748225434816 shampoo_preconditioner_list.py:375] Rank 5: AdaGradPreconditionerList Numel Breakdown: (0, 0, 0, 0, 0, 67108864, 0, 0)
I0316 14:43:25.274265 139748225434816 shampoo_preconditioner_list.py:378] Rank 5: AdaGradPreconditionerList Bytes Breakdown: (0, 0, 0, 0, 0, 268435456, 0, 0)
I0316 14:43:25.274261 139986877789376 submission_runner.py:279] Initializing metrics bundle.
I0316 14:43:25.274298 139748225434816 shampoo_preconditioner_list.py:381] Rank 5: AdaGradPreconditionerList Total Elements: 67108864
I0316 14:43:25.274327 139748225434816 shampoo_preconditioner_list.py:384] Rank 5: AdaGradPreconditionerList Total Bytes: 268435456
I0316 14:43:25.274476 140249194808512 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_1 (torch.Size([524288, 128])) for Parameter 1 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 14:43:25.274518 139986877789376 submission_runner.py:301] Initializing checkpoint and logger.
I0316 14:43:25.274586 140249194808512 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 14:43:25.274651 140249194808512 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_1 (torch.Size([0])) for Parameter 2 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 14:43:25.274650 140461126632640 logger_utils.py:262] Unable to record workload.train_mean information. Continuing without it.
I0316 14:43:25.274718 140249194808512 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 14:43:25.274718 140461126632640 logger_utils.py:262] Unable to record workload.train_stddev information. Continuing without it.
I0316 14:43:25.274770 140249194808512 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_1 (torch.Size([0])) for Parameter 3 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 14:43:25.274817 140249194808512 shampoo_preconditioner_list.py:375] Rank 3: AdaGradPreconditionerList Numel Breakdown: (0, 0, 0, 67108864, 0, 0, 0, 0)
I0316 14:43:25.274860 140249194808512 shampoo_preconditioner_list.py:378] Rank 3: AdaGradPreconditionerList Bytes Breakdown: (0, 0, 0, 268435456, 0, 0, 0, 0)
I0316 14:43:25.274890 140249194808512 shampoo_preconditioner_list.py:381] Rank 3: AdaGradPreconditionerList Total Elements: 67108864
I0316 14:43:25.274919 140249194808512 shampoo_preconditioner_list.py:384] Rank 3: AdaGradPreconditionerList Total Bytes: 268435456
I0316 14:43:25.274959 140472875287744 logger_utils.py:262] Unable to record workload.train_mean information. Continuing without it.
I0316 14:43:25.275026 140472875287744 logger_utils.py:262] Unable to record workload.train_stddev information. Continuing without it.
I0316 14:43:25.275029 139986877789376 submission_runner.py:321] Saving meta data to /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_2/criteo1tb_pytorch/trial_1/meta_data_0.json.
I0316 14:43:25.275225 139986877789376 logger_utils.py:262] Unable to record workload.train_mean information. Continuing without it.
I0316 14:43:25.275278 139986877789376 logger_utils.py:262] Unable to record workload.train_stddev information. Continuing without it.
I0316 14:43:25.275444 139780749235392 logger_utils.py:262] Unable to record workload.train_mean information. Continuing without it.
I0316 14:43:25.275459 139906580186304 logger_utils.py:262] Unable to record workload.train_mean information. Continuing without it.
I0316 14:43:25.275512 139780749235392 logger_utils.py:262] Unable to record workload.train_stddev information. Continuing without it.
I0316 14:43:25.275516 139906580186304 logger_utils.py:262] Unable to record workload.train_stddev information. Continuing without it.
I0316 14:43:25.275605 140370416096448 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 14:43:25.275711 140370416096448 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_1 (torch.Size([0])) for Parameter 0 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 14:43:25.275791 140370416096448 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 14:43:25.275802 139748225434816 logger_utils.py:262] Unable to record workload.train_mean information. Continuing without it.
I0316 14:43:25.275847 140370416096448 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_1 (torch.Size([0])) for Parameter 1 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 14:43:25.275870 139748225434816 logger_utils.py:262] Unable to record workload.train_stddev information. Continuing without it.
I0316 14:43:25.275902 140370416096448 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 14:43:25.275954 140370416096448 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_1 (torch.Size([0])) for Parameter 2 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 14:43:25.276006 140370416096448 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 14:43:25.276172 140249194808512 logger_utils.py:262] Unable to record workload.train_mean information. Continuing without it.
I0316 14:43:25.276239 140249194808512 logger_utils.py:262] Unable to record workload.train_stddev information. Continuing without it.
I0316 14:43:25.276493 140370416096448 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_1 (torch.Size([524288, 128])) for Parameter 3 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 14:43:25.276585 140370416096448 shampoo_preconditioner_list.py:375] Rank 7: AdaGradPreconditionerList Numel Breakdown: (0, 0, 0, 0, 0, 0, 0, 67108864)
I0316 14:43:25.276627 140370416096448 shampoo_preconditioner_list.py:378] Rank 7: AdaGradPreconditionerList Bytes Breakdown: (0, 0, 0, 0, 0, 0, 0, 268435456)
I0316 14:43:25.276663 140370416096448 shampoo_preconditioner_list.py:381] Rank 7: AdaGradPreconditionerList Total Elements: 67108864
I0316 14:43:25.276698 140370416096448 shampoo_preconditioner_list.py:384] Rank 7: AdaGradPreconditionerList Total Bytes: 268435456
I0316 14:43:25.278086 140370416096448 logger_utils.py:262] Unable to record workload.train_mean information. Continuing without it.
I0316 14:43:25.278165 140370416096448 logger_utils.py:262] Unable to record workload.train_stddev information. Continuing without it.
I0316 14:43:25.585612 139986877789376 submission_runner.py:325] Saving flags to /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_2/criteo1tb_pytorch/trial_1/flags_0.json.
I0316 14:43:25.623085 139986877789376 submission_runner.py:337] Starting training loop.
I0316 14:43:31.598814 139955293644544 logging_writer.py:48] [0] global_step=0, grad_norm=3.73213, loss=0.598146
I0316 14:43:31.615033 139986877789376 submission.py:265] 0) loss = 0.598, grad_norm = 3.732
I0316 14:43:32.051209 139986877789376 spec.py:321] Evaluating on the training split.
I0316 14:48:36.676681 139986877789376 spec.py:333] Evaluating on the validation split.
I0316 14:53:37.953604 139986877789376 spec.py:349] Evaluating on the test split.
I0316 14:59:16.467412 139986877789376 submission_runner.py:469] Time since start: 950.84s, 	Step: 1, 	{'train/loss': 0.5967804705839551, 'validation/loss': 0.5995959329565766, 'validation/num_examples': 83274637, 'test/loss': 0.5980994658922697, 'test/num_examples': 95000000, 'score': 5.9928412437438965, 'total_duration': 950.8444991111755, 'accumulated_submission_time': 5.9928412437438965, 'accumulated_eval_time': 944.4162709712982, 'accumulated_logging_time': 0}
I0316 14:59:16.475839 139944966141696 logging_writer.py:48] [1] accumulated_eval_time=944.416, accumulated_logging_time=0, accumulated_submission_time=5.99284, global_step=1, preemption_count=0, score=5.99284, test/loss=0.598099, test/num_examples=95000000, total_duration=950.844, train/loss=0.59678, validation/loss=0.599596, validation/num_examples=83274637
I0316 14:59:17.142417 139944957748992 logging_writer.py:48] [1] global_step=1, grad_norm=3.72888, loss=0.59819
I0316 14:59:17.145541 139986877789376 submission.py:265] 1) loss = 0.598, grad_norm = 3.729
I0316 14:59:17.337418 139944966141696 logging_writer.py:48] [2] global_step=2, grad_norm=3.66707, loss=0.580556
I0316 14:59:17.340517 139986877789376 submission.py:265] 2) loss = 0.581, grad_norm = 3.667
I0316 14:59:17.531275 139944957748992 logging_writer.py:48] [3] global_step=3, grad_norm=3.57851, loss=0.549852
I0316 14:59:17.534034 139986877789376 submission.py:265] 3) loss = 0.550, grad_norm = 3.579
I0316 14:59:17.724392 139944966141696 logging_writer.py:48] [4] global_step=4, grad_norm=3.42967, loss=0.507748
I0316 14:59:17.726981 139986877789376 submission.py:265] 4) loss = 0.508, grad_norm = 3.430
I0316 14:59:17.918267 139944957748992 logging_writer.py:48] [5] global_step=5, grad_norm=3.18097, loss=0.457699
I0316 14:59:17.921424 139986877789376 submission.py:265] 5) loss = 0.458, grad_norm = 3.181
I0316 14:59:18.112257 139944966141696 logging_writer.py:48] [6] global_step=6, grad_norm=2.82654, loss=0.405311
I0316 14:59:18.115522 139986877789376 submission.py:265] 6) loss = 0.405, grad_norm = 2.827
I0316 14:59:18.307800 139944957748992 logging_writer.py:48] [7] global_step=7, grad_norm=2.56329, loss=0.354439
I0316 14:59:18.310776 139986877789376 submission.py:265] 7) loss = 0.354, grad_norm = 2.563
I0316 14:59:18.502968 139944966141696 logging_writer.py:48] [8] global_step=8, grad_norm=2.23427, loss=0.302128
I0316 14:59:18.505818 139986877789376 submission.py:265] 8) loss = 0.302, grad_norm = 2.234
I0316 14:59:18.695360 139944957748992 logging_writer.py:48] [9] global_step=9, grad_norm=1.81759, loss=0.255844
I0316 14:59:18.698313 139986877789376 submission.py:265] 9) loss = 0.256, grad_norm = 1.818
I0316 14:59:18.889252 139944966141696 logging_writer.py:48] [10] global_step=10, grad_norm=1.35855, loss=0.218349
I0316 14:59:18.892122 139986877789376 submission.py:265] 10) loss = 0.218, grad_norm = 1.359
I0316 14:59:19.084654 139944957748992 logging_writer.py:48] [11] global_step=11, grad_norm=0.87472, loss=0.190922
I0316 14:59:19.087847 139986877789376 submission.py:265] 11) loss = 0.191, grad_norm = 0.875
I0316 14:59:19.288000 139944966141696 logging_writer.py:48] [12] global_step=12, grad_norm=0.437007, loss=0.172507
I0316 14:59:19.291136 139986877789376 submission.py:265] 12) loss = 0.173, grad_norm = 0.437
I0316 14:59:19.481855 139944957748992 logging_writer.py:48] [13] global_step=13, grad_norm=0.159693, loss=0.16592
I0316 14:59:19.484557 139986877789376 submission.py:265] 13) loss = 0.166, grad_norm = 0.160
I0316 14:59:19.675470 139944966141696 logging_writer.py:48] [14] global_step=14, grad_norm=0.361892, loss=0.169576
I0316 14:59:19.678411 139986877789376 submission.py:265] 14) loss = 0.170, grad_norm = 0.362
I0316 14:59:19.870692 139944957748992 logging_writer.py:48] [15] global_step=15, grad_norm=0.587242, loss=0.174789
I0316 14:59:19.874463 139986877789376 submission.py:265] 15) loss = 0.175, grad_norm = 0.587
I0316 14:59:20.066279 139944966141696 logging_writer.py:48] [16] global_step=16, grad_norm=0.774218, loss=0.184813
I0316 14:59:20.069407 139986877789376 submission.py:265] 16) loss = 0.185, grad_norm = 0.774
I0316 14:59:20.262495 139944957748992 logging_writer.py:48] [17] global_step=17, grad_norm=0.938252, loss=0.198009
I0316 14:59:20.265765 139986877789376 submission.py:265] 17) loss = 0.198, grad_norm = 0.938
I0316 14:59:20.456332 139944966141696 logging_writer.py:48] [18] global_step=18, grad_norm=1.03517, loss=0.206747
I0316 14:59:20.459358 139986877789376 submission.py:265] 18) loss = 0.207, grad_norm = 1.035
I0316 14:59:20.649998 139944957748992 logging_writer.py:48] [19] global_step=19, grad_norm=1.04259, loss=0.205181
I0316 14:59:20.652926 139986877789376 submission.py:265] 19) loss = 0.205, grad_norm = 1.043
I0316 14:59:20.840950 139944966141696 logging_writer.py:48] [20] global_step=20, grad_norm=1.09672, loss=0.210948
I0316 14:59:20.843805 139986877789376 submission.py:265] 20) loss = 0.211, grad_norm = 1.097
I0316 14:59:21.039206 139944957748992 logging_writer.py:48] [21] global_step=21, grad_norm=1.08379, loss=0.208206
I0316 14:59:21.044268 139986877789376 submission.py:265] 21) loss = 0.208, grad_norm = 1.084
I0316 14:59:21.235000 139944966141696 logging_writer.py:48] [22] global_step=22, grad_norm=1.09102, loss=0.209112
I0316 14:59:21.237967 139986877789376 submission.py:265] 22) loss = 0.209, grad_norm = 1.091
I0316 14:59:21.429099 139944957748992 logging_writer.py:48] [23] global_step=23, grad_norm=1.02559, loss=0.201395
I0316 14:59:21.432048 139986877789376 submission.py:265] 23) loss = 0.201, grad_norm = 1.026
I0316 14:59:21.623371 139944966141696 logging_writer.py:48] [24] global_step=24, grad_norm=0.966123, loss=0.19623
I0316 14:59:21.626423 139986877789376 submission.py:265] 24) loss = 0.196, grad_norm = 0.966
I0316 14:59:21.824321 139944957748992 logging_writer.py:48] [25] global_step=25, grad_norm=0.808236, loss=0.178401
I0316 14:59:21.827265 139986877789376 submission.py:265] 25) loss = 0.178, grad_norm = 0.808
I0316 14:59:22.018514 139944966141696 logging_writer.py:48] [26] global_step=26, grad_norm=0.688956, loss=0.169529
I0316 14:59:22.021590 139986877789376 submission.py:265] 26) loss = 0.170, grad_norm = 0.689
I0316 14:59:22.212919 139944957748992 logging_writer.py:48] [27] global_step=27, grad_norm=0.522725, loss=0.157788
I0316 14:59:22.216699 139986877789376 submission.py:265] 27) loss = 0.158, grad_norm = 0.523
I0316 14:59:22.406167 139944966141696 logging_writer.py:48] [28] global_step=28, grad_norm=0.365097, loss=0.153017
I0316 14:59:22.409286 139986877789376 submission.py:265] 28) loss = 0.153, grad_norm = 0.365
I0316 14:59:22.599970 139944957748992 logging_writer.py:48] [29] global_step=29, grad_norm=0.201276, loss=0.150586
I0316 14:59:22.602965 139986877789376 submission.py:265] 29) loss = 0.151, grad_norm = 0.201
I0316 14:59:22.794051 139944966141696 logging_writer.py:48] [30] global_step=30, grad_norm=0.0974177, loss=0.145945
I0316 14:59:22.812589 139986877789376 submission.py:265] 30) loss = 0.146, grad_norm = 0.097
I0316 14:59:23.172907 139944957748992 logging_writer.py:48] [31] global_step=31, grad_norm=0.155565, loss=0.146297
I0316 14:59:23.176490 139986877789376 submission.py:265] 31) loss = 0.146, grad_norm = 0.156
I0316 14:59:24.644002 139944966141696 logging_writer.py:48] [32] global_step=32, grad_norm=0.182405, loss=0.143356
I0316 14:59:24.647240 139986877789376 submission.py:265] 32) loss = 0.143, grad_norm = 0.182
I0316 14:59:25.393846 139944957748992 logging_writer.py:48] [33] global_step=33, grad_norm=0.113954, loss=0.143898
I0316 14:59:25.397510 139986877789376 submission.py:265] 33) loss = 0.144, grad_norm = 0.114
I0316 14:59:26.880468 139944966141696 logging_writer.py:48] [34] global_step=34, grad_norm=0.056047, loss=0.14126
I0316 14:59:26.883765 139986877789376 submission.py:265] 34) loss = 0.141, grad_norm = 0.056
I0316 14:59:27.503494 139944957748992 logging_writer.py:48] [35] global_step=35, grad_norm=0.0692749, loss=0.139261
I0316 14:59:27.507027 139986877789376 submission.py:265] 35) loss = 0.139, grad_norm = 0.069
I0316 14:59:29.192223 139944966141696 logging_writer.py:48] [36] global_step=36, grad_norm=0.0948965, loss=0.139249
I0316 14:59:29.195885 139986877789376 submission.py:265] 36) loss = 0.139, grad_norm = 0.095
I0316 14:59:30.225461 139944957748992 logging_writer.py:48] [37] global_step=37, grad_norm=0.0887469, loss=0.140974
I0316 14:59:30.228969 139986877789376 submission.py:265] 37) loss = 0.141, grad_norm = 0.089
I0316 14:59:31.652891 139944966141696 logging_writer.py:48] [38] global_step=38, grad_norm=0.0792355, loss=0.148539
I0316 14:59:31.656494 139986877789376 submission.py:265] 38) loss = 0.149, grad_norm = 0.079
I0316 14:59:32.712830 139944957748992 logging_writer.py:48] [39] global_step=39, grad_norm=0.0421334, loss=0.147804
I0316 14:59:32.716139 139986877789376 submission.py:265] 39) loss = 0.148, grad_norm = 0.042
I0316 14:59:34.013727 139944966141696 logging_writer.py:48] [40] global_step=40, grad_norm=0.0337125, loss=0.148228
I0316 14:59:34.017244 139986877789376 submission.py:265] 40) loss = 0.148, grad_norm = 0.034
I0316 14:59:35.015910 139944957748992 logging_writer.py:48] [41] global_step=41, grad_norm=0.032454, loss=0.147059
I0316 14:59:35.019507 139986877789376 submission.py:265] 41) loss = 0.147, grad_norm = 0.032
I0316 14:59:36.702600 139944966141696 logging_writer.py:48] [42] global_step=42, grad_norm=0.036533, loss=0.147519
I0316 14:59:36.705961 139986877789376 submission.py:265] 42) loss = 0.148, grad_norm = 0.037
I0316 14:59:37.211051 139944957748992 logging_writer.py:48] [43] global_step=43, grad_norm=0.0281968, loss=0.146182
I0316 14:59:37.214485 139986877789376 submission.py:265] 43) loss = 0.146, grad_norm = 0.028
I0316 14:59:38.876054 139944966141696 logging_writer.py:48] [44] global_step=44, grad_norm=0.0500485, loss=0.149134
I0316 14:59:38.879886 139986877789376 submission.py:265] 44) loss = 0.149, grad_norm = 0.050
I0316 14:59:40.007166 139944957748992 logging_writer.py:48] [45] global_step=45, grad_norm=0.0492873, loss=0.147278
I0316 14:59:40.010808 139986877789376 submission.py:265] 45) loss = 0.147, grad_norm = 0.049
I0316 14:59:41.225878 139944966141696 logging_writer.py:48] [46] global_step=46, grad_norm=0.0393334, loss=0.147532
I0316 14:59:41.229213 139986877789376 submission.py:265] 46) loss = 0.148, grad_norm = 0.039
I0316 14:59:42.592403 139944957748992 logging_writer.py:48] [47] global_step=47, grad_norm=0.0375796, loss=0.145333
I0316 14:59:42.595908 139986877789376 submission.py:265] 47) loss = 0.145, grad_norm = 0.038
I0316 14:59:43.355521 139944966141696 logging_writer.py:48] [48] global_step=48, grad_norm=0.0438939, loss=0.145242
I0316 14:59:43.358864 139986877789376 submission.py:265] 48) loss = 0.145, grad_norm = 0.044
I0316 14:59:44.904932 139944957748992 logging_writer.py:48] [49] global_step=49, grad_norm=0.0397541, loss=0.144493
I0316 14:59:44.908035 139986877789376 submission.py:265] 49) loss = 0.144, grad_norm = 0.040
I0316 14:59:45.911222 139944966141696 logging_writer.py:48] [50] global_step=50, grad_norm=0.036877, loss=0.143507
I0316 14:59:45.914192 139986877789376 submission.py:265] 50) loss = 0.144, grad_norm = 0.037
I0316 14:59:46.943799 139944957748992 logging_writer.py:48] [51] global_step=51, grad_norm=0.0478082, loss=0.14063
I0316 14:59:46.946815 139986877789376 submission.py:265] 51) loss = 0.141, grad_norm = 0.048
I0316 14:59:48.005939 139944966141696 logging_writer.py:48] [52] global_step=52, grad_norm=0.0631549, loss=0.14036
I0316 14:59:48.008880 139986877789376 submission.py:265] 52) loss = 0.140, grad_norm = 0.063
I0316 14:59:49.104779 139944957748992 logging_writer.py:48] [53] global_step=53, grad_norm=0.0349658, loss=0.1455
I0316 14:59:49.108069 139986877789376 submission.py:265] 53) loss = 0.146, grad_norm = 0.035
I0316 14:59:50.431950 139944966141696 logging_writer.py:48] [54] global_step=54, grad_norm=0.0140177, loss=0.14169
I0316 14:59:50.434931 139986877789376 submission.py:265] 54) loss = 0.142, grad_norm = 0.014
I0316 14:59:51.321277 139944957748992 logging_writer.py:48] [55] global_step=55, grad_norm=0.0410012, loss=0.145182
I0316 14:59:51.324342 139986877789376 submission.py:265] 55) loss = 0.145, grad_norm = 0.041
I0316 14:59:52.628355 139944966141696 logging_writer.py:48] [56] global_step=56, grad_norm=0.089178, loss=0.142691
I0316 14:59:52.631222 139986877789376 submission.py:265] 56) loss = 0.143, grad_norm = 0.089
I0316 14:59:53.535735 139944957748992 logging_writer.py:48] [57] global_step=57, grad_norm=0.146172, loss=0.145621
I0316 14:59:53.538898 139986877789376 submission.py:265] 57) loss = 0.146, grad_norm = 0.146
I0316 14:59:54.666444 139944966141696 logging_writer.py:48] [58] global_step=58, grad_norm=0.167476, loss=0.145667
I0316 14:59:54.669398 139986877789376 submission.py:265] 58) loss = 0.146, grad_norm = 0.167
I0316 14:59:55.706967 139944957748992 logging_writer.py:48] [59] global_step=59, grad_norm=0.129928, loss=0.144658
I0316 14:59:55.710095 139986877789376 submission.py:265] 59) loss = 0.145, grad_norm = 0.130
I0316 14:59:56.823370 139944966141696 logging_writer.py:48] [60] global_step=60, grad_norm=0.0799874, loss=0.142673
I0316 14:59:56.826364 139986877789376 submission.py:265] 60) loss = 0.143, grad_norm = 0.080
I0316 14:59:57.848389 139944957748992 logging_writer.py:48] [61] global_step=61, grad_norm=0.0559341, loss=0.142284
I0316 14:59:57.851492 139986877789376 submission.py:265] 61) loss = 0.142, grad_norm = 0.056
I0316 14:59:59.113156 139944966141696 logging_writer.py:48] [62] global_step=62, grad_norm=0.0406833, loss=0.140629
I0316 14:59:59.116364 139986877789376 submission.py:265] 62) loss = 0.141, grad_norm = 0.041
I0316 15:00:00.074940 139944957748992 logging_writer.py:48] [63] global_step=63, grad_norm=0.0565222, loss=0.142013
I0316 15:00:00.078045 139986877789376 submission.py:265] 63) loss = 0.142, grad_norm = 0.057
I0316 15:00:01.248536 139944966141696 logging_writer.py:48] [64] global_step=64, grad_norm=0.0562595, loss=0.140677
I0316 15:00:01.251640 139986877789376 submission.py:265] 64) loss = 0.141, grad_norm = 0.056
I0316 15:00:02.085210 139944957748992 logging_writer.py:48] [65] global_step=65, grad_norm=0.077023, loss=0.142983
I0316 15:00:02.088305 139986877789376 submission.py:265] 65) loss = 0.143, grad_norm = 0.077
I0316 15:00:03.166467 139944966141696 logging_writer.py:48] [66] global_step=66, grad_norm=0.109994, loss=0.141346
I0316 15:00:03.169359 139986877789376 submission.py:265] 66) loss = 0.141, grad_norm = 0.110
I0316 15:00:04.287272 139944957748992 logging_writer.py:48] [67] global_step=67, grad_norm=0.112447, loss=0.139955
I0316 15:00:04.290368 139986877789376 submission.py:265] 67) loss = 0.140, grad_norm = 0.112
I0316 15:00:05.324962 139944966141696 logging_writer.py:48] [68] global_step=68, grad_norm=0.0909333, loss=0.141704
I0316 15:00:05.328125 139986877789376 submission.py:265] 68) loss = 0.142, grad_norm = 0.091
I0316 15:00:05.952383 139944957748992 logging_writer.py:48] [69] global_step=69, grad_norm=0.0654115, loss=0.139956
I0316 15:00:05.956232 139986877789376 submission.py:265] 69) loss = 0.140, grad_norm = 0.065
I0316 15:00:07.621078 139944966141696 logging_writer.py:48] [70] global_step=70, grad_norm=0.0675258, loss=0.140773
I0316 15:00:07.624318 139986877789376 submission.py:265] 70) loss = 0.141, grad_norm = 0.068
I0316 15:00:08.269591 139944957748992 logging_writer.py:48] [71] global_step=71, grad_norm=0.0799004, loss=0.141367
I0316 15:00:08.272679 139986877789376 submission.py:265] 71) loss = 0.141, grad_norm = 0.080
I0316 15:00:10.248047 139944966141696 logging_writer.py:48] [72] global_step=72, grad_norm=0.0994155, loss=0.141918
I0316 15:00:10.251051 139986877789376 submission.py:265] 72) loss = 0.142, grad_norm = 0.099
I0316 15:00:11.233973 139944957748992 logging_writer.py:48] [73] global_step=73, grad_norm=0.0812541, loss=0.137068
I0316 15:00:11.236782 139986877789376 submission.py:265] 73) loss = 0.137, grad_norm = 0.081
I0316 15:00:12.947635 139944966141696 logging_writer.py:48] [74] global_step=74, grad_norm=0.0661766, loss=0.137434
I0316 15:00:12.950676 139986877789376 submission.py:265] 74) loss = 0.137, grad_norm = 0.066
I0316 15:00:13.714004 139944957748992 logging_writer.py:48] [75] global_step=75, grad_norm=0.0510953, loss=0.135105
I0316 15:00:13.716925 139986877789376 submission.py:265] 75) loss = 0.135, grad_norm = 0.051
I0316 15:00:15.482363 139944966141696 logging_writer.py:48] [76] global_step=76, grad_norm=0.0756754, loss=0.131823
I0316 15:00:15.485445 139986877789376 submission.py:265] 76) loss = 0.132, grad_norm = 0.076
I0316 15:00:16.172221 139944957748992 logging_writer.py:48] [77] global_step=77, grad_norm=0.0695447, loss=0.128009
I0316 15:00:16.175384 139986877789376 submission.py:265] 77) loss = 0.128, grad_norm = 0.070
I0316 15:00:18.073023 139944966141696 logging_writer.py:48] [78] global_step=78, grad_norm=0.0206022, loss=0.12943
I0316 15:00:18.076104 139986877789376 submission.py:265] 78) loss = 0.129, grad_norm = 0.021
I0316 15:00:18.992321 139944957748992 logging_writer.py:48] [79] global_step=79, grad_norm=0.0106164, loss=0.128225
I0316 15:00:18.995820 139986877789376 submission.py:265] 79) loss = 0.128, grad_norm = 0.011
I0316 15:00:20.761287 139944966141696 logging_writer.py:48] [80] global_step=80, grad_norm=0.00824707, loss=0.126929
I0316 15:00:20.764238 139986877789376 submission.py:265] 80) loss = 0.127, grad_norm = 0.008
I0316 15:00:21.703901 139944957748992 logging_writer.py:48] [81] global_step=81, grad_norm=0.0176837, loss=0.127236
I0316 15:00:21.706828 139986877789376 submission.py:265] 81) loss = 0.127, grad_norm = 0.018
I0316 15:00:22.885790 139944966141696 logging_writer.py:48] [82] global_step=82, grad_norm=0.0143806, loss=0.12876
I0316 15:00:22.888693 139986877789376 submission.py:265] 82) loss = 0.129, grad_norm = 0.014
I0316 15:00:24.088194 139944957748992 logging_writer.py:48] [83] global_step=83, grad_norm=0.0156857, loss=0.128368
I0316 15:00:24.091277 139986877789376 submission.py:265] 83) loss = 0.128, grad_norm = 0.016
I0316 15:00:25.321095 139944966141696 logging_writer.py:48] [84] global_step=84, grad_norm=0.0190299, loss=0.127669
I0316 15:00:25.324022 139986877789376 submission.py:265] 84) loss = 0.128, grad_norm = 0.019
I0316 15:00:25.884134 139944957748992 logging_writer.py:48] [85] global_step=85, grad_norm=0.0158378, loss=0.125405
I0316 15:00:25.887750 139986877789376 submission.py:265] 85) loss = 0.125, grad_norm = 0.016
I0316 15:00:27.248688 139944966141696 logging_writer.py:48] [86] global_step=86, grad_norm=0.0130107, loss=0.127831
I0316 15:00:27.251809 139986877789376 submission.py:265] 86) loss = 0.128, grad_norm = 0.013
I0316 15:00:28.648666 139944957748992 logging_writer.py:48] [87] global_step=87, grad_norm=0.0315473, loss=0.125994
I0316 15:00:28.651518 139986877789376 submission.py:265] 87) loss = 0.126, grad_norm = 0.032
I0316 15:00:29.637705 139944966141696 logging_writer.py:48] [88] global_step=88, grad_norm=0.0483837, loss=0.125612
I0316 15:00:29.640608 139986877789376 submission.py:265] 88) loss = 0.126, grad_norm = 0.048
I0316 15:00:30.673083 139944957748992 logging_writer.py:48] [89] global_step=89, grad_norm=0.062897, loss=0.127272
I0316 15:00:30.676183 139986877789376 submission.py:265] 89) loss = 0.127, grad_norm = 0.063
I0316 15:00:31.752701 139944966141696 logging_writer.py:48] [90] global_step=90, grad_norm=0.0544267, loss=0.125899
I0316 15:00:31.755721 139986877789376 submission.py:265] 90) loss = 0.126, grad_norm = 0.054
I0316 15:00:32.969657 139944957748992 logging_writer.py:48] [91] global_step=91, grad_norm=0.0524801, loss=0.126045
I0316 15:00:32.972630 139986877789376 submission.py:265] 91) loss = 0.126, grad_norm = 0.052
I0316 15:00:34.065936 139944966141696 logging_writer.py:48] [92] global_step=92, grad_norm=0.0546334, loss=0.125813
I0316 15:00:34.068899 139986877789376 submission.py:265] 92) loss = 0.126, grad_norm = 0.055
I0316 15:00:35.205201 139944957748992 logging_writer.py:48] [93] global_step=93, grad_norm=0.0413125, loss=0.127207
I0316 15:00:35.208088 139986877789376 submission.py:265] 93) loss = 0.127, grad_norm = 0.041
I0316 15:00:36.513271 139944966141696 logging_writer.py:48] [94] global_step=94, grad_norm=0.0210016, loss=0.125504
I0316 15:00:36.516844 139986877789376 submission.py:265] 94) loss = 0.126, grad_norm = 0.021
I0316 15:00:38.322229 139944957748992 logging_writer.py:48] [95] global_step=95, grad_norm=0.00651115, loss=0.126017
I0316 15:00:38.325171 139986877789376 submission.py:265] 95) loss = 0.126, grad_norm = 0.007
I0316 15:00:38.778250 139944966141696 logging_writer.py:48] [96] global_step=96, grad_norm=0.00818661, loss=0.125214
I0316 15:00:38.781262 139986877789376 submission.py:265] 96) loss = 0.125, grad_norm = 0.008
I0316 15:00:40.565318 139944957748992 logging_writer.py:48] [97] global_step=97, grad_norm=0.00710855, loss=0.123693
I0316 15:00:40.568255 139986877789376 submission.py:265] 97) loss = 0.124, grad_norm = 0.007
I0316 15:00:42.222455 139944966141696 logging_writer.py:48] [98] global_step=98, grad_norm=0.00681889, loss=0.124404
I0316 15:00:42.225577 139986877789376 submission.py:265] 98) loss = 0.124, grad_norm = 0.007
I0316 15:00:44.379219 139944957748992 logging_writer.py:48] [99] global_step=99, grad_norm=0.00747732, loss=0.123919
I0316 15:00:44.382474 139986877789376 submission.py:265] 99) loss = 0.124, grad_norm = 0.007
I0316 15:00:45.643827 139944966141696 logging_writer.py:48] [100] global_step=100, grad_norm=0.0101537, loss=0.127609
I0316 15:00:45.647686 139986877789376 submission.py:265] 100) loss = 0.128, grad_norm = 0.010
I0316 15:01:17.864032 139986877789376 spec.py:321] Evaluating on the training split.
I0316 15:06:26.684559 139986877789376 spec.py:333] Evaluating on the validation split.
I0316 15:10:44.600499 139986877789376 spec.py:349] Evaluating on the test split.
I0316 15:15:43.109408 139986877789376 submission_runner.py:469] Time since start: 1937.49s, 	Step: 121, 	{'train/loss': 0.12929045388498148, 'validation/loss': 0.12952914730248755, 'validation/num_examples': 83274637, 'test/loss': 0.13216064602251554, 'test/num_examples': 95000000, 'score': 126.44877076148987, 'total_duration': 1937.4865198135376, 'accumulated_submission_time': 126.44877076148987, 'accumulated_eval_time': 1809.6618392467499, 'accumulated_logging_time': 0.015914440155029297}
I0316 15:15:43.119964 139944957748992 logging_writer.py:48] [121] accumulated_eval_time=1809.66, accumulated_logging_time=0.0159144, accumulated_submission_time=126.449, global_step=121, preemption_count=0, score=126.449, test/loss=0.132161, test/num_examples=95000000, total_duration=1937.49, train/loss=0.12929, validation/loss=0.129529, validation/num_examples=83274637
/usr/local/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
I0316 15:17:43.804688 139986877789376 spec.py:321] Evaluating on the training split.
I0316 15:22:56.529162 139986877789376 spec.py:333] Evaluating on the validation split.
I0316 15:27:14.713142 139986877789376 spec.py:349] Evaluating on the test split.
I0316 15:32:11.350093 139986877789376 submission_runner.py:469] Time since start: 2925.73s, 	Step: 239, 	{'train/loss': 0.12654524661543606, 'validation/loss': 0.12861747392733178, 'validation/num_examples': 83274637, 'test/loss': 0.13079545356654118, 'test/num_examples': 95000000, 'score': 246.1599748134613, 'total_duration': 2925.727224111557, 'accumulated_submission_time': 246.1599748134613, 'accumulated_eval_time': 2677.2073838710785, 'accumulated_logging_time': 0.03339338302612305}
I0316 15:32:11.360841 139944966141696 logging_writer.py:48] [239] accumulated_eval_time=2677.21, accumulated_logging_time=0.0333934, accumulated_submission_time=246.16, global_step=239, preemption_count=0, score=246.16, test/loss=0.130795, test/num_examples=95000000, total_duration=2925.73, train/loss=0.126545, validation/loss=0.128617, validation/num_examples=83274637
I0316 15:34:12.218685 139986877789376 spec.py:321] Evaluating on the training split.
I0316 15:39:35.001197 139986877789376 spec.py:333] Evaluating on the validation split.
I0316 15:43:57.393599 139986877789376 spec.py:349] Evaluating on the test split.
I0316 15:49:06.336579 139986877789376 submission_runner.py:469] Time since start: 3940.71s, 	Step: 359, 	{'train/loss': 0.1261007231785237, 'validation/loss': 0.1272863995107928, 'validation/num_examples': 83274637, 'test/loss': 0.12953205299983778, 'test/num_examples': 95000000, 'score': 366.06922721862793, 'total_duration': 3940.7137293815613, 'accumulated_submission_time': 366.06922721862793, 'accumulated_eval_time': 3571.325374364853, 'accumulated_logging_time': 0.05113697052001953}
I0316 15:49:06.346944 139944957748992 logging_writer.py:48] [359] accumulated_eval_time=3571.33, accumulated_logging_time=0.051137, accumulated_submission_time=366.069, global_step=359, preemption_count=0, score=366.069, test/loss=0.129532, test/num_examples=95000000, total_duration=3940.71, train/loss=0.126101, validation/loss=0.127286, validation/num_examples=83274637
I0316 15:51:07.547302 139986877789376 spec.py:321] Evaluating on the training split.
I0316 15:56:25.023219 139986877789376 spec.py:333] Evaluating on the validation split.
I0316 16:00:54.785161 139986877789376 spec.py:349] Evaluating on the test split.
I0316 16:06:04.265617 139986877789376 submission_runner.py:469] Time since start: 4958.64s, 	Step: 483, 	{'train/loss': 0.12557340696457348, 'validation/loss': 0.12669867375029917, 'validation/num_examples': 83274637, 'test/loss': 0.12897471061553956, 'test/num_examples': 95000000, 'score': 486.29980754852295, 'total_duration': 4958.6427545547485, 'accumulated_submission_time': 486.29980754852295, 'accumulated_eval_time': 4468.043854236603, 'accumulated_logging_time': 0.10363411903381348}
I0316 16:06:04.276214 139944966141696 logging_writer.py:48] [483] accumulated_eval_time=4468.04, accumulated_logging_time=0.103634, accumulated_submission_time=486.3, global_step=483, preemption_count=0, score=486.3, test/loss=0.128975, test/num_examples=95000000, total_duration=4958.64, train/loss=0.125573, validation/loss=0.126699, validation/num_examples=83274637
I0316 16:06:08.144315 139944957748992 logging_writer.py:48] [500] global_step=500, grad_norm=0.0558567, loss=0.132493
I0316 16:06:08.147688 139986877789376 submission.py:265] 500) loss = 0.132, grad_norm = 0.056
I0316 16:08:05.449526 139986877789376 spec.py:321] Evaluating on the training split.
I0316 16:13:18.095798 139986877789376 spec.py:333] Evaluating on the validation split.
I0316 16:17:39.049237 139986877789376 spec.py:349] Evaluating on the test split.
I0316 16:22:37.765468 139986877789376 submission_runner.py:469] Time since start: 5952.14s, 	Step: 607, 	{'train/loss': 0.12718614765711148, 'validation/loss': 0.12779377596865843, 'validation/num_examples': 83274637, 'test/loss': 0.12981382291191504, 'test/num_examples': 95000000, 'score': 606.6448118686676, 'total_duration': 5952.142598867416, 'accumulated_submission_time': 606.6448118686676, 'accumulated_eval_time': 5340.359853506088, 'accumulated_logging_time': 0.12112855911254883}
I0316 16:22:37.775012 139944966141696 logging_writer.py:48] [607] accumulated_eval_time=5340.36, accumulated_logging_time=0.121129, accumulated_submission_time=606.645, global_step=607, preemption_count=0, score=606.645, test/loss=0.129814, test/num_examples=95000000, total_duration=5952.14, train/loss=0.127186, validation/loss=0.127794, validation/num_examples=83274637
I0316 16:24:38.577915 139986877789376 spec.py:321] Evaluating on the training split.
I0316 16:29:53.120742 139986877789376 spec.py:333] Evaluating on the validation split.
I0316 16:34:06.416385 139986877789376 spec.py:349] Evaluating on the test split.
I0316 16:39:04.438360 139986877789376 submission_runner.py:469] Time since start: 6938.82s, 	Step: 731, 	{'train/loss': 0.124876698293085, 'validation/loss': 0.1262972372140991, 'validation/num_examples': 83274637, 'test/loss': 0.12854859651312578, 'test/num_examples': 95000000, 'score': 726.5362474918365, 'total_duration': 6938.815535068512, 'accumulated_submission_time': 726.5362474918365, 'accumulated_eval_time': 6206.2204513549805, 'accumulated_logging_time': 0.13717937469482422}
I0316 16:39:04.448204 139944957748992 logging_writer.py:48] [731] accumulated_eval_time=6206.22, accumulated_logging_time=0.137179, accumulated_submission_time=726.536, global_step=731, preemption_count=0, score=726.536, test/loss=0.128549, test/num_examples=95000000, total_duration=6938.82, train/loss=0.124877, validation/loss=0.126297, validation/num_examples=83274637
I0316 16:41:05.407104 139986877789376 spec.py:321] Evaluating on the training split.
I0316 16:46:16.602771 139986877789376 spec.py:333] Evaluating on the validation split.
I0316 16:50:33.635882 139986877789376 spec.py:349] Evaluating on the test split.
I0316 16:55:37.094972 139986877789376 submission_runner.py:469] Time since start: 7931.47s, 	Step: 854, 	{'train/loss': 0.1258747030588971, 'validation/loss': 0.12625315455303676, 'validation/num_examples': 83274637, 'test/loss': 0.1285452047476116, 'test/num_examples': 95000000, 'score': 846.6516311168671, 'total_duration': 7931.472109079361, 'accumulated_submission_time': 846.6516311168671, 'accumulated_eval_time': 7077.90834236145, 'accumulated_logging_time': 0.15391778945922852}
I0316 16:55:37.105902 139944966141696 logging_writer.py:48] [854] accumulated_eval_time=7077.91, accumulated_logging_time=0.153918, accumulated_submission_time=846.652, global_step=854, preemption_count=0, score=846.652, test/loss=0.128545, test/num_examples=95000000, total_duration=7931.47, train/loss=0.125875, validation/loss=0.126253, validation/num_examples=83274637
I0316 16:57:37.571650 139986877789376 spec.py:321] Evaluating on the training split.
I0316 17:02:35.482760 139986877789376 spec.py:333] Evaluating on the validation split.
I0316 17:06:48.643131 139986877789376 spec.py:349] Evaluating on the test split.
I0316 17:12:02.281830 139986877789376 submission_runner.py:469] Time since start: 8916.66s, 	Step: 979, 	{'train/loss': 0.12760293394518266, 'validation/loss': 0.12571204081501033, 'validation/num_examples': 83274637, 'test/loss': 0.12815281998282985, 'test/num_examples': 95000000, 'score': 966.311705827713, 'total_duration': 8916.659004926682, 'accumulated_submission_time': 966.311705827713, 'accumulated_eval_time': 7942.618627786636, 'accumulated_logging_time': 0.1713721752166748}
I0316 17:12:02.291484 139944957748992 logging_writer.py:48] [979] accumulated_eval_time=7942.62, accumulated_logging_time=0.171372, accumulated_submission_time=966.312, global_step=979, preemption_count=0, score=966.312, test/loss=0.128153, test/num_examples=95000000, total_duration=8916.66, train/loss=0.127603, validation/loss=0.125712, validation/num_examples=83274637
I0316 17:12:06.879231 139944966141696 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.034089, loss=0.131249
I0316 17:12:06.882490 139986877789376 submission.py:265] 1000) loss = 0.131, grad_norm = 0.034
I0316 17:14:03.276273 139986877789376 spec.py:321] Evaluating on the training split.
I0316 17:18:43.472617 139986877789376 spec.py:333] Evaluating on the validation split.
I0316 17:22:38.370476 139986877789376 spec.py:349] Evaluating on the test split.
I0316 17:27:26.182489 139986877789376 submission_runner.py:469] Time since start: 9840.56s, 	Step: 1101, 	{'train/loss': 0.12254381910809845, 'validation/loss': 0.12614165408277692, 'validation/num_examples': 83274637, 'test/loss': 0.12845067598716334, 'test/num_examples': 95000000, 'score': 1086.5382115840912, 'total_duration': 9840.559624433517, 'accumulated_submission_time': 1086.5382115840912, 'accumulated_eval_time': 8745.524825811386, 'accumulated_logging_time': 0.18803715705871582}
I0316 17:27:26.193598 139944957748992 logging_writer.py:48] [1101] accumulated_eval_time=8745.52, accumulated_logging_time=0.188037, accumulated_submission_time=1086.54, global_step=1101, preemption_count=0, score=1086.54, test/loss=0.128451, test/num_examples=95000000, total_duration=9840.56, train/loss=0.122544, validation/loss=0.126142, validation/num_examples=83274637
I0316 17:29:27.458126 139986877789376 spec.py:321] Evaluating on the training split.
I0316 17:33:36.785372 139986877789376 spec.py:333] Evaluating on the validation split.
I0316 17:37:14.095588 139986877789376 spec.py:349] Evaluating on the test split.
I0316 17:41:42.193107 139986877789376 submission_runner.py:469] Time since start: 10696.57s, 	Step: 1225, 	{'train/loss': 0.12351775492894013, 'validation/loss': 0.12536619297428597, 'validation/num_examples': 83274637, 'test/loss': 0.12767585662319786, 'test/num_examples': 95000000, 'score': 1206.948977470398, 'total_duration': 10696.570142507553, 'accumulated_submission_time': 1206.948977470398, 'accumulated_eval_time': 9480.259698152542, 'accumulated_logging_time': 0.2496354579925537}
I0316 17:41:42.202339 139944966141696 logging_writer.py:48] [1225] accumulated_eval_time=9480.26, accumulated_logging_time=0.249635, accumulated_submission_time=1206.95, global_step=1225, preemption_count=0, score=1206.95, test/loss=0.127676, test/num_examples=95000000, total_duration=10696.6, train/loss=0.123518, validation/loss=0.125366, validation/num_examples=83274637
I0316 17:43:43.369881 139986877789376 spec.py:321] Evaluating on the training split.
I0316 17:46:43.135404 139986877789376 spec.py:333] Evaluating on the validation split.
I0316 17:49:32.772709 139986877789376 spec.py:349] Evaluating on the test split.
I0316 17:54:02.159811 139986877789376 submission_runner.py:469] Time since start: 11436.54s, 	Step: 1349, 	{'train/loss': 0.12562536856864243, 'validation/loss': 0.1254817203281314, 'validation/num_examples': 83274637, 'test/loss': 0.1277385695533351, 'test/num_examples': 95000000, 'score': 1327.182540655136, 'total_duration': 11436.536950588226, 'accumulated_submission_time': 1327.182540655136, 'accumulated_eval_time': 10099.049802780151, 'accumulated_logging_time': 0.26505446434020996}
I0316 17:54:02.169635 139944957748992 logging_writer.py:48] [1349] accumulated_eval_time=10099, accumulated_logging_time=0.265054, accumulated_submission_time=1327.18, global_step=1349, preemption_count=0, score=1327.18, test/loss=0.127739, test/num_examples=95000000, total_duration=11436.5, train/loss=0.125625, validation/loss=0.125482, validation/num_examples=83274637
I0316 17:56:03.038944 139986877789376 spec.py:321] Evaluating on the training split.
I0316 17:58:05.758066 139986877789376 spec.py:333] Evaluating on the validation split.
I0316 18:00:09.671176 139986877789376 spec.py:349] Evaluating on the test split.
I0316 18:03:39.730639 139986877789376 submission_runner.py:469] Time since start: 12014.11s, 	Step: 1474, 	{'train/loss': 0.12529670949540694, 'validation/loss': 0.1257376246198763, 'validation/num_examples': 83274637, 'test/loss': 0.12811998187649376, 'test/num_examples': 95000000, 'score': 1447.1283748149872, 'total_duration': 12014.10776591301, 'accumulated_submission_time': 1447.1283748149872, 'accumulated_eval_time': 10555.74162864685, 'accumulated_logging_time': 0.2825963497161865}
I0316 18:03:39.740953 139944966141696 logging_writer.py:48] [1474] accumulated_eval_time=10555.7, accumulated_logging_time=0.282596, accumulated_submission_time=1447.13, global_step=1474, preemption_count=0, score=1447.13, test/loss=0.12812, test/num_examples=95000000, total_duration=12014.1, train/loss=0.125297, validation/loss=0.125738, validation/num_examples=83274637
I0316 18:03:45.334928 139944957748992 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.00611359, loss=0.112982
I0316 18:03:45.338639 139986877789376 submission.py:265] 1500) loss = 0.113, grad_norm = 0.006
I0316 18:05:41.084172 139986877789376 spec.py:321] Evaluating on the training split.
I0316 18:07:47.357838 139986877789376 spec.py:333] Evaluating on the validation split.
I0316 18:09:51.378941 139986877789376 spec.py:349] Evaluating on the test split.
I0316 18:14:44.134487 139986877789376 submission_runner.py:469] Time since start: 12678.51s, 	Step: 1600, 	{'train/loss': 0.1261838787527927, 'validation/loss': 0.12585323729033052, 'validation/num_examples': 83274637, 'test/loss': 0.12829686543450605, 'test/num_examples': 95000000, 'score': 1567.5214393138885, 'total_duration': 12678.511667251587, 'accumulated_submission_time': 1567.5214393138885, 'accumulated_eval_time': 11098.792139530182, 'accumulated_logging_time': 0.299191951751709}
I0316 18:14:44.144526 139944966141696 logging_writer.py:48] [1600] accumulated_eval_time=11098.8, accumulated_logging_time=0.299192, accumulated_submission_time=1567.52, global_step=1600, preemption_count=0, score=1567.52, test/loss=0.128297, test/num_examples=95000000, total_duration=12678.5, train/loss=0.126184, validation/loss=0.125853, validation/num_examples=83274637
I0316 18:16:45.521971 139986877789376 spec.py:321] Evaluating on the training split.
I0316 18:19:02.827651 139986877789376 spec.py:333] Evaluating on the validation split.
I0316 18:21:07.022356 139986877789376 spec.py:349] Evaluating on the test split.
I0316 18:24:24.324308 139986877789376 submission_runner.py:469] Time since start: 13258.70s, 	Step: 1725, 	{'train/loss': 0.12455017461961627, 'validation/loss': 0.12511478402270426, 'validation/num_examples': 83274637, 'test/loss': 0.12751124952356438, 'test/num_examples': 95000000, 'score': 1687.9676945209503, 'total_duration': 13258.701464891434, 'accumulated_submission_time': 1687.9676945209503, 'accumulated_eval_time': 11557.594665765762, 'accumulated_logging_time': 0.3350973129272461}
I0316 18:24:24.333461 139944957748992 logging_writer.py:48] [1725] accumulated_eval_time=11557.6, accumulated_logging_time=0.335097, accumulated_submission_time=1687.97, global_step=1725, preemption_count=0, score=1687.97, test/loss=0.127511, test/num_examples=95000000, total_duration=13258.7, train/loss=0.12455, validation/loss=0.125115, validation/num_examples=83274637
I0316 18:26:25.488314 139986877789376 spec.py:321] Evaluating on the training split.
I0316 18:29:27.209087 139986877789376 spec.py:333] Evaluating on the validation split.
I0316 18:31:31.299223 139986877789376 spec.py:349] Evaluating on the test split.
I0316 18:35:35.634786 139986877789376 submission_runner.py:469] Time since start: 13930.01s, 	Step: 1849, 	{'train/loss': 0.12282904865986612, 'validation/loss': 0.1254858090437514, 'validation/num_examples': 83274637, 'test/loss': 0.1279028516106455, 'test/num_examples': 95000000, 'score': 1808.1840012073517, 'total_duration': 13930.011937141418, 'accumulated_submission_time': 1808.1840012073517, 'accumulated_eval_time': 12107.741386175156, 'accumulated_logging_time': 0.35057520866394043}
I0316 18:35:35.644508 139944966141696 logging_writer.py:48] [1849] accumulated_eval_time=12107.7, accumulated_logging_time=0.350575, accumulated_submission_time=1808.18, global_step=1849, preemption_count=0, score=1808.18, test/loss=0.127903, test/num_examples=95000000, total_duration=13930, train/loss=0.122829, validation/loss=0.125486, validation/num_examples=83274637
I0316 18:37:36.712194 139986877789376 spec.py:321] Evaluating on the training split.
I0316 18:40:32.477443 139986877789376 spec.py:333] Evaluating on the validation split.
I0316 18:42:37.082516 139986877789376 spec.py:349] Evaluating on the test split.
I0316 18:46:58.773410 139986877789376 submission_runner.py:469] Time since start: 14613.15s, 	Step: 1973, 	{'train/loss': 0.12423397589964305, 'validation/loss': 0.12527993945969187, 'validation/num_examples': 83274637, 'test/loss': 0.12751773589630128, 'test/num_examples': 95000000, 'score': 1928.320625782013, 'total_duration': 14613.150525808334, 'accumulated_submission_time': 1928.320625782013, 'accumulated_eval_time': 12669.802761793137, 'accumulated_logging_time': 0.36676549911499023}
I0316 18:46:58.784858 139944957748992 logging_writer.py:48] [1973] accumulated_eval_time=12669.8, accumulated_logging_time=0.366765, accumulated_submission_time=1928.32, global_step=1973, preemption_count=0, score=1928.32, test/loss=0.127518, test/num_examples=95000000, total_duration=14613.2, train/loss=0.124234, validation/loss=0.12528, validation/num_examples=83274637
I0316 18:47:04.587523 139944966141696 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.010076, loss=0.124685
I0316 18:47:04.590877 139986877789376 submission.py:265] 2000) loss = 0.125, grad_norm = 0.010
I0316 18:48:59.612526 139986877789376 spec.py:321] Evaluating on the training split.
I0316 18:51:32.917313 139986877789376 spec.py:333] Evaluating on the validation split.
I0316 18:53:37.815193 139986877789376 spec.py:349] Evaluating on the test split.
I0316 18:57:45.183908 139986877789376 submission_runner.py:469] Time since start: 15259.56s, 	Step: 2100, 	{'train/loss': 0.12506163210584978, 'validation/loss': 0.12515588373828967, 'validation/num_examples': 83274637, 'test/loss': 0.1274141676658229, 'test/num_examples': 95000000, 'score': 2048.278680086136, 'total_duration': 15259.561057567596, 'accumulated_submission_time': 2048.278680086136, 'accumulated_eval_time': 13195.374210357666, 'accumulated_logging_time': 0.3855879306793213}
I0316 18:57:45.193187 139944957748992 logging_writer.py:48] [2100] accumulated_eval_time=13195.4, accumulated_logging_time=0.385588, accumulated_submission_time=2048.28, global_step=2100, preemption_count=0, score=2048.28, test/loss=0.127414, test/num_examples=95000000, total_duration=15259.6, train/loss=0.125062, validation/loss=0.125156, validation/num_examples=83274637
I0316 18:59:46.166693 139986877789376 spec.py:321] Evaluating on the training split.
I0316 19:02:12.643484 139986877789376 spec.py:333] Evaluating on the validation split.
I0316 19:04:17.404565 139986877789376 spec.py:349] Evaluating on the test split.
I0316 19:08:52.965038 139986877789376 submission_runner.py:469] Time since start: 15927.34s, 	Step: 2227, 	{'train/loss': 0.12340524428579594, 'validation/loss': 0.1253952899879546, 'validation/num_examples': 83274637, 'test/loss': 0.1278679569109465, 'test/num_examples': 95000000, 'score': 2168.3095784187317, 'total_duration': 15927.342170715332, 'accumulated_submission_time': 2168.3095784187317, 'accumulated_eval_time': 13742.17267203331, 'accumulated_logging_time': 0.402069091796875}
I0316 19:08:52.975921 139944966141696 logging_writer.py:48] [2227] accumulated_eval_time=13742.2, accumulated_logging_time=0.402069, accumulated_submission_time=2168.31, global_step=2227, preemption_count=0, score=2168.31, test/loss=0.127868, test/num_examples=95000000, total_duration=15927.3, train/loss=0.123405, validation/loss=0.125395, validation/num_examples=83274637
I0316 19:10:54.486365 139986877789376 spec.py:321] Evaluating on the training split.
I0316 19:13:30.075184 139986877789376 spec.py:333] Evaluating on the validation split.
I0316 19:15:34.422885 139986877789376 spec.py:349] Evaluating on the test split.
I0316 19:20:00.177795 139986877789376 submission_runner.py:469] Time since start: 16594.55s, 	Step: 2352, 	{'train/loss': 0.12429536269516334, 'validation/loss': 0.12517434331362925, 'validation/num_examples': 83274637, 'test/loss': 0.12770973048942968, 'test/num_examples': 95000000, 'score': 2288.947000980377, 'total_duration': 16594.554953336716, 'accumulated_submission_time': 2288.947000980377, 'accumulated_eval_time': 14287.864269018173, 'accumulated_logging_time': 0.41991639137268066}
I0316 19:20:00.187940 139944957748992 logging_writer.py:48] [2352] accumulated_eval_time=14287.9, accumulated_logging_time=0.419916, accumulated_submission_time=2288.95, global_step=2352, preemption_count=0, score=2288.95, test/loss=0.12771, test/num_examples=95000000, total_duration=16594.6, train/loss=0.124295, validation/loss=0.125174, validation/num_examples=83274637
I0316 19:22:01.716672 139986877789376 spec.py:321] Evaluating on the training split.
I0316 19:24:26.123773 139986877789376 spec.py:333] Evaluating on the validation split.
I0316 19:26:32.250473 139986877789376 spec.py:349] Evaluating on the test split.
I0316 19:28:56.199361 139986877789376 submission_runner.py:469] Time since start: 17130.58s, 	Step: 2475, 	{'train/loss': 0.12421866557264956, 'validation/loss': 0.12547087323802827, 'validation/num_examples': 83274637, 'test/loss': 0.12796865269646895, 'test/num_examples': 95000000, 'score': 2409.581680059433, 'total_duration': 17130.57651257515, 'accumulated_submission_time': 2409.581680059433, 'accumulated_eval_time': 14702.347155570984, 'accumulated_logging_time': 0.4376943111419678}
I0316 19:28:56.210221 139944966141696 logging_writer.py:48] [2475] accumulated_eval_time=14702.3, accumulated_logging_time=0.437694, accumulated_submission_time=2409.58, global_step=2475, preemption_count=0, score=2409.58, test/loss=0.127969, test/num_examples=95000000, total_duration=17130.6, train/loss=0.124219, validation/loss=0.125471, validation/num_examples=83274637
I0316 19:29:01.573635 139944957748992 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.00981819, loss=0.13059
I0316 19:29:01.576901 139986877789376 submission.py:265] 2500) loss = 0.131, grad_norm = 0.010
I0316 19:30:57.137961 139986877789376 spec.py:321] Evaluating on the training split.
I0316 19:33:01.227176 139986877789376 spec.py:333] Evaluating on the validation split.
I0316 19:35:05.096567 139986877789376 spec.py:349] Evaluating on the test split.
I0316 19:37:27.980319 139986877789376 submission_runner.py:469] Time since start: 17642.36s, 	Step: 2598, 	{'train/loss': 0.12479094837343505, 'validation/loss': 0.1252643962353461, 'validation/num_examples': 83274637, 'test/loss': 0.12765837583047968, 'test/num_examples': 95000000, 'score': 2529.5888845920563, 'total_duration': 17642.357486486435, 'accumulated_submission_time': 2529.5888845920563, 'accumulated_eval_time': 15093.189688682556, 'accumulated_logging_time': 0.45548081398010254}
I0316 19:37:27.991397 139944966141696 logging_writer.py:48] [2598] accumulated_eval_time=15093.2, accumulated_logging_time=0.455481, accumulated_submission_time=2529.59, global_step=2598, preemption_count=0, score=2529.59, test/loss=0.127658, test/num_examples=95000000, total_duration=17642.4, train/loss=0.124791, validation/loss=0.125264, validation/num_examples=83274637
I0316 19:39:29.300322 139986877789376 spec.py:321] Evaluating on the training split.
I0316 19:41:33.990257 139986877789376 spec.py:333] Evaluating on the validation split.
I0316 19:43:38.979843 139986877789376 spec.py:349] Evaluating on the test split.
I0316 19:46:02.225748 139986877789376 submission_runner.py:469] Time since start: 18156.60s, 	Step: 2724, 	{'train/loss': 0.12328262261924677, 'validation/loss': 0.12501506667993337, 'validation/num_examples': 83274637, 'test/loss': 0.12746548940220884, 'test/num_examples': 95000000, 'score': 2650.0613491535187, 'total_duration': 18156.602919578552, 'accumulated_submission_time': 2650.0613491535187, 'accumulated_eval_time': 15486.115355968475, 'accumulated_logging_time': 0.4859898090362549}
I0316 19:46:02.237609 139944957748992 logging_writer.py:48] [2724] accumulated_eval_time=15486.1, accumulated_logging_time=0.48599, accumulated_submission_time=2650.06, global_step=2724, preemption_count=0, score=2650.06, test/loss=0.127465, test/num_examples=95000000, total_duration=18156.6, train/loss=0.123283, validation/loss=0.125015, validation/num_examples=83274637
I0316 19:48:03.317458 139986877789376 spec.py:321] Evaluating on the training split.
I0316 19:50:07.334706 139986877789376 spec.py:333] Evaluating on the validation split.
I0316 19:52:12.409023 139986877789376 spec.py:349] Evaluating on the test split.
I0316 19:54:35.940733 139986877789376 submission_runner.py:469] Time since start: 18670.32s, 	Step: 2851, 	{'train/loss': 0.12452472169874755, 'validation/loss': 0.125381837365852, 'validation/num_examples': 83274637, 'test/loss': 0.12780912855859053, 'test/num_examples': 95000000, 'score': 2770.1883223056793, 'total_duration': 18670.317764997482, 'accumulated_submission_time': 2770.1883223056793, 'accumulated_eval_time': 15878.73873257637, 'accumulated_logging_time': 0.5085866451263428}
I0316 19:54:35.954294 139944966141696 logging_writer.py:48] [2851] accumulated_eval_time=15878.7, accumulated_logging_time=0.508587, accumulated_submission_time=2770.19, global_step=2851, preemption_count=0, score=2770.19, test/loss=0.127809, test/num_examples=95000000, total_duration=18670.3, train/loss=0.124525, validation/loss=0.125382, validation/num_examples=83274637
I0316 19:56:36.907656 139986877789376 spec.py:321] Evaluating on the training split.
I0316 19:58:41.522891 139986877789376 spec.py:333] Evaluating on the validation split.
I0316 20:00:46.790101 139986877789376 spec.py:349] Evaluating on the test split.
I0316 20:03:10.237023 139986877789376 submission_runner.py:469] Time since start: 19184.61s, 	Step: 2977, 	{'train/loss': 0.12228472697473997, 'validation/loss': 0.12515656402759043, 'validation/num_examples': 83274637, 'test/loss': 0.12733072388410066, 'test/num_examples': 95000000, 'score': 2890.2491257190704, 'total_duration': 19184.614201784134, 'accumulated_submission_time': 2890.2491257190704, 'accumulated_eval_time': 16272.068244934082, 'accumulated_logging_time': 0.5289978981018066}
I0316 20:03:10.247952 139944957748992 logging_writer.py:48] [2977] accumulated_eval_time=16272.1, accumulated_logging_time=0.528998, accumulated_submission_time=2890.25, global_step=2977, preemption_count=0, score=2890.25, test/loss=0.127331, test/num_examples=95000000, total_duration=19184.6, train/loss=0.122285, validation/loss=0.125157, validation/num_examples=83274637
I0316 20:03:15.225916 139944966141696 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.0126974, loss=0.122278
I0316 20:03:15.229057 139986877789376 submission.py:265] 3000) loss = 0.122, grad_norm = 0.013
I0316 20:05:11.846906 139986877789376 spec.py:321] Evaluating on the training split.
I0316 20:07:16.390000 139986877789376 spec.py:333] Evaluating on the validation split.
I0316 20:09:21.244560 139986877789376 spec.py:349] Evaluating on the test split.
I0316 20:11:44.229607 139986877789376 submission_runner.py:469] Time since start: 19698.61s, 	Step: 3099, 	{'train/loss': 0.12130272276257059, 'validation/loss': 0.12497562339099547, 'validation/num_examples': 83274637, 'test/loss': 0.12697630198589124, 'test/num_examples': 95000000, 'score': 3010.984312772751, 'total_duration': 19698.606773376465, 'accumulated_submission_time': 3010.984312772751, 'accumulated_eval_time': 16664.451105833054, 'accumulated_logging_time': 0.5475137233734131}
I0316 20:11:44.241245 139944957748992 logging_writer.py:48] [3099] accumulated_eval_time=16664.5, accumulated_logging_time=0.547514, accumulated_submission_time=3010.98, global_step=3099, preemption_count=0, score=3010.98, test/loss=0.126976, test/num_examples=95000000, total_duration=19698.6, train/loss=0.121303, validation/loss=0.124976, validation/num_examples=83274637
I0316 20:13:45.068073 139986877789376 spec.py:321] Evaluating on the training split.
I0316 20:15:48.306115 139986877789376 spec.py:333] Evaluating on the validation split.
I0316 20:17:52.539645 139986877789376 spec.py:349] Evaluating on the test split.
I0316 20:20:14.801850 139986877789376 submission_runner.py:469] Time since start: 20209.18s, 	Step: 3218, 	{'train/loss': 0.1240839149189859, 'validation/loss': 0.12473288380814102, 'validation/num_examples': 83274637, 'test/loss': 0.1269867311092979, 'test/num_examples': 95000000, 'score': 3130.98251080513, 'total_duration': 20209.1790163517, 'accumulated_submission_time': 3130.98251080513, 'accumulated_eval_time': 17054.185025453568, 'accumulated_logging_time': 0.5672502517700195}
I0316 20:20:14.812641 139944966141696 logging_writer.py:48] [3218] accumulated_eval_time=17054.2, accumulated_logging_time=0.56725, accumulated_submission_time=3130.98, global_step=3218, preemption_count=0, score=3130.98, test/loss=0.126987, test/num_examples=95000000, total_duration=20209.2, train/loss=0.124084, validation/loss=0.124733, validation/num_examples=83274637
I0316 20:22:15.761796 139986877789376 spec.py:321] Evaluating on the training split.
I0316 20:24:18.634533 139986877789376 spec.py:333] Evaluating on the validation split.
I0316 20:26:22.552004 139986877789376 spec.py:349] Evaluating on the test split.
I0316 20:30:23.692900 139986877789376 submission_runner.py:469] Time since start: 20818.07s, 	Step: 3341, 	{'train/loss': 0.12567785640106818, 'validation/loss': 0.12487115903259279, 'validation/num_examples': 83274637, 'test/loss': 0.12724782019629227, 'test/num_examples': 95000000, 'score': 3250.9902317523956, 'total_duration': 20818.07008433342, 'accumulated_submission_time': 3250.9902317523956, 'accumulated_eval_time': 17542.116401433945, 'accumulated_logging_time': 0.5983700752258301}
I0316 20:30:23.702848 139944957748992 logging_writer.py:48] [3341] accumulated_eval_time=17542.1, accumulated_logging_time=0.59837, accumulated_submission_time=3250.99, global_step=3341, preemption_count=0, score=3250.99, test/loss=0.127248, test/num_examples=95000000, total_duration=20818.1, train/loss=0.125678, validation/loss=0.124871, validation/num_examples=83274637
I0316 20:32:25.540621 139986877789376 spec.py:321] Evaluating on the training split.
I0316 20:34:54.949941 139986877789376 spec.py:333] Evaluating on the validation split.
I0316 20:36:59.465265 139986877789376 spec.py:349] Evaluating on the test split.
I0316 20:41:47.486331 139986877789376 submission_runner.py:469] Time since start: 21501.86s, 	Step: 3467, 	{'train/loss': 0.12622615718571337, 'validation/loss': 0.12479017747074063, 'validation/num_examples': 83274637, 'test/loss': 0.12711589282993518, 'test/num_examples': 95000000, 'score': 3371.9157547950745, 'total_duration': 21501.863481521606, 'accumulated_submission_time': 3371.9157547950745, 'accumulated_eval_time': 18104.062248706818, 'accumulated_logging_time': 0.6150655746459961}
I0316 20:41:47.498128 139944966141696 logging_writer.py:48] [3467] accumulated_eval_time=18104.1, accumulated_logging_time=0.615066, accumulated_submission_time=3371.92, global_step=3467, preemption_count=0, score=3371.92, test/loss=0.127116, test/num_examples=95000000, total_duration=21501.9, train/loss=0.126226, validation/loss=0.12479, validation/num_examples=83274637
I0316 20:41:58.572905 139944957748992 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.0274912, loss=0.128288
I0316 20:41:58.575981 139986877789376 submission.py:265] 3500) loss = 0.128, grad_norm = 0.027
I0316 20:43:48.306689 139986877789376 spec.py:321] Evaluating on the training split.
I0316 20:46:26.216987 139986877789376 spec.py:333] Evaluating on the validation split.
I0316 20:48:30.712801 139986877789376 spec.py:349] Evaluating on the test split.
I0316 20:53:01.604309 139986877789376 submission_runner.py:469] Time since start: 22175.98s, 	Step: 3588, 	{'train/loss': 0.12493945824243634, 'validation/loss': 0.1248606487833527, 'validation/num_examples': 83274637, 'test/loss': 0.1271577419879311, 'test/num_examples': 95000000, 'score': 3491.8086977005005, 'total_duration': 22175.981496334076, 'accumulated_submission_time': 3491.8086977005005, 'accumulated_eval_time': 18657.35998058319, 'accumulated_logging_time': 0.6338131427764893}
I0316 20:53:01.615144 139944966141696 logging_writer.py:48] [3588] accumulated_eval_time=18657.4, accumulated_logging_time=0.633813, accumulated_submission_time=3491.81, global_step=3588, preemption_count=0, score=3491.81, test/loss=0.127158, test/num_examples=95000000, total_duration=22176, train/loss=0.124939, validation/loss=0.124861, validation/num_examples=83274637
I0316 20:55:02.034796 139986877789376 spec.py:321] Evaluating on the training split.
I0316 20:57:22.079391 139986877789376 spec.py:333] Evaluating on the validation split.
I0316 20:59:26.439243 139986877789376 spec.py:349] Evaluating on the test split.
I0316 21:03:41.988179 139986877789376 submission_runner.py:469] Time since start: 22816.37s, 	Step: 3711, 	{'train/loss': 0.12271506807052468, 'validation/loss': 0.12498088101956117, 'validation/num_examples': 83274637, 'test/loss': 0.12747368079605104, 'test/num_examples': 95000000, 'score': 3611.419188261032, 'total_duration': 22816.365347623825, 'accumulated_submission_time': 3611.419188261032, 'accumulated_eval_time': 19177.313400268555, 'accumulated_logging_time': 0.6510157585144043}
I0316 21:03:41.999018 139944957748992 logging_writer.py:48] [3711] accumulated_eval_time=19177.3, accumulated_logging_time=0.651016, accumulated_submission_time=3611.42, global_step=3711, preemption_count=0, score=3611.42, test/loss=0.127474, test/num_examples=95000000, total_duration=22816.4, train/loss=0.122715, validation/loss=0.124981, validation/num_examples=83274637
I0316 21:05:42.785487 139986877789376 spec.py:321] Evaluating on the training split.
I0316 21:07:47.495679 139986877789376 spec.py:333] Evaluating on the validation split.
I0316 21:09:53.339521 139986877789376 spec.py:349] Evaluating on the test split.
I0316 21:12:17.053660 139986877789376 submission_runner.py:469] Time since start: 23331.43s, 	Step: 3833, 	{'train/loss': 0.12458286968955981, 'validation/loss': 0.1254453387102873, 'validation/num_examples': 83274637, 'test/loss': 0.1278129607993678, 'test/num_examples': 95000000, 'score': 3731.267385005951, 'total_duration': 23331.43080496788, 'accumulated_submission_time': 3731.267385005951, 'accumulated_eval_time': 19571.58188867569, 'accumulated_logging_time': 0.6902680397033691}
I0316 21:12:17.065303 139944966141696 logging_writer.py:48] [3833] accumulated_eval_time=19571.6, accumulated_logging_time=0.690268, accumulated_submission_time=3731.27, global_step=3833, preemption_count=0, score=3731.27, test/loss=0.127813, test/num_examples=95000000, total_duration=23331.4, train/loss=0.124583, validation/loss=0.125445, validation/num_examples=83274637
I0316 21:14:18.431724 139986877789376 spec.py:321] Evaluating on the training split.
I0316 21:16:22.668925 139986877789376 spec.py:333] Evaluating on the validation split.
I0316 21:18:27.458417 139986877789376 spec.py:349] Evaluating on the test split.
I0316 21:20:50.870525 139986877789376 submission_runner.py:469] Time since start: 23845.25s, 	Step: 3957, 	{'train/loss': 0.12149816660363298, 'validation/loss': 0.12471971914226485, 'validation/num_examples': 83274637, 'test/loss': 0.1270592860603734, 'test/num_examples': 95000000, 'score': 3851.760407924652, 'total_duration': 23845.247673511505, 'accumulated_submission_time': 3851.760407924652, 'accumulated_eval_time': 19964.02088713646, 'accumulated_logging_time': 0.7090356349945068}
I0316 21:20:50.882564 139944957748992 logging_writer.py:48] [3957] accumulated_eval_time=19964, accumulated_logging_time=0.709036, accumulated_submission_time=3851.76, global_step=3957, preemption_count=0, score=3851.76, test/loss=0.127059, test/num_examples=95000000, total_duration=23845.2, train/loss=0.121498, validation/loss=0.12472, validation/num_examples=83274637
I0316 21:21:13.771432 139944966141696 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.0285954, loss=0.121424
I0316 21:21:13.775231 139986877789376 submission.py:265] 4000) loss = 0.121, grad_norm = 0.029
I0316 21:22:52.391099 139986877789376 spec.py:321] Evaluating on the training split.
I0316 21:24:56.792081 139986877789376 spec.py:333] Evaluating on the validation split.
I0316 21:27:01.711066 139986877789376 spec.py:349] Evaluating on the test split.
I0316 21:29:25.468640 139986877789376 submission_runner.py:469] Time since start: 24359.85s, 	Step: 4078, 	{'train/loss': 0.12378081426271971, 'validation/loss': 0.12468606476285714, 'validation/num_examples': 83274637, 'test/loss': 0.1270273658237658, 'test/num_examples': 95000000, 'score': 3972.3173167705536, 'total_duration': 24359.845747232437, 'accumulated_submission_time': 3972.3173167705536, 'accumulated_eval_time': 20357.098534584045, 'accumulated_logging_time': 0.7285640239715576}
I0316 21:29:25.480083 139944957748992 logging_writer.py:48] [4078] accumulated_eval_time=20357.1, accumulated_logging_time=0.728564, accumulated_submission_time=3972.32, global_step=4078, preemption_count=0, score=3972.32, test/loss=0.127027, test/num_examples=95000000, total_duration=24359.8, train/loss=0.123781, validation/loss=0.124686, validation/num_examples=83274637
I0316 21:31:26.151568 139986877789376 spec.py:321] Evaluating on the training split.
I0316 21:33:30.142951 139986877789376 spec.py:333] Evaluating on the validation split.
I0316 21:35:34.455467 139986877789376 spec.py:349] Evaluating on the test split.
I0316 21:37:58.115564 139986877789376 submission_runner.py:469] Time since start: 24872.49s, 	Step: 4200, 	{'train/loss': 0.12351428697632522, 'validation/loss': 0.12455166686106513, 'validation/num_examples': 83274637, 'test/loss': 0.12687963547178568, 'test/num_examples': 95000000, 'score': 4092.0875413417816, 'total_duration': 24872.492672920227, 'accumulated_submission_time': 4092.0875413417816, 'accumulated_eval_time': 20749.06259894371, 'accumulated_logging_time': 0.7707626819610596}
I0316 21:37:58.126379 139944966141696 logging_writer.py:48] [4200] accumulated_eval_time=20749.1, accumulated_logging_time=0.770763, accumulated_submission_time=4092.09, global_step=4200, preemption_count=0, score=4092.09, test/loss=0.12688, test/num_examples=95000000, total_duration=24872.5, train/loss=0.123514, validation/loss=0.124552, validation/num_examples=83274637
I0316 21:39:59.967875 139986877789376 spec.py:321] Evaluating on the training split.
I0316 21:42:04.581007 139986877789376 spec.py:333] Evaluating on the validation split.
I0316 21:44:09.430158 139986877789376 spec.py:349] Evaluating on the test split.
I0316 21:46:33.190993 139986877789376 submission_runner.py:469] Time since start: 25387.57s, 	Step: 4320, 	{'train/loss': 0.12428829293384211, 'validation/loss': 0.12458359610939657, 'validation/num_examples': 83274637, 'test/loss': 0.1270097317562304, 'test/num_examples': 95000000, 'score': 4213.053079366684, 'total_duration': 25387.568189144135, 'accumulated_submission_time': 4213.053079366684, 'accumulated_eval_time': 21142.28586101532, 'accumulated_logging_time': 0.7884151935577393}
I0316 21:46:33.201413 139944957748992 logging_writer.py:48] [4320] accumulated_eval_time=21142.3, accumulated_logging_time=0.788415, accumulated_submission_time=4213.05, global_step=4320, preemption_count=0, score=4213.05, test/loss=0.12701, test/num_examples=95000000, total_duration=25387.6, train/loss=0.124288, validation/loss=0.124584, validation/num_examples=83274637
I0316 21:48:33.763517 139986877789376 spec.py:321] Evaluating on the training split.
I0316 21:50:38.051290 139986877789376 spec.py:333] Evaluating on the validation split.
I0316 21:52:43.097860 139986877789376 spec.py:349] Evaluating on the test split.
I0316 21:55:06.830243 139986877789376 submission_runner.py:469] Time since start: 25901.21s, 	Step: 4446, 	{'train/loss': 0.12296701333146215, 'validation/loss': 0.12463263138169361, 'validation/num_examples': 83274637, 'test/loss': 0.12699972023620607, 'test/num_examples': 95000000, 'score': 4332.754045963287, 'total_duration': 25901.20740222931, 'accumulated_submission_time': 4332.754045963287, 'accumulated_eval_time': 21535.352833747864, 'accumulated_logging_time': 0.8056685924530029}
I0316 21:55:06.840872 139944966141696 logging_writer.py:48] [4446] accumulated_eval_time=21535.4, accumulated_logging_time=0.805669, accumulated_submission_time=4332.75, global_step=4446, preemption_count=0, score=4332.75, test/loss=0.127, test/num_examples=95000000, total_duration=25901.2, train/loss=0.122967, validation/loss=0.124633, validation/num_examples=83274637
I0316 21:55:41.374725 139944957748992 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.0464975, loss=0.132576
I0316 21:55:41.378817 139986877789376 submission.py:265] 4500) loss = 0.133, grad_norm = 0.046
I0316 21:57:07.299032 139986877789376 spec.py:321] Evaluating on the training split.
I0316 21:59:11.614988 139986877789376 spec.py:333] Evaluating on the validation split.
I0316 22:01:16.235047 139986877789376 spec.py:349] Evaluating on the test split.
I0316 22:03:39.604866 139986877789376 submission_runner.py:469] Time since start: 26413.98s, 	Step: 4573, 	{'train/loss': 0.12299708577051403, 'validation/loss': 0.1247469869771901, 'validation/num_examples': 83274637, 'test/loss': 0.127256416310481, 'test/num_examples': 95000000, 'score': 4452.261522531509, 'total_duration': 26413.982054948807, 'accumulated_submission_time': 4452.261522531509, 'accumulated_eval_time': 21927.65883374214, 'accumulated_logging_time': 0.822965145111084}
I0316 22:03:39.616044 139944966141696 logging_writer.py:48] [4573] accumulated_eval_time=21927.7, accumulated_logging_time=0.822965, accumulated_submission_time=4452.26, global_step=4573, preemption_count=0, score=4452.26, test/loss=0.127256, test/num_examples=95000000, total_duration=26414, train/loss=0.122997, validation/loss=0.124747, validation/num_examples=83274637
I0316 22:05:40.397721 139986877789376 spec.py:321] Evaluating on the training split.
I0316 22:07:44.648576 139986877789376 spec.py:333] Evaluating on the validation split.
I0316 22:09:49.025145 139986877789376 spec.py:349] Evaluating on the test split.
I0316 22:12:12.407858 139986877789376 submission_runner.py:469] Time since start: 26926.79s, 	Step: 4699, 	{'train/loss': 0.1248974298962859, 'validation/loss': 0.1246522753990138, 'validation/num_examples': 83274637, 'test/loss': 0.12700907233075592, 'test/num_examples': 95000000, 'score': 4572.159146785736, 'total_duration': 26926.785030841827, 'accumulated_submission_time': 4572.159146785736, 'accumulated_eval_time': 22319.669251918793, 'accumulated_logging_time': 0.8414256572723389}
I0316 22:12:12.419785 139944957748992 logging_writer.py:48] [4699] accumulated_eval_time=22319.7, accumulated_logging_time=0.841426, accumulated_submission_time=4572.16, global_step=4699, preemption_count=0, score=4572.16, test/loss=0.127009, test/num_examples=95000000, total_duration=26926.8, train/loss=0.124897, validation/loss=0.124652, validation/num_examples=83274637
I0316 22:14:13.016842 139986877789376 spec.py:321] Evaluating on the training split.
I0316 22:16:17.237336 139986877789376 spec.py:333] Evaluating on the validation split.
I0316 22:18:21.712646 139986877789376 spec.py:349] Evaluating on the test split.
I0316 22:20:44.848174 139986877789376 submission_runner.py:469] Time since start: 27439.23s, 	Step: 4822, 	{'train/loss': 0.12422485603688359, 'validation/loss': 0.12465599421139004, 'validation/num_examples': 83274637, 'test/loss': 0.12711116010661877, 'test/num_examples': 95000000, 'score': 4691.888939142227, 'total_duration': 27439.225314855576, 'accumulated_submission_time': 4691.888939142227, 'accumulated_eval_time': 22711.500751018524, 'accumulated_logging_time': 0.8600952625274658}
I0316 22:20:44.877277 139944966141696 logging_writer.py:48] [4822] accumulated_eval_time=22711.5, accumulated_logging_time=0.860095, accumulated_submission_time=4691.89, global_step=4822, preemption_count=0, score=4691.89, test/loss=0.127111, test/num_examples=95000000, total_duration=27439.2, train/loss=0.124225, validation/loss=0.124656, validation/num_examples=83274637
I0316 22:22:45.869394 139986877789376 spec.py:321] Evaluating on the training split.
I0316 22:24:50.572371 139986877789376 spec.py:333] Evaluating on the validation split.
I0316 22:26:55.280274 139986877789376 spec.py:349] Evaluating on the test split.
I0316 22:29:18.872533 139986877789376 submission_runner.py:469] Time since start: 27953.25s, 	Step: 4946, 	{'train/loss': 0.12294913369371825, 'validation/loss': 0.12502464820980833, 'validation/num_examples': 83274637, 'test/loss': 0.12755271683172928, 'test/num_examples': 95000000, 'score': 4811.971235990524, 'total_duration': 27953.249688386917, 'accumulated_submission_time': 4811.971235990524, 'accumulated_eval_time': 23104.504133462906, 'accumulated_logging_time': 0.9380571842193604}
I0316 22:29:18.884791 139944957748992 logging_writer.py:48] [4946] accumulated_eval_time=23104.5, accumulated_logging_time=0.938057, accumulated_submission_time=4811.97, global_step=4946, preemption_count=0, score=4811.97, test/loss=0.127553, test/num_examples=95000000, total_duration=27953.2, train/loss=0.122949, validation/loss=0.125025, validation/num_examples=83274637
I0316 22:29:54.180875 139944966141696 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.00986191, loss=0.119815
I0316 22:29:54.184542 139986877789376 submission.py:265] 5000) loss = 0.120, grad_norm = 0.010
I0316 22:31:19.925679 139986877789376 spec.py:321] Evaluating on the training split.
I0316 22:33:24.445173 139986877789376 spec.py:333] Evaluating on the validation split.
I0316 22:35:28.624379 139986877789376 spec.py:349] Evaluating on the test split.
I0316 22:37:52.079662 139986877789376 submission_runner.py:469] Time since start: 28466.46s, 	Step: 5071, 	{'train/loss': 0.12433508531289945, 'validation/loss': 0.12458647602663173, 'validation/num_examples': 83274637, 'test/loss': 0.12685982146835328, 'test/num_examples': 95000000, 'score': 4932.076211214066, 'total_duration': 28466.456826925278, 'accumulated_submission_time': 4932.076211214066, 'accumulated_eval_time': 23496.6583507061, 'accumulated_logging_time': 0.9632937908172607}
I0316 22:37:52.091625 139944957748992 logging_writer.py:48] [5071] accumulated_eval_time=23496.7, accumulated_logging_time=0.963294, accumulated_submission_time=4932.08, global_step=5071, preemption_count=0, score=4932.08, test/loss=0.12686, test/num_examples=95000000, total_duration=28466.5, train/loss=0.124335, validation/loss=0.124586, validation/num_examples=83274637
I0316 22:39:52.512036 139986877789376 spec.py:321] Evaluating on the training split.
I0316 22:41:56.886691 139986877789376 spec.py:333] Evaluating on the validation split.
I0316 22:44:00.831823 139986877789376 spec.py:349] Evaluating on the test split.
I0316 22:46:23.559844 139986877789376 submission_runner.py:469] Time since start: 28977.94s, 	Step: 5190, 	{'train/loss': 0.1248338533576487, 'validation/loss': 0.12415056748721312, 'validation/num_examples': 83274637, 'test/loss': 0.12657204358476337, 'test/num_examples': 95000000, 'score': 5051.599883794785, 'total_duration': 28977.937035799026, 'accumulated_submission_time': 5051.599883794785, 'accumulated_eval_time': 23887.70643877983, 'accumulated_logging_time': 0.9826030731201172}
I0316 22:46:23.571646 139944966141696 logging_writer.py:48] [5190] accumulated_eval_time=23887.7, accumulated_logging_time=0.982603, accumulated_submission_time=5051.6, global_step=5190, preemption_count=0, score=5051.6, test/loss=0.126572, test/num_examples=95000000, total_duration=28977.9, train/loss=0.124834, validation/loss=0.124151, validation/num_examples=83274637
I0316 22:48:25.079030 139986877789376 spec.py:321] Evaluating on the training split.
I0316 22:50:29.289674 139986877789376 spec.py:333] Evaluating on the validation split.
I0316 22:52:33.251605 139986877789376 spec.py:349] Evaluating on the test split.
I0316 22:54:54.949097 139986877789376 submission_runner.py:469] Time since start: 29489.33s, 	Step: 5315, 	{'train/loss': 0.12423565536996703, 'validation/loss': 0.12420020001356366, 'validation/num_examples': 83274637, 'test/loss': 0.12660823988535028, 'test/num_examples': 95000000, 'score': 5172.187809705734, 'total_duration': 29489.326233148575, 'accumulated_submission_time': 5172.187809705734, 'accumulated_eval_time': 24277.576689243317, 'accumulated_logging_time': 1.008660078048706}
I0316 22:54:54.961798 139944957748992 logging_writer.py:48] [5315] accumulated_eval_time=24277.6, accumulated_logging_time=1.00866, accumulated_submission_time=5172.19, global_step=5315, preemption_count=0, score=5172.19, test/loss=0.126608, test/num_examples=95000000, total_duration=29489.3, train/loss=0.124236, validation/loss=0.1242, validation/num_examples=83274637
I0316 22:56:55.837409 139986877789376 spec.py:321] Evaluating on the training split.
I0316 22:58:59.712647 139986877789376 spec.py:333] Evaluating on the validation split.
I0316 23:01:03.712740 139986877789376 spec.py:349] Evaluating on the test split.
I0316 23:03:26.601510 139986877789376 submission_runner.py:469] Time since start: 30000.98s, 	Step: 5439, 	{'train/loss': 0.12237444226001577, 'validation/loss': 0.12466223047277361, 'validation/num_examples': 83274637, 'test/loss': 0.12700093340092708, 'test/num_examples': 95000000, 'score': 5292.217370271683, 'total_duration': 30000.978651046753, 'accumulated_submission_time': 5292.217370271683, 'accumulated_eval_time': 24668.3408575058, 'accumulated_logging_time': 1.0277798175811768}
I0316 23:03:26.613183 139944966141696 logging_writer.py:48] [5439] accumulated_eval_time=24668.3, accumulated_logging_time=1.02778, accumulated_submission_time=5292.22, global_step=5439, preemption_count=0, score=5292.22, test/loss=0.127001, test/num_examples=95000000, total_duration=30001, train/loss=0.122374, validation/loss=0.124662, validation/num_examples=83274637
I0316 23:04:11.163037 139944957748992 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.00857089, loss=0.117501
I0316 23:04:11.166347 139986877789376 submission.py:265] 5500) loss = 0.118, grad_norm = 0.009
I0316 23:05:27.075361 139986877789376 spec.py:321] Evaluating on the training split.
I0316 23:07:30.017410 139986877789376 spec.py:333] Evaluating on the validation split.
I0316 23:09:33.844761 139986877789376 spec.py:349] Evaluating on the test split.
I0316 23:11:56.231625 139986877789376 submission_runner.py:469] Time since start: 30510.61s, 	Step: 5562, 	{'train/loss': 0.12309596578565103, 'validation/loss': 0.12435470075432375, 'validation/num_examples': 83274637, 'test/loss': 0.12656558547748767, 'test/num_examples': 95000000, 'score': 5411.794659614563, 'total_duration': 30510.608814001083, 'accumulated_submission_time': 5411.794659614563, 'accumulated_eval_time': 25057.497400045395, 'accumulated_logging_time': 1.0460309982299805}
I0316 23:11:56.243357 139944966141696 logging_writer.py:48] [5562] accumulated_eval_time=25057.5, accumulated_logging_time=1.04603, accumulated_submission_time=5411.79, global_step=5562, preemption_count=0, score=5411.79, test/loss=0.126566, test/num_examples=95000000, total_duration=30510.6, train/loss=0.123096, validation/loss=0.124355, validation/num_examples=83274637
I0316 23:13:57.603842 139986877789376 spec.py:321] Evaluating on the training split.
I0316 23:16:01.459202 139986877789376 spec.py:333] Evaluating on the validation split.
I0316 23:18:05.451764 139986877789376 spec.py:349] Evaluating on the test split.
I0316 23:20:28.421990 139986877789376 submission_runner.py:469] Time since start: 31022.80s, 	Step: 5689, 	{'train/loss': 0.12277620498114068, 'validation/loss': 0.12419676794078235, 'validation/num_examples': 83274637, 'test/loss': 0.12643836911460474, 'test/num_examples': 95000000, 'score': 5532.273994207382, 'total_duration': 31022.799132823944, 'accumulated_submission_time': 5532.273994207382, 'accumulated_eval_time': 25448.315858840942, 'accumulated_logging_time': 1.0642931461334229}
I0316 23:20:28.434586 139944957748992 logging_writer.py:48] [5689] accumulated_eval_time=25448.3, accumulated_logging_time=1.06429, accumulated_submission_time=5532.27, global_step=5689, preemption_count=0, score=5532.27, test/loss=0.126438, test/num_examples=95000000, total_duration=31022.8, train/loss=0.122776, validation/loss=0.124197, validation/num_examples=83274637
I0316 23:22:30.083097 139986877789376 spec.py:321] Evaluating on the training split.
I0316 23:24:33.446016 139986877789376 spec.py:333] Evaluating on the validation split.
I0316 23:26:37.242220 139986877789376 spec.py:349] Evaluating on the test split.
I0316 23:28:59.541316 139986877789376 submission_runner.py:469] Time since start: 31533.92s, 	Step: 5814, 	{'train/loss': 0.12389250463439097, 'validation/loss': 0.12426123714297471, 'validation/num_examples': 83274637, 'test/loss': 0.12652646537459525, 'test/num_examples': 95000000, 'score': 5653.040225505829, 'total_duration': 31533.918437719345, 'accumulated_submission_time': 5653.040225505829, 'accumulated_eval_time': 25837.7742562294, 'accumulated_logging_time': 1.1082062721252441}
I0316 23:28:59.577803 139944966141696 logging_writer.py:48] [5814] accumulated_eval_time=25837.8, accumulated_logging_time=1.10821, accumulated_submission_time=5653.04, global_step=5814, preemption_count=0, score=5653.04, test/loss=0.126526, test/num_examples=95000000, total_duration=31533.9, train/loss=0.123893, validation/loss=0.124261, validation/num_examples=83274637
I0316 23:31:00.711894 139986877789376 spec.py:321] Evaluating on the training split.
I0316 23:33:03.376187 139986877789376 spec.py:333] Evaluating on the validation split.
I0316 23:35:06.297540 139986877789376 spec.py:349] Evaluating on the test split.
I0316 23:37:27.696663 139986877789376 submission_runner.py:469] Time since start: 32042.07s, 	Step: 5938, 	{'train/loss': 0.12175452094122924, 'validation/loss': 0.1241455003310799, 'validation/num_examples': 83274637, 'test/loss': 0.12630939612298261, 'test/num_examples': 95000000, 'score': 5773.279890298843, 'total_duration': 32042.073843240738, 'accumulated_submission_time': 5773.279890298843, 'accumulated_eval_time': 26224.75915670395, 'accumulated_logging_time': 1.1514701843261719}
I0316 23:37:27.708341 139944957748992 logging_writer.py:48] [5938] accumulated_eval_time=26224.8, accumulated_logging_time=1.15147, accumulated_submission_time=5773.28, global_step=5938, preemption_count=0, score=5773.28, test/loss=0.126309, test/num_examples=95000000, total_duration=32042.1, train/loss=0.121755, validation/loss=0.124146, validation/num_examples=83274637
I0316 23:38:13.327095 139944966141696 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.0296333, loss=0.13035
I0316 23:38:13.330658 139986877789376 submission.py:265] 6000) loss = 0.130, grad_norm = 0.030
I0316 23:39:28.911373 139986877789376 spec.py:321] Evaluating on the training split.
I0316 23:41:31.743648 139986877789376 spec.py:333] Evaluating on the validation split.
I0316 23:43:33.863001 139986877789376 spec.py:349] Evaluating on the test split.
I0316 23:45:55.047845 139986877789376 submission_runner.py:469] Time since start: 32549.43s, 	Step: 6062, 	{'train/loss': 0.12262679893407474, 'validation/loss': 0.12427901395397312, 'validation/num_examples': 83274637, 'test/loss': 0.12668251021045884, 'test/num_examples': 95000000, 'score': 5893.560338497162, 'total_duration': 32549.425038337708, 'accumulated_submission_time': 5893.560338497162, 'accumulated_eval_time': 26610.895847797394, 'accumulated_logging_time': 1.1698927879333496}
I0316 23:45:55.059641 139944957748992 logging_writer.py:48] [6062] accumulated_eval_time=26610.9, accumulated_logging_time=1.16989, accumulated_submission_time=5893.56, global_step=6062, preemption_count=0, score=5893.56, test/loss=0.126683, test/num_examples=95000000, total_duration=32549.4, train/loss=0.122627, validation/loss=0.124279, validation/num_examples=83274637
I0316 23:47:56.248199 139986877789376 spec.py:321] Evaluating on the training split.
I0316 23:49:59.006061 139986877789376 spec.py:333] Evaluating on the validation split.
I0316 23:52:01.998808 139986877789376 spec.py:349] Evaluating on the test split.
I0316 23:54:22.640278 139986877789376 submission_runner.py:469] Time since start: 33057.02s, 	Step: 6187, 	{'train/loss': 0.12268381970145795, 'validation/loss': 0.12394535974545536, 'validation/num_examples': 83274637, 'test/loss': 0.12628360456310073, 'test/num_examples': 95000000, 'score': 6013.80818271637, 'total_duration': 33057.01746273041, 'accumulated_submission_time': 6013.80818271637, 'accumulated_eval_time': 26997.28810954094, 'accumulated_logging_time': 1.1887176036834717}
I0316 23:54:22.651801 139944966141696 logging_writer.py:48] [6187] accumulated_eval_time=26997.3, accumulated_logging_time=1.18872, accumulated_submission_time=6013.81, global_step=6187, preemption_count=0, score=6013.81, test/loss=0.126284, test/num_examples=95000000, total_duration=33057, train/loss=0.122684, validation/loss=0.123945, validation/num_examples=83274637
I0316 23:56:24.609316 139986877789376 spec.py:321] Evaluating on the training split.
I0316 23:58:27.197810 139986877789376 spec.py:333] Evaluating on the validation split.
I0317 00:00:29.290785 139986877789376 spec.py:349] Evaluating on the test split.
I0317 00:02:50.508626 139986877789376 submission_runner.py:469] Time since start: 33564.89s, 	Step: 6314, 	{'train/loss': 0.1222927982279195, 'validation/loss': 0.12405813671971737, 'validation/num_examples': 83274637, 'test/loss': 0.12633043124502083, 'test/num_examples': 95000000, 'score': 6135.070326566696, 'total_duration': 33564.88579106331, 'accumulated_submission_time': 6135.070326566696, 'accumulated_eval_time': 27383.18753170967, 'accumulated_logging_time': 1.2063744068145752}
I0317 00:02:50.520217 139944957748992 logging_writer.py:48] [6314] accumulated_eval_time=27383.2, accumulated_logging_time=1.20637, accumulated_submission_time=6135.07, global_step=6314, preemption_count=0, score=6135.07, test/loss=0.12633, test/num_examples=95000000, total_duration=33564.9, train/loss=0.122293, validation/loss=0.124058, validation/num_examples=83274637
I0317 00:04:51.229051 139986877789376 spec.py:321] Evaluating on the training split.
I0317 00:06:53.810779 139986877789376 spec.py:333] Evaluating on the validation split.
I0317 00:08:55.765600 139986877789376 spec.py:349] Evaluating on the test split.
I0317 00:11:17.245481 139986877789376 submission_runner.py:469] Time since start: 34071.62s, 	Step: 6439, 	{'train/loss': 0.12261034332189402, 'validation/loss': 0.12428007647760606, 'validation/num_examples': 83274637, 'test/loss': 0.12666987094686408, 'test/num_examples': 95000000, 'score': 6254.854168176651, 'total_duration': 34071.62265944481, 'accumulated_submission_time': 6254.854168176651, 'accumulated_eval_time': 27769.20412325859, 'accumulated_logging_time': 1.2425200939178467}
I0317 00:11:17.257004 139944966141696 logging_writer.py:48] [6439] accumulated_eval_time=27769.2, accumulated_logging_time=1.24252, accumulated_submission_time=6254.85, global_step=6439, preemption_count=0, score=6254.85, test/loss=0.12667, test/num_examples=95000000, total_duration=34071.6, train/loss=0.12261, validation/loss=0.12428, validation/num_examples=83274637
I0317 00:12:02.310046 139944957748992 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.0127638, loss=0.123598
I0317 00:12:02.312847 139986877789376 submission.py:265] 6500) loss = 0.124, grad_norm = 0.013
I0317 00:13:18.933402 139986877789376 spec.py:321] Evaluating on the training split.
I0317 00:15:22.274671 139986877789376 spec.py:333] Evaluating on the validation split.
I0317 00:17:24.949277 139986877789376 spec.py:349] Evaluating on the test split.
I0317 00:19:46.913015 139986877789376 submission_runner.py:469] Time since start: 34581.29s, 	Step: 6564, 	{'train/loss': 0.1236904086156677, 'validation/loss': 0.12406670213253225, 'validation/num_examples': 83274637, 'test/loss': 0.12639469705497342, 'test/num_examples': 95000000, 'score': 6375.659338951111, 'total_duration': 34581.2901904583, 'accumulated_submission_time': 6375.659338951111, 'accumulated_eval_time': 28157.18403029442, 'accumulated_logging_time': 1.2619872093200684}
I0317 00:19:46.925411 139944966141696 logging_writer.py:48] [6564] accumulated_eval_time=28157.2, accumulated_logging_time=1.26199, accumulated_submission_time=6375.66, global_step=6564, preemption_count=0, score=6375.66, test/loss=0.126395, test/num_examples=95000000, total_duration=34581.3, train/loss=0.12369, validation/loss=0.124067, validation/num_examples=83274637
I0317 00:21:47.401354 139986877789376 spec.py:321] Evaluating on the training split.
I0317 00:23:50.873125 139986877789376 spec.py:333] Evaluating on the validation split.
I0317 00:25:54.045787 139986877789376 spec.py:349] Evaluating on the test split.
I0317 00:28:16.150595 139986877789376 submission_runner.py:469] Time since start: 35090.53s, 	Step: 6690, 	{'train/loss': 0.1226907485344627, 'validation/loss': 0.12397244078212111, 'validation/num_examples': 83274637, 'test/loss': 0.12623934590221203, 'test/num_examples': 95000000, 'score': 6495.253893136978, 'total_duration': 35090.52767205238, 'accumulated_submission_time': 6495.253893136978, 'accumulated_eval_time': 28545.933460235596, 'accumulated_logging_time': 1.2810475826263428}
I0317 00:28:16.163269 139944957748992 logging_writer.py:48] [6690] accumulated_eval_time=28545.9, accumulated_logging_time=1.28105, accumulated_submission_time=6495.25, global_step=6690, preemption_count=0, score=6495.25, test/loss=0.126239, test/num_examples=95000000, total_duration=35090.5, train/loss=0.122691, validation/loss=0.123972, validation/num_examples=83274637
I0317 00:30:17.484931 139986877789376 spec.py:321] Evaluating on the training split.
I0317 00:32:20.266635 139986877789376 spec.py:333] Evaluating on the validation split.
I0317 00:34:22.732444 139986877789376 spec.py:349] Evaluating on the test split.
I0317 00:36:44.080353 139986877789376 submission_runner.py:469] Time since start: 35598.46s, 	Step: 6815, 	{'train/loss': 0.12305403754458465, 'validation/loss': 0.12394697510237317, 'validation/num_examples': 83274637, 'test/loss': 0.1261858419416327, 'test/num_examples': 95000000, 'score': 6615.686529874802, 'total_duration': 35598.457434892654, 'accumulated_submission_time': 6615.686529874802, 'accumulated_eval_time': 28932.529005527496, 'accumulated_logging_time': 1.3006353378295898}
I0317 00:36:44.092499 139944966141696 logging_writer.py:48] [6815] accumulated_eval_time=28932.5, accumulated_logging_time=1.30064, accumulated_submission_time=6615.69, global_step=6815, preemption_count=0, score=6615.69, test/loss=0.126186, test/num_examples=95000000, total_duration=35598.5, train/loss=0.123054, validation/loss=0.123947, validation/num_examples=83274637
I0317 00:38:45.574985 139986877789376 spec.py:321] Evaluating on the training split.
I0317 00:40:49.147360 139986877789376 spec.py:333] Evaluating on the validation split.
I0317 00:42:52.720664 139986877789376 spec.py:349] Evaluating on the test split.
I0317 00:45:14.994201 139986877789376 submission_runner.py:469] Time since start: 36109.37s, 	Step: 6937, 	{'train/loss': 0.1218845704796693, 'validation/loss': 0.12400140782139847, 'validation/num_examples': 83274637, 'test/loss': 0.1263985596414265, 'test/num_examples': 95000000, 'score': 6736.276186943054, 'total_duration': 36109.37138962746, 'accumulated_submission_time': 6736.276186943054, 'accumulated_eval_time': 29321.948405981064, 'accumulated_logging_time': 1.3193466663360596}
I0317 00:45:15.051154 139944957748992 logging_writer.py:48] [6937] accumulated_eval_time=29321.9, accumulated_logging_time=1.31935, accumulated_submission_time=6736.28, global_step=6937, preemption_count=0, score=6736.28, test/loss=0.126399, test/num_examples=95000000, total_duration=36109.4, train/loss=0.121885, validation/loss=0.124001, validation/num_examples=83274637
I0317 00:46:03.465052 139944966141696 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.0231234, loss=0.131389
I0317 00:46:03.468682 139986877789376 submission.py:265] 7000) loss = 0.131, grad_norm = 0.023
I0317 00:47:15.790632 139986877789376 spec.py:321] Evaluating on the training split.
I0317 00:49:18.624219 139986877789376 spec.py:333] Evaluating on the validation split.
I0317 00:51:21.350346 139986877789376 spec.py:349] Evaluating on the test split.
I0317 00:53:42.662100 139986877789376 submission_runner.py:469] Time since start: 36617.04s, 	Step: 7061, 	{'train/loss': 0.1211855189356522, 'validation/loss': 0.12385684483621552, 'validation/num_examples': 83274637, 'test/loss': 0.12621983461102937, 'test/num_examples': 95000000, 'score': 6856.094105005264, 'total_duration': 36617.03927612305, 'accumulated_submission_time': 6856.094105005264, 'accumulated_eval_time': 29708.820055246353, 'accumulated_logging_time': 1.4069323539733887}
I0317 00:53:42.673917 139944957748992 logging_writer.py:48] [7061] accumulated_eval_time=29708.8, accumulated_logging_time=1.40693, accumulated_submission_time=6856.09, global_step=7061, preemption_count=0, score=6856.09, test/loss=0.12622, test/num_examples=95000000, total_duration=36617, train/loss=0.121186, validation/loss=0.123857, validation/num_examples=83274637
I0317 00:55:43.146225 139986877789376 spec.py:321] Evaluating on the training split.
I0317 00:57:46.775131 139986877789376 spec.py:333] Evaluating on the validation split.
I0317 00:59:50.451872 139986877789376 spec.py:349] Evaluating on the test split.
I0317 01:02:12.618308 139986877789376 submission_runner.py:469] Time since start: 37127.00s, 	Step: 7187, 	{'train/loss': 0.1208851519014196, 'validation/loss': 0.12398135594389302, 'validation/num_examples': 83274637, 'test/loss': 0.126351320727258, 'test/num_examples': 95000000, 'score': 6975.676568508148, 'total_duration': 37126.995483636856, 'accumulated_submission_time': 6975.676568508148, 'accumulated_eval_time': 30098.292333841324, 'accumulated_logging_time': 1.4253897666931152}
I0317 01:02:12.630746 139944966141696 logging_writer.py:48] [7187] accumulated_eval_time=30098.3, accumulated_logging_time=1.42539, accumulated_submission_time=6975.68, global_step=7187, preemption_count=0, score=6975.68, test/loss=0.126351, test/num_examples=95000000, total_duration=37127, train/loss=0.120885, validation/loss=0.123981, validation/num_examples=83274637
I0317 01:04:13.541639 139986877789376 spec.py:321] Evaluating on the training split.
I0317 01:06:16.063018 139986877789376 spec.py:333] Evaluating on the validation split.
I0317 01:08:19.057741 139986877789376 spec.py:349] Evaluating on the test split.
I0317 01:10:39.506208 139986877789376 submission_runner.py:469] Time since start: 37633.88s, 	Step: 7309, 	{'train/loss': 0.12319936492622702, 'validation/loss': 0.12377459209460696, 'validation/num_examples': 83274637, 'test/loss': 0.12610510275718287, 'test/num_examples': 95000000, 'score': 7095.7635107040405, 'total_duration': 37633.88337945938, 'accumulated_submission_time': 7095.7635107040405, 'accumulated_eval_time': 30484.257103443146, 'accumulated_logging_time': 1.4443144798278809}
I0317 01:10:39.518959 139944957748992 logging_writer.py:48] [7309] accumulated_eval_time=30484.3, accumulated_logging_time=1.44431, accumulated_submission_time=7095.76, global_step=7309, preemption_count=0, score=7095.76, test/loss=0.126105, test/num_examples=95000000, total_duration=37633.9, train/loss=0.123199, validation/loss=0.123775, validation/num_examples=83274637
I0317 01:12:40.034145 139986877789376 spec.py:321] Evaluating on the training split.
I0317 01:14:42.781485 139986877789376 spec.py:333] Evaluating on the validation split.
I0317 01:16:46.543905 139986877789376 spec.py:349] Evaluating on the test split.
I0317 01:19:07.838155 139986877789376 submission_runner.py:469] Time since start: 38142.22s, 	Step: 7436, 	{'train/loss': 0.12147637310791662, 'validation/loss': 0.12385566411989682, 'validation/num_examples': 83274637, 'test/loss': 0.12622606647210372, 'test/num_examples': 95000000, 'score': 7215.454039096832, 'total_duration': 38142.21532678604, 'accumulated_submission_time': 7215.454039096832, 'accumulated_eval_time': 30872.061285972595, 'accumulated_logging_time': 1.4649038314819336}
I0317 01:19:07.873320 139944966141696 logging_writer.py:48] [7436] accumulated_eval_time=30872.1, accumulated_logging_time=1.4649, accumulated_submission_time=7215.45, global_step=7436, preemption_count=0, score=7215.45, test/loss=0.126226, test/num_examples=95000000, total_duration=38142.2, train/loss=0.121476, validation/loss=0.123856, validation/num_examples=83274637
I0317 01:19:56.510848 139944957748992 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.00784148, loss=0.123679
I0317 01:19:56.514333 139986877789376 submission.py:265] 7500) loss = 0.124, grad_norm = 0.008
I0317 01:21:08.329392 139986877789376 spec.py:321] Evaluating on the training split.
I0317 01:23:10.922699 139986877789376 spec.py:333] Evaluating on the validation split.
I0317 01:25:14.457635 139986877789376 spec.py:349] Evaluating on the test split.
I0317 01:29:31.723496 139986877789376 submission_runner.py:469] Time since start: 38766.10s, 	Step: 7558, 	{'train/loss': 0.12301178235510636, 'validation/loss': 0.12374395844285775, 'validation/num_examples': 83274637, 'test/loss': 0.12601901574960006, 'test/num_examples': 95000000, 'score': 7334.98770737648, 'total_duration': 38766.10066652298, 'accumulated_submission_time': 7334.98770737648, 'accumulated_eval_time': 31375.455677747726, 'accumulated_logging_time': 1.5584418773651123}
I0317 01:29:31.787917 139944966141696 logging_writer.py:48] [7558] accumulated_eval_time=31375.5, accumulated_logging_time=1.55844, accumulated_submission_time=7334.99, global_step=7558, preemption_count=0, score=7334.99, test/loss=0.126019, test/num_examples=95000000, total_duration=38766.1, train/loss=0.123012, validation/loss=0.123744, validation/num_examples=83274637
I0317 01:31:33.611638 139986877789376 spec.py:321] Evaluating on the training split.
I0317 01:33:36.081628 139986877789376 spec.py:333] Evaluating on the validation split.
I0317 01:35:39.326383 139986877789376 spec.py:349] Evaluating on the test split.
I0317 01:40:22.850330 139986877789376 submission_runner.py:469] Time since start: 39417.23s, 	Step: 7682, 	{'train/loss': 0.120139558812024, 'validation/loss': 0.123754202662769, 'validation/num_examples': 83274637, 'test/loss': 0.12609394029340243, 'test/num_examples': 95000000, 'score': 7455.904982328415, 'total_duration': 39417.22752761841, 'accumulated_submission_time': 7455.904982328415, 'accumulated_eval_time': 31904.694541931152, 'accumulated_logging_time': 1.6304645538330078}
I0317 01:40:22.861376 139944957748992 logging_writer.py:48] [7682] accumulated_eval_time=31904.7, accumulated_logging_time=1.63046, accumulated_submission_time=7455.9, global_step=7682, preemption_count=0, score=7455.9, test/loss=0.126094, test/num_examples=95000000, total_duration=39417.2, train/loss=0.12014, validation/loss=0.123754, validation/num_examples=83274637
I0317 01:42:23.527771 139986877789376 spec.py:321] Evaluating on the training split.
I0317 01:44:25.332543 139986877789376 spec.py:333] Evaluating on the validation split.
I0317 01:46:27.045603 139986877789376 spec.py:349] Evaluating on the test split.
I0317 01:51:40.809712 139986877789376 submission_runner.py:469] Time since start: 40095.19s, 	Step: 7809, 	{'train/loss': 0.1210230333031949, 'validation/loss': 0.12371318014709813, 'validation/num_examples': 83274637, 'test/loss': 0.12606208488757484, 'test/num_examples': 95000000, 'score': 7575.675233840942, 'total_duration': 40095.18686699867, 'accumulated_submission_time': 7575.675233840942, 'accumulated_eval_time': 32461.97672510147, 'accumulated_logging_time': 1.647735595703125}
I0317 01:51:40.821453 139944966141696 logging_writer.py:48] [7809] accumulated_eval_time=32462, accumulated_logging_time=1.64774, accumulated_submission_time=7575.68, global_step=7809, preemption_count=0, score=7575.68, test/loss=0.126062, test/num_examples=95000000, total_duration=40095.2, train/loss=0.121023, validation/loss=0.123713, validation/num_examples=83274637
I0317 01:51:41.220885 139944957748992 logging_writer.py:48] [7809] global_step=7809, preemption_count=0, score=7575.68
I0317 01:51:41.945601 139986877789376 submission_runner.py:646] Tuning trial 1/5
I0317 01:51:41.945860 139986877789376 submission_runner.py:647] Hyperparameters: Hyperparameters(learning_rate=0.0017486387539278373, one_minus_beta1=0.06733926164, one_minus_beta2=0.00448403102, epsilon=1e-08, one_minus_momentum=0.0, use_momentum=False, weight_decay=0.08121616522670176, max_preconditioner_dim=1024, precondition_frequency=100, start_preconditioning_step=-1, inv_root_override=0, exponent_multiplier=1.0, grafting_type='ADAM', grafting_epsilon=1e-08, use_normalized_grafting=False, communication_dtype='FP32', communicate_params=True, use_cosine_decay=True, warmup_factor=0.02, label_smoothing=0.0, dropout_rate=0.1, use_nadam=True, step_hint_factor=1.0)
I0317 01:51:41.946919 139986877789376 submission_runner.py:648] Metrics: {'eval_results': [(1, {'train/loss': 0.5967804705839551, 'validation/loss': 0.5995959329565766, 'validation/num_examples': 83274637, 'test/loss': 0.5980994658922697, 'test/num_examples': 95000000, 'score': 5.9928412437438965, 'total_duration': 950.8444991111755, 'accumulated_submission_time': 5.9928412437438965, 'accumulated_eval_time': 944.4162709712982, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (121, {'train/loss': 0.12929045388498148, 'validation/loss': 0.12952914730248755, 'validation/num_examples': 83274637, 'test/loss': 0.13216064602251554, 'test/num_examples': 95000000, 'score': 126.44877076148987, 'total_duration': 1937.4865198135376, 'accumulated_submission_time': 126.44877076148987, 'accumulated_eval_time': 1809.6618392467499, 'accumulated_logging_time': 0.015914440155029297, 'global_step': 121, 'preemption_count': 0}), (239, {'train/loss': 0.12654524661543606, 'validation/loss': 0.12861747392733178, 'validation/num_examples': 83274637, 'test/loss': 0.13079545356654118, 'test/num_examples': 95000000, 'score': 246.1599748134613, 'total_duration': 2925.727224111557, 'accumulated_submission_time': 246.1599748134613, 'accumulated_eval_time': 2677.2073838710785, 'accumulated_logging_time': 0.03339338302612305, 'global_step': 239, 'preemption_count': 0}), (359, {'train/loss': 0.1261007231785237, 'validation/loss': 0.1272863995107928, 'validation/num_examples': 83274637, 'test/loss': 0.12953205299983778, 'test/num_examples': 95000000, 'score': 366.06922721862793, 'total_duration': 3940.7137293815613, 'accumulated_submission_time': 366.06922721862793, 'accumulated_eval_time': 3571.325374364853, 'accumulated_logging_time': 0.05113697052001953, 'global_step': 359, 'preemption_count': 0}), (483, {'train/loss': 0.12557340696457348, 'validation/loss': 0.12669867375029917, 'validation/num_examples': 83274637, 'test/loss': 0.12897471061553956, 'test/num_examples': 95000000, 'score': 486.29980754852295, 'total_duration': 4958.6427545547485, 'accumulated_submission_time': 486.29980754852295, 'accumulated_eval_time': 4468.043854236603, 'accumulated_logging_time': 0.10363411903381348, 'global_step': 483, 'preemption_count': 0}), (607, {'train/loss': 0.12718614765711148, 'validation/loss': 0.12779377596865843, 'validation/num_examples': 83274637, 'test/loss': 0.12981382291191504, 'test/num_examples': 95000000, 'score': 606.6448118686676, 'total_duration': 5952.142598867416, 'accumulated_submission_time': 606.6448118686676, 'accumulated_eval_time': 5340.359853506088, 'accumulated_logging_time': 0.12112855911254883, 'global_step': 607, 'preemption_count': 0}), (731, {'train/loss': 0.124876698293085, 'validation/loss': 0.1262972372140991, 'validation/num_examples': 83274637, 'test/loss': 0.12854859651312578, 'test/num_examples': 95000000, 'score': 726.5362474918365, 'total_duration': 6938.815535068512, 'accumulated_submission_time': 726.5362474918365, 'accumulated_eval_time': 6206.2204513549805, 'accumulated_logging_time': 0.13717937469482422, 'global_step': 731, 'preemption_count': 0}), (854, {'train/loss': 0.1258747030588971, 'validation/loss': 0.12625315455303676, 'validation/num_examples': 83274637, 'test/loss': 0.1285452047476116, 'test/num_examples': 95000000, 'score': 846.6516311168671, 'total_duration': 7931.472109079361, 'accumulated_submission_time': 846.6516311168671, 'accumulated_eval_time': 7077.90834236145, 'accumulated_logging_time': 0.15391778945922852, 'global_step': 854, 'preemption_count': 0}), (979, {'train/loss': 0.12760293394518266, 'validation/loss': 0.12571204081501033, 'validation/num_examples': 83274637, 'test/loss': 0.12815281998282985, 'test/num_examples': 95000000, 'score': 966.311705827713, 'total_duration': 8916.659004926682, 'accumulated_submission_time': 966.311705827713, 'accumulated_eval_time': 7942.618627786636, 'accumulated_logging_time': 0.1713721752166748, 'global_step': 979, 'preemption_count': 0}), (1101, {'train/loss': 0.12254381910809845, 'validation/loss': 0.12614165408277692, 'validation/num_examples': 83274637, 'test/loss': 0.12845067598716334, 'test/num_examples': 95000000, 'score': 1086.5382115840912, 'total_duration': 9840.559624433517, 'accumulated_submission_time': 1086.5382115840912, 'accumulated_eval_time': 8745.524825811386, 'accumulated_logging_time': 0.18803715705871582, 'global_step': 1101, 'preemption_count': 0}), (1225, {'train/loss': 0.12351775492894013, 'validation/loss': 0.12536619297428597, 'validation/num_examples': 83274637, 'test/loss': 0.12767585662319786, 'test/num_examples': 95000000, 'score': 1206.948977470398, 'total_duration': 10696.570142507553, 'accumulated_submission_time': 1206.948977470398, 'accumulated_eval_time': 9480.259698152542, 'accumulated_logging_time': 0.2496354579925537, 'global_step': 1225, 'preemption_count': 0}), (1349, {'train/loss': 0.12562536856864243, 'validation/loss': 0.1254817203281314, 'validation/num_examples': 83274637, 'test/loss': 0.1277385695533351, 'test/num_examples': 95000000, 'score': 1327.182540655136, 'total_duration': 11436.536950588226, 'accumulated_submission_time': 1327.182540655136, 'accumulated_eval_time': 10099.049802780151, 'accumulated_logging_time': 0.26505446434020996, 'global_step': 1349, 'preemption_count': 0}), (1474, {'train/loss': 0.12529670949540694, 'validation/loss': 0.1257376246198763, 'validation/num_examples': 83274637, 'test/loss': 0.12811998187649376, 'test/num_examples': 95000000, 'score': 1447.1283748149872, 'total_duration': 12014.10776591301, 'accumulated_submission_time': 1447.1283748149872, 'accumulated_eval_time': 10555.74162864685, 'accumulated_logging_time': 0.2825963497161865, 'global_step': 1474, 'preemption_count': 0}), (1600, {'train/loss': 0.1261838787527927, 'validation/loss': 0.12585323729033052, 'validation/num_examples': 83274637, 'test/loss': 0.12829686543450605, 'test/num_examples': 95000000, 'score': 1567.5214393138885, 'total_duration': 12678.511667251587, 'accumulated_submission_time': 1567.5214393138885, 'accumulated_eval_time': 11098.792139530182, 'accumulated_logging_time': 0.299191951751709, 'global_step': 1600, 'preemption_count': 0}), (1725, {'train/loss': 0.12455017461961627, 'validation/loss': 0.12511478402270426, 'validation/num_examples': 83274637, 'test/loss': 0.12751124952356438, 'test/num_examples': 95000000, 'score': 1687.9676945209503, 'total_duration': 13258.701464891434, 'accumulated_submission_time': 1687.9676945209503, 'accumulated_eval_time': 11557.594665765762, 'accumulated_logging_time': 0.3350973129272461, 'global_step': 1725, 'preemption_count': 0}), (1849, {'train/loss': 0.12282904865986612, 'validation/loss': 0.1254858090437514, 'validation/num_examples': 83274637, 'test/loss': 0.1279028516106455, 'test/num_examples': 95000000, 'score': 1808.1840012073517, 'total_duration': 13930.011937141418, 'accumulated_submission_time': 1808.1840012073517, 'accumulated_eval_time': 12107.741386175156, 'accumulated_logging_time': 0.35057520866394043, 'global_step': 1849, 'preemption_count': 0}), (1973, {'train/loss': 0.12423397589964305, 'validation/loss': 0.12527993945969187, 'validation/num_examples': 83274637, 'test/loss': 0.12751773589630128, 'test/num_examples': 95000000, 'score': 1928.320625782013, 'total_duration': 14613.150525808334, 'accumulated_submission_time': 1928.320625782013, 'accumulated_eval_time': 12669.802761793137, 'accumulated_logging_time': 0.36676549911499023, 'global_step': 1973, 'preemption_count': 0}), (2100, {'train/loss': 0.12506163210584978, 'validation/loss': 0.12515588373828967, 'validation/num_examples': 83274637, 'test/loss': 0.1274141676658229, 'test/num_examples': 95000000, 'score': 2048.278680086136, 'total_duration': 15259.561057567596, 'accumulated_submission_time': 2048.278680086136, 'accumulated_eval_time': 13195.374210357666, 'accumulated_logging_time': 0.3855879306793213, 'global_step': 2100, 'preemption_count': 0}), (2227, {'train/loss': 0.12340524428579594, 'validation/loss': 0.1253952899879546, 'validation/num_examples': 83274637, 'test/loss': 0.1278679569109465, 'test/num_examples': 95000000, 'score': 2168.3095784187317, 'total_duration': 15927.342170715332, 'accumulated_submission_time': 2168.3095784187317, 'accumulated_eval_time': 13742.17267203331, 'accumulated_logging_time': 0.402069091796875, 'global_step': 2227, 'preemption_count': 0}), (2352, {'train/loss': 0.12429536269516334, 'validation/loss': 0.12517434331362925, 'validation/num_examples': 83274637, 'test/loss': 0.12770973048942968, 'test/num_examples': 95000000, 'score': 2288.947000980377, 'total_duration': 16594.554953336716, 'accumulated_submission_time': 2288.947000980377, 'accumulated_eval_time': 14287.864269018173, 'accumulated_logging_time': 0.41991639137268066, 'global_step': 2352, 'preemption_count': 0}), (2475, {'train/loss': 0.12421866557264956, 'validation/loss': 0.12547087323802827, 'validation/num_examples': 83274637, 'test/loss': 0.12796865269646895, 'test/num_examples': 95000000, 'score': 2409.581680059433, 'total_duration': 17130.57651257515, 'accumulated_submission_time': 2409.581680059433, 'accumulated_eval_time': 14702.347155570984, 'accumulated_logging_time': 0.4376943111419678, 'global_step': 2475, 'preemption_count': 0}), (2598, {'train/loss': 0.12479094837343505, 'validation/loss': 0.1252643962353461, 'validation/num_examples': 83274637, 'test/loss': 0.12765837583047968, 'test/num_examples': 95000000, 'score': 2529.5888845920563, 'total_duration': 17642.357486486435, 'accumulated_submission_time': 2529.5888845920563, 'accumulated_eval_time': 15093.189688682556, 'accumulated_logging_time': 0.45548081398010254, 'global_step': 2598, 'preemption_count': 0}), (2724, {'train/loss': 0.12328262261924677, 'validation/loss': 0.12501506667993337, 'validation/num_examples': 83274637, 'test/loss': 0.12746548940220884, 'test/num_examples': 95000000, 'score': 2650.0613491535187, 'total_duration': 18156.602919578552, 'accumulated_submission_time': 2650.0613491535187, 'accumulated_eval_time': 15486.115355968475, 'accumulated_logging_time': 0.4859898090362549, 'global_step': 2724, 'preemption_count': 0}), (2851, {'train/loss': 0.12452472169874755, 'validation/loss': 0.125381837365852, 'validation/num_examples': 83274637, 'test/loss': 0.12780912855859053, 'test/num_examples': 95000000, 'score': 2770.1883223056793, 'total_duration': 18670.317764997482, 'accumulated_submission_time': 2770.1883223056793, 'accumulated_eval_time': 15878.73873257637, 'accumulated_logging_time': 0.5085866451263428, 'global_step': 2851, 'preemption_count': 0}), (2977, {'train/loss': 0.12228472697473997, 'validation/loss': 0.12515656402759043, 'validation/num_examples': 83274637, 'test/loss': 0.12733072388410066, 'test/num_examples': 95000000, 'score': 2890.2491257190704, 'total_duration': 19184.614201784134, 'accumulated_submission_time': 2890.2491257190704, 'accumulated_eval_time': 16272.068244934082, 'accumulated_logging_time': 0.5289978981018066, 'global_step': 2977, 'preemption_count': 0}), (3099, {'train/loss': 0.12130272276257059, 'validation/loss': 0.12497562339099547, 'validation/num_examples': 83274637, 'test/loss': 0.12697630198589124, 'test/num_examples': 95000000, 'score': 3010.984312772751, 'total_duration': 19698.606773376465, 'accumulated_submission_time': 3010.984312772751, 'accumulated_eval_time': 16664.451105833054, 'accumulated_logging_time': 0.5475137233734131, 'global_step': 3099, 'preemption_count': 0}), (3218, {'train/loss': 0.1240839149189859, 'validation/loss': 0.12473288380814102, 'validation/num_examples': 83274637, 'test/loss': 0.1269867311092979, 'test/num_examples': 95000000, 'score': 3130.98251080513, 'total_duration': 20209.1790163517, 'accumulated_submission_time': 3130.98251080513, 'accumulated_eval_time': 17054.185025453568, 'accumulated_logging_time': 0.5672502517700195, 'global_step': 3218, 'preemption_count': 0}), (3341, {'train/loss': 0.12567785640106818, 'validation/loss': 0.12487115903259279, 'validation/num_examples': 83274637, 'test/loss': 0.12724782019629227, 'test/num_examples': 95000000, 'score': 3250.9902317523956, 'total_duration': 20818.07008433342, 'accumulated_submission_time': 3250.9902317523956, 'accumulated_eval_time': 17542.116401433945, 'accumulated_logging_time': 0.5983700752258301, 'global_step': 3341, 'preemption_count': 0}), (3467, {'train/loss': 0.12622615718571337, 'validation/loss': 0.12479017747074063, 'validation/num_examples': 83274637, 'test/loss': 0.12711589282993518, 'test/num_examples': 95000000, 'score': 3371.9157547950745, 'total_duration': 21501.863481521606, 'accumulated_submission_time': 3371.9157547950745, 'accumulated_eval_time': 18104.062248706818, 'accumulated_logging_time': 0.6150655746459961, 'global_step': 3467, 'preemption_count': 0}), (3588, {'train/loss': 0.12493945824243634, 'validation/loss': 0.1248606487833527, 'validation/num_examples': 83274637, 'test/loss': 0.1271577419879311, 'test/num_examples': 95000000, 'score': 3491.8086977005005, 'total_duration': 22175.981496334076, 'accumulated_submission_time': 3491.8086977005005, 'accumulated_eval_time': 18657.35998058319, 'accumulated_logging_time': 0.6338131427764893, 'global_step': 3588, 'preemption_count': 0}), (3711, {'train/loss': 0.12271506807052468, 'validation/loss': 0.12498088101956117, 'validation/num_examples': 83274637, 'test/loss': 0.12747368079605104, 'test/num_examples': 95000000, 'score': 3611.419188261032, 'total_duration': 22816.365347623825, 'accumulated_submission_time': 3611.419188261032, 'accumulated_eval_time': 19177.313400268555, 'accumulated_logging_time': 0.6510157585144043, 'global_step': 3711, 'preemption_count': 0}), (3833, {'train/loss': 0.12458286968955981, 'validation/loss': 0.1254453387102873, 'validation/num_examples': 83274637, 'test/loss': 0.1278129607993678, 'test/num_examples': 95000000, 'score': 3731.267385005951, 'total_duration': 23331.43080496788, 'accumulated_submission_time': 3731.267385005951, 'accumulated_eval_time': 19571.58188867569, 'accumulated_logging_time': 0.6902680397033691, 'global_step': 3833, 'preemption_count': 0}), (3957, {'train/loss': 0.12149816660363298, 'validation/loss': 0.12471971914226485, 'validation/num_examples': 83274637, 'test/loss': 0.1270592860603734, 'test/num_examples': 95000000, 'score': 3851.760407924652, 'total_duration': 23845.247673511505, 'accumulated_submission_time': 3851.760407924652, 'accumulated_eval_time': 19964.02088713646, 'accumulated_logging_time': 0.7090356349945068, 'global_step': 3957, 'preemption_count': 0}), (4078, {'train/loss': 0.12378081426271971, 'validation/loss': 0.12468606476285714, 'validation/num_examples': 83274637, 'test/loss': 0.1270273658237658, 'test/num_examples': 95000000, 'score': 3972.3173167705536, 'total_duration': 24359.845747232437, 'accumulated_submission_time': 3972.3173167705536, 'accumulated_eval_time': 20357.098534584045, 'accumulated_logging_time': 0.7285640239715576, 'global_step': 4078, 'preemption_count': 0}), (4200, {'train/loss': 0.12351428697632522, 'validation/loss': 0.12455166686106513, 'validation/num_examples': 83274637, 'test/loss': 0.12687963547178568, 'test/num_examples': 95000000, 'score': 4092.0875413417816, 'total_duration': 24872.492672920227, 'accumulated_submission_time': 4092.0875413417816, 'accumulated_eval_time': 20749.06259894371, 'accumulated_logging_time': 0.7707626819610596, 'global_step': 4200, 'preemption_count': 0}), (4320, {'train/loss': 0.12428829293384211, 'validation/loss': 0.12458359610939657, 'validation/num_examples': 83274637, 'test/loss': 0.1270097317562304, 'test/num_examples': 95000000, 'score': 4213.053079366684, 'total_duration': 25387.568189144135, 'accumulated_submission_time': 4213.053079366684, 'accumulated_eval_time': 21142.28586101532, 'accumulated_logging_time': 0.7884151935577393, 'global_step': 4320, 'preemption_count': 0}), (4446, {'train/loss': 0.12296701333146215, 'validation/loss': 0.12463263138169361, 'validation/num_examples': 83274637, 'test/loss': 0.12699972023620607, 'test/num_examples': 95000000, 'score': 4332.754045963287, 'total_duration': 25901.20740222931, 'accumulated_submission_time': 4332.754045963287, 'accumulated_eval_time': 21535.352833747864, 'accumulated_logging_time': 0.8056685924530029, 'global_step': 4446, 'preemption_count': 0}), (4573, {'train/loss': 0.12299708577051403, 'validation/loss': 0.1247469869771901, 'validation/num_examples': 83274637, 'test/loss': 0.127256416310481, 'test/num_examples': 95000000, 'score': 4452.261522531509, 'total_duration': 26413.982054948807, 'accumulated_submission_time': 4452.261522531509, 'accumulated_eval_time': 21927.65883374214, 'accumulated_logging_time': 0.822965145111084, 'global_step': 4573, 'preemption_count': 0}), (4699, {'train/loss': 0.1248974298962859, 'validation/loss': 0.1246522753990138, 'validation/num_examples': 83274637, 'test/loss': 0.12700907233075592, 'test/num_examples': 95000000, 'score': 4572.159146785736, 'total_duration': 26926.785030841827, 'accumulated_submission_time': 4572.159146785736, 'accumulated_eval_time': 22319.669251918793, 'accumulated_logging_time': 0.8414256572723389, 'global_step': 4699, 'preemption_count': 0}), (4822, {'train/loss': 0.12422485603688359, 'validation/loss': 0.12465599421139004, 'validation/num_examples': 83274637, 'test/loss': 0.12711116010661877, 'test/num_examples': 95000000, 'score': 4691.888939142227, 'total_duration': 27439.225314855576, 'accumulated_submission_time': 4691.888939142227, 'accumulated_eval_time': 22711.500751018524, 'accumulated_logging_time': 0.8600952625274658, 'global_step': 4822, 'preemption_count': 0}), (4946, {'train/loss': 0.12294913369371825, 'validation/loss': 0.12502464820980833, 'validation/num_examples': 83274637, 'test/loss': 0.12755271683172928, 'test/num_examples': 95000000, 'score': 4811.971235990524, 'total_duration': 27953.249688386917, 'accumulated_submission_time': 4811.971235990524, 'accumulated_eval_time': 23104.504133462906, 'accumulated_logging_time': 0.9380571842193604, 'global_step': 4946, 'preemption_count': 0}), (5071, {'train/loss': 0.12433508531289945, 'validation/loss': 0.12458647602663173, 'validation/num_examples': 83274637, 'test/loss': 0.12685982146835328, 'test/num_examples': 95000000, 'score': 4932.076211214066, 'total_duration': 28466.456826925278, 'accumulated_submission_time': 4932.076211214066, 'accumulated_eval_time': 23496.6583507061, 'accumulated_logging_time': 0.9632937908172607, 'global_step': 5071, 'preemption_count': 0}), (5190, {'train/loss': 0.1248338533576487, 'validation/loss': 0.12415056748721312, 'validation/num_examples': 83274637, 'test/loss': 0.12657204358476337, 'test/num_examples': 95000000, 'score': 5051.599883794785, 'total_duration': 28977.937035799026, 'accumulated_submission_time': 5051.599883794785, 'accumulated_eval_time': 23887.70643877983, 'accumulated_logging_time': 0.9826030731201172, 'global_step': 5190, 'preemption_count': 0}), (5315, {'train/loss': 0.12423565536996703, 'validation/loss': 0.12420020001356366, 'validation/num_examples': 83274637, 'test/loss': 0.12660823988535028, 'test/num_examples': 95000000, 'score': 5172.187809705734, 'total_duration': 29489.326233148575, 'accumulated_submission_time': 5172.187809705734, 'accumulated_eval_time': 24277.576689243317, 'accumulated_logging_time': 1.008660078048706, 'global_step': 5315, 'preemption_count': 0}), (5439, {'train/loss': 0.12237444226001577, 'validation/loss': 0.12466223047277361, 'validation/num_examples': 83274637, 'test/loss': 0.12700093340092708, 'test/num_examples': 95000000, 'score': 5292.217370271683, 'total_duration': 30000.978651046753, 'accumulated_submission_time': 5292.217370271683, 'accumulated_eval_time': 24668.3408575058, 'accumulated_logging_time': 1.0277798175811768, 'global_step': 5439, 'preemption_count': 0}), (5562, {'train/loss': 0.12309596578565103, 'validation/loss': 0.12435470075432375, 'validation/num_examples': 83274637, 'test/loss': 0.12656558547748767, 'test/num_examples': 95000000, 'score': 5411.794659614563, 'total_duration': 30510.608814001083, 'accumulated_submission_time': 5411.794659614563, 'accumulated_eval_time': 25057.497400045395, 'accumulated_logging_time': 1.0460309982299805, 'global_step': 5562, 'preemption_count': 0}), (5689, {'train/loss': 0.12277620498114068, 'validation/loss': 0.12419676794078235, 'validation/num_examples': 83274637, 'test/loss': 0.12643836911460474, 'test/num_examples': 95000000, 'score': 5532.273994207382, 'total_duration': 31022.799132823944, 'accumulated_submission_time': 5532.273994207382, 'accumulated_eval_time': 25448.315858840942, 'accumulated_logging_time': 1.0642931461334229, 'global_step': 5689, 'preemption_count': 0}), (5814, {'train/loss': 0.12389250463439097, 'validation/loss': 0.12426123714297471, 'validation/num_examples': 83274637, 'test/loss': 0.12652646537459525, 'test/num_examples': 95000000, 'score': 5653.040225505829, 'total_duration': 31533.918437719345, 'accumulated_submission_time': 5653.040225505829, 'accumulated_eval_time': 25837.7742562294, 'accumulated_logging_time': 1.1082062721252441, 'global_step': 5814, 'preemption_count': 0}), (5938, {'train/loss': 0.12175452094122924, 'validation/loss': 0.1241455003310799, 'validation/num_examples': 83274637, 'test/loss': 0.12630939612298261, 'test/num_examples': 95000000, 'score': 5773.279890298843, 'total_duration': 32042.073843240738, 'accumulated_submission_time': 5773.279890298843, 'accumulated_eval_time': 26224.75915670395, 'accumulated_logging_time': 1.1514701843261719, 'global_step': 5938, 'preemption_count': 0}), (6062, {'train/loss': 0.12262679893407474, 'validation/loss': 0.12427901395397312, 'validation/num_examples': 83274637, 'test/loss': 0.12668251021045884, 'test/num_examples': 95000000, 'score': 5893.560338497162, 'total_duration': 32549.425038337708, 'accumulated_submission_time': 5893.560338497162, 'accumulated_eval_time': 26610.895847797394, 'accumulated_logging_time': 1.1698927879333496, 'global_step': 6062, 'preemption_count': 0}), (6187, {'train/loss': 0.12268381970145795, 'validation/loss': 0.12394535974545536, 'validation/num_examples': 83274637, 'test/loss': 0.12628360456310073, 'test/num_examples': 95000000, 'score': 6013.80818271637, 'total_duration': 33057.01746273041, 'accumulated_submission_time': 6013.80818271637, 'accumulated_eval_time': 26997.28810954094, 'accumulated_logging_time': 1.1887176036834717, 'global_step': 6187, 'preemption_count': 0}), (6314, {'train/loss': 0.1222927982279195, 'validation/loss': 0.12405813671971737, 'validation/num_examples': 83274637, 'test/loss': 0.12633043124502083, 'test/num_examples': 95000000, 'score': 6135.070326566696, 'total_duration': 33564.88579106331, 'accumulated_submission_time': 6135.070326566696, 'accumulated_eval_time': 27383.18753170967, 'accumulated_logging_time': 1.2063744068145752, 'global_step': 6314, 'preemption_count': 0}), (6439, {'train/loss': 0.12261034332189402, 'validation/loss': 0.12428007647760606, 'validation/num_examples': 83274637, 'test/loss': 0.12666987094686408, 'test/num_examples': 95000000, 'score': 6254.854168176651, 'total_duration': 34071.62265944481, 'accumulated_submission_time': 6254.854168176651, 'accumulated_eval_time': 27769.20412325859, 'accumulated_logging_time': 1.2425200939178467, 'global_step': 6439, 'preemption_count': 0}), (6564, {'train/loss': 0.1236904086156677, 'validation/loss': 0.12406670213253225, 'validation/num_examples': 83274637, 'test/loss': 0.12639469705497342, 'test/num_examples': 95000000, 'score': 6375.659338951111, 'total_duration': 34581.2901904583, 'accumulated_submission_time': 6375.659338951111, 'accumulated_eval_time': 28157.18403029442, 'accumulated_logging_time': 1.2619872093200684, 'global_step': 6564, 'preemption_count': 0}), (6690, {'train/loss': 0.1226907485344627, 'validation/loss': 0.12397244078212111, 'validation/num_examples': 83274637, 'test/loss': 0.12623934590221203, 'test/num_examples': 95000000, 'score': 6495.253893136978, 'total_duration': 35090.52767205238, 'accumulated_submission_time': 6495.253893136978, 'accumulated_eval_time': 28545.933460235596, 'accumulated_logging_time': 1.2810475826263428, 'global_step': 6690, 'preemption_count': 0}), (6815, {'train/loss': 0.12305403754458465, 'validation/loss': 0.12394697510237317, 'validation/num_examples': 83274637, 'test/loss': 0.1261858419416327, 'test/num_examples': 95000000, 'score': 6615.686529874802, 'total_duration': 35598.457434892654, 'accumulated_submission_time': 6615.686529874802, 'accumulated_eval_time': 28932.529005527496, 'accumulated_logging_time': 1.3006353378295898, 'global_step': 6815, 'preemption_count': 0}), (6937, {'train/loss': 0.1218845704796693, 'validation/loss': 0.12400140782139847, 'validation/num_examples': 83274637, 'test/loss': 0.1263985596414265, 'test/num_examples': 95000000, 'score': 6736.276186943054, 'total_duration': 36109.37138962746, 'accumulated_submission_time': 6736.276186943054, 'accumulated_eval_time': 29321.948405981064, 'accumulated_logging_time': 1.3193466663360596, 'global_step': 6937, 'preemption_count': 0}), (7061, {'train/loss': 0.1211855189356522, 'validation/loss': 0.12385684483621552, 'validation/num_examples': 83274637, 'test/loss': 0.12621983461102937, 'test/num_examples': 95000000, 'score': 6856.094105005264, 'total_duration': 36617.03927612305, 'accumulated_submission_time': 6856.094105005264, 'accumulated_eval_time': 29708.820055246353, 'accumulated_logging_time': 1.4069323539733887, 'global_step': 7061, 'preemption_count': 0}), (7187, {'train/loss': 0.1208851519014196, 'validation/loss': 0.12398135594389302, 'validation/num_examples': 83274637, 'test/loss': 0.126351320727258, 'test/num_examples': 95000000, 'score': 6975.676568508148, 'total_duration': 37126.995483636856, 'accumulated_submission_time': 6975.676568508148, 'accumulated_eval_time': 30098.292333841324, 'accumulated_logging_time': 1.4253897666931152, 'global_step': 7187, 'preemption_count': 0}), (7309, {'train/loss': 0.12319936492622702, 'validation/loss': 0.12377459209460696, 'validation/num_examples': 83274637, 'test/loss': 0.12610510275718287, 'test/num_examples': 95000000, 'score': 7095.7635107040405, 'total_duration': 37633.88337945938, 'accumulated_submission_time': 7095.7635107040405, 'accumulated_eval_time': 30484.257103443146, 'accumulated_logging_time': 1.4443144798278809, 'global_step': 7309, 'preemption_count': 0}), (7436, {'train/loss': 0.12147637310791662, 'validation/loss': 0.12385566411989682, 'validation/num_examples': 83274637, 'test/loss': 0.12622606647210372, 'test/num_examples': 95000000, 'score': 7215.454039096832, 'total_duration': 38142.21532678604, 'accumulated_submission_time': 7215.454039096832, 'accumulated_eval_time': 30872.061285972595, 'accumulated_logging_time': 1.4649038314819336, 'global_step': 7436, 'preemption_count': 0}), (7558, {'train/loss': 0.12301178235510636, 'validation/loss': 0.12374395844285775, 'validation/num_examples': 83274637, 'test/loss': 0.12601901574960006, 'test/num_examples': 95000000, 'score': 7334.98770737648, 'total_duration': 38766.10066652298, 'accumulated_submission_time': 7334.98770737648, 'accumulated_eval_time': 31375.455677747726, 'accumulated_logging_time': 1.5584418773651123, 'global_step': 7558, 'preemption_count': 0}), (7682, {'train/loss': 0.120139558812024, 'validation/loss': 0.123754202662769, 'validation/num_examples': 83274637, 'test/loss': 0.12609394029340243, 'test/num_examples': 95000000, 'score': 7455.904982328415, 'total_duration': 39417.22752761841, 'accumulated_submission_time': 7455.904982328415, 'accumulated_eval_time': 31904.694541931152, 'accumulated_logging_time': 1.6304645538330078, 'global_step': 7682, 'preemption_count': 0}), (7809, {'train/loss': 0.1210230333031949, 'validation/loss': 0.12371318014709813, 'validation/num_examples': 83274637, 'test/loss': 0.12606208488757484, 'test/num_examples': 95000000, 'score': 7575.675233840942, 'total_duration': 40095.18686699867, 'accumulated_submission_time': 7575.675233840942, 'accumulated_eval_time': 32461.97672510147, 'accumulated_logging_time': 1.647735595703125, 'global_step': 7809, 'preemption_count': 0})], 'global_step': 7809}
I0317 01:51:41.947024 139986877789376 submission_runner.py:649] Timing: 7575.675233840942
I0317 01:51:41.947061 139986877789376 submission_runner.py:651] Total number of evals: 64
I0317 01:51:41.947090 139986877789376 submission_runner.py:652] ====================
I0317 01:51:41.947213 139986877789376 submission_runner.py:750] Final criteo1tb score: 0
