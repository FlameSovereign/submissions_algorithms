torchrun --redirects 1:0,2:0,3:0,4:0,5:0,6:0,7:0 --standalone --nnodes=1 --nproc_per_node=8 submission_runner.py --framework=pytorch --workload=criteo1tb --submission_path=submissions_algorithms/leaderboard/external_tuning/shampoo_submission/submission.py --data_dir=/data/criteo1tb --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=submissions/rolling_leaderboard/external_tuning/shampoo/study_2 --overwrite=True --save_checkpoints=False --rng_seed=-1175070779 --torch_compile=true --tuning_ruleset=external --tuning_search_space=submissions_algorithms/leaderboard/external_tuning/shampoo_submission/tuning_search_space.json --num_tuning_trials=5 --hparam_start_index=4 --hparam_end_index=5 2>&1 | tee -a /logs/criteo1tb_pytorch_03-16-2025-15-41-19.log
W0316 15:41:20.979000 9 site-packages/torch/distributed/run.py:793] 
W0316 15:41:20.979000 9 site-packages/torch/distributed/run.py:793] *****************************************
W0316 15:41:20.979000 9 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0316 15:41:20.979000 9 site-packages/torch/distributed/run.py:793] *****************************************
2025-03-16 15:41:22.050965: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-03-16 15:41:22.050969: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-03-16 15:41:22.050965: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-03-16 15:41:22.050968: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-03-16 15:41:22.050977: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-03-16 15:41:22.050965: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-03-16 15:41:22.050964: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-03-16 15:41:22.050977: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1742139682.073569      48 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1742139682.073571      50 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1742139682.073570      49 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1742139682.073569      47 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1742139682.073581      46 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1742139682.073569      51 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1742139682.073567      45 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1742139682.073790      44 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1742139682.080629      49 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
E0000 00:00:1742139682.080630      47 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
E0000 00:00:1742139682.080629      46 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
E0000 00:00:1742139682.080629      50 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
E0000 00:00:1742139682.080629      48 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
E0000 00:00:1742139682.080632      51 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
E0000 00:00:1742139682.080633      45 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
E0000 00:00:1742139682.080731      44 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
[rank1]:[W316 15:41:28.906549498 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank3]:[W316 15:41:29.987995829 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank4]:[W316 15:41:29.023365410 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 4]  using GPU 4 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
[rank0]:[W316 15:41:29.076435144 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank7]:[W316 15:41:29.077772532 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 7]  using GPU 7 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank5]:[W316 15:41:29.111983790 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 5]  using GPU 5 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank6]:[W316 15:41:29.112235496 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 6]  using GPU 6 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank2]:[W316 15:41:29.304554289 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
I0316 15:41:31.003587 139931299886272 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_2/criteo1tb_pytorch.
I0316 15:41:31.003587 139652994569408 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_2/criteo1tb_pytorch.
I0316 15:41:31.003587 140078117823680 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_2/criteo1tb_pytorch.
I0316 15:41:31.003588 140219325035712 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_2/criteo1tb_pytorch.
I0316 15:41:31.003598 140125290058944 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_2/criteo1tb_pytorch.
I0316 15:41:31.003596 140181208745152 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_2/criteo1tb_pytorch.
I0316 15:41:31.003607 140175372387520 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_2/criteo1tb_pytorch.
I0316 15:41:31.003680 139897529996480 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_2/criteo1tb_pytorch.
I0316 15:41:32.186256 139931299886272 logger_utils.py:97] Saving hparams to /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_2/criteo1tb_pytorch/trial_5/hparams.json.
I0316 15:41:32.186263 139897529996480 logger_utils.py:97] Saving hparams to /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_2/criteo1tb_pytorch/trial_5/hparams.json.
I0316 15:41:32.186271 140078117823680 logger_utils.py:97] Saving hparams to /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_2/criteo1tb_pytorch/trial_5/hparams.json.
I0316 15:41:32.186671 140125290058944 logger_utils.py:97] Saving hparams to /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_2/criteo1tb_pytorch/trial_5/hparams.json.
I0316 15:41:32.187103 139652994569408 logger_utils.py:97] Saving hparams to /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_2/criteo1tb_pytorch/trial_5/hparams.json.
I0316 15:41:32.187850 140219325035712 logger_utils.py:97] Saving hparams to /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_2/criteo1tb_pytorch/trial_5/hparams.json.
I0316 15:41:32.189019 140175372387520 logger_utils.py:97] Saving hparams to /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_2/criteo1tb_pytorch/trial_5/hparams.json.
I0316 15:41:32.189849 140181208745152 submission_runner.py:606] Using RNG seed -1175070779
I0316 15:41:32.191048 140181208745152 submission_runner.py:615] --- Tuning run 5/5 ---
I0316 15:41:32.191166 140181208745152 submission_runner.py:620] Creating tuning directory at /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_2/criteo1tb_pytorch/trial_5.
I0316 15:41:32.191404 140181208745152 logger_utils.py:97] Saving hparams to /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_2/criteo1tb_pytorch/trial_5/hparams.json.
I0316 15:41:32.522053 140181208745152 submission_runner.py:218] Initializing dataset.
I0316 15:41:32.522241 140181208745152 submission_runner.py:229] Initializing model.
W0316 15:41:38.259547 140181208745152 submission_runner.py:254] These workloads cannot be fully compiled under current PyTorch version. Proceeding without `torch.compile`.
I0316 15:41:38.259719 140181208745152 submission_runner.py:272] Initializing optimizer.
W0316 15:41:38.260766 140181208745152 distributed_shampoo.py:341] start_preconditioning_step set to -1. Setting start_preconditioning_step equal to precondition frequency 100 by default.
W0316 15:41:38.260872 140181208745152 distributed_shampoo.py:359] Nesterov flag is enabled but momentum parameter is zero! Continuing without using momentum or Nesterov acceleration...
W0316 15:41:38.260972 140125290058944 submission_runner.py:254] These workloads cannot be fully compiled under current PyTorch version. Proceeding without `torch.compile`.
W0316 15:41:38.261213 140219325035712 submission_runner.py:254] These workloads cannot be fully compiled under current PyTorch version. Proceeding without `torch.compile`.
W0316 15:41:38.261316 140175372387520 submission_runner.py:254] These workloads cannot be fully compiled under current PyTorch version. Proceeding without `torch.compile`.
W0316 15:41:38.261458 140078117823680 submission_runner.py:254] These workloads cannot be fully compiled under current PyTorch version. Proceeding without `torch.compile`.
W0316 15:41:38.261545 139897529996480 submission_runner.py:254] These workloads cannot be fully compiled under current PyTorch version. Proceeding without `torch.compile`.
W0316 15:41:38.261606 139652994569408 submission_runner.py:254] These workloads cannot be fully compiled under current PyTorch version. Proceeding without `torch.compile`.
W0316 15:41:38.261715 139931299886272 submission_runner.py:254] These workloads cannot be fully compiled under current PyTorch version. Proceeding without `torch.compile`.
W0316 15:41:38.262130 140125290058944 distributed_shampoo.py:341] start_preconditioning_step set to -1. Setting start_preconditioning_step equal to precondition frequency 100 by default.
W0316 15:41:38.262263 140125290058944 distributed_shampoo.py:359] Nesterov flag is enabled but momentum parameter is zero! Continuing without using momentum or Nesterov acceleration...
W0316 15:41:38.262285 140219325035712 distributed_shampoo.py:341] start_preconditioning_step set to -1. Setting start_preconditioning_step equal to precondition frequency 100 by default.
W0316 15:41:38.262396 140219325035712 distributed_shampoo.py:359] Nesterov flag is enabled but momentum parameter is zero! Continuing without using momentum or Nesterov acceleration...
W0316 15:41:38.262403 140175372387520 distributed_shampoo.py:341] start_preconditioning_step set to -1. Setting start_preconditioning_step equal to precondition frequency 100 by default.
W0316 15:41:38.262519 140175372387520 distributed_shampoo.py:359] Nesterov flag is enabled but momentum parameter is zero! Continuing without using momentum or Nesterov acceleration...
W0316 15:41:38.262579 140078117823680 distributed_shampoo.py:341] start_preconditioning_step set to -1. Setting start_preconditioning_step equal to precondition frequency 100 by default.
W0316 15:41:38.262642 139897529996480 distributed_shampoo.py:341] start_preconditioning_step set to -1. Setting start_preconditioning_step equal to precondition frequency 100 by default.
W0316 15:41:38.262709 140078117823680 distributed_shampoo.py:359] Nesterov flag is enabled but momentum parameter is zero! Continuing without using momentum or Nesterov acceleration...
W0316 15:41:38.262762 139897529996480 distributed_shampoo.py:359] Nesterov flag is enabled but momentum parameter is zero! Continuing without using momentum or Nesterov acceleration...
W0316 15:41:38.262705 139652994569408 distributed_shampoo.py:341] start_preconditioning_step set to -1. Setting start_preconditioning_step equal to precondition frequency 100 by default.
W0316 15:41:38.262819 139652994569408 distributed_shampoo.py:359] Nesterov flag is enabled but momentum parameter is zero! Continuing without using momentum or Nesterov acceleration...
W0316 15:41:38.262988 139931299886272 distributed_shampoo.py:341] start_preconditioning_step set to -1. Setting start_preconditioning_step equal to precondition frequency 100 by default.
W0316 15:41:38.263108 139931299886272 distributed_shampoo.py:359] Nesterov flag is enabled but momentum parameter is zero! Continuing without using momentum or Nesterov acceleration...
I0316 15:41:38.263313 140181208745152 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 0.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 0 (torch.Size([512, 13])), Block block_0 (torch.Size([512, 13])).
I0316 15:41:38.263471 140181208745152 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 1.block_0 ([torch.Size([0])]) for Parameter 1 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 15:41:38.263613 140181208745152 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 2.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 2 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 15:41:38.263719 140181208745152 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 3.block_0 ([torch.Size([0])]) for Parameter 3 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 15:41:38.263852 140181208745152 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 4.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 4 (torch.Size([128, 256])), Block block_0 (torch.Size([128, 256])).
I0316 15:41:38.263950 140181208745152 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 5.block_0 ([torch.Size([0])]) for Parameter 5 (torch.Size([128])), Block block_0 (torch.Size([128])).
I0316 15:41:38.264076 140181208745152 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 6.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 6 (torch.Size([1024, 506])), Block block_0 (torch.Size([1024, 506])).
I0316 15:41:38.264183 140181208745152 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 7.block_0 ([torch.Size([0])]) for Parameter 7 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 15:41:38.264404 140125290058944 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 0.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 0 (torch.Size([512, 13])), Block block_0 (torch.Size([512, 13])).
I0316 15:41:38.264990 140175372387520 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 0.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 0 (torch.Size([512, 13])), Block block_0 (torch.Size([512, 13])).
I0316 15:41:38.265133 140181208745152 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 8.block_0 ([torch.Size([1024, 1024]), torch.Size([1024, 1024])]) for Parameter 8 (torch.Size([1024, 1024])), Block block_0 (torch.Size([1024, 1024])).
I0316 15:41:38.265158 140175372387520 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 1.block_0 ([torch.Size([0])]) for Parameter 1 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 15:41:38.265303 140181208745152 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 9.block_0 ([torch.Size([0])]) for Parameter 9 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 15:41:38.265328 140175372387520 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 2.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 2 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 15:41:38.265300 140078117823680 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 0.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 0 (torch.Size([512, 13])), Block block_0 (torch.Size([512, 13])).
I0316 15:41:38.265402 140125290058944 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 1.block_0 ([torch.Size([512, 512])]) for Parameter 1 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 15:41:38.265445 140181208745152 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 10.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 10 (torch.Size([512, 1024])), Block block_0 (torch.Size([512, 1024])).
I0316 15:41:38.265449 140175372387520 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 3.block_0 ([torch.Size([0])]) for Parameter 3 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 15:41:38.265473 140078117823680 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 1.block_0 ([torch.Size([0])]) for Parameter 1 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 15:41:38.265455 139897529996480 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 0.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 0 (torch.Size([512, 13])), Block block_0 (torch.Size([512, 13])).
I0316 15:41:38.265529 140181208745152 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 11.block_0 ([torch.Size([0])]) for Parameter 11 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 15:41:38.265511 140219325035712 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 0.block_0 ([torch.Size([512, 512]), torch.Size([13, 13])]) for Parameter 0 (torch.Size([512, 13])), Block block_0 (torch.Size([512, 13])).
I0316 15:41:38.265564 140175372387520 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 4.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 4 (torch.Size([128, 256])), Block block_0 (torch.Size([128, 256])).
I0316 15:41:38.265556 139652994569408 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 0.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 0 (torch.Size([512, 13])), Block block_0 (torch.Size([512, 13])).
I0316 15:41:38.265593 140125290058944 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 2.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 2 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 15:41:38.265640 140181208745152 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 12.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 12 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 15:41:38.265627 140078117823680 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 2.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 2 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 15:41:38.265636 139897529996480 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 1.block_0 ([torch.Size([0])]) for Parameter 1 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 15:41:38.265649 140175372387520 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 5.block_0 ([torch.Size([0])]) for Parameter 5 (torch.Size([128])), Block block_0 (torch.Size([128])).
I0316 15:41:38.265686 140219325035712 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 1.block_0 ([torch.Size([0])]) for Parameter 1 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 15:41:38.265731 140181208745152 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 13.block_0 ([torch.Size([0])]) for Parameter 13 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 15:41:38.265736 140078117823680 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 3.block_0 ([torch.Size([0])]) for Parameter 3 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 15:41:38.265733 139652994569408 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 1.block_0 ([torch.Size([0])]) for Parameter 1 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 15:41:38.265764 140125290058944 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 3.block_0 ([torch.Size([256, 256])]) for Parameter 3 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 15:41:38.265780 139897529996480 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 2.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 2 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 15:41:38.265821 140181208745152 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 14.block_0 ([torch.Size([0])]) for Parameter 14 (torch.Size([1, 256])), Block block_0 (torch.Size([256])).
I0316 15:41:38.265851 140078117823680 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 4.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 4 (torch.Size([128, 256])), Block block_0 (torch.Size([128, 256])).
I0316 15:41:38.265840 140219325035712 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 2.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 2 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 15:41:38.265878 139652994569408 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 2.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 2 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 15:41:38.265889 139897529996480 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 3.block_0 ([torch.Size([0])]) for Parameter 3 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 15:41:38.265911 140181208745152 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 15.block_0 ([torch.Size([0])]) for Parameter 15 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0316 15:41:38.265919 140125290058944 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 4.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 4 (torch.Size([128, 256])), Block block_0 (torch.Size([128, 256])).
I0316 15:41:38.265950 140219325035712 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 3.block_0 ([torch.Size([0])]) for Parameter 3 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 15:41:38.265954 140078117823680 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 5.block_0 ([torch.Size([0])]) for Parameter 5 (torch.Size([128])), Block block_0 (torch.Size([128])).
I0316 15:41:38.265994 139652994569408 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 3.block_0 ([torch.Size([0])]) for Parameter 3 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 15:41:38.266000 140181208745152 shampoo_preconditioner_list.py:612] Rank 0: ShampooPreconditionerList Numel Breakdown: (524626, 524288, 655360, 131072, 163840, 32768, 2609224, 2097152, 4194304, 2097152, 2621440, 524288, 655360, 131072, 131072, 2)
I0316 15:41:38.266006 139897529996480 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 4.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 4 (torch.Size([128, 256])), Block block_0 (torch.Size([128, 256])).
I0316 15:41:38.266037 140181208745152 shampoo_preconditioner_list.py:615] Rank 0: ShampooPreconditionerList Bytes Breakdown: (2098504,)
I0316 15:41:38.266080 140181208745152 shampoo_preconditioner_list.py:618] Rank 0: ShampooPreconditionerList Total Elements: 17093020
I0316 15:41:38.266079 140078117823680 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 6.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 6 (torch.Size([1024, 506])), Block block_0 (torch.Size([1024, 506])).
I0316 15:41:38.266083 140219325035712 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 4.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 4 (torch.Size([128, 256])), Block block_0 (torch.Size([128, 256])).
I0316 15:41:38.266088 140125290058944 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 5.block_0 ([torch.Size([128, 128])]) for Parameter 5 (torch.Size([128])), Block block_0 (torch.Size([128])).
I0316 15:41:38.266059 139931299886272 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 0.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 0 (torch.Size([512, 13])), Block block_0 (torch.Size([512, 13])).
I0316 15:41:38.266114 140181208745152 shampoo_preconditioner_list.py:621] Rank 0: ShampooPreconditionerList Total Bytes: 2098504
I0316 15:41:38.266119 139897529996480 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 5.block_0 ([torch.Size([0])]) for Parameter 5 (torch.Size([128])), Block block_0 (torch.Size([128])).
I0316 15:41:38.266170 140078117823680 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 7.block_0 ([torch.Size([0])]) for Parameter 7 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 15:41:38.266188 140219325035712 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 5.block_0 ([torch.Size([0])]) for Parameter 5 (torch.Size([128])), Block block_0 (torch.Size([128])).
I0316 15:41:38.266244 140125290058944 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 6.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 6 (torch.Size([1024, 506])), Block block_0 (torch.Size([1024, 506])).
I0316 15:41:38.266247 139897529996480 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 6.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 6 (torch.Size([1024, 506])), Block block_0 (torch.Size([1024, 506])).
I0316 15:41:38.266235 140175372387520 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 6.block_0 ([torch.Size([1024, 1024]), torch.Size([506, 506])]) for Parameter 6 (torch.Size([1024, 506])), Block block_0 (torch.Size([1024, 506])).
I0316 15:41:38.266241 139652994569408 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 4.block_0 ([torch.Size([128, 128]), torch.Size([256, 256])]) for Parameter 4 (torch.Size([128, 256])), Block block_0 (torch.Size([128, 256])).
I0316 15:41:38.266242 139931299886272 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 1.block_0 ([torch.Size([0])]) for Parameter 1 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 15:41:38.266255 140181208745152 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([512, 13])), Block block_0 (torch.Size([512, 13])).
I0316 15:41:38.266307 140078117823680 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 8.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 8 (torch.Size([1024, 1024])), Block block_0 (torch.Size([1024, 1024])).
I0316 15:41:38.266315 140181208745152 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 15:41:38.266315 140219325035712 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 6.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 6 (torch.Size([1024, 506])), Block block_0 (torch.Size([1024, 506])).
I0316 15:41:38.266353 139897529996480 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 7.block_0 ([torch.Size([0])]) for Parameter 7 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 15:41:38.266354 139652994569408 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 5.block_0 ([torch.Size([0])]) for Parameter 5 (torch.Size([128])), Block block_0 (torch.Size([128])).
I0316 15:41:38.266366 140181208745152 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 15:41:38.266373 140175372387520 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 7.block_0 ([torch.Size([0])]) for Parameter 7 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 15:41:38.266402 140078117823680 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 9.block_0 ([torch.Size([0])]) for Parameter 9 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 15:41:38.266408 140219325035712 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 7.block_0 ([torch.Size([0])]) for Parameter 7 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 15:41:38.266406 140125290058944 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 7.block_0 ([torch.Size([1024, 1024])]) for Parameter 7 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 15:41:38.266421 140181208745152 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 15:41:38.266469 140181208745152 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 4.block_0 (torch.Size([0])) for Parameter 4 (torch.Size([128, 256])), Block block_0 (torch.Size([128, 256])).
I0316 15:41:38.266474 139897529996480 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 8.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 8 (torch.Size([1024, 1024])), Block block_0 (torch.Size([1024, 1024])).
I0316 15:41:38.266477 139652994569408 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 6.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 6 (torch.Size([1024, 506])), Block block_0 (torch.Size([1024, 506])).
I0316 15:41:38.266489 140175372387520 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 8.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 8 (torch.Size([1024, 1024])), Block block_0 (torch.Size([1024, 1024])).
I0316 15:41:38.266516 140219325035712 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 8.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 8 (torch.Size([1024, 1024])), Block block_0 (torch.Size([1024, 1024])).
I0316 15:41:38.266525 140181208745152 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 5.block_0 (torch.Size([0])) for Parameter 5 (torch.Size([128])), Block block_0 (torch.Size([128])).
I0316 15:41:38.266525 140078117823680 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 10.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 10 (torch.Size([512, 1024])), Block block_0 (torch.Size([512, 1024])).
I0316 15:41:38.266532 140125290058944 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 8.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 8 (torch.Size([1024, 1024])), Block block_0 (torch.Size([1024, 1024])).
I0316 15:41:38.266560 139897529996480 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 9.block_0 ([torch.Size([0])]) for Parameter 9 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 15:41:38.266573 140181208745152 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 6.block_0 (torch.Size([0])) for Parameter 6 (torch.Size([1024, 506])), Block block_0 (torch.Size([1024, 506])).
I0316 15:41:38.266577 140175372387520 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 9.block_0 ([torch.Size([0])]) for Parameter 9 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 15:41:38.266577 139652994569408 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 7.block_0 ([torch.Size([0])]) for Parameter 7 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 15:41:38.266610 140078117823680 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 11.block_0 ([torch.Size([0])]) for Parameter 11 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 15:41:38.266610 140219325035712 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 9.block_0 ([torch.Size([0])]) for Parameter 9 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 15:41:38.266625 140181208745152 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 7.block_0 (torch.Size([0])) for Parameter 7 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 15:41:38.266685 140175372387520 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 10.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 10 (torch.Size([512, 1024])), Block block_0 (torch.Size([512, 1024])).
I0316 15:41:38.266691 139652994569408 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 8.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 8 (torch.Size([1024, 1024])), Block block_0 (torch.Size([1024, 1024])).
I0316 15:41:38.266717 140181208745152 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 8.block_0 (torch.Size([1024, 1024])) for Parameter 8 (torch.Size([1024, 1024])), Block block_0 (torch.Size([1024, 1024])).
I0316 15:41:38.266724 140219325035712 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 10.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 10 (torch.Size([512, 1024])), Block block_0 (torch.Size([512, 1024])).
I0316 15:41:38.266775 139652994569408 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 9.block_0 ([torch.Size([0])]) for Parameter 9 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 15:41:38.266779 140175372387520 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 11.block_0 ([torch.Size([0])]) for Parameter 11 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 15:41:38.266789 140181208745152 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 9.block_0 (torch.Size([0])) for Parameter 9 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 15:41:38.266808 140219325035712 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 11.block_0 ([torch.Size([0])]) for Parameter 11 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 15:41:38.266838 140181208745152 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 10.block_0 (torch.Size([0])) for Parameter 10 (torch.Size([512, 1024])), Block block_0 (torch.Size([512, 1024])).
I0316 15:41:38.266891 140181208745152 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 11.block_0 (torch.Size([0])) for Parameter 11 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 15:41:38.266901 139652994569408 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 10.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 10 (torch.Size([512, 1024])), Block block_0 (torch.Size([512, 1024])).
I0316 15:41:38.266902 140175372387520 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 12.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 12 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 15:41:38.266928 140219325035712 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 12.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 12 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 15:41:38.266945 140181208745152 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 12.block_0 (torch.Size([0])) for Parameter 12 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 15:41:38.266908 139931299886272 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 2.block_0 ([torch.Size([256, 256]), torch.Size([512, 512])]) for Parameter 2 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 15:41:38.266981 139652994569408 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 11.block_0 ([torch.Size([0])]) for Parameter 11 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 15:41:38.266981 140175372387520 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 13.block_0 ([torch.Size([0])]) for Parameter 13 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 15:41:38.266991 140181208745152 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 13.block_0 (torch.Size([0])) for Parameter 13 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 15:41:38.267012 140219325035712 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 13.block_0 ([torch.Size([0])]) for Parameter 13 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 15:41:38.267036 140181208745152 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 14.block_0 (torch.Size([0])) for Parameter 14 (torch.Size([1, 256])), Block block_0 (torch.Size([256])).
I0316 15:41:38.267062 140175372387520 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 14.block_0 ([torch.Size([0])]) for Parameter 14 (torch.Size([1, 256])), Block block_0 (torch.Size([256])).
I0316 15:41:38.267071 139931299886272 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 3.block_0 ([torch.Size([0])]) for Parameter 3 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 15:41:38.267087 140181208745152 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 15.block_0 (torch.Size([0])) for Parameter 15 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0316 15:41:38.267104 140219325035712 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 14.block_0 ([torch.Size([0])]) for Parameter 14 (torch.Size([1, 256])), Block block_0 (torch.Size([256])).
I0316 15:41:38.267105 139652994569408 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 12.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 12 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 15:41:38.267146 140181208745152 shampoo_preconditioner_list.py:375] Rank 0: AdaGradPreconditionerList Numel Breakdown: (0, 0, 0, 0, 0, 0, 0, 0, 1048576, 0, 0, 0, 0, 0, 0, 0)
I0316 15:41:38.267149 140175372387520 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 15.block_0 ([torch.Size([0])]) for Parameter 15 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0316 15:41:38.267184 140181208745152 shampoo_preconditioner_list.py:378] Rank 0: AdaGradPreconditionerList Bytes Breakdown: (0, 0, 0, 0, 0, 0, 0, 0, 4194304, 0, 0, 0, 0, 0, 0, 0)
I0316 15:41:38.267192 139652994569408 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 13.block_0 ([torch.Size([0])]) for Parameter 13 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 15:41:38.267197 140219325035712 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 15.block_0 ([torch.Size([0])]) for Parameter 15 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0316 15:41:38.267227 140181208745152 shampoo_preconditioner_list.py:381] Rank 0: AdaGradPreconditionerList Total Elements: 1048576
I0316 15:41:38.267245 140175372387520 shampoo_preconditioner_list.py:612] Rank 2: ShampooPreconditionerList Numel Breakdown: (524626, 524288, 655360, 131072, 163840, 32768, 2609224, 2097152, 4194304, 2097152, 2621440, 524288, 655360, 131072, 131072, 2)
I0316 15:41:38.267243 139931299886272 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 4.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 4 (torch.Size([128, 256])), Block block_0 (torch.Size([128, 256])).
I0316 15:41:38.267260 140181208745152 shampoo_preconditioner_list.py:384] Rank 0: AdaGradPreconditionerList Total Bytes: 4194304
I0316 15:41:38.267276 139652994569408 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 14.block_0 ([torch.Size([0])]) for Parameter 14 (torch.Size([1, 256])), Block block_0 (torch.Size([256])).
I0316 15:41:38.267289 140175372387520 shampoo_preconditioner_list.py:615] Rank 2: ShampooPreconditionerList Bytes Breakdown: (2098504,)
I0316 15:41:38.267290 140219325035712 shampoo_preconditioner_list.py:612] Rank 6: ShampooPreconditionerList Numel Breakdown: (524626, 524288, 655360, 131072, 163840, 32768, 2609224, 2097152, 4194304, 2097152, 2621440, 524288, 655360, 131072, 131072, 2)
I0316 15:41:38.267288 140125290058944 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 9.block_0 ([torch.Size([1024, 1024])]) for Parameter 9 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 15:41:38.267320 140175372387520 shampoo_preconditioner_list.py:618] Rank 2: ShampooPreconditionerList Total Elements: 17093020
I0316 15:41:38.267327 140219325035712 shampoo_preconditioner_list.py:615] Rank 6: ShampooPreconditionerList Bytes Breakdown: (2098504,)
I0316 15:41:38.267348 140175372387520 shampoo_preconditioner_list.py:621] Rank 2: ShampooPreconditionerList Total Bytes: 2098504
I0316 15:41:38.267358 140219325035712 shampoo_preconditioner_list.py:618] Rank 6: ShampooPreconditionerList Total Elements: 17093020
I0316 15:41:38.267358 139652994569408 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 15.block_0 ([torch.Size([0])]) for Parameter 15 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0316 15:41:38.267355 139931299886272 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 5.block_0 ([torch.Size([0])]) for Parameter 5 (torch.Size([128])), Block block_0 (torch.Size([128])).
I0316 15:41:38.267386 140219325035712 shampoo_preconditioner_list.py:621] Rank 6: ShampooPreconditionerList Total Bytes: 2098504
I0316 15:41:38.267443 139652994569408 shampoo_preconditioner_list.py:612] Rank 5: ShampooPreconditionerList Numel Breakdown: (524626, 524288, 655360, 131072, 163840, 32768, 2609224, 2097152, 4194304, 2097152, 2621440, 524288, 655360, 131072, 131072, 2)
I0316 15:41:38.267459 140125290058944 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 10.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 10 (torch.Size([512, 1024])), Block block_0 (torch.Size([512, 1024])).
I0316 15:41:38.267477 140175372387520 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([512, 13])), Block block_0 (torch.Size([512, 13])).
I0316 15:41:38.267484 139652994569408 shampoo_preconditioner_list.py:615] Rank 5: ShampooPreconditionerList Bytes Breakdown: (2098504,)
I0316 15:41:38.267515 139652994569408 shampoo_preconditioner_list.py:618] Rank 5: ShampooPreconditionerList Total Elements: 17093020
I0316 15:41:38.267510 139931299886272 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 6.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 6 (torch.Size([1024, 506])), Block block_0 (torch.Size([1024, 506])).
I0316 15:41:38.267534 140175372387520 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 15:41:38.267547 139652994569408 shampoo_preconditioner_list.py:621] Rank 5: ShampooPreconditionerList Total Bytes: 2098504
I0316 15:41:38.267517 139897529996480 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 10.block_0 ([torch.Size([512, 512]), torch.Size([1024, 1024])]) for Parameter 10 (torch.Size([512, 1024])), Block block_0 (torch.Size([512, 1024])).
I0316 15:41:38.267570 140219325035712 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([512, 13])) for Parameter 0 (torch.Size([512, 13])), Block block_0 (torch.Size([512, 13])).
I0316 15:41:38.267596 140175372387520 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 15:41:38.267588 140181208745152 submission.py:105] Large parameters (embedding tables) detected (dim >= 1_000_000). Instantiating Row-Wise AdaGrad...
I0316 15:41:38.267617 139931299886272 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 7.block_0 ([torch.Size([0])]) for Parameter 7 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 15:41:38.267650 139897529996480 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 11.block_0 ([torch.Size([0])]) for Parameter 11 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 15:41:38.267657 140175372387520 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 15:41:38.267660 140219325035712 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 15:41:38.267648 140078117823680 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 12.block_0 ([torch.Size([256, 256]), torch.Size([512, 512])]) for Parameter 12 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
W0316 15:41:38.267696 140181208745152 distributed_shampoo.py:359] Nesterov flag is enabled but momentum parameter is zero! Continuing without using momentum or Nesterov acceleration...
I0316 15:41:38.267697 139652994569408 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([512, 13])), Block block_0 (torch.Size([512, 13])).
I0316 15:41:38.267743 140175372387520 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 4.block_0 (torch.Size([0])) for Parameter 4 (torch.Size([128, 256])), Block block_0 (torch.Size([128, 256])).
I0316 15:41:38.267751 140219325035712 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 15:41:38.267777 139652994569408 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 15:41:38.267804 140175372387520 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 5.block_0 (torch.Size([0])) for Parameter 5 (torch.Size([128])), Block block_0 (torch.Size([128])).
I0316 15:41:38.267805 140219325035712 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 15:41:38.267823 139897529996480 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 12.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 12 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 15:41:38.267830 139931299886272 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 8.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 8 (torch.Size([1024, 1024])), Block block_0 (torch.Size([1024, 1024])).
I0316 15:41:38.267841 139652994569408 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 15:41:38.267857 140219325035712 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 4.block_0 (torch.Size([0])) for Parameter 4 (torch.Size([128, 256])), Block block_0 (torch.Size([128, 256])).
I0316 15:41:38.267847 140078117823680 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 13.block_0 ([torch.Size([0])]) for Parameter 13 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 15:41:38.267894 139652994569408 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 15:41:38.267917 140219325035712 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 5.block_0 (torch.Size([0])) for Parameter 5 (torch.Size([128])), Block block_0 (torch.Size([128])).
I0316 15:41:38.267914 140175372387520 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 6.block_0 (torch.Size([1024, 506])) for Parameter 6 (torch.Size([1024, 506])), Block block_0 (torch.Size([1024, 506])).
I0316 15:41:38.267923 139897529996480 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 13.block_0 ([torch.Size([0])]) for Parameter 13 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 15:41:38.267946 139931299886272 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 9.block_0 ([torch.Size([0])]) for Parameter 9 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 15:41:38.267953 140078117823680 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 14.block_0 ([torch.Size([0])]) for Parameter 14 (torch.Size([1, 256])), Block block_0 (torch.Size([256])).
I0316 15:41:38.267971 140219325035712 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 6.block_0 (torch.Size([0])) for Parameter 6 (torch.Size([1024, 506])), Block block_0 (torch.Size([1024, 506])).
I0316 15:41:38.267991 140175372387520 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 7.block_0 (torch.Size([0])) for Parameter 7 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 15:41:38.268016 139897529996480 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 14.block_0 ([torch.Size([0])]) for Parameter 14 (torch.Size([1, 256])), Block block_0 (torch.Size([256])).
I0316 15:41:38.268012 139652994569408 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 4.block_0 (torch.Size([128, 256])) for Parameter 4 (torch.Size([128, 256])), Block block_0 (torch.Size([128, 256])).
I0316 15:41:38.268009 140125290058944 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 11.block_0 ([torch.Size([512, 512])]) for Parameter 11 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 15:41:38.268038 140219325035712 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 7.block_0 (torch.Size([0])) for Parameter 7 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 15:41:38.268044 140078117823680 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 15.block_0 ([torch.Size([0])]) for Parameter 15 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0316 15:41:38.268058 140175372387520 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 8.block_0 (torch.Size([0])) for Parameter 8 (torch.Size([1024, 1024])), Block block_0 (torch.Size([1024, 1024])).
I0316 15:41:38.268083 139652994569408 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 5.block_0 (torch.Size([0])) for Parameter 5 (torch.Size([128])), Block block_0 (torch.Size([128])).
I0316 15:41:38.268091 140219325035712 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 8.block_0 (torch.Size([0])) for Parameter 8 (torch.Size([1024, 1024])), Block block_0 (torch.Size([1024, 1024])).
I0316 15:41:38.268112 140175372387520 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 9.block_0 (torch.Size([0])) for Parameter 9 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 15:41:38.268108 139931299886272 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 10.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 10 (torch.Size([512, 1024])), Block block_0 (torch.Size([512, 1024])).
I0316 15:41:38.268123 139897529996480 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 15.block_0 ([torch.Size([0])]) for Parameter 15 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0316 15:41:38.268143 140219325035712 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 9.block_0 (torch.Size([0])) for Parameter 9 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 15:41:38.268141 140078117823680 shampoo_preconditioner_list.py:612] Rank 4: ShampooPreconditionerList Numel Breakdown: (524626, 524288, 655360, 131072, 163840, 32768, 2609224, 2097152, 4194304, 2097152, 2621440, 524288, 655360, 131072, 131072, 2)
I0316 15:41:38.268148 139652994569408 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 6.block_0 (torch.Size([0])) for Parameter 6 (torch.Size([1024, 506])), Block block_0 (torch.Size([1024, 506])).
I0316 15:41:38.268178 140175372387520 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 10.block_0 (torch.Size([0])) for Parameter 10 (torch.Size([512, 1024])), Block block_0 (torch.Size([512, 1024])).
I0316 15:41:38.268181 140078117823680 shampoo_preconditioner_list.py:615] Rank 4: ShampooPreconditionerList Bytes Breakdown: (2098504,)
I0316 15:41:38.268180 140125290058944 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 12.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 12 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 15:41:38.268211 140219325035712 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 10.block_0 (torch.Size([0])) for Parameter 10 (torch.Size([512, 1024])), Block block_0 (torch.Size([512, 1024])).
I0316 15:41:38.268221 139652994569408 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 7.block_0 (torch.Size([0])) for Parameter 7 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 15:41:38.268225 140078117823680 shampoo_preconditioner_list.py:618] Rank 4: ShampooPreconditionerList Total Elements: 17093020
I0316 15:41:38.268230 139931299886272 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 11.block_0 ([torch.Size([0])]) for Parameter 11 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 15:41:38.268235 139897529996480 shampoo_preconditioner_list.py:612] Rank 1: ShampooPreconditionerList Numel Breakdown: (524626, 524288, 655360, 131072, 163840, 32768, 2609224, 2097152, 4194304, 2097152, 2621440, 524288, 655360, 131072, 131072, 2)
I0316 15:41:38.268241 140175372387520 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 11.block_0 (torch.Size([0])) for Parameter 11 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 15:41:38.268258 140078117823680 shampoo_preconditioner_list.py:621] Rank 4: ShampooPreconditionerList Total Bytes: 2098504
I0316 15:41:38.268270 140219325035712 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 11.block_0 (torch.Size([0])) for Parameter 11 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 15:41:38.268277 139897529996480 shampoo_preconditioner_list.py:615] Rank 1: ShampooPreconditionerList Bytes Breakdown: (2098504,)
I0316 15:41:38.268276 139652994569408 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 8.block_0 (torch.Size([0])) for Parameter 8 (torch.Size([1024, 1024])), Block block_0 (torch.Size([1024, 1024])).
I0316 15:41:38.268293 140175372387520 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 12.block_0 (torch.Size([0])) for Parameter 12 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 15:41:38.268310 139897529996480 shampoo_preconditioner_list.py:618] Rank 1: ShampooPreconditionerList Total Elements: 17093020
I0316 15:41:38.268327 139652994569408 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 9.block_0 (torch.Size([0])) for Parameter 9 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 15:41:38.268327 140219325035712 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 12.block_0 (torch.Size([0])) for Parameter 12 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 15:41:38.268341 139897529996480 shampoo_preconditioner_list.py:621] Rank 1: ShampooPreconditionerList Total Bytes: 2098504
I0316 15:41:38.268350 140175372387520 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 13.block_0 (torch.Size([0])) for Parameter 13 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 15:41:38.268377 140219325035712 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 13.block_0 (torch.Size([0])) for Parameter 13 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 15:41:38.268385 139652994569408 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 10.block_0 (torch.Size([0])) for Parameter 10 (torch.Size([512, 1024])), Block block_0 (torch.Size([512, 1024])).
I0316 15:41:38.268395 139931299886272 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 12.block_0 ([torch.Size([0]), torch.Size([0])]) for Parameter 12 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 15:41:38.268401 140175372387520 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 14.block_0 (torch.Size([0])) for Parameter 14 (torch.Size([1, 256])), Block block_0 (torch.Size([256])).
I0316 15:41:38.268396 140078117823680 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([512, 13])), Block block_0 (torch.Size([512, 13])).
I0316 15:41:38.268433 140219325035712 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 14.block_0 (torch.Size([0])) for Parameter 14 (torch.Size([1, 256])), Block block_0 (torch.Size([256])).
I0316 15:41:38.268436 139652994569408 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 11.block_0 (torch.Size([0])) for Parameter 11 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 15:41:38.268451 140175372387520 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 15.block_0 (torch.Size([0])) for Parameter 15 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0316 15:41:38.268457 140078117823680 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 15:41:38.268485 139652994569408 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 12.block_0 (torch.Size([0])) for Parameter 12 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 15:41:38.268490 140219325035712 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 15.block_0 (torch.Size([0])) for Parameter 15 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0316 15:41:38.268490 139897529996480 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([512, 13])), Block block_0 (torch.Size([512, 13])).
I0316 15:41:38.268514 140175372387520 shampoo_preconditioner_list.py:375] Rank 2: AdaGradPreconditionerList Numel Breakdown: (0, 0, 0, 0, 0, 0, 518144, 0, 0, 0, 0, 0, 0, 0, 0, 0)
I0316 15:41:38.268512 139931299886272 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 13.block_0 ([torch.Size([0])]) for Parameter 13 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 15:41:38.268522 140078117823680 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 15:41:38.268540 139652994569408 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 13.block_0 (torch.Size([0])) for Parameter 13 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 15:41:38.268549 140175372387520 shampoo_preconditioner_list.py:378] Rank 2: AdaGradPreconditionerList Bytes Breakdown: (0, 0, 0, 0, 0, 0, 2072576, 0, 0, 0, 0, 0, 0, 0, 0, 0)
I0316 15:41:38.268553 140219325035712 shampoo_preconditioner_list.py:375] Rank 6: AdaGradPreconditionerList Numel Breakdown: (6656, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)
I0316 15:41:38.268556 139897529996480 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 15:41:38.268580 140175372387520 shampoo_preconditioner_list.py:381] Rank 2: AdaGradPreconditionerList Total Elements: 518144
I0316 15:41:38.268573 140078117823680 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 15:41:38.268589 140219325035712 shampoo_preconditioner_list.py:378] Rank 6: AdaGradPreconditionerList Bytes Breakdown: (26624, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)
I0316 15:41:38.268596 139652994569408 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 14.block_0 (torch.Size([0])) for Parameter 14 (torch.Size([1, 256])), Block block_0 (torch.Size([256])).
I0316 15:41:38.268610 139897529996480 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 15:41:38.268617 140175372387520 shampoo_preconditioner_list.py:384] Rank 2: AdaGradPreconditionerList Total Bytes: 2072576
I0316 15:41:38.268620 140219325035712 shampoo_preconditioner_list.py:381] Rank 6: AdaGradPreconditionerList Total Elements: 6656
I0316 15:41:38.268622 139931299886272 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 14.block_0 ([torch.Size([0])]) for Parameter 14 (torch.Size([1, 256])), Block block_0 (torch.Size([256])).
I0316 15:41:38.268633 140078117823680 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 4.block_0 (torch.Size([0])) for Parameter 4 (torch.Size([128, 256])), Block block_0 (torch.Size([128, 256])).
I0316 15:41:38.268645 139652994569408 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 15.block_0 (torch.Size([0])) for Parameter 15 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0316 15:41:38.268650 140219325035712 shampoo_preconditioner_list.py:384] Rank 6: AdaGradPreconditionerList Total Bytes: 26624
I0316 15:41:38.268676 139897529996480 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 15:41:38.268685 140078117823680 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 5.block_0 (torch.Size([0])) for Parameter 5 (torch.Size([128])), Block block_0 (torch.Size([128])).
I0316 15:41:38.268707 139652994569408 shampoo_preconditioner_list.py:375] Rank 5: AdaGradPreconditionerList Numel Breakdown: (0, 0, 0, 0, 32768, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)
I0316 15:41:38.268722 139931299886272 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 15.block_0 ([torch.Size([0])]) for Parameter 15 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0316 15:41:38.268737 140078117823680 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 6.block_0 (torch.Size([0])) for Parameter 6 (torch.Size([1024, 506])), Block block_0 (torch.Size([1024, 506])).
I0316 15:41:38.268739 139897529996480 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 4.block_0 (torch.Size([0])) for Parameter 4 (torch.Size([128, 256])), Block block_0 (torch.Size([128, 256])).
I0316 15:41:38.268748 139652994569408 shampoo_preconditioner_list.py:378] Rank 5: AdaGradPreconditionerList Bytes Breakdown: (0, 0, 0, 0, 131072, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)
I0316 15:41:38.268780 139652994569408 shampoo_preconditioner_list.py:381] Rank 5: AdaGradPreconditionerList Total Elements: 32768
I0316 15:41:38.268792 139897529996480 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 5.block_0 (torch.Size([0])) for Parameter 5 (torch.Size([128])), Block block_0 (torch.Size([128])).
I0316 15:41:38.268795 140078117823680 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 7.block_0 (torch.Size([0])) for Parameter 7 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 15:41:38.268815 139652994569408 shampoo_preconditioner_list.py:384] Rank 5: AdaGradPreconditionerList Total Bytes: 131072
I0316 15:41:38.268805 140125290058944 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 13.block_0 ([torch.Size([256, 256])]) for Parameter 13 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 15:41:38.268826 139931299886272 shampoo_preconditioner_list.py:612] Rank 3: ShampooPreconditionerList Numel Breakdown: (524626, 524288, 655360, 131072, 163840, 32768, 2609224, 2097152, 4194304, 2097152, 2621440, 524288, 655360, 131072, 131072, 2)
I0316 15:41:38.268845 139897529996480 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 6.block_0 (torch.Size([0])) for Parameter 6 (torch.Size([1024, 506])), Block block_0 (torch.Size([1024, 506])).
I0316 15:41:38.268847 140078117823680 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 8.block_0 (torch.Size([0])) for Parameter 8 (torch.Size([1024, 1024])), Block block_0 (torch.Size([1024, 1024])).
I0316 15:41:38.268870 139931299886272 shampoo_preconditioner_list.py:615] Rank 3: ShampooPreconditionerList Bytes Breakdown: (2098504,)
I0316 15:41:38.268898 140078117823680 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 9.block_0 (torch.Size([0])) for Parameter 9 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 15:41:38.268923 139897529996480 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 7.block_0 (torch.Size([0])) for Parameter 7 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 15:41:38.268926 139931299886272 shampoo_preconditioner_list.py:618] Rank 3: ShampooPreconditionerList Total Elements: 17093020
I0316 15:41:38.268958 139931299886272 shampoo_preconditioner_list.py:621] Rank 3: ShampooPreconditionerList Total Bytes: 2098504
I0316 15:41:38.268966 140078117823680 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 10.block_0 (torch.Size([0])) for Parameter 10 (torch.Size([512, 1024])), Block block_0 (torch.Size([512, 1024])).
I0316 15:41:38.268973 139897529996480 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 8.block_0 (torch.Size([0])) for Parameter 8 (torch.Size([1024, 1024])), Block block_0 (torch.Size([1024, 1024])).
I0316 15:41:38.268977 140125290058944 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 14.block_0 ([torch.Size([256, 256])]) for Parameter 14 (torch.Size([1, 256])), Block block_0 (torch.Size([256])).
I0316 15:41:38.269012 140078117823680 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 11.block_0 (torch.Size([0])) for Parameter 11 (torch.Size([512])), Block block_0 (torch.Size([512])).
W0316 15:41:38.268996 140175372387520 distributed_shampoo.py:359] Nesterov flag is enabled but momentum parameter is zero! Continuing without using momentum or Nesterov acceleration...
I0316 15:41:38.269021 139897529996480 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 9.block_0 (torch.Size([0])) for Parameter 9 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
W0316 15:41:38.269026 140219325035712 distributed_shampoo.py:359] Nesterov flag is enabled but momentum parameter is zero! Continuing without using momentum or Nesterov acceleration...
I0316 15:41:38.269103 139931299886272 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([512, 13])), Block block_0 (torch.Size([512, 13])).
I0316 15:41:38.269112 139897529996480 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 10.block_0 (torch.Size([512, 1024])) for Parameter 10 (torch.Size([512, 1024])), Block block_0 (torch.Size([512, 1024])).
I0316 15:41:38.269137 140125290058944 shampoo_preconditioner_list.py:574] Instantiated Shampoo Preconditioner 15.block_0 ([torch.Size([1, 1])]) for Parameter 15 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0316 15:41:38.269175 139897529996480 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 11.block_0 (torch.Size([0])) for Parameter 11 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 15:41:38.269184 139931299886272 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 15:41:38.269243 139897529996480 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 12.block_0 (torch.Size([0])) for Parameter 12 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 15:41:38.269271 140125290058944 shampoo_preconditioner_list.py:612] Rank 7: ShampooPreconditionerList Numel Breakdown: (524626, 524288, 655360, 131072, 163840, 32768, 2609224, 2097152, 4194304, 2097152, 2621440, 524288, 655360, 131072, 131072, 2)
I0316 15:41:38.269309 139897529996480 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 13.block_0 (torch.Size([0])) for Parameter 13 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 15:41:38.269333 140125290058944 shampoo_preconditioner_list.py:615] Rank 7: ShampooPreconditionerList Bytes Breakdown: (2098504, 2097152, 2621440, 524288, 655360, 131072, 10436896, 8388608, 16777216)
I0316 15:41:38.269364 139897529996480 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 14.block_0 (torch.Size([0])) for Parameter 14 (torch.Size([1, 256])), Block block_0 (torch.Size([256])).
I0316 15:41:38.269369 140125290058944 shampoo_preconditioner_list.py:618] Rank 7: ShampooPreconditionerList Total Elements: 17093020
I0316 15:41:38.269398 140125290058944 shampoo_preconditioner_list.py:621] Rank 7: ShampooPreconditionerList Total Bytes: 43730536
I0316 15:41:38.269416 139897529996480 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 15.block_0 (torch.Size([0])) for Parameter 15 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0316 15:41:38.269480 139897529996480 shampoo_preconditioner_list.py:375] Rank 1: AdaGradPreconditionerList Numel Breakdown: (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 524288, 0, 0, 0, 0, 0)
I0316 15:41:38.269517 139897529996480 shampoo_preconditioner_list.py:378] Rank 1: AdaGradPreconditionerList Bytes Breakdown: (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2097152, 0, 0, 0, 0, 0)
I0316 15:41:38.269532 140125290058944 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([512, 13])), Block block_0 (torch.Size([512, 13])).
I0316 15:41:38.269550 139897529996480 shampoo_preconditioner_list.py:381] Rank 1: AdaGradPreconditionerList Total Elements: 524288
I0316 15:41:38.269578 139897529996480 shampoo_preconditioner_list.py:384] Rank 1: AdaGradPreconditionerList Total Bytes: 2097152
I0316 15:41:38.269637 140125290058944 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([512])) for Parameter 1 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 15:41:38.269709 140125290058944 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
W0316 15:41:38.269701 139652994569408 distributed_shampoo.py:359] Nesterov flag is enabled but momentum parameter is zero! Continuing without using momentum or Nesterov acceleration...
I0316 15:41:38.269791 140125290058944 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([256])) for Parameter 3 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 15:41:38.269856 140125290058944 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 4.block_0 (torch.Size([0])) for Parameter 4 (torch.Size([128, 256])), Block block_0 (torch.Size([128, 256])).
I0316 15:41:38.269836 140078117823680 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 12.block_0 (torch.Size([256, 512])) for Parameter 12 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 15:41:38.269929 140078117823680 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 13.block_0 (torch.Size([0])) for Parameter 13 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 15:41:38.269934 140125290058944 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 5.block_0 (torch.Size([128])) for Parameter 5 (torch.Size([128])), Block block_0 (torch.Size([128])).
W0316 15:41:38.269927 139897529996480 distributed_shampoo.py:359] Nesterov flag is enabled but momentum parameter is zero! Continuing without using momentum or Nesterov acceleration...
I0316 15:41:38.269993 140078117823680 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 14.block_0 (torch.Size([0])) for Parameter 14 (torch.Size([1, 256])), Block block_0 (torch.Size([256])).
I0316 15:41:38.269994 140125290058944 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 6.block_0 (torch.Size([0])) for Parameter 6 (torch.Size([1024, 506])), Block block_0 (torch.Size([1024, 506])).
I0316 15:41:38.270047 140078117823680 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 15.block_0 (torch.Size([0])) for Parameter 15 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0316 15:41:38.270070 140125290058944 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 7.block_0 (torch.Size([1024])) for Parameter 7 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 15:41:38.270111 140078117823680 shampoo_preconditioner_list.py:375] Rank 4: AdaGradPreconditionerList Numel Breakdown: (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 131072, 0, 0, 0)
I0316 15:41:38.270136 140125290058944 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 8.block_0 (torch.Size([0])) for Parameter 8 (torch.Size([1024, 1024])), Block block_0 (torch.Size([1024, 1024])).
I0316 15:41:38.270148 140078117823680 shampoo_preconditioner_list.py:378] Rank 4: AdaGradPreconditionerList Bytes Breakdown: (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 524288, 0, 0, 0)
I0316 15:41:38.270177 140078117823680 shampoo_preconditioner_list.py:381] Rank 4: AdaGradPreconditionerList Total Elements: 131072
I0316 15:41:38.270215 140078117823680 shampoo_preconditioner_list.py:384] Rank 4: AdaGradPreconditionerList Total Bytes: 524288
I0316 15:41:38.270224 140125290058944 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 9.block_0 (torch.Size([1024])) for Parameter 9 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 15:41:38.270205 140181208745152 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([524288, 128])) for Parameter 0 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 15:41:38.270290 140125290058944 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 10.block_0 (torch.Size([0])) for Parameter 10 (torch.Size([512, 1024])), Block block_0 (torch.Size([512, 1024])).
I0316 15:41:38.270269 139931299886272 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([256, 512])) for Parameter 2 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 15:41:38.270312 140181208745152 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_1 (torch.Size([0])) for Parameter 0 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 15:41:38.270366 140125290058944 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 11.block_0 (torch.Size([512])) for Parameter 11 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 15:41:38.270386 140181208745152 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 15:41:38.270386 139931299886272 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 15:41:38.270431 140125290058944 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 12.block_0 (torch.Size([0])) for Parameter 12 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 15:41:38.270445 140181208745152 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_1 (torch.Size([0])) for Parameter 1 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 15:41:38.270446 139931299886272 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 4.block_0 (torch.Size([0])) for Parameter 4 (torch.Size([128, 256])), Block block_0 (torch.Size([128, 256])).
I0316 15:41:38.270495 140181208745152 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 15:41:38.270501 139931299886272 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 5.block_0 (torch.Size([0])) for Parameter 5 (torch.Size([128])), Block block_0 (torch.Size([128])).
I0316 15:41:38.270505 140125290058944 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 13.block_0 (torch.Size([256])) for Parameter 13 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 15:41:38.270550 140181208745152 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_1 (torch.Size([0])) for Parameter 2 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 15:41:38.270555 139931299886272 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 6.block_0 (torch.Size([0])) for Parameter 6 (torch.Size([1024, 506])), Block block_0 (torch.Size([1024, 506])).
I0316 15:41:38.270573 140125290058944 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 14.block_0 (torch.Size([256])) for Parameter 14 (torch.Size([1, 256])), Block block_0 (torch.Size([256])).
W0316 15:41:38.270575 140078117823680 distributed_shampoo.py:359] Nesterov flag is enabled but momentum parameter is zero! Continuing without using momentum or Nesterov acceleration...
I0316 15:41:38.270605 140181208745152 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 15:41:38.270616 139931299886272 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 7.block_0 (torch.Size([0])) for Parameter 7 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 15:41:38.270648 140125290058944 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 15.block_0 (torch.Size([1])) for Parameter 15 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0316 15:41:38.270653 140181208745152 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_1 (torch.Size([0])) for Parameter 3 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 15:41:38.270677 139931299886272 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 8.block_0 (torch.Size([0])) for Parameter 8 (torch.Size([1024, 1024])), Block block_0 (torch.Size([1024, 1024])).
I0316 15:41:38.270698 140181208745152 shampoo_preconditioner_list.py:375] Rank 0: AdaGradPreconditionerList Numel Breakdown: (67108864, 0, 0, 0, 0, 0, 0, 0)
I0316 15:41:38.270729 140181208745152 shampoo_preconditioner_list.py:378] Rank 0: AdaGradPreconditionerList Bytes Breakdown: (268435456, 0, 0, 0, 0, 0, 0, 0)
I0316 15:41:38.270718 140125290058944 shampoo_preconditioner_list.py:375] Rank 7: AdaGradPreconditionerList Numel Breakdown: (0, 512, 0, 256, 0, 128, 0, 1024, 0, 1024, 0, 512, 0, 256, 256, 1)
I0316 15:41:38.270747 139931299886272 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 9.block_0 (torch.Size([0])) for Parameter 9 (torch.Size([1024])), Block block_0 (torch.Size([1024])).
I0316 15:41:38.270761 140125290058944 shampoo_preconditioner_list.py:378] Rank 7: AdaGradPreconditionerList Bytes Breakdown: (0, 2048, 0, 1024, 0, 512, 0, 4096, 0, 4096, 0, 2048, 0, 1024, 1024, 4)
I0316 15:41:38.270763 140181208745152 shampoo_preconditioner_list.py:381] Rank 0: AdaGradPreconditionerList Total Elements: 67108864
I0316 15:41:38.270790 140125290058944 shampoo_preconditioner_list.py:381] Rank 7: AdaGradPreconditionerList Total Elements: 3969
I0316 15:41:38.270792 140181208745152 shampoo_preconditioner_list.py:384] Rank 0: AdaGradPreconditionerList Total Bytes: 268435456
I0316 15:41:38.270810 139931299886272 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 10.block_0 (torch.Size([0])) for Parameter 10 (torch.Size([512, 1024])), Block block_0 (torch.Size([512, 1024])).
I0316 15:41:38.270823 140125290058944 shampoo_preconditioner_list.py:384] Rank 7: AdaGradPreconditionerList Total Bytes: 15876
I0316 15:41:38.270870 139931299886272 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 11.block_0 (torch.Size([0])) for Parameter 11 (torch.Size([512])), Block block_0 (torch.Size([512])).
I0316 15:41:38.270930 139931299886272 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 12.block_0 (torch.Size([0])) for Parameter 12 (torch.Size([256, 512])), Block block_0 (torch.Size([256, 512])).
I0316 15:41:38.270907 140175372387520 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 15:41:38.270989 139931299886272 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 13.block_0 (torch.Size([0])) for Parameter 13 (torch.Size([256])), Block block_0 (torch.Size([256])).
I0316 15:41:38.270995 140175372387520 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_1 (torch.Size([0])) for Parameter 0 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 15:41:38.271048 139931299886272 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 14.block_0 (torch.Size([0])) for Parameter 14 (torch.Size([1, 256])), Block block_0 (torch.Size([256])).
I0316 15:41:38.271025 140219325035712 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 15:41:38.271108 139931299886272 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 15.block_0 (torch.Size([0])) for Parameter 15 (torch.Size([1])), Block block_0 (torch.Size([1])).
I0316 15:41:38.271111 140219325035712 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_1 (torch.Size([0])) for Parameter 0 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 15:41:38.271178 140219325035712 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 15:41:38.271183 139931299886272 shampoo_preconditioner_list.py:375] Rank 3: AdaGradPreconditionerList Numel Breakdown: (0, 0, 131072, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)
I0316 15:41:38.271219 139931299886272 shampoo_preconditioner_list.py:378] Rank 3: AdaGradPreconditionerList Bytes Breakdown: (0, 0, 524288, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)
I0316 15:41:38.271243 140219325035712 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_1 (torch.Size([0])) for Parameter 1 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 15:41:38.271258 139931299886272 shampoo_preconditioner_list.py:381] Rank 3: AdaGradPreconditionerList Total Elements: 131072
I0316 15:41:38.271289 139931299886272 shampoo_preconditioner_list.py:384] Rank 3: AdaGradPreconditionerList Total Bytes: 524288
I0316 15:41:38.271299 140219325035712 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
W0316 15:41:38.271301 140125290058944 distributed_shampoo.py:359] Nesterov flag is enabled but momentum parameter is zero! Continuing without using momentum or Nesterov acceleration...
I0316 15:41:38.271311 139652994569408 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 15:41:38.271347 140219325035712 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_1 (torch.Size([0])) for Parameter 2 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 15:41:38.271420 139652994569408 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_1 (torch.Size([0])) for Parameter 0 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 15:41:38.271495 139652994569408 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 15:41:38.271555 139652994569408 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_1 (torch.Size([0])) for Parameter 1 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 15:41:38.271613 139652994569408 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 15:41:38.271595 139897529996480 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 15:41:38.271712 140181208745152 submission_runner.py:279] Initializing metrics bundle.
W0316 15:41:38.271774 139931299886272 distributed_shampoo.py:359] Nesterov flag is enabled but momentum parameter is zero! Continuing without using momentum or Nesterov acceleration...
I0316 15:41:38.271884 140181208745152 submission_runner.py:301] Initializing checkpoint and logger.
I0316 15:41:38.271928 140175372387520 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([524288, 128])) for Parameter 1 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 15:41:38.272046 140175372387520 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_1 (torch.Size([0])) for Parameter 1 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 15:41:38.272116 140175372387520 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 15:41:38.272184 140175372387520 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_1 (torch.Size([0])) for Parameter 2 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 15:41:38.272246 140175372387520 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 15:41:38.272258 140219325035712 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([524288, 128])) for Parameter 3 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 15:41:38.272271 140078117823680 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 15:41:38.272305 140175372387520 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_1 (torch.Size([0])) for Parameter 3 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 15:41:38.272354 140175372387520 shampoo_preconditioner_list.py:375] Rank 2: AdaGradPreconditionerList Numel Breakdown: (0, 0, 67108864, 0, 0, 0, 0, 0)
I0316 15:41:38.272338 140181208745152 submission_runner.py:321] Saving meta data to /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_2/criteo1tb_pytorch/trial_5/meta_data_0.json.
I0316 15:41:38.272362 140078117823680 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_1 (torch.Size([0])) for Parameter 0 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 15:41:38.272366 140219325035712 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_1 (torch.Size([0])) for Parameter 3 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 15:41:38.272394 140175372387520 shampoo_preconditioner_list.py:378] Rank 2: AdaGradPreconditionerList Bytes Breakdown: (0, 0, 268435456, 0, 0, 0, 0, 0)
I0316 15:41:38.272419 140219325035712 shampoo_preconditioner_list.py:375] Rank 6: AdaGradPreconditionerList Numel Breakdown: (0, 0, 0, 0, 0, 0, 67108864, 0)
I0316 15:41:38.272423 140078117823680 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 15:41:38.272430 140175372387520 shampoo_preconditioner_list.py:381] Rank 2: AdaGradPreconditionerList Total Elements: 67108864
I0316 15:41:38.272459 140175372387520 shampoo_preconditioner_list.py:384] Rank 2: AdaGradPreconditionerList Total Bytes: 268435456
I0316 15:41:38.272460 140219325035712 shampoo_preconditioner_list.py:378] Rank 6: AdaGradPreconditionerList Bytes Breakdown: (0, 0, 0, 0, 0, 0, 268435456, 0)
I0316 15:41:38.272478 140078117823680 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_1 (torch.Size([0])) for Parameter 1 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 15:41:38.272490 140219325035712 shampoo_preconditioner_list.py:381] Rank 6: AdaGradPreconditionerList Total Elements: 67108864
I0316 15:41:38.272494 140181208745152 logger_utils.py:262] Unable to record workload.train_mean information. Continuing without it.
I0316 15:41:38.272519 140219325035712 shampoo_preconditioner_list.py:384] Rank 6: AdaGradPreconditionerList Total Bytes: 268435456
I0316 15:41:38.272534 140181208745152 logger_utils.py:262] Unable to record workload.train_stddev information. Continuing without it.
I0316 15:41:38.272652 139652994569408 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_1 (torch.Size([524288, 128])) for Parameter 2 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 15:41:38.272763 139652994569408 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 15:41:38.272768 139897529996480 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_1 (torch.Size([524288, 128])) for Parameter 0 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 15:41:38.272824 139652994569408 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_1 (torch.Size([0])) for Parameter 3 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 15:41:38.272875 139652994569408 shampoo_preconditioner_list.py:375] Rank 5: AdaGradPreconditionerList Numel Breakdown: (0, 0, 0, 0, 0, 67108864, 0, 0)
I0316 15:41:38.272886 139897529996480 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 15:41:38.272912 139652994569408 shampoo_preconditioner_list.py:378] Rank 5: AdaGradPreconditionerList Bytes Breakdown: (0, 0, 0, 0, 0, 268435456, 0, 0)
I0316 15:41:38.272945 139652994569408 shampoo_preconditioner_list.py:381] Rank 5: AdaGradPreconditionerList Total Elements: 67108864
I0316 15:41:38.272952 139897529996480 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_1 (torch.Size([0])) for Parameter 1 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 15:41:38.272977 139652994569408 shampoo_preconditioner_list.py:384] Rank 5: AdaGradPreconditionerList Total Bytes: 268435456
I0316 15:41:38.273003 139897529996480 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 15:41:38.273088 139897529996480 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_1 (torch.Size([0])) for Parameter 2 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 15:41:38.273164 139897529996480 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 15:41:38.273235 139897529996480 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_1 (torch.Size([0])) for Parameter 3 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 15:41:38.273259 140125290058944 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 15:41:38.273304 139897529996480 shampoo_preconditioner_list.py:375] Rank 1: AdaGradPreconditionerList Numel Breakdown: (0, 67108864, 0, 0, 0, 0, 0, 0)
I0316 15:41:38.273340 139897529996480 shampoo_preconditioner_list.py:378] Rank 1: AdaGradPreconditionerList Bytes Breakdown: (0, 268435456, 0, 0, 0, 0, 0, 0)
I0316 15:41:38.273375 139897529996480 shampoo_preconditioner_list.py:381] Rank 1: AdaGradPreconditionerList Total Elements: 67108864
I0316 15:41:38.273379 140125290058944 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_1 (torch.Size([0])) for Parameter 0 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 15:41:38.273409 139897529996480 shampoo_preconditioner_list.py:384] Rank 1: AdaGradPreconditionerList Total Bytes: 268435456
I0316 15:41:38.273442 140125290058944 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 15:41:38.273504 140125290058944 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_1 (torch.Size([0])) for Parameter 1 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 15:41:38.273564 140125290058944 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 15:41:38.273620 140125290058944 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_1 (torch.Size([0])) for Parameter 2 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 15:41:38.273677 140125290058944 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 15:41:38.273734 140078117823680 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([524288, 128])) for Parameter 2 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 15:41:38.273844 140078117823680 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_1 (torch.Size([0])) for Parameter 2 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 15:41:38.273910 140078117823680 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 15:41:38.273969 140078117823680 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_1 (torch.Size([0])) for Parameter 3 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 15:41:38.273985 139931299886272 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_0 (torch.Size([0])) for Parameter 0 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 15:41:38.274023 140078117823680 shampoo_preconditioner_list.py:375] Rank 4: AdaGradPreconditionerList Numel Breakdown: (0, 0, 0, 0, 67108864, 0, 0, 0)
I0316 15:41:38.274056 140078117823680 shampoo_preconditioner_list.py:378] Rank 4: AdaGradPreconditionerList Bytes Breakdown: (0, 0, 0, 0, 268435456, 0, 0, 0)
I0316 15:41:38.274091 140078117823680 shampoo_preconditioner_list.py:381] Rank 4: AdaGradPreconditionerList Total Elements: 67108864
I0316 15:41:38.274103 139931299886272 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 0.block_1 (torch.Size([0])) for Parameter 0 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 15:41:38.274124 140078117823680 shampoo_preconditioner_list.py:384] Rank 4: AdaGradPreconditionerList Total Bytes: 268435456
I0316 15:41:38.274192 139931299886272 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_0 (torch.Size([0])) for Parameter 1 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 15:41:38.274453 140125290058944 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_1 (torch.Size([524288, 128])) for Parameter 3 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 15:41:38.274543 140125290058944 shampoo_preconditioner_list.py:375] Rank 7: AdaGradPreconditionerList Numel Breakdown: (0, 0, 0, 0, 0, 0, 0, 67108864)
I0316 15:41:38.274586 140125290058944 shampoo_preconditioner_list.py:378] Rank 7: AdaGradPreconditionerList Bytes Breakdown: (0, 0, 0, 0, 0, 0, 0, 268435456)
I0316 15:41:38.274586 140175372387520 logger_utils.py:262] Unable to record workload.train_mean information. Continuing without it.
I0316 15:41:38.274621 140125290058944 shampoo_preconditioner_list.py:381] Rank 7: AdaGradPreconditionerList Total Elements: 67108864
I0316 15:41:38.274651 140125290058944 shampoo_preconditioner_list.py:384] Rank 7: AdaGradPreconditionerList Total Bytes: 268435456
I0316 15:41:38.274662 140175372387520 logger_utils.py:262] Unable to record workload.train_stddev information. Continuing without it.
I0316 15:41:38.274677 140219325035712 logger_utils.py:262] Unable to record workload.train_mean information. Continuing without it.
I0316 15:41:38.274754 140219325035712 logger_utils.py:262] Unable to record workload.train_stddev information. Continuing without it.
I0316 15:41:38.274861 139931299886272 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 1.block_1 (torch.Size([524288, 128])) for Parameter 1 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 15:41:38.274938 139652994569408 logger_utils.py:262] Unable to record workload.train_mean information. Continuing without it.
I0316 15:41:38.274990 139931299886272 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_0 (torch.Size([0])) for Parameter 2 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 15:41:38.275015 139652994569408 logger_utils.py:262] Unable to record workload.train_stddev information. Continuing without it.
I0316 15:41:38.275064 139931299886272 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 2.block_1 (torch.Size([0])) for Parameter 2 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 15:41:38.275132 139931299886272 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_0 (torch.Size([0])) for Parameter 3 (torch.Size([1048576, 128])), Block block_0 (torch.Size([524288, 128])).
I0316 15:41:38.275221 139931299886272 shampoo_preconditioner_list.py:352] Instantiated Adagrad Preconditioner 3.block_1 (torch.Size([0])) for Parameter 3 (torch.Size([1048576, 128])), Block block_1 (torch.Size([524288, 128])).
I0316 15:41:38.275210 139897529996480 logger_utils.py:262] Unable to record workload.train_mean information. Continuing without it.
I0316 15:41:38.275274 139897529996480 logger_utils.py:262] Unable to record workload.train_stddev information. Continuing without it.
I0316 15:41:38.275281 139931299886272 shampoo_preconditioner_list.py:375] Rank 3: AdaGradPreconditionerList Numel Breakdown: (0, 0, 0, 67108864, 0, 0, 0, 0)
I0316 15:41:38.275320 139931299886272 shampoo_preconditioner_list.py:378] Rank 3: AdaGradPreconditionerList Bytes Breakdown: (0, 0, 0, 268435456, 0, 0, 0, 0)
I0316 15:41:38.275359 139931299886272 shampoo_preconditioner_list.py:381] Rank 3: AdaGradPreconditionerList Total Elements: 67108864
I0316 15:41:38.275397 139931299886272 shampoo_preconditioner_list.py:384] Rank 3: AdaGradPreconditionerList Total Bytes: 268435456
I0316 15:41:38.275688 140078117823680 logger_utils.py:262] Unable to record workload.train_mean information. Continuing without it.
I0316 15:41:38.275763 140078117823680 logger_utils.py:262] Unable to record workload.train_stddev information. Continuing without it.
I0316 15:41:38.276005 140125290058944 logger_utils.py:262] Unable to record workload.train_mean information. Continuing without it.
I0316 15:41:38.276082 140125290058944 logger_utils.py:262] Unable to record workload.train_stddev information. Continuing without it.
I0316 15:41:38.276812 139931299886272 logger_utils.py:262] Unable to record workload.train_mean information. Continuing without it.
I0316 15:41:38.276893 139931299886272 logger_utils.py:262] Unable to record workload.train_stddev information. Continuing without it.
I0316 15:41:38.531347 140181208745152 submission_runner.py:325] Saving flags to /experiment_runs/submissions/rolling_leaderboard/external_tuning/shampoo/study_2/criteo1tb_pytorch/trial_5/flags_0.json.
I0316 15:41:38.609022 140181208745152 submission_runner.py:337] Starting training loop.
I0316 15:41:43.184772 140153583552256 logging_writer.py:48] [0] global_step=0, grad_norm=7.65363, loss=0.704023
I0316 15:41:43.204118 140181208745152 submission.py:265] 0) loss = 0.704, grad_norm = 7.654
I0316 15:41:43.614059 140181208745152 spec.py:321] Evaluating on the training split.
I0316 15:47:01.997720 140181208745152 spec.py:333] Evaluating on the validation split.
I0316 15:52:07.954545 140181208745152 spec.py:349] Evaluating on the test split.
I0316 15:58:14.141383 140181208745152 submission_runner.py:469] Time since start: 995.53s, 	Step: 1, 	{'train/loss': 0.704038417173838, 'validation/loss': 0.7047587307571139, 'validation/num_examples': 83274637, 'test/loss': 0.7043476190105238, 'test/num_examples': 95000000, 'score': 4.59600305557251, 'total_duration': 995.5325169563293, 'accumulated_submission_time': 4.59600305557251, 'accumulated_eval_time': 990.5274004936218, 'accumulated_logging_time': 0}
I0316 15:58:14.150375 140139170866944 logging_writer.py:48] [1] accumulated_eval_time=990.527, accumulated_logging_time=0, accumulated_submission_time=4.596, global_step=1, preemption_count=0, score=4.596, test/loss=0.704348, test/num_examples=95000000, total_duration=995.533, train/loss=0.704038, validation/loss=0.704759, validation/num_examples=83274637
I0316 15:58:14.772369 140139162474240 logging_writer.py:48] [1] global_step=1, grad_norm=7.67574, loss=0.704015
I0316 15:58:14.776216 140181208745152 submission.py:265] 1) loss = 0.704, grad_norm = 7.676
I0316 15:58:14.969244 140139170866944 logging_writer.py:48] [2] global_step=2, grad_norm=7.64008, loss=0.698577
I0316 15:58:14.972328 140181208745152 submission.py:265] 2) loss = 0.699, grad_norm = 7.640
I0316 15:58:15.162810 140139162474240 logging_writer.py:48] [3] global_step=3, grad_norm=7.54664, loss=0.689136
I0316 15:58:15.165625 140181208745152 submission.py:265] 3) loss = 0.689, grad_norm = 7.547
I0316 15:58:15.355216 140139170866944 logging_writer.py:48] [4] global_step=4, grad_norm=7.43431, loss=0.676
I0316 15:58:15.358061 140181208745152 submission.py:265] 4) loss = 0.676, grad_norm = 7.434
I0316 15:58:15.569443 140139162474240 logging_writer.py:48] [5] global_step=5, grad_norm=7.26462, loss=0.659582
I0316 15:58:15.576905 140181208745152 submission.py:265] 5) loss = 0.660, grad_norm = 7.265
I0316 15:58:15.770974 140139170866944 logging_writer.py:48] [6] global_step=6, grad_norm=7.09934, loss=0.639814
I0316 15:58:15.774830 140181208745152 submission.py:265] 6) loss = 0.640, grad_norm = 7.099
I0316 15:58:15.968659 140139162474240 logging_writer.py:48] [7] global_step=7, grad_norm=6.88123, loss=0.617365
I0316 15:58:15.972059 140181208745152 submission.py:265] 7) loss = 0.617, grad_norm = 6.881
I0316 15:58:16.164470 140139170866944 logging_writer.py:48] [8] global_step=8, grad_norm=6.60434, loss=0.59267
I0316 15:58:16.167613 140181208745152 submission.py:265] 8) loss = 0.593, grad_norm = 6.604
I0316 15:58:16.358855 140139162474240 logging_writer.py:48] [9] global_step=9, grad_norm=6.27322, loss=0.566233
I0316 15:58:16.362064 140181208745152 submission.py:265] 9) loss = 0.566, grad_norm = 6.273
I0316 15:58:16.555060 140139170866944 logging_writer.py:48] [10] global_step=10, grad_norm=5.91842, loss=0.538463
I0316 15:58:16.558493 140181208745152 submission.py:265] 10) loss = 0.538, grad_norm = 5.918
I0316 15:58:16.767013 140139162474240 logging_writer.py:48] [11] global_step=11, grad_norm=5.59019, loss=0.509695
I0316 15:58:16.770941 140181208745152 submission.py:265] 11) loss = 0.510, grad_norm = 5.590
I0316 15:58:16.963997 140139170866944 logging_writer.py:48] [12] global_step=12, grad_norm=5.2592, loss=0.480948
I0316 15:58:16.967280 140181208745152 submission.py:265] 12) loss = 0.481, grad_norm = 5.259
I0316 15:58:17.157995 140139162474240 logging_writer.py:48] [13] global_step=13, grad_norm=4.92994, loss=0.45146
I0316 15:58:17.161003 140181208745152 submission.py:265] 13) loss = 0.451, grad_norm = 4.930
I0316 15:58:17.361119 140139170866944 logging_writer.py:48] [14] global_step=14, grad_norm=4.47767, loss=0.421999
I0316 15:58:17.364819 140181208745152 submission.py:265] 14) loss = 0.422, grad_norm = 4.478
I0316 15:58:17.558300 140139162474240 logging_writer.py:48] [15] global_step=15, grad_norm=4.11358, loss=0.397834
I0316 15:58:17.561419 140181208745152 submission.py:265] 15) loss = 0.398, grad_norm = 4.114
I0316 15:58:17.754300 140139170866944 logging_writer.py:48] [16] global_step=16, grad_norm=3.81951, loss=0.375486
I0316 15:58:17.757247 140181208745152 submission.py:265] 16) loss = 0.375, grad_norm = 3.820
I0316 15:58:17.949449 140139162474240 logging_writer.py:48] [17] global_step=17, grad_norm=3.56371, loss=0.349579
I0316 15:58:17.953296 140181208745152 submission.py:265] 17) loss = 0.350, grad_norm = 3.564
I0316 15:58:18.149336 140139170866944 logging_writer.py:48] [18] global_step=18, grad_norm=3.27693, loss=0.325208
I0316 15:58:18.152382 140181208745152 submission.py:265] 18) loss = 0.325, grad_norm = 3.277
I0316 15:58:18.344727 140139162474240 logging_writer.py:48] [19] global_step=19, grad_norm=3.00364, loss=0.298823
I0316 15:58:18.347400 140181208745152 submission.py:265] 19) loss = 0.299, grad_norm = 3.004
I0316 15:58:18.539153 140139170866944 logging_writer.py:48] [20] global_step=20, grad_norm=2.6708, loss=0.278439
I0316 15:58:18.541984 140181208745152 submission.py:265] 20) loss = 0.278, grad_norm = 2.671
I0316 15:58:18.731607 140139162474240 logging_writer.py:48] [21] global_step=21, grad_norm=2.36249, loss=0.258238
I0316 15:58:18.734730 140181208745152 submission.py:265] 21) loss = 0.258, grad_norm = 2.362
I0316 15:58:18.933919 140139170866944 logging_writer.py:48] [22] global_step=22, grad_norm=2.04165, loss=0.242323
I0316 15:58:18.937985 140181208745152 submission.py:265] 22) loss = 0.242, grad_norm = 2.042
I0316 15:58:19.130401 140139162474240 logging_writer.py:48] [23] global_step=23, grad_norm=1.77891, loss=0.225786
I0316 15:58:19.133564 140181208745152 submission.py:265] 23) loss = 0.226, grad_norm = 1.779
I0316 15:58:19.324150 140139170866944 logging_writer.py:48] [24] global_step=24, grad_norm=1.506, loss=0.210974
I0316 15:58:19.326830 140181208745152 submission.py:265] 24) loss = 0.211, grad_norm = 1.506
I0316 15:58:19.532781 140139162474240 logging_writer.py:48] [25] global_step=25, grad_norm=1.24059, loss=0.199811
I0316 15:58:19.535912 140181208745152 submission.py:265] 25) loss = 0.200, grad_norm = 1.241
I0316 15:58:19.727920 140139170866944 logging_writer.py:48] [26] global_step=26, grad_norm=1.0108, loss=0.189183
I0316 15:58:19.730903 140181208745152 submission.py:265] 26) loss = 0.189, grad_norm = 1.011
I0316 15:58:19.920692 140139162474240 logging_writer.py:48] [27] global_step=27, grad_norm=0.782045, loss=0.180991
I0316 15:58:19.923678 140181208745152 submission.py:265] 27) loss = 0.181, grad_norm = 0.782
I0316 15:58:20.114575 140139170866944 logging_writer.py:48] [28] global_step=28, grad_norm=0.564011, loss=0.175684
I0316 15:58:20.117651 140181208745152 submission.py:265] 28) loss = 0.176, grad_norm = 0.564
I0316 15:58:20.312762 140139162474240 logging_writer.py:48] [29] global_step=29, grad_norm=0.379221, loss=0.170598
I0316 15:58:20.316153 140181208745152 submission.py:265] 29) loss = 0.171, grad_norm = 0.379
I0316 15:58:20.506822 140139170866944 logging_writer.py:48] [30] global_step=30, grad_norm=0.216277, loss=0.169362
I0316 15:58:20.509711 140181208745152 submission.py:265] 30) loss = 0.169, grad_norm = 0.216
I0316 15:58:21.640340 140139162474240 logging_writer.py:48] [31] global_step=31, grad_norm=0.134934, loss=0.166965
I0316 15:58:21.643445 140181208745152 submission.py:265] 31) loss = 0.167, grad_norm = 0.135
I0316 15:58:22.952682 140139170866944 logging_writer.py:48] [32] global_step=32, grad_norm=0.216333, loss=0.170394
I0316 15:58:22.955733 140181208745152 submission.py:265] 32) loss = 0.170, grad_norm = 0.216
I0316 15:58:24.105702 140139162474240 logging_writer.py:48] [33] global_step=33, grad_norm=0.316652, loss=0.169262
I0316 15:58:24.108610 140181208745152 submission.py:265] 33) loss = 0.169, grad_norm = 0.317
I0316 15:58:25.521017 140139170866944 logging_writer.py:48] [34] global_step=34, grad_norm=0.406207, loss=0.168783
I0316 15:58:25.523940 140181208745152 submission.py:265] 34) loss = 0.169, grad_norm = 0.406
I0316 15:58:26.253055 140139162474240 logging_writer.py:48] [35] global_step=35, grad_norm=0.515386, loss=0.171398
I0316 15:58:26.256235 140181208745152 submission.py:265] 35) loss = 0.171, grad_norm = 0.515
I0316 15:58:27.558151 140139170866944 logging_writer.py:48] [36] global_step=36, grad_norm=0.622018, loss=0.175741
I0316 15:58:27.561242 140181208745152 submission.py:265] 36) loss = 0.176, grad_norm = 0.622
I0316 15:58:28.913419 140139162474240 logging_writer.py:48] [37] global_step=37, grad_norm=0.706072, loss=0.179518
I0316 15:58:28.916498 140181208745152 submission.py:265] 37) loss = 0.180, grad_norm = 0.706
I0316 15:58:29.542550 140139170866944 logging_writer.py:48] [38] global_step=38, grad_norm=0.838742, loss=0.188087
I0316 15:58:29.545945 140181208745152 submission.py:265] 38) loss = 0.188, grad_norm = 0.839
I0316 15:58:31.066793 140139162474240 logging_writer.py:48] [39] global_step=39, grad_norm=0.939305, loss=0.195958
I0316 15:58:31.069829 140181208745152 submission.py:265] 39) loss = 0.196, grad_norm = 0.939
I0316 15:58:31.941880 140139170866944 logging_writer.py:48] [40] global_step=40, grad_norm=1.01184, loss=0.201125
I0316 15:58:31.945614 140181208745152 submission.py:265] 40) loss = 0.201, grad_norm = 1.012
I0316 15:58:33.847070 140139162474240 logging_writer.py:48] [41] global_step=41, grad_norm=1.06261, loss=0.204697
I0316 15:58:33.850112 140181208745152 submission.py:265] 41) loss = 0.205, grad_norm = 1.063
I0316 15:58:34.962740 140139170866944 logging_writer.py:48] [42] global_step=42, grad_norm=1.06026, loss=0.201478
I0316 15:58:34.965825 140181208745152 submission.py:265] 42) loss = 0.201, grad_norm = 1.060
I0316 15:58:36.540020 140139162474240 logging_writer.py:48] [43] global_step=43, grad_norm=1.16273, loss=0.213163
I0316 15:58:36.543071 140181208745152 submission.py:265] 43) loss = 0.213, grad_norm = 1.163
I0316 15:58:37.541905 140139170866944 logging_writer.py:48] [44] global_step=44, grad_norm=1.18633, loss=0.215124
I0316 15:58:37.544804 140181208745152 submission.py:265] 44) loss = 0.215, grad_norm = 1.186
I0316 15:58:38.952748 140139162474240 logging_writer.py:48] [45] global_step=45, grad_norm=1.18754, loss=0.213773
I0316 15:58:38.955938 140181208745152 submission.py:265] 45) loss = 0.214, grad_norm = 1.188
I0316 15:58:40.143713 140139170866944 logging_writer.py:48] [46] global_step=46, grad_norm=1.19003, loss=0.212173
I0316 15:58:40.146852 140181208745152 submission.py:265] 46) loss = 0.212, grad_norm = 1.190
I0316 15:58:41.346189 140139162474240 logging_writer.py:48] [47] global_step=47, grad_norm=1.2685, loss=0.22271
I0316 15:58:41.349067 140181208745152 submission.py:265] 47) loss = 0.223, grad_norm = 1.268
I0316 15:58:42.153167 140139170866944 logging_writer.py:48] [48] global_step=48, grad_norm=1.26585, loss=0.220999
I0316 15:58:42.156295 140181208745152 submission.py:265] 48) loss = 0.221, grad_norm = 1.266
I0316 15:58:43.757197 140139162474240 logging_writer.py:48] [49] global_step=49, grad_norm=1.27706, loss=0.221789
I0316 15:58:43.760082 140181208745152 submission.py:265] 49) loss = 0.222, grad_norm = 1.277
I0316 15:58:44.873527 140139170866944 logging_writer.py:48] [50] global_step=50, grad_norm=1.29598, loss=0.224481
I0316 15:58:44.876950 140181208745152 submission.py:265] 50) loss = 0.224, grad_norm = 1.296
I0316 15:58:46.101386 140139162474240 logging_writer.py:48] [51] global_step=51, grad_norm=1.29263, loss=0.223555
I0316 15:58:46.104385 140181208745152 submission.py:265] 51) loss = 0.224, grad_norm = 1.293
I0316 15:58:47.013666 140139170866944 logging_writer.py:48] [52] global_step=52, grad_norm=1.25588, loss=0.218721
I0316 15:58:47.016875 140181208745152 submission.py:265] 52) loss = 0.219, grad_norm = 1.256
I0316 15:58:48.346001 140139162474240 logging_writer.py:48] [53] global_step=53, grad_norm=1.26512, loss=0.21954
I0316 15:58:48.348956 140181208745152 submission.py:265] 53) loss = 0.220, grad_norm = 1.265
I0316 15:58:49.444902 140139170866944 logging_writer.py:48] [54] global_step=54, grad_norm=1.25248, loss=0.217978
I0316 15:58:49.448030 140181208745152 submission.py:265] 54) loss = 0.218, grad_norm = 1.252
I0316 15:58:50.290214 140139162474240 logging_writer.py:48] [55] global_step=55, grad_norm=1.2003, loss=0.211338
I0316 15:58:50.295053 140181208745152 submission.py:265] 55) loss = 0.211, grad_norm = 1.200
I0316 15:58:52.101348 140139170866944 logging_writer.py:48] [56] global_step=56, grad_norm=1.16558, loss=0.206953
I0316 15:58:52.104327 140181208745152 submission.py:265] 56) loss = 0.207, grad_norm = 1.166
I0316 15:58:53.221009 140139162474240 logging_writer.py:48] [57] global_step=57, grad_norm=1.07187, loss=0.200076
I0316 15:58:53.224036 140181208745152 submission.py:265] 57) loss = 0.200, grad_norm = 1.072
I0316 15:58:54.660057 140139170866944 logging_writer.py:48] [58] global_step=58, grad_norm=1.02307, loss=0.195767
I0316 15:58:54.662999 140181208745152 submission.py:265] 58) loss = 0.196, grad_norm = 1.023
I0316 15:58:55.786849 140139162474240 logging_writer.py:48] [59] global_step=59, grad_norm=0.98092, loss=0.19264
I0316 15:58:55.789923 140181208745152 submission.py:265] 59) loss = 0.193, grad_norm = 0.981
I0316 15:58:56.769191 140139170866944 logging_writer.py:48] [60] global_step=60, grad_norm=0.943517, loss=0.190222
I0316 15:58:56.772217 140181208745152 submission.py:265] 60) loss = 0.190, grad_norm = 0.944
I0316 15:58:58.360747 140139162474240 logging_writer.py:48] [61] global_step=61, grad_norm=0.85703, loss=0.182843
I0316 15:58:58.363837 140181208745152 submission.py:265] 61) loss = 0.183, grad_norm = 0.857
I0316 15:58:59.117689 140139170866944 logging_writer.py:48] [62] global_step=62, grad_norm=0.789259, loss=0.177611
I0316 15:58:59.120724 140181208745152 submission.py:265] 62) loss = 0.178, grad_norm = 0.789
I0316 15:59:00.595915 140139162474240 logging_writer.py:48] [63] global_step=63, grad_norm=0.747398, loss=0.176455
I0316 15:59:00.598943 140181208745152 submission.py:265] 63) loss = 0.176, grad_norm = 0.747
I0316 15:59:01.483042 140139170866944 logging_writer.py:48] [64] global_step=64, grad_norm=0.676092, loss=0.173056
I0316 15:59:01.485960 140181208745152 submission.py:265] 64) loss = 0.173, grad_norm = 0.676
I0316 15:59:02.709090 140139162474240 logging_writer.py:48] [65] global_step=65, grad_norm=0.611235, loss=0.170596
I0316 15:59:02.712088 140181208745152 submission.py:265] 65) loss = 0.171, grad_norm = 0.611
I0316 15:59:03.778968 140139170866944 logging_writer.py:48] [66] global_step=66, grad_norm=0.501175, loss=0.164272
I0316 15:59:03.781925 140181208745152 submission.py:265] 66) loss = 0.164, grad_norm = 0.501
I0316 15:59:05.091999 140139162474240 logging_writer.py:48] [67] global_step=67, grad_norm=0.447616, loss=0.16538
I0316 15:59:05.095328 140181208745152 submission.py:265] 67) loss = 0.165, grad_norm = 0.448
I0316 15:59:06.052477 140139170866944 logging_writer.py:48] [68] global_step=68, grad_norm=0.32237, loss=0.159393
I0316 15:59:06.055449 140181208745152 submission.py:265] 68) loss = 0.159, grad_norm = 0.322
I0316 15:59:07.263024 140139162474240 logging_writer.py:48] [69] global_step=69, grad_norm=0.24858, loss=0.159194
I0316 15:59:07.266199 140181208745152 submission.py:265] 69) loss = 0.159, grad_norm = 0.249
I0316 15:59:08.518841 140139170866944 logging_writer.py:48] [70] global_step=70, grad_norm=0.144984, loss=0.156483
I0316 15:59:08.521912 140181208745152 submission.py:265] 70) loss = 0.156, grad_norm = 0.145
I0316 15:59:09.420840 140139162474240 logging_writer.py:48] [71] global_step=71, grad_norm=0.108257, loss=0.158748
I0316 15:59:09.424310 140181208745152 submission.py:265] 71) loss = 0.159, grad_norm = 0.108
I0316 15:59:11.078784 140139170866944 logging_writer.py:48] [72] global_step=72, grad_norm=0.125904, loss=0.158879
I0316 15:59:11.082141 140181208745152 submission.py:265] 72) loss = 0.159, grad_norm = 0.126
I0316 15:59:12.280300 140139162474240 logging_writer.py:48] [73] global_step=73, grad_norm=0.219451, loss=0.156212
I0316 15:59:12.283714 140181208745152 submission.py:265] 73) loss = 0.156, grad_norm = 0.219
I0316 15:59:13.347984 140139170866944 logging_writer.py:48] [74] global_step=74, grad_norm=0.280876, loss=0.156681
I0316 15:59:13.351289 140181208745152 submission.py:265] 74) loss = 0.157, grad_norm = 0.281
I0316 15:59:14.629855 140139162474240 logging_writer.py:48] [75] global_step=75, grad_norm=0.334311, loss=0.15713
I0316 15:59:14.633408 140181208745152 submission.py:265] 75) loss = 0.157, grad_norm = 0.334
I0316 15:59:15.845538 140139170866944 logging_writer.py:48] [76] global_step=76, grad_norm=0.323749, loss=0.156978
I0316 15:59:15.848936 140181208745152 submission.py:265] 76) loss = 0.157, grad_norm = 0.324
I0316 15:59:16.988183 140139162474240 logging_writer.py:48] [77] global_step=77, grad_norm=0.303057, loss=0.159116
I0316 15:59:16.991667 140181208745152 submission.py:265] 77) loss = 0.159, grad_norm = 0.303
I0316 15:59:18.399558 140139170866944 logging_writer.py:48] [78] global_step=78, grad_norm=0.309303, loss=0.158873
I0316 15:59:18.403154 140181208745152 submission.py:265] 78) loss = 0.159, grad_norm = 0.309
I0316 15:59:19.869533 140139162474240 logging_writer.py:48] [79] global_step=79, grad_norm=0.303785, loss=0.158076
I0316 15:59:19.873243 140181208745152 submission.py:265] 79) loss = 0.158, grad_norm = 0.304
I0316 15:59:21.495497 140139170866944 logging_writer.py:48] [80] global_step=80, grad_norm=0.27753, loss=0.157635
I0316 15:59:21.499009 140181208745152 submission.py:265] 80) loss = 0.158, grad_norm = 0.278
I0316 15:59:22.327792 140139162474240 logging_writer.py:48] [81] global_step=81, grad_norm=0.238674, loss=0.157183
I0316 15:59:22.331189 140181208745152 submission.py:265] 81) loss = 0.157, grad_norm = 0.239
I0316 15:59:23.813364 140139170866944 logging_writer.py:48] [82] global_step=82, grad_norm=0.196016, loss=0.155654
I0316 15:59:23.816902 140181208745152 submission.py:265] 82) loss = 0.156, grad_norm = 0.196
I0316 15:59:24.891956 140139162474240 logging_writer.py:48] [83] global_step=83, grad_norm=0.127881, loss=0.156126
I0316 15:59:24.895477 140181208745152 submission.py:265] 83) loss = 0.156, grad_norm = 0.128
I0316 15:59:25.952574 140139170866944 logging_writer.py:48] [84] global_step=84, grad_norm=0.0924124, loss=0.153809
I0316 15:59:25.956063 140181208745152 submission.py:265] 84) loss = 0.154, grad_norm = 0.092
I0316 15:59:27.477639 140139162474240 logging_writer.py:48] [85] global_step=85, grad_norm=0.0703866, loss=0.15425
I0316 15:59:27.481101 140181208745152 submission.py:265] 85) loss = 0.154, grad_norm = 0.070
I0316 15:59:28.729400 140139170866944 logging_writer.py:48] [86] global_step=86, grad_norm=0.116447, loss=0.155265
I0316 15:59:28.732832 140181208745152 submission.py:265] 86) loss = 0.155, grad_norm = 0.116
I0316 15:59:29.931792 140139162474240 logging_writer.py:48] [87] global_step=87, grad_norm=0.148329, loss=0.153659
I0316 15:59:29.935637 140181208745152 submission.py:265] 87) loss = 0.154, grad_norm = 0.148
I0316 15:59:31.348229 140139170866944 logging_writer.py:48] [88] global_step=88, grad_norm=0.194058, loss=0.154123
I0316 15:59:31.351852 140181208745152 submission.py:265] 88) loss = 0.154, grad_norm = 0.194
I0316 15:59:32.550245 140139162474240 logging_writer.py:48] [89] global_step=89, grad_norm=0.198706, loss=0.151296
I0316 15:59:32.553753 140181208745152 submission.py:265] 89) loss = 0.151, grad_norm = 0.199
I0316 15:59:34.055408 140139170866944 logging_writer.py:48] [90] global_step=90, grad_norm=0.229188, loss=0.153071
I0316 15:59:34.058701 140181208745152 submission.py:265] 90) loss = 0.153, grad_norm = 0.229
I0316 15:59:35.153912 140139162474240 logging_writer.py:48] [91] global_step=91, grad_norm=0.186032, loss=0.148782
I0316 15:59:35.156850 140181208745152 submission.py:265] 91) loss = 0.149, grad_norm = 0.186
I0316 15:59:36.862097 140139170866944 logging_writer.py:48] [92] global_step=92, grad_norm=0.21107, loss=0.152326
I0316 15:59:36.865043 140181208745152 submission.py:265] 92) loss = 0.152, grad_norm = 0.211
I0316 15:59:37.693037 140139162474240 logging_writer.py:48] [93] global_step=93, grad_norm=0.189389, loss=0.152935
I0316 15:59:37.696463 140181208745152 submission.py:265] 93) loss = 0.153, grad_norm = 0.189
I0316 15:59:39.480799 140139170866944 logging_writer.py:48] [94] global_step=94, grad_norm=0.163024, loss=0.153625
I0316 15:59:39.483857 140181208745152 submission.py:265] 94) loss = 0.154, grad_norm = 0.163
I0316 15:59:40.737535 140139162474240 logging_writer.py:48] [95] global_step=95, grad_norm=0.128749, loss=0.15238
I0316 15:59:40.740483 140181208745152 submission.py:265] 95) loss = 0.152, grad_norm = 0.129
I0316 15:59:42.344996 140139170866944 logging_writer.py:48] [96] global_step=96, grad_norm=0.0915764, loss=0.152866
I0316 15:59:42.348018 140181208745152 submission.py:265] 96) loss = 0.153, grad_norm = 0.092
I0316 15:59:43.098517 140139162474240 logging_writer.py:48] [97] global_step=97, grad_norm=0.0575171, loss=0.153668
I0316 15:59:43.101735 140181208745152 submission.py:265] 97) loss = 0.154, grad_norm = 0.058
I0316 15:59:44.715973 140139170866944 logging_writer.py:48] [98] global_step=98, grad_norm=0.0496151, loss=0.154638
I0316 15:59:44.719114 140181208745152 submission.py:265] 98) loss = 0.155, grad_norm = 0.050
I0316 15:59:45.917617 140139162474240 logging_writer.py:48] [99] global_step=99, grad_norm=0.0634869, loss=0.153438
I0316 15:59:45.920598 140181208745152 submission.py:265] 99) loss = 0.153, grad_norm = 0.063
I0316 15:59:47.097470 140139170866944 logging_writer.py:48] [100] global_step=100, grad_norm=0.127867, loss=0.152293
I0316 15:59:47.100595 140181208745152 submission.py:265] 100) loss = 0.152, grad_norm = 0.128
I0316 16:00:14.779700 140181208745152 spec.py:321] Evaluating on the training split.
I0316 16:05:42.390387 140181208745152 spec.py:333] Evaluating on the validation split.
I0316 16:10:09.218275 140181208745152 spec.py:349] Evaluating on the test split.
I0316 16:15:32.459672 140181208745152 submission_runner.py:469] Time since start: 2033.85s, 	Step: 120, 	{'train/loss': 0.13714099504149235, 'validation/loss': 0.13917363571742725, 'validation/num_examples': 83274637, 'test/loss': 0.14245255075703672, 'test/num_examples': 95000000, 'score': 124.41312885284424, 'total_duration': 2033.8508625030518, 'accumulated_submission_time': 124.41312885284424, 'accumulated_eval_time': 1908.2074134349823, 'accumulated_logging_time': 0.01680278778076172}
I0316 16:15:32.469945 140139162474240 logging_writer.py:48] [120] accumulated_eval_time=1908.21, accumulated_logging_time=0.0168028, accumulated_submission_time=124.413, global_step=120, preemption_count=0, score=124.413, test/loss=0.142453, test/num_examples=95000000, total_duration=2033.85, train/loss=0.137141, validation/loss=0.139174, validation/num_examples=83274637
I0316 16:17:34.492869 140181208745152 spec.py:321] Evaluating on the training split.
I0316 16:22:56.453061 140181208745152 spec.py:333] Evaluating on the validation split.
I0316 16:27:19.228262 140181208745152 spec.py:349] Evaluating on the test split.
I0316 16:32:31.607136 140181208745152 submission_runner.py:469] Time since start: 3053.00s, 	Step: 240, 	{'train/loss': 0.1273241730131313, 'validation/loss': 0.12946048384131043, 'validation/num_examples': 83274637, 'test/loss': 0.13188586504476446, 'test/num_examples': 95000000, 'score': 245.53588128089905, 'total_duration': 3052.998327732086, 'accumulated_submission_time': 245.53588128089905, 'accumulated_eval_time': 2805.3217902183533, 'accumulated_logging_time': 0.0342249870300293}
I0316 16:32:31.617115 140139170866944 logging_writer.py:48] [240] accumulated_eval_time=2805.32, accumulated_logging_time=0.034225, accumulated_submission_time=245.536, global_step=240, preemption_count=0, score=245.536, test/loss=0.131886, test/num_examples=95000000, total_duration=3053, train/loss=0.127324, validation/loss=0.12946, validation/num_examples=83274637
I0316 16:34:32.156279 140181208745152 spec.py:321] Evaluating on the training split.
I0316 16:39:50.232820 140181208745152 spec.py:333] Evaluating on the validation split.
I0316 16:44:14.257464 140181208745152 spec.py:349] Evaluating on the test split.
I0316 16:49:24.830945 140181208745152 submission_runner.py:469] Time since start: 4066.22s, 	Step: 358, 	{'train/loss': 0.1258894091799266, 'validation/loss': 0.12774878886112603, 'validation/num_examples': 83274637, 'test/loss': 0.1301677219848633, 'test/num_examples': 95000000, 'score': 365.16081953048706, 'total_duration': 4066.2221281528473, 'accumulated_submission_time': 365.16081953048706, 'accumulated_eval_time': 3697.9967181682587, 'accumulated_logging_time': 0.05136609077453613}
I0316 16:49:24.840628 140139162474240 logging_writer.py:48] [358] accumulated_eval_time=3698, accumulated_logging_time=0.0513661, accumulated_submission_time=365.161, global_step=358, preemption_count=0, score=365.161, test/loss=0.130168, test/num_examples=95000000, total_duration=4066.22, train/loss=0.125889, validation/loss=0.127749, validation/num_examples=83274637
I0316 16:51:26.278140 140181208745152 spec.py:321] Evaluating on the training split.
I0316 16:56:47.805962 140181208745152 spec.py:333] Evaluating on the validation split.
I0316 17:01:11.344603 140181208745152 spec.py:349] Evaluating on the test split.
I0316 17:06:23.027838 140181208745152 submission_runner.py:469] Time since start: 5084.42s, 	Step: 485, 	{'train/loss': 0.12745379960204187, 'validation/loss': 0.12795691064402312, 'validation/num_examples': 83274637, 'test/loss': 0.13062639380979035, 'test/num_examples': 95000000, 'score': 485.6574819087982, 'total_duration': 5084.418993473053, 'accumulated_submission_time': 485.6574819087982, 'accumulated_eval_time': 4594.74662065506, 'accumulated_logging_time': 0.15074467658996582}
I0316 17:06:23.038442 140139170866944 logging_writer.py:48] [485] accumulated_eval_time=4594.75, accumulated_logging_time=0.150745, accumulated_submission_time=485.657, global_step=485, preemption_count=0, score=485.657, test/loss=0.130626, test/num_examples=95000000, total_duration=5084.42, train/loss=0.127454, validation/loss=0.127957, validation/num_examples=83274637
I0316 17:06:26.538152 140139162474240 logging_writer.py:48] [500] global_step=500, grad_norm=0.0105833, loss=0.122243
I0316 17:06:26.541910 140181208745152 submission.py:265] 500) loss = 0.122, grad_norm = 0.011
I0316 17:08:24.137070 140181208745152 spec.py:321] Evaluating on the training split.
I0316 17:13:41.897028 140181208745152 spec.py:333] Evaluating on the validation split.
I0316 17:18:07.465510 140181208745152 spec.py:349] Evaluating on the test split.
I0316 17:23:26.349364 140181208745152 submission_runner.py:469] Time since start: 6107.74s, 	Step: 603, 	{'train/loss': 0.1264434402276559, 'validation/loss': 0.12702386434176258, 'validation/num_examples': 83274637, 'test/loss': 0.1294443539575677, 'test/num_examples': 95000000, 'score': 605.9041678905487, 'total_duration': 6107.740545511246, 'accumulated_submission_time': 605.9041678905487, 'accumulated_eval_time': 5496.958979606628, 'accumulated_logging_time': 0.1686539649963379}
I0316 17:23:26.359903 140139170866944 logging_writer.py:48] [603] accumulated_eval_time=5496.96, accumulated_logging_time=0.168654, accumulated_submission_time=605.904, global_step=603, preemption_count=0, score=605.904, test/loss=0.129444, test/num_examples=95000000, total_duration=6107.74, train/loss=0.126443, validation/loss=0.127024, validation/num_examples=83274637
I0316 17:25:27.478517 140181208745152 spec.py:321] Evaluating on the training split.
I0316 17:30:45.528383 140181208745152 spec.py:333] Evaluating on the validation split.
I0316 17:35:12.200973 140181208745152 spec.py:349] Evaluating on the test split.
I0316 17:40:33.396824 140181208745152 submission_runner.py:469] Time since start: 7134.79s, 	Step: 725, 	{'train/loss': 0.1254690603433518, 'validation/loss': 0.12684450910585068, 'validation/num_examples': 83274637, 'test/loss': 0.1292723991727327, 'test/num_examples': 95000000, 'score': 726.1170611381531, 'total_duration': 7134.787986755371, 'accumulated_submission_time': 726.1170611381531, 'accumulated_eval_time': 6402.877424240112, 'accumulated_logging_time': 0.18576908111572266}
I0316 17:40:33.407537 140139162474240 logging_writer.py:48] [725] accumulated_eval_time=6402.88, accumulated_logging_time=0.185769, accumulated_submission_time=726.117, global_step=725, preemption_count=0, score=726.117, test/loss=0.129272, test/num_examples=95000000, total_duration=7134.79, train/loss=0.125469, validation/loss=0.126845, validation/num_examples=83274637
I0316 17:42:34.059355 140181208745152 spec.py:321] Evaluating on the training split.
I0316 17:47:43.155190 140181208745152 spec.py:333] Evaluating on the validation split.
I0316 17:52:06.590367 140181208745152 spec.py:349] Evaluating on the test split.
I0316 17:57:18.290011 140181208745152 submission_runner.py:469] Time since start: 8139.68s, 	Step: 846, 	{'train/loss': 0.12646456184334717, 'validation/loss': 0.12670995566858775, 'validation/num_examples': 83274637, 'test/loss': 0.12909858413415207, 'test/num_examples': 95000000, 'score': 845.8772261142731, 'total_duration': 8139.681214809418, 'accumulated_submission_time': 845.8772261142731, 'accumulated_eval_time': 7287.1082100868225, 'accumulated_logging_time': 0.20389342308044434}
I0316 17:57:18.299775 140139170866944 logging_writer.py:48] [846] accumulated_eval_time=7287.11, accumulated_logging_time=0.203893, accumulated_submission_time=845.877, global_step=846, preemption_count=0, score=845.877, test/loss=0.129099, test/num_examples=95000000, total_duration=8139.68, train/loss=0.126465, validation/loss=0.12671, validation/num_examples=83274637
I0316 17:59:19.233531 140181208745152 spec.py:321] Evaluating on the training split.
I0316 18:04:14.322560 140181208745152 spec.py:333] Evaluating on the validation split.
I0316 18:08:37.394888 140181208745152 spec.py:349] Evaluating on the test split.
I0316 18:13:53.623091 140181208745152 submission_runner.py:469] Time since start: 9135.01s, 	Step: 973, 	{'train/loss': 0.12624180262136786, 'validation/loss': 0.12650072101511792, 'validation/num_examples': 83274637, 'test/loss': 0.12899522043545372, 'test/num_examples': 95000000, 'score': 965.9831962585449, 'total_duration': 9135.01430106163, 'accumulated_submission_time': 965.9831962585449, 'accumulated_eval_time': 8161.49792265892, 'accumulated_logging_time': 0.22065377235412598}
I0316 18:13:53.633082 140139162474240 logging_writer.py:48] [973] accumulated_eval_time=8161.5, accumulated_logging_time=0.220654, accumulated_submission_time=965.983, global_step=973, preemption_count=0, score=965.983, test/loss=0.128995, test/num_examples=95000000, total_duration=9135.01, train/loss=0.126242, validation/loss=0.126501, validation/num_examples=83274637
I0316 18:13:59.375116 140139170866944 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.0295486, loss=0.124097
I0316 18:13:59.378747 140181208745152 submission.py:265] 1000) loss = 0.124, grad_norm = 0.030
/usr/local/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/usr/local/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
I0316 18:15:54.950646 140181208745152 spec.py:321] Evaluating on the training split.
I0316 18:20:41.985517 140181208745152 spec.py:333] Evaluating on the validation split.
I0316 18:25:00.555329 140181208745152 spec.py:349] Evaluating on the test split.
I0316 18:30:14.986339 140181208745152 submission_runner.py:469] Time since start: 10116.38s, 	Step: 1097, 	{'train/loss': 0.12575007939299696, 'validation/loss': 0.12681081687458648, 'validation/num_examples': 83274637, 'test/loss': 0.12938142558842708, 'test/num_examples': 95000000, 'score': 1086.4497978687286, 'total_duration': 10116.37748503685, 'accumulated_submission_time': 1086.4497978687286, 'accumulated_eval_time': 9021.533694505692, 'accumulated_logging_time': 0.23719191551208496}
I0316 18:30:14.996917 140139162474240 logging_writer.py:48] [1097] accumulated_eval_time=9021.53, accumulated_logging_time=0.237192, accumulated_submission_time=1086.45, global_step=1097, preemption_count=0, score=1086.45, test/loss=0.129381, test/num_examples=95000000, total_duration=10116.4, train/loss=0.12575, validation/loss=0.126811, validation/num_examples=83274637
I0316 18:32:16.439456 140181208745152 spec.py:321] Evaluating on the training split.
I0316 18:36:24.254232 140181208745152 spec.py:333] Evaluating on the validation split.
I0316 18:40:36.536849 140181208745152 spec.py:349] Evaluating on the test split.
I0316 18:45:08.062081 140181208745152 submission_runner.py:469] Time since start: 11009.45s, 	Step: 1219, 	{'train/loss': 0.12803096882330178, 'validation/loss': 0.1261891595487554, 'validation/num_examples': 83274637, 'test/loss': 0.1284277568742451, 'test/num_examples': 95000000, 'score': 1207.014039516449, 'total_duration': 11009.453217506409, 'accumulated_submission_time': 1207.014039516449, 'accumulated_eval_time': 9793.156419277191, 'accumulated_logging_time': 0.2787332534790039}
I0316 18:45:08.072874 140139170866944 logging_writer.py:48] [1219] accumulated_eval_time=9793.16, accumulated_logging_time=0.278733, accumulated_submission_time=1207.01, global_step=1219, preemption_count=0, score=1207.01, test/loss=0.128428, test/num_examples=95000000, total_duration=11009.5, train/loss=0.128031, validation/loss=0.126189, validation/num_examples=83274637
I0316 18:47:10.078449 140181208745152 spec.py:321] Evaluating on the training split.
I0316 18:50:09.915129 140181208745152 spec.py:333] Evaluating on the validation split.
I0316 18:53:58.079779 140181208745152 spec.py:349] Evaluating on the test split.
I0316 18:57:43.588334 140181208745152 submission_runner.py:469] Time since start: 11764.98s, 	Step: 1341, 	{'train/loss': 0.1254815259491598, 'validation/loss': 0.1260857712543762, 'validation/num_examples': 83274637, 'test/loss': 0.12847336326723602, 'test/num_examples': 95000000, 'score': 1328.158063173294, 'total_duration': 11764.979552030563, 'accumulated_submission_time': 1328.158063173294, 'accumulated_eval_time': 10426.666378736496, 'accumulated_logging_time': 0.2958965301513672}
I0316 18:57:43.598679 140139162474240 logging_writer.py:48] [1341] accumulated_eval_time=10426.7, accumulated_logging_time=0.295897, accumulated_submission_time=1328.16, global_step=1341, preemption_count=0, score=1328.16, test/loss=0.128473, test/num_examples=95000000, total_duration=11765, train/loss=0.125482, validation/loss=0.126086, validation/num_examples=83274637
I0316 18:59:44.394152 140181208745152 spec.py:321] Evaluating on the training split.
I0316 19:01:51.486669 140181208745152 spec.py:333] Evaluating on the validation split.
I0316 19:04:44.950565 140181208745152 spec.py:349] Evaluating on the test split.
I0316 19:07:40.414851 140181208745152 submission_runner.py:469] Time since start: 12361.81s, 	Step: 1463, 	{'train/loss': 0.12660289508612696, 'validation/loss': 0.12580876034419636, 'validation/num_examples': 83274637, 'test/loss': 0.1283589380595157, 'test/num_examples': 95000000, 'score': 1448.1522409915924, 'total_duration': 12361.805845022202, 'accumulated_submission_time': 1448.1522409915924, 'accumulated_eval_time': 10902.68696975708, 'accumulated_logging_time': 0.31264829635620117}
I0316 19:07:40.425695 140139170866944 logging_writer.py:48] [1463] accumulated_eval_time=10902.7, accumulated_logging_time=0.312648, accumulated_submission_time=1448.15, global_step=1463, preemption_count=0, score=1448.15, test/loss=0.128359, test/num_examples=95000000, total_duration=12361.8, train/loss=0.126603, validation/loss=0.125809, validation/num_examples=83274637
I0316 19:07:56.834650 140139162474240 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.00568041, loss=0.117428
I0316 19:07:56.837894 140181208745152 submission.py:265] 1500) loss = 0.117, grad_norm = 0.006
I0316 19:09:40.841529 140181208745152 spec.py:321] Evaluating on the training split.
I0316 19:11:43.842602 140181208745152 spec.py:333] Evaluating on the validation split.
I0316 19:13:46.846723 140181208745152 spec.py:349] Evaluating on the test split.
I0316 19:16:09.243464 140181208745152 submission_runner.py:469] Time since start: 12870.63s, 	Step: 1585, 	{'train/loss': 0.12619341049612134, 'validation/loss': 0.12573149591436816, 'validation/num_examples': 83274637, 'test/loss': 0.1279760661213925, 'test/num_examples': 95000000, 'score': 1567.7167251110077, 'total_duration': 12870.634662151337, 'accumulated_submission_time': 1567.7167251110077, 'accumulated_eval_time': 11291.089179039001, 'accumulated_logging_time': 0.3302571773529053}
I0316 19:16:09.254541 140139170866944 logging_writer.py:48] [1585] accumulated_eval_time=11291.1, accumulated_logging_time=0.330257, accumulated_submission_time=1567.72, global_step=1585, preemption_count=0, score=1567.72, test/loss=0.127976, test/num_examples=95000000, total_duration=12870.6, train/loss=0.126193, validation/loss=0.125731, validation/num_examples=83274637
I0316 19:18:10.871011 140181208745152 spec.py:321] Evaluating on the training split.
I0316 19:20:14.142112 140181208745152 spec.py:333] Evaluating on the validation split.
I0316 19:22:17.130671 140181208745152 spec.py:349] Evaluating on the test split.
I0316 19:24:39.323604 140181208745152 submission_runner.py:469] Time since start: 13380.71s, 	Step: 1708, 	{'train/loss': 0.12486367374358098, 'validation/loss': 0.12617248857079533, 'validation/num_examples': 83274637, 'test/loss': 0.12865505533439234, 'test/num_examples': 95000000, 'score': 1688.409184217453, 'total_duration': 13380.714816570282, 'accumulated_submission_time': 1688.409184217453, 'accumulated_eval_time': 11679.5419652462, 'accumulated_logging_time': 0.36582398414611816}
I0316 19:24:39.335054 140139162474240 logging_writer.py:48] [1708] accumulated_eval_time=11679.5, accumulated_logging_time=0.365824, accumulated_submission_time=1688.41, global_step=1708, preemption_count=0, score=1688.41, test/loss=0.128655, test/num_examples=95000000, total_duration=13380.7, train/loss=0.124864, validation/loss=0.126172, validation/num_examples=83274637
I0316 19:26:40.735096 140181208745152 spec.py:321] Evaluating on the training split.
I0316 19:28:43.615944 140181208745152 spec.py:333] Evaluating on the validation split.
I0316 19:30:47.335903 140181208745152 spec.py:349] Evaluating on the test split.
I0316 19:33:10.708745 140181208745152 submission_runner.py:469] Time since start: 13892.10s, 	Step: 1828, 	{'train/loss': 0.12663096405459287, 'validation/loss': 0.1256523662665877, 'validation/num_examples': 83274637, 'test/loss': 0.12797576905228464, 'test/num_examples': 95000000, 'score': 1808.9412958621979, 'total_duration': 13892.099910259247, 'accumulated_submission_time': 1808.9412958621979, 'accumulated_eval_time': 12069.515704154968, 'accumulated_logging_time': 0.38375329971313477}
I0316 19:33:10.720359 140139170866944 logging_writer.py:48] [1828] accumulated_eval_time=12069.5, accumulated_logging_time=0.383753, accumulated_submission_time=1808.94, global_step=1828, preemption_count=0, score=1808.94, test/loss=0.127976, test/num_examples=95000000, total_duration=13892.1, train/loss=0.126631, validation/loss=0.125652, validation/num_examples=83274637
I0316 19:35:12.248793 140181208745152 spec.py:321] Evaluating on the training split.
I0316 19:37:15.595890 140181208745152 spec.py:333] Evaluating on the validation split.
I0316 19:39:19.694718 140181208745152 spec.py:349] Evaluating on the test split.
I0316 19:41:42.648590 140181208745152 submission_runner.py:469] Time since start: 14404.04s, 	Step: 1949, 	{'train/loss': 0.12474707067171274, 'validation/loss': 0.1254349648099846, 'validation/num_examples': 83274637, 'test/loss': 0.1278154770072937, 'test/num_examples': 95000000, 'score': 1929.595417022705, 'total_duration': 14404.039784193039, 'accumulated_submission_time': 1929.595417022705, 'accumulated_eval_time': 12459.915635347366, 'accumulated_logging_time': 0.4019780158996582}
I0316 19:41:42.660472 140139162474240 logging_writer.py:48] [1949] accumulated_eval_time=12459.9, accumulated_logging_time=0.401978, accumulated_submission_time=1929.6, global_step=1949, preemption_count=0, score=1929.6, test/loss=0.127815, test/num_examples=95000000, total_duration=14404, train/loss=0.124747, validation/loss=0.125435, validation/num_examples=83274637
I0316 19:42:17.277323 140139170866944 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.0162151, loss=0.128262
I0316 19:42:17.280823 140181208745152 submission.py:265] 2000) loss = 0.128, grad_norm = 0.016
I0316 19:43:43.183649 140181208745152 spec.py:321] Evaluating on the training split.
I0316 19:45:48.101245 140181208745152 spec.py:333] Evaluating on the validation split.
I0316 19:47:53.146755 140181208745152 spec.py:349] Evaluating on the test split.
I0316 19:50:16.500209 140181208745152 submission_runner.py:469] Time since start: 14917.89s, 	Step: 2070, 	{'train/loss': 0.12751152978227212, 'validation/loss': 0.1253650161864698, 'validation/num_examples': 83274637, 'test/loss': 0.12783301283834356, 'test/num_examples': 95000000, 'score': 2049.2589604854584, 'total_duration': 14917.891419649124, 'accumulated_submission_time': 2049.2589604854584, 'accumulated_eval_time': 12853.232397317886, 'accumulated_logging_time': 0.42183566093444824}
I0316 19:50:16.513583 140139162474240 logging_writer.py:48] [2070] accumulated_eval_time=12853.2, accumulated_logging_time=0.421836, accumulated_submission_time=2049.26, global_step=2070, preemption_count=0, score=2049.26, test/loss=0.127833, test/num_examples=95000000, total_duration=14917.9, train/loss=0.127512, validation/loss=0.125365, validation/num_examples=83274637
I0316 19:52:16.899405 140181208745152 spec.py:321] Evaluating on the training split.
I0316 19:54:21.651414 140181208745152 spec.py:333] Evaluating on the validation split.
I0316 19:56:26.882957 140181208745152 spec.py:349] Evaluating on the test split.
I0316 19:58:50.552789 140181208745152 submission_runner.py:469] Time since start: 15431.94s, 	Step: 2189, 	{'train/loss': 0.12294206385113762, 'validation/loss': 0.12528423299138927, 'validation/num_examples': 83274637, 'test/loss': 0.12764202533191882, 'test/num_examples': 95000000, 'score': 2168.7942774295807, 'total_duration': 15431.943986654282, 'accumulated_submission_time': 2168.7942774295807, 'accumulated_eval_time': 13246.885951042175, 'accumulated_logging_time': 0.447589635848999}
I0316 19:58:50.564228 140139170866944 logging_writer.py:48] [2189] accumulated_eval_time=13246.9, accumulated_logging_time=0.44759, accumulated_submission_time=2168.79, global_step=2189, preemption_count=0, score=2168.79, test/loss=0.127642, test/num_examples=95000000, total_duration=15431.9, train/loss=0.122942, validation/loss=0.125284, validation/num_examples=83274637
I0316 20:00:51.000553 140181208745152 spec.py:321] Evaluating on the training split.
I0316 20:02:54.508169 140181208745152 spec.py:333] Evaluating on the validation split.
I0316 20:04:57.975753 140181208745152 spec.py:349] Evaluating on the test split.
I0316 20:07:20.908994 140181208745152 submission_runner.py:469] Time since start: 15942.30s, 	Step: 2306, 	{'train/loss': 0.12390799015753459, 'validation/loss': 0.12544444737346924, 'validation/num_examples': 83274637, 'test/loss': 0.1277707720414011, 'test/num_examples': 95000000, 'score': 2288.3137640953064, 'total_duration': 15942.300192832947, 'accumulated_submission_time': 2288.3137640953064, 'accumulated_eval_time': 13636.79459643364, 'accumulated_logging_time': 0.46681809425354004}
I0316 20:07:20.968487 140139162474240 logging_writer.py:48] [2306] accumulated_eval_time=13636.8, accumulated_logging_time=0.466818, accumulated_submission_time=2288.31, global_step=2306, preemption_count=0, score=2288.31, test/loss=0.127771, test/num_examples=95000000, total_duration=15942.3, train/loss=0.123908, validation/loss=0.125444, validation/num_examples=83274637
I0316 20:09:22.327636 140181208745152 spec.py:321] Evaluating on the training split.
I0316 20:11:26.070464 140181208745152 spec.py:333] Evaluating on the validation split.
I0316 20:13:29.822514 140181208745152 spec.py:349] Evaluating on the test split.
I0316 20:15:52.721395 140181208745152 submission_runner.py:469] Time since start: 16454.11s, 	Step: 2423, 	{'train/loss': 0.125402417306684, 'validation/loss': 0.12561407068774919, 'validation/num_examples': 83274637, 'test/loss': 0.12810982191696166, 'test/num_examples': 95000000, 'score': 2408.7660586833954, 'total_duration': 16454.112558841705, 'accumulated_submission_time': 2408.7660586833954, 'accumulated_eval_time': 14027.188392877579, 'accumulated_logging_time': 0.5336756706237793}
I0316 20:15:52.732348 140139170866944 logging_writer.py:48] [2423] accumulated_eval_time=14027.2, accumulated_logging_time=0.533676, accumulated_submission_time=2408.77, global_step=2423, preemption_count=0, score=2408.77, test/loss=0.12811, test/num_examples=95000000, total_duration=16454.1, train/loss=0.125402, validation/loss=0.125614, validation/num_examples=83274637
I0316 20:16:59.484998 140139162474240 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.0131834, loss=0.120772
I0316 20:16:59.488137 140181208745152 submission.py:265] 2500) loss = 0.121, grad_norm = 0.013
I0316 20:17:53.183072 140181208745152 spec.py:321] Evaluating on the training split.
I0316 20:19:56.563509 140181208745152 spec.py:333] Evaluating on the validation split.
I0316 20:22:00.732776 140181208745152 spec.py:349] Evaluating on the test split.
I0316 20:24:22.924821 140181208745152 submission_runner.py:469] Time since start: 16964.32s, 	Step: 2544, 	{'train/loss': 0.1253378141863162, 'validation/loss': 0.1254119847378924, 'validation/num_examples': 83274637, 'test/loss': 0.12765511603048224, 'test/num_examples': 95000000, 'score': 2528.344512939453, 'total_duration': 16964.316038131714, 'accumulated_submission_time': 2528.344512939453, 'accumulated_eval_time': 14416.930371522903, 'accumulated_logging_time': 0.5516526699066162}
I0316 20:24:22.936398 140139170866944 logging_writer.py:48] [2544] accumulated_eval_time=14416.9, accumulated_logging_time=0.551653, accumulated_submission_time=2528.34, global_step=2544, preemption_count=0, score=2528.34, test/loss=0.127655, test/num_examples=95000000, total_duration=16964.3, train/loss=0.125338, validation/loss=0.125412, validation/num_examples=83274637
I0316 20:26:23.664561 140181208745152 spec.py:321] Evaluating on the training split.
I0316 20:28:26.687395 140181208745152 spec.py:333] Evaluating on the validation split.
I0316 20:30:30.273669 140181208745152 spec.py:349] Evaluating on the test split.
I0316 20:32:52.667681 140181208745152 submission_runner.py:469] Time since start: 17474.06s, 	Step: 2664, 	{'train/loss': 0.12424610870700317, 'validation/loss': 0.1251654935117062, 'validation/num_examples': 83274637, 'test/loss': 0.12734806865037618, 'test/num_examples': 95000000, 'score': 2648.175448179245, 'total_duration': 17474.058896303177, 'accumulated_submission_time': 2648.175448179245, 'accumulated_eval_time': 14805.933679819107, 'accumulated_logging_time': 0.6064298152923584}
I0316 20:32:52.678964 140139162474240 logging_writer.py:48] [2664] accumulated_eval_time=14805.9, accumulated_logging_time=0.60643, accumulated_submission_time=2648.18, global_step=2664, preemption_count=0, score=2648.18, test/loss=0.127348, test/num_examples=95000000, total_duration=17474.1, train/loss=0.124246, validation/loss=0.125165, validation/num_examples=83274637
I0316 20:34:53.470588 140181208745152 spec.py:321] Evaluating on the training split.
I0316 20:36:57.582638 140181208745152 spec.py:333] Evaluating on the validation split.
I0316 20:39:01.854356 140181208745152 spec.py:349] Evaluating on the test split.
I0316 20:41:24.355419 140181208745152 submission_runner.py:469] Time since start: 17985.75s, 	Step: 2782, 	{'train/loss': 0.12511900132978213, 'validation/loss': 0.12523092317305753, 'validation/num_examples': 83274637, 'test/loss': 0.1275285794981304, 'test/num_examples': 95000000, 'score': 2768.0757772922516, 'total_duration': 17985.746609210968, 'accumulated_submission_time': 2768.0757772922516, 'accumulated_eval_time': 15196.818707466125, 'accumulated_logging_time': 0.6250576972961426}
I0316 20:41:24.366483 140139170866944 logging_writer.py:48] [2782] accumulated_eval_time=15196.8, accumulated_logging_time=0.625058, accumulated_submission_time=2768.08, global_step=2782, preemption_count=0, score=2768.08, test/loss=0.127529, test/num_examples=95000000, total_duration=17985.7, train/loss=0.125119, validation/loss=0.125231, validation/num_examples=83274637
I0316 20:43:24.841978 140181208745152 spec.py:321] Evaluating on the training split.
I0316 20:45:27.874392 140181208745152 spec.py:333] Evaluating on the validation split.
I0316 20:47:31.284784 140181208745152 spec.py:349] Evaluating on the test split.
I0316 20:49:54.217775 140181208745152 submission_runner.py:469] Time since start: 18495.61s, 	Step: 2900, 	{'train/loss': 0.12341966435589709, 'validation/loss': 0.12517706683442023, 'validation/num_examples': 83274637, 'test/loss': 0.1275261687695955, 'test/num_examples': 95000000, 'score': 2887.6543431282043, 'total_duration': 18495.608956098557, 'accumulated_submission_time': 2887.6543431282043, 'accumulated_eval_time': 15586.194686174393, 'accumulated_logging_time': 0.6427402496337891}
I0316 20:49:54.228764 140139162474240 logging_writer.py:48] [2900] accumulated_eval_time=15586.2, accumulated_logging_time=0.64274, accumulated_submission_time=2887.65, global_step=2900, preemption_count=0, score=2887.65, test/loss=0.127526, test/num_examples=95000000, total_duration=18495.6, train/loss=0.12342, validation/loss=0.125177, validation/num_examples=83274637
I0316 20:51:37.096990 140139170866944 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.0499063, loss=0.128952
I0316 20:51:37.100378 140181208745152 submission.py:265] 3000) loss = 0.129, grad_norm = 0.050
I0316 20:51:55.469519 140181208745152 spec.py:321] Evaluating on the training split.
I0316 20:53:58.353877 140181208745152 spec.py:333] Evaluating on the validation split.
I0316 20:56:02.086331 140181208745152 spec.py:349] Evaluating on the test split.
I0316 20:58:24.920120 140181208745152 submission_runner.py:469] Time since start: 19006.31s, 	Step: 3015, 	{'train/loss': 0.12441105778285468, 'validation/loss': 0.12499402348831463, 'validation/num_examples': 83274637, 'test/loss': 0.12741937175172505, 'test/num_examples': 95000000, 'score': 3008.0095298290253, 'total_duration': 19006.31128835678, 'accumulated_submission_time': 3008.0095298290253, 'accumulated_eval_time': 15975.645382642746, 'accumulated_logging_time': 0.660679817199707}
I0316 20:58:24.932084 140139162474240 logging_writer.py:48] [3015] accumulated_eval_time=15975.6, accumulated_logging_time=0.66068, accumulated_submission_time=3008.01, global_step=3015, preemption_count=0, score=3008.01, test/loss=0.127419, test/num_examples=95000000, total_duration=19006.3, train/loss=0.124411, validation/loss=0.124994, validation/num_examples=83274637
I0316 21:00:25.418866 140181208745152 spec.py:321] Evaluating on the training split.
I0316 21:02:27.940958 140181208745152 spec.py:333] Evaluating on the validation split.
I0316 21:04:31.545163 140181208745152 spec.py:349] Evaluating on the test split.
I0316 21:06:53.610658 140181208745152 submission_runner.py:469] Time since start: 19515.00s, 	Step: 3139, 	{'train/loss': 0.12492931626570201, 'validation/loss': 0.12516638537550678, 'validation/num_examples': 83274637, 'test/loss': 0.1274088441752785, 'test/num_examples': 95000000, 'score': 3127.5661492347717, 'total_duration': 19515.00183224678, 'accumulated_submission_time': 3127.5661492347717, 'accumulated_eval_time': 16363.837295293808, 'accumulated_logging_time': 0.7215862274169922}
I0316 21:06:53.621998 140139170866944 logging_writer.py:48] [3139] accumulated_eval_time=16363.8, accumulated_logging_time=0.721586, accumulated_submission_time=3127.57, global_step=3139, preemption_count=0, score=3127.57, test/loss=0.127409, test/num_examples=95000000, total_duration=19515, train/loss=0.124929, validation/loss=0.125166, validation/num_examples=83274637
I0316 21:08:54.545625 140181208745152 spec.py:321] Evaluating on the training split.
I0316 21:10:57.577906 140181208745152 spec.py:333] Evaluating on the validation split.
I0316 21:13:00.859852 140181208745152 spec.py:349] Evaluating on the test split.
I0316 21:15:23.155094 140181208745152 submission_runner.py:469] Time since start: 20024.55s, 	Step: 3253, 	{'train/loss': 0.12544730028922493, 'validation/loss': 0.1253004133192221, 'validation/num_examples': 83274637, 'test/loss': 0.12773625989572626, 'test/num_examples': 95000000, 'score': 3247.6260638237, 'total_duration': 20024.54629135132, 'accumulated_submission_time': 3247.6260638237, 'accumulated_eval_time': 16752.44696354866, 'accumulated_logging_time': 0.7393789291381836}
I0316 21:15:23.166842 140139162474240 logging_writer.py:48] [3253] accumulated_eval_time=16752.4, accumulated_logging_time=0.739379, accumulated_submission_time=3247.63, global_step=3253, preemption_count=0, score=3247.63, test/loss=0.127736, test/num_examples=95000000, total_duration=20024.5, train/loss=0.125447, validation/loss=0.1253, validation/num_examples=83274637
I0316 21:17:24.417469 140181208745152 spec.py:321] Evaluating on the training split.
I0316 21:19:27.474043 140181208745152 spec.py:333] Evaluating on the validation split.
I0316 21:21:30.727303 140181208745152 spec.py:349] Evaluating on the test split.
I0316 21:23:53.132812 140181208745152 submission_runner.py:469] Time since start: 20534.52s, 	Step: 3373, 	{'train/loss': 0.12316155323226276, 'validation/loss': 0.1250868167563791, 'validation/num_examples': 83274637, 'test/loss': 0.1273301917857923, 'test/num_examples': 95000000, 'score': 3367.9772799015045, 'total_duration': 20534.524040937424, 'accumulated_submission_time': 3367.9772799015045, 'accumulated_eval_time': 17141.162449359894, 'accumulated_logging_time': 0.7582144737243652}
I0316 21:23:53.144125 140139170866944 logging_writer.py:48] [3373] accumulated_eval_time=17141.2, accumulated_logging_time=0.758214, accumulated_submission_time=3367.98, global_step=3373, preemption_count=0, score=3367.98, test/loss=0.12733, test/num_examples=95000000, total_duration=20534.5, train/loss=0.123162, validation/loss=0.125087, validation/num_examples=83274637
I0316 21:25:54.407332 140181208745152 spec.py:321] Evaluating on the training split.
I0316 21:27:56.765716 140181208745152 spec.py:333] Evaluating on the validation split.
I0316 21:29:58.892642 140181208745152 spec.py:349] Evaluating on the test split.
I0316 21:32:20.118348 140181208745152 submission_runner.py:469] Time since start: 21041.51s, 	Step: 3492, 	{'train/loss': 0.12536589375272184, 'validation/loss': 0.12493952061231464, 'validation/num_examples': 83274637, 'test/loss': 0.12726151162579186, 'test/num_examples': 95000000, 'score': 3488.3377919197083, 'total_duration': 21041.509493350983, 'accumulated_submission_time': 3488.3377919197083, 'accumulated_eval_time': 17526.873532772064, 'accumulated_logging_time': 0.7763538360595703}
I0316 21:32:20.129326 140139162474240 logging_writer.py:48] [3492] accumulated_eval_time=17526.9, accumulated_logging_time=0.776354, accumulated_submission_time=3488.34, global_step=3492, preemption_count=0, score=3488.34, test/loss=0.127262, test/num_examples=95000000, total_duration=21041.5, train/loss=0.125366, validation/loss=0.12494, validation/num_examples=83274637
I0316 21:32:22.335739 140139170866944 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.0246702, loss=0.120902
I0316 21:32:22.339299 140181208745152 submission.py:265] 3500) loss = 0.121, grad_norm = 0.025
I0316 21:34:21.087078 140181208745152 spec.py:321] Evaluating on the training split.
I0316 21:36:23.262210 140181208745152 spec.py:333] Evaluating on the validation split.
I0316 21:38:26.145714 140181208745152 spec.py:349] Evaluating on the test split.
I0316 21:40:48.526418 140181208745152 submission_runner.py:469] Time since start: 21549.92s, 	Step: 3610, 	{'train/loss': 0.12413498955950457, 'validation/loss': 0.12511678360452258, 'validation/num_examples': 83274637, 'test/loss': 0.12746903855875918, 'test/num_examples': 95000000, 'score': 3608.4204602241516, 'total_duration': 21549.917605400085, 'accumulated_submission_time': 3608.4204602241516, 'accumulated_eval_time': 17914.313052415848, 'accumulated_logging_time': 0.8120391368865967}
I0316 21:40:48.601170 140139162474240 logging_writer.py:48] [3610] accumulated_eval_time=17914.3, accumulated_logging_time=0.812039, accumulated_submission_time=3608.42, global_step=3610, preemption_count=0, score=3608.42, test/loss=0.127469, test/num_examples=95000000, total_duration=21549.9, train/loss=0.124135, validation/loss=0.125117, validation/num_examples=83274637
I0316 21:42:49.671493 140181208745152 spec.py:321] Evaluating on the training split.
I0316 21:44:52.878262 140181208745152 spec.py:333] Evaluating on the validation split.
I0316 21:46:56.121346 140181208745152 spec.py:349] Evaluating on the test split.
I0316 21:49:17.693689 140181208745152 submission_runner.py:469] Time since start: 22059.08s, 	Step: 3729, 	{'train/loss': 0.12328017278813147, 'validation/loss': 0.12493787949674454, 'validation/num_examples': 83274637, 'test/loss': 0.12737648322047182, 'test/num_examples': 95000000, 'score': 3728.5908682346344, 'total_duration': 22059.08487510681, 'accumulated_submission_time': 3728.5908682346344, 'accumulated_eval_time': 18302.335471391678, 'accumulated_logging_time': 0.8941290378570557}
I0316 21:49:17.705620 140139170866944 logging_writer.py:48] [3729] accumulated_eval_time=18302.3, accumulated_logging_time=0.894129, accumulated_submission_time=3728.59, global_step=3729, preemption_count=0, score=3728.59, test/loss=0.127376, test/num_examples=95000000, total_duration=22059.1, train/loss=0.12328, validation/loss=0.124938, validation/num_examples=83274637
I0316 21:51:19.002819 140181208745152 spec.py:321] Evaluating on the training split.
I0316 21:53:23.607560 140181208745152 spec.py:333] Evaluating on the validation split.
I0316 21:55:26.655733 140181208745152 spec.py:349] Evaluating on the test split.
I0316 21:57:50.296618 140181208745152 submission_runner.py:469] Time since start: 22571.69s, 	Step: 3847, 	{'train/loss': 0.12592793947276107, 'validation/loss': 0.12479008274334377, 'validation/num_examples': 83274637, 'test/loss': 0.12704827024487947, 'test/num_examples': 95000000, 'score': 3848.9881432056427, 'total_duration': 22571.6878221035, 'accumulated_submission_time': 3848.9881432056427, 'accumulated_eval_time': 18693.62938308716, 'accumulated_logging_time': 0.9135735034942627}
I0316 21:57:50.308066 140139162474240 logging_writer.py:48] [3847] accumulated_eval_time=18693.6, accumulated_logging_time=0.913574, accumulated_submission_time=3848.99, global_step=3847, preemption_count=0, score=3848.99, test/loss=0.127048, test/num_examples=95000000, total_duration=22571.7, train/loss=0.125928, validation/loss=0.12479, validation/num_examples=83274637
I0316 21:59:51.474897 140181208745152 spec.py:321] Evaluating on the training split.
I0316 22:01:56.270803 140181208745152 spec.py:333] Evaluating on the validation split.
I0316 22:04:00.166951 140181208745152 spec.py:349] Evaluating on the test split.
I0316 22:06:22.973128 140181208745152 submission_runner.py:469] Time since start: 23084.36s, 	Step: 3968, 	{'train/loss': 0.12345738324451955, 'validation/loss': 0.12488310064329237, 'validation/num_examples': 83274637, 'test/loss': 0.1273480261637236, 'test/num_examples': 95000000, 'score': 3969.272940158844, 'total_duration': 23084.364325761795, 'accumulated_submission_time': 3969.272940158844, 'accumulated_eval_time': 19085.127770662308, 'accumulated_logging_time': 0.9327123165130615}
I0316 22:06:22.984301 140139170866944 logging_writer.py:48] [3968] accumulated_eval_time=19085.1, accumulated_logging_time=0.932712, accumulated_submission_time=3969.27, global_step=3968, preemption_count=0, score=3969.27, test/loss=0.127348, test/num_examples=95000000, total_duration=23084.4, train/loss=0.123457, validation/loss=0.124883, validation/num_examples=83274637
I0316 22:06:33.199074 140139162474240 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.0115294, loss=0.128987
I0316 22:06:33.202805 140181208745152 submission.py:265] 4000) loss = 0.129, grad_norm = 0.012
I0316 22:08:24.554990 140181208745152 spec.py:321] Evaluating on the training split.
I0316 22:10:28.164800 140181208745152 spec.py:333] Evaluating on the validation split.
I0316 22:12:31.782751 140181208745152 spec.py:349] Evaluating on the test split.
I0316 22:14:53.040153 140181208745152 submission_runner.py:469] Time since start: 23594.43s, 	Step: 4089, 	{'train/loss': 0.12250469551193956, 'validation/loss': 0.12500144729512205, 'validation/num_examples': 83274637, 'test/loss': 0.12741363510011372, 'test/num_examples': 95000000, 'score': 4089.9683067798615, 'total_duration': 23594.431305885315, 'accumulated_submission_time': 4089.9683067798615, 'accumulated_eval_time': 19473.613102674484, 'accumulated_logging_time': 0.9594626426696777}
I0316 22:14:53.051872 140139170866944 logging_writer.py:48] [4089] accumulated_eval_time=19473.6, accumulated_logging_time=0.959463, accumulated_submission_time=4089.97, global_step=4089, preemption_count=0, score=4089.97, test/loss=0.127414, test/num_examples=95000000, total_duration=23594.4, train/loss=0.122505, validation/loss=0.125001, validation/num_examples=83274637
I0316 22:16:54.793293 140181208745152 spec.py:321] Evaluating on the training split.
I0316 22:18:58.472457 140181208745152 spec.py:333] Evaluating on the validation split.
I0316 22:21:00.846821 140181208745152 spec.py:349] Evaluating on the test split.
I0316 22:23:23.267943 140181208745152 submission_runner.py:469] Time since start: 24104.66s, 	Step: 4207, 	{'train/loss': 0.1229230084570743, 'validation/loss': 0.12496453158105235, 'validation/num_examples': 83274637, 'test/loss': 0.12731289096824244, 'test/num_examples': 95000000, 'score': 4210.821482658386, 'total_duration': 24104.659150838852, 'accumulated_submission_time': 4210.821482658386, 'accumulated_eval_time': 19862.087928533554, 'accumulated_logging_time': 0.9785106182098389}
I0316 22:23:23.280057 140139162474240 logging_writer.py:48] [4207] accumulated_eval_time=19862.1, accumulated_logging_time=0.978511, accumulated_submission_time=4210.82, global_step=4207, preemption_count=0, score=4210.82, test/loss=0.127313, test/num_examples=95000000, total_duration=24104.7, train/loss=0.122923, validation/loss=0.124965, validation/num_examples=83274637
I0316 22:25:25.221654 140181208745152 spec.py:321] Evaluating on the training split.
I0316 22:27:29.239384 140181208745152 spec.py:333] Evaluating on the validation split.
I0316 22:29:32.538196 140181208745152 spec.py:349] Evaluating on the test split.
I0316 22:31:54.339407 140181208745152 submission_runner.py:469] Time since start: 24615.73s, 	Step: 4329, 	{'train/loss': 0.12398556189409277, 'validation/loss': 0.1247866769325142, 'validation/num_examples': 83274637, 'test/loss': 0.12730782007044741, 'test/num_examples': 95000000, 'score': 4331.883793115616, 'total_duration': 24615.73056745529, 'accumulated_submission_time': 4331.883793115616, 'accumulated_eval_time': 20251.205891132355, 'accumulated_logging_time': 0.9975364208221436}
I0316 22:31:54.351030 140139170866944 logging_writer.py:48] [4329] accumulated_eval_time=20251.2, accumulated_logging_time=0.997536, accumulated_submission_time=4331.88, global_step=4329, preemption_count=0, score=4331.88, test/loss=0.127308, test/num_examples=95000000, total_duration=24615.7, train/loss=0.123986, validation/loss=0.124787, validation/num_examples=83274637
I0316 22:33:56.075936 140181208745152 spec.py:321] Evaluating on the training split.
I0316 22:35:58.928328 140181208745152 spec.py:333] Evaluating on the validation split.
I0316 22:38:01.838054 140181208745152 spec.py:349] Evaluating on the test split.
I0316 22:40:23.229939 140181208745152 submission_runner.py:469] Time since start: 25124.62s, 	Step: 4447, 	{'train/loss': 0.12487581839598826, 'validation/loss': 0.12463583014707705, 'validation/num_examples': 83274637, 'test/loss': 0.12692093808079769, 'test/num_examples': 95000000, 'score': 4452.722540378571, 'total_duration': 25124.620949983597, 'accumulated_submission_time': 4452.722540378571, 'accumulated_eval_time': 20638.35988521576, 'accumulated_logging_time': 1.0167925357818604}
I0316 22:40:23.241014 140139162474240 logging_writer.py:48] [4447] accumulated_eval_time=20638.4, accumulated_logging_time=1.01679, accumulated_submission_time=4452.72, global_step=4447, preemption_count=0, score=4452.72, test/loss=0.126921, test/num_examples=95000000, total_duration=25124.6, train/loss=0.124876, validation/loss=0.124636, validation/num_examples=83274637
I0316 22:41:00.554655 140139170866944 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.0204302, loss=0.122956
I0316 22:41:00.557878 140181208745152 submission.py:265] 4500) loss = 0.123, grad_norm = 0.020
I0316 22:42:23.978240 140181208745152 spec.py:321] Evaluating on the training split.
I0316 22:44:27.163152 140181208745152 spec.py:333] Evaluating on the validation split.
I0316 22:46:29.705841 140181208745152 spec.py:349] Evaluating on the test split.
I0316 22:48:50.670161 140181208745152 submission_runner.py:469] Time since start: 25632.06s, 	Step: 4566, 	{'train/loss': 0.12309217113999131, 'validation/loss': 0.12471049510776996, 'validation/num_examples': 83274637, 'test/loss': 0.12712802523265637, 'test/num_examples': 95000000, 'score': 4572.588643789291, 'total_duration': 25632.061377763748, 'accumulated_submission_time': 4572.588643789291, 'accumulated_eval_time': 21025.051978349686, 'accumulated_logging_time': 1.0343739986419678}
I0316 22:48:50.681867 140139162474240 logging_writer.py:48] [4566] accumulated_eval_time=21025.1, accumulated_logging_time=1.03437, accumulated_submission_time=4572.59, global_step=4566, preemption_count=0, score=4572.59, test/loss=0.127128, test/num_examples=95000000, total_duration=25632.1, train/loss=0.123092, validation/loss=0.12471, validation/num_examples=83274637
I0316 22:50:51.874967 140181208745152 spec.py:321] Evaluating on the training split.
I0316 22:52:54.966647 140181208745152 spec.py:333] Evaluating on the validation split.
I0316 22:54:58.124820 140181208745152 spec.py:349] Evaluating on the test split.
I0316 22:57:19.670593 140181208745152 submission_runner.py:469] Time since start: 26141.06s, 	Step: 4688, 	{'train/loss': 0.12356198981407786, 'validation/loss': 0.12463870838899889, 'validation/num_examples': 83274637, 'test/loss': 0.12700023189367998, 'test/num_examples': 95000000, 'score': 4692.90238904953, 'total_duration': 26141.061794757843, 'accumulated_submission_time': 4692.90238904953, 'accumulated_eval_time': 21412.8477370739, 'accumulated_logging_time': 1.0536458492279053}
I0316 22:57:19.700527 140139170866944 logging_writer.py:48] [4688] accumulated_eval_time=21412.8, accumulated_logging_time=1.05365, accumulated_submission_time=4692.9, global_step=4688, preemption_count=0, score=4692.9, test/loss=0.127, test/num_examples=95000000, total_duration=26141.1, train/loss=0.123562, validation/loss=0.124639, validation/num_examples=83274637
I0316 22:59:21.441566 140181208745152 spec.py:321] Evaluating on the training split.
I0316 23:01:24.766379 140181208745152 spec.py:333] Evaluating on the validation split.
I0316 23:03:28.164139 140181208745152 spec.py:349] Evaluating on the test split.
I0316 23:05:50.127707 140181208745152 submission_runner.py:469] Time since start: 26651.52s, 	Step: 4808, 	{'train/loss': 0.12193011551177099, 'validation/loss': 0.12458671936076056, 'validation/num_examples': 83274637, 'test/loss': 0.12701947590805857, 'test/num_examples': 95000000, 'score': 4813.704155445099, 'total_duration': 26651.518929243088, 'accumulated_submission_time': 4813.704155445099, 'accumulated_eval_time': 21801.53401017189, 'accumulated_logging_time': 1.155029296875}
I0316 23:05:50.140299 140139162474240 logging_writer.py:48] [4808] accumulated_eval_time=21801.5, accumulated_logging_time=1.15503, accumulated_submission_time=4813.7, global_step=4808, preemption_count=0, score=4813.7, test/loss=0.127019, test/num_examples=95000000, total_duration=26651.5, train/loss=0.12193, validation/loss=0.124587, validation/num_examples=83274637
I0316 23:07:51.529783 140181208745152 spec.py:321] Evaluating on the training split.
I0316 23:09:54.625736 140181208745152 spec.py:333] Evaluating on the validation split.
I0316 23:11:57.775178 140181208745152 spec.py:349] Evaluating on the test split.
I0316 23:14:19.534454 140181208745152 submission_runner.py:469] Time since start: 27160.93s, 	Step: 4930, 	{'train/loss': 0.12356177344286132, 'validation/loss': 0.12448319022650305, 'validation/num_examples': 83274637, 'test/loss': 0.12682237285132159, 'test/num_examples': 95000000, 'score': 4934.240280151367, 'total_duration': 27160.92564678192, 'accumulated_submission_time': 4934.240280151367, 'accumulated_eval_time': 22189.538902521133, 'accumulated_logging_time': 1.1752219200134277}
I0316 23:14:19.547091 140139170866944 logging_writer.py:48] [4930] accumulated_eval_time=22189.5, accumulated_logging_time=1.17522, accumulated_submission_time=4934.24, global_step=4930, preemption_count=0, score=4934.24, test/loss=0.126822, test/num_examples=95000000, total_duration=27160.9, train/loss=0.123562, validation/loss=0.124483, validation/num_examples=83274637
I0316 23:15:16.355682 140139162474240 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.0083436, loss=0.117676
I0316 23:15:16.359526 140181208745152 submission.py:265] 5000) loss = 0.118, grad_norm = 0.008
I0316 23:16:20.586956 140181208745152 spec.py:321] Evaluating on the training split.
I0316 23:18:23.376785 140181208745152 spec.py:333] Evaluating on the validation split.
I0316 23:20:26.412219 140181208745152 spec.py:349] Evaluating on the test split.
I0316 23:22:48.542039 140181208745152 submission_runner.py:469] Time since start: 27669.93s, 	Step: 5051, 	{'train/loss': 0.12272110379229738, 'validation/loss': 0.12499923404315055, 'validation/num_examples': 83274637, 'test/loss': 0.12748129179346185, 'test/num_examples': 95000000, 'score': 5054.439323663712, 'total_duration': 27669.933230400085, 'accumulated_submission_time': 5054.439323663712, 'accumulated_eval_time': 22577.494059324265, 'accumulated_logging_time': 1.1949405670166016}
I0316 23:22:48.554479 140139170866944 logging_writer.py:48] [5051] accumulated_eval_time=22577.5, accumulated_logging_time=1.19494, accumulated_submission_time=5054.44, global_step=5051, preemption_count=0, score=5054.44, test/loss=0.127481, test/num_examples=95000000, total_duration=27669.9, train/loss=0.122721, validation/loss=0.124999, validation/num_examples=83274637
I0316 23:24:50.021770 140181208745152 spec.py:321] Evaluating on the training split.
I0316 23:26:53.473198 140181208745152 spec.py:333] Evaluating on the validation split.
I0316 23:28:57.503291 140181208745152 spec.py:349] Evaluating on the test split.
I0316 23:31:17.971748 140181208745152 submission_runner.py:469] Time since start: 28179.36s, 	Step: 5170, 	{'train/loss': 0.12135522890850292, 'validation/loss': 0.1244163494665454, 'validation/num_examples': 83274637, 'test/loss': 0.1268556314465573, 'test/num_examples': 95000000, 'score': 5175.086061477661, 'total_duration': 28179.362927675247, 'accumulated_submission_time': 5175.086061477661, 'accumulated_eval_time': 22965.444279670715, 'accumulated_logging_time': 1.2145025730133057}
I0316 23:31:17.984081 140139162474240 logging_writer.py:48] [5170] accumulated_eval_time=22965.4, accumulated_logging_time=1.2145, accumulated_submission_time=5175.09, global_step=5170, preemption_count=0, score=5175.09, test/loss=0.126856, test/num_examples=95000000, total_duration=28179.4, train/loss=0.121355, validation/loss=0.124416, validation/num_examples=83274637
I0316 23:33:18.673136 140181208745152 spec.py:321] Evaluating on the training split.
I0316 23:35:21.602330 140181208745152 spec.py:333] Evaluating on the validation split.
I0316 23:37:23.440853 140181208745152 spec.py:349] Evaluating on the test split.
I0316 23:39:44.135624 140181208745152 submission_runner.py:469] Time since start: 28685.53s, 	Step: 5293, 	{'train/loss': 0.12123466394995579, 'validation/loss': 0.1243782672435212, 'validation/num_examples': 83274637, 'test/loss': 0.1267080962635241, 'test/num_examples': 95000000, 'score': 5294.916199207306, 'total_duration': 28685.52682518959, 'accumulated_submission_time': 5294.916199207306, 'accumulated_eval_time': 23350.906963586807, 'accumulated_logging_time': 1.2398149967193604}
I0316 23:39:44.147320 140139170866944 logging_writer.py:48] [5293] accumulated_eval_time=23350.9, accumulated_logging_time=1.23981, accumulated_submission_time=5294.92, global_step=5293, preemption_count=0, score=5294.92, test/loss=0.126708, test/num_examples=95000000, total_duration=28685.5, train/loss=0.121235, validation/loss=0.124378, validation/num_examples=83274637
I0316 23:41:44.981047 140181208745152 spec.py:321] Evaluating on the training split.
I0316 23:43:47.981649 140181208745152 spec.py:333] Evaluating on the validation split.
I0316 23:45:50.796087 140181208745152 spec.py:349] Evaluating on the test split.
I0316 23:48:12.031231 140181208745152 submission_runner.py:469] Time since start: 29193.42s, 	Step: 5417, 	{'train/loss': 0.12217985026593522, 'validation/loss': 0.12429058499766185, 'validation/num_examples': 83274637, 'test/loss': 0.12676774083504927, 'test/num_examples': 95000000, 'score': 5414.87371969223, 'total_duration': 29193.422446489334, 'accumulated_submission_time': 5414.87371969223, 'accumulated_eval_time': 23737.957298994064, 'accumulated_logging_time': 1.2583882808685303}
I0316 23:48:12.043252 140139162474240 logging_writer.py:48] [5417] accumulated_eval_time=23738, accumulated_logging_time=1.25839, accumulated_submission_time=5414.87, global_step=5417, preemption_count=0, score=5414.87, test/loss=0.126768, test/num_examples=95000000, total_duration=29193.4, train/loss=0.12218, validation/loss=0.124291, validation/num_examples=83274637
I0316 23:49:24.955001 140139170866944 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.0134041, loss=0.124152
I0316 23:49:24.958341 140181208745152 submission.py:265] 5500) loss = 0.124, grad_norm = 0.013
I0316 23:50:12.668295 140181208745152 spec.py:321] Evaluating on the training split.
I0316 23:52:15.608901 140181208745152 spec.py:333] Evaluating on the validation split.
I0316 23:54:18.257421 140181208745152 spec.py:349] Evaluating on the test split.
I0316 23:56:40.049120 140181208745152 submission_runner.py:469] Time since start: 29701.44s, 	Step: 5540, 	{'train/loss': 0.12285530136485176, 'validation/loss': 0.12453258623101815, 'validation/num_examples': 83274637, 'test/loss': 0.127008178195592, 'test/num_examples': 95000000, 'score': 5534.608779907227, 'total_duration': 29701.440313100815, 'accumulated_submission_time': 5534.608779907227, 'accumulated_eval_time': 24125.338285684586, 'accumulated_logging_time': 1.2776923179626465}
I0316 23:56:40.061200 140139162474240 logging_writer.py:48] [5540] accumulated_eval_time=24125.3, accumulated_logging_time=1.27769, accumulated_submission_time=5534.61, global_step=5540, preemption_count=0, score=5534.61, test/loss=0.127008, test/num_examples=95000000, total_duration=29701.4, train/loss=0.122855, validation/loss=0.124533, validation/num_examples=83274637
I0316 23:58:40.516299 140181208745152 spec.py:321] Evaluating on the training split.
I0317 00:00:42.779587 140181208745152 spec.py:333] Evaluating on the validation split.
I0317 00:02:46.082576 140181208745152 spec.py:349] Evaluating on the test split.
I0317 00:05:06.977330 140181208745152 submission_runner.py:469] Time since start: 30208.37s, 	Step: 5663, 	{'train/loss': 0.12298277395521075, 'validation/loss': 0.12445934673369424, 'validation/num_examples': 83274637, 'test/loss': 0.1268309693783007, 'test/num_examples': 95000000, 'score': 5654.239840507507, 'total_duration': 30208.368506669998, 'accumulated_submission_time': 5654.239840507507, 'accumulated_eval_time': 24511.79935193062, 'accumulated_logging_time': 1.2971458435058594}
I0317 00:05:06.989212 140139170866944 logging_writer.py:48] [5663] accumulated_eval_time=24511.8, accumulated_logging_time=1.29715, accumulated_submission_time=5654.24, global_step=5663, preemption_count=0, score=5654.24, test/loss=0.126831, test/num_examples=95000000, total_duration=30208.4, train/loss=0.122983, validation/loss=0.124459, validation/num_examples=83274637
I0317 00:07:08.673782 140181208745152 spec.py:321] Evaluating on the training split.
I0317 00:09:11.864265 140181208745152 spec.py:333] Evaluating on the validation split.
I0317 00:11:15.056751 140181208745152 spec.py:349] Evaluating on the test split.
I0317 00:13:36.267640 140181208745152 submission_runner.py:469] Time since start: 30717.66s, 	Step: 5787, 	{'train/loss': 0.11917405712239433, 'validation/loss': 0.12439383293324505, 'validation/num_examples': 83274637, 'test/loss': 0.126859343144467, 'test/num_examples': 95000000, 'score': 5775.0367612838745, 'total_duration': 30717.658841609955, 'accumulated_submission_time': 5775.0367612838745, 'accumulated_eval_time': 24899.393341064453, 'accumulated_logging_time': 1.3299486637115479}
I0317 00:13:36.280434 140139162474240 logging_writer.py:48] [5787] accumulated_eval_time=24899.4, accumulated_logging_time=1.32995, accumulated_submission_time=5775.04, global_step=5787, preemption_count=0, score=5775.04, test/loss=0.126859, test/num_examples=95000000, total_duration=30717.7, train/loss=0.119174, validation/loss=0.124394, validation/num_examples=83274637
I0317 00:15:37.193842 140181208745152 spec.py:321] Evaluating on the training split.
I0317 00:17:40.500108 140181208745152 spec.py:333] Evaluating on the validation split.
I0317 00:19:43.592818 140181208745152 spec.py:349] Evaluating on the test split.
I0317 00:22:05.285919 140181208745152 submission_runner.py:469] Time since start: 31226.68s, 	Step: 5909, 	{'train/loss': 0.12340580258931076, 'validation/loss': 0.1245556802468622, 'validation/num_examples': 83274637, 'test/loss': 0.12710951797228362, 'test/num_examples': 95000000, 'score': 5895.0895302295685, 'total_duration': 31226.67712521553, 'accumulated_submission_time': 5895.0895302295685, 'accumulated_eval_time': 25287.485607862473, 'accumulated_logging_time': 1.349191427230835}
I0317 00:22:05.298124 140139170866944 logging_writer.py:48] [5909] accumulated_eval_time=25287.5, accumulated_logging_time=1.34919, accumulated_submission_time=5895.09, global_step=5909, preemption_count=0, score=5895.09, test/loss=0.12711, test/num_examples=95000000, total_duration=31226.7, train/loss=0.123406, validation/loss=0.124556, validation/num_examples=83274637
I0317 00:23:30.508070 140139162474240 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.0288685, loss=0.130871
I0317 00:23:30.511141 140181208745152 submission.py:265] 6000) loss = 0.131, grad_norm = 0.029
I0317 00:24:06.500606 140181208745152 spec.py:321] Evaluating on the training split.
I0317 00:26:09.448663 140181208745152 spec.py:333] Evaluating on the validation split.
I0317 00:28:11.890722 140181208745152 spec.py:349] Evaluating on the test split.
I0317 00:30:31.901657 140181208745152 submission_runner.py:469] Time since start: 31733.29s, 	Step: 6029, 	{'train/loss': 0.12315157757012624, 'validation/loss': 0.12431556490713876, 'validation/num_examples': 83274637, 'test/loss': 0.1267030977824161, 'test/num_examples': 95000000, 'score': 6015.399273395538, 'total_duration': 31733.292843580246, 'accumulated_submission_time': 6015.399273395538, 'accumulated_eval_time': 25672.88678574562, 'accumulated_logging_time': 1.3677093982696533}
I0317 00:30:31.913359 140139170866944 logging_writer.py:48] [6029] accumulated_eval_time=25672.9, accumulated_logging_time=1.36771, accumulated_submission_time=6015.4, global_step=6029, preemption_count=0, score=6015.4, test/loss=0.126703, test/num_examples=95000000, total_duration=31733.3, train/loss=0.123152, validation/loss=0.124316, validation/num_examples=83274637
I0317 00:32:32.338243 140181208745152 spec.py:321] Evaluating on the training split.
I0317 00:34:36.920586 140181208745152 spec.py:333] Evaluating on the validation split.
I0317 00:36:41.602847 140181208745152 spec.py:349] Evaluating on the test split.
I0317 00:39:04.833317 140181208745152 submission_runner.py:469] Time since start: 32246.22s, 	Step: 6150, 	{'train/loss': 0.12166755169459836, 'validation/loss': 0.12423944409555382, 'validation/num_examples': 83274637, 'test/loss': 0.12655657462495504, 'test/num_examples': 95000000, 'score': 6134.955957651138, 'total_duration': 32246.224467515945, 'accumulated_submission_time': 6134.955957651138, 'accumulated_eval_time': 26065.381944656372, 'accumulated_logging_time': 1.3858916759490967}
I0317 00:39:04.845304 140139162474240 logging_writer.py:48] [6150] accumulated_eval_time=26065.4, accumulated_logging_time=1.38589, accumulated_submission_time=6134.96, global_step=6150, preemption_count=0, score=6134.96, test/loss=0.126557, test/num_examples=95000000, total_duration=32246.2, train/loss=0.121668, validation/loss=0.124239, validation/num_examples=83274637
I0317 00:41:05.827869 140181208745152 spec.py:321] Evaluating on the training split.
I0317 00:43:08.568509 140181208745152 spec.py:333] Evaluating on the validation split.
I0317 00:45:12.143782 140181208745152 spec.py:349] Evaluating on the test split.
I0317 00:47:33.737439 140181208745152 submission_runner.py:469] Time since start: 32755.13s, 	Step: 6271, 	{'train/loss': 0.12135904260610378, 'validation/loss': 0.12417653811950548, 'validation/num_examples': 83274637, 'test/loss': 0.12653266436466418, 'test/num_examples': 95000000, 'score': 6255.084678888321, 'total_duration': 32755.12862610817, 'accumulated_submission_time': 6255.084678888321, 'accumulated_eval_time': 26453.29153943062, 'accumulated_logging_time': 1.4204180240631104}
I0317 00:47:33.749042 140139170866944 logging_writer.py:48] [6271] accumulated_eval_time=26453.3, accumulated_logging_time=1.42042, accumulated_submission_time=6255.08, global_step=6271, preemption_count=0, score=6255.08, test/loss=0.126533, test/num_examples=95000000, total_duration=32755.1, train/loss=0.121359, validation/loss=0.124177, validation/num_examples=83274637
I0317 00:49:34.302634 140181208745152 spec.py:321] Evaluating on the training split.
I0317 00:51:37.529490 140181208745152 spec.py:333] Evaluating on the validation split.
I0317 00:53:40.539739 140181208745152 spec.py:349] Evaluating on the test split.
I0317 00:56:01.906923 140181208745152 submission_runner.py:469] Time since start: 33263.30s, 	Step: 6394, 	{'train/loss': 0.12404263507709098, 'validation/loss': 0.1241142463495907, 'validation/num_examples': 83274637, 'test/loss': 0.12653920686436704, 'test/num_examples': 95000000, 'score': 6374.777480363846, 'total_duration': 33263.29815340042, 'accumulated_submission_time': 6374.777480363846, 'accumulated_eval_time': 26840.8960146904, 'accumulated_logging_time': 1.4390699863433838}
I0317 00:56:01.918262 140139162474240 logging_writer.py:48] [6394] accumulated_eval_time=26840.9, accumulated_logging_time=1.43907, accumulated_submission_time=6374.78, global_step=6394, preemption_count=0, score=6374.78, test/loss=0.126539, test/num_examples=95000000, total_duration=33263.3, train/loss=0.124043, validation/loss=0.124114, validation/num_examples=83274637
I0317 00:57:43.261377 140139170866944 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.0209665, loss=0.124305
I0317 00:57:43.265094 140181208745152 submission.py:265] 6500) loss = 0.124, grad_norm = 0.021
I0317 00:58:02.832008 140181208745152 spec.py:321] Evaluating on the training split.
I0317 01:00:04.941060 140181208745152 spec.py:333] Evaluating on the validation split.
I0317 01:02:07.998873 140181208745152 spec.py:349] Evaluating on the test split.
I0317 01:04:29.918518 140181208745152 submission_runner.py:469] Time since start: 33771.31s, 	Step: 6516, 	{'train/loss': 0.12135663798045732, 'validation/loss': 0.12415910087553349, 'validation/num_examples': 83274637, 'test/loss': 0.1264881684854608, 'test/num_examples': 95000000, 'score': 6494.826027393341, 'total_duration': 33771.30974674225, 'accumulated_submission_time': 6494.826027393341, 'accumulated_eval_time': 27227.982669353485, 'accumulated_logging_time': 1.4568839073181152}
I0317 01:04:29.930550 140139162474240 logging_writer.py:48] [6516] accumulated_eval_time=27228, accumulated_logging_time=1.45688, accumulated_submission_time=6494.83, global_step=6516, preemption_count=0, score=6494.83, test/loss=0.126488, test/num_examples=95000000, total_duration=33771.3, train/loss=0.121357, validation/loss=0.124159, validation/num_examples=83274637
I0317 01:06:31.107992 140181208745152 spec.py:321] Evaluating on the training split.
I0317 01:08:34.259674 140181208745152 spec.py:333] Evaluating on the validation split.
I0317 01:10:37.240023 140181208745152 spec.py:349] Evaluating on the test split.
I0317 01:12:57.025937 140181208745152 submission_runner.py:469] Time since start: 34278.42s, 	Step: 6637, 	{'train/loss': 0.12285944768225805, 'validation/loss': 0.12405932304511992, 'validation/num_examples': 83274637, 'test/loss': 0.12632373783962852, 'test/num_examples': 95000000, 'score': 6615.15429520607, 'total_duration': 34278.417143821716, 'accumulated_submission_time': 6615.15429520607, 'accumulated_eval_time': 27613.900813102722, 'accumulated_logging_time': 1.4759395122528076}
I0317 01:12:57.038748 140139170866944 logging_writer.py:48] [6637] accumulated_eval_time=27613.9, accumulated_logging_time=1.47594, accumulated_submission_time=6615.15, global_step=6637, preemption_count=0, score=6615.15, test/loss=0.126324, test/num_examples=95000000, total_duration=34278.4, train/loss=0.122859, validation/loss=0.124059, validation/num_examples=83274637
I0317 01:14:57.999242 140181208745152 spec.py:321] Evaluating on the training split.
I0317 01:17:01.095500 140181208745152 spec.py:333] Evaluating on the validation split.
I0317 01:19:04.010749 140181208745152 spec.py:349] Evaluating on the test split.
I0317 01:21:25.313778 140181208745152 submission_runner.py:469] Time since start: 34786.70s, 	Step: 6758, 	{'train/loss': 0.12375294668772192, 'validation/loss': 0.12403767903251268, 'validation/num_examples': 83274637, 'test/loss': 0.12633192928619386, 'test/num_examples': 95000000, 'score': 6735.238496303558, 'total_duration': 34786.704961299896, 'accumulated_submission_time': 6735.238496303558, 'accumulated_eval_time': 28001.21543264389, 'accumulated_logging_time': 1.5407261848449707}
I0317 01:21:25.326906 140139162474240 logging_writer.py:48] [6758] accumulated_eval_time=28001.2, accumulated_logging_time=1.54073, accumulated_submission_time=6735.24, global_step=6758, preemption_count=0, score=6735.24, test/loss=0.126332, test/num_examples=95000000, total_duration=34786.7, train/loss=0.123753, validation/loss=0.124038, validation/num_examples=83274637
I0317 01:23:26.597135 140181208745152 spec.py:321] Evaluating on the training split.
I0317 01:25:29.732205 140181208745152 spec.py:333] Evaluating on the validation split.
I0317 01:27:32.237679 140181208745152 spec.py:349] Evaluating on the test split.
I0317 01:29:54.110494 140181208745152 submission_runner.py:469] Time since start: 35295.50s, 	Step: 6883, 	{'train/loss': 0.11919287744279787, 'validation/loss': 0.12392968877975175, 'validation/num_examples': 83274637, 'test/loss': 0.1261584628999409, 'test/num_examples': 95000000, 'score': 6855.638538599014, 'total_duration': 35295.501693964005, 'accumulated_submission_time': 6855.638538599014, 'accumulated_eval_time': 28388.728944063187, 'accumulated_logging_time': 1.5613288879394531}
I0317 01:29:54.123278 140139170866944 logging_writer.py:48] [6883] accumulated_eval_time=28388.7, accumulated_logging_time=1.56133, accumulated_submission_time=6855.64, global_step=6883, preemption_count=0, score=6855.64, test/loss=0.126158, test/num_examples=95000000, total_duration=35295.5, train/loss=0.119193, validation/loss=0.12393, validation/num_examples=83274637
I0317 01:31:51.402569 140139162474240 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.00900966, loss=0.118798
I0317 01:31:51.405994 140181208745152 submission.py:265] 7000) loss = 0.119, grad_norm = 0.009
I0317 01:31:55.418710 140181208745152 spec.py:321] Evaluating on the training split.
I0317 01:33:58.514291 140181208745152 spec.py:333] Evaluating on the validation split.
I0317 01:36:01.612368 140181208745152 spec.py:349] Evaluating on the test split.
I0317 01:38:23.327484 140181208745152 submission_runner.py:469] Time since start: 35804.72s, 	Step: 7004, 	{'train/loss': 0.12444886490146548, 'validation/loss': 0.12396466572273408, 'validation/num_examples': 83274637, 'test/loss': 0.12636229081726075, 'test/num_examples': 95000000, 'score': 6976.12876868248, 'total_duration': 35804.7186357975, 'accumulated_submission_time': 6976.12876868248, 'accumulated_eval_time': 28776.637721776962, 'accumulated_logging_time': 1.5809059143066406}
I0317 01:38:23.340081 140139170866944 logging_writer.py:48] [7004] accumulated_eval_time=28776.6, accumulated_logging_time=1.58091, accumulated_submission_time=6976.13, global_step=7004, preemption_count=0, score=6976.13, test/loss=0.126362, test/num_examples=95000000, total_duration=35804.7, train/loss=0.124449, validation/loss=0.123965, validation/num_examples=83274637
I0317 01:40:24.683973 140181208745152 spec.py:321] Evaluating on the training split.
I0317 01:42:27.982704 140181208745152 spec.py:333] Evaluating on the validation split.
I0317 01:44:31.134280 140181208745152 spec.py:349] Evaluating on the test split.
I0317 01:46:53.128805 140181208745152 submission_runner.py:469] Time since start: 36314.52s, 	Step: 7125, 	{'train/loss': 0.12493567109146815, 'validation/loss': 0.12404675320626427, 'validation/num_examples': 83274637, 'test/loss': 0.12644923569472463, 'test/num_examples': 95000000, 'score': 7096.604103088379, 'total_duration': 36314.520011901855, 'accumulated_submission_time': 7096.604103088379, 'accumulated_eval_time': 29165.082663297653, 'accumulated_logging_time': 1.6008546352386475}
I0317 01:46:53.203632 140139162474240 logging_writer.py:48] [7125] accumulated_eval_time=29165.1, accumulated_logging_time=1.60085, accumulated_submission_time=7096.6, global_step=7125, preemption_count=0, score=7096.6, test/loss=0.126449, test/num_examples=95000000, total_duration=36314.5, train/loss=0.124936, validation/loss=0.124047, validation/num_examples=83274637
I0317 01:48:53.733351 140181208745152 spec.py:321] Evaluating on the training split.
I0317 01:50:56.507213 140181208745152 spec.py:333] Evaluating on the validation split.
I0317 01:52:59.644195 140181208745152 spec.py:349] Evaluating on the test split.
I0317 01:55:19.402984 140181208745152 submission_runner.py:469] Time since start: 36820.79s, 	Step: 7245, 	{'train/loss': 0.12484628254435583, 'validation/loss': 0.12400216520401706, 'validation/num_examples': 83274637, 'test/loss': 0.12630540111172325, 'test/num_examples': 95000000, 'score': 7216.238992929459, 'total_duration': 36820.79420351982, 'accumulated_submission_time': 7216.238992929459, 'accumulated_eval_time': 29550.752522945404, 'accumulated_logging_time': 1.6964330673217773}
I0317 01:55:19.415466 140139170866944 logging_writer.py:48] [7245] accumulated_eval_time=29550.8, accumulated_logging_time=1.69643, accumulated_submission_time=7216.24, global_step=7245, preemption_count=0, score=7216.24, test/loss=0.126305, test/num_examples=95000000, total_duration=36820.8, train/loss=0.124846, validation/loss=0.124002, validation/num_examples=83274637
I0317 01:57:21.281264 140181208745152 spec.py:321] Evaluating on the training split.
I0317 01:59:24.584754 140181208745152 spec.py:333] Evaluating on the validation split.
I0317 02:01:27.589485 140181208745152 spec.py:349] Evaluating on the test split.
I0317 02:03:48.793874 140181208745152 submission_runner.py:469] Time since start: 37330.19s, 	Step: 7364, 	{'train/loss': 0.12225591461984951, 'validation/loss': 0.12398705394431331, 'validation/num_examples': 83274637, 'test/loss': 0.1262943579285471, 'test/num_examples': 95000000, 'score': 7337.275809288025, 'total_duration': 37330.18508911133, 'accumulated_submission_time': 7337.275809288025, 'accumulated_eval_time': 29938.265198469162, 'accumulated_logging_time': 1.7156744003295898}
I0317 02:03:48.806033 140139162474240 logging_writer.py:48] [7364] accumulated_eval_time=29938.3, accumulated_logging_time=1.71567, accumulated_submission_time=7337.28, global_step=7364, preemption_count=0, score=7337.28, test/loss=0.126294, test/num_examples=95000000, total_duration=37330.2, train/loss=0.122256, validation/loss=0.123987, validation/num_examples=83274637
I0317 02:05:49.872599 140181208745152 spec.py:321] Evaluating on the training split.
I0317 02:07:53.681918 140181208745152 spec.py:333] Evaluating on the validation split.
I0317 02:09:57.403059 140181208745152 spec.py:349] Evaluating on the test split.
I0317 02:12:18.364561 140181208745152 submission_runner.py:469] Time since start: 37839.76s, 	Step: 7481, 	{'train/loss': 0.12115653709049688, 'validation/loss': 0.12401164670955134, 'validation/num_examples': 83274637, 'test/loss': 0.12629014404734562, 'test/num_examples': 95000000, 'score': 7457.494492053986, 'total_duration': 37839.75578594208, 'accumulated_submission_time': 7457.494492053986, 'accumulated_eval_time': 30326.757284402847, 'accumulated_logging_time': 1.7349376678466797}
I0317 02:12:18.405519 140139170866944 logging_writer.py:48] [7481] accumulated_eval_time=30326.8, accumulated_logging_time=1.73494, accumulated_submission_time=7457.49, global_step=7481, preemption_count=0, score=7457.49, test/loss=0.12629, test/num_examples=95000000, total_duration=37839.8, train/loss=0.121157, validation/loss=0.124012, validation/num_examples=83274637
I0317 02:12:22.999597 140139162474240 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.0131779, loss=0.124888
I0317 02:12:23.004134 140181208745152 submission.py:265] 7500) loss = 0.125, grad_norm = 0.013
I0317 02:14:19.206965 140181208745152 spec.py:321] Evaluating on the training split.
I0317 02:16:22.826136 140181208745152 spec.py:333] Evaluating on the validation split.
I0317 02:18:27.438262 140181208745152 spec.py:349] Evaluating on the test split.
I0317 02:20:50.782979 140181208745152 submission_runner.py:469] Time since start: 38352.17s, 	Step: 7602, 	{'train/loss': 0.1207529849799897, 'validation/loss': 0.1238975946431092, 'validation/num_examples': 83274637, 'test/loss': 0.12611857605100932, 'test/num_examples': 95000000, 'score': 7577.397085428238, 'total_duration': 38352.174194812775, 'accumulated_submission_time': 7577.397085428238, 'accumulated_eval_time': 30718.333451271057, 'accumulated_logging_time': 1.7828757762908936}
I0317 02:20:50.796062 140139170866944 logging_writer.py:48] [7602] accumulated_eval_time=30718.3, accumulated_logging_time=1.78288, accumulated_submission_time=7577.4, global_step=7602, preemption_count=0, score=7577.4, test/loss=0.126119, test/num_examples=95000000, total_duration=38352.2, train/loss=0.120753, validation/loss=0.123898, validation/num_examples=83274637
I0317 02:22:51.293237 140181208745152 spec.py:321] Evaluating on the training split.
I0317 02:24:54.744272 140181208745152 spec.py:333] Evaluating on the validation split.
I0317 02:26:57.980202 140181208745152 spec.py:349] Evaluating on the test split.
I0317 02:29:19.625843 140181208745152 submission_runner.py:469] Time since start: 38861.02s, 	Step: 7723, 	{'train/loss': 0.12375526981911608, 'validation/loss': 0.12395954214548215, 'validation/num_examples': 83274637, 'test/loss': 0.12627279322766755, 'test/num_examples': 95000000, 'score': 7697.026575803757, 'total_duration': 38861.01707100868, 'accumulated_submission_time': 7697.026575803757, 'accumulated_eval_time': 31106.66635465622, 'accumulated_logging_time': 1.8026072978973389}
I0317 02:29:19.638906 140139162474240 logging_writer.py:48] [7723] accumulated_eval_time=31106.7, accumulated_logging_time=1.80261, accumulated_submission_time=7697.03, global_step=7723, preemption_count=0, score=7697.03, test/loss=0.126273, test/num_examples=95000000, total_duration=38861, train/loss=0.123755, validation/loss=0.12396, validation/num_examples=83274637
I0317 02:31:19.729929 140139170866944 logging_writer.py:48] [7841] global_step=7841, preemption_count=0, score=7816.65
I0317 02:31:24.049356 140181208745152 submission_runner.py:646] Tuning trial 5/5
I0317 02:31:24.049565 140181208745152 submission_runner.py:647] Hyperparameters: Hyperparameters(learning_rate=0.001308209823469072, one_minus_beta1=0.02686663061, one_minus_beta2=0.00187670778, epsilon=1e-08, one_minus_momentum=0.0, use_momentum=False, weight_decay=0.16375311233774334, max_preconditioner_dim=1024, precondition_frequency=100, start_preconditioning_step=-1, inv_root_override=0, exponent_multiplier=1.0, grafting_type='ADAM', grafting_epsilon=1e-08, use_normalized_grafting=False, communication_dtype='FP32', communicate_params=True, use_cosine_decay=True, warmup_factor=0.1, label_smoothing=0.1, dropout_rate=0.0, use_nadam=True, step_hint_factor=1.0)
I0317 02:31:24.050432 140181208745152 submission_runner.py:648] Metrics: {'eval_results': [(1, {'train/loss': 0.704038417173838, 'validation/loss': 0.7047587307571139, 'validation/num_examples': 83274637, 'test/loss': 0.7043476190105238, 'test/num_examples': 95000000, 'score': 4.59600305557251, 'total_duration': 995.5325169563293, 'accumulated_submission_time': 4.59600305557251, 'accumulated_eval_time': 990.5274004936218, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (120, {'train/loss': 0.13714099504149235, 'validation/loss': 0.13917363571742725, 'validation/num_examples': 83274637, 'test/loss': 0.14245255075703672, 'test/num_examples': 95000000, 'score': 124.41312885284424, 'total_duration': 2033.8508625030518, 'accumulated_submission_time': 124.41312885284424, 'accumulated_eval_time': 1908.2074134349823, 'accumulated_logging_time': 0.01680278778076172, 'global_step': 120, 'preemption_count': 0}), (240, {'train/loss': 0.1273241730131313, 'validation/loss': 0.12946048384131043, 'validation/num_examples': 83274637, 'test/loss': 0.13188586504476446, 'test/num_examples': 95000000, 'score': 245.53588128089905, 'total_duration': 3052.998327732086, 'accumulated_submission_time': 245.53588128089905, 'accumulated_eval_time': 2805.3217902183533, 'accumulated_logging_time': 0.0342249870300293, 'global_step': 240, 'preemption_count': 0}), (358, {'train/loss': 0.1258894091799266, 'validation/loss': 0.12774878886112603, 'validation/num_examples': 83274637, 'test/loss': 0.1301677219848633, 'test/num_examples': 95000000, 'score': 365.16081953048706, 'total_duration': 4066.2221281528473, 'accumulated_submission_time': 365.16081953048706, 'accumulated_eval_time': 3697.9967181682587, 'accumulated_logging_time': 0.05136609077453613, 'global_step': 358, 'preemption_count': 0}), (485, {'train/loss': 0.12745379960204187, 'validation/loss': 0.12795691064402312, 'validation/num_examples': 83274637, 'test/loss': 0.13062639380979035, 'test/num_examples': 95000000, 'score': 485.6574819087982, 'total_duration': 5084.418993473053, 'accumulated_submission_time': 485.6574819087982, 'accumulated_eval_time': 4594.74662065506, 'accumulated_logging_time': 0.15074467658996582, 'global_step': 485, 'preemption_count': 0}), (603, {'train/loss': 0.1264434402276559, 'validation/loss': 0.12702386434176258, 'validation/num_examples': 83274637, 'test/loss': 0.1294443539575677, 'test/num_examples': 95000000, 'score': 605.9041678905487, 'total_duration': 6107.740545511246, 'accumulated_submission_time': 605.9041678905487, 'accumulated_eval_time': 5496.958979606628, 'accumulated_logging_time': 0.1686539649963379, 'global_step': 603, 'preemption_count': 0}), (725, {'train/loss': 0.1254690603433518, 'validation/loss': 0.12684450910585068, 'validation/num_examples': 83274637, 'test/loss': 0.1292723991727327, 'test/num_examples': 95000000, 'score': 726.1170611381531, 'total_duration': 7134.787986755371, 'accumulated_submission_time': 726.1170611381531, 'accumulated_eval_time': 6402.877424240112, 'accumulated_logging_time': 0.18576908111572266, 'global_step': 725, 'preemption_count': 0}), (846, {'train/loss': 0.12646456184334717, 'validation/loss': 0.12670995566858775, 'validation/num_examples': 83274637, 'test/loss': 0.12909858413415207, 'test/num_examples': 95000000, 'score': 845.8772261142731, 'total_duration': 8139.681214809418, 'accumulated_submission_time': 845.8772261142731, 'accumulated_eval_time': 7287.1082100868225, 'accumulated_logging_time': 0.20389342308044434, 'global_step': 846, 'preemption_count': 0}), (973, {'train/loss': 0.12624180262136786, 'validation/loss': 0.12650072101511792, 'validation/num_examples': 83274637, 'test/loss': 0.12899522043545372, 'test/num_examples': 95000000, 'score': 965.9831962585449, 'total_duration': 9135.01430106163, 'accumulated_submission_time': 965.9831962585449, 'accumulated_eval_time': 8161.49792265892, 'accumulated_logging_time': 0.22065377235412598, 'global_step': 973, 'preemption_count': 0}), (1097, {'train/loss': 0.12575007939299696, 'validation/loss': 0.12681081687458648, 'validation/num_examples': 83274637, 'test/loss': 0.12938142558842708, 'test/num_examples': 95000000, 'score': 1086.4497978687286, 'total_duration': 10116.37748503685, 'accumulated_submission_time': 1086.4497978687286, 'accumulated_eval_time': 9021.533694505692, 'accumulated_logging_time': 0.23719191551208496, 'global_step': 1097, 'preemption_count': 0}), (1219, {'train/loss': 0.12803096882330178, 'validation/loss': 0.1261891595487554, 'validation/num_examples': 83274637, 'test/loss': 0.1284277568742451, 'test/num_examples': 95000000, 'score': 1207.014039516449, 'total_duration': 11009.453217506409, 'accumulated_submission_time': 1207.014039516449, 'accumulated_eval_time': 9793.156419277191, 'accumulated_logging_time': 0.2787332534790039, 'global_step': 1219, 'preemption_count': 0}), (1341, {'train/loss': 0.1254815259491598, 'validation/loss': 0.1260857712543762, 'validation/num_examples': 83274637, 'test/loss': 0.12847336326723602, 'test/num_examples': 95000000, 'score': 1328.158063173294, 'total_duration': 11764.979552030563, 'accumulated_submission_time': 1328.158063173294, 'accumulated_eval_time': 10426.666378736496, 'accumulated_logging_time': 0.2958965301513672, 'global_step': 1341, 'preemption_count': 0}), (1463, {'train/loss': 0.12660289508612696, 'validation/loss': 0.12580876034419636, 'validation/num_examples': 83274637, 'test/loss': 0.1283589380595157, 'test/num_examples': 95000000, 'score': 1448.1522409915924, 'total_duration': 12361.805845022202, 'accumulated_submission_time': 1448.1522409915924, 'accumulated_eval_time': 10902.68696975708, 'accumulated_logging_time': 0.31264829635620117, 'global_step': 1463, 'preemption_count': 0}), (1585, {'train/loss': 0.12619341049612134, 'validation/loss': 0.12573149591436816, 'validation/num_examples': 83274637, 'test/loss': 0.1279760661213925, 'test/num_examples': 95000000, 'score': 1567.7167251110077, 'total_duration': 12870.634662151337, 'accumulated_submission_time': 1567.7167251110077, 'accumulated_eval_time': 11291.089179039001, 'accumulated_logging_time': 0.3302571773529053, 'global_step': 1585, 'preemption_count': 0}), (1708, {'train/loss': 0.12486367374358098, 'validation/loss': 0.12617248857079533, 'validation/num_examples': 83274637, 'test/loss': 0.12865505533439234, 'test/num_examples': 95000000, 'score': 1688.409184217453, 'total_duration': 13380.714816570282, 'accumulated_submission_time': 1688.409184217453, 'accumulated_eval_time': 11679.5419652462, 'accumulated_logging_time': 0.36582398414611816, 'global_step': 1708, 'preemption_count': 0}), (1828, {'train/loss': 0.12663096405459287, 'validation/loss': 0.1256523662665877, 'validation/num_examples': 83274637, 'test/loss': 0.12797576905228464, 'test/num_examples': 95000000, 'score': 1808.9412958621979, 'total_duration': 13892.099910259247, 'accumulated_submission_time': 1808.9412958621979, 'accumulated_eval_time': 12069.515704154968, 'accumulated_logging_time': 0.38375329971313477, 'global_step': 1828, 'preemption_count': 0}), (1949, {'train/loss': 0.12474707067171274, 'validation/loss': 0.1254349648099846, 'validation/num_examples': 83274637, 'test/loss': 0.1278154770072937, 'test/num_examples': 95000000, 'score': 1929.595417022705, 'total_duration': 14404.039784193039, 'accumulated_submission_time': 1929.595417022705, 'accumulated_eval_time': 12459.915635347366, 'accumulated_logging_time': 0.4019780158996582, 'global_step': 1949, 'preemption_count': 0}), (2070, {'train/loss': 0.12751152978227212, 'validation/loss': 0.1253650161864698, 'validation/num_examples': 83274637, 'test/loss': 0.12783301283834356, 'test/num_examples': 95000000, 'score': 2049.2589604854584, 'total_duration': 14917.891419649124, 'accumulated_submission_time': 2049.2589604854584, 'accumulated_eval_time': 12853.232397317886, 'accumulated_logging_time': 0.42183566093444824, 'global_step': 2070, 'preemption_count': 0}), (2189, {'train/loss': 0.12294206385113762, 'validation/loss': 0.12528423299138927, 'validation/num_examples': 83274637, 'test/loss': 0.12764202533191882, 'test/num_examples': 95000000, 'score': 2168.7942774295807, 'total_duration': 15431.943986654282, 'accumulated_submission_time': 2168.7942774295807, 'accumulated_eval_time': 13246.885951042175, 'accumulated_logging_time': 0.447589635848999, 'global_step': 2189, 'preemption_count': 0}), (2306, {'train/loss': 0.12390799015753459, 'validation/loss': 0.12544444737346924, 'validation/num_examples': 83274637, 'test/loss': 0.1277707720414011, 'test/num_examples': 95000000, 'score': 2288.3137640953064, 'total_duration': 15942.300192832947, 'accumulated_submission_time': 2288.3137640953064, 'accumulated_eval_time': 13636.79459643364, 'accumulated_logging_time': 0.46681809425354004, 'global_step': 2306, 'preemption_count': 0}), (2423, {'train/loss': 0.125402417306684, 'validation/loss': 0.12561407068774919, 'validation/num_examples': 83274637, 'test/loss': 0.12810982191696166, 'test/num_examples': 95000000, 'score': 2408.7660586833954, 'total_duration': 16454.112558841705, 'accumulated_submission_time': 2408.7660586833954, 'accumulated_eval_time': 14027.188392877579, 'accumulated_logging_time': 0.5336756706237793, 'global_step': 2423, 'preemption_count': 0}), (2544, {'train/loss': 0.1253378141863162, 'validation/loss': 0.1254119847378924, 'validation/num_examples': 83274637, 'test/loss': 0.12765511603048224, 'test/num_examples': 95000000, 'score': 2528.344512939453, 'total_duration': 16964.316038131714, 'accumulated_submission_time': 2528.344512939453, 'accumulated_eval_time': 14416.930371522903, 'accumulated_logging_time': 0.5516526699066162, 'global_step': 2544, 'preemption_count': 0}), (2664, {'train/loss': 0.12424610870700317, 'validation/loss': 0.1251654935117062, 'validation/num_examples': 83274637, 'test/loss': 0.12734806865037618, 'test/num_examples': 95000000, 'score': 2648.175448179245, 'total_duration': 17474.058896303177, 'accumulated_submission_time': 2648.175448179245, 'accumulated_eval_time': 14805.933679819107, 'accumulated_logging_time': 0.6064298152923584, 'global_step': 2664, 'preemption_count': 0}), (2782, {'train/loss': 0.12511900132978213, 'validation/loss': 0.12523092317305753, 'validation/num_examples': 83274637, 'test/loss': 0.1275285794981304, 'test/num_examples': 95000000, 'score': 2768.0757772922516, 'total_duration': 17985.746609210968, 'accumulated_submission_time': 2768.0757772922516, 'accumulated_eval_time': 15196.818707466125, 'accumulated_logging_time': 0.6250576972961426, 'global_step': 2782, 'preemption_count': 0}), (2900, {'train/loss': 0.12341966435589709, 'validation/loss': 0.12517706683442023, 'validation/num_examples': 83274637, 'test/loss': 0.1275261687695955, 'test/num_examples': 95000000, 'score': 2887.6543431282043, 'total_duration': 18495.608956098557, 'accumulated_submission_time': 2887.6543431282043, 'accumulated_eval_time': 15586.194686174393, 'accumulated_logging_time': 0.6427402496337891, 'global_step': 2900, 'preemption_count': 0}), (3015, {'train/loss': 0.12441105778285468, 'validation/loss': 0.12499402348831463, 'validation/num_examples': 83274637, 'test/loss': 0.12741937175172505, 'test/num_examples': 95000000, 'score': 3008.0095298290253, 'total_duration': 19006.31128835678, 'accumulated_submission_time': 3008.0095298290253, 'accumulated_eval_time': 15975.645382642746, 'accumulated_logging_time': 0.660679817199707, 'global_step': 3015, 'preemption_count': 0}), (3139, {'train/loss': 0.12492931626570201, 'validation/loss': 0.12516638537550678, 'validation/num_examples': 83274637, 'test/loss': 0.1274088441752785, 'test/num_examples': 95000000, 'score': 3127.5661492347717, 'total_duration': 19515.00183224678, 'accumulated_submission_time': 3127.5661492347717, 'accumulated_eval_time': 16363.837295293808, 'accumulated_logging_time': 0.7215862274169922, 'global_step': 3139, 'preemption_count': 0}), (3253, {'train/loss': 0.12544730028922493, 'validation/loss': 0.1253004133192221, 'validation/num_examples': 83274637, 'test/loss': 0.12773625989572626, 'test/num_examples': 95000000, 'score': 3247.6260638237, 'total_duration': 20024.54629135132, 'accumulated_submission_time': 3247.6260638237, 'accumulated_eval_time': 16752.44696354866, 'accumulated_logging_time': 0.7393789291381836, 'global_step': 3253, 'preemption_count': 0}), (3373, {'train/loss': 0.12316155323226276, 'validation/loss': 0.1250868167563791, 'validation/num_examples': 83274637, 'test/loss': 0.1273301917857923, 'test/num_examples': 95000000, 'score': 3367.9772799015045, 'total_duration': 20534.524040937424, 'accumulated_submission_time': 3367.9772799015045, 'accumulated_eval_time': 17141.162449359894, 'accumulated_logging_time': 0.7582144737243652, 'global_step': 3373, 'preemption_count': 0}), (3492, {'train/loss': 0.12536589375272184, 'validation/loss': 0.12493952061231464, 'validation/num_examples': 83274637, 'test/loss': 0.12726151162579186, 'test/num_examples': 95000000, 'score': 3488.3377919197083, 'total_duration': 21041.509493350983, 'accumulated_submission_time': 3488.3377919197083, 'accumulated_eval_time': 17526.873532772064, 'accumulated_logging_time': 0.7763538360595703, 'global_step': 3492, 'preemption_count': 0}), (3610, {'train/loss': 0.12413498955950457, 'validation/loss': 0.12511678360452258, 'validation/num_examples': 83274637, 'test/loss': 0.12746903855875918, 'test/num_examples': 95000000, 'score': 3608.4204602241516, 'total_duration': 21549.917605400085, 'accumulated_submission_time': 3608.4204602241516, 'accumulated_eval_time': 17914.313052415848, 'accumulated_logging_time': 0.8120391368865967, 'global_step': 3610, 'preemption_count': 0}), (3729, {'train/loss': 0.12328017278813147, 'validation/loss': 0.12493787949674454, 'validation/num_examples': 83274637, 'test/loss': 0.12737648322047182, 'test/num_examples': 95000000, 'score': 3728.5908682346344, 'total_duration': 22059.08487510681, 'accumulated_submission_time': 3728.5908682346344, 'accumulated_eval_time': 18302.335471391678, 'accumulated_logging_time': 0.8941290378570557, 'global_step': 3729, 'preemption_count': 0}), (3847, {'train/loss': 0.12592793947276107, 'validation/loss': 0.12479008274334377, 'validation/num_examples': 83274637, 'test/loss': 0.12704827024487947, 'test/num_examples': 95000000, 'score': 3848.9881432056427, 'total_duration': 22571.6878221035, 'accumulated_submission_time': 3848.9881432056427, 'accumulated_eval_time': 18693.62938308716, 'accumulated_logging_time': 0.9135735034942627, 'global_step': 3847, 'preemption_count': 0}), (3968, {'train/loss': 0.12345738324451955, 'validation/loss': 0.12488310064329237, 'validation/num_examples': 83274637, 'test/loss': 0.1273480261637236, 'test/num_examples': 95000000, 'score': 3969.272940158844, 'total_duration': 23084.364325761795, 'accumulated_submission_time': 3969.272940158844, 'accumulated_eval_time': 19085.127770662308, 'accumulated_logging_time': 0.9327123165130615, 'global_step': 3968, 'preemption_count': 0}), (4089, {'train/loss': 0.12250469551193956, 'validation/loss': 0.12500144729512205, 'validation/num_examples': 83274637, 'test/loss': 0.12741363510011372, 'test/num_examples': 95000000, 'score': 4089.9683067798615, 'total_duration': 23594.431305885315, 'accumulated_submission_time': 4089.9683067798615, 'accumulated_eval_time': 19473.613102674484, 'accumulated_logging_time': 0.9594626426696777, 'global_step': 4089, 'preemption_count': 0}), (4207, {'train/loss': 0.1229230084570743, 'validation/loss': 0.12496453158105235, 'validation/num_examples': 83274637, 'test/loss': 0.12731289096824244, 'test/num_examples': 95000000, 'score': 4210.821482658386, 'total_duration': 24104.659150838852, 'accumulated_submission_time': 4210.821482658386, 'accumulated_eval_time': 19862.087928533554, 'accumulated_logging_time': 0.9785106182098389, 'global_step': 4207, 'preemption_count': 0}), (4329, {'train/loss': 0.12398556189409277, 'validation/loss': 0.1247866769325142, 'validation/num_examples': 83274637, 'test/loss': 0.12730782007044741, 'test/num_examples': 95000000, 'score': 4331.883793115616, 'total_duration': 24615.73056745529, 'accumulated_submission_time': 4331.883793115616, 'accumulated_eval_time': 20251.205891132355, 'accumulated_logging_time': 0.9975364208221436, 'global_step': 4329, 'preemption_count': 0}), (4447, {'train/loss': 0.12487581839598826, 'validation/loss': 0.12463583014707705, 'validation/num_examples': 83274637, 'test/loss': 0.12692093808079769, 'test/num_examples': 95000000, 'score': 4452.722540378571, 'total_duration': 25124.620949983597, 'accumulated_submission_time': 4452.722540378571, 'accumulated_eval_time': 20638.35988521576, 'accumulated_logging_time': 1.0167925357818604, 'global_step': 4447, 'preemption_count': 0}), (4566, {'train/loss': 0.12309217113999131, 'validation/loss': 0.12471049510776996, 'validation/num_examples': 83274637, 'test/loss': 0.12712802523265637, 'test/num_examples': 95000000, 'score': 4572.588643789291, 'total_duration': 25632.061377763748, 'accumulated_submission_time': 4572.588643789291, 'accumulated_eval_time': 21025.051978349686, 'accumulated_logging_time': 1.0343739986419678, 'global_step': 4566, 'preemption_count': 0}), (4688, {'train/loss': 0.12356198981407786, 'validation/loss': 0.12463870838899889, 'validation/num_examples': 83274637, 'test/loss': 0.12700023189367998, 'test/num_examples': 95000000, 'score': 4692.90238904953, 'total_duration': 26141.061794757843, 'accumulated_submission_time': 4692.90238904953, 'accumulated_eval_time': 21412.8477370739, 'accumulated_logging_time': 1.0536458492279053, 'global_step': 4688, 'preemption_count': 0}), (4808, {'train/loss': 0.12193011551177099, 'validation/loss': 0.12458671936076056, 'validation/num_examples': 83274637, 'test/loss': 0.12701947590805857, 'test/num_examples': 95000000, 'score': 4813.704155445099, 'total_duration': 26651.518929243088, 'accumulated_submission_time': 4813.704155445099, 'accumulated_eval_time': 21801.53401017189, 'accumulated_logging_time': 1.155029296875, 'global_step': 4808, 'preemption_count': 0}), (4930, {'train/loss': 0.12356177344286132, 'validation/loss': 0.12448319022650305, 'validation/num_examples': 83274637, 'test/loss': 0.12682237285132159, 'test/num_examples': 95000000, 'score': 4934.240280151367, 'total_duration': 27160.92564678192, 'accumulated_submission_time': 4934.240280151367, 'accumulated_eval_time': 22189.538902521133, 'accumulated_logging_time': 1.1752219200134277, 'global_step': 4930, 'preemption_count': 0}), (5051, {'train/loss': 0.12272110379229738, 'validation/loss': 0.12499923404315055, 'validation/num_examples': 83274637, 'test/loss': 0.12748129179346185, 'test/num_examples': 95000000, 'score': 5054.439323663712, 'total_duration': 27669.933230400085, 'accumulated_submission_time': 5054.439323663712, 'accumulated_eval_time': 22577.494059324265, 'accumulated_logging_time': 1.1949405670166016, 'global_step': 5051, 'preemption_count': 0}), (5170, {'train/loss': 0.12135522890850292, 'validation/loss': 0.1244163494665454, 'validation/num_examples': 83274637, 'test/loss': 0.1268556314465573, 'test/num_examples': 95000000, 'score': 5175.086061477661, 'total_duration': 28179.362927675247, 'accumulated_submission_time': 5175.086061477661, 'accumulated_eval_time': 22965.444279670715, 'accumulated_logging_time': 1.2145025730133057, 'global_step': 5170, 'preemption_count': 0}), (5293, {'train/loss': 0.12123466394995579, 'validation/loss': 0.1243782672435212, 'validation/num_examples': 83274637, 'test/loss': 0.1267080962635241, 'test/num_examples': 95000000, 'score': 5294.916199207306, 'total_duration': 28685.52682518959, 'accumulated_submission_time': 5294.916199207306, 'accumulated_eval_time': 23350.906963586807, 'accumulated_logging_time': 1.2398149967193604, 'global_step': 5293, 'preemption_count': 0}), (5417, {'train/loss': 0.12217985026593522, 'validation/loss': 0.12429058499766185, 'validation/num_examples': 83274637, 'test/loss': 0.12676774083504927, 'test/num_examples': 95000000, 'score': 5414.87371969223, 'total_duration': 29193.422446489334, 'accumulated_submission_time': 5414.87371969223, 'accumulated_eval_time': 23737.957298994064, 'accumulated_logging_time': 1.2583882808685303, 'global_step': 5417, 'preemption_count': 0}), (5540, {'train/loss': 0.12285530136485176, 'validation/loss': 0.12453258623101815, 'validation/num_examples': 83274637, 'test/loss': 0.127008178195592, 'test/num_examples': 95000000, 'score': 5534.608779907227, 'total_duration': 29701.440313100815, 'accumulated_submission_time': 5534.608779907227, 'accumulated_eval_time': 24125.338285684586, 'accumulated_logging_time': 1.2776923179626465, 'global_step': 5540, 'preemption_count': 0}), (5663, {'train/loss': 0.12298277395521075, 'validation/loss': 0.12445934673369424, 'validation/num_examples': 83274637, 'test/loss': 0.1268309693783007, 'test/num_examples': 95000000, 'score': 5654.239840507507, 'total_duration': 30208.368506669998, 'accumulated_submission_time': 5654.239840507507, 'accumulated_eval_time': 24511.79935193062, 'accumulated_logging_time': 1.2971458435058594, 'global_step': 5663, 'preemption_count': 0}), (5787, {'train/loss': 0.11917405712239433, 'validation/loss': 0.12439383293324505, 'validation/num_examples': 83274637, 'test/loss': 0.126859343144467, 'test/num_examples': 95000000, 'score': 5775.0367612838745, 'total_duration': 30717.658841609955, 'accumulated_submission_time': 5775.0367612838745, 'accumulated_eval_time': 24899.393341064453, 'accumulated_logging_time': 1.3299486637115479, 'global_step': 5787, 'preemption_count': 0}), (5909, {'train/loss': 0.12340580258931076, 'validation/loss': 0.1245556802468622, 'validation/num_examples': 83274637, 'test/loss': 0.12710951797228362, 'test/num_examples': 95000000, 'score': 5895.0895302295685, 'total_duration': 31226.67712521553, 'accumulated_submission_time': 5895.0895302295685, 'accumulated_eval_time': 25287.485607862473, 'accumulated_logging_time': 1.349191427230835, 'global_step': 5909, 'preemption_count': 0}), (6029, {'train/loss': 0.12315157757012624, 'validation/loss': 0.12431556490713876, 'validation/num_examples': 83274637, 'test/loss': 0.1267030977824161, 'test/num_examples': 95000000, 'score': 6015.399273395538, 'total_duration': 31733.292843580246, 'accumulated_submission_time': 6015.399273395538, 'accumulated_eval_time': 25672.88678574562, 'accumulated_logging_time': 1.3677093982696533, 'global_step': 6029, 'preemption_count': 0}), (6150, {'train/loss': 0.12166755169459836, 'validation/loss': 0.12423944409555382, 'validation/num_examples': 83274637, 'test/loss': 0.12655657462495504, 'test/num_examples': 95000000, 'score': 6134.955957651138, 'total_duration': 32246.224467515945, 'accumulated_submission_time': 6134.955957651138, 'accumulated_eval_time': 26065.381944656372, 'accumulated_logging_time': 1.3858916759490967, 'global_step': 6150, 'preemption_count': 0}), (6271, {'train/loss': 0.12135904260610378, 'validation/loss': 0.12417653811950548, 'validation/num_examples': 83274637, 'test/loss': 0.12653266436466418, 'test/num_examples': 95000000, 'score': 6255.084678888321, 'total_duration': 32755.12862610817, 'accumulated_submission_time': 6255.084678888321, 'accumulated_eval_time': 26453.29153943062, 'accumulated_logging_time': 1.4204180240631104, 'global_step': 6271, 'preemption_count': 0}), (6394, {'train/loss': 0.12404263507709098, 'validation/loss': 0.1241142463495907, 'validation/num_examples': 83274637, 'test/loss': 0.12653920686436704, 'test/num_examples': 95000000, 'score': 6374.777480363846, 'total_duration': 33263.29815340042, 'accumulated_submission_time': 6374.777480363846, 'accumulated_eval_time': 26840.8960146904, 'accumulated_logging_time': 1.4390699863433838, 'global_step': 6394, 'preemption_count': 0}), (6516, {'train/loss': 0.12135663798045732, 'validation/loss': 0.12415910087553349, 'validation/num_examples': 83274637, 'test/loss': 0.1264881684854608, 'test/num_examples': 95000000, 'score': 6494.826027393341, 'total_duration': 33771.30974674225, 'accumulated_submission_time': 6494.826027393341, 'accumulated_eval_time': 27227.982669353485, 'accumulated_logging_time': 1.4568839073181152, 'global_step': 6516, 'preemption_count': 0}), (6637, {'train/loss': 0.12285944768225805, 'validation/loss': 0.12405932304511992, 'validation/num_examples': 83274637, 'test/loss': 0.12632373783962852, 'test/num_examples': 95000000, 'score': 6615.15429520607, 'total_duration': 34278.417143821716, 'accumulated_submission_time': 6615.15429520607, 'accumulated_eval_time': 27613.900813102722, 'accumulated_logging_time': 1.4759395122528076, 'global_step': 6637, 'preemption_count': 0}), (6758, {'train/loss': 0.12375294668772192, 'validation/loss': 0.12403767903251268, 'validation/num_examples': 83274637, 'test/loss': 0.12633192928619386, 'test/num_examples': 95000000, 'score': 6735.238496303558, 'total_duration': 34786.704961299896, 'accumulated_submission_time': 6735.238496303558, 'accumulated_eval_time': 28001.21543264389, 'accumulated_logging_time': 1.5407261848449707, 'global_step': 6758, 'preemption_count': 0}), (6883, {'train/loss': 0.11919287744279787, 'validation/loss': 0.12392968877975175, 'validation/num_examples': 83274637, 'test/loss': 0.1261584628999409, 'test/num_examples': 95000000, 'score': 6855.638538599014, 'total_duration': 35295.501693964005, 'accumulated_submission_time': 6855.638538599014, 'accumulated_eval_time': 28388.728944063187, 'accumulated_logging_time': 1.5613288879394531, 'global_step': 6883, 'preemption_count': 0}), (7004, {'train/loss': 0.12444886490146548, 'validation/loss': 0.12396466572273408, 'validation/num_examples': 83274637, 'test/loss': 0.12636229081726075, 'test/num_examples': 95000000, 'score': 6976.12876868248, 'total_duration': 35804.7186357975, 'accumulated_submission_time': 6976.12876868248, 'accumulated_eval_time': 28776.637721776962, 'accumulated_logging_time': 1.5809059143066406, 'global_step': 7004, 'preemption_count': 0}), (7125, {'train/loss': 0.12493567109146815, 'validation/loss': 0.12404675320626427, 'validation/num_examples': 83274637, 'test/loss': 0.12644923569472463, 'test/num_examples': 95000000, 'score': 7096.604103088379, 'total_duration': 36314.520011901855, 'accumulated_submission_time': 7096.604103088379, 'accumulated_eval_time': 29165.082663297653, 'accumulated_logging_time': 1.6008546352386475, 'global_step': 7125, 'preemption_count': 0}), (7245, {'train/loss': 0.12484628254435583, 'validation/loss': 0.12400216520401706, 'validation/num_examples': 83274637, 'test/loss': 0.12630540111172325, 'test/num_examples': 95000000, 'score': 7216.238992929459, 'total_duration': 36820.79420351982, 'accumulated_submission_time': 7216.238992929459, 'accumulated_eval_time': 29550.752522945404, 'accumulated_logging_time': 1.6964330673217773, 'global_step': 7245, 'preemption_count': 0}), (7364, {'train/loss': 0.12225591461984951, 'validation/loss': 0.12398705394431331, 'validation/num_examples': 83274637, 'test/loss': 0.1262943579285471, 'test/num_examples': 95000000, 'score': 7337.275809288025, 'total_duration': 37330.18508911133, 'accumulated_submission_time': 7337.275809288025, 'accumulated_eval_time': 29938.265198469162, 'accumulated_logging_time': 1.7156744003295898, 'global_step': 7364, 'preemption_count': 0}), (7481, {'train/loss': 0.12115653709049688, 'validation/loss': 0.12401164670955134, 'validation/num_examples': 83274637, 'test/loss': 0.12629014404734562, 'test/num_examples': 95000000, 'score': 7457.494492053986, 'total_duration': 37839.75578594208, 'accumulated_submission_time': 7457.494492053986, 'accumulated_eval_time': 30326.757284402847, 'accumulated_logging_time': 1.7349376678466797, 'global_step': 7481, 'preemption_count': 0}), (7602, {'train/loss': 0.1207529849799897, 'validation/loss': 0.1238975946431092, 'validation/num_examples': 83274637, 'test/loss': 0.12611857605100932, 'test/num_examples': 95000000, 'score': 7577.397085428238, 'total_duration': 38352.174194812775, 'accumulated_submission_time': 7577.397085428238, 'accumulated_eval_time': 30718.333451271057, 'accumulated_logging_time': 1.7828757762908936, 'global_step': 7602, 'preemption_count': 0}), (7723, {'train/loss': 0.12375526981911608, 'validation/loss': 0.12395954214548215, 'validation/num_examples': 83274637, 'test/loss': 0.12627279322766755, 'test/num_examples': 95000000, 'score': 7697.026575803757, 'total_duration': 38861.01707100868, 'accumulated_submission_time': 7697.026575803757, 'accumulated_eval_time': 31106.66635465622, 'accumulated_logging_time': 1.8026072978973389, 'global_step': 7723, 'preemption_count': 0})], 'global_step': 7841}
I0317 02:31:24.050530 140181208745152 submission_runner.py:649] Timing: 7816.6496477127075
I0317 02:31:24.050570 140181208745152 submission_runner.py:651] Total number of evals: 65
I0317 02:31:24.050602 140181208745152 submission_runner.py:652] ====================
I0317 02:31:24.050730 140181208745152 submission_runner.py:750] Final criteo1tb score: 4
