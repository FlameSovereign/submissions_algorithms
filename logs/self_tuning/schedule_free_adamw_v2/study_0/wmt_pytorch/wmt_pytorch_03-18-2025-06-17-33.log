torchrun --redirects 1:0,2:0,3:0,4:0,5:0,6:0,7:0 --standalone --nnodes=1 --nproc_per_node=8 submission_runner.py --framework=pytorch --workload=wmt --submission_path=submissions_algorithms/submissions/self_tuning/schedule_free_adamw_v2/submission.py --data_dir=/data/wmt --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=submissions/rolling_leaderboard/self_tuning/schedule_free_adamw_v2/study_0 --overwrite=True --save_checkpoints=False --rng_seed=-2101048796 --torch_compile=true --tuning_ruleset=self 2>&1 | tee -a /logs/wmt_pytorch_03-18-2025-06-17-33.log
W0318 06:17:34.853000 8 site-packages/torch/distributed/run.py:793] 
W0318 06:17:34.853000 8 site-packages/torch/distributed/run.py:793] *****************************************
W0318 06:17:34.853000 8 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0318 06:17:34.853000 8 site-packages/torch/distributed/run.py:793] *****************************************
2025-03-18 06:17:36.029059: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-03-18 06:17:36.029056: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-03-18 06:17:36.029073: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-03-18 06:17:36.029060: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-03-18 06:17:36.029092: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-03-18 06:17:36.029102: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-03-18 06:17:36.029132: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-03-18 06:17:36.029505: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1742278656.049619      50 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1742278656.049623      46 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1742278656.049618      44 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1742278656.049619      47 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1742278656.049618      49 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1742278656.049627      48 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1742278656.049620      43 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1742278656.051268      45 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1742278656.056172      47 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
E0000 00:00:1742278656.056175      46 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
E0000 00:00:1742278656.056181      44 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
E0000 00:00:1742278656.056195      48 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
E0000 00:00:1742278656.056204      43 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
E0000 00:00:1742278656.056206      50 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
E0000 00:00:1742278656.056274      49 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
E0000 00:00:1742278656.058201      45 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
[rank0]:[W318 06:17:44.346115879 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank1]:[W318 06:17:44.798130175 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank2]:[W318 06:17:44.798907695 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank5]:[W318 06:17:44.811299820 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 5]  using GPU 5 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank3]:[W318 06:17:44.811543069 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank6]:[W318 06:17:44.855024526 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 6]  using GPU 6 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank4]:[W318 06:17:44.857280347 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 4]  using GPU 4 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank7]:[W318 06:17:44.859091743 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 7]  using GPU 7 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
I0318 06:17:46.622025 140344243979456 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/self_tuning/schedule_free_adamw_v2/study_0/wmt_pytorch.
I0318 06:17:46.622029 140475682948288 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/self_tuning/schedule_free_adamw_v2/study_0/wmt_pytorch.
I0318 06:17:46.622025 139939588261056 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/self_tuning/schedule_free_adamw_v2/study_0/wmt_pytorch.
I0318 06:17:46.622025 140442017244352 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/self_tuning/schedule_free_adamw_v2/study_0/wmt_pytorch.
I0318 06:17:46.622025 139861633238208 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/self_tuning/schedule_free_adamw_v2/study_0/wmt_pytorch.
I0318 06:17:46.622029 140413665572032 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/self_tuning/schedule_free_adamw_v2/study_0/wmt_pytorch.
I0318 06:17:46.622032 139788152231104 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/self_tuning/schedule_free_adamw_v2/study_0/wmt_pytorch.
I0318 06:17:46.622121 140534512448704 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/self_tuning/schedule_free_adamw_v2/study_0/wmt_pytorch.
I0318 06:17:46.661115 140344243979456 submission_runner.py:665] Creating directory at /experiment_runs/submissions/rolling_leaderboard/self_tuning/schedule_free_adamw_v2/study_0/wmt_pytorch/trial_1.
I0318 06:17:46.951438 140344243979456 submission_runner.py:218] Initializing dataset.
[rank5]: Traceback (most recent call last):
[rank5]:   File "/algorithmic-efficiency/submission_runner.py", line 766, in <module>
[rank5]:     app.run(main)
[rank5]:   File "/usr/local/lib/python3.11/site-packages/absl/app.py", line 308, in run
[rank5]:     _run_main(main, args)
[rank5]:   File "/usr/local/lib/python3.11/site-packages/absl/app.py", line 254, in _run_main
[rank5]:     sys.exit(main(argv))
[rank5]:              ^^^^^^^^^^
[rank5]:   File "/algorithmic-efficiency/submission_runner.py", line 734, in main
[rank5]:     score = score_submission_on_workload(
[rank5]:             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank5]:   File "/algorithmic-efficiency/submission_runner.py", line 668, in score_submission_on_workload
[rank5]:     score, _ = train_once(
[rank5]:                ^^^^^^^^^^^
[rank5]:   File "/algorithmic-efficiency/submission_runner.py", line 223, in train_once
[rank5]:     with profiler.profile('Initializing dataset'):
[rank5]:          ^^^^^^^^^^^^^^^^
[rank5]: AttributeError: 'NoneType' object has no attribute 'profile'
[rank0]: Traceback (most recent call last):
[rank0]:   File "/algorithmic-efficiency/submission_runner.py", line 766, in <module>
[rank0]:     app.run(main)
[rank0]:   File "/usr/local/lib/python3.11/site-packages/absl/app.py", line 308, in run
[rank0]:     _run_main(main, args)
[rank0]:   File "/usr/local/lib/python3.11/site-packages/absl/app.py", line 254, in _run_main
[rank0]:     sys.exit(main(argv))
[rank0]:              ^^^^^^^^^^
[rank0]:   File "/algorithmic-efficiency/submission_runner.py", line 734, in main
[rank0]:     score = score_submission_on_workload(
[rank0]:             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/algorithmic-efficiency/submission_runner.py", line 668, in score_submission_on_workload
[rank0]:     score, _ = train_once(
[rank0]:                ^^^^^^^^^^^
[rank0]:   File "/algorithmic-efficiency/submission_runner.py", line 223, in train_once
[rank0]:     with profiler.profile('Initializing dataset'):
[rank0]:          ^^^^^^^^^^^^^^^^
[rank0]: AttributeError: 'NoneType' object has no attribute 'profile'
[rank1]: Traceback (most recent call last):
[rank1]:   File "/algorithmic-efficiency/submission_runner.py", line 766, in <module>
[rank1]:     app.run(main)
[rank1]:   File "/usr/local/lib/python3.11/site-packages/absl/app.py", line 308, in run
[rank1]:     _run_main(main, args)
[rank1]:   File "/usr/local/lib/python3.11/site-packages/absl/app.py", line 254, in _run_main
[rank1]:     sys.exit(main(argv))
[rank1]:              ^^^^^^^^^^
[rank1]:   File "/algorithmic-efficiency/submission_runner.py", line 734, in main
[rank1]:     score = score_submission_on_workload(
[rank1]:             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/algorithmic-efficiency/submission_runner.py", line 668, in score_submission_on_workload
[rank1]:     score, _ = train_once(
[rank1]:                ^^^^^^^^^^^
[rank1]:   File "/algorithmic-efficiency/submission_runner.py", line 223, in train_once
[rank1]:     with profiler.profile('Initializing dataset'):
[rank1]:          ^^^^^^^^^^^^^^^^
[rank1]: AttributeError: 'NoneType' object has no attribute 'profile'
[rank3]: Traceback (most recent call last):
[rank3]:   File "/algorithmic-efficiency/submission_runner.py", line 766, in <module>
[rank3]:     app.run(main)
[rank3]:   File "/usr/local/lib/python3.11/site-packages/absl/app.py", line 308, in run
[rank3]:     _run_main(main, args)
[rank3]:   File "/usr/local/lib/python3.11/site-packages/absl/app.py", line 254, in _run_main
[rank3]:     sys.exit(main(argv))
[rank3]:              ^^^^^^^^^^
[rank3]:   File "/algorithmic-efficiency/submission_runner.py", line 734, in main
[rank3]:     score = score_submission_on_workload(
[rank3]:             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/algorithmic-efficiency/submission_runner.py", line 668, in score_submission_on_workload
[rank3]:     score, _ = train_once(
[rank3]:                ^^^^^^^^^^^
[rank3]:   File "/algorithmic-efficiency/submission_runner.py", line 223, in train_once
[rank3]:     with profiler.profile('Initializing dataset'):
[rank3]:          ^^^^^^^^^^^^^^^^
[rank3]: AttributeError: 'NoneType' object has no attribute 'profile'
[rank2]: Traceback (most recent call last):
[rank2]:   File "/algorithmic-efficiency/submission_runner.py", line 766, in <module>
[rank2]:     app.run(main)
[rank2]:   File "/usr/local/lib/python3.11/site-packages/absl/app.py", line 308, in run
[rank2]:     _run_main(main, args)
[rank2]:   File "/usr/local/lib/python3.11/site-packages/absl/app.py", line 254, in _run_main
[rank2]:     sys.exit(main(argv))
[rank2]:              ^^^^^^^^^^
[rank2]:   File "/algorithmic-efficiency/submission_runner.py", line 734, in main
[rank2]:     score = score_submission_on_workload(
[rank2]:             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/algorithmic-efficiency/submission_runner.py", line 668, in score_submission_on_workload
[rank2]:     score, _ = train_once(
[rank2]:                ^^^^^^^^^^^
[rank2]:   File "/algorithmic-efficiency/submission_runner.py", line 223, in train_once
[rank2]:     with profiler.profile('Initializing dataset'):
[rank2]:          ^^^^^^^^^^^^^^^^
[rank2]: AttributeError: 'NoneType' object has no attribute 'profile'
[rank7]: Traceback (most recent call last):
[rank7]:   File "/algorithmic-efficiency/submission_runner.py", line 766, in <module>
[rank7]:     app.run(main)
[rank7]:   File "/usr/local/lib/python3.11/site-packages/absl/app.py", line 308, in run
[rank7]:     _run_main(main, args)
[rank7]:   File "/usr/local/lib/python3.11/site-packages/absl/app.py", line 254, in _run_main
[rank7]:     sys.exit(main(argv))
[rank7]:              ^^^^^^^^^^
[rank7]:   File "/algorithmic-efficiency/submission_runner.py", line 734, in main
[rank7]:     score = score_submission_on_workload(
[rank7]:             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank7]:   File "/algorithmic-efficiency/submission_runner.py", line 668, in score_submission_on_workload
[rank7]:     score, _ = train_once(
[rank7]:                ^^^^^^^^^^^
[rank7]:   File "/algorithmic-efficiency/submission_runner.py", line 223, in train_once
[rank7]:     with profiler.profile('Initializing dataset'):
[rank7]:          ^^^^^^^^^^^^^^^^
[rank7]: AttributeError: 'NoneType' object has no attribute 'profile'
[rank6]: Traceback (most recent call last):
[rank6]:   File "/algorithmic-efficiency/submission_runner.py", line 766, in <module>
[rank6]:     app.run(main)
[rank6]:   File "/usr/local/lib/python3.11/site-packages/absl/app.py", line 308, in run
[rank6]:     _run_main(main, args)
[rank6]:   File "/usr/local/lib/python3.11/site-packages/absl/app.py", line 254, in _run_main
[rank6]:     sys.exit(main(argv))
[rank6]:              ^^^^^^^^^^
[rank6]:   File "/algorithmic-efficiency/submission_runner.py", line 734, in main
[rank6]:     score = score_submission_on_workload(
[rank6]:             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank6]:   File "/algorithmic-efficiency/submission_runner.py", line 668, in score_submission_on_workload
[rank6]:     score, _ = train_once(
[rank6]:                ^^^^^^^^^^^
[rank6]:   File "/algorithmic-efficiency/submission_runner.py", line 223, in train_once
[rank6]:     with profiler.profile('Initializing dataset'):
[rank6]:          ^^^^^^^^^^^^^^^^
[rank6]: AttributeError: 'NoneType' object has no attribute 'profile'
[rank4]: Traceback (most recent call last):
[rank4]:   File "/algorithmic-efficiency/submission_runner.py", line 766, in <module>
[rank4]:     app.run(main)
[rank4]:   File "/usr/local/lib/python3.11/site-packages/absl/app.py", line 308, in run
[rank4]:     _run_main(main, args)
[rank4]:   File "/usr/local/lib/python3.11/site-packages/absl/app.py", line 254, in _run_main
[rank4]:     sys.exit(main(argv))
[rank4]:              ^^^^^^^^^^
[rank4]:   File "/algorithmic-efficiency/submission_runner.py", line 734, in main
[rank4]:     score = score_submission_on_workload(
[rank4]:             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank4]:   File "/algorithmic-efficiency/submission_runner.py", line 668, in score_submission_on_workload
[rank4]:     score, _ = train_once(
[rank4]:                ^^^^^^^^^^^
[rank4]:   File "/algorithmic-efficiency/submission_runner.py", line 223, in train_once
[rank4]:     with profiler.profile('Initializing dataset'):
[rank4]:          ^^^^^^^^^^^^^^^^
[rank4]: AttributeError: 'NoneType' object has no attribute 'profile'
[rank0]:[W318 06:17:48.107782896 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
W0318 06:17:48.880000 8 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 43 closing signal SIGTERM
W0318 06:17:48.880000 8 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 44 closing signal SIGTERM
W0318 06:17:48.881000 8 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 46 closing signal SIGTERM
W0318 06:17:48.881000 8 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 47 closing signal SIGTERM
W0318 06:17:48.881000 8 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 48 closing signal SIGTERM
W0318 06:17:48.881000 8 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 49 closing signal SIGTERM
W0318 06:17:48.881000 8 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 50 closing signal SIGTERM
E0318 06:17:49.493000 8 site-packages/torch/distributed/elastic/multiprocessing/api.py:869] failed (exitcode: 1) local_rank: 2 (pid: 45) of binary: /usr/local/bin/python3.11
Traceback (most recent call last):
  File "/usr/local/bin/torchrun", line 8, in <module>
    sys.exit(main())
             ^^^^^^
  File "/usr/local/lib/python3.11/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/site-packages/torch/distributed/run.py", line 919, in main
    run(args)
  File "/usr/local/lib/python3.11/site-packages/torch/distributed/run.py", line 910, in run
    elastic_launch(
  File "/usr/local/lib/python3.11/site-packages/torch/distributed/launcher/api.py", line 138, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/site-packages/torch/distributed/launcher/api.py", line 269, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
submission_runner.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-03-18_06:17:48
  host      : 6d2753c29e1b
  rank      : 2 (local_rank: 2)
  exitcode  : 1 (pid: 45)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
