torchrun --redirects 1:0,2:0,3:0,4:0,5:0,6:0,7:0 --standalone --nnodes=1 --nproc_per_node=8 submission_runner.py --framework=pytorch --workload=librispeech_deepspeech --submission_path=submissions_algorithms/submissions/self_tuning/schedule_free_adamw_v2/submission.py --data_dir=/data/librispeech --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=submissions/rolling_leaderboard/self_tuning/schedule_free_adamw_v2/study_0 --overwrite=True --save_checkpoints=False --rng_seed=-1899447686 --librispeech_tokenizer_vocab_path=/data/librispeech/spm_model.vocab --torch_compile=true --tuning_ruleset=self 2>&1 | tee -a /logs/librispeech_deepspeech_pytorch_03-18-2025-06-18-08.log
W0318 06:18:10.053000 9 site-packages/torch/distributed/run.py:793] 
W0318 06:18:10.053000 9 site-packages/torch/distributed/run.py:793] *****************************************
W0318 06:18:10.053000 9 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0318 06:18:10.053000 9 site-packages/torch/distributed/run.py:793] *****************************************
2025-03-18 06:18:11.132960: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-03-18 06:18:11.132950: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-03-18 06:18:11.132948: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-03-18 06:18:11.132948: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-03-18 06:18:11.132948: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-03-18 06:18:11.132957: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-03-18 06:18:11.132950: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-03-18 06:18:11.133372: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1742278691.153124      49 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1742278691.153121      47 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1742278691.153124      44 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1742278691.153127      48 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1742278691.153129      45 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1742278691.153123      51 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1742278691.153169      50 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1742278691.153974      46 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1742278691.159285      51 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
E0000 00:00:1742278691.159280      44 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
E0000 00:00:1742278691.159290      47 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
E0000 00:00:1742278691.159286      49 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
E0000 00:00:1742278691.159292      48 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
E0000 00:00:1742278691.159289      50 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
E0000 00:00:1742278691.159349      45 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
E0000 00:00:1742278691.160288      46 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
ERROR:root:Unable to import wandb.
Traceback (most recent call last):
  File "/algorithmic-efficiency/algoperf/logger_utils.py", line 27, in <module>
    import wandb  # pylint: disable=g-import-not-at-top
    ^^^^^^^^^^^^
ModuleNotFoundError: No module named 'wandb'
[rank1]:[W318 06:18:18.224202972 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank7]:[W318 06:18:18.242776657 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 7]  using GPU 7 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank4]:[W318 06:18:18.337062622 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 4]  using GPU 4 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank0]:[W318 06:18:18.384720333 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank2]:[W318 06:18:18.413177517 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank6]:[W318 06:18:18.423405856 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 6]  using GPU 6 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank5]:[W318 06:18:18.478852959 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 5]  using GPU 5 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank3]:[W318 06:18:18.498112880 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
I0318 06:18:20.327959 140525608973504 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/self_tuning/schedule_free_adamw_v2/study_0/librispeech_deepspeech_pytorch.
I0318 06:18:20.327959 139999575221440 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/self_tuning/schedule_free_adamw_v2/study_0/librispeech_deepspeech_pytorch.
I0318 06:18:20.327957 140275714041024 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/self_tuning/schedule_free_adamw_v2/study_0/librispeech_deepspeech_pytorch.
I0318 06:18:20.327958 140243771311296 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/self_tuning/schedule_free_adamw_v2/study_0/librispeech_deepspeech_pytorch.
I0318 06:18:20.327962 140252672767168 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/self_tuning/schedule_free_adamw_v2/study_0/librispeech_deepspeech_pytorch.
I0318 06:18:20.327958 139638559016128 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/self_tuning/schedule_free_adamw_v2/study_0/librispeech_deepspeech_pytorch.
I0318 06:18:20.327968 139668002682048 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/self_tuning/schedule_free_adamw_v2/study_0/librispeech_deepspeech_pytorch.
I0318 06:18:20.328026 140511280227520 logger_utils.py:81] Creating experiment directory at /experiment_runs/submissions/rolling_leaderboard/self_tuning/schedule_free_adamw_v2/study_0/librispeech_deepspeech_pytorch.
I0318 06:18:20.370715 139999575221440 submission_runner.py:665] Creating directory at /experiment_runs/submissions/rolling_leaderboard/self_tuning/schedule_free_adamw_v2/study_0/librispeech_deepspeech_pytorch/trial_1.
[rank2]: Traceback (most recent call last):
[rank2]:   File "/algorithmic-efficiency/submission_runner.py", line 766, in <module>
[rank2]:     app.run(main)
[rank2]:   File "/usr/local/lib/python3.11/site-packages/absl/app.py", line 308, in run
[rank2]:     _run_main(main, args)
[rank2]:   File "/usr/local/lib/python3.11/site-packages/absl/app.py", line 254, in _run_main
[rank2]:     sys.exit(main(argv))
[rank2]:              ^^^^^^^^^^
[rank2]:   File "/algorithmic-efficiency/submission_runner.py", line 734, in main
[rank2]:     score = score_submission_on_workload(
[rank2]:             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/algorithmic-efficiency/submission_runner.py", line 668, in score_submission_on_workload
[rank2]:     score, _ = train_once(
[rank2]:                ^^^^^^^^^^^
[rank2]:   File "/algorithmic-efficiency/submission_runner.py", line 223, in train_once
[rank2]:     with profiler.profile('Initializing dataset'):
[rank2]:          ^^^^^^^^^^^^^^^^
[rank2]: AttributeError: 'NoneType' object has no attribute 'profile'
[rank5]: Traceback (most recent call last):
[rank5]:   File "/algorithmic-efficiency/submission_runner.py", line 766, in <module>
[rank5]:     app.run(main)
[rank5]:   File "/usr/local/lib/python3.11/site-packages/absl/app.py", line 308, in run
[rank5]:     _run_main(main, args)
[rank5]:   File "/usr/local/lib/python3.11/site-packages/absl/app.py", line 254, in _run_main
[rank5]:     sys.exit(main(argv))
[rank5]:              ^^^^^^^^^^
[rank5]:   File "/algorithmic-efficiency/submission_runner.py", line 734, in main
[rank5]:     score = score_submission_on_workload(
[rank5]:             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank5]:   File "/algorithmic-efficiency/submission_runner.py", line 668, in score_submission_on_workload
[rank5]:     score, _ = train_once(
[rank5]:                ^^^^^^^^^^^
[rank5]:   File "/algorithmic-efficiency/submission_runner.py", line 223, in train_once
[rank5]:     with profiler.profile('Initializing dataset'):
[rank5]:          ^^^^^^^^^^^^^^^^
[rank5]: AttributeError: 'NoneType' object has no attribute 'profile'
[rank4]: Traceback (most recent call last):
[rank4]:   File "/algorithmic-efficiency/submission_runner.py", line 766, in <module>
[rank4]:     app.run(main)
[rank4]:   File "/usr/local/lib/python3.11/site-packages/absl/app.py", line 308, in run
[rank4]:     _run_main(main, args)
[rank4]:   File "/usr/local/lib/python3.11/site-packages/absl/app.py", line 254, in _run_main
[rank4]:     sys.exit(main(argv))
[rank4]:              ^^^^^^^^^^
[rank4]:   File "/algorithmic-efficiency/submission_runner.py", line 734, in main
[rank4]:     score = score_submission_on_workload(
[rank4]:             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank4]:   File "/algorithmic-efficiency/submission_runner.py", line 668, in score_submission_on_workload
[rank4]:     score, _ = train_once(
[rank4]:                ^^^^^^^^^^^
[rank4]:   File "/algorithmic-efficiency/submission_runner.py", line 223, in train_once
[rank4]:     with profiler.profile('Initializing dataset'):
[rank4]:          ^^^^^^^^^^^^^^^^
[rank4]: AttributeError: 'NoneType' object has no attribute 'profile'
[rank1]: Traceback (most recent call last):
[rank1]:   File "/algorithmic-efficiency/submission_runner.py", line 766, in <module>
[rank1]:     app.run(main)
[rank1]:   File "/usr/local/lib/python3.11/site-packages/absl/app.py", line 308, in run
[rank1]:     _run_main(main, args)
[rank1]:   File "/usr/local/lib/python3.11/site-packages/absl/app.py", line 254, in _run_main
[rank1]:     sys.exit(main(argv))
[rank1]:              ^^^^^^^^^^
[rank1]:   File "/algorithmic-efficiency/submission_runner.py", line 734, in main
[rank1]:     score = score_submission_on_workload(
[rank1]:             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/algorithmic-efficiency/submission_runner.py", line 668, in score_submission_on_workload
[rank1]:     score, _ = train_once(
[rank1]:                ^^^^^^^^^^^
[rank1]:   File "/algorithmic-efficiency/submission_runner.py", line 223, in train_once
[rank1]:     with profiler.profile('Initializing dataset'):
[rank1]:          ^^^^^^^^^^^^^^^^
[rank1]: AttributeError: 'NoneType' object has no attribute 'profile'
[rank3]: Traceback (most recent call last):
[rank3]:   File "/algorithmic-efficiency/submission_runner.py", line 766, in <module>
[rank3]:     app.run(main)
[rank3]:   File "/usr/local/lib/python3.11/site-packages/absl/app.py", line 308, in run
[rank3]:     _run_main(main, args)
[rank3]:   File "/usr/local/lib/python3.11/site-packages/absl/app.py", line 254, in _run_main
[rank3]:     sys.exit(main(argv))
[rank3]:              ^^^^^^^^^^
[rank3]:   File "/algorithmic-efficiency/submission_runner.py", line 734, in main
[rank3]:     score = score_submission_on_workload(
[rank3]:             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/algorithmic-efficiency/submission_runner.py", line 668, in score_submission_on_workload
[rank3]:     score, _ = train_once(
[rank3]:                ^^^^^^^^^^^
[rank3]:   File "/algorithmic-efficiency/submission_runner.py", line 223, in train_once
[rank3]:     with profiler.profile('Initializing dataset'):
[rank3]:          ^^^^^^^^^^^^^^^^
[rank3]: AttributeError: 'NoneType' object has no attribute 'profile'
[rank7]: Traceback (most recent call last):
[rank7]:   File "/algorithmic-efficiency/submission_runner.py", line 766, in <module>
[rank7]:     app.run(main)
[rank7]:   File "/usr/local/lib/python3.11/site-packages/absl/app.py", line 308, in run
[rank7]:     _run_main(main, args)
[rank7]:   File "/usr/local/lib/python3.11/site-packages/absl/app.py", line 254, in _run_main
[rank7]:     sys.exit(main(argv))
[rank7]:              ^^^^^^^^^^
[rank7]:   File "/algorithmic-efficiency/submission_runner.py", line 734, in main
[rank7]:     score = score_submission_on_workload(
[rank7]:             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank7]:   File "/algorithmic-efficiency/submission_runner.py", line 668, in score_submission_on_workload
[rank7]:     score, _ = train_once(
[rank7]:                ^^^^^^^^^^^
[rank7]:   File "/algorithmic-efficiency/submission_runner.py", line 223, in train_once
[rank7]:     with profiler.profile('Initializing dataset'):
[rank7]:          ^^^^^^^^^^^^^^^^
[rank7]: AttributeError: 'NoneType' object has no attribute 'profile'
[rank6]: Traceback (most recent call last):
[rank6]:   File "/algorithmic-efficiency/submission_runner.py", line 766, in <module>
[rank6]:     app.run(main)
[rank6]:   File "/usr/local/lib/python3.11/site-packages/absl/app.py", line 308, in run
[rank6]:     _run_main(main, args)
[rank6]:   File "/usr/local/lib/python3.11/site-packages/absl/app.py", line 254, in _run_main
[rank6]:     sys.exit(main(argv))
[rank6]:              ^^^^^^^^^^
[rank6]:   File "/algorithmic-efficiency/submission_runner.py", line 734, in main
[rank6]:     score = score_submission_on_workload(
[rank6]:             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank6]:   File "/algorithmic-efficiency/submission_runner.py", line 668, in score_submission_on_workload
[rank6]:     score, _ = train_once(
[rank6]:                ^^^^^^^^^^^
[rank6]:   File "/algorithmic-efficiency/submission_runner.py", line 223, in train_once
[rank6]:     with profiler.profile('Initializing dataset'):
[rank6]:          ^^^^^^^^^^^^^^^^
[rank6]: AttributeError: 'NoneType' object has no attribute 'profile'
I0318 06:18:20.680647 139999575221440 submission_runner.py:218] Initializing dataset.
[rank0]: Traceback (most recent call last):
[rank0]:   File "/algorithmic-efficiency/submission_runner.py", line 766, in <module>
[rank0]:     app.run(main)
[rank0]:   File "/usr/local/lib/python3.11/site-packages/absl/app.py", line 308, in run
[rank0]:     _run_main(main, args)
[rank0]:   File "/usr/local/lib/python3.11/site-packages/absl/app.py", line 254, in _run_main
[rank0]:     sys.exit(main(argv))
[rank0]:              ^^^^^^^^^^
[rank0]:   File "/algorithmic-efficiency/submission_runner.py", line 734, in main
[rank0]:     score = score_submission_on_workload(
[rank0]:             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/algorithmic-efficiency/submission_runner.py", line 668, in score_submission_on_workload
[rank0]:     score, _ = train_once(
[rank0]:                ^^^^^^^^^^^
[rank0]:   File "/algorithmic-efficiency/submission_runner.py", line 223, in train_once
[rank0]:     with profiler.profile('Initializing dataset'):
[rank0]:          ^^^^^^^^^^^^^^^^
[rank0]: AttributeError: 'NoneType' object has no attribute 'profile'
[rank0]:[W318 06:18:21.788298420 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
W0318 06:18:22.959000 9 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 44 closing signal SIGTERM
W0318 06:18:22.960000 9 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 45 closing signal SIGTERM
W0318 06:18:22.960000 9 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 46 closing signal SIGTERM
W0318 06:18:22.960000 9 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 47 closing signal SIGTERM
W0318 06:18:22.960000 9 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 48 closing signal SIGTERM
W0318 06:18:22.960000 9 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 49 closing signal SIGTERM
W0318 06:18:22.961000 9 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 51 closing signal SIGTERM
E0318 06:18:23.326000 9 site-packages/torch/distributed/elastic/multiprocessing/api.py:869] failed (exitcode: 1) local_rank: 6 (pid: 50) of binary: /usr/local/bin/python3.11
Traceback (most recent call last):
  File "/usr/local/bin/torchrun", line 8, in <module>
    sys.exit(main())
             ^^^^^^
  File "/usr/local/lib/python3.11/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/site-packages/torch/distributed/run.py", line 919, in main
    run(args)
  File "/usr/local/lib/python3.11/site-packages/torch/distributed/run.py", line 910, in run
    elastic_launch(
  File "/usr/local/lib/python3.11/site-packages/torch/distributed/launcher/api.py", line 138, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/site-packages/torch/distributed/launcher/api.py", line 269, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
submission_runner.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-03-18_06:18:22
  host      : 46758e90d643
  rank      : 6 (local_rank: 6)
  exitcode  : 1 (pid: 50)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
